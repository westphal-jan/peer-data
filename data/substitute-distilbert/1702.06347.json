{"id": "1702.06347", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Scalable Demand-Aware Recommendation", "abstract": "recommendation for e - commerce with a mix of durable and convenient goods has characteristics that distinguish it from the well - informed media recommendation problem. the demand for items is a combined effect of form utility and time utility, can. e., a product must additionally be intrinsically appealing to a consumer and the time must occur relevant for purchase. in evaluation for personal goods, cost correlation is a function of inter - purchase duration within product category because respondents are unlikely when purchase two options in the same category in close click succession. moreover, purchase data, in contrast to ratings data, is implicit with non - purchases not necessarily indicating products. together, these issues give rise if the positive - unlabeled demand - selection recommendation problem that we pose via joint low - rank tensor completion and product category inter - shipment duration vector estimation. we further relax this problem and propose a highly structured alternating likelihood approach with which we can solve problems with millions of users and items. we also show superior prediction accuracies near multiple real - world data sets.", "histories": [["v1", "Tue, 21 Feb 2017 12:10:28 GMT  (67kb,D)", "http://arxiv.org/abs/1702.06347v1", null], ["v2", "Tue, 23 May 2017 04:05:35 GMT  (68kb,D)", "http://arxiv.org/abs/1702.06347v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jinfeng yi", "cho-jui hsieh", "kush varshney", "lijun zhang", "yao li"], "accepted": true, "id": "1702.06347"}, "pdf": {"name": "1702.06347.pdf", "metadata": {"source": "CRF", "title": "Positive-Unlabeled Demand-Aware Recommendation", "authors": ["Jinfeng Yi", "Cho-Jui Hsieh", "Kush Varshney", "Lijun Zhang", "Yao Li"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "E-commerce recommender systems aim to present items with high utility to the consumers [20]. Utility may be decomposed into form utility: the item is desired as it is manifested, and time utility: the item is desired at the given point in time [30]; recommender systems should take both types of utility into account. Economists define items to be either durable goods or nondurable goods based on how long they are intended to last before being replaced [29]. A key characteristic of durable goods is the long duration of time between successive purchases within item categories whereas this duration for nondurable goods is much shorter, or even negligible. Thus, durable and nondurable goods have differing time utility characteristics which lead to differing demand characteristics.\nAlthough we have witnessed great success of collaborative filtering in media recommendation, we should be careful when expanding its application to general e-commerce recommendation involving both durable and nondurable goods due to the following reasons:\n1. Since media such as movies and music are nondurable goods, most users are quite receptive to buying or renting them in rapid succession. However, users only purchase durable goods when the time is right. For instance, most users will not buy televisions the day after they have already bought one. Therefore, recommending an item for which a user has no immediate demand can hurt user experience and waste an opportunity to drive sales.\nar X\niv :1\n70 2.\n06 34\n7v 1\n[ cs\n.L G\n] 2\n1 Fe\nb 20\n2. A key assumption made by matrix factorization- and approximation-based collaborative filtering algorithms is that the underlying rating matrix is of low-rank since only a few factors typically contribute to an individual\u2019s form utility [5, 34]. However, a user\u2019s demand is not only driven by form utility, but is the combined effect of both form utility and time utility. Hence, even if the underlying form utility matrix is of low-rank, the overall purchase intention matrix is usually of high-rank, and thus cannot be directly recovered by existing approaches. To see this, we construct a toy example with 50 users and 100 durable goods. Following the low-rank assumption, we assume the form utility matrix Z \u2208 R50\u00d7100 is generated by UV>, where U \u2208 R50\u00d710 and V \u2208 R100\u00d710 are both Gaussian random matrices with mean 1 and standard deviation 0.5. As discussed above, user i\u2019s purchase intention of item j is mediated by a time utility factor hij . We assume this factor is sampled from a rectified Gaussian distribution with mean 1 and standard deviation 0.5.1 Then the purchase intention matrix B \u2208 R50\u00d7100 is given by B = Z\u2212H, where H \u2208 R50\u00d7100 is the time utility factor matrix. Figure 1 shows the distributions of singular values for matrices Z and B. We observe that although the form utility matrix Z is of low-rank, the purchase intention matrix B is a full-rank matrix since all its singular values are greater than 0. This simple example illustrates that considering users\u2019 demands can make the underlying matrix no longer of low-rank, thus violating the key assumption made by many collaborative filtering algorithms.\nAn additional challenge faced by many real-world recommender systems is the one-sided sampling of implicit feedback [17, 25]. Unlike the Netflix-like setting that provides both positive and negative feedback (high and low ratings), in many e-commerce systems no negative feedback is available. For example, a user might not purchase an item because she does not derive utility from it, or just because she was simply unaware of it or plans to buy it in the future. In this sense, the labeled training data only draws from the positive class, and the unlabeled data is a mixture of positive and negative samples, a problem usually referred to as positive-unlabeled (PU) learning [11, 22, 15].\n1We use a rectified Gaussian distribution to ensure non-negative time utility factors. As introduced in Section 3, hij is a function of item j\u2019s inter-purchase duration d and the time gap t of user i\u2019s most recent purchase within the item j\u2019s category. If d and t are Gaussian random variables, then hij = max(0, d\u2212 t) follows a rectified Gaussian distribution.\nTo address these limitations, we study the problem of demand-aware recommendation. Given purchase triplets (user, item, time) and item categories, the objective is to make recommendations based on users\u2019 overall predicted combination of form utility and time utility.\nWe denote purchases by the sparse binary tensor P . To model implicit feedback, we assume that P is obtained by thresholding an underlying real-valued utility tensor to a binary tensor Y and then revealing a subset of Y\u2019s positive entries. The key to demand-aware recommendation is defining an appropriate utility measure for all (user, item, time) triplets. To this end, we quantify purchase intention as a combined effect of form utility and time utility. Specifically, we model a user\u2019s time utility for an item by comparing the time t since her most recent purchase within the item\u2019s category and the item category\u2019s underlying inter-purchase duration d; the smaller the value of t\u2212 d, the less likely she needs this item. In contrast, t \u2265 d may indicate that the item needs to be replaced, and she may be open to related recommendations. Therefore, the hinge loss h = max(0, d\u2212 t) can be employed to measure the time utility factor for a (user, item) pair. Then the purchase intention for a (user, item, time) triplet is given by x \u2212 h, where x denotes the user\u2019s form utility. This observation allows us to cast demand-aware recommendation as the problem of learning users\u2019 form utility tensor X and items\u2019 inter-purchase durations vector d given the binary tensor P .\nAlthough the learning problem can be naturally formulated as a tensor nuclear norm minimization problem, the high computational cost significantly limits its application to large-scale recommendation problems. To address this limitation, we first relax the problem to a matrix optimization problem with a class-dependent loss. We note that the problem after relaxation is still non-trivial to solve since it is a highly non-smooth problem with nested hinge losses. More severely, the optimization problem involves mnl entries, where m, n, and l are the number of users, items, and time slots, respectively. Thus a naive optimization algorithm will take at least O(mnl) time, and is intractable for large-scale recommendation problems. To overcome this limitation, we develop an efficient alternating minimization algorithm and show that its time complexity is only approximately proportional to the number of nonzero elements in the purchase records tensor P . Since P is usually very sparse, our algorithm is extremely efficient and can scale up to problems with millions of user and items. For instance, our synthetic study shows that the proposed algorithm is able to make demand-aware recommendations for a trillion (user, item) pairs in less than 2 hours.\nCompared to existing recommender systems, our work has the following contributions and advantages: (i) to the best of our knowledge, this is the first work that makes demand-aware recommendation by considering inter-purchase durations for durable and nondurable goods; (ii) the proposed algorithm is able to simultaneously infer items\u2019 inter-purchase durations and users\u2019 real-time purchase intentions, which can help e-retailers make more informed decisions on inventory planning and marketing strategy; (iii) by effectively exploiting sparsity, the proposed algorithm is extremely efficient and able to handle problems with a huge number of users and items."}, {"heading": "2 Related Work", "text": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 31, 9, 21], and PU learning [22, 10, 15, 16, 25, 2]. The extensive consumer modeling literature is concerned with descriptive and analytical models of choice rather than prediction or recommendation, but nonetheless forms the basis for our modeling approach. A variety of time-aware recommender systems have been proposed to exploit time information, but none of them explicitly consider the notion of time utility derived from inter-purchase durations in item categories. Much of the PU learning literature is focused on the binary classification problem, e.g. [22, 10], whereas we are in the collaborative filtering setting. For the papers that do examine collabo-\nrative filtering with PU learning or learning with implicit feedback [16, 25, 2], they mainly focus on media recommendation and overlook users\u2019 demands, thus are not suitable for durable goods recommendation.\nTemporal aspects of the recommendation problem have been examined in a few ways: as part of the cold-start problem [3], to capture dynamics in interests or ratings over time [19, 8, 31], and as part of the context in context-aware recommenders [1]. However, the problem we address in this paper is different from all of those aspects, and in fact could be combined with the other aspects in future solutions. To the best of our knowledge, there is no existing work that tries to take inter-purchase durations into account to better time recommendations as we do herein."}, {"heading": "3 Positive-Unlabeled Demand-Aware Recommendation", "text": "Throughout the paper, we use boldface Euler script letters, boldface capital letters, and boldface lower-case letters to denote tensors (e.g., A), matrices (e.g., A) and vectors (e.g., a), respectively. Scalars such as entries of tensors, matrices, and vectors are denoted by lowercase letters, e.g., a. In particular, the (i, j, k) entry of a third-order tensor A is denoted by aijk.\nGiven a set of m users, n items, and l time slots, we construct a third-order binary tensor P \u2208 {0, 1}m\u00d7n\u00d7l to represent the purchase history. Specifically, entry pijk = 1 indicates that user i has purchased item j in time slot k. We denote \u2016P\u20160 as the number of nonzero entries in tensor P . Since P is usually very sparse, we have \u2016P\u20160 mnl. Also, we assume that the n items belong to r item categories, with items in each category sharing similar inter-purchase durations.2 We use an n-dimensional vector c \u2208 {1, 2, . . . , r}n to represent the category membership of each item. Given P and c, we further generate a tensor T \u2208 Rm\u00d7r\u00d7l where ticjk denotes the number of time slots between user i\u2019s most recent purchase within item category cj until time k. If user i has not purchased within item category cj until time k, ticjk is set to +\u221e."}, {"heading": "3.1 Inferring Purchase Intentions from Users\u2019 Purchase Histories", "text": "In this work, we formulate users\u2019 utility as a combined effect of form utility and time utility. To this end, we use an underlying third-order tensor X \u2208 Rm\u00d7n\u00d7l to quantify form utility. In addition, we employ a nonnegative vector d \u2208 Rr+ to measure the underlying inter-purchase duration times of the r item categories. It is understood that the inter-purchase durations for durable good categories are large, while for nondurable good categories are small, or even zero. In this study, we focus on items\u2019 inherent properties and assume that the inter-purchase durations are user-independent. The problem of learning personalized durations will be studied in our future work.\nAs discussed above, the demand is mediated by the time elapsed since the last purchase of an item in the same category. Let dcj be the inter-purchase duration time of item j\u2019s category cj , and let ticjk be the time gap of user i\u2019s most recent purchase within item category cj until time k. Then if dcj > ticjk, a previously purchased item in category cj continues to be useful, and thus user i\u2019s utility from item j is weak. Intuitively, the greater the dcj \u2212 ticjk, the weaker the utility. On the other hand, dcj < ticjk indicates the item is nearing the end of its lifetime and the user may be open to recommendations in category cj . We use a hinge loss h(i, cj , k) = max(0, dcj\u2212ticjk) to model such time utility. The overall utility can be obtained by comparing form utility and time utility. In more detail, we model a binary utility indicator tensor Y \u2208 {0, 1}m\u00d7n\u00d7l as\n2To meet this requirement, the granularity of categories should be properly selected. For instance, the category \u2018Smart TV\u2019 is a better choice than the category \u2018Electrical Equipment\u2019, since the latter category covers a broad range of goods with different durations.\nbeing generated by the following thresholding process:\nyijk = 1[xijk \u2212 h(i, cj , k) > \u03c4 ], (1)\nwhere 1(\u00b7) : R\u2192 {0, 1} is the indicator function, and \u03c4 > 0 is a predefined threshold. Note that the positive entries of Y denote high purchase intentions, while the positive entries of P denote actual purchases. Generally speaking, a purchase only happens when the utility is high, but a high utility does not necessarily lead to a purchase. This observation allows us to link the binary tensors P and Y : P is generated by a one-sided sampling process that only reveals a subset of Y\u2019s positive entries. Following the setting of PU learning [11, 22, 15], we assume that the positive entries of P are uniformly sampled from positive entries of Y .\nIt is impossible to directly recover tensor X from P ,3 but fortunately, we need not recover X to make recommendations. Instead, we only need to infer the binary tensor Y whose entries yijk can be obtained by comparing xijk \u2212 h(i, cj , k) to \u03c4 . In this sense, our goal becomes learning a tensor Z \u2208 Rm\u00d7n\u00d7l such that the binary tensors obtained by thresholding Z and X to be the same. We note that tensor Z should be of low-rank to capture temporal dynamics of users\u2019 interests, which are generally believed to be dictated by a small number of latent factors [24].\nIn addition, since P is generated by Y via a one-sided sampling process, we follow [15] and include an asymmetric margin loss with a parameter \u03b7 trading the relative cost of positive and unlabeled samples [28]:\nL(Z,P)= \u03b7 \u2211\nijk: pijk=1\nmax[1\u2212 (zijk \u2212max(0, dcj \u2212 ticjk)), 0] 2 + (1\u2212 \u03b7) \u2211 ijk: pijk=0 l(zijk, 0),\nwhere l(x, c) = (x\u2212 c)2 denotes the squared loss. By combining asymmetric sampling and the low-rank property together, we recover the tensor Z and the inter-purchase duration vector d by solving the following tensor nuclear norm minimization (TNNM) problem:\nmin Z\u2208Rm\u00d7n\u00d7l, d\u2208Rr+\n\u03b7 \u2211\nijk: pijk=1\nmax[1\u2212 (zijk \u2212max(0, dcj \u2212 ticjk)), 0] 2\n+ (1\u2212 \u03b7) \u2211\nijk: pijk=0\nz2ijk + \u03bb \u2016Z\u2016\u2217, (2)\nwhere \u2016Z\u2016\u2217 denotes the tensor nuclear norm, a convex combination of nuclear norms of Z\u2019s unfolded matrices [23]. Given the learned Z\u0302 and d\u0302, the underlying binary tensor Y can be recovered by\nyijk = 1[z\u0302ijk \u2212max(0, d\u0302cj \u2212 ticjk) > \u03c4 ].\nWe note that although the TNNM problem (2) can be solved by optimization techniques such as block coordinate descent [23] or ADMM [12], they suffer from high computational cost since they need to be solved iteratively with multiple SVDs at each iteration. An alternative way to solve the problem is tensor factorization [18]. However, this also involves iterative singular vector estimation, and is thus not scalable enough. As a typical example, recovering a rank 10 tensor of size 500\u00d7 500\u00d7 500 takes the state-of-the-art tensor factorization algorithm TenALS 4 more than 20, 000 seconds on an Intel Xeon 2.40 GHz processor with 32 GB main memory.\n3To see this, consider a tensor X with all entries equaling \u03c1, which leads to an all one tensor Y if \u03c1 > \u03c4 + \u03c6, where \u03c6 is the upper bound of h(i, cj , k) As is apparent, \u03c1 cannot be uniquely identified by just observing Y .\n4http://web.engr.illinois.edu/\u02dcswoh/software/optspace/code.html"}, {"heading": "3.2 A Scalable Relaxation", "text": "In this subsection, we discuss how to significantly improve the scalability of the proposed demand-aware recommendation model. To this end, we assume that an individual\u2019s form utility does not change over time, an assumption widely-used in many collaborative filtering methods [27]. Under this assumption, the tensor Z is a repeated copy of its frontal slice Z::1, i.e.,\nZ = Z::1 \u25e6 e, (3)\nwhere e is an l-dimensional all-one vector and the symbol \u25e6 represents the outer product operation. In this way, we can relax the problem of learning a third-order tensor Z to the problem of learning its frontal slice, which is a second-order tensor (matrix). For notational simplicity, we use a matrix Z to denote the frontal slice Z::1, and use zij to denote the entry (i, j) of the matrix Z.\nSince Z is a low-rank tensor, its frontal slice Z should be of low-rank as well. With the assumption, the minimization problem (2) simplifies to:\nmin Z\u2208Rm\u00d7n\nd\u2208Rr\n\u03b7 \u2211\nijk: pijk=1\nmax[1\u2212 (zij \u2212max(0, dcj \u2212 ticjk)), 0] 2\n+ (1\u2212 \u03b7) \u2211\nijk: pijk=0\nz2ij + \u03bb \u2016Z\u2016\u2217 := f(Z,d), (4)\nwhere \u2016Z\u2016\u2217 stands for the matrix nuclear norm, the convex surrogate of the matrix rank function. By relaxing the optimization problem (2) to the problem (4), we recover a matrix instead of a tensor to infer users\u2019 purchase intentions. In Section 4, we discuss how to efficiently optimize the problem (4) to make demand-aware recommendations for a huge number of users and items."}, {"heading": "4 Optimization", "text": "Although the learning problem has been relaxed, optimizing (4) is still very challenging for two reasons: (i) the objective is highly non-smooth with nested hinge losses, and (ii) it contains mnl terms: a naive optimization algorithm will take at least O(mnl) time.\nTo address these challenges, we adopt an alternating minimization scheme that iteratively fixes one of d and Z and minimizes with respect to the other. Specifically, we apply an alternating minimization scheme to iteratively solve the following subproblems:\nd\u2190 arg min d f(Z,d). (5) Z\u2190 arg min Z f(Z,d) (6)\nWe note that both subproblems are non-trivial to solve because subproblem (6) is a nuclear norm minimization problem, and both subproblems involve nested hinge losses. In the following we discuss how to efficiently optimize subproblems (5) and (6):"}, {"heading": "4.1 Update d", "text": "Eq (5) can be written as\nmin d \u2211 ijk: pijk=1\n{ max ( 1\u2212 (zij \u2212max(0, dcj \u2212 ticjk)), 0 )2} := g(d) := \u2211 ijk: pijk=1 gijk(dcj ).\nWe then analyze the value of each gijk by comparing dcj and ticjk:\n1. If dcj \u2264 ticjk, we have gijk(dcj ) = max(1\u2212 zij , 0)2\n2. If dcj > ticjk, we have\ngijk(dcj ) = max(1\u2212 (zij \u2212 dcj + ticjk), 0) 2,\nwhich can be further separated into two cases:\ngijk(dcj ) = { 1\u2212 (zij \u2212 dcj + ticjk))2, if dcj > zij + ticjk \u2212 1 0, if dcj \u2264 zij + ticjk \u2212 1\nTherefore, we have the following observations:\n1. If zij \u2264 1, we have\ngijk(dcj ) = { max(1\u2212 zi,j , 0)2, if dcj \u2264 ticjk (1\u2212 (zij \u2212 dcj + ticjk))2, if dcj > ticjk\n2. If zij > 1, we have\ngijk(dcj ) = { (1\u2212 (zij \u2212 dcj + ticjk))2, if dcj > ticjk + zij \u2212 1 0, if dcj \u2264 ticjk + zij \u2212 1\nThis further implies\ngijk(dcj ) = { max(1\u2212 zij , 0)2, if dcj \u2264 ticjk + max(zij \u2212 1, 0) (1\u2212 (zij \u2212 dcj + ticjk))2, if dcj > ticjk + max(zij \u2212 1, 0)\nFor notational simplicity, we let sijk = ticjk + max(zij \u2212 1, 0) for all triplets (i, j, k) satisfying pijk = 1. Algorithm. For each category \u03ba, we collect the set Q = {(i, j, k) | pijk = 1 and cj = \u03ba} and calculate the corresponding sijks. We then sort sijks such that (si1j1k1) \u2264 \u00b7 \u00b7 \u00b7 \u2264 s(i|Q|j|Q|k|Q|) . For each interval [s(iqjqkq), s(iq+1jq+1kq+1)], the function is\ng\u03ba(d) = |Q|\u2211 t=q+1 max(1\u2212 zitjt , 0)2 + q\u2211 t=1 (d+ 1\u2212 zitjt \u2212 titcjtkt) 2\nBy letting\nRq = |Q|\u2211 t=q+1 max(1\u2212 zitjt , 0)2,\nFq = q\u2211 t=1 (1\u2212 zitjt \u2212 titcjtkt),\nWq = q\u2211 t=1 (1\u2212 zitjt \u2212 titcjtkt) 2,\nwe have\ng\u03ba(d) = qd 2 + 2Fqd+Wq +Rq\n= q ( d+\nFq q )2 \u2212 F 2q q +Wq +Rq.\nThus the optimal solution in the interval [s(iqjqkq), s(iq+1jq+1kq+1)] is given by\nd\u2217 = max ( s(iqjqkq), min ( s(iq+1jq+1kq+1), \u2212\nFq q\n)) ,\nand the optimal function value is gr(d\u2217). By going through all the intervals from small to large, we can obtain the optimal solution for the whole function. We note that each time when q \u21d2 q + 1, the constants Rq, Fq,Wq only change by one element. Thus the time complexity for going from q \u21d2 q + 1 is O(1), and the whole procedure has time complexity O(|Q|).\nIn summary, we can solve the subproblem (5) by the following steps:\n1. generate the set U\u03ba = {(i, j, k) | pijk = 1 and cj = \u03ba} for each category r,\n2. sort each list (costing O(|Q\u03ba| log |Q\u03ba|) time),\n3. compute R0, F0,W0 (costing O(|Q\u03ba|) time), and then\n4. search for the optimal solution for each q = 1, 2, \u00b7 \u00b7 \u00b7 , |Q\u03ba| (costing O(|Q\u03ba|) time).\nThe above steps lead to an overall time complexity O(\u2016P\u20160 log(\u2016P\u20160)), where \u2016P\u20160 is the number of nonzero elements in tensor P . Therefore, we can efficiently update d since P is a very sparse tensor with only a small number of nonzero elements."}, {"heading": "4.2 Update Z", "text": "By defining\naijk = { 1 + max(0, dcj \u2212 ticjk), if pijk = 1 0, otherwise\nthe subproblem (6) can be written as\nmin Z\u2208Rm\u00d7n\nh(Z) + \u03bb\u2016Z\u2016\u2217 where h(Z) := { \u03b7 \u2211 ijk: pijk=1 max(aijk \u2212 zij , 0)2 + (1\u2212 \u03b7) \u2211 ijk: pijk=0 z2ij } .\nSince there are O(mnl) terms in the objective function, a naive implementation will take O(mnl) time, which is computationally inefficient when the data is large. To address this issue, We use proximal gradient descent to solve the problem. At each iteration, Z is updated by\nZ\u2190 S\u03bb(Z\u2212 \u03b1\u2207h(Z)), (7)\nwhere S\u03bb(\u00b7) is the soft-thresholding operator for singular values 5.\nAlgorithm 1: Proximal Gradient Descent for Updating Z Input : P , Z0 (initialization), step size \u03b3 Output: A sequence of Zt converges to the optimal solution 1 for t = 1, . . . ,maxiter do 2 [U,\u03a3,V] = rand svd(Z\u2212 \u03b3\u2207h(Zt)) 3 \u03a3\u0304 = max(\u03a3\u2212 \u03b3\u03bb, 0) 4 k : number of nonzeros in \u03a3 5 Zt+1 = U(:, 1:k)\u03a3\u0304(1:k, 1:k)V(:, 1:k)T\nIn order to efficiently compute the top singular vectors of Z\u2212 \u03b1\u2207h(Z), we rewrite it as\nZ\u2212 \u03b1\u2207h(Z) = [1\u2212 2(1\u2212 \u03b7)l] Z + 2(1\u2212 \u03b7) \u2211 ijk: pijk=1 zij \u2212 2\u03b7 \u2211 ijk: pijk=1 max(aijk \u2212 zij , 0)  . (8) Since Z is a low-rank matrix, [1\u2212 2(1\u2212 \u03b7)l] Z is also of low-rank. Besides, since P is very sparse, the\nterm 2(1\u2212 \u03b7) \u2211 ijk: pijk=1 zij \u2212 2\u03b7 \u2211 ijk: pijk=1 max(aijk \u2212 zij , 0)  is also sparse because it only involves the nonzero elements of P . In this case, when we multiply (Z \u2212 \u03b1\u2207h(Z)) with a skinny m by k matrix, it can be computed in O(nk2 +mk2 + \u2016P\u20160k) time.\nAs shown in [14], each iteration of proximal gradient descent for nuclear norm minimization only requires a fixed number of iterations before convergence, thus the time complexity to update Z is O(nk2T + mk2T + \u2016P\u20160kT ), where T is the number of iterations."}, {"heading": "4.3 Overall Algorithm", "text": "Combining the two subproblems together, the time complexity of each iteration of the proposed algorithm is:\nO(\u2016P\u20160 log(\u2016P\u20160) + nk2T +mk2T + \u2016P\u20160kT ).\nRemark: Since each user should make at least one purchase and each item should be purchased at least once to be included in P , n and m are smaller than \u2016P\u20160. Also, since k and T are usually very small, the time complexity to solve problem (4) is dominated by the term \u2016P\u20160, which is a significant improvement over the naive approach with O(mnl) complexity.\nSince our problem has only two blocks d, Z and each subproblem is convex, our optimization algorithm is guaranteed to converge to a stationary point [13]. Indeed, it converges very fast in practice. As a concrete example, it takes only 10 iterations to optimize a problem with 1 million users, 1 million items, and more than 166 million purchase records.\n5If X has the singular value decomposition X = U\u03a3VT , then S\u03bb(X) = U(\u03a3\u2212 \u03bbI)+VT where a+ = max(0, a)."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experiment with Synthesized Data", "text": "We first conduct experiments with simulated data to verify that the proposed demand-aware recommendation algorithm is computationally efficient and robust to noise. To this end, we first construct a low-rank matrix Z = WHT , where W \u2208 Rm\u00d710 and H \u2208 Rn\u00d710 are random Gaussian matrices with entries drawn from N (1, 0.5), and then normalize Z to the range of [0, 1]. We randomly assign all the n items to r categories, with their inter-purchase durations d equaling [10, 20, . . . , 10r]. We then construct the high purchase intension set \u2126 = {(i, j, k) | ticjk \u2265 dcj and zij \u2265 0.5}, and sample a subset of its entries as the observed purchase records. We let n = m and vary them in the range {10, 000, 20, 000, 30, 000, 40, 000}. We also vary r in the range {10, 20, \u00b7 \u00b7 \u00b7 , 100}. Given the learned durations d\u2217, we use \u2016d\u2212 d\u2217\u20162/\u2016d\u20162 to measure the prediction errors. Accuracy Figure 2(a) and 2(b) clearly show that the proposed algorithm can perfectly recover the underlying inter-purchase durations with varied numbers of users, items, and categories. To further evaluate the robustness of the proposed algorithm, we randomly flip some entries in tensor P from 0 to 1 to simulate the rare cases of purchasing two items in the same category in close temporal succession. Figure 2(c) shows that when the ratios of noisy entries are not large, the predicted durations d\u0302 are close enough to the true durations, thus verifying the robustness of the proposed algorithm. Scalability To verify the scalability of the proposed algorithm, we fix the numbers of users and items to be 1 million, the number of time slots to be 1000, and vary the number of purchase records (i.e., \u2016P\u20160). Table 1 summarizes the running time of solving problem (4) on a computer with 32 GB main memory using a single thread. We observe that the proposed algorithm is extremely efficient, e.g., even with 1 million users, 1\nmillion items, and more than 166 million purchase records, the running time of the proposed algorithm is less than 2 hours."}, {"heading": "5.2 Experiment with Real-World Data", "text": "We then evaluate the proposed demand-aware recommendation algorithm on two real-world datasets Tmall6 and Amazon Review7. Due to the high computational costs of some of the baseline algorithms, only subsets of these two datasets are used in our study. For Tmall data set, we pick the top 70 item categories, which contain the purchase records of 438 users and 937 items. For Amazon Review data set, we randomly select 300 users who have provided reviews to at least 5 item categories on Amazon.com. This leads to a total of 5, 051 items belonging to 11 categories. Time information for both data sets is provided in days, and we have 184 and 3, 420 time slots for Tmall and Amazon Review datasets, respectively.\nWe compare the proposed recommendation algorithm to the following six state-of the-art recommendation algorithms: (a) M3F, maximum-margin matrix factorization [26], (b) PMF, probabilistic matrix factorization [27], (c) WR-MF, weighted regularized matrix factorization [16], (d) CP-APR, CandecompParafac alternating Poisson regression [7], (e) Rubik, knowledge-guided tensor factorization and completion method [32], and (f) BPTF, Bayesian probabilistic tensor factorization [33]. Among them, M3F and PMF are widely-used static collaborative filtering algorithms. We include these two algorithms as baselines to justify whether traditional collaborative filtering algorithms are suitable for general e-commerce recommendation involving both durable and nondurable goods. Since they require explicit ratings as inputs, we follow [2] to generate numerical ratings based on the frequencies of (user, item) consumption pairs. WR-MF is essentially the positive-unlabeled version of PMF and has shown to be very effective in modeling implicit feedback data. All the other three baselines, i.e., CP-APR, Rubik, and BPTF, are tensor-based methods that can consider time utility when making recommendations. We refer to the proposed recommendation algorithm as Demand-Aware Recommender for One-Sided Sampling, or DAROSS for short.\nFor each user, we randomly sample 90% of her purchase records as the positive class training data, and use the remaining 10% as the test data. In addition, we randomly sample 20% of her non-purchase triplets as the negative class training data and feed them to the baseline algorithms. We select this relatively small ratio since most of the unknown entries in implicit data are caused by the users not being aware of the items as opposed to users disliking the items. For each triplet (u, i, t) in the test set, we evaluate all the\n6http://ijcai-15.org/index.php/repeat-buyers-prediction-competition 7http://jmcauley.ucsd.edu/data/amazon/\nalgorithms on two tasks: (i) item prediction, and (ii) purchase time prediction. In the first task, we record the predicted ranking of item j among all items at time t; while in the second task, we record the error between the true purchase time t and its nearest predicted purchase time within item j\u2019s category. If no purchase is predicted within the item category, we set the error be the total number of time slots l. Ideally, good recommendations should have both small item rankings and small time errors. Thus we adopt the top percentages, i.e., ranking / n\u00d7 100% and error / l \u00d7 100%, as the evaluation metrics of item and purchase time prediction tasks, respectively. The algorithms M3F, PMF, and WR-MF are excluded from the purchase time prediction task since they are static models.\nFigure 3 displays the predictive performance of the seven recommendation algorithms. Compared to all the baseline algorithms, DAROSS, the proposed demand-aware recommendation algorithm yields the best performance for both datasets and tasks. As expected, we also observe that DAROSS achieves better performance on Tmall than Amazon Review. We conjecture that this is because the time stamps of Amazon Review reflect the review time instead of purchase time, and inter-reviewing durations could be different from inter-purchase durations. Table 2 reports the inter-reviewing durations of Amazon Review estimated by our algorithm DAROSS. Although they may not reflect the true inter-purchase durations, the estimated durations still make sense since they clearly distinguished between durable good categories, e.g., automotive, musical instruments, and non-durable good categories, e.g., instant video, apps, and food. Apparently, these categories are relatively coarse and may contain multiple sub-categories with different durations. By choosing a more proper category granularity (which is unfortunately not supported by Amazon Review), we expect to achieve more accurate duration estimations and also better recommendation performance. We do not report the estimated durations of Tmall herein since its item categories are anonymized.\nWe finally test our algorithm on the full set of Amazon Review8 to evaluate its performance on large-scale real-world data. After removing duplicate items, the data set contains more than 72 million product reviews from 19.8 million users and 7.7 million items that belong to 24 item categories. The collected reviews span a long range of time, i.e., from May 1996 to July 2014, which leads to 6, 639 time slots in total. Comparing to the small subset we evaluated earlier, the full set is a much more challenging data set both due to its much larger size and much lower sampling rate, i.e., many reviewers only provided a few reviews, and many items were only reviewed a few times. Indeed, most of our baseline algorithms fail to handle such a large data set and we only obtain the predictive performance of three algorithms DAROSS, WR-MF, and PMF. Their average item ranking percentages are 17.4%, 21.9%, and 38.6%, respectively. In addition, it only takes our algorithm 10 iterations and 37 minutes to find out the optimal solution. All these results clearly validate the scalability and effectiveness of our proposed algorithm."}, {"heading": "6 Conclusion", "text": "In this paper, we examine the problem of demand-aware recommendation in settings when inter-purchase duration within item categories affects users\u2019 purchase intention in combination with intrinsic properties\n8http://jmcauley.ucsd.edu/data/amazon/\nof the items themselves. Also accounting for the one-sided sampling from implicit feedback present in ecommerce, we formulate a TNNM problem that seeks to learn the form utility tensor and estimate a vector of inter-purchase durations jointly.\nThe TNNM formulation does not scale to large realistic problem sizes, and thus we develop a tractable matrix optimization relaxation that we solve via an alternating minimization scheme. We show that such an approach converges to a stationary point and, despite challenging subproblems, has tractable time complexity dominated by the number of purchases in the data rather than the cardinalities of users, items, and time slots.\nOur empirical studies show that the proposed approach can yield perfect recovery of duration vectors in noiseless settings; it is robust to noise and scalable as analyzed theoretically. On two real-world datasets, Tmall and Amazon Review, we show that our algorithm outperforms five state-of-the-art recommender systems on both metrics of our interest: item prediction and purchase time prediction. Thus overall, the algorithm proposed in this paper ought to be considered for practical deployment in e-commerce recommendation of durable goods both due to its accuracy and its scalability."}], "references": [{"title": "Context-aware recommender systems", "author": ["Gediminas Adomavicius", "Alexander Tuzhilin"], "venue": "In Recommender Systems Handbook,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Towards time-dependant recommendation based on implicit feedback", "author": ["Linas Baltrunas", "Xavier Amatriain"], "venue": "In Workshop on context-aware recommender systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "A collaborative filtering approach to mitigate the new user cold start problem", "author": ["Jes\u00fas Bobadilla", "Fernando Ortega", "Antonio Hernando", "Jes\u00fas Bernal"], "venue": "Knowl.-Based Syst.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Time-aware recommender systems: a comprehensive survey and analysis of existing evaluation protocols", "author": ["Pedro G. Campos", "Fernando D\u0131\u0301ez", "Iv\u00e1n Cantador"], "venue": "User Model. User-Adapt. Interact.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J. Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "A consumer purchasing model with erlang interpurchase times", "author": ["Christopher Chatfield", "Gerald J Goodhardt"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1973}, {"title": "On tensors, sparsity, and nonnegative factorizations", "author": ["Eric C. Chi", "Tamara G. Kolda"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Modeling temporal adoptions using dynamic matrix factorization", "author": ["Freddy Chong Tat Chua", "Richard J. Oentaryo", "Ee-Peng Lim"], "venue": "In ICDM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Time-sensitive recommendation from recurrent user activities", "author": ["Nan Du", "Yichen Wang", "Niao He", "Jimeng Sun", "Le Song"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Analysis of learning from positive and unlabeled data", "author": ["Marthinus Christoffel du Plessis", "Gang Niu", "Masashi Sugiyama"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Learning classifiers from only positive and unlabeled data", "author": ["Charles Elkan", "Keith Noto"], "venue": "In KDD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization", "author": ["Silvia Gandy", "Benjamin Recht", "Isao Yamada"], "venue": "Inverse Problems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "On the convergence of the block nonlinear Gauss-Seidel method under convex constraints", "author": ["L. Grippo", "M. Sciandrone"], "venue": "Operations Research Letters,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Nuclear norm minimization via active subspace selection", "author": ["C.-J. Hsieh", "P.A. Olsen"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "PU learning for matrix completion", "author": ["Cho-Jui Hsieh", "Nagarajan Natarajan", "Inderjit S. Dhillon"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "In Data Mining,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Yifan Hu", "Yehuda Koren", "Chris Volinsky"], "venue": "In ICDM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Provable tensor factorization with missing data", "author": ["Prateek Jain", "Sewoong Oh"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Collaborative filtering with temporal dynamics", "author": ["Yehuda Koren"], "venue": "Commun. ACM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Impact of recommender systems on sales volume and diversity", "author": ["Dokyun Lee", "Kartik Hosanagar"], "venue": "In Proc. Int. Conf. Inf. Syst.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Cross-domain collaborative filtering over time", "author": ["Bin Li", "Xingquan Zhu", "Ruijiang Li", "Chengqi Zhang", "Xiangyang Xue", "Xindong Wu"], "venue": "In IJCAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Building text classifiers using positive and unlabeled examples", "author": ["Bing Liu", "Yang Dai", "Xiaoli Li", "Wee Sun Lee", "Philip S. Yu"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["Ji Liu", "Przemyslaw Musialski", "Peter Wonka", "Jieping Ye"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Tensor factorization using auxiliary information", "author": ["A. Narita", "K. Hayashi", "R. Tomioka", "H. Kashima"], "venue": "In ECML/PKDD,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "BPR: bayesian personalized ranking from implicit feedback", "author": ["Steffen Rendle", "Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt-Thieme"], "venue": "In UAI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "author": ["Jason D.M. Rennie", "Nathan Srebro"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["Ruslan Salakhutdinov", "Andriy Mnih"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Calibrated asymmetric surrogate losses", "author": ["Clayton Scott"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Exploring Economics", "author": ["Robert L. Sexton"], "venue": "Cengage Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "The prejudice against marketing", "author": ["Robert L. Steiner"], "venue": "J. Marketing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1976}, {"title": "Collaborative Kalman filtering for dynamic matrix factorization", "author": ["John Z. Sun", "Dhruv Parthasarathy", "Kush R. Varshney"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Rubik: Knowledge guided tensor factorization and completion for health data analytics", "author": ["Yichen Wang", "Robert Chen", "Joydeep Ghosh", "Joshua C. Denny", "Abel N. Kho", "You Chen", "Bradley A. Malin", "Jimeng Sun"], "venue": "In SIGKDD,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Temporal collaborative filtering with bayesian probabilistic tensor factorization", "author": ["Liang Xiong", "Xi Chen", "Tzu-Kuo Huang", "Jeff G. Schneider", "Jaime G. Carbonell"], "venue": "In SDM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Inferring users preferences from crowdsourced pairwise comparisons: A matrix completion approach", "author": ["J. Yi", "R. Jin", "S. Jain", "A. Jain"], "venue": "In AAAI Conference on Human Computation and Crowdsourcing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "E-commerce recommender systems aim to present items with high utility to the consumers [20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 29, "context": "Utility may be decomposed into form utility: the item is desired as it is manifested, and time utility: the item is desired at the given point in time [30]; recommender systems should take both types of utility into account.", "startOffset": 151, "endOffset": 155}, {"referenceID": 28, "context": "Economists define items to be either durable goods or nondurable goods based on how long they are intended to last before being replaced [29].", "startOffset": 137, "endOffset": 141}, {"referenceID": 4, "context": "A key assumption made by matrix factorization- and approximation-based collaborative filtering algorithms is that the underlying rating matrix is of low-rank since only a few factors typically contribute to an individual\u2019s form utility [5, 34].", "startOffset": 236, "endOffset": 243}, {"referenceID": 33, "context": "A key assumption made by matrix factorization- and approximation-based collaborative filtering algorithms is that the underlying rating matrix is of low-rank since only a few factors typically contribute to an individual\u2019s form utility [5, 34].", "startOffset": 236, "endOffset": 243}, {"referenceID": 16, "context": "An additional challenge faced by many real-world recommender systems is the one-sided sampling of implicit feedback [17, 25].", "startOffset": 116, "endOffset": 124}, {"referenceID": 24, "context": "An additional challenge faced by many real-world recommender systems is the one-sided sampling of implicit feedback [17, 25].", "startOffset": 116, "endOffset": 124}, {"referenceID": 10, "context": "In this sense, the labeled training data only draws from the positive class, and the unlabeled data is a mixture of positive and negative samples, a problem usually referred to as positive-unlabeled (PU) learning [11, 22, 15].", "startOffset": 213, "endOffset": 225}, {"referenceID": 21, "context": "In this sense, the labeled training data only draws from the positive class, and the unlabeled data is a mixture of positive and negative samples, a problem usually referred to as positive-unlabeled (PU) learning [11, 22, 15].", "startOffset": 213, "endOffset": 225}, {"referenceID": 14, "context": "In this sense, the labeled training data only draws from the positive class, and the unlabeled data is a mixture of positive and negative samples, a problem usually referred to as positive-unlabeled (PU) learning [11, 22, 15].", "startOffset": 213, "endOffset": 225}, {"referenceID": 5, "context": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 31, 9, 21], and PU learning [22, 10, 15, 16, 25, 2].", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 31, 9, 21], and PU learning [22, 10, 15, 16, 25, 2].", "startOffset": 174, "endOffset": 188}, {"referenceID": 30, "context": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 31, 9, 21], and PU learning [22, 10, 15, 16, 25, 2].", "startOffset": 174, "endOffset": 188}, {"referenceID": 8, "context": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 31, 9, 21], and PU learning [22, 10, 15, 16, 25, 2].", "startOffset": 174, "endOffset": 188}, {"referenceID": 20, "context": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 31, 9, 21], and PU learning [22, 10, 15, 16, 25, 2].", "startOffset": 174, "endOffset": 188}, {"referenceID": 21, "context": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 31, 9, 21], and PU learning [22, 10, 15, 16, 25, 2].", "startOffset": 206, "endOffset": 229}, {"referenceID": 9, "context": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 31, 9, 21], and PU learning [22, 10, 15, 16, 25, 2].", "startOffset": 206, "endOffset": 229}, {"referenceID": 14, "context": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 31, 9, 21], and PU learning [22, 10, 15, 16, 25, 2].", "startOffset": 206, "endOffset": 229}, {"referenceID": 15, "context": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 31, 9, 21], and PU learning [22, 10, 15, 16, 25, 2].", "startOffset": 206, "endOffset": 229}, {"referenceID": 24, "context": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 31, 9, 21], and PU learning [22, 10, 15, 16, 25, 2].", "startOffset": 206, "endOffset": 229}, {"referenceID": 1, "context": "Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective [6], time-aware recommender systems [4, 31, 9, 21], and PU learning [22, 10, 15, 16, 25, 2].", "startOffset": 206, "endOffset": 229}, {"referenceID": 21, "context": "[22, 10], whereas we are in the collaborative filtering setting.", "startOffset": 0, "endOffset": 8}, {"referenceID": 9, "context": "[22, 10], whereas we are in the collaborative filtering setting.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "rative filtering with PU learning or learning with implicit feedback [16, 25, 2], they mainly focus on media recommendation and overlook users\u2019 demands, thus are not suitable for durable goods recommendation.", "startOffset": 69, "endOffset": 80}, {"referenceID": 24, "context": "rative filtering with PU learning or learning with implicit feedback [16, 25, 2], they mainly focus on media recommendation and overlook users\u2019 demands, thus are not suitable for durable goods recommendation.", "startOffset": 69, "endOffset": 80}, {"referenceID": 1, "context": "rative filtering with PU learning or learning with implicit feedback [16, 25, 2], they mainly focus on media recommendation and overlook users\u2019 demands, thus are not suitable for durable goods recommendation.", "startOffset": 69, "endOffset": 80}, {"referenceID": 2, "context": "Temporal aspects of the recommendation problem have been examined in a few ways: as part of the cold-start problem [3], to capture dynamics in interests or ratings over time [19, 8, 31], and as part of the context in context-aware recommenders [1].", "startOffset": 115, "endOffset": 118}, {"referenceID": 18, "context": "Temporal aspects of the recommendation problem have been examined in a few ways: as part of the cold-start problem [3], to capture dynamics in interests or ratings over time [19, 8, 31], and as part of the context in context-aware recommenders [1].", "startOffset": 174, "endOffset": 185}, {"referenceID": 7, "context": "Temporal aspects of the recommendation problem have been examined in a few ways: as part of the cold-start problem [3], to capture dynamics in interests or ratings over time [19, 8, 31], and as part of the context in context-aware recommenders [1].", "startOffset": 174, "endOffset": 185}, {"referenceID": 30, "context": "Temporal aspects of the recommendation problem have been examined in a few ways: as part of the cold-start problem [3], to capture dynamics in interests or ratings over time [19, 8, 31], and as part of the context in context-aware recommenders [1].", "startOffset": 174, "endOffset": 185}, {"referenceID": 0, "context": "Temporal aspects of the recommendation problem have been examined in a few ways: as part of the cold-start problem [3], to capture dynamics in interests or ratings over time [19, 8, 31], and as part of the context in context-aware recommenders [1].", "startOffset": 244, "endOffset": 247}, {"referenceID": 10, "context": "Following the setting of PU learning [11, 22, 15], we assume that the positive entries of P are uniformly sampled from positive entries of Y .", "startOffset": 37, "endOffset": 49}, {"referenceID": 21, "context": "Following the setting of PU learning [11, 22, 15], we assume that the positive entries of P are uniformly sampled from positive entries of Y .", "startOffset": 37, "endOffset": 49}, {"referenceID": 14, "context": "Following the setting of PU learning [11, 22, 15], we assume that the positive entries of P are uniformly sampled from positive entries of Y .", "startOffset": 37, "endOffset": 49}, {"referenceID": 23, "context": "We note that tensor Z should be of low-rank to capture temporal dynamics of users\u2019 interests, which are generally believed to be dictated by a small number of latent factors [24].", "startOffset": 174, "endOffset": 178}, {"referenceID": 14, "context": "In addition, since P is generated by Y via a one-sided sampling process, we follow [15] and include an asymmetric margin loss with a parameter \u03b7 trading the relative cost of positive and unlabeled samples [28]:", "startOffset": 83, "endOffset": 87}, {"referenceID": 27, "context": "In addition, since P is generated by Y via a one-sided sampling process, we follow [15] and include an asymmetric margin loss with a parameter \u03b7 trading the relative cost of positive and unlabeled samples [28]:", "startOffset": 205, "endOffset": 209}, {"referenceID": 22, "context": "where \u2016Z\u2016\u2217 denotes the tensor nuclear norm, a convex combination of nuclear norms of Z\u2019s unfolded matrices [23].", "startOffset": 107, "endOffset": 111}, {"referenceID": 22, "context": "We note that although the TNNM problem (2) can be solved by optimization techniques such as block coordinate descent [23] or ADMM [12], they suffer from high computational cost since they need to be solved iteratively with multiple SVDs at each iteration.", "startOffset": 117, "endOffset": 121}, {"referenceID": 11, "context": "We note that although the TNNM problem (2) can be solved by optimization techniques such as block coordinate descent [23] or ADMM [12], they suffer from high computational cost since they need to be solved iteratively with multiple SVDs at each iteration.", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "An alternative way to solve the problem is tensor factorization [18].", "startOffset": 64, "endOffset": 68}, {"referenceID": 26, "context": "To this end, we assume that an individual\u2019s form utility does not change over time, an assumption widely-used in many collaborative filtering methods [27].", "startOffset": 150, "endOffset": 154}, {"referenceID": 13, "context": "As shown in [14], each iteration of proximal gradient descent for nuclear norm minimization only requires a fixed number of iterations before convergence, thus the time complexity to update Z is O(nk2T + mk2T + \u2016P\u20160kT ), where T is the number of iterations.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "Since our problem has only two blocks d, Z and each subproblem is convex, our optimization algorithm is guaranteed to converge to a stationary point [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "5), and then normalize Z to the range of [0, 1].", "startOffset": 41, "endOffset": 47}, {"referenceID": 25, "context": "We compare the proposed recommendation algorithm to the following six state-of the-art recommendation algorithms: (a) M3F, maximum-margin matrix factorization [26], (b) PMF, probabilistic matrix factorization [27], (c) WR-MF, weighted regularized matrix factorization [16], (d) CP-APR, CandecompParafac alternating Poisson regression [7], (e) Rubik, knowledge-guided tensor factorization and completion method [32], and (f) BPTF, Bayesian probabilistic tensor factorization [33].", "startOffset": 159, "endOffset": 163}, {"referenceID": 26, "context": "We compare the proposed recommendation algorithm to the following six state-of the-art recommendation algorithms: (a) M3F, maximum-margin matrix factorization [26], (b) PMF, probabilistic matrix factorization [27], (c) WR-MF, weighted regularized matrix factorization [16], (d) CP-APR, CandecompParafac alternating Poisson regression [7], (e) Rubik, knowledge-guided tensor factorization and completion method [32], and (f) BPTF, Bayesian probabilistic tensor factorization [33].", "startOffset": 209, "endOffset": 213}, {"referenceID": 15, "context": "We compare the proposed recommendation algorithm to the following six state-of the-art recommendation algorithms: (a) M3F, maximum-margin matrix factorization [26], (b) PMF, probabilistic matrix factorization [27], (c) WR-MF, weighted regularized matrix factorization [16], (d) CP-APR, CandecompParafac alternating Poisson regression [7], (e) Rubik, knowledge-guided tensor factorization and completion method [32], and (f) BPTF, Bayesian probabilistic tensor factorization [33].", "startOffset": 268, "endOffset": 272}, {"referenceID": 6, "context": "We compare the proposed recommendation algorithm to the following six state-of the-art recommendation algorithms: (a) M3F, maximum-margin matrix factorization [26], (b) PMF, probabilistic matrix factorization [27], (c) WR-MF, weighted regularized matrix factorization [16], (d) CP-APR, CandecompParafac alternating Poisson regression [7], (e) Rubik, knowledge-guided tensor factorization and completion method [32], and (f) BPTF, Bayesian probabilistic tensor factorization [33].", "startOffset": 334, "endOffset": 337}, {"referenceID": 31, "context": "We compare the proposed recommendation algorithm to the following six state-of the-art recommendation algorithms: (a) M3F, maximum-margin matrix factorization [26], (b) PMF, probabilistic matrix factorization [27], (c) WR-MF, weighted regularized matrix factorization [16], (d) CP-APR, CandecompParafac alternating Poisson regression [7], (e) Rubik, knowledge-guided tensor factorization and completion method [32], and (f) BPTF, Bayesian probabilistic tensor factorization [33].", "startOffset": 410, "endOffset": 414}, {"referenceID": 32, "context": "We compare the proposed recommendation algorithm to the following six state-of the-art recommendation algorithms: (a) M3F, maximum-margin matrix factorization [26], (b) PMF, probabilistic matrix factorization [27], (c) WR-MF, weighted regularized matrix factorization [16], (d) CP-APR, CandecompParafac alternating Poisson regression [7], (e) Rubik, knowledge-guided tensor factorization and completion method [32], and (f) BPTF, Bayesian probabilistic tensor factorization [33].", "startOffset": 474, "endOffset": 478}, {"referenceID": 1, "context": "Since they require explicit ratings as inputs, we follow [2] to generate numerical ratings based on the frequencies of (user, item) consumption pairs.", "startOffset": 57, "endOffset": 60}], "year": 2017, "abstractText": "Recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. The demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. In particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. Moreover, purchase data, in contrast to ratings data, is implicit with non-purchases not necessarily indicating dislike. Together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint lowrank tensor completion and product category inter-purchase duration vector estimation. We further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and items. We also show superior prediction accuracies on multiple real-world data sets.", "creator": "LaTeX with hyperref package"}}}