{"id": "1702.06506", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "PixelNet: Representation of the pixels, by the pixels, and for the pixels", "abstract": "we explore design principles for general pixel - timing prediction problems, from low - level edge detection to mid - level surface normal estimation to high - detail semantic segmentation. tertiary predictors, namely as the fully - convolutional network ( fcn ), have achieved remarkable success over exploiting the spatial redundancy of neighboring pixels through convolutional processing. though computationally efficient, we point past that such approaches are not statistically efficient during learning precisely because spatial redundancy limits the information learned from neighboring pixels. we demonstrate that optimal classification of pixels allows finer search ( 1 ) add diversity from batch selection, speeding up learning ; ( 2 ) explore complex nonlinear predictors, hierarchical accuracy ; and ( 3 ) efficiently train name - of - the - art models tabula rasa ( i. e., \" from scratch \" ) for diverse pixel - labeling measurements. our single process produces state - of - the - art comparisons for color assessment on pascal - context dataset, surface width estimation on nyudv2 depth dataset, wide edge adjustment on bsds.", "histories": [["v1", "Tue, 21 Feb 2017 18:20:30 GMT  (8360kb,D)", "http://arxiv.org/abs/1702.06506v1", "Project Page:this http URLarXiv admin note: substantial text overlap witharXiv:1609.06694"]], "COMMENTS": "Project Page:this http URLarXiv admin note: substantial text overlap witharXiv:1609.06694", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["aayush bansal", "xinlei chen", "bryan russell", "abhinav gupta", "deva ramanan"], "accepted": false, "id": "1702.06506"}, "pdf": {"name": "1702.06506.pdf", "metadata": {"source": "CRF", "title": "PixelNet: Representation of the pixels, by the pixels, and for the pixels", "authors": ["Aayush Bansal", "Xinlei Chen", "Bryan Russell", "Abhinav Gupta", "Deva Ramanan"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "A number of computer vision problems can be formulated as a dense pixel-wise prediction problem. These include low-level tasks such as edge detection [21, 64, 94]\nand optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82]. Though such a formulation is attractive because of its generality, one obvious difficulty is the enormous associated output space. For example, a 100\u00d7 100 image with 10 discrete class labels per pixel yields an output label space of size 105. One strategy is to treat this as a spatially-invariant label prediction problem, where one predicts a separate label per pixel using a convolutional architecture. Neural networks with convolutional output predictions, also called Fully Convolutional Networks (FCNs) [16, 62, 65, 77], appear to be a promising architecture in this direction.\nBut is this the ideal formulation of dense pixel-labeling? While computationally efficient for generating predictions at test time, we argue that it is not statistically efficient for gradient-based learning. Stochastic gradient descent (SGD) assumes that training data are sampled independently and from an identical distribution (i.i.d.) [11]. Indeed, a commonly-used heuristic to ensure approximately i.i.d. samples is random permutation of the training data, which can significantly improve learnability [56]. It is well known that pixels in a given image are highly correlated and not independent [45]. Following this observation, one might be tempted to randomly permute pixels during learning, but this destroys the spatial regularity that convolutional architectures so cleverly exploit! In this paper, we explore the\n1\nar X\niv :1\n70 2.\n06 50\n6v 1\n[ cs\ntradeoff between statistical and computational efficiency for convolutional learning, and investigate simply sampling a modest number of pixels across a small number of images for each SGD batch update, exploiting convolutional processing where possible. Contributions: (1) We experimentally validate that, thanks to spatial correlations between pixels, just sampling a small number of pixels per image is sufficient for learning. More importantly, sampling allows us to train end-to-end particular non-linear models not earlier possible, and explore several avenues for improving both the efficiency and performance of FCN-based architectures. (2) In contrast to the vast majority of models that make use of pre-trained networks, we show that pixel-level optimization can be used to train models tabula rasa, or \u201cfrom scratch\u201d with simple random Gaussian initialization. Intuitively, pixel-level labels provide a large amount of supervision compared to image-level labels, given proper accounting of correlations. Without using any extra data, our model outperforms previous unsupervised/self-supervised approaches for semantic segmentation on PASCAL VOC-2012 [26], and is competitive to fine-tuning from pre-trained models for surface normal estimation. (3). Using a single architecture and without much modification in parameters, we show state-of-the-art performance for edge detection on BSDS [4], surface normal estimation on NYUDv2 depth dataset [83], and semantic segmentation on the PASCAL-Context dataset [68]."}, {"heading": "2. Background", "text": "In this section, we review related work by making use of a unified notation that will be used to describe our architecture. We address the pixel-wise prediction problem where, given an input image X , we seek to predict outputs Y . For pixel location p, the output can be binary Yp \u2208 {0, 1} (e.g., edge detection), multi-class Yp \u2208 {1, . . . ,K} (e.g., semantic segmentation), or real-valued Yp \u2208 RN (e.g., surface normal prediction). There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).\nConvolutional prediction: We explore spatiallyinvariant predictors f\u03b8,p(X) that are end-to-end trainable over model parameters \u03b8. The family of fully-convolutional and skip networks [65, 77] are illustrative examples that have been successfully applied to, e.g., edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75]. Because such architectures still produce separate predictions for each pixel, numerous approaches have explored post-processing steps that enforce spatial consistency across labels via e.g., bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93]. In contrast, our work does not make use\nof such contextual post-processing, in an effort to see how far a pure \u201cpixel-level\u201d architecture can be pushed.\nMultiscale features: Higher convolutional layers are typically associated with larger receptive fields that capture high-level global context. Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89]. Hariharan et al.[40] use the evocative term \u201chypercolumns\u201d to refer to features extracted from multiple layers that correspond to the same pixel. Let\nhp(X) = [c1(p), c2(p), . . . , cM (p)]\ndenote the multi-scale hypercolumn feature computed for pixel p, where ci(p) denotes the feature vector of convolutional responses from layer i centered at pixel p (and where we drop the explicit dependance on X to reduce clutter). Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme a\u0300 trous [63]), and deconvolution/unpooling [30, 62, 71]. We similarly make use of multi-scale features, along with sparse on-demand upsampling of filter responses, with the goal of reducing the memory footprints during learning.\nPixel-prediction: One may cast the pixel-wise prediction problem as operating over the hypercolumn features where, for pixel p, the final prediction is given by\nf\u03b8,p(X) = g(hp(X)).\nWe write \u03b8 to denote both parameters of the hypercolumn features h and the pixel-wise predictor g. Training involves back-propagating gradients via SGD to update \u03b8. Prior work has explored different designs for h and g. A dominant trend is defining a linear predictor on hypercolumn features, e.g., g = w \u00b7 hp. FCNs [62] point out that linear prediction can be efficiently implemented in a coarse-to-fine manner by upsampling coarse predictions (with deconvolution) rather than upsampling coarse features. DeepLab [16] incorporates filter dilation and applies similar deconvolution and linear-weighted fusion, in addition to reducing the dimensionality of the fully-connected layers to reduce memory footprint. ParseNet [60] added spatial context for a layer\u2019s responses by average pooling the feature responses, followed by normalization and concatenation. HED [94] output edge predictions from intermediate layers, which are deeply supervised, and fuses the predictions by linear weighting. Importantly, [67] and [27] are noteable exceptions to the linear trend in that non-linear predictors g are used. This does pose difficulties during learning - [67] precomputes and stores superpixel feature maps due to memory constraints, and so cannot be trained end-to-end.\nSampling: We demonstrate that sparse sampling of hypercolumn features allows for exploration of highly non-\nCNN\"Architecture\"\nlinear g, which in turn significantly boosts performance. Our insight is inspired by past approaches that use sampling to training networks for surface normal estimation [6] and image colorization [55], though we focus on general design principles by analyzing the impact of sampling for efficiency, accuracy, and tabula rasa learning for diverse tasks.\nAccelerating SGD: There exists a large literature on accelerating stochastic gradient descent. We refer the reader to [11] for an excellent introduction. Though naturally a sequential algorithm that processes one data example at a time, much recent work focuses on mini-batch methods that can exploit parallelism in GPU architectures [18] or clusters [18]. One general theme is efficient online approximation of second-order methods [10], which can model correlations between input features. Batch normalization [46] computes correlation statistics between samples in a batch, producing noticeable improvements in convergence speed. Our work builds similar insights directly into convolutional networks without explicit second-order statistics."}, {"heading": "3. PixelNet", "text": "This section describes our approach for pixel-wise prediction, making use of the notation introduced in the previous section. We first formalize our pixelwise prediction architecture, and then discuss statistically efficient mini-batch training.\nArchitecture: As in past work, our architecture makes use of multiscale convolutional features, which we write as a hypercolumn descriptor:\nhp = [c1(p), c2(p), . . . , cM (p)]\nWe learn a nonlinear predictor f\u03b8,p = g(hp) implemented as a multi-layer perceptron (MLP) [9] defined over hypercolumn features. We use a MLP, which can be implemented as a series of \u201cfully-connected\u201d layers followed by ReLU activation functions. Importantly, the last layer must be of size K, the number of class labels or real valued outputs being predicted. See Figure 2.\nSparse predictions: We now describe an efficient method for generating sparse pixel predictions, which will be used at train-time (for efficient mini-batch generation). Assume that we are given an image X and a sparse set of (sampled) pixel locations P \u2282 \u2126, where \u2126 is the set of all pixel positions.\n1. Perform a forward pass to compute dense convolutional responses at all layers {ci(p) : \u2200i, p \u2208 \u2126}\n2. For each sampled pixel p \u2208 P , compute its hypercolumn feature hp on demand as follows:\n(a) For each layer i, compute the 4 discrete locations in the feature map ci closest to p\n(b) Compute ci(p) via bilinear interpolation\n3. Rearrange the sparse of hypercolumn features {hp : p \u2208 P} into a matrix for downstream processing (e.g., MLP classification).\nThe above pipeline only computes |P | hypercolumn features rather than full dense set of size |\u2126|. We experimentally demonstrate that this approach offers an excellent tradeoff between amortized computation (to compute ci(p)) and reduced storage (to compute hp). Note that our multiscale sampling layer simply acts as a selection operation, for which a (sub) gradient can easily be defined. This means that backprop can also take advantage of sparse computations for nonlinear MLP layers and convolutional processing for the lower layers.\nMini-batch sampling: At each iteration of SGD training, the true gradient over the model parameters \u03b8 is approximated by computing the gradient over a relatively small set of samples from the training set. Approaches based on FCN [62] include features for all pixels from an image in a mini-batch. As nearby pixels in an image are highly correlated [45], sampling them will not hurt learning. To ensure a diverse set of pixels (while still enjoying the amortized benefits of convolutional processing), we use a modest number of pixels (\u223c2, 000) per image, but sample many images per batch. Naive computation of dense grid of hypercolumn descriptors takes almost all of the (GPU) memory, while 2, 000 samples takes a small amount using our sparse sampling layer. This allows us to explore more images per batch, significantly increasing sample diversity.\nDense predictions: We now describe an efficient method for generating dense pixel predictions with our network, which will be used at test-time. Dense prediction proceeds by following step (1) from above; and instead of sampling in (2) above, we take all the pixels now. This produces a dense grid of hypercolumn features, which are then (3) processed by pixel-wise MLPs implemented as 1x1 filters (representing each fully-connected layer). The memory intensive portion of this computation is the dense grid of\nhypercolumn features. This memory footprint is reasonable at test time because a single image can be processed at a time, but at train-time, we would like to train on batches containing many images as possible (to ensure diversity)."}, {"heading": "4. Analysis", "text": "In this section, we analyze the properties of pixel-level optimization using semantic segmentation and surface normal estimation to understand the design choices for pixellevel architectures. We chose the two varied tasks (classification and regression) for analysis to verify the generalizability of these findings. We use a single-scale 224 \u00d7 224 image as input. We also show sampling augmented with careful batch-normalization can allow for a model to be trained from scratch (without pre-trained ImageNet model as an initialization) for semantic segmentation and surface normal estimation. We explicitly compare the performance of our approach with previous approaches in Section 5. Default network: For most experiments we fine-tune a VGG-16 network [84]. VGG-16 has 13 convolutional layers and three fully-connected (fc) layers. The convolutional layers are denoted as {11, 12, 21, 22, 31, 32, 33, 41, 42, 43, 51, 52, 53}. Following [62], we transform the last two fc layers to convolutional filters1, and add them to the set of convolutional features that can be aggregated into our multiscale hypercolumn descriptor. To avoid confusion with the fc layers in our MLP, we will henceforth denote the fc layers of VGG-16 as conv-6 and conv-7. We use the following network architecture (unless otherwise specified): we extract hypercolumn features from conv-{12, 22, 33, 43, 53, 7} with on-demand interpolation. We define a MLP over hypercolumn features with 3 fully-connected (fc) layers of size 4, 096 followed by ReLU [53] activations, where the last layer outputs predictions for K classes (with a softmax/cross-entropy loss) or K outputs with a euclidean loss for regression. Semantic Segmentation: We use training images from PASCAL VOC-2012 [26] for semantic segmentation, and additional labels collected on 8498 images by Hariharan et al. [41]. We used the held-out (non-overlapping) validation set to show most analysis. However, at some places we have used the test set where we wanted to show comparison with previous approaches. We report results using the standard metrics of region intersection over union (IoU) averaged over classes (higher is better). We mention it as IoU (V) when using the validation set for evaluation, and IoU (T) when showing on test set. Surface Normal Estimation: The NYU Depth v2 dataset [83] is used to evaluate the surface normal maps.\n1For alignment purposes, we made a small change by adding a spatial padding of 3 cells for the convolutional counterpart of fc6 since the kernel size is 7\u00d7 7.\nThere are 1449 images, of which 795 are trainval and remaining 654 are used for evaluation. Additionally, there are 220, 000 frames extracted from raw Kinect data. We use the normals of Ladicky et al.[54] and Wang et al.[89], computed from depth data of Kinect, as ground truth for 1449 images and 220K images respectively. We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.25\u25e6, 22.5\u25e6, and 30\u25e6 \u2013 The first three criteria capture the mean, median, and RMSE of angular error, where lower is better. The last three criteria capture the percentage of pixels within a given angular error, where higher is better."}, {"heading": "4.1. Sampling", "text": "We examine how sampling a few pixels from a fixed set of images does not harm convergence. Given a fixed number of (5) images per batch, we find that sampling a small fraction (4%) of the pixels per image does not affect learnability (Figure 3 and Table 1). This validates our hypothesis that much of the training data for a pixel-level task is correlated within an image, implying that randomly sampling a few pixels is sufficient. Our results are consistent with those reported in Long et al. [62], who similarly examine the effect of sampling a fraction (25-50%) of patches per training image.\nLong et al. [62] also perform an additional experiment where the total number of pixels in a batch is kept constant when comparing different sampling strategies. While this ensures that each batch will contain more diverse pixels, each batch will also process a larger number of images. If there are no significant computational savings due to sampling, additional images will increase wall-clock time and slow convergence. In the next section, we show that adding additional computation after sampling (by replacing a linear classifier with a multi-layer perceptron) fundamentally changes this tradeoff (Table 4)."}, {"heading": "4.2. Linear vs. MLP", "text": "Most previous approaches have focussed on linear predictors combining the information from different convolutional layers (also called \u2018skip-connections\u2019). Here we contrast the performance of non-linear models via MLP with corresponding linear models. For this analysis, we use a VGG-16 (pre-trained on ImageNet) as initialization and use skip-connections from conv-{12, 22, 33, 43, 53, 7} layers to show the benefits of a non-linear model over a linear model. We randomly sample 2, 000 pixels per image from a set of five 224\u00d7224 images per SGD iteration for the optimization.\nA major challenge in using skip-connections is how to combine the information as the dynamic range varies across the different layers. The top-row in Table 2 shows how the model leads to degenerate outputs for semantic segmentation when \u2018naively\u2019 concatenating features from different convolutional layers in a linear model. Similar observation was made by [60]. To counter this issue, previous work has explored normalization [60], scaling [40], etc. We use batch-normalization [46] for the convolutional layers before concatenating them to properly train a model with a linear predictor. The middle-row in Table 2 shows how adding batch-normalization allows us to train a linear model for semantic segmentation, and improve the performance for surface normal estimation. While we have to take care of normalization for linear models, we do not need them while using a MLP and can naively concatenate features from different layers. The last row in Table 2 shows the performance on different tasks when using a MLP. Note that performance of linear model (with batch-normalization) is similar to one obtained by Hypercolum [40] (62.7%), and FCN [62] (62%). Deconvolution vs. on-demand compute: A naive implementation of our approach is to use deconvolution layers\nto upsample conv-layers, followed by feature concatenation, and mask out the pixel-level outputs. This is similar to the sampling experiment of Long et al. [62]. While reasonable for a linear model, naively computing a dense grid of hypercolumn descriptors and processing them with a MLP is impossible if conv-7 is included in the hypercolumn descriptor (the array dimensions exceed INT MAX). For practical purposes, if we consider skip-connections only from conv-{12, 22, 33, 43, 53} layers at cost of some performance, naive deconvolution would still take more than 12X memory compared to our approach. Slightly better would be masking the dense grid of hypercolumn descriptors before MLP processing, which is still 8X more expensive. Most computational savings come from not being required to keep an extra copy of the data required by deconvolution and concatenation operators. Table 3 highlights the differences in computational requirements between deconvolution vs. on-demand compute (for the more forgiving setting of {12, 33, 53}-layered hypercolumn features). Clearly, ondemand compute requires less resources.\nDoes statistical diversity matter? We now analyze the influence of statistical diversity on optimization given a fixed computational budget (7GB memory on a NVIDIA TITANX). We train a non-linear model using 1 image \u00d7 40,000 pixels per image vs. 5 images\u00d7 2,000 pixels per image. Table 4 shows that sampling fewer pixels from more images outperforms more pixels extracted from fewer images. This demonstrates that statistical diversity outweighs the computational savings in convolutional processing when a MLP\nclassifier is used."}, {"heading": "4.3. Training from scratch", "text": "Prevailing methods for training deep models make use of a pre-trained (e.g., ImageNet [79]) model as initialization for fine-tuning for the task at hand. Most network architectures (including ours) improve in performance with pre-trained models. A major concern is the availability of sufficient data to train deep models for pixel-level prediction problems. However, because our optimization is based on randomly-sampled pixels instead of images, there is potentially more unique data available for SGD to learn a model from a random initialization. We show how sampling and batch-normalization enables models to be trained from scratch. This enables our networks to be used in problems with limited training data, or where natural image data does not apply (e.g., molecular biology, tissue segmentation etc. [70, 95]). We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100]. Random Initialization: We randomly initialize the parameters of a VGG-16 network from a Gaussian distribution. Training a VGG-16 network architecture is not straight forward, and required stage-wise training for the image classification task [84]. It seems daunting to train such a model from scratch for a pixel-level task where we want to learn both coarse and fine information. In our experiments, we found batch normalization to be an effective tool for converging a model trained from scratch.\nWe train the models for semantic segmentation and surface normal estimation. The middle-row in Table 5 shows the performance for semantic segmentation and surface normal estimation trained from scratch. The model trained from scratch for surface normal estimation is within 2- 3% of current state-of-the-art performing method. The model for semantic segmentation achieves 48.7% on PASCAL VOC-2012 test set when trained from scratch. To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].\nSelf-Supervision via Geometry: We briefly present the performance of models trained from our pixel-level optimization in context of self-supervision. The task of surface normal estimation does not require any human-labels, and is primarily about capturing geometric information. In this section, we explore the applicability of fine-tuning a geometry model (trained from scratch) for more semantic tasks (such as semantic segmentation and object detection). Table 5 (last row) and Table 6 shows the performance of our approach on semantic segmentation and object detection respectively. Note that the NYU depth dataset is a small indoor scene dataset and does not contain most of the categories present in PASCAL VOC dataset. Despite this, it shows 4% (segmentation) and 9% (detection) improvement over naive scratch models. It is best known result for semantic segmentation in an unsupervised/self-supervised manner, and is competitive with the previous unsupervised work [20] on object detection2 that uses ImageNet (without labels), particularly on indoor scene furniture categories (e.g., chairs, sofa, table, tv, bottle). We posit that geometry is a good cue for unsupervised representation learning as it can learn from a few examples and can even generalize to previously unseen categories. Future work may utilize depth information from videos (c.f. [98]) and use them to train models for surface normal estimation. This can potentially provide knowledge about more general categories. Finally, we add a minor supervision by taking the geometrybased model fine-tuned for segmentation, and further finetuning it for object detection. We get an extra 5% boost over the performance."}, {"heading": "5. Generalizability", "text": "In this section we demonstrate the generalizability of PixelNet, and apply (with minor modifications) it to the high-level task of semantic segmentation, mid-level surface normal estimation, and the low-level task of edge detection. The in-depth analysis for each of these tasks are in appendices.\n2We used a single scale for object detection and use the same parameters as Fast-RCNN except a step-size of 70K, and fine-tuned it for 200K iterations. Doersch et al. [20] reports better results in a recent version by the use of multi-scale detection, and smarter initialization and rescaling."}, {"heading": "5.1. Semantic Segmentation", "text": "Training: For all the experiments we used the publicly available Caffe library [49]. All trained models and code will be released. We make use of ImageNet-pretrained values for all convolutional layers, but train our MLP layers \u201cfrom scratch\u201d with Gaussian initialization (\u03c3 = 10\u22123) and dropout [85] (r = 0.5). We fix momentum 0.9 and weight decay 0.0005 throughout the fine-tuning process. We use the following update schedule (unless otherwise specified): we tune the network for 80 epochs with a fixed learning rate (10\u22123), reducing the rate by 10\u00d7 twice every 8 epochs until we reach 10\u22125. Dataset: The PASCAL-Context dataset [4] augments the original sparse set of PASCAL VOC 2010 segmentation annotations [26] (defined for 20 categories) to pixel labels for the whole scene. While this requires more than 400 categories, we followed standard protocol and evaluate on the 59-class and 33-class subsets. The results for PASCAL VOC-2012 dataset [26] are in Appendix A. Evaluation Metrics: We report results on the standard metrics of pixel accuracy (AC) and region intersection over union (IU ) averaged over classes (higher is better). Both are calculated with DeepLab evaluation tools3. Results: Table 12 shows performance of our approach compared to previous work. Our approach without CRF does\n3https://bitbucket.org/deeplab/deeplab-public/\nbetter than previous approaches based on it. Due to space constraints, we show only one example output in Figure 6 and compare against FCN-8s [62]. Notice that we capture fine-scale details, such as the leg of birds. More analysis and details are in Appendix A."}, {"heading": "5.2. Surface Normal Estimation", "text": "We use NYU-v2 depth dataset, and same evaluation criteria as defined earlier in Section 4. We improve the stateof-the-art for surface normal estimation [6] using the analysis for general pixel-level optimization. While Bansal et al. [6] extracted hypercolumn features from 1\u00d71\u00d74096 conv7 of VGG-16, we provided sufficient padding at conv-6 to\nhave 4\u00d7 4\u00d7 4096 conv-7. This provided diversity in conv7 features for different pixels in a image instead of same conv-7 earlier. Further we use a multi-scale prediction to improve the results. Training: We use the same network architecture as described earlier. The last fc-layer of MLP has (\u03c3 = 5\u221710\u22123). We set the initial learning rate to 10\u22123, reducing the rate by 10\u00d7 after 50K SGD iteration. The network is trained for 60K iterations. Results: Table 13 compares our improved results with previous state-of-the-art approaches [6, 24]. More analysis and details are in Appendix B."}, {"heading": "5.3. Edge Detection", "text": "Dataset: The standard dataset for edge detection is BSDS500 [4], which consists of 200 training, 100 validation, and 200 testing images. Each image is annotated by\u223c5 humans to mark out the contours. We use the same augmented data (rotation, flipping, totaling 9600 images without resizing) used to train the state-of-the-art Holistically-nested edge detector (HED) [94]. We report numbers on the testing images. During training, we follow HED and only use positive labels where a consensus (\u2265 3 out of 5) of humans agreed. Training: We use the same baseline network and training strategy defined earlier in Section 5.1. A sigmoid crossentropy loss is used to determine the whether a pixel is belonging to an edge or not. Due to the highly skewed class distribution, we also normalized the gradients for positives and negatives in each batch (as in [94]). In case of skewed class label distribution, sampling offers the flexibility to let the model focus more on the rare classes. Results: Table 16 shows the performance of PixelNet for edge detection. The last 2 rows in Table 16 contrast the per-\nformance between uniform and biased sampling. Clearly biased sampling toward positives can help the performance. Qualitatively, we find our network tends to have better results for semantic-contours (e.g. around an object), particularly after including conv-7 features. Figure 9 shows some qualitative results comparing our network with the HED model. Interestingly, our model explicitly removed the edges inside the zebra, but when the model cannot recognize it (e.g. its head is out of the picture), it still marks the edges on the black-and-white stripes. Our model appears to be making use of more higher-level information than past work on edge detection. More analysis and details are in Appendix C."}, {"heading": "6. Discussion", "text": "We have described a convolutional pixel-level architecture that, with minor modifications, produces state-of-theart accuracy on diverse high-level, mid-level, and low-level tasks. We demonstrate results on highly-benchmarked semantic segmentation, surface normal estimation, and edge detection datasets. Our results are made possible by careful analysis of computational and statistical considerations associated with convolutional predictors. Convolution exploits spatial redundancy of pixel neighborhoods for efficient computation, but this redundancy also impedes learning. We propose a simple solution based on stratified sam-\npling that injects diversity while taking advantage of amortized convolutional processing. Finally, our efficient learning scheme allow us to explore nonlinear functions of multiscale features that encode both high-level context and lowlevel spatial detail, which appears relevant for most pixel prediction tasks.\nAppendices In Section A we present extended analysis of PixelNet architecture for semantic segmentation on PASCAL Context [68] and PASCAL VOC-2012 [26], and ablative analysis for parameter selection. In Section B we show comparison of improved surface normal with previous state-of-theart approaches on NYU-v2 depth dataset [83]. In Section C we compare our approach with prior work on edge detection on BSDS [4]. Note that we use the default network and training mentioned in the main draft."}, {"heading": "A. Semantic Segmentation", "text": "Dataset. The PASCAL-Context dataset [4] augments the original sparse set of PASCAL VOC 2010 segmentation annotations [26] (defined for 20 categories) to pixel labels for the whole scene. While this requires more than 400 categories, we followed standard protocol and evaluate on the 59-class and 33-class subsets. We also evaluated our approach on the standard PASCAL VOC-2012 dataset [26] to compare with a wide variety of approaches. Training. For all the experiments we used the publicly available Caffe library [49]. All trained models and code will be released. We make use of ImageNet-pretrained values for all convolutional layers, but train our MLP layers \u201cfrom scratch\u201d with Gaussian initialization (\u03c3 = 10\u22123) and dropout [85]. We fix momentum 0.9 and weight decay 0.0005 throughout the fine-tuning process. We use the following update schedule (unless otherwise specified): we tune the network for 80 epochs with a fixed learning rate (10\u22123), reducing the rate by 10\u00d7 twice every 8 epochs until we reach 10\u22125. Qualitative Results. We show qualitative outputs in Figure 6 and compare against FCN-8s [62]. Notice that we capture fine-scale details, such as the leg of birds (row 2) and plant leaves (row 3). Evaluation Metrics. We report results on the standard metrics of pixel accuracy (AC) and region intersection over union (IU ) averaged over classes (higher is better). Both are calculated with DeepLab evaluation tools4. Analysis-1: Dimension of MLP fc Layers. We analyze performance as a function of the size of the MLP fc layers. We experimented the following dimensions for our fc\n4https://bitbucket.org/deeplab/deeplab-public/\nlayers: {1024, 2048, 4096, 6144}. Table 10 lists the results. We use 5 images per SGD batch and sample 2000 pixels per image, and conv-{12, 22, 33, 43, 53} for skip connections to do this analysis. We can see that with more dimensions the network tends to learn better, potentially because it can capture more information (and with drop-out alleviating overfitting [85]). In the main paper we fix the size to 4, 096 as a good trade-off between performance and speed.\nAnalysis-2: Number of Mini-batch Samples. Continuing from our analysis on statistical diversity (Section 4.2 in main paper), we plot performance as a function of the number of sampled pixels per image. In the first sampling experiment, we fix the batch size to 5 images and sample {500, 1000, 2000, 4000} pixels from each image. We use conv-{12, 22, 33, 43, 53} for skip connections for this analysis. The results are shown in Table 11. We observe that: 1) even sampling only 500 pixels per image (on average 2% of the \u223c20, 000 pixels in an image) produces reasonable performance after just 96 epochs. 2) performance is roughly constant as we increase the number of samples.\nWe also perform experiments where the samples are drawn from the same image. When sampling 2000 pixels from a single image (comparable in size to batch of 500 pixels sampled from 5 images), performance dramatically drops. This phenomena consistently holds for additional pixels (Table 11, bottom rows), verifying our central thesis that statistical diversity of samples can trump the computational savings of convolutional processing during learning. Analysis-3: Adding conv-7. While our diagnostics reveal the importance of architecture design and sampling, our best results still do not quite reach the state-of-the-art. For example, a single-scale FCN-32s [62], without any lowlevel layers, can already achieve 35.1. This suggests that their penultimate conv-7 layer does capture cues relevant for pixel-level prediction. In practice, we find that simply concatenating conv-7 significantly improves performance.\nFollowing the same training process, the results of our model with conv-7 features are shown in Table 12. From this we can see that conv-7 is greatly helping the performance of semantic segmentation. Even with reduced scale, we are able to obtain a similar IU achieved by FCN-8s [62], without any extra modeling of context [16, 60, 93, 101].\nFor fair comparison, we also experimented with single scale training with 1) half scale 0.5\u00d7, and 2) full scale 1.0\u00d7 images. We use 5 images per SGD batch, and sample 2000 pixels per image. We find the results are better without 0.25\u00d7 training, reaching 37.4% and 37.6% IU , respectively, even closer to the FCN-8s performance (37.8% IU ). For the 33-class setting, we are already doing better with the baseline model plus conv-7.\nAnalysis-4: Multi-scale. All previous experiments process test images at a single scale (0.25\u00d7 or 0.5\u00d7 its original size), whereas most prior work [16, 60, 62, 101] use multiple scales from full-resolution images. A smaller scale allows the model to access more context when making a prediction, but this can hurt performance on small objects. Following past work, we explore test-time averaging of predictions across multiple scales. We tested combinations of 0.25\u00d7, 0.5\u00d7 and 1\u00d7. For efficiency, we just fine-tune\nthe model trained on small scales (right before reducing the learning rate for the first time) with an initial learning rate of 10\u22123 and step size of 8 epochs, and end training after 24 epochs. The results are also reported in Table 12. Multi-scale prediction generalizes much better (41.0% IU ). Note our pixel-wise predictions do not make use of contextual post-processing (even outperforming some methods that post-processes FCNs to do so [15, 101]). Evaluation on PASCAL VOC-2012 [26]. We use the same settings, and evaluate our approach on PASCAL VOC2012. Our approach, without any special consideration of parameters for this dataset, achieves mAP of 69.7%5. This is much better than previous approaches, e.g. 62.7% for Hypercolumns [40], 62% for FCN [62], 67% for DeepLab (without CRF) [16] etc. Our performance on VOC-2012\n5Per-class performance is available at http://host.robots.ox. ac.uk:8080/anonymous/PZH9WH.html.\nis similar to Mostajabi et al [67] despite the fact we use information from only 6 layers while they used information from all the layers. In addition, they use a rectangular region of 256\u00d7256 (called sub-scene) around the superpixels. We posit that fine-tuning (or back-propagating gradients to conv-layers) enables efficient and better learning with even lesser layers, and without extra sub-scene information in an end-to-end framework. Finally, the use of super-pixels in [67] inhibit capturing detailed segmentation mask (and rather gives \u201cblobby\u201d output), and it is computationally less-tractable to use their approach for per-pixel optimization as information for each pixel would be required to be stored on disk."}, {"heading": "B. Surface Normal Estimation", "text": "Dataset. The NYU Depth v2 dataset [83] is used to evaluate the surface normal maps. There are 1449 images, of which 795 are trainval and remaining 654 are used for evaluation. Additionally, there are 220, 000 frames extracted from raw Kinect data. We use the normals of Ladicky et al.[54] and Wang et al.[89], computed from depth data of Kinect, as ground truth for 1449 images and 220K images respectively. Evaluation Criteria. We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.25\u25e6, 22.5\u25e6, and 30\u25e6 \u2013 The first three criteria capture the mean, median, and RMSE of angular error, where lower is better. The last three criteria capture the percentage of pixels within a given angular error, where higher is better. Qualitative Results. We show two examples in Figure 7 demonstrating where the improvement comes for the sur-\nface normal estimation. Note how with multi-scale prediction, the room-layout including painting on the wall improved. Analysis-1: Global Scene Layout. We follow Bansal et al. [6], and present our results both with and without Manhattan-world rectification to fairly compare against previous approaches, as [31, 32, 89] use it and [24] do not. We rectify our normals using the vanishing point estimates from Hedau et al. [42]. Table 13 compares our approach with existing work. Similar to Bansal et al. [6], our approach performs worse with Manhattan-world rectification (unlike Fouhey et al. [31]) though it improves slightly on this criteria as well. Analysis-2: Local Object Layout. Bansal et al. [6] stressed the importance of fine details in the scene generally available around objects. We followed their [6] local object evaluation that considers only those pixels which belong to a particular object (such as chair, sofa and bed). Table 14 shows comparison of our approach with Wang et al. [89], Eigen and Fergus [24], and MarrRevisited (Bansal et al. [6]). We consistently improve the performance by 1-3% on all statistics for all the objects."}, {"heading": "C. Edge Detection", "text": "In this section, we demonstrate that our same architecture can produce state-of-the-art results for low-level edge detection. The standard dataset for edge detection is BSDS500 [4], which consists of 200 training, 100 validation, and 200 testing images. Each image is annotated by\u223c5 humans to mark out the contours. We use the same augmented data (rotation, flipping, totaling 9600 images without resizing) used to train the state-of-the-art Holistically-nested edge detector (HED) [94]. We report numbers on the testing images. During training, we follow HED and only use positive labels where a consensus (\u2265 3 out of 5) of humans agreed. Baseline. We use the same baseline network that was defined for semantic segmentation, only making use of pretrained conv layers. A sigmoid cross-entropy loss is used to determine the whether a pixel is belonging to an edge or not. Due to the highly skewed class distribution, we also normalized the gradients for positives and negatives in each batch (as in [94]). Training. We use our previous training strategy, consisting of batches of 5 images with a total sample size of 10, 000 pixels. Each image is randomly resized to half its scale (so 0.5 and 1.0 times) during learning. The initial learning rate is again set to 10\u22123. However, since the training data is already augmented, we found the network converges much faster than when training for segmentation. To avoid overtraining and over-fitting, we reduce the learning rate at 15 epochs and 20 epochs (by a factor of 10) and end training at 25 epochs. Baseline Results. The results on BSDS, along with other\nconcurrent methods, are reported in Table 16. We apply standard non-maximal suppression and thinning technique using the code provided by [21]. We evaluate the detection performance using three standard measures: fixed contour threshold (ODS), per-image best threshold (OIS), and average precision (AP).\nAnalysis-1: Sampling. Whereas uniform sampling sufficed for semantic segmentation [62], we found the extreme rarity of positive pixels in edge detection required focused sampling of positives. We compare different strategies for sampling a fixed number (2000 pixels per image) training\nexamples in Table 15. Two obvious approaches are uniform and balanced sampling with an equal ratio of positives and negatives (shown to be useful for object detection [17, 35]). We tried ratios of 0.25, 0.5 and 0.75, and found that balancing consistently improved performance6. Analysis-2: Adding conv-7. We previously found that adding features from higher layers is helpful for semantic segmentation. Are such high-level features also helpful for edge detection, generally regarded as a low-level task? To\n6Note that simple class balancing [94] in each batch is already used, so the performance gain is unlikely from label re-balancing.\nanswer this question, we again concatenated conv-7 features\nwith other conv layers { 12, 22, 33, 43, 53 }. Please refer to the results at Table 16, using the second sampling strategy. We find it still helps performance a bit, but not as significantly for semantic segmentation (clearly a high-level task). Our final results as a single output classifier are very com-\npetitive to the state-of-the-art.\nQualitatively, we find our network tends to have better results for semantic-contours (e.g. around an object), particularly after including conv-7 features. Figure 9 shows some qualitative results comparing our network with the HED model. Interestingly, our model explicitly removed the edges inside the zebra, but when the model cannot rec-\nognize it (e.g. its head is out of the picture), it still marks the edges on the black-and-white stripes. Our model appears to be making use of much higher-level information than past work on edge detection. Note to Readers: An earlier version of this work appeared on arXiv7. We have incorporated extensive analysis to un-\n7https://arxiv.org/pdf/1609.06694.pdf\nderstand the underlying design principles of general pixelprediction tasks, and how they can be trained from scratch. We will release the source code and required models on our project page. Acknowledgements: This work was in part supported by NSF Grants IIS 0954083, IIS 1618903, and support from Google and Facebook, and Uber Presidential Fellowship to AB."}], "references": [{"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "ICCV 2015, pages 37\u201345,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Analyzing the performance of multilayer neural networks for object recognition", "author": ["P. Agrawal", "R. Girshick", "J. Malik"], "venue": "ECCV.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic segmentation using regions and parts", "author": ["P. Arbel\u00e1ez", "B. Hariharan", "C. Gu", "S. Gupta", "L. Bourdev", "J. Malik"], "venue": "CVPR. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Contour detection and hierarchical image segmentation", "author": ["P. Arbelaez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "TPAMI,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A database and evaluation methodology for optical flow", "author": ["S. Baker", "D. Scharstein", "J. Lewis", "S. Roth", "M.J. Black", "R. Szeliski"], "venue": "IJCV, 92(1),", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Marr Revisited: 2D- 3D model alignment via surface normal prediction", "author": ["A. Bansal", "B. Russell", "A. Gupta"], "venue": "CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Midlevel elements for object detection", "author": ["A. Bansal", "A. Shrivastava", "C. Doersch", "A. Gupta"], "venue": "CoRR, abs/1504.07284,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "The fast bilateral solver", "author": ["J.T. Barron", "B. Poole"], "venue": "CoRR, abs/1511.03296,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": "Oxford university press,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "Sgd-qn: Careful quasi-newton stochastic gradient descent", "author": ["A. Bordes", "L. Bottou", "P. Gallinari"], "venue": "JMLR, 10,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Scene labeling with lstm recurrent neural networks", "author": ["W. Byeon", "T.M. Breuel", "F. Raue", "M. Liwicki"], "venue": "CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Realtime multiperson 2d pose estimation using part affinity fields", "author": ["Z. Cao", "T. Simon", "S.-E. Wei", "Y. Sheikh"], "venue": "arXiv preprint arXiv:1611.08050,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic segmentation with second-order pooling", "author": ["J. Carreira", "R. Caseiro", "J. Batista", "C. Sminchisescu"], "venue": "ECCV.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "CoRR, abs/1606.00915,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected CRFs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "An implementation of faster rcnn with study for region sampling", "author": ["X. Chen", "A. Gupta"], "venue": "arXiv preprint arXiv:1702.02138,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Structured forests for fast edge detection", "author": ["P. Doll\u00e1r", "C. Zitnick"], "venue": "ICCV,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast edge detection using structured forests", "author": ["P. Doll\u00e1r", "C.L. Zitnick"], "venue": "TPAMI, 37(8),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "CoRR, abs/1605.09782,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "ICCV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "NIPS,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "The PASCAL Visual Object Classes (VOC) Challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "TPAMI, 35(8),", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Object detection with discriminatively trained partbased models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan"], "venue": "PAMI, 32(9),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient graphbased image segmentation", "author": ["P.F. Felzenszwalb", "D.P. Huttenlocher"], "venue": "IJCV, 59(2),", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["P. Fischer", "A. Dosovitskiy", "E. Ilg", "P. H\u00e4usser", "C. Haz\u0131rba\u015f", "V. Golkov", "P. van der Smagt", "D. Cremers", "T. Brox"], "venue": "arXiv preprint arXiv:1504.06852,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Data-driven 3D primitives for single image understanding", "author": ["D.F. Fouhey", "A. Gupta", "M. Hebert"], "venue": "ICCV,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Unfolding an indoor origami world", "author": ["D.F. Fouhey", "A. Gupta", "M. Hebert"], "venue": "ECCV,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Single image 3D without a single 3D image", "author": ["D.F. Fouhey", "A. Gupta", "M. Hebert"], "venue": "ICCV,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised CNN for single view depth estimation: Geometry to the rescue", "author": ["R. Garg", "B.G.V. Kumar", "G. Carneiro", "I.D. Reid"], "venue": "ECCV, pages 740\u2013756,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast r-cnn", "author": ["R. Girshick"], "venue": "ICCV,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Using k-poselets for detecting people and localizing their keypoints", "author": ["G. Gkioxari", "B. Hariharan", "R. Girshick", "J. Malik"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Unsupervised learning of spatiotemporally coherent metrics", "author": ["R. Goroshin", "J. Bruna", "J. Tompson", "D. Eigen", "Y. Le- Cun"], "venue": "ICCV 2015, pages 4086\u20134093,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Decomposing a scene into geometric and semantically consistent regions", "author": ["S. Gould", "R. Fulton", "D. Koller"], "venue": "ICCV. IEEE,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Oriented edge forests for boundary detection", "author": ["S. Hallman", "C.C. Fowlkes"], "venue": "CVPR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "CVPR,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic contours from inverse detectors", "author": ["B. Hariharan", "P. Arbelez", "L. Bourdev", "S. Maji", "J. Malik"], "venue": "ICCV,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Recovering the spatial layout of cluttered rooms", "author": ["V. Hedau", "D. Hoiem", "D. Forsyth"], "venue": "ICCV,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Densebox: Unifying landmark localization with end to end object detection", "author": ["L. Huang", "Y. Yang", "Y. Deng", "Y. Yu"], "venue": "arXiv preprint arXiv:1509.04874,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Pixel-wise deep learning for contour detection", "author": ["J.-J. Hwang", "T.-L. Liu"], "venue": "arXiv preprint arXiv:1504.01989,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural Image Statistics: A Probabilistic Approach to Early Computational Vision., volume 39", "author": ["A. Hyv\u00e4rinen", "J. Hurri", "P.O. Hoyer"], "venue": "Springer Science & Business Media,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "D. Blei and F. Bach, editors, Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 448\u2013456. JMLR Workshop and Conference Proceedings,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Crisp boundary detection using pointwise mutual information", "author": ["P. Isola", "D. Zoran", "D. Krishnan", "E.H. Adelson"], "venue": "Computer Vision\u2013ECCV 2014, pages 799\u2013814. Springer,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning image representations tied to ego-motion", "author": ["D. Jayaraman", "K. Grauman"], "venue": "ICCV", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "ACMMM,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual boundary prediction: A deep neural prediction network and quality dissection", "author": ["J.J. Kivinen", "C.K. Williams", "N. Heess", "D. Technologies"], "venue": "AISTATS, volume 1, page 9,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory", "author": ["I. Kokkinos"], "venue": "CoRR, abs/1609.02132,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "NIPS,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminatively trained dense surface normal estimation", "author": ["L. Ladicky", "B. Zeisl", "M. Pollefeys"], "venue": "ECCV,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning representations for automatic colorization", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural networks: Tricks of the trade, pages 9\u201348. Springer,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised visual representation learning by graph-based consistent constraints", "author": ["D. Li", "W.-C. Hung", "J.-B. Huang", "S. Wang", "N. Ahuja", "M.-H. Yang"], "venue": "ECCV,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Sketch tokens: A learned mid-level representation for contour and object detection", "author": ["J. Lim", "C. Zitnick", "P. Doll\u00e1r"], "venue": "CVPR,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonparametric scene parsing via label transfer", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "TPAMI, 33(12),", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2011}, {"title": "Parsenet: Looking wider to see better", "author": ["W. Liu", "A. Rabinovich", "A.C. Berg"], "venue": "arXiv preprint arXiv:1506.04579,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CoRR, abs/1411.4038,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional models for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2015}, {"title": "A real-time algorithm for signal analysis with the help of the wavelet transform", "author": ["J.M.M. Holschneider", "R. Kronland-Martinet", "P. Tchamitchian"], "venue": "Wavelets, Time-Frequency Methods and Phase Space, pages 289\u2013 297,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning to detect natural image boundaries using local brightness, color, and texture cues", "author": ["D.R. Martin", "C.C. Fowlkes", "J. Malik"], "venue": "TPAMI, 26(5),", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2004}, {"title": "Multidigit recognition using a space displacement neural network", "author": ["O. Matan", "C.J. Burges", "Y. LeCun", "J.S. Denker"], "venue": "NIPS, pages 488\u2013495,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1991}, {"title": "Shuffle and Learn: Unsupervised Learning using Temporal Order Verification", "author": ["I. Misra", "C.L. Zitnick", "M. Hebert"], "venue": "ECCV,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2016}, {"title": "Feedforward semantic segmentation with zoom-out features", "author": ["M. Mostajabi", "P. Yadollahpour", "G. Shakhnarovich"], "venue": "CVPR, pages 3376\u20133385,", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2015}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "CVPR,", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2014}, {"title": "Stacked hierarchical labeling", "author": ["D. Munoz", "J.A. Bagnell", "M. Hebert"], "venue": "Computer Vision\u2013ECCV 2010, pages 57\u201370. Springer,", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2010}, {"title": "A visual approach to proteomics", "author": ["S. Nickell", "C. Kofler", "A.P. Leis", "W. Baumeister"], "venue": "Nature reviews Molecular cell biology, 7(3):225\u2013230,", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "ICCV,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "P. Favaro"], "venue": "ECCV,", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2016}, {"title": "Ambient sound provides supervision for visual learning", "author": ["A. Owens", "J. Wu", "J.H. McDermott", "W.T. Freeman", "A. Torralba"], "venue": "ECCV, pages 801\u2013816,", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Kr\u00e4henb\u00fchl", "J. Donahue", "T. Darrell", "A. Efros"], "venue": "CVPR,", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent convolutional neural networks for scene parsing", "author": ["P.H. Pinheiro", "R. Collobert"], "venue": "ICML,", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2014}, {"title": "The curious robot: Learning visual representations via physical interactions", "author": ["L. Pinto", "D. Gandhi", "Y. Han", "Y. Park", "A. Gupta"], "venue": "ECCV, pages 3\u201318,", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2016}, {"title": "Postal address block location using a convolutional locator network", "author": ["J.C. Platt", "R. Wolf"], "venue": "NIPS,", "citeRegEx": "77", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning to parse images of articulated bodies", "author": ["D. Ramanan"], "venue": "NIPS.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2007}, {"title": "ImageNet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "IJCV,", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2015}, {"title": "Associative hierarchical crfs for object class image segmentation", "author": ["C. Russell", "P. Kohli", "P.H. Torr"], "venue": "In ICCV. IEEE,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2009}, {"title": "3-d depth reconstruction from a single still image", "author": ["A. Saxena", "S.H. Chung", "A.Y. Ng"], "venue": "IJCV, 76(1),", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2008}, {"title": "Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context", "author": ["J. Shotton", "J. Winn", "C. Rother", "A. Criminisi"], "venue": "Int. Journal of Computer Vision (IJCV), January", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2009}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "ECCV,", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR, 15(1),", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to extract motion from videos in convolutional neural networks", "author": ["D. Teney", "M. Hebert"], "venue": "CoRR, abs/1601.07532,", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2016}, {"title": "Superparsing: scalable nonparametric image parsing with superpixels", "author": ["J. Tighe", "S. Lazebnik"], "venue": "Computer Vision\u2013ECCV 2010, pages 352\u2013365. Springer,", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2010}, {"title": "Auto-context and its application to highlevel vision tasks and 3d brain image segmentation", "author": ["Z. Tu", "X. Bai"], "venue": "TPAMI, 32(10),", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2010}, {"title": "Designing deep networks for surface normal estimation", "author": ["X. Wang", "D. Fouhey", "A. Gupta"], "venue": "CVPR,", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "ICCV 2015, pages 2794\u2013 2802,", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional pose machines", "author": ["S.-E. Wei", "V. Ramakrishna", "T. Kanade", "Y. Sheikh"], "venue": "CVPR,", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminatively trained sparse code gradients for contour detection", "author": ["R. Xiaofeng", "L. Bo"], "venue": "NIPS,", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional pseudo-prior for structured labeling", "author": ["S. Xie", "X. Huang", "Z. Tu"], "venue": "arXiv preprint arXiv:1511.07409,", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2015}, {"title": "Holistically-nested edge detection", "author": ["S. Xie", "Z. Tu"], "venue": "ICCV,", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2015}, {"title": "Automated target segmentation and real space fast alignment methods for high-throughput classification and averaging of crowded cryo-electron subtomograms", "author": ["M. Xu", "F. Alber"], "venue": "Bioinformatics, 29(13):i274\u2013i282,", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2013}, {"title": "Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation", "author": ["J. Yao", "S. Fidler", "R. Urtasun"], "venue": "CVPR. IEEE,", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "ICLR,", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2016}, {"title": "Consistent depth maps recovery from a video sequence", "author": ["G. Zhang", "J. Jia", "T.-T. Wong", "H. Bao"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(6):974\u2013 988,", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2009}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "ECCV,", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2016}, {"title": "Split-brain autoencoders: Unsupervised learning by cross-channel prediction", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "arXiv,", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H. Torr"], "venue": "ICCV,", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 53, "endOffset": 65}, {"referenceID": 63, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 53, "endOffset": 65}, {"referenceID": 93, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 53, "endOffset": 65}, {"referenceID": 4, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 83, "endOffset": 94}, {"referenceID": 29, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 83, "endOffset": 94}, {"referenceID": 85, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 83, "endOffset": 94}, {"referenceID": 5, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 142, "endOffset": 161}, {"referenceID": 23, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 142, "endOffset": 161}, {"referenceID": 24, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 142, "endOffset": 161}, {"referenceID": 80, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 142, "endOffset": 161}, {"referenceID": 88, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 142, "endOffset": 161}, {"referenceID": 12, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 212, "endOffset": 228}, {"referenceID": 35, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 212, "endOffset": 228}, {"referenceID": 77, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 212, "endOffset": 228}, {"referenceID": 90, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 212, "endOffset": 228}, {"referenceID": 42, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 247, "endOffset": 251}, {"referenceID": 15, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 279, "endOffset": 303}, {"referenceID": 26, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 279, "endOffset": 303}, {"referenceID": 39, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 279, "endOffset": 303}, {"referenceID": 61, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 279, "endOffset": 303}, {"referenceID": 66, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 279, "endOffset": 303}, {"referenceID": 81, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 279, "endOffset": 303}, {"referenceID": 15, "context": "Neural networks with convolutional output predictions, also called Fully Convolutional Networks (FCNs) [16, 62, 65, 77], appear to be a promising architecture in this direction.", "startOffset": 103, "endOffset": 119}, {"referenceID": 61, "context": "Neural networks with convolutional output predictions, also called Fully Convolutional Networks (FCNs) [16, 62, 65, 77], appear to be a promising architecture in this direction.", "startOffset": 103, "endOffset": 119}, {"referenceID": 64, "context": "Neural networks with convolutional output predictions, also called Fully Convolutional Networks (FCNs) [16, 62, 65, 77], appear to be a promising architecture in this direction.", "startOffset": 103, "endOffset": 119}, {"referenceID": 76, "context": "Neural networks with convolutional output predictions, also called Fully Convolutional Networks (FCNs) [16, 62, 65, 77], appear to be a promising architecture in this direction.", "startOffset": 103, "endOffset": 119}, {"referenceID": 10, "context": ") [11].", "startOffset": 2, "endOffset": 6}, {"referenceID": 55, "context": "samples is random permutation of the training data, which can significantly improve learnability [56].", "startOffset": 97, "endOffset": 101}, {"referenceID": 44, "context": "It is well known that pixels in a given image are highly correlated and not independent [45].", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "Without using any extra data, our model outperforms previous unsupervised/self-supervised approaches for semantic segmentation on PASCAL VOC-2012 [26], and is competitive to fine-tuning from pre-trained models for surface normal estimation.", "startOffset": 146, "endOffset": 150}, {"referenceID": 3, "context": "Using a single architecture and without much modification in parameters, we show state-of-the-art performance for edge detection on BSDS [4], surface normal estimation on NYUDv2 depth dataset [83], and semantic segmentation on the PASCAL-Context dataset [68].", "startOffset": 137, "endOffset": 140}, {"referenceID": 82, "context": "Using a single architecture and without much modification in parameters, we show state-of-the-art performance for edge detection on BSDS [4], surface normal estimation on NYUDv2 depth dataset [83], and semantic segmentation on the PASCAL-Context dataset [68].", "startOffset": 192, "endOffset": 196}, {"referenceID": 67, "context": "Using a single architecture and without much modification in parameters, we show state-of-the-art performance for edge detection on BSDS [4], surface normal estimation on NYUDv2 depth dataset [83], and semantic segmentation on the PASCAL-Context dataset [68].", "startOffset": 254, "endOffset": 258}, {"referenceID": 2, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 13, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 20, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 37, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 58, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 68, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 79, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 81, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 86, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 87, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 95, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 64, "context": "The family of fully-convolutional and skip networks [65, 77] are illustrative examples that have been successfully applied to, e.", "startOffset": 52, "endOffset": 60}, {"referenceID": 76, "context": "The family of fully-convolutional and skip networks [65, 77] are illustrative examples that have been successfully applied to, e.", "startOffset": 52, "endOffset": 60}, {"referenceID": 93, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 15, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 26, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 29, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 61, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 59, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 66, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 70, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 74, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 15, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 57, "endOffset": 70}, {"referenceID": 51, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 57, "endOffset": 70}, {"referenceID": 100, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 57, "endOffset": 70}, {"referenceID": 7, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 92, "endOffset": 95}, {"referenceID": 96, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 138, "endOffset": 142}, {"referenceID": 92, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 176, "endOffset": 180}, {"referenceID": 18, "context": "Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89].", "startOffset": 161, "endOffset": 185}, {"referenceID": 23, "context": "Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89].", "startOffset": 161, "endOffset": 185}, {"referenceID": 24, "context": "Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89].", "startOffset": 161, "endOffset": 185}, {"referenceID": 26, "context": "Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89].", "startOffset": 161, "endOffset": 185}, {"referenceID": 74, "context": "Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89].", "startOffset": 161, "endOffset": 185}, {"referenceID": 88, "context": "Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89].", "startOffset": 161, "endOffset": 185}, {"referenceID": 39, "context": "[40] use the evocative term \u201chypercolumns\u201d to refer to features extracted from multiple layers that correspond to the same pixel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme \u00e0 trous [63]), and deconvolution/unpooling [30, 62, 71].", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme \u00e0 trous [63]), and deconvolution/unpooling [30, 62, 71].", "startOffset": 120, "endOffset": 124}, {"referenceID": 62, "context": "Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme \u00e0 trous [63]), and deconvolution/unpooling [30, 62, 71].", "startOffset": 161, "endOffset": 165}, {"referenceID": 29, "context": "Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme \u00e0 trous [63]), and deconvolution/unpooling [30, 62, 71].", "startOffset": 196, "endOffset": 208}, {"referenceID": 61, "context": "Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme \u00e0 trous [63]), and deconvolution/unpooling [30, 62, 71].", "startOffset": 196, "endOffset": 208}, {"referenceID": 70, "context": "Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme \u00e0 trous [63]), and deconvolution/unpooling [30, 62, 71].", "startOffset": 196, "endOffset": 208}, {"referenceID": 61, "context": "FCNs [62] point out that linear prediction can be efficiently implemented in a coarse-to-fine manner by upsampling coarse predictions (with deconvolution) rather than upsampling coarse features.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "DeepLab [16] incorporates filter dilation and applies similar deconvolution and linear-weighted fusion, in addition to reducing the dimensionality of the fully-connected layers to reduce memory footprint.", "startOffset": 8, "endOffset": 12}, {"referenceID": 59, "context": "ParseNet [60] added spatial context for a layer\u2019s responses by average pooling the feature responses, followed by normalization and concatenation.", "startOffset": 9, "endOffset": 13}, {"referenceID": 93, "context": "HED [94] output edge predictions from intermediate layers, which are deeply supervised, and fuses the predictions by linear weighting.", "startOffset": 4, "endOffset": 8}, {"referenceID": 66, "context": "Importantly, [67] and [27] are noteable exceptions to the linear trend in that non-linear predictors g are used.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Importantly, [67] and [27] are noteable exceptions to the linear trend in that non-linear predictors g are used.", "startOffset": 22, "endOffset": 26}, {"referenceID": 66, "context": "This does pose difficulties during learning - [67] precomputes and stores superpixel feature maps due to memory constraints, and so cannot be trained end-to-end.", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": "Our insight is inspired by past approaches that use sampling to training networks for surface normal estimation [6] and image colorization [55], though we focus on general design principles by analyzing the impact of sampling for efficiency, accuracy, and tabula rasa learning for diverse tasks.", "startOffset": 112, "endOffset": 115}, {"referenceID": 54, "context": "Our insight is inspired by past approaches that use sampling to training networks for surface normal estimation [6] and image colorization [55], though we focus on general design principles by analyzing the impact of sampling for efficiency, accuracy, and tabula rasa learning for diverse tasks.", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "We refer the reader to [11] for an excellent introduction.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "Though naturally a sequential algorithm that processes one data example at a time, much recent work focuses on mini-batch methods that can exploit parallelism in GPU architectures [18] or clusters [18].", "startOffset": 180, "endOffset": 184}, {"referenceID": 17, "context": "Though naturally a sequential algorithm that processes one data example at a time, much recent work focuses on mini-batch methods that can exploit parallelism in GPU architectures [18] or clusters [18].", "startOffset": 197, "endOffset": 201}, {"referenceID": 9, "context": "One general theme is efficient online approximation of second-order methods [10], which can model correlations between input features.", "startOffset": 76, "endOffset": 80}, {"referenceID": 45, "context": "Batch normalization [46] computes correlation statistics between samples in a batch, producing noticeable improvements in convergence speed.", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "We learn a nonlinear predictor f\u03b8,p = g(hp) implemented as a multi-layer perceptron (MLP) [9] defined over hypercolumn features.", "startOffset": 90, "endOffset": 93}, {"referenceID": 61, "context": "Approaches based on FCN [62] include features for all pixels from an image in a mini-batch.", "startOffset": 24, "endOffset": 28}, {"referenceID": 44, "context": "As nearby pixels in an image are highly correlated [45], sampling them will not hurt learning.", "startOffset": 51, "endOffset": 55}, {"referenceID": 83, "context": "Default network: For most experiments we fine-tune a VGG-16 network [84].", "startOffset": 68, "endOffset": 72}, {"referenceID": 61, "context": "Following [62], we transform the last two fc layers to convolutional filters1, and add them to the set of convolutional features that can be aggregated into our multiscale hypercolumn descriptor.", "startOffset": 10, "endOffset": 14}, {"referenceID": 52, "context": "We define a MLP over hypercolumn features with 3 fully-connected (fc) layers of size 4, 096 followed by ReLU [53] activations, where the last layer outputs predictions for K classes (with a softmax/cross-entropy loss) or K outputs with a euclidean loss for regression.", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "Semantic Segmentation: We use training images from PASCAL VOC-2012 [26] for semantic segmentation, and additional labels collected on 8498 images by Hariharan et al.", "startOffset": 67, "endOffset": 71}, {"referenceID": 40, "context": "[41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 82, "context": "Surface Normal Estimation: The NYU Depth v2 dataset [83] is used to evaluate the surface normal maps.", "startOffset": 52, "endOffset": 56}, {"referenceID": 53, "context": "[54] and Wang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 88, "context": "[89], computed from depth data of Kinect, as ground truth for 1449 images and 220K images respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 23, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 30, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 31, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 32, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 88, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 61, "context": "[62], who similarly examine the effect of sampling a fraction (25-50%) of patches per training image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[62] also perform an additional experiment where the total number of pixels in a batch is kept constant when comparing different sampling strategies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "Similar observation was made by [60].", "startOffset": 32, "endOffset": 36}, {"referenceID": 59, "context": "To counter this issue, previous work has explored normalization [60], scaling [40], etc.", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "To counter this issue, previous work has explored normalization [60], scaling [40], etc.", "startOffset": 78, "endOffset": 82}, {"referenceID": 45, "context": "We use batch-normalization [46] for the convolutional layers before concatenating them to properly train a model with a linear predictor.", "startOffset": 27, "endOffset": 31}, {"referenceID": 39, "context": "Note that performance of linear model (with batch-normalization) is similar to one obtained by Hypercolum [40] (62.", "startOffset": 106, "endOffset": 110}, {"referenceID": 61, "context": "7%), and FCN [62] (62%).", "startOffset": 13, "endOffset": 17}, {"referenceID": 61, "context": "FCN-32s [62] 4,096 50,176 2,010 518 20.", "startOffset": 8, "endOffset": 12}, {"referenceID": 61, "context": "0 FCN-8s [62] 4,864 50,176 2,056 570 19.", "startOffset": 9, "endOffset": 13}, {"referenceID": 61, "context": "We compared our network with FCN [62] where a deconvolution layer is used to upsample the result in various settings.", "startOffset": 33, "endOffset": 37}, {"referenceID": 61, "context": "Besides FCN-8s and FCN-32s here we first compute the upsampled feature map, and then apply the classifiers for FCN [62].", "startOffset": 115, "endOffset": 119}, {"referenceID": 61, "context": "[62].", "startOffset": 0, "endOffset": 4}, {"referenceID": 78, "context": ", ImageNet [79]) model as initialization for fine-tuning for the task at hand.", "startOffset": 11, "endOffset": 15}, {"referenceID": 69, "context": "[70, 95]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 94, "context": "[70, 95]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 19, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 22, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 33, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 36, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 47, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 56, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 54, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 65, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 71, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 72, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 73, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 75, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 89, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 98, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 99, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 83, "context": "Training a VGG-16 network architecture is not straight forward, and required stage-wise training for the image classification task [84].", "startOffset": 131, "endOffset": 135}, {"referenceID": 19, "context": "To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].", "startOffset": 189, "endOffset": 209}, {"referenceID": 54, "context": "To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].", "startOffset": 189, "endOffset": 209}, {"referenceID": 73, "context": "To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].", "startOffset": 189, "endOffset": 209}, {"referenceID": 89, "context": "To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].", "startOffset": 189, "endOffset": 209}, {"referenceID": 98, "context": "To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].", "startOffset": 189, "endOffset": 209}, {"referenceID": 78, "context": "To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].", "startOffset": 244, "endOffset": 248}, {"referenceID": 19, "context": "It is best known result for semantic segmentation in an unsupervised/self-supervised manner, and is competitive with the previous unsupervised work [20] on object detection2 that uses ImageNet (without labels), particularly on indoor scene furniture categories (e.", "startOffset": 148, "endOffset": 152}, {"referenceID": 97, "context": "[98]) and use them to train models for surface normal estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] reports better results in a recent version by the use of multi-scale detection, and smarter initialization and rescaling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "DPM-v5 [28] 33.", "startOffset": 7, "endOffset": 11}, {"referenceID": 6, "context": "HOG+MID [7] 51.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "RCNN-Scratch [2] 49.", "startOffset": 13, "endOffset": 16}, {"referenceID": 19, "context": "VGG-16-Scratch [20] 56.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "VGG-16-Context-v2 [20] 63.", "startOffset": 18, "endOffset": 22}, {"referenceID": 34, "context": "VGG-16-ImageNet [35] 73.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Evaluation on VOC-2007: Our model (VGG-16-Geometry) trained on a few indoor scene examples of NYU-depth dataset performs 9% better than scratch, and is competitive with [20] that used images from ImageNet (without labels) to train.", "startOffset": 169, "endOffset": 173}, {"referenceID": 19, "context": "[20] required training for around 8 weeks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "Training: For all the experiments we used the publicly available Caffe library [49].", "startOffset": 79, "endOffset": 83}, {"referenceID": 84, "context": "We make use of ImageNet-pretrained values for all convolutional layers, but train our MLP layers \u201cfrom scratch\u201d with Gaussian initialization (\u03c3 = 10\u22123) and dropout [85] (r = 0.", "startOffset": 164, "endOffset": 168}, {"referenceID": 3, "context": "Dataset: The PASCAL-Context dataset [4] augments the original sparse set of PASCAL VOC 2010 segmentation annotations [26] (defined for 20 categories) to pixel labels for the whole scene.", "startOffset": 36, "endOffset": 39}, {"referenceID": 25, "context": "Dataset: The PASCAL-Context dataset [4] augments the original sparse set of PASCAL VOC 2010 segmentation annotations [26] (defined for 20 categories) to pixel labels for the whole scene.", "startOffset": 117, "endOffset": 121}, {"referenceID": 25, "context": "The results for PASCAL VOC-2012 dataset [26] are in Appendix A.", "startOffset": 40, "endOffset": 44}, {"referenceID": 60, "context": "FCN-8s [61] 46.", "startOffset": 7, "endOffset": 11}, {"referenceID": 61, "context": "5 FCN-8s [62] 50.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "8 DeepLab (v2 [15]) 37.", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "DeepLab (v2) + CRF [15] 39.", "startOffset": 19, "endOffset": 23}, {"referenceID": 100, "context": "6 CRF-RNN [101] 39.", "startOffset": 10, "endOffset": 15}, {"referenceID": 92, "context": "3 ConvPP-8 [93] 41.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "Evaluation on PASCAL-Context [4]: Most recent approaches [15, 93, 101] except FCN-8s, use spatial context postprocessing.", "startOffset": 29, "endOffset": 32}, {"referenceID": 14, "context": "Evaluation on PASCAL-Context [4]: Most recent approaches [15, 93, 101] except FCN-8s, use spatial context postprocessing.", "startOffset": 57, "endOffset": 70}, {"referenceID": 92, "context": "Evaluation on PASCAL-Context [4]: Most recent approaches [15, 93, 101] except FCN-8s, use spatial context postprocessing.", "startOffset": 57, "endOffset": 70}, {"referenceID": 100, "context": "Evaluation on PASCAL-Context [4]: Most recent approaches [15, 93, 101] except FCN-8s, use spatial context postprocessing.", "startOffset": 57, "endOffset": 70}, {"referenceID": 61, "context": "Due to space constraints, we show only one example output in Figure 6 and compare against FCN-8s [62].", "startOffset": 97, "endOffset": 101}, {"referenceID": 5, "context": "We improve the stateof-the-art for surface normal estimation [6] using the analysis for general pixel-level optimization.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "[6] extracted hypercolumn features from 1\u00d71\u00d74096 conv7 of VGG-16, we provided sufficient padding at conv-6 to", "startOffset": 0, "endOffset": 3}, {"referenceID": 93, "context": "Notice that our approach generates more semantic edges for zebra compared to HED [94].", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "[31] 35.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "E-F (VGG-16) [24] 20.", "startOffset": 13, "endOffset": 17}, {"referenceID": 50, "context": "UberNet (1-Task) [51] 21.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "MarrRevisited [6] 19.", "startOffset": 14, "endOffset": 17}, {"referenceID": 82, "context": "Evaluation on NYUv2 depth dataset [83]: We improve the previous state-of-the-art [6] using the analysis from general pixel-level prediction problems, and multi-scale prediction.", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "Evaluation on NYUv2 depth dataset [83]: We improve the previous state-of-the-art [6] using the analysis from general pixel-level prediction problems, and multi-scale prediction.", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "Results: Table 13 compares our improved results with previous state-of-the-art approaches [6, 24].", "startOffset": 90, "endOffset": 97}, {"referenceID": 23, "context": "Results: Table 13 compares our improved results with previous state-of-the-art approaches [6, 24].", "startOffset": 90, "endOffset": 97}, {"referenceID": 3, "context": "Dataset: The standard dataset for edge detection is BSDS500 [4], which consists of 200 training, 100 validation, and 200 testing images.", "startOffset": 60, "endOffset": 63}, {"referenceID": 93, "context": "We use the same augmented data (rotation, flipping, totaling 9600 images without resizing) used to train the state-of-the-art Holistically-nested edge detector (HED) [94].", "startOffset": 166, "endOffset": 170}, {"referenceID": 93, "context": "Due to the highly skewed class distribution, we also normalized the gradients for positives and negatives in each batch (as in [94]).", "startOffset": 127, "endOffset": 131}, {"referenceID": 3, "context": "Human [4] .", "startOffset": 6, "endOffset": 9}, {"referenceID": 21, "context": "SE-Var [22] .", "startOffset": 7, "endOffset": 11}, {"referenceID": 38, "context": "803 OEF [39] .", "startOffset": 8, "endOffset": 12}, {"referenceID": 49, "context": "DeepNets [50] .", "startOffset": 9, "endOffset": 13}, {"referenceID": 43, "context": "758 CSCNN [44] .", "startOffset": 10, "endOffset": 14}, {"referenceID": 93, "context": "798 HED [94] (Updated version) .", "startOffset": 8, "endOffset": 12}, {"referenceID": 50, "context": "840 UberNet (1-Task) [51] .", "startOffset": 21, "endOffset": 25}, {"referenceID": 3, "context": "Evaluation on BSDS [4]: Our approach performs better than previous approaches specifically trained for edge detection.", "startOffset": 19, "endOffset": 22}, {"referenceID": 67, "context": "In Section A we present extended analysis of PixelNet architecture for semantic segmentation on PASCAL Context [68] and PASCAL VOC-2012 [26], and ablative analysis for parameter selection.", "startOffset": 111, "endOffset": 115}, {"referenceID": 25, "context": "In Section A we present extended analysis of PixelNet architecture for semantic segmentation on PASCAL Context [68] and PASCAL VOC-2012 [26], and ablative analysis for parameter selection.", "startOffset": 136, "endOffset": 140}, {"referenceID": 82, "context": "In Section B we show comparison of improved surface normal with previous state-of-theart approaches on NYU-v2 depth dataset [83].", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "In Section C we compare our approach with prior work on edge detection on BSDS [4].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "The PASCAL-Context dataset [4] augments the original sparse set of PASCAL VOC 2010 segmentation annotations [26] (defined for 20 categories) to pixel labels for the whole scene.", "startOffset": 27, "endOffset": 30}, {"referenceID": 25, "context": "The PASCAL-Context dataset [4] augments the original sparse set of PASCAL VOC 2010 segmentation annotations [26] (defined for 20 categories) to pixel labels for the whole scene.", "startOffset": 108, "endOffset": 112}, {"referenceID": 25, "context": "We also evaluated our approach on the standard PASCAL VOC-2012 dataset [26] to compare with a wide variety of approaches.", "startOffset": 71, "endOffset": 75}, {"referenceID": 48, "context": "For all the experiments we used the publicly available Caffe library [49].", "startOffset": 69, "endOffset": 73}, {"referenceID": 84, "context": "We make use of ImageNet-pretrained values for all convolutional layers, but train our MLP layers \u201cfrom scratch\u201d with Gaussian initialization (\u03c3 = 10\u22123) and dropout [85].", "startOffset": 164, "endOffset": 168}, {"referenceID": 61, "context": "We show qualitative outputs in Figure 6 and compare against FCN-8s [62].", "startOffset": 67, "endOffset": 71}, {"referenceID": 84, "context": "We can see that with more dimensions the network tends to learn better, potentially because it can capture more information (and with drop-out alleviating overfitting [85]).", "startOffset": 167, "endOffset": 171}, {"referenceID": 61, "context": "For example, a single-scale FCN-32s [62], without any lowlevel layers, can already achieve 35.", "startOffset": 36, "endOffset": 40}, {"referenceID": 61, "context": "Even with reduced scale, we are able to obtain a similar IU achieved by FCN-8s [62], without any extra modeling of context [16, 60, 93, 101].", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "Even with reduced scale, we are able to obtain a similar IU achieved by FCN-8s [62], without any extra modeling of context [16, 60, 93, 101].", "startOffset": 123, "endOffset": 140}, {"referenceID": 59, "context": "Even with reduced scale, we are able to obtain a similar IU achieved by FCN-8s [62], without any extra modeling of context [16, 60, 93, 101].", "startOffset": 123, "endOffset": 140}, {"referenceID": 92, "context": "Even with reduced scale, we are able to obtain a similar IU achieved by FCN-8s [62], without any extra modeling of context [16, 60, 93, 101].", "startOffset": 123, "endOffset": 140}, {"referenceID": 100, "context": "Even with reduced scale, we are able to obtain a similar IU achieved by FCN-8s [62], without any extra modeling of context [16, 60, 93, 101].", "startOffset": 123, "endOffset": 140}, {"referenceID": 15, "context": "5\u00d7 its original size), whereas most prior work [16, 60, 62, 101] use multiple scales from full-resolution images.", "startOffset": 47, "endOffset": 64}, {"referenceID": 59, "context": "5\u00d7 its original size), whereas most prior work [16, 60, 62, 101] use multiple scales from full-resolution images.", "startOffset": 47, "endOffset": 64}, {"referenceID": 61, "context": "5\u00d7 its original size), whereas most prior work [16, 60, 62, 101] use multiple scales from full-resolution images.", "startOffset": 47, "endOffset": 64}, {"referenceID": 100, "context": "5\u00d7 its original size), whereas most prior work [16, 60, 62, 101] use multiple scales from full-resolution images.", "startOffset": 47, "endOffset": 64}, {"referenceID": 14, "context": "Note our pixel-wise predictions do not make use of contextual post-processing (even outperforming some methods that post-processes FCNs to do so [15, 101]).", "startOffset": 145, "endOffset": 154}, {"referenceID": 100, "context": "Note our pixel-wise predictions do not make use of contextual post-processing (even outperforming some methods that post-processes FCNs to do so [15, 101]).", "startOffset": 145, "endOffset": 154}, {"referenceID": 25, "context": "Evaluation on PASCAL VOC-2012 [26].", "startOffset": 30, "endOffset": 34}, {"referenceID": 39, "context": "7% for Hypercolumns [40], 62% for FCN [62], 67% for DeepLab (without CRF) [16] etc.", "startOffset": 20, "endOffset": 24}, {"referenceID": 61, "context": "7% for Hypercolumns [40], 62% for FCN [62], 67% for DeepLab (without CRF) [16] etc.", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "7% for Hypercolumns [40], 62% for FCN [62], 67% for DeepLab (without CRF) [16] etc.", "startOffset": 74, "endOffset": 78}, {"referenceID": 66, "context": "is similar to Mostajabi et al [67] despite the fact we use information from only 6 layers while they used information from all the layers.", "startOffset": 30, "endOffset": 34}, {"referenceID": 66, "context": "Finally, the use of super-pixels in [67] inhibit capturing detailed segmentation mask (and rather gives \u201cblobby\u201d output), and it is computationally less-tractable to use their approach for per-pixel optimization as information for each pixel would be required to be stored on disk.", "startOffset": 36, "endOffset": 40}, {"referenceID": 82, "context": "The NYU Depth v2 dataset [83] is used to evaluate the surface normal maps.", "startOffset": 25, "endOffset": 29}, {"referenceID": 53, "context": "[54] and Wang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 88, "context": "[89], computed from depth data of Kinect, as ground truth for 1449 images and 220K images respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 23, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 30, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 31, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 32, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 88, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 5, "context": "[6], and present our results both with and without Manhattan-world rectification to fairly compare against previous approaches, as [31, 32, 89] use it and [24] do not.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[6], and present our results both with and without Manhattan-world rectification to fairly compare against previous approaches, as [31, 32, 89] use it and [24] do not.", "startOffset": 131, "endOffset": 143}, {"referenceID": 31, "context": "[6], and present our results both with and without Manhattan-world rectification to fairly compare against previous approaches, as [31, 32, 89] use it and [24] do not.", "startOffset": 131, "endOffset": 143}, {"referenceID": 88, "context": "[6], and present our results both with and without Manhattan-world rectification to fairly compare against previous approaches, as [31, 32, 89] use it and [24] do not.", "startOffset": 131, "endOffset": 143}, {"referenceID": 23, "context": "[6], and present our results both with and without Manhattan-world rectification to fairly compare against previous approaches, as [31, 32, 89] use it and [24] do not.", "startOffset": 155, "endOffset": 159}, {"referenceID": 41, "context": "[42].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6], our approach performs worse with Manhattan-world rectification (unlike Fouhey et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[31]) though it improves slightly on this criteria as well.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] stressed the importance of fine details in the scene generally available around objects.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "We followed their [6] local object evaluation that considers only those pixels which belong to a particular object (such as chair, sofa and bed).", "startOffset": 18, "endOffset": 21}, {"referenceID": 88, "context": "[89], Eigen and Fergus [24], and MarrRevisited (Bansal et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[89], Eigen and Fergus [24], and MarrRevisited (Bansal et al.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "[6]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The standard dataset for edge detection is BSDS500 [4], which consists of 200 training, 100 validation, and 200 testing images.", "startOffset": 51, "endOffset": 54}, {"referenceID": 93, "context": "We use the same augmented data (rotation, flipping, totaling 9600 images without resizing) used to train the state-of-the-art Holistically-nested edge detector (HED) [94].", "startOffset": 166, "endOffset": 170}, {"referenceID": 93, "context": "Due to the highly skewed class distribution, we also normalized the gradients for positives and negatives in each batch (as in [94]).", "startOffset": 127, "endOffset": 131}, {"referenceID": 60, "context": "FCN-8s [61] 46.", "startOffset": 7, "endOffset": 11}, {"referenceID": 61, "context": "5 FCN-8s [62] 50.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "8 DeepLab (v2 [15]) - 37.", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "DeepLab (v2) + CRF [15] - 39.", "startOffset": 19, "endOffset": 23}, {"referenceID": 100, "context": "6 CRF-RNN [101] - 39.", "startOffset": 10, "endOffset": 15}, {"referenceID": 59, "context": "3 ParseNet [60] - 40.", "startOffset": 11, "endOffset": 15}, {"referenceID": 92, "context": "4 ConvPP-8 [93] - 41.", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "Note that while most recent approaches spatial context postprocessing [15, 60, 93, 101], we focus on the FCN [62] per-pixel predictor as most approaches are its descendants.", "startOffset": 70, "endOffset": 87}, {"referenceID": 59, "context": "Note that while most recent approaches spatial context postprocessing [15, 60, 93, 101], we focus on the FCN [62] per-pixel predictor as most approaches are its descendants.", "startOffset": 70, "endOffset": 87}, {"referenceID": 92, "context": "Note that while most recent approaches spatial context postprocessing [15, 60, 93, 101], we focus on the FCN [62] per-pixel predictor as most approaches are its descendants.", "startOffset": 70, "endOffset": 87}, {"referenceID": 100, "context": "Note that while most recent approaches spatial context postprocessing [15, 60, 93, 101], we focus on the FCN [62] per-pixel predictor as most approaches are its descendants.", "startOffset": 70, "endOffset": 87}, {"referenceID": 61, "context": "Note that while most recent approaches spatial context postprocessing [15, 60, 93, 101], we focus on the FCN [62] per-pixel predictor as most approaches are its descendants.", "startOffset": 109, "endOffset": 113}, {"referenceID": 5, "context": "[6]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "We apply standard non-maximal suppression and thinning technique using the code provided by [21].", "startOffset": 92, "endOffset": 96}, {"referenceID": 61, "context": "Whereas uniform sampling sufficed for semantic segmentation [62], we found the extreme rarity of positive pixels in edge detection required focused sampling of positives.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "Two obvious approaches are uniform and balanced sampling with an equal ratio of positives and negatives (shown to be useful for object detection [17, 35]).", "startOffset": 145, "endOffset": 153}, {"referenceID": 34, "context": "Two obvious approaches are uniform and balanced sampling with an equal ratio of positives and negatives (shown to be useful for object detection [17, 35]).", "startOffset": 145, "endOffset": 153}, {"referenceID": 93, "context": "6Note that simple class balancing [94] in each batch is already used, so the performance gain is unlikely from label re-balancing.", "startOffset": 34, "endOffset": 38}, {"referenceID": 30, "context": "[31] 35.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "E-F (VGG-16) [24] 20.", "startOffset": 13, "endOffset": 17}, {"referenceID": 50, "context": "UberNet (1-Task) [51] 21.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "MarrRevisited [6] 19.", "startOffset": 14, "endOffset": 17}, {"referenceID": 88, "context": "[89] 26.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] 35.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] 36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "MarrRevisited [6] 23.", "startOffset": 14, "endOffset": 17}, {"referenceID": 82, "context": "Evaluation on NYUv2 depth dataset [83]: Global Scene Layout.", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "We improve the previous state-of-the-art [6] using the analysis from general pixel-level prediction problems, and multi-scale prediction.", "startOffset": 41, "endOffset": 44}, {"referenceID": 88, "context": "[89] 44.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "E-F (AlexNet) [24] 38.", "startOffset": 14, "endOffset": 18}, {"referenceID": 23, "context": "E-F (VGG-16) [24] 33.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "MarrRevisited [6] 32.", "startOffset": 14, "endOffset": 17}, {"referenceID": 88, "context": "[89] 36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "E-F (AlexNet) [24] 27.", "startOffset": 14, "endOffset": 18}, {"referenceID": 23, "context": "E-F (VGG-16) [24] 21.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "MarrRevisited [6] 20.", "startOffset": 14, "endOffset": 17}, {"referenceID": 88, "context": "[89] 28.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "E-F (AlexNet) [24] 23.", "startOffset": 14, "endOffset": 18}, {"referenceID": 23, "context": "E-F (VGG-16) [24] 19.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "MarrRevisited [6] 19.", "startOffset": 14, "endOffset": 17}, {"referenceID": 82, "context": "Evaluation on NYUv2 depth dataset [83]: Local Object Layout.", "startOffset": 34, "endOffset": 38}, {"referenceID": 3, "context": "Human [4] .", "startOffset": 6, "endOffset": 9}, {"referenceID": 28, "context": "580 Felz-Hutt [29] .", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "gPb-owt-ucm [4] .", "startOffset": 12, "endOffset": 15}, {"referenceID": 57, "context": "696 Sketch Tokens [58] .", "startOffset": 18, "endOffset": 22}, {"referenceID": 91, "context": "780 SCG [92] .", "startOffset": 8, "endOffset": 12}, {"referenceID": 46, "context": "PMI [47] .", "startOffset": 4, "endOffset": 8}, {"referenceID": 21, "context": "SE-Var [22] .", "startOffset": 7, "endOffset": 11}, {"referenceID": 38, "context": "803 OEF [39] .", "startOffset": 8, "endOffset": 12}, {"referenceID": 49, "context": "DeepNets [50] .", "startOffset": 9, "endOffset": 13}, {"referenceID": 43, "context": "758 CSCNN [44] .", "startOffset": 10, "endOffset": 14}, {"referenceID": 93, "context": "798 HED [94] .", "startOffset": 8, "endOffset": 12}, {"referenceID": 93, "context": "833 HED [94] (Updated version) .", "startOffset": 8, "endOffset": 12}, {"referenceID": 93, "context": "811 HED merging [94] (Updated version) .", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "Evaluation on BSDS [4].", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "Results on BSDS [4].", "startOffset": 16, "endOffset": 19}, {"referenceID": 93, "context": "Notice that our approach generates more semantic edges for zebra, eagle, and giraffe compared to HED [94].", "startOffset": 101, "endOffset": 105}], "year": 2017, "abstractText": "We explore design principles for general pixel-level prediction problems, from low-level edge detection to midlevel surface normal estimation to high-level semantic segmentation. Convolutional predictors, such as the fullyconvolutional network (FCN), have achieved remarkable success by exploiting the spatial redundancy of neighboring pixels through convolutional processing. Though computationally efficient, we point out that such approaches are not statistically efficient during learning precisely because spatial redundancy limits the information learned from neighboring pixels. We demonstrate that stratified sampling of pixels allows one to (1) add diversity during batch updates, speeding up learning; (2) explore complex nonlinear predictors, improving accuracy; and (3) efficiently train state-of-the-art models tabula rasa (i.e., \u201cfrom scratch\u201d) for diverse pixel-labeling tasks. Our single architecture produces state-of-the-art results for semantic segmentation on PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset, and edge detection on BSDS.", "creator": "LaTeX with hyperref package"}}}