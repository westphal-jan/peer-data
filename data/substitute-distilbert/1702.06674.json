{"id": "1702.06674", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Unsupervised Diverse Colorization via Generative Adversarial Networks", "abstract": "colorization of grayscale images has been a hot shot in computer vision. previous research mainly consisted on producing a colored image to match the original one. however, since component colors share the same gray value, an input grayscale image could be poorly colored throughout maintaining its reality. in this paper, we design its useful solution for a diverse colorization. specifically, we leverage conditional generative adversarial networks to model the phenomenon of real - world item vectors, in which we develop a fully convolutional generator with multi - layer noise to enhance diversity, with multi - step condition trees to sustain reality, and with stride sampling to keep fuzzy information. with such a novel network architecture, the model yields highly competitive computation on the open lsun raw dataset. the turing test of 80 humans further indicates our generated color schemes is highly convincible.", "histories": [["v1", "Wed, 22 Feb 2017 04:34:31 GMT  (4291kb,D)", "http://arxiv.org/abs/1702.06674v1", null], ["v2", "Sat, 1 Jul 2017 10:57:03 GMT  (4498kb,D)", "http://arxiv.org/abs/1702.06674v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["yun cao", "zhiming zhou", "weinan zhang", "yong yu"], "accepted": false, "id": "1702.06674"}, "pdf": {"name": "1702.06674.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Diverse Colorization via Generative Adversarial Networks", "authors": ["Yun Cao", "Zhiming Zhou", "Weinan Zhang", "Yong Yu"], "emails": ["yyu}@apex.sjtu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Image colorization assigns a color to each pixel of a target grayscale image. Early colorization methods [Levin et al., 2004; Qu et al., 2006] require users to provide considerable scribbles on the grayscale image, which is apparently timeconsuming and requires specialties. Later research provides more automatic colorization methods. Those colorization algorithms differ in the ways of how they model the correspondence between grayscale and color.\nGiven an input grayscale image, non-parametric methods first define one or more color reference images (provided by human or retrieved automatically) to be used as source data. Then, following the Image Analogies framework [Hertzmann et al., 2001], the color is transferred onto the input image from analogous regions of the reference image(s) [Welsh et al., 2002; Liu et al., 2008; Gupta et al., 2012; Chia et al., 2011]. Parametric methods, on the other hand, learn prediction functions from large datasets of color images in the training stage, posing the colorization problem as either regression in the continuous color space [Cheng et al., 2015; Deshpande et al., 2015; Zhang et al., 2016] or classification of quantized color values [Charpiat et al., 2008].\nWhichever seeking the reference images or learning a color prediction model, all above methods share a common goal, i.e. to provide a colored image closer to the original one. But as we know, many colors share the same gray value. Purely from a grayscale image, one cannot tell what color of clothes the girl is wearing or what color the bedroom wall is. Those methods all produce a deterministic mapping function, thus when an item could have diverse colors, their models tend to provide a weighted average brownish color as pointed out in [Koo, 2016].\nIn this paper, to avoid this sepia-toned colorization, we use conditional generative adversarial networks (GANs) to generate diverse colorizations for a single grayscale image while maintaining their reality. GAN is originally proposed by [Goodfellow et al., 2014] and is composed of two adversarial parts: a generative model G that captures the data distribution, and a discriminative model D that estimates reality probability of a real or generated sample. Unlike many other conditional GANs like [Isola et al., 2016] using convolution layers as encoder and deconvolution layers as decoder, we build a fully convolutional generator and each convolutional layer is splinted by a concatenate layer to continuously render the conditional grayscale information and a batch normalization layer to provide internal covariate shift. Additionally, to maintain the spatial information, we set all convolution stride to 1 to avoid downsizing data. We also concatenate noise channels to first half convolutional layers of the generator to attain more diversity in the colored image generation process. As the generator G would capture the color distribution, we can alter the colorization result by changing the input noise. Thus we no longer need to train an additional independent model for each color scheme like [Cheng et al., 2015].\nAs our goal alters from producing the original colors to producing realistic diverse colors, we conduct questionnaire surveys as a Turing test instead of calculating the root mean squared error (RMSE) comparing the original image to measure our colorization result. The feedback from 80 subjects indicates our model successfully produce high-reality colored images, yielding more than 62.6% positive feedback while the rate of ground truth images is 70.0%. Furthermore, a significance t-test between the percentages of humans rating as real image for each test case shows the p-value is 0.1359 > 0.05, which indicates our generated results have no significant difference with the ground truth images.\nar X\niv :1\n70 2.\n06 67\n4v 1\n[ cs\n.C V\n] 2\n2 Fe\nb 20\n17"}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Diverse Colorization", "text": "The problem of colorization was proposed from last century, but the research of diverse colorization was not paid much attention until this decade. [Cheng et al., 2015] used additionally trained model to handle diverse colorization of a scene image particularly in day and dawn. [Zhang et al., 2016] posed the diverse colorization problem as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. [Deshpande et al., 2016] learned a low dimensional embedding of color fields using a variational auto-encoder (VAE). They constructed loss terms for the VAE decoder that avoid blurry outputs and take into account the uneven distribution of pixel colors and finally developed a conditional model for the multi-modal distribution between gray-level image and the color field embeddings.\nCompared with above work, our solution uses conditional generative adversarial networks to achieve unsupervised diverse colorization in a generic way with little domain knowledge of the images."}, {"heading": "2.2 Conditional GAN", "text": "Generative adversarial networks (GANs) [Goodfellow et al., 2014] have attained much attention in unsupervised learning research during the recent 3 years. Conditional GANs have been widely used in various computer vision scenarios. [Reed et al., 2016] used text to generate image by applying adversarial networks. [Isola et al., 2016] provided a general-purpose image-to-image translation model that handles tasks like label to scene, aerial to map, day to night, edges to photo and also grayscale to color.\nSome of above work may share a similar goal with us, but our conditional GAN structure differs a lot from previous work in several architectural choices mainly for the generator. Unlike other generators which employ an encoder-like front part consisting of multiple convolution layers and a decoderlike end part consisting of multiple deconvolution layers, our generator uses only convolution layers all over the architecture, and does not downsize data shape by applying convolution stride no more than 1 and no pooling operation. Additionally, we add multi-layer noise to generate more diverse colorization, while using multi-layer conditional information to keep the generated image highly realistic."}, {"heading": "3 Methods", "text": ""}, {"heading": "3.1 Problem formulation", "text": "GANs are generative models that learn a mapping from random noise vector z to output color image x: G : z \u2192 x. Compared with GANs, conditional GANs learn a mapping from observed grayscale image y and random noise vector z, to x: G : {y, z} \u2192 x. The generator G is trained to produce outputs that cannot be distinguished from \u201creal\u201d images by an adversarially trained discriminator D, which is trained with the aim at detecting the \u201cfake\u201d images produced by the generator. This training procedure is shown in Figure 1.\nThe objective of a GAN can be expressed as\nLGAN(G,D) =Ex\u223cPdata(x)[logD(x)]+ Ez\u223cPz(z)[log (1\u2212D(G(z)))],\n(1)\nwhile the objective of a conditional GAN is\nLcGAN(G,D) =Ex\u223cPdata(x)[logD(x)]+ (2) Ey\u223cPgray(y),z\u223cPz(z)[log (1\u2212D(G(y, z)))],\nwhere G tries to minimize this objective against an adversarial D that tries to maximize it. i.e.\nG\u2217 = argmin G max D LcGAN(G,D) (3)\nWithout z, the generator could still learn a mapping from y to x, but would produce deterministic outputs. That is why GAN is more suitable for diverse colorization tasks than other deterministic neural networks."}, {"heading": "3.2 Architecture and implementation details", "text": "The high-level structure of our conditional GAN is consistent with traditional ones [Isola et al., 2016; Deshpande et al., 2016], while the detailed architecture of our generator G differs a lot."}, {"heading": "Convolution or deconvolution", "text": "Convolution and deconvolution layers are two basic components of image generators. Convolution layers are used mainly to exact conditional features. And additionally, many researches [Zhang et al., 2016; Isola et al., 2016; Deshpande et al., 2016] use superposition of multiple convolution layers with stride more than 1 to downsize the data shape, which works as a data encoder. Deconvolution layers are then used to upsize the data shape as a decoder of the data representation [Nguyen et al., 2016a; Isola et al., 2016; Deshpande et al., 2016]. While many other researches share this encoder-decoder structure, we choose to use only convolution layers in our generator G. Firstly, convolution layers is well capable of feature extraction and transmission.\nMeanwhile, all the convolution stride is set to 1 to prevent data shape from downsizing, thus the important spatial information can be kept along the data flow till the final generation layer. Other work [Ronneberger et al., 2015; Isola et al., 2016] also takes this spatial information into consideration. They added skip connections between each layer i and layer n\u2212 i to form a \u201cU-Net\u201d structure, where n is the total number of layers. Each skip connection simply concatenates all channels at layer i with those at layer n\u2212 i. But our modification is more straightforward and easy to implement."}, {"heading": "YUV or RGB", "text": "A color image can be represented in different forms. The most common representation is RGB form which splits a color pixel into red, green, blue three channels. Most computer vision tasks use RGB representation like [Deshpande et al., 2015; Isola et al., 2016; Nguyen et al., 2016b] due to its generality. Other kinds of representations are also included like Lab [Charpiat et al., 2008; Zhang et al., 2016; Limmer and Lensch, 2016], Y UV (or Y CrCb) [Cheng et al., 2015]. In colorization tasks, we have grayscale image as conditional information, so there is convenience using Y UV representation, because the Y channel or so called Luminance channel represents exactly the grayscale information. So while using Y UV representation, we can just predict 2 channels and then concatenate with the grayscale channel to give a full color image. Additionally, if you use RGB as image representation, all result channels are predicted, thus to keep the grayscale of generated color image consistent with the original grayscale image, we need to add an additional loss Eq. (4) as a controller to make sure Gray(x) ' y: LL1(G) = Ey\u223cPgray(y),z\u223cPz(z)[\u2016y \u2212Gray(G(y, z))\u2016], (4) where function Y = Gray(RGB) is the well-known psychological formulation:\nY = 0.299R+ 0.587G+ 0.114B. (5) Note that Eq. (4) can still remain colorization diversity. Then the objective function will be modified to:\nG\u2217 = argmin G max D LcGAN(G,D) + \u03bbLL1(G) (6)\nBut no matter how we change the \u03bb parameter to balance two loss parts, the L1 part will not reach zero, so there will always be blurry unless the model is overfitting. We do comparison experiments on both RGB and Y UV representations, results are shown in Section 4.2."}, {"heading": "Multi-layer noise", "text": "In the work of [Isola et al., 2016], they mentioned noise ignorance while training the generator. To handle this problem, they provide noise only in the form of dropout, applied on several layers of the generator at both training and testing time. We also noticed this problem. Traditional GANs and condition GANs receive noise information at the very start layer, during the continuous data transformation through the network, the noise information attenuated a lot. To overcome this problem and make the colorization results more diversified, we concatenate the noise channel onto the first half of the generator layers (the first three layers in our case). We do comparison experiments on both one-layer noise and multilayer noise representations, with results shown in Section 4.2.\nAlgorithm 1 Training phase of our conditional GANs, with the default parameters kD = 1, kG = 1,m = 64, sz = 100, s = 64.\n1: for the number of training iterations do 2: for kD steps do 3: Generate minibatch of m randomly sampled noise {z(1), . . . , z(m)} each of size [sz]. 4: Sample minibatch of m grayscale images {y(1), . . . ,y(m)} each of shape [s, s, 1]. 5: Get the corresponding minibatch of m color images {x(1), . . . ,x(m)} from data distribution pdata(x). 6: Update the discriminator D by:\n\u2207\u03b8d 1\nm m\u2211 i=1 [logD(x(i)) + log (1\u2212D(G(y(i),z(i))))]\n7: end for 8: for kG steps do 9: Generate minibatch of m randomly sampled noise\n{z(1), . . . , z(m)} 10: Sample minibatch of m grayscale images {y(1), . . . ,y(m)} 11: Update the generator by:\n\u2207\u03b8g 1\nm m\u2211 i=1 log(1\u2212D(G(y(i),z(i))))\n12: end for 13: end for"}, {"heading": "Multi-layer conditional information", "text": "Other conditional GANs usually add conditional information only in the first layer, this is because the layer shape of previous generators changes along their convolution and deconvolution layers. But due to the consistent layer shape of our generator, we can apply concatenation of conditional grayscale information throughout the whole generator layers which can provide sustained conditional supervision. Though the \u201cUNet\u201d skip structure of [Isola et al., 2016] can also help posterior layers receive conditional information, our model modification is still more straightforward and convenient. Illustration of details of our model structure is shown in Figure 2."}, {"heading": "3.3 Training and testing procedure", "text": "The training phase of our conditional GANs is presented in Algorithm 1. While to keep the BatchNorm layers work correctly, one cannot feed an image batch of same images to test various noise response. So we use multi-round testing with same batch and rearrange them to test different noise responses of each image, which is described in Algorithm 2."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset", "text": "There are various kinds of color image datasets, and we choose the open LSUN bedroom dataset1 [Yu et al., 2015] to conduct our experiment. LSUN is a large color image dataset generated iteratively by human labeling with automatic deep\n1LSUN dataset is available at http://lsun.cs.princeton.edu\n.\nAlgorithm 2 Testing phase of our conditional GANs with the default parameters m = 64, sz = 100.\n1: Sample minibatch of m grayscale images {y(1), . . . ,y(m)} 2: for round i in ktest rounds do 3: Generate minibatch of m randomly sampled noise {z(1,i), . . . ,z(m,i)} each of size [sz]. 4: Generate color image use trained model G:\n{x(1,i), . . . ,x(m,i)} \u2190 G({y(1), . . . ,y(m)}, {z(1,i), . . . , z(m,i)})\n5: end for 6: Rearrange generated results x into {x(1,1), . . . ,x(1,ktest)}, . . . , {x(m,1), . . . ,x(m,ktest)}\nneural classification. It contains around one million labeled images for each of 10 scene categories and 20 object categories. Among them we choose indoor scene bedroom because it has enough samples (more than 3 million) and unlike outdoor scenes in which trees are almost green and sky is always blue, items in indoor scenes like bedroom have various colors, as shown in Figure 3. This is exactly what we need to fully represent the capability of our condition GAN model. In experiments, we use 503, 900 bedroom images randomly picked from the LSUN dataset. We crop a maximum center square out of each image and reshape them into 64\u00d764 pixel as preprocessing."}, {"heading": "4.2 Comparison Experiments", "text": ""}, {"heading": "YUV and RGB", "text": "The generated colorization results of same grayscale images using Y UV representation and RGB representation with additional L1 loss are shown in Figure 4. Focus on the results in red boxes, we can see RGB representation suffers from structural miss due to the additional L1 loss. Moreover, the model using RGB representation shall predict 3 color\nchannels while Y UV representation only predicts 2 channels, which makes the RGB model training much more unstable."}, {"heading": "Single-layer and multi-layer noise", "text": "The generated colorization results of same grayscale images using single-layer noise model and multi-layer noise model are shown in Figure 5. We can see from the result that multilayer noise leads to much more diversity."}, {"heading": "Single-layer and multi-layer condition", "text": "The generated colorization results of same grayscale images using single-layer condition model and multi-layer condition model are shown in Figure 6. We can see from the results that the multi-layer condition model give the generator more structural information and thus the results of multi-layer condition model are more stable while the single-layer condition model suffers from colorization derivation.\nMore results and discussion of our final model will be shown in the next section."}, {"heading": "5 Results and Evaluation", "text": ""}, {"heading": "5.1 Colorization Results", "text": "A variety of image colorization results by our conditional GANs are provided in Figure 7. Apparently, our fully convolutional (without stride) generator with multi-layer noise and multi-layer condition concatenation produces various kinds of colorization schemes while maintaining good reality. Almost all color parts kept within correct components without deviation."}, {"heading": "5.2 Evaluation via Human Study", "text": "Previous methods share a common goal to provide a color image closer to the original one. That is why many of their models [Jung and Kang, 2016; Perarnau et al., 2016; Deshpande et al., 2016] take image distance like RMSE (Root Mean Square Error) and PSNR (Peak Signal-to-Noise Ratio) as their measurements. And others [Iizuka et al., 2016; Isola et al., 2016] use additional classifiers to predict if colorized image can be detected or still correctly classified. But our goal is to generate diverse colorization schemes, so we cannot take those distance as our measurements as there exist reasonable colorizations that diverge a lot from the original color image. Note that some previous work [Cheng et al., 2015; Dong et al., 2017; Nguyen et al., 2016b; Nguyen et al., 2016a] do not provide quantified measurements.\nTherefore, just like some previous researches [Isola et al., 2016; Zhang et al., 2016], we provide questionnaire surveys as a Turing test to measure our colorization results. We ask"}, {"heading": "Single-layer condition", "text": ""}, {"heading": "Multi-layer condition", "text": "each of the total 80 participants 20 questions in which we display 5 color images, one of them is the ground truth image, the others are our generated colorizations of the grayscale image of the ground truth, and ask them if any one of them is of poor reality. We add ground truth image among them as a reference in case of participants do not think any of them is real. And we arrange all images randomly so that participants won\u2019t find the ground truth from their positions. The feedback from 80 participants indicates more than 62.6% of our generated color images are convincible while the rate of ground truth images is 70.0%. Furthermore, we run significance t-test between the percentages of humans rating as real image for each test case. The p-value is 0.1359 > 0.05, indicating our generated results have no significant difference with the ground truth images. Also we calculate the average credibility rank (higher ranking of a image means higher percentage of participants think it is real) of the ground truth image among those generated images is only 2.5 out of 5, which means we successfully fooled about half of the participants."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed a novel conditional generative adversarial network architecture that will automatically generate diverse colorization schemes for a grayscale image while maintaining their reality. We introduce a novel generator architecture which consists of fully convolutional (without stride) structure with multi-layer noise and multi-layer condition concatenation. With this structure, our model successfully generated diversified high-quality colored images for each input grayscale image. We performed a questionnaire survey as a Turing test to measure our colorization result. The feedback from 80 participants indicates our generated colorization schemes are quite convincible.\nFor future work, we plan to explore more application scenarios and to achieve a more general purpose unsupervised diverse generative model."}], "references": [{"title": "Automatic image colorization via multimodal predictions", "author": ["Charpiat et al", "2008] Guillaume Charpiat", "Matthias Hofmann", "Bernhard Scholkopf"], "venue": "In Computer Vision - ECCV 2008, 10th European Conference on Computer Vision,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "In ICCV 2015", "author": ["Zezhou Cheng", "Qingxiong Yang", "Bin Sheng. Deep colorization"], "venue": "Santiago, Chile, December 7-13, 2015, pages 415\u2013423,", "citeRegEx": "Cheng et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "ACM Trans", "author": ["Alex Yong Sang Chia", "Shaojie Zhuo", "Raj Kumar Gupta", "Yu-Wing Tai", "Siu-Yeung Cho", "Ping Tan", "Stephen Lin. Semantic colorization with internet images"], "venue": "Graph., 30(6):156:1\u2013156:8,", "citeRegEx": "Chia et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In ICCV 2015", "author": ["Aditya Deshpande", "Jason Rock", "David A. Forsyth. Learning large-scale automatic image colorization"], "venue": "Santiago, Chile, December 7-13, 2015, pages 567\u2013575,", "citeRegEx": "Deshpande et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CoRR", "author": ["Aditya Deshpande", "Jiajun Lu", "Mao-Chuang Yeh", "David A. Forsyth. Learning diverse image colorization"], "venue": "abs/1612.01958,", "citeRegEx": "Deshpande et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "CoRR", "author": ["Hao Dong", "Paarth Neekhara", "Chao Wu", "Yike Guo. Unsupervised image-to-image translation with generative adversarial networks"], "venue": "abs/1701.02676,", "citeRegEx": "Dong et al.. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Generative adversarial nets", "author": ["Goodfellow et al", "2014] Ian J. Goodfellow", "Jean PougetAbadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Image colorization using similar images", "author": ["Gupta et al", "2012] Raj Kumar Gupta", "Alex Yong Sang Chia", "Deepu Rajan", "Ee Sin Ng", "Zhiyong Huang"], "venue": "In Proceedings of the 20th ACM Multimedia Conference, MM \u201912,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "In SIGGRAPH 2001", "author": ["Aaron Hertzmann", "Charles E. Jacobs", "Nuria Oliver", "Brian Curless", "David Salesin. Image analogies"], "venue": "Los Angeles, California, USA, August 12-17, 2001, pages 327\u2013340,", "citeRegEx": "Hertzmann et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification", "author": ["Satoshi Iizuka", "Edgar Simo-Serra", "Hiroshi Ishikawa"], "venue": "ACM Trans. Graph., 35(4):110:1\u2013110:11,", "citeRegEx": "Iizuka et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "CoRR", "author": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei A. Efros. Image-to-image translation with conditional adversarial networks"], "venue": "abs/1611.07004,", "citeRegEx": "Isola et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Variational image colorization models using higher-order", "author": ["Jung", "Kang", "2016] Miyoun Jung", "Myungjoo Kang"], "venue": null, "citeRegEx": "Jung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jung et al\\.", "year": 2016}, {"title": "Automatic colorization with deep convolutional generative adversarial networks. 2016", "author": ["Stephen Koo"], "venue": "[Koo,", "citeRegEx": "Koo.,? \\Q2016\\E", "shortCiteRegEx": "Koo.", "year": 2016}, {"title": "ACM Trans", "author": ["Anat Levin", "Dani Lischinski", "Yair Weiss. Colorization using optimization"], "venue": "Graph., 23(3):689\u2013694,", "citeRegEx": "Levin et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In ICMLA 2016", "author": ["Matthias Limmer", "Hendrik P.A. Lensch. Infrared colorization using deep convolutional neural networks"], "venue": "Anaheim, CA, USA, December 18-20, 2016, pages 61\u201368,", "citeRegEx": "Limmer and Lensch. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "ACM Trans", "author": ["Xiaopei Liu", "Liang Wan", "Yingge Qu", "TienTsin Wong", "Stephen Lin", "Chi-Sing Leung", "PhengAnn Heng. Intrinsic colorization"], "venue": "Graph., 27(5):152:1\u2013152:9,", "citeRegEx": "Liu et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "CoRR", "author": ["Tung Duc Nguyen", "Kazuki Mori", "Ruck Thawonmas. Image colorization using a deep convolutional neural network"], "venue": "abs/1604.07904,", "citeRegEx": "Nguyen et al.. 2016a", "shortCiteRegEx": null, "year": 2016}, {"title": "In VISIGRAPP 2016 - Volume 4: VISAPP", "author": ["Van Nguyen", "Vicky Sintunata", "Terumasa Aoki. Automatic image colorization based on feature lines"], "venue": "Rome, Italy, February 27-29, 2016., pages 126\u2013133,", "citeRegEx": "Nguyen et al.. 2016b", "shortCiteRegEx": null, "year": 2016}, {"title": "CoRR", "author": ["Guim Perarnau", "Joost van de Weijer", "Bogdan Raducanu", "Jose M. \u00c1 lvarez. Invertible conditional gans for image editing"], "venue": "abs/1611.06355,", "citeRegEx": "Perarnau et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "ACM Trans", "author": ["Yingge Qu", "Tien-Tsin Wong", "PhengAnn Heng. Manga colorization"], "venue": "Graph., 25(3):1214\u20131220,", "citeRegEx": "Qu et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "In ICML 2016", "author": ["Scott E. Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee. Generative adversarial text to image synthesis"], "venue": "New York City, NY, USA, June 19-24, 2016, pages 1060\u20131069,", "citeRegEx": "Reed et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "U-net: Convolutional networks for biomedical image segmentation", "author": ["Olaf Ronneberger", "Philipp Fischer", "Thomas Brox"], "venue": "MICCAI 2015 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III, pages 234\u2013241,", "citeRegEx": "Ronneberger et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In SIGGRAPH 2002", "author": ["Tomihisa Welsh", "Michael Ashikhmin", "Klaus Mueller. Transferring color to greyscale images"], "venue": "San Antonio, Texas, USA, July 23-26, 2002, pages 277\u2013280,", "citeRegEx": "Welsh et al.. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Lsun: Construction of a largescale image dataset using deep learning with humans in the loop", "author": ["Fisher Yu", "Yinda Zhang", "Shuran Song", "Ari Seff", "Jianxiong Xiao"], "venue": "CoRR, abs/1506.03365,", "citeRegEx": "Yu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Computer Vision - ECCV 2016 - 14th European Conference", "author": ["Richard Zhang", "Phillip Isola", "Alexei A. Efros. Colorful image colorization"], "venue": "Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III, pages 649\u2013666,", "citeRegEx": "Zhang et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Early colorization methods [Levin et al., 2004; Qu et al., 2006] require users to provide considerable scribbles on the grayscale image, which is apparently timeconsuming and requires specialties.", "startOffset": 27, "endOffset": 64}, {"referenceID": 19, "context": "Early colorization methods [Levin et al., 2004; Qu et al., 2006] require users to provide considerable scribbles on the grayscale image, which is apparently timeconsuming and requires specialties.", "startOffset": 27, "endOffset": 64}, {"referenceID": 8, "context": "Then, following the Image Analogies framework [Hertzmann et al., 2001], the color is transferred onto the input image from analogous regions of the reference image(s) [Welsh et al.", "startOffset": 46, "endOffset": 70}, {"referenceID": 22, "context": ", 2001], the color is transferred onto the input image from analogous regions of the reference image(s) [Welsh et al., 2002; Liu et al., 2008; Gupta et al., 2012; Chia et al., 2011].", "startOffset": 104, "endOffset": 181}, {"referenceID": 15, "context": ", 2001], the color is transferred onto the input image from analogous regions of the reference image(s) [Welsh et al., 2002; Liu et al., 2008; Gupta et al., 2012; Chia et al., 2011].", "startOffset": 104, "endOffset": 181}, {"referenceID": 2, "context": ", 2001], the color is transferred onto the input image from analogous regions of the reference image(s) [Welsh et al., 2002; Liu et al., 2008; Gupta et al., 2012; Chia et al., 2011].", "startOffset": 104, "endOffset": 181}, {"referenceID": 1, "context": "Parametric methods, on the other hand, learn prediction functions from large datasets of color images in the training stage, posing the colorization problem as either regression in the continuous color space [Cheng et al., 2015; Deshpande et al., 2015; Zhang et al., 2016] or classification of quantized color values [Charpiat et al.", "startOffset": 208, "endOffset": 272}, {"referenceID": 3, "context": "Parametric methods, on the other hand, learn prediction functions from large datasets of color images in the training stage, posing the colorization problem as either regression in the continuous color space [Cheng et al., 2015; Deshpande et al., 2015; Zhang et al., 2016] or classification of quantized color values [Charpiat et al.", "startOffset": 208, "endOffset": 272}, {"referenceID": 24, "context": "Parametric methods, on the other hand, learn prediction functions from large datasets of color images in the training stage, posing the colorization problem as either regression in the continuous color space [Cheng et al., 2015; Deshpande et al., 2015; Zhang et al., 2016] or classification of quantized color values [Charpiat et al.", "startOffset": 208, "endOffset": 272}, {"referenceID": 12, "context": "Those methods all produce a deterministic mapping function, thus when an item could have diverse colors, their models tend to provide a weighted average brownish color as pointed out in [Koo, 2016].", "startOffset": 186, "endOffset": 197}, {"referenceID": 10, "context": "Unlike many other conditional GANs like [Isola et al., 2016] using convolution layers as encoder and deconvolution layers as decoder, we build a fully convolutional generator and each convolutional layer is splinted by a concatenate layer to continuously render the conditional grayscale information and a batch normalization layer to provide internal covariate shift.", "startOffset": 40, "endOffset": 60}, {"referenceID": 1, "context": "Thus we no longer need to train an additional independent model for each color scheme like [Cheng et al., 2015].", "startOffset": 91, "endOffset": 111}, {"referenceID": 1, "context": "[Cheng et al., 2015] used additionally trained model to handle diverse colorization of a scene image particularly in day and dawn.", "startOffset": 0, "endOffset": 20}, {"referenceID": 24, "context": "[Zhang et al., 2016] posed the diverse colorization problem as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "[Deshpande et al., 2016] learned a low dimensional embedding of color fields using a variational auto-encoder (VAE).", "startOffset": 0, "endOffset": 24}, {"referenceID": 20, "context": "[Reed et al., 2016] used text to generate image by applying adversarial networks.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "[Isola et al., 2016] provided a general-purpose image-to-image translation model that handles tasks like label to scene, aerial to map, day to night, edges to photo and also grayscale to color.", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "The high-level structure of our conditional GAN is consistent with traditional ones [Isola et al., 2016; Deshpande et al., 2016], while the detailed architecture of our generator G differs a lot.", "startOffset": 84, "endOffset": 128}, {"referenceID": 4, "context": "The high-level structure of our conditional GAN is consistent with traditional ones [Isola et al., 2016; Deshpande et al., 2016], while the detailed architecture of our generator G differs a lot.", "startOffset": 84, "endOffset": 128}, {"referenceID": 24, "context": "And additionally, many researches [Zhang et al., 2016; Isola et al., 2016; Deshpande et al., 2016] use superposition of multiple convolution layers with stride more than 1 to downsize the data shape, which works as a data encoder.", "startOffset": 34, "endOffset": 98}, {"referenceID": 10, "context": "And additionally, many researches [Zhang et al., 2016; Isola et al., 2016; Deshpande et al., 2016] use superposition of multiple convolution layers with stride more than 1 to downsize the data shape, which works as a data encoder.", "startOffset": 34, "endOffset": 98}, {"referenceID": 4, "context": "And additionally, many researches [Zhang et al., 2016; Isola et al., 2016; Deshpande et al., 2016] use superposition of multiple convolution layers with stride more than 1 to downsize the data shape, which works as a data encoder.", "startOffset": 34, "endOffset": 98}, {"referenceID": 16, "context": "Deconvolution layers are then used to upsize the data shape as a decoder of the data representation [Nguyen et al., 2016a; Isola et al., 2016; Deshpande et al., 2016].", "startOffset": 100, "endOffset": 166}, {"referenceID": 10, "context": "Deconvolution layers are then used to upsize the data shape as a decoder of the data representation [Nguyen et al., 2016a; Isola et al., 2016; Deshpande et al., 2016].", "startOffset": 100, "endOffset": 166}, {"referenceID": 4, "context": "Deconvolution layers are then used to upsize the data shape as a decoder of the data representation [Nguyen et al., 2016a; Isola et al., 2016; Deshpande et al., 2016].", "startOffset": 100, "endOffset": 166}, {"referenceID": 21, "context": "Other work [Ronneberger et al., 2015; Isola et al., 2016] also takes this spatial information into consideration.", "startOffset": 11, "endOffset": 57}, {"referenceID": 10, "context": "Other work [Ronneberger et al., 2015; Isola et al., 2016] also takes this spatial information into consideration.", "startOffset": 11, "endOffset": 57}, {"referenceID": 3, "context": "Most computer vision tasks use RGB representation like [Deshpande et al., 2015; Isola et al., 2016; Nguyen et al., 2016b] due to its generality.", "startOffset": 55, "endOffset": 121}, {"referenceID": 10, "context": "Most computer vision tasks use RGB representation like [Deshpande et al., 2015; Isola et al., 2016; Nguyen et al., 2016b] due to its generality.", "startOffset": 55, "endOffset": 121}, {"referenceID": 17, "context": "Most computer vision tasks use RGB representation like [Deshpande et al., 2015; Isola et al., 2016; Nguyen et al., 2016b] due to its generality.", "startOffset": 55, "endOffset": 121}, {"referenceID": 24, "context": "Other kinds of representations are also included like Lab [Charpiat et al., 2008; Zhang et al., 2016; Limmer and Lensch, 2016], Y UV (or Y CrCb) [Cheng et al.", "startOffset": 58, "endOffset": 126}, {"referenceID": 14, "context": "Other kinds of representations are also included like Lab [Charpiat et al., 2008; Zhang et al., 2016; Limmer and Lensch, 2016], Y UV (or Y CrCb) [Cheng et al.", "startOffset": 58, "endOffset": 126}, {"referenceID": 1, "context": ", 2016; Limmer and Lensch, 2016], Y UV (or Y CrCb) [Cheng et al., 2015].", "startOffset": 51, "endOffset": 71}, {"referenceID": 10, "context": "Multi-layer noise In the work of [Isola et al., 2016], they mentioned noise ignorance while training the generator.", "startOffset": 33, "endOffset": 53}, {"referenceID": 10, "context": "Though the \u201cUNet\u201d skip structure of [Isola et al., 2016] can also help posterior layers receive conditional information, our model modification is still more straightforward and convenient.", "startOffset": 36, "endOffset": 56}, {"referenceID": 23, "context": "There are various kinds of color image datasets, and we choose the open LSUN bedroom dataset1 [Yu et al., 2015] to conduct our experiment.", "startOffset": 94, "endOffset": 111}, {"referenceID": 18, "context": "That is why many of their models [Jung and Kang, 2016; Perarnau et al., 2016; Deshpande et al., 2016] take image distance like RMSE (Root Mean Square Error) and PSNR (Peak Signal-to-Noise Ratio) as their measurements.", "startOffset": 33, "endOffset": 101}, {"referenceID": 4, "context": "That is why many of their models [Jung and Kang, 2016; Perarnau et al., 2016; Deshpande et al., 2016] take image distance like RMSE (Root Mean Square Error) and PSNR (Peak Signal-to-Noise Ratio) as their measurements.", "startOffset": 33, "endOffset": 101}, {"referenceID": 9, "context": "And others [Iizuka et al., 2016; Isola et al., 2016] use additional classifiers to predict if colorized image can be detected or still correctly classified.", "startOffset": 11, "endOffset": 52}, {"referenceID": 10, "context": "And others [Iizuka et al., 2016; Isola et al., 2016] use additional classifiers to predict if colorized image can be detected or still correctly classified.", "startOffset": 11, "endOffset": 52}, {"referenceID": 1, "context": "Note that some previous work [Cheng et al., 2015; Dong et al., 2017; Nguyen et al., 2016b; Nguyen et al., 2016a] do not provide quantified measurements.", "startOffset": 29, "endOffset": 112}, {"referenceID": 5, "context": "Note that some previous work [Cheng et al., 2015; Dong et al., 2017; Nguyen et al., 2016b; Nguyen et al., 2016a] do not provide quantified measurements.", "startOffset": 29, "endOffset": 112}, {"referenceID": 17, "context": "Note that some previous work [Cheng et al., 2015; Dong et al., 2017; Nguyen et al., 2016b; Nguyen et al., 2016a] do not provide quantified measurements.", "startOffset": 29, "endOffset": 112}, {"referenceID": 16, "context": "Note that some previous work [Cheng et al., 2015; Dong et al., 2017; Nguyen et al., 2016b; Nguyen et al., 2016a] do not provide quantified measurements.", "startOffset": 29, "endOffset": 112}, {"referenceID": 10, "context": "Therefore, just like some previous researches [Isola et al., 2016; Zhang et al., 2016], we provide questionnaire surveys as a Turing test to measure our colorization results.", "startOffset": 46, "endOffset": 86}, {"referenceID": 24, "context": "Therefore, just like some previous researches [Isola et al., 2016; Zhang et al., 2016], we provide questionnaire surveys as a Turing test to measure our colorization results.", "startOffset": 46, "endOffset": 86}], "year": 2017, "abstractText": "Colorization of grayscale images has been a hot topic in computer vision. Previous research mainly focuses on producing a colored image to match the original one. However, since many colors share the same gray value, an input grayscale image could be diversely colored while maintaining its reality. In this paper, we design a novel solution for unsupervised diverse colorization. Specifically, we leverage conditional generative adversarial networks to model the distribution of real-world item colors, in which we develop a fully convolutional generator with multi-layer noise to enhance diversity, with multi-layer condition concatenation to maintain reality, and with stride 1 to keep spatial information. With such a novel network architecture, the model yields highly competitive performance on the open LSUN bedroom dataset. The Turing test of 80 humans further indicates our generated color schemes are highly convincible.", "creator": "LaTeX with hyperref package"}}}