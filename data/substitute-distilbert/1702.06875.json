{"id": "1702.06875", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Triaging Content Severity in Online Mental Health Forums", "abstract": "mental health forums are online communities where people express their issues and seek help from moderators and other communities. in such forums, individuals contain often posts with severe content indicating that the user is in acute distress and there is a risk of attempted self - harm. moderators need to respond to these severe posts in a timely framework further prevent potential self - harm. however, the tremendous volume of daily posted content makes it difficult for future moderators to locate and respond to these critical posts. we have a framework for triaging user content into four severity categories which are defined based on consideration of self - harm treatment. our models act based on a feature - rich classification framework which includes lexical, psycholinguistic, contextual and topic modeling features. our approaches improve the state all the art in triaging mental content severity in mental health forums by large margins ( up to 17 % improvement over the f - graded prediction ). using said proposed model, we analyze predicting mental state of users and we show that reasonable, long - term users per del forum demonstrate a decreased severity of risk over time. our analysis on participant interaction of the moderators with the users further indicates that without an automatic way to identify critical content, it is indeed challenging for fellow moderators to avoid coherent response to the users considerable need.", "histories": [["v1", "Wed, 22 Feb 2017 16:14:12 GMT  (84kb,D)", "http://arxiv.org/abs/1702.06875v1", "Accepted for publication in Journal of the Association for Information Science and Technology (2017)"]], "COMMENTS": "Accepted for publication in Journal of the Association for Information Science and Technology (2017)", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.SI", "authors": ["arman cohan", "sydney young", "rew yates", "nazli goharian"], "accepted": false, "id": "1702.06875"}, "pdf": {"name": "1702.06875.pdf", "metadata": {"source": "CRF", "title": "Triaging Content Severity in Online Mental Health Forums\u2217", "authors": ["Arman Cohan", "Sydney Young", "Andrew Yates", "Nazli Goharian"], "emails": ["arman@ir.cs.georgetown.edu,", "nazli@ir.cs.georgetown.edu,", "sey24@georgetown.edu,", "ayates@mpi-inf.mpg.de"], "sections": [{"heading": null, "text": "Mental health forums are online communities where people express their issues and seek help from moderators and other users. In such forums, there are often posts with severe content indicating that the user is in acute distress and there is a risk of attempted self-harm. Moderators need to respond to these severe posts in a timely manner to prevent potential selfharm. However, the large volume of daily posted content makes it difficult for the moderators to locate and respond to these critical posts. We present a framework for triaging user content into four severity categories which are defined based on indications of self-harm ideation. Our models are based on a feature-rich classification framework which includes lexical, psycholinguistic, contextual and topic modeling features. Our approaches improve the state of the art in triaging the content severity in mental health forums by large margins (up to 17% improvement over the F-1 scores). Using the proposed model, we analyze the mental state of users and we show that overall, long-term users of the forum demonstrate a decreased severity of risk over time. Our analysis on the interaction of the moderators with the users further indicates that without an automatic way to identify critical content, it is indeed challenging for the moderators to provide timely response to the users in need.\n\u2217 This is a preprint of an article accepted for publication in Journal of the Association for Information Science and Technology c\u00a9 2017 (Association for Information Science and Technology)."}, {"heading": "1 Introduction", "text": "Mental health is an increasingly important healthrelated challenge in society; mental health conditions are associated with impaired health-related quality of life and social functioning (Saarni et al., 2007; Strine et al., 2015). Self-harm and suicide, as serious mental health conditions, are leading reasons of death world-wide (Nock et al., 2008; American Foundation for Suicide Prevention, 2016). Each year an estimated number of 43,000 Americans die by suicide, on average there are 117 suicides per day, and about 500,000 people visit hospital for injuries due to self-harm (Karch et al., 2009; Centers for Disease Control and Prevention, 2015; American Foundation for Suicide Prevention, 2016).\nDespite its pervasiveness, our understanding of suicide and self-harm related issues is limited. A notable reason for this is lack of large scale data on suicide. Most existing research on suicide is based on sparse curated data from a limited number of health-care centers. Furthermore, detecting and preventing potential self-harm acts remain a significant challenge due to reasons including lack of real-time data, privacy and confidentiality issues, and existence of bias in studies (Coppersmith et al., 2016a).\nAs social media usage has increased dramatically, individuals have tried to resolve their health problems by sharing them online, asking other users\u2019 opinions and seeking support. Therefore, social media have provided a valuable platform for large-scale analysis of mental health data and this analyses have offered great insights into mental health. Generally, it has been shown that social media can have broad applicability for public health research as the data from social media can reflect a variety of characteristics about individuals (Paul and Dredze, 2011; Eichstaedt et al., 2015).\nar X\niv :1\n70 2.\n06 87\n5v 1\n[ cs\n.C L\n] 2\n2 Fe\nb 20\n17\nOnline forums are a type of social media which are essentially communities in which users engage in discussion about topics of common interest. Mental health forums are centered around users who have directly or indirectly been involved in mental health conditions.\nGeneral social media platforms such as Twitter and Facebook are less topic-centric and more general purpose, in the sense that millions of users use them to discuss mundane events in their lives. While the signals coming from general social systems such as Twitter and Facebook are subtle and not directly about mental health, they are relevant and they have been previously utilized to support certain important tasks (e.g. (Coppersmith et al., 2015; Tsugawa et al., 2015; Schwartz et al., 2014)). On the other hand, online forums are specifically designed for discussion around specific topics and they attract users with similar interests and goals (De Choudhury and De, 2014). Some users in general social media such as Twitter can choose to be pseudonymous or anonymous, however, the identity of majority of the users are known. On the other hand, to protect their users, many online mental health forums such as ReachOut specifically ask their users to have anonymous profiles. The moderators in many of these forums further actively redact any post that could reveal the identity of the user. Such support for anonymity further encourages users to engage in sensitive mental health discussions and express their real thoughts and feelings. In this paper, we are focusing on online mental health forums as anonymous support platforms centered around people with similar experiences and problems.\nThere are three stages that lead to suicidal action among individuals who are in some sort of mental distress (Silverman and Maris, 1995; De Choudhury et al., 2016): 1- thinking, 2- ambivalence and 3- decision making. In the first two stages the individual is experiencing thoughts of distress, hopelessness, and low self-esteem. In the decision making stage, the individual might show explicit plans of taking their life. Individuals might seek support in any of these stages and online health forums are a ready platform enabling these individuals to ask for support. In many online mental health forums, there are moderators or more senior members who help the users with mental distress. Troubled users who are at risk of self-harm need to be attended to as quickly as pos-\nsible to prevent a potential self-harm act. However, the volume of newly posted content each day makes it difficult for the moderators to locate and respond to more critical posts. Effective online manual triaging of all the forum contents is highly costly and not scalable.\nWe propose an approach for automated triaging of the severity of user content in online forums based on indication of self-harm thoughts. Triaging the content severity makes it possible for moderators to identify critical posts and help a troubled user in a timely manner to hopefully reduce the risk of self-harm to the user. We propose a featurerich supervised classification framework that takes advantage of various types of features in the forums. The features include lexical, psycholinguistic, contextual, topic modeling, and dense representation features. We evaluate our approach on data provided by ReachOut1, a large mental health forum. We show that our approach can effectively identify the critical content which will assist the moderators in attending to the in-need users in a timely manner. We show that without an automatic way for identifying critical posts, the moderator\u2019s response time does not correlate with the severity of the posts, which further confirms that manually identifying these posts is a challenge for moderators. Finally, analysis of the user content on this forum shows that on average, the content severity of users tends to decline as they interact with the forum which is evidenced by the transition from more critical to less critical content.\nThe contributions of this work are as follows: (i) an effective approach for triaging the content severity in online mental health forums based on indication of self-harm ideation; (ii) providing insight into the effect of online mental health forums on users through analysis of their content; (iii) analyzing the interaction of moderators with users; and (iv) extensive evaluation of the proposed approach on a real-world dataset."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Healthcare and Mental Health through Social Media", "text": "In recent years, healthcare has benefited enormously from social media data (Dredze, 2012). Many studies have investigated public health surveillance by utilizing the Twitter public data (Paul and Dredze, 2011; Lamb et al., 2013; Parker 1www.ReachOut.com\net al., 2015; Chen et al., 2016; Paul et al., 2016). Results of these studies show consistency with other information resources for public health such as official reports released by governments, reports released by Centers for Disease Control and Prevention (CDC) and other online sources such as Google Flu Trends1.\nSocial media has also become a popular platform for people with mental health conditions to express their feelings and seek support from other users. It has helped individuals with depression by providing them means to connect to people with shared experiences who can answer their questions (Dao et al., 2015; Olteanu et al., 2016). Consequently, the information from social media has become a significant resource providing more insight into psychological and mental conditions and problems. De Choudhury et al. (2013b) explored social media to identify and diagnose depression among individuals. They analyzed the posting of a set of Twitter users through time and identified signals for characterizing the onset of depression in individuals. Park et al. (2013) showed that depressed individuals perceived social media (Twitter) as a tool for social awareness and emotional interaction while non-depressed individuals are mostly regular information consumers. Schwartz et al. (2014) used Facebook data to build a regression model to predict degree of depression in individuals. Portier et al. (2013) conducted sentiment analysis on the cancer survivor forum content and compared the sentiment change of the user content before and after interaction with the community. There exist many other works on analysis of social media for mental health problems such as depressive disorders (De Choudhury et al., 2013a; Tsugawa et al., 2015), addiction (Murnane and Counts, 2014), insomnia (Jamison-Powell et al., 2012), schizophrenia (Mitchell et al., 2015) and various other conditions (Coppersmith et al., 2015).\nWhile many of the aforementioned mental health disorders are closely related to suicidal behaviors and could lead to suicidal ideation, our focus in this paper is to identify the severity of the content based on indication of self-harm risk to individuals."}, {"heading": "2.2 Social Media and Suicide", "text": "Previous work has studied self-harm and suicidal behavior through text analysis. Some researchers\n1https://www.google.org/flutrends/\nexplored the language usage in content relating to suicide to identify signals of this behavior to predict suicidal actions. Thompson et al. (2014) predicted the risk of suicide in military personnel and veterans using the clinical notes and online social media data (Facebook posts). They used a model based on Random Forest classifier (Breiman, 2001) with bag-of-words features. Jones and Bennell (2007) developed statistical prediction rules to discriminate between genuine and simulated suicide notes. Lester (2010) analyzed the language of suicide notes to better understand suicidal behaviors in individuals. Coppersmith et al. (2016b) examined data from Twitter users who have attempted to take their life and provided an exploratory analysis of patterns in language around their attempt. . Some researchers have analyzed suicidal behaviors through detecting sentiment and emotional variations of the content (Cherry et al., 2012; Pestian et al., 2012; Desmet and Hoste, 2013). Prior work has also explored classification of suicidal content. Burnap et al. (2015) proposed an ensemble classification approach to classify tweets into suicide related topics such as suicidal ideation, reporting of a suicide, memorial, campaigning and support. Braithwaite et al. (2016) conducted a user study on a group of individuals and analyzed their Twitter posts using Decision Tree classifier to differentiate individuals with higher suicide risks from individuals who are not at risk. Finally De Choudhury et al. (2016) proposed that social media could be used to predict shifts from mental health discussions to expression of suicide thoughts. Specifically, they analyzed language in Reddit2mental health community and employed a framework based on propensity score matching (Rosenbaum and Rubin, 1984) to predict suicidal shifts in users. Unlike these works, our focus is triaging the content severity in mental health online forums based on the risk of self-harm to the users.\nThe closest work to ours is the recent shared task (Milne et al., 2016) on automatic identification of content severity in mental health forums by the 2016 Computational Linguistics and Clinical Psychology Workshop (Hollingshead and Ungar, 2016). 16 teams participated in this challenge and a variety of methods have been proposed. Most of the systems, generally used Support Vector Machine (SVM) classifiers (Cortes and Vapnik, 1995)\n2https://www.reddit.com/\nor an ensemble of some other standard classifiers for identifying the content severity. We briefly describe the top 3 approaches: Kim et al. (2016) used a Stochastic Gradient Decent classification framework. They utilized the body of the text as the main source for feature extraction and represented the post by weighted TF-IDF1unigrams and distributed representation of documents (Le and Mikolov, 2014). Malmasi et al. (2016) used a hierarchical classification framework. They employed a Random Forest meta-classification approach on top of a set of base classifiers. Finally, Brew (2016) used SVM with Radial Basis Function (RBF) kernel; they utilized TF-IDF unigram and bigram features, author type, post information and position of the post in the thread as the features for the classifier.\nIn contrast to these works, our approach is feature-rich; many features that we use are not present in the aforementioned prior work, such as psycholinguistic, contextual, topic modeling and skip thought features (see the Methods section for details). We also utilize an ensemble classifier using different subsets of features. Our proposed model outperforms the state-of-the-art by large margins.\nThis work extends our earlier effort in the CLPsych workshop where we used a Logistic Regression classifier to identify the severity of the posts (Cohan et al., 2016). We achieve up to 24% F1 score improvements over our previous results at CLPsych 2016. The improvements are due to utilizing a better learning algorithm, extending the feature sets and introducing our new ensemble model. Our models outperforms the state-of-theart by large margins.\nWhile the aforementioned works only focus on triaging the content severity, we further utilize the triaging model to perform analysis of user interactions in this forum to gain insight on the impact of the forum on the users with mental health issues. We analyze the moderators\u2019 response time to users and show that without an accurate and efficient content triaging system, manually identifying severe posts in forums with large number of users is indeed difficult."}, {"heading": "3 Severity Triaging", "text": "Our main objective is to determine the severity of the mental health forum posts based on signs of 1Term Frequency - Inverse Document Frequency\nself-harm thoughts in the content. Triaging content severity enables moderators to attend to severe cases in a timely manner and hopefully prevent a potential self-harm attempt.\nOur approach for triaging the content severity is a supervised learning framework. In the following, we first define the severity categories, then we explain the features that we use for the classification and finally, we describe the learning algorithm."}, {"heading": "3.1 Severity categories", "text": "We consider the following 4 levels of severity for the post content, as defined by (Milne et al., 2016):\n\u2022 Green - posts that do not show any signs or discussions about self-harm and thus do not require direct input from the moderators. These posts are usually general statements or follow up discussions that do not reflect any major concern.\n\u2022 Amber - posts that include minor clues that might indicate signs of struggle by the user. These posts need the moderator\u2019s attention at some point, but prompt intervention is not necessary.\n\u2022 Red - posts indicating that the user is in acute distress and moderators should attend to them as soon as possible.\n\u2022 Crisis - posts indicating that the user is in imminent risk of self-harm. These posts could be about the authors themselves or someone that the author of the post knows. Moderators should prioritize these cases above all others.\nTable 1 shows synthesized examples of posts in each of these severity categories2. Following the terminology used by Milne et al. (2016), we consider the union of CRISIS, RED and AMBER categories as FLAGGED posts, because they indicate that user might be at risk and needs attention at some point. Similarly, we consider the union of two more critical categories, i.e CRISIS and RED as URGENT.\nDue to large volume of posts produced each day, it is not possible for moderators to identify all the 2The provided examples throughout this paper are very similar to the ones in the ReachOut forum. According to the data collection policies on protecting users\u2019 identities, we are unable to include the exact posts from the forum.\ncritical posts in a timely manner. Our goal is to predict the severity of the forum posts\u2019 content so that the moderators can locate critical cases and attend to them as soon as possible. We propose a feature-rich machine learning approach utilizing psycholinguistic, topic modeling and contextual features."}, {"heading": "3.2 Features", "text": "Since the forum posts are written in unstructured raw text, we extract representative features from the text that are helpful for the supervised learning. Particularly, we extract the following categories of features:\n\u2022 Bag of words An standard approach for text representation is to model the text with bag of its constituent words. This results in a sparse vector for each text in which each element associates with a word in the vocabulary and is weighted according to some weighting scheme. We use the unigram and bigram bag of words representation of text with frequency of terms as their weights. Throughout the paper, when we refer to some textual content (e.g. post body) as features, we are essentially referring to the unigram and bigram bag of words representation of that text, unless otherwise noted. Before representing the text with bag of words features, we perform standard minimal preprocessing on it by lowercasing and removing stopwords.\n\u2022 Psycholintuistic The psycholinguistic features are meant to capture the different dimensions of a user\u2019s mental state through analysis of their language usage.\n\u2013 LIWC: Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2015) is a tool that captures quantitative data regarding various psychological dimensions given the user\u2019s textual writings. It utilizes several psychological lexicons along with a text analysis module that associates text with different psychologically-relevant\ncategories. We use this tool to extract different psychological attributes from the language expressed in the users\u2019 posts. While LIWC provides over 100 distinct attributes, our experimentation showed that the affective attributes, drive attributes, tonality, informal language usage, anxiety attributes and negation are the most helpful for this task. \u2013 Emotions: Emotions are very closely related to suicide. Therefore, the emotion that is reflected by the post can be a good indicator about level of severity of the content. For example, if a user\u2019s post indicates the \u201canger\u201d emotion, it is more likely to be severe in comparison with a post that shows the \u201chappiness\u201d emotion. To quantify the emotions associated with a specific post, we use DepecheMood (Staiano and Guerini, 2014), a lexicon with emotional probabilities associated with more than 37000 terms. The emotions considered by the lexicon are \u201cfear\u201d, \u201camusement\u201d, \u201canger\u201d, \u201cannoy\u201d, \u201capathy\u201d, \u201chappiness\u201d, \u201cinspiration\u201d and \u201csadness\u201d. To obtain the overall distribution of emotion over these categories for a post, we average the emotion distribution of all words in the post to obtain probability of each emotion given the post. We use these probabilities as features for the classification. In addition to the specific probabilities, we also consider the dominant emotion of the post as a separate feature. \u2013 Subjectivity: Similarly, subjective posts are more likely to be related to a severe post than an objective post. We utilize the MPQA subjectivity lexicon (Wilson et al., 2005) to differentiate between the subjective and objective posts. This lexicon contains contextual subjectivity about words or phrases that indicates expression of an emotion, opinion, stance, etc.\n\u2022 Contextual One characteristic of online forums is that they are designed to support user discussion. Therefore, having information about the context of a given post in the discussion thread provides additional information about its content. We extract the following contextual features:\n\u2013 Author\u2019s prior posts: Author\u2019s prior posts in the thread captures the development of thoughts by the user and also in combination with the body of the post captures whether the post deviates from the author\u2019s prior posts in a significant way.\n\u2013 Prior discussion: The posts preceding a target post and written by other users help in capturing surrounding discussion and development of thoughts for the target user. Specifically, we consider a window of 3 posts by other users preceding the target post as the context of the post in the thread. Limiting the window size to 3 is due to our observation that in long threads, the discussion usually deviates after a few posts, hence considering all the posts would introduce noise to the model1. We could also consider the posts succeeding the target post as additional features, however, that would not correspond to a real-world scenario. In a realistic setting, the goal is to triage the content on the forum as soon as they are posted and therefore, to comply with this setting, we do not consider any features relating to content submitted after the target post.\n\u2013 Last sentence: Finally, some critical posts are long, and mostly about some mundane and usual events that happen; in these posts, there is a sudden change at the end of the post indicating that the user might be at risk. Take the following example which is a snippet from the beginning and ending part of a longer post (Parts indicated with [...] are omitted for brevity):\n\u201cNow, I think we all know what it\u2019s like to be rejected by friends, dates, etc. While I have been stood up by a certain friend a few times, this really got to me. My dad said on tuesday [...] ... I woke up today and I since morning just don\u2019t know what to do anymore. I feel like I have nothing to live for and nothing makes me happy anymore.\u201d\nIn this example, most of the body of the post does not indicate any immediate risk to the user. However, this sudden change in the user\u2019s mental state shows that this content is potentially a severe case. If we only rely on the features capturing the entire post, the mental state shift will not be apparent as most of the post do not show any signs of risk. Therefore, we also consider the last sentence as a separate feature; we utilize the LIWC\n1We experimented with context window of sizes 1 to 5. The best performance was for context size of 3, therefore we chose window of 3 posts as the context size.\nattributes for the last sentence to focus on the final mental state of the user and to eliminate some of the dilution that may occur in longer posts.\n\u2022 Topic modeling We use the abstract \u201ctopics\u201d that occur in the collections of posts as another set of features for classification. Topic modeling (Blei, 2012) is a widely used approach for discovering the latent semantic structures (\u201ctopics\u201d) in a text body. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a generative model that describes how the documents in a dataset are created. A brief description of the LDA generative process is as follows:\n1. For each document: (a) Draw a distribution over topics (b) Generate each word in the document by:\ni. Drawing a topic \u03b2j according to the distribution selected in step (a).\nii. Drawing one word from the V words in the topic \u03b2j\nUsing this generative process, the LDA model tries to find a set of topics that are likely to have generated the collection. We trained the LDA topic model on the entire forum posts to obtain the latent topics associated with each post and we used these topics as additional features2.\n\u2022 Skip thought vectors Bag of words representation of the post is a sparse representation in which most of the entries are zero. More recently, approaches have been proposed for obtaining a dense representation of sentences that can encode syntactic and semantic properties of sentences in vectors. Skip thought vectors (Kiros et al., 2015) are one such model that use \u201csequence to sequence\u201d models on pairs of consecutive sentences to learn the sentence encoding. Their model consist of a encoder-decoder framework in which the encoder maps words to a sentence vector and a decoder is used to generate the surrounding sentences. By analysis through several tasks, Kiros et al. (2015) showed that this approach results in good sentence encodings when trained on a sufficiently large corpus. We use this model to encode the forum posts in dense representations. We average the vector representation of all sentences in the post to encode the entire post. 2We limited the number of topics to 100. We experimented with 20,50,100, and 200 topics and 100 topics was the optimal choice.\n\u2022 Forum metadata Forum metadata such as number of post views, length of the thread, and number of post \u201ckudos\u201d, a ReachOut feature similar to \u201clikes\u201d on Facebook, are additional features that we considered. Motivated by previous research that identified the time of day of online activity as a useful mental health signal (Coppersmith et al., 2014; De Choudhury et al., 2013c), we also consider the broad temporal categories (day and night) as well as more fine-grained intervals (morning, afternoon, evening, and night). However, we did not observe an increase in the classifier\u2019s performance with the addition of the temporal metadata attributes."}, {"heading": "3.3 Learning algorithm", "text": "After extracting features, we use supervised multiclass classification for triaging the user posts into different severity categories. We use the XGBoost Tree Boosting (Chen and Guestrin, 2016) as the learning algorithm. We experimented with several other standard classifiers such as logistic regression, random forest, and SVM, but XGBoost showed the best results.\nLet the datasetD = {xi, yi}ni=1 consist of n different training instances in which the i th instance is represented by a feature vector xi and label yi. In matrix notation, the entire feature vector and the labels are represented as (X,y). Given this dataset D, the XGBoost tree ensemble model uses an ensemble of K additive functions (regression trees) to predict the output y\u0302i:\ny\u0302i = \u03c6(xi) = K\u2211 k=1 fk(xi), fk \u2208 F (1)\nwhere \u03c6 represents the model that predicts the output given the feature vector xi, F is the space of all regression trees, and K is the total number of regression trees used. The essential part of the model is regression trees fi. To learn f , given the model output y\u0302 and the true class labels y, the following regularized objective function is optimized over the training data:\nL = n\u2211\ni=1 l(yi, y\u0302i) + K\u2211 k=1 \u2126(fk) (2)\nwhere l is a differentiable convex loss function (e.g. squared loss l(yi, y\u0302i) = (yi \u2212 y\u0302i)2), and \u2126(fk) is the regularizing function that penalizes\nthe complexity of the functions to prevent overfitting. The model is trained additively by greedily adding fk that most improves the model based on equation 2. The additive function fk is also learned by a greedy tree growth algorithm. Several approximations are used that can quickly optimize the objective function. For more details on these steps, refer to the XGBoost reference (Chen and Guestrin, 2016).\nIn addition to the single classification model, we also utilize the ensemble of several XGBoost classifiers, each trained on a different subset of features from the entire feature space. We empirically determine the optimal subsets of features. By ensembling, we use multiple classifiers to obtain better performance than individual classifiers. Intuitively, we take advantage of several conceptually different models (each of which obtained by training on a different feature set), and we aggregate their predictions to obtain the final class label. We use the majority voting ensembling approach which has been shown to balance out the weaknesses of individual classifiers (Lam and Suen, 1997; Opitz and Maclin, 1999).\nFormally, let {\u03c6(1), ..., \u03c6(m)} be m models obtained by training the classifier on m different feature sets {X(1), ... ,X(m)}. Similarly let {y\u0302(1), ..., y\u0302(m)} represent the output predicted by models {\u03c6(1), ..., \u03c6(m)}. For the i th instance in the dataset, the majority voting ensembling approach predicts the class label y\u0302i according to the following:\ny\u0302i = argmax c\u2208{c1,...,cT } (\u2223\u2223\u2223{j \u2208 {1, ...,m} : y\u0302(j)i = c}\u2223\u2223\u2223) (3)\nwhere {c1, ..., cT } is the set of all possible class labels.\nXGBoost has several hyperparameters including the learning rate (\u03b7), the minimum sum of the weigths of all observations in a child (minweight), and the maximum depth of the tree (max-depth). We used the default parameters which are \u03b7 = 0.3, min-weight = 1 and maxdepth = 6. We did not observe any performance gain by modifying the default recommended hyperparameters."}, {"heading": "4 Experimental setup", "text": ""}, {"heading": "4.1 Data", "text": "The data that we use in this research are forum posts from ReachOut.com which is a very large and popular mental health forum in Australia and receives about 1.8 yearly visits (Millen, 2015). While this forum provides a discussion platform for ordinary topics such as life, family and friendship, its main purpose is to support discussions around more critical topics such as addiction, sexuality, identity and mental health problems. Most of the users and visitors are young people aging between 14 to 25 years old. ReachOut employs several senior moderators as well as younger people who volunteer for forum moderation. These moderators focus on cases that require attention and try to help these individuals by engaging in the discussion, showing compassion and support, and providing links and resources to the individuals.\nWe use a subset of the ReachOut forum containing 65,755 posts, 1,188 of which had been labeled by moderators based on 4 different categories of severity. The dataset contains separate training and testing sets; its characteristics are outlined in Table 2. The posts occurred between July 2012 and June 2015, with labeled posts being from May 2015 to June 2015. The posts were written by 1,647 unique authors. Each post contains several fields such as the post date and time, username of the author, number of kudos, subject of the thread, and the textual body of the post.\nData collection. The full details of the data collection and the discussion on the ethical issues are discussed by Milne et al. (2016). While analysis of the mental health forum data provides many benefits, there are always trade-offs between the benefits and the risk to the privacy of the individuals. Milne et al. (2016) identified three groups of participants to whom the data collection and annotation process could cause harm: to the researchers who annotated the data, to the researchers who ac-\ncessed the data, and to the people who authored the content. The data collection process ensured that the researchers were aware of the distressing nature of the content. To protect its users, forum members of the ReachOut are instructed to keep themselves safe and anonymous. Furthermore, the moderators in the forum actively redact any content that might reveal the identity of the users. The organizers further protected the forum member\u2019s anonymity by restricting researchers in contacting the individuals in the forum, distributing the data, and cross-referencing individuals against other social media."}, {"heading": "4.2 Evaluation", "text": "Following Milne et al. (2016), we use the accuracy and F-1 scores for evaluating the classification performance to be able to directly compare the performance of our approach with the state-ofthe-art. To aggregate the scores for the individual categories, Milne et al. (2016) used the macro average of F-scores for the non-GREEN (critical) categories as the official metric for the CLPsych 2016 shared task. This metric emphasizes the importance of triaging among the critical categories. They also consider the F-1 and accuracy scores for binary classification of FLAGGED (i.e. CRISIS \u222a RED \u222a AMBER) vs. GREEN, and URGENT (i.e. CRISIS \u222a RED) vs. non-URGENT categories to capture the performance of systems in identifying critical posts. We also use these additional metrics to further evaluate the performance of our approach. FLAGGED classification shows that the post contains content indicating risk of self-harm to the user while URGENT indicates that the user is at a more imminent risk and needs prompt attention (see the Method Section for complete definitions of severity categories)."}, {"heading": "4.3 Baselines and comparison", "text": "We compare our methods with the top 4 performing systems among 16 total participating teams in the CLPsych 2016 shared task. To better evaluate\nour methods, we also consider a simple baseline which is SVM classifier with unigram and bigram bag-of-words features extracted from the body of the post (refer to bag-of-words features explained in Methods section for details)."}, {"heading": "5 Results and analysis", "text": "The results of our models for triaging the content severity compared with the baseline and state of the art systems is presented in Table 3; it includes results on the test set 3(a), as well as stratified cross-validation1 results on the training set 3(b). For prior work, we report the official results that are percentages without any precision points. The single model indicates the performance of our proposed model using a single classifier while the ensemble model is a model based on 6 different clas1The stratified cross validation in contrast to the regular cross validation preserves the distribution of the classes when splitting the data into train and test sets.\nsifiers. The features used in each of the models are presented in Table 4. In the Analysis Section, we will discuss the effect of different features on the performance. As illustrated in Table 3(a), our models outperform the baseline and all top performing state of the art systems by large margins. We observe that the non-GREEN macro average F1 score for the individual and ensemble models improves over the best system (Kim et al., 2016) by +12% and +17%, respectively. Similarly, we observe that the F1 scores for the FLAGGED category is 3% and 5% higher than the best system with the individual and ensemble models, respectively. Finally, in URGENT category, the individual and ensemble models achieve 73.1% and 75.1% F1 scores respectively, which shows large improvement over the state of the art. We observe similar improvements in the cross-validation results on the training set (Table 3(b)). Since we have 10 different folds on the training set, we also perform a\nstatistical significance test and we observe statistically significant improvement over the baseline for both the single and ensemble methods (Student\u2019s t-test); the ensemble method also outperforms the single method statistically in virtually all metrics. In particular, the single and ensemble models achieve 48% and 53% improvements over the baseline based on non-GREEN macro average F1 scores.\nTable 5 shows the breakdown of results by each category. We present results on the test set in Table 5(a) and cross validation results on training set in Table 5(b). It should be noted that there was only 1 CRISIS case in the test set and no team out of 16 teams were able to correctly identify this case. While our models were also unable to find the single CRISIS case, they show improvements over the state of the art in other categories. Specifically, we observe that the ensemble model achieves F1 score of 75.5% in RED which improves over the best performance (65%) by 16%. Similarly, we observe large improvement of F1 for the AMBER category (10%). Finally, our model also slightly improves upon the state of the art on the GREEN category. We also report results on the training set evaluated by 10 fold stratified cross validation (Table 5(b)). As illustrated, our methods achieve statistically significant improvement over the baseline in all severity categories. The overall lower performance on the CRISIS category is mainly due to the limited training data in this category. As shown in Table 2, there are only 40 CRISIS posts in the training set which is not enough for a supervised learning model to accurately estimate the optimal parameters.\nOverall, the results show that both our single and ensemble models can effectively identify posts with critical content (FLAGGED) with F1 and accuracy of 92% and 93%, respectively, on the test set, providing large improvements over the state of the art.\nIn the rest of this section, we first analyze the effect of different features that we proposed to use for triaging the content severity. Then we analyze the types of errors that our model makes to better understand the robustness of our proposed approach. Finally, using the proposed triaging model, we investigate the potential effect of the mental health forum on the individuals."}, {"heading": "5.1 Feature Analysis", "text": "In the \u201cSeverity Triaging\u201d Section, we presented our proposed features for the task of triaging the content severity. Table 6 shows the effect of each of the features when added to the classification model. We do not show the combinations of features that perform significantly worse than the body of the text. As illustrated, we observe that most of the proposed features have a positive effect on the performance of the system with the exception of skip thought vectors. The bag of words features of the body of the text achieve F1 score of 34.8% on the test set. Adding contextual features (prior posts by other users and user\u2019s previous posts in the thread) improves the results to 38.5%. Similarly, we observe that addition of forum metadata features (length, kudos, and post views), subjectivity and emotion features, and features from the last sentence also improve the performance. Topic modeling yields further boost to the performance of the system which indicates the effectiveness of latent topics inferred from the forum posts using the LDA model. We observe that LIWC features by themselves do not improve the results as much as topic modeling, however when combined with topic modeling features, greatest improvement is achieved (47.2% F1). This row (indicated by \u2217) comprises all features in the single model reported in tables 3 and 5.\nWe build an ensemble of distinct models each of which trained on a different feature set. We experimented with various ensembles of the features. Last row of Table 6 shows the performance of the best ensemble model. We do not report other ensembles that resulted in suboptimal performance. The ensemble model that obtains the best results is comprised of 6 different feature sets outlined in Table 4. As evidenced by Table 6, each of these sets are helpful features that can capture different characteristics of the associated forum post; therefore when combined by ensembling, the weaknesses of single set of features on some instances are compensated by the others. Therefore, as the results show, the ensemble model is more effective in comparison with the single models.\nWe note that skip thought vectors (second row in Table 6) did not improve the baseline. We also experimented with encoding the prior posts and authors posts with skip thought vectors but we did not observe any improvements. As shown by Kiros et al. (2015), when trained on a suffi-\nciently large data, skip thought vectors encode text in dense vectors that can capture underlying semantic and syntactic properties of the text; and thus useful to be used as features for classification. However, in this task we observe that classification using skip thought vectors does not result in any improvements. The lack of improvement by these vectors indicates that the vectors are not able to capture any information beyond what is provided by other features. This could be due to averaging the sentence vectors. We represent the post which consists of several sentences by averaging the vectors corresponding to each sentence; some of the information of the individual sentences might be lost when averaged with other sentences. There-\nfore, a better approach for composing the post vectors of its constituent sentence vectors could lead to better results."}, {"heading": "5.2 Error Analysis", "text": "Error analysis shows that misclassification of content severity is mainly due to the following reasons:\n1. Brevity of the posts and lack of sufficient background context.\nSome URGENT categories that were misclassified are associated with a rather short post from which limited information can be obtained. For example, the following post is taken from a long\ndiscussion thread and is labeled as GREEN by the classifier while the actual label is RED.\n\u201cI got the reply from x about my complaint. All they did was make excuses for themselves. no help at all.\u201d\nThis post on its own does not show any risk to the user. However, reading the entire associated thread in the forum reveals that the author of the post had experienced a problem with their counseling service for their mental distress, and they were in need for mental help and support. To infer this context about this specific post, the immediate surrounding posts are not sufficient and one needs to read the entire conversation.\nIn the model, we already consider the immediate surrounding posts as the context for the post. However, this may not capture the context in very long discussion threads (such as the above example). When we increased the number of previous posts to be considered as the context, we observed an overall suboptimal performance. This is because, generally, in long threads the discussion tends to change after a few posts. Thus, considering longer window of posts in a thread as context for a target post might result in adding posts that are not necessarily relevant to the target post and consequently introduce noise to the model.\n2. Variations in tone.\nIn some misclassification cases, we observe sudden changes and variations in the tone of the post expressed by the user and that makes it difficult for the learning algorithm to correctly classify the associated severity. For instance consider the following post:\n\u201c I went to my favorite show last week and it was amazing. I usually feel very low, specially at nights. This was one of the rare times that I was actually happy for some time... Five days ago at school one classmate of mine bullies me and he shouts that he wishes me dead. I ignored him completely at the moment and I was totally fine. But when I got back home I felt like a total loser and the bad thoughts about myself started coming back.\u201d\nIn this post we observe that the user starts with a positive tone and then it changes to negative. Then the tone switches between positive and negative multiple times. This specific example is an AMBER case and the classifier mislabeled it as RED.\nIn the proposed triaging model, we capture the user\u2019s final state of the mind by considering features from the last sentence. However, when there are too many tone variations in the post, the exact severity of the post might be misclassified. We note that the size of the training dataset was limited and therefore capturing these subtle cases requires more of similar training instances. Future work could investigate whether these variability of various psychological variables (e.g. tone) can be considered as a risk factor for individuals.\n3. Long posts with only a small part containing concerning content.\nIn a few long posts, we observe only a small part showing signs of distress to the user, while the rest of the post has a neutral to positive tone. A misclassified example with actual label of RED is shown below (Parts indicated with [...] are omitted for brevity):\n\u201cThis book series is a roller coaster. Maze runner series, I\u2019m onto the prequel book now. They are amazing [...] I\u2019ve always been too resilient. I just hate everything and it confuses me. Maybe I\u2019m tired of all this and want to do something.. I just... nothing is set. Yesterday Lora called and we talked like a lot about school, friends [...] It feels good to say, or type, all this.\u201d\nThis snippet is from a much longer post and as it can be observed, only the underlined part contains content that indicate mental distress to the user.\nIn such posts, the effect of the small negative part of the post is played down by the larger dominant neutral tone and therefore the model could mispredict this. In this case although still correctly identified as critical, the classifier misclassifies the severity level as AMBER instead of RED.\nOverall, most of classification errors occur within the FLAGGED category; there are very few cases in the FLAGGED posts that are missed by the classifier and labeled as GREEN. This can also be observed in Table 3 in FLAGGED category performance which obtains F1 and accuracy scores of 92.2% and 93.4%, respectively. Our results are encouraging since they show that the model can effectively capture FLAGGED posts, i.e. all posts that indicate some signs of harm to the user."}, {"heading": "5.3 User Analysis", "text": "We study the user content severity in the forum over time to analyze if it is helpful to the indi-\nviduals. For the purposes of user analysis, we mostly rely on the binary classification of URGENT (CRISIS and RED) vs. non-URGENT, and FLAGGED (CRISIS \u222a RED \u222a AMBER) vs. GREEN categories. In these categories, as shown in Tables 3 (a and b), the ensemble classification model obtains F-1 scores of 90% and 75% respectively (accuracy of 91% and 93%) and thus it is relatively reliable for studying larger scale trends of content severity in the entire forum. Figure 1 shows the results of severity triaging throughout all the posts in the dataset. As illustrated, there is a steady increase in the amount of FLAGGED posts. Given this trend, we examine patterns of post severity to understand the effects that the forum might have on the individuals. Specifically, we investigate the following research questions:\nQ-1. Does engaging with the forum have a positive effect on the users?\nOur analysis indicated a decline in the average content severity over time, which may indicate a positive effect of the forum on its users. This correlative effect suggests that further controlled trials should be conducted to carefully ascertain the causal nature of this relationship.\nThe dataset includes posts from the forum in a time window of 36 months during which we quantify the behavior of users. To measure the relation of user interaction with the forum, we split the users into two groups. Users are considered active\nif they have posted for two or more months on the forum, and inactive if they had only posted during a single month. We only consider active users for the analysis because for inactive users, the activity period of one month is too short to present a significant relation. In these 36 months, there are a total of 452 active users and 1,195 inactive users. We analyze the severity of the first post and last posts of users, average post severity during their first and last months of activity and finally, the trend lines of severity during entire time of interaction with the forum.\nTables 7(a) and 7(b) show the number by the severity of their first and last posts on the forum. A Chi-square test on the contingency tables was performed to ensure that the difference between the cells are interpretable. For both table 7(a) and 7(b) we found significant interaction, \u03c72 = 58.4, p < .001 and \u03c72 = 21.4, p < .001, respectively. In general, we observe that the users\u2019 last posts tend to be of lower severity than their first post. 81% of users whose first post received an URGENT label had a final post with a non-URGENT label. Only 10% of users whose first post was non-URGENT had a final post of URGENT. In both the FLAGGED and URGENT matrices, there were more users whose final posts was GREEN or nonURGENT than users who had FLAGGED or URGENT first posts.\nTables 8(a) and 8(b) show the comparison of the average user content severity in the first and\nlast month of users\u2019 activity in the forum (Chisquare test showed that the results are interpretable with a significant difference of \u03c72=86.47 (p < 0.001) and \u03c72=52.82 (p < 0.001) for Tables 8(a) and 8(b), respectively.). We observe a similar positive trend in the URGENT category; in the FLAGGED category, the number of users whose average initial content and last content is FLAGGED (120 users) is more than those whose content is shifted from FLAGGED to GREEN (78 users). However, there are very few GREEN users whose content eventually turned FLAGGED (46 users). Furthermore, the total number of users with first month FLAGGED posts (198) is higher than number of users with last month FLAGGED posts (166). These results also indicate that users\u2019 last posts tend less severe than their first posts.\nWe believe this is because many users join this type of forums to get immediate support for a moment of crisis or acute mental distress. After some time, this initial distress is decreased, as reflected in the patterns of post severity. That is why the initial activity of users in general tend to be more severe than their final posts. We believe this could\nbe for the following reasons: (i) Pattern of post severity drops off once the user is in a more stable mental state compared with their initial state of crisis. (ii) Interaction with the forum and engaging in discussion with other forum users might have resulted in reducing the acute distress in users (verifying the exact causal relation requires further user level controlled trials).\nIn addition to first and last months of activity, we also analyze the trends throughout the entire time of user activity. To do so, we consider the average severity of the posts in each month as a data point for that month, and we then fit a trend line to the data points. We consider the following numeric values for each category to be able to quantify the average severity in each month: CRISIS = 1.0, RED = 0.66, AMBER = 0.33, GREEN = 0.0. Using these numeric equivalent of severity classes, for each user, we associate an average severity for all their posts in each month. Then, we fit a linear model on this data to show the trend line of the content severity over time. Figure 2 shows a sample plot of the post severity for a user over time and its associated trend line.\nTo fit an appropriate trend line to the data, we minimize the squared error between the target trend line and the actual severity data points. Specifically, the equation of a trend line for variable x is given by p(x) = m.x + b where m and b are the slope and intercept of the line, respectively. A negative (positive) trend line slope indicates that overall, the severity of user content has declined (increased). Given D severity data points {(xi, yi)}Di=1, the values of m and b are found by minimizing the squared error over the data:\nE = D\u2211 i=0 |p(xi)\u2212 yi|2 (4)\nTo check if a linear model is a applicable for our case, we calculated the r values associated with\nthe trendlines. In particular, for each user we calculated the r value of their content severity trend lines and we calculated the average and the standard deviation of these values (Table 9). As illustrated, the average of r values are 0.68 (-0.72) for the positive (negative) trends which is around 0.7 (-0.7). Absolute r values greater than 0.5 indicate high to strong linear relationship in the data (Tabachnick et al., 2001). Thus, linear trend analysis is a reasonable fit to this data.\nTo analyze overall trends in the content severity, we calculate the content severity trend line for each user and then analyze the overall trend line statistics for the users. We observed that many users have steady trend lines with a slope of near zero. To eliminate the noise caused by these neutral trends from our analysis, we filter out the users whose content severity trend lines are essentially flat. These users are either the moderators of the forum or are users that show consistent behavior over time. We then analyze how the content severity of the other users with varying content severity changes over time. Table 10 shows the statistics for all the trends lines among all the active users. To eliminate trend lines having a slope near zero, we consider a threshold. We analyze results based on different values of this threshold. For example, for the threshold \u03c4 , the corresponding row on the Table only considers trend lines with slope m such that m < \u2212\u03c4 or m > \u03c4 and fil-\nters out all other lines having |m| <= \u03c4 . We also show the results in the case that there is no threshold (last row of the Table). FLAGGED vs GREEN corresponds to plots with numeric severity value of 1.0 for a FLAGGED post and 0.0 for a GREEN post; Fine-grained severity categories corresponds to plots with following numerics severity values: CRISIS = 1.0, RED = 0.66, AMBER = 0.33, GREEN = 0.0. As illustrated in Table 10, we observe an average negative trend line slope for all the values of the threshold. This indicates a decline of average content severity among all the users. Furthermore, we observe that majority of users have a trend line with a negative slope and thus, decreasing severity of content.\nThese results indicate that overall there is a decline in the content severity of the users as they interact with the forum, which could be due to the potential positive effect of the forum on its users. This effect could be attributed to the users expressing their feelings and emotions, receiving support and feedback from the moderators, and discussing issues with users experiencing similar problems. However, we note that here we only observe the negative trend of content severity; to study the exact causal relationship between interaction with the forum and content severity, further controlled trials on the forum users should be conducted.\nQ-2. What effect does the duration of engagement with the forum have on the users?\nWe analyze how the duration of a user\u2019s engagement with the forum impacts the severity of their posts over time. Tables 11(a) and 11(b) show that users with a first month severity of FLAGGED or URGENT posts interacted with the forum for 3-4 months, while other users interacted with the forum for 5-7 months. These tables are essentially showing that users with less critical posts in the first month tend to interact with the forum in a more long-term basis in comparison with users whose initial posts are critical. The difference in the duration of user interaction by their initial content severity indicates that there are users who visit the forum for immediate assistance in a critical moment and those who use the forum as a longerterm support resource. This result suggests that users whose first posts are more severe could be on the forum for immediate support and will only stay active until their critical mental state reaches a safe equilibrium again. In contrast, the users whose first month is GREEN or non-URGENT may be seeking a long-term resource and a community of users with shared experiences.\nThis difference between the activity period of users by their initial content reveals an opportunity for moderators to improve their response time to FLAGGED and URGENT posts. Faster moderator attention to FLAGGED and URGENT posts would provide better quality of help to these short-term users and encourage them to further interact with the forum for receiving support. Triaging the forum posts to allow moderators improve their response time would benefit all user groups, and particularly users who currently visit the forum for an immediate support.\nQ-3. What is the effect of moderator response time on the user\u2019s forum behavior?\nSince the focus of this research is on triaging the severity of mental health forum posts, we seek to understand how quickly moderators are currently responding to posts by their severity. Table 12\nshows the average time for a moderator to respond, as well as the percentage of cases in which the moderators were the first to respond to a user. It shows that in cases where a moderator was the first to respond to a FLAGGED or URGENT post, they took on average more than four hours to respond. Unfortunately, four hours might be too long for users with imminent risks and it is very important to reduce this response time to prevent a potential self harm. Additionally, we observe that moderators are the first responders on less than 33% of non-GREEN posts, meaning the other forum users responded to majority of posts earlier than moderators. This further stresses the value of triaging content severity, so that moderators can quickly respond to critical posts rather than having to identify such posts on the forum manually."}, {"heading": "6 Conclusions", "text": "We presented an approach for triaging the content severity in mental peer support forums with a specific goal of identifying cases with potential risk of self-harm. Triaging the content severity helps the forum moderators to locate the critical cases and attend to them as soon as possible. We used a feature-rich classifier with various sets of features including psycholinguistic, contextual, topic modeling and forum metadata features for triaging the content into different severity categories. In addition to a single classifier, we also built an ensemble classifier by using different sets of features. We evaluated our approach on the data from ReachOut.com, a large mental health forum. We showed that our approaches can effectively improve over the state-of-the-art by large margins (up to 17% macro-average F1 scores of critical categories). We showed that the content severity of the users tend to decrease as they interact with the forum. Results further indicated that there is a need for effective and efficient triaging of forum post data to assist the moderators in attending the users with potential risk of self-harm.\nThe impact of this research is important from two perspectives. It stresses the importance of mental health forums as a support platform for users with mental health problems. It furthermore provides an efficient and effective way for moderators to asses the content severity of the forum, and consequently help individuals in need and prevent self harm incidents."}, {"heading": "7 Acknowledgments", "text": "This work was partially supported by National Science Foundation (NSF) through grant CNS1204347."}], "references": [{"title": "Suicide statistics 2016", "author": ["American Foundation for Suicide Prevention."], "venue": "AFSP; New York, NY .", "citeRegEx": "Prevention.,? 2016", "shortCiteRegEx": "Prevention.", "year": 2016}, {"title": "Probabilistic topic models", "author": ["David M Blei."], "venue": "Communications of the ACM 55(4):77\u201384.", "citeRegEx": "Blei.,? 2012", "shortCiteRegEx": "Blei.", "year": 2012}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "Journal of machine Learning research 3(Jan):993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Validating machine learning algorithms for twitter data against established measures of suicidality", "author": ["Scott R Braithwaite", "Christophe Giraud-Carrier", "Josh West", "Michael D Barnes", "Carl Lee Hanson."], "venue": "JMIR mental health 3(2):e21.", "citeRegEx": "Braithwaite et al\\.,? 2016", "shortCiteRegEx": "Braithwaite et al\\.", "year": 2016}, {"title": "Random forests", "author": ["Leo Breiman."], "venue": "Machine learning 45(1):5\u201332.", "citeRegEx": "Breiman.,? 2001", "shortCiteRegEx": "Breiman.", "year": 2001}, {"title": "Classifying reachout posts with a radial basis function svm", "author": ["Chris Brew."], "venue": "Proceedings of the Third Workshop on Computational Lingusitics and Clinical Psychology. Association for Computational Linguistics, San Diego, CA, USA, pages 138\u2013142.", "citeRegEx": "Brew.,? 2016", "shortCiteRegEx": "Brew.", "year": 2016}, {"title": "Machine classification and analysis of suicide-related communication on twitter", "author": ["Pete Burnap", "Walter Colombo", "Jonathan Scourfield."], "venue": "Proceedings of the 26th ACM Conference on Hypertext & Social Media. ACM, pages 75\u201384.", "citeRegEx": "Burnap et al\\.,? 2015", "shortCiteRegEx": "Burnap et al\\.", "year": 2015}, {"title": "Suicide facts at a glance 2015", "author": ["Centers for Disease Control", "Prevention."], "venue": "CDC; Atlanta, GA: Department of Health and Human Services http://www.cdc.gov/violenceprevention/suicide/statistics/.", "citeRegEx": "Control and Prevention.,? 2015", "shortCiteRegEx": "Control and Prevention.", "year": 2015}, {"title": "Syndromic surveillance of flu on twitter using weakly supervised temporal topic models", "author": ["Liangzhe Chen", "K.S.M. Tozammel Hossain", "Patrick Butler", "Naren Ramakrishnan", "B. Aditya Prakash."], "venue": "Data Mining and Knowledge Discov-", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Xgboost: A scalable tree boosting system", "author": ["Tianqui Chen", "Carlos Guestrin."], "venue": "Proceedings of the 22th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, New York, NY, USA, KDD \u201916.", "citeRegEx": "Chen and Guestrin.,? 2016", "shortCiteRegEx": "Chen and Guestrin.", "year": 2016}, {"title": "Binary classifiers and latent sequence models for emotion detection in suicide notes", "author": ["Colin Cherry", "Saif M Mohammad", "Berry De Bruijn."], "venue": "Biomedical Informatics Insights 5(Suppl 1):147.", "citeRegEx": "Cherry et al\\.,? 2012", "shortCiteRegEx": "Cherry et al\\.", "year": 2012}, {"title": "Triaging mental health forum posts", "author": ["Arman Cohan", "Sydney Young", "Nazli Goharian."], "venue": "Proceedings of the Third Workshop on Computational Lingusitics and Clinical Psychology. Association for Computational Linguistics, San Diego, CA, USA,", "citeRegEx": "Cohan et al\\.,? 2016", "shortCiteRegEx": "Cohan et al\\.", "year": 2016}, {"title": "Quantifying mental health signals in twitter", "author": ["Glen Coppersmith", "Mark Dredze", "Craig Harman."], "venue": "Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality. pages 51\u201360.", "citeRegEx": "Coppersmith et al\\.,? 2014", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2014}, {"title": "From adhd to sad: Analyzing the language of mental health on twitter through self-reported diagnoses", "author": ["Glen Coppersmith", "Mark Dredze", "Craig Harman", "Kristy Hollingshead."], "venue": "Proceedings of the 2nd Workshop on Computational Linguistics", "citeRegEx": "Coppersmith et al\\.,? 2015", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2015}, {"title": "Exploratory analysis of social media prior to a suicide attempt", "author": ["Glen Coppersmith", "Kim Ngo", "Ryan Leary", "Anthony Wood."], "venue": "Proceedings of the Third Workshop on Computational Lingusitics and Clinical Psychology. Association for Computa-", "citeRegEx": "Coppersmith et al\\.,? 2016a", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2016}, {"title": "Exploratory analysis of social media prior to a suicide attempt", "author": ["Glen Coppersmith", "Kim Ngo", "Ryan Leary", "Anthony Wood."], "venue": "Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. Association for Com-", "citeRegEx": "Coppersmith et al\\.,? 2016b", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2016}, {"title": "Supportvector networks", "author": ["Corinna Cortes", "Vladimir Vapnik."], "venue": "Machine Learning 20(3):273\u2013297. https://doi.org/10.1007/BF00994018.", "citeRegEx": "Cortes and Vapnik.,? 1995", "shortCiteRegEx": "Cortes and Vapnik.", "year": 1995}, {"title": "Nonparametric discovery of online mental health-related communities", "author": ["Bo Dao", "Thin Nguyen", "Svetha Venkatesh", "Dinh Phung."], "venue": "International Conference on Data Science and Advanced Analytics. IEEE, DSAA \u201915, pages 1\u201310.", "citeRegEx": "Dao et al\\.,? 2015", "shortCiteRegEx": "Dao et al\\.", "year": 2015}, {"title": "Social media as a measurement tool of depression in populations", "author": ["Munmun De Choudhury", "Scott Counts", "Eric Horvitz."], "venue": "Proceedings of the 5th Annual ACM Web Science Conference. ACM, New York, NY, USA, WebSci \u201913, pages 47\u201356.", "citeRegEx": "Choudhury et al\\.,? 2013a", "shortCiteRegEx": "Choudhury et al\\.", "year": 2013}, {"title": "Mental health discourse on reddit: Self-disclosure, social support, and anonymity", "author": ["Munmun De Choudhury", "Sushovan De."], "venue": "ICWSM.", "citeRegEx": "Choudhury and De.,? 2014", "shortCiteRegEx": "Choudhury and De.", "year": 2014}, {"title": "Predicting depression via social media", "author": ["Munmun De Choudhury", "Michael Gamon", "Scott Counts", "Eric Horvitz."], "venue": "Proceedings of the Seventh International AAAI Conference on Weblogs and Social Media (ICWSM). AAAI.", "citeRegEx": "Choudhury et al\\.,? 2013b", "shortCiteRegEx": "Choudhury et al\\.", "year": 2013}, {"title": "Predicting depression via social media", "author": ["Munmun De Choudhury", "Michael Gamon", "Scott Counts", "Eric Horvitz."], "venue": "ICWSM. page 2.", "citeRegEx": "Choudhury et al\\.,? 2013c", "shortCiteRegEx": "Choudhury et al\\.", "year": 2013}, {"title": "Discovering shifts to suicidal ideation from mental health content in social media", "author": ["Munmun De Choudhury", "Emre Kiciman", "Mark Dredze", "Glen Coppersmith", "Mrinal Kumar."], "venue": "Proceedings of the 2016 CHI Conference on Hu-", "citeRegEx": "Choudhury et al\\.,? 2016", "shortCiteRegEx": "Choudhury et al\\.", "year": 2016}, {"title": "Emotion detection in suicide notes", "author": ["Bart Desmet", "V\u00e9ronique Hoste."], "venue": "Expert Systems with Applications 40(16):6351\u20136358.", "citeRegEx": "Desmet and Hoste.,? 2013", "shortCiteRegEx": "Desmet and Hoste.", "year": 2013}, {"title": "How social media will change public health", "author": ["Mark Dredze."], "venue": "IEEE Intelligent Systems 27(4):81\u2013", "citeRegEx": "Dredze.,? 2012", "shortCiteRegEx": "Dredze.", "year": 2012}, {"title": "Psychological language on twitter predicts county", "author": ["Johannes C Eichstaedt", "Hansen Andrew Schwartz", "Margaret L Kern", "Gregory Park", "Darwin R Labarthe", "Raina M Merchant", "Sneha Jha", "Megha Agrawal", "Lukasz A Dziurzynski", "Maarten Sap"], "venue": null, "citeRegEx": "Eichstaedt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Eichstaedt et al\\.", "year": 2015}, {"title": "I can\u2019t get no sleep: discussing# insomnia on twitter", "author": ["Sue Jamison-Powell", "Conor Linehan", "Laura Daley", "Andrew Garbett", "Shaun Lawson."], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, pages 1501\u2013", "citeRegEx": "Jamison.Powell et al\\.,? 2012", "shortCiteRegEx": "Jamison.Powell et al\\.", "year": 2012}, {"title": "The development and validation of statistical prediction rules for discriminating between genuine and simulated suicide notes", "author": ["Natalie J Jones", "Craig Bennell."], "venue": "Archives of Suicide Research 11(2):219\u2013 233.", "citeRegEx": "Jones and Bennell.,? 2007", "shortCiteRegEx": "Jones and Bennell.", "year": 2007}, {"title": "Surveillance for violent deathsnational violent death", "author": ["Debra L Karch", "Linda L Dahlberg", "Nimesh Patel", "Terry W Davis", "Joseph E Logan", "Holly A Hill", "L Ortega"], "venue": "reporting system,", "citeRegEx": "Karch et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Karch et al\\.", "year": 2009}, {"title": "Data61-csiro systems at the clpsych 2016 shared task", "author": ["Sunghwan Mac Kim", "Yufei Wang", "Stephen Wan", "Cecile Paris."], "venue": "Proceedings of the Third Workshop on Computational Lingusitics and Clinical Psychology. Association for Computational", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."], "venue": "arXiv preprint arXiv:1506.06726 .", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Application of majority voting to pattern recognition: an analysis of its behavior and performance", "author": ["Louisa Lam", "SY Suen."], "venue": "IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans 27(5):553\u2013568.", "citeRegEx": "Lam and Suen.,? 1997", "shortCiteRegEx": "Lam and Suen.", "year": 1997}, {"title": "Separating fact from fear: Tracking flu infections on twitter", "author": ["Alex Lamb", "Michael J. Paul", "Mark Dredze."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Lamb et al\\.,? 2013", "shortCiteRegEx": "Lamb et al\\.", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "ICML. volume 14, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "The final hours: A linguistic analysis of the final words of a suicide", "author": ["David Lester."], "venue": "Psychological reports 106(3):791\u2013797.", "citeRegEx": "Lester.,? 2010", "shortCiteRegEx": "Lester.", "year": 2010}, {"title": "Predicting post severity in mental health forums", "author": ["Shervin Malmasi", "Marcos Zampieri", "Mark Dras."], "venue": "Proceedings of the Third Workshop on Computational Lingusitics and Clinical Psychology. Association for Computational Linguistics, San", "citeRegEx": "Malmasi et al\\.,? 2016", "shortCiteRegEx": "Malmasi et al\\.", "year": 2016}, {"title": "Reachout annual report 2013/2014", "author": ["Doug Millen."], "venue": "http://about.au.reachout.com/us/annualreports-financials.", "citeRegEx": "Millen.,? 2015", "shortCiteRegEx": "Millen.", "year": 2015}, {"title": "Clpsych 2016 shared task: Triaging content in online peer-support forums", "author": ["David N. Milne", "Glen Pink", "Ben Hachey", "Rafael A. Calvo."], "venue": "Proceedings of the Third Workshop on Computational Lingusitics and Clinical Psychology. Association for", "citeRegEx": "Milne et al\\.,? 2016", "shortCiteRegEx": "Milne et al\\.", "year": 2016}, {"title": "Quantifying the language of schizophrenia in social media", "author": ["Margaret Mitchell", "Kristy Hollingshead", "Glen Coppersmith."], "venue": "Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to", "citeRegEx": "Mitchell et al\\.,? 2015", "shortCiteRegEx": "Mitchell et al\\.", "year": 2015}, {"title": "Unraveling abstinence and relapse: smoking cessation reflected in social media", "author": ["Elizabeth L Murnane", "Scott Counts."], "venue": "Proceedings of the 32nd annual ACM conference on Human factors in computing systems. ACM, pages 1345\u20131354.", "citeRegEx": "Murnane and Counts.,? 2014", "shortCiteRegEx": "Murnane and Counts.", "year": 2014}, {"title": "Crossnational prevalence and risk factors for suicidal", "author": ["Matthew K Nock", "Guilherme Borges", "Evelyn J Bromet", "Jordi Alonso", "Matthias Angermeyer", "Annette Beautrais", "Ronny Bruffaerts", "Wai Tat Chiu", "Giovanni De Girolamo", "Semyon Gluzman"], "venue": null, "citeRegEx": "Nock et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nock et al\\.", "year": 2008}, {"title": "Towards an open-domain framework for distilling the outcomes of personal experiences from social media timelines", "author": ["Alexandra Olteanu", "Onur Varol", "Emre K\u0131c\u0131man."], "venue": "Proceedings of International AAAI Conference on Web and Social Media.", "citeRegEx": "Olteanu et al\\.,? 2016", "shortCiteRegEx": "Olteanu et al\\.", "year": 2016}, {"title": "Popular ensemble methods: An empirical study", "author": ["David Opitz", "Richard Maclin."], "venue": "Journal of Artificial Intelligence Research 11:169\u2013198.", "citeRegEx": "Opitz and Maclin.,? 1999", "shortCiteRegEx": "Opitz and Maclin.", "year": 1999}, {"title": "Perception differences between the depressed and non-depressed users in twitter", "author": ["Minsu Park", "David W McDonald", "Meeyoung Cha."], "venue": "Proceedings of International AAAI Conference on Web and Social Media (ICWSM).", "citeRegEx": "Park et al\\.,? 2013", "shortCiteRegEx": "Park et al\\.", "year": 2013}, {"title": "Health-related hypothesis generation using social media data", "author": ["Jon Parker", "Andrew Yates", "Nazli Goharian", "Ophir Frieder."], "venue": "Social Network Analysis and Mining 5(1):1\u201315. https://doi.org/10.1007/s13278-014-0239-8.", "citeRegEx": "Parker et al\\.,? 2015", "shortCiteRegEx": "Parker et al\\.", "year": 2015}, {"title": "You are what you tweet: Analyzing twitter for public health", "author": ["Michael J Paul", "Mark Dredze."], "venue": "International Conference On Web And Social Media (ICWSM) 20:265\u2013272.", "citeRegEx": "Paul and Dredze.,? 2011", "shortCiteRegEx": "Paul and Dredze.", "year": 2011}, {"title": "Social media mining for public health monitoring and surveillance", "author": ["Michael J Paul", "Abeed Sarker", "John S Brownstein", "Azadeh Nikfarjam", "Matthew Scotch", "Karen L Smith", "Graciela Gonzalez."], "venue": "Pacific Symposium on Biocomputing. Pacific Sympo-", "citeRegEx": "Paul et al\\.,? 2016", "shortCiteRegEx": "Paul et al\\.", "year": 2016}, {"title": "The development and psychometric properties of liwc2015", "author": ["James W Pennebaker", "Ryan L Boyd", "Kayla Jordan", "Kate Blackburn."], "venue": "UT Faculty/Researcher Works .", "citeRegEx": "Pennebaker et al\\.,? 2015", "shortCiteRegEx": "Pennebaker et al\\.", "year": 2015}, {"title": "Sentiment analysis of suicide notes: A shared task", "author": ["John P Pestian", "Pawel Matykiewicz", "Michelle LinnGust", "Brett South", "Ozlem Uzuner", "Jan Wiebe", "Kevin B Cohen", "John Hurdle", "Christopher Brew."], "venue": "Biomedical Informatics Insights 5(Suppl. 1):3.", "citeRegEx": "Pestian et al\\.,? 2012", "shortCiteRegEx": "Pestian et al\\.", "year": 2012}, {"title": "Understanding topics and sentiment in an online cancer survivor community", "author": ["Kenneth Portier", "Greta E Greer", "Lior Rokach", "Nir Ofek", "Yafei Wang", "Prakhar Biyani", "Mo Yu", "Siddhartha Banerjee", "Kang Zhao", "Prasenjit Mitra"], "venue": "JNCI Monographs", "citeRegEx": "Portier et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Portier et al\\.", "year": 2013}, {"title": "Reducing bias in observational studies using subclassification on the propensity score", "author": ["Paul R Rosenbaum", "Donald B Rubin."], "venue": "Journal of the American statistical Association 79(387):516\u2013524.", "citeRegEx": "Rosenbaum and Rubin.,? 1984", "shortCiteRegEx": "Rosenbaum and Rubin.", "year": 1984}, {"title": "Impact of psychiatric disorders on health-related quality of life: general population survey", "author": ["Samuli I Saarni", "Jaana Suvisaari", "Harri Sintonen", "Sami Pirkola", "Seppo Koskinen", "Arpo Aromaa", "JOUKO L\u00d6NNQVIST."], "venue": "The British journal of psychiatry", "citeRegEx": "Saarni et al\\.,? 2007", "shortCiteRegEx": "Saarni et al\\.", "year": 2007}, {"title": "Towards assessing changes in degree of depression through facebook", "author": ["H Andrew Schwartz", "Johannes Eichstaedt", "Margaret L Kern", "Gregory Park", "Maarten Sap", "David Stillwell", "Michal Kosinski", "Lyle Ungar."], "venue": "Proceedings of the Workshop on", "citeRegEx": "Schwartz et al\\.,? 2014", "shortCiteRegEx": "Schwartz et al\\.", "year": 2014}, {"title": "The prevention of suicidal behaviors: An overview", "author": ["Morton M Silverman", "Ronald W Maris."], "venue": "Suicide and Life-Threatening Behavior 25(1):10\u201321.", "citeRegEx": "Silverman and Maris.,? 1995", "shortCiteRegEx": "Silverman and Maris.", "year": 1995}, {"title": "Depeche mood: a lexicon for emotion analysis from crowd annotated news", "author": ["Jacopo Staiano", "Marco Guerini."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "Staiano and Guerini.,? 2014", "shortCiteRegEx": "Staiano and Guerini.", "year": 2014}, {"title": "Depression and anxiety in the united states: findings from the 2006 behavioral risk factor surveillance system", "author": ["Tara W Strine", "Ali H Mokdad", "Lina S Balluz", "Olinda Gonzalez", "Raquel Crider", "Joyce T Berry", "Kurt Kroenke."], "venue": "Psychiatric Services .", "citeRegEx": "Strine et al\\.,? 2015", "shortCiteRegEx": "Strine et al\\.", "year": 2015}, {"title": "Using multivariate statistics", "author": ["Barbara G Tabachnick", "Linda S Fidell", "Steven J Osterlind"], "venue": null, "citeRegEx": "Tabachnick et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Tabachnick et al\\.", "year": 2001}, {"title": "Predicting military and veteran suicide risk: Cultural aspects", "author": ["Paul Thompson", "Craig Bryan", "Chris Poulin."], "venue": "Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality. Asso-", "citeRegEx": "Thompson et al\\.,? 2014", "shortCiteRegEx": "Thompson et al\\.", "year": 2014}, {"title": "Recognizing depression from twitter activity", "author": ["Sho Tsugawa", "Yusuke Kikuchi", "Fumio Kishino", "Kosuke Nakajima", "Yuichi Itoh", "Hiroyuki Ohsaki."], "venue": "Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems.", "citeRegEx": "Tsugawa et al\\.,? 2015", "shortCiteRegEx": "Tsugawa et al\\.", "year": 2015}, {"title": "Recognizing contextual polarity in phraselevel sentiment analysis", "author": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann."], "venue": "Proceedings of the conference on human language technology and empirical methods in natural language processing. Associ-", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 51, "context": "Mental health is an increasingly important healthrelated challenge in society; mental health conditions are associated with impaired health-related quality of life and social functioning (Saarni et al., 2007; Strine et al., 2015).", "startOffset": 187, "endOffset": 229}, {"referenceID": 55, "context": "Mental health is an increasingly important healthrelated challenge in society; mental health conditions are associated with impaired health-related quality of life and social functioning (Saarni et al., 2007; Strine et al., 2015).", "startOffset": 187, "endOffset": 229}, {"referenceID": 14, "context": "Furthermore, detecting and preventing potential self-harm acts remain a significant challenge due to reasons including lack of real-time data, privacy and confidentiality issues, and existence of bias in studies (Coppersmith et al., 2016a).", "startOffset": 212, "endOffset": 239}, {"referenceID": 45, "context": "Generally, it has been shown that social media can have broad applicability for public health research as the data from social media can reflect a variety of characteristics about individuals (Paul and Dredze, 2011; Eichstaedt et al., 2015).", "startOffset": 192, "endOffset": 240}, {"referenceID": 25, "context": "Generally, it has been shown that social media can have broad applicability for public health research as the data from social media can reflect a variety of characteristics about individuals (Paul and Dredze, 2011; Eichstaedt et al., 2015).", "startOffset": 192, "endOffset": 240}, {"referenceID": 53, "context": "There are three stages that lead to suicidal action among individuals who are in some sort of mental distress (Silverman and Maris, 1995; De Choudhury et al., 2016): 1- thinking, 2- ambivalence and 3- decision making.", "startOffset": 110, "endOffset": 164}, {"referenceID": 24, "context": "In recent years, healthcare has benefited enormously from social media data (Dredze, 2012).", "startOffset": 76, "endOffset": 90}, {"referenceID": 17, "context": "It has helped individuals with depression by providing them means to connect to people with shared experiences who can answer their questions (Dao et al., 2015; Olteanu et al., 2016).", "startOffset": 142, "endOffset": 182}, {"referenceID": 41, "context": "It has helped individuals with depression by providing them means to connect to people with shared experiences who can answer their questions (Dao et al., 2015; Olteanu et al., 2016).", "startOffset": 142, "endOffset": 182}, {"referenceID": 18, "context": "De Choudhury et al. (2013b) explored social media to identify and diagnose depression among individuals.", "startOffset": 3, "endOffset": 28}, {"referenceID": 43, "context": "Park et al. (2013) showed that depressed individuals perceived social media (Twitter) as a tool for social awareness and emotional interaction while non-depressed individuals are mostly regular information consumers.", "startOffset": 0, "endOffset": 19}, {"referenceID": 49, "context": "Portier et al. (2013) conducted sentiment analysis on the cancer survivor forum content and compared the sentiment change of the user content", "startOffset": 0, "endOffset": 22}, {"referenceID": 58, "context": "There exist many other works on analysis of social media for mental health problems such as depressive disorders (De Choudhury et al., 2013a; Tsugawa et al., 2015), addiction (Murnane and Counts, 2014), insomnia (Jamison-Powell et al.", "startOffset": 113, "endOffset": 163}, {"referenceID": 39, "context": ", 2015), addiction (Murnane and Counts, 2014), insomnia (Jamison-Powell et al.", "startOffset": 19, "endOffset": 45}, {"referenceID": 26, "context": ", 2015), addiction (Murnane and Counts, 2014), insomnia (Jamison-Powell et al., 2012), schizophrenia (Mitchell et al.", "startOffset": 56, "endOffset": 85}, {"referenceID": 38, "context": ", 2012), schizophrenia (Mitchell et al., 2015) and various other conditions (Coppersmith et al.", "startOffset": 23, "endOffset": 46}, {"referenceID": 13, "context": ", 2015) and various other conditions (Coppersmith et al., 2015).", "startOffset": 37, "endOffset": 63}, {"referenceID": 4, "context": "They used a model based on Random Forest classifier (Breiman, 2001) with bag-of-words features.", "startOffset": 52, "endOffset": 67}, {"referenceID": 50, "context": "Thompson et al. (2014) predicted the risk of suicide in military personnel and veterans using the clinical notes and online social media data (Facebook posts).", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "They used a model based on Random Forest classifier (Breiman, 2001) with bag-of-words features. Jones and Bennell (2007) developed statistical prediction rules to discriminate between genuine and simulated suicide notes.", "startOffset": 53, "endOffset": 121}, {"referenceID": 4, "context": "They used a model based on Random Forest classifier (Breiman, 2001) with bag-of-words features. Jones and Bennell (2007) developed statistical prediction rules to discriminate between genuine and simulated suicide notes. Lester (2010) analyzed the language of suicide notes to better understand suicidal behaviors in individuals.", "startOffset": 53, "endOffset": 235}, {"referenceID": 4, "context": "They used a model based on Random Forest classifier (Breiman, 2001) with bag-of-words features. Jones and Bennell (2007) developed statistical prediction rules to discriminate between genuine and simulated suicide notes. Lester (2010) analyzed the language of suicide notes to better understand suicidal behaviors in individuals. Coppersmith et al. (2016b) examined data from Twitter users who have attempted to take their life and provided an exploratory analysis of patterns in language around their attempt.", "startOffset": 53, "endOffset": 357}, {"referenceID": 10, "context": "have analyzed suicidal behaviors through detecting sentiment and emotional variations of the content (Cherry et al., 2012; Pestian et al., 2012; Desmet and Hoste, 2013).", "startOffset": 101, "endOffset": 168}, {"referenceID": 48, "context": "have analyzed suicidal behaviors through detecting sentiment and emotional variations of the content (Cherry et al., 2012; Pestian et al., 2012; Desmet and Hoste, 2013).", "startOffset": 101, "endOffset": 168}, {"referenceID": 23, "context": "have analyzed suicidal behaviors through detecting sentiment and emotional variations of the content (Cherry et al., 2012; Pestian et al., 2012; Desmet and Hoste, 2013).", "startOffset": 101, "endOffset": 168}, {"referenceID": 3, "context": "Braithwaite et al. (2016) conducted a user study on", "startOffset": 0, "endOffset": 26}, {"referenceID": 18, "context": "Finally De Choudhury et al. (2016) proposed that social media could be", "startOffset": 11, "endOffset": 35}, {"referenceID": 50, "context": "Specifically, they analyzed language in Reddit2mental health community and employed a framework based on propensity score matching (Rosenbaum and Rubin, 1984) to predict suicidal shifts in users.", "startOffset": 131, "endOffset": 158}, {"referenceID": 37, "context": "The closest work to ours is the recent shared task (Milne et al., 2016) on automatic identification of content severity in mental health forums by the 2016 Computational Linguistics and Clinical Psychology Workshop (Hollingshead and Ungar, 2016).", "startOffset": 51, "endOffset": 71}, {"referenceID": 16, "context": "Most of the systems, generally used Support Vector Machine (SVM) classifiers (Cortes and Vapnik, 1995)", "startOffset": 77, "endOffset": 102}, {"referenceID": 33, "context": "They utilized the body of the text as the main source for feature extraction and represented the post by weighted TF-IDF1unigrams and distributed representation of documents (Le and Mikolov, 2014).", "startOffset": 174, "endOffset": 196}, {"referenceID": 28, "context": "We briefly describe the top 3 approaches: Kim et al. (2016) used a Stochastic Gradient Decent classification framework.", "startOffset": 42, "endOffset": 60}, {"referenceID": 28, "context": "We briefly describe the top 3 approaches: Kim et al. (2016) used a Stochastic Gradient Decent classification framework. They utilized the body of the text as the main source for feature extraction and represented the post by weighted TF-IDF1unigrams and distributed representation of documents (Le and Mikolov, 2014). Malmasi et al. (2016) used a hierarchical classification framework.", "startOffset": 42, "endOffset": 340}, {"referenceID": 5, "context": "Finally, Brew (2016) used SVM with Radial Basis Function (RBF) kernel; they utilized TF-IDF unigram and bigram features, author type, post information and position of the post in the thread as the features for the classifier.", "startOffset": 9, "endOffset": 21}, {"referenceID": 11, "context": "posts (Cohan et al., 2016).", "startOffset": 6, "endOffset": 26}, {"referenceID": 37, "context": "We consider the following 4 levels of severity for the post content, as defined by (Milne et al., 2016):", "startOffset": 83, "endOffset": 103}, {"referenceID": 37, "context": "Following the terminology used by Milne et al. (2016), we consider the union of CRISIS, RED and AMBER categories as FLAGGED posts, because they indicate that user might be at risk and needs attention at some point.", "startOffset": 34, "endOffset": 54}, {"referenceID": 47, "context": "\u2013 LIWC: Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2015) is a tool that captures quantitative data regarding various psychological dimensions given the user\u2019s textual writings.", "startOffset": 49, "endOffset": 74}, {"referenceID": 54, "context": "To quantify the emotions associated with a specific post, we use DepecheMood (Staiano and Guerini, 2014), a lex-", "startOffset": 77, "endOffset": 104}, {"referenceID": 59, "context": "We utilize the MPQA subjectivity lexicon (Wilson et al., 2005) to differentiate between the subjective and objective posts.", "startOffset": 41, "endOffset": 62}, {"referenceID": 1, "context": "Topic modeling (Blei, 2012) is a widely used approach for discovering the latent semantic structures (\u201ctopics\u201d) in a text body.", "startOffset": 15, "endOffset": 27}, {"referenceID": 2, "context": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a generative model that describes how the documents in a dataset are created.", "startOffset": 34, "endOffset": 53}, {"referenceID": 30, "context": "Skip thought vectors (Kiros et al., 2015) are one such model that use \u201csequence to sequence\u201d models on pairs of consecutive sentences to learn the sentence encoding.", "startOffset": 21, "endOffset": 41}, {"referenceID": 30, "context": "By analysis through several tasks, Kiros et al. (2015) showed that this approach results in good sentence encodings when trained on a sufficiently large corpus.", "startOffset": 35, "endOffset": 55}, {"referenceID": 12, "context": "Motivated by previous research that identified the time of day of online activity as a useful mental health signal (Coppersmith et al., 2014; De Choudhury et al., 2013c), we also consider the broad temporal categories (day and night) as well as more fine-grained intervals (morning, afternoon, evening, and night).", "startOffset": 115, "endOffset": 169}, {"referenceID": 9, "context": "We use the XGBoost Tree Boosting (Chen and Guestrin, 2016) as the", "startOffset": 33, "endOffset": 58}, {"referenceID": 9, "context": "For more details on these steps, refer to the XGBoost reference (Chen and Guestrin, 2016).", "startOffset": 64, "endOffset": 89}, {"referenceID": 31, "context": "We use the majority voting ensembling approach which has been shown to balance out the weaknesses of individual classifiers (Lam and Suen, 1997; Opitz and Maclin, 1999).", "startOffset": 124, "endOffset": 168}, {"referenceID": 42, "context": "We use the majority voting ensembling approach which has been shown to balance out the weaknesses of individual classifiers (Lam and Suen, 1997; Opitz and Maclin, 1999).", "startOffset": 124, "endOffset": 168}, {"referenceID": 36, "context": "8 yearly visits (Millen, 2015).", "startOffset": 16, "endOffset": 30}, {"referenceID": 37, "context": "The full details of the data collection and the discussion on the ethical issues are discussed by Milne et al. (2016). While analysis of the mental health forum data provides many benefits, there are always trade-offs between the benefits and the risk to the privacy of the individuals.", "startOffset": 98, "endOffset": 118}, {"referenceID": 37, "context": "The full details of the data collection and the discussion on the ethical issues are discussed by Milne et al. (2016). While analysis of the mental health forum data provides many benefits, there are always trade-offs between the benefits and the risk to the privacy of the individuals. Milne et al. (2016) identified three groups of participants to whom the data collection and annotation process could cause harm: to the researchers who annotated the data, to the researchers who accessed the data, and to the people who authored the content.", "startOffset": 98, "endOffset": 307}, {"referenceID": 37, "context": "Following Milne et al. (2016), we use the accuracy and F-1 scores for evaluating the classification performance to be able to directly compare", "startOffset": 10, "endOffset": 30}, {"referenceID": 37, "context": "To aggregate the scores for the individual categories, Milne et al. (2016) used the macro average of F-scores for the non-GREEN (critical) categories as the official metric for the CLPsych 2016 shared task.", "startOffset": 55, "endOffset": 75}, {"referenceID": 10, "context": "GREEN URGENT vs non-URGENT F1 Acc F1 Acc F1 Acc Baseline 31 78 75 86 38 89 Cohan et al. (2016) 41 80 81 87 67 92 Brew (2016) 42 79 78 85 69 93 Malmasi et al.", "startOffset": 75, "endOffset": 95}, {"referenceID": 5, "context": "(2016) 41 80 81 87 67 92 Brew (2016) 42 79 78 85 69 93 Malmasi et al.", "startOffset": 25, "endOffset": 37}, {"referenceID": 5, "context": "(2016) 41 80 81 87 67 92 Brew (2016) 42 79 78 85 69 93 Malmasi et al. (2016) 42 83 87 91 64 93 Kim et al.", "startOffset": 25, "endOffset": 77}, {"referenceID": 5, "context": "(2016) 41 80 81 87 67 92 Brew (2016) 42 79 78 85 69 93 Malmasi et al. (2016) 42 83 87 91 64 93 Kim et al. (2016) 42 85 85 91 62 91 This work (Single model) 47.", "startOffset": 25, "endOffset": 113}, {"referenceID": 29, "context": "We observe that the non-GREEN macro average F1 score for the individual and ensemble models improves over the best system (Kim et al., 2016) by +12% and +17%, respectively.", "startOffset": 122, "endOffset": 140}, {"referenceID": 30, "context": "As shown by Kiros et al. (2015), when trained on a suffi-", "startOffset": 12, "endOffset": 32}, {"referenceID": 10, "context": "Severity categories Methods CRISIS (1) RED (27) AMBER (47) GREEN (69) Baseline 0 39 53 90 Cohan et al. (2016) 0 59 64 90 Brew (2016) 0 65 61 88 Malmasi et al.", "startOffset": 90, "endOffset": 110}, {"referenceID": 5, "context": "(2016) 0 59 64 90 Brew (2016) 0 65 61 88 Malmasi et al.", "startOffset": 18, "endOffset": 30}, {"referenceID": 5, "context": "(2016) 0 59 64 90 Brew (2016) 0 65 61 88 Malmasi et al. (2016) 0 58 69 93 Kim et al.", "startOffset": 18, "endOffset": 63}, {"referenceID": 5, "context": "(2016) 0 59 64 90 Brew (2016) 0 65 61 88 Malmasi et al. (2016) 0 58 69 93 Kim et al. (2016) 0 65 61 94 Single model 0 67.", "startOffset": 18, "endOffset": 92}, {"referenceID": 56, "context": "5 indicate high to strong linear relationship in the data (Tabachnick et al., 2001).", "startOffset": 58, "endOffset": 83}], "year": 2017, "abstractText": "Mental health forums are online communities where people express their issues and seek help from moderators and other users. In such forums, there are often posts with severe content indicating that the user is in acute distress and there is a risk of attempted self-harm. Moderators need to respond to these severe posts in a timely manner to prevent potential selfharm. However, the large volume of daily posted content makes it difficult for the moderators to locate and respond to these critical posts. We present a framework for triaging user content into four severity categories which are defined based on indications of self-harm ideation. Our models are based on a feature-rich classification framework which includes lexical, psycholinguistic, contextual and topic modeling features. Our approaches improve the state of the art in triaging the content severity in mental health forums by large margins (up to 17% improvement over the F-1 scores). Using the proposed model, we analyze the mental state of users and we show that overall, long-term users of the forum demonstrate a decreased severity of risk over time. Our analysis on the interaction of the moderators with the users further indicates that without an automatic way to identify critical content, it is indeed challenging for the moderators to provide timely response to the users in", "creator": "LaTeX with hyperref package"}}}