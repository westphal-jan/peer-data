{"id": "1702.07281", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "A Probabilistic Framework for Location Inference from Social Media", "abstract": "investigators study the connection to facebook we can infer users'geographical locations from social media. location inference from social media can cross many applications, such as disaster management, targeted advertising, and news content tailoring. past recent years, a number of algorithms have been proposed for identifying the locations on social media platforms such as networks and facebook from message contents, friend networks, natural interactions between organisations. in this paper, we propose a novel probabilistic theory based on web graphs for location inference that offers several unique advantages for this task. first, the model generalizes seo methods by incorporating content, network, and deep features learned from social context. customer model is also flexible enough that support both supervised learning and meta - supervised learning. second, we explore several learning algorithms for the optimization model, and present own two - chain metropolis - response ( mh + ) algorithm, which improves the inference accuracy. third, we validate the proposed model on three different genres of data - twitter, weibo, and facebook - and predict that the query model can substantially improve the inference response ( + 3. 3 - 18. 5 % by f1 - score ) over that comparing several state - old - than - art methods.", "histories": [["v1", "Thu, 23 Feb 2017 16:34:07 GMT  (722kb,D)", "https://arxiv.org/abs/1702.07281v1", null], ["v2", "Wed, 1 Mar 2017 16:23:25 GMT  (725kb,D)", "http://arxiv.org/abs/1702.07281v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.SI", "authors": ["yujie qian", "jie tang", "zhilin yang", "binxuan huang", "wei wei", "kathleen m carley"], "accepted": false, "id": "1702.07281"}, "pdf": {"name": "1702.07281.pdf", "metadata": {"source": "META", "title": "A Probabilistic Framework for Location Inference from Social Media", "authors": ["Yujie Qian", "Jie Tang", "Zhilin Yang", "Binxuan Huang", "Wei Wei", "Kathleen M. Carley"], "emails": ["qyj13@mails.tsinghua.edu.cn", "jietang@tsinghua.edu.cn", "zhiliny@cs.cmu.edu", "binxuanh@andrew.cmu.edu", "weiwei@cs.cmu.edu", "kathleen.carley@cs.cmu.edu"], "sections": [{"heading": null, "text": "KEYWORDS Location Inference, Factor Graph Model, Social Media"}, {"heading": "1 INTRODUCTION", "text": "In many social media platforms, such as Twi er, Facebook, and Weibo, location is a very important demographic a ribute to support friend and message recommendation. For example, one observation is that the number of friends tends to decrease as distance increases [3, 27]. Another study also shows that, even on the \u201cglobal\u201d Internet, the user\u2019s ego-network tends to stay local: the average number of friends between users from the same time zone is about 50 times higher than the number between users with a distance of three time zones [15]. However, the geographical location information is usually unavailable. Statistics show that only 26% of users on Twi er input their locations [7]. Twi er, Facebook, and Weibo also have a function to allow adding per-tweet geo-tags; however, it turns out that only 0.42% of all tweets contain a geo-tag.\nIn this work, our goal is to design a general framework for inferring users\u2019 geographical locations from social media data. Di erent from previous works [3, 7, 24] that deal with this problem in a\nspeci c scenario (e.g., determining USA cities only) or with speci c data (e.g., Twi er), we aim to design a method that is general enough to be applied to di erent scenarios, and is also exible enough to incorporate a variety of information \u2014 contents, user pro les, network structure, and deep representation features.\nPrevious work on location inference. e location inference problem has been widely studied by researchers from di erent communities. Roughly speaking, existing literature can be divided into two categories. e rst focuses on studying content. For example, Eisenstein et al. [10] proposed the Geographic Topic Model to predict a user\u2019s geo-location from text. Cheng et al. [7] used a probabilistic framework and illustrated how to nd local words and overcome tweet sparsity. Ryoo et al. [30] applied a similar idea to a Korean Twi er dataset. Ikawa et al. [16] used a rulebased approach to predict a user\u2019s current location based on former tweets. Wing et al. [31] used language models and information retrieval approaches for location prediction. Chen et al. [5] used topic models to determine users\u2019 interests and mapped interests to location. e other line of research infers user locations using network structure information. For example, Backstrom et al. [3] assumed that an unknown user would be co-located with one of its friends and sought the ones with the maximum probabilities. McGee et al. [25] integrated social tie strengths between users to improve location estimation. Jurgens [17] and Davis Jr et al. [34] used the idea of label propagation to infer user locations according to their network distances from users with known locations. However, these methods do not consider content. Li et al. [24] proposed a uni ed discriminative in uence model and utilized both the content and the social network to predict user locations. However, they focused on US users, and only considered location names in tweets. Another study using user pro les can be found in [34]. Table 1 summarizes the most closely related works on location inference. Survey and analysis of location inference techniques on Twi er can be also found in [2, 18]. Unlike the existing literature, our goal is to propose a general method that is able to deal with text in any languages by incorporating content, network, and other factors.\nProblem formulation. e problem of location inference can be generally formalized as a prediction problem. For each user, we aim to predict which country/state/city the user comes from. Our input is a partially labeled network G = (V ,E,Y L ,X) derived from social media data, whereV denotes the set of |V | = N users withV L \u2282 V indicating the subset of labeled users (with locations) and VU \u2282 V the subset of unlabeled users (without locations), E \u2286 V \u00d7V is the\nar X\niv :1\n70 2.\n07 28\n1v 2\n[ cs\n.A I]\n1 M\nar 2\n01 7\nset of relationships between users, Y L corresponds to the set of locations of users inV L , andX is the feature matrix associated with users in V with each row corresponding to a user, each column corresponding to a feature, and xi j the value of the jth feature of user vi . Given the input, we de ne the problem of inferring user locations as follows:\nProblem 1. Geo-location Inference. Given a partially labeled networkG = (V ,E,Y L ,X), the objective is to learn a predictive function f in order to predict the locations of unlabeled users VU\nf : G = (V ,E,Y L ,X) \u2192 YU (1) where YU is the set of predicted locations for unlabeled users VU .\nWithout loss of generality, we assume that all predicted locations are among the locations occurring in the labeled set Y L . It worth noting that our formulation of user location inference is slightly di erent from the previous work we mentioned above. e problem is de ned as a semi-supervised learning problem for networked data \u2014 we have a network with some labeled nodes and many unlabeled nodes. Our goal is to leverage both the local a ributes X and the network structure E to learn the predictive function f .\nOur solution and contributions. In this paper, we propose a probabilistic framework based on factor graph model to address the location inference problem. e model seamlessly combines content information and network structure into a probabilistic graphical model. In the graphical model, labeled locations are propagated to unlabeled users. In this way, the model supports both supervised learning and semi-supervised learning. e model also supports incorporating deep representation features learned from social context. To improve the learning e ectiveness and e ciency, we explore several learning algorithms. It shows that Loopy Belief Propagation (LBP) achieves good performance, but is not scalable to large networks. So max regression (SR) and Metropolis-Hastings (MH) algorithms have successfully addressed the e ciency problem. Moreover, we present a Two-chain Metropolis-Hastings (MH+) algorithm, which further improves the inference accuracy.\nWe conduct experiments on several datasets of di erent genres: Twi er, Weibo, and Facebook. e results show that the proposed\nprobabilistic model signi cantly improves the location inference accuracy (+3.3-18.5% by F1-score), compared to several state-of-theart methods. In terms of time cost of model training, the proposed MH+ algorithm achieves more than 100\u00d7 speedup over LBP.\nOrganization. Section 2 presents the proposed methodology. Section 3 presents experimental results that validate the e ectiveness of our methodology. Section 4 concludes this work."}, {"heading": "2 PROBABILISTIC FRAMEWORK", "text": "In this section, we propose a general probabilistic framework based on factor graphs for geographical location inference from social media. We rst give themodel de nition, and then introduce several learning algorithms."}, {"heading": "2.1 Proposed Model", "text": "For inferring user locations, we have three basic intuitions. First, the user\u2019s pro le may contain implicit information about the user\u2019s location, such as time zone and the language selected by the user. Second, the tweets posted by a user may reveal the user\u2019s location. For example, Table 2 lists the most popular \u201clocal\u201d words in ve Englishspeaking countries, including locations (Melbourne, Dublin), organizations (HealthSouth, UMass), sports (hockey, rugby), local idiom (Ctfu, wyd, lad), etc. ird, network structure can be very helpful for geo-location inference. In Twi er, for example, users can follow each other, retweet each other\u2019s tweets, and mention other users in their tweets. e principle of homophily [22] \u2014 \u201cbirds of a feather ock together\u201d [26] \u2014 suggests that \u201cconnected\u201d users may come from the same place. is tendency was observed between Twi er reciprocal friends in [15, 21]. Moreover, we found that the homophily phenomenon also exists in the mention network. Table 3 shows the statistics for USA, UK, and China Twi er users. We can see when user A mentions (@) user B, the probability that A and B come from the same country is signi cantly higher than that they come from di erent countries. Interestingly, when a USA user A mentions another user B in Twi er, the chance that user B is also from the USA is 95% , while if user A comes from UK, the probability sharply drops to 85%, and further drops to 80% for users from China. We also did the statistics at the state-level and found that there is an 82.13% chance that users A and B come from the same state if one mentions the other in her/his tweets.\nBased on the above intuitions, we propose a Semi-Supervised Factor Graph Model (SSFGM) for location inference. Figure 1 shows the graphical representation of the SSFGM. e graphical model SSFGM consists of two kinds of variables: observations {x} and latent variables {y}. In our problem, each user vi corresponds to an observation xi and is also associated with a latent variable yi . e observation xi represents the user\u2019s personal a ributes and the latent variable yi represents the user\u2019s location. We denote Y = {y1,y2, . . . ,yN }, and Y can be divided into labeled set Y L and unlabeled set YU . e latent variables {yi }i=1, \u00b7 \u00b7 \u00b7 ,N are correlated with each other, representing relationships between users. In SSFGM, such correlations can be de ned as factor functions.\nNow we explain the SSFGM in details. Given a partially labeled network as input, we de ne two factor functions:\n\u2022 Attribute factor: f (xi ,yi ) represents the relationship between observation (features) xi and the latent variable yi ; \u2022 Correlation factor: h(yi ,yj ) denotes the correlation between users vi and vj .\ne factor functions can be instantiated in di erent ways. In this paper, we de ne the a ribute factor as an exponential-linear function\nf (xi ,yi ) = exp ( \u03b1T\u03a6(xi ,yi ) ) \u03a6k (xi ,yi ) = 1(yi=k )xi , k \u2208 {1, . . . ,C}\n(2)\nwhere\u03b1 = (\u03b11, \u00b7 \u00b7 \u00b7 ,\u03b1C )T is theweighting vector,\u03a6 = (\u03a61, \u00b7 \u00b7 \u00b7 ,\u03a6C )T is the vector of feature functions, C is the number of location categories, and 1(yi=k ) is an indicator function.\ne correlation factor is de ned as h(yi ,yj ) = exp ( \u03b3T \u2126(yi ,yj ) ) (3)\nwhere \u03b3 is also a weighting vector, and \u2126 is a vector of indicator functions \u2126kl (yi ,yj ) = 1(yi=k,yj=l ). Correlation can be directed (e.g., mention), or undirected (e.g., reciprocal follow, Facebook friend). For undirected correlation, we need to guarantee \u03b3kl = \u03b3lk in the model.\nModel enhancement with deep factors. Recently, deep neural networks have achieved excellent performance in various elds including image classi cation and phrase representations [8, 19]. e proposed SSFGM is exible enough to incorporate deep feature representations. To do this, we de ne a deep factor \u0434(xi ,yi ) in the SSFGM model to represent the deep (non-linear) relationship between xi and yi . e \u0434(xi ,yi ) represents a function between the location label yi and the latent representation learned by a deep neural network (Cf. the right side of Figure 1).\nSpeci cally, the deep factor is de ned by a two-layer neural network. e input vector x is fed into a neural network with two fully-connected layers, denoted as h1(x) and h2(x):\nh1(x) = ReLU(W1x + b1) h2(x) = ReLU(W2h1(x) + b2)\n(4)\nwhere W1,W2, b1, b2 are parameters of the neural network, and we use ReLU(x) = max(0,x) [12] as the activation function. Similar to the de nition of a ribute factor, we de ne\n\u0434(xi ,yi ) = exp ( \u03b2T \u03a8(xi ,yi ) ) \u03a8k (xi ,yi ) = 1(yi=k )h 2(xi ), k \u2208 {1, . . . ,C} (5)\nwhere \u03b2 is the weighting vector for the output of the neural network.\nus, we have the following joint distribution over Y :\np(Y |G) = 1 Z \u220f vi \u2208V f (xi ,yi )\u0434(xi ,yi ) \u220f (vi ,vj )\u2208E h(yi ,yj ) (6)\nwhere Z is the normalization factor to ensure \u2211 Y p(Y |G) = 1.\nFeature de nition. For the a ribute factor, we de ne two categories of features: pro le and content.\nPro le features include information from the user pro les, such as time zone, user-selected language, gender, age, number of followers and followees, etc.\nContent features capture the characteristics of tweet content. e easiest way to de ne content features is using a bag-of-words representation. But it su ers from high dimensionality, especially in Twi er, which has hundreds of languages. In our work, we employ Mutual Information (MI) [32]. We compute MI using the corpus in the training data, and de ne content features as the aggregated MI to each location category. We use two aggregation approaches, max and avera\u0434e , to aggregate all the words in a user\u2019s tweets.\nModel learning. Learning a Semi-Supervised Factor GraphModel involves two parts: learning parametersW, b for the neural network of the deep factor, and learning parameters \u03b1 , \u03b2 ,\u03b3 for the graphical model. In this paper, we use a separate two-step learning approach. We leave the joint learning of the neural network and the graphical model as our future work.\nWe rst introduce our approach for learning the neural network. Cheng et al. [6] suggested that joint learning of wide and deep models can gain the bene ts of memorization and generalization. In our work, we combine the a ribute factor and deep factor by adding a so max layer on the top of twomodels to generate the outputy, as illustrated in Figure 1 on the right. We train the model with labeled data only, and use squared loss as the loss function. We adopt stochastic gradient descent (SGD) to perform back propagation on the neural network and update the parameters. A er the neural network is trained, we can compute deep factors for all users, and then move to learn the factor graph model.\ne next step is to learn the graphical model \u2013 i.e., determine a parameter con guration \u03b8 = (\u03b1T , \u03b2T ,\u03b3T )T such that the loglikelihood of the labeled data can be maximized. For simplicity, we denote s(yi ) = (\u03a6(xi ,yi )T ,\u03a8(xi ,yi )T , 12 \u2211 yj \u2126(yi ,yj )\nT )T , and S(Y ) = \u2211i s(yi ). e joint probability de ned in Eq. 6 can be rewri en as\np(Y |G) = 1 Z \u220f i exp ( \u03b8T s(yi ) ) = 1 Z exp ( \u03b8T \u2211 i s(yi ) ) =\n1 Z exp\n( \u03b8T S ) (7) e input of SSFGM is partially labeled, which makes the model learning very challenging. e general idea here is to maximize the marginal likelihood of labeled data. We denote Y |Y L as the label con guration that satis es all the known labels. en we can\nde ne the following MLE objective function O(\u03b8 ): O(\u03b8 ) = logp(Y L |G) = log \u2211 Y |Y L 1 Z exp(\u03b8T S)\n= log \u2211 Y |Y L exp(\u03b8T S) \u2212 log \u2211 Y exp(\u03b8T S) (8)\nNow the learning problem is cast as nding the best parameter con guration that maximizes the objective function, i.e.,\n\u03b8\u0302 = argmax \u03b8\nlogp(Y L |G) (9)\nWe can use gradient descent to solve the above optimization problem. First, we derive the gradient of each parameter \u03b8 :\n\u2202O(\u03b8 ) \u2202\u03b8 = \u2202 \u2202\u03b8 \u00a9\u00ablog \u2211 Y |Y L exp(\u03b8T S) \u2212 log \u2211 Y exp(\u03b8T S)\u00aa\u00ae\u00ac =\n\u2211 Y |Y L exp\u03b8T S \u00b7 S\u2211 Y |Y L exp\u03b8T S \u2212 \u2211 Y exp\u03b8T S \u00b7 S\u2211 Y exp\u03b8T S\n= Ep\u03b8 (Y |Y L,G)S \u2212 Ep\u03b8 (Y |G)S = \u2211 i Ep\u03b8 (Y |Y L,G)s(yi ) \u2212 \u2211 i Ep\u03b8 (Y |G)s(yi )\n(10)\ne gradient equals to the di erence of two expectations of S, which are under two di erent distributions. e rst onep\u03b8 (Y |Y L ,G) is the model distribution conditioned on labeled data, and the second one p\u03b8 (Y |G) is the unconditional model distribution. Both of them are intractable and cannot be computed directly."}, {"heading": "2.2 Basic Learning Algorithms", "text": "We start with two basic algorithms to tackle the learning problem in SSFGM.\nLoppy Belief Propagation (LBP). A traditional method to estimate the gradient in graphical models is Loopy Belief Propagation (LBP) [28], which provides a way to approximately calculate marginal probabilities on factor graphs. LBP performs message passing between variables and factor nodes according to the sum-product rule [20]. In each step of gradient descent, we perform LBP twice to estimate p\u03b8 (Y |G) and p\u03b8 (Y |Y L ,G) respectively, and then calculate the gradient according to Eq. 10.\nHowever, this LBP-based learning algorithm is computationally expensive. Its time complexity is O(I1I2(|V |C + |E |C2)), where I1 is the number of iterations for gradient descent, I2 is the number of iterations for belief propagation, andC is the number of the location categories (usually 30-200). is algorithm is time-consuming, and not applicable when we have millions of users and edges.\nMetropolis-Hastings (MH). In addition to LBP, Markov Chain Monte Carlo (MCMC) methods have proved successful for estimating parameters in complex graphical models, such as SampleRank [29]. In our work, we employ Metropolis-Hastings sampling [13] to obtain a sequence of random samples from the model distribution and update the parameters. e learning algorithm is described in Algorithm 1.\nWe now explain Algorithm 1 in detail. At rst, we initialize parameters \u03b8 randomly. e Metropolis-Hastings algorithm simulates\nAlgorithm 1:Metropolis-Hastings (MH) Input :G = (V ,E,Y L ,X), learning rate \u03b7; Output : learned parameters \u03b8 ;\n1 Initialize \u03b8 and Y randomly; 2 repeat 3 Select yi uniformly at random; 4 Generate y\u2217i \u223c q(\u00b7|Y ); // Y \u2217 = {y1, . . . , yi\u22121, y\u2217i , yi+1, . . . } 5 Generate u \u223c U (0, 1); 6 \u03c4 \u2190 min { 1, p\u03b8 (Y\n\u2217 |G)q(Y |Y \u2217) p\u03b8 (Y |G)q(Y \u2217 |Y )\n} ;\n7 if u < \u03c4 then // accept 8 if ACC(Y \u2217) > ACC(Y ) and p\u03b8 (Y \u2217) < p\u03b8 (Y ) then 9 \u03b8 \u2190 \u03b8 + \u03b7 \u00b7 \u2207 (logp\u03b8 (Y \u2217) \u2212 logp\u03b8 (Y )) 10 if ACC(Y \u2217) < ACC(Y ) and p\u03b8 (Y \u2217) > p\u03b8 (Y ) then 11 \u03b8 \u2190 \u03b8 \u2212 \u03b7 \u00b7 \u2207 (logp\u03b8 (Y \u2217) \u2212 logp\u03b8 (Y )) 12 Y \u2190 Y \u2217; 13 until convergence;\nrandom samples from the model distribution p\u03b8 (Y |G). In each iteration, it generates a candidate con guration Y \u2217 from a proposal distribution q(\u00b7|Y ), and accepts it with an acceptance ratio \u03c4 (line 6). If the candidate Y \u2217 is accepted, the algorithm continues to update the parameters \u03b8 (line 8-11). We compare the accuracy (ACC) and likelihood of the two con gurationsY andY \u2217. Ideally, if one\u2019s accuracy is higher than the other, its likelihood should also be larger; otherwise the model should be adjusted. us there are two cases to update the parameters: 1) if the accuracy ofY \u2217 is higher but its likelihood is smaller, update with \u03b8new \u2190 \u03b8old + \u03b7 \u00b7 \u2207 (logp\u03b8 (Y \u2217) \u2212 logp\u03b8 (Y )); 2) if the accuracy of Y \u2217 is lower but its likelihood is larger, update with \u03b8new \u2190 \u03b8old \u2212 \u03b7 \u00b7 \u2207 (logp\u03b8 (Y \u2217) \u2212 logp\u03b8 (Y )). Note that we only have to calculate unnormalized likelihoods of p\u03b8 (Y \u2217) and p\u03b8 (Y ). us, the gradient can be easily calculated,\n\u2207 ( logp\u03b8 (Y \u2217) \u2212 logp\u03b8 (Y ) ) = \u2207 ( \u03b8T S(Y \u2217) \u2212 \u03b8T S(Y ) ) = S(Y \u2217)\u2212S(Y )\n(11) is algorithm is more e cient than the previous LBP algorithm, but also has some shortcomings. e algorithm updates the model when larger likelihood leads to worse accuracy. It actually optimizes an alternative max-margin objective instead of the original maximum likelihood objective. In addition, it relies on an external metric (accuracy in our work), which could be arbitrary and engineering-oriented, since multiple metrics are o en available for evaluation."}, {"heading": "2.3 Enhanced Learning Algorithms", "text": "We now present two enhanced learning algorithms \u2014 So max Regression (SR) and Two-chain Metropolis Hastings (MH+) for SSFGM. SR is very e cient, much faster than the basic learning algorithms and MH+ achieves a be er prediction performance.\nSo max Regression (SR). We propose a new learning algorithm based on so max regression (also called multinomial logistic regression). Estimation of joint probability Eq. 7 is an intractable problem because of the normalization factor Z , which sums over all\nAlgorithm 2: Two-chain Metropolis-Hastings (MH+) Input :G = (V ,E,Y L ,X), learning rate \u03b7, and batch size; Output : learned parameters \u03b8 ;\n1 Initialize \u03b8 with SR-based learning; 2 Initialize Y1 with Y L xed, and YU randomly; 3 Initialize Y2 randomly; 4 repeat 5 \u03b8\u2217 \u2190 \u03b8 ; 6 for t = 1, 2, . . . ,batch size do 7 Select yi uniformly at random; 8 Generate y\u2217i \u223c q(\u00b7|Y ); 9 Accept y\u2217i in Y \u2217 1 such that Y1 \u223c p\u03b8 (Y |G,Y\nL); 10 Accept y\u2217i in Y \u2217 2 such that Y2 \u223c p\u03b8 (Y |G); 11 if y\u2217i is accepted in Y1 or Y2 or both then 12 \u03b8\u2217 \u2190 \u03b8\u2217 + \u03b7 \u00b7 (s(y\u2217i |Y1) \u2212 s(y \u2217 i |Y2))\n13 \u03b8 \u2190 \u03b8\u2217; 14 until early stopping criteria satis ed;\nthe possible con gurations Y . Our idea here is to only consider yi and x all the other variables, its marginal can be easily calculated as a so max function,\np(yi |G,Y \u2212 {yi }) = exp\n( \u03b8T s\u0302(yi ) ) \u2211 y\u2032i exp ( \u03b8T s\u0302(y\u2032i )\n) (12) where s\u0302(yi ) = (\u03a6(xi ,yi )T ,\u03a8(xi ,yi )T , \u2211 yj \u2126(yi ,yj )\nT )T . Eq. 12 has the same form as so max regression. e di erence is that the neighborhood information is also contained in feature function s\u0302(yi ). So max regression can be trained using gradient descent, and the gradient is much easier to compute than factor graph models. We then design an approximate learning algorithm based on so max regression:\nStep 1. Conduct so max regression to learn \u03b1 and \u03b2 , with labeled data {(xi ,yi )|yi \u2208 Y L} only;1 Step 2. Predict the labels YU for unlabeled users; Step 3. Conduct so max regression to learn\u03b8 according to Eq. 12; Step 4. Predict the labels YU for unlabeled users. If the predic-\ntion accuracy on validation set increases, go to Step 3; otherwise, stop.\nis algorithm is an approximation method for learning SSFGM, but it is both e ective and very e cient. We can use SR to initialize the model parameters for the other learning algorithms.\nTwo-chain Metropolis-Hastings (MH+). e classical MH algorithm does not directly maximize the log-likelihood [29] and relies on additional evaluation metrics. We introduce a new idea to directly optimize the objective function in Eq. 8 without using additional heuristic metrics. We refer to this algorithm as Two-chain Metropolis-Hastings (MH+).\nAlgorithm 2 illustrates the new algorithm. In Eq. 10, the gradient consists of two expectation terms. To obtain an unbiased estimation of the gradient, we sample Y1 from pdata = p\u03b8 (Y |G,Y L) and sample\n1Here we use p(yi |xi ) = sof tmax ( \u03b1Tyi \u03a6(xi , yi ) + \u03b2 T yi \u03a8(xi , yi ) ) .\nY2 from pmodel = p\u03b8 (Y |G). Bearing a similar merit to stochastic gradient descent [4], the gradient of our model can be computed as s(Y1) \u2212 s(Y2).\nIn order to sample from the two distributions pdata and pmodel, we maintain two Markov chains and employ the MH sampling method. To apply the MH sampling method, a proposal distribution q(\u00b7|Y ) is de ned as follows. Given the current label con guration Y , we randomly sample an index i according to a uniform distribution, and set yi as a random value y\u2217i according to another uniform distribution. In each iteration, we select the same index i and propose the same y\u2217i for both chains Y1 and Y2 to ensure stable gradient estimation. However, the new label con gurations in the two chains are accepted with di erent acceptance ratios (\u03c41 and \u03c42) such that they follow the corresponding distributions, i.e.,\nfor chain Y1, \u03c41 = min { 1, p\u03b8 (Y \u22171 |G,Y\nL) p\u03b8 (Y1 |G,Y L) } for chain Y2, \u03c42 = min { 1, p\u03b8 (Y \u22172 |G) p\u03b8 (Y2 |G)\n} (13) To further speed up the learning algorithm, we approximate the gradient by only considering a local update\u2014i.e., s(y\u2217i |Y1)\u2212s(y \u2217 i |Y2)\u2014 rather than a global update S(Y1) \u2212 S(Y2). If y\u2217i is rejected in both chains, we do not update the parameters. Our choice of proposal distribution has several advantages: 1) empirically the algorithm\u2019s performance is be er than changing multiple variables each time or changing di erent variables in two chains, and 2) it simpli es the calculation of acceptance ratio and gradient, since we only need to consider local variables and factors. We also tried other strategies such as sampling yi with probabilities proportional to the marginal probabilities, but do not obtain further improvements.\nWe use several techniques to improve the stability and e ciency of the learning algorithm. e rst technique is mini-batch gradient descent. We compute the gradient for a mini-batch of sampling steps, and then update the parameters with the sum of these gradients. batch size is a hyperparameter. e second technique is early stopping. We divide the labeled data into a training set and a validation set. During the learning process, we only use the labels in the training set. We evaluate the model a er each \u03b4 iterations, and if the prediction accuracy on the validation set does not increase for \u03b5 evaluations, we stop the algorithm and return the parameter con guration \u03b8 that achieves the best accuracy on the validation set. \u03b4 and \u03b5 are also hyperparameters. ese techniques can also be used in the previous MH algorithm.\nIn contrast to the basic MH algorithm based on only the model distribution pmodel, our algorithm utilizes both pmodel and pdata so that we can directly optimize the log likelihood. LBP also maximizes the likelihood, but it requires traversing the entire network in each iteration to compute the gradient. Instead, we leverage MH sampling to estimate the gradient e ciently. Similar ideas based on Markov chains have been adopted for training restricted Boltzmann machines [14], but those algorithms do not apply to a partiallylabeled factor graph model as in our work.\nParallel learning. To scale up the proposed model to handle large networks, we have developed parallel learning algorithms\nfor SSFGM. For the SR algorithm, so max regression can be easily parallelized. e gradient is a summation over all the training examples, and the computation is independent. For MH and MH+, we parallelize the learning algorithm by dividing the batch. Each thread generates random samples and computes gradients independently, and with a smaller batch size. en master thread gathers the gradients from di erent threads and updates the parameters."}, {"heading": "2.4 Prediction Algorithm", "text": "We can apply the learned SSFGM to predict unknown locations \u2014 i.e., to nd the most likely con guration of Y\u0302 for unlabeled users based on the learned parameters \u03b8 ,\nY\u0302 = arg max Y |Y L p\u03b8 (Y |G,Y L) (14)\nFor prediction, we consider two algorithms. e rst one is based on LBP. It uses the max-sum algorithm instead of sum-product in LBP to nd the Y\u0302 that maximizes the likelihood. e second algorithm is based on Metropolis-Hastings sampling. We keep sampling with the estimated \u03b8 , and nally return the con guration Y\u0302 with the maximum likelihood. 2"}, {"heading": "3 EXPERIMENTS", "text": "We evaluate the proposed model on three di erent data: Twi er, Weibo, and Facebook. All datasets and codes used in this paper are available online.3"}, {"heading": "3.1 Experimental Setup", "text": "Datasets. We conduct experiments on four datasets. Table 4 shows the basic statistics of the datasets.\n\u2022 Twitter (World): We collected geo-tagged tweets posted in 2011 through Twi er API. ere are 243,000,000 tweets posted by 3,960,000 users in our collected data. A er data preprocessing, we obtain a dataset consisting of 1.5 million users from 159 countries around the world. e task then is to infer the user\u2019s country. Due to limitation of the Twi er API, we did not crawl the following relationships, thus we use mention (\u201c@\u201d) to derive the social relationships. \u2022 Twitter (USA): is dataset is constructed from the same raw data as that of Twi er (World). e di erence is that we only keep USA users here. e task on this dataset is to infer the user\u2019s state.\n2In our work, we use LBP for prediction when the model is trained with the LBP algorithm, and use MH for prediction when the model is trained with MH/MH+. 3h p://dev.aminer.org/thomas0809/MH\n\u2022 Weibo [33]: Weibo is the most popular Chinese microblog. e dataset consists of about 1,700,000 users, with up to 1,000 most recent microblogs posted by each user. e task is to infer the user\u2019s province. We use reciprocal following relationships as edges in this dataset. \u2022 Facebook [23]: e dataset is an ego-facebook dataset from SNAP, and does not contain content information. In our experiments, we only consider users who indicate their hometowns in their pro les. Locations are anonymized in this dataset. We use Facebook friendships as edges.\nData preprocessing. We now introduce how we preprocess the data. First, we lter out users who have fewer than 10 tweets in the dataset. en, we tokenize the tweet content into words. In Twi er, we split the sentences by punctuations and spaces. For languages that do not use spaces to separate words (such as Chinese and Japanese), we further split each character. In the Weibo data provided by [33], the content has been tokenized into Chinese words. For each user, we combine all her/his tweets and derive content features as de ned in Section 2.1. e ground truth location is de ned by di erent ways in each dataset. In the two Twi er datasets, we convert the GPS-tag on tweets to its country/state, and only keep the users who posted all tweets in the same country/state. (In our data, for > 90% users, all tweets are posted in the same country in a year, and for > 80% USA users, all tweets are in the same state.) In Weibo and Facebook, the locations are extracted from user pro les, which have been divided into provinces/cities. In all datasets, we remove the locations with fewer than 10 users.\nComparison methods. We compare the following methods for location inference:\n\u2022 Logistic Regression (LR): We consider this method as the baseline method, which applies logistic regression as the classi cation model to predict user location. We use the same feature set as our proposed model. \u2022 SupportVectorMachine (SVM) [34]: Zubiaga et al. have applied SVM to classify tweet location. We choose a linear function as the kernel of SVM. \u2022 Graph-based Label Propagation (GLP) [9]: It infers locations of unlabeled users by counting the most popular location among one\u2019s friends with a simple majority voting.\nFor baseline methods, we use the implementation of Liblinear [11]. For the proposed method, all experiment codes are implemented in C++ and Python. e deep factor in our model has been implemented using TensorFlow [1]. We empirically set up the hyperparameters as the following: for LR and SVM, we set the regularization penalty c = 1; for our method, we set the learning rate \u03b7 = 0.1 in MH and \u03b7 = 1 in MH+, batch size = 5000, and early stopping threshold \u03b4 = 1000, \u03b5 = 20; the deep factor is set as a two-layer neural network, where the rst layer has 200 hidden units and the second layer has 100 hidden units.\nEvaluation metrics. In our experiments, we divide each dataset into three parts: 50% for training, 10% for validation, and 40% for testing. For the methods which do not require validation, the validation data is also used for training.\nWe evaluate location inference performance using the measures for multi-class classi cation. Speci cally, we use four measures: Micro-Accuracy (percentage of the users whose locations are predicted correctly), and Macro-Precision, Recall, and F1-score (the average precision, recall and F1-score of each class)."}, {"heading": "3.2 Experiment Results", "text": "Inference performance. We compare the performance of all the methods on the datasets. Table 5 lists the performance of comparison methods for geo-location inference. SSFGM consistently outperforms all the comparison methods in terms of accuracy and F1-score on all datasets. In Twi er (World), linear models (LR and SVM) can achieve an accuracy of 94.4% for classifying users into 159 countries. Our SSFGM improves the accuracy to 95.8% by incorporating social network. In Twi er (USA), SSFGM achieves a signi cant improvement in terms of both accuracy and F1-score. is is because for inferring user country, the content information might be more important, as users from di erent countries may use di erent languages; while for inferring location at the state-level, the e ect of network information increases. e proposed SSFGM can leverage the network structure information to help improve the inference accuracy. is can be con rmed by taking a closer look at the Weibo data, where we have a complete following network. From the Facebook data, we have another observation, it seems that the SSFGM (LBP) still achieves the best performance. e only problem to SSFGM (LBP) is its low e ciency. We cannot obtain results on the other three datasets by SSFGM (LBP). We will analyze the computational e ciency of di erent learning algorithms later. Finally, we can observe that in general the deep factor indeed helps to improve inference accuracy. Deep factors can incorporate non-linear, cross-dimensional representations of input features, and thus help the model to improve its performance.\nComparison of di erent learning algorithms. We compare the performance of four learning algorithms for SSFGM. e traditional LBP-based learning still results in a very good inference accuracy, but its computational cost is high. Comparing so max regression (SR) and Metropolis-Hastings (MH), they result in similar performance \u2014 SR outperforms (0.3-1.6% by F1-score) MH on Twi er (World) and Twi er (USA), but underperforms (3.3-3.6% by F1-score) on Weibo and Facebook. e proposed MH+ algorithm further improves the performance and achieves consistently be er performances (0.7-5.3% by F1-score) than SR and MH.\nCompared with the traditional LBP-based learning algorithm, the studied SR and MH algorithms are much more e cient. Table 6 shows the training time of SSFGM used by di erent learning algorithms, where each algorithm is running with a single computer core. LBP uses about 16 minutes to train SSFGM on our smallest dataset, and needs more than one hundred days on the other large datasets. SR seems to be the most e cient among the four learning algorithms. MH and MH+ take SR as parameter initialization and further improve the accuracy. Generally speaking, they are very e cient on large datasets and over 100\u00d7 faster than LBP. eir running time varies a li le in di erent datasets because of the di erence in convergence speed.\nScalability performance. We have implemented parallel learning algorithms for the MH and MH+ algorithms utilizing OpenMP architecture. We now evaluate the scalability of the two learning algorithms on the Twi er (USA) dataset. Figure 2 shows the scalability performance with di erent numbers of cores (2-8). e speedup curve of MH is close to the perfect line at the beginning.\nough the speedup inevitably decreases due to the increase of the communication cost between di erent computer nodes, the parallel algorithms can still achieve \u223c 3.3 \u2212 4.5\u00d7 speedup with 8 cores.\nFactor contribution analysis. We now evaluate how di erent factors (content, pro le, and network) contribute to location inference in the proposed model. We use the two Twi er datasets in this study. Speci cally, we remove each factor from our SSFGM and then evaluate the model performance decrease. A large decrease means more importance of the factor to the model. Figure 3 shows the results on the Twi er datasets. We see that di erent factors contribute di erently on the two datasets. e content-based features seem to be the most useful in the proposed model for inferring location on the Twi er datasets. On the other hand, all features are helpful. is analysis con rms the necessity of the exibility to incorporate various features in the proposed model.\nHyperparameter sensitivity. Finally, we analyze how the hyperparameters in the proposed model a ect the inference performance.\nLearning rate (\u03b7): Figure 4(a) shows the accuracy performance of SSFGM (MH and MH+) on Twi er (USA) dataset when varying the learning rate \u03b7 from 10\u22123 to 101. Generally speaking, the performance is not sensitive to the learning rate over a wide range. However, a too large or too small learning rate will hurt the performance.\nBatch size: Figure 4(b) shows the accuracy performance comparison when varying the batch size. e results indicate that the performance is not sensitive to the batch size, while larger batch size usually leads to slightly be er performance. When the batch size becomes larger, the gradient estimation tends to be more accurate. But, at the same time, a larger batch size also means more training time. We set batch size to 5, 000 in our experiments."}, {"heading": "4 CONCLUSIONS", "text": "In this paper, we studied the problem of inferring user locations from social media. We proposed a general probabilistic model based on factor graphs. e model generalizes previous methods by incorporating content, network, and deep features learned from social context. e model is su ciently exible to support both supervised learning and semi-supervised learning. We presented several learning algorithms and proposed an Two-chain MetropolisHastings (MH+) algorithm, which improves the inference accuracy. Our experiments on four di erent datasets validate the e ectiveness and the e ciency of the proposed model. We also implemented a parallel learning algorithm for the proposed model, which enables the proposed model to handle large-scale data.\nInferring user demographics from social media is a fundamental issue and represents a new and interesting research direction. As for future work, it would be intriguing to apply the proposed model to infer other demographic a ributes such as gender and age. It is also interesting to connect the study to some real applications \u2013 for example, advertising and recommendation \u2013 to further evaluate how inferred location can help real applications. Finally, we also consider how to integrate social theories such as social status and structural holes to understand the underlying mechanism behind user behavior and network dynamics."}], "references": [{"title": "Tensor\u0083ow: A system for large-scale machine learning", "author": ["M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving", "M. Isard"], "venue": "In OSDI\u201916,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "A survey of location inference techniques on twi\u008aer", "author": ["O. Ajao", "J. Hong", "W. Liu"], "venue": "Journal of Information Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Find me if you can: improving geographical prediction with social and spatial proximity. InWWW\u201910", "author": ["L. Backstrom", "E. Sun", "C. Marlow"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bo\u008aou"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "From interest to function: Location estimation in social media", "author": ["Y. Chen", "J. Zhao", "X. Hu", "X. Zhang", "Z. Li", "T.-S. Chua"], "venue": "In AAAI\u201913,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Wide & deep learning for recommender systems", "author": ["H.-T. Cheng", "L. Koc", "J. Harmsen", "T. Shaked", "T. Chandra", "H. Aradhye", "G. Anderson", "G. Corrado", "W. Chai", "M. Ispir"], "venue": "In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "You are where you tweet: a content-based approach to geo-locating twi\u008aer users", "author": ["Z. Cheng", "J. Caverlee", "K. Lee"], "venue": "In CIKM\u201910,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B.V. Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Computer Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Inferring the location of twi\u008aer messages based on user relationships", "author": ["C.A. Davis Jr.", "G.L. Pappa", "D.R.R. de Oliveira", "F. de L Arcanjo"], "venue": "Transactions in GIS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "A latent variable model for geographic lexical variation", "author": ["J. Eisenstein", "B. O\u2019Connor", "N.A. Smith", "E.P. Xing"], "venue": "In EMNLP\u201910,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Liblinear: A library for large linear classi\u0080cation", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of machine learning research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Deep sparse recti\u0080er neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In AISTATS\u201911,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Monte carlo sampling methods using markov chains and their applications", "author": ["W.K. Hastings"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1970}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Whowill follow you back? reciprocal relationship prediction", "author": ["J. Hopcro", "T. Lou", "J. Tang"], "venue": "In CIKM\u201911,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Location inference using microblog messages. InWWW\u201912, pages 687\u2013690", "author": ["Y. Ikawa", "M. Enoki", "M. Tatsubori"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "\u008cat\u2019s what friends are for: Inferring location in online social media platforms based on social relationships", "author": ["D. Jurgens"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Geolocation prediction in twi\u008aer using social networks: A critical analysis and review of current practice", "author": ["D. Jurgens", "T. Finethy", "J. McCorriston", "Y.T. Xu", "D. Ruths"], "venue": "In ICWSM\u201915,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Imagenet classi\u0080cation with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS\u201912,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Factor graphs and the sumproduct algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "H.-A. Loeliger"], "venue": "IEEE Transactions on information theory,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "What is twi\u008aer, a social network or a news media? InWWW\u201910", "author": ["H. Kwak", "C. Lee", "H. Park", "S. Moon"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Friendship as a social process: A substantive and methodological analysis", "author": ["P.F. Lazarsfeld", "R.K. Merton"], "venue": "Freedom and control in modern society,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1954}, {"title": "Krevl. SNAP Datasets: Stanford large network dataset collection", "author": ["A.J. Leskovec"], "venue": "h\u008ap://snap.stanford.edu/data,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Towards social user pro\u0080ling: uni\u0080ed and discriminative in\u0083uencemodel for inferring home locations", "author": ["R. Li", "S. Wang", "H. Deng", "R. Wang", "K.C.-C. Chang"], "venue": "In KDD\u201912,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Location prediction in social media based on tie strength", "author": ["J. McGee", "J. Caverlee", "Z. Cheng"], "venue": "In CIKM\u201913,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Birds of a feather: Homophily in social networks", "author": ["M. McPherson", "L. Smith-Lovin", "J. Cook"], "venue": "Annual review of sociology,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "Did distance ma\u008aer before the internet?: Interpersonal contact and support in the 1970s", "author": ["D. Mok", "B. Wellman"], "venue": "Social networks,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": "In UAI\u201999,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Samplerank: Training factor graphs with atomic gradients", "author": ["K. Rohanimanesh", "K. Bellare", "A. Culo\u008aa", "A. McCallum", "M.L. Wick"], "venue": "In ICML\u201911,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Inferring twi\u008aer user locations with 10 km accuracy", "author": ["K. Ryoo", "S. Moon"], "venue": "In WWW\u201914,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Simple supervised document geolocation with geodesic grids", "author": ["B.P. Wing", "J. Baldridge"], "venue": "In ACL\u201911,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "A comparative study on feature selection in text categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "In ICML\u201997,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1997}, {"title": "Social in\u0083uence locality for modeling retweeting behaviors", "author": ["J. Zhang", "B. Liu", "J. Tang", "T. Chen", "J. Li"], "venue": "In IJCAI\u201913,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Towards real-time, country-level location classi\u0080cation of worldwide tweets", "author": ["A. Zubiaga", "A. Voss", "R. Procter", "M. Liakata", "B. Wang", "A. Tsakalidis"], "venue": "arXiv preprint arXiv:1604.07236,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "For example, one observation is that the number of friends tends to decrease as distance increases [3, 27].", "startOffset": 99, "endOffset": 106}, {"referenceID": 26, "context": "For example, one observation is that the number of friends tends to decrease as distance increases [3, 27].", "startOffset": 99, "endOffset": 106}, {"referenceID": 14, "context": "Another study also shows that, even on the \u201cglobal\u201d Internet, the user\u2019s ego-network tends to stay local: the average number of friends between users from the same time zone is about 50 times higher than the number between users with a distance of three time zones [15].", "startOffset": 265, "endOffset": 269}, {"referenceID": 6, "context": "Statistics show that only 26% of users on Twi\u008aer input their locations [7].", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "Di\u0082erent from previous works [3, 7, 24] that deal with this problem in a speci\u0080c scenario (e.", "startOffset": 29, "endOffset": 39}, {"referenceID": 6, "context": "Di\u0082erent from previous works [3, 7, 24] that deal with this problem in a speci\u0080c scenario (e.", "startOffset": 29, "endOffset": 39}, {"referenceID": 23, "context": "Di\u0082erent from previous works [3, 7, 24] that deal with this problem in a speci\u0080c scenario (e.", "startOffset": 29, "endOffset": 39}, {"referenceID": 9, "context": "[10] proposed the Geographic Topic Model to predict a user\u2019s geo-location from text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] used a probabilistic framework and illustrated how to \u0080nd local words and overcome tweet sparsity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[30] applied a similar idea to a Korean Twi\u008aer dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] used a rulebased approach to predict a user\u2019s current location based on former tweets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] used language models and information retrieval approaches for location prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] used topic models to determine users\u2019 interests and mapped interests to location.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] assumed that an unknown user would be co-located with one of its friends and sought the ones with the maximum probabilities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[25] integrated social tie strengths between users to improve location estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Jurgens [17] and Davis Jr et al.", "startOffset": 8, "endOffset": 12}, {"referenceID": 33, "context": "[34] used the idea of label propagation to infer user locations according to their network distances from users with known locations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] proposed a uni\u0080ed discriminative in\u0083uence model and utilized both the content and the social network to predict user locations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "Another study using user pro\u0080les can be found in [34].", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "Survey and analysis of location inference techniques on Twi\u008aer can be also found in [2, 18].", "startOffset": 84, "endOffset": 91}, {"referenceID": 17, "context": "Survey and analysis of location inference techniques on Twi\u008aer can be also found in [2, 18].", "startOffset": 84, "endOffset": 91}, {"referenceID": 9, "context": "[10] US All User Cheng et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] US All User Ryoo et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[30] Korea Korean User Ikawa et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Japan English, Japanese Tweet Wing et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] US English User Chen et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] Beijing Chinese Tweet", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] World All User McGee et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[25] World All User Jurgens [17] World All User Davis Jr et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[25] World All User Jurgens [17] World All User Davis Jr et al.", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "[9] World All User Content + Network Li et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "[24] US English User", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] World All Tweet", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "* Top-10 by mutual information [32], among the words that #occurrence> 5000.", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "\u008ce principle of homophily [22] \u2014 \u201cbirds of a feather \u0083ock together\u201d [26] \u2014 suggests that \u201cconnected\u201d users may come from the same place.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "\u008ce principle of homophily [22] \u2014 \u201cbirds of a feather \u0083ock together\u201d [26] \u2014 suggests that \u201cconnected\u201d users may come from the same place.", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "\u008cis tendency was observed between Twi\u008aer reciprocal friends in [15, 21].", "startOffset": 63, "endOffset": 71}, {"referenceID": 20, "context": "\u008cis tendency was observed between Twi\u008aer reciprocal friends in [15, 21].", "startOffset": 63, "endOffset": 71}, {"referenceID": 7, "context": "Recently, deep neural networks have achieved excellent performance in various \u0080elds including image classi\u0080cation and phrase representations [8, 19].", "startOffset": 141, "endOffset": 148}, {"referenceID": 18, "context": "Recently, deep neural networks have achieved excellent performance in various \u0080elds including image classi\u0080cation and phrase representations [8, 19].", "startOffset": 141, "endOffset": 148}, {"referenceID": 11, "context": "where W1,W2, b1, b2 are parameters of the neural network, and we use ReLU(x) = max(0,x) [12] as the activation function.", "startOffset": 88, "endOffset": 92}, {"referenceID": 31, "context": "In our work, we employ Mutual Information (MI) [32].", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "[6] suggested that joint learning of wide and deep models can gain the bene\u0080ts of memorization and generalization.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "A traditional method to estimate the gradient in graphical models is Loopy Belief Propagation (LBP) [28], which provides a way to approximately calculate marginal probabilities on factor graphs.", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "LBP performs message passing between variables and factor nodes according to the sum-product rule [20].", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "In addition to LBP, Markov Chain Monte Carlo (MCMC) methods have proved successful for estimating parameters in complex graphical models, such as SampleRank [29].", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "In our work, we employ Metropolis-Hastings sampling [13] to obtain a sequence of random samples from the model distribution and update the parameters.", "startOffset": 52, "endOffset": 56}, {"referenceID": 28, "context": "\u008ce classical MH algorithm does not directly maximize the log-likelihood [29] and relies on additional evaluation metrics.", "startOffset": 72, "endOffset": 76}, {"referenceID": 3, "context": "Bearing a similar merit to stochastic gradient descent [4], the gradient of our model can be computed as s(Y1) \u2212 s(Y2).", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "Similar ideas based on Markov chains have been adopted for training restricted Boltzmann machines [14], but those algorithms do not apply to a partiallylabeled factor graph model as in our work.", "startOffset": 98, "endOffset": 102}, {"referenceID": 33, "context": "61 SVM [34] 94.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "30 GLP [9] 72.", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "53 FindMe [3] 72.", "startOffset": 10, "endOffset": 13}, {"referenceID": 32, "context": "\u2022 Weibo [33]: Weibo is the most popular Chinese microblog.", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "\u2022 Facebook [23]: \u008ce dataset is an ego-facebook dataset from SNAP, and does not contain content information.", "startOffset": 11, "endOffset": 15}, {"referenceID": 32, "context": "In the Weibo data provided by [33], the content has been tokenized into Chinese words.", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": "\u2022 SupportVectorMachine (SVM) [34]: Zubiaga et al.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "\u2022 Graph-based Label Propagation (GLP) [9]: It infers locations of unlabeled users by counting the most popular location among one\u2019s friends with a simple majority voting.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "\u2022 FindMe [3]: It infers user locations with social and spatial proximity.", "startOffset": 9, "endOffset": 12}, {"referenceID": 10, "context": "For baseline methods, we use the implementation of Liblinear [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "\u008ce deep factor in our model has been implemented using TensorFlow [1].", "startOffset": 66, "endOffset": 69}], "year": 2017, "abstractText": "We study the extent to which we can infer users\u2019 geographical locations from social media. Location inference from social media can bene\u0080t many applications, such as disaster management, targeted advertising, and news content tailoring. In recent years, a number of algorithms have been proposed for identifying user locations on social media platforms such as Twi\u008aer and Facebook from message contents, friend networks, and interactions between users. In this paper, we propose a novel probabilistic model based on factor graphs for location inference that o\u0082ers several unique advantages for this task. First, the model generalizes previous methods by incorporating content, network, and deep features learned from social context. \u008ce model is also \u0083exible enough to support both supervised learning and semi-supervised learning. Second, we explore several learning algorithms for the proposed model, and present a Two-chain Metropolis-Hastings (MH+) algorithm, which improves the inference accuracy. \u008cird, we validate the proposed model on three di\u0082erent genres of data \u2013 Twi\u008aer, Weibo, and Facebook \u2013 and demonstrate that the proposed model can substantially improve the inference accuracy (+3.3-18.5% by F1-score) over that of several state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}