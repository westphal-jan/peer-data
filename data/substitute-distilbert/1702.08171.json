{"id": "1702.08171", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Fixed-point optimization of deep neural networks with adaptive step size retraining", "abstract": "specified - point optimization of deep neural infrastructure took an important role in hardware based design and low - power implementations. many embedded neural networks guarantee fairly good performance even with 2 - or 3 - bit precision certain quantized ones are fine - tuned by retraining. we imagine an actual fixedpoint optimization algorithm that estimates the quantization step size dynamically during the retraining. including addition, a gradual quantization scheme is also tested, which sequentially applies fixed - point optimizations from high - down low - precision. the studies are conducted for feed - forward deep neural networks ( pots ), convolutional neural networks ( cnns ), and recurrent neural networks ( ip ).", "histories": [["v1", "Mon, 27 Feb 2017 08:00:58 GMT  (2562kb)", "http://arxiv.org/abs/1702.08171v1", "This paper is accepted in ICASSP 2017"]], "COMMENTS": "This paper is accepted in ICASSP 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sungho shin", "yoonho boo", "wonyong sung"], "accepted": false, "id": "1702.08171"}, "pdf": {"name": "1702.08171.pdf", "metadata": {"source": "CRF", "title": "FIXED-POINT OPTIMIZATION OF DEEP NEURAL NETWORKS WITH ADAPTIVE STEP SIZE RETRAINING", "authors": ["Sungho Shin", "Yoonho Boo"], "emails": ["sungho.develop@gmail.com,", "yhboo.research@gmail.com,", "wysung@snu.ac.kr"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n08 17\n1v 1\n[ cs\n.L G\n] 2\n7 Fe\nb 20\nportant role in hardware based design and low-power implementations. Many deep neural networks show fairly good performance even with 2- or 3-bit precision when quantized weights are fine-tuned by retraining. We propose an improved fixedpoint optimization algorithm that estimates the quantization step size dynamically during the retraining. In addition, a gradual quantization scheme is also tested, which sequentially applies fixed-point optimizations from high- to low-precision. The experiments are conducted for feed-forward deep neural networks (FFDNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs).\nIndex Terms\u2014 deep neural networks, recurrent neural net-\nworks, fixed-point quantization, step size adaptation\n1. INTRODUCTION\nDeep neural networks (DNNs) show very high performance in various fields such as speech recognition [1] and image classification [2]. However, real-time implementation of DNNs usually demands many arithmetic and weight fetch operations. Thus, word-length optimization is needed in embedded applications to reduce the strength of arithmetic and the size of the weight storage. However, reducing the word length too much tends to degrade the performance. Thus, developing optimum quantization methods is greatly needed for efficient implementation of neural network algorithms.\nDirect quantization of deep neural networks usually does not show satisfactory performance with very low precision weights. However, when the quantized weights are optimized by retraining, the fixed-point performance improves dramatically. Even ternary valued weights (+1, 0, and -1) for a DNN have yielded satisfactory performance [3, 4]. Recently, several improved fixed-point optimization methods are developed by employing retraining based fine tuning [5, 6]. Also, VLSI and FPGA based deep neural networks have been implemented using fixed-point weights [7, 8, 9, 10, 11].\nThis work was supported in part by the Brain Korea 21 Plus Project and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2015R1A2A1A10056051).\nIn this work, an improved retraining algorithm is developed for fixed-point optimization of deep neural networks. The previous works decide the optimum quantization step size based on the distribution of floating-point weights and freezes the stepsize during the retraining period [4, 5]. The proposed algorithm adaptively determines the step-size at the re-quantization step during retraining. Since the weight values change much at the beginning of retraining, this approach is especially effective when applied at initial retraining epochs. In order to change the weight values less abruptly, we also propose and evaluate the gradual quantization method. In this schemes, floatingpoint weights are converted to, for example, 6-bit weights, which are then converted to 4-bit weights, and so on. We evaluate the proposed schemes in three different networks: feed-forward deep neural networks (FFDNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs). The proposed methods yielded better results compared to the previous retrainbased quantization schemes.\nThe rest of this paper is organized as follows. Section 2 presents the proposed quantization with step size adaptation during the retraining procedure. The gradual quantization scheme is also explained. Experimental results on FFDNN, CNN, and RNN applications are shown in Section 3. Concluding remarks follow in Section 4."}, {"heading": "2. STEP SIZE ADAPTATION AND GRADUAL QUANTIZATION FOR RETRAINING OF DEEP NEURAL NETWORKS", "text": "In this section, we explain the conventional retrain based fixed-point optimization algorithm, and present adaptive step size retraining and gradual quantization methods."}, {"heading": "2.1. Retrain-based fixed-point quantization review", "text": "The original retrain based fixed-point optimization algorithm can be represented briefly as shown in Figure 1. Note that, conventional algorithms [3, 4, 12] do not compute \u2206new at the \u2018weights update\u2019 stage. In this figure, after obtaining the floating-point weights by training, the quantization step size, \u2206, is determined by minimizing the L2 error between the floating-point and fixed-point weights. For the convenience of arithmetic, uniform quantization is assumed. Two algorithms\nhave been developed for the quantization step size optimization. One is an exhaustive search, which decides the initial quantization step size \u2206initial by considering the weight distribution, and then searches the best performing step size between \u2206initial/2 and 2 \u00b7 \u2206initial by testing the quantized network with the evaluation set [4]. The second approach is deciding the quantization step size by measuring the mean and the variance of the floating-point weights [5]. Then, in the second stage of Figure 1, floating-point weights are rounded to fixed-point values by using the determined quantization step size. The third stage is the inferencing or the forward stage with the quantized network, w(q). The error signal is calculated and used for backward propagation. The gradient is calculated and weight update is conducted. Note that the floating-point weights, instead of the fixed-point values, are updated because the amount of weight update is usually much smaller than the quantization step size. Then, the fixed-point weight update, yieldingw (q) ij,new , is accomplished by quantizing the updated floating-point weights. Note that determining \u2206new is not performed in the conventional method, and the same quantization step size is used at every iteration."}, {"heading": "2.2. Step-size adaptation during retraining", "text": "As described in Section 2.1, the conventionalmethod freezes the step size during the retraining. However, in many cases, the weight values change much by retraining. Note that the amount of change decreases as the retraining iteration progresses. Thus, it is advantageous for improving the performance to adjust the quantization step size during the retraining. Especially, the need of step size adaptation is greater at the beginning of retraining. The proposed scheme adds the determination of \u2206new at the weight update stage of Figure 1.\nWe do not perform \u2018exhaustive search\u2019 anymore but update the quantization step size during retraining by using the L2 error minimization between the floating-point and fixed-point weights. We consider two different quantization step size update timing. The first one is \u2018epoch-level update\u2019, and the other is \u20181 epoch update & fix\u2019. The \u2018epoch-level update\u2019 changes the step size at every epoch. The \u20181 epoch update & fix\u2019 updates the step size only during one or two epochs and freezes it for the remaining epochs. In our empirical evaluation, the first scheme is good for FFDNNs, but the second one shows better results for CNNs and RNNs. The specific results will be given in Section 3."}, {"heading": "2.3. Gradual quantization scheme", "text": "We also propose another step size adaptation approachwhich is similar to the curriculum learning. The curriculum learning is a training strategy to move the goal from an easy level to more complex one gradually [13]. One of the important points in curriculum learning is how to organize the tasks from easy to complex ones. We consider that the fixed-point optimization with a small number of bits is a more difficult problem than that with a large one.\nIn the proposed scheme, we begin fixed-point optimization with a fairly high precision, such as 6 bits, and then keep lowering the word-length by one bit with retraining for each precision. At each retraining process with a given precision, we also combine the proposed quantization step size adaptation scheme. The experiments are conducted for FFDNNs."}, {"heading": "3. EXPERIMENTAL RESULTS", "text": "The proposed step size adaptation is evaluated for three applications. We employ FFDNNs for phoneme recognition, CNNs for house number recognition, and RNNs for language modeling. To analyze the effect of step size adaptation, we change the size of each network and their word lengths."}, {"heading": "3.1. Phoneme recognition using feed-forward deep neural networks", "text": "The FFDNN is trained with the TIMIT corpus [14], and the detailed experimental condition for the data preprocessing is the same with [15]. We construct 11 consecutive frames as the network input. The output layer supports 61 labels, and the labels are merged into 39 classes for the final evaluation. For performance evaluation, the number of units in each layer increases from 64 to 1024. We train the floating-point networks using\nTable 1: Frame-level phoneme error rate (%) on the test set with the TIMIT phoneme recognition examples. Note that \u2018conventional\u2019 is the baseline [4] and \u2018adaptive\u2019 is the proposed scheme.\nWithout BN With BN\nSize of each layer 64 128 256 512 1024 64 128 256 512 1024\nFloating result 34.38 31.63 30.17 29.61 29.53 33.82 30.81 29.79 29.77 29.59\n2-bit\n(3 point)\nDirect 80.25 84.12 81.92 83.30 75.05 89.82 88.79 87.57 85.73 86.10 Conventional 43.73 37.80 33.70 31.43 29.99 41.81 35.88 33.12 31.21 30.22\nAdaptive 42.06 36.88 32.61 30.61 29.49 37.87 33.46 31.48 30.73 30.09\n3-bit\n(7 point)\nDirect 68.13 63.65 60.33 51.46 48.61 80.41 69.55 69.42 81.60 64.55 Conventional 40.63 34.73 31.41 30.49 29.33 36.88 32.58 30.53 30.14 29.76\nAdaptive 37.89 33.80 30.74 29.83 29.40 35.29 31.94 30.32 30.10 29.65\n4-bit\n(15 point)\nDirect 58.90 50.58 42.15 38.05 36.53 65.63 50.43 46.46 43.80 39.77 Conventional 36.51 32.65 30.79 29.95 29.44 34.17 31.34 29.86 29.81 29.70\nAdaptive 35.50 32.09 30.50 29.54 29.29 33.91 30.86 29.47 29.87 29.52\n0 2 4 6 8 10 12 14 16 18\n0.2\n0.4\nNumber of epochs\n\u2206 a d a p t\nLayer1 Layer2 Layer3\nFig. 2: Training curves in terms of \u2206adapt with the 256 size of the FFDNN.\nthe stochastic gradient descent (SGD) with Nesterov momentum [16]. The learning rate decreases from 2e-3 to 3.90625e6 with a factor of 2 when the development set does not show improvements for 4 consecutive evaluations. For fixed-point networks training, all other conditions are the same with the floating-point case but the initial learning rate is 5e-4.\nThe results of fixed-point optimization for FFDNNs with and without the step size adaptation are reported in Table 1. The experiments also show the results with batch normalization (BN) [17]. The step size is updated using the \u2018epoch-level update\u2019 until the end of the retraining. Table 1 shows that the floating-point network performance saturates at 512 units size when BN is applied, and at 256 units when BN is not used. When the unit size in each layer is 512 or smaller, the proposed algorithm yields better performance in both cases. For example, if the 512 units size network is quantized in 2-bit without BN, the differences between the floating-point and the fixed-point networks are 1.82% and 1% for \u2018conventional\u2019 and \u2018adaptive\u2019 schemes, respectively. In addition, the phoneme error rate of the 3-bit network optimized with the \u2018adaptive\u2019 scheme (29.83%) is lower than that of the 4-bit quantized network with the \u2018conventional\u2019 scheme (29.95%).\nBN improves the performance of both floating-point and fixed-point networks. Applying the \u2018adaptive\u2019 method improves the performance. For example, if the layer unit size is 128 and 2-bit quantization is used, BN brings the performance gain of 3.42% when \u2018adaptive\u2019 scheme is used. Therefore, the proposed\n\u2018adaptive\u2019 method can be efficiently used with BN.\nWhen the unit size is large enough, the quantization scheme does not affect the performance much because a larger size network has a better resiliency to quantization [18]. Even the 4-bit quantized 512 units size network without BN shows the performance almost comparable to the floating-point 1024 units size network. When the network is trained with BN, it shows a similar trend.\nFigure 2 shows the step size \u2206 of the proposed adaptive scheme as the retraining progresses. Note that the step size is renewed at each epoch during retraining. As shown in this figure, the step size of the last layer varies much, while that of the first layer is almost constant. The step size adaptation is much needed for the last layer.\nWe also evaluate the performance of the gradual quantization scheme. The results are reported in Table 2. The floating point results show 29.61% error rate on the test set. The 6- bit word length shows slightly better accuracy than the floating point. Thus, we define the easiest task as the 6-bit quantization. In Table 2, the \u2018gradual\u2019 scheme yields better performance than the \u2018conventional\u2019 strategy, but shows worse or similar results compared to the \u2018adaptive\u2019 quantization. The combined strategy of the \u2018adaptive\u2019 and \u2018gradual\u2019 shows slightly better ac-\ncuracy than the \u2018adaptive\u2019 strategy in 4- and 3-bit quantization, but it is worse than the \u2018adaptive\u2019 scheme in 2-bit quantization. Since there is no performance difference between the \u2018adaptive\u2019 and \u2018adaptive & gradual\u2019 scheme, we only employ the \u2019adaptive\u2019 scheme for CNN and RNN experiments."}, {"heading": "3.2. Image classification using convolutional neural networks", "text": "Image classification experiments are performed on the SVHN dataset [19]. The dataset includes 600,000 labeled 32x32 three channel images from real world house numbers. For the data preprocessing, we employ the same method with [20]. The output label has ten units which represent the numbers from 0 to 9. For evaluation of the proposed scheme, we employ three different structures. We name the networks as \u2018L\u2019, \u2018C\u2019, and \u2018V\u2019 which have the trainable parameters 60k, 84k, and 435k, respectively. The \u2018L\u2019 network is Lenet5 [21], \u2018C\u2019 network is from [22], and \u2018V\u2019 network is constructed as VGG style which\nis from [23]. We train the floating-point networks using SGD with Nesterov momentum. The learning rate is decreased from 2-e2 to 3.125e-4 with a factor of 2 when the development set does not show improvement for 4 consecutive evaluations. For the fixed-point network training, the initial learning rate was 5e4. The effects of step size adaptation in the CNNs are examined in Table 3. The step size is updated using the \u20181 epoch update & fix\u2019 strategy. Our algorithm works well for \u2018L\u2019 and \u2018V\u2019 networks regardless of the weight precision, 2, 3, or 4 bits. However, the \u2018C\u2019 networks with the conventional retraining show a better result when the weight precision is 4bits. Overall, the proposed method yields improved performances."}, {"heading": "3.3. Language modeling using recurrent neural networks", "text": "Character-level language modeling predicts the next character, and is used for speech recognition and text generation. Since the input and output layers consider only alphabets, the input and output complexity are much lower than the word level language model. We adopt English Wikipedia dataset for training the character level language modeling. The dataset contains 100 MB English Wikipedia text. The input and output layers are composed of 256 units for one-hot encoded ASCII code. The RNN consists of three Long Short-Term Memory (LSTM) layers with a different number of memory cells ranging from 64 to 256 [24]. We train the RNNs using AdaDelta based SGDwith 64 parallel input streams. The networks are unrolled 256 times and weights update is performed for128 forward steps. The learning rate starts from 5e-4 and decreases until 5e-8. For the step size adaptation, \u20181 epoch update & fix\u2019 strategy is employed. The fixed-point optimization results are reported in Table 4. As with our previous FFDNN and CNN results, it shows much improved performances on low-precision weights or small size networks."}, {"heading": "4. CONCLUDING REMARKS", "text": "We have developed improved fixed-point weight optimization methods for deep neural networks. The first one adaptively determines the quantization step size by measuring the weight distribution during the retraining procedure. The second one is a curriculum style fixed-point optimization technique, which conducts fixed-point optimization from high- to low-precision gradually. The proposed work yields better quantization results in FFDNN, CNN, and RNN experiments. Especially the effectiveness of the proposed techniques increases when the number of quantization levels is small and the network size is not large enough."}, {"heading": "5. REFERENCES", "text": "[1] Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl\nCase, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, et al., \u201cDeep speech 2: End-to-end speech recognition in english and mandarin,\u201d in ICML 2016: 33rd International Conf. Machine Learning, 2016.\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun,\n\u201cDeep residual learning for image recognition,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n[3] Sungho Shin, Kyuyeon Hwang, and Wonyong Sung,\n\u201cFixed-point performance analysis of recurrent neural networks,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 976\u2013980.\n[4] Kyuyeon Hwang and Wonyong Sung, \u201cFixed-point feed-\nforward deep neural network design using weights +1, 0, and -1,\u201d in 2014 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2014, pp. 1\u20136.\n[5] Darryl D Lin, Sachin S Talathi, and V Sreekanth Anna-\npureddy, \u201cFixed point quantization of deep convolutional networks,\u201d in ICML 2016: 33rd International Conf. Machine Learning, 2016.\n[6] Song Han, Huizi Mao, and William J Dally, \u201cDeep com-\npression: Compressing deep neural network with pruning, trained quantization and huffman coding,\u201d CoRR, abs/1510.00149, vol. 2, 2015.\n[7] Jonghong Kim, Kyuyeon Hwang, and Wonyong Sung,\n\u201cX1000 real-time phoneme recognition vlsi using feedforward deep neural networks,\u201d in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 7510\u20137514.\n[8] Jinhwan Park and Wonyong Sung, \u201cFPGA based imple-\nmentation of deep neural networks using on-chip memory only,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 1011\u20131015.\n[9] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pe-\ndram, Mark A. Horowitz, and William J. Dally, \u201cEIE: efficient inference engine on compressed deep neural network,\u201d in 43rd ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2016, Seoul, South Korea, June 18-22, 2016, 2016, pp. 243\u2013254.\n[10] Minjae Lee, Kyuyeon Hwang, Jinhwan Park, Choi Sung-\nwook, Shin Sungho, and Wonyong Sung, \u201cFPGA-based low-power speech recognition with recurrent neural networks,\u201d in 2016 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2016.\n[11] Nicholas J Fraser, Yaman Umuroglu, Giulio Gambardella,\nMichaela Blott, Philip Leong,Magnus Jahre, and Kees Vissers, \u201cScaling binarized neural networks on reconfigurable logic,\u201d in To appear in the PARMA-DITAM workshop at HiPEAC, 2017, vol. 2017.\n[12] Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung,\n\u201cFixed point optimization of deep convolutional neural networks for object recognition,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 1131\u20131135.\n[13] Yoshua Bengio, Je\u0301ro\u0302me Louradour, Ronan Collobert, and\nJason Weston, \u201cCurriculum learning,\u201d in Proceedings of the 26th annual international conference on machine learning. ACM, 2009, pp. 41\u201348.\n[14] John S Garofolo, Lori F Lamel, William M Fisher,\nJonathon G Fiscus, and David S Pallett, \u201cDARPA TIMIT acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1,\u201d NASA STI/Recon technical report n, vol. 93, 1993.\n[15] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hin-\nton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6645\u2013 6649.\n[16] Ilya Sutskever, James Martens, George E Dahl, and Geof-\nfrey E Hinton, \u201cOn the importance of initialization and momentum in deep learning.,\u201d ICML (3), vol. 28, pp. 1139\u2013 1147, 2013.\n[17] Sergey Ioffe and Christian Szegedy, \u201cBatch normalization:\nAccelerating deep network training by reducing internal covariate shift,\u201d in ICML, 2015, pp. 448\u2013456.\n[18] Wonyong Sung, Sungho Shin, and Kyuyeon Hwang, \u201cRe-\nsiliency of deep neural networks under quantization,\u201d arXiv preprint arXiv:1511.06488, 2015.\n[19] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-\nsacco, Bo Wu, and Andrew Y Ng, \u201cReading digits in natural images with unsupervised feature learning,\u201d 2011.\n[20] Pierre Sermanet, Soumith Chintala, and Yann LeCun,\n\u201cConvolutional neural networks applied to house numbers digit classification,\u201d in Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 3288\u2013 3291.\n[21] Yann LeCun, Bernhard Boser, John S Denker, Donnie\nHenderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel, \u201cBackpropagation applied to handwritten zip code recognition,\u201d Neural computation, vol. 1, no. 4, pp. 541\u2013551, 1989.\n[22] Alex Krizhevsky, \u201ccuda-convnet: High-performance\nc++/cuda implementation of convolutional neural networks,\u201d 2012.\n[23] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre\nDavid, \u201cBinaryconnect: Training deep neural networks with binary weights during propagations,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 3123\u2013 3131.\n[24] Felix A Gers, Nicol N Schraudolph, and Ju\u0308rgen Schmid-\nhuber, \u201cLearning precise timing with lstm recurrent networks,\u201d Journal of machine learning research, vol. 3, no. Aug, pp. 115\u2013143, 2002."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos"], "venue": "ICML 2016: 33rd International Conf. Machine Learning, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Fixed-point performance analysis of recurrent neural networks", "author": ["Sungho Shin", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 976\u2013980.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Fixed-point feedforward deep neural network design using weights +1, 0, and -1", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "2014 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2014, pp. 1\u20136.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Fixed point quantization of deep convolutional networks", "author": ["Darryl D Lin", "Sachin S Talathi", "V Sreekanth Annapureddy"], "venue": "ICML 2016: 33rd International Conf. Machine Learning, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "CoRR, abs/1510.00149, vol. 2, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "X1000 real-time phoneme recognition vlsi using feedforward deep neural networks", "author": ["Jonghong Kim", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 7510\u20137514.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "FPGA based implementation of deep neural networks using on-chip memory only", "author": ["Jinhwan Park", "Wonyong Sung"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 1011\u20131015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "EIE: efficient inference engine on compressed deep neural network", "author": ["Song Han", "Xingyu Liu", "Huizi Mao", "Jing Pu", "Ardavan Pedram", "Mark A. Horowitz", "William J. Dally"], "venue": "43rd ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2016, Seoul, South Korea, June 18-22, 2016, 2016, pp. 243\u2013254.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "FPGA-based low-power speech recognition with recurrent neural networks", "author": ["Minjae Lee", "Kyuyeon Hwang", "Jinhwan Park", "Choi Sungwook", "Shin Sungho", "Wonyong Sung"], "venue": "2016 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Scaling binarized neural networks on reconfigurable logic", "author": ["Nicholas J Fraser", "Yaman Umuroglu", "Giulio Gambardella", "Michaela Blott", "Philip Leong", "Magnus Jahre", "Kees Vissers"], "venue": "To appear in the PARMA-DITAM workshop at HiPEAC, 2017, vol. 2017.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 1131\u20131135.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "Proceedings of the 26th annual international conference on machine learning. ACM, 2009, pp. 41\u201348.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "DARPA TIMIT acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1", "author": ["John S Garofolo", "Lori F Lamel", "William M Fisher", "Jonathon G Fiscus", "David S Pallett"], "venue": "NASA STI/Recon technical report n, vol. 93, 1993.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1993}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6645\u2013 6649.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E Dahl", "Geoffrey E Hinton"], "venue": "ICML (3), vol. 28, pp. 1139\u2013 1147, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "ICML, 2015, pp. 448\u2013456.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Resiliency of deep neural networks under quantization", "author": ["Wonyong Sung", "Sungho Shin", "Kyuyeon Hwang"], "venue": "arXiv preprint arXiv:1511.06488, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["Pierre Sermanet", "Soumith Chintala", "Yann LeCun"], "venue": "Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 3288\u2013 3291.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation, vol. 1, no. 4, pp. 541\u2013551, 1989.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "cuda-convnet: High-performance c++/cuda implementation of convolutional neural networks", "author": ["Alex Krizhevsky"], "venue": "2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 3123\u2013 3131.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Felix A Gers", "Nicol N Schraudolph", "J\u00fcrgen Schmidhuber"], "venue": "Journal of machine learning research, vol. 3, no. Aug, pp. 115\u2013143, 2002.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) show very high performance in various fields such as speech recognition [1] and image classification [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "Deep neural networks (DNNs) show very high performance in various fields such as speech recognition [1] and image classification [2].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "Even ternary valued weights (+1, 0, and -1) for a DNN have yielded satisfactory performance [3, 4].", "startOffset": 92, "endOffset": 98}, {"referenceID": 3, "context": "Even ternary valued weights (+1, 0, and -1) for a DNN have yielded satisfactory performance [3, 4].", "startOffset": 92, "endOffset": 98}, {"referenceID": 4, "context": "Recently, several improved fixed-point optimization methods are developed by employing retraining based fine tuning [5, 6].", "startOffset": 116, "endOffset": 122}, {"referenceID": 5, "context": "Recently, several improved fixed-point optimization methods are developed by employing retraining based fine tuning [5, 6].", "startOffset": 116, "endOffset": 122}, {"referenceID": 6, "context": "Also, VLSI and FPGA based deep neural networks have been implemented using fixed-point weights [7, 8, 9, 10, 11].", "startOffset": 95, "endOffset": 112}, {"referenceID": 7, "context": "Also, VLSI and FPGA based deep neural networks have been implemented using fixed-point weights [7, 8, 9, 10, 11].", "startOffset": 95, "endOffset": 112}, {"referenceID": 8, "context": "Also, VLSI and FPGA based deep neural networks have been implemented using fixed-point weights [7, 8, 9, 10, 11].", "startOffset": 95, "endOffset": 112}, {"referenceID": 9, "context": "Also, VLSI and FPGA based deep neural networks have been implemented using fixed-point weights [7, 8, 9, 10, 11].", "startOffset": 95, "endOffset": 112}, {"referenceID": 10, "context": "Also, VLSI and FPGA based deep neural networks have been implemented using fixed-point weights [7, 8, 9, 10, 11].", "startOffset": 95, "endOffset": 112}, {"referenceID": 3, "context": "The previous works decide the optimum quantization step size based on the distribution of floating-point weights and freezes the stepsize during the retraining period [4, 5].", "startOffset": 167, "endOffset": 173}, {"referenceID": 4, "context": "The previous works decide the optimum quantization step size based on the distribution of floating-point weights and freezes the stepsize during the retraining period [4, 5].", "startOffset": 167, "endOffset": 173}, {"referenceID": 2, "context": "Note that, conventional algorithms [3, 4, 12] do not compute \u2206new at the \u2018weights update\u2019 stage.", "startOffset": 35, "endOffset": 45}, {"referenceID": 3, "context": "Note that, conventional algorithms [3, 4, 12] do not compute \u2206new at the \u2018weights update\u2019 stage.", "startOffset": 35, "endOffset": 45}, {"referenceID": 11, "context": "Note that, conventional algorithms [3, 4, 12] do not compute \u2206new at the \u2018weights update\u2019 stage.", "startOffset": 35, "endOffset": 45}, {"referenceID": 3, "context": "One is an exhaustive search, which decides the initial quantization step size \u2206initial by considering the weight distribution, and then searches the best performing step size between \u2206initial/2 and 2 \u00b7 \u2206initial by testing the quantized network with the evaluation set [4].", "startOffset": 268, "endOffset": 271}, {"referenceID": 4, "context": "The second approach is deciding the quantization step size by measuring the mean and the variance of the floating-point weights [5].", "startOffset": 128, "endOffset": 131}, {"referenceID": 12, "context": "The curriculum learning is a training strategy to move the goal from an easy level to more complex one gradually [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "The FFDNN is trained with the TIMIT corpus [14], and the detailed experimental condition for the data preprocessing is the same with [15].", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "The FFDNN is trained with the TIMIT corpus [14], and the detailed experimental condition for the data preprocessing is the same with [15].", "startOffset": 133, "endOffset": 137}, {"referenceID": 3, "context": "Note that \u2018conventional\u2019 is the baseline [4] and \u2018adaptive\u2019 is the proposed scheme.", "startOffset": 41, "endOffset": 44}, {"referenceID": 15, "context": "the stochastic gradient descent (SGD) with Nesterov momentum [16].", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "The experiments also show the results with batch normalization (BN) [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "When the unit size is large enough, the quantization scheme does not affect the performance much because a larger size network has a better resiliency to quantization [18].", "startOffset": 167, "endOffset": 171}, {"referenceID": 18, "context": "Image classification experiments are performed on the SVHN dataset [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "For the data preprocessing, we employ the same method with [20].", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "The \u2018L\u2019 network is Lenet5 [21], \u2018C\u2019 network is from [22], and \u2018V\u2019 network is constructed as VGG style which is from [23].", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "The \u2018L\u2019 network is Lenet5 [21], \u2018C\u2019 network is from [22], and \u2018V\u2019 network is constructed as VGG style which is from [23].", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "The \u2018L\u2019 network is Lenet5 [21], \u2018C\u2019 network is from [22], and \u2018V\u2019 network is constructed as VGG style which is from [23].", "startOffset": 116, "endOffset": 120}, {"referenceID": 23, "context": "The RNN consists of three Long Short-Term Memory (LSTM) layers with a different number of memory cells ranging from 64 to 256 [24].", "startOffset": 126, "endOffset": 130}], "year": 2017, "abstractText": "Fixed-point optimization of deep neural networks plays an important role in hardware based design and low-power implementations. Many deep neural networks show fairly good performance even with 2or 3-bit precision when quantized weights are fine-tuned by retraining. We propose an improved fixedpoint optimization algorithm that estimates the quantization step size dynamically during the retraining. In addition, a gradual quantization scheme is also tested, which sequentially applies fixed-point optimizations from highto low-precision. The experiments are conducted for feed-forward deep neural networks (FFDNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs).", "creator": "LaTeX with hyperref package"}}}