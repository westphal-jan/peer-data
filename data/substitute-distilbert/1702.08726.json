{"id": "1702.08726", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Stacked Thompson Bandits", "abstract": "we introduce stacked logical bandits ( stb ) more efficiently generating plans that are likely to obey a given bounded temporal logic requirement. stb creates a simulation for evaluation of plans, and follow a bayesian approach namely consider the resulting information to guide its search. in particular, we show that stacking multiarmed bandits and using thompson sampling nets guide the action selection process for each bandit enables stb operations generate plans that satisfy requirements with a high probability while only searching a fraction of the search space.", "histories": [["v1", "Tue, 28 Feb 2017 10:19:30 GMT  (621kb,D)", "http://arxiv.org/abs/1702.08726v1", "Accepted at SEsCPS @ ICSE 2017"]], "COMMENTS": "Accepted at SEsCPS @ ICSE 2017", "reviews": [], "SUBJECTS": "cs.SE cs.AI cs.SY", "authors": ["lenz belzner", "thomas gabor"], "accepted": false, "id": "1702.08726"}, "pdf": {"name": "1702.08726.pdf", "metadata": {"source": "CRF", "title": "Stacked Thompson Bandits", "authors": ["Lenz Belzner"], "emails": ["belzner@ifi.lmu.de", "belzner@ifi.lmu.de"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nIn many cases, system requirements can be formalized in a bounded temporal logic [1], [2]. For example, consider a mobile robot with planning capabilities in a area with obstacles. Here, a requirement could be that when executing a given sequential plan of ten actions, there should occur less than e.g. three collisions with obstacles. One could write this requirement as a temporal logic formula similar to the following:\n\u03c6 = h\u226410(collisions \u2264 2)\nIn this paper, we propose Stacked Thompson Bandits (STB) as an algorithm for constructing plans that are likely to satisfy a given requirement. STB works by simulation: It repeatedly samples a plan from its current policy, simulates the result of this plan w.r.t. a given requirement, and updates its policy according to the result in order to increase the quality of further sampled plans. As we will see, STB leverages Bayesian statistics to guide the exploration-exploitation tradeoff when searching the space of possible plans for feasible solutions.\nSTB is an open loop planner [3], [4]: It does only search the space of plans. That is, action selection is not conditioned w.r.t. intermediate states, but only w.r.t. previously selected actions. STB does only account for intermediate states when evaluating requirement satisfaction of a given plan.\nThe key idea of STB is to model the plan selection policy by an open loop sequence of multiarmed bandits. Each of the bandits represents an action choice for plan construction at a certain point in time. Exploration of the search space and exploitation of already gathered results are balanced with Thompson sampling [5], [6]. Information gathered from simulating sampled plans is used to increase the likelihood that plans constructed by STB satisfy a given requirement.\nThe remainder of the paper is structured as follows: Section II recaps open loop planning, multiarmed bandits and Thompson sampling. Section III introduces Stacked Thompson\nBandits. In Section IV we discuss our empirical findings. Section V discusses related work. We conclude in Section VI and give pointers to potential future work."}, {"heading": "II. PRELIMINARIES", "text": "In this Section, we recap open loop planning, multiarmed bandits and Thompson sampling."}, {"heading": "A. Open Loop Planning", "text": "In our setting, open loop planning ([3], [4]) is an approach to find plans that result in satisfaction of a given goal without storing information about the intermediate states that are encountered while executing the plan. I.e. given a set of actions A, we are only interested in finding a plan p \u2208 A\u2217, and we are only keeping information about the action sequences in order to guide the planning process.\nThis is in contrast to closed loop planning, such as e.g. Monte Carlo Tree Search [7], where action selection is typically conditioned by the history of previously encountered states and executed actions."}, {"heading": "B. Multiarmed Bandits", "text": "Multiarmed bandits (MAB) are a core framework for decision making. A bandit consists of a number of arms, each representing an agent\u2019s choice. In our setting, each arm represents an action a \u2208 A. Each arms provides a particular payoff, and the agent\u2019s goal is to identify the most preferable arm. It can explore the bandit by pulling one arm at a time, and observe the corresponding payoff.\nAn MAB can be interpreted as a simple Markov decision process with a single state. In their basic formulation, MABs already provide a clear framework for studying the exploration-exploitation tradeoff inherent to decision making under uncertainty: Should the agent select the arm that previously showed to be most promising? Or should it go on exploring other options? For a recent survey of MAB and its variants, see [8]."}, {"heading": "C. Thompson Sampling", "text": "Thompson sampling (TS) is a Bayesian algorithm for solving an MAB. It was proposed decades ago [5], but only recently its astonishing effectiveness and generality have been identified [9], [10], [11].\nIn the case of Bernoulli rewards (as in the case of STB), the parameter of each arm to be estimated is a probability\nar X\niv :1\n70 2.\n08 72\n6v 1\n[ cs\n.S E\n] 2\n8 Fe\nb 20\n17\np \u2208 [0; 1]. TS infers a posterior distribution over p based on the observed arm payoffs and a prior assumption about the distribution of p. In general, the posterior is proportional to the likelihood of observed data D (i.e. an arm\u2019s observed payoffs), multiplied by the prior distribution P (\u03b8) over the parameters of interest, \u03b8 = p in our case (Equation 1).\nP (\u03b8|D) \u221d P (D|\u03b8)P (\u03b8) (1)\nWe can model the uncertainty about p by a Beta distribution, the conjugate prior of the Bernoulli distribution. This approach ensures that the posterior is of the same form as the prior distribution, and thus enables efficient sequential updating of the distribution. The Beta distribution is parametrized by two parameters a, b \u2208 R+. In our case, a and b are given by the successes and failures of the arm pulls. Given s successes, f failures, and assuming an uninformative prior over p, the posterior (for \u03b8 = p) is determined by Equation 2.\nP (p|D) = Beta(s+ 1, f + 1) (2)\nTS maintains such a distribution P (\u03b8) for each arm. The algorithm then samples a potential value for each arm from these distributions. It then plays the arm from whose distribution the maximum value has been sampled, observes the payoff, and uses this observation to update the corresponding distribution. Repeating this process results in almost sure identification of the arm with the highest payoff. TS is schematically shown in Algorithm 1.\nAlgorithm 1 Thompson Sampling 1: procedure THOMPSON SAMPLING 2: \u2200a \u2208 A : p\u0302a \u223c Pa(p) 3: play arg maxa p\u0302a and observe result 4: update Pa(p) w.r.t. result"}, {"heading": "III. STACKED THOMPSON BANDITS", "text": "We now present Stacked Thompson Bandits (STB). STB works by stacking a number of MAB, treating them as a sequential decision problem. In order to construct a plan, STB sequentially selects an action from each MAB using Thompson sampling. The resulting plan checked for requirement satisfaction using a simulation M . Given a set of states S, a set of actions A and the set of bounded temporal formulae \u03a6, M is a conditional probability distribution as follows.\nM : P (Bool|S,A\u2217,\u03a6)\nI.e. running a simulation for a given initial state s \u2208 S, a given plan p \u2208 A\u2217 and a given requirement \u03c6 \u2208 \u03a6 can be interpreted as a Bernoulli experiment. That is, M(s, p, \u03c6) defines a Bernoulli distribution, where the result indicates whether p satisfies \u03c6 when executed in s. STB uses the Bernoulli result of such a simulation to update each of the MABs w.r.t. Equation 2.\nSTB is shown in Algorithm 2. It takes the following input:\n\u2022 The current system state s \u2208 S. \u2022 A bounded temporal formula \u03c6 \u2208 \u03a6. \u2022 A simulation model M . First, STB initializes the parameters of each stacked bandit. In particular, for each step i up to the horizon of the requirement \u03c6 and each action a \u2208 A, it maintains a count of successes sa,i (i.e. satisfactions) and failures fa,i (i.e. violations) (lines 2 \u2013 4).\nThen, STB repeats the following steps until interruption (e.g. due to some budget, or because an event occurred). \u2022 An estimated satisfaction probability is sampled from the\nstacked bandit, i.e. from each beta distribution for each step i and for each action a (line 8). \u2022 The actions yielding the maximum estimate for each step are combined to a plan (lines 9 and 10). \u2022 The sampled plan is simulated in M w.r.t. system state and given requirement. The result (satisfaction or violation) is observed (line 11). \u2022 The bandit parameters of all actions in the simulated plan are updated w.r.t. the simulation result (lines 12 \u2013 16).\nAlgorithm 2 Stacked Thompson Bandits 1: procedure STB(s, \u03c6,M ) 2: for a \u2208 A, i \u2208 0...h(\u03c6) do 3: sa,i \u2190 0 4: fa,i \u2190 0 5: while not interrupted do 6: p\u2190 nil 7: for i \u2208 0...h(\u03c6) do 8: \u2200a \u2208 A : p\u0302a \u223c Beta(sa,i + 1, fa,i + 1) 9: ai \u2190 arg maxa\u2208A p\u0302a 10: p\u2190 p :: ai 11: sat \u223cM(s, p, \u03c6) 12: for ai \u2208 p do 13: if sat then 14: sa,i \u2190 sa,i + 1 15: else 16: fa,i \u2190 fa,i + 1\nThe algorithm terminates on some external interruption signal that is to be specified by the user. A possible decision rule on which plan to execute would be to select the action with maximum distribution mode (the value with most probability mass) for each MAB in the stack. The mode of a beta distribution with parameters a, b is given by:\na+ 1\na+ b+ 2\nMode plan selection for STB is therefore performed as shown in Algorithm 3."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "We implemented STB and observed whether it is able to generate plans with increasing probability of satisfaction requirement when performing more search iterations.\nAlgorithm 3 STB mode plan selection 1: for i \u2208 0...h do 2: ai \u2190 arg maxa\u2208A sa,i+1sa,i+fa,i+2 3: p\u2190 p :: ai"}, {"heading": "A. Setup", "text": "The state s is constituted by a 10 x 10 grid world, with the agent at position (0, 0). Obstacles are randomly positioned, at an obstacle to free position ratio of 0.2. Actions are movements in four directions up, down, left, right with obvious semantics.\nThe agent has a Bernoulli action failure probability pfail uniformly sampled from [0; 1]. Action failure results in the inverse movement (e.g. failing up yields down). This constitutes domain noise in the simulation M available to the agent.\nThe task of STB is to generate a plan of length 10 that yields less than three collisions with obstacles when executed:\n\u03c6 = h\u226410(collisions \u2264 2)\nThis setup yields a search space cardinality 410 = 1048576. However, note that due to probabilistic domain dynamics (i.e. the branching due to potential action failures) a plan has to be evaluated many times to obtain an adequate estimate of its satisfaction probability, yielding a very hard search problem. We approximated the ground truth satisfaction probability of a plan by taking the maximum likelihood estimate of satisfaction probability based on 1000 simulation runs.\nOur implementation of the setup and STB is available at https://github.com/jazzbob/stb."}, {"heading": "B. Results", "text": "In our experimental runs, we observed that STB is able to generate plans with increasing probability of satisfying the given requirement. In particular, STB was able to find potentially close to optimal solutions searching only a fraction of the search space. Figure 1 shows an exemplary run of STB.\nAs a sanity check, we also observed the average mode value as well as the coefficient of variation (CV) of sampled plan arm distributions and mode (i.e. best) plan arm distributions respectively. While the mode value gives a rough estimate about value of an arm (i.e. an action), the CV is defined as the ratio of standard deviation to mean \u03c3\u00b5 of a distribution. CV is suitable to measure the accuracy of a distribution when its mean value changes [12], as is the case in STB. We expect the average mode value to increase in the long run for both sampled and best plans. We also expect the CV of sampled and best plans to decrease in the long run. We could empirically establish both expected results (cf. Figure 2).\nHowever, we also observed STB to get stuck in local optima (cf. Figure 3). This is a property of many stochastic search algorithms. One could potentially reduce the risk of getting stuck by using an ensemble of STB planners in order to reduce the probability of such premature convergence."}, {"heading": "V. RELATED WORK", "text": "Our work on STB is strongly influenced by existing open loop planners. In particular, Cross Entropy Open Loop Planning is an approach for planning in large-scale continuous MDPs [4]. It is however not applicable to discrete domains as STB. Recently, cross entropy planning has been used for searching sequences that satisfy a given temporal logic formula [13] in a continuous motion planning setting.\nSTB is subtly related to statistical model checking (SMC) [14], [15], and Bayesian statistical model checking in particular [16], [17], [18]. Here, the setting is to guarantee a\nminimal required satisfaction probability for a given, fixed sequence of actions. SMC approaches are able to provide such a result, potentially with a quantifiable confidence. STB is not able to provide a quantification of satisfaction probability, as is samples every plan only once. Also, the distributions maintained in the stacked bandits to not provide an accurate absolute quantification due to the sampling nature of search and the corresponding concept drift. On the other hand, STB is able to generate plans with high satisfaction probability, whereas SMC only can determine this probability. In practice, a combination of both approaches is possible and seems useful."}, {"heading": "VI. CONCLUSION", "text": "We have presented Stacked Thompson Bandits (STB), an open loop planning algorithm for generating action sequences that satisfy a given bounded temporal logic requirement with high probability. STB works by maintaining a stack of multiarmed bandits via Thompson sampling. We have preliminarily and empirically evaluated the effectiveness of STB on a toy example.\nThere are various directions for future work. As mentioned, it would be interesting to see if an ensemble approach with STB could reduce the probability of getting stuck in local minima. Another interesting venue would be to combine temporal action abstraction with STB. See [19], [12] for previous work of one of the authors on temporal abstraction in open loop planning. Also, guiding the sampling process in a QoS-aware manner based on required confidences could prove worthwhile. See e.g. [20] for previous work of the authors on QoS-aware sampling in multiarmed bandits. It would also be interesting to explore STB under model uncertainty, e.g. when the simulation model is learned from incomplete information."}], "references": [{"title": "Principles of model checking", "author": ["C. Baier", "J.-P. Katoen", "K.G. Larsen"], "venue": "MIT press,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "PRISM 4.0: Verification of probabilistic real-time systems", "author": ["M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "Proc. 23rd International Conference on Computer Aided Verification (CAV\u201911), ser. LNCS, G. Gopalakrishnan and S. Qadeer, Eds., vol. 6806. Springer, 2011, pp. 585\u2013591.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Open loop optimistic planning.", "author": ["S. Bubeck", "R. Munos"], "venue": "COLT,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Open-loop planning in large-scale stochastic domains.", "author": ["A. Weinstein", "M.L. Littman"], "venue": "in AAAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika, vol. 25, no. 3/4, pp. 285\u2013294, 1933.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1933}, {"title": "Analysis of thompson sampling for the multiarmed bandit problem.", "author": ["S. Agrawal", "N. Goyal"], "venue": "COLT,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Monte-carlo tree search", "author": ["G. Chaslot"], "venue": "Maastricht: Universiteit Maastricht, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Algorithms for multi-armed bandit problems", "author": ["V. Kuleshov", "D. Precup"], "venue": "arXiv preprint arXiv:1402.6028, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A bayesian rule for adaptive control based on causal interventions", "author": ["P.A. Ortega", "D.A. Braun"], "venue": "arXiv preprint arXiv:0911.5104, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Advances in neural information processing systems, 2011, pp. 2249\u2013 2257.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Thompson sampling: An asymptotically optimal finite-time analysis", "author": ["E. Kaufmann", "N. Korda", "R. Munos"], "venue": "International Conference on Algorithmic Learning Theory. Springer, 2012, pp. 199\u2013213.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Simulation-based autonomous systems in discrete and continuous domains", "author": ["L. Belzner"], "venue": "Ph.D. dissertation, LMU, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-entropy temporal logic motion planning", "author": ["S.C. Livingston", "E.M. Wolff", "R.M. Murray"], "venue": "Proceedings of the 18th International Conference on Hybrid Systems: Computation and Control. ACM, 2015, pp. 269\u2013278.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Numerical vs. statistical probabilistic model checking", "author": ["H.L. Younes", "M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "International Journal on Software Tools for Technology Transfer, vol. 8, no. 3, pp. 216\u2013228, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Statistical model checking: An overview", "author": ["A. Legay", "B. Delahaye", "S. Bensalem"], "venue": "International Conference on Runtime Verification. Springer, 2010, pp. 122\u2013135.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalized queries and bayesian statistical model checking in dynamic bayesian networks: Application to personalized medicine", "author": ["C.J. Langmead"], "venue": "2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "A bayesian approach to model checking biological systems", "author": ["S.K. Jha", "E.M. Clarke", "C.J. Langmead", "A. Legay", "A. Platzer", "P. Zuliani"], "venue": "International Conference on Computational Methods in Systems Biology. Springer, 2009, pp. 218\u2013234.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayesian statistical model checking with application to simulink/stateflow verification", "author": ["P. Zuliani", "A. Platzer", "E.M. Clarke"], "venue": "Proceedings of the 13th ACM international conference on Hybrid systems: computation and control. ACM, 2010, pp. 243\u2013252.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Time-adaptive cross entropy planning", "author": ["L. Belzner"], "venue": "Proceedings of the 31st Annual ACM Symposium on Applied Computing. ACM, 2016, pp. 254\u2013259.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Qos-aware multi-armed bandits", "author": ["L. Belzner", "T. Gabor"], "venue": "Foundations and Applications of Self* Systems, IEEE International Workshops on. IEEE, 2016, pp. 118\u2013119.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In many cases, system requirements can be formalized in a bounded temporal logic [1], [2].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "In many cases, system requirements can be formalized in a bounded temporal logic [1], [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "STB is an open loop planner [3], [4]: It does only search the space of plans.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "STB is an open loop planner [3], [4]: It does only search the space of plans.", "startOffset": 33, "endOffset": 36}, {"referenceID": 4, "context": "Exploration of the search space and exploitation of already gathered results are balanced with Thompson sampling [5], [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "Exploration of the search space and exploitation of already gathered results are balanced with Thompson sampling [5], [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "In our setting, open loop planning ([3], [4]) is an approach to find plans that result in satisfaction of a given goal without storing information about the intermediate states that are encountered while executing the plan.", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "In our setting, open loop planning ([3], [4]) is an approach to find plans that result in satisfaction of a given goal without storing information about the intermediate states that are encountered while executing the plan.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "Monte Carlo Tree Search [7], where action selection is typically conditioned by the history of previously encountered states and executed actions.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "In their basic formulation, MABs already provide a clear framework for studying the exploration-exploitation tradeoff inherent to decision making under uncertainty: Should the agent select the arm that previously showed to be most promising? Or should it go on exploring other options? For a recent survey of MAB and its variants, see [8].", "startOffset": 335, "endOffset": 338}, {"referenceID": 4, "context": "It was proposed decades ago [5], but only recently its astonishing effectiveness and generality have been identified [9], [10], [11].", "startOffset": 28, "endOffset": 31}, {"referenceID": 8, "context": "It was proposed decades ago [5], but only recently its astonishing effectiveness and generality have been identified [9], [10], [11].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "It was proposed decades ago [5], but only recently its astonishing effectiveness and generality have been identified [9], [10], [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "It was proposed decades ago [5], but only recently its astonishing effectiveness and generality have been identified [9], [10], [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "CV is suitable to measure the accuracy of a distribution when its mean value changes [12], as is the case in STB.", "startOffset": 85, "endOffset": 89}, {"referenceID": 3, "context": "In particular, Cross Entropy Open Loop Planning is an approach for planning in large-scale continuous MDPs [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 12, "context": "Recently, cross entropy planning has been used for searching sequences that satisfy a given temporal logic formula [13] in a continuous motion planning setting.", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "STB is subtly related to statistical model checking (SMC) [14], [15], and Bayesian statistical model checking in particular [16], [17], [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "STB is subtly related to statistical model checking (SMC) [14], [15], and Bayesian statistical model checking in particular [16], [17], [18].", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "STB is subtly related to statistical model checking (SMC) [14], [15], and Bayesian statistical model checking in particular [16], [17], [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "STB is subtly related to statistical model checking (SMC) [14], [15], and Bayesian statistical model checking in particular [16], [17], [18].", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "STB is subtly related to statistical model checking (SMC) [14], [15], and Bayesian statistical model checking in particular [16], [17], [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "See [19], [12] for previous work of one of the authors on temporal abstraction in open loop planning.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "See [19], [12] for previous work of one of the authors on temporal abstraction in open loop planning.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "[20] for previous work of the authors on QoS-aware sampling in multiarmed bandits.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "We introduce Stacked Thompson Bandits (STB) for efficiently generating plans that are likely to satisfy a given bounded temporal logic requirement. STB uses a simulation for evaluation of plans, and takes a Bayesian approach to using the resulting information to guide its search. In particular, we show that stacking multiarmed bandits and using Thompson sampling to guide the action selection process for each bandit enables STB to generate plans that satisfy requirements with a high probability while only searching a fraction of the search space.", "creator": "LaTeX with hyperref package"}}}