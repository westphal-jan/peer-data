{"id": "1702.08866", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Studying Positive Speech on Twitter", "abstract": "we present results of empirical validation on positive people on twitter. by positive speech we understand speech that works for the betterment serving a given situation, in this case relations between different factions in a conflict - prone country. we worked with autonomous twitter data sets. through semi - manual opinion mining, we found that positive thinking accounted for & lt ; 1 % of sensor data. in fully automated studies, we tested two approaches : unsupervised choice analysis, and supervised voice reinforcement based on distributed word representation. we discuss benefits and challenges of those approaches and report empirical evidence obtained in the study.", "histories": [["v1", "Fri, 24 Feb 2017 21:49:45 GMT  (832kb)", "http://arxiv.org/abs/1702.08866v1", "13 pages, 6 tables"]], "COMMENTS": "13 pages, 6 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marina sokolova", "vera sazonova", "kanyi huang", "rudraneel chakraboty", "stan matwin"], "accepted": false, "id": "1702.08866"}, "pdf": {"name": "1702.08866.pdf", "metadata": {"source": "CRF", "title": "Studying Positive Speech on Twitter", "authors": ["Marina Sokolova", "Vera Sazonova", "Kanyi Huang", "Rudraneel Chakraboty", "Stan Matwin"], "emails": [], "sections": [{"heading": null, "text": "understand speech that works for the betterment of a given situation, in this case relations between different communities in a conflict-prone country. We worked with four Twitter data sets. Through semimanual opinion mining, we found that positive speech accounted for < 1% of the data . In fully automated studies, we tested two approaches: unsupervised statistical analysis, and supervised text classification based on distributed word representation. We discuss benefits and challenges of those approaches and report empirical evidence obtained in the study."}, {"heading": "Introduction", "text": "Dynamic Social Impact Theory shows that overall influence a person experiences from others is a function of the strength, immediacy, and number of communications from other people [7]. Social network Twitter is the fastest growing medium of interpersonal communication. It is visible on public Internet and often cited by other media [24]. Persuasion and social influence that operate through the network affect millions of its users and the general public at large [6, 10, 20]. Social Mining and Social NLP studies became involved in analysis of sentiments and emotions exhibited on Twitter. Sentiments are studied in tweets related to Olympic Games 2010 in Vancouver [10], a major 2012 earthquake in Japan [20], the 2011 Spanish legislative elections, and the 2012 US presidential elections [1]. Positive,\nconsolidation speech is a part of a multi-faceted peace-building process [28]. It contributes to the\nconstruction of a culture of peace to replace a structure of violence [15]. Although the promises of social\nnetworks in peace-building and post-conflict re-consolidations are palpable, considerable research is\nrequired to assess effect of consolidation speech in conflict-stressed environments [3].\nUntil now Text Data Mining community did not pay as much attention to positive speech as to offensive and hate speech. Whereas several types of offensive and hate speech were studied in Sentiment Analysis [17, 21, 26], we could not find Sentiment Analysis/Opinion Mining studies focused on consolidation aspects of speech or near-synonymous concepts. In this paper we report analysis of positive speech and present empirical results obtained on four Twitter data sets. The data has been collected in Kenya through UMATI project 1 . We worked with tweets written in English. Through semi-manual opinion mining, we found that positive speech accounted for < 1% of the data. This severe class imbalance was a major challenge for fully automated studies, as traditional data imbalance methods - under-sampling of majority class and over-sampling of minority class - do not work well with class imbalance of 1/99. Instead, we opted to test two fully automated methods: unsupervised topic allocation by Latent Dirichlet Allocation and supervised text classification based on distributed word representation. The approaches were applied independently. The obtained results of the two approaches collaborate each other. Further in the paper, we revise relevant work, identify the scope of the study, introduce the data sets, and report empirical results that support our approach; discussion concludes the paper.\n1 https://ihub.co.ke/research/projects/23 * This work done when the author was at Dalhousie University. ** This work was done when the author was at University of Ottawa. + Corresponding author: sokolova@uottawa.ca"}, {"heading": "Relevant Work", "text": "By speech we understand the means of communication used by humans to express ideas and thoughts by means of words and lexical constructions [27]. Positive speech works for the process of betterment of a given situation. It is context-dependent. On our case, data is collected from conflict-ridden zones in Kenya, where clashes happen along ethnical/religious lines. In this context, positive speech helps to build bridges between two different population groups or find common grounds between different opinions. Hence, consolidation is an important topic of positive speech. Documentary reports on reconstruction efforts belong to positive speech too. Expressions of sorrow and mourning for all victims represent another major type of positive speech in the given context as well as speech directly countering hate speech. The following tweet exempts are considered positive speech: we need to be united beyond the borders of religion, [XX] has donated Sh3 million for the reconstruction of the Gikomba market Dont celebrate over Makaburi\u2019s death! Be kind ,Life is interesting ,Life is unpredictable!\nThe micro-bloging network Twitter has become a world-wide connector, having 284 million monthly active users and 500 million tweets sent per day 2 . Twitter data have become a fertile ground for many\nresearch projects: advanced search Twitter, sentiment analysis, From 2011 yields > 5,500 hits on Google Scholar. In this section we review several papers that analyze negative and positive polarity of speech on Twitter.\nTwitter users are keen to post negative sentiments and emotions more than they post positive sentiments and emotions [10]. In part, this imbalance can be explained by correlation between social anxiety and negative cognitive bias: people with negative cognitive bias tend to be more socially anxious, and vis-averse [13]. The negative balance can be exaggerated by the fact that Twitter users who tweet about politics tend to have extreme ideological preferences [1]. Offensive speech, i.e., speech containing offensive terms, is a special type of negative speech. Offensiveness of tweets can be estimated with a reasonable error rate of 12.4% obtained by SVM in binary offensive \u2013 non-offensive classification [6]. In this case, a set of non-offensive tweets was a complement of well-defined offensive tweets, i.e., the authors first found offensive tweets by using queries of offensive terms and then built a set of nonoffensive tweets by randomly sampling the remaining tweets; the working assumption was that the remaining tweets have a lower offensiveness score. We, on other hand, build rules to identify positive, consolidation speech. In its extreme version, negative sentiment can deteriorate into hate speech which is often directed against identifiable population groups [17, 27] or\u201ctrigger events\u201dfor hate crime 3 . Sarcasm and irony are often found on Twitter. Their expressions manifest the so-called \u201cbad language\u201d of nonstandard spelling, informal vocabulary, slang (dats why pluto is pluto it can neva b a star) [8] and negate what is literally said in the text [22]. For the purpose of this study, we consider sarcasm and irony to be nullifiers of positive speech.\nBermingham and Smeaton [2] studied Twitter data during the Irish General Election of 2011. Their\nannotation categories consisted of three sentiment classes (positive, negative, mixed) and one non-\nsentiment class (neutral). Tweets deemed un-annotatable, non-relevant, unclear and with contradictory\nannotations were dismissed from the study. From manually annotated 7 203 documents, only 12% of\n2 https://about.twitter.com/company, January 2015 3 http://orca.cf.ac.uk/68385/\ntweets represented positive sentiments. The authors trained Adaboost MNB classifier and deployed it to\nclassify 32,578 relevant tweets. Identified positive and negative shares of the volume were used as\npredictors for the election results. Accuracy of prediction varied widely, depending on the time the data\nwas collected. In most cases, the share of positive volume gave more accurate prediction than the negative\nvolume. Interestingly, the manually labeled data had worse predictive power than the classified data.\nBollen et al [5] used OpinionFinder and Google Profile of Mood States (GPOMS) to assess predictive\npower of Twitter data in stock exchange. Publicly available OpinionFinder identifies subjective sentences\nand positive and negative sentiment expressions, as well as sources of opinion, direct subjective expressions and speech events 4 . GPOMS, built by Bollen et al, measures mood through six dimensions:\nCalm, Alert, Sure, Vital, Kind and Happy. The dimensions were derived from a rating scale applied to\nassess multiple dimensions of affect [19]. The authors showed that positivity and Calm better predicted\npolitical events and changes in stock market than other variables, although considerable controversy was raised about significance of the results 5 . Empirical evidence shows that messages rebuking hate speech\n(counter-hate speech) have been posted on Twitter and, albeit very rare, helped to alter aggressive behavior 6 .Although counter-hate speech is promoted as a potent tool of betterment of relations between\nindividuals and groups, there are no reports of studies of counter-hate phenomenon done on massive\nTwitter data. Thus, our current results can help other researcher in searching for positive speech in large\nvolumes of Twitter data."}, {"heading": "Semantic Analysis of Positive Speech", "text": "We work with four Twitter data sets collected in Kenya by UMATI project. Each data sets was collected\nfollowing a violent episode: a grenade attack on a popular Gikomba market (set 1 \u2013 482 tweets), clashes\nbetween ethnical/religious groups in Mandera (set 2 \u2013 1156 tweets), assassination of a Muslim cleric\nMakaburi (set 3\u2013 20,300 tweets), and clashes between ethnical/religious groups in Mpeketoni (set 4 \u2013\n106,300 tweets). Our first task was to detect and extract tweets written in English. We used LingPipe\u2019s Language Identification 7 and Apache\u2019s LangDetect 8 for this task. Our next task was to find unique tweets.\n(We did not know why those multiple tweets appeared in the data nor we knew representativeness of such\nrepetitions, e.g., were all re-tweets presented or only part of them.) The two operations reduced the data\nsets as follow: set 1 \u2013 365 tweets, set 2 \u2013 655 tweets, set 3 \u20138,900 tweets, and set 4 \u2013 41,800 tweets. In\nsemantic analysis of the data, we had to decide on four factors: choice of sentiment categories,\nsentiment sources in data, sentiment assessment, including interpretation of sarcasm and irony, and\nsentiment composition.\n1. Choice of sentiment categories depends on the goal of sentiment analysis. Sentiments could be polar, e.g., positive and negative in studies of reactions on sport events [10], or multi-categorical, e.g.,\nconfusion, gratitude, encouragement in studies of health-related messages [4]. We differentiate\n4 http://mpqa.cs.pitt.edu/opinionfinder/ 5 https://folpmers.wordpress.com/2014/04/17/the-twitter-predictor-of-the-dow-the-rise-and-fall/ 6 http://www.ethanzuckerman.com/blog/2014/03/25/susan-benesch-on-dangerous-speech-and counterspeech/ 7 http://alias-i.com/lingpipe/index.html 8 https://stanbol.apache.org/docs/trunk/components/enhancer/engines/langdetectengine\nbetween peace-building sentiments (e.g., unity, peace, reconstruction) and support for violence (e.g, anger, intolerance). Peace-building sentiments are the ones that identify positive consolidation speech. Sentiments can be gauged from different parts of tweets: text, URLs, emoticons, hashtags. In some cases tweets are assigned with sentiments of emoticons and hash-tags [16], in others the message content provides the sentiment label [14]. We excluded URLs from the scope of the current study due to uncertainty of their collection. The data did not contain emoticons. Analysis of the data has shown that hashtags were an unreliable source in sentiment identification: in some tweets, hashtags with strong positive sentiments were supported by positive text. In other tweets sentiments were of opposite polarity. Thus, we chose to treat hashtags as regular text. 2. Assessment of sentiments in the message content can be done in several ways. Some apply a composite score of positive and negative sentiments [10], whereas others label text with predominant\nsentiments [14]. We assess each tweet for presence of positive and negative sentiments.\n3. Sentiment composition establishes rules for derivation of sentiments in compound phrases and lexical\nstructures. Some systems consider construction negative + negative to be positive, e.g., kill\nbacteria is labeled positive by TheySay [18]. We, on other hand, mark negative + negative as negative.\n4. Interpretation of irony and sarcasm depends on topics and subjects of those expressions [22]. We adhere to the notion that sarcasm and irony negate the consolidation aspect of speech; in fact, they\ncan re-make consolidation content into offensive, e.g., adding HaHa and Lol to R.I.P. (rest in peace) in response to killing of a political figure.\nWe hypothesized that examples of positive speech can be very rare, if exist at all. Manual analysis is the\nmost precise tool for extraction and analysis of rare events in text. However, sheer volume of Twitter data\nprohibits fully manual analysis. We applied semantic semi-manual analysis instead. For each set, we\nidentified content-dependent seed keywords (e.g., pray, peace) and used them in bootstrapping to extract\nmore characteristics of positive speech from the remaining data (e.g., dont celebrate, condolences). We\nwanted to evaluate whether WordNet and SentiWordNet can be used to generalize on the found\nkeywords. Unfortunately, the volume of positive speech was too negligent to assess those resources. In\nterms of speech extraction, we aim to achieve a strong confidence that the identified speech is indeed\npositive consolidation speech. For this purpose, we seek to populate the target class (i.e., positive speech)\nwith strong examples. Consequently, we accept that the target class misses some of weaker, albeit\nrelevant, examples. In other words, we aim for a higher Precision, even at expense of a lower Recall.\nOur semantic analysis was event-based. Each data set discussed one event, and those events begged\ndifferent reactions from Twitter users. Thus, we studied sentiment categories individually for each set.\nStrength of the speech and ranking of tweets were outside the scope of this study. Thus, when multiple\nseed words were present in the same text, this did not affect the resulting label.\nGikomba set covered blasts on Gikomba market, a popular business hub of second-hand clothes. From 365 tweets, most tweets conveyed factual information (e.g., blasts, victims); many users showed frustration toward security incompetence and government impotence. We did not find examples of hate or antagonistic speech, only a few tweets allured to a possible terrorist attack. At the same time, not much compassion was expressed either. In this set, we found only three examples of positive speech (<1.0% of the set). Those tweets spoke about reconstruction of the area.\nMandera set was collected in the wake of ethnic and religious clashes in Mandera. In 655 tweets, most tweets reported on inter-clan clashes, and considerable text volume conveyed dissatisfaction with\ngovernment actions or accused it in escalating the conflict. We found 16 examples of positive speech (< 2.0% of the set). In this case, positive speech called for unity and peace.\nMakaburi set consisted of 8,900 tweets commenting on a public assassination of a Muslim cleric Makaburi. Those tweets were strongly separated along ethnic/religious lines. Texts were stronger worded and higher in polarity if compared with the other sets. First, we studied a small sample of data. Majority of tweets belonged to factual type. Aside from them, we found following emotions: considerable anger and appreciation of the killing, including sarcastic tweets; mourning of the killed cleric. Positive speech contained calls for the unity of the nation as a whole, a call for prayer, and a few counter-hate tweets. We selected seed words [peace, bless, unite, pray, protect, love, support] and used boostrapping to search the whole set to identify more words and some phrases characteristic of positive speech. After rigorous analysis of extracted tweets, we have been able to identify 59 examples of positive speech (< 0.7% of the set).\nMpeketoni set, with 41,800 tweets, has been the largest data by far. Tweets were about a series of Mpeketoni attacks, blamed on Al Shabaab (Islamist militia) and the area\u2019s proximity to Somali. Aside of neutral and factual tweets, we identified the following categories of tweets: frustration with cycles of violence; blame of government, including sarcastic tweets; accusation and anger. Positive speech had calls for unity, calls for law and order, calls for prayer, and a few reconstruction and counter-hate tweets. We identified positive seed words [peace, pray, unite, condolence, god, condemn, thoughts, reclaim, one ] and used them to find other words and phrases which appear in possibly positive tweets. After 2nd round of extracting tweets and their manual analysis, we found 320 examples of positive speech ( < 0.8% of the sets)."}, {"heading": "Topic Modeling by Latent Dirichlet Allocation", "text": "Positive consolidation speech is shown to be most effective during a short window frame or operating in\ncertain localities [15,28]. Identifying it within a short time-frame in a noisy Twitter environment can be a\ndaunting task. A few topic-based models have exhibited advanced results. In these approaches a group of\ntopics, is learned from the raw (ex. Bag-of-words) representation of the text. The document is then\nrepresented by the projection into the topic space. These approaches include Latent Semantic Indexing\n(LSI) and its probabilistic version, probabilistic Latent Semantic Indexing. A more comprehensive\napproach is based on the Latent Dirichlet Allocation (LDA) model [12].\nLDA is an unsupervised Machine Learning technique that finds latent topics in large collection of\ndocuments. Each document is represented as a probability distribution \u03b8 over some topics, while each\ntopic is represented as a probability distribution \u03c6 over a number of words; \u03b8 and \u03c6 are assumed to have\nDirichlet prior. For each document, LDA picks a topic from \u03b8, samples a word from the distribution over\nthe words associated with the chosen topic, repeats the process for all the words in the document [12].\nThe method has been successfully used in topic identification in Twitter, including sentiment analysis [9].\nTo output meaningful results, the method requires considerable tuning, e.g., selection of the number of\ntopics, data pre-processing. We tested removal of stop-words, but decided against it after topic quality\nremarkably decreased. For each data set, we worked with 5,10,15,20, and 25 topics. For the largest\nMpeketoni set, we additionally worked with 35, 45 and 50 topics. From Text Data Mining perspective,\nLDA results were reliable and representative of the data sets contents. For all the data sets, LDA built\nseveral coherent topics that answered \u201cwhat\u201d, \u201cwhere\u201d and/or \u201cwho\u201d questions related to the main content\nof the set.\nOn each set, LDA required different # of topics to produce the most logical and articulated topics: Gikomba set: the most descriptive and informative topics were built with N= 10, 15. Individual topics became less informative, e.g., omitted location or the event, and more fragmented for N = 20, 25. Example of a topic: the, of, Gikomba, market, http:, Nairobi, by, at, blasts, fire, has, victims, area, RT, The, for\nMandera set: LDA with N = 10, 15 produced better results. Again, individual topics became less\ninformative with N = 20,25. Example of a topic: in, #Mandera, RT, as, #Wajir, militia, clan, are, not,\nthat, tribal, why, clashes, there, people , their, armed, war, were\nMakaburi set: N = 10, 15, 20 helped to build most descriptive and interesting topics. LDA performance\ndeclined for N = 25. Example of a topic: Makaburi, shot, in, dead, cleric, Abubakar, Muslim, Sheikh,\nMombasa, RT, has, alias, Shariff, Radical, been, #Makaburi, radical, is, dead., muslim\nMpeketoni set: it was the largest set. We obtained coherent topics with N = 35, 45, 50. For smaller N = 5,\n..., 25, the resulting topics omitted part of essential information. Example of a topic: in, Mpeketoni,\nattack, Kenya, people, 48, of, town, killed, on, #Kenya, Kenyan, least, at, #Mpeketoni, near, coastal, dead,\n50\nLDA also annotated every tweet with words that can be found in topics and the corresponding topic\nnumber. That made analysis of the data feasible. For example, we were interested in tweets which say\nabout victims of the Gikomba explosions. After a straight-forward search, we found victims(6) in tweets #\n12, 97, 137, etc. A critical advantage of LDA was its ability to find topics supported by a small portion of\ntext and in considerable semantic distance from the mainstream topics (i.e., topics- outliers).\nIn data sets Mandera and Mpeketoni, LDA was able to identify topics of peace (and, wajir, mandera, of,\nshould, be, peace, both, security, we, leaders, solution, must, @jageyo, communities, that, who,\n#KTNBottomline, well, In) and condolences (the, to, of, in, and, those, My, who, their, lost, families,\ncondolences, all, #MpeketoniAttack, God, my, for, victims, affected, May) despite small volumes of those\ntexts. In Makaburi set, LDA was able to identify sarcasm as a separate topic (RT, Makaburi, MAKABURI,\nR.I.P., #MAKABURI, RIP, A, Man, The, Al, (cont), Lol, ?, Good, IS, TO). This is remarkable\nachievement, given that sarcastic tweets appeared extremely rare in the data. LDA, however, was able to\npick up self-contradictory combinations of R.I.P, lol, good in those tweets. After analysis of the built\ntopics, we again were able to identify tweets that deliver these messages. For example, in Mandera set,\npeace(7) appeared in tweets # 20, 35, 49, 210, etc.\nThe output topics summarized main content in four separate Twitter data. On three data sets, the output\nincluded topics presented by a small number of text (<0.7%). Note that LDA obtained those results on\ndata that exhibited characteristics of online \u201c bad language\u201d: slang, spelling and grammatical variations,\nshort-comings [8]. Critically, LDA\u2019s analysis of tweets supports and independently validates the results\nobtained by semantic analysis."}, {"heading": "Text Representation: Word Vectors vs N-Grams", "text": "Bag-of-words (BOW) is a basic, classic model for text representation. In it, a document is represented by\na vector of its words\u2019 frequencies normalized over a set of documents. Despite of its simplicity this representation is still widely used for a plethora of different tasks (classification, information retrieval and others) and has been proved very successful. The two biggest drawbacks of this model are its extreme sparseness, particularly for short text such as tweets, and its lack of a representation for relationship between words. An N-gram model, is a natural extension of the BOW. It is based on the fact that the presence of a sequence of N words, an N-gram, does not necessarily carry the same semantic weight as the presence of its constituents. This model represents a document not only by the frequency of its words, but also by the frequencies of its N-grams, up to a certain value of n. In practice values of n=2, or n=3 are used. These models have been used for many years, and have shown great results, despite the fact that\nthey suffer ever more from the extreme sparseness problem of the BOW model. In our study, we\ncompare a) BOW or 1-gram, represents the document by the normalized frequencies of the document\nwords, with stop words removed; b) 2,3-grams models represent the document by the normalized\nfrequencies of the document words and 2-word phrases (or 2-word and 3-word phrases) which are formed\nwithout removing the stop words; c) NB-SVM is the exact algorithm presented in [25] which by\ncombining the generative and discriminative classifiers provides, in resume, an Support Vector Machine.\nRecently there have been developments in the distributed word representation. These representations are\nlearned from a text corpus by a neural network model whose goal is to predict the next word given a\ncertain context. Words, similar both grammatically and semantically, tend to appear in similar context and\nthus get assigned similar co- appearance coefficients. The resulting word vectors can be considered as a\nmapping of a word into an N-dimensional space, where linguistic regularities can be observed. Such a\nrepresentation is a perfect illustration of a famous quote \u201cYou shall know a word by the company it keeps\n(Firth, J. R. 1957:11)\u201d. Since their introduction, these models have been applied to a multitude of\napplications, including text classification [29,30].\nIn classification experiments, we used original data sets, without extracting English tweets first. We chose to preserve as much of the original information in the tweet as possible and kept the pre-processing to a minimum: the links replaced by a tag URL, all usernames were replaced by a tag USER and all \"#\" were stripped from the hashtags. Punctuation was separated from words and kept. Finally, the text was converted to lower case. We used an off-shell python implementation of the skip-gram model provided by the gensim package 9 . Training parameters were crucial in the applicability of the resulting model. These were found manually using words association (see next section) as feed-back. For both types of datasets, the best parameters were: no negative sampling, a sampling threshold of 0.001 and a min count of 1.\nDistributed word models have been so far obtained on rather large datasets. In this work, we are dealing with datasets that are as small as 1000 samples (Gikomba set), so the pertinence of the built model has to be verified prior to its use in a data mining task. We performed this verification manually, using the fact that the model places semantically similar words close in the embedded space, allowing us to probe the model locally by constructing word associations for a few query words and to check that the surrounding embeddings are meaningful. If successful, we probe the model globally by running a clustering algorithm on the word embeddings and verifying the relevancy of the resulting clusters. Clusters from Mpeketoni sets include: hanging handling wednesday ruthless golden hurt 98 pursue masterminds, educate whaaaat cow veteran resettle mau landless 1960 settlemen10\n9 http://is.muni.cz/publication/884893/en 10 Reference to re-settlement of Kenyan tribes in 1960-ies.\nWe query the model with a word or an ensemble of words that would represent a topic in the dataset and check the near-by embeddings, by looking at the most similar words in the model according to cosine similarity. For example for Mpeketoni sets - a dataset dealing with a bombing likely by a terrorist organization Al Shabaab - a query on \"Al-Shabaab\" would be meaningful. The 10 most similar words to it were: claim, attacks, fake, shabab, discussion, twitter, responsibility, rejects, na\u00efve. As we see, the response contains words relating to attack, responsibility, and the name of the group. These words are relevant and we can conclude that the model did capture the semantics of the corpus.\nTo further test the quality of the model we performed Dirichlet Process Gaussian Mixture Model (DPGMM) based clustering on the embedded vocabulary, with the expectation that semantically or grammatically similar words would form some meaningful clusters. Gaussian Mixture Models (GMM) have already been used in text classification based on distributed vector models with very good results [29]. We used the DPGMM model from the scikit-learn toolkit 11 with the diagonal covariance and a maximum of thirty clusters. To speed up the process, we only used 9000 mediumfrequency words from the total corpus vocabulary.\nThe word vector model provides us with a way of representing individual words, but these individual vectors need to be combined to produce a text vector of a constant length for the entire document. In other words, a proper pooling method of representing a multi-set of word vectors into a single high dimensional vector needs to be found. Common techniques concentrate on both representation of the centroid (i.e., mean-vector pooling) and on distribution of word vectors. This has been achieved by means of a class specific or global Gaussian Mixture Models (GMM)[29]. Our approach differs from the above in that we do not fit a distribution model to the multi-set of word vectors, but rather employ statistical analysis measures such as averaging and standard deviation to describe this ensemble and its transformations.\nLet the matrix M denote a multi-set of word vectors, where the rows, Mi , represent the words of the tweet in the order they appear and columns the dimensions in the embedded space. For a tweet of length N and a d -dimensional model, M is a N\u00d7 d matrix. The rows in this matrix define an ordered set of points, a path, in the d-dimensional space. For this multi-set we can easily define a mean \u03bc and a standard deviation \u03c3 as a standard deviation of the ensemble of vectors, Mi (rows), constituting the set M:\n\u2211\n, \u221a \u2211\nThese two features \u00b5, describe the centroid and the spread of the set of points in the d-dimensional space defined by the word vectors."}, {"heading": "Data Classification", "text": "Our classification experiments consisted of 5 repetitions. In each repetition a word vector model was\ntrained on all of the data. The full dataset would then be vectored using some of the features described in\nabove. The resulting matrix was split into a training and testing sets according to a 5-fold cross validation\nand passed to a linear regression classifier (LR) and a support vector machine (SVM). The resulting\naccuracy, precision, recall and the f-score for the minority class were all recorded for each of the 25 total\nruns. The experiment was repeated for different sets of features and different datasets. We used the classification framework from scikit-learn toolkit 11 .\n11\nhttp://scikit-learn.org/stable/\nDue to the nature of the Kenyan datasets, positive speech is extremely rare (as low as a few percent of the data) and thus finding such tweets presents a challenge as much for a human as for a classifier. Before venturing into a full study, we looked closely at the results of the classification of our largest in- house labelled Mpeketoni set, using \u03bc,\u03c3 features. The results were: 538 true positives, and 301 false positives. Upon manual inspection of the 301 false positives we discovered that actually 210 of them were indeed real positives. We then implemented an iterative procedure, in which we inspected and relabelled the incorrect false positives found by the classification algorithm. With this simple procedure, in only four iterations we increased the number of positively labelled instances by almost 50%, 965 positive tweets in total. Some of the missed tweets were the incorrectly labelled re-tweets of correctly labelled messages. However there were equally many new original examples of positives phrases missed by the human and identified by the algorithm: In the end we all Africans..let the Games Roarr.#TeamAfrica, My God of peace and mercy comfort the people of mpeketoni\nOn the next step, we launched an investigation of the effect of different features on classification results. We pay a special attention to training time of the word-to-vector models, as this can critically influence their applicability - see Table 1. We repost classification F-score in Table 2; Table 3 reports on time spent on classification. Empirical evidence showed that SVM consistently obtained a better accuracy on N-gram based features than on word-embedding features; LR obtained better accuracy on distributed word representations. As we expected, time efficiency favored 1-gram text representation."}, {"heading": "Classification of Publicly Available Data", "text": "We replicated our experiments on publicly available Twitter data. The dataset 12 contains 1.6 million\nemoticons-containing tweets on a variety of subjects from gadgets, products, people and events. The\ntweets were labelled according to positive, negative or neutral sentiment conveyed by the emoticons;\nemoticons were later stripped off the tweet. A small, hand-labelled test set was provided. We randomly\nextract subsets of 0.1% (1,600 tweets), 1% (16,000) and 10% (160,000) from the full dataset and run the\nclassification experiment in the same manner as described above. The random sampling of tweets for\nlabelled dataset was done with care to preserve the original balance of the data (50 / 50). The qualitative\ndependence of the f-score\u2019s behaviour for various feature set for different training corpus sizes is similar\nto that found in Kenyan data. For 0.1% of tweets the mean-vector is the best, similar to Mandera. For 1%\ndataset, roughly of the same size as the Makaburi dataset, all features in the table are informative, but the\nf-score increase is small. For 10% of the data - the same size as the Mpeketoni dataset, the effect of\ncomplex features increasing the classification is clearly visible, as it is for the full dataset. Tables 4- 6\nreport the results.\nThe goal of this study was to investigate the use of novel text representation techniques in classification\nof Twitter data sets; the data is challenging for automated analysis. As such we did not make any effort in\noptimizing the classification results per se, and reported the results for only one end-classifier (LR). We obtain very similar dependencies on all mode parameters with other base classifiers (SVM) 13 , however the\nactual numbers for f-score differ. Nonetheless in order to compare our results with the previous work on\nthe sentiment dataset we performed experiments in the similar manner: we trained our model on the full\ndataset, and the tested it on the test set provided. We observed accuracies ranging from 81% to 83%\nconsistent with that previously reported."}, {"heading": "Discussion and Future Work", "text": "In this study, we presented a method for analysis of positive speech on Twitter. Albeit occurring on Twitter, positive speech is severely under-represented when the data is collected from conflict zones (e.g., < 1% of collected data). We have shown that despite the severe imbalance, automated methods are capable of identification and classification of positive speech. We have shown that LDA can successfully identify main topics in each data (i.e., factual information), as well as find and extract topics supported by a small number of tweets (i.e., positive speech, sarcasm). We have compared N-gram based text representation with a novel technique of representing tweets as a vector based on distributed word models. In our experiments, N-grams outperformed distributed word models. To see the benefit from the distributed word models, datasets on the order of 10 5 samples are required. There is also interplay between the size of labeled data and dimensionality of the word representation; increasing one or the other leads to a better classification results, however, increasing both is not necessary. In the future we would like to apply this technique for severely imbalanced data classification and rare event detection in short texts, as well as use this representation for texts visualization.\nIn study of positive speech on Twitter, we see following directions of future work: a) test other methods of topic identification in Twitter data, for example, non-negative matrix factorization which is able to find topics-outliers in Twitter data [23]; b) use multiple-view learning [11] to analyze positive and negative speech in Twitter data; c) apply fine-grained semantic analysis of counter- hate speech."}, {"heading": "Acknowledgements", "text": "We thank UMATI project providing the data sets. This study was in part supported through NSERC\nDiscovery grant available, NSERC CREATE grant and Polish National Scientific Centre NCN grant."}], "references": [{"title": "On using twitter to monitor political sentiment and predict election results", "author": ["A. Bermingham", "A. Smeaton"], "venue": "In Proceedings of SAAIP", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Peacebuilding in a networked world", "author": ["M.L. Best"], "venue": "Commun. ACM, 56(4):30\u201332,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Recognition of sentiment sequences in online discussions", "author": ["V. Bobicev", "M. Sokolova", "M. Oakes"], "venue": "Proceedings of SocialNLP 2014, pages 44 \u2013 49,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Twitter mood predicts the stock market", "author": ["J. Bollen", "H. Mao", "X. Zeng"], "venue": "Journal of Computational  Science , 2(1):1\u20138,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Content+context=classification: Examining the roles of social interactions and linguist content in twitter user classification", "author": ["W. Campbell", "E. Baseman", "K. Greenfield"], "venue": "Proceedings of SocialNLP 2014 ,pages 59 \u2013 65,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "How communication shapes culture", "author": ["L. Conway", "M. Schaller"], "venue": "Social Communication", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "What to do about bad language on the internet", "author": ["J. Eisenstein"], "venue": "Proceedings of NAACL-HLT, pages 359 \u2013 369,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature based sentiment analysis of tweets in multiple languages", "author": ["M. Erdmann", "K. Ikeda", "H. Ishizaki", "G. Hattori", "Y. Takishima"], "venue": "Web Information Systems Engineering\u2013WISE 2014 , pages 109\u2013124. Springer,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Is happiness contagious online? a case of twitter and the 2010 winter olympic", "author": ["A. Gruzd", "S. Doiron", "P. Mai"], "venue": "System Sciences (HICSS) , pages 1\u2013 9. IEEE,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Comment-based multi-view clustering of web 2.0 items", "author": ["X. He", "M.-Y. Kan", "P. Xie", "X. Chen"], "venue": "In Proceedings of the 23rd WWW Conference ,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Empirical study of topic modeling in twitter", "author": ["L. Hong", "B.D. Davison"], "venue": "Proceedings of the First Workshop on Social Media Analytics, pages 80\u201388. ACM,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Bis, bas, and bias: The role of personality and cognitive bias in social anxiety", "author": ["N.A. Kimbrel", "R.O. Nelson-Gray", "J.T. Mitchell"], "venue": "Personality and Individual Differences , 52:395\u2013400,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Sentiment analysis of short informal texts", "author": ["S. Kiritchenko", "X. Zhu", "S.M. Mohammad"], "venue": "Journal of Artificial Intelligence Research , 50:723\u2013762,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluating recent trends in peacebuilding research", "author": ["W.A. Knight"], "venue": "International Relations of the Asia-Pacific , 3(2):241\u2013264,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Twitter sentiment analysis: The good the bad and the omg! In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media, pages 538\u2013 541", "author": ["E. Kouloumpis", "T. Wilson", "J. Moore"], "venue": "AAAI,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Locate the hate: Detecting tweets against blacks", "author": ["I. Kwok", "Y. Wang"], "venue": "Proceedings of AAAI , pages 1621\u20131622. AAAI,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Sentiment composition", "author": ["K. Moilanen", "S. Pulman"], "venue": "Proceedings of Recent Advances in Natural Language Processing , pages 378\u2013382,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Factor structure of the profile of mood states (poms): two partial replications", "author": ["J. Norcross", "E. Guadagnoli", "J. Prochaska"], "venue": "Journal of Clinical Psychology,40(5):1270\u20131277,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1984}, {"title": "Exploiting community emotion for microblog event detection", "author": ["G. Ou", "W. Chen", "T. Wang", "Z. Wei", "B. Li", "D. Yang", "K.-F. Wong"], "venue": "Proceedings of EMNLP 2014, pages 1159\u20131168,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Offensive language detection using multi-level classification", "author": ["A. Razavi", "D. Inkpen", "S. Urisky", "S. Matwin"], "venue": "Advances in Artificial Intelligence, pages 16\u201327. Springer,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Socialirony", "author": ["P. Rosso"], "venue": "Proceedings of SocialNLP 2014 , page Invited talk,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Detecting anomalies in microblogging via nonnegative matrix tri-factorization", "author": ["G. Shen", "W. Yang", "W. Wang", "M. Yu", "G. Dong"], "venue": "Social Media Processing, pages 55\u201366. Springer,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Big questions for social media big data: Representativeness, validity and other methodological pitfalls", "author": ["Z. Tufekci"], "venue": "ICWSM,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL \u201912", "author": ["S. Wang", "C. Manning"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Detecting hate speech on the world wide web", "author": ["W. Warner", "J. Hirschberg"], "venue": "Proc. of the 2012 Workshop on LSM, pages 19 \u2013 26,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Pragmatics of human communication: A study of interactional patterns, pathologies and paradoxes", "author": ["P. Watzlawick", "J.B. Bavelas", "D.D. Jackson"], "venue": "WW Norton and Company,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Peacebuilding in Postconflict Societies: Strategy and Process", "author": ["H. won Jeong"], "venue": "Lynne Rienner Publishers,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "Document classification with distributions of word vectors", "author": ["C. Xing", "D. Wang", "X. Zhang", "C. Liu"], "venue": "In Asia-Pacific Signal and Information Processing Association,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Learning composition models for phrase embeddings", "author": ["M. Yu", "M. Dredze"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Dynamic Social Impact Theory shows that overall influence a person experiences from others is a function of the strength, immediacy, and number of communications from other people [7].", "startOffset": 180, "endOffset": 183}, {"referenceID": 22, "context": "It is visible on public Internet and often cited by other media [24].", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "Persuasion and social influence that operate through the network affect millions of its users and the general public at large [6, 10, 20].", "startOffset": 126, "endOffset": 137}, {"referenceID": 8, "context": "Persuasion and social influence that operate through the network affect millions of its users and the general public at large [6, 10, 20].", "startOffset": 126, "endOffset": 137}, {"referenceID": 18, "context": "Persuasion and social influence that operate through the network affect millions of its users and the general public at large [6, 10, 20].", "startOffset": 126, "endOffset": 137}, {"referenceID": 8, "context": "Sentiments are studied in tweets related to Olympic Games 2010 in Vancouver [10], a major 2012 earthquake in Japan [20], the 2011 Spanish legislative elections, and the 2012 US presidential elections [1].", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "Sentiments are studied in tweets related to Olympic Games 2010 in Vancouver [10], a major 2012 earthquake in Japan [20], the 2011 Spanish legislative elections, and the 2012 US presidential elections [1].", "startOffset": 115, "endOffset": 119}, {"referenceID": 26, "context": "Positive, consolidation speech is a part of a multi-faceted peace-building process [28].", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "construction of a culture of peace to replace a structure of violence [15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "Although the promises of social networks in peace-building and post-conflict re-consolidations are palpable, considerable research is required to assess effect of consolidation speech in conflict-stressed environments [3].", "startOffset": 218, "endOffset": 221}, {"referenceID": 15, "context": "Whereas several types of offensive and hate speech were studied in Sentiment Analysis [17, 21, 26], we could not find Sentiment Analysis/Opinion Mining studies focused on consolidation aspects of speech or near-synonymous concepts.", "startOffset": 86, "endOffset": 98}, {"referenceID": 19, "context": "Whereas several types of offensive and hate speech were studied in Sentiment Analysis [17, 21, 26], we could not find Sentiment Analysis/Opinion Mining studies focused on consolidation aspects of speech or near-synonymous concepts.", "startOffset": 86, "endOffset": 98}, {"referenceID": 24, "context": "Whereas several types of offensive and hate speech were studied in Sentiment Analysis [17, 21, 26], we could not find Sentiment Analysis/Opinion Mining studies focused on consolidation aspects of speech or near-synonymous concepts.", "startOffset": 86, "endOffset": 98}, {"referenceID": 25, "context": "By speech we understand the means of communication used by humans to express ideas and thoughts by means of words and lexical constructions [27].", "startOffset": 140, "endOffset": 144}, {"referenceID": 8, "context": "Twitter users are keen to post negative sentiments and emotions more than they post positive sentiments and emotions [10].", "startOffset": 117, "endOffset": 121}, {"referenceID": 11, "context": "In part, this imbalance can be explained by correlation between social anxiety and negative cognitive bias: people with negative cognitive bias tend to be more socially anxious, and vis-averse [13].", "startOffset": 193, "endOffset": 197}, {"referenceID": 4, "context": "4% obtained by SVM in binary offensive \u2013 non-offensive classification [6].", "startOffset": 70, "endOffset": 73}, {"referenceID": 15, "context": "In its extreme version, negative sentiment can deteriorate into hate speech which is often directed against identifiable population groups [17, 27] or\u201ctrigger events\u201dfor hate crime 3 .", "startOffset": 139, "endOffset": 147}, {"referenceID": 25, "context": "In its extreme version, negative sentiment can deteriorate into hate speech which is often directed against identifiable population groups [17, 27] or\u201ctrigger events\u201dfor hate crime 3 .", "startOffset": 139, "endOffset": 147}, {"referenceID": 6, "context": "Their expressions manifest the so-called \u201cbad language\u201d of nonstandard spelling, informal vocabulary, slang (dats why pluto is pluto it can neva b a star) [8] and negate what is literally said in the text [22].", "startOffset": 155, "endOffset": 158}, {"referenceID": 20, "context": "Their expressions manifest the so-called \u201cbad language\u201d of nonstandard spelling, informal vocabulary, slang (dats why pluto is pluto it can neva b a star) [8] and negate what is literally said in the text [22].", "startOffset": 205, "endOffset": 209}, {"referenceID": 0, "context": "Bermingham and Smeaton [2] studied Twitter data during the Irish General Election of 2011.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "Bollen et al [5] used OpinionFinder and Google Profile of Mood States (GPOMS) to assess predictive power of Twitter data in stock exchange.", "startOffset": 13, "endOffset": 16}, {"referenceID": 17, "context": "The dimensions were derived from a rating scale applied to assess multiple dimensions of affect [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 8, "context": ", positive and negative in studies of reactions on sport events [10], or multi-categorical, e.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": ", confusion, gratitude, encouragement in studies of health-related messages [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 14, "context": "In some cases tweets are assigned with sentiments of emoticons and hash-tags [16], in others the message content provides the sentiment label [14].", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "In some cases tweets are assigned with sentiments of emoticons and hash-tags [16], in others the message content provides the sentiment label [14].", "startOffset": 142, "endOffset": 146}, {"referenceID": 8, "context": "Some apply a composite score of positive and negative sentiments [10], whereas others label text with predominant sentiments [14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 12, "context": "Some apply a composite score of positive and negative sentiments [10], whereas others label text with predominant sentiments [14].", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": ", kill bacteria is labeled positive by TheySay [18].", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "Interpretation of irony and sarcasm depends on topics and subjects of those expressions [22].", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "Positive consolidation speech is shown to be most effective during a short window frame or operating in certain localities [15,28].", "startOffset": 123, "endOffset": 130}, {"referenceID": 26, "context": "Positive consolidation speech is shown to be most effective during a short window frame or operating in certain localities [15,28].", "startOffset": 123, "endOffset": 130}, {"referenceID": 10, "context": "A more comprehensive approach is based on the Latent Dirichlet Allocation (LDA) model [12].", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "the words associated with the chosen topic, repeats the process for all the words in the document [12].", "startOffset": 98, "endOffset": 102}, {"referenceID": 7, "context": "The method has been successfully used in topic identification in Twitter, including sentiment analysis [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "Note that LDA obtained those results on data that exhibited characteristics of online \u201c bad language\u201d: slang, spelling and grammatical variations, short-comings [8].", "startOffset": 161, "endOffset": 164}, {"referenceID": 23, "context": "without removing the stop words; c) NB-SVM is the exact algorithm presented in [25] which by combining the generative and discriminative classifiers provides, in resume, an Support Vector Machine.", "startOffset": 79, "endOffset": 83}, {"referenceID": 27, "context": "Since their introduction, these models have been applied to a multitude of applications, including text classification [29,30].", "startOffset": 119, "endOffset": 126}, {"referenceID": 28, "context": "Since their introduction, these models have been applied to a multitude of applications, including text classification [29,30].", "startOffset": 119, "endOffset": 126}, {"referenceID": 27, "context": "Gaussian Mixture Models (GMM) have already been used in text classification based on distributed vector models with very good results [29].", "startOffset": 134, "endOffset": 138}, {"referenceID": 27, "context": "This has been achieved by means of a class specific or global Gaussian Mixture Models (GMM)[29].", "startOffset": 91, "endOffset": 95}, {"referenceID": 21, "context": "In study of positive speech on Twitter, we see following directions of future work: a) test other methods of topic identification in Twitter data, for example, non-negative matrix factorization which is able to find topics-outliers in Twitter data [23]; b) use multiple-view learning [11] to analyze positive and negative speech in Twitter data; c) apply fine-grained semantic analysis of counter- hate speech.", "startOffset": 248, "endOffset": 252}, {"referenceID": 9, "context": "In study of positive speech on Twitter, we see following directions of future work: a) test other methods of topic identification in Twitter data, for example, non-negative matrix factorization which is able to find topics-outliers in Twitter data [23]; b) use multiple-view learning [11] to analyze positive and negative speech in Twitter data; c) apply fine-grained semantic analysis of counter- hate speech.", "startOffset": 284, "endOffset": 288}], "year": 2017, "abstractText": "We present results of empirical studies on positive speech on Twitter. By positive speech we understand speech that works for the betterment of a given situation, in this case relations between different communities in a conflict-prone country. We worked with four Twitter data sets. Through semimanual opinion mining, we found that positive speech accounted for < 1% of the data . In fully automated studies, we tested two approaches: unsupervised statistical analysis, and supervised text classification based on distributed word representation. We discuss benefits and challenges of those approaches and report empirical evidence obtained in the study.", "creator": "Microsoft\u00ae Word 2010"}}}