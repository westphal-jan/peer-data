{"id": "1702.08887", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning", "abstract": "many real - world problems, such as network item routing and urban cache control, are naturally arising as multi - agent inverse learning ( rl ) problems. however, existing multi - agent rl methods typically scale poorly in complexity problem size. therefore, a key contribution is to translate the success of factor learning on both - agent rl to reverse independent - agent setting. a key stumbling block is that independent q - learning, the most popular multi - agent rl method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep rl relies. this paper specifies two methods that address this problem : 1 ) conditioning each agent'semantic value function on memory footprint that disambiguates the age of insignificant data sampled from the replay memory and 2 ) using a multi - agent variant of importance sampling to naturally decay obsolete data. advances on a challenging management variant of starcraft unit micromanagement confirm that the challenges enable the alternative combination of impact replay with multi - agent rl.", "histories": [["v1", "Tue, 28 Feb 2017 17:56:41 GMT  (346kb,D)", "http://arxiv.org/abs/1702.08887v1", null], ["v2", "Mon, 12 Jun 2017 22:00:56 GMT  (1940kb,D)", "http://arxiv.org/abs/1702.08887v2", "Camera-ready version, International Conference of Machine Learning 2017"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.MA", "authors": ["jakob n foerster", "nantas nardelli", "gregory farquhar", "triantafyllos afouras", "philip h s torr", "pushmeet kohli", "shimon whiteson"], "accepted": true, "id": "1702.08887"}, "pdf": {"name": "1702.08887.pdf", "metadata": {"source": "META", "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning", "authors": ["Jakob Foerster", "Nantas Nardelli", "Gregory Farquhar", "Philip. H. S. Torr", "Pushmeet Kohli", "Shimon Whiteson"], "emails": ["<jakob.foerster@cs.ox.ac.uk>,", "<nantas@robots.ox.ac.uk>."], "sections": [{"heading": "1. Introduction", "text": "Reinforcement learning (RL), which enables an agent to learn control policies on-line given only sequences of observations and rewards, has emerged as a dominant paradigm for training autonomous systems. However, many real-world problems, such as network packet delivery (Ye et al., 2015), rubbish removal (Makar et al., 2001), and urban traffic control (Kuyer et al., 2008; Van der Pol & Oliehoek, 2016), are naturally modeled as cooperative multi-agent systems. Unfortunately, tackling such problems with traditional RL is not straightforward.\nIf all agents observe the true state, then we can model a co-\n*Equal contribution 1University of Oxford, Oxford, United Kingdom 2Microsoft Research, Redmond, USA. Correspondence to: Jakob Foerster <jakob.foerster@cs.ox.ac.uk>, Nantas Nardelli <nantas@robots.ox.ac.uk>.\noperative multi-agent system as a single meta-agent. However, the size of this meta-agent\u2019s action space grows exponentially in the number of agents. Furthermore, it is not applicable when each agent receives different observations that may not disambiguate the state, in which case decentralised policies must be learned.\nA popular alternative is independent Q-learning (IQL) (Tan, 1993), in which each agent independently learns its own policy, treating other agents as part of the environment. While IQL avoids the scalability problems of centralised learning, it introduces a new problem: the environment becomes nonstationary from the point of view each agent, as it contains other agents who are themselves learning, ruling out any convergence guarantees. Fortunately, substantial empirical evidence has shown that IQL often works well in practice (Matignon et al., 2012).\nRecently, the use of deep neural networks has dramatically improved the scalability of single-agent RL (Mnih et al., 2015). However, one element key to the success of such approaches is the reliance on an experience replay memory, which stores experience tuples that are sampled during training. Experience replay not only helps to stabilise the training of a deep neural network, it also improves sample efficiency by repeatedly reusing experience tuples. Unfortunately, the combination of experience replay with IQL appears to be problematic: the nonstationarity introduced by IQL means that the dynamics that generated the data in the agent\u2019s replay memory no longer reflect the current dynamics in which it is learning. While IQL without a replay memory can learn well despite nonstationarity so long as each agent is able to gradually track the other agents\u2019 policies, that seems hopeless with a replay memory constantly confusing the agent with obsolete experience.\nTo avoid this problem, previous work on deep multi-agent RL has limited the use of experience replay to short, recent buffers (Leibo et al., 2017) or simply disabled replay altogether (Foerster et al., 2016). However, these workarounds limit the sample efficiency and threaten the stability of multi-agent RL. Consequently, the incompatibility of experience replay with IQL is emerging as a key stumbling block to scaling deep multi-agent RL to complex tasks.\nIn this paper, we propose two approaches for effectively\nar X\niv :1\n70 2.\n08 88\n7v 1\n[ cs\n.A I]\n2 8\nFe b\n20 17\nincorporating experience replay into multi-agent RL. The first approach is inspired by hyper Q-learning (Tesauro, 2003), which avoids the nonstationarity of IQL by having each agent learn a policy that conditions on an estimate of the other agents\u2019 policies inferred from observing their behaviour. While it may seem hopeless to learn Q-functions in this much larger space, especially when each agent\u2019s policy is a deep neural network, we show that doing so is feasible as each agent need only condition on a low-dimensional fingerprint that is sufficient to disambiguate where in the replay memory an experience tuple was sampled from.\nThe second approach interprets the experience in the replay memory as off-environment data (Ciosek & Whiteson, 2017). By augmenting each tuple in the replay memory with the probability of the joint action in that tuple, according to the policies in use at that time, we can compute an importance sampling correction when the tuple is later sampled for training. Since older data tends to generate lower importance weights, this approach naturally decays data as it becomes obsolete, preventing the confusion that a nonstationary replay memory would otherwise create.\nWe evaluate these methods on a decentralised variant of StarCraft unit micromanagement,1 a challenging multi-agent benchmark problem with a high dimensional, stochastic environment that vastly exceeds the complexity of most commonly used multi-agent testbeds. Our results confirm that, thanks to our proposed methods, experience replay can indeed be successfully combined with multiagent RL to allow for stable training of deep multi-agent RL."}, {"heading": "2. Related Work", "text": "Multi-agent RL has a rich history (Busoniu et al., 2008; Yang & Gu, 2004) but has mostly focused on tabular settings and simple environments.\nThe most commonly used method is independent Qlearning(Tan, 1993; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014), which we discuss further in Section 3.2.\nMethods like hyper Q-learning (Tesauro, 2003) - which we also discuss in Section 3.2 - and AWESOME (Conitzer & Sandholm, 2007) try to tackle the nonstationarity by tracking and conditioning each agents\u2019 learning process on their teammates\u2019 current policy, while Da Silva et al. (2006) propose detecting and tracking different classes of traces on which to condition policy learning. Kok & Vlassis (2006) show that coordination can be learnt by estimating a global Q-function in the classical distributed setting\n1StarCraft and its expansion StarCraft: Brood War are trademarks of Blizzard EntertainmentTM.\nsupplemented with a coordination graph. In general, these techniques have so far not successfully been scaled to highdimensional state spaces.\nLauer & Riedmiller (2000) demonstrate a variation of the coordination-free method called distributed Q-learning. However, they also argue that the simple estimation of the value function in the standard model-free fashion is not enough to solve multi-agent problems, and coordination through means such as communication (Mataric, 1998) is required to ground separate observations to the full state function.\nMore recent work tries to leverage deep learning in multiagent RL, mostly as a means to reason about the emergence of inter-agent communication. Tampuu et al. (2015) apply a framework that combines DQN with independent Q-learning to two-player pong. Foerster et al. (2016) propose DIAL, a end-to-end differentiable architecture that allows agents to learn to communicate and has since used by Jorge et al. (2016) in a similar setting. Sukhbaatar et al. (2016) also show that it is possible to learn to communicate by backpropagation. Leibo et al. (2017) analyse the emergence of cooperation and defection when using multiagent RL in mixed-cooperation environments such as the wolfpack problem. He et al. (2016) addresses multi-agent learning by explicitly marginalising the opponents strategy using a mixture of experts in the DQN. Unlike our contributions, none of these papers directly aim to address the nonstationarity arising in multi-agent learning.\nOur work is also broadly related to methods that attempt to allow for faster convergence of policy networks such as prioritized experience replay (Schaul et al., 2015), a version of the standard replay memory that biases the sampling distribution based on the TD error. However, this method does not account for non-stationary environments, and does not take advantage of the unique properties of the multi-agent setting.\nWang et al. (2016) describe an importance sampling method for using off-policy experience in a single-agent actor-critic algorithm. However, to calculate policygradients, the importance ratios become products over potentially lengthy trajectories, introducing high variance that must be partially compensated for by a truncation. Instead we address off-environment learning, and show that the multi-agent structure results in importance ratios that are simply products over the agents\u2019 policies.\nFinally, in the context of StarCraft micromanagement, Usunier et al. (2016) demonstrated some success learning a centralised policy using standard single-agent RL. This agent is able to control all the units owned by the player, and observes the full state of the game in some parametrised form. By contrast we consider a decentralised\ntask in which each unit has only partial observability."}, {"heading": "3. Background", "text": "We begin with background on single-agent and multi-agent reinforcement learning."}, {"heading": "3.1. Single-Agent Reinforcement Learning", "text": "In a traditional RL problem, the agent aims to maximise its expected discounted return Rt = \u2211\u221e t=0 \u03b3\ntrt, where rt is the reward the agent receives at time t and \u03b3 \u2208 [0, 1) is the discount factor (Sutton & Barto, 1998). In a fully observable setting, the agent observes the true state of the environment st \u2208 S, and chooses an action ut \u2208 U according to a policy \u03c0(u|s).\nThe action-value function Q of a policy \u03c0 is Q\u03c0(s, u) = E [Rt|st = s, ut = u]. The Bellman optimality operator T Q(s, u) = Es\u2032 [r + \u03b3maxu\u2032 Q(s\u2032, u\u2032)] is a contraction operator in supremum norm with a unique fixed point, the optimal Q-function Q\u2217(s, u) = max\u03c0 Q\u03c0(s, u), which in turn yields the optimal greedy policy \u03c0\u2217(s, u) = \u03b4(arg maxu\u2032 Q(s, u\n\u2032) \u2212 u). Q-learning (Watkins, 1989) uses a sample-based approximation of T to iteratively improve the Q-function. In deep Q-learning (Mnih et al., 2015), the Q-function is represented by a neural network parametrised by \u03b8. During training, actions are chosen at each timestep according to an exploration policy, such as an -greedy policy that selects the currently-estimated best action arg maxuQ(s, u) with probability 1 \u2212 , and takes a random exploratory action with probability . The reward and next state are observed, and the tuple \u3008s, u, r, s\u2032\u3009 is stored in a replay memory. The parameters \u03b8 are learned by sampling batches of b transitions from the replay memory, and minimising the squared TD-error:\nL(\u03b8) = b\u2211 i=1 [(yDQNi \u2212Q(s, u; \u03b8)) 2], (1)\nwith a target yDQNi = ri + \u03b3maxu\u2032i Q(s \u2032 i, u \u2032 i; \u03b8 \u2212), where \u03b8\u2212 are the parameters of a target network periodically copied from \u03b8 and frozen for a number of iterations. The replay memory stabilises learning, prevents the network from overfitting to recent experiences, and improves sample efficiency. In partially observable settings, agents must in general condition on their entire action-observation history, or a sufficient stastistic thereof. In deep RL, this is accomplished by modelling the Q-function with a recurrent neural network (Hausknecht & Stone, 2015), utilising a gated architecture such as LSTM (Hochreiter & Schmidhuber, 1997) or GRU (Chung et al., 2014)."}, {"heading": "3.2. Multi-Agent Reinforcement Learning", "text": "We consider a fully cooperative multi-agent setting in which n agents identified by a \u2208 A \u2261 {1, ..., n} participate in a stochastic game, G, described by a tuple G = \u3008S,U, P, r, Z,O, n, \u03b3\u3009. The environment occupies states s \u2208 S, in which, at every time step, each agent takes an action ua \u2208 U , forming a joint action u \u2208 U \u2261 Un. State transition probabilities are defined by P (s\u2032|s,u) : S \u00d7U \u00d7 S \u2192 [0, 1]. As the agents are fully cooperative they share the same reward function r(s,u) : S \u00d7U\u2192 R.\nEach agent\u2019s observations z \u2208 Z are governed by an observation function O(s, a) : S \u00d7 A \u2192 Z. For notational simplicity, this observation function is deterministic, i.e., we model only perceptual aliasing and not noise. However, extending our methods to noisy observation functions is straightforward. Each agent a conditions its behaviour on its own action-observation history \u03c4a \u2208 T \u2261 (Z \u00d7 U)\u2217, according to its policy \u03c0a(ua|\u03c4a) : T \u00d7 U \u2192 [0, 1]. After each transition, the action ua and new observation O(s, a) are added to \u03c4a, forming \u03c4 \u2032a. We denote joint quantities over agents in bold, and joint quantities over agents other than a with the subscript \u2212a, so that, e.g., u = [ua,u\u2212a].\nIn independent Q-learning (IQL) (Tan, 1993), the simplest and most popular approach to multi-agent RL, each agent learns its own Q-function that conditions only on the state and its own action. Since our setting is partially observable, IQL can be implemented by having each agent condition on its action-observation history, i.e., Qa(\u03c4a, ua). In deep RL, this can be achieved by having each agent perform DQN using a recurrent neural network trained on its own observations and actions.\nIQL is appealing because it avoids the scalability problems of trying to learn a joint Q-function that conditions on u, since |U| grows exponentially in the number of agents. It is also naturally suited to partially observable settings, since, by construction, it learns decentralised policies in which each agent\u2019s action conditions only on its own observations.\nHowever, IQL introduces a key problem: the environment becomes nonstationary from the point of view each agent, as it contains other agents who are themselves learning, ruling out any convergence guarantees. On the one hand, the conventional wisdom is that this problem is not severe in practice, and substantial empirical results have demonstrated success with IQL (Matignon et al., 2012). On the other hand, such results do not involve deep learning.\nAs discussed earlier, deep RL relies heavily on experience replay and the combination of experience replay with IQL appears to be problematic: the nonstationarity introduced by IQL means that the dynamics that generated the data in the agent\u2019s replay memory no longer reflect the current dy-\nnamics in which it is learning. While IQL without a replay memory can learn well despite nonstationarity so long as each agent is able to gradually track the other agents\u2019 policies, that seems hopeless with a replay memory constantly confusing the agent with obsolete experience. In the next section, we propose methods to address this problem."}, {"heading": "4. Methods", "text": "To avoid the difficulty of combining IQL with experience replay, previous work on deep multi-agent RL has limited the use of experience replay to short, recent buffers (Leibo et al., 2017) or simply disabled replay altogether (Foerster et al., 2016). However, these workarounds limit the sample efficiency and threaten the stability of multi-agent RL. In this section, we propose two approaches for effectively incorporating experience replay into multi-agent RL."}, {"heading": "4.1. Multi-Agent RL with Fingerprints", "text": "The weakness of IQL is that, by treating other agents as part of the environment, it ignores the fact that such agents\u2019 policies are changing over time, rendering its own Q-function nonstationary. This implies that the Q-function could be made stationary if it conditioned on the policies of the other agents. This is exactly the philosophy behind hyper Q-learning (Tesauro, 2003): each agent\u2019s state space is augmented with an estimate of the other agents\u2019 policies computed via Bayesian inference. Intuitively, this reduces each agent\u2019s learning problem to a standard, single-agent problem in a stationary, but much larger, environment.\nThe practical difficulty of hyper Q-learning is, of course, that the dimensionality of the Q-function has increased, making it potentially infeasible to learn. This problem is exacerbated in deep learning, when the other agents\u2019 policies consist of high-dimensional deep neural networks. Consider a naive approach to combining hyper Q-learning with deep RL that includes the weights of the other agents\u2019 networks, \u03b8\u2212a, in the observation function. The new observation function is then O\u2032(s) = {O(s),\u03b8\u2212a}. The agent could in principle then learn a mapping from the weights \u03b8\u2212a, and its own trajectory \u03c4 , into expected returns. Clearly, if the other agents are using deep models, then \u03b8\u2212a is far too large to include as input to the Q-function. However, a key observation is that, to stabilise experience replay, each agent does not need to be able to condition on any possible \u03b8\u2212a, but only those values of \u03b8\u2212a that actually occur in its replay memory. The sequence of policies that generated the data in this buffer can be thought of as following a single, one-dimensional trajectory through the high-dimensional policy space. To stabilise experience replay, it should be sufficient if each agent\u2019s observations disambiguate where along this trajectory the current training sample originated from.\nThe question then, is how to design a low-dimensional fingerprint that contains this information. Note that the two factors that change the expected return for a single agent\u2019s policy are the rate of exploration and the quality of the greedy policy of the other agents. These variables change smoothly over the course of training, which implies that it should be sufficient for the fingerprint to consist simply of the episode number of the training sample. However, adding this information to the observation is not straightforward, since the collection time clearly increases monotonically over the course of training. If we simply input the episode number as a scalar, the neural network has to continuously work with inputs that are outside the range of the previous training data, leading to instability. Therfore, we must design a representation that encodes this information such that early episodes are drawn from a distribution similar to those of late episodes. In addition, to aid generalisation, the Euclidean distance between the fingerprints of similar episodes should be small. To meet these require-\nments, we propose a fingerprint that consists of a binary vector that is sampled uniformly at the beginning of training. To create the temporal correlation and include temporal structure in the fingerprint, a single randomly chosen bit in the vector is flipped periodically after a fixed number of episodes, as illustrated in Figure 1. The observation function thus becomes: O\u2032(s) = {O(s), F (e)}, where F (e) is the fingerprint associated with episode e."}, {"heading": "4.2. Multi-Agent Importance Sampling", "text": "Instead of preventing IQL\u2019s nonstationarity by augmenting each agent\u2019s input with a fingerprint, we can instead cor-\nrect for it by developing an importance sampling scheme for the multi-agent setting. Just as an RL agent can use importance sampling to learn off-policy from data gathered when its own policy was different, so too can it learn off-environment (Ciosek & Whiteson, 2017) from data gathered in a different environment. Since IQL treats other agents\u2019 policies as part of the environment, offenvironment importance sampling corrections can be used to stabilise experience replay. For simplicity, consider first a fully-observable multi-agent setting. Since Q-functions can now condition directly on the true state s, we can write the Bellman optimality equation for a single agent given the policies of all other agents:\nQ\u2217a(s, ua|\u03c0\u2212a) = \u2211 u\u2212a \u03c0\u2212a(u\u2212a|s) [ r(ua,u\u2212a, s)+\n\u03b3 \u2211 s\u2032 P (s\u2032|s, ua,u\u2212a) max u\u2032a Q\u2217a(s \u2032, u\u2032a) ] . (2)\nThe nonstationary component of this equation is \u03c0\u2212a(u\u2212a|s) = \u03a0i\u2208\u2212a\u03c0i(ui|s), which changes as the other agents\u2019 policies change over time. Therefore, to enable importance sampling, at the time of collection tc, we record \u03c0tc\u2212a(u\u2212a|s) in the replay memory, forming an augmented transition tuple \u3008s, ua, r, \u03c0(u\u2212a|s), s\u2032\u3009(tc).\nAt the time of replay tr, we train off-environment by minimising an importance weighted loss function:\nL(\u03b8) = b\u2211 i=1 \u03c0tr\u2212a(u\u2212a|s) \u03c0ti\u2212a(u\u2212a|s) [(yDQNi \u2212Q(s, u; \u03b8)) 2], (3)\nwhere ti is the time of collection of the i-th sample.\nThe derivation of the importance weights in the partially observable multi-agent setting is considerably more complex as the agents\u2019 action-observation histories are correlated in a complex fashion that depends on the agents\u2019 policies as well as the transition and observation functions.\nTo make progress, we can define an augmented state space s\u0302 = {s, \u03c4\u2212a} \u2208 S\u0302 = S \u00d7 Tn\u22121. This state space includes both the original state, s, and the action-observation history of the other agents, \u03c4\u2212a. We also define a corresponding observation function, O\u0302 s.t. O\u0302(s\u0302, a) = O(s, a). With these definitions in place we define a new reward function r\u0302(s\u0302, u) = \u2211 u\u2212a \u03c0\u2212a(u\u2212a|\u03c4\u2212a)r(s,u).\nLastly we define a new transition function,\nP\u0302 (s\u0302\u2032|s\u0302, u) = P (s\u2032, \u03c4 \u2032|s, \u03c4, u) =\u2211 u\u2212a \u03c0\u2212a(u\u2212a|\u03c4\u2212a)P (s\u2032|s,u)p(\u03c4 \u2032\u2212a|\u03c4\u2212a,u\u2212a, s\u2032) (4)\nAll other elements of the augmented game, G\u0302, are adopted from the original game G. This also includes T , the space of action-observation histories. The augmented game is then specified by G\u0302 = \u3008S\u0302, U, P\u0302 , r\u0302, Z, O\u0302, n, \u03b3\u3009.\nWith these definitions we can write a Bellman equation for the above defined G\u0302:\nQ(\u03c4, u) = \u2211 s\u0302 p(s\u0302|\u03c4) [ r\u0302(s\u0302, u) +\n\u03b3 \u2211\n\u03c4 \u2032,s\u0302\u2032,u\u2032\nP\u0302 (s\u0302\u2032|s\u0302, u)\u03c0(u\u2032, \u03c4 \u2032)p(\u03c4 \u2032|\u03c4, s\u0302\u2032, u)Q(\u03c4 \u2032, u\u2032) ] (5)\nSubstituting back in the definitions of the quantities in G\u0302, we arrive at a Bellman equation of a form similar to Equation 2, where \u03c0\u2212a(u\u2212a|\u03c4\u2212a) multiplies right hand side:\nQ(\u03c4, u) = \u2211 s\u0302 p(s\u0302|\u03c4) \u2211 u\u2212a \u03c0\u2212a(u\u2212a|\u03c4\u2212a) [ r(s,u) +\n\u03b3 \u2211\n\u03c4 \u2032,s\u0302\u2032,u\u2032\nP (s\u2032|s,u)p(\u03c4 \u2032\u2212a|\u03c4\u2212a,u\u2212a, s\u2032) \u00b7\n\u03c0(u\u2032, \u03c4 \u2032)p(\u03c4 \u2032|\u03c4, s\u0302\u2032, u)Q(\u03c4 \u2032, u\u2032) ] (6)\nWe note that unlike in the fully observable case, the right hand side contains several terms that indirectly depend on the policies of the other agents and are to the best of our knowledge intractable. However, we show empirically in the next section that the importance ratio defined above, \u03c0tr\u2212a(u\u2212a|s) \u03c0\nti \u2212a(u\u2212a|s)\n, which is only an approximation in the partially\nobservable setting, nonetheless works well in practice."}, {"heading": "5. Experiments", "text": "In this section, we describe our experiments applying experience replay with fingerprints (XP-FP) and with importance sampling (XP-IS) to the StarCraft domain. Since FP requires the network to be able to represent Q-values for a range of different policies of the other agents, we hypothesise that IS will outperform it when the network\u2019s representational capacity is sufficiently limited. We also investigate whether, in some settings and given enough recurrent capacity, the agent can infer the episode of a given training sample from its action-observation history alone, without requiring a fingerprint. To test these hypotheses, we need to be able to manipulate the informativeness of the actionobservation histories and the capacity of the RNN. As a proxy for the capacity of the RNN, we vary the number of neurons in its hidden state between a \u2018small\u2019 and \u2018large\u2019 setting, of 16 and 128 units respectively. Furthermore we manipulate the informativeness of \u03c4 by changing the number of unroll steps between two settings. In \u2018fully unrolled\u2019 the RNN is unrolled over the entire episode during training, while in \u2018short unroll\u2019 hidden states are only passed for 10 steps."}, {"heading": "5.1. Decentralised StarCraft Micromanagement", "text": "StarCraft is an example of a complex, stochastic environment whose dynamics cannot easily be simulated. This differs from standard multi-agent settings such as Packet World (Weyns et al., 2005) and simulated RoboCup (Hausknecht et al., 2016), where often entire episodes can be fully replayed and analysed. This difficulty is typical of real-world problems, and is well suited to the modelfree approaches common in deep RL. In StarCraft, micromanagement refers to the subtask of controlling single or grouped units to move them around the map and fight enemy units. In our multi-agent variant of StarCraft micromanagement, the centralised player is replaced by a set of agents, each assigned to one unit on the map. Each agent observes a square subset of the map centred on the unit it controls, as shown in Figure 2, and must select from a restricted set of durative actions: move[direction], attack[enemy id], stop, and noop. During an episode, each unit is identified by a positive integer initialised on the first step, which the agents observe on the map, and by the number of health points it has, which is\nnot directly observable by other units.\nAll units are Terran Marines, ground units with a fixed range of fire about the length of 4 stacked units. Reward is the sum of the damage inflicted against opponent units during that timestep, with an additional terminal reward equal to the sum of the health of all units on the team. This is a variation of a naturally arising battle signal, comparable with the one used by Usunier et al. (2016). A few timesteps after the agents are spawned, the agents are attacked by opponent units (of the same type). Opponents are controlled by the game AI, which is set to attack all the time. We consider two variations: 3 marines vs 3 marines (m3v3), and 5 marines vs 5 marines (m5v5). Both of these require the agents to coordinate their movements to get the opponents into their range of fire with good positioning, and to focus their firing on each enemy unit so as to destroy them more quickly. We note that skilled StarCraft players can typically solve these tasks.\nWe build our models in Torch7 Collobert et al. (2011), and we run our StarCraft experiments with TorchCraft (Synnaeve et al., 2016), a library that provides the functionality to enact the standard reinforcement learning step in StarCraft: BroodWar in a centralised way (which we extend to enable multi-agent control)."}, {"heading": "5.2. Architecture", "text": "We use the recurrent DQN architecture described by Foerster et al. (2016) with a few modifications. Since we do not consider communicating agents, the number of bits in the message is set to 0. As mentioned above, we use two different sizes for the hidden state of the RNN, a \u2018small setting\u2019 of 16 units and a \u2018large setting\u2019 of 128. We begin training with er = 50 fully random exploratory episodes, after which we employ an annealing schedule of 1/(1 + \u221a e\u2212 er), with an exponential smoothing window of 20 episodes. We train the network for emax = 600 training episodes. In the standard training loop we collect 5 full episodes and add them to the replay memory at each training step. During standard training we sample uniformly from the replay memory and do full unrolling of the episodes. In \u2018short unroll\u2019 experiments we only unroll for 10 steps, from an initial hidden state set to zeros. We also introduce a convolutional neural network to process the 2D representation of the input and a fully connected linear layer to process the fingerprint when used. In order to bound the variance of the multi-agent importance weights, we apply clipping in the range [0.01, 2].\nAll other hyperparameters are identical to Foerster et al. (2016)."}, {"heading": "6. Results", "text": "In this section, we present the results of our StarCraft experiments."}, {"heading": "6.1. Disambiguate and Represent", "text": "Figure 3a shows that a small sized RNN (16 units) performs poorly in the 3 v 3 task using vanilla experience replay (XP) and without experience replay (NOXP). In particular, during the second half of training, the returns drop both for XP and NOXP. Without experience replay, the model overfits to the greedy policy when has dropped sufficiently low. With XP, the model unsuccessfully tries to simultaneously learn a best response policy to every historical policy of the other agents. This drop for XP can be accounted for by either the limited disambiguating information, or limited representational capacity, available to the model. However, we also note that in our particular problem the sampled action-observation history is highly informative about the policies of the other agents, as can be seen in Figure 5. For example, the agent can observe that it or its allies have taken many seemingly random actions, and infer that the sampled experience comes from early in training. Consequently, we hypothesise that the performance of the small RNN on this task is in fact capacity limited.\nFigure 3b shows that by increasing the RNN size to 128 units, it is able to successfully train even with vanilla XP, resulting in returns of around 14 points. This perhaps surprising result demonstrates the power of recurrent architectures in sequential tasks with partial observability. This\nconfirms that by observing its own action-observation history and the behaviour of the other agents, the model can distinguish between policies at different stages of training without any additional information. This is particularly the case in our StarCraft micromanagement scenarios as the units are close to each other at their starting positions, when compared to their visibility range. As a consequence they can observe a large portion of each others\u2019 trajectories.\nWe next test whether we can limit the ability of the RNN to disambiguate episodes by reducing the informativeness of the action-observation histories. Figure 3c shows the performance of the large RNN in the \u2018short unroll\u2019 setting, where the network is unrolled for only 10 time-steps during training. We observe that the performance of the RNN drops to under 10 points, similar to the performance of the XP and NOXP baselines in Figure 3a. In order to confirm whether the performance in this \u2018short unroll\u2019 case is limited by the ability to disambiguate experiences at different stages in training, we must examine whether fingerprints can help recover performance. To do so, we can look at Figure 3c, which shows that by adding fingerprints, XP+FP partially recovers performance on the task compared to the XP baseline, as it helps the RNN to disambiguate experiences drawn from different environments, i.e., different policies of the other agents. By contrast, in Figure 3a, the RNN is capacity limited and the fingerprints are less useful, as evidenced by the low performance of XP+FP.\nWe also compare against a naive baseline method, XP + t, , which replaces the fingerprint, F (e), with a twocomponent vector containing a scaled version of the\nepisode number and the value of at the time of collection, i.e. Fnaive(e) = {e/emax, (e)}. Figure 3c shows that this naive method leads to instability in training, rather than improving performance. Unlike XP+FP, XP+IS attempts to compensate for the nonstationarity of samples in the replay buffer. As a consequence, it should be advantageous both when the RNN is capacity limited and when it is information limited. Figure 3a shows that XP+IS improves the performance of a capacity limited RNN (16 bits), while in Figure 3c, XP+IS improves the performance of the information limited RNN.\nIn figure Figure 4 we show results for a 5 vs 5 battle, increasing the number of agents. In contrast to Figure 3a,\nFigure 4a shows all methods perform quite well in this setting. We hypothesise that with more agents and the relatively broad field of view, it becomes possible to disambiguate between different episodes from even a single observation, allowing the CNN component of the network to compensate for the limited RNN capacity. For example, if the agents are scattered aimlessly at one point in time, they are likely to be employing poor policies - the network can use this information to accurately predict low returns.\nWe test this hypothesis by giving the agents \u2018team blindness,\u2019 which removes the positions of their allies from their observations, and limiting the training to short unrolls, such that the agents can learn little from even their own \u03c4 . This case is shown in figure Figure 4b, where XP+FP improves over the other methods. By processing information correlated to the collection time, the network now manages to learn effectively using the replay memory. Finally, we test the same scenario in the 3v3 case, shown in Figure 4c. Compared to Figure 3c, the further restriction of information about the policies of other agents (team blindness), leads to a greater out-performance by our methods over XP / NOXP."}, {"heading": "7. Conclusion", "text": "This paper presented two methods for stabilising experience replay in deep multi-agent reinforcement learning. We showed that using fingerprints can help agents to disambiguate between episodes from different parts of the training process and thereby partially recover performance in the face of nonstationarity. Furthermore, we showed that a\nvariant of importance sampling applied to the multi-agent setting with replay memory recovers most of the performance which is lost due to nonstationary training samples. We also presented ablation studies on the input to and architecture of the RNN to illustrate the importance of disambiguating between episodes from different stages of training when using recurrent learning in nonstationary environments."}, {"heading": "Acknowledgements", "text": "This work was supported by the Oxford-Google DeepMind Graduate Scholarship, the Microsoft Research PhD Scolarship Program, and EPSRC. We would like to thank Nando de Freitas, Yannis Assael, and Brendan Shillingford for the helpful comments and discussion. We also thank Gabriel Synnaeve, Zeming Lin, and the rest of the TorchCraft team at FAIR for all the help with the interface."}], "references": [{"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["Busoniu", "Lucian", "Babuska", "Robert", "De Schutter", "Bart"], "venue": "IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews,", "citeRegEx": "Busoniu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2008}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Awesome: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents", "author": ["Conitzer", "Vincent", "Sandholm", "Tuomas"], "venue": "Machine Learning,", "citeRegEx": "Conitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Conitzer et al\\.", "year": 2007}, {"title": "Dealing with non-stationary environments using context detection", "author": ["Da Silva", "Bruno C", "Basso", "Eduardo W", "Bazzan", "Ana LC", "Engel", "Paulo M"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Silva et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2006}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["Foerster", "Jakob", "Assael", "Yannis M", "de Freitas", "Nando", "Whiteson", "Shimon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Foerster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "Deep recurrent qlearning for partially observable mdps", "author": ["Hausknecht", "Matthew", "Stone", "Peter"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "Hausknecht et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2015}, {"title": "Half field offense: an environment for multiagent learning and ad hoc teamwork", "author": ["Hausknecht", "Matthew", "Mupparaju", "Prannoy", "Subramanian", "Sandeep", "S Kalyanakrishnan", "P. Stone"], "venue": "In AAMAS Adaptive Learning Agents (ALA) Workshop,", "citeRegEx": "Hausknecht et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2016}, {"title": "Opponent modeling in deep reinforcement learning", "author": ["He", "Boyd-Graber", "Jordan", "Kwok", "Kevin", "Daum\u00e9 III", "Hal"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Learning to play guess who? and inventing a grounded language as a consequence", "author": ["Jorge", "Emilio", "K\u00e5geb\u00e4ck", "Mikael", "Gustavsson", "Emil"], "venue": "arXiv preprint arXiv:1611.03218,", "citeRegEx": "Jorge et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jorge et al\\.", "year": 2016}, {"title": "Collaborative multiagent reinforcement learning by payoff propagation", "author": ["Kok", "Jelle R", "Vlassis", "Nikos"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kok et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kok et al\\.", "year": 2006}, {"title": "Multiagent reinforcement learning for urban traffic control using coordination graphs", "author": ["Kuyer", "Lior", "Whiteson", "Shimon", "Bakker", "Bram", "Vlassis", "Nikos"], "venue": "In ECML 2008: Proceedings of the Nineteenth European Conference on Machine Learning,", "citeRegEx": "Kuyer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kuyer et al\\.", "year": 2008}, {"title": "An algorithm for distributed reinforcement learning in cooperative multiagent systems", "author": ["Lauer", "Martin", "Riedmiller"], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning. Citeseer,", "citeRegEx": "Lauer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lauer et al\\.", "year": 2000}, {"title": "Multi-agent reinforcement learning in sequential social dilemmas", "author": ["Leibo", "Joel Z", "Zambaldi", "Vinicius", "Lanctot", "Marc", "Marecki", "Janusz", "Graepel", "Thore"], "venue": "arXiv preprint arXiv:1702.03037,", "citeRegEx": "Leibo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Leibo et al\\.", "year": 2017}, {"title": "Hierarchical multi-agent reinforcement learning", "author": ["Makar", "Rajbala", "Mahadevan", "Sridhar", "Ghavamzadeh", "Mohammad"], "venue": "In Proceedings of the fifth international conference on Autonomous agents,", "citeRegEx": "Makar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Makar et al\\.", "year": 2001}, {"title": "Using communication to reduce locality in distributed multiagent learning", "author": ["Mataric", "Maja J"], "venue": "Journal of experimental & theoretical artificial intelligence,", "citeRegEx": "Mataric and J.,? \\Q1998\\E", "shortCiteRegEx": "Mataric and J.", "year": 1998}, {"title": "Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems", "author": ["Matignon", "Laetitia", "Laurent", "Guillaume J", "Le Fort-Piat", "Nadine"], "venue": "The Knowledge Engineering Review,", "citeRegEx": "Matignon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matignon et al\\.", "year": 2012}, {"title": "Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations", "author": ["Y. Shoham", "K. Leyton-Brown"], "venue": null, "citeRegEx": "Shoham and Leyton.Brown,? \\Q2009\\E", "shortCiteRegEx": "Shoham and Leyton.Brown", "year": 2009}, {"title": "Learning multiagent communication with backpropagation", "author": ["Sukhbaatar", "Sainbayar", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Torchcraft: a library for machine learning research on real-time strategy games", "author": ["Synnaeve", "Gabriel", "Nardelli", "Nantas", "Auvolat", "Alex", "Chintala", "Soumith", "Lacroix", "Timoth\u00e9e", "Lin", "Zeming", "Richoux", "Florian", "Usunier", "Nicolas"], "venue": "arXiv preprint arXiv:1611.00625,", "citeRegEx": "Synnaeve et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Synnaeve et al\\.", "year": 2016}, {"title": "Multiagent cooperation and competition with deep reinforcement learning", "author": ["Tampuu", "Ardi", "Matiisen", "Tambet", "Kodelja", "Dorian", "Kuzovkin", "Ilya", "Korjus", "Kristjan", "Aru", "Juhan", "Jaan", "Vicente", "Raul"], "venue": "arXiv preprint arXiv:1511.08779,", "citeRegEx": "Tampuu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tampuu et al\\.", "year": 2015}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["Tan", "Ming"], "venue": "In Proceedings of the tenth international conference on machine learning,", "citeRegEx": "Tan and Ming.,? \\Q1993\\E", "shortCiteRegEx": "Tan and Ming.", "year": 1993}, {"title": "Extending q-learning to general adaptive multi-agent systems", "author": ["Tesauro", "Gerald"], "venue": "In NIPS,", "citeRegEx": "Tesauro and Gerald.,? \\Q2003\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 2003}, {"title": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks", "author": ["Usunier", "Nicolas", "Synnaeve", "Gabriel", "Lin", "Zeming", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1609.02993,", "citeRegEx": "Usunier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Usunier et al\\.", "year": 2016}, {"title": "Coordinated deep reinforcement learners for traffic light control", "author": ["Van der Pol", "Elise", "Oliehoek", "Frans A"], "venue": "In NIPS\u201916 Workshop on Learning, Inference and Control of Multi-Agent Systems,", "citeRegEx": "Pol et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pol et al\\.", "year": 2016}, {"title": "Sample efficient actor-critic with experience", "author": ["Wang", "Ziyu", "Bapst", "Victor", "Heess", "Nicolas", "Mnih", "Volodymyr", "Munos", "Remi", "Kavukcuoglu", "Koray", "de Freitas", "Nando"], "venue": "replay. arXiv preprint arXiv:1611.01224,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Learning from delayed rewards", "author": ["Watkins", "Christopher John Cornish Hellaby"], "venue": "PhD thesis,", "citeRegEx": "Watkins and Hellaby.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and Hellaby.", "year": 1989}, {"title": "The packet-world: A test bed for investigating situated multi-agent systems. In Software Agent-Based Applications, Platforms and Development Kits", "author": ["Weyns", "Danny", "Helleboogh", "Alexander", "Holvoet", "Tom"], "venue": null, "citeRegEx": "Weyns et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weyns et al\\.", "year": 2005}, {"title": "Multiagent reinforcement learning for multi-robot systems: A survey", "author": ["Yang", "Erfu", "Gu", "Dongbing"], "venue": "Technical report, tech. rep,", "citeRegEx": "Yang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2004}, {"title": "A multiagent framework for packet routing in wireless sensor", "author": ["Ye", "Dayong", "Zhang", "Minjie", "Yang", "Yun"], "venue": "networks. sensors,", "citeRegEx": "Ye et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2015}, {"title": "Empirically evaluating multiagent learning algorithms", "author": ["E. Zawadzki", "A. Lipson", "K. Leyton-Brown"], "venue": "arXiv preprint 1401.8074,", "citeRegEx": "Zawadzki et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zawadzki et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 31, "context": "However, many real-world problems, such as network packet delivery (Ye et al., 2015), rubbish removal (Makar et al.", "startOffset": 67, "endOffset": 84}, {"referenceID": 15, "context": ", 2015), rubbish removal (Makar et al., 2001), and urban traffic control (Kuyer et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 12, "context": ", 2001), and urban traffic control (Kuyer et al., 2008; Van der Pol & Oliehoek, 2016), are naturally modeled as cooperative multi-agent systems.", "startOffset": 35, "endOffset": 85}, {"referenceID": 17, "context": "Fortunately, substantial empirical evidence has shown that IQL often works well in practice (Matignon et al., 2012).", "startOffset": 92, "endOffset": 115}, {"referenceID": 14, "context": "To avoid this problem, previous work on deep multi-agent RL has limited the use of experience replay to short, recent buffers (Leibo et al., 2017) or simply disabled replay altogether (Foerster et al.", "startOffset": 126, "endOffset": 146}, {"referenceID": 5, "context": ", 2017) or simply disabled replay altogether (Foerster et al., 2016).", "startOffset": 45, "endOffset": 68}, {"referenceID": 0, "context": "Multi-agent RL has a rich history (Busoniu et al., 2008; Yang & Gu, 2004) but has mostly focused on tabular settings and simple environments.", "startOffset": 34, "endOffset": 73}, {"referenceID": 32, "context": "The most commonly used method is independent Qlearning(Tan, 1993; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014), which we discuss further in Section 3.", "startOffset": 54, "endOffset": 117}, {"referenceID": 4, "context": "2 - and AWESOME (Conitzer & Sandholm, 2007) try to tackle the nonstationarity by tracking and conditioning each agents\u2019 learning process on their teammates\u2019 current policy, while Da Silva et al. (2006) propose detecting and tracking different classes of traces on which to condition policy learning.", "startOffset": 182, "endOffset": 202}, {"referenceID": 4, "context": "2 - and AWESOME (Conitzer & Sandholm, 2007) try to tackle the nonstationarity by tracking and conditioning each agents\u2019 learning process on their teammates\u2019 current policy, while Da Silva et al. (2006) propose detecting and tracking different classes of traces on which to condition policy learning. Kok & Vlassis (2006) show that coordination can be learnt by estimating a global Q-function in the classical distributed setting", "startOffset": 182, "endOffset": 321}, {"referenceID": 17, "context": "Tampuu et al. (2015) apply a framework that combines DQN with independent Q-learning to two-player pong.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "Foerster et al. (2016) propose DIAL, a end-to-end differentiable architecture that allows agents to learn to communicate and has since used by Jorge et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "Foerster et al. (2016) propose DIAL, a end-to-end differentiable architecture that allows agents to learn to communicate and has since used by Jorge et al. (2016) in a similar setting.", "startOffset": 0, "endOffset": 163}, {"referenceID": 5, "context": "Foerster et al. (2016) propose DIAL, a end-to-end differentiable architecture that allows agents to learn to communicate and has since used by Jorge et al. (2016) in a similar setting. Sukhbaatar et al. (2016) also show that it is possible to learn to communicate by backpropagation.", "startOffset": 0, "endOffset": 210}, {"referenceID": 5, "context": "Foerster et al. (2016) propose DIAL, a end-to-end differentiable architecture that allows agents to learn to communicate and has since used by Jorge et al. (2016) in a similar setting. Sukhbaatar et al. (2016) also show that it is possible to learn to communicate by backpropagation. Leibo et al. (2017) analyse the emergence of cooperation and defection when using multiagent RL in mixed-cooperation environments such as the wolfpack problem.", "startOffset": 0, "endOffset": 304}, {"referenceID": 5, "context": "Foerster et al. (2016) propose DIAL, a end-to-end differentiable architecture that allows agents to learn to communicate and has since used by Jorge et al. (2016) in a similar setting. Sukhbaatar et al. (2016) also show that it is possible to learn to communicate by backpropagation. Leibo et al. (2017) analyse the emergence of cooperation and defection when using multiagent RL in mixed-cooperation environments such as the wolfpack problem. He et al. (2016) addresses multi-agent learning by explicitly marginalising the opponents strategy using a mixture of experts in the DQN.", "startOffset": 0, "endOffset": 461}, {"referenceID": 25, "context": "Finally, in the context of StarCraft micromanagement, Usunier et al. (2016) demonstrated some success learning a centralised policy using standard single-agent RL.", "startOffset": 54, "endOffset": 76}, {"referenceID": 1, "context": "In deep RL, this is accomplished by modelling the Q-function with a recurrent neural network (Hausknecht & Stone, 2015), utilising a gated architecture such as LSTM (Hochreiter & Schmidhuber, 1997) or GRU (Chung et al., 2014).", "startOffset": 205, "endOffset": 225}, {"referenceID": 17, "context": "On the one hand, the conventional wisdom is that this problem is not severe in practice, and substantial empirical results have demonstrated success with IQL (Matignon et al., 2012).", "startOffset": 158, "endOffset": 181}, {"referenceID": 14, "context": "To avoid the difficulty of combining IQL with experience replay, previous work on deep multi-agent RL has limited the use of experience replay to short, recent buffers (Leibo et al., 2017) or simply disabled replay altogether (Foerster et al.", "startOffset": 168, "endOffset": 188}, {"referenceID": 5, "context": ", 2017) or simply disabled replay altogether (Foerster et al., 2016).", "startOffset": 45, "endOffset": 68}, {"referenceID": 29, "context": "This differs from standard multi-agent settings such as Packet World (Weyns et al., 2005) and simulated RoboCup (Hausknecht et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 7, "context": ", 2005) and simulated RoboCup (Hausknecht et al., 2016), where often entire episodes can be fully replayed and analysed.", "startOffset": 30, "endOffset": 55}, {"referenceID": 25, "context": "This is a variation of a naturally arising battle signal, comparable with the one used by Usunier et al. (2016). A few timesteps after the agents are spawned, the agents are attacked by opponent units (of the same type).", "startOffset": 90, "endOffset": 112}, {"referenceID": 21, "context": "(2011), and we run our StarCraft experiments with TorchCraft (Synnaeve et al., 2016), a library that provides the functionality to enact the standard reinforcement learning step in StarCraft: BroodWar in a centralised way (which we extend to enable multi-agent control).", "startOffset": 61, "endOffset": 84}, {"referenceID": 2, "context": "We build our models in Torch7 Collobert et al. (2011), and we run our StarCraft experiments with TorchCraft (Synnaeve et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 5, "context": "We use the recurrent DQN architecture described by Foerster et al. (2016) with a few modifications.", "startOffset": 51, "endOffset": 74}, {"referenceID": 5, "context": "All other hyperparameters are identical to Foerster et al. (2016).", "startOffset": 43, "endOffset": 66}], "year": 2017, "abstractText": "Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on singleagent RL to the multi-agent setting. A key stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep RL relies. This paper proposes two methods that address this problem: 1) conditioning each agent\u2019s value function on a footprint that disambiguates the age of the data sampled from the replay memory and 2) using a multi-agent variant of importance sampling to naturally decay obsolete data. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL.", "creator": "LaTeX with hyperref package"}}}