{"id": "1703.00440", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Fast k-Nearest Neighbour Search via Prioritized DCI", "abstract": "most exact methods for k - nearest neighbour search suffer from the phenomenon of dimensionality ; that is, their query times exhibit exponential dependence on either the information or the intrinsic dimensionality. dynamic continuous tracking ( dci ) offers a promising way of circumventing the design by avoiding space partitioning and achieves a query time that grows sublinearly in the intrinsic footprint. in this paper, we develop a variant of dci, typically we call prioritized dci, and show a further improvement reflecting the dependence on the intrinsic dimensionality closer to standard dci, thereby improving the performance of dci on datasets whose high intrinsic dimensionality. we also demonstrate however that coding complexity compares favourably to standard dci and locality - wide criteria ( lsh ) variants in terms of running searches representing space consumption at all levels of approximation quality. in particular, contrary to lsh, prioritized database reduces the global computed distance evaluations by a factor of 5 to 30 and the space consumption by a factor of 47 to 55.", "histories": [["v1", "Wed, 1 Mar 2017 18:51:13 GMT  (170kb,D)", "http://arxiv.org/abs/1703.00440v1", "11 pages, 5 figures"], ["v2", "Thu, 20 Jul 2017 17:46:04 GMT  (289kb,D)", "http://arxiv.org/abs/1703.00440v2", "14 pages, 6 figures; International Conference on Machine Learning (ICML), 2017"]], "COMMENTS": "11 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.DS cs.IR stat.ML", "authors": ["ke li", "jitendra malik"], "accepted": true, "id": "1703.00440"}, "pdf": {"name": "1703.00440.pdf", "metadata": {"source": "META", "title": "Fast k-Nearest Neighbour Search via Prioritized DCI", "authors": ["Ke Li", "Jitendra Malik"], "emails": ["<ke.li@eecs.berkeley.edu>."], "sections": [{"heading": "1. Introduction", "text": "The method of k-nearest neighbours is a fundamental building block of many machine learning methods. Consequently, devising a fast algorithm that scales to highdimensional space is a problem that has for decades intrigued the artificial intelligence and theoretical computer science communities alike. A wide variety of methods have been proposed; unfortunately, they suffer from the curse of dimensionality; that is, their query times scale exponentially in either the ambient or the intrinsic dimensionality1 of the dataset. As a result, on datasets with moderately\n1University of California, Berkeley, CA 94720, United States. Correspondence to: Ke Li <ke.li@eecs.berkeley.edu>.\n1The measure of intrinsic dimensionality used throughout this paper is the expansion dimension, also known as the KRdimension, which is defined as log2 c, where c is the expansion rate introduced in (Karger & Ruhl, 2002). Intuitively, intrinsic dimensionality measures the rate at which the number of points inside a ball grows as a function of its radius and is independent of\nlarge intrinsic dimensionality, practitioners often have resort to na\u0131\u0308ve exhaustive search.\nRecently, Li & Malik (2016) proposed an approach known as Dynamic Continuous Indexing (DCI) that offers a possible way out of this quandary. The key observation is that the difficulties encountered by many existing methods, including k-d trees (Bentley, 1975), Locality-Sensitive Hashing (LSH) (Indyk & Motwani, 1998) and RP trees (Dasgupta & Freund, 2008), may arise from their reliance on space partitioning, which is a popular divide-and-conquer strategy. It works by partitioning the vector space into discrete cells and maintaining a data structure that keeps track of the points lying in each cell. At query time, these methods simply look up of the contents of the cell containing the query and possibly adjacent cells and perform brute-force search over points lying in these cells. Conceptually, these methods are able to eliminate large parts of the search space consisting of points that are far from the query and concentrate only on nearby points, thereby providing a speedup. While this works well in low-dimensional settings, would it work in high dimension?\nLi & Malik (2016) identified a number of shortcomings with this approach that become prohibitive in highdimensional space. First, the volume of the space grows exponentially in dimensionality. As a result, either the number or the volumes of cells must grow exponentially. Second, the discretization of the space essentially limits the \u201cfield of view\u201d of the algorithm, as it is unaware of points that lie in adjacent cells. This is especially problematic when the query lies near a cell boundary, as there could points in adjacent cells that are much closer to the query. Third, as dimensionality increases, surface area grows faster than volume; as a result, points are increasingly likely to lie near cell boundaries. Fourth, when the dataset exhibits varying density across space, choosing a good partitioning is non-trivial. Furthermore, once chosen, the partitioning is fixed and cannot adapt to changes in density arising from updates to the dataset.\nIn light of these observations, DCI is built on the idea of avoiding partitioning the vector space. Instead, it constructs a number of indices, each of which imposes an or-\nambient dimensionality, which is the dimensionality of the space data points are embedded in.\nar X\niv :1\n70 3.\n00 44\n0v 1\n[ cs\n.L G\n] 1\nM ar\n2 01\n7\ndering of all data points. Each index is constructed so that two points with similar ranks in the associated ordering are nearby along a certain random direction. These indices are then combined to allow for retrieval of points that are close to the query along multiple random directions. It has been shown that the query time of DCI depends sublinearly on the intrinsic dimensionality in the exact setting, which improves on the exponential dependence exhibited by other methods. In this paper, we build on DCI and aim to further improve its running time, especially on datasets with high intrinsic dimensionality. To this end, we propose a variant of DCI, which assigns a priority to each index that is used to determine which index to process in the upcoming iteration. For this reason, we will refer to this algorithm as Prioritized DCI. This simple change results in a significant improvement in the dependence of query time on intrinsic dimensionality relative to standard DCI; specifically, we show that a linear increase in intrinsic dimensionality can be mostly counteracted with a corresponding linear increase in the number of indices. Finally, we also demonstrate empirically that Prioritized DCI runs faster and requires less memory than standard DCI and LSH at all levels of approximation quality. In particular, compared to LSH, it achieves a 5- to 30-fold reduction in the number of distance evaluations and a 47- to 55-fold reduction in the memory usage."}, {"heading": "2. Related Work", "text": "There is a vast literature on algorithms for nearest neighbour search. They can be divided into two categories: exact algorithms and approximate algorithms. Early exact algorithms are deterministic and store points in treebased data structures. Examples include k-d trees (Bentley, 1975), R-trees (Guttman, 1984) and X-trees (Berchtold et al., 1996; 1998), which divide the vector space into a hierarchy of half-spaces, hyper-rectangles or Voronoi polygons and keep track of the points that lie in each cell. While their query times are logarithmic in the size of the dataset, they exhibit exponential dependence on the ambient dimensionality. A different method (Meiser, 1993) partitions the space by intersecting multiple hyperplanes. It effectively trades off space for time and achieves polynomial query time in ambient dimensionality at the cost of exponential space complexity in ambient dimensionality.\nTo avoid worst-case configurations of the data, exact randomized algorithms have been proposed. Spill trees (Liu et al., 2004), RP trees (Dasgupta & Freund, 2008) and virtual spill trees (Dasgupta & Sinha, 2015) extend k-d trees by randomizing the hyperplanes that partition the space into half-spaces at each node of the tree. While randomization enables them to avoid exponential dependence on the ambient dimensionality, their query times still scale ex-\nponentially in the intrinsic dimensionality. Whereas these methods rely on space partitioning, other algorithms (Orchard, 1991; Clarkson, 1999; Karger & Ruhl, 2002) have been proposed that utilize local search strategies. These methods start with a random point and look in the neighbourhood of the current point to find a new point that is closer to the query than the original in each iteration. Like space partitioning-based approaches, the query time of (Karger & Ruhl, 2002) scales exponentially in the intrinsic dimensionality. While the query times of (Orchard, 1991; Clarkson, 1999) do not exhibit such undesirable dependence, their space complexities are quadratic in the size of the dataset, making them impractical for large datasets. A different class of algorithms performs search in a coarse-to-fine manner. Examples include navigating nets (Krauthgamer & Lee, 2004), cover trees (Beygelzimer et al., 2006) and rank cover trees (Houle & Nett, 2015), which maintain sets of subsampled data points at different levels of granularity and descend through the hierarchy of neighbourhoods of decreasing radii around the query. Unfortunately, the query times of these methods again scale exponentially in the intrinsic dimensionality.\nDue to the difficulties of devising efficient algorithms for the exact version of the problem, there has been extensive work on approximate algorithms. Under the approximate setting, returning any point whose distance to the query is within a factor of 1 + of the distance between the query and the true nearest neighbour is acceptable. Many of the same strategies are employed by approximate algorithms. Methods based on tree-based space partition-\nIntrinsic Dimensionality (d\u2032)\nO ( d\u2032d \u2032 +logn ) O ( 23d \u2032 logn\n) O ( dmax(logn,n1\u22121/d \u2032 ) ) O ( dmax(logn,n1\u2212m/d \u2032 ) +(mlogm)max(logn,n1\u22121/d \u2032 ) )\nFigure 1. Visualization of the query time complexities of various algorithms as a function of the intrinsic dimensionality d\u2032.\ning (Arya et al., 1998) and local search (Arya & Mount, 1993) have been developed; like many exact algorithms, their query times also scale exponentially in the ambient dimensionality. Locality-Sensitive Hashing (LSH) (Indyk & Motwani, 1998; Datar et al., 2004; Andoni & Indyk, 2006) partitions the space into regular cells, whose exact shapes are implicitly defined by the choice of the hash function. It achieves a query time of O(dn\u03c1) using O(dn1+\u03c1) space, where d is the ambient dimensionality, n is the dataset size and \u03c1 \u2248 1/(1 + )2 for large n in Euclidean space, though dependence on the intrinsic dimensionality is not made explicit. In practice, the performance of LSH degrades on datasets with large variations in density, due to the uneven distribution of points across cells. Consequently, various data-dependent hashing schemes have been proposed (Pauleve\u0301 et al., 2010; Weiss et al., 2009; Andoni & Razenshteyn, 2015); unlike data-independent hashing schemes, however, they do not allow dynamic updates to the dataset. A different approach (Anagnostopoulos et al., 2015) projects the data to a lower dimensional space that approximately preserves approximate nearest neighbour relationships and applies other approximate algorithms like BBD trees (Arya et al., 1998) to the projected data. Its query time is also linear in ambient dimensionality and sublinear in the dataset size, but uses space linear in the dataset size, at the cost of longer query time than LSH. Unfortunately, its query time is exponential in intrinsic dimensionality.\nOur work is most closely related to Dynamic Continuous Indexing (DCI) (Li & Malik, 2016), which is an exact randomized algorithm for Euclidean space whose query time is linear in ambient dimensionality, sublinear in dataset size and sublinear in intrinsic dimensionality and uses space linear in the dataset size. Rather than partitioning the vector space, it uses multiple one-dimensional indices, each of which orders data points along a certain random direction and combines these indices to locate points that are\nnear the query along multiple random directions. The proposed algorithm builds on the ideas introduced by DCI and achieves a further improvement in dependence on intrinsic dimensionality compared to standard DCI.\nA summary of the query times of various prior algorithms and the proposed algorithm is presented in Table 1 and their growth as a function of intrinsic dimensionality is illustrated in Figure 1."}, {"heading": "3. Prioritized DCI", "text": "DCI constructs a data structure consisting of multiple composite indices of data points, each of which in turn consists of a number of simple indices. Each simple index orders data points according to their projections along a particular random direction. Given a query, for every composite index, the algorithm finds points that are near the query in every constituent simple index, which are known as candidate points, and adds them to a set known as the candidate set. The true distances from the query to every candidate point are evaluated and the ones that are among the k closest to the query are returned.\nAlgorithm 1 Data structure construction procedure Require: A datasetD of n points p1, . . . , pn, the number of sim-\nple indicesm that constitute a composite index and the number of composite indices L function CONSTRUCT(D,m,L) {ujl}j\u2208[m],l\u2208[L] \u2190 mL random unit vectors in Rd {Tjl}j\u2208[m],l\u2208[L] \u2190 mL empty binary search trees or skip\nlists for j = 1 to m do\nfor l = 1 to L do for i = 1 to n do\npijl \u2190 \u3008pi, ujl\u3009 Insert (pijl, i) into Tjl with p i jl being the key and\ni being the value end for\nend for end for return {(Tjl, ujl)}j\u2208[m],l\u2208[L]\nend function\nMore concretely, each simple index is associated with a random direction and stores the projections of every data point along the direction. They are implemented using standard data structures that can maintain one-dimensional ordered sequences of elements, like self-balancing binary search trees or a skip lists. At query time, the algorithm projects the query along the projection directions associated with each simple index and finds the position where the query would have been inserted in each simple index, which takes logarithmic time. It then iterates over, or visits, data points in each simple index in the order of their distances to the query under projection, which takes constant time for each iteration. As it iterates, it keeps track of\nhow many times each data point has been visited across all simple indices of each composite index. If a data point has been visited in every constituent simple index, it is added to the candidate set and is said to have been retrieved from the composite index.\nAlgorithm 2 k-nearest neighbour querying procedure\nRequire: Query point q in Rd, binary search trees/skip lists and their associated projection vectors {(Tjl, ujl)}j\u2208[m],l\u2208[L], the number of points to retrieve k0 and the number of points to visit k1 in each composite index function QUERY(q, {(Tjl, ujl)}j,l, k0, k1)\nCl \u2190 array of size n with entries initialized to 0 \u2200l \u2208 [L] qjl \u2190 \u3008q, ujl\u3009 \u2200j \u2208 [m], l \u2208 [L] Sl \u2190 \u2205 \u2200l \u2208 [L] Pl \u2190 empty priority queue \u2200l \u2208 [L] for l = 1 to L do\nfor j = 1 to m do (p\n(1) jl , h (1) jl )\u2190 the node in Tjl whose key is the\nclosest to qjl Insert (p(1)jl , h (1) jl ) with priority \u2212 \u2223\u2223\u2223p(1)jl \u2212 qjl\u2223\u2223\u2223 into Pl\nend for end for for i\u2032 = 1 to k1 \u2212 1 do\nfor l = 1 to L do if |Sl| < k0 then\n(p (i) jl , h (i) jl )\u2190 the node with the highest priority in Pl Remove (p(i)jl , h (i) jl ) from Pl and insert the node\nin Tjl whose key is the next closest to qjl, which is denoted as (p(i+1)jl , h (i+1) jl ), with\npriority \u2212 \u2223\u2223\u2223p(i+1)jl \u2212 qjl\u2223\u2223\u2223 into Pl\nCl[h (i) jl ]\u2190 Cl[h (i) jl ] + 1 if Cl[h(i)jl ] = m then Sl \u2190 Sl \u222a {h(i)jl }\nend if end if\nend for end for return k points in \u22c3 l\u2208[L] Sl that are the closest in\nEuclidean distance in Rd to q end function\nDCI has a number of appealing properties compared to methods based on space partitioning. Because points are visited by rank rather than location in space, DCI performs well on datasets with large variations in data density. It naturally skips over sparse regions of the space and concentrates more on dense regions of the space. Since construction of the data structure does not depend on the dataset, the algorithm supports dynamic updates to the dataset and automatically adapts to changes in data density. Furthermore, because data points are represented in the indices as continuous values without being discretized, the granularity of discretization does not need to be chosen at con-\nstruction time. Consequently, the same data structure can support queries at varying desired levels of accuracy, which enables different speed-vs-accuracy trade-offs to be made for different queries.\nPrioritized DCI differs from standard DCI in the order in which points from different simple indices are visited. In standard DCI, the algorithm cycles through all constituent simple indices of a composite index in regular intervals and visits exactly one point from each simple index in each pass. In Prioritized DCI, the algorithm assigns a priority to each constituent simple index; in each iteration, it visits the upcoming point from the simple index with the highest priority and updates the priority at the end of the iteration. The priority of a simple index is set to the negative absolute difference between the query projection and the next data point projection in the index.\nIntuitively, this ensures data points are visited in the order of their distances to the query under projection. Because data points are only retrieved from a composite index when they have been visited in all constituent simple indices, data points are retrieved in the order of the maximum of their distances to the query along multiple projection directions. Since distance under projection forms a lower bound on the true distance, the maximum projected distance approaches the true distance as the number of projection directions increases. Hence, in the limit as the number of simple indices goes to infinity, data points are retrieved in the ideal order, that is, the order of their true distances to the query.\nThe construction and querying procedures of Prioritized DCI are presented formally in Algorithms 1 and 2. To ensure the algorithm retrieves the exact knearest neighbours with high probability, the analysis in the next section shows that one should choose k0 \u2208 \u2126(kmax(log(n/k), (n/k)1\u2212m/d \u2032 )) and k1 \u2208 \u2126(mkmax(log(n/k), (n/k)1\u22121/d \u2032 )), where d\u2032 denotes the intrinsic dimensionality. Though because this assumes worst-case configuration of data points, it may be overly conservative in practice; so, these parameters may be chosen by cross-validation."}, {"heading": "4. Analysis", "text": "We analyze the time and space complexity of Prioritized DCI below and derive the stopping conditions for the dataindependent and data-dependent versions of the algorithm. Because the algorithm uses standard data structures, analysis of the construction time, insertion time, deletion time and space complexity is straightforward. Hence, this section focuses mostly on analyzing the query time.\nIn high-dimensional space, query time is dominated by the time spent on evaluating true distances between candidate points and the query. Therefore, we need to find the num-\nber of candidate points that must be retrieved to ensure the algorithm succeeds with high probability. To this end, we derive an upper bound on the failure probability for any given number of candidate points. The algorithm fails if sufficiently many distant points are retrieved from each composite index before some of the true k-nearest neighbours. We decompose this event into multiple (dependent) events, each of which is the event that a particular distant point is retrieved before some true k-nearest neighbours. Since points are retrieved in the order of their maximum projected distance, this event happens when the maximum projected distance of the distant point is less than that of a true k-nearest neighbour. We start by finding an upper bound on the probability of this event. To simplify notation, we initially consider displacement vectors from the query to each data point, and so relationships between projected distances of triplets of points translate relationships between projected lengths of pairs of displacement vectors.\nWe summarize the main results in Table 2. Notably, the first term of the query complexity, which dominates when the ambient dimensionality d is large, has a more favourable dependence on the intrinsic dimensionality d\u2032 than the query complexity of standard DCI. In particular, a linear increase in the intrinsic dimensionality, which corresponds to an exponential increase in the expansion rate, can be mitigated by just a linear increase in the number of simple indices m. This suggests that Prioritized DCI can better handle datasets with high intrinsic dimensionality than standard DCI, which is confirmed by empirical evidence later in this paper.\nOur analysis starts by examining the event that a vector under random one-dimensional projection satisfies some geometric constraint. We then find an upper bound on the probability that some combinations of these events occur, which is related to the failure probability of the algorithm.\nLemma 1. Let vl, vs \u2208 Rd be such that \u2225\u2225vl\u2225\u2225 2 > \u2225\u2225vs\u2225\u2225 2 ,{\nu\u2032j }M j=1\nbe i.i.d. unit vectors in Rd drawn uniformly at random. Then Pr ( maxj {\u2223\u2223\u3008vl, u\u2032j\u3009\u2223\u2223} \u2264 \u2225\u2225vs\u2225\u22252) =( 1\u2212 2\u03c0 cos \u22121 (\u2225\u2225vs\u2225\u2225 2 / \u2225\u2225vl\u2225\u2225 2 ))M .\nProof. The event { maxj {\u2223\u2223\u3008vl, u\u2032j\u3009\u2223\u2223} \u2264 \u2016vs\u20162} is equiva-\nlent to the event that {\u2223\u2223\u3008vl, u\u2032j\u3009\u2223\u2223 \u2264 \u2016vs\u20162 \u2200j}, which is the\nintersection of the events {\u2223\u2223\u3008vl, u\u2032j\u3009\u2223\u2223 \u2264 \u2016vs\u20162}. Because u\u2032j\u2019s are drawn independently, these are independent. Let \u03b8j be the angle between vl and u\u2032j , so that \u3008vl, u\u2032j\u3009 =\u2225\u2225vl\u2225\u2225 2\ncos \u03b8j . Since u\u2032j is drawn uniformly, \u03b8j is uniformly distributed on [0, 2\u03c0]. Hence,\nPr ( max j {\u2223\u2223\u2223\u3008vl, u\u2032j\u3009\u2223\u2223\u2223} \u2264 \u2016vs\u20162) =\nM\u220f j=1 Pr (\u2223\u2223\u2223\u3008vl, u\u2032j\u3009\u2223\u2223\u2223 \u2264 \u2016vs\u20162)\n= M\u220f j=1 Pr ( |cos \u03b8j | \u2264 \u2016vs\u20162 \u2016vl\u20162 )\n= M\u220f j=1 ( 2Pr ( \u03b8j \u2208 [ cos\u22121 ( \u2016vs\u20162 \u2016vl\u20162 ) , \u03c0 \u2212 cos\u22121 ( \u2016vs\u20162 \u2016vl\u20162 )]))\n= ( 1\u2212 2\n\u03c0 cos\u22121 ( \u2016vs\u20162 \u2016vl\u20162 ))M\nLemma 2. For any set of events {Ei}Ni=1, the probability that k\u2032 of them happen is at most 1k\u2032 \u2211N i=1 Pr (Ei).\nProof. For any set T \u2286 [N ], define E\u0303T to be the intersection of events indexed by T and complements of events not indexed by T , i.e. E\u0303T = (\u22c2 i\u2208T Ei ) \u2229 (\u22c2 i/\u2208T Ei ) . Ob-\nserve that events { E\u0303T } T\u2286[N ] are disjoint and that for any\nI \u2286 [N ], \u22c2 i\u2208I Ei = \u22c3 T\u2287I E\u0303T .\nIn particular, E\u0303T \u2286 Et for any t \u2208 T . It also follows that for any E\u0303T \u2286 \u22c2 i\u2208I Ei, |T | \u2265 |I|.\nThe event that k\u2032 of Ei\u2019s happen is \u22c3 I\u2286[N ]:|I|=k\u2032 \u22c2 i\u2208I Ei.\nLet T = { T \u2223\u2223\u2223E\u0303T \u2286 \u22c3I\u2286[N ]:|I|=k\u2032 \u22c2i\u2208I Ei}. Thus,\u22c3\nT\u2208T E\u0303T = \u22c3 I\u2286[N ]:|I|=k\u2032 \u22c2 i\u2208I Ei.\nWe enumerate all T \u2208 T . We know that for any t \u2208 T , E\u0303T \u2286 Et. Since |T | \u2265 |I| = k\u2032, there are at least k\u2032 Ei\u2019s that contain E\u0303T .\nSince different E\u0303T \u2019s are disjoint, \u2211N i=1 Pr (Ei) \u2265\u2211\nT\u2208T k \u2032Pr ( E\u0303T ) = k\u2032Pr (\u22c3 I\u2286[N ]:|I|=k\u2032 \u22c2 i\u2208I Ei ) .\nTheorem 1. Let { vli }N i=1 and { vsi\u2032 }N \u2032 i\u2032=1\nbe sets of vectors such that \u2225\u2225vli\u2225\u22252 > \u2016vsi\u2032\u20162 \u2200i \u2208 [N ], i\u2032 \u2208 [N \u2032]. Furthermore, let { u\u2032ij } i\u2208[N ],j\u2208[M ] be random uniformly distributed unit vectors such that u\u2032i1, . . . , u \u2032 iM are independent for any given i. Consider the events{ \u2203vsi\u2032 s.t. maxj\n{\u2223\u2223\u3008vli, u\u2032ij\u3009\u2223\u2223} \u2264 \u2225\u2225vsi\u2032\u2225\u22252}Ni=1. The probability that at least k\u2032 of these events occur is at most 1k\u2032 \u2211N i=1 ( 1\u2212 2\u03c0 cos \u22121 (\u2225\u2225vsmax\u2225\u22252 / \u2225\u2225vli\u2225\u22252))M , where\nFast k-Nearest Neighbour Search via Prioritized DCI\u2225\u2225vsmax\u2225\u22252 = maxi\u2032 {\u2225\u2225vsi\u2032\u2225\u22252}. Furthermore, if k\u2032 = N , it is at most mini\u2208[N ] {( 1\u2212 2\u03c0 cos\n\u22121 (\u2225\u2225vsmax\u2225\u22252 /\u2225\u2225vli\u2225\u22252))M}. Proof. The event that \u2203vsi\u2032 s.t. maxj\n{\u2223\u2223\u3008vli, u\u2032ij\u3009\u2223\u2223} \u2264\u2225\u2225vsi\u2032\u2225\u22252 is equivalent to the event that maxj {\u2223\u2223\u3008vli, u\u2032ij\u3009\u2223\u2223} \u2264 maxi\u2032\n{\u2225\u2225vsi\u2032\u2225\u22252} = \u2225\u2225vsmax\u2225\u22252. Take Ei to be the event that maxj\n{\u2223\u2223\u3008vli, u\u2032ij\u3009\u2223\u2223} \u2264 \u2225\u2225vsmax\u2225\u22252. By Lemma 1, Pr(Ei) \u2264 ( 1\u2212 2\u03c0 cos\n\u22121 (\u2225\u2225vsmax\u2225\u22252 /\u2225\u2225vli\u2225\u22252))M . It follows from Lemma 2 that the probability that k\u2032 of Ei\u2019s occur is at most 1k\u2032 \u2211N i=1 Pr (Ei) \u2264 1 k\u2032 \u2211N i=1 ( 1\u2212 2\u03c0 cos\n\u22121 (\u2225\u2225vsmax\u2225\u22252 / \u2225\u2225vli\u2225\u22252))M . If k\u2032 = N , we use the fact that \u22c2N i\u2032=1Ei\u2032 \u2286 Ei \u2200i, which\nimplies that Pr (\u22c2N i\u2032=1Ei\u2032 ) \u2264 mini\u2208[N ] Pr (Ei) \u2264\nmini\u2208[N ] {( 1\u2212 2\u03c0 cos \u22121 (\u2225\u2225vsmax\u2225\u22252 / \u2225\u2225vli\u2225\u22252))M}. We now apply the results above to analyze specific properties of the algorithm. For convenience, instead of working directly with intrinsic dimensionality, we will analyze the query time in terms of a related quantity, global relative sparsity, as defined in (Li & Malik, 2016). We reproduce its definition below for completeness.\nDefinition 1. Given a dataset D \u2286 Rd, let Bp(r) be the set of points in D that are within a ball of radius r around a point p. A dataset D has global relative sparsity of (\u03c4, \u03b3) if for all r and p \u2208 Rd such that |Bp(r)| \u2265 \u03c4 , |Bp(\u03b3r)| \u2264 2 |Bp(r)|, where \u03b3 \u2265 1.\nGlobal relative sparsity is related to the expansion rate (Karger & Ruhl, 2002) and intrinsic dimensionality in the following way: a dataset with global relative sparsity of (\u03c4, \u03b3) has (\u03c4, 2(1/ log2 \u03b3))-expansion and intrinsic dimensionality of 1/ log2 \u03b3.\nBelow we derive two upper bounds on the probability that some of the true k-nearest neighbours are missing from the set of candidate points retrieved from a given composite index, which are in expressed in terms of k0 and k1 respectively. These results inform us how k0 and k1 should be chosen to ensure the querying procedure returns the correct results with high probability. Using these settings of k0 and k1, we can then derive a bound on the query time. We start with some intermediate results, the proofs of which are found in the supplementary material. In the results that follow, we use {p(i)}ni=1 to denote a re-ordering of the points {pi}ni=1 so that p(i) is the ith closest point to the query q. Lemma 3. Consider points in the order they are retrieved from a composite index that consists of m simple indices. The probability that there are at least n0 points that are not the true k-nearest neighbours but are retrieved before some of them is at most\n1 n0\u2212k \u2211n i=2k+1 (\u2225\u2225p(k) \u2212 q\u2225\u2225 2 / \u2225\u2225p(i) \u2212 q\u2225\u2225 2 )m .\nLemma 4. Consider point projections in a composite index that consists of m simple indices in the order they are visited. The probability that n0 point projections that are not of the true k-nearest neighbours are visited before all true k-nearest neighbours have been retrieved is at most\nm n0\u2212mk \u2211n i=2k+1 (\u2225\u2225p(k) \u2212 q\u2225\u2225 2 / \u2225\u2225p(i) \u2212 q\u2225\u2225 2 ) . Lemma 5. On a dataset with global relative sparsity (k, \u03b3), the quantity\u2211n i=2k+1 (\u2225\u2225p(k) \u2212 q\u2225\u2225 2 / \u2225\u2225p(i) \u2212 q\u2225\u2225 2 )m is at most\nO ( kmax(log(n/k), (n/k)1\u2212m log2 \u03b3) ) .\nLemma 6. For a dataset with global relative sparsity (k, \u03b3) and a given composite index consisting of m simple indices, there is some k0 \u2208 \u2126(kmax(log(n/k), (n/k)1\u2212m log2 \u03b3)) such that the probability that the candidate points retrieved from the composite index do not include some of the true k-nearest neighbours is at most some constant \u03b10 < 1.\nLemma 7. For a dataset with global relative sparsity (k, \u03b3) and a given composite index consisting of m simple indices, there is some k1 \u2208 \u2126(mkmax(log(n/k), (n/k)1\u2212log2 \u03b3)) such that the probability that the candidate points retrieved from the composite index do not include some of the true k-nearest neighbours is at most some constant \u03b11 < 1.\nTheorem 2. For a dataset with global relative sparsity (k, \u03b3), for any > 0, there is some L, k0 \u2208 \u2126(kmax(log(n/k), (n/k)1\u2212m log2 \u03b3)) and k1 \u2208 \u2126(mkmax(log(n/k), (n/k)1\u2212log2 \u03b3)) such that the algorithm returns the correct set of k-nearest neighbours with probability of at least 1\u2212 .\nProof. For a given composite index, by Lemma 6, there is some k0 \u2208 \u2126(kmax(log(n/k), (n/k)1\u2212m log2 \u03b3)) such that the probability that some of the true k-nearest neighbours are missed is at most some constant \u03b10 < 1. Likewise, by Lemma 7, there is some k1 \u2208 \u2126(mkmax(log(n/k), (n/k)1\u2212log2 \u03b3)) such that this probability is at most some constant \u03b11 < 1. By choosing such k0 and k1, this probability is therefore at most min{\u03b10, \u03b11} < 1. For the algorithm to fail, all composite indices must miss some k-nearest neighbours. Since each composite index is constructed independently, the algorithm fails with probability of at most (min{\u03b10, \u03b11})L, and so must succeed with probability of at least 1 \u2212 (min{\u03b10, \u03b11})L. Since min{\u03b10, \u03b11} < 1, there is some L that makes 1\u2212 (min{\u03b10, \u03b11})L \u2265 1\u2212 .\nTheorem 3. For a given number of simple indices m, the algorithm takes O ( dkmax(log(n/k), (n/k)1\u2212m/d \u2032 )+\nmk logm ( max(log(n/k), (n/k)1\u22121/d \u2032 ) )) time to re-\ntrieve the k-nearest neighbours at query time, where d\u2032 denotes the intrinsic dimensionality.\nProof. Computing projections of the query point along all ujl\u2019s takes O(dm) time, since L is a constant. Searching in the binary search trees/skip lists Tjl\u2019s takes O(m log n) time. The total number of point projections that are visited is at most \u0398(mkmax(log(n/k), (n/k)1\u2212log2 \u03b3)). Because determining the next point to visit requires popping and pushing a priority queue, which takes O(logm) time, the total time spent on visiting points is O(mk logmmax(log(n/k), (n/k)1\u2212log2 \u03b3)). The total number of candidate points retrieved is at most \u0398(kmax(log(n/k), (n/k)1\u2212m log2 \u03b3)). Because true distances are computed for every candidate point, the total time spent on distance computation is O(dkmax(log(n/k), (n/k)1\u2212m log2 \u03b3)). We can find the k closest points to the query among the candidate points using a selection algorithm like quickselect, which takes O(kmax(log(n/k), (n/k)1\u2212m log2 \u03b3)) time on average. Since the time for visiting points and for computing distances dominate, the entire algorithm takes O(dkmax(log(n/k), (n/k)1\u2212m log2 \u03b3) + mk logmmax(log(n/k), (n/k)1\u2212log2 \u03b3)) time. Substituting 1/d\u2032 for log2 \u03b3 yields the desired expression.\nWe now proceed by analyzing the time complexities for construction, insertion and deletion and the space complexity.\nTheorem 4. For a given number of simple indices m, the algorithm takesO(m(dn+n log n)) time to preprocess the data points in D at construction time.\nProof. Computing projections of all n points along all ujl\u2019s takes O(dmn) time, since L is a constant. Inserting all n points into mL self-balancing binary search trees/skip lists takes O(mn log n) time.\nTheorem 5. The algorithm requires O(m(d+log n)) time to insert a new data point and O(m log n) time to delete a data point.\nProof. In order to insert a data point, we need to compute its projection along all ujl\u2019s and insert it into each binary search tree or skip list. Computing the projections takes O(md) time and inserting them into the corresponding selfbalancing binary search trees or skip lists takesO(m log n) time. In order to delete a data point, we simply remove its projections from each of the binary search trees or skip lists, which takes O(m log n) time.\nTheorem 6. The algorithm requires O(mn) space in addition to the space used to store the data.\nProof. The only additional information that needs to be stored are the mL binary search trees or skip lists. Since\nn entries are stored in each binary search tree/skip list, the total additional space required is O(mn)."}, {"heading": "5. Experiments", "text": "We compare the performance of Prioritized DCI to that of standard DCI and LSH, which is perhaps the algorithm that is most widely used in practice in high-dimensional settings. Because LSH operates under the approximate setting, in which the performance metric of interest is how close the returned points are to the query rather than whether they are the true k-nearest neighbours. All algorithms are evaluated in terms of the time they would need to achieve varying levels of approximation quality.\nEvaluation is performed on two datasets, CIFAR-100 and MNIST. CIFAR-100 consists of 60, 000 colour images of 100 types of different objects in natural scenes and MNIST consists of 70, 000 grayscale images of handwritten digits. The images in CIFAR-100 have a size of 32\u00d7 32 and three colour channels, and the images in MNIST have a size of 28\u00d7 28 and a single colour channel. We reshape each image into a vector whose entries represent pixel intensities at different locations and colour channels in the image. So, each vector has a dimensionality of 32\u00d732\u00d73 = 3072 for CIFAR-100 and 28 \u00d7 28 = 784 for MNIST. Note that the dimensionalities under consideration are much higher than those typically used to evaluate prior methods.\nFor the purposes of nearest neighbour search, MNIST is a more challenging dataset than CIFAR-100. This is because images in MNIST are concentrated around a few modes; consequently, data points form dense clusters, leading to higher intrinsic dimensionality. On the other hand, images in CIFAR-100 are more diverse, and so data points are more dispersed in space. Intuitively, it is much harder to find the closest digit to a query among 6999 other digits of the same category that are all plausible near neighbours than to find the most similar natural image among a few other natural images with similar appearance. Later results show that less time is needed by all algorithms to achieve the same level approximation of quality on CIFAR100 than on MNIST.\nWe randomize the split of query vs. data points using tenfold cross-validation. For each fold, we random choose 100 points from the dataset as queries and use each algorithm to retrieve their 25 nearest neighbours among the remaining points. We report mean performance and standard deviation over all folds.\nApproximation quality is measured using the approximation ratio, which is defined to be the ratio of the radius of the ball containing the set of true k-nearest neighbours to the radius of the ball containing the set of approximate knearest neighbours returned by the algorithm. The closer\nthe approximation ratio is to 1, the higher the approximation quality. In high dimensions, the time taken to compute true distances between the query and the candidate points dominates query time, so the number of distance evaluations can be used as an implementation-independent proxy for the query time.\nFor LSH, we used 24 hashes per table and 100 tables. For standard DCI, we used the same hyparameter settings used in (Li & Malik, 2016) (m = 25 and L = 2 on CIFAR-100 and m = 15 and L = 3 on MNIST). For Prioritized DCI, we used m = 10 and L = 2 on both datasets.\nWe plot the number of distance evaluations that each algorithm requires to achieve each desired level of approximation ratio in Figure 2. As shown, on CIFAR-100, Prioritized DCI requires 70.8% to 82.3% fewer distance evaluations than standard DCI, and 74.2% to 85.8% fewer distance evaluations than LSH to achieve same levels approximation quality, which represents a 4-fold reduction in the number of distance evaluations relative to LSH and a 5- fold reduction relative to LSH on average. On MNIST, which has higher intrinsic dimensionality, Prioritized DCI requires 81.9% to 92.9% fewer distance evaluations than standard DCI, and 96.1% to 97.4% fewer distance evaluations than LSH, which represents a 8-fold reduction in the number of distance evaluations relative to DCI and a 30- fold reduction relative to LSH on average.\nWe compare the space efficiency of Prioritized DCI to that of standard DCI and LSH. As shown in Figure 3, Prioritized DCI reduces memory usage by 59.9% and 55.5% relative to standard DCI on CIFAR-100 and MNIST respectively. Compared to LSH, it uses 98.2% and 97.9% less space, which represents a 55-fold and a 47-fold reduction in space usage respectively."}, {"heading": "6. Conclusion", "text": "In this paper, we presented a new exact randomized algorithm for performing k-nearest neighbour search, which we refer to as Prioritized DCI. We showed that Prioritized DCI further improves the dependence of query time on the intrinsic dimensionality compared to standard DCI. In particular, a linear increase in the intrinsic dimensionality can be mostly counteracted by just a linear increase in the number of simple indices. Empirical results validated the advantages of Prioritized DCI in practice. Specifically, Prioritized DCI requires significantly fewer distance evaluations and lower memory usage than standard DCI and LSH at all levels of approximation quality."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["Andoni", "Alexandr", "Indyk", "Piotr"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Andoni et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Andoni et al\\.", "year": 2006}, {"title": "Optimal datadependent hashing for approximate near neighbors", "author": ["Andoni", "Alexandr", "Razenshteyn", "Ilya"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "Andoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Andoni et al\\.", "year": 2015}, {"title": "Approximate nearest neighbor queries in fixed dimensions", "author": ["Arya", "Sunil", "Mount", "David M"], "venue": "In SODA,", "citeRegEx": "Arya et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Arya et al\\.", "year": 1993}, {"title": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions", "author": ["Arya", "Sunil", "Mount", "David M", "Netanyahu", "Nathan S", "Silverman", "Ruth", "Wu", "Angela Y"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Arya et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Arya et al\\.", "year": 1998}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["Bentley", "Jon Louis"], "venue": "Communications of the ACM,", "citeRegEx": "Bentley and Louis.,? \\Q1975\\E", "shortCiteRegEx": "Bentley and Louis.", "year": 1975}, {"title": "The X-tree : An Index Structure for HighDimensional Data", "author": ["Berchtold", "Stefan", "Keim", "Daniel A", "peter Kriegel", "Hans"], "venue": "In Very Large Data Bases, pp", "citeRegEx": "Berchtold et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berchtold et al\\.", "year": 1996}, {"title": "Fast nearest neighbor search in high-dimensional space", "author": ["Berchtold", "Stefan", "Ertl", "Bernhard", "Keim", "Daniel A", "Kriegel", "H-P", "Seidl", "Thomas"], "venue": "In Data Engineering,", "citeRegEx": "Berchtold et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Berchtold et al\\.", "year": 1998}, {"title": "Cover trees for nearest neighbor", "author": ["Beygelzimer", "Alina", "Kakade", "Sham", "Langford", "John"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2006}, {"title": "Nearest neighbor queries in metric spaces", "author": ["Clarkson", "Kenneth L"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "Clarkson and L.,? \\Q1999\\E", "shortCiteRegEx": "Clarkson and L.", "year": 1999}, {"title": "Random projection trees and low dimensional manifolds", "author": ["Dasgupta", "Sanjoy", "Freund", "Yoav"], "venue": "In Proceedings of the fortieth annual ACM symposium on Theory of computing,", "citeRegEx": "Dasgupta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2008}, {"title": "Randomized partition trees for nearest neighbor", "author": ["Dasgupta", "Sanjoy", "Sinha", "Kaushik"], "venue": "search. Algorithmica,", "citeRegEx": "Dasgupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2015}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["Datar", "Mayur", "Immorlica", "Nicole", "Indyk", "Piotr", "Mirrokni", "Vahab S"], "venue": "In Proceedings of the twentieth annual symposium on Computational geometry,", "citeRegEx": "Datar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Datar et al\\.", "year": 2004}, {"title": "R-trees: a dynamic index structure for spatial searching, volume", "author": ["Guttman", "Antonin"], "venue": null, "citeRegEx": "Guttman and Antonin.,? \\Q1984\\E", "shortCiteRegEx": "Guttman and Antonin.", "year": 1984}, {"title": "Rank-based similarity search: Reducing the dimensional dependence", "author": ["Houle", "Michael E", "Nett", "Michael"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Houle et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Houle et al\\.", "year": 2015}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Indyk", "Piotr", "Motwani", "Rajeev"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "citeRegEx": "Indyk et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Indyk et al\\.", "year": 1998}, {"title": "Finding nearest neighbors in growth-restricted metrics", "author": ["Karger", "David R", "Ruhl", "Matthias"], "venue": "In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,", "citeRegEx": "Karger et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2002}, {"title": "Navigating nets: simple algorithms for proximity search", "author": ["Krauthgamer", "Robert", "Lee", "James R"], "venue": "In Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Krauthgamer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Krauthgamer et al\\.", "year": 2004}, {"title": "Fast k-nearest neighbour search via Dynamic Continuous Indexing", "author": ["Li", "Ke", "Malik", "Jitendra"], "venue": "In ICML,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "An investigation of practical approximate nearest neighbor algorithms", "author": ["Liu", "Ting", "Moore", "Andrew W", "Yang", "Ke", "Gray", "Alexander G"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "Point location in arrangements of hyperplanes", "author": ["Meiser", "Stefan"], "venue": "Information and Computation,", "citeRegEx": "Meiser and Stefan.,? \\Q1993\\E", "shortCiteRegEx": "Meiser and Stefan.", "year": 1993}, {"title": "A fast nearest-neighbor search algorithm", "author": ["Orchard", "Michael T"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Orchard and T.,? \\Q1991\\E", "shortCiteRegEx": "Orchard and T.", "year": 1991}, {"title": "Locality sensitive hashing: A comparison of hash function types and querying mechanisms", "author": ["Paulev\u00e9", "Lo\u0131\u0308c", "J\u00e9gou", "Herv\u00e9", "Amsaleg", "Laurent"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Paulev\u00e9 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Paulev\u00e9 et al\\.", "year": 2010}, {"title": "Spectral hashing", "author": ["Weiss", "Yair", "Torralba", "Antonio", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Weiss et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "Examples include k-d trees (Bentley, 1975), R-trees (Guttman, 1984) and X-trees (Berchtold et al., 1996; 1998), which divide the vector space into a hierarchy of half-spaces, hyper-rectangles or Voronoi polygons and keep track of the points that lie in each cell.", "startOffset": 80, "endOffset": 110}, {"referenceID": 18, "context": "Spill trees (Liu et al., 2004), RP trees (Dasgupta & Freund, 2008) and virtual spill trees (Dasgupta & Sinha, 2015) extend k-d trees by randomizing the hyperplanes that partition the space into half-spaces at each node of the tree.", "startOffset": 12, "endOffset": 30}, {"referenceID": 7, "context": "Examples include navigating nets (Krauthgamer & Lee, 2004), cover trees (Beygelzimer et al., 2006) and rank cover trees (Houle & Nett, 2015), which maintain sets of subsampled data points at different levels of granularity and descend through the hierarchy of neighbourhoods of decreasing radii around the query.", "startOffset": 72, "endOffset": 98}, {"referenceID": 3, "context": "ing (Arya et al., 1998) and local search (Arya & Mount, 1993) have been developed; like many exact algorithms, their query times also scale exponentially in the ambient dimensionality.", "startOffset": 4, "endOffset": 23}, {"referenceID": 11, "context": "Locality-Sensitive Hashing (LSH) (Indyk & Motwani, 1998; Datar et al., 2004; Andoni & Indyk, 2006) partitions the space into regular cells, whose exact shapes are implicitly defined by the choice of the hash function.", "startOffset": 33, "endOffset": 98}, {"referenceID": 21, "context": "Consequently, various data-dependent hashing schemes have been proposed (Paulev\u00e9 et al., 2010; Weiss et al., 2009; Andoni & Razenshteyn, 2015); unlike data-independent hashing schemes, however, they do not allow dynamic updates to the dataset.", "startOffset": 72, "endOffset": 142}, {"referenceID": 22, "context": "Consequently, various data-dependent hashing schemes have been proposed (Paulev\u00e9 et al., 2010; Weiss et al., 2009; Andoni & Razenshteyn, 2015); unlike data-independent hashing schemes, however, they do not allow dynamic updates to the dataset.", "startOffset": 72, "endOffset": 142}, {"referenceID": 3, "context": ", 2015) projects the data to a lower dimensional space that approximately preserves approximate nearest neighbour relationships and applies other approximate algorithms like BBD trees (Arya et al., 1998) to the projected data.", "startOffset": 184, "endOffset": 203}], "year": 2017, "abstractText": "Most exact methods for k-nearest neighbour search suffer from the curse of dimensionality; that is, their query times exhibit exponential dependence on either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing (DCI) (Li & Malik, 2016) offers a promising way of circumventing the curse by avoiding space partitioning and achieves a query time that grows sublinearly in the intrinsic dimensionality. In this paper, we develop a variant of DCI, which we call Prioritized DCI, and show a further improvement in the dependence on the intrinsic dimensionality compared to standard DCI, thereby improving the performance of DCI on datasets with high intrinsic dimensionality. We also demonstrate empirically that Prioritized DCI compares favourably to standard DCI and Locality-Sensitive Hashing (LSH) both in terms of running time and space consumption at all levels of approximation quality. In particular, relative to LSH, Prioritized DCI reduces the number of distance evaluations by a factor of 5 to 30 and the space consumption by a factor of 47 to 55.", "creator": "LaTeX with hyperref package"}}}