{"id": "1703.00729", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Mixing Complexity and its Applications to Neural Networks", "abstract": "we suggest analyzing neural networks through the prism of space constraints. we observe that most training applications proposed in practice use minimal memory, which enables us to use a new notion introduced in the study representing space - time tradeoffs that we call mixing complexity. this notion was devised taken order to measure the ( in ) ability to learn using a bounded - memory algorithm. in this paper we incorporate how we suggest mixing complexity to seek new results on what can and cannot be learned using local networks.", "histories": [["v1", "Thu, 2 Mar 2017 11:34:38 GMT  (86kb,D)", "http://arxiv.org/abs/1703.00729v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michal moshkovitz", "naftali tishby"], "accepted": false, "id": "1703.00729"}, "pdf": {"name": "1703.00729.pdf", "metadata": {"source": "META", "title": "Mixing Complexity and its Applications to Neural Networks", "authors": ["Michal Moshkovitz", "Naftali Tishby"], "emails": ["<michal.moshkovitz@mail.huji.ac.il>,", "<tishby@cs.huji.ac.il>."], "sections": [{"heading": "1. Introduction", "text": "Understanding neural network learning is an active research area in machine learning and neuroscience (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014; Tishby & Zaslavsky, 2015; Kadmon & Sompolinsky, 2016). In this paper we view this problem through a different lens \u2014 that of space constraints. We observe that learning with neural networks, either artificial or biological, is almost always done using a bounded-memory algorithm. In the setting of machine learning, artificial neural networks most often use the Stochastic Gradient Descent (SGD) algorithm, one example of a bounded-memory algorithm. In neuroscience, biological neural networks, i.e., the nervous system, inherently perform a boundedmemory computation under the accepted assumption that learning should be biologically plausible. This places the problem of learning with neural networks in the framework of bounded-memory learning.\nIn recent years, several works have shown that under memory constraints numerous examples are needed in order to learn certain hypothesis classes (Shamir, 2014; Raz, 2016; Kol et al., 2016; Moshkovitz & Moshkovitz, 2017; Raz, 2017). Expanding on the work of (Moshkovitz & Moshkovitz, 2017) we define a new complexity measure,\n1The Hebrew University of Jerusalem, Israel. Correspondence to: Michal Moshkovitz <michal.moshkovitz@mail.huji.ac.il>, Naftali Tishby <tishby@cs.huji.ac.il>.\nmixing complexity, in order to evaluate the difficulty of learning a class under memory constraints. For a class H we denote its mixing complexity by MC(H); the larger it is, the more \u201ccomplex\u201d H is. Roughly speaking, MC(H) assesses the closeness of H to a random class. It does so by viewing H as a bipartite graph and comparing its edge distribution to that of a truly random graph.\nIn this paper we explore mixing complexity and its applications to neural networks. Our first application shows that any classH with mixing complexity MC(H) = \u2126( \u221a |H|) cannot be learned by neural networks. One implication is that most classes cannot be learned by neural networks.\nThe above might seem to contradict the fact that, empirically, neural networks do indeed learn (Krizhevsky et al., 2012; LeCun et al., 2015). To bridge this gap, we suggest that \u201cnatural\u201d hypothesis classes have certain \u201csymmetries\u201d. We formalize this notion and prove that such natural classes have high mixing complexity. Thus, mixing complexity sheds a new light on our understanding of what can and cannot be learned using neural networks.\nIn addition we discuss an application of mixing complexity as a response to a question raised by (Zhang et al., 2017). They showed that classification of natural images by a specific neural network achieves a small generalization error. However, for random labels the same neural network suffers from a large generalization error. They showed that classical measures fail to explain these findings, and left as an open problem the task of finding a more suitable complexity measure. We show that mixing complexity does in fact distinguish between natural images and random classes.\nGiven the usefulness of mixing complexity, as demonstrated by the above applications, we prove that it has several desirable properties. First, we prove that any class H with mixing complexity MC(H) = \u2126( \u221a |H|) has a VCdimension \u2126(log |H|). Since the VC-dimension of every classH is at most log2 |H|, we get that such classes are the hardest to learn (up to a constant factor) without memory constraints. Furthermore, we prove that mixing complexity is robust under small perturbations. That is, if a small number of labels is changed, the mixing complexity is approximately unchanged.\nar X\niv :1\n70 3.\n00 72\n9v 1\n[ cs\n.L G\n] 2\nM ar\n2 01\n7"}, {"heading": "1.1. Paper overview", "text": "In Section 2 we briefly review the definitions of learning, the VC-dimension, and bounded-memory algorithms. In Section 3 we show that artificial and biological neural networks each use a bounded-memory algorithm. In Section 4 we define mixing complexity and exemplify it. In Section 5 we restate the main theorem proved in (Moshkovitz & Moshkovitz, 2017). It claims that bounded-memory algorithm cannot learn hypothesis classes that are \u201cmixing\u201d. In Section 6 we explore a set of natural classes and prove they are not mixing; i.e., a bounded-memory algorithm may be able to learn these classes. In Section 7 we describe the work done in (Zhang et al., 2017) and its connection to mixing complexity. In Section 8 we prove some desirable properties of mixing complexity. Section 9 summarizes the results and leaves some open problems for future work."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Learning", "text": "Learning is the process of converting experience into expertise. A learner receives labeled examples (x, b) \u2208 X\u00d7{0, 1} one after another as experience and after enough examples the learner outputs a hypothesis h : X \u2192 {0, 1}. The goal is to return h that minimizes the test error, which is defined as the probability to return a different answer than the true underlying hypothesis f\nL(D,f)(h) = Pr x\u223cD\n[h(x) 6= f(x)]\nThe examples in the learning process are drawn independently from some unknown distribution D and an underlying hypothesis f . Definition 1 (PAC learnable, (Valiant, 1984)). A hypothesis class H is PAC learnable if there exists a function mH : (0, 1)\n2 \u2192 N and a learning algorithm A with the following property: For every , \u03b4 \u2208 (0, 1), for every distribution D over X , and for every underlying hypothesis f : X \u2192 {0, 1}, when running the learning algorithm on m \u2265 mH( , \u03b4) i.i.d examples generated by D and labeled by f , the algorithm returns a hypothesis f such that, with a probability of at least 1\u2212 \u03b4 (over the choice of the examples),\nL(D,f)(h) \u2264 .\nGiven a series of examples S = {(xi, yi)} we define the training error as\nLS(h) = 1\n|S| |S|\u2211 i=1 Ih(xi)6=yi ,\nwhere IP returns 1 if the predicate P is true, else 0. The difference between the training error and the test error is called the generalization error."}, {"heading": "2.2. The VC-dimension Complexity", "text": "The VC-dimension is a complexity measure for hypothesis classes, as proven in the Fundamental Theorem of Statistical Learning.\nDefinition 2 (restriction). Let H be a hypothesis class for binary classification over the domain X . Let S = {x1, . . . , xn} be a set of examples. The restriction of H into S is the set\nHS = {(h(x1), . . . , h(xn)) : h \u2208 H}.\nDefinition 3 (shattering). A hypothesis class H shatters a set S \u2286 X if |HS | = 2|S|. Definition 4 (VC-dimension). The VC-dimension of a hypotheses classH is\nV Cdim(H) = sup{|S| : H shatters S}\nTheorem 5 (Fundamental Theorem of Statistical Learning). Let H be a hypothesis class over the domain X , and let d = V Cdim(H). Then, there are absolute constants C1, C2 such thatH is PAC learnable with sample complexity\nC1 d+ log 1\u03b4 \u2264 mH( , \u03b4) \u2264 C2 d log 1 + log 1 \u03b4 .\nFor convenience, we henceforth set , \u03b4 = 1/4. It is known that any class H can be learned with O(log |H|) examples without memory constraints. If a bounded-memory learning algorithm must use at least |H|c, for some constant c > 0, we say that the class is unlearnable with these memory constraints."}, {"heading": "2.3. Bounded-Memory Learning Algorithm", "text": "One approach to designing a learning algorithm is to save all the examples received and return a hypothesis with a minimal training error. Notice that this approach does not use a bounded memory. In this paper, however, we focus on bounded-memory algorithms.\nA bounded-memory algorithm is a Turing machine with a bounded size tape s with each cell in the tape being either \u20320\u2032 or \u20321\u2032. It is useful to think of such an algorithm as a graph on \u039b = 2s vertices. Each vertex is one possible memory state. In each step, the algorithm is in one memory state. When faced with a new example, the algorithm transition to another memory state. In the final step, the algorithm outputs a hypothesis that depends on the memory state where it ended up.\nIn this paper we investigate hypothesis classes that are unlearnable with a bounded-memory algorithm. We adhere to the realizability assumption, which means that there is a\nhypothesis in the class with a test error equal to 0. Notice that using this assumption only strengthens the unlearnability result. Also note that this paper focuses on the statistical aspect (i.e., how many examples are needed to learn) and not the computational aspect (i.e., how much time is needed to learn). Notice that proving that bounded-memory algorithms must use many examples (the statistical aspect) immediately yields that the bounded-memory algorithms must run slowly (the computational aspect)."}, {"heading": "3. Neural Networks and Bounded-Memory Algorithms", "text": "In this section we discuss the connection between boundedmemory algorithms and neural networks in machine learning and neuroscience. From the perspective of machine learning, neural networks define a hypothesis class. The algorithm that is almost always used to find a hypothesis from this class is a bounded-memory algorithm (i.e., the stochastic gradient descent algorithm), as explained below. From the perspective of neuroscience, we show that under the accepted assumptions, any computation made by the nervous system must be a bounded-memory algorithm."}, {"heading": "3.1. Neural Networks and Machine Learning", "text": "Artificial neural networks have dramatically improved the state-of-the-art in many fields (see (LeCun et al., 2015) and referenced therein). In general, however, learning a neural network is NP-hard (Blum & Rivest, 1988). This has led many researchers to attempt to understand the reasons for the astonishing success despite the proven hardness (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014).\nIn this section we establish one property of the widely used algorithm for learning neural networks: it is a boundedmemory algorithm. A feed-forward artificial neural network is composed of layers of neurons and directed edges between consecutive layers, this is known as the architecture of the neural network. Each neuron computes the mapping \u03c3(w \u00b7 x + b), where x is the input to the neuron, w is a vector that represents the weight of each input, b is a bias term, and \u03c3 : R\u2192 R is some activation function.\nThe stochastic gradient descent (SGD) method is a popular way to learn the weights of a neural network. When it gets a new example (or a small number of examples) it changes the current weights of the neural network, based on the appropriate gradient. Thus it is a bounded-memory algorithm."}, {"heading": "3.2. Neural Networks and Neuroscience", "text": "The nervous system is responsible for processing all the information an organism receives and acting accordingly. It is composed of a large number of neurons that are connected to one another and together form a biological neural network. Two neurons are connected through a synapse, a structure that permits the transmission of an electrical or chemical signal from one neuron (called the pre-synaptic neuron) to another (called the post-synaptic neuron). Each pre-synaptic neuron can influence the post-synaptic neuron differently, depending on various biological parameters (e.g., the amount of neurotransmitter released into the synapse, the myelination).\nIn the standard model of a neural network, each synapse that connects a pre-synaptic neuron i and a post-synaptic neuron j is represented by some weight wi,j that scales the input from the pre-synaptic neuron activity xi. The postsynaptic neuron computes the inner product of the multiple pre-synaptic neurons\u2019 activities and the synaptic weights \u3008wi, x\u3009 = \u2211 i wi,jxi. This inner product passes through a nonlinearity \u03c3 called the activation function and the result \u03c3(\u3008wi, x\u3009) is transmitted to the post-synaptic neuron.\nIn the standard model, learning in the nervous system manifests itself by a change in the synaptic weight. Neuroscientists focus solely on changes that are biologically plausible; i.e., those that fulfill some biological constraints. The most important constraint, for our purposes, is that the changes in weights are only a function of the current sensory input. This means that the brain computes a bounded-memory algorithm."}, {"heading": "4. Mixing Complexity", "text": ""}, {"heading": "4.1. Hypotheses Graphs", "text": "A hypothesis class can be viewed as a bipartite graph in which on one side there is a vertex for each hypothesis h and on the other side there is a vertex for each example x, and there is an edge (h, x) if and only if h(x) = 1. To illustrate it, focus on the class Hth of discrete threshold functions in [0, 1]:\n\u2022 the examples are the numbers X = { 0 = 0\n|X | \u2212 1 ,\n1\n|X | \u2212 1 , . . . , |X | \u2212 1 |X | \u2212 1 = 1 } \u2022 the hypotheses correspond to the |X | + 1 thresh-\nolds b \u2208 { \u22121, 12(|X |\u22121) , 3 2(|X |\u22121) , . . . , 2|X | 2(|X |\u22121) } and hb(x) = 1 if x \u2264 b and 0 otherwise. The number of hypotheses is equal to |Hth| = |X |+ 1.\nThe graph that corresponds to Hth is a bipartite graph of size (|X |+1)\u00d7|X | and the edge (hb, x) exists in the graph\nif hb(x) = 1, see Figure 1. Viewing the class as a graph enables us to examine some properties of the graph; e.g., the number of edges in this graph is |X |(|X |+1)2 = |Hth||X | 2 which is exactly half of the maximal number of edges |Hth||X |.\nAnother hypothesis class we consider is paritiesHparity :\n\u2022 the examples are all the binary points in {0, 1}log2 |X |, where |X | is a power of 2.\n\u2022 the hypotheses correspond to all subsets C \u2286 {0, 1}n, except the empty one and hC(x) = \u2211 i\u2208C xi, where\nxi is the i coordinate of x. The size of the hypothesis class is |Hparity| = |X | \u2212 1\nThe number of edges in the graph is equal to |Hparity| |X |2 which is half of all the possible edges |Hparity||X |."}, {"heading": "4.2. Mixing Graphs", "text": "In Section 5 we will utilize this new view of hypothesis classes as graphs to deduce the unlearnability of some classes when the learning algorithm has bounded memory. This result will hold for classes with hypotheses graphs that are \u201cclose\u201d to random. There are many ways to be close to random. One natural way is by using the edge-distribution, as discussed below.\nFor convenience we only consider hypothesis classes where on average over the hypotheses h, the number of examples x with h(x) = 1 is roughly the same as the number of examples with h(x) = 0; i.e., Ex,h[h(x) = 1] is close to 1/2. Equivalently, out of all the possible numbers of edges in the graph, |H| \u00b7 |X |, there will be very close to |H|\u00b7|X |\n2 edges in the graph. Stated differently, the expected number of edges between random subsets of vertices S, T with |S| = s, |T | = t is approximately e(S, T ) \u2248 st2 . In a random graph we expect that the number of edges between any two subsets will be close to their average, up to the standard deviation \u221a st. More formally,\nDefinition 6 (d-mixing). A bipartite graphG = (A,B,E) is d-mixing if for any T \u2286 A,S \u2286 B with |S| = s, |T | = t it holds that \u2223\u2223\u2223\u2223e(S, T )\u2212 st2\n\u2223\u2223\u2223\u2223 \u2264 d\u221ast. We remark that the latter definition can be stated for graphs whose density, e(A,B)|A||B| , is differs from half (Krivelevich & Sudakov, 2006)1.\nNote that the graph is closer to random as d gets smaller. We can find an immediate upper bound for d since we can\n1A more general definition of d-mixing is: for any T \u2286\nbound the number of edges between any sets S and T by 0 \u2264 e(S, T ) \u2264 st, thus\n|e(S, T )\u2212 st/2| \u2264 st/2 = \u221a st\n2\n\u221a st \u2264\n\u221a |A||B|\n2\n\u221a st.\nThus we get that 0 \u2264 d \u2264 \u221a |A||B|. Now we are ready to formally define mixing complexity.\nDefinition 7 (mixing complexity). For hypothesis class H over X denote by dmin(H) the minimal value such that H is dmin-mixing. The mixing complexity ofH is\nMC(H) = \u221a |H||X |\ndmin(H) .\nWe say that a class is mixing if it is d-mixing with d = O( \u221a X ), or equivalently if MC(H) = \u2126( \u221a H).\nLet us explore the mixing complexity of the hypothesis classes that we considered earlier. We will prove that the class Hth is \u2126( \u221a |Hth||X |)-mixing; i.e., MC(Hth) = O(1), which means that this class is mixing. To show this take the first half of the hypothesis T0 = {hb : 0 \u2264 b < 1/2} with |T0| = t and the last half of the examples S0 = {x |1/2 < x \u2264 1} with |S0| = s. By the definition of Hth there are no edges between these two sets, i.e., e(S0, T0) = 0, see Figure 2. However, we expect a great deal of edges st/2 = \u2126(|Hth||X |) between these two large sets. Hence, ifHth is d-mixing, then\nd \u2265 \u2223\u2223e(S0, T0)\u2212 st2 \u2223\u2223\u221a\nst =\n\u221a st\n2 = \u2126(\n\u221a |Hth||X |).\nOn the other hand, the class Hparity is very close to random: Lindsey\u2019s Lemma states that MC(Hparity) = \u2126( \u221a |Hparity|).\nOne might wonder what kind of classes are more abundant, the d-mixing with small or large d. Apparently almost all classes H have small d = O( \u221a |X |), i.e., MC(H) =\n\u2126( \u221a |H|) (Krivelevich & Sudakov, 2006). This fact can be proved using the Chernoff bound and the union bound, similar to the way one can prove that O(log |H|) examples are enough to learn any classH without memory constraints."}, {"heading": "5. Unlearnability of Mixing Classes with Bounded-Memory Algorithm", "text": "In this section we restate the main theorem proved in (Moshkovitz & Moshkovitz, 2017). This theorem shows\nA,S \u2286 B with |S| = s, |T | = t it holds that\u2223\u2223\u2223\u2223\u2223e(S, T )\u2212 ste(A,B) |A||B| \u2223\u2223\u2223\u2223\u2223 \u2264 d\u221ast.\nthat for any hypothesis class H that is mixing, any learning algorithm with a bounded memory cannot learn H. By cannot learn we mean that the number of examples needed to learn the class is at least |H|c, for some small constant c > 0. Recall that if the memory is not bounded, number of examples needed to learnH is O(log |H|). The intuition behind this result is that if the memory is bounded, the learning algorithm must cope with the situation that many labeled examples S will lead to the same memory state m. For graphs that are mixing, most hypotheses h are almost equally alike from the point of view of the memory m.\nWe are interested in classes that are close enough to random such that d2 = O(|X ||H|a), for some small enough constant a \u2265 0. For the parity class we described in the last section this holds since d2 = O(|X |). For such classes\n(Moshkovitz & Moshkovitz, 2017) proved that a boundedmemory algorithm cannot learn this class. Specifically,\nTheorem 8. Suppose that hypothesis classH over domain X is d-mixing with d2 = |X ||H|a for some constant a \u2208 [0, 1], |H| is at least some constant, and \u2223\u2223\u2223 e(H,X )|H| \u2212 |X |2 \u2223\u2223\u2223 \u2264 d \u221a |X | |H| , then for any constant s \u2208 (0, 1) there is a constant s\u2032 > 0 such that any learning algorithm for H that has at most\n|H|1.25\u2212s\u22123a\nmemory states and returns the underlying hypothesis (or an approximation of it) with a probability of at least 1/3 must observe at least |H|s\u2032 labeled examples.\nNote that the statement is trivially true if the number of memory states is smaller than |H| (since |H|memory states are needed to exactly distinguish |H| possible hypotheses). Thus, the theorem is interesting solely for a < 1/12. Recall that most problems with |H| \u2248 |X | (and specifically parity) have d2 = O(|X |). Thus, from the previous theorem we have that for a learning algorithm with (roughly) at most |H|1.25 memory states, the number of examples needed is exponentially larger than in the case of unbounded memory.\nLet us revisit the classes we introduced in the previous section and explore their unlearnability with a boundedmemory learning algorithm. We proved that the class Hth is \u2126( \u221a |Hth||X |)-mixing and indeed this class can be learned with a bounded-memory algorithm: on each step it saves a lower and an upper threshold blower \u2264 bupper that represent a regime where the correct hypothesis must be located. If the algorithm received a labeled example (x, 0) with blower \u2264 x \u2264 bupper then bupper := x else blower := x. This algorithm does not use many examples to find an approximation of the underlying hypothesis (the examples are expected to decrease the regime bupper\u2212 blower by some constant factor). Note that even a generalization of Hth, the class of classification using halfspaces, can be learned with a bounded-memory algorithm, called the Perceptron algorithm (Rosenblatt, 1958; Shalev-Shwartz & Ben-David, 2014).\nWe mentioned that the parity class Hparity is O( \u221a |X |)- mixing and indeed it was recently discovered that it cannot be learned with even slightly bounded memory (Raz, 2016). In the last section we proved that most problems are close to random; thus, we can deduce that most problems cannot be learned with memory that is bounded by nearly |H|1.25."}, {"heading": "5.1. The Inability of a Neural Network to Learn Most Classes", "text": "In Section 3.1 we showed that the most frequently used training algorithm implemented for learning a neural network is the SGD which is a bounded-memory algorithm. In Section 3.2 we explained why any computation made by the brain (i.e., a biologically plausible computation) must be a bounded-memory algorithm. From Theorem 8 presented in this section we get as a corollary that classes that are O( \u221a |X |)-mixing cannot be learned by neural networks. From Section 4, we also get that most hypothesis classes cannot be learned by neural networks."}, {"heading": "6. What Can be Learned", "text": "In the previous section we explained why most hypothesis classes cannot be learned with a bounded-memory algorithm and specifically by a neural network. One might wonder which (and how) classes can be learned with a boundedmemory algorithm. Can all the classes that are d-mixing with large d be learned with a bounded-memory algorithm? In Section 5 we gave as an example the class Hth of discrete threshold functions in [0, 1] that is \u2126( \u221a |Hth||X |)- mixing and can easily be learned with a bounded-memory algorithm. In this section we consider other natural classes that have sufficient partitions (will be defined formally later, but for now think of them as \u201csymmetries\u201d that exist in the class). We prove that such classes are d-mixing with large d. We then cite evidence (empirically and theoretically) that these problems can be learned with a boundedmemory algorithm.\nA convolutional neural network (CNN) is a type of artificial neural network inspired from the animal visual cortex. It is a powerful model to solve problems in machine learning and computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015). The core idea is to utilize the translation symmetry that exist in images. Recently this idea has been generalized to other symmetries (Gens & Domingos, 2014; Dieleman et al., 2016; Cohen & Welling, 2016).\nAnother form of symmetry was presented in (Kadmon & Sompolinsky, 2016). They considered classes where a small change in the input is considered as noise and thus should be labeled similarly. They presented a bounded memory algorithm in the form of a small neural network that is able to learn this class.\nMore generally we say that a class H has an r-sufficient partition if there is a partition X = \u22c3\u0307 Xi of all the examples into r parts, such that all hypotheses h \u2208 H assign the same value to each part (i.e., for each i it holds that for all x \u2208 Xi, h(x) is equal to each other). In the next claim we prove that if a class has an r-sufficient partition with small\nr, its mixing complexity is small, and thus it is possible that H be learned by a bounded-memory algorithm. Claim 9. For any classH that has an r-sufficient partition, its mixing complexity is bounded by MC(H) = O( \u221a r).\nProof. Since there are r parts in the partition there is at least one part X with size at least |X |r . At least half of the hypotheses H \u2286 H (i.e., |H| \u2265 |H|/2) either all agree or all disagree with the examples in X . Thus,\u2223\u2223\u2223\u2223e(H,X)\u2212 |H||X|2\n\u2223\u2223\u2223\u2223 \u2265 |H||X|2 . Hence, if the classH is d-mixing, then\n2d \u2265 |H||X|\u221a |H||X|\n= \u221a |H||X| \u2265 \u221a |H| 2 |X | r .\nThusH is \u2126 (\u221a\n|H||X | r\n) -mixing."}, {"heading": "7. Rethinking Generalization to Understand Deep Learning", "text": "In this section we suggest an answer to the open problem presented in (Zhang et al., 2017):\nWhat distinguishes neural networks that generalize well from those that don\u2019t?\nElegantly (Zhang et al., 2017) put their finger on a tremendous gap in current research on deep learning: the inability to know when the test and training errors are close. In other words, it is desired to distinguish the classes that have an inherent large generalization error, regardless of the specific learning algorithm used. This contrasts with to uniform stability (Kearns & Ron, 1999; Bousquet & Elisseeff, 2002) that consider whether a specific learning algorithm does not have a large generalization error. In other words, uniform stability is a property of an algorithm and not of the hypothesis class.\nThese authors (Zhang et al., 2017) illustrated this gap nicely in the following set of experiments. They used two known natural image classification datasets (CIFAR10 dataset, (Krizhevsky & Hinton, 2009), and the ImageNet (Russakovsky et al., 2015) dataset). They trained a few known neural network architectures using SGD and got small training and test errors; i.e., a small generalization error. Then they conducted several experiments, each involving changes in the datasets in some random way. In one experiment they changed the labels to be completely random. In another experiment they used varying levels of label corruptions. They also experimented with randomly\nchanging the pixels of the image. In all of these experiments the test error naturally increased. Perhaps surprisingly these state-of-the-art convolutional networks for image classification trained with stochastic gradient methods had a very small training error; i.e., we view a large generalization error. They also tried to use different kinds of regularizations (e.g., dropout and weight decay) but concluded that this is unlikely that it is the fundamental reason for generalization.\nThey also explained why known complexity measures used in machine learning cannot account for their results. For example, they pointed out that in the regime they considered the VC-dimension leads to trivial bounds. Specifically, they considered neural networks with a number of parameters that was large compared to the sample size. Since the VC-dimension of a class with more than n parameters is at least n, it does not help to answer their open problem. The concept of uniform stability does not help either because it is a property of an algorithm and not of the hypothesis class. Thus, the conventional wisdom that that tries to answer their open problem by using either properties of the hypothesis class (e.g., VC-dimension), or regularization techniques used during training is flawed.\nUnlike other measures mentioned in (Zhang et al., 2017), the mixing complexity is able to distinguish between a random hypothesis class and class with natural images. As we proved in Section 4 a random class is d-mixing with small d. On the other hand, natural images have some symmetries, and thus not mixing (see Section 6), and there has been an extensive use of the fact that natural images contain a great deal of structure (e.g., (Krizhevsky et al., 2012; Gens & Domingos, 2014; Dieleman et al., 2016; Cohen & Welling, 2016))\nNote that we cannot use (Raz, 2017; Moshkovitz & Moshkovitz, 2017) to justify the use of the mixing complexity in this context. In these papers the assumption is that the examples are randomly chosen from X on each step. However, in (Zhang et al., 2017), there was excessive use of the training data; i.e., similar examples were used multiple times. This begs the question as to whether the proof of (Raz, 2017; Moshkovitz & Moshkovitz, 2017) can be generalized to the setting in (Zhang et al., 2017) as well."}, {"heading": "8. Hardness and Robustness of Mixing Classes", "text": "In this section we prove that mixing complexity has several desirable properties. First, we prove that any class H that is mixing has a VC-dimension \u2126(log |H|). Since the VCdimension of every classH is at most log2 |H|, we get that such classes are the hardest to learn (up to a constant factor) without memory constraints. Furthermore, we prove\nthat mixing complexity is robust under small perturbations. That is, if a small number of labels is changed, the mixing complexity is approximately unchanged."}, {"heading": "8.1. Mixing Hypothesis Classes and VC-dimension", "text": "In this section we focus on hypothesis classes that are mixing and we try to understand the hardness of learning these classes without memory constraints. As discussed in Section 2.2, the VC-dimension is used to measure the complexity of learning a class (without memory constraints). We prove that mixing classes H have V Cdim(H) = \u2126(logH). To show this, we will find a set of k = \u2126(logH) examples such that restriction ofH to these k examples results in 2k different vectors of length k. To find these k samples we note that the first example x1 splits the set of all hypotheses inH into two: all hypotheses h with h(x1) = 1 all hypotheses h with h(x1) = 0. The idea of the proof is that for most examples the size of the two sets is almost equal (see Claim 10). The second example x2 splits each of these sets into two again, depending on whether h(x2) = 1 or not. So in total we have a partition of H into four parts, one that contains all hypotheses with h(x1) = 1 and h(x2) = 1, the second that contains all hypotheses with h(x1) = 1 and h(x2) = 0, the third that contains all hypotheses with h(x1) = 0 and h(x2) = 1, and the fourth that contains all hypotheses with h(x1) = 0 and h(x2) = 0. We will prove that for most examples these four parts are almost equal (we apply Claim 10 again). Continuing in a similar way for k = \u2126(logH) examples we get that there are k examples that define a partition with 2k parts where none of them is empty.\nClaim 10. For any d-mixing graph (A,B,E), any T \u2286 B, and any > 0, except for 2d 2\n|T | 2 vertices in a \u2208 A,\u2223\u2223\u2223\u2223|\u0393(a) \u2229 T | \u2212 |T |2 \u2223\u2223\u2223\u2223 \u2264 |T |\nProof. Fix T \u2286 B. Denote by A1 the set of vertices that have too many edges into T ; i.e.,\nA1 = {a \u2208 A : |\u0393(a) \u2229 T | > (1/2 + )|T |}.\nThus, e(A1, T ) > (1/2+ )|T ||A1|. From the mixing property we know that\ne(A1, T ) \u2264 |T ||A1| 2 + d \u221a |A1||T |.\nCombining the last two inequalities we get that\n(1/2 + )|T ||A1| < |T ||A1| 2 + d \u221a |T ||A1|.\nThus,\n|A1| < d2\n|T | 2\nSimilarly we can define the set A2 \u2286 A that have too few edges into T ; i.e., A2 = {a \u2208 A : |\u0393(a) \u2229 T | < (1/2 \u2212 )|T |}, and prove that this set is small.\nCorollary 11. For any d-mixing hypothesis classH its VCdimension is at least\nV Cdim(H) = \u2126 ( min { log |H||X |\nd2 , logH\n}) .\nProof. Use Claim 10 with = 1/4. In each step i \u2264 k, each part will be of size at least |H|4i .\nIn each step i \u2264 k we remove at most 2i+1 d 24i\n|H| 2 examples (since there are 2i parts in the partition in each step). In total we remove at most 8k+3 d 2\n|H| examples which is smaller\nthan |X | for k + 3 \u2264 log8 |H||X | d2 .\nThus, in the last step, k, each part is of size at least 1 for k \u2264 log4 |H|, as we wanted to prove.\nFrom the previous corollary we can deduce that mixing hypothesis classes are the hardest problems since V Cdim(H) = O(logH).\nIn (Haussler et al., 1996; Langford & McAllester, 2000) it was suggested that VC-dimension is too crude to be a measure of the number of examples needed to learn. They showed how to use the shell decomposition method to get better bounds. In this method the hypotheses are split according to their test errors. Hypotheses with similar test error are considered in the same shell. The number of samples needed to learn can be smaller than the one required by the VC-dimension if the size of the shells is not too large. For mixing classes, different hypotheses differ on a substantial number of labeled examples (for the most part), (Moshkovitz & Moshkovitz, 2017). Thus for classes that are mixing the size of the shells is large. This fact strengthens our understanding that classes that are mixing are indeed the hardest problems to learn."}, {"heading": "8.2. Small Perturbation of Mixing Classes", "text": "In our exploration of the mixing complexity, we would like to know how a small perturbation of a class can change the mixing property. Specifically, we would like to know whether a small change to a class that is d-mixing with small d is a d\u2032-mixing with small d\u2032. The next claim answers this question in the affirmative.\nClaim 12. If a hypothesis class H is d-mixing then by changing the labels of at most b examples, the resulting class is d + \u221a b mixing.\nProof. For any T \u2286 H, S \u2286 X denote by e\u2032(S, T ) the number of edges between S and T in the hypotheses graph\nafter the change of b labels. Fix T \u2286 H, S \u2286 X with |S| = s and |T | = t. Our goal is to show that\u2223\u2223\u2223\u2223e\u2032(S, T )\u2212 st2\n\u2223\u2223\u2223\u2223 \u2264 (d +\u221ab)\u221ast. Let us bound the left hand side\u2223\u2223\u2223\u2223e\u2032(S, T )\u2212 st2 \u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223e(S, T )\u2212 st2 \u2223\u2223\u2223\u2223+ min(b, st)\n\u2264 d \u221a st+ min(b, st) \u2264 (d + \u221a b) \u221a st"}, {"heading": "9. Conclusions and Open Problems", "text": "In this paper we showed the relationships between both artificial and biological neural networks to mixing complexity. We showed that it follows from previous papers that if the data is drawn i.i.d from X , artificial and biological neural network cannot learn most classes. Empirically it is known that problems of interest do get solved. One possible explanation for this apparent contradiction is that the data processed by neural networks in practice might not be i.i.d. For example, in many applications of artificial neural networks the same example repeats itself many times. Thus, it would be interesting to generalize the results in (Raz, 2017; Moshkovitz & Moshkovitz, 2017) to this data acquisition setting. Another possible explanation is that problems of interest are not mixing and have a great deal of \u201cstructure\u201d to them. We suggested using the notion of rsufficient partitions to formalize the notion of \u201cstructure\u201d. We showed that classes that have such a partition are not mixing. It would be interesting to prove that these classes can be learned with a bounded-memory algorithm.\nIn this paper we also showed that hypothesis classes that are mixing are the hardest learning problems since their VC-dimension is \u0398(log |H|), which is the maximal value possible. We also showed that these classes are robust in the sense that under a small perturbation of the labels of at most b examples, the mixing complexity can increase by at most \u221a b."}, {"heading": "Acknowledgements", "text": "This work is partially supported by the Gatsby Charitable Foundation, The Israel Science Foundation, and Intel ICRICI center. M.M. is grateful to the Harry and Sylvia Hoffman Leadership and Responsibility Program."}], "references": [{"title": "Training a 3-node neural network is NP-complete", "author": ["Blum", "Avrim", "Rivest", "Ronald L"], "venue": "In Proceedings of the 1st International Conference on Neural Information Processing Systems,", "citeRegEx": "Blum et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1988}, {"title": "Stability and generalization", "author": ["Bousquet", "Olivier", "Elisseeff", "Andr\u00e9"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2002}, {"title": "Group equivariant convolutional networks", "author": ["Cohen", "Taco S", "Welling", "Max"], "venue": "In ICML,", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "author": ["Daniely", "Amit", "Frostig", "Roy", "Singer", "Yoram"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Daniely et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2016}, {"title": "Exploiting cyclic symmetry in convolutional neural networks", "author": ["Dieleman", "Sander", "De Fauw", "Jeffrey", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1602.02660,", "citeRegEx": "Dieleman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2016}, {"title": "The power of depth for feedforward neural networks", "author": ["Eldan", "Ronen", "Shamir", "Ohad"], "venue": "In COLT,", "citeRegEx": "Eldan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eldan et al\\.", "year": 2016}, {"title": "Deep symmetry networks", "author": ["Gens", "Robert", "Domingos", "Pedro M"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Gens et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gens et al\\.", "year": 2014}, {"title": "Rigorous learning curve bounds from statistical mechanics", "author": ["Haussler", "David", "Kearns", "Michael", "Seung", "H Sebastian", "Tishby", "Naftali"], "venue": "Machine Learning,", "citeRegEx": "Haussler et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Haussler et al\\.", "year": 1996}, {"title": "Optimal architectures in a solvable model of deep networks", "author": ["Kadmon", "Jonathan", "Sompolinsky", "Haim"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kadmon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadmon et al\\.", "year": 2016}, {"title": "Algorithmic stability and sanity-check bounds for leave-one-out cross-validation", "author": ["Kearns", "Michael", "Ron", "Dana"], "venue": "Neural computation,", "citeRegEx": "Kearns et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "Time-space hardness of learning sparse parities", "author": ["G. Kol", "R. Raz", "A. Tal"], "venue": "Electronic Colloquium on Computational Complexity (ECCC),", "citeRegEx": "Kol et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kol et al\\.", "year": 2016}, {"title": "Pseudo-random graphs. In More sets, graphs and numbers, pp. 199\u2013262", "author": ["Krivelevich", "Michael", "Sudakov", "Benny"], "venue": null, "citeRegEx": "Krivelevich et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Krivelevich et al\\.", "year": 2006}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Computable shell decomposition bounds", "author": ["Langford", "John", "McAllester", "David A"], "venue": "In COLT, pp", "citeRegEx": "Langford et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2000}, {"title": "On the computational efficiency of training neural networks", "author": ["Livni", "Roi", "Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Livni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Livni et al\\.", "year": 2014}, {"title": "Mixing implies lower bounds for space bounded learning", "author": ["Moshkovitz", "Dana", "Michal"], "venue": "ECCC,", "citeRegEx": "Moshkovitz et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Moshkovitz et al\\.", "year": 2017}, {"title": "On the expressive power of deep neural networks", "author": ["Raghu", "Maithra", "Poole", "Ben", "Kleinberg", "Jon", "Ganguli", "Surya", "Sohl-Dickstein", "Jascha"], "venue": "arXiv preprint arXiv:1606.05336,", "citeRegEx": "Raghu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raghu et al\\.", "year": 2016}, {"title": "Fast learning requires good memory: A timespace lower bound for parity learning", "author": ["Raz", "Ran"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Raz and Ran.,? \\Q2016\\E", "shortCiteRegEx": "Raz and Ran.", "year": 2016}, {"title": "A time-space lower bound for a large class of learning problems", "author": ["Raz", "Ran"], "venue": "ECCC,", "citeRegEx": "Raz and Ran.,? \\Q2017\\E", "shortCiteRegEx": "Raz and Ran.", "year": 2017}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["Rosenblatt", "Frank"], "venue": "Psychological review,", "citeRegEx": "Rosenblatt and Frank.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt and Frank.", "year": 1958}, {"title": "Depth separation in relu networks for approximating smooth non-linear functions", "author": ["Safran", "Itay", "Shamir", "Ohad"], "venue": "arXiv preprint arXiv:1610.09887,", "citeRegEx": "Safran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Safran et al\\.", "year": 2016}, {"title": "Understanding machine learning: From theory to algorithms", "author": ["Shalev-Shwartz", "Shai", "Ben-David"], "venue": "Cambridge university press,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2014}, {"title": "Fundamental limits of online and distributed algorithms for statistical learning and estimation", "author": ["O. Shamir"], "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems,", "citeRegEx": "Shamir,? \\Q2014\\E", "shortCiteRegEx": "Shamir", "year": 2014}, {"title": "Distribution-specific hardness of learning neural networks", "author": ["Shamir", "Ohad"], "venue": "arXiv preprint arXiv:1609.01037,", "citeRegEx": "Shamir and Ohad.,? \\Q2016\\E", "shortCiteRegEx": "Shamir and Ohad.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Deep learning and the information bottleneck principle", "author": ["Tishby", "Naftali", "Zaslavsky", "Noga"], "venue": "In Information Theory Workshop (ITW),", "citeRegEx": "Tishby et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tishby et al\\.", "year": 2015}, {"title": "A theory of the learnable", "author": ["Valiant", "Leslie G"], "venue": "Communications of the ACM,", "citeRegEx": "Valiant and G.,? \\Q1984\\E", "shortCiteRegEx": "Valiant and G.", "year": 1984}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"], "venue": "In Internation Conference on Learning Representations (oral presentation),", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 3, "context": "Understanding neural network learning is an active research area in machine learning and neuroscience (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014; Tishby & Zaslavsky, 2015; Kadmon & Sompolinsky, 2016).", "startOffset": 102, "endOffset": 297}, {"referenceID": 17, "context": "Understanding neural network learning is an active research area in machine learning and neuroscience (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014; Tishby & Zaslavsky, 2015; Kadmon & Sompolinsky, 2016).", "startOffset": 102, "endOffset": 297}, {"referenceID": 15, "context": "Understanding neural network learning is an active research area in machine learning and neuroscience (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014; Tishby & Zaslavsky, 2015; Kadmon & Sompolinsky, 2016).", "startOffset": 102, "endOffset": 297}, {"referenceID": 23, "context": "In recent years, several works have shown that under memory constraints numerous examples are needed in order to learn certain hypothesis classes (Shamir, 2014; Raz, 2016; Kol et al., 2016; Moshkovitz & Moshkovitz, 2017; Raz, 2017).", "startOffset": 146, "endOffset": 231}, {"referenceID": 10, "context": "In recent years, several works have shown that under memory constraints numerous examples are needed in order to learn certain hypothesis classes (Shamir, 2014; Raz, 2016; Kol et al., 2016; Moshkovitz & Moshkovitz, 2017; Raz, 2017).", "startOffset": 146, "endOffset": 231}, {"referenceID": 13, "context": "The above might seem to contradict the fact that, empirically, neural networks do indeed learn (Krizhevsky et al., 2012; LeCun et al., 2015).", "startOffset": 95, "endOffset": 140}, {"referenceID": 28, "context": "In addition we discuss an application of mixing complexity as a response to a question raised by (Zhang et al., 2017).", "startOffset": 97, "endOffset": 117}, {"referenceID": 28, "context": "In Section 7 we describe the work done in (Zhang et al., 2017) and its connection to mixing complexity.", "startOffset": 42, "endOffset": 62}, {"referenceID": 3, "context": "This has led many researchers to attempt to understand the reasons for the astonishing success despite the proven hardness (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014).", "startOffset": 123, "endOffset": 264}, {"referenceID": 17, "context": "This has led many researchers to attempt to understand the reasons for the astonishing success despite the proven hardness (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014).", "startOffset": 123, "endOffset": 264}, {"referenceID": 15, "context": "This has led many researchers to attempt to understand the reasons for the astonishing success despite the proven hardness (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014).", "startOffset": 123, "endOffset": 264}, {"referenceID": 13, "context": "It is a powerful model to solve problems in machine learning and computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015).", "startOffset": 81, "endOffset": 156}, {"referenceID": 4, "context": "Recently this idea has been generalized to other symmetries (Gens & Domingos, 2014; Dieleman et al., 2016; Cohen & Welling, 2016).", "startOffset": 60, "endOffset": 129}, {"referenceID": 28, "context": "In this section we suggest an answer to the open problem presented in (Zhang et al., 2017):", "startOffset": 70, "endOffset": 90}, {"referenceID": 28, "context": "Elegantly (Zhang et al., 2017) put their finger on a tremendous gap in current research on deep learning: the inability to know when the test and training errors are close.", "startOffset": 10, "endOffset": 30}, {"referenceID": 28, "context": "These authors (Zhang et al., 2017) illustrated this gap nicely in the following set of experiments.", "startOffset": 14, "endOffset": 34}, {"referenceID": 28, "context": "Unlike other measures mentioned in (Zhang et al., 2017), the mixing complexity is able to distinguish between a random hypothesis class and class with natural images.", "startOffset": 35, "endOffset": 55}, {"referenceID": 13, "context": ", (Krizhevsky et al., 2012; Gens & Domingos, 2014; Dieleman et al., 2016; Cohen & Welling, 2016))", "startOffset": 2, "endOffset": 96}, {"referenceID": 4, "context": ", (Krizhevsky et al., 2012; Gens & Domingos, 2014; Dieleman et al., 2016; Cohen & Welling, 2016))", "startOffset": 2, "endOffset": 96}, {"referenceID": 28, "context": "However, in (Zhang et al., 2017), there was excessive use of the training data; i.", "startOffset": 12, "endOffset": 32}, {"referenceID": 28, "context": "This begs the question as to whether the proof of (Raz, 2017; Moshkovitz & Moshkovitz, 2017) can be generalized to the setting in (Zhang et al., 2017) as well.", "startOffset": 130, "endOffset": 150}, {"referenceID": 7, "context": "In (Haussler et al., 1996; Langford & McAllester, 2000) it was suggested that VC-dimension is too crude to be a measure of the number of examples needed to learn.", "startOffset": 3, "endOffset": 55}], "year": 2017, "abstractText": "We suggest analyzing neural networks through the prism of space constraints. We observe that most training algorithms applied in practice use bounded memory, which enables us to use a new notion introduced in the study of spacetime tradeoffs that we call mixing complexity. This notion was devised in order to measure the (in)ability to learn using a bounded-memory algorithm. In this paper we describe how we use mixing complexity to obtain new results on what can and cannot be learned using neural networks.", "creator": "LaTeX with hyperref package"}}}