{"id": "1703.00955", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Toward Controlled Generation of Text", "abstract": "generic generation and manipulation of text is challenging and has limited success compared to recent deep category modeling in visual domain. this paper aims at generating plausible natural language sentences, whose identities are dynamically linked by learning disentangled sentence representations with designated domains. we propose a new neural evolution software which combines 2d auto - encoders and holistic attribute discriminators for effective treatment of semantic structures. with differentiable approximation combining discrete text samples, explicit constraints on independent model controls, and efficient collaborative learning of generator and discriminators, our mouse learns highly interpretable representations from even only word annotations, and produces realistic sentences establishing desired accuracy. quantitative evaluation validates the accuracy of sentence and attribute generation.", "histories": [["v1", "Thu, 2 Mar 2017 21:23:47 GMT  (84kb,D)", "http://arxiv.org/abs/1703.00955v1", "updated discussions and references"], ["v2", "Tue, 11 Jul 2017 21:15:43 GMT  (85kb,D)", "http://arxiv.org/abs/1703.00955v2", "ICML 2017 camera-ready + more discussions"]], "COMMENTS": "updated discussions and references", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL stat.ML", "authors": ["zhiting hu", "zichao yang", "xiaodan liang", "ruslan salakhutdinov", "eric p xing"], "accepted": true, "id": "1703.00955"}, "pdf": {"name": "1703.00955.pdf", "metadata": {"source": "META", "title": "Controllable Text Generation", "authors": ["Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P. Xing"], "emails": ["zhitingh@cs.cmu.edu", "zichaoy@cs.cmu.edu", "xiaodan1@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "There is a surge of research interest in deep generative models, such as Variational Autoencoders (VAEs) (Kingma & Welling, 2013), Generative Adversarial Nets (GANs) (Goodfellow et al., 2014), and auto-regressive models (van den Oord et al., 2016). Despite their impressive advances in visual domain, such as image generation (Radford et al., 2015), learning interpretable image representations (Chen et al., 2016), and image editing (Zhu et al., 2016), applications to natural language generation have been relatively less studied. Generating realistic text is challenging as the generative models are required to capture complex semantic structures underlying sentences. Previous work have been mostly limited to task-specific applications in supervised settings, including machine translation (Bahdanau et al., 2014) and image captioning (Vinyals et al., 2015). Very few recent attempts of using VAEs (Bowman et al., 2015; Tang et al., 2016) and GANs (Yu et al., 2017; Zhang et al., 2016)\nhave been made to investigate generic text generation, while their generated text is largely randomized and uncontrollable.\nIn this paper we tackle the problem of controllable text generation. That is, we focus on generating realistic natural language sentences, whose attributes can be dynamically controlled by learning disentangled latent representations. To enable the manipulation of generated sentences, a few challenges need to be addressed.\nA first challenge comes from the discrete nature of text samples. The resulting non-differentiability hinders the use of global discriminators that assess generated samples and back-propagate gradients to guide the optimization of generators in a holistic manner, as shown to be highly effective in continuous image generation and representation modeling (Chen et al., 2016; Larsen et al., 2016; Dosovitskiy & Brox, 2016). A number of recent approaches attempt to address the non-differentiability through policy learning (Yu et al., 2017) which tends to suffer from high variance during training, or continuous approximations (Zhang et al., 2016; Kusner & Hernndez-Lobato, 2016) where only preliminary qualitative results are presented. As an alternative to the discriminator based learning, semi-supervised VAEs (Kingma et al., 2014) minimize element-wise reconstruction error on observed examples and are applicable to discrete visibles. This, however, loses the holistic view of full sentences and can be inferior especially for modeling global abstract attributes (e.g., sentiment).\nAnother challenge for controllable generation relates to learning disentangled latent representations. Interpretability expects each part of the latent representation to govern and only focus on one aspect of the samples. Prior methods (Chen et al., 2016; Odena et al., 2016) on structured representation learning lack explicit enforcement of the independence property on the full latent representation, and varying individual code may result in unexpected variation of other unspecified attributes in addition to the desired one.\nIn this paper, we propose a new text generative model that addresses the above issues, permitting highly disentangled representations with designated semantic structure,\nar X\niv :1\n70 3.\n00 95\n5v 1\n[ cs\n.L G\n] 2\nM ar\n2 01\n7\nand generating sentences with dynamically specified attributes. We base our generator on VAEs in combination with holistic discriminators of attributes for effective imposition of structures on the latent code. End-to-end optimization is enabled with differentiable softmax approximation which anneals smoothly to discrete case and helps fast convergence. The probabilistic encoder of VAE also functions as an additional discriminator to capture variations of implicitly modeled aspects, and guide the generator to avoid unexpected entanglement during attribute code manipulation.\nOur model can be interpreted as enhancing VAEs with an extended wake-sleep procedure (Hinton et al., 1995), where the sleep phase enables incorporation of generated samples for learning both the generator and discriminators in an alternating manner. The generator and the discriminators effectively provide feedback signals to each other, resulting in an efficient mutual bootstrapping framework. We show a little supervision (e.g., 100s of annotated sentences) is sufficient to learn structured representations.\nBoth quantitative and qualitative experiments demonstrate the efficacy of our method. We apply our model to generate sentences with controlled sentiment and tenses. Our method improves over previous generative models on the accuracy of generating specified attributes as well as performing classification using generated samples. We show our method learns highly disentangled representations from only word-level labels, and produces plausible sentences."}, {"heading": "2. Related Work", "text": "Recent research has made remarkable progress in deep generative models. Variational Autoencoders (VAEs) (Kingma & Welling, 2013) consist of encoder and generator networks which encode a data example to a latent representation and generate samples from the latent space, respectively. The model is trained by maximizing a variational lower bound on the log-likelihood of observed data under the generative model. The vanilla VAEs are incompatible with discrete latent variables as they hinder differentiable reparamerization for learning the encoder. The wake-sleep algorithm (Hinton et al., 1995) introduced for training deep directed graphical models shares similarity with VAEs by also combining an inference network with the generator. The wake phase updates the generator with the samples generated from the inference network on the training data, while the sleep phase updates the inference network based on the samples from the generator. Our method effectively combines VAEs with an extended wake-sleep algorithm in which the sleep procedure updates both the generator and the inference network (i.e., discriminators), enabling efficient collaborative semi-supervised learning.\nBesides reconstruction in the raw data space, discriminatorbased metric provides a different way for learning the generator, in which the discriminator assesses synthesized samples from the generator and feedbacks learning signals. For instance, GANs (Goodfellow et al., 2014) use a discriminator to feedback the probability of a sample being recognized as a real example. Larsen et al. (2016) combine VAEs with GANs for enhanced image generation. Dosovitskiy & Brox (2016); Gatys et al. (2015) use discriminator networks for measuring high-level perceptual similarity. Applying discriminators to text generation is difficult due to the non-differentiability of the discrete samples. Yu et al. (2017) resort to policy learning which tends to have high variance during training. Zhang et al. (2016); Kusner & Hernndez-Lobato (2016) apply continuous approximations with only preliminary qualitative results. Bowman et al. (2015); Tang et al. (2016) instead use VAEs without discriminators. All these text generation methods do not learn disentangled latent representations, resulting in randomized and uncontrollable samples. In contrast, disentangled generation in visual domain has made impressive progress. For instance, InfoGAN (Chen et al., 2016), which resembles the sleep procedure of our extended VAE/wakesleep algorithm, disentangles latent representation in an unsupervised manner, while the semantic of each dimension is observed after training rather than designated by users in a controllable way. Siddharth et al. (2017); Kulkarni et al. (2015); Kingma et al. (2014) build in the context of VAEs and obtain disentangled image representations with semi-supervised learning. In contrast, our model combines VAEs with discriminators which provide a better, holistic metric compared to element-wise reconstruction, and thus boost effective imposition of semantic structures. Moreover, most of these approaches have only focused on the disentanglement of the structured part of latent representations, while ignoring potential dependence of the structured latent code with attributes not explicitly encoded. We address this by introducing an independency constraint, and show its effectiveness for improving interpretability."}, {"heading": "3. Controllable Text Generation", "text": "Our model aims to generate plausible sentences conditioned on representation vectors which are endowed with designated semantic structures. For instance, to control sentence sentiment, our model allocates one dimension of the latent representation to encode \u201cpositive\u201d and \u201cnegative\u201d semantics, and generates samples with desired sentiment by simply specifying a particular code. Benefiting from the disentangled structure, each such code is able to capture a salient attribute and is independent with other features. Our deep text generative model possesses several merits compared to prior work, as it 1) facilitates effective imposition of latent code semantics by enabling global dis-\ncriminators to guide the discrete text generator learning; 2) improves model interpretability by explicitly enforcing the constraints on independent attribute controls; 3) permits efficient semi-supervised learning and bootstrapping by synthesizing variational auto-encoders with a tailored wake-sleep approach. In the next, we first present the intuitions and overview of our framework (section 3.1), and then describe our model in detail (section 3.2)."}, {"heading": "3.1. Model Overview", "text": "We build our framework starting from variational autoencoders (section 2) which have been used for text generation (Bowman et al., 2015; Semeniuta et al., 2017), where sentence x\u0302 is generated conditioned on latent code z. The vanilla VAE employs an unstructured vector z in which the dimensions are entangled. To model and control the attributes of interest in an interpretable way, we augment the unstructured variables z with a set of structured variables c each of which targets a salient and independent semantic feature of sentences.\nWe want our sentence generator to condition on the combined vector (z, c), and generate samples that fulfill the attributes as specified in the structured code c. Conditional generation in the context of VAEs is often learned by reconstructing observed examples given their feature code. However, as demonstrated in visual domain, compared to computing element-wise distances in the data space, computing distances in the feature space allows invariance to distracting transformations and provides a better, holistic metric. Thus, for each attribute code in c, we set up an individual discriminator to measure how well the generated samples match the desired attributes, and drive the generator to produce improved results. The difficulty of applying discriminators in our context is that text samples are discrete and non-differentiable, which breaks down gradient propagation from the discriminators to the generator. We use a continuous approximation based on softmax with a decreasing temperature, which anneals to the discrete case as training proceeds. This simple yet effective approach enjoys low variance and fast convergence.\nIntuitively, having an interpretable representation would imply that each structured code in c can independently control its target feature, without entangling with other attributes, especially those not explicitly modeled. We encourage the independency by enforcing those irrelevant attributes to be completely captured in the unstructured code z and thus be separated from c that we will manipulate. To this end, we reuse the VAE encoder as an additional discriminator for recognizing the attributes modeled in z, and train the generator so that these unstructured attributes can be recovered from the generated samples. As a result, varying different attribute codes will keep the unstructured\nattributes invariant as long as z is unchanged.\nFigure 1 shows the overall model structure. Our complete model thus incorporates VAEs and attribute discriminators, in which the VAE component trains the generator to reconstruct real sentences for generating plausible text, while the discriminators enforce the generator to produce attributes coherent with the conditioned code. The attribute discriminators are learned to fit labeled examples to entail designated semantics, as well as trained to explain samples from the generator. That is, the generator and the discriminators form a pair of collaborative learners and provide feedback signals to each other. This collaborative optimization mechanism can be regarded as resembling the wake-sleep algorithm for deep generative models. We show the combined VAE/wake-sleep learning system enables a highly efficient semi-supervised framework, which requires only a little supervision to construct interpretable representation and generation."}, {"heading": "3.2. Model Structure", "text": "We now describe our model in detail, by presenting the learning of generator and discriminators, respectively."}, {"heading": "Generator Learning", "text": "The generator G is constructed with an LSTM-RNN for generating token sequence x\u0302 = {x\u03021, . . . , x\u0302T } conditioned on the latent code (z, c), which depicts a generative distribution:\nx\u0302 \u223c G(z, c) = pG(x\u0302|z, c) = \u220f\nt p(x\u0302t|x\u0302<t,z, c),\n(1)\nwhere x\u0302<t indicates the tokens preceding x\u0302t. The generation thus involves a sequence of discrete decision making which samples a token from a multinomial distribution parametrized using softmax function at each time step t.\nThat is,\nx\u0302t \u223c softmax(ot/\u03c4), (2)\nwhere ot is the logit vector as the inputs to the softmax function, and \u03c4 > 0 is the temperature which is normally set to 1.\nThe unstructured part z of the latent representation is modeled as continuous variables with standard Gaussian prior p(z), while the structured code c can contain both continuous and discrete variables to encode different attributes (e.g., sentiment categories and formality degrees) with appropriate prior p(c). Given observation x, the base VAE also includes a conditional probabilistic encoder E to infer the latents z :\nz \u223c E(x) = qE(z|x). (3)\nLet \u03b8G and \u03b8E denote the parameters of the generator G and the encoder E, respectively. The VAE is then optimized to minimize the reconstruction error of observed real sentences, and at the same time regularize the encoder to be close to the prior p(z):\nLVAE(\u03b8G,\u03b8E ;x) = \u2212 KL(qE(z|x)\u2016p(z)) + EqE(z|x)qD(c|x) [log pG(x|z, c)] , (4)\nwhere KL(\u00b7\u2016\u00b7) is the KL-divergence; and qD(c|x) is the conditional distribution defined by the discriminator D for each structured variable in c:\nD(x) = qD(c|x). (5)\nHere, for notational simplicity, we assume only one structured variable and thus one discriminator, though our model specification can straightforwardly be applied to many attributes. Besides the reconstruction loss which drives the generator to produce realistic sentences, the discriminator provides extra learning signals which enforce the generator to produce coherent attribute that matches the structured code in c. However, as it is impossible to propagate gradients from the discriminator through the discrete samples, we resort to a deterministic continuous approximation. The approximation replaces the sampled token x\u0302t (represented as a one-hot vector) at each step with the probability vector in Eq.(2) which is differentiable w.r.t the generator\u2019s parameters. The probability vector is used as the output at the current step and the input to the next step along the sequence of decision making. The resulting \u201csoft\u201d generated sentence, denoted as G\u0303\u03c4 (z, c), is fed into the discriminator1 to measure the fitness to the target attribute, leading to the following loss for improving G:\nLAttr,c(\u03b8G) = Ep(z)p(c) [ log qD(c|G\u0303\u03c4 (z, c)) ] . (6)\n1The probability vector thus functions to average over the word embedding matrix to obtain a \u201csoft\u201d word embedding at each step.\nThe temperature \u03c4 (Eq.(2)) is set to \u03c4 \u2192 0 as training proceeds, yielding increasingly peaked distributions that finally emulate discrete case. The simple deterministic approximation effectively leads to reduced variance and fast convergence during training, which enables efficient learning of the conditional generator. The diversity of generation results is guaranteed since we use the approximation only for attribute modeling and the base sentence generation is learned through VAEs.\nWith the objective in Eq.(6), each structured attribute of generated sentences is controlled through the corresponding code in c and is independent with other variables in the latent representation. However, it is still possible that other attributes not explicitly modeled may also entangle with the code in c, and thus varying a dimension of c can yield unexpected variation of these attributes we are not interested in. To address this, we introduce the independency constraint which separates these attributes with c by enforcing them to be fully captured by the unstructured part z. Therefore, besides the attributes explicitly encoded in c, we also train the generator so that other non-explicit attributes can be correctly recognized from the generated samples and match the unstructured code z. Instead of building a new discriminator, we reuse the variational encoder E which serves precisely to infer the latents z in the base VAE. The loss is in the same form as with Eq.(6) except replacing the discriminator conditional qD with the encoder conditional qE :\nLAttr,z(\u03b8G) = Ep(z)p(c) [ log qE(z|G\u0303\u03c4 (z, c)) ] . (7)\nNote that, as the discriminator in Eq.(6), the encoder now performs inference over generated samples from the prior, as opposed to observed examples as in VAEs.\nCombining Eqs.(4)-(7) we obtain the objective of learning the generator:\nmin\u03b8G LG = LVAE + \u03bbcLAttr,c + \u03bbzLAttr,z, (8)\nwhere \u03bbc and \u03bbz are balancing parameters. The variational encoder is trained by minimizing the VAE loss, i.e., min\u03b8E LVAE."}, {"heading": "Discriminator Learning", "text": "The discriminator D is trained to accurately infer the sentence attribute and evaluate the error of recovering the desired feature as specified in the latent code. For instance, for categorical attribute, the discriminator can be formulated as a sentence classifier; while for continuous target a probabilistic regressor can be used. The discriminator is learned in a different way compared to the VAE encoder, since the target attributes can be discrete which are not supported in the VAE framework. Moreover, in contrast to the unstructured code z which is learned in an unsupervised\nAlgorithm 1 Controllable Text Generation Input: A large corpus of unlabeled sentences X = {x}\nA few sentence attribute labels XL = {(xL, cL)} Parameters: \u03bbc, \u03bbz, \u03bbu, \u03b2 \u2013 balancing parameters\n1: Initialize the base VAE by minimizing Eq.(4) on X with c sampled from prior p(c) 2: repeat 3: Train the discriminator D by Eq.(11) 4: Train the generator G and the encoder E by Eq.(8) and minimizing Eq.(4), respectively. 5: until convergence Output: Sentence generator G conditioned on disentangled representation (z, c)\nmanner, the structured variable c uses labeled examples to entail designated semantics. We derive an efficient semisupervised learning method for the discriminator.\nFormally, let \u03b8D denote the parameters of the discriminator. To learn specified semantic meaning, we use a set of labeled examples XL = {(xL, cL)} to train the discriminator D with the following objective:\nLs(\u03b8D) = EXL [log qD(cL|xL)] . (9)\nBesides, the conditional generatorG is also capable of synthesizing (noisy) sentence-attribute pairs (x\u0302, c) which can be used to augment training data for semi-supervised learning. To alleviate the issue of noisy data and ensure robustness of model optimization, we incorporate a minimum entropy regularization term (Grandvalet et al., 2004; Reed et al., 2014). The resulting objective is thus: Lu(\u03b8D) = EpG(x\u0302|z,c)p(z)p(c) [ log qD(c|x\u0302) + \u03b2H(qD(c\u2032|x\u0302)) ] ,\n(10)\nwhere H(qD(c\u2032|x\u0302)) is the empirical Shannon entropy of distribution qD evaluated on the generated sentence x\u0302; and \u03b2 is the balancing parameter. Intuitively, the minimum entropy regularization encourages the model to have high confidence in predicting labels.\nThe joint training objective of the discriminator using both labeled examples and synthesized samples is then given as:\nmin\u03b8D LD = Ls + \u03bbuLu, (11)\nwhere \u03bbu is the balancing parameter."}, {"heading": "Summarization and Discussion", "text": "We have derived our model and its learning procedure in the above sections. The generator is first initialized by training the base VAE on a large corpus of unlabeled sentences, through the objective of minimizing Eq.(4) with the latent code c at this time sampled from the prior distribution p(c). Learning of the whole model then proceeds by\nalternating the optimization of the generator and the discriminator. The detailed learning procedure is summarized in Algorithm 1.\nOur model can be viewed as combining the VAE framework with an extended wake-sleep method, as illustrated in Figure 2. Specifically, in Eq.(10), samples are produced by the generator and are used as targets for maximum likelihood training of the discriminator. This resembles the sleep phase of the wake-sleep algorithm. Eqs.(6)-(7) further leverage the generated samples to improve the generator. We can see the above together as an extended sleep procedure based on \u201cdream\u201d samples obtained by ancestral sampling from the generative network. On the other hand, Eq.(4) samples c from the discriminator distribution qD(c|x) on observation x, to form a target for training the generator, which corresponds to the wake phase. The effective combination enables discrete latent code, holistic discriminator metrics, and efficient mutual bootstrapping.\nTraining of the discriminators need supervised data to impose designated semantics. Discriminators for different attributes can be trained independently on separate labeled sets. That is, the model does not require a sentence to be annotated with all attributes, but instead needs only independent labeled data for each individual attribute. Moreover, as the labeled data are used only for learning attribute semantics instead of direct sentence generation, we are allowed to extend the data scope beyond labeled sentences to, e.g., labeled words or phrases. As shown in the experiments (section 4), our method is able to effectively lift the word level knowledge to sentence level and generate convincing sentences. Finally, with the augmented unsupervised training in the sleep phrase, we show a little supervision is sufficient for learning structured representations."}, {"heading": "4. Experiments", "text": "We evaluate our approach with both quantitative and qualitative experiments. Our model is demonstrated to be ef-\nfective on sentence generation with controlled attributes of sentiment and tense. Our method improves over previous generative models on the accuracy of both attribute generation and classification. Highly disentangled representation is learned from a few labeled examples or even only word annotations. We validate the effect of the proposed independency constraint for interpretable generation, and show our model is able to produce convincing sentences with desired attributes."}, {"heading": "4.1. Setup", "text": ""}, {"heading": "Datasets", "text": "Sentence corpus. We use a large IMDB text corpus (Diao et al., 2014) for training the generative models. This is a collection of 350K movie reviews. We select sentences containing at most 16 words, and replace infrequent words with the token \u201c<unk>\u201d. The resulting dataset contains around 1.4M sentences with the vocabulary size of 16K.\nSentiment. To control the sentiment (\u201cpositive\u201d or \u201cnegative\u201d) of generated sentences, we test on the following labeled sentiment data: (1) Stanford Sentiment Treebank-2 (SST-full) (Socher et al., 2013) consists of 6920/872/1821 movie review sentences with binary sentiment annotations in the train/dev/test sets, respectively. We use the 2837 training examples with sentence length \u2264 16, and evaluate classification accuracy on the original test set. (2) SSTsmall. To study the size of labeled data required in the semi-supervised learning for accurate attribute control, we sample a small subset from SST-full, containing only 250 labeled sentences for training. (3) Lexicon. We also investigate the effectiveness of our model in terms of using word-level labels for sentence-level control. The lexicon from (Wilson et al., 2005) contains 2700 words with sentiment labels. We use the lexicon for training by treating the words as sentences, and evaluate on the SST-full test set. (4) IMDB. We collect a dataset from the IMDB corpus by randomly selecting positive and negative movie reviews. The dataset contains 5K/1K/10K sentences in the train/dev/test sets, respectively.\nTense. The second attribute is the tense of the main verb in a sentence. Though no corpus with sentence tense annotations is readily available, our method is able to learn from only labeled words and generate desired sentences. We compile from the TimeBank2 dataset and obtain a lexicon of 5250 words and phrases labeled with one of {\u201cpast\u201d, \u201cpresent\u201d, \u201cfuture\u201d}. The lexicon mainly consists of verbs in different tenses (e.g., \u201cwas\u201d, \u201cwill be\u201d) as well as time expressions (e.g., \u201cin the future\u201d)."}, {"heading": "Parameter Setting", "text": "We implement both the generator and the encoder as single-\n2www.timeml.org\nlayer LSTM RNNs with input and hidden dimension of 300 and maximum sample length of 16. Discriminators are parameterized as convolutional networks. We give the detailed structure configurations in the supplements. To avoid vanishingly small KL term in the VAE (Bowman et al., 2015), we use a KL term weight linearly annealing from 0 to 1 during training. Neural parameters are trained using SGD with the Adam update rule (Kingma & Ba, 2014). We set the balancing parameters to \u03bbc = \u03bbz = \u03bbu = 0.1, and \u03b2 is selected on the dev sets. All experiments were performed on a TITAN-X GPU and 128GB RAM. We implemented neural networks based on Tensorflow3."}, {"heading": "4.2. Accuracy of Generated Attributes", "text": "We quantitatively measure the performance of attribute control for sentences, by evaluating the accuracy of generating designated sentiment, and the effect of using the generated samples for sentiment classification. We compare our model with the semi-supervised variational autoencoder (S-VAE) (Kingma et al., 2014), one of the few existing deep generative models capable of conditional sentence generation. S-VAE learns to reconstruct observed sentences given the attribute code, and no discriminators are used.\nWe use a state-of-the-art sentiment classifier (Hu et al., 2016a), which exhibits around 90% accuracy on the SST test set, to automatically evaluate the sentiment generation accuracy. Specifically, we generate sentences given sentiment code c, and then use the pre-trained sentiment classifier to assign sentiment labels to the generated sentences. We treat the predicted labels as ground truth, and compare with the sentiment code c to compute the attribute generation accuracy. Table 1 shows the results on 30K sentences produced by the two models which are trained with the SST-full, SST-small, and Lexicon datasets, respectively. It can be observed that our method consistently outperforms S-VAE on all datasets. In particular, trained with only 250 labeled examples provided in SST-\n3www.tensorflow.org\nsmall, our model achieves reasonable generation accuracy, demonstrating the ability of our model to learn disentangled representations with very little supervision. More importantly, given only word-level annotations in Lexicon, our model successfully transfers the knowledge onto the sentence level and generates desired sentiments reasonably well. Compared to our method that drives learning by directly assessing generated sentences, S-VAE attempts to capture the sentiment semantic only by reconstructing labeled words, which is less efficient and results in inferior performance.\nWe next use the generated samples to augment the sentiment datasets and train sentiment classifiers. While not aiming to build best-performing classifiers on these datasets, the classification accuracy serves as an auxiliary measure of the sentence generation quality, that is, higher-quality sentences with more accurate sentiment attribute can predictably help yield stronger sentiment classifiers. Figure 3 shows the accuracy of classifiers trained on the four datasets with different augmentations. \u201cStd\u201d is a convolutional network trained on the standard original datasets, with the same network structure as with the sentiment discriminator in our model. \u201cH-reg\u201d additionally imposes the minimum entropy regularization on the generated sentences for enhancing the sentiment classifier. \u201cOurs\u201d incorporates the minimum entropy regularization and the sentiment attribute code c of the generated sentences, as in Eq.(10). S-VAE uses the same protocol as our method to augment with the data generated by the semisupervised VAE model. Accuracy comparison in Figure 3 demonstrates that our method consistently gives the best performance on four datasets. For instance, on the Lexicon dataset, our approach achieves 0.733 accuracy, compared to 0.701 of \u201cStd\u201d. The improvement of \u201cH-Reg\u201d over \u201cStd\u201d shows positive effect of the minimum entropy regularization on generated sentences. Further incorporating the conditioned sentiment code of the generated sam-\nples, as in \u201cOurs\u201d and \u201cS-VAE\u201d, provides additional performance gains, indicating the advantages of conditional generation for automatic creation of labeled data. Consistent with the above experiment, our model outperforms S-VAE."}, {"heading": "4.3. Disentangled Representation", "text": "We now investigate the interpretable representations learned by our model. We demonstrate the importance of the explicit independency constraint we introduce for disentangled attribute control and interpretable text generation. In addition, we show our model produces convincing natural language sentences and enables convenient and accurate control of the generated attributes.\nWe first study the effect of the independency constraint on unstructured code z as introduced in Eq.(7). Table 2 compares the samples generated by models with and without the constraint term, respectively. In the left column where the constraint applies, each pair of sentences, conditioned on different sentiment codes, are highly relevant in terms of, e.g., subject, tone, and wording which are not explicitly modeled in the structured code c while instead implicitly encoded in the unstructured code z. Varying the sentiment code precisely changes the sentiment of the generated sentences (and paraphrases slightly when necessary to ensure fluency), while keeping other aspects unchanged. In contrast, the results in the right column, where the independency constraint is unactivated, show that varying the sentiment code not only changes the polarity of samples, but can also result in variations of other aspects not expected to controlled, making the generation results less interpretable and predictable.\nWe demonstrate the power of learned disentangled representation by varying one attribute variable at a time. Table 3 shows the generation results. We see that each attribute variable in our model successfully controls its corresponding attribute of the generated sentences, and is disentangled with other attribute code. The right column of the table shows meaningful variation of sentence tense as the tense code varies. Note that the semantic of tense is learned only from a lexicon without complete sentence examples. Our model successfully captures the key ingredients (e.g., verb \u201cwas\u201d for past tense and \u201cwill be\u201d for future tense) and combines with the knowledge of well-formed sentences to generate realistic samples with specified tense attributes.\nTable 4 further shows generated sentences with varying code z in different settings of structured attribute factors. We obtain samples that are diverse in content while consistent in sentiment and tense. This further validates that our model is able to generate convincing sentences and attributes of interest in a controlled manner."}, {"heading": "5. Conclusions & Discussions", "text": "We have proposed a deep generative model capable of learning interpretable latent representations and generating realistic sentences with specified attributes. Our approach combines VAEs with attribute discriminators and imposes explicit independency constraints on attribute controls, enabling learning disentangled latent code. Semi-supervised learning within the extended VAE/wake-sleep framework is effective with little or incomplete supervision. We show meaningful generation results and improved generation accuracy. Our method shows strong promise for generating more complex text and structured data.\nInterpretability of the latent representations not only allows dynamic control of generated attributes, but also provides an interface that connects the end-to-end neural model with conventional structured methods. For instance, we can encode structured constraints (e.g., logic rules or probabilistic structured models) on the interpretable latent code, to incorporate prior knowledge or human intentions (Hu et al., 2016a;b); or plug the disentangled generation model into dialog systems to generate natural language responses from structured dialog states (Young et al., 2013).\nThough we have focused on the generation capacity of our model, the proposed collaborative semi-supervised learning framework also helps improve the discriminators by generating labeled samples for data augmentation (e.g., see Figure 3). More generally, for any discriminative task, we can build a conditional generative model to synthesize additional labeled data. The accurate attribute generation of our approach can offer larger performance gains compared to previous generative methods."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Generating sentences from a continuous space", "author": ["Bowman", "Samuel R", "Vilnis", "Luke", "Vinyals", "Oriol", "Dai", "Andrew M", "Jozefowicz", "Rafal", "Bengio", "Samy"], "venue": "arXiv preprint arXiv:1511.06349,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Chen", "Xi", "Duan", "Yan", "Houthooft", "Rein", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": "In NIPS,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)", "author": ["Diao", "Qiming", "Qiu", "Minghui", "Wu", "Chao-Yuan", "Smola", "Alexander J", "Jiang", "Jing", "Wang", "Chong"], "venue": "In KDD,", "citeRegEx": "Diao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Diao et al\\.", "year": 2014}, {"title": "Generating images with perceptual similarity metrics based on deep networks", "author": ["Dosovitskiy", "Alexey", "Brox", "Thomas"], "venue": "arXiv preprint arXiv:1602.02644,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2016}, {"title": "A neural algorithm of artistic style", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Grandvalet", "Yves", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Grandvalet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Grandvalet et al\\.", "year": 2004}, {"title": "The \u201cwake-sleep\u201d algorithm for unsupervised neural networks", "author": ["Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Harnessing deep neural networks with logic rules", "author": ["Hu", "Zhiting", "Ma", "Xuezhe", "Liu", "Zhengzhong", "Hovy", "Eduard", "Xing", "Eric"], "venue": "In ACL,", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Deep neural networks with massive learned knowledge", "author": ["Hu", "Zhiting", "Yang", "Zichao", "Salakhutdinov", "Ruslan", "Xing", "Eric P"], "venue": "In EMNLP,", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Deep convolutional inverse graphics network", "author": ["Kulkarni", "Tejas D", "Whitney", "William F", "Kohli", "Pushmeet", "Tenenbaum", "Josh"], "venue": "In NIPS, pp", "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "GANs for sequences of discrete elements with the gumbel-softmax distribution", "author": ["Kusner", "Matt", "Hernndez-Lobato", "Jos"], "venue": "arXiv preprint arXiv:1611.04051,", "citeRegEx": "Kusner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kusner et al\\.", "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Larsen", "Anders Boesen Lindbo", "S\u00f8nderby", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "In ICML,", "citeRegEx": "Larsen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2016}, {"title": "Conditional image synthesis with auxiliary classifier gans", "author": ["Odena", "Augustus", "Olah", "Christopher", "Shlens", "Jonathon"], "venue": "arXiv preprint arXiv:1610.09585,", "citeRegEx": "Odena et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Odena et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Training deep neural networks on noisy labels with bootstrapping", "author": ["Reed", "Scott", "Lee", "Honglak", "Anguelov", "Dragomir", "Szegedy", "Christian", "Erhan", "Dumitru", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1412.6596,", "citeRegEx": "Reed et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2014}, {"title": "A hybrid convolutional variational autoencoder for text generation", "author": ["Semeniuta", "Stanislau", "Severyn", "Aliaksei", "Barth", "Erhardt"], "venue": "arXiv preprint arXiv:1702.02390,", "citeRegEx": "Semeniuta et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Semeniuta et al\\.", "year": 2017}, {"title": "Learning disentangled representations in deep generative models", "author": ["N. Siddharth", "Paige", "Brooks", "Desmaison", "Alban", "Meent", "Jan-Willem van de", "Wood", "Frank", "Goodman", "Noah D", "Kohli", "Pushmeet", "Torr", "Philip H.S"], "venue": null, "citeRegEx": "Siddharth et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Siddharth et al\\.", "year": 2017}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean Y", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Unsupervised sentence representation learning with adversarial auto-encoder", "author": ["Tang", "Shuai", "Jin", "Hailin", "Fang", "Chen", "Wang", "Zhaowen"], "venue": null, "citeRegEx": "Tang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "In ICML,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["Wilson", "Theresa", "Wiebe", "Janyce", "Hoffmann", "Paul"], "venue": "In EMNLP,", "citeRegEx": "Wilson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Young", "Steve", "Ga\u0161i\u0107", "Milica", "Thomson", "Blaise", "Williams", "Jason D"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "SeqGAN: sequence generative adversarial nets with policy gradient", "author": ["Yu", "Lantao", "Zhang", "Weinan", "Wang", "Jun", "Yong"], "venue": "In AAAI,", "citeRegEx": "Yu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2017}, {"title": "Generating text via adversarial training", "author": ["Zhang", "Yizhe", "Gan", "Zhe", "Carin", "Lawrence"], "venue": "In NIPS Workshop on Adversarial Training,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["Zhu", "Jun-Yan", "Kr\u00e4henb\u00fchl", "Philipp", "Shechtman", "Eli", "Efros", "Alexei A"], "venue": "In ECCV,", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "There is a surge of research interest in deep generative models, such as Variational Autoencoders (VAEs) (Kingma & Welling, 2013), Generative Adversarial Nets (GANs) (Goodfellow et al., 2014), and auto-regressive models (van den Oord et al.", "startOffset": 166, "endOffset": 191}, {"referenceID": 18, "context": "Despite their impressive advances in visual domain, such as image generation (Radford et al., 2015), learning interpretable image representations (Chen et al.", "startOffset": 77, "endOffset": 99}, {"referenceID": 2, "context": ", 2015), learning interpretable image representations (Chen et al., 2016), and image editing (Zhu et al.", "startOffset": 54, "endOffset": 73}, {"referenceID": 30, "context": ", 2016), and image editing (Zhu et al., 2016), applications to natural language generation have been relatively less studied.", "startOffset": 27, "endOffset": 45}, {"referenceID": 0, "context": "Previous work have been mostly limited to task-specific applications in supervised settings, including machine translation (Bahdanau et al., 2014) and", "startOffset": 123, "endOffset": 146}, {"referenceID": 25, "context": "image captioning (Vinyals et al., 2015).", "startOffset": 17, "endOffset": 39}, {"referenceID": 1, "context": "Very few recent attempts of using VAEs (Bowman et al., 2015; Tang et al., 2016) and GANs (Yu et al.", "startOffset": 39, "endOffset": 79}, {"referenceID": 23, "context": "Very few recent attempts of using VAEs (Bowman et al., 2015; Tang et al., 2016) and GANs (Yu et al.", "startOffset": 39, "endOffset": 79}, {"referenceID": 28, "context": ", 2016) and GANs (Yu et al., 2017; Zhang et al., 2016) have been made to investigate generic text generation, while their generated text is largely randomized and uncontrollable.", "startOffset": 17, "endOffset": 54}, {"referenceID": 29, "context": ", 2016) and GANs (Yu et al., 2017; Zhang et al., 2016) have been made to investigate generic text generation, while their generated text is largely randomized and uncontrollable.", "startOffset": 17, "endOffset": 54}, {"referenceID": 2, "context": "The resulting non-differentiability hinders the use of global discriminators that assess generated samples and back-propagate gradients to guide the optimization of generators in a holistic manner, as shown to be highly effective in continuous image generation and representation modeling (Chen et al., 2016; Larsen et al., 2016; Dosovitskiy & Brox, 2016).", "startOffset": 289, "endOffset": 355}, {"referenceID": 16, "context": "The resulting non-differentiability hinders the use of global discriminators that assess generated samples and back-propagate gradients to guide the optimization of generators in a holistic manner, as shown to be highly effective in continuous image generation and representation modeling (Chen et al., 2016; Larsen et al., 2016; Dosovitskiy & Brox, 2016).", "startOffset": 289, "endOffset": 355}, {"referenceID": 28, "context": "A number of recent approaches attempt to address the non-differentiability through policy learning (Yu et al., 2017) which tends to suffer from high variance during training, or continuous approximations (Zhang et al.", "startOffset": 99, "endOffset": 116}, {"referenceID": 29, "context": ", 2017) which tends to suffer from high variance during training, or continuous approximations (Zhang et al., 2016; Kusner & Hernndez-Lobato, 2016) where only preliminary qualitative results are presented.", "startOffset": 95, "endOffset": 147}, {"referenceID": 11, "context": "As an alternative to the discriminator based learning, semi-supervised VAEs (Kingma et al., 2014) minimize element-wise reconstruction error on observed examples and are applicable to discrete visibles.", "startOffset": 76, "endOffset": 97}, {"referenceID": 2, "context": "Prior methods (Chen et al., 2016; Odena et al., 2016) on structured representation learning lack explicit enforcement of the independence property on the full latent representation, and varying individual code may result in unexpected variation of other unspecified attributes in addition to the desired one.", "startOffset": 14, "endOffset": 53}, {"referenceID": 17, "context": "Prior methods (Chen et al., 2016; Odena et al., 2016) on structured representation learning lack explicit enforcement of the independence property on the full latent representation, and varying individual code may result in unexpected variation of other unspecified attributes in addition to the desired one.", "startOffset": 14, "endOffset": 53}, {"referenceID": 8, "context": "Our model can be interpreted as enhancing VAEs with an extended wake-sleep procedure (Hinton et al., 1995), where the sleep phase enables incorporation of generated samples for learning both the generator and discriminators in an alternating manner.", "startOffset": 85, "endOffset": 106}, {"referenceID": 8, "context": "The wake-sleep algorithm (Hinton et al., 1995) introduced for training deep directed graphical models shares similarity with VAEs by also combining an inference network with the generator.", "startOffset": 25, "endOffset": 46}, {"referenceID": 6, "context": "For instance, GANs (Goodfellow et al., 2014) use a discriminator to feedback the probability of a sample being recognized as a real example.", "startOffset": 19, "endOffset": 44}, {"referenceID": 2, "context": "For instance, InfoGAN (Chen et al., 2016), which resembles the sleep procedure of our extended VAE/wakesleep algorithm, disentangles latent representation in an unsupervised manner, while the semantic of each dimension is observed after training rather than designated by users in a controllable way.", "startOffset": 22, "endOffset": 41}, {"referenceID": 3, "context": "For instance, GANs (Goodfellow et al., 2014) use a discriminator to feedback the probability of a sample being recognized as a real example. Larsen et al. (2016) combine VAEs with GANs for enhanced image generation.", "startOffset": 20, "endOffset": 162}, {"referenceID": 3, "context": "For instance, GANs (Goodfellow et al., 2014) use a discriminator to feedback the probability of a sample being recognized as a real example. Larsen et al. (2016) combine VAEs with GANs for enhanced image generation. Dosovitskiy & Brox (2016); Gatys et al.", "startOffset": 20, "endOffset": 242}, {"referenceID": 3, "context": "Dosovitskiy & Brox (2016); Gatys et al. (2015) use discriminator networks for measuring high-level perceptual similarity.", "startOffset": 27, "endOffset": 47}, {"referenceID": 3, "context": "Dosovitskiy & Brox (2016); Gatys et al. (2015) use discriminator networks for measuring high-level perceptual similarity. Applying discriminators to text generation is difficult due to the non-differentiability of the discrete samples. Yu et al. (2017) resort to policy learning which tends to have high variance during training.", "startOffset": 27, "endOffset": 253}, {"referenceID": 3, "context": "Dosovitskiy & Brox (2016); Gatys et al. (2015) use discriminator networks for measuring high-level perceptual similarity. Applying discriminators to text generation is difficult due to the non-differentiability of the discrete samples. Yu et al. (2017) resort to policy learning which tends to have high variance during training. Zhang et al. (2016); Kusner & Hernndez-Lobato (2016) apply continuous approximations with only preliminary qualitative results.", "startOffset": 27, "endOffset": 350}, {"referenceID": 3, "context": "Dosovitskiy & Brox (2016); Gatys et al. (2015) use discriminator networks for measuring high-level perceptual similarity. Applying discriminators to text generation is difficult due to the non-differentiability of the discrete samples. Yu et al. (2017) resort to policy learning which tends to have high variance during training. Zhang et al. (2016); Kusner & Hernndez-Lobato (2016) apply continuous approximations with only preliminary qualitative results.", "startOffset": 27, "endOffset": 383}, {"referenceID": 1, "context": "Bowman et al. (2015); Tang et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Bowman et al. (2015); Tang et al. (2016) instead use VAEs without discriminators.", "startOffset": 0, "endOffset": 41}, {"referenceID": 1, "context": "Bowman et al. (2015); Tang et al. (2016) instead use VAEs without discriminators. All these text generation methods do not learn disentangled latent representations, resulting in randomized and uncontrollable samples. In contrast, disentangled generation in visual domain has made impressive progress. For instance, InfoGAN (Chen et al., 2016), which resembles the sleep procedure of our extended VAE/wakesleep algorithm, disentangles latent representation in an unsupervised manner, while the semantic of each dimension is observed after training rather than designated by users in a controllable way. Siddharth et al. (2017); Kulkarni et al.", "startOffset": 0, "endOffset": 627}, {"referenceID": 1, "context": "Bowman et al. (2015); Tang et al. (2016) instead use VAEs without discriminators. All these text generation methods do not learn disentangled latent representations, resulting in randomized and uncontrollable samples. In contrast, disentangled generation in visual domain has made impressive progress. For instance, InfoGAN (Chen et al., 2016), which resembles the sleep procedure of our extended VAE/wakesleep algorithm, disentangles latent representation in an unsupervised manner, while the semantic of each dimension is observed after training rather than designated by users in a controllable way. Siddharth et al. (2017); Kulkarni et al. (2015); Kingma et al.", "startOffset": 0, "endOffset": 651}, {"referenceID": 1, "context": "Bowman et al. (2015); Tang et al. (2016) instead use VAEs without discriminators. All these text generation methods do not learn disentangled latent representations, resulting in randomized and uncontrollable samples. In contrast, disentangled generation in visual domain has made impressive progress. For instance, InfoGAN (Chen et al., 2016), which resembles the sleep procedure of our extended VAE/wakesleep algorithm, disentangles latent representation in an unsupervised manner, while the semantic of each dimension is observed after training rather than designated by users in a controllable way. Siddharth et al. (2017); Kulkarni et al. (2015); Kingma et al. (2014) build in the context of VAEs and obtain disentangled image representations with semi-supervised learning.", "startOffset": 0, "endOffset": 673}, {"referenceID": 1, "context": "We build our framework starting from variational autoencoders (section 2) which have been used for text generation (Bowman et al., 2015; Semeniuta et al., 2017), where sentence x\u0302 is generated conditioned on latent code z.", "startOffset": 115, "endOffset": 160}, {"referenceID": 20, "context": "We build our framework starting from variational autoencoders (section 2) which have been used for text generation (Bowman et al., 2015; Semeniuta et al., 2017), where sentence x\u0302 is generated conditioned on latent code z.", "startOffset": 115, "endOffset": 160}, {"referenceID": 7, "context": "To alleviate the issue of noisy data and ensure robustness of model optimization, we incorporate a minimum entropy regularization term (Grandvalet et al., 2004; Reed et al., 2014).", "startOffset": 135, "endOffset": 179}, {"referenceID": 19, "context": "To alleviate the issue of noisy data and ensure robustness of model optimization, we incorporate a minimum entropy regularization term (Grandvalet et al., 2004; Reed et al., 2014).", "startOffset": 135, "endOffset": 179}, {"referenceID": 3, "context": "We use a large IMDB text corpus (Diao et al., 2014) for training the generative models.", "startOffset": 32, "endOffset": 51}, {"referenceID": 22, "context": "To control the sentiment (\u201cpositive\u201d or \u201cnegative\u201d) of generated sentences, we test on the following labeled sentiment data: (1) Stanford Sentiment Treebank-2 (SST-full) (Socher et al., 2013) consists of 6920/872/1821 movie review sentences with binary sentiment annotations in the train/dev/test sets, respectively.", "startOffset": 170, "endOffset": 191}, {"referenceID": 26, "context": "The lexicon from (Wilson et al., 2005) contains 2700 words with sentiment labels.", "startOffset": 17, "endOffset": 38}, {"referenceID": 11, "context": "model (Kingma et al., 2014).", "startOffset": 6, "endOffset": 27}, {"referenceID": 1, "context": "To avoid vanishingly small KL term in the VAE (Bowman et al., 2015), we use a KL term weight linearly annealing from 0 to 1 during training.", "startOffset": 46, "endOffset": 67}, {"referenceID": 11, "context": "We compare our model with the semi-supervised variational autoencoder (S-VAE) (Kingma et al., 2014), one of the few existing deep generative models capable of conditional sentence generation.", "startOffset": 78, "endOffset": 99}, {"referenceID": 27, "context": ", 2016a;b); or plug the disentangled generation model into dialog systems to generate natural language responses from structured dialog states (Young et al., 2013).", "startOffset": 143, "endOffset": 163}], "year": 2017, "abstractText": "Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.", "creator": "LaTeX with hyperref package"}}}