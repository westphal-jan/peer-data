{"id": "1703.01014", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Active Learning for Cost-Sensitive Classification", "abstract": "we design an action learning algorithm for cost - sensitive multiclass classification : problems where different errors constitute larger costs. our algorithm, coal, makes predictions by regressing on each label's cost and predicting the smallest. on a new machine, it uses manually set sample scales that perform well on past data to estimate possible costs for each label. it queries only specific labels that could be the best, ignoring the sure losers. \u2022 prove coal can be only implemented for any regression reconstruction that admits squared independent optimization ; it also enjoys strong guarantees with respect to predictive performance than labeling effort. we empirically compare coal to passive learning, showing significant improvements in labeling effort and test cost.", "histories": [["v1", "Fri, 3 Mar 2017 02:17:13 GMT  (178kb,D)", "http://arxiv.org/abs/1703.01014v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akshay krishnamurthy", "alekh agarwal", "tzu-kuo huang", "hal daum\u00e9 iii", "john langford"], "accepted": true, "id": "1703.01014"}, "pdf": {"name": "1703.01014.pdf", "metadata": {"source": "CRF", "title": "Active Learning for Cost-Sensitive Classification", "authors": ["Akshay Krishnamurthy", "Alekh Agarwal", "Tzu-Kuo Huang", "John Langford"], "emails": ["akshay@cs.umass.edu", "alekha@microsoft.com", "tkhuang@protonmail.com", "hal@umiacs.umd.edu", "jcl@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The field of active learning studies how to efficiently elicit relevant information so learning algorithms can make good decisions. Almost all active learning algorithms are designed for binary classification problems, leading to the natural question: How can active learning address more complex prediction problems? Multiclass and importance-weighted classification require only minor modifications but we know of no active learning algorithms that enjoy theoretical guarantees for more complex problems.\nOne such problem is cost-sensitive multiclass classification (CSMC). In CSMC with K classes, passive learners receive input examples x and cost vectors c \u2208 RK , where c(y) is the cost of predicting label y on x.1 A natural design for an active CSMC learner then is to adaptively query the costs of only a (possibly empty) subset of labels on each x. Since measuring label complexity is more nuanced in CSMC (e.g., is it more \u2217akshay@cs.umass.edu \u2020alekha@microsoft.com \u2021tkhuang@protonmail.com \u00a7hal@umiacs.umd.edu \u00b6jcl@microsoft.com 1Cost here refers to prediction cost and not labeling effort or the cost of acquiring different labels.\nar X\niv :1\n70 3.\n01 01\n4v 1\n[ cs\n.L G\nexpensive to query three costs on a single example or one cost on three examples?), we track both the number of examples for which at least one cost is queried, along with the total number of queries issued. The first corresponds to a fixed human effort for inspecting x. The second captures the additional effort for judging the cost of each prediction, which depends on the number of labels queried. (By querying a label, we mean querying the cost of that label given an example.)\nIn this setup, we develop a new active learning algorithm for CSMC called Cost Overlapped Active Learning (COAL). COAL assumes access to a set of regression functions, and, when processing an example x, it uses the functions with good past performance to compute the range of possible costs that each label might take. Naturally, COAL only queries labels with large cost range, but furthermore, it only queries y\u2019s that could possibly have the smallest cost, avoiding the uncertain, but surely suboptimal labels. The key algorithmic innovation is an efficient way to compute the cost range realized by good regressors. This computation, and COAL as a whole, only requires that the regression set admits efficient squared loss optimization, in contrast with prior algorithms that require 0/1 loss optimization [6, 16].\nAmong our results, we prove that when processing n (unlabeled) examples with K classes and N regressors, 1. The algorithm needs to solve O(Kn2 log n) regression problems over the function class (Cor. 2), which\ncan be done in polynomial time for convex regression sets. 2. With no assumptions on the noise in the problem, the algorithm achieves generalization error O\u0303( \u221a K lnN/n)\nand requests O\u0303(n\u03b82 \u221a K lnN) costs from O\u0303(n\u03b81 \u221a K lnN) examples (Thms. 3 and 5) where \u03b81, \u03b82 are the disagreement coefficients (Def. 1)2. The worst case offers minimal improvement over passive learning, akin to binary classification. 3. With a favorable noise assumption (As. 2), the algorithm achieves generalization error O\u0303(K lnN/n) while requesting O\u0303(Kc1/\u03b2n\u03b2\u03b82 lnN) labels from O\u0303(c1/\u03b2n\u03b2\u03b81K lnN) examples (Cor. 4 and Thm. 6), where \u03b2 \u2208 (0, 1) is a safety parameter of the algorithm and c is a constant. We also discuss some intuitive examples highlighting the benefits of using COAL. CSMC provides a much more expressive language for success and failure than plain multiclass classification, which allows algorithms to make the tradeoffs necessary for good performance and broadens the potential applications. For example, CSMC can naturally express partial failure in hierarchical classification [27]. Experimentally, we show that on a number of hierarchical classification datasets, COAL\n2O\u0303(\u00b7) suppresses logarithmic dependence on n and K.\nsubstantially outperforms the passive learning baseline with orders of magnitude savings in the labeling effort (see Figure 1 for an example on Reuters text categorization comparing passive learning to COAL).\nCSMC also forms the basis of learning to avoid cascading failures in joint prediction tasks [13, 23, 11] like structured prediction and reinforcement learning. As our second application, we consider learning to search algorithms for joint (or structured) prediction [11], which operate by a reduction to CSMC. In this reduction, evaluating the cost of a class often involves a computationally expensive \u201croll-out,\u201d so using an active learning algorithm inside such a (passive) joint prediction method can lead to significant computational savings. We show that using COAL within the AGGRAVATE algorithm [23, 11] reduces the number of roll-outs by a factor of 14 to 3 4 on several joint prediction tasks.\nRelated Work. Active learning is a thriving research area with many theoretical and empirical studies. We recommend the survey of Settles [25] for an overview of more empirical research. We focus here on theoretical results.\nTheoretical results come in several flavors. Castro and Nowak [9] study active learning for binary classification with non-parametric decision sets, while Balcan et al. [5], Balcan and Long [3] focus on linear representations under distributional assumptions. Additionally, the selective sampling framework from the online learning community derives regret and label complexity bounds for stream-based active learning of linear separators under adversarial assumptions [10, 14, 22, 1].\nOur work falls into the framework of disagreement-based active learning, which studies general hypothesis spaces typically in an agnostic setup (see Hanneke [16] for an excellent survey). Existing results study binary classification, while our work generalizes to CSMC. The main differences are that our query rule additionally checks the range of predicted costs for a label, and we use a square loss oracle to search the version space.\nIn contrast, prior work either explicitly enumerates the version space [4, 30] or uses a 0/1 loss classification oracle for the search [12, 6, 7, 17]. In most instantiations, the oracle solves an NP-hard problem and so does not directly lead to an efficient active learning algorithm, although practical implementations using heuristics are still quite effective. In contrast, our approach uses a squared-loss regression oracle, which can often be implemented efficiently via convex optimization leading to a polynomial time algorithm.\nSupervised learning oracles that solve NP-hard optimization problems in the worst case have been used in other problems including contextual bandits [2, 28] and structured prediction [13]. Thus we hope that our work can inspire new algorithms for these settings as well."}, {"heading": "2 Problem Setting and Notations", "text": "We study a cost-sensitive multiclass classification problem with K classes, where there is an instance space X , a label space Y = {1, . . . ,K}, and a distribution D supported on X \u00d7 [0, 1]K .3 If (x, c) \u223c D, we refer to c as the cost-vector where c(y) is the cost of predicting y \u2208 Y . A classifier h : X \u2192 Y has expected cost E(x,c)\u223cD[c(h(x))] and we aim to find a classifier with minimal expected cost.\nLet G , {g : X 7\u2192 [0, 1]} denote a set of base regressors and let F , GK denote a set of vector regressors where the yth coordinate of f \u2208 F is written as f(\u00b7; y). The set of classifiers under consideration is H , {hf | f \u2208 F} where each f defines a classifier hf : X 7\u2192 Y by\nhf (x) , argmin y f(x; y). (1)\n3In general, labels just serve as indices for the cost vector in CSMC, and the data distribution is over (x, c) pairs instead of (x, y) pairs as in binary and multiclass classification.\nGiven a set of examples and queried costs, we often restrict attention to regression functions that predict these costs well, and assess the uncertainty in their predictions given a new example x. For a set of regressors G, we measure uncertainty over possible cost values for y given x with\n\u03b3(x,G) , c+(x,G)\ufe38 \ufe37\ufe37 \ufe38 ,maxg\u2208G g(x) \u2212 c\u2212(x,G)\ufe38 \ufe37\ufe37 \ufe38 ,ming\u2208G g(x) . (2)\nFor vector regressors F \u2282 F , we define the cost range for a label y given x as \u03b3(x, y, F ) , \u03b3(x,GF (y)) where GF (y) , {f(\u00b7; y) | f \u2208 F} is the induced set of base regressors by F for y.\nWhen using a set of regression functions for a classification task, it is natural to assume that the expected costs under D can be predicted well by the some function in the set. This motivates the following realizability assumption.\nAssumption 1 (Realizability). Define the Bayes-optimal regressor f?, which has f?(x; y) = Ec[c(y)|x],\u2200x \u2208 X (with D(x) > 0), y \u2208 Y . We assume that f? \u2208 F .\nf? is always well defined although the cost may be noisy, as given by the joint distribution D on (x, c). In comparison with our assumption, the existence of a zero-cost classifier inH (which is often assumed in active learning work) is stronger, while the existence of hf? in H is weaker but has not been leveraged in active learning.\nIn typical settings, the set G is extremely large, which introduces a computational challenge in managing the version space. To address this challenge, we leverage existing algorithmic research on supervised learning and assume access to a regression oracle for G. Given an importance-weighted dataset D = {xi, ci, wi}ni=1 the regression oracle computes\nORACLE(D) \u2208 argmin g\u2208G n\u2211 i=1 wi(g(xi)\u2212 ci)2. (3)\nIn many cases this is a convex problem and can be solved efficiently. In the special case of linear functions, this is just least squares and can be computed in closed form.\nTo measure the labeling effort, we track the number of examples for which even a single cost is queried as well as the total number of queries. Thus we capture settings where the editorial effort for inspecting an example is high, but each cost requires minimal further effort as well as those where the goal is to just minimize the total number of queries. Formally, we define Qi(y) to be the indicator that the algorithm queries label y on the ith example and measure\nL1 , n\u2211 i=1 \u2228 y Qi(y), and L2 , n\u2211 i=1 \u2211 y Qi(y). (4)"}, {"heading": "3 Cost Overlapped Active Learning", "text": "The pseudocode for our algorithm, Cost Overlapped Active Learning (COAL), is given in Algorithm 1. Given an example x, COAL queries the costs of some of the labels y for x. These costs are chosen by (1) computing an approximate version space based on the past data, (2) computing the range of predictions achievable by the version space for each y, and (3) querying each y that could be the best label and has substantial uncertainty. We now detail each step.\nAlgorithm 1: Cost Overlapped Active Learning (COAL)\n1: Input: Regressors G, failure probability \u03b4 \u2264 1/e, safety \u03b2 \u2208 (0, 1). 2: Set \u03b7i = 1/ \u221a i, \u03ba = 80, \u03bdn = log(2n2|G|K/\u03b4). 3: Set \u2206i = \u03ba i\u22121 i\u22121 , i = ( n i )\u03b2 \u03bdn. 4: for i = 1, 2, . . . , n do 5: gi,y \u2190 arg ming\u2208G R\u0302i(g; y). (See Eq. (5)) 6: Define fi \u2190 {gi,y}Ky=1. 7: Gi(y)\u2190 {g \u2208 G | R\u0302i(g; y) \u2264 R\u0302i(gi,y; y) + \u2206i}. 8: Receive new example x. 9: for every y \u2208 Y do\n10: c\u0302+(y)\u2190 MAXCOST((x, y),\u2206i, \u03b7i4\u221a3 , R\u0302i(\u00b7; y)). 11: c\u0302\u2212(y)\u2190 MINCOST((x, y),\u2206i, \u03b7i4\u221a3 , R\u0302i(\u00b7; y)). 12: end for 13: Y \u2032 \u2190 {y \u2208 Y | c\u0302\u2212(y) \u2264 miny\u2032 c\u0302+(y\u2032)}. 14: if |Y \u2032| > 1 then 15: Qi(y) = 1 if y \u2208 Y \u2032 and c\u0302+(y)\u2212 c\u0302\u2212(y) > \u03b7i. 16: end if 17: Query costs of each y with Qi(y) = 1. 18: end for\nTo compute an approximate version space we first, for each label y, find the regression function that minimizes the empirical risk for label y, which at round i is:\nR\u0302i(g; y) = 1\ni\u2212 1 i\u22121\u2211 j=1 (g(xj)\u2212 cj(y))2Qj(y). (5)\nRecall that Qj(y) is the indicator that we query label y on the jth example. Using the square loss is motivated by the realizability assumption and computing the minimizer requires one oracle call. We implicitly construct the version space Gi(y) in Line 7 as the regressors with low square loss regret to the empirical risk minimizer. The tolerance on this regret is \u2206i at round i, which depends on the safety parameter \u03b2 \u2208 (0, 1) in the algorithm. When \u03b2 is large, the tolerance is also large and the algorithm issues many queries. Conversely when \u03b2 is small the algorithm is more aggressive. However, for any strictly positive \u03b2, the definition of \u2206i ensures that f?(\u00b7; y) \u2208 Gi(y) for all i, y.\nCOAL then computes the maximum and minimum costs predicted by the version space Gi(y) on the new example x. Since the true expected cost is f?(x; y) and f?(\u00b7; y) \u2208 Gi(y), these quantities serve as a confidence bound for this value. The computation is done by the MAXCOST and MINCOST subroutines which produce approximations to c+(x, y,Gi(y)) and c\u2212(x, y,Gi(y)) (Eq. (2)) respectively.\nFinally, using the predicted costs, COAL issues a set of (possibly zero) queries. The algorithm queries any non-dominated label that has a sufficiently large cost range, where a label is non-dominated if its estimated minimum cost is smaller than the smallest maximum cost (among all labels) and the cost range is the difference between the label\u2019s estimated maximum and minimum costs.\nIntuitively, COAL queries the cost of every label which cannot be ruled out as having the smallest cost on x, and where there is sufficient ambiguity about the actual value of the cost. The idea is that labels with little disagreement do not provide much information for further reducing the version space, since by construction\nAlgorithm 2: MAXCOST\n1: Input: (x, y), \u2206, , risk functional R\u0302(\u00b7; y) 2: gmin = argminginG R\u0302(g; y). 3: ` = 0, h = 1, c = 1. 4: while |h\u2212 `| \u2265 2 \u221a 3 do 5: gc \u2190 argming\u2208G R\u0303(g,\u2206/ 2, c; y) (see Eq. 6). 6: If gc \u2208 G(\u2206; y) (see Eq. 7), output gc(x) + . 7: (gl, gh)\u2190 BSEARCH((x, y, c), ,\u2206, R\u0302(\u00b7; y)). 8: If gh \u2208 G(4\u2206; y), output gh(x). 9: Else `\u2190 max{gl(x), `}, h\u2190 gh(x), c\u2190 h+`2 .\n10: end while 11: return c.\nall functions would suffer similar loss. Moreover, only the labels that could be the best need to be queried at all, since the cost-sensitive performance of a hypothesis hf depends only on the label that it predicts to be the best. Hence labels that are dominated or have small cost range need not be queried.\nSimilar querying strategies were used in prior works on binary and multiclass classification [22, 14, 1], but specialized to linear representations. The key advantage of the linear case is that the set F (formally, a different set with similar properties) can be maintained in closed form. This further leads to closed form solutions for c+(y) and c\u2212(y), so that the algorithms are easily implemented. However, with a general set G and a regression oracle, computing these confidence intervals is less straightforward. We use the MAXCOST and MINCOST subroutines, and discuss this aspect of our algorithm next."}, {"heading": "3.1 Efficient Computation of Cost Range", "text": "In this section, we describe the subroutines MAXCOST and MINCOST which use the oracle to compute approximations to the maximum and minimum cost on label y realized by Gi(y), the current version space (Eq. (2)).\nDescribing the algorithm requires some additional notation. Given the empirical risk functional R\u0302(g; y) over a set of examples (we suppress the subscript as the number of examples is fixed here), we define a weighted risk functional incorporating a fresh unlabeled example x as\nR\u0303(g, w, c; y) = R\u0302(g; y) + w(g(x)\u2212 c)2. (6)\nFinding argming R\u0303(g, w, c; y) involves a single oracle call. We also define a set of near-optimal regressors G(\u2206; y) = { g \u2208 G | R\u0302(g; y)\u2212min\ng\u2032 R\u0302(g\u2032; y) \u2264 \u2206\n} . (7)\nThus at round i, the set Gi(y) in COAL is equivalent to G(\u2206i; y), although we will use different radii here. The algorithm for the maximum cost approximation, displayed in Algorithm 2, is based on a form of binary search. (The minimum cost approximation is analogous.) The key idea is to solve a sequence of carefully designed regression problems involving the data accumulated so far along with the (x, y) pair in question with different weights.\nWhen invoked with a radius parameter \u2206, the algorithm maintains an interval [`, h] that contains c+(x, y,G(\u2206; y)) and uses a binary search to refine the interval. Using a fixed cost c and starting with\nAlgorithm 3: BINARYSEARCH(BSEARCH)\n1: Input: (x, y, c), , \u2206, risk functional R\u0302(\u00b7; y). 2: w1,` = 0, w1,h = \u2206/\n2, t = 1. 3: while |wt,` \u2212 wt,h| \u2265 2\u2206 do 4: wt \u2190 wt,`+wt,h2 , gt = argming\u2208G R\u0303(g, wt, c; y). 5: If gt \u2208 G(\u2206; y), wt+1,` \u2190 wt, wt+1,h \u2190 wt,h. 6: Else wt+1,` \u2190 wt,`, wt+1,h \u2190 wt. 7: t\u2190 t+ 1. 8: end while 9: return g` = argming R\u0303(g, wt,`, c; y), and gh = argming R\u0303(g, wt,h, c; y).\nsome initial weight w, at each iteration, the binary search computes argming R\u0303(g, w, c; y) and verifies if the resulting regressor belongs to G(\u2206; y). If it does, it increases w, and otherwise it shrinks w. Once a termination criteria is reached, the BINARYSEARCH routine outputs two regressors (g`, gh) that provide new upper and lower bounds on c+(x, y,G(\u2206; y)).\nThe MAXCOST routine terminates and outputs gh(x) if it has reasonable empirical regret. Otherwise, it updates parameters for the next binary search based on g`(x), gh(x).\nOur main algorithmic result guarantees that this procedure produces an adequate approximation to c+(x, y,G(\u2206; y)) without requiring too many oracle calls.\nTheorem 1. For any (x, y),\u2206, and , the MAXCOST algorithm outputs c\u0302 satisfying\nc+(x, y,G(\u2206; y)) \u2264 c\u0302 \u2264 c+(x, y,G(4\u2206; y)) + \u221a 3 .\nFurther, the algorithm uses O( \u22122 log(1/ )) oracle calls.\nAn immediate consequence of the theorem is a bound on the oracle complexity of COAL.\nCorollary 2. Over the course of n examples, COAL makes O(Kn2 log(n)) calls to the square loss oracle.\nThus COAL can be implemented in polynomial time for any set G that admits efficient square loss optimization. However, in practice, the number of oracle calls and the oracle itself are too computationally demanding to scale to larger problems. Our implementation alleviates this with an alternative heuristic approximation based on a sensitivity analysis of the oracle, which we detail in Section 6."}, {"heading": "4 Generalization Analysis", "text": "In this section, we derive generalization guarantees for COAL. Our analysis assumes that the regressor set G is large, but finite. We study two different settings: one with minimal assumptions and one low-noise setting.\nOur low-noise assumption is related to the Massart noise condition [21], which in binary classification posits that the Bayes optimal predictor is bounded away from 1/2 for all x. Our condition generalizes this to CSMC and posits that, the expected cost of the best label is separated from the expected cost of all other labels.\nAssumption 2. A distribution D supported over (x, c) pairs satisfies the Massart noise condition, if there exists \u03c4 > 0 such that for all x (with D(x) > 0),\nf?(x; y?(x)) \u2264 min y 6=y?(x) f?(x; y)\u2212 \u03c4,\nwhere y?(x) = argminy f ?(x; y).\nThe Massart noise condition describes favorable prediction problems that lead to sharper generalization and label complexity bounds for COAL. COAL can also be analyzed under a milder assumption inspired by the Tsybakov noise condition, an analysis that we defer to an extended version.\nOur results depend on the noise level in the problem, which we define using the following quantity, given any \u03b6 > 0.\nP\u03b6 , Pr x\u223cD [ min y 6=y?(x)\nf?(x; y)\u2212 f?(x; y?(x)) \u2264 \u03b6]. (8)\nP\u03b6 describes the probability that the expected cost of the best label, which is y?(x), is close to the expected cost of the second best label. When P\u03b6 is small for large \u03b6 the labels are well-separated so learning is easier. For instance, under a Massart condition P\u03b6 = 0 for all \u03b6 \u2264 \u03c4 .\nWe now state our generalization guarantee.\nTheorem 3. For any \u03b4 < 1/e, for all i \u2208 [n], with probability at least 1\u2212 \u03b4, we have\nEx,c[c(hfi+1(x))\u2212 c(hf?(x))] \u2264 min \u03b6>0\n{ \u03b6P\u03b6 +\n2\u03baK\u03bdn \u03b6i\n} ,\nwhere \u03ba = 80, \u03bdn = log ( 2n2|G|K \u03b4 ) , fi is as defined in Line 6 of Algorithm 1, and hfi is defined in Equation (1).\nIn the worst case, we bound P\u03b6 by 1 and optimize for \u03b6 to obtain an O\u0303( \u221a K log(|G|/\u03b4)/i) bound after i\nsamples. To compare, the standard generalization bound is O\u0303( \u221a\nlog(|F|/\u03b4)/i) [20], which agrees with our bound since |F| = |G|K in our case.\nHowever, since the bound captures the difficulty of the CSMC problem as measured by P\u03b6 , we can obtain a sharper result under Assumption 2 by setting \u03b6 = \u03c4 .\nCorollary 4. Under Assumption 2, for any \u03b4 < 1/e, for all i \u2208 [n], with probability at least 1\u2212 \u03b4, we have\nEx,c[c(hfi+1(x))\u2212 c(hf?(x))] \u2264 2\u03baK\u03bdn i\u03c4 .\nThus, Massart-type conditions lead to a faster O\u0303(1/n) convergence rate. This agrees with the literature on active learning for classification [21] and can be viewed as a generalization to CSMC. Importantly, both generalization bounds recover the optimal rates and are independent of the safety parameter \u03b2."}, {"heading": "5 Label Complexity Analysis", "text": "Without distributional assumptions, the label complexity of COAL can be O(n), just as in the binary classification case, since there may always be confusing labels that force querying. In line with prior work,\nwe introduce two disagreement coefficients that characterize favorable distributional properties. We first define a set of good classifiers, the cost-sensitive regret ball:\nFcsr(r) = { f \u2208 F \u2223\u2223\u2223 E [c(hf (x))\u2212 c(hf?(x))] \u2264 r} . We may now define the disagreement coefficients.\nDefinition 1 (Disagreement coefficients). Define\n\u03b3r(x, y) = \u03b3(x, y,Fcsr(r)), and DIS(r, y) = {x | \u2203f, f \u2032 \u2208 Fcsr(r), hf (x) = y 6= hf \u2032(x)}.\nThen the disagreement coefficients are defined as:\n\u03b81 , sup \u03b71,r>0 \u03b71 r P (\u2203y | \u03b3r(x, y) > \u03b71 \u2227 x \u2208 DIS(r, y))\n\u03b82 , sup \u03b71,r>0 \u03b71 r \u2211 y P (\u03b3r(x, y) > \u03b71 \u2227 x \u2208 DIS(r, y)) .\nIntuitively, the conditions in both coefficients correspond to the checks on the domination and cost range of a label in Lines 13 and 15 of Algorithm 1. Specifically, when x \u2208 DIS(r, y), there is confusion about whether y is the optimal label or not, and hence y is not dominated. The condition on \u03b3r(x, y) additionally captures the fact that a small cost range provides little information, even when y is non-dominated. Collectively, the coefficients capture the probability of an example x where the good classifiers disagree substantially on x in both predicted costs and labels. Importantly, the notion of good classifiers is via the algorithm-independent set Fcsr(r), and is only a property of F and D.\nThe definition is a natural adaptation from binary classification [16], where a similar disagreement region to DIS(r, y) is used. Our definition asks for confusion about the optimality of a specific label y, which provides more detailed information about the cost-structure than simply asking for any confusion among the good classifiers. The 1/r scaling leads to bounded coefficients in many examples [16], and we also scale by the cost range parameter \u03b71, so that the favorable settings for active learning can be concisely expressed as having \u03b81, \u03b82 bounded, as opposed to a complex function of \u03b71.\nThe next two results bound the labeling effort (Def. (4)) in the high noise and low noise cases respectively. The low noise assumption enables a significantly sharper bound.\nTheorem 5. With probability at least 1 \u2212 2\u03b4, the label complexity of the algorithm over n examples is bounded by,\nL1 = O ( (25)1/\u03b2 ( n\u03b81 \u221a K\u03bdn + log(1/\u03b4) )) L2 = O ( (25)1/\u03b2 ( n\u03b82 \u221a K\u03bdn +K log(1/\u03b4) )) ,\nwhere \u03bdn = log ( 2n2|G|K \u03b4 ) .\nTheorem 6. Assume the Massart noise condition holds. With probability at least 1\u2212 2\u03b4 the label complexity of the algorithm over n examples is at most,\nL1 = O ( 251/\u03b2 \u03c42 ( n\u03b2K log(n)\u03bdn\u03b81 + log(1/\u03b4) )) L2 = O ( 251/\u03b2K \u03c42 ( n\u03b2 log(n)\u03bdn [K\u03b81 + \u03b82] + log(1/\u03b4) )) .\nIn the high-noise case, the bounds scales with n\u03b8 for the respective coefficients. This agrees with results in binary classification, where at best constant-factor savings over passive learning are possible, when the disagreement coefficient is small. On the other hand, in the low noise case, the label complexity scales as O\u0303(c1/\u03b2n\u03b2\u03b8/\u03c42) for the appropriate coefficient, which is a polynomial improvement over passive learning. However, the constant in the label complexity scales exponentially with 1/\u03b2 so \u03b2 should not be chosen to be arbitrarily small. The influence of \u03b2 in our bound arises from using shrinking radii to ensure bad regressors do not influence the query rule. However we do believe that sharper bounds are possible.\nNote that \u03b82 can be much smaller than K\u03b81, as demonstrated through an example in the next section. In such cases, only a few labels are ever queried and the L2 bound in the high noise case is more interesting. Unfortunately, under Massart-noise, the L2 bound depends directly on K\u03b81 along with \u03b82, so that we do not benefit when \u03b82 K\u03b81. If we allow the \u03b7i parameter to depend on the noise level \u03c4 , we can obtain a better bound solely depending on \u03b82 for L2. However, we prefer to use the more robust choice 1/ \u221a i which still allows COAL to partially adapt to low-noise and achieves low label complexity. Unfortunately, translating our bounds to binary classification reveals suboptimality here. In particular, under Massart noise and bounded coefficients, the label complexity is typically log(n)/\u03c42 which contrasts with our n\u03b2/\u03c42 rate. Information-theoretically, the logarithmic rate is possible in CSMC, but it remains open whether an efficient algorithm can achieve it.\nOur loss in rate arises from setting \u03b2 > 0, but when \u03b2 = 0, sub-optimal regressors will leave the version space at some round i < n, at which point we stop accumulating evidence against them. Since with \u03b2 = 0 the radius \u2206i is non-decreasing, these regressors may re-enter the space at some later round and cause us to issue more queries. In binary classification, ideas based on hallucinating labels for unqueried examples address this issue [12], but this technique does not seem applicable here since the only safe choice of hallucinated cost that avoids eliminating f\u2217 appears to be f\u2217(x; y), which is naturally unknown. Our solution uses \u03b2 > 0 to ensure a shrinking radius. However, in order to avoid eliminating f?, the initial radius \u22061 must be larger than is required for standard concentration arguments, so the algorithm is somewhat conservative."}, {"heading": "5.1 Some Examples", "text": "We now describe two examples to give more intuition for COAL and our label complexity bounds.\nOur first example shows that querying only the non-dominated labels can dramatically reduce the label complexity. Consider a problem under Assumption 2, where the optimal cost is predicted perfectly, the second best cost is \u03c4 worse and all the other costs are substantially worse, but with variability in the predictions. Since all classifiers predict the right label, we get \u03b81 = \u03b82 = 0, so our label complexity bound is O(1). More intuitively, since every regressor is always certain of the optimal label and its cost, we actually make zero queries. On the other hand, all of the suboptimal labels have large cost ranges, and hence querying based solely on a cost range criteria leads to a large label complexity.\nA related example demonstrates the improvement in our query rule over more na\u00efve approaches where we query either no label or all labels, which is the natural generalization of query rules from multiclass\nclassification [1]. In the above example, if the best and second best labels are confused occasionally \u03b81 may be large, but we expect \u03b82 K\u03b81 since only the second best label can have mr(x, y) small. Thus, the L2 bound for our algorithm is a factor of K smaller than with a naive query rule since COAL only queries the best and second best labels."}, {"heading": "6 Experiments", "text": "For computational efficiency, we implemented an approximate version of Algorithm 1 using online optimization, based on online linear least-squares regression. The algorithm processes the data in one pass, computing an approximate ERM and cost ranges as described below.\nThe idea is to replace gi,y , the ERM, with an approximation goi,y obtained by online updates, and compute the minimum and maximum costs via a sensitivity analysis of the online update. Specifically, we define a sensitivity value s(x, c, goi,y) \u2265 0, which is the derivative of the prediction on x as a function of the importance weight w, for a fresh example x and cost c = 0 or c = 1 (for approximating c\u2212 and c+ respectively). Then we approximate c\u2212 via goi,y(x)\u2212 wo \u00b7 s(x, 0, goi,y) where wo is the largest weight w satisfying\nw(goi,y(x) 2 \u2212 (goi,y(x)\u2212 ws(x, 0, goi,y))2) \u2264 \u2206i,\nwhere \u2206i is the radius used at round i. We use an analogous technique to approximate the maximum cost. See Appendix A for more details."}, {"heading": "6.1 Simulated Active Learning", "text": "We performed simulated active learning experiments with three datasets. ImageNet 20 and 40 are sub-trees of the ImageNet hierarchy covering 20 and 40 most frequent classes, where each example has a single zero-cost label and the cost for incorrect labels is the tree-distance to the correct one. The feature vectors are the top layer of the Inception neural network [29]. The third dataset, RCV1-v2 [19], is a multilabel text-categorization dataset, which has 103 topic labels, organized as a tree with similar tree-distance cost structure as the ImageNet data. Some dataset statistics are in Figure 2 (upper right).\nWe compare our online version of COAL to passive online learning. We use the cost-sensitive oneagainst-all (CSOAA) implementation in Vowpal Wabbit4, which performs online linear regression for each label separately. There are two tuning parameters in our implementation. First, instead of \u2206i, we set the radius of the version space to \u2206\u2032i = \u03ba\u03bdi\u22121 i\u22121 (i.e. \u03b2 = 0 and the log factor \u03bdi = log ( 2(i\u22121)2|G|K \u03b4 ) scales with i) and instead tune the constant \u03ba. This alternate \u201cmellowness\" parameter controls how aggressive the query strategy is. The second parameter is the learning rate used by online linear regression5.\nFor each parameter setting and each dataset, we make one pass through the training set and check the test cost (which is just the normalized expected cost) of the model every doubling number of queries. We repeat this on 100 random permutations of the training data and plot the results in Figure 2. For each mellowness, we show the results of the best learning rate, which maximizes a notion of AUC that reflects the tradeoff between test cost and number of queries (see Eq. (11) in Appendix A).\nFigure 2 shows, for each dataset and mellowness, the number of queries against the median test cost along with bars extending from the 15th to 85th quantile. Overall, COAL achieves a better trade-off between\n4http://hunch.net/~vw 5We use the default online learning algorithm in Vowpal Wabbit, which is a scale-free [24] importance weight invariant [18] form of\nAdaGrad [15].\nperformance and queries. With proper mellowness parameter, active learning achieve similar test cost as passive learning with a factor of 8 to 32 less queries. On ImageNet 40 and RCV1-v2 (recall Figure 1), active learning achieves better test cost with a factor of 16 less queries. On RCV1-v2, COAL queries like passive up to around 256k queries, since the data is very sparse, and linear regression has the property that the cost range is maximal when an example has a new unseen feature. Once COAL sees all features a few times, it queries much more efficiently than passive. Note that these plots correspond to the label complexity L2, with similar results for L1 in Appendix A.3.\nWhile not always the best, we recommend using a mellowness setting of 0.01 in practice as it achieves reasonable performance on all three datasets. This is also confirmed by the learning-to-search experiments, which we discuss next."}, {"heading": "6.2 Learning to Search", "text": "We also experiment with COAL as the base leaner in learning-to-search [13, 11], which reduces joint prediction problems to CSMC. In this framework, a joint prediction example defines a search space, where a sequence of decisions are made to generate the structured label. We focus here on sequence labeling tasks, where the input is a sequence of words and the output is a sequence of labels (specifically, parts of speech or named entities).\nLearning-to-search solves joint prediction problems by generating the output one label at a time, conditioning the input x on all past decisions. Since mistakes may lead to compounding errors, it is natural to represent the decision space as a CSMC problem, where the classes are the \u201cactions\u201d available (possible\nlabels for a word) and the costs reflect the long term loss of each choice. Intuitively, we should be able to avoid expensive computation of long term loss on decisions like \u201cis \u2018the\u2019 a DETERMINER?\u201d once we are quite sure of the answer. Similar ideas motivate adaptive sampling for structured prediction. [26].\nWe specifically use AGGRAVATE [23, 11], which runs a learned policy to produce a backbone sequence of labels. For each position in the input sentence, it then considers all possible deviation actions and executes an oracle for the rest of the sequence. The loss on this complete output is used as the cost for the deviating action. Run in this way, AGGRAVATE requires mK roll-outs when the input sentence has m words and each word can take one of K possible labels.\nSince each roll-out takes O(m) time, this can be computationally prohibitive, and hence we use active learning to reduce the number of roll-outs. We use COAL and a passive learning baseline inside AGGRAVATE on three joint prediction datasets (dataset statistics are in Figure 2, upper right). As above, we use several mellowness values and the same AUC criteria to select the best learning rate. The results are in the bottom row of Figure 2, with the black arrow pointing to test cost for our recommended mellowness of 0.01.\nOverall, active learning reduces the number of roll-outs required, but the improvements vary on the three datasets. On the Wikipedia data, COAL performs a factor of 4 less rollouts to achieve similar performance to passive learning, and it also achieves substantially better test performance. A similar, but less dramatic, behavior arises on the NER task. On the other hand, COAL offers minimal improvement over passive learning on the POS-tagging task."}, {"heading": "7 Discussion", "text": "This paper presents a new active learning algorithm for cost-sensitive multiclass classification. The algorithm enjoys strong theoretical guarantees on running time, generalization error, and label complexity, and also outperforms passive baselines both in CSMC and structured prediction.\nWe close with some intriguing questions: 1. Can we use a square loss oracle in other partial information problems like contextual bandits? 2. Can we avoid the safety parameter to achieve the optimal complexity in the low noise case? We hope to answer these questions in future work."}, {"heading": "A Experimental Details", "text": ""}, {"heading": "A.1 Finding Cost Ranges with Online Approximation", "text": "Consider the maximum and minimum costs for a fixed label y at round i, both of which may be suppressed. Owing to the monotonicity property of R\u0302(g, w, c; y) (Lemma 1), an alternative to MINCOST and MAXCOST is to find\nw := max{w | R\u0302(g w )\u2212 R\u0302(gi,y) \u2264 \u2206i} (9)\nw := max{w | R\u0302(gw)\u2212 R\u0302(gi,y) \u2264 \u2206i} (10)\nand return g w (x) and gw(x) as the minimum and maximum costs, where\ng w , arg min g\u2208G R\u0302(g) + w(g(x)\u2212 0)2\ngw , arg min g\u2208G\nR\u0302(g) + w(g(x)\u2212 1)2\nand gi,y , argming\u2208G R\u0302(g) as in Algorithm 1. We use two steps of approximation here. Using the definition of gw and gw we have:\nR\u0302(g w )\u2212 R\u0302(gi,y) \u2264 w \u00b7 gi,y(x)2 \u2212 w \u00b7 gw(x) 2\nR\u0302(gw)\u2212 R\u0302(gi,y) \u2264 w \u00b7 (gi,y(x)\u2212 1)2 \u2212 w \u00b7 (gw(x)\u2212 1)2.\nWe use this upper bound in place of R\u0302(gw)\u2212 R\u0302(gi,y) in Eqs. (9) and (10). Second, we replace gi,y , gw, and gw with approximations obtained by online updates. More specifically, we replace gi,y with g o i,y , the current regressor produced by all online updates so far, and approximate the others by\ng w (x) \u2248 goi,y(x)\u2212 w \u00b7 s(x, 0, goi,y) gw(x) \u2248 goi,y(x) + w \u00b7 s(x, 1, goi,y)\nwhere s(x, y, goi,y) \u2265 0 is a sensitivity value that approximates the change in prediction on x resulting from an online update to goi,y with features x and label y. The computation of this sensitivity value is governed by the actual online update where we compute the derivative of the change in the prediction as a function of the importance weight w for a hypothetical example with cost 0 or cost 1 and the same features. This is possible for essentially all online update rules on importance weighted examples where it corresponds to taking the limit as w \u2192 0 of the change in prediction due to an update divided by w. By inspection this requires only O(d) time per example, where d is the average number of non-zero features. With these two steps, we obtain approximate minimum and maximum costs using:\ngoi,y(x)\u2212 wo \u00b7 s(x, 0, goi,y) goi,y(x) + w o \u00b7 s(x, 1, goi,y)\nwhere\nwo , max{w | w ( goi,y,(x) 2 \u2212 (goi,y(x)\u2212 w \u00b7 s(x, 0, goi,y))2 ) \u2264 \u2206i}\nwo , max{w | w ( (goi,y,(x)\u2212 1)2 \u2212 (goi,y(x) + w \u00b7 s(x, 1, goi,y)\u2212 1)2 ) \u2264 \u2206i}.\nThe online update guarantees that goi,y(x) \u2208 [0, 1]. Since the minimum cost is lower bounded by 0, we have wo \u2208 ( 0, goi,y(x)\ns(x,0,goi,y)\n] . Finally, because the objective w(goi,y(x))\n2 \u2212 w(goi,y(x) \u2212 w \u00b7 s(x, 0, goi,y))2 is increasing in w within this range (which can be seen by inspecting the derivative), we can find wo with binary search. Using the same techniques, we also obtain an approximate maximum cost.\nIt is worth noting that the approximate cost ranges (without the sensitivity trick) are contained in the exact cost ranges because we approximate the difference in squared error by an upper bound. Hence, the query rule in this online algorithm should be more aggressive than the query rule in Algorithm 1."}, {"heading": "A.2 Choosing the Learning Rate", "text": "For all experiments, we show the results obtained by the best learning rate for each mellowness on each dataset. We choose the best learning rate as follows. For each dataset let perf(m, l, q, t) denote the test performance of the algorithm using mellowness m and learning rate l on the tth permutation of the training data under a query budget of 2(q\u22121) \u00b7 10 \u00b7 K, q \u2265 1. Let query(m, l, q, t) denote the number of queries actually made. Note that query(m, l, q, t) < 2(q\u22121) \u00b7 10 \u00b7K if the algorithm runs out of the training data before reaching the qth query budget6 . To evaluate the trade-off between test performance and number of queries, we define the following performance measure:\nAUC(m, l, t) = 1\n2 qmax\u2211 q=1 ( perf(m, l, q + 1, t) + perf(m, l, q, t) ) \u00b7 ( log2 query(m, l, q + 1, t) query(m, l, q, t) ) , (11)\nwhere qmax is the minimum q such that 2(q\u22121) \u00b710 is larger than the size of the training data. This performance measure is the area under the curve of test performance against numbers of queries in log2 scale. A large value means the test performance quickly improves with the number of queries. The best learning rate for mellowness m is then chosen as\nl?(m) , arg max l median1\u2264t\u2264100 AUC(m, l, t).\nThe best learning rates for different datasets and mellowness settings are in Table 1."}, {"heading": "A.3 Additional Figures for Simulated Active Learning", "text": "In Figure 3, we plot the test error as a function of the number of examples for which at least one query was requested, for each dataset and mellowness parameter. This experimentally corresponds to the L1 term in our label complexity analysis.\n6In fact, we check the test performance only in between examples, so query(m, l, q, t) may be larger than 2(q\u22121) \u00b7 10 \u00b7K by an additive factor of K, which is negligibly small.\nIn comparison with Figure 2 involving the total number of queries, the improvements offered by active learning are slightly less dramatic here. This suggests that our algorithm queries just a few labels for each example, but does end up issuing at least one query on most of the examples. Nevertheless, one can still achieve test cost competitive with passive learning using a factor of 2-16 less labeling effort, as measured by L1."}, {"heading": "B Running time analysis", "text": "Throughout this section, fix an x, y pair, an iteration i, as well a radius \u2206 and an accuracy . We focus on approximating c+(x,G(\u2206; y)) (See Eqs. (2) and (7)), approximating the minimum cost is very similar. To simplify notation, we drop dependence on x and y. We recall our earlier notation R\u0302i(g; y) (Eq. (5)), except we drop the dependence on both y and i which are fixed throughout this appendix. We also recall some other important pieces of notation which are accordingly simplified for brevity\nR\u0302(g) = E\u0302[(g(x)\u2212 c(y))21 (y queried on x)]\nG(\u2206) = {g \u2208 G : R\u0302(g)\u2212min g R\u0302(g) \u2264 \u2206}\ngmin = argmin g\u2208G R\u0302(g)\nR\u0303(g, w, c) = R\u0302(g) + w(g(x)\u2212 c)2\nc+(\u03b1\u2206) = max g\u2208G(\u03b1\u2206) g(x)\nc? = c+(\u2206)\nR\u0302(g) is the empirical square loss used to define the set of good regressors G(\u2206) in the algorithm. The precise form of R\u0302(g) does not matter in this section. gmin is the empirical square loss minimizer, which is the center of the ball G(\u2206). R\u0303(g, w, c) is the empirical square loss with one additional example, with features x, target c, and weight w. This functional is used to define new square loss problems in our algorithm. Our goal is to find a number c\u0302 such that,\nc? \u2264 c\u0302 \u2264 c+(4\u2206) + \u221a 3 .\nFinally, let g? be any function such that g?(x) = c? and R\u0302(g?)\u2212 R\u0302(gmin) \u2264 \u2206. In other words g? realizes the maximum cost on example x. Note that g? is not the same regressor that satisfies the realizability condition.\nWe start the running time analysis with several lemmas that characterize the behavior of various components of the algorithm.\nAn important structure to the square loss problem is a monotonicity property of both the risk functional and the predictions.\nLemma 1. For any c and for w\u2032 \u2265 w \u2265 0, let g = argming R\u0303(g, w, c) and g\u2032 = argming R\u0303(g, w\u2032, c). Then\nR\u0302(g\u2032) \u2265 R\u0302(g) and (g\u2032(x)\u2212 c)2 \u2264 (g(x)\u2212 c)2.\nProof. By the definitions,\nR\u0302(g\u2032) + w\u2032(g\u2032(x)\u2212 c)2 = R\u0303(g\u2032, w\u2032, c)\n\u2264 R\u0302(g) + w\u2032(g(x)\u2212 c)2\n= R\u0302(g) + w(g(x)\u2212 c)2 + (w\u2032 \u2212 w)(g(x)\u2212 c)2\n\u2264 R\u0302(g\u2032) + w(g\u2032(x)\u2212 c)2 + (w\u2032 \u2212 w)(g(x)\u2212 c)2.\nRearranging shows that\n(w\u2032 \u2212 w)(g\u2032(x)\u2212 c)2 \u2264 (w\u2032 \u2212 w)(g(x)\u2212 c)2.\nSince w\u2032 \u2265 w, we must have (g\u2032(x)\u2212 c)2 \u2264 (g(x)\u2212 c)2, which is the second claim. For the first claim, the definition of g gives\nR\u0302(g) + w(g(x)\u2212 c)2 \u2264 R\u0302(g\u2032) + w(g\u2032(x)\u2212 c)2\nRearranging this inequality gives,\nR\u0302(g\u2032)\u2212 R\u0302(g) \u2265 w((g(x)\u2212 c)2 \u2212 (g\u2032(x)\u2212 c)2) \u2265 0.\nThe next critical lemma shows that the termination condition in Line 6 of MAXCOST meets the accuracy guarantee.\nLemma 2. If c \u2265 c?, w \u2265 \u2206/ 2 and g = argming R\u0303(g, w, c) then g(x) \u2265 c? \u2212 . Further, if g \u2208 G(\u2206), then g(x) \u2264 c?.\nProof. The second claim is straightforward by the definition of c?. For the first claim, we work to establish a contradiction. Suppose that g(x) < c? \u2212 . By the facts that g is the minimizer of R\u0303(g, w, c), gmin is the minimizer of R\u0302(g), and c \u2265 c?, we have\nw(c\u2212 (c? \u2212 ))2 < w(c\u2212 g(x))2 \u2264 R\u0303(g, w, c)\u2212 R\u0302(gmin) \u2264 R\u0303(g?, w, c)\u2212 R\u0302(gmin) \u2264 \u2206 + w(c\u2212 c?)2.\nWe may further lower bound (again using c \u2265 c?),\n(c\u2212 c? + )2 = (c\u2212 c?)2 + 2(c\u2212 c?) + 2 \u2265 (c\u2212 c?)2 + 2.\nRe-arranging proves that\nw < \u2206/ 2.\nThe contrapositive is that if w \u2265 \u2206/ 2, then we must have g(x) \u2265 c? \u2212 , which is the desired claim.\nThe next lemma is the main result for the BINARYSEARCH subroutine.\nLemma 3. Suppose we invoke the subroutine BINARYSEARCH with parameters and \u2206. Then it terminates in polynomial time with O(log2(1/(\n2))) oracle calls. The algorithm outputs two regressors (g`, gh) and if c \u2265 c? is passed as input then c? \u2208 [g`(x), gh(x)]. If additionally, gh /\u2208 G(4\u2206) then c? \u2264 (g`(x) + gh(x))/2.\nProof. The logarithmic running time is fairly straightforward since in each iteration the algorithm halves the interval, has initial interval of size \u2206/ 2 and terminates when the interval is smaller than 2\u2206. Thus for T \u2265 log2(1/(2 2)) the interval has size at most\n2\u2212T (\u2206/ 2) \u2264 2\u2212 log2(1/(2 2))(\u2206/ 2) = 2\u2206\nHence the number of iteration is upper bounded by dlog2(1/(2 2))e. For the first termination claim, the invariant that we maintain is that for all t \u2265 1, gt,h = argming R\u0303(g, wt,h, c) satisfies gt,h(x) \u2265 c? while gt,` = argming R\u0303(g, wt,`, c) satisfies gt,`(x) \u2264 c?. For gt,h, we first establish the base case. Observe that g1,h = gc (computed in MAXCOST just before the invocation of BINARYSEARCH) and R\u0302(gc) \u2265 R\u0302(gmin) + \u2206 by the termination check in Line 6. By construction, in this iteration and in all others, we have that gt,h /\u2208 G(\u2206), since this is the requirement for updating wt,h. But since gt,h minimizes the risk function R\u0303(g, wt,h, c) we get,\nR\u0302(gmin) + \u2206 + wt,h(gt,h(x)\u2212 c)2 \u2264 R\u0303(gt,h, wt,h, c) \u2264 R\u0303(g?, wt,h, c) \u2264 R\u0302(gmin) + \u2206 + wt,h(c? \u2212 c)2.\nSince c \u2265 c?, this implies that gt,h(x) \u2265 c?. The proof for gt,` is simpler, since we only shrink the interval up if we find something in G(\u2206). By definition of c? the cost of gt,`(x) for these iterations satisfies gt,`(x) \u2264 c?. For the second termination claim we must use the condition |wt,h \u2212 wt,`| \u2264 2\u2206 by the termination condition and gh /\u2208 G(4\u2206). Let t be the terminal iteration, so g` = argming R\u0303(g, wt,`, c) and analogously for gh. Assume for the sake of contradiction that c? \u2265 (gh(x) + g`(x))/2. Since c \u2265 c?, this implies that\nR\u0302(gmin) + 4\u2206 + wt,h(gh(x)\u2212 c)2 \u2264 R\u0303(gh, wt,h, c) \u2264 R\u0303(g?, wt,h, c) \u2264 R\u0302(gmin) + \u2206 + wt,h(c\u2212 c?)2 \u2264 R\u0302(gmin) + \u2206 + wt,h ( c\u2212 gh(x) + g`(x)\n2\n)2 .\nSimilarly we have R\u0302(gmin) + wt,`(g`(x)\u2212 c)2 \u2264 R\u0303(g`, wt,`, c) \u2264 R\u0303(g?, wt,`, c) \u2264 R\u0302(gmin) + \u2206 + wt,` ( c\u2212 gh(x) + g`(x)\n2\n)2 .\nAdding the two equations gives 2\u2206 + wt,`(c\u2212 g`(x))2 + wt,h(c\u2212 gh(x))2 \u2264 (wt,h + wt,`) ( c\u2212 gh(x) + g`(x)\n2 )2 \u21d2 2\u2206 + wt,h [ (c\u2212 g`(x))2 + (c\u2212 gh(x))2 ] \u2264 2wt,h ( c\u2212 gh(x) + g`(x)\n2\n)2 + (wt,h \u2212 wt,`)(c\u2212 g`(x))2\n\u21d2 2\u2206 + wt,h [ (c\u2212 g`(x))2 + (c\u2212 gh(x))2 ] \u2264 2wt,h ( c\u2212 gh(x) + g`(x)\n2\n)2 + 2\u2206 since c, g`(x) \u2208 [0, 1]\n\u21d2 1 2 (c\u2212 g`(x))2 + 1 2 (c\u2212 gh(x))2 \u2264\n( c\u2212 gh(x) + g`(x)\n2\n)2 .\nThe last line is a contradiction since E[f(Z)] \u2265 f(E[Z]) for convex f , which can be applied by taking Z = Unif({g`(x), gh(x)}) and f(y) = (c\u2212 y)2.\nThe last lemma ensures sufficient progress in the case when gh /\u2208 G(4\u2206), which is crucial for the oracle complexity bound. Lemma 4. Suppose c \u2265 c? and that there exists g \u2208 G(\u2206) such that c\u2212 g(x) = \u03b4 with \u03b4 \u2208 [ \u221a\n3 , 1]. Then if the output (g`, gh) of BINARYSEARCH satisfies R\u0302(gh)\u2212 R\u0302(gmin) > 4\u2206, then gh(x) \u2264 c+ \u03b4 \u2212 2.\nProof. We know that we never use a weight larger than \u2206/ 2 by the initialization of w1,`, w1,h. Now suppose that we output gh such that R\u0302(gh) > 4\u2206, which by construction is the minimizer of R\u0303(g, w, c) for some w \u2264 \u2206/ 2. Then\nR\u0302(gmin) + 4\u2206 + w(gh(x)\u2212 c)2 \u2264 R\u0303(gh, w, c) \u2264 R\u0303(g, w, c) \u2264 \u2206 + w(g(x)\u2212 c)2 = R\u0302(gmin) + \u2206 + w\u03b42.\nRearranging and using the upper bound w gives\n(gh(x)\u2212 c)2 \u2264 \u03b42 \u2212 3 2.\nThe condition on \u03b4 ensures that the right hand side is non-negative. It is easy to see that \u03b42\u22123 2 \u2264 (\u03b4\u2212 2/\u03b4)2 simply by expanding the square. Hence we get that\n|gh(x)\u2212 c| \u2264 |\u03b4 \u2212 2/\u03b4| \u2264 \u03b4 \u2212 2.\nWe can safely remove the absolute value since we have the condition that \u03b4 \u2265 \u221a\n3 , which ensures that \u03b4 \u2212 2/\u03b4 is non-negative. Since gh is the result of an oracle call with weight w \u2264 \u2206/ 2, either it has R\u0302(gh)\u2212 R\u0302(gmin) \u2264 4\u2206, or it must have gh(x) \u2264 c+ \u03b4 \u2212 2.\nWe are now ready to prove Theorem 1\nProof of Theorem 1. The first step of the proof is to inductively verify that c \u2265 c?, h \u2265 c?, ` \u2264 c? at all steps in the algorithm execution. These invariants are clearly maintained at the onset of the algorithm. Now suppose they are maintained at the onset of some iteration. If gc satisfies R\u0302(gc) \u2264 R\u0302(gmin) + \u2206, then by Lemma 2 we are done. Otherwise, we obtain two regressors (g`, gh) from BINARYSEARCH. For the lower bound, we always have g`(x) \u2264 c? by Lemma 3, which verifies the inductive step for `. For the upper bound to c?, if R\u0302(gh)\u2212 R\u0302(gmin) \u2264 4\u2206 then by Lemma 3, we know that c? \u2264 gh(x), but we also know that gh(x) \u2264 c+(4\u2206) by the definition, so we are done. The last case is when R\u0302(gh) > 4\u2206, but here we may apply the second statement of Lemma 3, which asserts that c? \u2264 (gh(x) + gl(x))/2. The settings of `, h, c now verify the inductive claim, since ` \u2265 gl implies that (h+ l)/2 \u2265 (gh(x) + gl(x))/2 \u2265 c?.\nThis immediately proves the correctness of the algorithm, since the loop stopping condition, along with the invariant, guarantees that c \u2265 c? \u2265 ` which means that\nc\u0302\u2212 c? \u2264 c\u2212 ` = h\u2212 ` 2 \u2264 \u221a 3 .\nFor the iteration complexity, we must apply Lemma 4. In particular, we use the width of the interval [`, h] which contains c? as a potential function and show that it decreases with every step. Let \u03b4t denote h \u2212 `, which is the width of the interval before the tth iteration (so \u03b41 = 1). Every non-terminal iteration satisfies c \u2265 c?. Moreover, for any t > 1, we use as the regressor g, the one that achieved the value ` used to define c.\nThis ensures that g(x) = `. Furthermore, in application of Lemma 4, we set \u03b4 , c\u2212 g(x) = c\u2212 `, which conveniently gives 2\u03b4 = \u03b4t = h\u2212 `. Recall that we entered the loop at tth iteration, meaning that \u03b4t \u2265 2 \u221a 3\nand hence \u03b4 \u2208 [ \u221a\n3 , 1]. Lemma 4 states that either we terminate successfully, or we are guaranteed that gh(x) \u2264 c+ \u03b4 \u2212 2. This means that\n\u03b4t+1 = gh(x)\u2212max{`, g`(x)} \u2264 c+ \u03b4 \u2212 2 \u2212 ` = \u03b4 + \u03b4 \u2212 2 = \u03b4t \u2212 2,\nwhere the first equality used c\u2212 ` = \u03b4 which is true by definition. Since we terminate at the first T such that \u03b4T \u2264 2 \u221a 3 , we require at most O(1/ 2) iterations. By Lemma 3, each iteration takes O(log(1/ )) oracle calls."}, {"heading": "C Generalization analysis", "text": "To bound the generalization error of Algorithm 1, we start by defining the central random variable in the analysis. At round i, recall our notation Qi(y) = 1 (query y on example xi) which indicates the query rule. The central random variable we study is,\nMi(g; y) , ( (g(xi)\u2212 ci(y))2 \u2212 (f?(xi; y)\u2212 ci(y))2 ) Qi(y). (12)\nHere (xi, ci) is the ith example and cost presented to the algorithm. For simplicity, we write Mi when the dependence on g and y is clear from context. For a vector regressor f , we write\nMi(f ; y) ,Mi(f(\u00b7; y); y).\nWe also recall some of the key constants and notations which were defined in Algorithm 1 and are heavily used throughout this appendix.\n\u2206i = \u03ba i\u22121 i\u2212 1 , i = (n i )\u03b2 log ( 2n2|G|K \u03b4 ) , \u03ba = 80.\nR\u0302i(g; y) = 1\ni\u2212 1 i\u22121\u2211 j=1 [ (g(xj)\u2212 cj(y))2Qj(y) ] .\ngi,y = argmin g\u2208G\nR\u0302i(g; y), and fi = {gi,y}Ky=1.\nGi(y) = {g \u2208 G|\u2200y, R\u0302i(g; y) \u2264 R\u0302i(gi,y; y) + \u2206i},\nFi = {f \u2208 GK |\u2200y, R\u0302i(f(\u00b7; y); y) \u2264 R\u0302i(gi,y; y) + \u2206i}.\nFor \u22061 we use the convention that 1/0 =\u221e so the initial radius is infinite. Let Ei[\u00b7] and Vari[\u00b7] denote the expectation and variance conditioned on all randomness up to round i\u2212 1. With these definitions, we turn to several supporting claims."}, {"heading": "C.1 Supporting Lemmata", "text": "Theorem 7 (Freedman-type Inequality [8, 2]). Let X1, . . . , XT be a sequence of real-valued random variables. Assume for all t \u2208 {1, . . . , T} that |Xt| \u2264 R and E[Xt|X1, . . . , Xt\u22121] = 0. Define S = \u2211T t=1Xt\nand V = \u2211T t=1 E[X2t |X1, . . . , Xt\u22121]. For any \u03b4 \u2208 (0, 1) and \u03bb \u2208 [0, 1/R], with probability at least 1\u2212 \u03b4,\nS \u2264 (e\u2212 2)\u03bbV + ln(1/\u03b4) \u03bb\nLemma 5 (Concentration of squared loss). For any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4, the following holds for all g \u2208 G, y \u2208 Y, i \u2208 [n], t \u2208 [n],\u2223\u2223\u2223\u2223\u2223\u2223 i+t\u22121\u2211 j=i Ej [Mj ]\u2212 i+t\u22121\u2211 j=i Mj \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 2 \u221a\u221a\u221a\u221ai+t\u22121\u2211 j=i Var j [Mj ]\u03bdn + 2\u03bdn where \u03bdn , log ( 2n2|G|K \u03b4 ) .\nNote that t = nt \u03b2\u03bdn, is a scaled version of the confidence bound here, where the scaling shrinks\npolynomially with t.\nProof. First observe that by the rescaling of the failure parameter, we can apply Freedman\u2019s inequality for each i, t, y, g and for each tail and a union bound proves the result.\nWe now apply the Freedman-type inequality in Theorem 7. For a fixed g \u2208 G, y \u2208 Y , the random variable Mi is measurable with respect to the filtration {(xj , {(c(xj ; y), Qj(y))}y\u2208Y )}ij=1), so Mi \u2212 Ei[Mi] forms a martingale difference sequence, where Ei[\u00b7] denotes expectation conditioned on all randomness up to round i\u2212 1. Moreover Mi \u2212 Ei[Mi] and Ei[Mi]\u2212Mi are both conditionally centered and clearly at most 2. Thus Freedman\u2019s inequality gives,\ni+t\u22121\u2211 j=i Mj \u2212 Ej [Mj ] \u2264 2 \u221a\u221a\u221a\u221ai+t\u22121\u2211 j=i Var j [Mj ]\u03bdn + 2\u03bdn,\nexcept with probability \u03b42n2|G|K . This follows by observing that (e \u2212 2) \u2264 1 and setting \u03bb = \u221a \u03bdn/V ,\nprovided it meets the constraint \u03bb \u2264 1/R. Otherwise we set \u03bb = 1/R and use the fact that 1/R \u2264 \u221a \u03bdn/V .\nThe bound on the right hand side also holds for the lower tail, again except with same probability. Thus a union bound over both tails, all g \u2208 G, y \u2208 Y and pairs i, t gives the result.\nLemma 6 (Bounding variance of regression regret). We have for all (g, y) \u2208 G \u00d7 Y ,\nEi[Mi] = Ei [ Qi(y)(g(xi)\u2212 f?(xi; y))2 ] ,\nVar i\n[Mi] \u2264 4Ei[Mi].\nProof. We take expectation of Mi over the cost conditioned on a fixed example xi = x and a fixed query outcome Qi(y):\nE[Mi | xi = x,Qi(y)] = Qi(y)\u00d7 Ec[g(x)2 \u2212 f?(x; y)2 \u2212 2c(y)(g(x)\u2212 f?(x; y)) | xi = x] = Qi(y) ( g(x)2 \u2212 f?(x; y)2 \u2212 2f?(x; y)(g(x)\u2212 f?(x; y)) ) = Qi(y)(g(x)\u2212 f?(x; y))2.\nThe second equality is by Assumption 1, which implies E[c(y) | xi = x] = f?(x; y). Taking expectation over xi and Qi(y), we have\nEi[Mi] = Ei [ Qi(y)(g(xi)\u2212 f?(xi; y))2 ] .\nFor the variance:\nVar i\n[Mi] \u2264 Ei[M2i ] = \u00b7Ei [ Qi(y)(g(xi)\u2212 f?(xi; y))2(g(xi) + f?(xi; y)\u2212 2c(y))2 ] \u2264 4 \u00b7 Ei [ Qi(y)(g(xi)\u2212 f?(xi; y))2\n] = 4Ei[Mi].\nLemma 7 (Sharp cost-sensitive bound). For all i > 0 if f? \u2208 Fi, then for all f \u2208 Fi\nEx,c[c(hf (x))\u2212 c(hf?(x))] \u2264 min \u03b6>0\n{ \u03b6P\u03b6 + 1 (\u03b6 \u2264 2\u03b7i) 2\u03b7i +\n4\u03b72i \u03b6 + 6 \u03b6 \u2211 y Ei [Mi(f ; y)]\n} ,\nwhere P\u03b6 = Prx\u223cD[miny 6=hf? (x) f ?(x, y) \u2264 f?(x, hf?(x)) + \u03b6] is the probability that the expected cost of second best and best label are within \u03b6 of each other.\nProof. Let y(x) = hf (x) and y?(x) = hf?(x) for shorthand. Define S\u03b6(x) = 1 (f?(x, y(x)) \u2264 f?(x, y?(x)) + \u03b6) and S\u2032\u03b6(x) = 1 ( miny 6=y?(x) f ?(x, y) \u2264 f?(x, y?(x)) + \u03b6 ) . Observe that for fixed \u03b6 , S\u03b6(x)1 (y(x) 6= y?(x)) \u2264 S\u2032\u03b6(x) for all x. We can also majorize the complementary indicator to obtain the inequality\nSC\u03b6 (x) \u2264 (f?(x, y(x))\u2212 f?(x, y?(x))\n\u03b6 .\nWe begin with the definition of realizability, which gives\nEx,c[c(hf (x))\u2212 c(hf?(x)] = Ex [f?(x, y(x))\u2212 f?(x, y?(x))1 (y(x) 6= y?(x))] = Ex [( S\u03b6(x) + S C \u03b6 (x) ) (f?(x, y(x))\u2212 f?(x, y?(x)))1 (y(x) 6= y?(x)) ] \u2264 \u03b6ExS\u2032\u03b6(x) + Ei [ SC\u03b6 (x)1 (y(x) 6= y?(x)) (f?(x, y(x))\u2212 f?(x, y?(x)))\n] The first term here is exactly the \u03b6P\u03b6 term in the bound. We now focus on the second term, which depends on our query rule. For this we must consider three cases.\nCase 1. If both y(x) and y?(x) are not queried, then it must be the case that both have small cost ranges. This follows since f \u2208 Fi and hf (x) = y(x) so y?(x) does not dominate y(x). Moreover, since the cost ranges are small on both y(x) and y?(x), since we know that f? is well separated under event SC\u03b6 (x), the relationship between \u03b6 and \u03b7i governs whether we make a mistake or not. Specifically, we get that SC\u03b6 (x)1 (y(x) 6= y?(x))1 (no query) \u2264 1 (\u03b6 \u2264 2\u03b7i) at round i. In other words, if we do not query and the separation is big but we make a mistake, then it must mean that the cost range threshold \u03b7i is also big.\nUsing this argument, we can bound the second term as,\nEi [ SC\u03b6 (x)1 (y(x) 6= y?(x))1 (y(x), y?(x) not queried) (f?(x, y(x))\u2212 f?(x, y?(x))) ] \u2264 Ei [ SC\u03b6 (x)1 (y(x) 6= y?(x))1 (y(x), y?(x) not queried) (f?(x, y(x))\u2212 f(x, y(x) + f(x, y?(x))\u2212 f?(x, y?(x)))\n] \u2264 Ei [ SC\u03b6 (x)1 (y(x) 6= y?(x))1 (y(x), y?(x) not queried) 2\u03b7i\n] \u2264 Ei [1 (\u03b6 \u2264 2\u03b7i) 2\u03b3] = 1 (\u03b6 \u2264 2\u03b7i) 2\u03b7i.\nCase 2. If both y(x) and y?(x) are queried, we can easily relate the second term to the square loss,\nEi [ SC\u03b6 (x)1 (y(x), y ?(x) both queried) (f?(x, y(x))\u2212 f?(x, y?(x))) ]\n\u2264 1 \u03b6 Ei [ 1 (y(x), y?(x) both queried) (f?(x, y(x))\u2212 f?(x, y?(x)))2 ] \u2264 1 \u03b6 Ei [ 1 (y(x), y?(x) both queried) (f?(x, y(x))\u2212 f(x, y(x)) + f(x, y?(x))\u2212 f?(x, y?(x)))2\n] \u2264 2 \u03b6 Ei [ 1 (y(x) queried) (f?(x, y(x))\u2212 f(x, y(x)))2 + 1 (y?(x) queried) (f(x, y?(x))\u2212 f?(x, y?(x)))2\n] \u2264 2 \u03b6 \u2211 y Ei [ Qi(y)(f ?(x, y(x))\u2212 f(x, y(x)))2 ] = 2 \u03b6 \u2211 y Ei [Mi(f ; y)] .\nPassing from the second to third line here is justified by the fact that f?(x, y(x)) \u2265 f?(x, y?(x)) and f(x, y(x)) \u2264 f(x, y?(x)) so we added two non-negative quantities together. The last step uses Lemma 6. While not written, we also use the event 1 (y(x) 6= y?(x)) to avoid losing a factor of 2.\nCase 3. The last case is if one label is queried and the other is not. Both cases here are analogous, so we do the derivation for when y(x) is queried but y?(x) is not. Since in this case, y?(x) is not dominated (hf (x) is never dominated provided f \u2208 Fi), we know that the cost range for y?(x) must be small. Using this fact, and essentially the same argument as in case 2, we get\nEi [ SC\u03b6 (x)1 (y(x) queried, y ?(x) not) (f?(x, y(x))\u2212 f?(x, y?(x))) ]\n1 \u03b6 Ei [ 1 (y(x) queried, y?(x) not) (f?(x, y(x))\u2212 f?(x, y?(x)))2 ] \u2264 2 \u03b6 Ei [ 1 (y(x) queried, y?(x) not) (f?(x, y(x))\u2212 f(x, y(x)))2 + (f(x, y?(x))\u2212 f?(x, y?(x)))2\n] \u2264 2\u03b7 2 i\n\u03b6 +\n2 \u03b6 Ei [ 1 (y(x) queried) (f?(x, y(x))\u2212 f(x, y(x)))2 ] \u2264 2\u03b7 2 i\n\u03b6 +\n2\n\u03b6 \u2211 y Ei [Mi(f ; y)] .\nWe also obtain this term for the other case where y?(x) is queried by y(x) is not. To summarize, adding up the contributions from these cases (which is an over-estimate since at most one case can occur and all are non-negative), we get\nEx,c[c(hf (x))\u2212 c(hf?(x)] \u2264 \u03b6P\u03b6 + 1 (\u03b6 \u2264 2\u03b7i) 2\u03b7i + 4\u03b72i \u03b6 + 6 \u03b6 \u2211 y Ei [Mi(f ; y)] .\nThis bound holds for any \u03b6, so it holds for the minimum."}, {"heading": "C.2 Proof of Theorem 3", "text": "Conditioning on the high-probability event in Lemma 5, we prove the theorem by induction. Define\n\u2206\u2032i = min{1, \u03ba\u03bdn i\u2212 1\n}, \u03bdn = log (\n2n2|G|K \u03b4\n) .\nHere and throughout we will make use of the following simple fact, which applies since i \u2264 n, so the premultiplier on i is at least 1.\nFact 1. For all i \u2208 [n], we have\n\u03bdn \u2264 i.\nConcretely we consider the inductive hypothesis:\n\u2200i \u2265 1, R\u0302i(f?(\u00b7; y); y) \u2264 min g\u2208G R\u0302i(g; y) + c0\u03bdn i\u2212 1 and Ex,c[c(hfi(x))\u2212 c(hf?(x))] \u2264 min \u03b6>0\n{ \u03b6P\u03b6 +\n2K\u2206\u2032i \u03b6 } (13)\nwhere c0 = 10. The first claim in particular implies that f?(\u00b7; y) \u2208 Fi since we chose \u2206i+1 = \u03ba i/i and using Fact 1. For the base case i = 1, observe that the right hand side of the first inequality is infinity but the empirical squared loss is 0 for all regressors. Hence the first claim is trivially satisfied. Moreover, because the excess cost-sensitive classification risk is always upper-bounded by 1, it is trivially bounded by 2K\u2206 \u2032 1\n\u03b6 for any \u03b6 \u2208 [0, 1]. For \u03b6 > 1, we have \u03b6P\u03b6 = \u03b6 so again the bound is trivial.\nNow assume the inductive hypothesis holds for the first i rounds, i \u2265 1. We want to analyze the set Fi+1, which is computed at the end of the ith iteration of Algorithm 1 based on i examples (technically the beginning of the (i+ 1)st iteration). Invoking Lemma 5, with parameters 1 and i, and Lemma 6, we have for all (g, y) \u2208 G \u00d7 Y ,\ni\u2211 j=1 Ej [Mj(g; y)]\u2212 i\u2211 j=1 Mj(g; y) \u2264 2 \u221a\u221a\u221a\u221a4\u03bdn i\u2211 j=1 Ej [Mj(g; y)] + 2\u03bdn\n\u2264 2 4\u03bdn + 1 4 i\u2211 j=1 Ej [Mj(g; y)] + 2\u03bdn = 10\u03bdn + 1\n2 i\u2211 j=1 Ej [Mj ].\nThis bound implies that\n\u2212 i\u2211\nj=1\nMj \u2264 10\u03bdn, (since Ej [Mj(g; y)] \u2265 0 by Lemma 6)\nand therefore R\u0302i+1(f\n?(\u00b7; y); y) \u2264 R\u0302i+1(g; y) + c0\u03bdn i . (14)\nSince this bound applies for all g \u2208 G, it proves the first part of the inductive claim. Next we prove that the empirical squared loss minimizer fi+1 after iteration i has small excess risk. Fix some label y. To simplify notations, we drop the dependence on y and define for any j:\ngj , fj(\u00b7; y), g? , f?(\u00b7; y), Gj , Gj(y), R\u0302j(g) , R\u0302j(g; y).\nLet Mj be defined for gi+1 and y according to Eq. (12). We first prove that since gi+1 is the empirical loss minimizer at round i for label y, it must have been in the version space Gj(y) for all j \u2208 {1, . . . , i+ 1}.\nBecause gi+1 is the loss minimizer for label y after round i, we have\ni\u2211 j=1 Mj = i\u2211 j=1 Mj(gi+1; y) \u2264 0.\nNow suppose gi+1 /\u2208 Gt+1 for some t \u2208 {0, . . . , i}. We have\nt\u2211 j=1 Mj = t ( R\u0302t+1(gi+1)\u2212 R\u0302t+1(g?) ) = t ( R\u0302t+1(gi+1)\u2212 R\u0302t+1(gt+1) + R\u0302t+1(gt+1)\u2212 R\u0302t+1(g?)\n) \u2265 \u03ba t \u2212 c0\u03bdn. (15)\nThe last inequality here follows since gi+1 /\u2208 Gt+1 so it must have R\u0302t+1(gi+1) \u2212 R\u0302t+1(gt+1) \u2265 \u03ba t/t by the elimination rule. Simultaneously, we use Eq. (14) which lower bounds the second term. Combining this inequality with the fact that \u2211i j=1Mj \u2264 0 gives\ni\u2211 j=t+1 Mj \u2264 c0\u03bdn \u2212 \u03ba t. (16)\nApplying Lemmas 5 and 6 along with the inequality \u221a 4ab \u2264 a/\u03b1+ \u03b1b for all \u03b1 > 0, gives\ni\u2211 j=t+1 Ej [Mj ]\u2212 i\u2211 j=t+1 Mj \u2264 2 \u221a\u221a\u221a\u221a4\u03bdn i\u2211 j=t+1 Ej [Mj ] + 2\u03bdn \u2264 1 2 i\u2211 j=t+1 Ej [Mj ] + 10\u03bdn. (17)\nCombining the last inequality and Eq. (16), we get\ni\u2211 j=t+1 Ej [Mj ] \u2264 20\u03bdn + 2c0\u03bdn \u2212 2\u03ba t \u2264 (40\u2212 2\u03ba) t < 0.\nThe strict inequality here is based on Fact 1 and the parameter setting \u03ba = 80. This is a contradiction since Ej [Mj ] is a quadratic form and hence non-negative by Lemma 6. The same analysis applies to every y. Therefore, we know that the empirical square loss vector regressor fi+1 is in Fj for all j \u2208 {1, . . . , i+ 1}, and hence we can apply Lemma 7 for all of these rounds, to obtain\ni ( Ex,c[c(hfi+1(x))\u2212 c(hf?(x))] ) \u2264 min\n\u03b6>0  i\u2211\nj=1\n( \u03b6P\u03b6 + 1 (\u03b6 \u2264 2\u03b7j) 2\u03b7j +\n4\u03b72j \u03b6 + 6 \u03b6 \u2211 y Ej [Mj(fi+1; y)] ) \u2264 min\n\u03b6>0 i\u03b6P\u03b6 + i\u2211\nj=1\n( 1 (\u03b6 \u2264 2\u03b7j) 2\u03b7j +\n4\u03b72j \u03b6 + 6 \u03b6 \u2211 y Ej [Mj(fi+1; y)] ) .\nWe study the four terms separately. The first one is straightforward and contributes \u03b6P\u03b6 to the instantaneous cost sensitive regret. Using our definition of \u03b7j = 1/ \u221a j the second term can be bounded as\ni\u2211 j=1 1 (\u03b6 < 2\u03b7j) 2\u03b7j = d4/\u03b62e\u2211 j=1 2\u221a j \u2264 4 \u221a d4/\u03b62e \u2264 12 \u03b6 .\nThe inequality above, \u2211n i=1 1\u221a i \u2264 2 \u221a n, is well known. For the third term, using our definition of \u03b7j gives\ni\u2211 j=1 4\u03b72j \u03b6 = 4 \u03b6 i\u2211 j=1 1 j \u2264 4 \u03b6 (1 + log(i)).\nFinally, the fourth term can be bounded using Lemma 5 (Eq. (17) with t = 0), which reveals\ni\u2211 j=1 Ej [Mj ] \u2264 2 i\u2211 j=1 Mj + 20\u03bdn\nSince for each y, \u2211i j=1Mj(fi; y) \u2264 0 for the empirical square loss minimizer (which is what we are considering now), we get\n6\n\u03b6 \u2211 y i\u2211 j=1 Ej [Mj(fi+1; y)] \u2264 120 \u03b6 K\u03bdn.\nAnd hence, we obtain\nEx,c[c(x;hfi+1(x))\u2212 c(x;hf?(x))] \u2264 min \u03b6>0\n{ \u03b6P\u03b6 + 1\n\u03b6i (4 log(i) + 16 + 120K\u03bdn) } \u2264 min\n\u03b6>0\n{ \u03b6P\u03b6 +\n140K\u03bdn \u03b6i\n} \u2264 min\n\u03b6>0\n{ \u03b6P\u03b6 +\n2\u03baK\u03bdn \u03b6i } To obtain this last bound, we observe that 1 \u2264 log(i) \u2264 \u03bdn under our assumption that \u03b4 < 1/e so the coefficient in the numerator is at most 140. The inductive claim follows by the definition of \u2206\u2032i+1. Or more precisely, if \u2206\u2032i+1 = 1 then the inductive claim is trivial and otherwise we have proved what is required."}, {"heading": "D Label complexity analysis", "text": ""}, {"heading": "D.1 Supporting Lemmata", "text": "Our label complexity analysis builds on the following lemma, which uses the sets G?i and Gi whose definitions we reproduce here.\nGi(\u2206; y) , {g | R\u0302i(g; y)\u2212 min g\u2032\u2208G R\u0302i(g \u2032; y) \u2264 \u2206}, (18)\nG?i (\u2206; y) , { g \u2223\u2223\u2223 1 i i\u2211 j=1 Qj(y)(g(xj)\u2212 f?(xj ; y))2 \u2264 \u2206 } . (19)\nThroughout we use the definitions.\n\u2206i , \u03ba i\u22121/(i\u2212 1), \u03ba , 80, c0 , 10, c1 , 25/3, c2 , 1/3, \u03b7i , 1/ \u221a i.\n\u03bdn , log\n( 2n2|G|K\n\u03b4 ) These are the constants defined in Algorithm 1 with some additional numerical constants that we use in the analysis. We also require a new definition:\nI\u03b2(i) = max{t \u2208 N|(t\u2212 1) \u2264 (c2/c1)1/\u03b2(i\u2212 1)}. (20)\nNote that I\u03b2(i) is well defined for i \u2265 1 since the right hand side is non-negative. However I\u03b2(i) could be as small as 1. We first study the I\u03b2 functional.\nFact 2. Define i\u03b2 , 2(c1/c2)1/\u03b2 + 1. Then for i \u2265 i\u03b2 , we have\nI\u03b2(i)\u2212 1 \u2265 max{(c2/c1)1/\u03b2(i\u2212 1)/2, 2}.\nProof. The proof is by direct calculation.\nI\u03b2(i)\u2212 1 = b(c2/c1)1/\u03b2(i\u2212 1)c \u2265 b(c2/c1)1/\u03b2(i\u03b2 \u2212 1)c = 2\nI\u03b2(i)\u2212 1 \u2265 (c2/c1)1/\u03b2(i\u2212 1)\u2212 1 = (c2/c1)1/\u03b2(i\u2212 1)\u2212 (c2/c1) 1/\u03b2(i\u03b2 \u2212 1) 2 \u2265 (c2/c1) 1/\u03b2(i\u2212 1) 2 .\nWe now turn to the more intricate lemmas.\nLemma 8. For any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4, for all i \u2265 1 and all y,\nG?i (c2\u2206i; y) \u2282 Gi(\u2206i; y) \u2282 Gi(4\u2206i; y) \u2282 G?i (c1\u2206i; y) \u2282 G?I\u03b2(i)(c2\u2206I\u03b2(i); y),\nwhere I\u03b2(i) is in Eq. (20).\nProof. The second containment is trivial. Recall our earlier definition that for a fixed g \u2208 G and y \u2208 Y ,\nMj , ( (g(xj)\u2212 c(xj ; y))2 \u2212 (f?(xj ; y)\u2212 c(xj ; y))2 ) Qj(y).\nLet Ec[Mj ] and Varc[Mj ] denote the expectation and variance taken with respect to the cost c at round j, conditioned on all randomness up to round j \u2212 1 and on xj . Following the same proof for Lemma 6, we have that\nEc[Mj ] = Qj(y)(g(xj)\u2212 f?(xj ; y))2, Var c [Mj ] \u2264 4Ec[Mj(g; y)].\nIt is also easy to prove a concentration result similar to Lemma 5 where Ej [Mj ] and Varj [Mj ] are replaced by Ec[Mj ] and Varc[Mj ], respectively. Thus we have for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4, the following holds for all (g, y) \u2208 G \u00d7 Y and all i, t \u2208 [n]:\u2223\u2223\u2223\u2223\u2223\u2223 i+t\u22121\u2211 j=i Ec[Mj ]\u2212 i+t\u22121\u2211 j=i Mj \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 2 \u221a\u221a\u221a\u221a4\u03bdn i+t\u22121\u2211 j=i Ec[Mj ] + 2\u03bdn, (21)\nwhere \u03bdn = log ( 2n2|G|K \u03b4 ) as in Lemma 5. This bound, via the inequality \u221a 4ab \u2264 \u03b1a+ b/\u03b1 implies\ni+t\u22121\u2211 j=i Ec[Mj ] \u2264 2 i+t\u22121\u2211 j=i Mj + 20\u03bdn (22)\ni+t\u22121\u2211 j=i Mj \u2264 3 2 i+t\u22121\u2211 j=i Ec[Mj ] + 10\u03bdn (23)\nWe start with proving the first containment. Fix some round i, some label y and some g \u2208 G?i (c2\u2206i; y). Conditioning on the above high-probability event, and starting with Eq. (23) we have\ni\u22121\u2211 j=1 Mj \u2264 3 2 \u00b7 i\u22121\u2211 j=1 Ec[Mj ] + 10\u03bdn \u2264 3 2 \u00b7 (i\u2212 1) \u00b7 c2\u2206i + 10\u03bdn\n= 3\n2 c2\u03ba i\u22121 + 10\u03bdn \u2264 (\u03ba 2 + c0 ) i\u22121.\nAbove, the second inequality is by\ni\u22121\u2211 j=1 Ec[Mj ] = i\u22121\u2211 j=1 Qj(y)(g(xj)\u2212 f?(xj ; y))2 \u2264 c2\u2206i \u00d7 (i\u2212 1)\nsince g \u2208 G?i (c2\u2206i; y), and the final inequality uses \u03bdn \u2264 i\u22121 (Fact 1) and our choices of \u03ba, c0 and c2. Using the above bound and with gi = argming\u2208G R\u0302i(g; y), we have\n(i\u2212 1) \u00b7 ( R\u0302i(g; y)\u2212 R\u0302i(gi; y) ) = i\u22121\u2211 j=1 Mj + (i\u2212 1) ( R\u0302i(f ?; y)\u2212 R\u0302i(gi; y) )\n\u2264 (\u03ba/2 + c0) i\u22121 + c0\u03bdn \u2264 \u03ba i\u22121,\nwhere the first inequality is by the above upper bound on \u2211i\u22121 j=1Mj and Eq. (14), which upper bounds the excess empirical square loss of f?. Thus, g \u2208 Gi(\u2206i; y) \u2282 Gi(4\u2206i; y). To prove the third containment, we fix some i, y, and g \u2208 Gi(4\u2206i; y). Starting from (22) we have\ni\u22121\u2211 j=1 Ec[Mj ] \u2264 2 i\u22121\u2211 j=1 Mj + 20\u03bdn\n= 2(i\u2212 1) \u00b7 (R\u0302i(g; y)\u2212 R\u0302i(f?; y)) + 20\u03bdn \u2264 2(i\u2212 1) \u00b7 (R\u0302i(g; y)\u2212 R\u0302i(gi; y)) + 20\u03bdn \u2264 8\u03ba i\u22121 + 20\u03bdn \u2264 c1\u03ba i\u22121,\nwhere the second inequality is by the fact that gi is the minimizer of the squared loss at round i for label y, the third inequality is by g \u2208 Gi(4\u2206i; y), and the last inequality is by \u03bdn \u2264 i\u22121 (Fact 1) and our choices of c1 and \u03ba. Thus, g \u2208 G?i (c1\u2206i; y).\nFor the final containment, observe that\n(i\u2212 1)c1\u2206i = c1\u03ba i\u22121 = c1\u03ba\n(( n\ni\u2212 1\n)\u03b2 \u03bdn ) = c2\u03ba [(c1 c2 )1/\u03b2 n i\u2212 1 ]\u03b2 \u03bdn  Using the definition of I\u03b2(i) in Eq. (20), we get that (i \u2212 1)c1\u2206i \u2264 (I\u03b2(i) \u2212 1)c2\u2206I\u03b2(i). Of course we always have I\u03b2(i)\u2212 1 \u2264 i\u2212 1 since c2 \u2264 c1. Hence, I\u03b2(i)\u22121\u2211 j=1 EjQj(y)(g(xj)\u2212 f?(xj ; y))2 \u2264 i\u22121\u2211 j=1 EjQj(y)(g(xj)\u2212 f?(xj ; y))2 \u2264 (i\u2212 1)c1\u2206i \u2264 (I\u03b2(i)\u2212 1)c2\u2206I\u03b2(i).\nThus we get that G?i (c1\u2206i) \u2282 G?I\u03b2(i)(c2\u2206I\u03b2(i)).\nBefore bounding the label complexity, we first prove the following regret bound:\nLemma 9. For any \u03b4 \u2264 1/e, with probability at least 1 \u2212 \u03b4, for all i \u2265 1 and for all vector regressors f \u2208 F?i (c2\u2206i) , \u220f y G?i (c2\u2206i; y),\nEx,c [c(x, hf (x))\u2212 c(x, hf?(x))] \u2264 min \u03b6>0\n{ \u03b6P\u03b6 +\n14K\u2206i \u03b6\n} .\nNote that this cost-sensitive regret bound is polynomially worse than the one in Theorem 3 that we prove just for the empirical risk minimizer fi. This is because we set the confidence radius \u2206i using a polynomial function of n/i, which will be important for our label complexity analysis.\nProof. The proof follows a similar argument to that of Lemma 7 in that we must argue that each g \u2208 G?i (c2\u2206i; y) is involved in driving the query rule for a large fraction of the rounds. First observe that f? \u2208 F?i (c2\u2206i) for i \u2265 1 by the definition of F?i .\nNext, fix a label y and a function g \u2208 G?i+1(c2\u2206i+1; y) for i \u2265 0. We prove that g \u2208 Gt+1(\u2206t) for all t \u2208 {0, . . . , i}. In search of a contradiction, suppose that g /\u2208 Gt+1(\u2206t+1) for some t \u2208 {0, . . . , i}. First, since g \u2208 G?i+1(c2\u2206i+1; y), using the Freedman-style deviation bound in Eq. (23), we have\ni\u2211 j=1 Mj \u2264 3 2 i\u2211 j=1 Ec[Mj ] + 10\u03bdn \u2264 ( 3 2 c2\u03ba+ c0 ) i.\nHere we also use the definition of \u2206i+1 = \u03ba i/i, c0 = 10, and Fact 1. At the same time, since g /\u2208 Gt+1(\u2206t+1; y), we know that\n\u2206t+1 < R\u0302t+1(g)\u2212 R\u0302t+1(gt+1) < R\u0302t+1(g)\u2212 R\u0302t+1(g?) + c0\u03bdn t .\nThe last inequality uses Eq. (14). Together with the above, this implies that\ni\u2211 j=t+1 Mj \u2264 ( 3 2 c2\u03ba+ c0 ) i \u2212 \u03ba t + c0\u03bdn.\nNow, since i \u2265 t and \u03b2 \u2208 (0, 1), we get that i < t. Now, using Eq. (22) as before, we get\ni\u2211 j=t+1 Ec[Mj ] \u2264 2 i\u2211 j=t+1 Mj + 20\u03bdn \u2264 2 ( 3 2 c2\u03ba+ c0 ) i \u2212 2\u03ba t + 4c0\u03bdn \u2264 (\u2212\u03ba+ 6c0) t < 0.\nThe last non-strict inequality follows from the fact that t \u2265 i \u2265 \u03bdn since i \u2265 t, and then the strict inequality is by our choices for the constants. This is a contradiction since the left hand side is a quadratic form and so, g \u2208 Gt+1(\u2206t+1) for all t \u2208 {0, . . . , i}.\nThis argument applies for all y, and hence, for these rounds we may apply Lemma 7, so that for all regressors f \u2208 F?i (c2\u2206i+1),\ni \u00b7 (Ex,c[c(x, hf (x))\u2212 c(x;hf?(x))]) \u2264 min \u03b6>0 i\u03b6P\u03b6 + i\u2211\nj=1\n( 1 (\u03b6 \u2264 2\u03b7j) 2\u03b7j +\n4\u03b72j \u03b6 + 6 \u03b6 \u2211 y Ej [Mj(f ; y)] ) \u2264 min\n\u03b6>0 i\u03b6P\u03b6 + 16 + 4 log(i)\u03b6 + 6\u03b6\u2211 y i\u2211 j=1 Ej [Mj(f ; y)]  . The last inequality here uses identical bounds as the proof of Theorem 3.\nIn a similar way to (17), we use Lemma 5 to obtain\ni\u2211 j=1 Ej [Mj(f ; y)] \u2264 2 i\u2211 j=1 Mj(f ; y) + 20\u03bdn = 2i \u00b7 ( R\u0302i+1(f ; y)\u2212 R\u0302i+1(f?; y) ) + 20\u03bdn\n\u2264 2i \u00b7 ( R\u0302i+1(f ; y)\u2212 R\u0302i+1(fi+1; y) ) + 20\u03bdn\n\u2264 (2\u03ba+ 20) i\nThe last bound uses the definition of \u2206i+1 and Fact 1, along with the fact that G?i+1(c2\u2206i+1; y) \u2282 Gi+1(\u2206i+1; y) so we know the empirical risk to fi+1 is controlled. Finally, we collect the latter three terms and collect the constant 6(2\u03ba+ 20) + 20 (which requires \u03b4 < 1/e). This gives,\nEx,c [c(x, hf (x))\u2212 c(x, hf?(x))] \u2264 min \u03b6>0\n{ \u03b6P\u03b6 +\n14\u03baK i i\u03b6\n} .\nThis proves the statement since we are considering f \u2208 F?i+1(c2\u2206i+1) and \u03ba i/i = \u2206i+1.\nFor the rest of the analysis, it will be convenient to introduce the shorthand \u03b3\u0302(xi, y) = c\u0302+(xi, y) \u2212 c\u0302\u2212(xi, y), where c\u0302+(xi, y) and c\u0302\u2212(xi, y) are the approximate maximum and minimum costs computed in Algorithm 1 at round i.\nLemma 10 (Cost Range Translation). Fix i and suppose that the conclusions of Lemmas 8 and 9 hold. Then for any x, y pair, we have\n\u03b3\u0302(xi, y) \u2264 \u03b3(xi, y,Fcsr(rI\u03b2(i))) + \u03b7i/2,\nwhere ri = min\u03b6>0 { \u03b6P\u03b6 + 14K\u2206i \u03b6 } and I\u03b2(i) is in Eq. (20).\nProof. We have\n\u03b3\u0302(xi, y) \u2264 \u03b3(xi, y,G?i (c1\u2206i; y)) + \u03b7i 2\n(By Theorem 1, setting of in Algorithm 1 and Lemma 8)\n\u2264 \u03b3(xi, y,Fcsr(rI\u03b2(i)) + \u03b7i 2\n(By Lemmas 8 and 9)\nLemma 11. Fix i and suppose that the conclusions of Lemmas 8 and 9 hold. Define y?i = argminy f?(xi; y), y\u0304i = argminy c\u0302+(xi,Gi(y)), y\u0303i = argminy 6=y?i c\u0302\u2212(xi,Gi(y)). Then for y 6= y ? i , we have\ny \u2208 Yi \u21d2 f?(xi; y)\u2212 f?(xi; y?i )\u2212 \u03b7i 2 \u2264 (\u03b3(xi, y) + \u03b3(xi, y?i )) ,\nand for y?i :\n|Yi| > 1 \u2227 y?i \u2208 Yi \u21d2 f?(xi; y\u0303i)\u2212 f?(xi; y?i )\u2212 \u03b7i 2 \u2264 (\u03b3(xi, y\u0303i) + \u03b3(xi, y?i )) ."}, {"heading": "In both bounds, all the cost ranges are computed using Fcsr(rI\u03b2(i)).", "text": "Proof. Suppose y 6= y?i y \u2208 Yi \u21d2 c\u0302\u2212(xi,Gi(y)) \u2264 c\u0302+(xi,Gi(y\u0304i))\n\u21d2 c\u0302\u2212(xi,Gi(y)) \u2264 c\u0302+(xi,Gi(y?i )) \u21d2 c\u2212(xi,G?i (c1\u2206i; y)) \u2264 c+(xi,G?i (c1\u2206i; y?i )) + \u03b7i 2 \u21d2 f?(xi; y)\u2212 \u03b3(xi,G?i (c1\u2206i; y)) \u2264 f?(xi; y?i ) + \u03b3(xi,G?i (c1\u2206i; y?i ))) + \u03b7i 2 \u21d2 f?(xi; y)\u2212 f?(xi; y?i )\u2212 \u03b7i 2 \u2264 \u03b3(xi,G?i (c1\u2206i; y)) + \u03b3(xi,G?i (c1\u2206i; y?i ))\n\u21d2 f?(xi; y)\u2212 f?(xi; y?i )\u2212 \u03b7i 2 \u2264 ( \u03b3(xi, y,Fcsr(rI\u03b2(i))) + \u03b3(xi, y ? i ,Fcsr(rI\u03b2(i))) ) .\nFor y?i we need to consider two cases. First assume y ? i = y\u0304i. Then\n|Yi| > 1 \u2227 y?i \u2208 Yi \u2227 y?i = y\u0304i \u21d2 c\u0302\u2212(xi,Gi(y\u0303i)) \u2264 c\u0302+(xi,Gi(y?i ))\n\u21d2 f?(xi, y\u0303i)\u2212 f?(xi, y?i )\u2212 \u03b7i 2 \u2264 \u03b3(xi, y\u0303i) + \u03b3(xi, y?i ).\nThis is true since if |Yi| > 1 then it must be the case that y\u0303i is confused, since it has the minimal lower cost estimate. On the other hand if y?i 6= y\u0304i then\n|Yi| > 1 \u2227 y?i \u2208 Yi \u2227 y?i 6= y\u0304i \u21d2 c\u0302+(xi,Gi(y\u0304i)) \u2264 c\u0302+(xi,Gi(y?i )) \u21d2 c\u0302\u2212(xi,Gi(y\u0303i)) \u2264 c\u0302+(xi,Gi(y?i ))\n\u21d2 f?(xi, y\u0303i)\u2212 f?(xi, y?i )\u2212 \u03b7i 2 \u2264 \u03b3(xi, y\u0303i) + \u03b3(xi, y?i ).\nThe second step here is because the search for y\u0303i includes y\u0304i, since the latter is not y?i . Thus we obtain\n|Yi| > 1 \u2227 y?i \u2208 Yi \u21d2 c\u0302\u2212(xi,Gi(y\u0303i)) \u2264 c\u0302+(xi,Gi(y?i ))\n\u21d2 f?(xi; y\u0303i)\u2212 f?(xi; y?i )\u2212 \u03b7i 2 \u2264 (\u03b3(xi, y\u0303i) + \u03b3(xi, y?i )) ,\nas desired."}, {"heading": "D.2 Low Noise (Massart) Case (Theorem 6)", "text": "Fix some round i. Let Fi be the set of vector regressors used at round i of COAL and let Gi(y) be the corresponding regressors for label y. Let y\u0304i , argminy c\u0302+(xi,Gi(y)), y?i = argminy f?(xi; y), and y\u0303i , argminy 6=y?i c\u0302\u2212(xi,Gi(y)). Assume Lemmas 8 and 9 hold. The label complexity L2 for round i is\u2211\ny Qi(y) = \u2211 y 1 (|Yi| > 1 \u2227 y \u2208 Yi)1 (\u03b3\u0302(xi, y,Fi) > \u03b7i) = \u2211 y 1 (|Yi| > 1 \u2227 y \u2208 Yi)Qi(y).\nWe need to do two things with Qi(y), so we have duplicated it here. First, observe that y \u2208 Yi implies that there exists a vector regressor f \u2208 Fi such that hf (xi) = y. This follows since the domination condition means that there exists g \u2208 Gi(y) such that g(xi) \u2264 miny\u2032 6=y maxg\u2032\u2208Gi(y\u2032) g\u2032(xi). Since we are using a factored representation, we can take f to use g on the yth coordinate and use the maximizers for all the other coordinates. Moreover, |Yi| > 1 implies there exists a regressor that does not predict y. Of course, through Lemmas 8 and 9, we know that Fi \u2282 Fcsr(rI\u03b2(i)), and so we get the bound:\n1 (|Yi| > 1 \u2227 y \u2208 Yi) \u2264 1 ( \u2203f, f \u2032 \u2208 Fcsr(rI\u03b2(i)) | hf (xi) = y \u2227 hf \u2032(xi) 6= y ) .\nFor y 6= y?i , we take f \u2032 to be f? which is always in the cost-sensitive regret ball. For y?i , we take f \u2032 to be any regressor such that hf \u2032(xi) = y\u0303i, which must exist in the ball if |Yi| > 1. We will use these as an upper bound on Qi(y) momentarily.\nSecondly, we apply Lemma 11 along with the Massart noise assumption. For y 6= y?i 1 (|Yi| > 1 \u2227 y \u2208 Yi) \u2264 1 ( f?(xi; y)\u2212 f?(xi; y?i )\u2212\n\u03b7i 2 \u2264 \u03b3(xi, y) + \u03b3(xi, y?i ) ) \u2264 1 ( \u03c4 \u2212 \u03b7i\n2 \u2264 \u03b3(xi, y) + \u03b3(xi, y?i )\n) .\nRecall that we use the convention that all quantities without an explicit regressor ball use Fcsr(rI\u03b2(i)). For y?i we obtain the same inequality but using y\u0303i via Lemma 11. Together this gives the bound:\nL2 \u2264 \u2211 y 6=y?i 1 (\u03c4 \u2212 \u03b7i/2 \u2264 \u03b3(xi, y) + \u03b3(xi, y?i ))\u00d7Qi(y) + 1 (\u03c4 \u2212 \u03b7i/2 \u2264 \u03b3(xi, y\u0303i) + \u03b3(xi, y?i ))\u00d7Qi(y?i )\nLet us focus on just one of these terms (say where y 6= y?i ) and consider any round i where \u03c4 \u2265 2\u03b7i.\n1 (\u03c4 \u2212 \u03b7i/2 \u2264 \u03b3(xi, y) + \u03b3(xi, y?i ))Qi(y) \u2264 1 (\u03c4/2 \u2264 \u03b3(xi, y) + \u03b3(xi, y?i ))Qi(y) \u2264 1 (\u03c4/4 \u2264 \u03b3(xi, y))Qi(y) + 1 (\u03c4/4 \u2264 \u03b3(xi, y?i ))Qi(y)\nUsing the upper bound on Qi(y), the first term here is clearly bounded by 1 (\u03c4/4 \u2264 \u03b3(xi, y))1 ( \u2203f, f \u2032 \u2208 Fcsr(rI\u03b2(i)) | hf (xi) = y \u2227 hf \u2032(xi) 6= y ) , Di(y).\nFortunately, the second term is bounded in the same way, since we know that hf? \u2208 Fcsr(rI\u03b2(i)), the fact that some f with hf (xi) = y 6= y?i exists implies that the second term is at most Di(y?i ).\nThe last term, which involves Qi(y?i ) is bounded in essentially the same way, since we know that when |Yi| > 1 (which is all we are considering), we know there exists two functions f, f \u2032 \u2208 Fi such that hf (xi) = y\u0303i and hf \u2032(xi) = y?i . Thus we can bound the label complexity at round i by\nDi(y\u0303i) +Di(y ? i ) + \u2211 y 6=y?i (Di(y) +Di(y ? i )) \u2264 KDi(y?i ) + 2 \u2211 y Di(y).\nFor the rounds i where \u03c4 < 2\u03b7i we simply upper bound the label complexity by K. The last step in the proof is to apply Freedman\u2019s inequality to the sequence of indicators. The conditional mean of each term is at most (for rounds i where \u03c4 > 2\u03b7i),\nEi [ KDi(y ? i ) + 2 \u2211 y Di(y) ] \u2264 4rI\u03b2(i) \u03c4 [K\u03b81 + 2\u03b82] .\nThe part involving \u03b82 is straightforward and the pre-multiplier follows since we are measuring the probability of querying with a cost range parameter of \u03c4/4 and over a cost-sensitive regret ball of radius rI\u03b2(i) in Di(y). To obtain \u03b81 we use the fact that if Di(y?i ) = 1, then certainly there exists some confused label, namely y ? i , and hence the indicator in \u03b81 is also 1. The range is 3K since Di(y) \u2208 {0, 1} and since the terms are non-negative, the variance is at most the range times the mean. In such cases, Freedman\u2019s inequality gives\nX \u2264 EX + 2 \u221a REX log(1/\u03b4) + 2R log(1/\u03b4) \u2264 2EX + 3R log(1/\u03b4),\nwith probability at least 1\u2212 \u03b4 where X is the non-negative random variable with range R and expectation EX . The last step is by the fact that 2 \u221a ab \u2264 a+ b.\nIn our case, we get that with probability at least 1\u2212 \u03b4, n\u2211\ni=i?\nKDi(y ? i ) + 2 \u2211 y Di(y) \u2264 n\u2211 i=i? 8rI\u03b2(i) \u03c4 [K\u03b81 + 2\u03b82] + 9K log(1/\u03b4).\nHere we only consider rounds i \u2265 i? where i? is the smallest index such that \u03c4 < 2\u03b7i? and i? \u2265 i\u03b2 (Recall Fact 2). For the first i? rounds, we will upper bound the per-round label complexity by K, so that the overall label complexity is at most\nKi? + n\u2211 i=i? 8rI\u03b2(i) \u03c4 [K\u03b81 + 2\u03b82] + 9K log(1/\u03b4)\n\u2264 K n\u2211 i=1 1 (\u03c4 \u2264 2\u03b7i) +Ki\u03b2 + + n\u2211\ni=i\u03b2\n8rI\u03b2(i)\n\u03c4 [K\u03b81 + 2\u03b82] + 9K log(1/\u03b4)\nUsing our choice of \u03b7i = 1/ \u221a i, the first term is at most Kd4/\u03c42e. The second term is bounded by Fact 2. The last step is to use the definition of rI\u03b2(i) to simplify the sum. Since we are in the Massart noise case, we will set \u03b6 = \u03c4 in the definition of ri in Lemma 10. Since P\u03c4 = 0 by the definition of the noise condition, this yields ri = 14K\u2206i/\u03c4 . Substituting this choice, along with our definition of \u2206i yields\nn\u2211 i=i\u03b2 rI\u03b2(i) = 14\u03ban\u03b2K\u03bdn \u03c4 n\u2211 i=i\u03b2 (I\u03b2(i)\u2212 1)\u22121\u2212\u03b2\n\u2264 14\u03ban \u03b2K\u03bdn \u03c4 \u00d7 2(1+\u03b2) \u00d7 (c1 c2 ) 1+\u03b2 \u03b2 n\u2211 i=i\u03b2 (i\u2212 1)\u22121\u2212\u03b2 \n\u2264 56(c1/c2)\u03ban \u03b2K\u03bdn\n\u03c4\n[( c1 c2 ) 1 \u03b2 n\u2211 i=2 (i\u2212 1)\u22121 ]\n\u2264 56(c1/c2)\u03ban \u03b2K\u03bdn\n\u03c4\n( c1 c2 ) 1 \u03b2 (2\u00d7 log(n)) .\nIncluding the extra O(K) term, the overall bound is\nK ( d 4 \u03c42 e+ 2(c1/c2)1/\u03b2 + 1 ) + 8\u00d7 56\u00d7 25\u00d7 2\u03ban\u03b2K\u03bdn \u03c42 ( c1 c2 ) 1 \u03b2 log(n)[K\u03b81 + 2\u03b82] + 9K log(1/\u03b4)\n\u2264 a0251/\u03b2 ( n\u03b2K log(n)\u03bdn\n\u03c42 [K\u03b81 + 2\u03b82] +\nK log(1/\u03b4)\n\u03c42\n) ,\nwhere a0 is a universal constant. For L1 we can use a very similar argument. First,\nL1 = \u2211 i 1 (|Yi| > 1 \u2227 \u2203y \u2208 Yi, \u03b3\u0302(xi, y,Fi) > \u03b7i) \u2264 \u2211 i 1 ( |Yi| > 1 \u2227 \u2203y \u2208 Yi, \u03b3(xi, y,Fcsr(rI\u03b2(i))) > \u03b7i/2 ) .\nThis inequality is an application of Lemma 10. Now as above, we know that,\n|Yi| > 1 \u2227 y \u2208 Yi \u21d2 \u2203f, f \u2032 \u2208 Fcsr(rI\u03b2(i)), hf (xi) = y \u2227 hf \u2032(xi) 6= y,\nsince if y \u2208 Yi then some classifier must select it, and since |Yi| > 1, something else must also be selected. We also know that we can always take f \u2032 to be f? when y 6= y?i . For y?i we can always take the classifier to be the one that predicts y\u0303i.\nMoreover we also have that when \u03c4 \u2265 2\u03b7i,\n|Yi| > 1 \u2227 y \u2208 Yi \u21d2 f?(xi; y)\u2212 f?(xi; y?i )\u2212 \u03b7i/2 \u2264 \u03b3(xi, y) + \u03b3(xi, y?i ) \u21d2 \u03c4/4 \u2264 \u03b3(xi, y) \u2228 \u03c4/4 \u2264 \u03b3(xi, y?i )\nThus, putting things together, and considering only rounds where \u03c4 \u2265 2\u03b7i we get\nL1 \u2264 n\u2211 i=1 1 (\u03c4 < 2\u03b7i) + n\u2211 i=1 1 ( \u2203y | \u2203f, f \u2032 \u2208 Fcsr(rI\u03b2(i)), hf (xi) = y \u2227 hf \u2032(xi) 6= y \u2227 \u03b3(x, y) \u2265 \u03c4/4 ) .\nHere we seemingly dropped the \u03b3(x; y?i ) \u2265 \u03c4/4 term from consideration since y?i is always in Yi and hence the term gets included in the existential quantifier when the chosen label y = y?i . Now we may apply Freedman\u2019s inequality to upper bound L1 by\nL1 \u2264 i\u03b2 + d4/\u03c42e+ 2 n\u2211\ni=i\u03b2\n4rI\u03b2(i)\n\u03c4 \u03b81 + 2 log(1/\u03b4) \u2264 a0251/\u03b2\n( n\u03b2K log(n)\u03bdn\n\u03c42 \u03b81 +\nlog(1/\u03b4)\n\u03c42\n) ,\nwhere a0 is a universal constant."}, {"heading": "D.3 High noise case (Theorem 5)", "text": "Fix some round i. Let Fi be the set of vector regressors used at round i of COAL and let Gi(y) be the corresponding regressors for label y. Let y\u0304i , argminy c\u0302+(xi,Gi(y)), y?i = argminy f?(xi; y), and y\u0303i , argminy 6=y?i c\u0302\u2212(xi,Gi(y)). Assume Lemmas 8 and 9 hold. The label complexity L2 for round i is\u2211\ny Qi(y) = \u2211 y 1 (|Yi| > 1 \u2227 y \u2208 Yi)1 (\u03b3\u0302(xi, y,Fi) > \u03b7i)\nFirst we apply Lemma 10 on the latter indicator to get\u2211 y 1 (|Yi| > 1 \u2227 y \u2208 Yi)1 ( \u03b3(xi, y,Fcsr(rI\u03b2(i))) \u2265 \u03b7i/2 ) .\nFor the former indicator, observe that y \u2208 Yi implies that there exists a vector regressor f \u2208 Fi such that hf (xi) = y. This follows since the domination condition means that there exists g \u2208 Gi(y) such that g(xi) \u2264 miny\u2032 maxg\u2032\u2208Gi(y\u2032) g\u2032(xi). Since we are using a factored representation, we can take f to use g on the yth coordinate and use the maximizers for all the other coordinates.\nSince y \u2208 Yi implies there exists f \u2208 Fi such that hf (xi) = y, and by Lemmas 8 and 9, we get that f \u2208 Fcsr(rI\u03b2(i)). Similarly there exists f \u2032 \u2208 Fi such that hf \u2032(xi) 6= y. Thus we can bound the the label complexity for round i as,\u2211\ny\n1 ( \u2203f, f \u2032 \u2208 Fcsr(rI\u03b2(i)) | hf (xi) = y 6= hf \u2032(xi) ) 1 ( \u03b3(xi, y,Fcsr(rI\u03b2(i))) \u2265 \u03b7i/2 ) = \u2211 y 1 ( x \u2208 DIS(rI\u03b2(i), y) \u2227 \u03b3(xi, y,Fcsr(rI\u03b2(i))) \u2265 \u03b7i/2 ) .\nNow we can apply Freedman\u2019s inequality on the sequence here to find that with probability at least 1\u2212 \u03b4,\nL2 \u2264 Ki\u03b2 + n\u2211\ni=i\u03b2\n4rI\u03b2(i)\n\u03b7i \u03b82 + 3K log(1/\u03b4)\nAgain i\u03b2 = 2(c1/c2)1/\u03b2 + 1 is from Fact 2. We just need to bound upper bound the sequence\nn\u2211 i=i\u03b2 rI\u03b2(i) \u03b7i = 2 n\u2211 i=i\u03b2 \u221a i\n\u221a 14K\u03ban\u03b2\u03bdn\n(I\u03b2(i)\u2212 1)1+\u03b2\n\u2264 2 \u221a 14K\u03ban\u03b2\u03bdn \u00d7 n\u2211\ni=i\u03b2\n\u221a 21+\u03b2i\n(c2/c1) 1+\u03b2 \u03b2 (i\u2212 1)1+\u03b2\n\u2264 2 \u221a 14K\u03ban\u03b2\u03bdn \u00d7 n\u2211\ni=i\u03b2\n\u221a 22+\u03b2\n(c2/c1) 1+\u03b2 \u03b2 (i\u2212 1)\u03b2\n\u2264 \u221a 448(c1/c2) 1+\u03b2 \u03b2 K\u03ban\u03b2\u03bdn \u00d7 n\u22121\u2211 i=1 i\u2212\u03b2/2\n\u2264 2 \u221a 448(c1/c2) 1+\u03b2 \u03b2 K\u03ban\u03b2\u03bdn \u00d7 n1\u2212\u03b2/2\n\u2264 2 \u221a 448(c1/c2) 1+\u03b2 \u03b2 K\u03ba\u03bdn \u00d7 n.\nThe first line follows by the definition of \u03b7i and by optimizing the bound in Lemma 9 using the definition of \u2206i. The second line uses Fact 2. The remaining steps are simple calculations using \u03b2 \u2208 (0, 1) and an integral bound.\nThus in total we get a label complexity of L2 \u2264 a0(25)1/\u03b2 ( n\u03b82 \u221a K\u03bdn +K log(1/\u03b4) ) .\nSimilarly for L1 we can derive the bound\nL1 \u2264 \u2211 i 1 ( \u2203y | \u03b3(xi, y,Fcsr(rI\u03b2(i))) \u2265 \u03b7i/2 \u2227 x \u2208 DIS(rI\u03b2(i), y) ) .\nand then apply Freedman\u2019s inequality to this sequence to obtain that with probability at least 1\u2212 \u03b4\nL1 \u2264 i\u03b2 + 2 n\u2211\ni=i\u03b2\n2rI\u03b2 \u03b7i\n\u03b81 + 3 log(1/\u03b4) \u2264 a0(25)1/\u03b2 ( n\u03b81 \u221a K\u03bdn + log(1/\u03b4) ) ."}], "references": [{"title": "Selective sampling algorithms for cost-sensitive multiclass prediction", "author": ["A. Agarwal"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["A. Agarwal", "D. Hsu", "S. Kale", "J. Langford", "L. Li", "R.E. Schapire"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Active and passive learning of linear separators under log-concave distributions", "author": ["M.-F. Balcan", "P. Long"], "venue": "In Conference on Learning Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Margin based active learning", "author": ["M.-F. Balcan", "A. Broder", "T. Zhang"], "venue": "In Conference on Learning Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Importance weighted active learning", "author": ["A. Beygelzimer", "S. Dasgupta", "J. Langford"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Agnostic active learning without constraints", "author": ["A. Beygelzimer", "D. Hsu", "J. Langford", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["A. Beygelzimer", "J. Langford", "L. Li", "L. Reyzin", "R.E. Schapire"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Minimax bounds for active learning", "author": ["R.M. Castro", "R.D. Nowak"], "venue": "Transaction on Information Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Learning noisy linear classifiers via adaptive and selective sampling", "author": ["G. Cavallanti", "N. Cesa-Bianchi", "C. Gentile"], "venue": "Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Learning to search better than your teacher", "author": ["K.-W. Chang", "A. Krishnamurthy", "A. Agarwal", "H. Daum\u00e9 III", "J. Langford"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "D. Hsu", "C. Monteleoni"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Search-based structured prediction", "author": ["H. Daum\u00e9 III", "J. Langford", "D. Marcu"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Robust selective sampling from single and multiple teachers", "author": ["O. Dekel", "C. Gentile", "K. Sridharan"], "venue": "In Conference on Learning Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "In Conference on Learning Theory,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Theory of disagreement-based active learning", "author": ["S. Hanneke"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Efficient and parsimonious agnostic active learning", "author": ["T.-K. Huang", "A. Agarwal", "D.J. Hsu", "J. Langford", "R.E. Schapire"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Online importance weight aware updates", "author": ["N. Karampatziakis", "J. Langford"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "In Foundations of Computer Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1989}, {"title": "Risk bounds for statistical learning", "author": ["P. Massart", "\u00c9. N\u00e9d\u00e9lec"], "venue": "The Annals of Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Better algorithms for selective sampling", "author": ["F. Orabona", "N. Cesa-Bianchi"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Reinforcement and imitation learning via interactive no-regret learning", "author": ["S. Ross", "J.A. Bagnell"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Normalized online learning", "author": ["S. Ross", "P. Mineiro", "J. Langford"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Active learning", "author": ["B. Settles"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Learning where to sample in structured prediction", "author": ["T. Shi", "J. Steinhardt", "P. Liang"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "A survey of hierarchical classification across different application domains", "author": ["C.N. Silla Jr.", "A.A. Freitas"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Efficient algorithms for adversarial contextual learning", "author": ["V. Syrgkanis", "A. Krishnamurthy", "R.E. Schapire"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S.E. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Beyond disagreement-based agnostic active learning", "author": ["C. Zhang", "K. Chaudhuri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "This computation, and COAL as a whole, only requires that the regression set admits efficient squared loss optimization, in contrast with prior algorithms that require 0/1 loss optimization [6, 16].", "startOffset": 190, "endOffset": 197}, {"referenceID": 15, "context": "This computation, and COAL as a whole, only requires that the regression set admits efficient squared loss optimization, in contrast with prior algorithms that require 0/1 loss optimization [6, 16].", "startOffset": 190, "endOffset": 197}, {"referenceID": 26, "context": "For example, CSMC can naturally express partial failure in hierarchical classification [27].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "CSMC also forms the basis of learning to avoid cascading failures in joint prediction tasks [13, 23, 11] like structured prediction and reinforcement learning.", "startOffset": 92, "endOffset": 104}, {"referenceID": 22, "context": "CSMC also forms the basis of learning to avoid cascading failures in joint prediction tasks [13, 23, 11] like structured prediction and reinforcement learning.", "startOffset": 92, "endOffset": 104}, {"referenceID": 10, "context": "CSMC also forms the basis of learning to avoid cascading failures in joint prediction tasks [13, 23, 11] like structured prediction and reinforcement learning.", "startOffset": 92, "endOffset": 104}, {"referenceID": 10, "context": "As our second application, we consider learning to search algorithms for joint (or structured) prediction [11], which operate by a reduction to CSMC.", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "We show that using COAL within the AGGRAVATE algorithm [23, 11] reduces the number of roll-outs by a factor of 14 to 3 4 on several joint prediction tasks.", "startOffset": 55, "endOffset": 63}, {"referenceID": 10, "context": "We show that using COAL within the AGGRAVATE algorithm [23, 11] reduces the number of roll-outs by a factor of 14 to 3 4 on several joint prediction tasks.", "startOffset": 55, "endOffset": 63}, {"referenceID": 24, "context": "We recommend the survey of Settles [25] for an overview of more empirical research.", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "Castro and Nowak [9] study active learning for binary classification with non-parametric decision sets, while Balcan et al.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "[5], Balcan and Long [3] focus on linear representations under distributional assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[5], Balcan and Long [3] focus on linear representations under distributional assumptions.", "startOffset": 21, "endOffset": 24}, {"referenceID": 9, "context": "Additionally, the selective sampling framework from the online learning community derives regret and label complexity bounds for stream-based active learning of linear separators under adversarial assumptions [10, 14, 22, 1].", "startOffset": 209, "endOffset": 224}, {"referenceID": 13, "context": "Additionally, the selective sampling framework from the online learning community derives regret and label complexity bounds for stream-based active learning of linear separators under adversarial assumptions [10, 14, 22, 1].", "startOffset": 209, "endOffset": 224}, {"referenceID": 21, "context": "Additionally, the selective sampling framework from the online learning community derives regret and label complexity bounds for stream-based active learning of linear separators under adversarial assumptions [10, 14, 22, 1].", "startOffset": 209, "endOffset": 224}, {"referenceID": 0, "context": "Additionally, the selective sampling framework from the online learning community derives regret and label complexity bounds for stream-based active learning of linear separators under adversarial assumptions [10, 14, 22, 1].", "startOffset": 209, "endOffset": 224}, {"referenceID": 15, "context": "Our work falls into the framework of disagreement-based active learning, which studies general hypothesis spaces typically in an agnostic setup (see Hanneke [16] for an excellent survey).", "startOffset": 157, "endOffset": 161}, {"referenceID": 3, "context": "In contrast, prior work either explicitly enumerates the version space [4, 30] or uses a 0/1 loss classification oracle for the search [12, 6, 7, 17].", "startOffset": 71, "endOffset": 78}, {"referenceID": 29, "context": "In contrast, prior work either explicitly enumerates the version space [4, 30] or uses a 0/1 loss classification oracle for the search [12, 6, 7, 17].", "startOffset": 71, "endOffset": 78}, {"referenceID": 11, "context": "In contrast, prior work either explicitly enumerates the version space [4, 30] or uses a 0/1 loss classification oracle for the search [12, 6, 7, 17].", "startOffset": 135, "endOffset": 149}, {"referenceID": 5, "context": "In contrast, prior work either explicitly enumerates the version space [4, 30] or uses a 0/1 loss classification oracle for the search [12, 6, 7, 17].", "startOffset": 135, "endOffset": 149}, {"referenceID": 6, "context": "In contrast, prior work either explicitly enumerates the version space [4, 30] or uses a 0/1 loss classification oracle for the search [12, 6, 7, 17].", "startOffset": 135, "endOffset": 149}, {"referenceID": 16, "context": "In contrast, prior work either explicitly enumerates the version space [4, 30] or uses a 0/1 loss classification oracle for the search [12, 6, 7, 17].", "startOffset": 135, "endOffset": 149}, {"referenceID": 1, "context": "Supervised learning oracles that solve NP-hard optimization problems in the worst case have been used in other problems including contextual bandits [2, 28] and structured prediction [13].", "startOffset": 149, "endOffset": 156}, {"referenceID": 27, "context": "Supervised learning oracles that solve NP-hard optimization problems in the worst case have been used in other problems including contextual bandits [2, 28] and structured prediction [13].", "startOffset": 149, "endOffset": 156}, {"referenceID": 12, "context": "Supervised learning oracles that solve NP-hard optimization problems in the worst case have been used in other problems including contextual bandits [2, 28] and structured prediction [13].", "startOffset": 183, "endOffset": 187}, {"referenceID": 0, "context": ",K}, and a distribution D supported on X \u00d7 [0, 1] .", "startOffset": 43, "endOffset": 49}, {"referenceID": 0, "context": "Let G , {g : X 7\u2192 [0, 1]} denote a set of base regressors and let F , G denote a set of vector regressors where the yth coordinate of f \u2208 F is written as f(\u00b7; y).", "startOffset": 18, "endOffset": 24}, {"referenceID": 21, "context": "Similar querying strategies were used in prior works on binary and multiclass classification [22, 14, 1], but specialized to linear representations.", "startOffset": 93, "endOffset": 104}, {"referenceID": 13, "context": "Similar querying strategies were used in prior works on binary and multiclass classification [22, 14, 1], but specialized to linear representations.", "startOffset": 93, "endOffset": 104}, {"referenceID": 0, "context": "Similar querying strategies were used in prior works on binary and multiclass classification [22, 14, 1], but specialized to linear representations.", "startOffset": 93, "endOffset": 104}, {"referenceID": 20, "context": "Our low-noise assumption is related to the Massart noise condition [21], which in binary classification posits that the Bayes optimal predictor is bounded away from 1/2 for all x.", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "To compare, the standard generalization bound is \u00d5( \u221a log(|F|/\u03b4)/i) [20], which agrees with our bound since |F| = |G| in our case.", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "This agrees with the literature on active learning for classification [21] and can be viewed as a generalization to CSMC.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "The definition is a natural adaptation from binary classification [16], where a similar disagreement region to DIS(r, y) is used.", "startOffset": 66, "endOffset": 70}, {"referenceID": 15, "context": "The 1/r scaling leads to bounded coefficients in many examples [16], and we also scale by the cost range parameter \u03b71, so that the favorable settings for active learning can be concisely expressed as having \u03b81, \u03b82 bounded, as opposed to a complex function of \u03b71.", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "In binary classification, ideas based on hallucinating labels for unqueried examples address this issue [12], but this technique does not seem applicable here since the only safe choice of hallucinated cost that avoids eliminating f\u2217 appears to be f\u2217(x; y), which is naturally unknown.", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "classification [1].", "startOffset": 15, "endOffset": 18}, {"referenceID": 28, "context": "The feature vectors are the top layer of the Inception neural network [29].", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "The third dataset, RCV1-v2 [19], is a multilabel text-categorization dataset, which has 103 topic labels, organized as a tree with similar tree-distance cost structure as the ImageNet data.", "startOffset": 27, "endOffset": 31}, {"referenceID": 23, "context": "net/~vw 5We use the default online learning algorithm in Vowpal Wabbit, which is a scale-free [24] importance weight invariant [18] form of AdaGrad [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "net/~vw 5We use the default online learning algorithm in Vowpal Wabbit, which is a scale-free [24] importance weight invariant [18] form of AdaGrad [15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 14, "context": "net/~vw 5We use the default online learning algorithm in Vowpal Wabbit, which is a scale-free [24] importance weight invariant [18] form of AdaGrad [15].", "startOffset": 148, "endOffset": 152}, {"referenceID": 12, "context": "2 Learning to Search We also experiment with COAL as the base leaner in learning-to-search [13, 11], which reduces joint prediction problems to CSMC.", "startOffset": 91, "endOffset": 99}, {"referenceID": 10, "context": "2 Learning to Search We also experiment with COAL as the base leaner in learning-to-search [13, 11], which reduces joint prediction problems to CSMC.", "startOffset": 91, "endOffset": 99}, {"referenceID": 25, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "We specifically use AGGRAVATE [23, 11], which runs a learned policy to produce a backbone sequence of labels.", "startOffset": 30, "endOffset": 38}, {"referenceID": 10, "context": "We specifically use AGGRAVATE [23, 11], which runs a learned policy to produce a backbone sequence of labels.", "startOffset": 30, "endOffset": 38}, {"referenceID": 0, "context": "The online update guarantees that g i,y(x) \u2208 [0, 1].", "startOffset": 45, "endOffset": 51}, {"referenceID": 0, "context": "\u21d2 2\u2206 + wt,h [ (c\u2212 g`(x)) + (c\u2212 gh(x)) ] \u2264 2wt,h ( c\u2212 gh(x) + g`(x) 2 )2 + 2\u2206 since c, g`(x) \u2208 [0, 1] \u21d2 1 2 (c\u2212 g`(x)) + 1 2 (c\u2212 gh(x)) \u2264 ( c\u2212 gh(x) + g`(x) 2 )2 .", "startOffset": 94, "endOffset": 100}, {"referenceID": 7, "context": "1 Supporting Lemmata Theorem 7 (Freedman-type Inequality [8, 2]).", "startOffset": 57, "endOffset": 63}, {"referenceID": 1, "context": "1 Supporting Lemmata Theorem 7 (Freedman-type Inequality [8, 2]).", "startOffset": 57, "endOffset": 63}, {"referenceID": 0, "context": "Moreover, because the excess cost-sensitive classification risk is always upper-bounded by 1, it is trivially bounded by 2K\u2206 \u2032 1 \u03b6 for any \u03b6 \u2208 [0, 1].", "startOffset": 143, "endOffset": 149}], "year": 2017, "abstractText": "We design an active learning algorithm for cost-sensitive multiclass classification: problems where different errors have different costs. Our algorithm, COAL, makes predictions by regressing on each label\u2019s cost and predicting the smallest. On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort. We empirically compare COAL to passive learning, showing significant improvements in labeling effort and test cost.", "creator": "LaTeX with hyperref package"}}}