{"id": "1703.01703", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Third-Person Imitation Learning", "abstract": "reinforcement learning ( rl ) makes it available to train agents seeking of achiev - ing sophisticated goals in difficult and uncertain environments. a key advantage in reinforcement learning is specifying a reward function for the network to optimize. traditionally, processing memory in rl has strategy used to overcome this problem. here, hitherto imitation learning methods tend to address that demonstra - tions are supplied in basic first - person : the agent is provided with a sequence which states and a specification expressing the actions that it should have taken. while useful, this nature of imitation learning is limited by the relatively hard problem of collect - ing first - person demonstrations. humans address this problem by learning from third - contact demonstrations : they observe other humans perform tasks, infer the task, and accomplish the same task themselves.", "histories": [["v1", "Mon, 6 Mar 2017 02:02:34 GMT  (2204kb,D)", "http://arxiv.org/abs/1703.01703v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bradly c stadie", "pieter abbeel", "ilya sutskever"], "accepted": true, "id": "1703.01703"}, "pdf": {"name": "1703.01703.pdf", "metadata": {"source": "CRF", "title": "THIRD-PERSON IMITATION LEARNING", "authors": ["Bradly C. Stadie", "Pieter Abbeel", "Ilya Sutskever"], "emails": ["ilyasu@openai.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Reinforcement learning (RL) is a framework for training agents to maximize rewards in large, unknown, stochastic environments. In recent years, combining techniques from deep learning with reinforcement learning has yielded a string of successful applications in game playing and robotics Mnih et al. (2015; 2016); Schulman et al. (2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications.\nOne of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al. (2006); Ziebart et al. (2008); Levine et al. (2011); Ho & Ermon (2016); Finn et al. (2016)).\nWhile IRL algorithms are appealing, they impose the somewhat unrealistic requirement that the demonstrations should be provided from the first-person point of view with respect to the agent. Human beings learn to imitate entirely from third-person demonstrations \u2013 i.e., by observing other humans achieve goals. Indeed, in many situations, first-person demonstrations are outright impossible to obtain. Meanwhile, third-person demonstrations are often relatively easy to obtain.\nar X\niv :1\n70 3.\n01 70\n3v 1\n[ cs\n.L G\n] 6\nM ar\n2 01\n7\nThe goal of this paper is to develop an algorithm for third-person imitation learning. Future advancements in this class of algorithms would significantly improve the state of robotics, because it will enable people to easily teach robots news skills and abilities. Importantly, we want our algorithm to be unsupervised: it should be able to observe another agent perform a task, infer that there is an underlying correspondence to itself, and find a way to accomplish the same task.\nWe offer an approach to this problem by borrowing ideas from domain confusion Tzeng et al. (2014) and generative adversarial networks (GANs) Goodfellow et al. (2014). The high-level idea is to introduce an optimizer under which we can recover both a domain-agnostic representation of the agent\u2019s observations, and a cost function which utilizes this domain-agnostic representation to capture the essence of expert trajectories. We formulate this as a third-person RL-GAN problem, and our solution builds on the first-person RL-GAN formulation by Ho & Ermon (2016).\nSurprisingly, we find that this simple approach has been able to solve the problems that are presented in this paper (illustrated in Figure 1), even though the student\u2019s observations are related in a complicated way to the teacher\u2019s demonstrations (given that the observations and the demonstrations are pixel-level). As techniques for training GANs become more stable and capable, we expect our algorithm to be able to infer solve harder third-person imitation tasks without any direct supervision."}, {"heading": "2 RELATED WORK", "text": "Imitation learning (also learning from demonstrations or programming by demonstration) considers the problem of acquiring skills from observing demonstrations. Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009). Two main lines of work within imitation learning are: 1) behavioral cloning, where the demonstrations are used to directly learn a mapping from observations to actions using supervised learning, potentially with interleaving learning and data collection (e.g., Pomerleau (1989); Ross et al. (2011)). 2) Inverse reinforcement learning (Ng et al., 2000), where a reward function is estimated that explains the demonstrations as (near) optimal behavior. This reward function could be represented as nearness to a trajectory (Calinon et al., 2007; Abbeel et al., 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al., 2006; Ramachandran & Amir, 2007; Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan et al., 2013; Doerr et al., 2015), or could also involve feature learning (Ratliff et al., 2007; Levine et al., 2011; Wulfmeier et al., 2015; Finn et al., 2016; Ho & Ermon, 2016).\nThis past work, however, is not directly applicable to the third person imitation learning setting. In third-person imitation learning, the observations and actions obtained from the demonstrations are not the same as what the imitator agent will be faced with. A typical scenario would be: the imitator agent watches a human perform a demonstration, and then has to execute that same task. As discussed in Nehaniv & Dautenhahn (2001) the \u201dwhat and how to imitate\u201d questions become significantly more challenging in this setting. To directly apply existing behavioral cloning or inverse reinforcement learning techniques would require knowledge of a mapping between observations and actions in the demonstrator space to observations and actions in the imitator space. Such a mapping is often difficult to obtain, and it typically relies on providing feature representations that captures the invariance between both environments Carpenter et al. (2002); Shon et al. (2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available.\nThe most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting.\nOur work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014).\nThe problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al. (2012); Hoffman et al. (2013); Long & Wang (2015). It has also been shown that features trained in one domain can often be relevant to other domains Donahue et al. (2014). The work most closely related to ours is Tzeng et al. (2014; 2015), who also consider an explicit domain confusion loss, forcing trained classifiers to rely on features that don\u2019t allow to distinguish between two domains. This work in turn relates to earlier work by Bromley et al. (1993); Chopra et al. (2005), which also considers supervised training of deep feature embeddings.\nOur approach to third-person imitation learning relies on reinforcement learning from raw sensory data in the imitator domain. Several recent advances in deep reinforcement learning have made this practical, including Deep Q-Networks (Mnih et al., 2015), Trust Region Policy Optimization (Schulman et al., 2015a), A3C Mnih et al. (2016), and Generalized Advantage Estimation (Schulman et al., 2015b). Our approach uses Trust Region Policy Optimization."}, {"heading": "3 BACKGROUND AND PRELIMINARIES", "text": "A discrete-time finite-horizon discounted Markov decision process (MDP) is represented by a tuple M = (S,A,P, r, \u03c10, \u03b3, T ), in which S is a state set, A an action set, P : S \u00d7 A \u00d7 S \u2192 R+ a transition probability distribution, r : S \u00d7 A \u2192 R a reward function, \u03c10 : S \u2192 R+ an initial state distribution, \u03b3 \u2208 [0, 1] a discount factor, and T the horizon. In the reinforcement learning setting, the goal is to find a policy \u03c0\u03b8 : S \u00d7 A \u2192 R+ parametrized by \u03b8 that maximizes the expected discounted sum of rewards incurred, \u03b7(\u03c0\u03b8) = E\u03c0\u03b8 [ \u2211T t=0 \u03b3\ntc(st)], where s0 \u223c \u03c10(s0), at \u223c \u03c0\u03b8(at|st), and st+1 \u223c P(st+1|st, at). In the (first-person) imitation learning setting, we are not given the reward function. Instead we are given traces (i.e., sequences of states traversed) by an expert who acts according to an unknown policy \u03c0E . The goal is to find a policy \u03c0\u03b8 that performs as well as the expert against the unknown reward function. It was shown in Abbeel & Ng (2004) that this can be achieved through inverse reinforcement learning by finding a policy \u03c0\u03b8 that matches the expert\u2019s empirical expectation over discounted sum of all features that might contribute to the reward function. The work by Ho & Ermon (2016) generalizes this to the setting when no features are provided as follows: Find a policy \u03c0\u03b8 that makes it impossible for a discriminator (in their work a deep neural net) to distinguish states visited by the expert from states visited by the imitator agent. This can be formalized as follows:\nmax \u03c0\u03b8 min DR \u2212 E\u03c0\u03b8 [logDR(s)]\u2212 E\u03c0E [log(1\u2212DR(s))] (1)\nHere, the expectations are over the states experienced by the policy of the imitator agent, \u03c0\u03b8, and by the policy of the expert, \u03c0E , respectively. DR is the discriminator, which outputs the probability of a state having originated from a trace from the imitator policy \u03c0\u03b8. If the discriminator is perfectly able to distinguish which policy originated state-action pairs, then DR will consistently output a probability of 1 in the first term, and a probability of 0 in the second term, making the objective its lowest possible value of zero. It is the role of the imitator agent \u03c0\u03b8 to find a policy that makes it difficult for the discriminator to make that distinction. The desired equilibrium has the imitator agent making it impractical for the discriminator to distinguish, hence forcing the discriminator to assign probability 0.5 in all cases. Ho & Ermon (2016) present a practical approach for solving this type of game when representing both \u03c0\u03b8 and DR as deep neural networks. Their approach repeatedly performs gradient updates on each of them. Concretely, for a current policy \u03c0\u03b8 traces can be collected, which together with the expert traces form a data-set on which DR can be trained with supervised learning minimizing the negative log-likelihood (in practice only performing a modest number of updates). For a fixed DR, this is a policy optimization problem where \u2212 logDR(s, a) is the reward, and policy gradients can be computed from those same traces. Their approach uses trust region policy optimization (Schulman et al., 2015a) to update the imitator policy \u03c0\u03b8 from those gradients.\nIn our work we will have more terms in the objective, so for compactness of notation, we will realize the discriminative minimization from Eqn. (1) as follows:\nmax \u03c0\u03b8 min DR LR = \u2211 i CE(DR(si), c`i) (2)\nWhere si is state i, c`i is the correct class label (was the state si obtained from an expert vs. from a non-expert), and CE is the standard cross entropy loss."}, {"heading": "4 A FORMAL DEFINITION OF THE THIRD-PERSON IMITATION LEARNING PROBLEM", "text": "Formally, the third-person imitation learning problem can be stated as follows. Suppose we are given two Markov Decision Processes M\u03c0E and M\u03c0\u03b8 . Suppose further there exists a set of traces \u03c1 = {(s1, . . . , sn)}ni=0 which were generated under a policy \u03c0E acting optimally under some unknown reward R\u03c0E . In third-person imitation learning, one attempts to recover by proxy through \u03c1 a policy \u03c0\u03b8 = f(\u03c1) which acts optimally with respect to R\u03c0\u03b8 ."}, {"heading": "5 A THIRD-PERSON IMITATION LEARNING ALGORITHM", "text": ""}, {"heading": "5.1 GAME FORMULATION", "text": "In this section, we discuss a simple algorithm for third-person imitation learning. This algorithm is able to successfully discriminate between expert and novice policies, even when the policies are executed under different environments. Subsequently, this discrimination signal can be used to train expert policies in new domains via RL by training the novice policy to fool the discriminator, thus forcing it to match the expert policy.\nIn third-person learning, observations are more typically available rather than direct state access, so going forward we will work with observations ot instead of states st as representing the expert traces. The top row of Figure 8 illustrates what these observations are like in our experiments.\nWe begin by recalling that in the algorithm proposed by Ho & Ermon (2016) the loss in Equation 2 is utilized to train a discriminator DR capable of distinguishing expert vs non-expert policies. Unfortunately, (2) will likely fail in cases when the expert and non-expert act in different environments, since DR will quickly learn these differences and use them as a strong classification signal. To handle the third-person setting, where expert and novice are in different environments, we consider that DR works by first extracting features from ot, and then using these features to make a\nclassification. Suppose then that we partition DR into a feature extractor DF and the actual classifier which assigns probabilities to the outputs of DF . Overloading notation, we will refer to the classifier as DR going forward. For example, in case of a deep neural net representation, DF would correspond to the earlier layers, and DR to the later layers. The problem is then to ensure that DF contains no information regarding the rollout\u2019s domain label d` (i.e., expert vs. novice domain). This can be realized as\nmax \u03c0\u03b8 minLR = \u2211 i CE(DR(DF (oi)), c`i)\ns.t. MI(DF (oi); dl) = 0\nWhere MI is mutual information and hence we have abused notation by using DR, DF , and d` to mean the classifier, feature extractor, and the domain label respectively as well as distributions over these objects.\nThe mutual information term can be instantiated by introducing another classifier DD, which takes features produced by DF and outputs the probability that those features were produced by in the expert vs. non-expert environment. (See Bridle et al. (1992); Barber & Agakov (2005); Krause et al. (2010); Chen et al. (2016) for further discussion on instantiating the information term by introducing another classifier.) If \u03c3i = DF (oi), then the problem can be written as\nmax \u03c0\u03b8 min DR max DD LR + LD = \u2211 i CE(DR(\u03c3i), c`i) + CE(DD(\u03c3i), d`i) (3)\nIn words, we wish to minimize class loss while maximizing domain confusion.\nOften, it can be difficult for even humans to judge a static image as expert vs. non-expert because it does not convey any information about the environmental change affected by the agent\u2019s actions. For example, if a pointmass is attempting to move to a target location and starts far away from its goal state, it can be difficult to judge if the policy itself is bad or the initialization was simply unlucky. In response to this difficulty, we give DR access to not only the image at time t, but also at some future time t + n. Define \u03c3t = DF (ot) and \u03c3t+n = DF (ot+n). The classifier then makes a prediction DR(\u03c3t, \u03c3t+n) = c\u0302`. This renders the following formulation:\nmax \u03c0\u03b8 min DR max DD LR + LD = \u2211 i CE(DR(\u03c3i, \u03c3i+n), c`i) + CE(DD(\u03c3i), d`i) (4)\nNote we also want to optimize overDF , the feature extractor, but it feeds both intoDR and intoDD, which are competing (hidden under \u03c3), which we will address now.\nTo deal with the competition over DF , we introduce a function G that acts as the identity when moving forward through a directed acyclic graph and flips the sign when backpropagating through the graph. This technique has enjoyed recent success in computer vision. See, for example, (Ganin & Lempitsky, 2014). With this trick, the problem reduces to its final form\nmax \u03c0\u03b8 min DR,DD,DF LR + LD = \u2211 i CE(DR(\u03c3i, \u03c3i+n), c`i) + \u03bb CE(DD(G(\u03c3i), d`i) (5)\nIn Equation (5), we flip the gradient\u2019s sign during backpropagation ofDF with respect to the domain classification loss. This corresponds to stochastic gradient ascent away from features that are useful for domain classification, thus ensuring that DF produces domain agnostic features. Equation 5 can be solved efficiently with stochastic gradient descent. Here \u03bb is a hyperparameter that determines the trade-off made between the objectives that are competing over DF . To ensure sufficient signal for discrimination between expert and non-expert, we collect third-person demonstrations in the expert domain from both an expert and from a non-expert.\nOur complete formulation is graphically summarized in Figure 2."}, {"heading": "5.2 ALGORITHM", "text": "To solve the game formulation in Equation (5), we perform alternating (partial) optimization over the policy \u03c0\u03b8 and the reward function and domain confusion encoded through DR,DD,DF . The optimization over DR,DD,DF is done through stochastic gradient descent with ADAM Kingma & Ba (2014).\nOur generator (\u03c0\u03b8) step is similar to the generator step in the algorithm by (Ho & Ermon, 2016). We simply use \u2212 logDR as the reward. Using policy gradient methods (TRPO), we train the generator to minimize this cost and thus push the policy further towards replicating expert behavior. Once the generator step is done, we start again with the discriminator step. The entire process is summarized in algorithm 1."}, {"heading": "6 EXPERIMENTS", "text": "We seek to answer the following questions through experiments:\n1. Is it possible to solve the third-person imitation learning problem in simple settings? I.e., given a collection of expert image-based rollouts in one domain, is it possible to train a policy in a different domain that replicates the essence of the original behavior?\n2. Does the algorithm we propose benefit from both domain confusion and velocity?\n3. How sensitive is our proposed algorithm to the selection of hyper-parameters used in deployment?\n4. How sensitive is our proposed algorithm to changes in camera angle?\n5. How does our method compare against some reasonable baselines?\nAlgorithm 1 A third-person imitation learning algorithm. 1: Let CE be the standard cross entropy loss. 2: Let G be a function that flips the gradient sign during backpropogation and acts as the identity\nmap otherwise. 3: Initialize two domains, E and N for the expert and novice. 4: Initialize a memory bank \u2126 of expert success and of failure in domainE. Each trajectory \u03c9 \u2208 \u2126\ncomprises a rollout of images o = o1, . . . , ot, . . . on, a class label c`, and a domain label d`. 5: Initialize D = DF ,DR,DD, a domain invariant discriminator. 6: Initialize a novice policy \u03c0\u03b8. 7: Initialize numiters, the number of inner policy optimization iterations we wish to run. 8: for iter in numiters do 9: Sample a set of successes and failures \u03c9E from \u2126.\n10: Collect on policy samples \u03c9N 11: Set \u03c9 = \u03c9E \u222a \u03c9N . 12: Shuffle \u03c9 13: for o, c`, d` in \u03c9 do 14: for ot in o do 15: \u03c3t = DF (ot) 16: \u03c3t+4 = DF (ot+4) 17: LR = CE(DR(\u03c3t, \u03c3t+4), c`) 18: Ld = CE(DD(G(\u03c3t)), d`) 19: L = \u03bb \u00b7 Ld + LR 1 20: minimize L with ADAM. 21: end for 22: end for 23: Collect on policy samples \u03c9N from \u03c0\u03b8. 24: for \u03c9 in \u03c9N do 25: for \u03c9t in \u03c9 do 26: \u03c3t = DF (ot) 27: \u03c3t+4 = DF (ot+4) 28: c\u0302` = DR(\u03c3t, \u03c3t+4) 29: r = c\u0302`[0], the probability that ot, ot+4 were generated via expert rollouts. 30: Use r to train \u03c0\u03b8 with via policy gradients (TRPO). 31: end for 32: end for 33: end for 34: return optimized policy \u03c0\u03b8"}, {"heading": "6.1 ENVIRONMENTS", "text": "To evaluate our algorithm, we consider three environments in the MuJoCo physics simulator. There are two different versions of each environment, an expert variant and a novice variant. Our goal is to train a cost function that is domain agnostic, and hence can be trained with images on the expert domain but nevertheless produce a reasonable cost on the novice domain. See Figure 1 for a visualization of the differences between expert and novice environments for the three tasks.\nPoint: A pointmass attempts to reach a point in a plane. The color of the target and the camera angle change between domains.\nReacher: A two DOF arm attempts to reach a designated point in the plane. The camera angle, the length of the arms, and the color of the target point are changed between domains. Note that changing the camera angle significantly alters the image background color from largely gray to roughly 30 percent black. This presents a significant challenge for our method.\nInverted Pendulum: A classic RL task wherein a pendulum must be made to balance via control. For this domain, We only change the color of the pendulum and not the camera angle. Since there is no target point, we found that changing the camera angle left the domain invariant representations with too little information and resulted in a failure case. In contrast to some traditional renderings\nof this problem, we do not terminate an episode when the agent falls but rather allow data collection to continue for a fixed horizon."}, {"heading": "6.2 EVALUATIONS", "text": "Is it possible to solve the third-person imitation learning problem in simple settings? In Figure 3, we see that our proposed algorithm is indeed able to recover reasonable policies for all three tasks we examined. Initially, the training is quite unstable due to the domain confusion wreaking havoc on the learned cost. However, after several iterations the policies eventually head towards reasonable local minima and the standard deviation over the reward distribution shrinks substantially. Finally, we note that the extracted feature representations used to complete this task are in fact domain-agnostic, as seen in Figure 9. Hence, the learning is properly taking place from a third-person perspective.\nDoes the algorithm we propose benefit from both domain confusion and the multi-time step input? We answer this question with the experiments summarized in Figure 5. This experiment compares our approach with: (i) our approach without the domain confusion loss; (ii) our approach without the multi-time step input; (iii) our approach without the domain confusion loss and without the multitime step input (which is very similar to the approach in Ho & Ermon (2016)). We see that adding domain confusion is essential for getting strong performance in all three experiments. Meanwhile, adding multi-time step input marginally improves the results. See also Figure 7 for an analysis of the effects of multi-time step input on the final results.\nHow sensitive is our proposed algorithm to the selection of hyper-parameters used in deployment? Figure 6 shows the effect of the domain confusion coefficient \u03bb, which trades off how much we should weight the domain confusion objective vs. the standard cost-recovery objective, on the final performance of the algorithm. Setting \u03bb too low results in slower learning and features that are not domain-invariant. Setting \u03bb too high results in an objective that is too quick to destroy information, which makes it impossible to recover an accurate cost.\nFor multi-time step input, one must choose the number of look-ahead frames that are utilized. If too small a window is chosen, the agent\u2019s actions have not affected a large amount of change in the environment and it is difficult to discern any additional class signal over static images. If too large a time-frame passes, causality becomes difficult to interpolate and the agent does worse than simply being trained on static frames. Figure 7 illustrates that no number of look-ahead frames is consistently optimal across tasks. However, a value of 4 showed good performance over all tasks, and so this value was utilized in all other experiments.\nHow sensitive is our algorithm to changes in camera angle? We present graphs for the reacher and point experiments wherein we exam the final reward obtained by a policy trained with thirdperson imitation learning vs the camera angle difference between the first-person and third-person perspective. We omit the inverted double pendulum experiment, as the color and not the camera angle changes in that setting and we found the case of slowly transitioning the color to be the definition of uninteresting science.\nHow does our method compare against reasonable baselines? We consider the following baselines for comparisons against third-person imitation learning. 1) Standard reinforcement learning with using full state information and the true reward signal. This agent is trained via TRPO. 2)\nStandard GAIL (first-person imitation learning). Here, the agent receives first-person demonstration and attempts to imitate the correct behavior. This is an upper bound on how well we can expect to do, since we have the correct perspective. 3) Training a policy using first-person data and applying it to the third-person environment.\nWe compare all three of these baselines to third-person imitation learning. As we see in figure 9: 1) Standard RL, which (unlike the imitation learning approaches) has access to full state and true reward, helps calibrate performance of the other approaches. 2) First-person imitation learning is faced with a simpler imitation problem and accordingly outperforms third-person imitation, yet third-person imitation learning is nevertheless competitive. 3) Applying the first-person policy to the third-person agent fails miserably, illustrating that explicitly considering third-person imitation is important in these settings.\nSomewhat unfortunately, the different reward function scales make it difficult to capture information on the variance of each learning curve. Consequently, in Appendix A we have included the full learning curves for these experiments with variance bars, each plotted with an appropriate scale to examine the variance of the individual curves."}, {"heading": "7 DISCUSSION AND FUTURE WORK", "text": "In this paper, we presented the problem of third-person imitation learning. We argue that this problem will be important going forward, as techniques in reinforcement learning and generative adversarial learning improve and the cost of collecting first-person samples remains high. We presented an algorithm which builds on Generative Adversarial Imitation Learning and is capable of solving simple third-person imitation tasks.\nOne promising direction of future work in this area is to jointly train policy features and cost features at the pixel level, allowing the reuse of image features. Code to train a third person imitation learning agent on the domains from this paper is presented here: https://github.com/bstadie/ third_person_im"}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was done partially at OpenAI and partially at Berkeley. Work done at Berkeley was supported in part by Darpa under the Simplex program and the FunLoL program."}, {"heading": "8 APPENDIX A: LEARNING CURVES FOR BASELINES", "text": "Here, we plot the learning curves for each of the baselines mentioned in the experiments section as a standalone plot. This allows one to better examine the variance of each individual learning curve."}, {"heading": "9 APPENDIX B: ARCHITECTURE PARAMETERS", "text": "Joint Feature Extractor: Input is images are size 50 x 50 with 3 channels, RGB. Layers are 2 convolutional layers each followed by a max pooling layer of size 2. Layers use 5 filters of size 3 each.\nDomain Discriminator and the Class Discriminator: Input is domain agnostic output of convolutional layers. Layers are two feed forward layers of size 128 followed by a final feed forward layer of size 2 and a soft-max layer to get the log probabilities.\nADAM is used for discriminator training with a learning rate of 0.001. The RL generator uses the off-the-shelf TRPO implementation available in RLLab."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A. Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Abbeel and Ng.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel and Ng.", "year": 2004}, {"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["Pieter Abbeel", "Adam Coates", "Andrew Y Ng"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Abbeel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2010}, {"title": "A survey of robot learning from demonstration", "author": ["Brenna D Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and autonomous systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Tabula rasa: Model transfer for object category detection", "author": ["Yusuf Aytar", "Andrew Zisserman"], "venue": "In 2011 International Conference on Computer Vision,", "citeRegEx": "Aytar and Zisserman.,? \\Q2011\\E", "shortCiteRegEx": "Aytar and Zisserman.", "year": 2011}, {"title": "Kernelized infomax clustering", "author": ["D. Barber", "F.V. Agakov"], "venue": null, "citeRegEx": "Barber and Agakov.,? \\Q2005\\E", "shortCiteRegEx": "Barber and Agakov.", "year": 2005}, {"title": "Relative entropy inverse reinforcement learning", "author": ["A. Boularias", "J. Kober", "J. Peters"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Boularias et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boularias et al\\.", "year": 2011}, {"title": "Unsupervised classifiers, mutual information and phantom targets", "author": ["J.S. Bridle", "A.J. Heading", "D.J. MacKay"], "venue": null, "citeRegEx": "Bridle et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Bridle et al\\.", "year": 1992}, {"title": "Signature verification using a siamese time delay neural network", "author": ["Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Robot programming by demonstration", "author": ["Sylvain Calinon"], "venue": "EPFL Press,", "citeRegEx": "Calinon.,? \\Q2009\\E", "shortCiteRegEx": "Calinon.", "year": 2009}, {"title": "On learning, representing, and generalizing a task in a humanoid robot", "author": ["Sylvain Calinon", "Florent Guenter", "Aude Billard"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),", "citeRegEx": "Calinon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Calinon et al\\.", "year": 2007}, {"title": "Understanding prior intentions enables two\u2013year\u2013olds to imitatively learn a complex task", "author": ["Malinda Carpenter", "Josep Call", "Michael Tomasello"], "venue": "Child development,", "citeRegEx": "Carpenter et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Carpenter et al\\.", "year": 2002}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Sumit Chopra", "Raia Hadsell", "Yann LeCun"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Direct loss minimization inverse optimal control", "author": ["A. Doerr", "N. Ratliff", "J. Bohg", "M. Toussaint", "S. Schaal"], "venue": "In Proceedings of Robotics: Science and Systems (R:SS),", "citeRegEx": "Doerr et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Doerr et al\\.", "year": 2015}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "In ICML, pp", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Learning with augmented features for heterogeneous domain adaptation", "author": ["Lixin Duan", "Dong Xu", "Ivor Tsang"], "venue": "arXiv preprint arXiv:1206.4660,", "citeRegEx": "Duan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2012}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["C. Finn", "S. Levine", "P. Abbeel"], "venue": null, "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "Arxiv preprint 1409.7495,", "citeRegEx": "Ganin and Lempitsky.,? \\Q2014\\E", "shortCiteRegEx": "Ganin and Lempitsky.", "year": 2014}, {"title": "An object-based approach to map human hand synergies onto robotic hands with dissimilar kinematics", "author": ["G Gioioso", "G Salvietti", "M Malvezzi", "D Prattichizzo"], "venue": "Robotics: Science and Systems VIII, pp", "citeRegEx": "Gioioso et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gioioso et al\\.", "year": 2013}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Learning dexterous manipulation for a soft robotic hand from human demonstration", "author": ["Abhishek Gupta", "Clemens Eppner", "Sergey Levine", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1603.06348,", "citeRegEx": "Gupta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2016}, {"title": "Generative adversarial imitation learning", "author": ["J. Ho", "S. Ermon"], "venue": "arXiv pre-print: 1606.03476,", "citeRegEx": "Ho and Ermon.,? \\Q2016\\E", "shortCiteRegEx": "Ho and Ermon.", "year": 2016}, {"title": "Efficient learning of domain-invariant image representations", "author": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Trevor Darrell", "Kate Saenko"], "venue": "arXiv preprint arXiv:1301.3224,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Learning objective functions for manipulation", "author": ["M. Kalakrishnan", "P. Pastor", "L. Righetti", "S. Schaal"], "venue": "In International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Kalakrishnan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalakrishnan et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Discriminative clustering by regularized information maximization", "author": ["A. Krause", "P. Perona", "R.G. Gomes"], "venue": null, "citeRegEx": "Krause et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2010}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["Brian Kulis", "Kate Saenko", "Trevor Darrell"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kulis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2011}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Levine et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2011}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Learning transferable features with deep adaptation networks", "author": ["Mingsheng Long", "Jianmin Wang"], "venue": "CoRR, abs/1502.02791,", "citeRegEx": "Long and Wang.,? \\Q2015\\E", "shortCiteRegEx": "Long and Wang.", "year": 2015}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "arXiv preprint arXiv:0902.3430,", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Nine billion correspondence problems. Imitation and Social Learning in Robots, Humans and Animals: Behavioural, Social and Communicative Dimensions", "author": ["Chrystopher L Nehaniv"], "venue": null, "citeRegEx": "Nehaniv.,? \\Q2007\\E", "shortCiteRegEx": "Nehaniv.", "year": 2007}, {"title": "Like me?-measures of correspondence and imitation", "author": ["Chrystopher L Nehaniv", "Kerstin Dautenhahn"], "venue": "Cybernetics & Systems,", "citeRegEx": "Nehaniv and Dautenhahn.,? \\Q2001\\E", "shortCiteRegEx": "Nehaniv and Dautenhahn.", "year": 2001}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A. Ng", "S. Russell"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Ng and Russell,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell", "year": 2000}, {"title": "Alvinn: An autonomous land vehicle in a neural network", "author": ["Dean A Pomerleau"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Pomerleau.,? \\Q1989\\E", "shortCiteRegEx": "Pomerleau.", "year": 1989}, {"title": "Bayesian inverse reinforcement learning", "author": ["D. Ramachandran", "E. Amir"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ramachandran and Amir.,? \\Q2007\\E", "shortCiteRegEx": "Ramachandran and Amir.", "year": 2007}, {"title": "Maximum margin planning", "author": ["N. Ratliff", "J.A. Bagnell", "M.A. Zinkevich"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Boosting structured prediction for imitation learning", "author": ["N. Ratliff", "D. Bradley", "J.A. Bagnell", "J. Chestnutt"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ratliff et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoffrey J Gordon", "Drew Bagnell"], "venue": "In AISTATS, pp", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Is imitation learning the route to humanoid robots", "author": ["Stefan Schaal"], "venue": "Trends in cognitive sciences,", "citeRegEx": "Schaal.,? \\Q1999\\E", "shortCiteRegEx": "Schaal.", "year": 1999}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I. Jordan", "Pieter Abbeel"], "venue": "Arxiv preprint 1502.05477,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Learning shared latent structure for image synthesis and robotic imitation", "author": ["Aaron Shon", "Keith Grochow", "Aaron Hertzmann", "Rajesh P Rao"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Shon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Shon et al\\.", "year": 2005}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["Eric Tzeng", "Judy Hoffman", "Ning Zhang", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1412.3474,", "citeRegEx": "Tzeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2014}, {"title": "Towards adapting deep visuomotor representations from simulated to real environments", "author": ["Eric Tzeng", "Coline Devin", "Judy Hoffman", "Chelsea Finn", "Xingchao Peng", "Pieter Abbeel", "Sergey Levine", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1511.07111,", "citeRegEx": "Tzeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2015}, {"title": "Maximum entropy deep inverse reinforcement learning", "author": ["M. Wulfmeier", "P. Ondruska", "I. Posner"], "venue": null, "citeRegEx": "Wulfmeier et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wulfmeier et al\\.", "year": 2017}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B. Ziebart", "A. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": null, "citeRegEx": "Ziebart et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 28, "context": "In recent years, combining techniques from deep learning with reinforcement learning has yielded a string of successful applications in game playing and robotics Mnih et al. (2015; 2016); Schulman et al. (2015a); Levine et al.", "startOffset": 162, "endOffset": 212}, {"referenceID": 26, "context": "(2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications.", "startOffset": 9, "endOffset": 30}, {"referenceID": 26, "context": "(2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications. One of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al.", "startOffset": 9, "endOffset": 880}, {"referenceID": 26, "context": "(2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications. One of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al. (2006); Ziebart et al.", "startOffset": 9, "endOffset": 903}, {"referenceID": 26, "context": "(2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications. One of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al. (2006); Ziebart et al. (2008); Levine et al.", "startOffset": 9, "endOffset": 926}, {"referenceID": 26, "context": "(2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications. One of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al. (2006); Ziebart et al. (2008); Levine et al. (2011); Ho & Ermon (2016); Finn et al.", "startOffset": 9, "endOffset": 948}, {"referenceID": 26, "context": "(2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications. One of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al. (2006); Ziebart et al. (2008); Levine et al. (2011); Ho & Ermon (2016); Finn et al.", "startOffset": 9, "endOffset": 967}, {"referenceID": 16, "context": "(2011); Ho & Ermon (2016); Finn et al. (2016)).", "startOffset": 27, "endOffset": 46}, {"referenceID": 44, "context": "We offer an approach to this problem by borrowing ideas from domain confusion Tzeng et al. (2014) and generative adversarial networks (GANs) Goodfellow et al.", "startOffset": 78, "endOffset": 98}, {"referenceID": 19, "context": "(2014) and generative adversarial networks (GANs) Goodfellow et al. (2014). The high-level idea is to introduce an optimizer under which we can recover both a domain-agnostic representation of the agent\u2019s observations, and a cost function which utilizes this domain-agnostic representation to capture the essence of expert trajectories.", "startOffset": 50, "endOffset": 75}, {"referenceID": 19, "context": "(2014) and generative adversarial networks (GANs) Goodfellow et al. (2014). The high-level idea is to introduce an optimizer under which we can recover both a domain-agnostic representation of the agent\u2019s observations, and a cost function which utilizes this domain-agnostic representation to capture the essence of expert trajectories. We formulate this as a third-person RL-GAN problem, and our solution builds on the first-person RL-GAN formulation by Ho & Ermon (2016). Surprisingly, we find that this simple approach has been able to solve the problems that are presented in this paper (illustrated in Figure 1), even though the student\u2019s observations are related in a complicated way to the teacher\u2019s demonstrations (given that the observations and the demonstrations are pixel-level).", "startOffset": 50, "endOffset": 473}, {"referenceID": 41, "context": "Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009).", "startOffset": 84, "endOffset": 134}, {"referenceID": 8, "context": "Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009).", "startOffset": 84, "endOffset": 134}, {"referenceID": 2, "context": "Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009).", "startOffset": 84, "endOffset": 134}, {"referenceID": 9, "context": "This reward function could be represented as nearness to a trajectory (Calinon et al., 2007; Abbeel et al., 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al.", "startOffset": 70, "endOffset": 113}, {"referenceID": 1, "context": "This reward function could be represented as nearness to a trajectory (Calinon et al., 2007; Abbeel et al., 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al.", "startOffset": 70, "endOffset": 113}, {"referenceID": 38, "context": ", 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al., 2006; Ramachandran & Amir, 2007; Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan et al., 2013; Doerr et al., 2015), or could also involve feature learning (Ratliff et al.", "startOffset": 47, "endOffset": 208}, {"referenceID": 5, "context": ", 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al., 2006; Ramachandran & Amir, 2007; Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan et al., 2013; Doerr et al., 2015), or could also involve feature learning (Ratliff et al.", "startOffset": 47, "endOffset": 208}, {"referenceID": 23, "context": ", 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al., 2006; Ramachandran & Amir, 2007; Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan et al., 2013; Doerr et al., 2015), or could also involve feature learning (Ratliff et al.", "startOffset": 47, "endOffset": 208}, {"referenceID": 13, "context": ", 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al., 2006; Ramachandran & Amir, 2007; Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan et al., 2013; Doerr et al., 2015), or could also involve feature learning (Ratliff et al.", "startOffset": 47, "endOffset": 208}, {"referenceID": 39, "context": ", 2015), or could also involve feature learning (Ratliff et al., 2007; Levine et al., 2011; Wulfmeier et al., 2015; Finn et al., 2016; Ho & Ermon, 2016).", "startOffset": 48, "endOffset": 152}, {"referenceID": 27, "context": ", 2015), or could also involve feature learning (Ratliff et al., 2007; Levine et al., 2011; Wulfmeier et al., 2015; Finn et al., 2016; Ho & Ermon, 2016).", "startOffset": 48, "endOffset": 152}, {"referenceID": 16, "context": ", 2015), or could also involve feature learning (Ratliff et al., 2007; Levine et al., 2011; Wulfmeier et al., 2015; Finn et al., 2016; Ho & Ermon, 2016).", "startOffset": 48, "endOffset": 152}, {"referenceID": 1, "context": "Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009). Two main lines of work within imitation learning are: 1) behavioral cloning, where the demonstrations are used to directly learn a mapping from observations to actions using supervised learning, potentially with interleaving learning and data collection (e.g., Pomerleau (1989); Ross et al.", "startOffset": 114, "endOffset": 414}, {"referenceID": 1, "context": "Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009). Two main lines of work within imitation learning are: 1) behavioral cloning, where the demonstrations are used to directly learn a mapping from observations to actions using supervised learning, potentially with interleaving learning and data collection (e.g., Pomerleau (1989); Ross et al. (2011)).", "startOffset": 114, "endOffset": 434}, {"referenceID": 31, "context": "Several recent advances in deep reinforcement learning have made this practical, including Deep Q-Networks (Mnih et al., 2015), Trust Region Policy Optimization (Schulman et al.", "startOffset": 107, "endOffset": 126}, {"referenceID": 17, "context": "As discussed in Nehaniv & Dautenhahn (2001) the \u201dwhat and how to imitate\u201d questions become significantly more challenging in this setting.", "startOffset": 16, "endOffset": 44}, {"referenceID": 7, "context": "Such a mapping is often difficult to obtain, and it typically relies on providing feature representations that captures the invariance between both environments Carpenter et al. (2002); Shon et al.", "startOffset": 161, "endOffset": 185}, {"referenceID": 7, "context": "Such a mapping is often difficult to obtain, and it typically relies on providing feature representations that captures the invariance between both environments Carpenter et al. (2002); Shon et al. (2005); Calinon et al.", "startOffset": 161, "endOffset": 205}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al.", "startOffset": 8, "endOffset": 46}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al.", "startOffset": 8, "endOffset": 69}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available.", "startOffset": 8, "endOffset": 90}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al.", "startOffset": 8, "endOffset": 288}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al.", "startOffset": 8, "endOffset": 307}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data.", "startOffset": 8, "endOffset": 332}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al.", "startOffset": 8, "endOffset": 685}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016).", "startOffset": 8, "endOffset": 774}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014).", "startOffset": 8, "endOffset": 793}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al.", "startOffset": 8, "endOffset": 898}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al.", "startOffset": 8, "endOffset": 1073}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al.", "startOffset": 8, "endOffset": 1096}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al.", "startOffset": 8, "endOffset": 1117}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al.", "startOffset": 8, "endOffset": 1143}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al. (2012); Hoffman et al.", "startOffset": 8, "endOffset": 1163}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al. (2012); Hoffman et al. (2013); Long & Wang (2015).", "startOffset": 8, "endOffset": 1186}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al. (2012); Hoffman et al. (2013); Long & Wang (2015). It has also been shown that features trained in one domain can often be relevant to other domains Donahue et al.", "startOffset": 8, "endOffset": 1206}, {"referenceID": 7, "context": "(2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available. The most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting. Our work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014). The problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al. (2012); Hoffman et al. (2013); Long & Wang (2015). It has also been shown that features trained in one domain can often be relevant to other domains Donahue et al. (2014). The work most closely related to ours is Tzeng et al.", "startOffset": 8, "endOffset": 1327}, {"referenceID": 7, "context": "This work in turn relates to earlier work by Bromley et al. (1993); Chopra et al.", "startOffset": 45, "endOffset": 67}, {"referenceID": 7, "context": "This work in turn relates to earlier work by Bromley et al. (1993); Chopra et al. (2005), which also considers supervised training of deep feature embeddings.", "startOffset": 45, "endOffset": 89}, {"referenceID": 7, "context": "This work in turn relates to earlier work by Bromley et al. (1993); Chopra et al. (2005), which also considers supervised training of deep feature embeddings. Our approach to third-person imitation learning relies on reinforcement learning from raw sensory data in the imitator domain. Several recent advances in deep reinforcement learning have made this practical, including Deep Q-Networks (Mnih et al., 2015), Trust Region Policy Optimization (Schulman et al., 2015a), A3C Mnih et al. (2016), and Generalized Advantage Estimation (Schulman et al.", "startOffset": 45, "endOffset": 496}, {"referenceID": 6, "context": "(See Bridle et al. (1992); Barber & Agakov (2005); Krause et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 6, "context": "(See Bridle et al. (1992); Barber & Agakov (2005); Krause et al.", "startOffset": 5, "endOffset": 50}, {"referenceID": 6, "context": "(See Bridle et al. (1992); Barber & Agakov (2005); Krause et al. (2010); Chen et al.", "startOffset": 5, "endOffset": 72}, {"referenceID": 6, "context": "(See Bridle et al. (1992); Barber & Agakov (2005); Krause et al. (2010); Chen et al. (2016) for further discussion on instantiating the information term by introducing another classifier.", "startOffset": 5, "endOffset": 92}], "year": 2017, "abstractText": "Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.", "creator": "LaTeX with hyperref package"}}}