{"id": "1703.01961", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Multiplicative Normalizing Flows for Variational Bayesian Neural Networks", "abstract": "we reinterpret multiplicative noise in neural propagation removing auxiliary random variables that augment the approximate posterior over a variational setting for local neural networks. laboratory show that through this interpretation it is inherently efficient and straightforward to improve the approximation by employing normalizing flows while still allowing for local reparametrizations and a tractable lower bound. in experiments we show that with this straightforward approximation we directly significantly improve upon classical correlation field for bayesian neural networks on both predictive accuracy as well towards predictive uncertainty.", "histories": [["v1", "Mon, 6 Mar 2017 16:39:16 GMT  (970kb,D)", "http://arxiv.org/abs/1703.01961v1", "Submitted to ICML 2017"], ["v2", "Mon, 12 Jun 2017 21:05:58 GMT  (1136kb,D)", "http://arxiv.org/abs/1703.01961v2", "Appearing at the International Conference on Machine Learning (ICML) 2017"]], "COMMENTS": "Submitted to ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["christos louizos", "max welling"], "accepted": true, "id": "1703.01961"}, "pdf": {"name": "1703.01961.pdf", "metadata": {"source": "META", "title": "Multiplicative Normalizing Flows for Variational Bayesian Neural Networks", "authors": ["Christos Louizos", "Max Welling"], "emails": ["<c.louizos@uva.nl>."], "sections": [{"heading": "1. Introduction", "text": "Neural networks have been the driving force behind the success of deep learning applications. Given enough training data they are able to robustly model input-output relationships and as a result provide high predictive accuracy. However, they do have some drawbacks. In the absence of enough data they tend to overfit considerably; this restricts them from being applied in scenarios were labeled data are scarce, e.g. in medical applications such as MRI classification. Even more importantly, deep neural networks trained with maximum likelihood or MAP procedures tend to be overconfident and as a result do not provide accurate confidence intervals, particularly for inputs that are far from the training data distribution. A simple example can be seen at Figure 1a; the predictive distribution becomes overly overconfident, i.e. assigns a high softmax probability, towards the wrong class for things it hasn\u2019t seen before (e.g. an MNIST 3 rotated by 90 degrees). This in effect makes them unsuitable for applications where decisions are made, e.g. when a doctor determines the disease of a patient based on\n1University of Amsterdam, Netherlands 2TNO Intelligent Imaging, Netherlands 3Canadian Institute For Advanced Research (CIFAR). Correspondence to: Christos Louizos <c.louizos@uva.nl>.\nthe output of such a network.\nA principled approach to address both of the aforementioned shortcomings is through a Bayesian inference procedure. Under this framework instead of doing a point estimate for the network parameters we infer a posterior distribution. These distributions capture the parameter uncertainty of the network, and by subsequently integrating over them we can obtain better uncertainties about the predictions of the model. We can see that this is indeed the case at Figure 1b; the confidence of the network for the unseen digits is drastically reduced when we are using a Bayesian model, thus resulting into more realistic predictive distributions. Obtaining the posterior distributions is however no easy task, as the nonlinear nature of neural networks makes the problem intractable. For this reason approximations have to be made.\nMany works have considered the task of approximate Bayesian inference for neural networks using either Markov Chain Monte Carlo (MCMC) with Hamiltonian Dynamics (Neal, 1995), distilling SGD with Langevin Dynamics (Welling & Teh, 2011; Korattikara et al., 2015) or deterministic techniques such as the Laplace Approximation (MacKay, 1992), Expectation Propagation (Herna\u0301ndez-Lobato & Adams, 2015; Herna\u0301ndezLobato et al., 2015) and variational inference (Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016).\nIn this paper we will also tackle the problem of Bayesian inference in neural networks. We will adopt a stochastic gradient variational inference (Kingma & Welling, 2014; Rezende et al., 2014) procedure in order to estimate the posterior distribution over the weight matrices of the network. Arguably one of the most important ingredients of variational inference is the flexibility of the approximate posterior distribution; it determines how well we are able to capture the true posterior distribution and thus the true uncertainty of our models. In Section 2 we will show how we can produce very flexible distributions in an efficient way by employing auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016) and normalizing flows (Rezende & Mohamed, 2015). In Section 3 we will discuss related work, whereas in Section 4 we will evaluate and discuss ar X\niv :1\n70 3.\n01 96\n1v 1\n[ st\nat .M\nL ]\n6 M\nar 2\n01 7\nthe proposed framework. Finally we will conclude with Section 5, where we will provide some final thoughts along with promising directions for future research."}, {"heading": "2. Multiplicative normalizing flows", "text": ""}, {"heading": "2.1. Variational inference for Bayesian Neural Networks", "text": "Let D be a dataset consisting of input output pairs {(x1,y1), . . . , (xn,yn)} and let W1:L denote the weight matrices of L layers. Assuming that p(Wi), q\u03c6(Wi) are the prior and approximate posterior over the parameters of the i\u2019th layer we can derive the following lower bound on the marginal log-likelihood of the dataset D using variational Bayes (Peterson, 1987; Hinton & Van Camp, 1993; Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016):\nL(\u03c6) = Ep\u0303(x,y) [ Eq\u03c6(W1:L) [ log p(y|x,W1:L)+\n+ log p(W1:L)\u2212 log q\u03c6(W1:L) ]]\n(1)\nwhere p\u0303(x,y) denotes the training data distribution and \u03c6 the parameters of the variational posterior. For continuous q(\u00b7) distributions that allow for the reparametrization trick (Kingma & Welling, 2014) or stochastic backpropagation (Rezende et al., 2014) we can reparametrize the random sampling from q(\u00b7) of the lower bound in terms of\nnoise variables and deterministic functions f(\u03c6, ): L = Ep\u0303(x,y) [ Ep( ) [ log p(y|x, f(\u03c6, ))+\n+ log p(f(\u03c6, ))\u2212 log q\u03c6(f(\u03c6, )) ]]\n(2)\nThis reparametrization allow us to treat approximate parameter posterior inference as a straightforward optimization problem that can be optimized with off-the-shelf (stochastic) gradient ascent techniques."}, {"heading": "2.2. Improving the variational approximation", "text": "For Bayesian neural networks the most common family for the approximate posterior is that of mean field with independent Gaussian distributions for each weight. Despite the fact that this leads to a straightforward lower bound for optimization, the approximation capability is quite limiting; it corresponds to just a unimodal \u201cbump\u201d on the very high dimensional space of the parameters of the neural network. There have been attempts to improve upon this approximation with works such as (Gal & Ghahramani, 2015b) with mixtures of delta peaks and (Louizos & Welling, 2016) with matrix Gaussians that allow for nontrivial covariances among the weights. Nevertheless, both of the aforementioned methods are still, in a sense, limited; the true parameter posterior is more complex than delta peaks or correlated Gaussians.\nThere has been a lot of recent work on ways to improve the posterior approximation in latent variable models; normalizing flows (Rezende & Mohamed, 2015) and auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016)\nhave shown that we can improve the flexibility of the approximate posterior in e.g. Deep Latent Gaussian Models (Kingma & Welling, 2014; Rezende et al., 2014). Nevertheless, applying these ideas to the parameters in a neural network has not yet been explored. While it is straightforward to apply normalizing flows to a sample of the weight matrix from q(W), this quickly becomes very expensive; for example with planar flows (Rezende & Mohamed, 2015) we will need two extra matrices for each step of the flow. Furthermore, by utilizing this procedure we also lose the benefits of local reparametrizations (Kingma et al., 2015; Louizos & Welling, 2016) which are possible with Gaussian approximate posteriors.\nIn order to simultaneously maintain the benefits of local reparametrizations and increase the flexibility of the approximate posteriors in a Bayesian neural network we will rely on auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016); more specifically we will exploit the well known \u201cmultiplicative noise\u201d concept, e.g. as in (Gaussian) Dropout (Srivastava et al., 2014), in neural networks and we will parametrize the approximate posterior with the following process:\nz \u223c q\u03c6(z); W \u223c q\u03c6(W|z) (3)\nwhere now the approximate posterior becomes a compound distribution, q(W) = \u222b q(W|z)q(z)dz, with z being a vector of random variables distributed according to the mixing density q(z). To allow for local reparametrizations we will parametrize the conditional distribution for the weights to be a fully factorized Gaussian. Therefore we assume the following form for the fully connected layers:\nq\u03c6(W|z) = Din\u220f i=1 Dout\u220f j=1 N (zi\u00b5ij , \u03c32ij) (4)\nwhere Din, Dout is the input and output dimensionality, and the following form for the kernels in convolutional networks:\nq\u03c6(W|z) = Dh\u220f i=1 Dw\u220f j=1 Df\u220f k=1 N (zk\u00b5ijk, \u03c32ijk) (5)\nwhere Dh, Dw, Df are the height, width and number of filters for each kernel. Note that we did not let z affect the variance of the Gaussian approximation; in a pilot study we found that this parametrization was prone to local optima due to large variance gradients, an effect also observed with the multiplicative parametrization of the Gaussian posterior (Kingma et al., 2015; Molchanov et al., 2017). We have now reduced the problem of increasing the flexibility of the approximate posterior over the weights W to that of increasing the flexibility of the mixing density q(z). Since\nz is of much lower dimension, compared to W, it is now straightforward to apply normalizing flows to q(z); in this way we can significantly enhance our approximation and allow for e.g. multimodality and nonlinear dependencies between the elements of the weight matrix. This will in turn better capture the properties of the true posterior distribution, thus leading to better performance and predictive uncertainties. We will coin the term multiplicative normalizing flows (MNFs) for this family of approximate posteriors. Algorithms 1, 2 describe the forward pass using local reparametrizations for fully connected and convolutional layers with this type of approximate posterior.\nAlgorithm 1 Forward propagation for each fully connected layer h. Mw,\u03a3w are the means and variances of each layer, H is a minibatch of activations and NF(\u00b7) is the normalizing flow described at eq. 6. For the first layer we have that H = X where X is the minibatch of inputs.\nRequire: H,Mw,\u03a3w 1: Z0 \u223c q(z0) 2: ZTf = NF(Z0) 3: Mh = (H ZTf )Mw 4: Vh = H2\u03a3w 5: E \u223c N (0, 1) 6: return Mh + \u221a Vh E\nAlgorithm 2 Forward propagation for each convolutional layer h. Nf are the number of convolutional filters, \u2217 is the convolution operator and we assume the [batch, height, width, feature maps] convention.\nRequire: H,Mw,\u03a3w 1: z0 \u223c q(z0) 2: zTf = NF(z0) 3: Mh = H \u2217 (Mw reshape(zTf , [1, 1, Df ])) 4: Vh = H2 \u2217\u03a3w 5: E \u223c N (0, 1) 6: return Mh + \u221a Vh E\nFor the normalizing flow of q(z) we will use the masked RealNVP (Dinh et al., 2016) using the numerically stable updates introduced in Inverse Autoregressive Flow (IAF) (Kingma et al., 2016):\nm \u223c Bern(0.5); h = tanh(f(m zt)) \u00b5 = g(h); \u03c3 = \u03c3(k(h))\nzt+1 = m zt+(1\u2212m) (zt \u03c3 + (1\u2212 \u03c3) \u00b5) (6)\nlog \u2223\u2223\u2223\u2223\u2202zt+1\u2202zt \u2223\u2223\u2223\u2223 = (1\u2212m)T log\u03c3\nwhere corresponds to element-wise multiplication, \u03c3(\u00b7)\nis the sigmoid function1 and f(\u00b7), g(\u00b7), k(\u00b7) are linear mappings. We resampled the mask m every time in order to avoid a specific splitting over the dimensions of z. For the starting point of the flow q(z0) we used a simple fully factorized Gaussian and we will refer to the final iterate as zTf ."}, {"heading": "2.3. Bounding the entropy", "text": "Unfortunately, parametrizing the posterior distribution as eq. 3 makes the lower bound intractable as generally we do not have a closed form density function for q(W). This makes the calculation of the entropy \u2212Eq(W)[log q(W)] challenging. Fortunately we can make the lower bound tractable again by further lower bounding the entropy in terms of an auxiliary distribution r(z|W) (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016). This can be seen as if we are performing variational inference on the augmented probability space p(D,W1:L, z1:L), that maintains the same true posterior distribution p(W|D) (as we can always marginalize out r(z|W) to obtain the original model). The lower bound in this case becomes: L(\u03c6, \u03b8) = Ep\u0303(x,y) [ Eq\u03c6(z1:L,W1:L) [ log p(y|x,W1:L, z1:L)+\n+ log p(W1:L) + log r\u03b8(z1:L|W1:L)\u2212 \u2212 log q\u03c6(W1:L|z1:L)\u2212 log q\u03c6(z1:L) ]]\n(7)\nwhere \u03b8 are the parameters of the auxiliary distribution r(\u00b7). This bound is looser than the previous bound, however the extra flexibility of q(W) can compensate and allow for a tighter bound. Furthermore, the tightness of the bound also depends on the ability of r(z|W) to approximate the \u201cauxiliary\u201d posterior distribution q(z|W) = q(W|z)q(z)q(W) . Therefore, to allow for a flexible r(z|W) we will follow (Ranganath et al., 2015) and we will parametrize it with inverse normalizing flows as follows:\nr(zTb |W) = Dz\u220f i=1 N (\u00b5\u0303i, \u03c3\u03032i ) (8)\nwhere for fully connected layers we have that:\n\u00b5\u0303i = ( b1 \u2297 tanh(cTW) ) (1 D\u22121out) (9)\n\u03c3\u0303i = \u03c3 (( b2 \u2297 tanh(cTW) ) (1 D\u22121out) ) (10)\n1f(x) = 1 1+exp(\u2212x)\nand for convolutional:\n\u00b5\u0303i = ( tanh(mat(W)c)\u2297 b1 ) (1 (DhDw)\u22121) (11)\n\u03c3\u0303i = \u03c3 (( tanh(mat(W)c)\u2297 b2 ) (1 (DhDw)\u22121) ) (12)\nwhere b1,b2, c are trainable vectors that have the same dimensionality as z, Dz , 1 corresponds to a vector of 1s, \u2297 corresponds to the outer product and mat(\u00b7) corresponds to the matricization2 operator. The zTb variable corresponds to the fully factorized variable that is transformed by a normalizing flow to zTf or else the variable obtained by the inverse normalizing flow, zTb = NF\n\u22121(zTf ). We will parametrize this inverse directly with the procedure described at eq. 6. Notice that we can employ local reparametrizations also in eq. 9,10,11,12, so as to avoid sampling the, potentially big, matrix W. With the standard normal prior and the fully factorized Gaussian posterior of eq. 4 the KL-divergence between the prior and the posterior can be computed as follows:\n\u2212KL(q(W)||p(W)) = = Eq(W,zT )[\u2212KL(q(W|zTf )||p(W))+ + log r(zTf |W)\u2212 log q(zTf )] (13)\nwhere each of the terms corresponds to:\n\u2212KL(q(W|zTf )||p(W)) =\n= 1\n2 \u2211 i,j (\u2212 log \u03c32i,j + \u03c32i,j + z2Tfi\u00b5 2 i,j \u2212 1) (14)\nlog r(zTf |W) = log r(zTb |W) + Tf+Tb\u2211 t=Tf log \u2223\u2223\u2223\u2223\u2202zt+1\u2202zt \u2223\u2223\u2223\u2223\n(15)\nlog q(zTf ) = log q(z0)\u2212 Tf\u2211 t=1 log \u2223\u2223\u2223\u2223\u2202zt+1\u2202zt \u2223\u2223\u2223\u2223 (16)\nIt should be noted that this bound is a generalization of the bound proposed by (Gal & Ghahramani, 2015b). We can arrive at the bound of (Gal & Ghahramani, 2015b) if we trivially parametrize the auxiliary model r(z|W) = q(z) (which provides a less tight bound (Ranganath et al., 2015)) use a standard normal prior for W, a Bernoulli q(z) with probability of success \u03c0 and then let the variance of our conditional Gaussian q(W|z) go to zero. This will result into the lower bound being infinite due to the log of the variances; nevertheless since we are not optimizing over \u03c3 we can simply disregard those terms. After a little bit of algebra we can show that the only term that will remain in the\n2Converting the multidimensional tensor to a matrix.\nKL-divergence between q(W) and p(W) will be the expectation of the trace of the square of the mean matrix3, i.e. Eq(z)[ 12 tr((diag(z)M)) T (diag(z)M))] = \u03c02 \u2016M\u2016 2 2, with 1\u2212 \u03c0 being the dropout rate.\nWe also found that in general it is beneficial to \u201cconstrain\u201d the standard deviations \u03c3ij of the conditional Gaussian posterior q(W|z) during the forward pass for the computation of the likelihood to a lower than the true range, e.g. [0, \u03b1] instead of the [0, 1] we have with a standard normal prior. This results into a small bias and a looser lower bound, however it helps in avoiding bad local minima in the variational objective. This is akin to the free bits objective described at (Kingma et al., 2016)."}, {"heading": "3. Related work", "text": "Approximate inference for Bayesian neural networks has been pioneered by (MacKay, 1992) and (Neal, 1995). Laplace approximation (MacKay, 1992) provides a deterministic approximation to the posterior that is easy to obtain; it is a Gaussian centered at the MAP estimate of the parameters with a covariance determined by the inverse of the Hessian of the log-likelihood. Despite the fact that it is straightforward to implement, its scalability is limited unless approximations are made, which generally reduces performance. Hamiltonian Monte Carlo (Neal, 1995) is so far the golden standard for approximate Bayesian inference; nevertheless it is also not scalable to large networks and datasets due to the fact that we have to explicitly store the samples from the posterior. Furthermore as it is an MCMC method, assessing convergence is non trivial. Nevertheless there is interesting work that tries to improve upon those issues with stochastic gradient MCMC (Chen et al.) and distillation methods (Korattikara et al., 2015).\nDeterministic methods for approximate inference in Bayesian neural networks have recently attained much attention. One of the first applications of variational inference in neural networks was in (Peterson, 1987) and (Hinton & Van Camp, 1993). More recently (Graves, 2011) proposed a practical method for variational inference in this setting with a simple (but biased) estimator for a fully factorized posterior distribution. (Blundell et al., 2015) improved upon this work with an unbiased estimator and a scale mixture prior. (Herna\u0301ndez-Lobato & Adams, 2015) proposed to use Expectation Propagation (Minka, 2001) with fully factorized posteriors and showed good results on regression tasks. (Kingma et al., 2015) showed how Gaussian dropout can be interpreted as performing approximate inference with log-uniform priors, multiplicative Gaussian posteriors and local reparametrizations, thus allowing straightforward learning of the dropout rates.\n3The matrix that has M[i, j] = \u00b5ij\nSimilarly (Gal & Ghahramani, 2015b) showed interesting connections between Bernoulli Dropout (Srivastava et al., 2014) networks and approximate Bayesian inference in deep Gaussian Processes (Damianou & Lawrence, 2013) thus allowing the extraction of uncertainties in a principled way. Similarly (Louizos & Welling, 2016) arrived at the same result through structured posterior approximations via matrix Gaussians and local reparametrizations (Kingma et al., 2015).\nIt should also be mentioned that uncertainty estimation in neural networks can also be performed without the Bayesian paradigm; frequentist methods such as Bootstrap (Osband et al., 2016) and ensembles (Lakshminarayanan et al., 2016) have shown that in certain scenarios they can provide reasonable confidence intervals."}, {"heading": "4. Experiments", "text": "All of the experiments were coded in Tensorflow (Abadi et al., 2016) and optimization was done with Adam (Kingma & Ba, 2015) using the default hyperparameters. We used the LeNet 54 (LeCun et al., 1998) convolutional architecture with ReLU (Nair & Hinton, 2010) nonlinearities. The means M of the conditional Gaussian q(W|z) were initialized with the scheme proposed in (He et al., 2015), whereas the log of the variances were initialized by sampling from N (\u22129, 0.001). Unless explicitly mentioned otherwise we use flows of length two for q(z) and r(z|W) with 50 hidden units for each step of the flow of q(z) and 100 hidden units for each step of the flow of r(z|W). We used 100 posterior samples to estimate the predictive distribution for all of the models during testing and 1 posterior sample during training."}, {"heading": "4.1. Predictive performance and uncertainty", "text": "MNIST We trained on MNIST LeNet architectures using the priors and posteriors described at Table 1. We trained Dropout with the way described at (Gal & Ghahramani, 2015a) using 0.5 for the dropout rate and for Deep Ensembles (Lakshminarayanan et al., 2016) we used 10 members and = .25 for the adversarial example generation. For the models with the Gaussian prior we constrained the standard deviation of the conditional posterior to be \u2264 .5 during the forward pass. The classification performance of each model can be seen at Table 2; while our overall focus is not classification accuracy per se, we see that with the MNF posteriors we improve upon mean field reaching similar accuracies with Deep Ensembles.\nnotMNIST To evaluate the predictive uncertainties of each model we performed the task described at (Lakshminarayanan et al., 2016); we estimated the entropy of the predictive distributions on notMNIST5 from the LeNet architectures trained on MNIST. Since we a-priori know that none of the notMNIST classes correspond to a trained class (since they are letters and not digits) the ideal predictive distribution is uniform over the MNIST digits, i.e. a maximum entropy distribution. Contrary to (Lakshminarayanan et al., 2016) we do not plot the histogram of the entropies across the images but we instead use the empirical CDF, which we think is more informative. Curves that are closer to the bottom right part of the plot are preferable, as it denotes that the probability of observing a high confidence prediction is low. At Figure 2 we show the empirical CDF over the range of possible entropies, [0, 2.5], for all of the models.\nIt is clear from the plot that the uncertainty estimates from MNFs are better than the other approaches, since the prob-\n5Can be found at http://yaroslavvb.blogspot.co.uk/2011/09/notmnistdataset.html\nability of a low entropy prediction is overall lower. The network trained with just weight decay was, as expected, the most overconfident with an almost zero median entropy while Dropout seems to be in the middle ground. The Bayesian neural net with the log-uniform prior also showed overconfidence in this task; we hypothesize that this is due to the induced sparsity (Molchanov et al., 2017) which results into the pruning of almost all irrelevant sources of variation in the parameters thus not providing enough variability to allow for uncertainty in the predictions. The sparsity levels6 are 62%, 95.2% for the two convolutional layers and 99.5%, 93.3% for the two fully connected. Similar effects would probably be also observed if we optimized the dropout rates for Dropout. The only source of randomness in the neural network is from the Bernoulli random variables (r.v.) z. By employing the Central Limit Theorem7 we can express the distribution of the activations as a Gaussian (Wang & Manning, 2013) with variance affected by the variance of the Bernoulli r.v., V(z) = \u03c0(1\u2212\u03c0). The maximum variance of the Bernoulli r.v. is when \u03c0 = 0.5, therefore any tuning of the Dropout rate will result into a decrease in the variance of the r.v. and therefore a decrease in the variance of the Gaussian at the hidden units. This will subsequently lead into less predictive variance and more confidence.\nFinally, whereas it was shown at (Lakshminarayanan et al., 2016) that Deep Ensembles provide good uncertainty estimates (better than Dropout) on this task using fully connected networks, this result did not seem to apply for the LeNet architecture we considered. We hypothesize that they are sensitive to the hyperparameters (e.g. adversarial noise, number of members in the ensemble) and it requires more tuning in order to improve upon Dropout on this architecture.\nCIFAR 10 We performed a similar experiment on CIFAR 10. To artificially create the \u201dunobserved class\u201d scenario, we hid 5 of the labels (dog, frog, horse, ship, truck) and trained on the rest (airplane, automobile, bird, cat, deer). For this task we used the larger LeNet architecture8 described at (Gal & Ghahramani, 2015a). For the models with the Gaussian prior we similarly constrained the standard deviation during the forward pass to be\u2264 .4. For Deep Ensembles we used five members with = .1 for the adversarial example generation. The predictive performance on these five classes can be seen in Table 2, with Dropout and MNFs achieving the overall better accuracies. We subsequently measured the entropy of the predictive distribution\n6Computed by pruning weights where log \u03c32 \u2212 log\u00b52 \u2265 5 (Molchanov et al., 2017).\n7Assuming that the network is wide enough. 8192 filters at each convolutional layer and 1000 hidden units\nfor the fully connected layer.\non the classes that were hidden, with the resulting empirical CDFs visualized in Figure 3.\nWe similarly observe that the network with just weight decay was the most overconfident. Furthermore, Deep Ensembles and Dropout had similar uncertainties, with Deep Ensembles having lower accuracy on the observed classes. The networks with the Gaussian priors also had similar uncertainty with the network with the log uniform prior, nevertheless the MNF posterior had much better accuracy on the observed classes. The sparsity levels for the network with the log-uniform prior now were 94.9%, 99.8% for the convolutional layers and 99.9%, 92.7% for the fully connected. Overall, the network with the MNF posteriors seem to provide the better trade-off in uncertainty and accuracy on the observed classes."}, {"heading": "4.2. Accuracy and uncertainty on adversarial examples", "text": "We also measure how robust our models and uncertainties are against adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014) by generating examples using the fast sign method (Goodfellow et al., 2014) for each of the previously trained architectures using Cleverhans (Papernot et al., 2016). For this task we do not include Deep Ensembles as they are trained on adversarial examples.\nMNIST On this scenario we observe interesting results if we plot the change in accuracy and entropy by varying the magnitude of the adversarial perturbation. The resulting plot can be seen in Figure 4. Overall Dropout seems to have better accuracies on adversarial examples; never-\ntheless, those come at an \u201doverconfident\u201d price since the entropy of the predictive distributions is quite low thus resulting into predictions that have, on average, above 0.7 probability for the dominant class. This is in contrast with MNFs; while the accuracy almost immediately drops close to random, the uncertainty simultaneously increases to almost maximum entropy. This implies that the predictive distribution is more or less uniform over those examples. So despite the fact that our model cannot overcome adversarial examples at least it \u201cknows that it doesn\u2019t know\u201d.\nCIFAR We performed the same experiment also on the five class subset of CIFAR 10. The results can be seen in Figure 5. Here we however observe a different picture, compared to MNIST, since all of the methods experienced overconfidence. We hypothesize that adversarial examples are harder to escape and be uncertain about in this dataset, due to the higher dimensionality, and therefore further investigation is needed."}, {"heading": "4.3. Regression on toy dataset", "text": "For the final experiment we visualize the predictive distributions obtained with the different models on the toy regression task introduced at (Herna\u0301ndez-Lobato & Adams, 2015). We generated 20 training inputs from U [\u22124, 4] and then obtained the corresponding targets via y = x3 + , where \u223c N (0, 9). We fixed the likelihood noise to its true value and then fitted a Dropout network with \u03c0 = 0.5 for the hidden layer9, an FFLU network and an MNFG. We also fitted a Dropout network where we also learned the dropout probability \u03c0 of the hidden layer according to the bound described at section 2.3 (which is equivalent to the one described at (Gal & Ghahramani, 2015b)) using REINFORCE (Williams, 1992) and a global baseline (Mnih & Gregor, 2014). The resulting predictive distributions can be seen at Figure 6.\nAs we can observe, MNF posteriors provide more realistic predictive distributions, closer to the true posterior (which can be seen at (Herna\u0301ndez-Lobato & Adams, 2015)) and with the network being more uncertain on areas where we do not observed any data. The uncertainties obtained by Dropout with fixed \u03c0 = 0.5 did not diverge as much in those areas but overall they were better compared to the uncertainties obtained with FFLU. We could probably attribute the latter to the sparsification of the network since 95% and 44% of the parameters were pruned for each layer respectively.\nInterestingly the uncertainties obtained with the network with the learned Dropout probability were the most \u201coverfitted\u201d. This might suggest that Dropout uncertainty is probably not a good posterior approximation since by optimizing the dropout rates we do not seem to move closer to the true posterior predictive distribution. This is in contrast with MNFs; they are flexible enough to allow for optimizing all of their parameters in a way that does better approximate the true posterior distribution. This result also empirically verifies the claim we previously made; by learning the dropout rates the entropy of the posterior predictive will de-\n9No Dropout was used for the input layer since it is 1- dimensional.\ncrease thus resulting into more overconfident predictions."}, {"heading": "5. Conclusion", "text": "We introduce multiplicative normalizing flows (MNFs); a family of approximate posteriors for the parameters of a variational Bayesian neural network. We have shown that through this approximation we can significantly improve upon mean field on both predictive performance as well as predictive uncertainty. We compared our uncertainty on notMNIST and CIFAR with Dropout (Srivastava et al., 2014; Gal & Ghahramani, 2015b) and Deep Ensembles (Lakshminarayanan et al., 2016) using convolutional architectures and found that MNFs achieve more realistic uncertainties while providing predictive capabilities on par with Dropout. We suspect that the predictive capabilities of MNFs can be further improved through more appropriate optimizers that avoid the bad local minima in the variational objective. Finally, we also highlighted limitations of Dropout approximations and empirically showed that MNFs can overcome them.\nThere are a couple of promising directions for future research. One avenue would be to explore how much can MNFs sparsify and compress neural networks under either sparsity inducing priors, such as the log-uniform prior (Kingma et al., 2015; Molchanov et al., 2017), or empirical priors (Ullrich et al., 2017). Another promising direction is that of designing better priors for Bayesian neural networks. For example (Neal, 1995) has identified limitations of Gaussian priors and proposes alternative priors such as the Cauchy. Furthermore, the prior over the parameters also affects the type of uncertainty we get in our predictions; for instance we observed in our experiments a significant difference in uncertainty between Gaussian and log-uniform priors. Since different problems require different types of uncertainty it makes sense to choose the prior accordingly, e.g. use an informative prior so as to alleviate adversarial examples."}, {"heading": "Acknowledgements", "text": "We would like to thank Klamer Schutte, Matthias Reisser and Karen Ullrich for valuable feedback. This research is supported by TNO, Scyfer B.V., NWO, Google and Facebook."}], "references": [{"title": "An auxiliary variational method", "author": ["Agakov", "Felix V", "Barber", "David"], "venue": "In International Conference on Neural Information Processing,", "citeRegEx": "Agakov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Agakov et al\\.", "year": 2004}, {"title": "Weight uncertainty in neural networks", "author": ["Blundell", "Charles", "Cornebise", "Julien", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "Blundell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2015}, {"title": "Deep gaussian processes", "author": ["Damianou", "Andreas C", "Lawrence", "Neil D"], "venue": "In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Damianou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Damianou et al\\.", "year": 2013}, {"title": "Density estimation using real nvp", "author": ["Dinh", "Laurent", "Sohl-Dickstein", "Jascha", "Bengio", "Samy"], "venue": "arXiv preprint arXiv:1605.08803,", "citeRegEx": "Dinh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Bayesian convolutional neural networks with bernoulli approximate variational inference", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "arXiv preprint arXiv:1506.02158,", "citeRegEx": "Gal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2015}, {"title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "arXiv preprint arXiv:1506.02142,", "citeRegEx": "Gal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Practical variational inference for neural networks", "author": ["Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves and Alex.,? \\Q2011\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2011}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Probabilistic backpropagation for scalable learning of bayesian neural networks", "author": ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Adams", "Ryan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2015}, {"title": "Black-box \u03b1-divergence minimization", "author": ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Li", "Yingzhen", "Daniel", "Bui", "Thang", "Turner", "Richard E"], "venue": "arXiv preprint arXiv:1511.03243,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2015}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["Hinton", "Geoffrey E", "Van Camp", "Drew"], "venue": "In Proceedings of the sixth annual conference on Computational learning theory,", "citeRegEx": "Hinton et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1993}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "International Conference on Learning Representations (ICLR), San Diego,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Variational dropout and the local reparametrization trick", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "arXiv preprint arXiv:1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Bayesian dark knowledge", "author": ["Korattikara", "Anoop", "Rathod", "Vivek", "Murphy", "Kevin", "Welling", "Max"], "venue": "arXiv preprint arXiv:1506.04416,", "citeRegEx": "Korattikara et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Korattikara et al\\.", "year": 2015}, {"title": "Simple and scalable predictive uncertainty estimation using deep ensembles", "author": ["Lakshminarayanan", "Balaji", "Pritzel", "Alexander", "Blundell", "Charles"], "venue": "arXiv preprint arXiv:1612.01474,", "citeRegEx": "Lakshminarayanan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lakshminarayanan et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Structured and efficient variational deep learning with matrix gaussian posteriors", "author": ["Louizos", "Christos", "Welling", "Max"], "venue": "arXiv preprint arXiv:1603.04733,", "citeRegEx": "Louizos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Louizos et al\\.", "year": 2016}, {"title": "Auxiliary deep generative models", "author": ["Maal\u00f8e", "Lars", "S\u00f8nderby", "Casper Kaae", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "arXiv preprint arXiv:1602.05473,", "citeRegEx": "Maal\u00f8e et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maal\u00f8e et al\\.", "year": 2016}, {"title": "A practical bayesian framework for backpropagation networks", "author": ["MacKay", "David JC"], "venue": "Neural computation,", "citeRegEx": "MacKay and JC.,? \\Q1992\\E", "shortCiteRegEx": "MacKay and JC.", "year": 1992}, {"title": "Expectation propagation for approximate bayesian inference", "author": ["Minka", "Thomas P"], "venue": "In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Minka and P.,? \\Q2001\\E", "shortCiteRegEx": "Minka and P.", "year": 2001}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Bayesian learning for neural networks", "author": ["Neal", "Radford M"], "venue": "PhD thesis, Citeseer,", "citeRegEx": "Neal and M.,? \\Q1995\\E", "shortCiteRegEx": "Neal and M.", "year": 1995}, {"title": "Deep exploration via bootstrapped dqn", "author": ["Osband", "Ian", "Blundell", "Charles", "Pritzel", "Alexander", "Van Roy", "Benjamin"], "venue": "arXiv preprint arXiv:1602.04621,", "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "cleverhans v1.0.0: an adversarial machine learning library", "author": ["Papernot", "Nicolas", "Goodfellow", "Ian", "Sheatsley", "Ryan", "Feinman", "Reuben", "McDaniel", "Patrick"], "venue": "arXiv preprint arXiv:1610.00768,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "A mean field theory learning algorithm for neural networks", "author": ["Peterson", "Carsten"], "venue": "Complex systems,", "citeRegEx": "Peterson and Carsten.,? \\Q1987\\E", "shortCiteRegEx": "Peterson and Carsten.", "year": 1987}, {"title": "Hierarchical variational models", "author": ["Ranganath", "Rajesh", "Tran", "Dustin", "Blei", "David M"], "venue": "arXiv preprint arXiv:1511.02386,", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Fixed-form variational posterior approximation through stochastic linear regression", "author": ["Salimans", "Tim", "Knowles", "David A"], "venue": "Bayesian Analysis,", "citeRegEx": "Salimans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2013}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Soft weight-sharing for neural network compression", "author": ["Ullrich", "Karen", "Meeds", "Edward", "Welling", "Max"], "venue": "arXiv preprint arXiv:1702.04008,", "citeRegEx": "Ullrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ullrich et al\\.", "year": 2017}, {"title": "Fast dropout training", "author": ["Wang", "Sida", "Manning", "Christopher"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["Welling", "Max", "Teh", "Yee W"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 12, "context": "We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows (Rezende & Mohamed, 2015) while still allowing for local reparametrizations (Kingma et al., 2015) and a tractable lower bound (Ranganath et al.", "startOffset": 218, "endOffset": 239}, {"referenceID": 29, "context": ", 2015) and a tractable lower bound (Ranganath et al., 2015; Maal\u00f8e et al., 2016).", "startOffset": 36, "endOffset": 81}, {"referenceID": 20, "context": ", 2015) and a tractable lower bound (Ranganath et al., 2015; Maal\u00f8e et al., 2016).", "startOffset": 36, "endOffset": 81}, {"referenceID": 16, "context": "Many works have considered the task of approximate Bayesian inference for neural networks using either Markov Chain Monte Carlo (MCMC) with Hamiltonian Dynamics (Neal, 1995), distilling SGD with Langevin Dynamics (Welling & Teh, 2011; Korattikara et al., 2015) or deterministic techniques such as the Laplace Approximation (MacKay, 1992), Expectation Propagation (Hern\u00e1ndez-Lobato & Adams, 2015; Hern\u00e1ndezLobato et al.", "startOffset": 213, "endOffset": 260}, {"referenceID": 1, "context": ", 2015) and variational inference (Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016).", "startOffset": 34, "endOffset": 142}, {"referenceID": 12, "context": ", 2015) and variational inference (Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016).", "startOffset": 34, "endOffset": 142}, {"referenceID": 31, "context": "We will adopt a stochastic gradient variational inference (Kingma & Welling, 2014; Rezende et al., 2014) procedure in order to estimate the posterior distribution over the weight matrices of the network.", "startOffset": 58, "endOffset": 104}, {"referenceID": 32, "context": "In Section 2 we will show how we can produce very flexible distributions in an efficient way by employing auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016) and normalizing flows (Rezende & Mohamed, 2015).", "startOffset": 133, "endOffset": 224}, {"referenceID": 29, "context": "In Section 2 we will show how we can produce very flexible distributions in an efficient way by employing auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016) and normalizing flows (Rezende & Mohamed, 2015).", "startOffset": 133, "endOffset": 224}, {"referenceID": 20, "context": "In Section 2 we will show how we can produce very flexible distributions in an efficient way by employing auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016) and normalizing flows (Rezende & Mohamed, 2015).", "startOffset": 133, "endOffset": 224}, {"referenceID": 1, "context": "Assuming that p(Wi), q\u03c6(Wi) are the prior and approximate posterior over the parameters of the i\u2019th layer we can derive the following lower bound on the marginal log-likelihood of the dataset D using variational Bayes (Peterson, 1987; Hinton & Van Camp, 1993; Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016):", "startOffset": 218, "endOffset": 367}, {"referenceID": 12, "context": "Assuming that p(Wi), q\u03c6(Wi) are the prior and approximate posterior over the parameters of the i\u2019th layer we can derive the following lower bound on the marginal log-likelihood of the dataset D using variational Bayes (Peterson, 1987; Hinton & Van Camp, 1993; Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016):", "startOffset": 218, "endOffset": 367}, {"referenceID": 31, "context": "For continuous q(\u00b7) distributions that allow for the reparametrization trick (Kingma & Welling, 2014) or stochastic backpropagation (Rezende et al., 2014) we can reparametrize the random sampling from q(\u00b7) of the lower bound in terms of noise variables and deterministic functions f(\u03c6, ):", "startOffset": 132, "endOffset": 154}, {"referenceID": 32, "context": "There has been a lot of recent work on ways to improve the posterior approximation in latent variable models; normalizing flows (Rezende & Mohamed, 2015) and auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016)", "startOffset": 185, "endOffset": 276}, {"referenceID": 29, "context": "There has been a lot of recent work on ways to improve the posterior approximation in latent variable models; normalizing flows (Rezende & Mohamed, 2015) and auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016)", "startOffset": 185, "endOffset": 276}, {"referenceID": 20, "context": "There has been a lot of recent work on ways to improve the posterior approximation in latent variable models; normalizing flows (Rezende & Mohamed, 2015) and auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maal\u00f8e et al., 2016)", "startOffset": 185, "endOffset": 276}, {"referenceID": 31, "context": "Deep Latent Gaussian Models (Kingma & Welling, 2014; Rezende et al., 2014).", "startOffset": 28, "endOffset": 74}, {"referenceID": 12, "context": "Furthermore, by utilizing this procedure we also lose the benefits of local reparametrizations (Kingma et al., 2015; Louizos & Welling, 2016) which are possible with Gaussian approximate posteriors.", "startOffset": 95, "endOffset": 141}, {"referenceID": 32, "context": "In order to simultaneously maintain the benefits of local reparametrizations and increase the flexibility of the approximate posteriors in a Bayesian neural network we will rely on auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016); more specifically we will exploit the well known \u201cmultiplicative noise\u201d concept, e.", "startOffset": 208, "endOffset": 305}, {"referenceID": 29, "context": "In order to simultaneously maintain the benefits of local reparametrizations and increase the flexibility of the approximate posteriors in a Bayesian neural network we will rely on auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016); more specifically we will exploit the well known \u201cmultiplicative noise\u201d concept, e.", "startOffset": 208, "endOffset": 305}, {"referenceID": 20, "context": "In order to simultaneously maintain the benefits of local reparametrizations and increase the flexibility of the approximate posteriors in a Bayesian neural network we will rely on auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016); more specifically we will exploit the well known \u201cmultiplicative noise\u201d concept, e.", "startOffset": 208, "endOffset": 305}, {"referenceID": 12, "context": "Note that we did not let z affect the variance of the Gaussian approximation; in a pilot study we found that this parametrization was prone to local optima due to large variance gradients, an effect also observed with the multiplicative parametrization of the Gaussian posterior (Kingma et al., 2015; Molchanov et al., 2017).", "startOffset": 279, "endOffset": 324}, {"referenceID": 3, "context": "For the normalizing flow of q(z) we will use the masked RealNVP (Dinh et al., 2016) using the numerically stable updates introduced in Inverse Autoregressive Flow (IAF) (Kingma et al.", "startOffset": 64, "endOffset": 83}, {"referenceID": 15, "context": ", 2016) using the numerically stable updates introduced in Inverse Autoregressive Flow (IAF) (Kingma et al., 2016):", "startOffset": 93, "endOffset": 114}, {"referenceID": 32, "context": "Fortunately we can make the lower bound tractable again by further lower bounding the entropy in terms of an auxiliary distribution r(z|W) (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016).", "startOffset": 139, "endOffset": 236}, {"referenceID": 29, "context": "Fortunately we can make the lower bound tractable again by further lower bounding the entropy in terms of an auxiliary distribution r(z|W) (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016).", "startOffset": 139, "endOffset": 236}, {"referenceID": 20, "context": "Fortunately we can make the lower bound tractable again by further lower bounding the entropy in terms of an auxiliary distribution r(z|W) (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maal\u00f8e et al., 2016).", "startOffset": 139, "endOffset": 236}, {"referenceID": 29, "context": "Therefore, to allow for a flexible r(z|W) we will follow (Ranganath et al., 2015) and we will parametrize it with inverse normalizing flows as follows:", "startOffset": 57, "endOffset": 81}, {"referenceID": 29, "context": "We can arrive at the bound of (Gal & Ghahramani, 2015b) if we trivially parametrize the auxiliary model r(z|W) = q(z) (which provides a less tight bound (Ranganath et al., 2015)) use a standard normal prior for W, a Bernoulli q(z) with probability of success \u03c0 and then let the variance of our conditional Gaussian q(W|z) go to zero.", "startOffset": 153, "endOffset": 177}, {"referenceID": 15, "context": "This is akin to the free bits objective described at (Kingma et al., 2016).", "startOffset": 53, "endOffset": 74}, {"referenceID": 16, "context": ") and distillation methods (Korattikara et al., 2015).", "startOffset": 27, "endOffset": 53}, {"referenceID": 1, "context": "(Blundell et al., 2015) improved upon this work with an unbiased estimator and a scale mixture prior.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "(Kingma et al., 2015) showed how Gaussian dropout can be interpreted as performing approximate inference with log-uniform priors, multiplicative Gaussian posteriors and local reparametrizations, thus allowing straightforward learning of the dropout rates.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "Similarly (Louizos & Welling, 2016) arrived at the same result through structured posterior approximations via matrix Gaussians and local reparametrizations (Kingma et al., 2015).", "startOffset": 157, "endOffset": 178}, {"referenceID": 26, "context": "It should also be mentioned that uncertainty estimation in neural networks can also be performed without the Bayesian paradigm; frequentist methods such as Bootstrap (Osband et al., 2016) and ensembles (Lakshminarayanan et al.", "startOffset": 166, "endOffset": 187}, {"referenceID": 17, "context": ", 2016) and ensembles (Lakshminarayanan et al., 2016) have shown that in certain scenarios they can provide reasonable confidence intervals.", "startOffset": 22, "endOffset": 53}, {"referenceID": 18, "context": "We used the LeNet 54 (LeCun et al., 1998) convolutional architecture with ReLU (Nair & Hinton, 2010) nonlinearities.", "startOffset": 21, "endOffset": 41}, {"referenceID": 8, "context": "The means M of the conditional Gaussian q(W|z) were initialized with the scheme proposed in (He et al., 2015), whereas the log of the variances were initialized by sampling from N (\u22129, 0.", "startOffset": 92, "endOffset": 109}, {"referenceID": 17, "context": "ble to the model used in (Lakshminarayanan et al., 2016), FFG to", "startOffset": 25, "endOffset": 56}, {"referenceID": 1, "context": "the Bayesian neural network employed in (Blundell et al., 2015),", "startOffset": 40, "endOffset": 63}, {"referenceID": 6, "context": "Ensembles use adversarial training (Goodfellow et al., 2014).", "startOffset": 35, "endOffset": 60}, {"referenceID": 17, "context": "5 for the dropout rate and for Deep Ensembles (Lakshminarayanan et al., 2016) we used 10 members and = .", "startOffset": 46, "endOffset": 77}, {"referenceID": 17, "context": "notMNIST To evaluate the predictive uncertainties of each model we performed the task described at (Lakshminarayanan et al., 2016); we estimated the entropy of the predictive distributions on notMNIST5 from the LeNet architectures trained on MNIST.", "startOffset": 99, "endOffset": 130}, {"referenceID": 17, "context": "Contrary to (Lakshminarayanan et al., 2016) we do not plot the histogram of the entropies across the images but we instead use the empirical CDF, which we think is more informative.", "startOffset": 12, "endOffset": 43}, {"referenceID": 17, "context": "Finally, whereas it was shown at (Lakshminarayanan et al., 2016) that Deep Ensembles provide good uncertainty estimates (better than Dropout) on this task using fully connected networks, this result did not seem to apply for the LeNet architecture we considered.", "startOffset": 33, "endOffset": 64}, {"referenceID": 35, "context": "We also measure how robust our models and uncertainties are against adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014) by generating examples using the fast sign method (Goodfellow et al.", "startOffset": 89, "endOffset": 136}, {"referenceID": 6, "context": "We also measure how robust our models and uncertainties are against adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014) by generating examples using the fast sign method (Goodfellow et al.", "startOffset": 89, "endOffset": 136}, {"referenceID": 6, "context": ", 2014) by generating examples using the fast sign method (Goodfellow et al., 2014) for each of the previously trained architectures using Cleverhans (Papernot et al.", "startOffset": 58, "endOffset": 83}, {"referenceID": 27, "context": ", 2014) for each of the previously trained architectures using Cleverhans (Papernot et al., 2016).", "startOffset": 74, "endOffset": 97}, {"referenceID": 17, "context": ", 2014; Gal & Ghahramani, 2015b) and Deep Ensembles (Lakshminarayanan et al., 2016) using convolutional architectures and found that MNFs achieve more realistic uncertainties while providing predictive capabilities on par with Dropout.", "startOffset": 52, "endOffset": 83}, {"referenceID": 12, "context": "One avenue would be to explore how much can MNFs sparsify and compress neural networks under either sparsity inducing priors, such as the log-uniform prior (Kingma et al., 2015; Molchanov et al., 2017), or empirical priors (Ullrich et al.", "startOffset": 156, "endOffset": 201}, {"referenceID": 36, "context": ", 2017), or empirical priors (Ullrich et al., 2017).", "startOffset": 29, "endOffset": 51}], "year": 2017, "abstractText": "We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows (Rezende & Mohamed, 2015) while still allowing for local reparametrizations (Kingma et al., 2015) and a tractable lower bound (Ranganath et al., 2015; Maal\u00f8e et al., 2016). In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.", "creator": "LaTeX with hyperref package"}}}