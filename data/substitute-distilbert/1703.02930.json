{"id": "1703.02930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Nearly-tight VC-dimension bounds for piecewise linear neural networks", "abstract": "we prove new upper limited lower bounds on the vc - dimension of deep distribution networks with the relu activation function. these bounds are tight for almost the entire range without parameters. letting $ w $ include the number of weights \u03b4 $ l $ be the number of layers, we prove that the vc - item is $ o ( w l \\ log ( w ) ) $ and $ \\ omega ( w l \\ log ( ng / l ) ) $. this improves both the previously known upper bounds and lower bounds. in terms describing generating number $ u $ of non - linear units, we proved a tight bound $ \\ rn ( w \u03b8 ) $ on the vc - scale. all of certain results generalize to arbitrary piecewise linear algebraic functions.", "histories": [["v1", "Wed, 8 Mar 2017 17:35:17 GMT  (75kb,D)", "http://arxiv.org/abs/1703.02930v1", "11 pages, 2 figures"], ["v2", "Sun, 4 Jun 2017 19:13:36 GMT  (89kb,D)", "http://arxiv.org/abs/1703.02930v2", "Accepted for presentation at Conference on Learning Theory (COLT) 2017. Minor corrections from first version, 12 pages, 2 figures"], ["v3", "Mon, 16 Oct 2017 01:29:59 GMT  (96kb,D)", "http://arxiv.org/abs/1703.02930v3", "Extended abstract appeared in COLT 2017; the upper bound was presented at the 2016 ACM Conference on Data Science. This version includes all the proofs and a refinement of the upper bound, Theorem 6. 16 pages, 2 figures"]], "COMMENTS": "11 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nick harvey", "chris liaw", "abbas mehrabian"], "accepted": false, "id": "1703.02930"}, "pdf": {"name": "1703.02930.pdf", "metadata": {"source": "CRF", "title": "Nearly-tight VC-dimension bounds for piecewise linear neural networks", "authors": ["Nick Harvey", "Chris Liaw", "Abbas Mehrabian"], "emails": ["nickhar@cs.ubc.ca", "cvliaw@cs.ubc.ca", "abbasmehrabian@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks underlie many of the recent breakthroughs of applied machine learning, particularly in image and speech recognition. These successes motivate a renewed study of these networks\u2019 theoretical properties.\nClassification is one of the learning tasks in which deep neural networks have been particularly successful, e.g., for image recognition. A natural foundational question that arises is: what are the theoretical limits on the classification power of these networks? The established way to formalize this question is by considering VCdimension, as it is well known that this asymptotically determines the sample complexity of PAC learning with such classifiers [3].\nIn this paper, we prove nearly-tight bounds on the VC-dimension of deep neural networks in which the non-linear activation function is a piecewise linear function with a constant number of pieces. For simplicity we will henceforth refer to such networks as \u201cpiecewise linear networks\u201d. The most common activation function used in practice is, by far, the rectified linear unit, also known as ReLU [7, 8]. The ReLU function is defined as \u03c3(x) = max{0, x}, so it is clearly piecewise linear. \u2217University of British Columbia. Email: nickhar@cs.ubc.ca \u2020University of British Columbia. Email: cvliaw@cs.ubc.ca \u2021University of British Columbia. Email: abbasmehrabian@gmail.com\nar X\niv :1\n70 3.\n02 93\n0v 1\n[ cs\n.L G\n] 8\nM ar\nIt is particularly interesting to consider how the VC-dimension is affected by the various attributes of the network: the number W of parameters (i.e., weights and biases), the number U of non-linear units (i.e., nodes), and the number L of layers. Among all networks with the same size (number of weights), is it true that those with more layers have more classification power (i.e., larger VC-dimension)?\nSuch a statement is indeed true, and previously known, thereby providing some justification for the advantages of deep neural networks. However, a tight characterization of how depth affects VC-dimension was unknown prior to this work.\nOur results. Our first main result is a new VC-dimension lower bound that holds even for the restricted family of ReLU networks.\nTheorem 1.1 (Main lower bound). There exists a universal constant C such that the following holds. Given any W,L with W > CL > C2, there exists a ReLU network with \u2264 L layers and \u2264W parameters with VC-dimension \u2265WL log(W/L)/C.\nThe proof appears in Section 3. Prior to our work, the best known lower bounds were \u2126(WL) [2, Theorem 2] and \u2126(W logW ) [10, Theorem 1]; we strictly improve both bounds to \u2126(WL log(W/L)).\nOur proof of Theorem 1.1 uses the \u201cbit extraction\u201d technique, which was also used in [2] to give an \u2126(WL) lower bound. We refine this technique to gain the additional logarithmic factor that appears in Theorem 1.1.\nUnfortunately there is a barrier to refining this technique any further. Our next theorem shows the hardness of computing the mod function, implying that the bit extraction technique cannot yield a stronger lower bound than Theorem 1.1. Further discussion of this connection may be found in Remark 3.\nTheorem 1.2. Assume there exists a piecewise linear network with W parameters and L layers that computes a function f : R \u2192 R, with the property that |f(x) \u2212 (xmod 2)| < 1/2 for all x \u2208 {0, 1, . . . , 2m\u22121}. Then we havem = O(L log(W/L)).\nThe proof of this theorem appears in Section 4. One interesting aspect of the proof is that it does not use Warren\u2019s lemma [13], which is a mainstay of VC-dimension upper bounds [6, 2, 1].\nOur next main result is an upper bound on the VC-dimension of neural networks with any piecewise linear activation function with a constant number of pieces. Recall that ReLU is an example of a piecewise linear activation function.\nTheorem 1.3 (Main upper bound). Consider a piecewise linear neural network withW parameters arranged in L layers. Let F be the set of (real-valued) functions computed by this network. Then VCDim(sgn(F)) = O(WL logW ).\nThe proof of this result appears in Section 5. Prior to our work, the best published upper bounds wereO(W 2) [6, Section 3.1] andO(WL logW+WL2) [2, Theorem 1], both of which hold for piecewise polynomial activation functions; we strictly improve both bounds to O(WL logW ) for the special case of piecewise linear functions.\nTo compare our upper and lower bounds, let d(W,L) denote the largest VC-dimension of a piecewise linear network with W parameters and L layers. Theorems 1.1 and 1.3 imply there exist constants c, C such that\nc \u00b7WL log(W/L) \u2264 d(W,L) \u2264 C \u00b7WL logW . (1)\nFor neural networks arising in practice it would certainly be the case that L is significantly smaller than W 0.99, in which case our results determine the asymptotic bound d(W,L) = \u0398(WL logW ). On the other hand, in the regime L = \u0398(W ), which is merely of theoretical interest, we also now have a tight bound d(W,L) = \u0398(WL), obtained by combining Theorem 1.1 with results of [6]. There is now only a very narrow regime, sayW 0.99 L W , in which the bounds of (1) are not asymptotically tight, and they differ only in the logarithmic factor.\nOur final result is a upper bound for VC-dimension in terms of W and U (the number of non-linear units, or nodes). This bound is tight in the case d = 1, as discussed in Remark 3.\nTheorem 1.4. Consider a neural network with W parameters and U units with activation functions that are piecewise polynomials of degree at most d. Let F be the set of (real-valued) functions computed by this network. Then VCDim(sgn(F)) = O(WU log(d+ 1)).\nThe proof of this result appears in Section 6. The best known upper bound before our work was O(W 2) (implicitly proven in [6, Section 3.1], for constant d). Our theorem improves this to the tight result O(WU).\nRelated Work. Recently there have been several theoretical papers that establish the power of depth in neural networks. Last year, two striking papers considered the problem of approximating a deep neural network with a shallower network. [12] shows that there is a ReLU network with L layers and U = \u0398(L) units such that any network approximating it with only O(L1/3) layers must have \u2126(2L 1/3\n) units; this phenomenon holds even for real-valued functions. [5] show an analogous result for a high-dimensional 3-layer network that cannot be approximated by a 2-layer network except with an exponential blow-up in the number of nodes.\nVery recently, several authors have shown that deep neural networks are capable of approximating broad classes of functions. [11] show that a sufficiently non-linear C2 function on [0, 1]d can be approximated with error in L2 by a ReLU network with O(polylog(1/ )) layers and weights, but any such approximation with O(1) layers requires \u2126(1/ ) weights. [14] shows that any Cn-function on [0, 1]d can be approximated with error in L\u221e by a ReLU network with O(log(1/ )) layers and O(( 1 )\nd/n log(1/ )) weights. [9] show that a sufficiently smooth univariate function can be approximated with error in L\u221e by a network with ReLU and threshold gates with \u0398(log(1/ )) layers and O(polylog(1/ )) weights, but that \u2126(poly(1/ )) weights would be required if there were only o(log(1/ )) layers; they also prove analogous results for multivariate functions. Lastly, [4] draw a connection to tensor factorizations to show that, for non-ReLU networks, the set of functions computable by a shallow network have measure zero among those computable by a deep networks."}, {"heading": "2 Preliminaries", "text": "A neural network is defined by an activation function \u03c8 : R \u2192 R, a directed acyclic graph, a weight for each edge of the graph, and a bias for each node of the graph. Let W denote the number of parameters (weights and biases) of the network, U denote the number of computation units (nodes), and L denote the number of layers.\nThe nodes at layer 0 are called input nodes, and simply output the real value given by the corresponding input to the network. For the purposes of this paper, we will assume that the graph has a single sink node, which is the unique node at layer L, (the output layer). In the jargon of neural networks, layers 1 through L\u22121 are called hidden layers.\nThe computation of a neural network proceeds as follows. For i = 1, . . . , L, the input into a computation unit u at layer i is w>x where x is the (real) vector corresponding to the output of the computational units with a directed edge to u and w is the corresponding edge weights. For layers 1, . . . , L \u2212 1, the output of u is \u03c8(w>x + b) where b is the bias parameter associated with u. For the output layer, we replace \u03c8 with the identity. Since we consider VC-dimension, we will always take the sign of the output of the network, to make the output lie in {0, 1} for binary classification. (Here, we define the sign function as sgn(x) = 1[x > 0].)\nA piecewise polynomial function with p pieces is a function f for which there exists disjoint intervals (pieces) I1, . . . , Ip and polynomials f1, . . . , fp such that if x \u2208 Ii then f(x) = fi(x). We assume that p is a constant independent ofW , U andL. A piecewise linear function is a piecewise polynomial function in which each fi is linear. The most common activation function used in practice is the rectified linear unit (ReLU) where I1 = (\u2212\u221e, 0], I2 = (0,\u221e) and f1(x) = 0, f2(x) = x. We denote this function by \u03c3(x) := max{0, x}. The set {1, 2, . . . , n} is denoted [n]."}, {"heading": "3 Proof of Theorem 1.1", "text": "The proof of our main lower bound uses the \u201cbit extraction\u201d technique that was developed by [2] to prove a \u2126(WL) lower bound. We refine their technique in a key way \u2014 we partition the input bits into blocks and extract multiple bits at a time instead of a single bit at a time. This yields a more efficient bit extraction network, and hence a stronger VC-dimension lower bound.\nWe show the following result, which immediately implies Theorem 1.1.\nTheorem 3.1. Let r,m, n be positive integers, and let k = dm/re. There exists a ReLU network with 3 + 5k layers, 2 + n+ 4m+ k((11 + r)2r + 2r+ 2) parameters, and m+ 2 + k(5\u00d7 2r + r + 1) computation units with VC-dimension \u2265 mn.\nRemark. Choosing r = 1 gives a network with W = O(m + n), U = O(m) and VC-dimension \u2126(mn) = \u2126(WU). This implies that the upper bound O(WU) given in Theorem 1.4 is tight.\nTo prove Theorem 1.1, apply Theorem 3.1 with m = rL/8, r = log2(W/L)/2, and n = W \u2212 5m2r. In the rest of this section we prove Theorem 3.1.\nLet Sn \u2286 Rn denote the standard basis. We shatter the set Sn \u00d7 Sm. Given an arbitrary function f : Sn \u00d7 Sm \u2192 {0, 1}, we build a ReLU neural network that inputs (x1, x2) \u2208 Sn \u00d7 Sm and outputs f(x1, x2). Define n numbers a1, a2, . . . , an \u2208 { 02m , 1 2m , . . . , 2m\u22121 2m } so that the ith digit of the binary representation of aj equals f(ej , ei). These numbers will be used as the parameters of the network, as described below.\nGiven input (x1, x2) \u2208 Sn \u00d7 Sm, assume that x1 = ei and x2 = ej . The network must output the ith bit of aj . This \u201cbit extraction approach\u201d was used in [2, Theorem 2] to give an \u2126(WL) lower bound for the VC-dimension. We use a similar approach but we introduce a novel idea: we split the bit extraction into blocks and extract r bits at a time instead of a single bit at a time. This allows us to prove a lower bound of \u2126(WL log(W/L)). One can ask, naturally, whether this approach can be pushed further. Our Theorem 1.2 implies that the bit extraction approach cannot give a lower bound better than \u2126(WL log(W/L)) (see Remark 3).\nThe first layer of the network \u201cselects\u201d aj , and the remaining layers \u201cextract\u201d the ith bit of aj . In the first layer we have a single computational unit that calculates\naj = (a1, . . . , an) >x1 = \u03c3 ( (a1, . . . , an) >x1 ) .\nThis part uses 1 layer, 1 computation unit, and 1 + n parameters. The rest of the network extracts all bits of aj and outputs the ith bit. The extraction is done in k steps, where in each step we extract the r most significant bits and zero them out. We will use the following building block for extracting r bits.\nLemma 3.2. Suppose positive integers r andm are given. There exists a ReLU network with 5 layers, 5\u00d72r+r+1 units and 11\u00d72r+r2r+2r+2 parameters that given the real number b = 0.b1b2 . . . bm (in binary representation) as input, outputs the (r+ 1)- dimensional vector (b1, b2, . . . , br, 0.br+1br+2 . . . bm).\nFigure 1 shows a schematic of the ReLU network in the above lemma.\nProof. Partition [0, 1) into 2r even subintervals. Observe that the values of b1, . . . , br are determined by knowing which such subinterval b lies in. We first show how to design a two-layer ReLU network that computes the indicator function for an interval to any arbitrary precision. Using 2r of these networks in parallel allows us to determine which subinterval b lies in and hence, determine the bits b1, . . . , br.\nFor any a \u2264 b and \u03b5 > 0, observe that the function f(x) := \u03c3(1\u2212\u03c3(a/\u03b5\u2212x/\u03b5))+ \u03c3(1\u2212 \u03c3(x/\u03b5\u2212 b/\u03b5))\u2212 1 has the property that, f(x) = 1 for x \u2208 [a, b], and f(x) = 0 for x /\u2208 (a \u2212 \u03b5, b + \u03b5), and f(x) \u2208 [0, 1] for all x. Thus we can use f to approximate the indicator function for [a, b], to any desired precision. We will choose \u03b5 = 2\u2212m\u22122 because we are working with m-digit numbers. Thus, the values b1, . . . , br can be generated by adding the corresponding indi-\ncator variables. (For instance, b1 = \u22112r\u22121 k=2r\u22121 1[b \u2208 [k \u00b7 2\u2212r, (k + 1) \u00b7 2\u2212r)].) Fi-\nnally, the remainder 0.br+1br+2 . . . bm can be computed as 0.br+1br+2 . . . bm = 2rb\u2212\u2211r k=1 2\nr\u2212kbk. Now we count the number of layers and parameters: we use 2r small networks that work in parallel for producing the indicators, each has 3 layers, 5 units and 11 parameters. To produce b1, . . . , br we need an additional layer, r\u00d7 (2r + 1) additional parameters, and r additional units. For producing the remainder we need 1 more layer, 1 more unit, and r + 2 more parameters.\nWe use dm/re of these blocks to extract the bits of aj , denoted by aj,1, . . . , aj,m. Extracting aj,i is now easy, noting that if x, y \u2208 {0, 1} then x\u2227 y = \u03c3(x+ y\u2212 1). So, since x2 = ei, we have\naj,i = m\u2211 t=1 x2,t \u2227 aj,t = m\u2211 t=1 \u03c3(x2,t + aj,t \u2212 1) = \u03c3 ( m\u2211 t=1 \u03c3(x2,t + aj,t \u2212 1) ) .\nThis calculation needs 2 layers, 1 +m units, and 1 + 4m parameters. Remark. Theorem 1.2 implies an inherent barrier to proving lower bounds using the \u201cbit extraction\u201d approach of [2]. Recall that this technique uses n binary numbers with m bits to encode a function f : Sn\u00d7Sm \u2192 {0, 1} to show an \u2126(mn) lower bound for VC-dimension, where Sk denotes the set of standard basis vectors in Rk. The network begins by selecting one of the n binary numbers, and then extracting a particular bit of that number. [2] showed it is possible to take m = \u2126(L) and n = \u2126(W ), thus proving a lower bound of \u2126(WL) for the VC-dimension. In Theorem 1.1 we showed we can increase m to \u2126(L log(W/L)), improving the lower bound to \u2126(WL log(W/L)). Theorem 1.2 implies that to extract just the least significant bit, one is forced to have m = O(L log(W/L)); on the other hand, we always have n \u2264 W . Hence there is no way to improve the VC-dimension lower bound by more than a constant via the bit extraction technique. In particular, closing the gap for general piecewise polynomial networks will require a different technique."}, {"heading": "4 Proof of Theorem 1.2", "text": "For a piecewise polynomial function R \u2192 R, breakpoints are the boundaries between the pieces. So if the function has p pieces, it has p\u2212 1 breakpoints.\nLemma 4.1. Let f1, . . . , fk : R \u2192 R be piecewise polynomial of degree D, and suppose the union of their breakpoints has size B. Let \u03c8 : R \u2192 R be piecewise polynomial of degree d with b breakpoints. Let w1, . . . , wk \u2208 R be arbitrary. The function g(x) := \u03c8( \u2211 i wifi(x)) is piecewise polynomial of degree Dd with at most (B + 1)(2 + bD)\u2212 1 breakpoints. Proof. Without loss of generality, assume that w1 = \u00b7 \u00b7 \u00b7 = wk = 1. The function\u2211 i fi has B+ 1 pieces. Consider one such interval I. We will prove that it will create\nat most 2 + bD pieces in g. In fact, if \u2211 i fi is constant on I, g will have 1 piece on I.\nOtherwise, for any point y, the equation \u2211 i fi(x) = y has at most D solutions on I.\nLet y1, . . . , yb be the breakpoints of\u03c8. Suppose we move along the curve (x, \u2211 i fi(x)) on I. Whenever we hit a point (t, yi) for some t, one new piece is created in g. So at most bD new pieces are created. In addition, we may have two pieces for the beginning and ending of I. This gives a total of 2 + bD pieces per interval, as required. Finally, note that the number of breakpoints is one fewer than the number of pieces.\nTheorem 4.2. Assume there exists a neural network with W parameters and L layers that computes a function f : R\u2192 R, with the property that |f(x)\u2212 (x mod 2)| < 1/2 for all x \u2208 {0, 1, . . . , 2m \u2212 1}. Also suppose the activation functions are piecewise polynomial of degree at most d \u2265 1 in each piece, and have at most p \u2265 1 pieces. Then we have\nm \u2264 L log2(13pd(L+1)/2 \u00b7W/L).\nIn the special case of ReLU functions, this gives m = O(L log(W/L)).\nProof. For a node v of the network, let f(v) count the number of directed paths from the input node to v. Applying Lemma 4.1 iteratively gives that for a node v at layer i \u2265 1, the number of breakpoints can be bounded by (6p)idi(i\u22121)/2f(v) \u2212 1. Let o denote the output node. Hence, o has at most (6p)LdL(L\u22121)/2f(o) pieces. The output of node o is piecewise polynomial of degree at most dL. On the other hand, as we increase x from 0 to 2m \u2212 1, the function x mod 2 flips 2m \u2212 1 many times, which implies the output of o becomes equal to 1/2 at least 2m \u2212 1 times, thus we get\n(6p)LdL(L\u22121)/2f(o)\u00d7 dL \u2265 2m \u2212 1. (2)\nLet us now relate f(o) with W and L. Suppose that, for i \u2208 [L], there are Wi edges between layer i and previous layers. By the AM-GM inequality,\nf(o) \u2264 \u220f i (1 +Wi) \u2264 (\u2211 i 1 +Wi L )L \u2264 (2W/L)L. (3)\nCombining Eqs. (2) and (3) gives the theorem.\n[12] showed how to construct a function f which satisfies f(x) = (x mod 2) for x \u2208 {0, 1, . . . , 2m\u22121} using a neural network withO(m) layers andO(m) parameters. By choosing m = k3, Telgarsky showed that any function g computable by a neural network with \u0398(k) layers and O(2k) nodes must necessarily have \u2016f \u2212 g\u20161 > c for some constant c > 0.\nOur theorem above implies a similar statement. In particular, if we choose m = k1+\u03b5 then for any function g computable by a neural network with \u0398(k) layers and O(2k \u03b5\n) parameters, there must exist x \u2208 {0, 1, . . . , 2m\u2212 1} such that |f(x)\u2212 g(x)| > 1/2."}, {"heading": "5 Proof of Theorem 1.3", "text": "The proof of this theorem is very similar to the proof of the upper bound for piecewise polynomial networks in [2, Theorem 1] but optimized for piecewise linear networks. Our proof requires the following lemma, which is a slight improvement of a result in [13].\nLemma 5.1 (Theorem 8.3 in [1], Lemma 1 in [2]). Let p1, . . . , pm be polynomials of degree at most d in n \u2264 m variables. Define\nK := |{(sgn(p1(x)), . . . , sgn(pm(x)) : x \u2208 Rn}|,\ni.e. K is the number of possible sign vectors given by the polynomials. Then K \u2264 2(2emd/n)n.\nof Theorem 1.3. Suppose the activation function has p pieces, and suppose, for simplicity, that the pieces are (\u2212\u221e, t1], (t1, t2], . . . , (tp\u22121,+\u221e). (It is straightforward to generalize to pieces of arbitrary form.) For i \u2208 [L], let Wi denote the total number of parameters up to layer i, and let ki denote the number of computational units at layer i. Note that WL = W , and the total number of computational units is k = \u2211L i=1 ki. Let m denote the VC-dimension of the network, let a be the number of input nodes, and let {x1, . . . , xm} \u2282 Ra be a shattered set. (Note that each xi is a real vector in Ra.) If m \u2264 W the theorem\u2019s conclusion already holds, so we may assume that m > W . Define\nK := |{(sgn(f(x1, w)), . . . , sgn(f(xm, w))) : w \u2208 RW }|.\nIn other words, K is the number of sign patterns that the neural network can output for the sequence of inputs (x1, . . . , xm). By definition of shattering, we haveK = 2m. We will prove geometric upper bounds for K, which will imply upper bounds for m. For the rest of the proof, x1, . . . , xm are fixed, and we view the parameters of the network, denoted w, as a collection of W real variables.\nTo bound K, we will find a partition P of RW such that, for all P \u2208 P , the functions f(x1, \u00b7), . . . , f(xm, \u00b7) are polynomials on P . Clearly,\nK \u2264 \u2211 P\u2208P |{(sgn(f(x1, w)), . . . , sgn(f(xm, w))) : w \u2208 P}|. (4)\nWe will define a sequence of partitions P1,P2, . . . ,PL = P inductively, such that Pi is a refinement of Pi\u22121. Starting from layer 1, for ` \u2208 [m] and j \u2208 [k1], let h1,j(x`, w) be the input into the jth computation unit in the first layer and let P1 be a partition of RW such that the vector (sgn(h1,j(x`, \u00b7)\u2212 ts))j\u2208[k1],`\u2208[m],s\u2208[p\u22121] is constant on each P \u2208 P1. Since each h1,j(x`, \u00b7) \u2212 ts is a polynomial of degree 1, by Lemma 5.1, we\ncan take |P1| \u2264 2(2ek1mp/W1)W1 . Observe that on each P \u2208 P1, the output of each computation unit in the first hidden layer is either a polynomial of degree 1 or the zero function.\nNow, suppose the partitions P1, . . . ,Pi\u22121 have been defined. Let hi,j(x`, w) be the input into the jth computation unit in the ith layer and assume that on any fixed P \u2208 Pi\u22121, the function hi,j(x`, \u00b7) is a polynomial of degree at most i. By Lemma 5.1, there exists a partition PP,i of P with |PP,i| \u2264 2(2ekimip/Wi)Wi such that on each P \u2032 \u2208 PP,i, the vector (sgn(hi,j(x`, \u00b7)\u2212 ts))j\u2208[ki],`\u2208[m],s\u2208[p\u22121] is constant. We define Pi := \u222aP\u2208Pi\u22121PP,i; which is a partition of RW such that the vector (sgn(hi,j(x`, \u00b7)\u2212 ts))j\u2208[ki],`\u2208[m],s\u2208[p\u22121] is constant on each P \u2208 Pi. Moreover, for any 1 < i \u2264 L, we have\n|Pi| \u2264 2 ( 2ekimip\nWi\n)Wi |Pi\u22121|.\nRecall that the last (output) layer has a single unit. Let PL be the partition of RW defined as above, such that on each P \u2208 PL, the output of the network is a polynomial of degree at most L in W variables. By Lemma 5.1, each term of the sum in (4) is at most 2(2emL/W )W . Moreover, by our inductive construction, and since ki \u2264 k, Wi \u2265 1, \u2211L i=1Wi \u2264WL, and i \u2264 L, we have\n|PL| \u2264 2L L\u220f i=1 ( 2ekimip Wi )Wi \u2264 2L(2ekmLp)WL.\nHence, 2m = K \u2264 2(2emL/W )W \u00d7 |PL| \u2264 ( 2(2ekmLp)W )L+1 .\nTaking logarithms and using the crude bounds k, L \u2264W gives the condition\nm \u2264 (L+ 1) +W (L+ 1) log2(2eW 2mp) \u2264 4W (L+ 1) log2(2eWmp)\nwhich implies m = O(WL log(pW )), as required."}, {"heading": "6 Proof of Theorem 1.4", "text": "The idea of the proof is that the sign of the output of a neural network can be expressed as a Boolean formula where each predicate is a polynomial inequality. For example, consider the following toy network, where the activation function of the hidden units is a ReLU.\nThe sign of the output of the network is sgn(y) = sgn(w3\u03c3(w1x) + w4\u03c3(w2x)). Define the following Boolean predicates: p1 = (w1x > 0), p2 = (w2x > 0), q1 =\n(w3w1x > 0), q2 = (w4w2x > 0), and q3 = (w3w1x + w4w2x > 0). Then, we can write\nsgn(y) = (\u00acp1 \u2227 \u00acp2 \u2227 0) \u2228 (p1 \u2227 \u00acp2 \u2227 q1) \u2228 (\u00acp1 \u2227 p2 \u2227 q2) \u2228 (p1 \u2227 p2 \u2227 q3).\nA theorem of Goldberg and Jerrum states that any class of functions that can be expressed using a relatively small number of distinct polynomial inequalities has small VC-dimension.\nTheorem 6.1 (Theorem 2.2 of [6]). Let k, n \u2208 N and f : Rn \u00d7 Rk \u2192 {0, 1} be a function that can be expressed as a Boolean formula containing s distinct atomic predicates where each atomic predicate is a polynomial inequality or equality in k+n variables of degree at most d. Let F = {f(\u00b7, w) : w \u2208 Rk}. Then VCDim(F) \u2264 2k log2(8eds).\nof Theorem 1.4. Consider a neural network with W weights and U computation units, and assume that the activation function \u03c8 is piecewise polynomial of degree at most d with p pieces. To apply Theorem 6.1, we will express the sign of the output of the network as a boolean function consisting of less than 2(1+p)U atomic predicates, each being a polynomial inequality of degree at most max{U + 1, 2dU}.\nSince the neural network graph is acyclic, it can be topologically sorted. For i \u2208 [U ], let ui denote the ith computation unit in the topological ordering. The input to each computation unit u lies in one of the p pieces of \u03c8. For i \u2208 [U ] and j \u2208 [p], we say \u201cui is in state j\u201d if the input to ui lies in the jth piece.\nFor u1 and any j, the predicate \u201cu1 is in state j\u201d is a single atomic predicate which is the quadratic inequality indicating whether its input lies in the corresponding interval. So, the state of u1 can be expressed as a function of p atomic predicates. Conditioned on u1 being in a certain state, the state of u2 can be determined using p atomic predicates, which are polynomial inequalities of degree at most 2d + 1. Consequently, the state of u2 can be determined using p+ p2 atomic predicates, each of which is a polynomial of degree at most 2d + 1. Continuing similarly, we obtain that for each i, the state of ui can be determined using p(1 + p)i\u22121 atomic predicates, each of which is a polynomial of degree at most di\u22121 + \u2211i\u22121 j=0 d\nj . Consequently, the state of all nodes can be determined using less than (1 + p)U atomic predicates, each of which is a polynomial of degree at most dU\u22121 + \u2211U\u22121 j=0 d\nj \u2264 max{U + 1, 2dU} (the output unit is linear). Conditioned on all nodes being in certain states, the sign of the output can be determined using one more atomic predicate, which is a polynomial inequality of degree at most max{U + 1, 2dU}.\nIn total, we have less than 2(1 + p)U atomic polynomial-inequality predicates and each polynomial has degree at most max{U + 1, 2dU}. Thus, by Theorem 6.1, we get an upper bound of 2W log(16e \u00b7max{U+1, 2dU}\u00b7(1+p)U ) = O(WU log((1+d)p)) for the VC-dimension."}], "references": [{"title": "Neural network learning: theoretical foundations", "author": ["Martin Anthony", "Peter Bartlett"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Almost linear VC-dimension bounds for piecewise polynomial networks", "author": ["Peter Bartlett", "Vitaly Maiorov", "Ron Meir"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Learnability and the Vapnik-Chervonenkis dimension", "author": ["A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M. Warmuth"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1989}, {"title": "On the expressive power of deep learning: A tensor analysis", "author": ["N. Cohen", "O. Sharir", "A. Shashua"], "venue": "In COLT,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "In COLT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers", "author": ["Paul W. Goldberg", "Mark R. Jerrum"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Deep Learning", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": "http://www.deeplearningbook.org", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Why deep neural networks", "author": ["Shyu Liang", "R. Srikant"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Neural nets with superlinear VC-dimension", "author": ["Wolfgang Maass"], "venue": "Neural Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Depth separation in relu networks for approximating smooth non-linear functions, October 2016", "author": ["I. Safran", "O. Shamir"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Benefits of depth in neural networks", "author": ["Matus Telgarsky"], "venue": "In COLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Lower bounds for approximation by nonlinear manifolds", "author": ["Hugh E. Warren"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1968}, {"title": "Error bounds for approximations with deep ReLU networks, October 2016", "author": ["Dmitry Yarotsky"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "A natural foundational question that arises is: what are the theoretical limits on the classification power of these networks? The established way to formalize this question is by considering VCdimension, as it is well known that this asymptotically determines the sample complexity of PAC learning with such classifiers [3].", "startOffset": 321, "endOffset": 324}, {"referenceID": 6, "context": "The most common activation function used in practice is, by far, the rectified linear unit, also known as ReLU [7, 8].", "startOffset": 111, "endOffset": 117}, {"referenceID": 1, "context": "1 uses the \u201cbit extraction\u201d technique, which was also used in [2] to give an \u03a9(WL) lower bound.", "startOffset": 62, "endOffset": 65}, {"referenceID": 11, "context": "One interesting aspect of the proof is that it does not use Warren\u2019s lemma [13], which is a mainstay of VC-dimension upper bounds [6, 2, 1].", "startOffset": 75, "endOffset": 79}, {"referenceID": 5, "context": "One interesting aspect of the proof is that it does not use Warren\u2019s lemma [13], which is a mainstay of VC-dimension upper bounds [6, 2, 1].", "startOffset": 130, "endOffset": 139}, {"referenceID": 1, "context": "One interesting aspect of the proof is that it does not use Warren\u2019s lemma [13], which is a mainstay of VC-dimension upper bounds [6, 2, 1].", "startOffset": 130, "endOffset": 139}, {"referenceID": 0, "context": "One interesting aspect of the proof is that it does not use Warren\u2019s lemma [13], which is a mainstay of VC-dimension upper bounds [6, 2, 1].", "startOffset": 130, "endOffset": 139}, {"referenceID": 5, "context": "1 with results of [6].", "startOffset": 18, "endOffset": 21}, {"referenceID": 10, "context": "[12] shows that there is a ReLU network with L layers and U = \u0398(L) units such that any network approximating it with only O(L) layers must have \u03a9(2 1/3 ) units; this phenomenon holds even for real-valued functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] show an analogous result for a high-dimensional 3-layer network that cannot be approximated by a 2-layer network except with an exponential blow-up in the number of nodes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11] show that a sufficiently non-linear C function on [0, 1] can be approximated with error in L2 by a ReLU network with O(polylog(1/ )) layers and weights, but any such approximation with O(1) layers requires \u03a9(1/ ) weights.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[11] show that a sufficiently non-linear C function on [0, 1] can be approximated with error in L2 by a ReLU network with O(polylog(1/ )) layers and weights, but any such approximation with O(1) layers requires \u03a9(1/ ) weights.", "startOffset": 55, "endOffset": 61}, {"referenceID": 12, "context": "[14] shows that any C-function on [0, 1] can be approximated with error in L\u221e by a ReLU network with O(log(1/ )) layers and O(( 1 ) d/n log(1/ )) weights.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[14] shows that any C-function on [0, 1] can be approximated with error in L\u221e by a ReLU network with O(log(1/ )) layers and O(( 1 ) d/n log(1/ )) weights.", "startOffset": 34, "endOffset": 40}, {"referenceID": 7, "context": "[9] show that a sufficiently smooth univariate function can be approximated with error in L\u221e by a network with ReLU and threshold gates with \u0398(log(1/ )) layers and O(polylog(1/ )) weights, but that \u03a9(poly(1/ )) weights would be required if there were only o(log(1/ )) layers; they also prove analogous results for multivariate functions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Lastly, [4] draw a connection to tensor factorizations to show that, for non-ReLU networks, the set of functions computable by a shallow network have measure zero among those computable by a deep networks.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "The proof of our main lower bound uses the \u201cbit extraction\u201d technique that was developed by [2] to prove a \u03a9(WL) lower bound.", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "For any a \u2264 b and \u03b5 > 0, observe that the function f(x) := \u03c3(1\u2212\u03c3(a/\u03b5\u2212x/\u03b5))+ \u03c3(1\u2212 \u03c3(x/\u03b5\u2212 b/\u03b5))\u2212 1 has the property that, f(x) = 1 for x \u2208 [a, b], and f(x) = 0 for x / \u2208 (a \u2212 \u03b5, b + \u03b5), and f(x) \u2208 [0, 1] for all x.", "startOffset": 195, "endOffset": 201}, {"referenceID": 1, "context": "2 implies an inherent barrier to proving lower bounds using the \u201cbit extraction\u201d approach of [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "[2] showed it is possible to take m = \u03a9(L) and n = \u03a9(W ), thus proving a lower bound of \u03a9(WL) for the VC-dimension.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[12] showed how to construct a function f which satisfies f(x) = (x mod 2) for x \u2208 {0, 1, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Our proof requires the following lemma, which is a slight improvement of a result in [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "3 in [1], Lemma 1 in [2]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "3 in [1], Lemma 1 in [2]).", "startOffset": 21, "endOffset": 24}, {"referenceID": 5, "context": "2 of [6]).", "startOffset": 5, "endOffset": 8}], "year": 2017, "abstractText": "We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting W be the number of weights and L be the number of layers, we prove that the VC-dimension is O(WL log(W )) and \u03a9(WL log(W/L)). This improves both the previously known upper bounds and lower bounds. In terms of the number U of non-linear units, we prove a tight bound \u0398(WU) on the VC-dimension. All of these results generalize to arbitrary piecewise linear activation functions.", "creator": "LaTeX with hyperref package"}}}