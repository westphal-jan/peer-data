{"id": "1703.02992", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "A Manifold Approach to Learning Mutually Orthogonal Subspaces", "abstract": "almost many machine learning algorithms involve learning subspaces knowing particular characteristics, optimizing a parameter matrix that is constrained to represent a subspace can be challenging. one solution is to use riemannian optimization methods that enforce such constraints implicitly, leveraging the fact that exactly feasible factor generators remain finite manifold. while riemannian methods exist for some specific problems, such as learning a single subspace, possibilities are more general subspace constraints that offer additional options when setting up real optimization problem, but have not been formulated as a manifold.", "histories": [["v1", "Wed, 8 Mar 2017 19:08:28 GMT  (564kb,D)", "http://arxiv.org/abs/1703.02992v1", "9 pages, 3 Figures"]], "COMMENTS": "9 pages, 3 Figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["stephen giguere", "francisco garcia", "sridhar mahadevan"], "accepted": false, "id": "1703.02992"}, "pdf": {"name": "1703.02992.pdf", "metadata": {"source": "CRF", "title": "A Manifold Approach to Learning Mutually Orthogonal Subspaces", "authors": ["Stephen Giguere", "Francisco Garcia", "Sridhar Mahadevan"], "emails": ["SGIGUERE@CS.UMASS.EDU", "FMGARCIA@CS.UMASS.EDU", "SRIDHAR.MAHADEVAN@SRI.COM"], "sections": [{"heading": null, "text": "We propose the partitioned subspace (PS) manifold for optimizing matrices that are constrained to represent one or more subspaces. Each point on the manifold defines a partitioning of the input space into mutually orthogonal subspaces, where the number of partitions and their sizes are defined by the user. As a result, distinct groups of features can be learned by defining different objective functions for each partition. We illustrate the properties of the manifold through experiments on multiple dataset analysis and domain adaptation."}, {"heading": "1. Introduction", "text": "The process of designing a model and learning its parameters by numerically optimizing a loss function is a cornerstone of machine learning. In this setting, it is common to place constraints on the model\u2019s parameters to ensure that they are valid, to promote desirable characteristics such as sparsity, to incorporate domain knowledge, or to make learning more efficient. Because they can significantly impact the difficulty of an optimization problem, it is useful to understand the properties of particular types of constraints and to develop optimization techniques that preserve them.\nRecently, there has been interest in developing Riemannian optimization methods that enforce certain constraints by leveraging the geometry of the parameters that satisfy them, called the feasible set. Specifically, if the feasible set forms a smooth manifold in the original parameter space, then Riemannian optimization can be applied. Unlike other strategies for enforcing constraints, such as augmenting the loss function with penalty terms or projecting the parameters back onto the feasible set, this approach eliminates the need to deal with constraints explictly by performing optimization on the constraint manifold directly. In many cases, using Riemannian optimization can simplify algorithms, provide convergence guarantees on learning, and ensure that constraints are satisfied exactly rather than approximately.\nIn this work, we investigate Riemannian optimization methods for enforcing subspace constraints, which we define as any constraint that forces a matrix of parameters to represent one or more subspaces. There are two commonly used subspace constraints that have well-established Riemannian optimization methods provided in (Edelman, Arias, and Smith, 1998). The first is applicable when optimizing over matrices that define k-dimensional bases in an n-dimensional ambient space. In this case, the feasible set consists of all n\u00d7 k matrices Y that satisfy Y TY = I , which corresponds to the Stiefel manifold. If the optimization is instead taken over all distinct k-dimensional subspaces, then in addition to the constraint that Y TY = I , two matrices must be considered identical if they have the same span. This second condition is important because it implies that during optimization, estimates must be updated to not only change the parameter matrix, but to do so in a way that changes the subspace it represents. The feasible set for this constraint corresponds to the Grassmannian manifold, which has proven useful for a large number of applications including background separation in video, human activity analysis, subspace tracking, and others (He, Balzano, and Lui, 2011; Turaga and Chellappa, 2009; He, Balzano, and Szlam, 2012). ar X iv :1 70 3.\n02 99\n2v 1\n[ cs\n.L G\n] 8\nM ar\n2 01\nWhile the constraints satisfied by optimizing on the Stiefel or Grassmannian manifolds are useful in practice, there are more general subspace constraints that are not captured by these manifolds and offer significant flexibility when setting up an optimization problem. To see this, first consider that a point on a Stiefel manifold can be viewed as a collection of k, 1-dimensional subspaces that are constrained to be mutually orthogonal. Similarly, a point on a Grassmannian manifold represents 1, k-dimensional subspace. It is therefore natural to consider a general constraint in which both the number of subspaces and their sizes can be specified by the user. These relationships are shown in figure 1. Importantly, subspace constraints of this form allow different set of features to be learned according to different criteria. In analogy to optimization on the Stiefel and Grassmannian manifolds, we aim to optimize over distinct sets of m mutually orthogonal subspaces with sizes [k1, k2, ..., km], which we refer to as partitions.\nIn this paper, we introduce a novel manifold that generalizes the Stiefel and Grassmannian manifolds to implicitly enforce the general subspace constraints described above. Individual points on our proposed manifold define partitions of n-dimensional space into mutually orthogonal subspaces of particular sizes. In addition, we derive update rules for performing Riemannian optimization on the manifold. This allows features of the original space to be grouped in useful ways by defining separate objective functions on each subspace. For example, given two datasets X0 and X1, the linear features that best describe the two can easily be partitioned into a pair of subspaces containing features unique to each dataset and a subspace of features that are shared between them. Because of these characteristics, we refer to this manifold as the partitioned subspace\n(PS) manifold. Finally, we provide several examples using both real and synthetic data to illustrate how the manifold can be applied in practice, and to establish intuition for its properties.\nRelated work The problem of learning parameters subject to subspace constraints has been investigated widely and for a variety of applications. In general, these applications use only simple subspace constraints, which effectively cause the feasible set to be either the Grassmannian manifold or Stiefel manifold. Optimization subject to general subspace constraints has been investigated much less. Kim, Kittler, and Cipolla (2010) proposed an algorithm for face recognition that is similar to our work in that they also learn a set of mutually-orthogonal subspaces. However, their approach learns subspaces by incrementally updating and reorthogonalizing the subspace matrix, whereas we optimize on the constraint manifold itself using Riemannian optimization. Our work presents, to our knowledge, the first formulation of general subspace constraints as a manifold.\nThe rest of this paper is organized as follows. In section 2, we describe background concepts and establish notation. Next, we define the partitioned subspace manifold and provide details necessary to use it for optimization in section 3. Finally, section 4 illustrates the various properties of the manifold and analyzes them through experiments on both real and synthetic data."}, {"heading": "2. Background", "text": "Before defining the partitioned subspace manifold, we briefly introduce some necessary background concepts from differential geometry."}, {"heading": "2.1. Riemannian Manifolds", "text": "A Riemannian manifold, or simply a manifold, can be described as a continuous set of points that appears locally Euclidean at every location. More specifically, a manifold is a topological space and a collection of differentiable, one-to-one mappings called charts. At any point on a ddimensional manifold, there is a chart that maps a neighborhood containing that point to the Euclidean space IRd (Absil, Mahony, and Sepulchre, 2009). This property allows a tangent space to be defined at every point, which is an Euclidean space consisting of directions that point along the manifold. Tangent spaces are particularly important for optimization because they characterize the set of update directions that can be followed without leaving the manifold. While the most familiar manifold is Euclidean space, a large number of other, more exotic manifolds can be defined. The manifolds discussed in this paper are examples of matrix manifolds, meaning that they consist of points that can be represented as matrices.\nGiven a manifold, it is often useful to define a quotient manifold by specifying an equivalence relation that associates sets of points on the original manifold with individual points on the quotient (Edelman, Arias, and Smith, 1998). Because each point on the quotient is an equivalence class of points on the parent manifold, we use the notation [p] to refer to a point on the quotient that contains p, where p is an element of the parent manifold. Quotient manifolds inherit a great deal of structure from their parent manifold. In particular, constraints that define the parent manifold also apply to the quotient.\nTo illustrate this, consider the Stiefel and Grassmannian manifolds, which can be defined as quotients of the group of orthogonal matrices. Later in section 3, the derivation for the partitioned subspace manifold will follow a similar pattern. The orthogonal group, denoted On, consists of all n\u00d7nmatricesQ that satisfy the conditionQTQ = I . Note that in this work, we use Q to refer to an element of this set. From this definition, we see that each point [Q]St on a Stiefel manifold is a set of matrices from the orthogonal group whose first k columns match those of Q. Similarly, each point [Q]Gr on a Grassmannian manifold corresponds to those orthogonal group matrices whose first k columns span the same subspace. These relationships can be expressed mathematically as follows:\nStiefel points [Q]St = {QE}E\u2208ESt Grassmannian points [Q]Gr = {QE}E\u2208EGr\nwhere\nESt = [ Ik On\u2212k ] , and\nEGr = [ Ok\nOn\u2212k\n]\nBecause they encode the relationship between matrices that map to the same point on the quotient, we refer to the sets ESt and EGr as the equivalence sets for their respective manifolds.\nRepresentative elements As defined above, points on the Stiefel and Grassmannian manifolds are equivalence classes [Q] of orthogonal matrices Q \u2208 On. However, in practice, these points are represented by single, n \u00d7 k matrices Y . This discrepancy occurs because in general, only the subspace spanned by the first k columns is of interest. Given a point [Q] on these manifolds, we refer to Y as a representative element defined by:\nY = Q\u2032 [ Ik 0 ] , s.t. Q\u2032 \u2208 [Q]"}, {"heading": "2.2. Riemannian Optimization", "text": "Optimization on manifolds requires specialized Riemannian optimization methods to ensure that the learned parameters remain valid. While many standard optimization methods have been derived on manifolds (Edelman, Arias, and Smith, 1998), we focus on Riemmannian gradient descent because of its simplicity. As in standard gradient descent, the goal is to use the gradient of a differentiable convex function f with respect to the matrix Q, denoted GQ, to construct a sequence of estimates Q\u2032 = Q \u2212 \u03b1GQ that converges to the minimum of f , where the parameter \u03b1 controls the size of the update at each step. However, in Riemannian gradient descent, the estimates are constrained to lie on a manifold.\nRiemannian gradient descent proceeds by iterating three steps, as illustrated in Figure 2. The first step is to compute the gradient at the current location, which is given by GQ = \u2207Qf .\nNext, the update direction is modified to reflect the geometry of the underlying manifold. Intuitively, if some component of the update direction is orthogonal to the manifold\u2019s surface, then any movement at all along that direction will immediately leave the manifold. To correct for this, the update direction is projected onto the tangent space at the current location. We denote the projected update direction as \u2206 = \u03c0TQM (GQ), where \u03c0TQM (\u00b7) projects its input onto the tangent space of manifold M at Q.\nThe final step of Riemmannian gradient descent computes a new location by moving along the manifold in the direction \u2206. This is accomplished using the exponential map, which maps elements of a tangent space onto the manifold by moving in a straight line along its surface. For matrix manifolds, the exponential map is given by the matrix exponential function exp(\u00b7), and the updated location is given by the following formula, as shown in (Edelman, Arias, and\nSmith, 1998):\nQ\u2032 = Q exp(\u03b1QT \u2206) (1)\nUnfortunately, computing updates in this way is inefficient due to the cost of evaluating the matrix exponential. Instead, it is common to use a retraction, which behaves similarly to the exponential map but may distort the magnitude of the update. Depending on the manifold, these alternatives may be comparably inexpensive. For example, updates on quotient manifolds of the orthogonal group can be performed using the Q-factor retraction, which maps an update \u2206 at the point Q to the location Q\u2032 = qr(Q + \u2206), where qr(\u00b7) denotes the Q matrix of the QR decomposition of its input (Absil, Mahony, and Sepulchre, 2009).\nThe steps of computing the standard gradient, projecting onto the tangent space at the current point Q, and updating the location using an appropriate retraction are repeated until convergence."}, {"heading": "3. The Partitioned Subspace Manifold", "text": "Having defined the necessary basic concepts, we now describe the partitioned subspace (PS) manifold, emphasizing the intuition behind its derivation. We also derive the components needed to apply Riemannian optimization on the manifold in section 3.2. Note that in this section, we use [Q] to denote [Q]PS for notational convenience."}, {"heading": "3.1. Formulation", "text": "The PS manifold is defined as a quotient of the orthogonal group, similarly to how the Stiefel and Grassmannian manifolds were defined in section 2.1. Here, the difference will be the use of a generalized equivalence relation. For the Stiefel manifold, the equivalence relation equates two orthogonal group matrices if they are related by a rotation of their last n\u2212k columns. The Grassmannian equivalence relation, on the other hand, equates two matrices if they are\nrelated by both a rotation of their last n\u2212 k columns and a rotation of their first k columns. The PS manifold equivalence relation generalizes this pattern by equating two matrices if they are related by rotation of the first k1 columns, or the next k2 columns, etc. Intuitively, rotations that only involve basis vectors for a single partition will not change the subspace they span, whereas rotations that mix basis vectors from different partitions will. Based on this, we define the partitioned subspace manifold as follows.\nDefinition 1 Let k1, k2, ..., km be subspace partition sizes, where m is the number of partitions. Furthermore, let n be the dimensionality of the ambient space, and let k =\u2211m\ni=1 ki \u2264 n denote the total subspace size. We define the partitioned subspace manifold as a quotient of the group of orthogonal matrices, with points on the manifold characterized by:\nPS points [Q]PS = {QE}E\u2208EPS (2)\nwhere\nEPS =  Ok1 . . . Okm\nOn\u2212k  (3) Similarly to the Stiefel and Grassmannian manifolds, the last n \u2212 k columns are generally ignored, and points on the PS manifold are represented by n \u00d7 k matrices Y = Q\u2032 [ Ik 0 ]T for some Q\u2032 \u2208 [Q]PS . It is straightforward to show that the partitioned subspace manifold is equivalent to a Grassmannian manifold when m = 1 and k1 = k, and equivalent to the Stiefel manifold when m = k and kj = 1 for j = 1, 2, ...,m.\nNext, we characterize the tangent space for the PS manifold, which will be required for Riemannian optimization. Let Q be any element of an equivalence class defining a point on the PS manifold. Because the manifold is a quotient of On, it is subject to the orthogonality contraint QTQ = I . As shown by Edelman, Arias, and Smith (1998), this implies that elements \u2206 of the tangent space at [Q] satisfy the skew-symmetry condition,QT \u2206 = \u2212\u2206TQ. It is straightforward to verify that \u2206 = QA satisfies this condition for all skew-symmetric A. However, this does not fully characterize the tangent space at [Q] because there are directions of this form that lead only to points that are in the same equivalence class [Q]. These directions are exactly the ones for which movement in that direction is the same as multiplication by an element of EPS :\nQ exp(\u03b1QT \u2206) = QE, E \u2208 EPS\nSolving this equation reveals that updating Q in the direction \u2206 will not change its equivalence class on the PS man-\nifold if \u2206 is of the form:\n\u2206 = Q  Ak1 . . . Akm\nAn\u2212k  , where each Ai is a ki \u00d7 ki skew-symmetric matrix. Note that to reach this conclusion, we rely on the observation that QT \u2206 = A, and that exp(A) \u2208 O because A is skewsymmetric (Edelman, Arias, and Smith, 1998). Given these observations, tangent spaces on the partitioned subspace manifold are characterized as follows. Let Bi,j denote an arbitrary ki \u00d7 kj matrix, and let B\u22a5,j and Bi,\u22a5 denote arbitary (n\u2212k)\u00d7kj and ki\u00d7 (n\u2212k) matrices, respectively. The tangent space at a point [Q] on the partitioned subspace manifold is the set of all n\u00d7 n matrices \u2206 given by:\n\u2206 = Q  0 \u2212BT2,1 \u00b7 \u00b7 \u00b7 \u2212BTm,1 \u2212BT\u22a5,1 B2,1 0 \u00b7 \u00b7 \u00b7 \u2212BTm,2 \u2212BT\u22a5,2 ... ... . . . ... ...\nBm,1 Bm,2 \u00b7 \u00b7 \u00b7 0 \u2212BT\u22a5,m B\u22a5,1 B\u22a5,2 \u00b7 \u00b7 \u00b7 B\u22a5,m 0\n (4)"}, {"heading": "3.2. Optimization on the PS Manifold", "text": "To apply Riemannian optimization on the partitioned subspace manifold, we must derive the tangent space projection operator and identify an appropriate retraction.\nThe projection of an arbitrary update direction Z onto the tangent space at [Q] can be derived as follows. First, we project Z onto the set of matrices of the formQA, whereA is skew-symmetric. The result is given by Q skew(QTZ), where skew(X) = 12 (X \u2212 X\nT ) (Edelman, Arias, and Smith, 1998). Next, we replace the diagonal blocks of this matrix with zeros to obtain the projection of Z onto matrices of the form given in equation 4.\nAlthough deriving expressions for the Bi,j blocks in equation 4 is tedious due to notation, it is straightforward and simplifies to the following. Let Zi and Qi denote the ki columns of Z and Q corresponding to the jth subspace partition. For an arbitrary matrix Z, the projection onto the tangent space of the PS manifold at a point Q is given by\n\u03c0TQPS(Z) = 1\n2 [ \u03c01 | \u03c02 | ... | \u03c0m | \u03c0n\u2212k ] , (5)\nwhere\n\u03c0i = (Zi \u2212QZTQi) + (QiZTi Qi \u2212QiQTi Zi). (6)\nWith the tangent space projection defined, we now consider retractions on the partitioned subspace manifold. Ideally\nAlgorithm 1 Riemannian gradient descent on the partitioned subspace manifold\nInput: loss function f(Q), initial estimate Q0, partition sizes {ki}mi=1, step size \u03b1 for i = 0, 1, 2, ... do compute GQ \u2190 \u2207Qf(Qi) compute \u2206\u2190 \u03c0TQiPS(GQ) (Eqns.5-6) compute Qi+1 \u2190 qr(Qi \u2212 \u03b1\u2206) if Qi \u2248 Qi+1 then\nreturn Qi+1\nthe exponential map would be used, but the computational cost associated with it makes it impractical. Fortunately, because our manifold is a quotient of the group of orthogonal matrices, the Q-factor retraction can be applied to efficiently compute updates.\nHaving derived the tangent space projection and an appropriate retraction, Riemannian gradient descent can be applied as described in section 3. Our approach is summarized in Algorithm 1."}, {"heading": "4. Applications", "text": "To illustrate how the PS manifold can be used in practice, we applied it to the problems of multiple dataset analysis and domain adaptation. Specifically, our intention throughout these applications was not to establish new state-of-theart methods, but rather to provide proof of concept and to demonstrate the properties of the PS manifold."}, {"heading": "4.1. Multiple Dataset Analysis", "text": "First, we use the PS manifold to extract features that describe a collection of datasets, grouped according to whether they are unique to one dataset in particular (the per-dataset features) or shared between several (the shared features). Given a collection of D datasets X = {Xi}Di=1, we define D+1 partitions, where the first D partitions will capture the per-dataset features and the last partition will capture the shared features. We note that in general, additional partitions could be defined to model more complex relationships, e.g., to learn features unique to distinct pairs or triplets of datasets. For simplicity, we consider only perdataset and shared features in this section, and leave the problem of learning more general partitioning schemes as a promising direction for future work."}, {"heading": "4.1.1. PROBLEM FORMULATION", "text": "Our approach is motivated by the principal components analysis (PCA), which computes the k-dimensional subspace that best reconstructs a single input dataset (Dunteman, 1989). The subspace returned by PCA can be viewed\nas the solution to the following optimization problem:\nYPCA = arg min Y \u2208Gr(n,k)\n||X \u2212XY Y T ||2F ,\nwhere Gr(n, k) denotes the Grassmannian manifold of kdimensional subspaces of IRn, and ||\u00b7||F denotes the Frobenius norm. The expression ||X \u2212XY Y T ||2F calculates the reconstruction error between the original dataset X and its projection onto Y , which is given byXY Y T . Based on this optimization problem, we propose Multiple-Dataset PCA to solve the following related problem:\nDefinition 2 Let Xi denote one of D datasets in a collection X , and let PS(n, kpd, ksh) be a partitioned subspace manifold consisting of D per-dataset partitions of size kpd and one shared partition of size ksh. Furthermore, given a point [Q] on the PS manifold, let Qi and Qsh denote the columns of Q that span the ith per-dataset partition and the shared partition, respectively. Finally, define Si = [ Qi | Qsh ] to be the union of the per-dataset partition for Xi and the shared partition. We define the MultiDataset PCA subspace to be the point on the PS manifold given by YMD-PCA = QMD-PCA [ Ik 0 ]T , where QMD-PCA is equal to:\nQMD-PCA = arg min Q\u2208PS(n,kpd,ksh) D\u2211 i=1 ||Xi \u2212XiSiSTi ||2F |Xi| . (7)\nGiven Algorithm 1 for performing Riemannian gradient descent on the partitioned subspace manifold, the MD-PCA algorithm is simple to state: apply Riemannian gradient descent using the gradient of the loss function in Equation 7 at each step. For MD-PCA, the gradient can be computed using the equations below.\n\u2202 \u2202Qi f(Q) = \u22122X T i XiQi |Xi| and\n\u2202\n\u2202Qsh f(Q) = \u22122 ( D\u2211 i=1 XTi Xi |Xi| ) Qsh"}, {"heading": "4.1.2. EXPERIMENTS", "text": "To evaluate MD-PCA, we used The Office+Caltech10 set, which is a standard object recognition benchmark containing four datasets of processed image data (Gong et al., 2012). Each dataset is drawn from a different source, which is either Amazon (A), DSLR (D), Caltech (C) or Webcam (W). The original images are encoded as 800-bin histograms of SURF features that have been normalized and z-scored to have zero mean and unit standard deviation in each dimension, as described in (Gong et al., 2012). Each image is associated with one of ten class labels, but these were not used in our experiments because MD-PCA, like PCA, is an unsupervised learning approach.\nTo evaluate the sensitivity of the MD-PCA results with respect to the chosen partition sizes, we performed an exhaustive sweep over values for kpd. Specifically, kpd was varied between 1 and 99, and the size of the shared partition was set to 400\u2212 4kpd. Figure 3 illustrates the result of this analysis."}, {"heading": "4.1.3. REMARKS", "text": "MD-PCA highlights several of the benefits and considerations of using the PS manifold in practice.\nFirst, the mutual orthogonality constraints between the subspace partitions guarantee that a feature in one partition is not contained in any other, i.e., the dataset-specific features truly are unique. Because the manifold enforces mutualorthogonality exactly, there is never redundancy in the features that might otherwise be introduced due to the constraints only being satisfied approximately.\nSecond, these same constraints cause common features to be included in Qsh, and unique features to be included in the corresponding subspace Qi, despite the lack of explicit discriminative terms in the loss function. Intuitively, if a feature is important for reconstructingXi, then the ith term in Equation 7 will encourage it to be contained in either Qi or Qsh. If that feature is also important for reconstructing a different dataset Xj , then the jth term will behave sim-\nilarly, and the pressure to include it in Qsh will dominate the pressure to include it in Qi or Qj . On the other hand, if the feature is unique to Xi, then the loss terms for the other datasets will encourageQsh to be populated with other features, so that the pressure to include it in Qi will be uncontested while the pressure to include it in Qsh will be neutralized. This behavior arises because the mutual orthogonality constraints captured by the PS manifold prevent a feature from being in multiple partitions simultaneously. If the partitions were not constrained in this way, for example if they were optimized as points on separate Grassmannian manifolds, then shared features would tend to mix with the per-dataset features, introducing redundancy and increasing the overall reconstruction error.\nFinally, the use of the PS manifold simplifies the presentation of the algorithm by eliminating the steps that would otherwise have to be taken to enforce the mutual orthogonality constraints.\nOn the other hand, there are limitations to this approach. Foremost, while MD-PCA extracts and partitions features for multiple datasets, those datasets must represent the data in a consistent way. In particular, if samples from two datasets have different dimensionality, as is often the case with image datasets for example, then MD-PCA cannot be applied without first processing the datasets to map them to a common feature space.\nAlso, MD-PCA requires the user to specify not only the total subspace size, as in PCA, but also the size of each partition. Figure 3 shows how the variance explained by the subspace partitions changes as the per-dataset partition size is varied. In general, the total variance explained for each dataset does not change much as the partition size changes, until the per-dataset partitions become large enough to cause the shared partition to be very small. This observation implies that in practice, the size of the perdataset partitions can be set liberally as long as the shared partition remains reasonably large. We hypothesize that the decrease in explained variance occurs when the shared partition becomes too small to contain all of the common features, which causes one dataset to \u201dsteal\u201d the feature into its partition while the reconstruction error suffers for the other datasets."}, {"heading": "4.2. Domain Adaptation", "text": "To further illustrate using the partitioned subspace manifold, we applied it to the problem of domain adaptation. Here, the goal is to leverage a labeled dataset Xs sampled from a source distribution, to improve accuracy when predicting an unlabeled target dataset Xt, sampled from a different target distribution. In this work, we consider the problem of performing classification in the target domain. Typical strategies for domain adaptation include project-\ning Xs and Xt onto a common space on which a classifier is then trained (Fernando et al., 2013; Gong et al., 2012; Gopalan, Li, and Chellappa, 2011), or finding a transformation of the datasets such that a classifier generalizes well to both domains (Kulis, Saenko, and Darrell, 2011; Chen et al., 2012).\nThe unique properties of the partitioned subspace manifold make it well suited to learning representations that generalize between the source and target. In the following, we provide an illustrative example of using the PS manifold in this way."}, {"heading": "4.2.1. PROBLEM FORMULATION", "text": "In our approach, we wish to learn a set of subspace partitions that captures the common features between the source and target domains, while providing a representation that makes it easy to distinguish between classes. To accomplish this, we hypothesize that there are class-specific subspaces on which the source and target distributions are equivalent, so that a classifier trained on the projected data will generalize well across domains. Secondly, we constrain the subspaces to be mutually orthogonal to promote better discrimination between classes after projection. The partitioned subspace manifold makes it straightforward to specify and implement this objective.\nWe note that while the strategy of identifying domain- or label-specific features is not new to domain adaptation, the use of explicitly partitioned subspaces to capture those features has, to the best of our knowledge, not been investigated.\nFirst, we define Q to be a partitioned subspace, where each partition will contain features that describe source samples with a given label y. To encourage the subspace to capture the structure of the source and target domains, we minimize the reconstruction error of approximating Xs and Xt by their projections onto Q. In addition, we promote discriminative subspaces by maximizing the magnitude of each source sample when projected onto its corresponding class partition, and minimizing the magnitude when projected onto the remaining partitions. Combining these, we define the Class Discriminative Transfer (CDT) subspace as follows.\nDefinition 3 Let Xs be a source dataset with labels Ys, where yx denotes the label for sample x, and let Xt be a target dataset. Let L denote total number of unique labels. For each label yi, we define a per-class partition Qyi of size kpc to contain features that describe source samples with that label. Let Q = [ Qy1 | Qy2 | . . . | QyL | Q\u22a5 ] be an orthonormal matrix containing these partitions in its first k columns, where k = kpcL is the combined size of the partitions. Finally, let Qy\u0304 be the union of the class-\nspecific partitions that do not correspond to label y. We define the Class Discriminative Transfer (CDT) subspace to be YCDT = QCDT [ Ik 0 ]T , where QCDT is equal to:\nQCDT = arg min Q\u2208PS(n,kpc) f(Q), (8)\nand\nf(Q) = ||Xs \u2212XsQQT ||2F + ||Xt \u2212XtQQT ||2F \u2212\u03bb \u2211\nx\u2208Xs\n( ||xQyxQTyx || 2 F \u2212 ||zQy\u0304xQTy\u0304x || 2 F ) (9) The first two terms in Equation 9 minimize the reconstruction error of the projected source and target data, while the third term encourages subspaces that allow samples from different classes to be easily distinguished. As a result, a set of partitions is learned that accurately reconstructs the source and target datasets while grouping the features into partitions that are useful for classification."}, {"heading": "4.2.2. EXPERIMENTS", "text": "We evaluated our approach using the Office+Caltech10 dataset described in Subsection 4.1.2. Unlike the multiple dataset analysis application, here we use the provided labels to conduct supervised training on the source domain and to evaluate test accuracy on the target domain. Experiments were conducted for every possible pair of source and target domains, leading to 12 configurations.\nTo measure the effectiveness of using the CST subspace for domain adaptation, we compared against two commonly used approaches that are related to ours. The first, called the Geodesic Flow Kernel (GFK) method, computes one subspace to represent the source data and another to represent the target. A similarity between samples is then\ncomputed by integrating their inner product after projection onto all intermediate subspaces between the source and target. The similarities are then used as input to a 1-NN classifier (Gong et al., 2012). We include results for two versions of GFK: in the configuration GFK(PCA,PCA), the source and target subspaces are learned using PCA, while in GFK(PLS,PCA), partial least squares (PLS) is used to learn a source subspace that captures covariance between the source data and the available labels (Wold, Sjo\u0308stro\u0308m, and Eriksson, 2001).\nThe second approach we compare to is Subspace Alignment (SA), which first computes source and target subspaces in a similar way to GFK. However, SA then projects each dataset onto its respective subspace and determines the optimal linear transformation to align the projected source data to the projected target data. After the transformation is applied, a support vector machine classifier is trained (Fernando et al., 2013). Finally, we compared our approach to the simple baseline of training an SVM on the original SIFT features without applying domain adaptation, which we call OrigFeat in our results. To ensure a fair comparison, we used the same evaluation protocols used for GFK and SA for our experiments (Gong et al., 2012; Fernando et al., 2013).\nFor our experiments, we conducted trials using partition sizes kcs of 20 and 80 dimensions. Note that because the Office+Caltech10 datasets consist of samples with 800 features across 10 classes, the maximum class-specific partition size is 80. In all cases, the 80-dimensional subspace partitions performed best. The value of \u03bb was fixed to be 2.0 for all experiments. After learning the CST subspace, we evaluated classification accuracy using Support Vector Machines (SVM) and Naive Bayes (NB). Tables 1 and 2\nshow the mean accuracy on all transfer tasks."}, {"heading": "4.2.3. REMARKS", "text": "When comparing accuracy using SVM as a classifier, the partitioned subspace manifold is able to achieve comparable performance to GFK and SA. This suggests that the different partitions are able to learn discriminative subspaces that generalize to the target domain. Our approach also seems to be particularly well suited to using Naive Bayes for classification. In this case, the predictions are not made according to distance between the data, but according to the probability distributions of each feature."}, {"heading": "5. Conclusion", "text": "In this work, we presented a formulation for the Partitioned Subspace Manifold, which captures the geometry of mutually orthogonal subspaces, and provided the necessary components for optimizing on the manifold using Riemannian optimization techniques. The ability to specify and easily optimize under general subspace constraints offers a significant amount of flexibility in designing new loss functions. To illustrate this, we proposed and analyzed multiple-dataset PCA, an extension of the principle component analysis which is able to group features from multiple datasets according to whether they are unique to a specific dataset or shared between them. In addition, we showed how the PS manifold can be used to learn class-specific features for domain adaptation and proposed a method that achieves comparable accuracy several standard approaches. We are actively interested in extending this formulation to allow subspace partitions to be adaptively resized so that they can be more effective in online settings, where the optimal partition sizes may change."}], "references": [{"title": "Optimization algorithms on matrix manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": "Princeton University Press.", "citeRegEx": "Absil et al\\.,? 2009", "shortCiteRegEx": "Absil et al\\.", "year": 2009}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K. Weinberger", "F. Sha"], "venue": "Langford, J., and Pineau, J., eds., Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML \u201912. New York, NY, USA: ACM.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Principal components analysis", "author": ["G.H. Dunteman"], "venue": "Number 69. Sage.", "citeRegEx": "Dunteman,? 1989", "shortCiteRegEx": "Dunteman", "year": 1989}, {"title": "The geometry of algorithms with orthogonality constraints", "author": ["A. Edelman", "T.A. Arias", "S.T. Smith"], "venue": "SIAM journal on Matrix Analysis and Applications 20(2):303\u2013353.", "citeRegEx": "Edelman et al\\.,? 1998", "shortCiteRegEx": "Edelman et al\\.", "year": 1998}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2960\u20132967.", "citeRegEx": "Fernando et al\\.,? 2013", "shortCiteRegEx": "Fernando et al\\.", "year": 2013}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, 2066\u20132073. IEEE.", "citeRegEx": "Gong et al\\.,? 2012", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "2011 international conference on computer vision, 999\u20131006. IEEE.", "citeRegEx": "Gopalan et al\\.,? 2011", "shortCiteRegEx": "Gopalan et al\\.", "year": 2011}, {"title": "Online robust subspace tracking from partial information", "author": ["J. He", "L. Balzano", "J. Lui"], "venue": "arXiv preprint arXiv:1109.3827.", "citeRegEx": "He et al\\.,? 2011", "shortCiteRegEx": "He et al\\.", "year": 2011}, {"title": "Incremental gradient on the grassmannian for online foreground and background separation in subsampled video", "author": ["J. He", "L. Balzano", "A. Szlam"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, 1568\u20131575. IEEE.", "citeRegEx": "He et al\\.,? 2012", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "On-line learning of mutually orthogonal subspaces for face recognition by image sets", "author": ["T.-K. Kim", "J. Kittler", "R. Cipolla"], "venue": "IEEE Transactions on Image Processing 19(4):1067\u20131074.", "citeRegEx": "Kim et al\\.,? 2010", "shortCiteRegEx": "Kim et al\\.", "year": 2010}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR \u201911, 1785\u20131792. Washington, DC, USA:", "citeRegEx": "Kulis et al\\.,? 2011", "shortCiteRegEx": "Kulis et al\\.", "year": 2011}, {"title": "Locally time-invariant models of human activities using trajectories on the grassmannian", "author": ["P. Turaga", "R. Chellappa"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, 2435\u2013 2441. IEEE.", "citeRegEx": "Turaga and Chellappa,? 2009", "shortCiteRegEx": "Turaga and Chellappa", "year": 2009}, {"title": "Plsregression: a basic tool of chemometrics", "author": ["S. Wold", "M. Sj\u00f6str\u00f6m", "L. Eriksson"], "venue": "Chemometrics and intelligent laboratory systems 58(2):109\u2013130.", "citeRegEx": "Wold et al\\.,? 2001", "shortCiteRegEx": "Wold et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 11, "context": "The feasible set for this constraint corresponds to the Grassmannian manifold, which has proven useful for a large number of applications including background separation in video, human activity analysis, subspace tracking, and others (He, Balzano, and Lui, 2011; Turaga and Chellappa, 2009; He, Balzano, and Szlam, 2012).", "startOffset": 235, "endOffset": 321}, {"referenceID": 2, "context": "Our approach is motivated by the principal components analysis (PCA), which computes the k-dimensional subspace that best reconstructs a single input dataset (Dunteman, 1989).", "startOffset": 158, "endOffset": 174}, {"referenceID": 5, "context": "To evaluate MD-PCA, we used The Office+Caltech10 set, which is a standard object recognition benchmark containing four datasets of processed image data (Gong et al., 2012).", "startOffset": 152, "endOffset": 171}, {"referenceID": 5, "context": "The original images are encoded as 800-bin histograms of SURF features that have been normalized and z-scored to have zero mean and unit standard deviation in each dimension, as described in (Gong et al., 2012).", "startOffset": 191, "endOffset": 210}, {"referenceID": 4, "context": "Typical strategies for domain adaptation include projecting Xs and Xt onto a common space on which a classifier is then trained (Fernando et al., 2013; Gong et al., 2012; Gopalan, Li, and Chellappa, 2011), or finding a transformation of the datasets such that a classifier generalizes well to both domains (Kulis, Saenko, and Darrell, 2011; Chen et al.", "startOffset": 128, "endOffset": 204}, {"referenceID": 5, "context": "Typical strategies for domain adaptation include projecting Xs and Xt onto a common space on which a classifier is then trained (Fernando et al., 2013; Gong et al., 2012; Gopalan, Li, and Chellappa, 2011), or finding a transformation of the datasets such that a classifier generalizes well to both domains (Kulis, Saenko, and Darrell, 2011; Chen et al.", "startOffset": 128, "endOffset": 204}, {"referenceID": 1, "context": ", 2012; Gopalan, Li, and Chellappa, 2011), or finding a transformation of the datasets such that a classifier generalizes well to both domains (Kulis, Saenko, and Darrell, 2011; Chen et al., 2012).", "startOffset": 143, "endOffset": 196}, {"referenceID": 5, "context": "The similarities are then used as input to a 1-NN classifier (Gong et al., 2012).", "startOffset": 61, "endOffset": 80}, {"referenceID": 4, "context": "After the transformation is applied, a support vector machine classifier is trained (Fernando et al., 2013).", "startOffset": 84, "endOffset": 107}, {"referenceID": 5, "context": "To ensure a fair comparison, we used the same evaluation protocols used for GFK and SA for our experiments (Gong et al., 2012; Fernando et al., 2013).", "startOffset": 107, "endOffset": 149}, {"referenceID": 4, "context": "To ensure a fair comparison, we used the same evaluation protocols used for GFK and SA for our experiments (Gong et al., 2012; Fernando et al., 2013).", "startOffset": 107, "endOffset": 149}], "year": 2017, "abstractText": "Although many machine learning algorithms involve learning subspaces with particular characteristics, optimizing a parameter matrix that is constrained to represent a subspace can be challenging. One solution is to use Riemannian optimization methods that enforce such constraints implicitly, leveraging the fact that the feasible parameter values form a manifold. While Riemannian methods exist for some specific problems, such as learning a single subspace, there are more general subspace constraints that offer additional flexibility when setting up an optimization problem but have not been formulated as a manifold. We propose the partitioned subspace (PS) manifold for optimizing matrices that are constrained to represent one or more subspaces. Each point on the manifold defines a partitioning of the input space into mutually orthogonal subspaces, where the number of partitions and their sizes are defined by the user. As a result, distinct groups of features can be learned by defining different objective functions for each partition. We illustrate the properties of the manifold through experiments on multiple dataset analysis and domain adaptation.", "creator": "LaTeX with hyperref package"}}}