{"id": "1703.03400", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2017", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "abstract": "we proposed an algorithm for factor - learning that is model - agnostic, in broader sense that insight is compatible with classic model trained with empirical descent skills applicable to a trilogy of different learning problems, including classification, regression, and lazy learning. the goal of meta - learning is to train a model on a series of target subjects, such that it can solve new learning tasks using only a small number of training tools. in our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small set of training data from a new target will produce good generalization performance on unknown task. in effect, our method trains the solution to be easy to fine - tune. we demonstrate that this approach leads to state - of - box - art performance on a few - shot image classification benchmark, produces good results on few - shot regression, and accelerates fine - pairing for policy gradient reinforcement learning with neural network policies.", "histories": [["v1", "Thu, 9 Mar 2017 18:58:03 GMT  (5061kb,D)", "http://arxiv.org/abs/1703.03400v1", "Videos of the reinforcement learning results are at sites.google.com/view/maml"], ["v2", "Tue, 9 May 2017 17:14:08 GMT  (5065kb,D)", "http://arxiv.org/abs/1703.03400v2", "Videos of the reinforcement learning results are atthis https URL"], ["v3", "Tue, 18 Jul 2017 16:45:29 GMT  (5063kb,D)", "http://arxiv.org/abs/1703.03400v3", "ICML 2017. Code atthis https URL, Videos of RL results atthis https URL, Blog post atthis http URL"]], "COMMENTS": "Videos of the reinforcement learning results are at sites.google.com/view/maml", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE", "authors": ["chelsea finn", "pieter abbeel", "sergey levine"], "accepted": true, "id": "1703.03400"}, "pdf": {"name": "1703.03400.pdf", "metadata": {"source": "META", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "authors": ["Chelsea Finn", "Pieter Abbeel", "Sergey Levine"], "emails": ["<cbfinn@eecs.berkeley.edu>."], "sections": [{"heading": "1. Introduction", "text": "Learning quickly is a hallmark of human intelligence, whether it involves recognizing objects from a few examples or quickly learning new skills after just minutes of experience. Our artificial agents should be able to do the same, learning and adapting quickly from only a few examples, and continuing to adapt as more data becomes available. This kind of fast and flexible learning is challenging, since the agent must integrate its prior experience with a small amount of new information, while avoiding overfitting to the new data. Furthermore, the form of prior experience and new data will depend on the task. As such, for the greatest applicability, the mechanism for learning to learn (or meta-learning) should be general to the task and the form of computation required to complete the task.\nIn this work, we propose a meta-learning algorithm that\n1University of California, Berkeley 2OpenAI. Correspondence to: Chelsea Finn <cbfinn@eecs.berkeley.edu>.\nis general and model-agnostic, in the sense that it can be directly applied to any learning problem and model that is trained with a gradient descent procedure. Our focus is on deep neural network models, but we illustrate how our approach can easily handle different architectures and different problem settings, including classification, regression, and policy gradient reinforcement learning, with minimal modification. In meta-learning, the goal of the trained model is to quickly learn a new task from a small amount of new data, and the model is trained by the meta-learner to be able to learn on a large number of different tasks. The key idea underlying our method is to train the model\u2019s initial parameters such that the model has maximal performance on a new task after the parameters have been updated through one or more gradient steps computed with a small amount of data from that new task. Unlike prior meta-learning methods that learn an update function or learning rule (Schmidhuber, 1987; Bengio et al., 1992; Andrychowicz et al., 2016; Ravi & Larochelle, 2017), our algorithm does not expand the number of learned parameters nor place constraints on the model architecture (e.g. by requiring a recurrent model (Santoro et al., 2016) or a Siamese network (Koch, 2015)), and it can be readily combined with fully connected, convolutional, or recurrent neural networks. It can also be used with a variety of loss functions, including differentiable supervised losses and nondifferentiable reinforcement learning objectives.\nThe process of training a model\u2019s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks. If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly (e.g. by primarily modifying the top layer weights in a feedforward model) can produce good results. In effect, our procedure optimizes for models that are easy and fast to fine-tune, allowing the adaptation to happen in the right space for fast learning. From a dynamical systems standpoint, our learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters: when the sensitivity is high, small local changes to the parameters can lead to large improvements in the task loss.\nThe primary contribution of this work is a simple model-\nar X\niv :1\n70 3.\n03 40\n0v 1\n[ cs\n.L G\n] 9\nM ar\n2 01\n7\nand task-agnostic algorithm for meta-learning that trains a model\u2019s parameters such that a small number of gradient updates will lead to fast learning on a new task. We demonstrate the algorithm on different model types, including fully connected and convolutional networks, and in several distinct domains, including few-shot regression, image classification, and reinforcement learning. Our evaluation shows that our meta-learning algorithm compares favorably to state-of-the-art one-shot learning methods designed specifically for supervised classification, while using fewer parameters, but that it can also be readily applied to regression and can accelerate reinforcement learning in the presence of task variability, substantially outperforming direct pretraining as initialization."}, {"heading": "2. Model-Agnostic Meta-Learning", "text": "We aim to train models that can achieve rapid adaptation, a problem setting that is often formalized as few-shot learning. In this section, we will define the problem setup and present the general form of our algorithm."}, {"heading": "2.1. Meta-Learning Problem Set-Up", "text": "The goal of few-shot meta-learning is to train a model that can quickly adapt to a new task using only a few datapoints and training iterations. To accomplish this, the model or learner is trained during a meta-learning phase on a set of tasks, such that the trained model can quickly adapt to new tasks using only a small number of examples or trials. In effect, the meta-learning problem treats entire tasks as training examples. In this section, we formalize this metalearning problem setting in a general manner, including brief examples of different learning domains. We will discuss two different learning domains in detail in Section 3.\nWe consider a model, denoted f , that maps observations x to outputs a. During meta-learning, the model is trained to be able to adapt to a large or infinite number of tasks. Since we would like to apply our framework to a variety of learning problems, from classification to reinforcement learning, we introduce a generic notion of a learning task below. Formally, each task T = {L(x1,a1, . . . ,xH ,aH), q(x1), q(xt+1|xt,at), H} consists of a loss function L, a distribution over initial observations q(x1), a transition distribution q(xt+1|xt,at), and an episode length H . In the case of i.i.d. supervised learning problems, the length is H = 1. The model may episodically generate samples of length H by choosing an output at at each timestep t. Task-specific feedback is provided by the loss function L(x1,a1, . . . ,xH ,aH) \u2192 R, which might be a misclassification loss or a cost function in a Markov decision process (MDP).\nIn our meta-learning scenario, we consider a distribution\nover tasks p(T ) that we want our model to be able to adapt to. In the K-shot learning setting, the model is trained to learn a new task Ti drawn from p(T ) from only K samples drawn from qi and feedback LTi generated by Ti. During meta-training, a task Ti is sampled from p(T ), the model is trained with K samples and feedback from the corresponding loss LTi from Ti, and then tested on new samples from Ti. The model f is then improved by considering how the test error on new data from qi changes with respect to the parameters. In effect, the test error on sampled tasks Ti serves as the training error of the meta-learning process. At the end of meta-training, new tasks are sampled from p(T ), and meta-performance is measured by the model\u2019s performance after learning from K samples. Generally, tasks used for meta-testing are held out during meta-training."}, {"heading": "2.2. A Model-Agnostic Meta-Learning Algorithm", "text": "In contrast to prior work, which has sought to train recurrent neural networks that ingest entire datasets (Santoro et al., 2016; Duan et al., 2016b) or feature embeddings that can be combined with nonparametric methods at test time (Vinyals et al., 2016; Koch, 2015), we propose a method that can learn the parameters of any standard model via meta-learning in such a way as to prepare that model for fast adaptation. The intuition behind this approach is that some internal representations are more transferrable than others. For example, a neural network might learn internal features that are broadly applicable to all tasks in p(T ), rather than a single individual task. How can we encourage the emergence of such general-purpose representations? We take an explicit approach to this problem: since the model will be fine-tuned using a gradient-based learning rule on a new task, we will aim to learn a model in such a way that this gradient-based learning rule can make rapid progress on new tasks drawn from p(T ), without overfitting. In effect, we will aim to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will produce large improvements on the loss function of any task drawn from p(T ), when altered in the direction of the gradient of that loss (see Figure 1). We make no assumption on the form of the model, other than\nto assume that it is parametrized by some parameter vector \u03b8, and that the loss function is smooth enough in \u03b8 that we can use gradient-based learning techniques.\nFormally, we consider a model represented by a parametrized function f\u03b8 with parameters \u03b8. When adapting to a new task Ti, the model\u2019s parameters \u03b8 become \u03b8\u2032i. In our method, the updated parameter vector \u03b8\u2032i is computed using one or more gradient descent updates on task Ti. For example, when using one gradient update,\n\u03b8\u2032i = \u03b8 \u2212 \u03b1\u2207\u03b8LTi(f\u03b8).\nThe step size \u03b1may be fixed as a hyperparameter or learned with the model parameters. For simplicity of notation, we will consider one gradient update for the rest of this section, but using multiple gradient updates is a straightforward extension.\nThe model parameters are trained by optimizing for the performance of f\u03b8\u2032i with respect to \u03b8 across tasks sampled from p(T ). More concretely, the meta-objective is as follows:\nmin \u03b8 \u2211 Ti\u223cp(T ) LTi(f\u03b8\u2032i) = \u2211 Ti\u223cp(T ) LTi(f\u03b8\u2212\u03b1\u2207\u03b8LTi (f\u03b8))\nNote that the meta-optimization is performed over the model parameters \u03b8, whereas the objective is computed using the updated model parameters \u03b8\u2032. In effect, our proposed method aims to optimize the model parameters such that one or a small number of gradient steps on a new task will produce maximally effective behavior on that task.\nThe meta-optimization across tasks is performed via stochastic gradient descent (SGD), such that the model parameters \u03b8 are updated as follows:\n\u03b8 \u2190 \u03b8 \u2212 \u03b2\u2207\u03b8 \u2211\nTi\u223cp(T )\nLTi(f\u03b8\u2032i)\nwhere \u03b2 is the meta step size. The full algorithm, in the general case, is outlined in Algorithm 1."}, {"heading": "3. Species of MAML", "text": "In this section, we discuss specific instantiations of our meta-learning algorithm for supervised learning and reinforcement learning. The domains differ in the form of loss function and in how data is generated by the task and presented to the model, but the same basic adaptation mechanism can be applied in both cases."}, {"heading": "3.1. Supervised Regression and Classification", "text": "Few-shot learning is well-studied in the domain of supervised tasks, where the goal is to learn a new function from only a few input/output pairs for that task, using prior data\nAlgorithm 1 Model-Agnostic Meta-Learning Require: p(T ): distribution over tasks Require: \u03b1, \u03b2: step size hyperparameters\n1: randomly initialize \u03b8 2: while not done do 3: Sample batch of tasks Ti \u223c p(T ) 4: for all Ti do 5: Evaluate\u2207\u03b8LTi(f\u03b8) with respect to K examples 6: Compute adapted parameters with gradient descent: \u03b8\u2032i = \u03b8 \u2212 \u03b1\u2207\u03b8LTi(f\u03b8) 7: end for 8: Update \u03b8 \u2190 \u03b8 \u2212 \u03b2\u2207\u03b8 \u2211 Ti\u223cp(T ) LTi(f\u03b8\u2032i) 9: end while\nfrom similar tasks for meta-learning. For example, the goal might be to classify images of a Segway after seeing only one or a few examples of a Segway, with a model that has previously seen many other types of objects. Likewise, in few-shot regression, the goal is to predict the outputs of a continuous-valued function from only a few datapoints sampled from that function, after training on many functions with similar statistical properties.\nTo formalize the supervised regression and classification problems in the context of the meta-learning definitions in Section 2.1, we can define the horizon H = 1 and drop the timestep subscript on xt, since the model accepts a single input and produces a single output, rather than a sequence of inputs and outputs. The task Ti generates K i.i.d. observations x from qi, and the task loss is represented by the error between the model\u2019s output for x and the corresponding target values y for that observation and task.\nTwo common loss functions used for supervised classification and regression are cross-entropy and mean-squared error (MSE), which we will describe below, though other supervised loss functions may be used as well. For regression tasks using mean-squared error, the loss takes the form:\nLTi(f\u03c6) = \u2211\nx(j),y(j)\u223cTi\n\u2016f\u03c6(x(j))\u2212 y(j)\u20162, (1)\nwhere x(j),y(j) are an input/output pair sampled from task Ti. In K-shot regression tasks, K input/output pairs are provided for learning for each task.\nSimilarly, for discrete classification tasks with a crossentropy loss, the loss takes the form:\nLTi(f\u03c6) = \u2211\nx(j),y(j)\u223cTi\ny(j) log f\u03c6(x (j))\n+ (1\u2212 y(j)) log(1\u2212 f\u03c6(x(j))) (2)\nNote that, according to the conventional terminology, Kshot classification tasks useK input/output pairs from each\nAlgorithm 2 MAML for Few-Shot Supervised Learning Require: p(T ): distribution over tasks Require: \u03b1, \u03b2: step size hyperparameters\n1: randomly initialize \u03b8 2: while not done do 3: Sample batch of tasks Ti \u223c p(T ) 4: for all Ti do 5: Sample K datapoints D = {x(j),y(j)} from Ti 6: Evaluate \u2207\u03b8LTi(f\u03b8) using D and LTi in Equation (1) or (2) 7: Compute adapted parameters with gradient descent: \u03b8\u2032i = \u03b8 \u2212 \u03b1\u2207\u03b8LTi(f\u03b8) 8: Sample datapoints D\u2032i = {x(j),y(j)} from Ti for the meta-update 9: end for\n10: Update \u03b8 \u2190 \u03b8\u2212 \u03b2\u2207\u03b8 \u2211 Ti\u223cp(T ) LTi(f\u03b8\u2032i) using each D \u2032 i and LTi in Equation 1 or 2 11: end while\nclass, for a total of NK data points for N -way classification. Given a distribution over tasks p(Ti), these loss functions can be directly inserted into the equations in Section 2.2 to perform meta-learning, as detailed in Algorithm 2."}, {"heading": "3.2. Reinforcement Learning", "text": "In reinforcement learning (RL), the goal of meta-learning or few-shot learning is to enable an agent to quickly acquire a policy for a new test task using only a small amount of experience in the test setting. This might involve adapting the agent to achieve a new goal or to succeed on a previously trained goal in a new environment. For example, an agent might learn to quickly figure out how to navigate mazes so that, when faced with a new maze, it can determine how to reliably reach the exit with only a few samples. In this section, we will discuss how MAML can be applied to metalearning for RL.\nEach RL task Ti contains an initial state distribution qi(x1) and a transition distribution qi(xt+1|xt,at), and the loss LTi corresponds to the (negative) reward function R. The entire task is therefore a Markov decision process (MDP) with horizon H , where the learner is allowed to query a limited number of sample trajectories for few-shot learning. Any aspect of the MDP may change across tasks in p(T ). The model being learned, f\u03b8, is a policy that maps from states xt to a distribution over actions at at each timestep t \u2208 {1, ...,H}. The loss for task Ti and model f\u03c6 takes the form\nLTi(f\u03c6) = \u2212Ext,at\u223cf\u03c6,qTi [ H\u2211 t=1 Ri(xt,at) ] . (3)\nIn K-shot reinforcement learning, K rollouts from f\u03b8 and task Ti, (x1,a1, ...xH), and the corresponding rewards\nAlgorithm 3 MAML for Reinforcement Learning Require: p(T ): distribution over tasks Require: \u03b1, \u03b2: step size hyperparameters\n1: randomly initialize \u03b8 2: while not done do 3: Sample batch of tasks Ti \u223c p(T ) 4: for all Ti do 5: Sample K trajectoriesD = {(x1,a1, ...xH)} using f\u03b8 in Ti 6: Evaluate\u2207\u03b8LTi(f\u03b8) using D and LTi in Equation 3 7: Compute adapted parameters with gradient descent: \u03b8\u2032i = \u03b8 \u2212 \u03b1\u2207\u03b8LTi(f\u03b8) 8: Sample trajectories D\u2032i = {(x1,a1, ...xH)} using f\u03b8\u2032i in Ti 9: end for\n10: Update \u03b8 \u2190 \u03b8\u2212 \u03b2\u2207\u03b8 \u2211 Ti\u223cp(T ) LTi(f\u03b8\u2032i) using each D \u2032 i and LTi in Equation 3 11: end while\nR(xt,at), may be used for adaptation on a new task Ti. Since the expected reward is generally not differentiable due to unknown dynamics, we use policy gradient methods to estimate the gradient both for the model gradient update(s) and the meta-optimization. Since policy gradients are an on-policy algorithm, each additional gradient step during the adaptation of f\u03b8 requires new samples from the current policy f\u03b8i\u2032 . We detail the algorithm in Algorithm 3. This algorithm has the same structure as Algorithm 2, with the principal difference being that steps 5 and 8 require sampling trajectories from the environment corresponding to task Ti. Practical implementations of this method may also use a variety of improvements recently proposed for policy gradient algorithms, including state or action-dependent baselines and trust regions (Schulman et al., 2015)."}, {"heading": "4. Related Work", "text": "The method that we propose in this paper addresses the general problem of meta-learning (Thrun & Pratt, 1998; Schmidhuber, 1987; Naik & Mammone, 1992), which includes one-shot and few-shot learning. One of the popular approaches for meta-learning with neural networks is to train a meta-learner that learns how to update the weights of the learner\u2019s model (Bengio et al., 1992; Schmidhuber, 1992; Bengio et al., 1990). This approach has been applied to learning to optimize neural networks (Hochreiter et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2017), as well as for learning dynamically changing recurrent neural networks (Ha et al., 2017). Similar methods have also been proposed that use evolutionary algorithms (Mengistu et al., 2016). One recent approach learns both the weight initialization and the optimizer, for the purpose of few-shot image recognition (Ravi & Larochelle, 2017). Unlike these prior methods, the MAML learner\u2019s weights are updated using\nthe gradient, rather than a learned update rule. Our method does not introduce any additional parameters into the learning process and does not require a particular learner model architecture.\nFew-shot learning methods have also been developed for specific tasks such as generative modeling (Edwards & Storkey, 2017; Rezende et al., 2016) and image recognition (Vinyals et al., 2016). One successful approach for few-shot classification is to learn to compare new examples using e.g. Siamese networks (Koch, 2015) or recurrence with attention mechanisms (Vinyals et al., 2016). These approaches have generated some of the most successful results, but are difficult to directly extend to other problems, such as reinforcement learning. Our method, in contrast, is agnostic to the form of the model and to the particular learning task.\nAnother approach to meta-learning is to train memoryaugmented models on many tasks, where each training episode corresponds to a new task and the learner adapts to the new task as the recurrent network is rolled out. Memory augmented networks have been applied to few-shot recognition (Santoro et al., 2016) and learning \u201cfast\u201d reinforcement learning agents with recurrent policies (Duan et al., 2016b; Wang et al., 2016). Memory augmentation can also take the form of a recurrent network with Hebbian learning updates (Ba et al., 2016). This latter approach is inspired by the idea of using \u201cfast weights\u201d in addition to the standard \u201cslow\u201d weights in neural networks (Hinton & Plaut, 1987). Our experiments show that our method outperforms the recurrent approach on few-shot classification. Furthermore, unlike the recurrent methods, our approach simply provides a good weight initialization, which means it is straightforward to fine-tune for additional gradient steps if time and data are available. This view is somewhat similar to the fast weight adaptation, where the initial weights change slowly across trials. However, in contrast to the recurrent and fast weights approaches, where the \u201cfast\u201d and \u201cslow\u201d learning are performed by very different mechanisms, our method uses the same gradient descent update for both.\nOur approach is also related to methods for weight initialization in neural networks, in that our meta-learning mechanism aims to learn weights that can be adapted rapidly to new tasks. Several prior works have explored both dataindependent and data-dependent weight initializations for accelerating learning. In computer vision, models pretrained on large-scale image classification tasks have been shown to learn effective features for a wide range of vision problems (Donahue et al., 2014). In contrast to this type of pretraining, our method explicitly optimizes the model for adaptability, allowing it to adapt to new tasks with a minuscule number of examples. As discussed in Section 1, our method can also be viewed as explicitly maximizing\nthe sensitivity of the losses of new tasks to the parameters of the network, so that small parameter changes can lead to large improvements in the loss. A number of prior works have explored the topic of sensitivity in deep networks, often in the context of studying initialization techniques for weight matrices (Saxe et al., 2014; Kirkpatrick et al., 2016). Most of these works have looked at good random initializations, though a number of papers have also addressed datadependent initializers (Kra\u0308henbu\u0308hl et al., 2016; Salimans & Kingma, 2016). In contrast, our method explicitly trains the parameters for sensitivity on a given task distribution, allowing for extremely efficient adaptation for problems such as K-shot learning and rapid reinforcement learning."}, {"heading": "5. Experimental Evaluation", "text": "The goal of our experimental evaluation is to answer the following questions: (1) Can MAML enable fast learning of new tasks? (2) Does MAML work for multiple different domains, namely supervised regression, classification, and reinforcement learning? (3) Can a model learned with MAML continue to improve with additional gradient updates and/or examples?\nTo this end, we evaluate our method on meta-learning problems from these three domains, exploring problems of varying difficulty and character. All of the problems we consider require some amount of adaptation to a new task at test-time. Whenever possible, we compare our results to an oracle that receives the identity of the task (which is a problem-dependent representation) as an additional input, as an upper bound on the performance of the model.\nAll of the experiments were performed using TensorFlow (Abadi et al., 2016), which allows for automatic differentiation through the gradient update(s) during metalearning. The code for all of the experiments will be released."}, {"heading": "5.1. Regression", "text": "We start with a simple regression problem that illustrates the basic principles of MAML. In this problem, the task is to regress from the input to the output of a sine wave, where the amplitude and phase of the sinusoid are varied between tasks. Thus, p(T ) is continuous, where the amplitude varies within [0.1, 5.0] and the phase varies within [0, \u03c0], and the input and output both have a dimensionality of 1. During training and testing, datapoints are sampled uniformly from the input range, x \u2208 [\u22125.0, 5.0], and the loss is the mean-squared error between the prediction f(x) and true value. The regressor is a neural network model with 2 hidden layers of size 40 with ReLU nonlinearities. When training with MAML, we use one gradient update withK = 10 examples with a fixed step size \u03b1 = 0.01, and\nuse Adam with the default hyperparameters as the metaoptimizer (Kingma & Ba, 2015). The baselines are likewise trained with Adam. To evaluate performance, we finetune a single meta-learned model on varying numbers ofK examples, and compare performance to two baselines: (a) pretraining on all of the tasks, which entails training a network to regress to random sinusoid functions and then, at test-time, fine-tuning with gradient descent on the K provided points, using an automatically tuned step size, and (b) an oracle which receives the true amplitude and phase as input.\nWe evaluate performance by fine-tuning the model learned by MAML and the pretrained model on K = {5, 10, 20} datapoints. During fine-tuning, each gradient step is computed using the same K datapoints. The qualitative results, shown in Figure 3 and further expanded on in Appendix B show that the learned model is able to quickly adapt with only 5 datapoints, shown as purple triangles, whereas the model that is pretrained using standard supervised learning on all tasks is unable to adequately adapt with so few datapoints without catastrophic overfitting. Crucially, when the K datapoints are all in one half of the input range, the model trained with MAML can still infer the amplitude and phase in the other half of the range, demonstrating that the MAML trained model f has learned to model the periodic nature of the sine wave. Furthermore, we observe both in the qualitative and quantitative results (Figure 2) that\nthe model learned with MAML is able to continue to improve with additional gradient steps, despite being trained for maximal performance after one gradient step. This improvement suggests that MAML optimizes the parameters such that they lie in a region that is amenable to fast adaptation and is sensitive to loss functions from p(T ), as discussed in Section 2.2, rather than overfitting to parameters vectors \u03b8 that can be maximally improved in one step."}, {"heading": "5.2. Classification", "text": "To evaluate MAML in comparison to prior meta-learning and few-shot learning algorithms, we evaluate our method on a few-shot character recognition task using the Omniglot dataset (Lake et al., 2011), which consists 20 instances of 1623 characters from 50 different alphabets. Each instance was drawn by a different person. Omniglot is a standard benchmark for few-shot learning used in a number of prior works (Vinyals et al., 2016; Santoro et al., 2016). We follow the experimental protocol proposed by Vinyals et al. (2016), which involves fast learning of N -way classification with 1 or 5 shots. The problem of N -way classification is set up as follows: select N unseen character classes, provide the model with K different instances of each of the N classes, and evaluate the model\u2019s ability to classify new instances within the N classes. We randomly select 1200 characters for training, irrespective of alphabet, and use the remaining for testing. The dataset\nTable 1. Few-Shot Omniglot classification on held-out characters. MAML achieves results that are comparable to the state-of-the-art with convolutional models, and outperforms recurrent models with fully-connected networks. Siamese nets, matching nets, and the memory module approaches are all specific to classification tasks, and are not directly applicable to regression or RL scenarios. The \u00b1 shows one standard deviation over tasks. Note that the results may not be strictly comparable in all cases since the train/test splits used in the prior work were not available.\n5-way Accuracy 20-way Accuracy 1-shot 5-shot 1-shot 5-shot\nMANN, no conv (Santoro et al., 2016) 82.8% 94.9% \u2013 \u2013 MAML, no conv (ours) 89.7\u00b1 2.8% 97.5\u00b1 1.2% \u2013 \u2013 Siamese nets (Koch, 2015) 97.3% 98.4% 88.2% 97.0% matching nets (Vinyals et al., 2016) 98.1% 98.9% 93.8% 98.5% neural statistician (Edwards & Storkey, 2017) 98.1% 99.5% 93.2% 98.1% memory mod. (Kaiser et al., 2017) 98.4% 99.6% 95.0% 98.6% MAML (ours) 98.7\u00b1 0.8% 99.9\u00b1 0.3% 93.1\u00b1 1.5% 98.8\u00b1 0.6%\nis augmented with rotations by multiples of 90 degrees, as proposed by Santoro et al. (2016).\nOur model follows the same architecture as the embedding function used by Vinyals et al. (2016), which has 4 modules with a 3\u00d7 3 convolutions and 64 filters, followed by batch normalization (Ioffe & Szegedy, 2015), a ReLU nonlinearity, and 2\u00d72 max-pooling. The Omniglot images are downsampled to 28\u00d728, so the dimensionality of the last hidden layer is 64. As in the baseline classifier used by Vinyals et al. (2016), the last layer is fed into a softmax. The only difference in our model is that we use strided convolutions instead of max-pooling. In order to also provide a fair comparison against memory-augmented neural networks (Santoro et al., 2016) and to test the flexibility of MAML, we also experiment with a non-convolutional network. For this, we use a network with 4 hidden layers with sizes 256, 128, 64, 64, each including batch normalization and ReLU nonlinearities, followed by a linear layer and softmax. For both models, the loss function is the cross-entropy error between the predicted and true class. Additional hyperparameter details are included in Appendix A.1.\nWe present the results in Table 1. The convolutional model learned by MAML compares well to the state-of-the-art results on this task, typically narrowly outperforming the prior methods. Some of these existing methods, such as matching networks, Siamese networks, and memory models are designed with few-shot classification in mind, and aren\u2019t readily applicable to domains such as reinforcement learning. Additionally, the model learned with MAML uses fewer overall parameters compared to matching networks, since the algorithm does not introduce any additional parameters beyond the weights of the classifier itself. Memory-augmented neural networks (Santoro et al., 2016) specifically, and recurrent meta-learning models in general, represent a more broadly applicable class of methods that can be used for other tasks such as reinforcement learning (Duan et al., 2016b; Wang et al., 2016). However, as shown in the non-convolutional comparison, MAML sig-\nnificantly outperforms memory-augmented networks on 5- way classification, both in the 1-shot and 5-shot case."}, {"heading": "5.3. Reinforcement Learning", "text": "To evaluate MAML on reinforcement learning problems, we constructed several sets of tasks based off of the simulated continuous control environments in the rllab benchmark suite (Duan et al., 2016a). We discuss the individual domains below. In all of the domains, the model trained by MAML is a neural network policy with two hidden layers of size 100, with ReLU nonlinearities. The gradient updates are computed using vanilla policy gradient (REINFORCE) (Williams, 1992), and we use trust-region policy optimization (TRPO) as the meta-optimizer (Schulman et al., 2015). In order to avoid computing third derivatives, we use finite differences to compute the Hessian-vector products for TRPO. For both learning and meta-learning updates, we use the standard linear feature baseline proposed by Duan et al. (2016a), which is fitted separately at each iteration for each sampled task in the batch. We compare to three baseline models: (a) pretraining one policy on all of the tasks and then fine-tuning, (b) training a policy from randomly initialized weights, and (c) an oracle policy which receives the parameters of the task as input, which\nfor the tasks below corresponds to a goal position, goal direction, or goal velocity for the agent. The baseline models of (a) and (b) are fine-tuned with gradient descent with a manually tuned step size. Videos of the learned policies can be viewed at sites.google.com/view/maml\n2D Navigation. In our first meta-RL experiment, we study a set of tasks where a point agent must move to different goal positions in 2D, randomly chosen for each task within a unit square. The observation is the current 2D position, and actions correspond to velocity commands clipped to be in the range [\u22120.1, 0.1]. The reward is the negative squared distance to the goal, and episodes terminate when the agent is within 0.01 of the goal or at the horizon ofH = 100. The policy was trained with MAML to maximize performance after 1 policy gradient update using 20 trajectories. Additional hyperparameter settings for this problem and the following RL problems are in Appendix A.2. In our evaluation, we compare adaptation to a new task with up to 4 gradient updates, each with 40 samples. The results in Figure 4 show the adaptation performance of models that are initialized with MAML, conventional pretraining on the same set of tasks, random initialization, and an oracle policy that receives the goal position as input. The results show that MAML can learn a model that adapts much more quickly in a single gradient update, and furthermore continues to improve with additional updates.\nLocomotion. To study how well MAML can scale to more complex deep RL problems, we also study adaptation on high-dimensional locomotion tasks with the MuJoCo simulator (Todorov et al., 2012). The tasks require two simulated robots \u2013 a planar cheetah and a 3D quadruped (the \u201cant\u201d) \u2013 to run in a particular direction or at a particular velocity. In the goal velocity experiments, the reward is the negative absolute value between the current velocity of the agent and a goal, which is chosen uniformly at random between 0.0 and 2.0 for the cheetah and between 0.0 and 3.0 for the ant. In the goal direction experiments, the reward is the magnitude of the velocity in either the forward or backward direction, chosen at random for each task in p(T ). The horizon is H = 200, with 20 rollouts per gradient step for all problems except the ant forward/backward\ntask, which used 40 rollouts per step. The results in Figure 5 show that MAML learns a model that can quickly adapt its velocity and direction with even just a single gradient update, and continues to improve with more gradient steps. The results also show that, on these challenging tasks, the MAML initialization substantially outperforms random initialization and pretraining. In fact, pretraining is in some cases worse than random initialization, a fact observed in prior RL work (Parisotto et al., 2016)."}, {"heading": "6. Discussion and Future Work", "text": "We introduced a meta-learning algorithm based on learning easily adaptable network parameters through gradient descent. Our approach has a number of benefits. It is simple and does not introduce any additional learned parameters in the meta-learning. It can be combined with any model representation that is amenable to training through gradient descent, and any differentiable objective, including classification, regression, and reinforcement learning. Lastly, since our method merely produces a good weight initialization, adaptation can be performed with any amount of data and any number of gradient steps, though we demonstrate that it can achieve state-of-the-art results on classification with only one or five examples per class. We also show that our method can adapt an RL agent through policy gradients using a very modest amount of experience.\nThough our method is simple and broadly applicable, it does have a number of limitations. Since the meta-learning process trains the weights with a fixed, small number of adaptation steps, it is possible for this phase to overfit, such that a multitude of gradient steps will actually increase test error on a new task. However, in all of our experiments, we did not observe this overfitting. Our method also introduces one additional hyperparameter, which is the model\u2019s step size \u03b1. In practice, we found that this step size was relatively straightforward to tune, since it acts very much like the learning rate in a conventional stochastic gradient descent optimizer. However, automatically learning this parameter during meta-learning is an interesting direction for future work.\nReusing knowledge from past tasks may be a crucial ingredient in making high-capacity scalable models, such as deep neural networks, amenable to fast training with small datasets. We believe that this work is one step toward a simple and general-purpose meta-learning technique that can be applied to any problem and any model. Further research in this area can make multitask initialization a standard ingredient in deep learning and reinforcement learning."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Xi Chen and Trevor Darrell for helpful discussions, Yan Duan and Alex Lee for technical advice, and Nikhil Mishra, Haoran Tang, and Greg Kahn for feedback on an early draft of the paper. This work was supported in part by an ONR PECASE award and an NSF GRFP award."}, {"heading": "A. Additional Experiment Details", "text": "In this section, we provide additional details of the experimental set-up and hyperparameters.\nA.1. Classification\nFor N-way classification, the model learned by MAML was trained for N-way, 1-shot classification and evaluated on 1-shot and 5-shot evaluation. For 5-shot evaluation, we simply increased the batch size used for each gradient update from N examples to 5N examples. The 5-way convolutional and non-convolutional MAML models were each trained with 1 gradient step with step size \u03b1 = 0.4 and a meta batch-size of 32 tasks. The network was evaluated using 3 gradient steps with the same step size \u03b1 = 0.4. The 20-way convolutional MAML model was trained and evaluated with 5 gradient steps with step size \u03b1 = 0.1. During training, the meta batch size was set to 16 tasks. All models were trained for 25000 iterations on a single NVIDIA Pascal Titan X GPU.\nA.2. Reinforcement Learning\nIn all reinforcement learning experiments, the MAML policy was trained using a single gradient step with \u03b1 = 0.1.\nDuring evaluation, we found that halving the learning rate after the first gradient step produced superior performance. Thus, the step size during adaptation was set to \u03b1 = 0.1 for the first step, and \u03b1 = 0.05 for all future steps. The step sizes for the baseline methods were manually tuned for each domain. In the 2D navigation, we used a meta batch size of 20; in the locomotion problems, we used a meta batch size of 40 tasks. The MAML models were trained for up to 500 meta-iterations, and the model with the best average return during training was used for evaluation.\nFor the ant goal velocity tasks, we included an alive reward bonus at each time-step such that the optimal policy is to reach the goal velocity rather than to die. The quantitative results are averaged over 40 randomly sampled tasks."}, {"heading": "B. Additional Sinusoid Visualizations", "text": "In Figure 6, we show the qualitative performance of MAML and the pretrained baseline on randomly sampled sinusoids."}], "references": [{"title": "Learning to learn by gradient descent by gradient descent", "author": ["Andrychowicz", "Marcin", "Denil", "Misha", "Gomez", "Sergio", "Hoffman", "Matthew W", "Pfau", "David", "Schaul", "Tom", "de Freitas", "Nando"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Using fast weights to attend to the recent past", "author": ["Ba", "Jimmy", "Hinton", "Geoffrey E", "Mnih", "Volodymyr", "Leibo", "Joel Z", "Ionescu", "Catalin"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Ba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "On the optimization of a synaptic learning rule", "author": ["Bengio", "Samy", "Yoshua", "Cloutier", "Jocelyn", "Gecsei", "Jan"], "venue": "In Optimality in Artificial and Biological Neural Networks, pp", "citeRegEx": "Bengio et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1992}, {"title": "Learning a synaptic learning", "author": ["Bengio", "Yoshua", "Samy", "Cloutier", "Jocelyn"], "venue": "rule. Universite\u0301 de Montre\u0301al, De\u0301partement d\u2019informatique et de recherche ope\u0301rationnelle,", "citeRegEx": "Bengio et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1990}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Donahue", "Jeff", "Jia", "Yangqing", "Vinyals", "Oriol", "Hoffman", "Judy", "Zhang", "Ning", "Tzeng", "Eric", "Darrell", "Trevor"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Rl2: Fast reinforcement learning via slow reinforcement learning", "author": ["Duan", "Yan", "Schulman", "John", "Chen", "Xi", "Bartlett", "Peter L", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1611.02779,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Towards a neural statistician", "author": ["Edwards", "Harrison", "Storkey", "Amos"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Edwards et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Edwards et al\\.", "year": 2017}, {"title": "Using fast weights to deblur old memories", "author": ["Hinton", "Geoffrey E", "Plaut", "David C"], "venue": "In Conference of the Cognitive Science Society (CogSci),", "citeRegEx": "Hinton et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1987}, {"title": "Learning to learn using gradient descent", "author": ["Hochreiter", "Sepp", "Younger", "A Steven", "Conwell", "Peter R"], "venue": "In International Conference on Artificial Neural Networks. Springer,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Learning to remember rare", "author": ["Kaiser", "Lukasz", "Nachum", "Ofir", "Roy", "Aurko", "Bengio", "Samy"], "venue": "events. International Conference on Learning Representations (ICLR),", "citeRegEx": "Kaiser et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Siamese neural networks for one-shot image recognition", "author": ["Koch", "Gregory"], "venue": "ICML Deep Learning Workshop,", "citeRegEx": "Koch and Gregory.,? \\Q2015\\E", "shortCiteRegEx": "Koch and Gregory.", "year": 2015}, {"title": "Data-dependent initializations of convolutional neural networks", "author": ["Kr\u00e4henb\u00fchl", "Philipp", "Doersch", "Carl", "Donahue", "Jeff", "Darrell", "Trevor"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Kr\u00e4henb\u00fchl et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kr\u00e4henb\u00fchl et al\\.", "year": 2016}, {"title": "One shot learning of simple visual concepts", "author": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan", "Gross", "Jason", "Tenenbaum", "Joshua B"], "venue": "In Conference of the Cognitive Science Society (CogSci),", "citeRegEx": "Lake et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2011}, {"title": "Learning to optimize", "author": ["Li", "Ke", "Malik", "Jitendra"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Evolvability search: Directly selecting for evolvability in order to study and produce", "author": ["Mengistu", "Henok", "Lehman", "Joel", "Clune", "Jeff"], "venue": null, "citeRegEx": "Mengistu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mengistu et al\\.", "year": 2016}, {"title": "Meta-neural networks that learn by learning", "author": ["Naik", "Devang K", "Mammone", "RJ"], "venue": "In International Joint Conference on Neural Netowrks (IJCNN),", "citeRegEx": "Naik et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Naik et al\\.", "year": 1992}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Parisotto", "Emilio", "Ba", "Jimmy Lei", "Salakhutdinov", "Ruslan"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Parisotto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2016}, {"title": "Optimization as a model for few-shot learning", "author": ["Ravi", "Sachin", "Larochelle", "Hugo"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Ravi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2017}, {"title": "One-shot generalization in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Danihelka", "Ivo", "Gregor", "Karol", "Wierstra", "Daan"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2016}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Salimans", "Tim", "Kingma", "Diederik P"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Meta-learning with memory-augmented neural networks", "author": ["Santoro", "Adam", "Bartunov", "Sergey", "Botvinick", "Matthew", "Wierstra", "Daan", "Lillicrap", "Timothy"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew", "McClelland", "James", "Ganguli", "Surya"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Evolutionary principles in selfreferential learning. On learning how to learn: The meta-meta-.", "author": ["Schmidhuber", "Jurgen"], "venue": "hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich,", "citeRegEx": "Schmidhuber and Jurgen.,? \\Q1987\\E", "shortCiteRegEx": "Schmidhuber and Jurgen.", "year": 1987}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1992}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Learning to learn", "author": ["Thrun", "Sebastian", "Pratt", "Lorien"], "venue": "Springer Science & Business Media,", "citeRegEx": "Thrun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1998}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Matching networks for one shot learning", "author": ["Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Tim", "Wierstra", "Daan"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Learning to reinforcement learn", "author": ["Wang", "Jane X", "Kurth-Nelson", "Zeb", "Tirumala", "Dhruva", "Soyer", "Hubert", "Leibo", "Joel Z", "Munos", "Remi", "Blundell", "Charles", "Kumaran", "Dharshan", "Botvinick", "Matt"], "venue": "arXiv preprint arXiv:1611.05763,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 2, "context": "Unlike prior meta-learning methods that learn an update function or learning rule (Schmidhuber, 1987; Bengio et al., 1992; Andrychowicz et al., 2016; Ravi & Larochelle, 2017), our algorithm does not expand the number of learned parameters nor place constraints on the model architecture (e.", "startOffset": 82, "endOffset": 174}, {"referenceID": 0, "context": "Unlike prior meta-learning methods that learn an update function or learning rule (Schmidhuber, 1987; Bengio et al., 1992; Andrychowicz et al., 2016; Ravi & Larochelle, 2017), our algorithm does not expand the number of learned parameters nor place constraints on the model architecture (e.", "startOffset": 82, "endOffset": 174}, {"referenceID": 22, "context": "by requiring a recurrent model (Santoro et al., 2016) or a Siamese network (Koch, 2015)), and it can be readily combined with fully connected, convolutional, or recurrent neural networks.", "startOffset": 31, "endOffset": 53}, {"referenceID": 22, "context": "In contrast to prior work, which has sought to train recurrent neural networks that ingest entire datasets (Santoro et al., 2016; Duan et al., 2016b) or feature embeddings that can be combined with nonparametric methods at test time (Vinyals et al.", "startOffset": 107, "endOffset": 149}, {"referenceID": 29, "context": ", 2016b) or feature embeddings that can be combined with nonparametric methods at test time (Vinyals et al., 2016; Koch, 2015), we propose a method that can learn the parameters of any standard model via meta-learning in such a way as to prepare that model for fast adaptation.", "startOffset": 92, "endOffset": 126}, {"referenceID": 26, "context": "Practical implementations of this method may also use a variety of improvements recently proposed for policy gradient algorithms, including state or action-dependent baselines and trust regions (Schulman et al., 2015).", "startOffset": 194, "endOffset": 217}, {"referenceID": 2, "context": "One of the popular approaches for meta-learning with neural networks is to train a meta-learner that learns how to update the weights of the learner\u2019s model (Bengio et al., 1992; Schmidhuber, 1992; Bengio et al., 1990).", "startOffset": 157, "endOffset": 218}, {"referenceID": 3, "context": "One of the popular approaches for meta-learning with neural networks is to train a meta-learner that learns how to update the weights of the learner\u2019s model (Bengio et al., 1992; Schmidhuber, 1992; Bengio et al., 1990).", "startOffset": 157, "endOffset": 218}, {"referenceID": 8, "context": "This approach has been applied to learning to optimize neural networks (Hochreiter et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2017), as well as for learning dynamically changing recurrent neural networks (Ha et al.", "startOffset": 71, "endOffset": 141}, {"referenceID": 0, "context": "This approach has been applied to learning to optimize neural networks (Hochreiter et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2017), as well as for learning dynamically changing recurrent neural networks (Ha et al.", "startOffset": 71, "endOffset": 141}, {"referenceID": 16, "context": "Similar methods have also been proposed that use evolutionary algorithms (Mengistu et al., 2016).", "startOffset": 73, "endOffset": 96}, {"referenceID": 20, "context": "Few-shot learning methods have also been developed for specific tasks such as generative modeling (Edwards & Storkey, 2017; Rezende et al., 2016) and image recognition (Vinyals et al.", "startOffset": 98, "endOffset": 145}, {"referenceID": 29, "context": ", 2016) and image recognition (Vinyals et al., 2016).", "startOffset": 30, "endOffset": 52}, {"referenceID": 29, "context": "Siamese networks (Koch, 2015) or recurrence with attention mechanisms (Vinyals et al., 2016).", "startOffset": 70, "endOffset": 92}, {"referenceID": 22, "context": "Memory augmented networks have been applied to few-shot recognition (Santoro et al., 2016) and learning \u201cfast\u201d reinforcement learning agents with recurrent policies (Duan et al.", "startOffset": 68, "endOffset": 90}, {"referenceID": 30, "context": ", 2016) and learning \u201cfast\u201d reinforcement learning agents with recurrent policies (Duan et al., 2016b; Wang et al., 2016).", "startOffset": 82, "endOffset": 121}, {"referenceID": 1, "context": "Memory augmentation can also take the form of a recurrent network with Hebbian learning updates (Ba et al., 2016).", "startOffset": 96, "endOffset": 113}, {"referenceID": 4, "context": "In computer vision, models pretrained on large-scale image classification tasks have been shown to learn effective features for a wide range of vision problems (Donahue et al., 2014).", "startOffset": 160, "endOffset": 182}, {"referenceID": 23, "context": "A number of prior works have explored the topic of sensitivity in deep networks, often in the context of studying initialization techniques for weight matrices (Saxe et al., 2014; Kirkpatrick et al., 2016).", "startOffset": 160, "endOffset": 205}, {"referenceID": 13, "context": "Most of these works have looked at good random initializations, though a number of papers have also addressed datadependent initializers (Kr\u00e4henb\u00fchl et al., 2016; Salimans & Kingma, 2016).", "startOffset": 137, "endOffset": 187}, {"referenceID": 14, "context": "To evaluate MAML in comparison to prior meta-learning and few-shot learning algorithms, we evaluate our method on a few-shot character recognition task using the Omniglot dataset (Lake et al., 2011), which consists 20 instances of 1623 characters from 50 different alphabets.", "startOffset": 179, "endOffset": 198}, {"referenceID": 29, "context": "Omniglot is a standard benchmark for few-shot learning used in a number of prior works (Vinyals et al., 2016; Santoro et al., 2016).", "startOffset": 87, "endOffset": 131}, {"referenceID": 22, "context": "Omniglot is a standard benchmark for few-shot learning used in a number of prior works (Vinyals et al., 2016; Santoro et al., 2016).", "startOffset": 87, "endOffset": 131}, {"referenceID": 14, "context": "To evaluate MAML in comparison to prior meta-learning and few-shot learning algorithms, we evaluate our method on a few-shot character recognition task using the Omniglot dataset (Lake et al., 2011), which consists 20 instances of 1623 characters from 50 different alphabets. Each instance was drawn by a different person. Omniglot is a standard benchmark for few-shot learning used in a number of prior works (Vinyals et al., 2016; Santoro et al., 2016). We follow the experimental protocol proposed by Vinyals et al. (2016), which involves fast learning of N -way classification with 1 or 5 shots.", "startOffset": 180, "endOffset": 526}, {"referenceID": 22, "context": "5-way Accuracy 20-way Accuracy 1-shot 5-shot 1-shot 5-shot MANN, no conv (Santoro et al., 2016) 82.", "startOffset": 73, "endOffset": 95}, {"referenceID": 29, "context": "matching nets (Vinyals et al., 2016) 98.", "startOffset": 14, "endOffset": 36}, {"referenceID": 10, "context": "(Kaiser et al., 2017) 98.", "startOffset": 0, "endOffset": 21}, {"referenceID": 22, "context": "is augmented with rotations by multiples of 90 degrees, as proposed by Santoro et al. (2016).", "startOffset": 71, "endOffset": 93}, {"referenceID": 22, "context": "In order to also provide a fair comparison against memory-augmented neural networks (Santoro et al., 2016) and to test the flexibility of MAML, we also experiment with a non-convolutional network.", "startOffset": 84, "endOffset": 106}, {"referenceID": 28, "context": "Our model follows the same architecture as the embedding function used by Vinyals et al. (2016), which has 4 modules with a 3\u00d7 3 convolutions and 64 filters, followed by batch normalization (Ioffe & Szegedy, 2015), a ReLU nonlinearity, and 2\u00d72 max-pooling.", "startOffset": 74, "endOffset": 96}, {"referenceID": 28, "context": "Our model follows the same architecture as the embedding function used by Vinyals et al. (2016), which has 4 modules with a 3\u00d7 3 convolutions and 64 filters, followed by batch normalization (Ioffe & Szegedy, 2015), a ReLU nonlinearity, and 2\u00d72 max-pooling. The Omniglot images are downsampled to 28\u00d728, so the dimensionality of the last hidden layer is 64. As in the baseline classifier used by Vinyals et al. (2016), the last layer is fed into a softmax.", "startOffset": 74, "endOffset": 417}, {"referenceID": 22, "context": "Memory-augmented neural networks (Santoro et al., 2016) specifically, and recurrent meta-learning models in general, represent a more broadly applicable class of methods that can be used for other tasks such as reinforcement learning (Duan et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 30, "context": ", 2016) specifically, and recurrent meta-learning models in general, represent a more broadly applicable class of methods that can be used for other tasks such as reinforcement learning (Duan et al., 2016b; Wang et al., 2016).", "startOffset": 186, "endOffset": 225}, {"referenceID": 26, "context": "The gradient updates are computed using vanilla policy gradient (REINFORCE) (Williams, 1992), and we use trust-region policy optimization (TRPO) as the meta-optimizer (Schulman et al., 2015).", "startOffset": 167, "endOffset": 190}, {"referenceID": 5, "context": "To evaluate MAML on reinforcement learning problems, we constructed several sets of tasks based off of the simulated continuous control environments in the rllab benchmark suite (Duan et al., 2016a). We discuss the individual domains below. In all of the domains, the model trained by MAML is a neural network policy with two hidden layers of size 100, with ReLU nonlinearities. The gradient updates are computed using vanilla policy gradient (REINFORCE) (Williams, 1992), and we use trust-region policy optimization (TRPO) as the meta-optimizer (Schulman et al., 2015). In order to avoid computing third derivatives, we use finite differences to compute the Hessian-vector products for TRPO. For both learning and meta-learning updates, we use the standard linear feature baseline proposed by Duan et al. (2016a), which is fitted separately at each iteration for each sampled task in the batch.", "startOffset": 179, "endOffset": 814}, {"referenceID": 28, "context": "complex deep RL problems, we also study adaptation on high-dimensional locomotion tasks with the MuJoCo simulator (Todorov et al., 2012).", "startOffset": 114, "endOffset": 136}, {"referenceID": 18, "context": "In fact, pretraining is in some cases worse than random initialization, a fact observed in prior RL work (Parisotto et al., 2016).", "startOffset": 105, "endOffset": 129}], "year": 2017, "abstractText": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on a fewshot image classification benchmark, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.", "creator": "LaTeX with hyperref package"}}}