{"id": "1703.04933", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Sharp Minima Can Generalize For Deep Nets", "abstract": "indicating their particular capacity to connect, formal learning architectures tend to generalize relatively well to unseen data, requiring them to be deployed in practice. however, explaining why this bias within case explains here an open area of research. one standing hypothesis that is gaining popularity, e. g. hochreiter & amp ; schmidhuber ( 1997 ) ; keskar et al. ( 2017 ), is all the flatness of minima of the loss function found by normal gradient based methods results in smooth generalization. this paper argues that most notions like flatness are problematic for ideal models and can not be directly assessed to explain efficiency. specifically, when focusing on deep networks with rectifier units, we can exploit the particular breadth of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily similar minima. furthermore, if we allow to reparametrize a function, the geometry of varying parameters can appear drastically accordingly affecting its generalization properties.", "histories": [["v1", "Wed, 15 Mar 2017 05:12:25 GMT  (147kb,D)", "http://arxiv.org/abs/1703.04933v1", "8.5 pages of main content, 2.5 of bibliography and 1 page of appendix"], ["v2", "Mon, 15 May 2017 23:33:19 GMT  (148kb,D)", "http://arxiv.org/abs/1703.04933v2", "8.5 pages of main content, 2.5 of bibliography and 1 page of appendix"]], "COMMENTS": "8.5 pages of main content, 2.5 of bibliography and 1 page of appendix", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["laurent dinh", "razvan pascanu", "samy bengio", "yoshua bengio"], "accepted": true, "id": "1703.04933"}, "pdf": {"name": "1703.04933.pdf", "metadata": {"source": "META", "title": "Sharp Minima Can Generalize For Deep Nets", "authors": ["Laurent Dinh", "Razvan Pascanu", "Samy Bengio", "Yoshua Bengio"], "emails": ["<laurent.dinh@umontreal.ca>."], "sections": [{"heading": "1 Introduction", "text": "Deep learning techniques have been very successful in several domains, like object recognition in images (e.g Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016), machine translation (e.g. Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2016) and speech recognition (e.g. Graves et al., 2013; Hannun et al., 2014; Chorowski et al., 2015; Chan et al., 2016; Collobert et al., 2016). Several arguments have been brought forward to justify these empirical results. From a representational point of view, it has been argued that deep networks can efficiently approximate certain functions (e.g. Montufar et al., 2014;\n1Universit\u00e9 of Montr\u00e9al, Montr\u00e9al, Canada 2DeepMind, London, United Kingdom 3Google Brain, Mountain View, United States 4CIFAR Senior Fellow. Correspondence to: Laurent Dinh <laurent.dinh@umontreal.ca>.\nRaghu et al., 2016). Other works (e.g Dauphin et al., 2014; Choromanska et al., 2015) have looked at the structure of the error surface to analyze how trainable these models are. Finally, another point of discussion is how well these models can generalize (Nesterov & Vial, 2008; Keskar et al., 2017; Zhang et al., 2017). These correspond, respectively, to low approximation, optimization and estimation error as described by Bottou (2010).\nOur work focuses on the analysis of the estimation error. In particular, different approaches had been used to look at the question of why stochastic gradient descent results in solutions that generalize well. For example, Bottou & LeCun (2005); Bottou & Bousquet (2008); Duchi et al. (2011); Nesterov & Vial (2008); Hardt et al. (2016); Bottou et al. (2016) rely on the concept of stochastic approximation. Another conjecture that was recently (Keskar et al., 2017) explored, but that could be traced back to Hochreiter & Schmidhuber (1997), relies on the geometry of the loss function around a given solution. It argues that flat minima, for some definition of flatness, lead to better generalization. Our work focuses on this particular conjecture, arguing that there are critical issues when applying the concept of flat minima to deep neural networks, which require rethinking what flatness actually means.\nWhile the concept of flat minima is not well defined, having slightly different meanings in different works, the intuition is relatively simple. If one imagines the error as a onedimensional curve, a minimum is flat if there is a wide region around it with roughly the same error, otherwise the minimum is sharp. When moving to higher dimensional spaces, defining flatness becomes more complicated. In Hochreiter & Schmidhuber (1997) it is defined as the size of the connected region around the minimum where the training loss is relatively similar. Chaudhari et al. (2017) relies, in contrast, on the curvature of the second order structure around the minimum, while Keskar et al. (2017) looks at the maximum loss in a bounded neighbourhood of the minimum. All these works rely on the fact that flatness results in robustness to low precision arithmetic or noise in the parameter space, which, using an minimum description length-based argument, suggests a low expected overfitting.\nHowever, several common architectures and parametrizations in deep learning are already at odds with this conjec-\nar X\niv :1\n70 3.\n04 93\n3v 1\n[ cs\n.L G\n] 1\n5 M\nar 2\n01 7\nture, requiring at least some degree of refinement in the statements made. In particular, we show how the geometry of the associated parameter space can alter the ranking between prediction functions when considering several measures of flatness/sharpness. We believe the reason for this contradiction stems from the Bayesian arguments about KLdivergence made to justify the generalization ability of flat minima (Hinton & Van Camp, 1993). Indeed, KullbackLiebler divergence is invariant to change of parameters whereas the notion of \"flatness\" is not. The demonstrations of Hochreiter & Schmidhuber (1997) are approximately based on a Gibbs formalism and rely on strong assumptions and approximations that can compromise the applicability of the argument, including the assumption of a discrete function space.\n2 Definitions of flatness/sharpness\nFor conciseness, we will restrict ourselves to supervised scalar output problems, but several conclusions in this paper can apply to other problems as well. We will consider a function f that takes as input an element x from an input space X and outputs a scalar y. We will denote by f\u03b8 the prediction function. This prediction function will be parametrized by a parameter vector \u03b8 in a parameter space \u0398. Often, this prediction function will be over-parametrized and two parameters (\u03b8, \u03b8\u2032) \u2208 \u03982 that yield the same prediction function everywhere, \u2200x \u2208 X , f\u03b8(x) = f\u03b8\u2032(x), are called observationally equivalent. The model is trained to minimize a loss function L which takes as argument the prediction function f\u03b8. We will often think of the loss L as a function of \u03b8 and adopt the notation L(\u03b8).\nThe notion of flatness/sharpness of a minimum is relative, therefore we will discuss metrics that can be used to compare the relative flatness between two minima. In this section we will formalize three used definitions of flatness in the literature.\nHochreiter & Schmidhuber (1997) defines a flat minimum as \"a large connected region in weight space where the\nerror remains approximately constant\". We interpret this formulation as follows: Definition 1. Given > 0, a minimum \u03b8, and a loss L, we define C(L, \u03b8, ) as the largest (using inclusion as the partial order over the subsets of \u0398) connected set containing \u03b8 such that \u2200\u03b8\u2032 \u2208 C(L, \u03b8, ), L(\u03b8\u2032) < L(\u03b8) + . The - flatness will be defined as the volume of C(L, \u03b8, ). We will call this measure the volume -flatness.\nIn Figure 1, C(L, \u03b8, ) will be the purple line at the top of the red area if the height is and its volume will simply be the length of the purple line.\nFlatness can also be defined using the local curvature of the loss function around the minimum if it is a critical point 1. Chaudhari et al. (2017); Keskar et al. (2017) suggest that this information is encoded in the eigenvalues of the Hessian. However, in order to compare how flat one minimum versus another, the eigenvalues need to be reduced to a single number. Here we consider the spectral norm and trace of the Hessian, two typical measurements of the eigenvalues of a matrix.\nAdditionally Keskar et al. (2017) defines the notion of - sharpness. In order to make proofs more readable, we will slightly modify their definition. However, because of norm equivalence in finite dimensional space, our results will transfer to the original definition in full space as well. Our modified definition is the following: Definition 2. Let B2( , \u03b8) be an Euclidean ball centered on a minimum \u03b8 with radius . Then, for a non-negative valued loss function L, the -sharpness will be defined as proportional to\nmax\u03b8\u2032\u2208B2( ,\u03b8) ( L(\u03b8\u2032)\u2212 L(\u03b8) ) 1 + L(\u03b8) .\nIn Figure 1, if the width of the red area is 2 then the height of the red area is max\u03b8\u2032\u2208B2( ,\u03b8) ( L(\u03b8\u2032)\u2212 L(\u03b8) ) .\n-sharpness can be related to the spectral norm of the Hessian. Indeed, a second-order Taylor expansion of L around a critical point minimum is written\nL(\u03b8\u2032) = L(\u03b8) + 1\n2 (\u03b8\u2032 \u2212 \u03b8) (\u22072L)(\u03b8)(\u03b8\u2032 \u2212 \u03b8)T\n+ o(\u2016\u03b8\u2032 \u2212 \u03b8\u201622).\nIn this second order approximation, the -sharpness at \u03b8 would be \u2223\u2223\u2223\u2223\u2223\u2223(\u22072L)(\u03b8)\u2223\u2223\u2223\u2223\u2223\u2223\n2 2 2 ( 1 + L(\u03b8) ) . 1In this paper, we will often assume that is the case when dealing with Hessian-based measures in order to have them welldefined."}, {"heading": "3 Properties of Deep Rectified Networks", "text": "Before moving forward to our results, in this section we first introduce the notation used in the rest of paper. Most of our results, for clarity, will be on the deep rectified feedforward networks with a linear output layer that we describe below, though they can easily be extended to other architectures (e.g. convolutional, etc.). Definition 3. GivenK weight matrices (\u03b8k)k\u2264K with nk = dim ( vec(\u03b8k) ) and n = \u2211K k=1 nk, the output y of a deep rectified feedforward networks with a linear output layer is:\ny = \u03c6rect ( \u03c6rect ( \u00b7 \u00b7 \u00b7\u03c6rect(x \u00b7 \u03b81) \u00b7 \u00b7 \u00b7 ) \u00b7 \u03b8K\u22121 ) \u00b7 \u03b8K ,\nwhere\n\u2022 x is the input to the model, a high-dimensional vector\n\u2022 \u03c6rect is the rectified elementwise activation function (Jarrett et al., 2009; Nair & Hinton, 2010; Glorot et al., 2011), which is the positive part (zi)i 7\u2192 (max(zi, 0))i.\n\u2022 vec reshapes a matrix into a vector.\nNote that in our definition we excluded the bias terms, usually found in any neural architecture. This is done mainly for convenience, to simplify the rendition of our arguments. However, the arguments can be extended to the case that includes biases. Another choice is that of the linear output layer. Having an output activation function does not affect our argument either: since the loss is a function of the output activation, it can be rephrased as a function of linear pre-activation.\nDeep rectifier models have certain properties that allows us in section 4 to arbitrary manipulate the flatness of a minimum.\nAn important topic for optimization of neural networks is understanding the non-Euclidean geometry of the parameter space as imposed by the neural architecture (see, for example Amari, 1998). In principle, when we take a step in parameter space what we expect to control is the change in the behavior of the model (i.e. the mapping of the input x to the output y). In principle we are not interested in the parameters per se, but rather only in the mapping they represent.\nIf one defines a measure for the change in the behavior of the model, which can be done under some assumptions, then, it can be used to define, at any point in the parameter space, a metric that says what is the equivalent change in the parameters for a unit of change in the behavior of the model. As it turns out, for neural networks, this metric is\nnot constant over \u0398. Intuitively, the metric is related to the curvature, and since neural networks can be highly nonlinear, the curvature will not be constant. See Amari (1998); Pascanu & Bengio (2014) for more details. Coming back to the concept of flatness or sharpness of a minimum, this metric should define the flatness.\nHowever, the geometry of the parameter space is more complicated. Regardless of the measure chosen to compare two instantiations of a neural network, because of the structure of the model, it also exhibits a large number of symmetric configurations that result in exactly the same behavior. Because the rectifier activation has the non-negative homogeneity property, as we will see shortly, one can construct a continuum of points that lead to the same behavior, hence the metric is singular. Which means that one can exploit these directions in which the model stays unchanged to shape the neighbourhood around a minimum in such a way that, by most definitions of flatness, this property can be controlled. See Figure 2 for a visual depiction, where the flatness (given here as the distance between the different level curves) can be changed by moving along the curve.\nLet us redefine, for convenience, the non-negative homogeneity property (Neyshabur et al., 2015; Lafond et al., 2016) below. Note that beside this property, the reason for studying the rectified linear activation is for its widespread adoption (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016).\nDefinition 4. A given a function \u03b8 is non-negative homoge-\nneous if\n\u2200(z, \u03b1) \u2208 R\u00d7 R+, \u03c6(\u03b1z) = \u03b1\u03c6(z)\n.\nTheorem 1. The rectified function \u03c6rect(x) = max(x, 0) is non-negative homogeneous.\nProof. Follows trivially from the constraint that \u03b1 > 0, given that x > 0\u21d2 \u03b1x > 0, iff \u03b1 > 0.\nFor a deep rectified neural network it means that: \u03c6rect ( x \u00b7 (\u03b1\u03b81) ) \u00b7 \u03b82 = \u03c6rect(x \u00b7 \u03b81) \u00b7 (\u03b1\u03b82),\nmeaning that for this one (hidden) layer neural network, the parameters (\u03b1\u03b81, \u03b82) is observationally equivalent to (\u03b81, \u03b1\u03b82).\nGiven this non-negative homogeneity, if (\u03b81, \u03b82) 6= (0, 0) then { (\u03b1\u03b81, \u03b1 \u22121\u03b82), \u03b1 > 0 }\nis an infinite set of observationally equivalent parameters, inducing a strong nonidentifiability in this learning scenario. Other models like leaky recitifers (He et al., 2015) or maxout networks (Goodfellow et al., 2013) also have this non-negative homogeneity property.\nIn what follows we will rely on such transformations, in particular we will rely on the following definition:\nDefinition 5. For a single hidden layer rectifier feedforward network we define the family of transformations\nT\u03b1 : (\u03b81, \u03b82) 7\u2192 (\u03b1\u03b81, \u03b1\u22121\u03b82)\nwhich we refer to as a \u03b1-scale transformation.\nNote that a \u03b1-scale transformation will not affect the generalization, as the behavior of the function is identical. Also while the transformation is only defined for a single layer rectified feedforward network, it can trivially be extended to any architecture having a single rectified network as a submodule, e.g. a deep rectified feedforward network. For simplicity and readability we will rely on this definition."}, {"heading": "4 Deep Rectified networks and flat minima", "text": "In this section we exploit the resulting strong nonidentifiability to showcase a few shortcomings of some definitions of flatness. Although \u03b1-scale transformation does not affect the function represented, it allows us to significantly decrease several measures of flatness. For another definition of flatness, \u03b1-scale transformation show that all minima are equally flat."}, {"heading": "4.1 Volume -flatness", "text": "Theorem 2. For a one-hidden layer rectified neural network of the form\ny = \u03c6rect(x \u00b7 \u03b81) \u00b7 \u03b82,\nand a minimum \u03b8 = (\u03b81, \u03b82), such that \u03b81 6= 0 and \u03b82 6= 0, \u2200 > 0 C(L, \u03b8, ) has an infinite volume.\nWe will not consider the solution \u03b8 where any of the weight matrices \u03b81, \u03b82 is zero, \u03b81 = 0 or \u03b82 = 0, as it results in a constant function which we will assume to give poor training performance. For \u03b1 > 0, the \u03b1-scale transformation T\u03b1 : (\u03b81, \u03b82) 7\u2192 (\u03b1\u03b81, \u03b1\u22121\u03b82) has Jacobian determinant \u03b1n1\u2212n2 , where once again n1 = dim ( vec(\u03b81) ) and\nn2 = dim ( vec(\u03b82) ) . Note that the Jacobian determinant represents the change in the volume induced by T\u03b1. We show below that there is a connected region containing \u03b8 with infinite volume and where the error remains approximately constant.\nProof. We will first introduce a small region with approximately constant error around \u03b8 with non-zero volume. Let\u2019s consider without loss of generality that n1 \u2265 n2 (otherwise we can replace \u03b1 by 1/\u03b1). Given > 0 and if we consider the loss function continuous with respect to the parameter, C(L, \u03b8, ) is an open set containing \u03b8. Let B\u221e(r, \u03b8) \u2286 C(L, \u03b8, ) be a L\u221e ball of radius r centered on \u03b8. 0 < \u2016\u03b81\u2016\u221e \u2264 \u2016\u03b8\u2016\u221e, so if we define r\u2032 = min(r, 12\u2016\u03b81\u2016\u221e) then \u2200(\u03b8 \u2032 1, \u03b8 \u2032 2) \u2208 B\u221e(r\u2032, \u03b8), \u03b8\u20321 6= 0 and B\u221e(r\u2032, \u03b8) \u2286 C(L, \u03b8, ). This ball of volume v = (2r\u2032)n1+n1 > 0 will be the subset that will seed the desired infinitely large region around \u03b8.\nLet C \u2032 = { T\u03b1\u2032(\u03b8 \u2032) = (\u03b1\u2032\u03b8\u20321, \u03b1 \u2032\u22121\u03b8\u20322), \u03b1 \u2032 > 0, (\u03b8\u20321, \u03b8 \u2032 2) \u2208 B\u221e(r \u2032, \u03b8) } . C \u2032 is a connected region where the error remains approximately constant, i.e. with an -interval. For \u03b1 > 1, T\u03b1 ( B\u221e(r \u2032, \u03b8) )\nwill have a volume higher than or equal to v because \u03b1n1\u2212n2 > 1 but we will show how to pick \u03b1 such that T\u03b1 ( B\u221e(r \u2032, \u03b8) )\nis disjoint of B\u221e(r\u2032, \u03b8), allowing us to sum their volume to lower bound the volume of C \u2032 (see Figure 3).\nLet \u03b1 = 2 \u2016\u03b81\u2016\u221e+r \u2032\n\u2016\u03b81\u2016\u221e\u2212r\u2032 . Because\nB\u221e(r \u2032, \u03b8) = B\u221e(r \u2032, \u03b81)\u00d7B\u221e(r\u2032, \u03b82),\nwhere \u00d7 is the Cartesian product, we have\nT\u03b1 ( B\u221e(r \u2032, \u03b8) ) = B\u221e(\u03b1r \u2032, \u03b1\u03b81)\u00d7B\u221e(\u03b1\u22121r\u2032, \u03b1\u22121\u03b82).\nTherefore, T\u03b1 ( B\u221e(r \u2032, \u03b8) ) \u2229B\u221e(r\u2032, \u03b8) = \u2205.\nWe can repeat this reasoning to show that B\u221e(r \u2032, \u03b8), T\u03b1 ( B\u221e(r \u2032, \u03b8) ) , T 2\u03b1 ( B\u221e(r \u2032, \u03b8) ) , . . . are disjoint and have volume greater than or equal to v. Since\nT k\u03b1\n(\nB\u221e(r \u2032, \u03b8)\n)\n= T\u03b1k\n(\nB\u221e(r \u2032, \u03b8)\n)\n, they are also subsets of C \u2032. The volume of C \u2032 will then be lower bounded by 0 < v + v + v + \u00b7 \u00b7 \u00b7 meaning that it has infinite volume.\nTherefore C(L, \u03b8, ) has infinite volume too, making the volume -flatness of \u03b8 infinite.\nThis theorem can generalize to rectified neural networks in general with a similar proof. Given that every minimum has an infinitely large region (volume-wise) in which the error remains approximately constant, that means that every minimum would be infinitely flat according to the volume -flatness. Since all minima are equally flat, it is not possible to use volume -flatness to gauge the generalization property of a minimum."}, {"heading": "4.2 Hessian-based measures", "text": "The non-Euclidean geometry of the parameter space, coupled with the manifolds of observationally equal behavior of the model, allows one to move from one region of the parameter space to another, changing the curvature of the model without actually changing the function. This approach has been used with success to improve optimization, by moving from a region of high curvature to a region of well behaved curvature (e.g. Desjardins et al., 2015; Salimans & Kingma, 2016). In this section we look at two widely used measures of the Hessian, the spectral radius and trace, showing that either of these values can be manipulated without actually changing the behavior of the function. If the flatness of a minimum is defined by any of these quantities, then it could also be easily manipulated.\nTheorem 3. The gradient and Hessian of the loss L with respect to \u03b8 can be modified by T\u03b1.\nProof.\nL(\u03b81, \u03b82) = L(\u03b1\u03b81, \u03b1 \u22121\u03b82),\nwe have then by differentiation (\u2207L)(\u03b81, \u03b82) = (\u2207L)(\u03b1\u03b81, \u03b1\u22121\u03b82) [ \u03b1In1 0\n0 \u03b1\u22121In2 ] \u21d4 (\u2207L)(\u03b1\u03b81, \u03b1\u22121\u03b82) = (\u2207L)(\u03b81, \u03b82) [ \u03b1\u22121In1 0\n0 \u03b1In2 ] and\n(\u22072L)(\u03b1\u03b81, \u03b1\u22121\u03b82)\n=\n[ \u03b1\u22121In1 0\n0 \u03b1In2\n] (\u22072L)(\u03b81, \u03b82) [ \u03b1\u22121In1 0\n0 \u03b1In2\n] .\nSharpest direction Through these transformations we can easily find, for any critical point which is a minimum with non-zero Hessian, an observationally equivalent parameter whose Hessian has an arbitrarily large spectral norm.\nTheorem 4. For a one-hidden layer rectified neural network of the form\ny = \u03c6rect(x \u00b7 \u03b81) \u00b7 \u03b82,\nand critical point \u03b8 = (\u03b81, \u03b82) being a minimum for L, such that (\u22072L)(\u03b8) 6= 0, \u2200M > 0,\u2203\u03b1 > 0, \u2223\u2223\u2223\u2223\u2223\u2223(\u22072L)(T\u03b1(\u03b8))\u2223\u2223\u2223\u2223\u2223\u22232 \u2265 M where \u2223\u2223\u2223\u2223\u2223\u2223(\u22072L)(T\u03b1(\u03b8))\u2223\u2223\u2223\u2223\u2223\u22232 is\nthe spectral norm of (\u22072L) ( T\u03b1(\u03b8) ) .\nProof. The trace of a symmetric matrix is the sum of its eigenvalues and a real symmetric matrix can be diagonalized in R, therefore if the Hessian is non-zero, there is one nonzero positive diagonal element. Without loss of generality, we will assume that this non-zero element of value d > 0 corresponds to an element in \u03b81. Therefore the Frobenius norm\n\u2223\u2223\u2223\u2223\u2223\u2223(\u22072L)(T\u03b1(\u03b8))\u2223\u2223\u2223\u2223\u2223\u2223F of (\u22072L)(\u03b1\u03b81, \u03b1\u22121\u03b82)\n=\n[ \u03b1\u22121In1 0\n0 \u03b1In2\n] (\u22072L)(\u03b81, \u03b82) [ \u03b1\u22121In1 0\n0 \u03b1In2\n] .\nis lower bounded by \u03b1\u22122d.\nSince all norms are equivalent in finite dimension, there exists a constant r > 0 such that r|||A|||F \u2264 |||A|||2 for all symmetric matrices A. So by picking \u03b1 < \u221a rd M , we are\nguaranteed that \u2223\u2223\u2223\u2223\u2223\u2223(\u22072L)(T\u03b1(\u03b8))\u2223\u2223\u2223\u2223\u2223\u22232 \u2265M .\nAny minimum with non-zero Hessian will be observationally equivalent to a minimum whose Hessian has an arbitrarily large spectral norm. Therefore for any minimum in the loss function, if there exists another minimum that generalizes better then there exists another minimum that generalizes better and is also sharper according the spectral norm of the Hessian. The spectral norm of critical points\u2019 Hessian becomes as a result less relevant as a measure of potential generalization error. Moreover, since the spectral norm lower bounds the trace for a positive semi-definite symmetric matrix, the same conclusion can be drawn for the trace.\nMany directions However, some notion of sharpness might take into account the entire eigenspectrum of the Hessian as opposed to its largest eigenvalue, for instance, Chaudhari et al. (2017) describe the notion of wide valleys, allowing the presence of very few large eigenvalues. We can generalize the transformations between observationally equivalent parameters to deeper neural networks with K\u22121 hidden layers: for \u03b1k > 0, T\u03b1 : (\u03b8k)k\u2264K 7\u2192 (\u03b1k\u03b8k)k\u2208K with \u220fK k=1 \u03b1k = 1. If we define\nD\u03b1 =  \u03b1\u221211 In1 0 \u00b7 \u00b7 \u00b7 0 0 \u03b1\u221212 In2 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 \u03b1\u22121K InK  then the first and second derivatives at T\u03b1(\u03b8) will be\n(\u2207L) ( T\u03b1(\u03b8) ) =(\u2207L)(\u03b8)D\u03b1\n(\u22072L) ( T\u03b1(\u03b8) ) =D\u03b1(\u22072L)(\u03b8)D\u03b1.\nWe will show to which extent you can increase several eigenvalues of (\u22072L) ( T\u03b1(\u03b8) ) by varying \u03b1.\nDefinition 6. For each n\u00d7n matrixA, we define the vector \u03bb(A) of sorted singular values of A with their multiplicity \u03bb1(A) \u2265 \u03bb2(A) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbn(A).\nIf A is symmetric positive semi-definite, \u03bb(A) is also the vector of its sorted eigenvalues.\nTheorem 5. For a (K \u2212 1)-hidden layer rectified neural network of the form\ny = \u03c6rect(\u03c6rect(\u00b7 \u00b7 \u00b7\u03c6rect(x \u00b7 \u03b81) \u00b7 \u00b7 \u00b7 ) \u00b7 \u03b8K\u22121) \u00b7 \u03b8K ,\nand critical point \u03b8 = (\u03b8k)k\u2264K being a minimum for L, such that (\u22072L)(\u03b8) has rank r = rank ( (\u22072L)(\u03b8) ) , \u2200M >\n0,\u2203\u03b1 > 0 such that ( r \u2212 mink\u2264K(nk) ) eigenvalues are\ngreater than M .\nProof. By matrix equivalence, we know that the eigenvalue spectrum of D\u03b1(\u22072L)(\u03b8)D\u03b1 is the same as that\nof (\u22072L)(\u03b8)D2\u03b1. Without loss of generality, we consider mink\u2264K(nk) = nK and choose \u2200k < K,\u03b1k = \u03b2\u22121 and \u03b1K = \u03b2K\u22121. Since D2\u03b1 and (\u22072L)(\u03b8) are positive semi-definite symmetric matrices, we can apply the multiplicative Horn inequalities (Klyachko, 2000) on the product (\u22072L)(\u03b8)D2\u03b1:\n\u2200i \u2264 n,j \u2264 (n\u2212 nK), \u03bbi+j\u2212n ( (\u22072L)(\u03b8)D2\u03b1 ) \u2265 \u03bbi ( (\u22072L)(\u03b8) ) \u03b22.\nBy choosing \u03b2 > \u221a\nM\n\u03bbr\n( (\u22072L)(\u03b8) ) , since we have \u2200i \u2264 r, \u03bbi ( (\u22072L)(\u03b8) ) \u2265 \u03bbr ( (\u22072L)(\u03b8) ) > 0 we can conclude that\n\u2200i \u2264 (r \u2212 nK), \u03bbi ( (\u22072L)(\u03b8)D2\u03b1 ) \u2265 \u03bbi+nk ( (\u22072L)(\u03b8) ) \u03b22\n\u2265 \u03bbr ( (\u22072L)(\u03b8) ) \u03b22 > M.\nIt means that there exists an observationally equivalent parameter with at least ( r \u2212mink\u2264K(nk) ) arbitrarily large\neigenvalues. Since Sagun et al. (2016) seems to suggests that rank deficiency in the Hessian is due to over-parametrization of the model, one could conjecture that ( r\u2212mink\u2264K(nk) ) can be high for thin and deep neural networks, resulting in a majority of large eigenvalues. Therefore, it would still be possible to obtain an equivalent parameter with large Hessian eigenvalues, i.e. sharp in multiple directions.\n4.3 -sharpness\nWe have redefined for > 0 the -sharpness of Keskar et al. (2017) as follow\nmax\u03b8\u2032\u2208B2( ,\u03b8) ( L(\u03b8\u2032)\u2212 L(\u03b8) ) 1 + L(\u03b8)\nwhere B2( , \u03b8) is the Euclidean ball of radius centered on \u03b8. This modification will demonstrate more clearly the\nissues of that metric as a measure of probable generalization. If we use K = 2 and (\u03b81, \u03b82) corresponding to a non-constant function, i.e. \u03b81 6= 0 and \u03b82 6= 0, then we can define \u03b1 = \u2016\u03b81\u20162 . We will now consider the observationally equivalent parameter T\u03b1(\u03b81, \u03b82) = ( \u03b81\u2016\u03b81\u20162 , \u03b1 \u22121\u03b82).\nGiven that \u2016\u03b81\u20162 \u2264 \u2016\u03b8\u20162, we have that (0, \u03b1\u22121\u03b82) \u2208 B2 ( , T\u03b1(\u03b8) ) , making the maximum loss in this neighborhood at least as high as the best constant-valued function, incurring relatively high sharpness. Figure 4 provides a visualization of the proof.\nFor rectified neural network every minimum is observationally equivalent to a minimum that generalizes as well but with high -sharpness. This also applies when using the full-space -sharpness used by Keskar et al. (2017). We can prove this similarly using the equivalence of norms in finite dimensional vector spaces and the fact that for c > 0, > 0, \u2264 (c + 1) (see Keskar et al. (2017)). We have not been able to show a similar problem with random subspace -sharpness used by Keskar et al. (2017), i.e. a restriction of the maximization to a random subspace, which could relate to the notion of wide valleys described by Chaudhari et al. (2017).\nBy exploiting the non-Euclidean geometry and nonidentifiability of rectified neural networks, we were able to demonstrate some of the limits of using typical definitions of minimum\u2019s flatness as core explanation for generalization."}, {"heading": "5 Allowing reparametrizations", "text": "In the previous section 4 we explored the case of a fixed parametrization, that of deep rectifier models. In this section we demonstrate a simple observation. If we are allowed to change the parametrization of some function f , we can obtain arbitrarily different geometries without affecting how the function evaluates on unseen data. The same holds for reparametrization of the input space. The implication is that the correlation between the geometry of the parameter space (and hence the error surface) and the behavior of a given function is meaningless if not preconditioned on the specific parametrization of the model."}, {"heading": "5.1 Model reparametrization", "text": "One thing that needs to be considered when relating flatness of minima to their probable generalization is that the choice of parametrization and its associated geometry are arbitrary. Since we are interested in finding a prediction function in a given family of functions, no reparametrization of this family should influence generalization of any of these functions. Given a bijection g onto \u03b8, we can define new transformed parameter \u03b7 = g\u22121(\u03b8). Since \u03b8 and \u03b7 represent in different\nspace the same prediction function, they should generalize as well.\nLet\u2019s call L\u03b7 = L \u25e6 g the loss function with respect to the new parameter \u03b7. We generalize the derivation of Subsection 4.2:\nL\u03b7(\u03b7) = L ( g(\u03b7) ) \u21d2 (\u2207L\u03b7)(\u03b7) = (\u2207L) ( g(\u03b7) ) (\u2207g)(\u03b7)\n\u21d2(\u22072L\u03b7)(\u03b7) = (\u2207g)(\u03b7)T (\u22072L) ( g(\u03b7) ) (\u2207g)(\u03b7) + (\u2207L) ( g(\u03b7) ) (\u22072g)(\u03b7).\nAt a differentiable critical point, we have by definition\n(\u2207L) ( g(\u03b7) ) = 0, therefore the transformed Hessian at a critical point becomes\n(\u22072L\u03b7)(\u03b7) = (\u2207g)(\u03b7)T (\u22072L) ( g(\u03b7) ) (\u2207g)(\u03b7).\nThis means that by reparametrizing the problem we can modify to a large extent the geometry of the loss function so as to have sharp minima of L in \u03b8 correspond to flat minima ofL\u03b7 in \u03b7 = g\u22121(\u03b8) and conversely. Figure 5 illustrates that point in one dimension. Several practical (Dinh et al., 2014; Rezende & Mohamed, 2015; Kingma et al., 2016; Dinh et al., 2016) and theoretical works (Hyv\u00e4rinen & Pajunen, 1999) show how powerful bijections can be. We can also note that the formula for the transformed Hessian at a critical point also applies if g is not invertible, g would just need to be surjective over \u0398 in order to cover exactly the same family of prediction functions\n{f\u03b8, \u03b8 \u2208 \u0398} = {fg(\u03b7), \u03b7 \u2208 H}.\nWe show in Appendix A, bijections that allow us to perturb the relative flatness between a finite number of minima.\nInstances of commonly used reparametrization are batch normalization (Ioffe & Szegedy, 2015), or the virtual batch normalization variant (Salimans et al., 2016), and weight normalization (Badrinarayanan et al., 2015; Salimans & Kingma, 2016; Arpit et al., 2016). Im et al. (2016) have plotted how the loss function landscape was affected by batch normalization. However, we will focus on weight normalization reparametrization as the analysis will be simpler, but the intuition with batch normalization will be similar. Weight normalization reparametrizes a nonzero weight w as w = s v\u2016v\u20162 with the new parameter being the scale s and the unnormalized weight v 6= 0.\nSince we can observe that w is invariant to scaling of v, reasoning similar to Section 3 can be applied with the simpler transformations T \u2032\u03b1 : v 7\u2192 \u03b1v for \u03b1 6= 0. Moreover, since this transformation is a simpler isotropic scaling, the conclusion that we can draw can be actually more powerful with respect to v:\n\u2022 every minimum has infinite volume -sharpness;\n\u2022 every minimum is observationally equivalent to an infinitely sharp minimum and to an infinitely flat minimum when considering nonzero eigenvalues of the Hessian;\n\u2022 every minimum is observationally equivalent to a minimum with arbitrarily low full-space and random subspace -sharpness and a minimum with high full-space -sharpness.\nThis further weakens the link between the flatness of a minimum and the generalization property of the associated\nprediction function when a specific parameter space has not been specified and explained beforehand."}, {"heading": "5.2 Input representation", "text": "As we conclude that the notion of flatness for a minimum in the loss function by itself is not sufficient to determine its generalization ability in the general case, we can choose to focus instead on properties of the prediction function instead. Motivated by some work in adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015) for deep neural networks, one could decide on its generalization property by analyzing the gradient of the prediction function on examples. Intuitively, if the gradient is small on typical points from the distribution or has a small Lipschitz constant, then a small change in the input should not incur a large change in the prediction.\nBut this infinitesimal reasoning is once again very dependent of the local geometry of the input space. For an invertible preprocessing \u03be\u22121, e.g. feature standardization, whitening or gaussianization (Chen & Gopinath, 2001), we will call f\u03be = f \u25e6 \u03be the prediction function on the preprocessed input u = \u03be\u22121(x). We can reproduce the derivation in Section 5 to obtain\n\u2202f\u03be \u2202uT\n( \u03be(u) ) = \u2202f \u2202xT ( \u03be(u) ) \u2202\u03be \u2202uT (u).\nAs we can alter significantly the relative magnitude of the gradient at each point, analyzing the amplitude of the gradient of the prediction function might prove problematic if the choice of the input space have not been explained beforehand. This remark applies in applications involving images, sound or other signals with invariances (Larsen et al., 2015). For example, Theis et al. (2016) show for images how a small drift of one to four pixels can incur a large difference in terms of L2 norm."}, {"heading": "6 Discussion", "text": "It has been observed empirically that minima found by standard deep learning algorithms that generalize well tend to be flatter than found minima that did not generalize well (Chaudhari et al., 2017; Keskar et al., 2017). However, when following several definitions of flatness, we have shown that the conclusion that flat minima should generalize better than sharp ones cannot be applied as is without further context. Previously used definitions fail to account for the complex geometry of some commonly used deep architectures. In particular, the non-identifiability of the model induced by symmetries, allows one to alter the flatness of a minimum without affecting the function it represents. Additionally the whole geometry of the error surface with respect to the parameters can be changed arbitrarily under different parametrizations. In the spirit of (Swirszcz et al., 2016), our\nwork indicates that more care is needed to define flatness to avoid degeneracies of the geometry of the model under study. Also such a concept can not be divorced from the particular parametrization of the model or input space."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Grzegorz S\u0301wirszcz for a insightful discussion of the paper, Harm De Vries, Yann Dauphin, Jascha Sohl-Dickstein and C\u00e9sar Laurent for useful discussions about optimization, Danilo Rezende for explaining universal approximation using normalizing flows and Kyle Kastner, Adriana Romero, Junyoung Chung, Nicolas Ballas, Aaron Courville, George Dahl, Yaroslav Ganin, Prajit Ramachandran, \u00c7ag\u0306lar G\u00fcl\u00e7ehre and Ahmed Touati for useful feedback."}, {"heading": "A Radial transformations", "text": "We show an elementary transformation to locally perturb the geometry of a finite-dimensional vector space and therefore affect the relative flatness between a finite number minima, at least in terms of spectral norm of the Hessian. We define the function:\n\u2200\u03b4 > 0,\u2200\u03c1 \u2208]0, \u03b4[,\u2200(r, r\u0302) \u2208 R+\u00d7]0, \u03b4[, \u03c8(r, r\u0302, \u03b4, \u03c1) = 1 ( r /\u2208 [0, \u03b4] ) r + 1 ( r \u2208 [0, r\u0302] ) \u03c1 r\nr\u0302 + 1 ( r \u2208]r\u0302, \u03b4] ) ( (\u03c1\u2212 \u03b4) r \u2212 \u03b4 r\u0302 \u2212 \u03b4 + \u03b4 )\n\u03c8\u2032(r, r\u0302, \u03b4, \u03c1) = 1 ( r /\u2208 [0, \u03b4] ) + 1 ( r \u2208 [0, r\u0302] ) \u03c1 r\u0302\n+ 1 ( r \u2208]r\u0302, \u03b4] ) \u03c1\u2212 \u03b4 r\u0302 \u2212 \u03b4\nFor a parameter \u03b8\u0302 \u2208 \u0398 and \u03b4 > 0, \u03c1 \u2208]0, \u03b4[, r\u0302 \u2208]0, \u03b4[, inspired by the radial flows (Rezende & Mohamed, 2015) in we can define the radial transformations\n\u2200\u03b8 \u2208 \u0398, g\u22121(\u03b8) = \u03c8 ( \u2016\u03b8 \u2212 \u03b8\u0302\u20162, r\u0302, \u03b4, \u03c1 ) \u2016\u03b8 \u2212 \u03b8\u0302\u20162 ( \u03b8 \u2212 \u03b8\u0302 ) + \u03b8\u0302\nwith Jacobian\n\u2200\u03b8 \u2208 \u0398, (\u2207g\u22121)(\u03b8) = \u03c8\u2032(r, r\u0302, \u03b4, \u03c1) In\n\u2212 1 ( r \u2208]r\u0302, \u03b4] ) \u03b4(r\u0302 \u2212 \u03c1) r3(r\u0302 \u2212 \u03b4) (\u03b8 \u2212 \u03b8\u0302)T (\u03b8 \u2212 \u03b8\u0302)\n+ 1 ( r \u2208]r\u0302, \u03b4] )\u03b4(r\u0302 \u2212 \u03c1) r(r\u0302 \u2212 \u03b4) In,\nwith r = \u2016\u03b8 \u2212 \u03b8\u0302\u20162.\nFirst, we can observe in Figure 6 that these transformations are purely local: they only have an effect inside the ball\nB2(\u03b8\u0302, \u03b4). Through these transformations, you can arbitrarily perturb the ranking between several minima in terms of flatness as described in Subsection 5.1."}, {"heading": "B Considering the bias parameter", "text": "When we consider the bias parameter for a one (hidden) layer neural network, the non-negative homogeneity property translates into\ny = \u03c6rect(x \u00b7 \u03b81 + b1) \u00b7 \u03b82 + b2 = \u03c6rect(x \u00b7 \u03b1\u03b81 + \u03b1b1) \u00b7 \u03b1\u22121\u03b82 + b2,\nwhich results in conclusions similar to section 4.\nFor a deeper rectified neural network, this property results in\ny = \u03c6rect ( \u03c6rect ( \u00b7 \u00b7 \u00b7\u03c6rect(x \u00b7 \u03b81 + b1) \u00b7 \u00b7 \u00b7 ) \u00b7 \u03b8K\u22121 + bK\u22121 ) \u00b7 \u03b8K + bK\n= \u03c6rect ( \u03c6rect ( \u00b7 \u00b7 \u00b7\u03c6rect(x \u00b7 \u03b11\u03b81 + \u03b11b1) \u00b7 \u00b7 \u00b7 ) \u00b7 \u03b1K\u22121\u03b8K\u22121 +\nK\u22121\u220f k=1 \u03b1kbK\u22121 ) \u00b7 \u03b1K\u03b8K + bK\nfor \u220fK k=1 \u03b1k = 1. This can decrease the amount of eigenvalues of the Hessian that can be arbitrarily influenced."}, {"heading": "C Euclidean distance and input representation", "text": "A natural consequence of Subsection 5.2 is that metrics relying on Euclidean metric like mean square error or Earthmover distance will rank very differently models depending on the input representation chosen. Therefore, the choice of input representation is critical when ranking different models based on these metrics. Indeed, bijective transformations as simple as feature standardization or whitening can change the metric significantly.\nOn the contrary, ranking resulting from metrics like fdivergence and log-likelihood are not perturbed by bijective transformations because of the change of variables formula."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["Amari", "Shun-Ichi"], "venue": "Neural Comput.,", "citeRegEx": "Amari and Shun.Ichi.,? \\Q1998\\E", "shortCiteRegEx": "Amari and Shun.Ichi.", "year": 1998}, {"title": "Normalization propagation: A parametric technique for removing internal covariate shift in deep networks", "author": ["Arpit", "Devansh", "Zhou", "Yingbo", "Kota", "Bhargava U", "Govindaraju", "Venu"], "venue": "arXiv preprint arXiv:1603.01431,", "citeRegEx": "Arpit et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arpit et al\\.", "year": 2016}, {"title": "Understanding symmetries in deep networks", "author": ["Badrinarayanan", "Vijay", "Mishra", "Bamdev", "Cipolla", "Roberto"], "venue": "arXiv preprint arXiv:1511.01029,", "citeRegEx": "Badrinarayanan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Badrinarayanan et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR\u20192015,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["Bottou", "L\u00e9on"], "venue": "In Proceedings of COMPSTAT\u20192010,", "citeRegEx": "Bottou and L\u00e9on.,? \\Q2010\\E", "shortCiteRegEx": "Bottou and L\u00e9on.", "year": 2010}, {"title": "The tradeoffs of large scale learning", "author": ["Bottou", "L\u00e9on", "Bousquet", "Olivier"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Bottou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2008}, {"title": "On-line learning for very large datasets", "author": ["Bottou", "L\u00e9on", "LeCun", "Yann"], "venue": "Applied Stochastic Models in Business and Industry,", "citeRegEx": "Bottou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2005}, {"title": "Optimization methods for large-scale machine learning", "author": ["Bottou", "L\u00e9on", "Curtis", "Frank E", "Nocedal", "Jorge"], "venue": "arXiv preprint arXiv:1606.04838,", "citeRegEx": "Bottou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2016}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Chan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "Entropy-sgd: Biasing gradient descent into wide valleys", "author": ["Chaudhari", "Pratik", "Choromanska", "Anna", "Soatto", "Stefano", "LeCun", "Yann"], "venue": "In ICLR\u20192017,", "citeRegEx": "Chaudhari et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chaudhari et al\\.", "year": 2017}, {"title": "The loss surfaces of multilayer networks", "author": ["Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Micha\u00ebl", "Arous", "G\u00e9rard Ben", "LeCun", "Yann"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan K", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Wav2letter: an end-to-end convnet-based speech recognition system", "author": ["Collobert", "Ronan", "Puhrsch", "Christian", "Synnaeve", "Gabriel"], "venue": "arXiv preprint arXiv:1609.03193,", "citeRegEx": "Collobert et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2016}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Dauphin", "Yann N", "Pascanu", "Razvan", "G\u00fcl\u00e7ehre", "\u00c7aglar", "Cho", "KyungHyun", "Ganguli", "Surya", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Nice: Nonlinear independent components estimation", "author": ["Dinh", "Laurent", "Krueger", "David", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1410.8516,", "citeRegEx": "Dinh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2014}, {"title": "Density estimation using real nvp", "author": ["Dinh", "Laurent", "Sohl-Dickstein", "Jascha", "Bengio", "Samy"], "venue": "In ICLR\u20192017,", "citeRegEx": "Dinh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A convolutional encoder model for neural machine translation", "author": ["Gehring", "Jonas", "Auli", "Michael", "Grangier", "David", "Dauphin", "Yann N"], "venue": "arXiv preprint arXiv:1611.02344,", "citeRegEx": "Gehring et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gehring et al\\.", "year": 2016}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In Aistats,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "In ICLR\u20192015", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "(icassp), 2013 ieee international conference on,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni Y", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam", "Ng", "Andrew Y"], "venue": "CoRR, abs/1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Hardt", "Moritz", "Recht", "Ben", "Singer", "Yoram"], "venue": "Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Hardt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE international conference on computer vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["Hinton", "Geoffrey E", "Van Camp", "Drew"], "venue": "In Proceedings of the sixth annual conference on Computational learning theory,", "citeRegEx": "Hinton et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1993}, {"title": "Nonlinear independent component analysis: Existence and uniqueness results", "author": ["Hyv\u00e4rinen", "Aapo", "Pajunen", "Petteri"], "venue": "Neural Networks,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 1999}, {"title": "An empirical analysis of deep network loss surfaces", "author": ["Im", "Daniel Jiwoong", "Tao", "Michael", "Branson", "Kristin"], "venue": "arXiv preprint arXiv:1612.04010,", "citeRegEx": "Im et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Im et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In Bach & Blei", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "LeCun", "Yann"], "venue": "In Computer Vision,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "On largebatch training for deep learning: Generalization gap and sharp minima", "author": ["Keskar", "Nitish Shirish", "Mudigere", "Dheevatsa", "Nocedal", "Jorge", "Smelyanskiy", "Mikhail", "Tang", "Ping Tak Peter"], "venue": "In ICLR\u20192017,", "citeRegEx": "Keskar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Keskar et al\\.", "year": 2017}, {"title": "Random walks on symmetric spaces and inequalities for matrix spectra", "author": ["Klyachko", "Alexander A"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Klyachko and A.,? \\Q2000\\E", "shortCiteRegEx": "Klyachko and A.", "year": 2000}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "About diagonal rescaling applied to neural nets", "author": ["Lafond", "Jean", "Vasilache", "Nicolas", "Bottou", "L\u00e9on"], "venue": "ICML Workshop on Optimization Methods for the Next Generation of Machine Learning,", "citeRegEx": "Lafond et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lafond et al\\.", "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity", "author": ["Larsen", "Anders Boesen Lindbo", "S\u00f8nderby", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "metric. CoRR,", "citeRegEx": "Larsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "On the number of linear regions of deep neural networks", "author": ["Montufar", "Guido F", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Confidence level solutions for stochastic programming", "author": ["Nesterov", "Yu", "Vial", "J-Ph"], "venue": null, "citeRegEx": "Nesterov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nesterov et al\\.", "year": 2008}, {"title": "Path-sgd: Path-normalized optimization in deep neural networks", "author": ["Neyshabur", "Behnam", "Salakhutdinov", "Ruslan R", "Srebro", "Nati"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "Revisiting natural gradient for deep networks", "author": ["Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "ICLR,", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "On the expressive power of deep neural networks", "author": ["Raghu", "Maithra", "Poole", "Ben", "Kleinberg", "Jon", "Ganguli", "Surya", "Sohl-Dickstein", "Jascha"], "venue": "arXiv preprint arXiv:1606.05336,", "citeRegEx": "Raghu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raghu et al\\.", "year": 2016}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "In Bach & Blei", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Singularity of the hessian in deep learning", "author": ["Sagun", "Levent", "Bottou", "L\u00e9on", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1611.07476,", "citeRegEx": "Sagun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2016}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Salimans", "Tim", "Kingma", "Diederik P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR\u20192015,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Local minima in training of deep", "author": ["Swirszcz", "Grzegorz", "Czarnecki", "Wojciech Marian", "Pascanu", "Razvan"], "venue": "networks. CoRR,", "citeRegEx": "Swirszcz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Swirszcz et al\\.", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "In ICLR\u20192014,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": "In ICLR\u20192016", "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Wu", "Yonghui", "Schuster", "Mike", "Chen", "Zhifeng", "Le", "Quoc V", "Norouzi", "Mohammad", "Macherey", "Wolfgang", "Krikun", "Maxim", "Cao", "Yuan", "Gao", "Qin", "Klaus"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"], "venue": "In ICLR\u20192017,", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 30, "context": "Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization.", "startOffset": 33, "endOffset": 54}, {"referenceID": 49, "context": "Deep learning techniques have been very successful in several domains, like object recognition in images (e.g Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016), machine translation (e.", "startOffset": 105, "endOffset": 201}, {"referenceID": 24, "context": "Deep learning techniques have been very successful in several domains, like object recognition in images (e.g Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016), machine translation (e.", "startOffset": 105, "endOffset": 201}, {"referenceID": 46, "context": ", 2016), machine translation (e.g. Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2016) and speech recognition (e.", "startOffset": 29, "endOffset": 138}, {"referenceID": 3, "context": ", 2016), machine translation (e.g. Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2016) and speech recognition (e.", "startOffset": 29, "endOffset": 138}, {"referenceID": 51, "context": ", 2016), machine translation (e.g. Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2016) and speech recognition (e.", "startOffset": 29, "endOffset": 138}, {"referenceID": 17, "context": ", 2016), machine translation (e.g. Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2016) and speech recognition (e.", "startOffset": 29, "endOffset": 138}, {"referenceID": 21, "context": ", 2016) and speech recognition (e.g. Graves et al., 2013; Hannun et al., 2014; Chorowski et al., 2015; Chan et al., 2016; Collobert et al., 2016).", "startOffset": 31, "endOffset": 145}, {"referenceID": 11, "context": ", 2016) and speech recognition (e.g. Graves et al., 2013; Hannun et al., 2014; Chorowski et al., 2015; Chan et al., 2016; Collobert et al., 2016).", "startOffset": 31, "endOffset": 145}, {"referenceID": 8, "context": ", 2016) and speech recognition (e.g. Graves et al., 2013; Hannun et al., 2014; Chorowski et al., 2015; Chan et al., 2016; Collobert et al., 2016).", "startOffset": 31, "endOffset": 145}, {"referenceID": 12, "context": ", 2016) and speech recognition (e.g. Graves et al., 2013; Hannun et al., 2014; Chorowski et al., 2015; Chan et al., 2016; Collobert et al., 2016).", "startOffset": 31, "endOffset": 145}, {"referenceID": 10, "context": "Other works (e.g Dauphin et al., 2014; Choromanska et al., 2015) have looked at the structure of the error surface to analyze how trainable these models are.", "startOffset": 12, "endOffset": 64}, {"referenceID": 30, "context": "Finally, another point of discussion is how well these models can generalize (Nesterov & Vial, 2008; Keskar et al., 2017; Zhang et al., 2017).", "startOffset": 77, "endOffset": 141}, {"referenceID": 52, "context": "Finally, another point of discussion is how well these models can generalize (Nesterov & Vial, 2008; Keskar et al., 2017; Zhang et al., 2017).", "startOffset": 77, "endOffset": 141}, {"referenceID": 10, "context": ", 2014; Choromanska et al., 2015) have looked at the structure of the error surface to analyze how trainable these models are. Finally, another point of discussion is how well these models can generalize (Nesterov & Vial, 2008; Keskar et al., 2017; Zhang et al., 2017). These correspond, respectively, to low approximation, optimization and estimation error as described by Bottou (2010).", "startOffset": 8, "endOffset": 388}, {"referenceID": 30, "context": "Another conjecture that was recently (Keskar et al., 2017) explored, but that could be traced back to Hochreiter & Schmidhuber (1997), relies on the geometry of the loss function around a given solution.", "startOffset": 37, "endOffset": 58}, {"referenceID": 13, "context": "For example, Bottou & LeCun (2005); Bottou & Bousquet (2008); Duchi et al. (2011); Nesterov & Vial (2008); Hardt et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 13, "context": "For example, Bottou & LeCun (2005); Bottou & Bousquet (2008); Duchi et al. (2011); Nesterov & Vial (2008); Hardt et al.", "startOffset": 62, "endOffset": 106}, {"referenceID": 13, "context": "For example, Bottou & LeCun (2005); Bottou & Bousquet (2008); Duchi et al. (2011); Nesterov & Vial (2008); Hardt et al. (2016); Bottou et al.", "startOffset": 62, "endOffset": 127}, {"referenceID": 5, "context": "(2016); Bottou et al. (2016) rely on the concept of stochastic approximation.", "startOffset": 8, "endOffset": 29}, {"referenceID": 5, "context": "(2016); Bottou et al. (2016) rely on the concept of stochastic approximation. Another conjecture that was recently (Keskar et al., 2017) explored, but that could be traced back to Hochreiter & Schmidhuber (1997), relies on the geometry of the loss function around a given solution.", "startOffset": 8, "endOffset": 212}, {"referenceID": 9, "context": "Chaudhari et al. (2017) relies, in contrast, on the curvature of the second order structure around the minimum, while Keskar et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Chaudhari et al. (2017) relies, in contrast, on the curvature of the second order structure around the minimum, while Keskar et al. (2017) looks at the maximum loss in a bounded neighbourhood of the minimum.", "startOffset": 0, "endOffset": 139}, {"referenceID": 9, "context": "Chaudhari et al. (2017); Keskar et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Chaudhari et al. (2017); Keskar et al. (2017) suggest that this information is encoded in the eigenvalues of the Hessian.", "startOffset": 0, "endOffset": 46}, {"referenceID": 30, "context": "Additionally Keskar et al. (2017) defines the notion of sharpness.", "startOffset": 13, "endOffset": 34}, {"referenceID": 29, "context": "\u2022 \u03c6rect is the rectified elementwise activation function (Jarrett et al., 2009; Nair & Hinton, 2010; Glorot et al., 2011), which is the positive part (zi)i 7\u2192 (max(zi, 0))i.", "startOffset": 57, "endOffset": 121}, {"referenceID": 18, "context": "\u2022 \u03c6rect is the rectified elementwise activation function (Jarrett et al., 2009; Nair & Hinton, 2010; Glorot et al., 2011), which is the positive part (zi)i 7\u2192 (max(zi, 0))i.", "startOffset": 57, "endOffset": 121}, {"referenceID": 38, "context": "Let us redefine, for convenience, the non-negative homogeneity property (Neyshabur et al., 2015; Lafond et al., 2016) below.", "startOffset": 72, "endOffset": 117}, {"referenceID": 33, "context": "Let us redefine, for convenience, the non-negative homogeneity property (Neyshabur et al., 2015; Lafond et al., 2016) below.", "startOffset": 72, "endOffset": 117}, {"referenceID": 32, "context": "Note that beside this property, the reason for studying the rectified linear activation is for its widespread adoption (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016).", "startOffset": 119, "endOffset": 211}, {"referenceID": 49, "context": "Note that beside this property, the reason for studying the rectified linear activation is for its widespread adoption (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016).", "startOffset": 119, "endOffset": 211}, {"referenceID": 24, "context": "Note that beside this property, the reason for studying the rectified linear activation is for its widespread adoption (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016).", "startOffset": 119, "endOffset": 211}, {"referenceID": 23, "context": "Other models like leaky recitifers (He et al., 2015) or maxout networks (Goodfellow et al.", "startOffset": 35, "endOffset": 52}, {"referenceID": 9, "context": "Many directions However, some notion of sharpness might take into account the entire eigenspectrum of the Hessian as opposed to its largest eigenvalue, for instance, Chaudhari et al. (2017) describe the notion of wide valleys, allowing the presence of very few large eigenvalues.", "startOffset": 166, "endOffset": 190}, {"referenceID": 42, "context": "Since Sagun et al. (2016) seems to suggests that rank deficiency in the Hessian is due to over-parametrization of the model, one could conjecture that ( r\u2212mink\u2264K(nk) )", "startOffset": 6, "endOffset": 26}, {"referenceID": 30, "context": "We have redefined for > 0 the -sharpness of Keskar et al. (2017) as follow", "startOffset": 44, "endOffset": 65}, {"referenceID": 29, "context": "This also applies when using the full-space -sharpness used by Keskar et al. (2017). We can prove this similarly using the equivalence of norms in finite dimensional vector spaces and the fact that for c > 0, > 0, \u2264 (c + 1) (see Keskar et al.", "startOffset": 63, "endOffset": 84}, {"referenceID": 29, "context": "This also applies when using the full-space -sharpness used by Keskar et al. (2017). We can prove this similarly using the equivalence of norms in finite dimensional vector spaces and the fact that for c > 0, > 0, \u2264 (c + 1) (see Keskar et al. (2017)).", "startOffset": 63, "endOffset": 250}, {"referenceID": 29, "context": "This also applies when using the full-space -sharpness used by Keskar et al. (2017). We can prove this similarly using the equivalence of norms in finite dimensional vector spaces and the fact that for c > 0, > 0, \u2264 (c + 1) (see Keskar et al. (2017)). We have not been able to show a similar problem with random subspace -sharpness used by Keskar et al. (2017), i.", "startOffset": 63, "endOffset": 361}, {"referenceID": 9, "context": "a restriction of the maximization to a random subspace, which could relate to the notion of wide valleys described by Chaudhari et al. (2017).", "startOffset": 118, "endOffset": 142}, {"referenceID": 14, "context": "Several practical (Dinh et al., 2014; Rezende & Mohamed, 2015; Kingma et al., 2016; Dinh et al., 2016) and theoretical works (Hyv\u00e4rinen & Pajunen, 1999) show how powerful bijections can be.", "startOffset": 18, "endOffset": 102}, {"referenceID": 15, "context": "Several practical (Dinh et al., 2014; Rezende & Mohamed, 2015; Kingma et al., 2016; Dinh et al., 2016) and theoretical works (Hyv\u00e4rinen & Pajunen, 1999) show how powerful bijections can be.", "startOffset": 18, "endOffset": 102}, {"referenceID": 43, "context": "Instances of commonly used reparametrization are batch normalization (Ioffe & Szegedy, 2015), or the virtual batch normalization variant (Salimans et al., 2016), and weight normalization (Badrinarayanan et al.", "startOffset": 137, "endOffset": 160}, {"referenceID": 2, "context": ", 2016), and weight normalization (Badrinarayanan et al., 2015; Salimans & Kingma, 2016; Arpit et al., 2016).", "startOffset": 34, "endOffset": 108}, {"referenceID": 1, "context": ", 2016), and weight normalization (Badrinarayanan et al., 2015; Salimans & Kingma, 2016; Arpit et al., 2016).", "startOffset": 34, "endOffset": 108}, {"referenceID": 1, "context": ", 2015; Salimans & Kingma, 2016; Arpit et al., 2016). Im et al. (2016) have plotted how the loss function landscape was affected by batch normalization.", "startOffset": 33, "endOffset": 71}, {"referenceID": 48, "context": "Motivated by some work in adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015) for deep neural networks, one could decide on its generalization property by analyzing the gradient of the prediction function on examples.", "startOffset": 47, "endOffset": 94}, {"referenceID": 19, "context": "Motivated by some work in adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015) for deep neural networks, one could decide on its generalization property by analyzing the gradient of the prediction function on examples.", "startOffset": 47, "endOffset": 94}, {"referenceID": 34, "context": "This remark applies in applications involving images, sound or other signals with invariances (Larsen et al., 2015).", "startOffset": 94, "endOffset": 115}, {"referenceID": 34, "context": "This remark applies in applications involving images, sound or other signals with invariances (Larsen et al., 2015). For example, Theis et al. (2016) show for images how a small drift of one to four pixels can incur a large difference in terms of L2 norm.", "startOffset": 95, "endOffset": 150}, {"referenceID": 9, "context": "It has been observed empirically that minima found by standard deep learning algorithms that generalize well tend to be flatter than found minima that did not generalize well (Chaudhari et al., 2017; Keskar et al., 2017).", "startOffset": 175, "endOffset": 220}, {"referenceID": 30, "context": "It has been observed empirically that minima found by standard deep learning algorithms that generalize well tend to be flatter than found minima that did not generalize well (Chaudhari et al., 2017; Keskar et al., 2017).", "startOffset": 175, "endOffset": 220}, {"referenceID": 47, "context": "In the spirit of (Swirszcz et al., 2016), our", "startOffset": 17, "endOffset": 40}], "year": 2017, "abstractText": "Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.", "creator": "LaTeX with hyperref package"}}}