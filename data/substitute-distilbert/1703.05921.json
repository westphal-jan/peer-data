{"id": "1703.05921", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery", "abstract": "obtaining models that capture characteristic markers relevant during disease progression and treatment monitoring yields challenging. models are typically based on large tags of data with annotated examples of known markers preserved while automating detection. high implementation effort and the limitation to a vocabulary of known markers limit the power of such approaches. here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers. we propose anogan, a deep convolutional generative adversarial network to learn a lot of normal anatomical memory, accompanying a geometric anomaly scoring scheme based on the mapping from image spaces to a latent space. applied to new data, the model labels identify, and scores image patches indicating their significance as the learned distribution. calculations on optical coherence tomography imagery of the retina explain that the approach correctly identifies anomalous images, such as images recalling retinal fluid or hyperreflective foci.", "histories": [["v1", "Fri, 17 Mar 2017 08:27:05 GMT  (1869kb,D)", "http://arxiv.org/abs/1703.05921v1", "To be published in the proceedings of the international conference on Information Processing in Medical Imaging (IPMI), 2017"]], "COMMENTS": "To be published in the proceedings of the international conference on Information Processing in Medical Imaging (IPMI), 2017", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["thomas schlegl", "philipp seeb\\\"ock", "sebastian m waldstein", "ursula schmidt-erfurth", "georg langs"], "accepted": false, "id": "1703.05921"}, "pdf": {"name": "1703.05921.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery", "authors": ["Thomas Schlegl", "Philipp Seeb\u00f6ck", "Sebastian M. Waldstein", "Ursula Schmidt-Erfurth", "Georg Langs"], "emails": ["thomas.schlegl@meduniwien.ac.at"], "sections": [{"heading": "1 Introduction", "text": "The detection and quantification of disease markers in imaging data is critical during diagnosis, and monitoring of disease progression, or treatment response. Relying on the vocabulary of known markers limits the use of imaging data containing far richer relevant information. Here, we demonstrate that relevant anomalies can be identified by unsupervised learning on large-scale imaging data.\nMedical imaging enables the observation of markers correlating with disease status, and treatment response. While there is a wide range of known markers (e.g., characteristic image appearance of brain tumors or calcification patterns in breast screening), many diseases lack a sufficiently broad set, while in others the predictive power of markers is limited. Furthermore, even if predictive\n? This work has received funding from IBM, FWF (I2714-B31), OeNB (15356, 15929), the Austrian Federal Ministry of Science, Research and Economy (CDL OPTIMA).\nar X\niv :1\n70 3.\n05 92\n1v 1\n[ cs\n.C V\n] 1\nmarkers are known, their computational detection in imaging data typically requires extensive supervised training using large amounts of annotated data such as labeled lesions. This limits our ability to exploit imaging data for treatment decisions.\nHere, we propose unsupervised learning to create a rich generative model of healthy local anatomical appearance. We show how generative adversarial networks (GANs) can solve the central problem of creating a sufficiently representative model of appearance, while at the same time learning a generative and discriminative component. We propose an improved technique for mapping from image space to latent space. We use both components to differentiate between observations that conform to the training data and such data that does not fit.\nRelated Work Anomaly detection is the task of identifying test data not fitting the normal data distribution seen during training. Approaches for anomaly detection exist in various domains, ranging from video analysis [1] to remote sensing [2]. They typically either use an explicit representation of the distribution of normal data in a feature space, and determine outliers based on the local density at the observations\u2019 position in the feature space. Carrera et al. [3] utilized convolutional sparse models to learn a dictionary of filters to detect anomalous regions in texture images. Erfani et al. [4] proposed a hybrid model for unsupervised anomaly detection that uses a one-class support vector machine (SVM). The SVM was trained from features that were learned by a deep belief network (DBN). The experiments in the aforementioned works were performed on real-life-datasets comprising 1D inputs, synthetic data or texture images, which have lower dimensionality or different data characteristics compared to medical images. An investigation of anomaly detection research papers can be found in [5]. In clinical optical coherence tomography (OCT) scan analysis, Venhuizen et al. [6] used bag-of-word features as a basis for supervised random forest classifier training to distinguish diseased patients from healthy subjects. Schlegl et al. [7] utilized convolutional neural networks to segment retinal fluid regions in OCT data via weakly supervised learning based on semantic descriptions of pathology-location pairs extracted from medical reports. In contrast to our approach, both works used some form of supervision for classifier training. Seebo\u0308ck et al. [8] identified anomalous regions in OCT images through unsupervised learning on healthy examples, using a convolutional autoencoder and a\none-class SVM, and explored different classes of anomalies. In contrast to this work, the SVM in [8] involved the need to choose a hyper-parameter that defined the amount of training points covered by the estimated healthy region.\nGANs enable to learn generative models generating detailed realistic images [9,10,11]. Radford et al. [12] introduced deep convolutional generative adversarial networks (DCGANs) and showed that GANs are capable of capturing semantic image content enabling vector arithmetic for visual concepts. Yeh et al. [13] trained GANs on natural images and applied the trained model for semantic image inpainting. Compared to Yeh et al. [13], we implement two adaptations for an improved mapping from images to the latent space. We condition the search in the latent space on the whole query image, and propose a novel variant to guide the search in the latent space (inspired by feature matching [14]). In addition, we define an anomaly score, which is not needed in an inpainting task. The main difference of this paper to aforementioned anomaly detection work is the representative power of the generative model and the coupled mapping schema, which utilizes a trained DCGAN and enables accurate discrimination between normal anatomy, and local anomalous appearance. This renders the detection of subtle anomalies at scale feasible.\nContribution In this paper, we propose adversarial training of a generative model of normal appearance (see blue block in Figure 1), described in Section 2.1, and a coupled mapping schema, described in Section 2.2, that enables the evaluation of novel data (Section 2.3) to identify anomalous images and segment anomalous regions within imaging data (see red block in Figure 1). Experiments on labeled test data, extracted from spectral-domain OCT (SD-OCT) scans, show that this approach identifies known anomalies with high accuracy, and at the same time detects other anomalies for which no voxel-level annotations are available. To the best of our knowledge, this is the first work, where GANs are used for anomaly or novelty detection. Additionally, we propose a novel mapping approach, wherewith the pre-image problem can be tackled."}, {"heading": "2 Generative Adversarial Representation Learning to Identify Anomalies", "text": "To identify anomalies, we learn a model representing normal anatomical variability based on GANs [13]. This method trains a generative model, and a discriminator to distinguish between generated and real data simultaneously (see Figure 2(a)). Instead of a single cost function optimization, it aims at the Nash equilibrium of costs, increasing the representative power and specificity of the generative model, while at the same time becoming more accurate in classifying real- from generated data and improving the corresponding feature mapping. In the following we explain how to build this model (Section 2.1), and how to use it to identify appearance not present in the training data (Sections 2.2 and 2.3)."}, {"heading": "2.1 Unsupervised Manifold Learning of Normal Anatomical Variability", "text": "We are given a set of M medical images Im showing healthy anatomy, with m = 1, 2, . . . ,M , where Im \u2208 Ra\u00d7b is an intensity image of size a \u00d7 b. From each image Im, we extract K 2D image patches xk,m of size c\u00d7 c from randomly sampled positions resulting in data x = xk,m \u2208 X , with k = 1, 2, . . . ,K. During training we are only given \u3008Im\u3009 and train a generative adversarial model to learn the manifold X (blue region in Figure 2(b)), which represents the variability of the training images, in an unsupervised fashion. For testing, we are given \u3008yn, ln\u3009, where yn are unseen images of size c \u00d7 c extracted from new testing data J and ln \u2208 {0, 1} is an array of binary image-wise ground-truth labels, with n = 1, 2, . . . , N . These labels are only given during testing, to evaluate the anomaly detection performance based on a given pathology.\nEncoding Anatomical Variability with a Generative Adversarial Network. A GAN consists of two adversarial modules, a generator G and a discriminator D. The generator G learns a distribution pg over data x via a mapping G(z) of samples z, 1D vectors of uniformly distributed input noise sampled from latent space Z, to 2D images in the image space manifold X , which is populated by healthy examples. In this setting, the network architecture of the generator G is equivalent to a convolutional decoder that utilizes a stack of strided convolutions. The discriminator D is a standard CNN that maps a 2D image to a single scalar value D(\u00b7). The discriminator output D(\u00b7) can be interpreted as probability that the given input to the discriminator D was a real image x sampled from training data X or generated G(z) by the generator G. D and G are simultaneously optimized through the following two-player minimax game with value function V (G,D) [9]:\nmin G max D V (D,G) = Ex\u223cpdata(x) [logD(x)] + Ez\u223cpz(z) [log(1\u2212D(G(z)))] . (1)\nThe discriminator is trained to maximize the probability of assigning real training examples the \u201creal\u201d and samples from pg the \u201cfake\u201d label. The generator G\nis simultaneously trained to fool D via minimizing V (G) = log(1 \u2212 D(G(z))), which is equivalent to maximizing\nV (G) = D(G(z)). (2)\nDuring adversarial training the generator improves in generating realistic images and the discriminator progresses in correctly identifying real and generated images."}, {"heading": "2.2 Mapping new Images to the Latent Space", "text": "When adversarial training is completed, the generator has learned the mapping G(z) = z 7\u2192 x from latent space representations z to realistic (normal) images x. But GANs do not automatically yield the inverse mapping \u00b5(x) = x 7\u2192 z for free. The latent space has smooth transitions [12], so sampling from two points close in the latent space generates two visually similar images. Given a query image x, we aim to find a point z in the latent space that corresponds to an image G(z) that is visually most similar to query image x and that is located on the manifold X . The degree of similarity of x and G(z) depends on to which extent the query image follows the data distribution pg that was used for training of the generator. To find the best z, we start with randomly sampling z1 from the latent space distribution Z and feed it into the trained generator to get a generated image G(z1). Based on the generated image G(z1) we define a loss function, which provides gradients for the update of the coefficients of z1 resulting in an updated position in the latent space, z2. In order to find the most similar image G(z\u0393 ), the location of z in the latent space Z is optimized in an iterative process via \u03b3 = 1, 2, . . . , \u0393 backpropagation steps.\nIn the spirit of [13], we define a loss function for the mapping of new images to the latent space that comprises two components, a residual loss and a discrimination loss. The residual loss enforces the visual similarity between the generated image G(z\u03b3) and query image x. The discrimination loss enforces the generated image G(z\u03b3) to lie on the learned manifold X . Therefore, both components of the trained GAN, the discriminator D and the generator G, are utilized to adapt the coefficients of z via backpropagation. In the following, we give a detailed description of both components of the loss function.\nResidual Loss The residual loss measures the visual dissimilarity between query image x and generated image G(z\u03b3) in the image space and is defined by\nLR(z\u03b3) = \u2211 |x\u2212G(z\u03b3)|. (3)\nUnder the assumption of a perfect generator G and a perfect mapping to latent space, for an ideal normal query case, images x and G(z\u03b3) are identical. In this case, the residual loss is zero.\nDiscrimination Loss For image inpainting, Yeh et al. [13] based the computation of the discrimination loss LD\u0302(z\u03b3) on the discriminator output by feeding the generated image G(z\u03b3) into the discriminator LD\u0302(z\u03b3) = \u03c3(D(G(z\u03b3)), \u03b1), where \u03c3 is the sigmoid cross entropy, which defined the discriminator loss of real images during adversarial training, with logits D(G(z\u03b3)) and targets \u03b1 = 1.\nAn improved discrimination loss based on feature matching In contrast to the work of Yeh et al. [13], where z\u03b3 is updated to fool D, we define an alternative discrimination loss LD(z\u03b3), where z\u03b3 is updated to match G(z\u03b3) with the learned distribution of normal images. This is inspired by the recently proposed feature matching technique [14].\nFeature matching addresses the instability of GANs due to over-training on the discriminator response [14]. In the feature matching technique, the objective function for optimizing the generator is adapted to improve GAN training. Instead of optimizing the parameters of the generator via maximizing the discriminator\u2019s output on generated examples (Eq. (2)), the generator is forced to generate data that has similar statistics as the training data, i.e. whose intermediate feature representation is similar to those of real images. Salimans et al. [14] found that feature matching is especially helpful when classification is the target task. Since we do not use any labeled data during adversarial training, we do not aim for learning class-specific discriminative features but we aim for learning good representations. Thus, we do not adapt the training objective of the generator during adversarial training, but instead use the idea of feature matching to improve the mapping to the latent space. Instead of using the scalar output of the discriminator for computing the discrimination loss, we propose to use a richer intermediate feature representation of the discriminator and define the discrimination loss as follows:\nLD(z\u03b3) = \u2211 |f(x)\u2212 f(G(z\u03b3))|, (4)\nwhere the output of an intermediate layer f(\u00b7) of the discriminator is used to specify the statistics of an input image. Based on this new loss term, the adaptation of the coordinates of z does not only rely on a hard decision of the trained discriminator, whether or not a generated image G(z\u03b3) fits the learned distribution of normal images, but instead takes the rich information of the feature representation, which is learned by the discriminator during adversarial training, into account. In this sense, our approach utilizes the trained discriminator not as classifier but as a feature extractor.\nFor the mapping to the latent space, we define the overall loss as weighted sum of both components:\nL(z\u03b3) = (1\u2212 \u03bb) \u00b7 LR(z\u03b3) + \u03bb \u00b7 LD(z\u03b3). (5)\nOnly the coefficients of z are adapted via backpropagation. The trained parameters of the generator and discriminator are kept fixed."}, {"heading": "2.3 Detection of Anomalies", "text": "During anomaly identification in new data we evaluate the new query image x as being a normal or anomalous image. Our loss function (Eq. (5)), used for mapping to the latent space, evaluates in every update iteration \u03b3 the compatibility of generated images G(z\u03b3) with images, seen during adversarial training. Thus, an anomaly score, which expresses the fit of a query image x to the model of normal images, can be directly derived from the mapping loss function (Eq. (5)):\nA(x) = (1\u2212 \u03bb) \u00b7R(x) + \u03bb \u00b7D(x), (6)\nwhere the residual score R(x) and the discrimination score D(x) are defined by the residual loss LR(z\u0393 ) and the discrimination loss LD(z\u0393 ) at the last (\u0393 th) update iteration of the mapping procedure to the latent space, respectively. The model yields a large anomaly score A(x) for anomalous images, whereas a small anomaly score means that a very similar image was already seen during training. We use the anomaly score A(x) for image based anomaly detection. Additionally, the residual image xR = |x\u2212G(z\u0393 )| is used for the identification of anomalous regions within an image. For purposes of comparison, we additionally define a reference anomaly score A\u0302(x) = (1\u2212\u03bb) \u00b7R(x) +\u03bb \u00b7 D\u0302(x), where D\u0302(x) = LD\u0302(z\u0393 ) is the reference discrimination score used by Yeh et al. [13]."}, {"heading": "3 Experiments", "text": "Data, Data Selection and Preprocessing We evaluated the method on clinical high resolution SD-OCT volumes of the retina with 49 B-scans (representing an image slice in zx-plane) per volume and total volume resolutions of 496\u00d7512\u00d749 voxels in z-, x-, and y direction, respectively. The GAN was trained on 2D image patches extracted from 270 clinical OCT volumes of healthy subjects, which were chosen based on the criterion that the OCT volumes do not contain fluid regions. For testing, patches were extracted from 10 additional healthy cases and 10 pathological cases, which contained retinal fluid. The OCT volumes were preprocessed in the following way. The gray values were normalized to range from -1 to 1. The volumes were resized in x-direction to a size of 22\u00b5m resulting in approximately 256 columns. The retinal area was extracted and flattened to adjust for variations in orientation, shape and thickness. We used an automatic layer segmentation algorithm following [15] to find the top and bottom layer of the retina that define the border of the retina in z-direction. From these normalized and flattened volumes, we extracted in total 1.000.000 2D training patches with an image resolution of 64\u00d764 pixels at randomly sampled positions. Raw data and preprocessed image representation are shown in Figure 1. The test set in total consisted of 8192 image patches and comprised normal and pathological samples from cases not included in the training set. For pathological OCT scans, voxel-wise annotations of fluid and non-fluid regions from clinical retina experts were available. These annotations were only used for statistical evaluation but were never fed to the network, neither during training nor in the\nevaluation phase. For the evaluation of the detection performance, we assigned a positive label to an image, if it contained at least a single pixel annotated as retinal fluid.\nEvaluation The manifold of normal images was solely learned on image data of healthy cases with the aim to model the variety of healthy appearance. For performance evaluation in anomaly detection we ran the following experiments. (1) We explored qualitatively whether the model can generate realistic images. This assessment was performed on image patches of healthy cases extracted from the training set or test set and on images of diseased cases extracted from the test set. (2) We evaluated quantitatively the anomaly detection accuracy of our approach on images extracted from the annotated test set. We based the anomaly detection on the anomaly score A(x) or only on one of both components, on the residual score R(x) or on the discrimination score D(x) and report receiver operating characteristic (ROC) curves of the corresponding anomaly detection performance on image level.\nBased on our proposed anomaly score A(x), we evaluated qualitatively the segmentation performance and if additional anomalies were identified. (3) To provide more details of individual components\u2019 roles, and the gain by the proposed approach, we evaluated the effect on the anomaly detection performance, when for manifold learning the adversarial training is not performed with a DCGAN but with an adversarial convolutional autoencoder (aCAE) [16], while leaving the definition of the anomaly score unchanged. An aCAE also implements a discriminator but replaces the generator by an encoder-decoder pipeline. The depth of the components of the trained aCAE was comparable to the depth of our adversarial model. As a second alternative approach, denoted as GANR, we evaluated the anomaly detection performance, when the reference anomaly score A\u0302(x), or the reference discrimination score D\u0302(x) were utilized for anomaly scoring and the corresponding losses were used for the mapping from image space to latent space, while the pre-trained GAN parameters of the AnoGAN were used. We report ROC curves for both alternative approaches. Furthermore, we calculated sensitivity, specificity, precision, and recall at the optimal cut-off point on the ROC curves, identified through the Youden\u2019s index and report results for the AnoGan and for both alternative approaches.\nImplementation details As opposed to historical attempts, Radford et al. [12] identified a DCGAN architecture that resulted in stable GAN training on images of sizes 64 \u00d7 64 pixels. Hence, we ran our experiments on image patches of the same size and used widley the same DCGAN architecture for GAN training (Section 2.1) as proposed by Radford et al. [12]1. We used four fractionallystrided convolution layers in the generator, and four convolution layers in the discriminator, all filters of sizes 5\u00d7 5. Since we processed gray-scale images, we utilized intermediate representations with 512\u2212256\u2212128\u221264 channels (instead 1 We adapted: https://github.com/bamos/dcgan-completion.tensorflow\nof 1024\u2212 512\u2212 256\u2212 128 used in [12]). DCGAN training was performed for 20 epochs utilizing Adam [17], a stochastic optimizer. We ran 500 backpropagation steps for the mapping (Section 2.2) of new images to the latent space. We used \u03bb = 0.1 in Equations (5) and (6), which was found empirically due to preceding experiments on a face detection dataset. All experiments were performed using Python 2.7 with the TensorFlow [18] library and run on a Titan X graphics processing unit using CUDA 8.0."}, {"heading": "3.1 Results", "text": "Results demonstrate the generative capability of the DCGAN and the appropriateness of our proposed mapping and scoring approach for anomaly detection. We report qualitative and quantitative results on segmentation performance and detection performance of our approach, respectively. Can the model generate realistic images? The trained model generates realistic looking medical images (second row in Figure 3) that are conditioned by sampling from latent representations z, which are found through our mapping approach, described in Section 2.2. In the case of normal image patches (see first and second block in Figure 3), our model is able to generate images that are visually similar to the query images (first row in Figure 3). But in the case of anomalous images, the pairs of input images and generated images show obvious intensity or textural differences (see third block in Figure 3). The tSNE embedding (Figure 2(b)) of normal and anomalous images in the feature representation of the last convolution layer of the discriminator that is utilized in the discrimination loss, illustrates the usability of the discriminator\u2019s features for\nanomaly detection and suggests that our AnoGAN learns a meaningful manifold of normal anatomical variability.\nCan the model detect anomalies? Figure 4(b) shows the ROC curves for image level anomaly detection based on the anomaly score A(x), or on one of both components, on the residual score R(x), or on the discrimination score D(x). The corresponding area under the ROC curve (AUC) is specified in parentheses. In addition, the distributions of the residual score R(x) (Figure 4(c)) and of the discrimination score D(x) (Figure 4(d)) over normal images from the training set and test set or over images extracted from diseased cases show that both components of the proposed adversarial score are suitable for the classification of normal and anomalous samples. Figure 3 shows pixel-level identification of anomalies in conjunction with pixel-level annotations of retinal fluid, which demonstrate high accuracy. Last column in Figure 3 demonstrates that the model successfully identifies additional retinal lesions, which in this case correspond to hyperreflective foci (HRF). On image level, the red and yellow bars in Figure 3 demonstrate that our model successfully identifies every example image from diseased cases of the test set as beeing anomalous based on the residual score and the discrimination score, respectively.\nHow does the model compare to other approaches? We evaluated the anomaly detection performance of the GANR, the aCAE and the AnoGAN on image-level labels. The results are summarized in Table 1 and the corresponding ROC curves are shown in Figure 4(a). Although aCAEs simultaneously yield a generative model and a direct mapping to the latent space, which is advantageous in terms of runtimes during testing, this model showed worse performance on the anomaly detection task compared to the AnoGAN. It turned out that aCAEs tend to over-adapt on anomalous images. Figure 4(b) demonstrates that anomaly detection based on our proposed discrimination score D(x) outperforms\nthe reference discrimination score D\u0302(x). Because the scores for the detection of anomalies are directly related to the losses for the mapping to latent space, these results give evidence that our proposed discrimination loss LD(z) is advantageous compared to the discrimination loss LD\u0302(z). Nevertheless, according to the AUC, computed based on the anomaly score, the AnoGAN and the GANR show comparable results (Figure 4(a)). This has to be attributed to the good performance of the residual score R(x). A good anomaly detection performance (cf. PD in Figure 4(a) and Table 1) can be obtained when the mapping to the latent space is skipped and a binary decision is derived from the discriminator output, conditioned directly on the query image."}, {"heading": "4 Conclusion", "text": "We propose anomaly detection based on deep generative adversarial networks. By concurrently training a generative model and a discriminator, we enable the identification of anomalies on unseen data based on unsupervised training of a model on healthy data. Results show that our approach is able to detect different known anomalies, such as retinal fluid and HRF, which have never been seen during training. Therefore, the model is expected to be capable to discover novel anomalies. While quantitative evaluation based on a subset of anomaly classes is limited, since false positives do not take novel anomalies into account, results demonstrate good sensitivity and the capability to segment anomalies. Discovering anomalies at scale enables the mining of data for marker candidates subject to future verification. In contrast to prior work, we show that the utilization of the residual loss alone yields good results for the mapping from image to latent space, and a slight improvement of the results can be achieved with the proposed adaptations."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Obtaining models that capture imaging markers relevant for<lb>disease progression and treatment monitoring is challenging. Models are<lb>typically based on large amounts of data with annotated examples of<lb>known markers aiming at automating detection. High annotation ef-<lb>fort and the limitation to a vocabulary of known markers limit the<lb>power of such approaches. Here, we perform unsupervised learning to<lb>identify anomalies in imaging data as candidates for markers. We pro-<lb>pose AnoGAN, a deep convolutional generative adversarial network to<lb>learn a manifold of normal anatomical variability, accompanying a novel<lb>anomaly scoring scheme based on the mapping from image space to a la-<lb>tent space. Applied to new data, the model labels anomalies, and scores<lb>image patches indicating their fit into the learned distribution. Results<lb>on optical coherence tomography images of the retina demonstrate that<lb>the approach correctly identifies anomalous images, such as images con-<lb>taining retinal fluid or hyperreflective foci.", "creator": "LaTeX with hyperref package"}}}