{"id": "1703.06541", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "Native Language Identification using Stacked Generalization", "abstract": "ensemble methods using symbolic classifiers have proven to be the most successful approach approaching the task of native language identification ( nli ), achieving the current solution of the array. however, a systematic examination of ensemble methods for python has yet to easily concluded. additionally, deeper conceptual architectures such as classifier stacking the not been closely evaluated. we gather a set of experiments using three ensemble - based models, experimenting each with specialized configurations and algorithms. this includes a rigorous application of meta - classification models for nli, achieving state - of - the - art results on three datasets from different languages. tables also present the first use of statistical significance testing for comparing nli readings, seeing that our results are significantly better than the previous state of my art. we make available a collection of test set predictions to facilitate future statistical tests.", "histories": [["v1", "Sun, 19 Mar 2017 23:42:28 GMT  (965kb,D)", "http://arxiv.org/abs/1703.06541v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shervin malmasi", "mark dras"], "accepted": false, "id": "1703.06541"}, "pdf": {"name": "1703.06541.pdf", "metadata": {"source": "CRF", "title": "Native Language Identification using Stacked Generalization", "authors": ["Shervin Malmasi", "Mark Dras"], "emails": ["shervin.malmasi@mq.edu.au", "mark.dras@mq.edu.au"], "sections": [{"heading": "1 Introduction", "text": "Native Language Identification (NLI) is the task of identifying a writer\u2019s native language (L1) based only on their writings in a second language (the L2). NLI works by identifying language use patterns that are common to groups of speakers of the same native language. This process is underpinned by the presupposition that an author\u2019s L1 disposes them towards certain language production patterns in their L2, as influenced by their mother tongue. This relates to cross-linguistic influence (CLI), a key topic in the field of Second Language Acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages (Ortega, 2009).\nIt has been noted in the linguistics literature since the 1950s that speakers of particular languages have characteristic production patterns when writing in a second language. This language transfer phenomenon has been investigated independently in various fields from different perspectives, including qualitative research in SLA and recently via predictive models in NLP (Jarvis and Crossley, 2012). Recently this has motivated studies in NLI, a subtype of text classification where the goal is to determine the native language of an author using texts they have written in a second language or L2 (Tetreault et al., 2013).\nThe motivations for NLI are manifold. Such techniques can help SLA researchers identify important L1-specific learning and teaching issues. In turn, the identification of such issues can enable researchers to develop pedagogical material that takes into consideration a learner\u2019s L1 and addresses them. It can also be applied in a forensic context, for example, to glean information about the discriminant L1 cues in an anonymous text.\nNLI is most commonly framed as a multi-class supervised classification task. Researchers have experimented with a range of machine learning algorithms, with Support Vector Machines having found the most success. However, some of the most successful approaches have made use of classifier ensemble methods to further improve performance on this task. This is a trend that has become apparent in recent work on this task, as we\u2019ll outline in \u00a72. In fact, all recent state-of-the-art systems have relied on some form of multiple classifier system.\nHowever, a thorough examination of ensemble methods for NLI \u2014 one empirically comparing different architectures and algorithms \u2014 has yet to be conducted. Additionally, more sophisticated ensemble architectures, such as stacked generalization (classifier stacking), have not been closely evaluated. This meta-classification approach is an advanced method that has proven to be effective in many classification tasks; its systematic application could improve the state of the art in NLI.\nar X\niv :1\n70 3.\n06 54\n1v 1\n[ cs\n.C L\n] 1\n9 M\nar 2\n01 7\nThis has links to the idea of adding layers to increase power in neural network-based deep learning, which has come to be an important approach in NLP over the last couple of years (Manning, 2015); Eldan and Shamir (2016) note that \u201cOverwhelming empirical evidence as well as intuition indicates that having depth in the neural network is indeed important\u201d. Deep neural networks can in fact be seen as layered classifiers (Goldberg, 2015), and ensemble methods as an alternative way of adding power via additional layers. In this article we look just at ensemble methods: deep learning has not yet produced state-of-the-art results on related tasks (Malmasi et al., 2016),1 and our goal is to understand what it is that has made ensemble methods to date in NLI so successful.\nThe primary focus of the present work is to address this gap by presenting a comprehensive and rigorous examination of how ensemble methods can be applied for NLI. We aim to examine several different ensemble and meta-classification architectures, each of which can utilize different configurations and algorithms.\nFurthermore, previous ensemble methods have not been tested on different datasets, making the generalizability of these models for NLI unclear. Ideally, the same method should be tested across multiple corpora to assess its validity. When working on a common dataset, authors should also aim to compare the performance of their methods directly. To this end, we also apply our methods to three datasets to evaluate their generalizability. NLI methods have been recently applied to different languages and we believe that this type of multilingual evaluation is an important trend for future NLI research. Our chosen datasets therefore include the most commonly used English NLI corpus as well as more recently used Chinese and Norwegian corpora.\nThe final aspect of this work deals with evaluation, which in NLI work thus far has relied mostly on direct comparisons between the reported accuracies of various systems and their relative differences. However, as the reported performance continues to rise, it becomes more important to compare and interpret these results objectively. Although statistical methods can facilitate such an objective interpretation and comparison between systems, they have not been used in NLI, for reasons which we will outline later. Consequently, the final objective of this study is to apply statistical methods for comparing different approaches. We not only compare our results against those previously reported, but also conduct statistical significance testing against other state-of-the-art NLI systems, something which has not been performed to date.\nTo summarize, the principal aims of the present study are to:\n1. Apply several advanced ensemble combination methods to NLI and evaluate their performance against previously used ensemble methods.\n2. Evaluate the use of meta-classifiers for NLI, applying different feature representations and a range of learning algorithms.\n3. Compare the performance of these methods to previous results and assess the methods on different languages/datasets.\n4. Investigate the use of statistical testing for comparing NLI systems.\nThe remainder of this paper is organized as follows. In \u00a72 we introduce ensemble classification and recap previous work in NLI. Our data is introduced in \u00a73, followed by our experimental setup in \u00a74, and our classification features in \u00a75. Our ensemble-based models are detailed in \u00a76 and experimental results are reported in \u00a77. We then conclude with a discussion in \u00a78."}, {"heading": "2 Related Work", "text": "This work draws on two broad areas of research: ensemble-based classification methods and work in NLI."}, {"heading": "2.1 Ensemble Classifiers", "text": "Classifier ensembles are a way of combining different classifiers or experts with the goal of improving overall accuracy through enhanced decision making. Instead of relying on decisions by a single expert, they attempt to reach a decision by utilizing the collective input from a committee of experts.\n1Traditional text classification methods substantially outperformed all deep learning approaches in the 2016 DSL Shared Task.\nThey have been applied to a wide range of real-world problems and shown to achieve better results compared to single-classifier methods (Oza and Tumer, 2008). Through aggregating the outputs of multiple classifiers in some way, their outputs are generally considered to be more robust. Ensemble methods continue to receive increasing attention from investigators and remain a focus of machine learning research (Woz\u0301niak et al., 2014; Kuncheva and Rodr\u0301\u0131guez, 2014).\nSuch ensemble-based systems often use a parallel architecture, where the classifiers are run independently and their outputs are aggregated using a fusion method. The specifics of how such systems work will be detailed in section 6.\nThey have been applied to various classification tasks with good results. Not surprisingly, researchers have attempted to use them for improving the performance of NLI, as we discuss in the next section."}, {"heading": "2.2 Native Language Identification", "text": "NLI work has been growing in recent years, using a wide range of syntactic and more recently, lexical features to distinguish the L1. A detailed review of NLI methods is omitted here for reasons of space, but a thorough exposition is presented in the report from the very first NLI Shared Task that was held in 2013 (Tetreault et al., 2013).\nMost English NLI work has been done using two corpora. The International Corpus of Learner English (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, Toefl11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre.\nResearch has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a,c). Recently, Malmasi and Dras (2014b) introduced the Jinan Chinese Learner Corpus (Wang et al., 2015) for NLI and their results indicate that feature performance may be similar across corpora and even L1-L2 pairs. Similarly, Malmasi et al. (2015a) also proposed using the ASK corpus to conduct NLI research using L2 Norwegian data. In this study we make use of three of these aforementioned corpora: Toefl11, JCLC and ASK; detailed descriptions will be provided in \u00a73.\nAs mentioned earlier, some of the most successful approaches to NLI have used ensemble learning methods. We now present an overview of this ensemble-based NLI research.\nTetreault et al. (2012) were the first to propose the use of classifier ensembles for NLI and performed a comprehensive evaluation of the feature types used until that point. In their study they used an ensemble of logistic regression learners using a wide range of features that included character and word n-grams, function words, parts of speech, spelling errors and writing quality markers. With regard to syntactic features, they also investigated the use of Tree Substitution Grammars and dependency features extracted using the Stanford parser. Furthermore, they also proposed using language models for this task and in their system used language model perplexity scores based on lexical 5-grams from each language in the corpus. The set of features used here was the largest of any NLI study to date. With this system, the authors reported state of the art accuracies of 90.1% and 80.9% on the ICLE and Toefl11 corpora, respectively. Tetreault et al. (2012) also conducted cross-corpus evaluation, using the 7 common L1 classes between the ICLE and Toefl11 corpora. Training on the ICLE data, they report an accuracy of 26.6%.\nThe very first shared task focusing on Native Language Identification was held in 2013, bringing further focus, interest and attention to the field. The NLI Shared Task 20133 was co-located with the eighth instalment of the Building Educational Applications Workshop at NAACL-HLT 2013.\nThe competition attracted entries from 29 teams. The winning entry for the shared task was that of Jarvis et al. (2013), with an accuracy of 83.6%. The features used in this system are n-grams of words, parts-of-speech as well as lemmas. In addition to normalizing each text to unit length, the authors also applied a log-entropy weighting schema to the normalized values, which clearly improved the accuracy of the model. An L2-regularized SVM classifier was used to create a single-model system. Furthermore, the authors employed their own procedure for optimizing the cost parameter (C) of the SVM. While they did not use a great number of features or introduce any new features for this task, we posit that their use of weighting schema and hyperparameter optimization gave their system an edge over their competitors, the majority of whom did not employ these techniques.\n2The issues exist as the corpus was not designed specifically for NLI. 3https://sites.google.com/site/nlisharedtask2013/home\nA notable trend among the other entries was the use of ensemble-based systems, which have been shown to achieve better results over systems based on single models. We will now briefly review the systems that took this approach.\nGyawali et al. (2013) utilized lexical and syntactic features based on n-grams of characters, words and part-of-speech tags (using both the Penn TreeBank and Universal Parts Of Speech tagsets), along with perplexity values of character n-grams to build four different models. These models were combined using a voting-based ensemble of SVM classifiers. Features values were weighted using the TF-IDF scheme. In particular, the authors set out to investigate whether a more coarse grained POS tagset would be useful for NLI. They explore the use of the Universal POS tagset which has 12 POS categories in the NLI shared task and compare the results with the fine-grained Penn TreeBank (PTB) tagset that includes 36 POS categories. The highest accuracy of their system in the shared task is 74.8%, achieved by combining all features into an ensemble. The authors found that the use of coarse grained Universal POS tags as features generalizes the syntactic information and reduces the discriminative power of the feature that comes from the fine granularity of the n-grams. For example, the PTB tagset distinguishes verbs into six distinct categories while the Universal POS tagset only has a single category for that grammatical class.\nIn the system designed by Cimino et al. (2013) the authors use a wide set of general purpose features that are designed to be portable across languages, domains and tasks. This set includes features that are lexical (sentence length, document length, type/token ration, character and word n-grams), morphosyntactic (coarse and fine-grained part-of-speech tag n-grams) and syntactic (parse tree and dependencybased features). They report that they found distributional differences across the L1s for many of these features, including average word and sentence lengths. However, we note that many of these differences are not of a large magnitude, and the authors did not run any statistical tests to measure the significance levels of these differences. Using this feature set, they experiment with a single-classifier system as well as classifier ensembles, using SVM and Maximum Entropy classifiers. In their ensemble, they experiment with using a majority voting system as well as a meta-classifier approach. The authors report that the ensemble methods outperform all single-classifier systems (by around 2%), and their best performance of 77.9% is provided by the meta-classifier system which used linear SVM and MaxEnt as the component classifiers and combined the results using a polynomial kernel SVM classifier. While the set of features used in this experiment is not widely different to other reported NLI research, their use of a meta-classifier is an interesting approach that warrants further study.\nIn their system, Goutte et al. (2013) used character, word and part-of-speech n-grams along with syntactic dependencies. They used an ensemble of SVM classifiers trained on each feature space, using a majority vote combiner method. To represent the feature values, they use two value normalization methods based on TF-IDF and cosine normalization. Their best entry achieved an accuracy of 81.8%, higher than many systems using the same standard features and more, demonstrating the effectiveness of using ensemble classifiers and appropriate feature value representation. The authors, like many others, also note that lexical features provided the best performance for a single feature in their system, but that this can be boosted by combining multiple predictors.\nThe MITRE system (Henderson et al., 2013) is another highly lexicalized system where the primary features used are word, part-of-speech and character n-grams. In this system, these features are used by independent classifiers (logistic regression, Winnow2 and language models) whose output is then combined into a final prediction using a Na\u0308\u0131ve Bayes model. Their best performing ensemble was 82.6% accurate in the shared task and the authors emphasize the value of ensemble methods that combine independent systems. Furthermore, the authors also optimized the parameters of their Naive Bayes model using a grid search over the development data.\nHladka et al. (2013) developed an ensemble classifier system using some standard features (lemma, word and part-of-speech n-grams, word skipgrams) with SVM classifiers. They obtained an accuracy of 72.5% in the shared task. They found that their ensemble, which is based on majority voting, outperformed other methods of combining the features. This is yet another piece of evidence pointing to the utility of using ensemble systems for NLI.\nAnother system that utilizes an ensemble is that of Bykh et al. (2013), where they used a probabilitybased ensemble. They use a set of 16 features, including recurring word-based n-grams, recurring Open Class POS (OCPOS) n-grams, dependencies, trees and lemmas. To combine the different feature types, they explored combining all feature into a single vector and also ensembles of SVM classifiers (each\ntrained on a single feature type). Their best shared task performance of 82.2% was achieved using an ensemble with all of their features. Their analysis shows that recurring word-based n-grams are the best performing single feature type, once again demonstrating the relevance and significant of lexical features in NLI.\nFollowing the shared task, Bykh and Meurers (2014) further explored the use of lexicalized and non-lexicalized phrase structure rules for NLI. They show that the inclusion of lexicalized production rules (i.e. preterminal nodes and terminals) provides improved results. In addition to the standard normalized frequency and binary feature representations they also propose two new representations based on a \u201cvariationist sociolinguistic\u201d perspective. Although they show that these representations outperform the normalized frequency approach, they do not compare this to other representations which have been shown to improve NLI accuracy, such as TF-IDF. They combine their lexicalized production rules feature with additional surface n-gram features in a tuned and optimized ensemble, reporting an accuracy of 84.82% on the Toefl11-Test set.\nIonescu et al. (2014) extend the previous work of Popescu and Ionescu (2013) which used string kernels to perform NLI using only character n-gram features. One improvement here is that several string kernels are combined through multiple kernel learning. Although this approach is not based on the types of ensembles we use here, it is similar in the sense that it attempts to combine multiple learners. The authors also perform parameter tuning to select the optimal settings for their system. They report an accuracy of 85.3% on the Toefl11-Test set, 1.7% higher than the winning shared task system. Recently they expanded their approach with additional experiments (Ionescu et al., 2016), although they did not achieve further improvements on Toefl11-Test. One shortcoming of this approach is that they do not present a single model that achieves best performance on the different sets; different parameters are used to achieved the best results for each set. This can be theoretically unsatisfying since it is possible that the different parameters could be overfitting the sets. Ideally, a single model with fixed parameters would obtain the best result across all sets.\nIn sum, we can see that ensemble-based approaches have yielded some of the most successful results in this field. However, we also believe that it is possible to employ ensemble models that are even more sophisticated, leading to improved results. This is the key research question being investigated by the present study."}, {"heading": "3 Data", "text": "We now introduce the three datasets used in this study. One of the goals of this study is to assess the generalizability of the methods and results across datasets, and this requires us to use multiple corpora. They have all been used in previous NLI work and cover different (second) languages: English, Chinese and Norwegian."}, {"heading": "3.1 The TOEFL11 Corpus", "text": "The Toefl11 corpus (Blanchard et al., 2013) \u2014 also known as the ETS Corpus of Non-Native Written English \u2014 is the first dataset designed specifically for the task of NLI and developed with the aim of addressing the above-mentioned deficiencies of other previously used corpora. By providing a common set of L1s and evaluation standards, the authors set out to facilitate the direct comparison of approaches and methodologies.\nFurthermore, as all of the texts were collected through the Educational Testing Service\u2019s electronic test delivery system, this ensures that all of the data files are encoded and stored in a consistent manner.4 The corpus is available through the the Linguistic Data Consortium.5\nIt consists of 12,100 learner texts from speakers of 11 different languages. The texts are independent task essays written in response to eight different prompts, and were collected in the process of administering the Test of English as a Foreign Language (TOEFL R\u00a9) between 2006-2007. The texts are divided into specific training (Toefl11-Train), development (Toefl11-Dev) and test (Toefl11Test) sets. It is also common to combine the training and development sets for training, which we refer to as Toefl11-TrainDev.\nThe 11 L1s are Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu and Turkish. This selection ensures that there are L1s from diverse language families, but also several from within certain families. The L1s and their language families are shown in Figure 1.\n4The essays are distributed as UTF-8 encoded text files. 5https://catalog.ldc.upenn.edu/LDC2014T06\nDravidian\nTelugu\nAfro-Asiatic\nArabic\nSino-Tibetan\nChinese\nAltaic\nTurkishKoreanJapanese\nIndo-European\nIndo-Iranian\nHindi\nGermanic\nGerman\nRomance\nSpanishItalianFrench\n16\nFigure 1: Language families in the Toefl11 corpus. The languages were selected to represent different families, but to also have several from within the same families. Diagram reproduced from Blanchard et al. (2013).\nThis dataset was designed specifically for NLI and the authors attempted to balance the texts by topic and native language. There are a total of eight essay prompts in the corpus, with the prompts setting each essay\u2019s topic or theme. Although they were not able to create a perfectly balanced corpus, the distribution of topics across L1s is very even. This distribution of essay prompts by L1 is shown in Figure 2."}, {"heading": "3.2 The ASK Corpus", "text": "In this study we use data from the ASK Corpus (Andrespr\u030aakskorpus, Second Language Corpus). The ASK Corpus (Tenfjord et al., 2013, 2006b,a) is a learner corpus composed of the writings of learners of Norwegian. These texts are essays written as part of a test of Norwegian as a second language. Each text also includes additional metadata about the author such as age or native language. An advantage of this corpus is that all the texts have been collected under the same conditions and time limits. The corpus also contains a control subcorpus of texts written by native Norwegians under the same test conditions. The corpus also includes error codes and corrections, although we do not make use of this information here.\nThere are a total of 1,700 essays written by learners of Norwegian as a second language with ten different first languages: German, Dutch, English, Spanish, Russian, Polish, Bosnian-Croatian-Serbian, Albanian, Vietnamese and Somali. The essays are written on a number of different topics, but these topics are not balanced across the L1s.\nDetailed word level annotations (lemma, POS tag and grammatical function) have been first obtained automatically using the Oslo-Bergen tagger. These annotations have then been manually post-edited by human annotators since the tagger\u2019s performance can be substantially degraded due to orthographic, syntactic and morphological learner errors. These manual corrections can deal with issues such as unknown vocabulary or wrongly disambiguated words.\nUnlike for TOEFL11, we generate artificial essays here, to mitigate the effects of imbalance in topics and proficiency. Manufacturing documents in this manner has a number of positive impacts. Firstly, it ensures that all documents are similar and comparable in length. If the data are being used to classify documents from another source, instead of cross-validation, the generation parameters could be changed so that the training set is similar to the test set in terms of length. Secondly, the random sampling used here means that the texts created for each class are a mix of different authorship styles, proficiencies and topics.\nIn this work we extracted 750k tokens of text from the ASK corpus in the form of individual sentences. Following a similar methodology to that of (Brooke and Hirst, 2011), we randomly select and combine the sentences from the same L1 to generate texts of approximately 300 tokens on average, creating a set of documents suitable for NLI.\nMore specifically, the dataset composed of artificial documents is generated as follows. For each class, all the available texts are processed and the individual sentences from these texts are placed into a single pool. Once this pool has been created, we begin the process of generating artificial documents.\nFor each artificial text to be generated, its required minimum length is first determined by randomly picking a value within a pre-specified range [M,N ]. This chosen value represents the minimum number of tokens or characters that are required to create a new document. By specifying this range parameter, instead of a single fixed value, we can create an artificial dataset where there is still some reasonable (and controlled) amount of variance in length between texts.\nFigure 2. Number of essays per language per prompt\n12\nSentences from the pool are then randomly allocated to the document until its length exceeds the required minimum value. The document is then considered complete; it is added to the new dataset and we proceed to generate another. It should also be noted that the document length may exceed the upper bound of the range parameter, depending on the length of the final sentence that crosses the minimum threshold. The sampling of sentences from the pool is done without replacement.\nThis process continues until there are insufficient sentences to create any more documents. The sentences remaining in the pool are then discarded. This procedure is performed for every class in the original dataset and yields a new dataset of artificial documents.\nThe 10 native languages and the number of texts generated per class are listed in Table 1. In addition to these we also generate 250 control texts written by natives. A histogram of the number of tokens per document is shown in Figure 3. The documents have an average length of 311 tokens with a standard deviation of 15 tokens."}, {"heading": "3.2.1 Part-of-Speech Tagset", "text": "The ASK corpus uses the Oslo-Bergen tagset6 which has been developed based on the Norwegian Reference Grammar (Faarlund et al., 1997).\nHere each POS tag is composed of a set of constituent morphosyntactic tags. For example, the tag subst-appell-mask-ub-fl signifies that the token has the categories \u201cnoun common masculine indefinite plural\u201d. Similarly, the tags verb-imp and verb-pres refer to imperative and present tense verbs, respectively.\nGiven its many morphosyntactic markers and detailed categories, the ASK dataset has a rich tagset with over 300 unique tags."}, {"heading": "3.3 The Jinan Chinese Learner Corpus", "text": "Growing interest has led to the recent development of the Jinan Chinese Learner Corpus (Wang et al., 2015), the first large-scale corpus of L2 Chinese consisting of university student essays. Learners from 59 countries are represented and proficiency levels are sampled representatively across beginner, intermediate and advanced levels. However, texts by learners from other Asian countries are disproportionately represented, with this likely being due to geographical proximity and links to China.\nFor this work we extracted 3.75 million tokens of text from the JCLC in the form of individual sentences.7 Following the methodology described in the previous section, we combine the sentences from the same L1 to generate texts of 600 tokens on average,8 creating a set of documents suitable for NLI.\nAlthough there are over 50 L1s available in the corpus, we choose the top 11 languages, shown in Table 2, to use in our experiments. This is due to two considerations. First, while many L1s are represented in the corpus, most have relatively few texts. Choosing the top 11 classes allows us to have a large number of classes and also ensure that there is sufficient data per-class. Secondly, this is the same number of classes used in the NLI 2013 shared task, enabling us to draw cross-language comparisons with the shared task results."}, {"heading": "4 Experimental Setup", "text": "In this study we employ a supervised multi-class classification approach. The learner texts are organized into classes according to the author\u2019s L1 and these documents are used for training and testing in our experiments. In this section we describe our experimental methodology, including evaluation and the algorithms we use. Our classification features will be described in \u00a75 and the three classification models we create using these algorithms and features are then described in \u00a76.\n6http://tekstlab.uio.no/obt-ny/english/tagset.html 7Full texts are not made available, only individual sentences with the relevant metadata (proficiency/nationality). 8A single Chinese character is considered a token."}, {"heading": "4.1 Evaluation", "text": "In the same manner as many previous NLI studies and also the NLI 2013 shared task, we report our results as classification accuracy under k-fold cross-validation, with k = 10. In recent years this has become a de facto standard for reporting NLI results. For creating our folds, we employ stratified crossvalidation which aims to ensure that the proportion of classes within each partition is equal (Kohavi, 1995). For Toefl11 we also test on the standard test set, which we call Toefl11-Test.\nWe use a random baseline and a majority class baseline for comparison purposes, using these to determine the lower bounds for accuracy. Oracles, which we describe in \u00a76.1.8, will also be used to estimate a potential upper bound for accuracy. We use them to assess how close our models are to achieving optimal performance. Additionally, we also compare our results against those reported in previous work. These were described earlier in \u00a72"}, {"heading": "4.2 Classification Algorithms", "text": "In this section we briefly describe the learning algorithms used in our experiments. All of these learners will be evaluated as meta-classifiers, but only SVMs will be used as base learners, for reasons we outline below (\u00a74.2.1). Although a thorough exposition of the methods is beyond the scope of this work, key references are provided for the interested reader."}, {"heading": "4.2.1 Linear Support Vector Machine", "text": "Linear Support Vector Machine (SVM) classifiers are a highly robust supervised classification method that has proven to be very effective for text classification (Joachims, 1998). Strong theoretical and empirical arguments have been made for the utility of SVMs for text classification, showing that their capabilities are well suited for several properties of text data, including extremely large feature spaces, very high sparsity and few irrelevant features.\nWith regards to NLI, post-hoc analysis of the shared task results revealed that many of the top systems, including the winning entry, followed two broad patterns: they used SVM classifiers along with frequency-based feature values. Given the better performance of SVM-based NLI systems, we used this learner to generate all of the base classifiers in this study.\nSVMs are inherently binary classifiers and a common way to adapt them for multi-class problems is through a one-vs-all (OVA) approach, also known as a one-vs-rest (OVR) approach. Another common alternative is a one-vs-one (OVO) method that builds N(N\u22121)2 binary classifiers for all pairwise combinations. It has been found that the OVR approach works best in NLI (Brooke and Hirst, 2012b), and our experiments confirmed this and we therefore adopt this approach. Additionally, an SVM is a margin-based classifier and does not output probability estimates for each class label, although there are additional methods to map the outputs to probabilities (Platt, 2000)."}, {"heading": "4.2.2 RBF-Kernel Support Vector Machine", "text": "SVMs with a Radial Basis Function (RBF) kernel are also popular for data points that are not linearly separable. This is because the kernel maps the data points in a non-linear manner, allowing for more flexible decision boundaries (Hsu et al., 2003). It should also be noted that this flexibility increases the risk of overfitting.\nFurthermore, this type of kernel may not work well for large feature spaces.9 Although they do not perform as well as linear SVMs for text classification, they can achieve very competitive results on problems with fewer features."}, {"heading": "4.2.3 Logistic Regression", "text": "Logistic regression is a type of linear regression model where the dependent variables are categorical. Supported by strong theoretical underpinnings as well as practical outcomes, maximum likelihood logistic regression has become a widely used machine learning algorithm. Although high-dimensional input poses a challenge for these models (Genkin et al., 2007), this issue can be addressed to some degree using regularization methods (Zhu and Hastie, 2004). This algorithm is inherently multi-class, meaning that OVA and OVO approaches are not required. The logistic regression classifier is also probabilistic and provides continuous probability estimates for each class label.\n9Appendix C of Hsu et al. (2003) examines this issue in greater detail."}, {"heading": "4.2.4 Perceptron", "text": "The Perceptron (Rosenblatt, 1958) is another linear learning algorithm that has been successful The algorithm learns a weight vector and a bias term which shifts the decision boundary from the origin. However, the algorithm will not converge if the data is not linearly separable. It supports online learning and each training instance is processed and weights updated according to a defined learning rate. Perceptrons have been successfully used for POS tagging (Collins, 2002) and parsing (Collins and Roark, 2004)."}, {"heading": "4.2.5 Ridge Regression", "text": "Classification using ridge regression is an approach based on a regression model that uses a linear least squares loss function (Zhang and Oles, 2001). The OVA approach is used for multiclass classification. Given that it is a linear model, it can work well for high-dimensional problems as they are often linearly separable."}, {"heading": "4.2.6 Decision Trees", "text": "One of the oldest and most commonly used supervised learning method, decision trees are a nonparametric method that attempt to learn a set of hierarchical decision rules based on the input features (Quinlan, 1993). They are inherently multiclass learners and require little data preprocessing. However, the trees can be unstable and may not generalize well beyond the training data."}, {"heading": "4.2.7 Linear Discriminant Analysis", "text": "A classic learning algorithm, Linear Discriminant Analysis (LDA, not to be confused with Latent Dirichlet Allocation) is a method based on a linear decision boundary (Fisher, 1936). It has been widely and successfully used for classification (Liu and Wechsler, 2002). LDA, a generative classification method, fits a conditional probability density function to each class and works under the assumption of homoscedasticity, i.e. all classes have the same covariance. It is non-parametric and inherently multiclass."}, {"heading": "4.2.8 Quadratic Discriminant Analysis", "text": "Quadratic Discriminant Analysis (QDA) is similar to LDA, except that it uses a quadratic decision surface (Hastie et al., 2009). Unlike LDA, however, it makes no assumption about equal class covariances, allowing them to be class-specific.\n4.2.9 k-nearest Neighbors\nA popular neighbor-based algorithm, k-nearest Neighbors (k-NN) is an instance-based classifier that does not build a statistical model (Cover and Hart, 1967). Training data are stored and test instances are labelled through a majority vote of the labels of the k nearest instances. The k parameter must be defined. This value is usually data-dependent and chosen experimentally."}, {"heading": "4.2.10 Nearest Centroid", "text": "The Nearest Centroid (NC) classification algorithm computes the centroid (i.e. mean) vector for each class (Tibshirani et al., 2002). Test instances are assigned to the class with the closest centroid. It is non-parametric but can perform poorly when classes have different variances for each feature."}, {"heading": "4.3 Classifier Output Representation", "text": "As we will describe in \u00a76, our meta-classifiers are trained on the outputs generated by individual classifiers. This output generally falls into two categories: discrete labels and continuous values. In this section we briefly describe these and how they are used for further classification."}, {"heading": "4.3.1 Discrete Label Values", "text": "The most elementary approach is to use the discrete class labels produced by the classifiers. In the case of multi-class classification this output is a single discrete value representing the hypothesis formed by the classifier and is available from virtually all learning algorithms. The number of possible values is the same as the number of possible classes in the dataset, K.\nTo use this categorical value as a classification feature, the outputs from any classifier must be represented as a feature vector. A common approach here to represent the values using one-hot encoding. This encoding creates a 1-of-K vector: this is a vector with K elements where one element will always be set to 1 while the rest are 0. This approach enables categorical data to be represented as continuous input, which is the input format expected by most learning algorithms."}, {"heading": "4.3.2 Continuous Output", "text": "Many classification algorithms can also produce continuous output associated with each class. This output can represent the confidence for each class label. Probabilistic classifiers, such as Logistic Regression, can provide confidence estimates for each of the possible K class labels. Margin-based classifiers, like Support Vector Machines, can provide the signed distance to the separating hyperplane.10\nWhere available, this output information can also be used to form a vector for classification. For each input, this would result in a K-element vector where each element is the continuous output associated with a class label. For confidence estimates all elements in the vector would sum to 1.\nThe confidence levels for each label can provide useful information; by considering the values for the other labels we may be able to make better predictions. They can also help prevent voting tie issues that can occur when only using the discrete class labels."}, {"heading": "5 Features", "text": "This study utilizes a standard set of NLI features widely used in previous work. This study focuses on comparing classification methodology for NLI and we do not confound this objective by introducing new features. Different feature types are extracted from each of our three datasets, as shown in Table 3.\nThe feature types for each dataset were chosen based on properties of the dataset and the availability of NLP resources for the L2.\nFor stylistic classification tasks like NLI, these content-based features can only be used if the training data is balanced for topic. Otherwise, topic balance will greatly impact the results and artificially inflate accuracy (Malmasi and Dras, 2015b). Accordingly, we only use these features for our experiments with Toefl11, which is balanced for topic. The NLP tools used to extract adaptor grammar and TSG fragment features are only available for English and thus limited to Toefl11. The paucity of NLP tools for Norwegian and the lack of topic balance in the data also limited our features to those manually annotated in the dataset.\nThis imbalance of feature types across the corpora is not an issue as we are comparing the performance of our models within each dataset and not between the L2s.\nThe remainder of this section describes the feature types listed in Table 3.\n10There exist additional methods to map these distances to probabilities (Platt, 2000)."}, {"heading": "5.1 Word, Lemma and Character n-grams", "text": "We extract commonly used surface features from the texts. These include word unigrams and bigrams, lemma unigrams and bigrams and character uni/bi/trigrams."}, {"heading": "5.2 Function Words", "text": "In contrast to content words, function words do not have any thematic meaning themselves, but rather can be seen as indicating the grammatical relations between other words. In a sense, they are the syntactic glue that hold much of the content words together and their role in assigning syntax to sentences is linguistically well-defined. They generally belong to a language\u2019s set of closed-class words and embody relations more than propositional content. Examples include articles, determiners, conjunctions and auxiliary verbs.\nFunction words are considered to be highly context- and topic-independent but other open-class words can also exhibit such such properties. In practical applications, such as Information Retrieval, such words are often removed as they are not informative and stoplists for different languages have been developed for this purpose. These lists contain \u2018stop words\u2019 and formulaic discourse expressions such as above-mentioned or on the other hand.\nFunction words\u2019 topic independence has led them to be widely used in studies of authorship attribution (Mosteller and Wallace, 1964) as well as NLI11 and they have been established to be informative for these tasks. Much like Information Retrieval, the function word lists used in these tasks are also often augmented with stoplists and this is also the approach that we take.\nSuch lists generally contain anywhere from 50 to several hundred words, depending on the granularity of the list and also the language in question. In this work, the English word list was obtained from the Onix Text Retrieval Toolkit.12 For Norwegian we used a list of 176 function words obtained from the distribution of the Apache Lucene search engine software.13 This list includes stop words for the Bokm\u030aal variant of the language and contains entries such as hvis (whose), ikke (not), jeg (I), s\u030aa (so) and hj\u030aa (at). We also make this list available on our website.14 For Chinese, we utilize the function word list described in Malmasi and Dras (2014b).\nIn addition to single function words, we also extract function word bigrams, as described by Malmasi et al. (2013). Function word bigrams are a type of word n-gram where content words are skipped: they are thus a specific subtype of the skip-grams discussed by Guthrie et al. (2006). For example, the sentence \u201cWe should all start taking the bus\u201d would be reduced to \u201cwe should all the\u201d, from which we would extract the n-grams."}, {"heading": "5.3 Part-of-Speech n-grams", "text": "Parts of Speech (POS) are linguistic categories (or word classes) assigned to words that signify their syntactic role. Basic categories include verbs, nouns and adjectives but these can be expanded to include additional morpho-syntactic information. The assignment of such categories to words in a text adds a level of linguistic abstraction.\nWe extract POS n-grams of order 1\u20133, which have been shown to be useful for NLI (Malmasi et al., 2013). These n-grams capture small and very local syntactic patterns of language production and were used as classification features. Previous work and our experiments showed that sequences of size 4 or greater achieve lower accuracy, possibly due to data sparsity, so we do not include them.\nFor English and Chinese, the Stanford CoreNLP15 suite of NLP tools (Manning et al., 2014) and the provided models were used to tokenize, POS tag and parse the unsegmented corpus texts. We did not use any NLP tools for Norwegian as the corpus we use is already annotated with POS tags.\nAdditionally, we extract a second set of POS n-grams for the Toefl11 data using the CLAWS dataset, which has been shown to perform well for NLI (Malmasi et al., 2013).\n11For example, the largest list used by Wong and Dras (2009) was a stopword list from Information Retrieval; given the size of their list, this was presumably also the case for Koppel et al. (2005), although the source there was not given.\n12http://www.lextek.com/manuals/onix/stopwords1.html 13https://github.com/apache/lucene-solr 14http://web.science.mq.edu.au/~smalmasi/data/norwegian-funcwords.txt 15http://nlp.stanford.edu/software/corenlp.shtml"}, {"heading": "5.4 Adaptor grammar collocations", "text": "For the Toefl11 data, we utilize an adaptor grammar to discover arbitrary length n-gram collocations. We explore both the pure part-of-speech (POS) n-grams as well as the more promising mixtures of POS and function words. We derive two adaptor grammars where each is associated with a different set of vocabulary: either pure POS or the mixture of POS and function words. We use the grammar proposed by (Johnson, 2010) for capturing topical collocations:\nSentence\u2192 Docj j \u2208 1, . . . ,m Docj \u2192 j j \u2208 1, . . . ,m Docj \u2192 Docj Topici i \u2208 1, . . . , t; j \u2208 1, . . . ,m Topici \u2192Words i \u2208 1, . . . , t Words\u2192Word Words\u2192Words Word Word\u2192 w w \u2208 Vpos;\nw \u2208 Vpos+fw\nVpos contains 119 distinct POS tags based on the Brown tagset and Vpos+fw is extended with 398 function words. The number of topics t is set to 50. The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carlo technique made available by Johnson (2010).16"}, {"heading": "5.5 Stanford dependencies", "text": "For English and Chinese we use Stanford dependencies as a syntactic feature: for each text we extract all the basic dependencies returned by the Stanford Parser (de Marneffe et al., 2006). We then generate all the variations for each of the dependencies (grammatical relations) by substituting each lemma with its corresponding POS tag. For instance, a grammatical relation of det(knowledge, the) yields the following variations: det(NN, the), det(knowledge, DT), and det(NN, DT)."}, {"heading": "5.6 CFG Rules", "text": "Also known as Phrase Structure Rules or Production Rules, these are the rules used to generate constituent parts of sentences, such as noun phrases. One way to obtain these is by first generating constituent parses for all sentences. The production rules, excluding lexicalizations, are then extracted. Figure 4 illustrates this with an example tree and its rules.\n16http://web.science.mq.edu.au/~mjohnson/Software.htm\n\u03b1 \u03b1\n\u03b1\nThese context-free phrase structure rules capture the overall structure of grammatical constructions and global syntactic patterns. They can also encode highly idiosyncratic constructions that are particular to some L1 group. They have been found to be useful for NLI (Wong and Dras, 2011). We use the Stanford parser to extract these features for both English and Chinese."}, {"heading": "5.7 Tree Substitution Grammar Fragments", "text": "Tree Substitution Grammar (TSG) fragments have been proposed by Swanson and Charniak (2012) as yet another type of syntactic feature for NLI or other syntactically motivated text classification tasks. They demonstrated that this feature type can achieve high classification accuracy.\nTSGs are a generalization of context-free grammars that allow non-terminals to rewrite as fragments which can have an arbitrary size (Post and Gildea, 2013), instead of being limited to a depth of one. A TSG fragment or elementary tree refers to these rules. Figure 5 shows several example fragments from a Tree Substitution Grammar capable of deriving the sentences \u201cGeorge hates broccoli\u201d and \u201cGeorge hates shoes\u201d. We only extract TSG fragments for the Toefl11 data as they include lexical terminal nodes."}, {"heading": "6 Classification Models", "text": "We conduct a set of three experiments, each based on different ensemble structures which we describe in this section. The first model is based on a traditional parallel ensemble structure while the second model examines meta-classification using classifier stacking. The third and final model is a hybrid approach, building an ensemble of meta-classifiers."}, {"heading": "6.1 Ensemble Classifiers", "text": "The most common ensemble structure, as described earlier in \u00a72.1, relies on a set of base classifiers whose decisions are combined using some predefined method. This is the approach for our first model.\nSuch systems often use a parallel architecture, as illustrated in Figure 6, where the classifiers are run independently and their outputs are aggregated using a fusion method. The first part of creating an ensemble is generating the individual classifiers. Various methods for creating these ensemble elements have been proposed. These involve using different algorithms, parameters or feature types; applying different preprocessing or feature scaling methods; and varying (e.g. distorting or resampling) the training data.\nFor example, Bagging (bootstrap aggregating) is a commonly used method for ensemble generation (Breiman, 1996) that can create multiple base classifiers. It works by creating multiple bootstrap training sets from the original training data and a separate classifier is trained from each set. The generated classifiers are said to be diverse because each training set is created by sampling with replacement and contains a random subset of the original data. Boosting (e.g. with the AdaBoost algorithm) is another method where the base models are created with different weight distributions over the training data with the aim of assigning higher weights to training instances that are misclassified (Freund and Schapire, 1996).\nIn our first model we follow the same approach as previous NLI research and train each classifier on a different feature type.17 However, our other models make use of the boosting and bagging techniques, which will be discussed in later sections. For reasons given in \u00a74.2.1, we use linear SVMs for these base classifiers.\nThe second part of ensemble design is choosing a combination or fusion rule to aggregate the outputs from the various learners, this is discussed in the next section. Most research to date has not compared different types of such combiners and we aim to evaluate a number of different strategies."}, {"heading": "6.1.1 Ensemble Fusion Methods", "text": "Once it has been decided how the set of base classifiers will be generated, selecting the classifier combination method is the next fundamental design question in ensemble construction.\nThe answer to this question depends on what output is available from the individual classifiers. The two different output types were discussed earlier in \u00a74.3. Some combination methods are designed to work with class labels, assuming that each learner outputs a single class label prediction for each data point. Other methods are designed to work with class-based continuous output, requiring that for each instance every classifier provides a measure of confidence18 for each class label. These outputs may correspond to probabilities for each class and consequently sum to 1 over all the classes. If an algorithm can provide both types of output, then all the methods can be tested. This is the case for the classifiers we will work with, as they are all SVMs.\nThese methods are usually based on some predefined rule or logic and cannot be trained. This can be considered an advantage, allowing them to be implemented and used without additional training of domain-specific combination models. On the other hand, they may not be able to exploit domain-specific trends and patterns in the input data.\n17This can also be achieved through training each classifier on a subspace of the entire feature set that includes all types. 18e.g. an estimate of the posterior probability for the class label.\nAlthough a number of different fusion methods have been proposed and tested, there is no single dominant method (Polikar, 2006). The performance of these methods is influenced by the nature of the problem and available training data, the size of the ensemble, the base classifiers used and the diversity between their outputs. This is an important motivation for comparatively assessing these methods on NLI data.\nThe selection of this method is often done empirically. Many researchers have compared and contrasted the performance of combiners on different problems, and most of these studies \u2013 both empirical and theoretical \u2013 do not reach a definitive conclusion (Kuncheva, 2014, p 178).\nIn the same spirit, we experiment with several classifier fusion methods which have been widely applied and discussed in the machine learning literature. Our selected methods are described below; a variety of other methods exist and the interested reader can refer to the thorough exposition by (Polikar, 2006)."}, {"heading": "6.1.2 Plurality voting", "text": "Each classifier votes for a single class label. The votes are tallied and the label with the highest number of votes wins.19 Ties are broken arbitrarily. This method is simple and does not have any parameters to tune. An extensive analysis of the method and its theoretical underpinnings can be found in Kuncheva (2004, p. 112)."}, {"heading": "6.1.3 Mean Probability Rule", "text": "The probability estimates for each class, provided by each individual classifier, are summed and the class label with the highest average probability is the winner. This is illustrated in Figure 7. This is equivalent to the probability sum combiner which does not require calculating the average for each class. An important aspect of using probability outputs in this way is that a classifier\u2019s support for the true class label is taken into account, even when it is not the predicted label (e.g. it could have the second highest probability). This method has been shown to work well on a wide range of problems and, in general, it is considered to be simple, intuitive, stable (Kuncheva, 2014, p. 155) and resilient to estimation errors (Kittler et al., 1998), making it one of the more robust combiners discussed in the literature."}, {"heading": "6.1.4 Median Probability Rule", "text": "Given that the mean probability used in the above rule is sensitive to outliers, an alternative is to use the median as a more robust estimate of the mean (Kittler et al., 1998). Under this rule, each class label\u2019s estimates are sorted and the median value is selected as the final score for that label. The label with the highest median value is picked as the winner. As with the mean combiner, this method measures the central tendency of support for each label as a means of reaching a consensus decision."}, {"heading": "6.1.5 Product Rule", "text": "For each class label, all of the probability estimates are multiplied together to create the label\u2019s final estimate (Polikar, 2006, p. 37). The label with the highest estimate is selected. This rule can theoretically provide the best overall estimate of posterior probability for a label, assuming that the individual estimates are accurate. A trade-off here is that this method is very sensitive to low probabilities: a single low score for a label from any classifier will essentially eliminate that class label."}, {"heading": "6.1.6 Highest Confidence", "text": "In this simple method, the class label that receives the vote with the largest degree of confidence is selected as the final prediction (Kuncheva, 2014, p. 150). In contrast to the previous methods, this combiner disregards the consensus opinion and instead picks the prediction of the expert with the highest degree of confidence.\n19This differs with a majority voting combiner where a label must obtain over 50% of the votes to win. However, the names are sometimes used interchangeably.\nNONTRAINABLE (FIXED) COMBINATION RULES 151\n\u25fb\u25fc Example 5.3 Simple nontrainable combiners The following example helps to clarify simple combiners. Let c = 3 and L = 5. Assume that for a certain x\nDP(x) = \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 0.1 0.5 0.4 0.0 0.0 1.0 0.4 0.3 0.4 0.2 0.7 0.1 0.1 0.8 0.2 \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 . (5.20)\nApplying the simple combiners column wise, we obtain:\nCombiner \ud835\udf071(x) \ud835\udf072(x) \ud835\udf073(x)\nAverage 0.16 0.46 0.42 Minimum 0.00 0.00 0.10 Maximum 0.40 0.80 1.00 Median 0.10 0.50 0.40 40% trimmed mean 0.13 0.50 0.33 Product 0.00 0.00 0.0032"}, {"heading": "6.1.7 Borda Count", "text": "This method works by using each classifier\u2019s confidence estimates to create a ranked list of the class labels in order of preference, with the predicted label at rank 1. The winning label is then selected using the Borda count20 algorithm (Ho et al., 1994). The algorithm works by assigning points to labels based on their ranks. If there are N different labels, then each classifier\u2019s preferences are assigned points as follows: the top-ranked label receives N points, the second place label receives N \u2212 1 points, third place receives N \u2212 2 points and so on with the last preference receiving a single point. These points are then tallied to select the winner with the highest score.\nThe most obvious advantage of this method is that it takes into account all of each classifier\u2019s preferences, making it possible for a label to win even if another label received the majority of the first preference votes."}, {"heading": "6.1.8 Oracle Combiners", "text": "Our final set of combiners are designed to assist with assessing the potential performance upper bound that could be achieved by a system, given a set of classifi rs. As such, they are primarily used for evaluation in the same manner that baselines help determine the lower bounds for performance. These combiners cannot be used to make predictions on unlabelled data.\nOne possible approach to estimating an upper-bound for classification accuracy, and one that we employ here, is the use of an \u201cOracle\u201d combiner. This method has previously been used to analyze the limits of majority vote classifier combination (Kuncheva et al., 2001). An oracle is a type of multiple classifier fusion method that can be used to combine the results of an ensemble of classifiers which are all used to classify a dataset.\nThe oracle will assign the correct class label for an instance if at least one of the constituent classifiers in the system produces the correct label for that data point. Some example oracle results for an ensemble of three classifiers are shown in Table 4. The probability of correct classification of a data point by the oracle is:\nPOracle = 1\u2212 P (All Classifiers Incorrect) 20This method is generally attributed to Jean-Charles de Borda (1733\u20131799), but evidence suggests that it was also\nproposed by Ramon Llull (1232\u20131315).\nOracles are usually used in comparative experiments and to gauge the performance and diversity of the classifiers chosen for an ensemble (Kuncheva, 2002; Kuncheva et al., 2003). They can help us quantify the potential upper limit of an ensemble\u2019s performance on the given data and how this performance varies with different ensemble configurations and combinations.\nTo account for the possibility that a classifier may predict the correct label essentially by chance (with a probability determined by the random baseline) and thus exaggerate the oracle score, we also use an Accuracy@N combiner. This method is inspired by the \u201cPrecision at k\u201d metric from Information Retrieval (Manning et al., 2008) which measures precision at fixed low levels of results (e.g. the top 10 results). Here, it is an extension of the Plurality vote combiner where instead of selecting the label with the highest votes, the labels are ranked by their vote counts and an instance is correctly classified if the true label is in the top N ranked candidates.21 Another way to view it is as a more restricted version of the Oracle combiner that is limited to the top N ranked candidates in order to minimize the influence of a single classifier having chosen the correct label by chance. In this study we experiment with N = 2 and 3. We also note that setting N = 1 is equivalent to the Plurality voting method and setting N to the number of class labels is equivalent to the Oracle combiner."}, {"heading": "6.2 Meta-Classifiers (Stacked Generalization)", "text": "While the combination methods in our first model are not trainable, other more sophisticated ensemble methods that rely on meta-learning employ a stacked architecture where the output from a first layer of classifiers is fed into a second level meta-classifier and so on. For our second model we expand our methodology to such a meta-classifier, also referred to as stacked generalization or classifier stacking (Wolpert, 1992). This methodology has not been tested for NLI thus far.\nA meta-classifier architecture is composed of an ensemble of base classifiers, just as in our first model. A key difference is that instead of employing a rule-based fusion method, the individual classifier outputs, along with the training labels, are used to train a second-level meta-classifier. This second meta-learner serves to predict the final decision for an input, given the decisions of the base classifiers. This setup is illustrated in Figure 8.\nThis meta-classifier attempts to learn from the collective knowledge represented by the ensemble of local classifiers and may be able to learn and exploit patterns and regularities in their output (Polikar, 2006, \u00a73.6). For example, it may be the case that a certain ensemble element is more accurate at classifying instances of a certain class or there may be interactions or correlations between the outputs of certain elements that could help improve results over a simple fusion method. So the meta-classifier may learn how the base classifiers commit errors and attempt to correct their biases.\nJust as there are different fusion methods for ensemble combination, different learning algorithms can be used for the meta-classifier element. In this study we experiment with all of the learning algorithms listed earlier in \u00a74.2. Additionally, we also test each learner using both the continuous and discrete output data in order to comparatively assess their performance. This approach allows us to evaluate whether one method performs better, and if certain algorithms are better suited for some specific input formats.\nSimilar to the base learners, the meta-classifier can generate both continuous and discrete output. In this model we take the discrete label output and use it as the final decision to be used in evaluating the model.\nFor training, the input for the meta-classifier can be obtained from the outputs of the base classifiers under cross-validation. That is to say, the classifier outputs from each test fold are paired with the original gold-standard label and this is used to train the meta-classifier.\n21In case of ties, we choose randomly from the labels with the same number of votes."}, {"heading": "6.3 Meta-Classifier Ensembles", "text": "The two models described thus far have relied on multiple classifier combination and meta-learning. While they both have their advantages, would it be possible to combine both approaches?\nTo this end, our final model is a hybrid of the previous two approaches that attempts to answer this question. Results from the set of base classifiers are provided to an ensemble of meta-classifiers, instead of a single one. The outputs from the meta-classifiers are then combined using a fusion method to reach a final verdict. The layout for this model is illustrated in Figure 9. This approach, while adding substantial complexity to the model, could potentially combine the benefits of stacking and ensemble combination.\nAdditionally, this approach also requires a method for generating the meta-classifier ensemble itself. While the first level ensemble is generated using a different feature type per classifier, that method cannot be applied here since we are using classifier outputs. For that purpose we experiment with boosting and bagging (as described in \u00a76.1). These methods have been widely used for creating decision tree ensembles (Geurts et al., 2006); we will experiment with random forests, extra trees and the AdaBoost algorithm. We will also experiment with bagging, which can be applied to any learner that can be applied for meta-classification, such as SVMs."}, {"heading": "7 Experiments and Results", "text": "We divide our experiments into two parts: comprehensive experiments on the English Toefl11 data (\u00a77.1), followed by comparative experiments on Chinese and Norwegian data (\u00a77.2)."}, {"heading": "7.1 Experiments on TOEFL11", "text": "Given that it is the largest and most widely used corpus, we evaluate all of our models on Toefl11, before comparing their performances on the other datasets.\nResults are compared against a random baseline and the oracle combiners. We also compare these results against the winning system from the 2013 NLI shared task (Jarvis et al., 2013) and two systems by (Bykh and Meurers, 2014) and (Ionescu et al., 2014) which presented state-of-the-art results following the task. They were all previously described in \u00a72.\nAs described in \u00a76, we create our ensembles from a set of linear models where each is trained on a different feature type. We first test our individual classifiers which form this first layer of our three models; this can inform us about their individual performance and the single best feature type. This is done by training the models on the combined Toefl11-Train and Toefl11-Dev data, which we refer to as Toefl11-TrainDev, and testing against the Toefl11-Test set. The results are shown in Figure 10. We observe that there is a range of performance, and some features achieve similar accuracy. We also note that the best single-model performance is approximately 78%. Having shown that our base classifiers achieve good results on their own, we now apply our three ensemble models.\nWe begin by applying the six ensemble combination methods discussed in \u00a76.1 as part of our first model. We do this using both cross-validation within Toefl11-TrainDev and by training our models on Toefl11-TrainDev and testing on Toefl11-Test. The results for all fusion methods, are shown in Table 5.\nThe mean probability combiner which uses continuous values for each class (\u00a76.1.3) achieves the best performance for both of our test sets. This is followed by the plurality vote and median probability combiners, both of which have similar performance. Although plurality voting achieves good results, the Borda Count method performs worse.\nIn our cross-validation experiments, some 2.7% of the instances resulted in voting ties which were broken arbitrarily. This randomness leads to some variance in the results for voting-based fusion methods; running the combiner on the same input can produce slightly different results.\nResults from the highest confidence and product rule combiners are the poorest amongst the set, and by a substantial margin. We hypothesize that this is due to fact that they are both highly sensitive to outputs from all classifiers. A single outlier or poor prediction can adversely affect the results. They should generally be used in circumstances where the base classifiers are known to be extremely accurate, which is not the case here. Accordingly, we do not experiment with any further.\nClassification Accuracy By Feature\nThese results from this first model comport with previous research reporting that ensembles outperform single-vector approaches (see \u00a72); our best ensemble result is some 5% higher than our best feature.\nWe next apply our meta-classifier (\u00a76.2) to both the discrete and continuous outputs generated by the base classifiers. While the base classifiers remain the same, we train a meta-classifier using each of the machine learning algorithms we listed earlier in \u00a74.2. This results in 15 meta-classification models. Each model is tested using both discrete and continuous input, using both cross-validation and the Toefl11-Test set. The results for all of these experiments are shown in Table 6.\nBroadly, we observe two important trends here: the meta-classification results are substantially better than the ensemble combination methods from Table 5, and that meta-classifiers trained on continuous output perform better than their discrete label counterparts. This last pattern is not all that surprising since we already observed that the probability-based ensemble combiners outperformed the voting-based combiners. Using continuous values associated with each label provides the meta-learner with more information than a single label, likely helping it make better decisions.\nWhile most of our algorithms perform well, the LDA meta-classifier yields the best results across both input types and test conditions. These results, 85.2% under cross-validation and 86.8% on Toefl11Test are already higher than the current state of the art on this data and well exceed the baselines listed in Table 5. It is also important to note that the same classifier achieves the best performance across all four testing conditions.\nThe linear and RBF SVMs also achieve competitive results here. Instance-based k-NN and Nearest Centroid classifiers also do well. On the other hand, decision trees, QDA, and the Perceptron algorithm have the poorest performance across both discrete and continuous inputs.\nWe have thus far shown that ensembles outperform a single-vector approach, and that meta-classifiers achieve state-of-the-art results. Our final Toefl11 experiment involves applying our hybrid ensemble of meta-classifiers (\u00a76.3) to determine if we can further improve these results. Given the results from the previous model, we only test using continuous classifier outputs.\nWe experiment with two general methods for creating ensembles of meta-classifiers: boosting and bagging. Although a single decision tree was not a good meta-classifier, it has been shown that ensembles of trees can perform very well (Banfield et al., 2007). We experiment with random forests, extra trees and AdaBoost for creating such tree-based ensembles. We also apply bagging to several of the best meta-classifiers from Table 6: SVMs, Logistic Regression, Ridge Regression and LDA. To combine the ensemble of meta-classifiers we use the mean probability combiner, given its better performance among the combiners listed in Table 5.\nResults from these models are shown in Table 7. As expected, we observe that the tree-based methods receive a substantial performance increase compared to the single decision tree meta-classifier from the previous model. Random forests provide the biggest boost, improving performance by almost 10%. However, this is still lower than our LDA meta-classifier.\nApplying bagging to our discriminative meta-classifiers, we observe that we gain a small improvement over the previous model. The LDA-based method again outperforms the others, and while the improvement is not huge, it sets a new upper bound for Toefl11. In fact, this result is only 9% lower than the oracle accuracy of 96%.\nIn designing this setup we were initially concerned that the addition of further layers could lead to the addition of errors in the deeper classifiers, resulting in performance degradation. However, this was not the case and accuracy increased, if only slightly.\nA confusion matrix of our best system\u2019s predictions on the Toefl11-Test set is presented in Figure 11. The labels in the matrix have been ordered in a way similar to Figure 1 in order to group similar languages together. We achieve our best performance on German texts, with only 4 misclassifications. In the top left corner we also observe some confusion between the romance languages. We also observe the asymmetric confusion between Hindi and Telugu, as discussed in previous research (Malmasi et al., 2015b). Another interesting observation is that Arabic, which has poor precision, receives misclassifications from every other class, except Italian. This trend can be observed in the last column of the matrix.\nWe also assessed per-class performance using precision, recall and the F1-score, with results listed in Table 8. As shown in the confusion matrix, Hindi and Telugu have the worst performance. Recalculating the values without those two classes, the average F1-score improves to 0.89."}, {"heading": "7.2 Experiments On Other Languages", "text": "The second set of our experiments focus on investigating the generalizability of our findings so far. The result patterns observed on Toefl11 have been stable across the training and test set, but we now apply them to other datasets to assess their generalizability on different languages and data sources.\nThe experiments in this section are conducted on the Chinese and Norwegian datasets described in \u00a73. As these datasets do not have a predefined test set like Toefl11, these experiments were performed using stratified cross-validation, as discussed in \u00a74.1. Previous experiments on these corpora have also been conducted using cross-validation only.\nWe utilize the top performing models tested in \u00a77.1: four ensemble combiners, four bagging-based meta-classifiers and four ensembles of meta-classifiers. These selected models, and their results, are listed in Table 9.\nThe oracle values for both datasets are quite high at over 90%, similar to Toefl11 (which was listed in Table 5). The ensemble model does well, beating the previously reported best result for Chinese and coming close for Norwegian. Just like our previous experiments, the mean probability combiner yields the best performance.\nThe meta-classifier model achieves a new state of the art for both datasets, just as it did for Toefl11. Also consistent with the previous experiment, LDA achieves the best results for both datasets. Finally, the ensemble of meta-classifiers yields additional improvement over the single meta-classifier model, achieving our best results. We achieve 76.5% accuracy on the Chinese data and 81.8% on the Norwegian data, both substantial improvements over previous work. These results show that these classification models are applicable to other datasets. The results followed the same pattern across all three datasets, with LDA-based meta-classification yielding top results."}, {"heading": "7.3 Statistical Significance Testing", "text": "An important question that arises in various contexts within machine learning deals with determining which methods outperform others on a given problem (Dietterich, 1998). Such questions are often addressed using statistical significance testing, which can help base such research and claims about results in a rigorous empirical foundation.\nHowever, this trend has not been adopted within NLI. Most publications have reported crossvalidation accuracy, or more recently, accuracy on the Toefl11-Test set. Given the increasing accuracies reported by recent research, we believe that the use of statistical tests can be very beneficial to future NLI research.\nReliable statistical tests for comparing two classifiers require the availability of a common test set, a need which has been recently addressed by the Toefl11-Test set.22 Since we are then evaluating the classifier outputs for the same samples, a test for paired nominal data is suitable. McNemar\u2019s test (McNemar, 1947) is a non-parametric method to test for significant differences in proportions for paired nominal data. In the context of machine learning it is often used to compare the performance of distinct algorithms on the same data (Dietterich, 1998) as it does not assume independent samples and has a low Type I error rate (Fu\u0308rnkranz, 2002). It is the most commonly used for pairwise classifier comparison and has been used in a wide range of machine learning applications (West, 2000; Aue and Gamon, 2005).\nThis is also the test we propose for use within NLI. In this section we briefly describe the test and demonstrate its application for NLI. The interested reader can find more details about methods for evaluating the statistical significance of classifier differences in the work of Foody (2004).\nMcNemar\u2019s test is a non-parametric method based on creating a 2\u00d72 contingency table for the outcomes of a pair of tests (classifiers in our case), tabulating the number of instances where their predictions agree or disagree. The row and column marginals are calculated, and a test statistic is then used to determine if the marginal probabilities for each classifier are the same. An example of such a contingency table for two classifiers Ca and Cb is given below in Table 10.\nThe four table cells represent the number of concordant and discordant classifications and misclassifications between the two methods. The null hypothesis states that both classifiers have equal error rates (n01 = n10, i.e. the discordant predictions are the same)\n23 and the alternative hypothesis is that the error rates differ. The test statistic is based on a chi-square distribution, with additional continuity correction to account for the fact that a continuous distribution is being used to represent a discrete one (Dietterich, 1998; Foody, 2004). The test statistic is given in Eq. 1.\n\u03c7 = (|n01 \u2212 n10| \u2212 1)2\nn01 + n10 (1)\nHaving defined the test, the final requirement for its use is the availability of predictions from different systems. However, this is an important factor that has hindered the adoption of statistical tests in NLI. Despite having a predefined test set, an obstacle here is that although the Toefl11 corpus has been used in most NLI work since the 2013 shared task, most researchers do not make their predictions available. The availability of these predictions enables the application of statistical significance testing for classifier evaluation. The work of Malmasi et al. (2015b) was an initial step in this direction by making available all 144 submissions from the 2013 shared task, including that of the winning system.24 As part of this work, we also make available the predictions of our best system on the Toefl11-Test set. During the course of this research, Ionescu et al. (2014) also provided us with the predictions from their state-ofthe-art system, which we also make available.25 The availability of this data can become increasingly important as state-of-the-art results move closer towards the oracle upper bounds.\nWe now evaluate the performance of our top model against that of the two previous state-of-the-art systems which were used as baselines in our experiments, using the aforementioned prediction data. We report the pairwise p values for the test, as listed in Table 11. They show that the improvement in our results is significantly better than both of the baselines. In contrast, they also show that the results of Ionescu et al. (2014) were not significantly better than the previous best result. This analysis highlights the utility of using such statistical tests for NLI.\n22This remains an issue for other datasets where no test set exists and only cross-validation results are usually reported, as we noted earlier.\n23This is sometimes stated as n01 n01+n10\n= 0.5 24Available from http://web.science.mq.edu.au/~smalmasi/resources/nli2013 25These two sets of predictions are available at\nhttp://web.science.mq.edu.au/~smalmasi/resources/nli-predictions"}, {"heading": "8 Discussion", "text": "We presented the first comprehensive study of meta-classification techniques for NLI, achieving state-ofthe-art accuracy on three major datasets for the task. This is the most comprehensive and systematic application of multiple classifier systems for NLI, evaluating three types of increasingly sophisticated classification models.\nWe applied many different methods from the armamentarium of machine learning algorithms, and the observed consistency was an important facet of our results. The performance patterns of our models were similar across different languages and dataset, with the same model configurations achieving the best results across different test sets and corpora. This differs to the work of Ionescu et al. (2014), where their best results on different sets were achieved using different parameters, or that of Bykh and Meurers (2014), who did not test their method on different datasets.\nThe application of these methods is not limited to cross-validation studies and we have attempted to apply them elsewhere. During the course of developing these methods we evaluated them under test conditions by using them to compete in several shared tasks in different tasks. Although a detailed exposition exceeds the scope of the present work, we briefly mention our results. The ensemble classifier was used to participate in the 2015 Discriminating Similar Language shared task, and was the winning entry among the 10 participating teams. The ensemble was also used to train a system to participate in the Complex Word Identification task at SemEval 2016 (Track 11), with our systems ranking in second and third place. Finally, the meta-classifier ensemble approach described here was the basis of an entry in the 2016 Computational Linguistics and Clinical Psychology (CLPsych) shared task, where it also ranked in first place among 60 systems. We believe that these results, in conjunction with the state-ofthe-art NLI performance reported in the present paper, highlight the utility of the classification models we described here for various NLP tasks.\nWe also introduced the possibility of statistical significance testing within NLI, making available two new sets of predictions to facilitate this. Work in NLI has not yet begun to use such statistical significance testing for comparing results, although this is something that becomes increasingly desirable as the relative differences between proposed methods begin to narrow and results get closer to the oracle upper bound. Although the predictions needed for such analyses are currently only available for Toefl11, we hope that their use will be adopted for future NLI work using other datasets.\nFuture work can be directed towards answering some of the following questions. Why does LDA outperform other meta-classification methods? In-depth examination of the trained models \u2013 something beyond the scope of this work \u2013 may reveal interesting clues about what the model is learning. This knowledge could possibly help improve meta-classifier feature engineering.\nHow does the amount of training data affect meta-classifier performance? This analysis, along with further evaluation of the models\u2019 learning curves, could inform us about training data requirements as well as bias-variance and overfitting issues.\nCan meta-classifiers improve cross-corpus performance? As additional datasets suitable for NLI become available, this has enabled the application of cross-corpus evaluation to assess how well the methods generalize across data from different genres and sources (Malmasi and Dras, 2015a). The metaclassifier approach has yet to be tested in such a scenario and future experiments in this context could provide insightful results."}], "references": [{"title": "Customizing sentiment classifiers to new domains: A case study", "author": ["Anthony Aue", "Michael Gamon"], "venue": "In Proceedings of recent advances in natural language processing (RANLP),", "citeRegEx": "Aue and Gamon.,? \\Q2005\\E", "shortCiteRegEx": "Aue and Gamon.", "year": 2005}, {"title": "A comparison of decision tree ensemble creation techniques", "author": ["Robert E Banfield", "Lawrence O Hall", "Kevin W Bowyer", "W Philip Kegelmeyer"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Banfield et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banfield et al\\.", "year": 2007}, {"title": "TOEFL11: A Corpus of Non-Native English", "author": ["Daniel Blanchard", "Joel Tetreault", "Derrick Higgins", "Aoife Cahill", "Martin Chodorow"], "venue": "Technical report, Educational Testing Service,", "citeRegEx": "Blanchard et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Blanchard et al\\.", "year": 2013}, {"title": "Bagging predictors", "author": ["Leo Breiman"], "venue": "In Machine Learning,", "citeRegEx": "Breiman.,? \\Q1996\\E", "shortCiteRegEx": "Breiman.", "year": 1996}, {"title": "Native Language Detection with \u201ccheap\u201d learner corpora. In Twenty Years of Learner Corpus Research", "author": ["Julian Brooke", "Graeme Hirst"], "venue": "Looking Back, Moving Ahead: Proceedings of the First Learner Corpus Research Conference (LCR 2011),", "citeRegEx": "Brooke and Hirst.,? \\Q2011\\E", "shortCiteRegEx": "Brooke and Hirst.", "year": 2011}, {"title": "Measuring interlanguage: Native language identification with L1influence metrics", "author": ["Julian Brooke", "Graeme Hirst"], "venue": "In Proceedings of the Eight International Conference on Language Resources and Evaluation", "citeRegEx": "Brooke and Hirst.,? \\Q2012\\E", "shortCiteRegEx": "Brooke and Hirst.", "year": 2012}, {"title": "Robust, Lexicalized Native Language Identification", "author": ["Julian Brooke", "Graeme Hirst"], "venue": "In Proceedings of COLING", "citeRegEx": "Brooke and Hirst.,? \\Q2012\\E", "shortCiteRegEx": "Brooke and Hirst.", "year": 2012}, {"title": "Exploring Syntactic Features for Native Language Identification: A Variationist Perspective on Feature Encoding and Ensemble Optimization", "author": ["Serhiy Bykh", "Detmar Meurers"], "venue": "In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "Bykh and Meurers.,? \\Q2014\\E", "shortCiteRegEx": "Bykh and Meurers.", "year": 2014}, {"title": "Combining shallow and linguistically motivated features in native language identification", "author": ["Serhiy Bykh", "Sowmya Vajjala", "Julia Krivanek", "Detmar Meurers"], "venue": "In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Bykh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bykh et al\\.", "year": 2013}, {"title": "Linguistic profiling based on general\u2013purpose features and native language identification", "author": ["Andrea Cimino", "Felice Dell\u2019Orletta", "Giulia Venturi", "Simonetta Montemagni"], "venue": "In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Cimino et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cimino et al\\.", "year": 2013}, {"title": "Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), chapter Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms", "author": ["Michael Collins"], "venue": null, "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Michael Collins", "Brian Roark"], "venue": "In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Collins and Roark.,? \\Q2004\\E", "shortCiteRegEx": "Collins and Roark.", "year": 2004}, {"title": "Nearest neighbor pattern classification", "author": ["Thomas M Cover", "Peter E Hart"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cover and Hart.,? \\Q1967\\E", "shortCiteRegEx": "Cover and Hart.", "year": 1967}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine de Marneffe", "Bill Maccartney", "Christopher D. Manning"], "venue": "In Proceedings of the Fifth International Conference on Language Resources and Evaluation", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Approximate statistical tests for comparing supervised classification learning algorithms", "author": ["Thomas G Dietterich"], "venue": "Neural computation,", "citeRegEx": "Dietterich.,? \\Q1998\\E", "shortCiteRegEx": "Dietterich.", "year": 1998}, {"title": "The Power of Depth for Feedforward Neural Networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "JMLR: Workshop and Conference Proceedings,", "citeRegEx": "Eldan and Shamir.,? \\Q2016\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2016}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["Ronald A Fisher"], "venue": "Annals of Eugenics,", "citeRegEx": "Fisher.,? \\Q1936\\E", "shortCiteRegEx": "Fisher.", "year": 1936}, {"title": "Thematic map comparison", "author": ["Giles M Foody"], "venue": "Photogrammetric Engineering & Remote Sensing,", "citeRegEx": "Foody.,? \\Q2004\\E", "shortCiteRegEx": "Foody.", "year": 2004}, {"title": "Experiments with a new boosting algorithm", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "In ICML,", "citeRegEx": "Freund and Schapire.,? \\Q1996\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1996}, {"title": "Round Robin Classification", "author": ["Johannes F\u00fcrnkranz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "F\u00fcrnkranz.,? \\Q2002\\E", "shortCiteRegEx": "F\u00fcrnkranz.", "year": 2002}, {"title": "Large-scale Bayesian logistic regression for text", "author": ["Alexander Genkin", "David D Lewis", "David Madigan"], "venue": "categorization. Technometrics,", "citeRegEx": "Genkin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Genkin et al\\.", "year": 2007}, {"title": "Extremely randomized trees", "author": ["Pierre Geurts", "Damien Ernst", "Louis Wehenkel"], "venue": "Machine learning,", "citeRegEx": "Geurts et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Geurts et al\\.", "year": 2006}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg"], "venue": "The Computing Research Repository (CoRR),", "citeRegEx": "Goldberg.,? \\Q2015\\E", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Feature space selection and combination for native language identification", "author": ["Cyril Goutte", "Serge L\u00e9ger", "Marine Carpuat"], "venue": "In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Goutte et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goutte et al\\.", "year": 2013}, {"title": "International Corpus of Learner English (Version 2)", "author": ["Sylviane Granger", "Estelle Dagneaux", "Fanny Meunier", "Magali Paquot"], "venue": "Presses Universitaires de Louvain, Louvian-la-Neuve,", "citeRegEx": "Granger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Granger et al\\.", "year": 2009}, {"title": "A Closer Look at Skip-gram Modelling", "author": ["David Guthrie", "Ben Allison", "Wei Liu", "Louise Guthrie", "Yorick Wilks"], "venue": "In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Guthrie et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Guthrie et al\\.", "year": 2006}, {"title": "Native language identification: a simple ngram based approach", "author": ["Binod Gyawali", "Gabriela Ramirez", "Thamar Solorio"], "venue": "In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Gyawali et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gyawali et al\\.", "year": 2013}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome H. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Discriminating non-native english with 350 words", "author": ["John Henderson", "Guido Zarrella", "Craig Pfeifer", "John D. Burger"], "venue": "In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Henderson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "Feature engineering in the nli shared task 2013: Charles university submission report", "author": ["Barbora Hladka", "Martin Holub", "Vincent Kriz"], "venue": "In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Hladka et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hladka et al\\.", "year": 2013}, {"title": "Decision combination in multiple classifier systems", "author": ["Tin Kam Ho", "Jonathan J. Hull", "Sargur N. Srihari"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Ho et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Ho et al\\.", "year": 1994}, {"title": "A practical guide to Support Vector classification", "author": ["Chih-Wei Hsu", "Chih-Chung Chang", "Chih-Jen Lin"], "venue": null, "citeRegEx": "Hsu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2003}, {"title": "Can characters reveal your native language? A language-independent approach to native language identification", "author": ["Radu Tudor Ionescu", "Marius Popescu", "Aoife Cahill"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Ionescu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ionescu et al\\.", "year": 2014}, {"title": "String Kernels for Native Language Identification: Insights from Behind the Curtains", "author": ["Radu Tudor Ionescu", "Marius Popescu", "Aoife Cahill"], "venue": null, "citeRegEx": "Ionescu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ionescu et al\\.", "year": 2016}, {"title": "Approaching Language Transfer Through Text Classification: Explorations in the Detection-based Approach, volume 64", "author": ["Scott Jarvis", "Scott Crossley", "editors"], "venue": "Multilingual Matters Limited,", "citeRegEx": "Jarvis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jarvis et al\\.", "year": 2012}, {"title": "Maximizing classification accuracy in native language identification", "author": ["Scott Jarvis", "Yves Bestgen", "Steve Pepper"], "venue": "In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Jarvis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jarvis et al\\.", "year": 2013}, {"title": "ECML-98: Proceedings of the 10th European Conference on Machine Learning, chapter Text categorization with Support Vector Machines: Learning with many relevant features, pages 137\u2013142", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q1998\\E", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Pcfgs, topic models, adaptor grammars and learning topical collocations and the structure of proper names", "author": ["Mark Johnson"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Johnson.,? \\Q2010\\E", "shortCiteRegEx": "Johnson.", "year": 2010}, {"title": "On combining classifiers", "author": ["Josef Kittler", "Mohamad Hatef", "Robert PW Duin", "Jiri Matas"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kittler et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kittler et al\\.", "year": 1998}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["Ron Kohavi"], "venue": "In IJCAI,", "citeRegEx": "Kohavi.,? \\Q1995\\E", "shortCiteRegEx": "Kohavi.", "year": 1995}, {"title": "Automatically determining an anonymous author\u2019s native language", "author": ["Moshe Koppel", "Jonathan Schler", "Kfir Zigdon"], "venue": "In Intelligence and Security Informatics,", "citeRegEx": "Koppel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Koppel et al\\.", "year": 2005}, {"title": "A theoretical study on six classifier fusion strategies", "author": ["Ludmila I Kuncheva"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "citeRegEx": "Kuncheva.,? \\Q2002\\E", "shortCiteRegEx": "Kuncheva.", "year": 2002}, {"title": "Combining Pattern Classifiers: Methods and Algorithms", "author": ["Ludmila I Kuncheva"], "venue": null, "citeRegEx": "Kuncheva.,? \\Q2004\\E", "shortCiteRegEx": "Kuncheva.", "year": 2004}, {"title": "Combining Pattern Classifiers: Methods and Algorithms", "author": ["Ludmila I Kuncheva"], "venue": "Wiley, second edition,", "citeRegEx": "Kuncheva.,? \\Q2014\\E", "shortCiteRegEx": "Kuncheva.", "year": 2014}, {"title": "Rod\u0155\u0131guez. A weighted voting framework for classifiers ensembles", "author": ["Ludmila I Kuncheva", "Juan J"], "venue": "Knowledge and Information Systems,", "citeRegEx": "Kuncheva and J,? \\Q2014\\E", "shortCiteRegEx": "Kuncheva and J", "year": 2014}, {"title": "Decision templates for multiple classifier fusion: an experimental comparison", "author": ["Ludmila I Kuncheva", "James C Bezdek", "Robert PW Duin"], "venue": "Pattern Recognition,", "citeRegEx": "Kuncheva et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kuncheva et al\\.", "year": 2001}, {"title": "Limits on the majority vote accuracy in classifier fusion", "author": ["Ludmila I Kuncheva", "Christopher J Whitaker", "Catherine A Shipp", "Robert PW Duin"], "venue": "Pattern Analysis & Applications,", "citeRegEx": "Kuncheva et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kuncheva et al\\.", "year": 2003}, {"title": "Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition", "author": ["Chengjun Liu", "Harry Wechsler"], "venue": "IEEE Transactions on Image processing,", "citeRegEx": "Liu and Wechsler.,? \\Q2002\\E", "shortCiteRegEx": "Liu and Wechsler.", "year": 2002}, {"title": "Arabic Native Language Identification", "author": ["Shervin Malmasi", "Mark Dras"], "venue": "In Proceedings of the Arabic Natural Language Processing Workshop,", "citeRegEx": "Malmasi and Dras.,? \\Q2014\\E", "shortCiteRegEx": "Malmasi and Dras.", "year": 2014}, {"title": "Chinese Native Language Identification", "author": ["Shervin Malmasi", "Mark Dras"], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Malmasi and Dras.,? \\Q2014\\E", "shortCiteRegEx": "Malmasi and Dras.", "year": 2014}, {"title": "Finnish Native Language Identification", "author": ["Shervin Malmasi", "Mark Dras"], "venue": "In Australasian Language Technology Association Workshop", "citeRegEx": "Malmasi and Dras.,? \\Q2014\\E", "shortCiteRegEx": "Malmasi and Dras.", "year": 2014}, {"title": "Large-scale Native Language Identification with Cross-Corpus Evaluation", "author": ["Shervin Malmasi", "Mark Dras"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT", "citeRegEx": "Malmasi and Dras.,? \\Q2015\\E", "shortCiteRegEx": "Malmasi and Dras.", "year": 2015}, {"title": "Multilingual Native Language Identification", "author": ["Shervin Malmasi", "Mark Dras"], "venue": "In Natural Language Engineering, pages 1\u201353,", "citeRegEx": "Malmasi and Dras.,? \\Q2015\\E", "shortCiteRegEx": "Malmasi and Dras.", "year": 2015}, {"title": "NLI Shared Task 2013: MQ Submission", "author": ["Shervin Malmasi", "Sze-Meng Jojo Wong", "Mark Dras"], "venue": "In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Malmasi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Malmasi et al\\.", "year": 2013}, {"title": "Norwegian Native Language Identification", "author": ["Shervin Malmasi", "Mark Dras", "Irina Temnikova"], "venue": "In Proceedings of Recent Advances in Natural Language Processing (RANLP", "citeRegEx": "Malmasi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malmasi et al\\.", "year": 2015}, {"title": "Oracle and Human Baselines for Native Language Identification", "author": ["Shervin Malmasi", "Joel Tetreault", "Mark Dras"], "venue": "In Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Malmasi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malmasi et al\\.", "year": 2015}, {"title": "Discriminating between similar languages and arabic dialect identification: A report on the third dsl shared task", "author": ["Shervin Malmasi", "Marcos Zampieri", "Nikola Ljube\u0161i\u0107", "Preslav Nakov", "Ahmed Ali", "Liling Tan", "J\u00f6rg Tiedemann"], "venue": "In Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects (LT4VarDial),", "citeRegEx": "Malmasi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Malmasi et al\\.", "year": 2016}, {"title": "Computational Linguistics and Deep Learning", "author": ["Christopher D. Manning"], "venue": "Computational Linguistics,", "citeRegEx": "Manning.,? \\Q2015\\E", "shortCiteRegEx": "Manning.", "year": 2015}, {"title": "Evaluation in information retrieval. In Introduction to Information Retrieval, pages 151\u2013175", "author": ["Christopher D Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "The Stanford CoreNLP Natural Language Processing Toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Note on the sampling error of the difference between correlated proportions or percentages", "author": ["Quinn McNemar"], "venue": null, "citeRegEx": "McNemar.,? \\Q1947\\E", "shortCiteRegEx": "McNemar.", "year": 1947}, {"title": "Inference and Disputed Authorship: The Federalist", "author": ["Frederick Mosteller", "David L. Wallace"], "venue": null, "citeRegEx": "Mosteller and Wallace.,? \\Q1964\\E", "shortCiteRegEx": "Mosteller and Wallace.", "year": 1964}, {"title": "Understanding Second Language Acquisition", "author": ["Lourdes Ortega"], "venue": "Hodder Education, Oxford,", "citeRegEx": "Ortega.,? \\Q2009\\E", "shortCiteRegEx": "Ortega.", "year": 2009}, {"title": "Classifier ensembles: Select real-world applications", "author": ["Nikunj C Oza", "Kagan Tumer"], "venue": "Information Fusion,", "citeRegEx": "Oza and Tumer.,? \\Q2008\\E", "shortCiteRegEx": "Oza and Tumer.", "year": 2008}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["John Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "Platt.,? \\Q2000\\E", "shortCiteRegEx": "Platt.", "year": 2000}, {"title": "Ensemble based systems in decision making", "author": ["Robi Polikar"], "venue": "Circuits and Systems Magazine, IEEE,", "citeRegEx": "Polikar.,? \\Q2006\\E", "shortCiteRegEx": "Polikar.", "year": 2006}, {"title": "The story of the characters, the dna and the native language", "author": ["Marius Popescu", "Radu Tudor Ionescu"], "venue": "In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Popescu and Ionescu.,? \\Q2013\\E", "shortCiteRegEx": "Popescu and Ionescu.", "year": 2013}, {"title": "Bayesian tree substitution grammars as a usage-based approach", "author": ["Matt Post", "Daniel Gildea"], "venue": "Language and speech,", "citeRegEx": "Post and Gildea.,? \\Q2013\\E", "shortCiteRegEx": "Post and Gildea.", "year": 2013}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain", "author": ["Frank Rosenblatt"], "venue": "Psychological review,", "citeRegEx": "Rosenblatt.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt.", "year": 1958}, {"title": "Native Language Detection with Tree Substitution Grammars. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 193\u2013197", "author": ["Benjamin Swanson", "Eugene Charniak"], "venue": null, "citeRegEx": "Swanson and Charniak.,? \\Q2012\\E", "shortCiteRegEx": "Swanson and Charniak.", "year": 2012}, {"title": "The \u201dHows\u201d and the \u201dWhys\u201d of Coding Categories in a Learner Corpus (or \u201dHow and Why an Error-Tagged Learner Corpus is not ipso facto One Big Comparative Fallacy\u201d)", "author": ["Kari Tenfjord", "Hilde Johansen", "Jon Erik Hagen"], "venue": "Rivista di psicolinguistica applicata,", "citeRegEx": "Tenfjord et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tenfjord et al\\.", "year": 2006}, {"title": "The ASK corpus: A language learner corpus of Norwegian as a second language", "author": ["Kari Tenfjord", "Paul Meurer", "Knut Hofland"], "venue": "In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC),", "citeRegEx": "Tenfjord et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tenfjord et al\\.", "year": 2006}, {"title": "Norsk andrespr\u030aakskorpus - A corpus of Norwegian as a second language", "author": ["Kari Tenfjord", "Paul Meurer", "Silje Ragnhildstveit"], "venue": "In Learner Corpus Research Conference (LCR", "citeRegEx": "Tenfjord et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tenfjord et al\\.", "year": 2013}, {"title": "Native Tongues, Lost and Found: Resources and Empirical Evaluations in Native Language Identification", "author": ["Joel Tetreault", "Daniel Blanchard", "Aoife Cahill", "Martin Chodorow"], "venue": "In Proceedings of COLING", "citeRegEx": "Tetreault et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tetreault et al\\.", "year": 2012}, {"title": "A report on the first native language identification shared task", "author": ["Joel Tetreault", "Daniel Blanchard", "Aoife Cahill"], "venue": "In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Tetreault et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tetreault et al\\.", "year": 2013}, {"title": "Diagnosis of multiple cancer types by shrunken centroids of gene expression", "author": ["Robert Tibshirani", "Trevor Hastie", "Balasubramanian Narasimhan", "Gilbert Chu"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Tibshirani et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2002}, {"title": "The Jinan Chinese Learner Corpus", "author": ["Maolin Wang", "Shervin Malmasi", "Mingxuan Huang"], "venue": "In Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Neural network credit scoring models", "author": ["David West"], "venue": "Computers & Operations Research,", "citeRegEx": "West.,? \\Q2000\\E", "shortCiteRegEx": "West.", "year": 2000}, {"title": "Stacked Generalization", "author": ["David H Wolpert"], "venue": "Neural Networks,", "citeRegEx": "Wolpert.,? \\Q1992\\E", "shortCiteRegEx": "Wolpert.", "year": 1992}, {"title": "Contrastive Analysis and Native Language Identification", "author": ["Sze-Meng Jojo Wong", "Mark Dras"], "venue": "In Proceedings of the Australasian Language Technology Association Workshop (ALTA),", "citeRegEx": "Wong and Dras.,? \\Q2009\\E", "shortCiteRegEx": "Wong and Dras.", "year": 2009}, {"title": "Exploiting Parse Structures for Native Language Identification", "author": ["Sze-Meng Jojo Wong", "Mark Dras"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Wong and Dras.,? \\Q2011\\E", "shortCiteRegEx": "Wong and Dras.", "year": 2011}, {"title": "A survey of multiple classifier systems as hybrid systems", "author": ["l Wo\u017aniak", "Manuel Gra\u00f1a", "Emilio Corchado"], "venue": "Information Fusion,", "citeRegEx": "Wo\u017aniak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wo\u017aniak et al\\.", "year": 2014}, {"title": "Text categorization based on regularized linear classification methods", "author": ["Tong Zhang", "Frank J Oles"], "venue": "Information retrieval,", "citeRegEx": "Zhang and Oles.,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Oles.", "year": 2001}, {"title": "Classification of gene microarrays by penalized logistic regression", "author": ["Ji Zhu", "Trevor Hastie"], "venue": "Biostatistics, 5(3):427\u2013443,", "citeRegEx": "Zhu and Hastie.,? \\Q2004\\E", "shortCiteRegEx": "Zhu and Hastie.", "year": 2004}], "referenceMentions": [{"referenceID": 62, "context": "This relates to cross-linguistic influence (CLI), a key topic in the field of Second Language Acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages (Ortega, 2009).", "startOffset": 182, "endOffset": 196}, {"referenceID": 74, "context": "Recently this has motivated studies in NLI, a subtype of text classification where the goal is to determine the native language of an author using texts they have written in a second language or L2 (Tetreault et al., 2013).", "startOffset": 198, "endOffset": 222}, {"referenceID": 57, "context": "This has links to the idea of adding layers to increase power in neural network-based deep learning, which has come to be an important approach in NLP over the last couple of years (Manning, 2015); Eldan and Shamir (2016) note that \u201cOverwhelming empirical evidence as well as intuition indicates that having depth in the neural network is indeed important\u201d.", "startOffset": 181, "endOffset": 196}, {"referenceID": 22, "context": "Deep neural networks can in fact be seen as layered classifiers (Goldberg, 2015), and ensemble methods as an alternative way of adding power via additional layers.", "startOffset": 64, "endOffset": 80}, {"referenceID": 56, "context": "In this article we look just at ensemble methods: deep learning has not yet produced state-of-the-art results on related tasks (Malmasi et al., 2016), and our goal is to understand what it is that has made ensemble methods to date in NLI so successful.", "startOffset": 127, "endOffset": 149}, {"referenceID": 15, "context": "This has links to the idea of adding layers to increase power in neural network-based deep learning, which has come to be an important approach in NLP over the last couple of years (Manning, 2015); Eldan and Shamir (2016) note that \u201cOverwhelming empirical evidence as well as intuition indicates that having depth in the neural network is indeed important\u201d.", "startOffset": 198, "endOffset": 222}, {"referenceID": 63, "context": "They have been applied to a wide range of real-world problems and shown to achieve better results compared to single-classifier methods (Oza and Tumer, 2008).", "startOffset": 136, "endOffset": 157}, {"referenceID": 81, "context": "Ensemble methods continue to receive increasing attention from investigators and remain a focus of machine learning research (Wo\u017aniak et al., 2014; Kuncheva and Rod\u0155\u0131guez, 2014).", "startOffset": 125, "endOffset": 177}, {"referenceID": 74, "context": "A detailed review of NLI methods is omitted here for reasons of space, but a thorough exposition is presented in the report from the very first NLI Shared Task that was held in 2013 (Tetreault et al., 2013).", "startOffset": 182, "endOffset": 206}, {"referenceID": 24, "context": "The International Corpus of Learner English (Granger et al., 2009) was widely used until recently, despite its shortcomings being widely noted (Brooke and Hirst, 2012a).", "startOffset": 44, "endOffset": 66}, {"referenceID": 2, "context": "More recently, Toefl11, the first corpus designed for NLI was released (Blanchard et al., 2013).", "startOffset": 71, "endOffset": 95}, {"referenceID": 76, "context": "Recently, Malmasi and Dras (2014b) introduced the Jinan Chinese Learner Corpus (Wang et al., 2015) for NLI and their results indicate that feature performance may be similar across corpora and even L1-L2 pairs.", "startOffset": 79, "endOffset": 98}, {"referenceID": 2, "context": "More recently, Toefl11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a,c). Recently, Malmasi and Dras (2014b) introduced the Jinan Chinese Learner Corpus (Wang et al.", "startOffset": 72, "endOffset": 342}, {"referenceID": 2, "context": "More recently, Toefl11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a,c). Recently, Malmasi and Dras (2014b) introduced the Jinan Chinese Learner Corpus (Wang et al., 2015) for NLI and their results indicate that feature performance may be similar across corpora and even L1-L2 pairs. Similarly, Malmasi et al. (2015a) also proposed using the ASK corpus to conduct NLI research using L2 Norwegian data.", "startOffset": 72, "endOffset": 552}, {"referenceID": 2, "context": "More recently, Toefl11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a,c). Recently, Malmasi and Dras (2014b) introduced the Jinan Chinese Learner Corpus (Wang et al., 2015) for NLI and their results indicate that feature performance may be similar across corpora and even L1-L2 pairs. Similarly, Malmasi et al. (2015a) also proposed using the ASK corpus to conduct NLI research using L2 Norwegian data. In this study we make use of three of these aforementioned corpora: Toefl11, JCLC and ASK; detailed descriptions will be provided in \u00a73. As mentioned earlier, some of the most successful approaches to NLI have used ensemble learning methods. We now present an overview of this ensemble-based NLI research. Tetreault et al. (2012) were the first to propose the use of classifier ensembles for NLI and performed a comprehensive evaluation of the feature types used until that point.", "startOffset": 72, "endOffset": 966}, {"referenceID": 2, "context": "More recently, Toefl11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a,c). Recently, Malmasi and Dras (2014b) introduced the Jinan Chinese Learner Corpus (Wang et al., 2015) for NLI and their results indicate that feature performance may be similar across corpora and even L1-L2 pairs. Similarly, Malmasi et al. (2015a) also proposed using the ASK corpus to conduct NLI research using L2 Norwegian data. In this study we make use of three of these aforementioned corpora: Toefl11, JCLC and ASK; detailed descriptions will be provided in \u00a73. As mentioned earlier, some of the most successful approaches to NLI have used ensemble learning methods. We now present an overview of this ensemble-based NLI research. Tetreault et al. (2012) were the first to propose the use of classifier ensembles for NLI and performed a comprehensive evaluation of the feature types used until that point. In their study they used an ensemble of logistic regression learners using a wide range of features that included character and word n-grams, function words, parts of speech, spelling errors and writing quality markers. With regard to syntactic features, they also investigated the use of Tree Substitution Grammars and dependency features extracted using the Stanford parser. Furthermore, they also proposed using language models for this task and in their system used language model perplexity scores based on lexical 5-grams from each language in the corpus. The set of features used here was the largest of any NLI study to date. With this system, the authors reported state of the art accuracies of 90.1% and 80.9% on the ICLE and Toefl11 corpora, respectively. Tetreault et al. (2012) also conducted cross-corpus evaluation, using the 7 common L1 classes between the ICLE and Toefl11 corpora.", "startOffset": 72, "endOffset": 1908}, {"referenceID": 2, "context": "More recently, Toefl11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a,c). Recently, Malmasi and Dras (2014b) introduced the Jinan Chinese Learner Corpus (Wang et al., 2015) for NLI and their results indicate that feature performance may be similar across corpora and even L1-L2 pairs. Similarly, Malmasi et al. (2015a) also proposed using the ASK corpus to conduct NLI research using L2 Norwegian data. In this study we make use of three of these aforementioned corpora: Toefl11, JCLC and ASK; detailed descriptions will be provided in \u00a73. As mentioned earlier, some of the most successful approaches to NLI have used ensemble learning methods. We now present an overview of this ensemble-based NLI research. Tetreault et al. (2012) were the first to propose the use of classifier ensembles for NLI and performed a comprehensive evaluation of the feature types used until that point. In their study they used an ensemble of logistic regression learners using a wide range of features that included character and word n-grams, function words, parts of speech, spelling errors and writing quality markers. With regard to syntactic features, they also investigated the use of Tree Substitution Grammars and dependency features extracted using the Stanford parser. Furthermore, they also proposed using language models for this task and in their system used language model perplexity scores based on lexical 5-grams from each language in the corpus. The set of features used here was the largest of any NLI study to date. With this system, the authors reported state of the art accuracies of 90.1% and 80.9% on the ICLE and Toefl11 corpora, respectively. Tetreault et al. (2012) also conducted cross-corpus evaluation, using the 7 common L1 classes between the ICLE and Toefl11 corpora. Training on the ICLE data, they report an accuracy of 26.6%. The very first shared task focusing on Native Language Identification was held in 2013, bringing further focus, interest and attention to the field. The NLI Shared Task 2013 was co-located with the eighth instalment of the Building Educational Applications Workshop at NAACL-HLT 2013. The competition attracted entries from 29 teams. The winning entry for the shared task was that of Jarvis et al. (2013), with an accuracy of 83.", "startOffset": 72, "endOffset": 2482}, {"referenceID": 28, "context": "The MITRE system (Henderson et al., 2013) is another highly lexicalized system where the primary features used are word, part-of-speech and character n-grams.", "startOffset": 17, "endOffset": 41}, {"referenceID": 23, "context": "Gyawali et al. (2013) utilized lexical and syntactic features based on n-grams of characters, words and part-of-speech tags (using both the Penn TreeBank and Universal Parts Of Speech tagsets), along with perplexity values of character n-grams to build four different models.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "In the system designed by Cimino et al. (2013) the authors use a wide set of general purpose features that are designed to be portable across languages, domains and tasks.", "startOffset": 26, "endOffset": 47}, {"referenceID": 8, "context": "In the system designed by Cimino et al. (2013) the authors use a wide set of general purpose features that are designed to be portable across languages, domains and tasks. This set includes features that are lexical (sentence length, document length, type/token ration, character and word n-grams), morphosyntactic (coarse and fine-grained part-of-speech tag n-grams) and syntactic (parse tree and dependencybased features). They report that they found distributional differences across the L1s for many of these features, including average word and sentence lengths. However, we note that many of these differences are not of a large magnitude, and the authors did not run any statistical tests to measure the significance levels of these differences. Using this feature set, they experiment with a single-classifier system as well as classifier ensembles, using SVM and Maximum Entropy classifiers. In their ensemble, they experiment with using a majority voting system as well as a meta-classifier approach. The authors report that the ensemble methods outperform all single-classifier systems (by around 2%), and their best performance of 77.9% is provided by the meta-classifier system which used linear SVM and MaxEnt as the component classifiers and combined the results using a polynomial kernel SVM classifier. While the set of features used in this experiment is not widely different to other reported NLI research, their use of a meta-classifier is an interesting approach that warrants further study. In their system, Goutte et al. (2013) used character, word and part-of-speech n-grams along with syntactic dependencies.", "startOffset": 26, "endOffset": 1551}, {"referenceID": 8, "context": "In the system designed by Cimino et al. (2013) the authors use a wide set of general purpose features that are designed to be portable across languages, domains and tasks. This set includes features that are lexical (sentence length, document length, type/token ration, character and word n-grams), morphosyntactic (coarse and fine-grained part-of-speech tag n-grams) and syntactic (parse tree and dependencybased features). They report that they found distributional differences across the L1s for many of these features, including average word and sentence lengths. However, we note that many of these differences are not of a large magnitude, and the authors did not run any statistical tests to measure the significance levels of these differences. Using this feature set, they experiment with a single-classifier system as well as classifier ensembles, using SVM and Maximum Entropy classifiers. In their ensemble, they experiment with using a majority voting system as well as a meta-classifier approach. The authors report that the ensemble methods outperform all single-classifier systems (by around 2%), and their best performance of 77.9% is provided by the meta-classifier system which used linear SVM and MaxEnt as the component classifiers and combined the results using a polynomial kernel SVM classifier. While the set of features used in this experiment is not widely different to other reported NLI research, their use of a meta-classifier is an interesting approach that warrants further study. In their system, Goutte et al. (2013) used character, word and part-of-speech n-grams along with syntactic dependencies. They used an ensemble of SVM classifiers trained on each feature space, using a majority vote combiner method. To represent the feature values, they use two value normalization methods based on TF-IDF and cosine normalization. Their best entry achieved an accuracy of 81.8%, higher than many systems using the same standard features and more, demonstrating the effectiveness of using ensemble classifiers and appropriate feature value representation. The authors, like many others, also note that lexical features provided the best performance for a single feature in their system, but that this can be boosted by combining multiple predictors. The MITRE system (Henderson et al., 2013) is another highly lexicalized system where the primary features used are word, part-of-speech and character n-grams. In this system, these features are used by independent classifiers (logistic regression, Winnow2 and language models) whose output is then combined into a final prediction using a N\u00e4\u0131ve Bayes model. Their best performing ensemble was 82.6% accurate in the shared task and the authors emphasize the value of ensemble methods that combine independent systems. Furthermore, the authors also optimized the parameters of their Naive Bayes model using a grid search over the development data. Hladka et al. (2013) developed an ensemble classifier system using some standard features (lemma, word and part-of-speech n-grams, word skipgrams) with SVM classifiers.", "startOffset": 26, "endOffset": 2946}, {"referenceID": 8, "context": "Another system that utilizes an ensemble is that of Bykh et al. (2013), where they used a probabilitybased ensemble.", "startOffset": 52, "endOffset": 71}, {"referenceID": 33, "context": "Recently they expanded their approach with additional experiments (Ionescu et al., 2016), although they did not achieve further improvements on Toefl11-Test.", "startOffset": 66, "endOffset": 88}, {"referenceID": 7, "context": "Following the shared task, Bykh and Meurers (2014) further explored the use of lexicalized and non-lexicalized phrase structure rules for NLI.", "startOffset": 27, "endOffset": 51}, {"referenceID": 7, "context": "Following the shared task, Bykh and Meurers (2014) further explored the use of lexicalized and non-lexicalized phrase structure rules for NLI. They show that the inclusion of lexicalized production rules (i.e. preterminal nodes and terminals) provides improved results. In addition to the standard normalized frequency and binary feature representations they also propose two new representations based on a \u201cvariationist sociolinguistic\u201d perspective. Although they show that these representations outperform the normalized frequency approach, they do not compare this to other representations which have been shown to improve NLI accuracy, such as TF-IDF. They combine their lexicalized production rules feature with additional surface n-gram features in a tuned and optimized ensemble, reporting an accuracy of 84.82% on the Toefl11-Test set. Ionescu et al. (2014) extend the previous work of Popescu and Ionescu (2013) which used string kernels to perform NLI using only character n-gram features.", "startOffset": 27, "endOffset": 866}, {"referenceID": 7, "context": "Following the shared task, Bykh and Meurers (2014) further explored the use of lexicalized and non-lexicalized phrase structure rules for NLI. They show that the inclusion of lexicalized production rules (i.e. preterminal nodes and terminals) provides improved results. In addition to the standard normalized frequency and binary feature representations they also propose two new representations based on a \u201cvariationist sociolinguistic\u201d perspective. Although they show that these representations outperform the normalized frequency approach, they do not compare this to other representations which have been shown to improve NLI accuracy, such as TF-IDF. They combine their lexicalized production rules feature with additional surface n-gram features in a tuned and optimized ensemble, reporting an accuracy of 84.82% on the Toefl11-Test set. Ionescu et al. (2014) extend the previous work of Popescu and Ionescu (2013) which used string kernels to perform NLI using only character n-gram features.", "startOffset": 27, "endOffset": 921}, {"referenceID": 2, "context": "1 The TOEFL11 Corpus The Toefl11 corpus (Blanchard et al., 2013) \u2014 also known as the ETS Corpus of Non-Native Written English \u2014 is the first dataset designed specifically for the task of NLI and developed with the aim of addressing the above-mentioned deficiencies of other previously used corpora.", "startOffset": 40, "endOffset": 64}, {"referenceID": 2, "context": "Diagram reproduced from Blanchard et al. (2013).", "startOffset": 24, "endOffset": 48}, {"referenceID": 4, "context": "Following a similar methodology to that of (Brooke and Hirst, 2011), we randomly select and combine the sentences from the same L1 to generate texts of approximately 300 tokens on average, creating a set of documents suitable for NLI.", "startOffset": 43, "endOffset": 67}, {"referenceID": 2, "context": "Figure reproduced from Blanchard et al. (2013).", "startOffset": 23, "endOffset": 47}, {"referenceID": 76, "context": "Growing interest has led to the recent development of the Jinan Chinese Learner Corpus (Wang et al., 2015), the first large-scale corpus of L2 Chinese consisting of university student essays.", "startOffset": 87, "endOffset": 106}, {"referenceID": 39, "context": "For creating our folds, we employ stratified crossvalidation which aims to ensure that the proportion of classes within each partition is equal (Kohavi, 1995).", "startOffset": 144, "endOffset": 158}, {"referenceID": 36, "context": "Linear Support Vector Machine (SVM) classifiers are a highly robust supervised classification method that has proven to be very effective for text classification (Joachims, 1998).", "startOffset": 162, "endOffset": 178}, {"referenceID": 64, "context": "Additionally, an SVM is a margin-based classifier and does not output probability estimates for each class label, although there are additional methods to map the outputs to probabilities (Platt, 2000).", "startOffset": 188, "endOffset": 201}, {"referenceID": 31, "context": "This is because the kernel maps the data points in a non-linear manner, allowing for more flexible decision boundaries (Hsu et al., 2003).", "startOffset": 119, "endOffset": 137}, {"referenceID": 20, "context": "Although high-dimensional input poses a challenge for these models (Genkin et al., 2007), this issue can be addressed to some degree using regularization methods (Zhu and Hastie, 2004).", "startOffset": 67, "endOffset": 88}, {"referenceID": 83, "context": ", 2007), this issue can be addressed to some degree using regularization methods (Zhu and Hastie, 2004).", "startOffset": 81, "endOffset": 103}, {"referenceID": 20, "context": "Although high-dimensional input poses a challenge for these models (Genkin et al., 2007), this issue can be addressed to some degree using regularization methods (Zhu and Hastie, 2004). This algorithm is inherently multi-class, meaning that OVA and OVO approaches are not required. The logistic regression classifier is also probabilistic and provides continuous probability estimates for each class label. 9Appendix C of Hsu et al. (2003) examines this issue in greater detail.", "startOffset": 68, "endOffset": 440}, {"referenceID": 68, "context": "4 Perceptron The Perceptron (Rosenblatt, 1958) is another linear learning algorithm that has been successful The algorithm learns a weight vector and a bias term which shifts the decision boundary from the origin.", "startOffset": 28, "endOffset": 46}, {"referenceID": 10, "context": "Perceptrons have been successfully used for POS tagging (Collins, 2002) and parsing (Collins and Roark, 2004).", "startOffset": 56, "endOffset": 71}, {"referenceID": 11, "context": "Perceptrons have been successfully used for POS tagging (Collins, 2002) and parsing (Collins and Roark, 2004).", "startOffset": 84, "endOffset": 109}, {"referenceID": 82, "context": "5 Ridge Regression Classification using ridge regression is an approach based on a regression model that uses a linear least squares loss function (Zhang and Oles, 2001).", "startOffset": 147, "endOffset": 169}, {"referenceID": 16, "context": "A classic learning algorithm, Linear Discriminant Analysis (LDA, not to be confused with Latent Dirichlet Allocation) is a method based on a linear decision boundary (Fisher, 1936).", "startOffset": 166, "endOffset": 180}, {"referenceID": 47, "context": "It has been widely and successfully used for classification (Liu and Wechsler, 2002).", "startOffset": 60, "endOffset": 84}, {"referenceID": 27, "context": "Quadratic Discriminant Analysis (QDA) is similar to LDA, except that it uses a quadratic decision surface (Hastie et al., 2009).", "startOffset": 106, "endOffset": 127}, {"referenceID": 12, "context": "9 k-nearest Neighbors A popular neighbor-based algorithm, k-nearest Neighbors (k-NN) is an instance-based classifier that does not build a statistical model (Cover and Hart, 1967).", "startOffset": 157, "endOffset": 179}, {"referenceID": 75, "context": "mean) vector for each class (Tibshirani et al., 2002).", "startOffset": 28, "endOffset": 53}, {"referenceID": 64, "context": "10There exist additional methods to map these distances to probabilities (Platt, 2000).", "startOffset": 73, "endOffset": 86}, {"referenceID": 61, "context": "Function words\u2019 topic independence has led them to be widely used in studies of authorship attribution (Mosteller and Wallace, 1964) as well as NLI and they have been established to be informative for these tasks.", "startOffset": 103, "endOffset": 132}, {"referenceID": 47, "context": "For Chinese, we utilize the function word list described in Malmasi and Dras (2014b). In addition to single function words, we also extract function word bigrams, as described by Malmasi et al.", "startOffset": 60, "endOffset": 85}, {"referenceID": 47, "context": "For Chinese, we utilize the function word list described in Malmasi and Dras (2014b). In addition to single function words, we also extract function word bigrams, as described by Malmasi et al. (2013). Function word bigrams are a type of word n-gram where content words are skipped: they are thus a specific subtype of the skip-grams discussed by Guthrie et al.", "startOffset": 60, "endOffset": 201}, {"referenceID": 25, "context": "Function word bigrams are a type of word n-gram where content words are skipped: they are thus a specific subtype of the skip-grams discussed by Guthrie et al. (2006). For example, the sentence \u201cWe should all start taking the bus\u201d would be reduced to \u201cwe should all the\u201d, from which we would extract the n-grams.", "startOffset": 145, "endOffset": 167}, {"referenceID": 53, "context": "We extract POS n-grams of order 1\u20133, which have been shown to be useful for NLI (Malmasi et al., 2013).", "startOffset": 80, "endOffset": 102}, {"referenceID": 59, "context": "For English and Chinese, the Stanford CoreNLP suite of NLP tools (Manning et al., 2014) and the provided models were used to tokenize, POS tag and parse the unsegmented corpus texts.", "startOffset": 65, "endOffset": 87}, {"referenceID": 53, "context": "Additionally, we extract a second set of POS n-grams for the Toefl11 data using the CLAWS dataset, which has been shown to perform well for NLI (Malmasi et al., 2013).", "startOffset": 144, "endOffset": 166}, {"referenceID": 52, "context": "We extract POS n-grams of order 1\u20133, which have been shown to be useful for NLI (Malmasi et al., 2013). These n-grams capture small and very local syntactic patterns of language production and were used as classification features. Previous work and our experiments showed that sequences of size 4 or greater achieve lower accuracy, possibly due to data sparsity, so we do not include them. For English and Chinese, the Stanford CoreNLP suite of NLP tools (Manning et al., 2014) and the provided models were used to tokenize, POS tag and parse the unsegmented corpus texts. We did not use any NLP tools for Norwegian as the corpus we use is already annotated with POS tags. Additionally, we extract a second set of POS n-grams for the Toefl11 data using the CLAWS dataset, which has been shown to perform well for NLI (Malmasi et al., 2013). 11For example, the largest list used by Wong and Dras (2009) was a stopword list from Information Retrieval; given the size of their list, this was presumably also the case for Koppel et al.", "startOffset": 81, "endOffset": 902}, {"referenceID": 40, "context": "11For example, the largest list used by Wong and Dras (2009) was a stopword list from Information Retrieval; given the size of their list, this was presumably also the case for Koppel et al. (2005), although the source there was not given.", "startOffset": 177, "endOffset": 198}, {"referenceID": 37, "context": "We use the grammar proposed by (Johnson, 2010) for capturing topical collocations: Sentence\u2192 Docj j \u2208 1, .", "startOffset": 31, "endOffset": 46}, {"referenceID": 37, "context": "We use the grammar proposed by (Johnson, 2010) for capturing topical collocations: Sentence\u2192 Docj j \u2208 1, . . . ,m Docj \u2192 j j \u2208 1, . . . ,m Docj \u2192 Docj Topici i \u2208 1, . . . , t; j \u2208 1, . . . ,m Topici \u2192Words i \u2208 1, . . . , t Words\u2192Word Words\u2192Words Word Word\u2192 w w \u2208 Vpos; w \u2208 Vpos+fw Vpos contains 119 distinct POS tags based on the Brown tagset and Vpos+fw is extended with 398 function words. The number of topics t is set to 50. The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carlo technique made available by Johnson (2010).", "startOffset": 32, "endOffset": 563}, {"referenceID": 69, "context": "Reproduced from Swanson and Charniak (2012).", "startOffset": 16, "endOffset": 44}, {"referenceID": 80, "context": "They have been found to be useful for NLI (Wong and Dras, 2011).", "startOffset": 42, "endOffset": 63}, {"referenceID": 67, "context": "TSGs are a generalization of context-free grammars that allow non-terminals to rewrite as fragments which can have an arbitrary size (Post and Gildea, 2013), instead of being limited to a depth of one.", "startOffset": 133, "endOffset": 156}, {"referenceID": 68, "context": "Tree Substitution Grammar (TSG) fragments have been proposed by Swanson and Charniak (2012) as yet another type of syntactic feature for NLI or other syntactically motivated text classification tasks.", "startOffset": 64, "endOffset": 92}, {"referenceID": 3, "context": "For example, Bagging (bootstrap aggregating) is a commonly used method for ensemble generation (Breiman, 1996) that can create multiple base classifiers.", "startOffset": 95, "endOffset": 110}, {"referenceID": 18, "context": "with the AdaBoost algorithm) is another method where the base models are created with different weight distributions over the training data with the aim of assigning higher weights to training instances that are misclassified (Freund and Schapire, 1996).", "startOffset": 226, "endOffset": 253}, {"referenceID": 65, "context": "Although a number of different fusion methods have been proposed and tested, there is no single dominant method (Polikar, 2006).", "startOffset": 112, "endOffset": 127}, {"referenceID": 65, "context": "Our selected methods are described below; a variety of other methods exist and the interested reader can refer to the thorough exposition by (Polikar, 2006).", "startOffset": 141, "endOffset": 156}, {"referenceID": 38, "context": "155) and resilient to estimation errors (Kittler et al., 1998), making it one of the more robust combiners discussed in the literature.", "startOffset": 40, "endOffset": 62}, {"referenceID": 38, "context": "Given that the mean probability used in the above rule is sensitive to outliers, an alternative is to use the median as a more robust estimate of the mean (Kittler et al., 1998).", "startOffset": 155, "endOffset": 177}, {"referenceID": 30, "context": "The winning label is then selected using the Borda count algorithm (Ho et al., 1994).", "startOffset": 67, "endOffset": 84}, {"referenceID": 45, "context": "This method has previously been used to analyze the limits of majority vote classifier combination (Kuncheva et al., 2001).", "startOffset": 99, "endOffset": 122}, {"referenceID": 41, "context": "Oracles are usually used in comparative experiments and to gauge the performance and diversity of the classifiers chosen for an ensemble (Kuncheva, 2002; Kuncheva et al., 2003).", "startOffset": 137, "endOffset": 176}, {"referenceID": 46, "context": "Oracles are usually used in comparative experiments and to gauge the performance and diversity of the classifiers chosen for an ensemble (Kuncheva, 2002; Kuncheva et al., 2003).", "startOffset": 137, "endOffset": 176}, {"referenceID": 58, "context": "This method is inspired by the \u201cPrecision at k\u201d metric from Information Retrieval (Manning et al., 2008) which measures precision at fixed low levels of results (e.", "startOffset": 82, "endOffset": 104}, {"referenceID": 78, "context": "For our second model we expand our methodology to such a meta-classifier, also referred to as stacked generalization or classifier stacking (Wolpert, 1992).", "startOffset": 140, "endOffset": 155}, {"referenceID": 21, "context": "These methods have been widely used for creating decision tree ensembles (Geurts et al., 2006); we will experiment with random forests, extra trees and the AdaBoost algorithm.", "startOffset": 73, "endOffset": 94}, {"referenceID": 35, "context": "We also compare these results against the winning system from the 2013 NLI shared task (Jarvis et al., 2013) and two systems by (Bykh and Meurers, 2014) and (Ionescu et al.", "startOffset": 87, "endOffset": 108}, {"referenceID": 7, "context": ", 2013) and two systems by (Bykh and Meurers, 2014) and (Ionescu et al.", "startOffset": 27, "endOffset": 51}, {"referenceID": 32, "context": ", 2013) and two systems by (Bykh and Meurers, 2014) and (Ionescu et al., 2014) which presented state-of-the-art results following the task.", "startOffset": 56, "endOffset": 78}, {"referenceID": 7, "context": "Baselines Random Baseline 9\u00b71 9\u00b71 2013 Shared Task Winner 84\u00b75 83\u00b70 Bykh and Meurers (2014) \u2014 84\u00b78 Ionescu et al.", "startOffset": 68, "endOffset": 92}, {"referenceID": 7, "context": "Baselines Random Baseline 9\u00b71 9\u00b71 2013 Shared Task Winner 84\u00b75 83\u00b70 Bykh and Meurers (2014) \u2014 84\u00b78 Ionescu et al. (2014) 84\u00b71 85\u00b73", "startOffset": 68, "endOffset": 121}, {"referenceID": 7, "context": "Baselines Random Baseline 9\u00b71 9\u00b71 2013 Shared Task Winner 84\u00b75 83\u00b70 Bykh and Meurers (2014) \u2014 84\u00b78 Ionescu et al.", "startOffset": 68, "endOffset": 92}, {"referenceID": 7, "context": "Baselines Random Baseline 9\u00b71 9\u00b71 2013 Shared Task Winner 84\u00b75 83\u00b70 Bykh and Meurers (2014) \u2014 84\u00b78 Ionescu et al. (2014) 84\u00b71 85\u00b73 Our LDA Meta-classifier (continuous) 85\u00b72 86\u00b78 Decision Tree Ensembles Random Forest 84\u00b72 84\u00b76 Extra Trees 81\u00b70 82\u00b77 AdaBoost 75\u00b73 76\u00b72", "startOffset": 68, "endOffset": 121}, {"referenceID": 1, "context": "Although a single decision tree was not a good meta-classifier, it has been shown that ensembles of trees can perform very well (Banfield et al., 2007).", "startOffset": 128, "endOffset": 151}, {"referenceID": 14, "context": "An important question that arises in various contexts within machine learning deals with determining which methods outperform others on a given problem (Dietterich, 1998).", "startOffset": 152, "endOffset": 170}, {"referenceID": 60, "context": "McNemar\u2019s test (McNemar, 1947) is a non-parametric method to test for significant differences in proportions for paired nominal data.", "startOffset": 15, "endOffset": 30}, {"referenceID": 14, "context": "In the context of machine learning it is often used to compare the performance of distinct algorithms on the same data (Dietterich, 1998) as it does not assume independent samples and has a low Type I error rate (F\u00fcrnkranz, 2002).", "startOffset": 119, "endOffset": 137}, {"referenceID": 19, "context": "In the context of machine learning it is often used to compare the performance of distinct algorithms on the same data (Dietterich, 1998) as it does not assume independent samples and has a low Type I error rate (F\u00fcrnkranz, 2002).", "startOffset": 212, "endOffset": 229}, {"referenceID": 77, "context": "It is the most commonly used for pairwise classifier comparison and has been used in a wide range of machine learning applications (West, 2000; Aue and Gamon, 2005).", "startOffset": 131, "endOffset": 164}, {"referenceID": 0, "context": "It is the most commonly used for pairwise classifier comparison and has been used in a wide range of machine learning applications (West, 2000; Aue and Gamon, 2005).", "startOffset": 131, "endOffset": 164}, {"referenceID": 0, "context": "It is the most commonly used for pairwise classifier comparison and has been used in a wide range of machine learning applications (West, 2000; Aue and Gamon, 2005). This is also the test we propose for use within NLI. In this section we briefly describe the test and demonstrate its application for NLI. The interested reader can find more details about methods for evaluating the statistical significance of classifier differences in the work of Foody (2004). McNemar\u2019s test is a non-parametric method based on creating a 2\u00d72 contingency table for the outcomes of a pair of tests (classifiers in our case), tabulating the number of instances where their predictions agree or disagree.", "startOffset": 144, "endOffset": 461}, {"referenceID": 14, "context": "The test statistic is based on a chi-square distribution, with additional continuity correction to account for the fact that a continuous distribution is being used to represent a discrete one (Dietterich, 1998; Foody, 2004).", "startOffset": 193, "endOffset": 224}, {"referenceID": 17, "context": "The test statistic is based on a chi-square distribution, with additional continuity correction to account for the fact that a continuous distribution is being used to represent a discrete one (Dietterich, 1998; Foody, 2004).", "startOffset": 193, "endOffset": 224}, {"referenceID": 51, "context": "The work of Malmasi et al. (2015b) was an initial step in this direction by making available all 144 submissions from the 2013 shared task, including that of the winning system.", "startOffset": 12, "endOffset": 35}, {"referenceID": 32, "context": "During the course of this research, Ionescu et al. (2014) also provided us with the predictions from their state-ofthe-art system, which we also make available.", "startOffset": 36, "endOffset": 58}, {"referenceID": 32, "context": "During the course of this research, Ionescu et al. (2014) also provided us with the predictions from their state-ofthe-art system, which we also make available. The availability of this data can become increasingly important as state-of-the-art results move closer towards the oracle upper bounds. We now evaluate the performance of our top model against that of the two previous state-of-the-art systems which were used as baselines in our experiments, using the aforementioned prediction data. We report the pairwise p values for the test, as listed in Table 11. They show that the improvement in our results is significantly better than both of the baselines. In contrast, they also show that the results of Ionescu et al. (2014) were not significantly better than the previous best result.", "startOffset": 36, "endOffset": 733}, {"referenceID": 32, "context": "Ionescu et al. Our Method Jarvis et al. (2013) \u2014 0.", "startOffset": 0, "endOffset": 47}, {"referenceID": 32, "context": "Ionescu et al. Our Method Jarvis et al. (2013) \u2014 0.1082 0.0001* Ionescu et al. (2014) \u2013 \u2014 0.", "startOffset": 0, "endOffset": 86}, {"referenceID": 31, "context": "This differs to the work of Ionescu et al. (2014), where their best results on different sets were achieved using different parameters, or that of Bykh and Meurers (2014), who did not test their method on different datasets.", "startOffset": 28, "endOffset": 50}, {"referenceID": 7, "context": "(2014), where their best results on different sets were achieved using different parameters, or that of Bykh and Meurers (2014), who did not test their method on different datasets.", "startOffset": 104, "endOffset": 128}], "year": 2017, "abstractText": "Ensemble methods using multiple classifiers have proven to be the most successful approach for the task of Native Language Identification (NLI), achieving the current state of the art. However, a systematic examination of ensemble methods for NLI has yet to be conducted. Additionally, deeper ensemble architectures such as classifier stacking have not been closely evaluated. We present a set of experiments using three ensemble-based models, testing each with multiple configurations and algorithms. This includes a rigorous application of meta-classification models for NLI, achieving state-of-the-art results on three datasets from different languages. We also present the first use of statistical significance testing for comparing NLI systems, showing that our results are significantly better than the previous state of the art. We make available a collection of test set predictions to facilitate future statistical tests.", "creator": "LaTeX with hyperref package"}}}