{"id": "1703.06554", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Object category understanding via eye fixations on freehand sketches", "abstract": "the study of eye gaze fixations on photographic images favors overwhelmingly active participant area. below contrast, the image sequence of freehand sketches has not shed as much attention for such studies. in this paper, we analyze the description of a free - sighted gaze fixation study conducted on 3904 freehand sketches distributed across 160 object categories. our analysis indicates that single sequences exhibit marked consistency includes 1 dictionary, across sketches of a category and even across suitably grouped sets of categories. this multi - level consistency is remarkable given the variability in depiction and extreme image content sparsity analysis characterizes hand - drawn object sketches. in our paper, measurements show that the multi - level consistency in the fixation data can be exploited to ( a ) predict a test sketch's category given only its fixation sequence and ( b ) build a computational model of predicts part - labels finger fixations on objects. we hope that recent findings motivate the community to deem sketch - like representations consist of gaze - based studies vis - a - secret photographic images.", "histories": [["v1", "Mon, 20 Mar 2017 01:13:33 GMT  (734kb,D)", "http://arxiv.org/abs/1703.06554v1", "Accepted for publication in Transactions on Image Processing (this http URL)"]], "COMMENTS": "Accepted for publication in Transactions on Image Processing (this http URL)", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["ravi kiran sarvadevabhatla", "sudharshan suresh", "r venkatesh babu"], "accepted": false, "id": "1703.06554"}, "pdf": {"name": "1703.06554.pdf", "metadata": {"source": "META", "title": "Object category understanding via eye fixations on freehand sketches", "authors": ["Ravi Kiran Sarvadevabhatla", "Sudharshan Suresh"], "emails": ["(ravika@gmail.com)", "(venky@cds.iisc.ac.in)"], "sections": [{"heading": null, "text": "Index Terms\u2014object category understanding, freehand sketch, visual saliency, object recognition\nI. INTRODUCTION\nWHEN shown photographic images under a free-viewing(i.e task-free) paradigm, human eyes preferentially fixate on image locations which are visually salient. Multiple studies [1]\u2013[5] have demonstrated that this fixation mechanism is bottom-up, predominantly driven by image content and richness of detail (color, texture etc.).\nThis explanation, while satisfactory for photographic images, seems inadequate for certain categories of images such as line drawings. In particular, one class of line drawings \u2013 hand-drawn sketches \u2013 are sparse and largely devoid of detailed content. In addition, they are typically binary images containing virtually no color-based information (see Fig. 1). Even so, multiple studies have demonstrated a \u201cfixations-intonothing\u201d phenomenon [6]\u2013[9], wherein the eye fixations on the same stimulus by multiple subjects fall on empty regions, yet exhibit enough regularity to make gaze-based inferences. One possible explanation is that the first eye fixation conveys all there is to know (\u2018Gestalt\u2019) about the underlying scene semantics [10] and the regularity in rest of the fixations is a statistical anomaly. However, a more intriguing explanation is that these empty region fixations aim to implicitly verify the overall consistency of the scene content depicted in the sketch [11], [12]. Which of these explanations is correct?\nOn a separate note, gaze-tracking studies have demonstrated that photos of objects trigger signature gaze patterns and go\nRavi Kiran Sarvadevabhatla (ravika@gmail.com) and R. Venkatesh Babu (venky@cds.iisc.ac.in) are with Department of Computational and Data Sciences, Indian Institute of Science, Bangalore 560012, India. Sudharshan Suresh was a summer intern at Video Analytics Lab.\nas far as to say that the underlying object category can be predicted from gaze patterns alone [2], [8]. Do hand-drawn sketches of objects elicit gaze patterns which are predictive of the object category as well? Suppose the category of the sketch is made known shortly prior to actual viewing of the sketch (\u2018priming\u2019). Do the gaze patterns exhibit correlation with any object-level attributes such as semantic-parts? [13].\nTo examine these questions and related issues, we conducted an eye fixation study on a large database containing handdrawn sketches of objects across 160 categories [14]. In this paper, we present some of the interesting findings from our study and discuss the larger implications of our study for visual object category understanding. We summarize our contributions and major findings below : \u2022 We provide our database \u2013 SketchFix-160 \u2013 of eye\nfixations for 3904 hand-drawn sketches across 160 visual object categories (Section III). \u2022 We use SketchFix-160 to create ground-truth eye fixation maps for object sketches. The large quantity of our maps (3904 compared to the previous largest set of 200) can be used in benchmarking generalized1 saliency prediction models [15]\u2013[17]. \u2022 The eye fixation sequences exhibit significant categorylevel structure (Section VI). In fact, we find this structure to be consistent enough to predict the sketch\u2019s category from its fixation sequence alone (Section VII), without recourse to sketch stroke data. \u2022 We map eye fixation sequences to semantic object-part label sequences by utilizing part contour annotations of sketches [18]. Our analysis of these label sequences shows that the sequencing of fixations corresponds to an implicit visitation of the sketch object\u2019s parts in a consistent, pre-defined order of importance. This result is quite remarkable, considering that the parts themselves are not explicitly delineated in the sketch stroke data (Section VIII). \u2022 Leveraging the part-level consistency of eye fixation sequences, we build object-specific computational models which predict the semantic object-part underlying an eye fixation (Section IX)\n1These approaches aim to predict saliency for multiple types of images, including non-photographic varieties such as sketches studied in this paper.\nar X\niv :1\n70 3.\n06 55\n4v 1\n[ cs\n.C V\n] 2\n0 M\nar 2\n01 7\nFrom an application point of view, sketch-like object depictions tend to be used as stylized representations of the underlying category in 3D model renderings [19], art displays, graphic icons and product logos. Therefore, eye-fixation data can provide insights into factors which affect the way such representations are perceived. As an illustrative example, we provide a category-level analysis based on fixation density (see Section VI and Figure 5). In other work, Reid et al. [20] use eye-gaze data to interpret customer selections among sketch-like consumer product renderings. Moreover, sketchlike representations often represent a simplification of visual content. This helps constrain the number of design variables to be studied for statistical significance, potentially reducing the complexity of the study design [20].\nOur sketch object eye-fixation data lays the ground for future studies which can uncover connections between eye fixation patterns on objects in photographic images and their sketched versions. Such connections can help understand depiction invariant attributes and aspects of visual object representations. Combined with data from similar studies on other modalities [13], our sketch gaze data can contribute towards new insights into cross-modal (photo, brush art, line drawing) object representations [21]. To the best of our knowledge, current theories and computational models of saliency, which implicitly assume photographs or real-world scenes as input, seem insufficient to explain our findings. Therefore, further investigation into our findings could lead to more general computational models of human visual saliency which can explain fixations regardless of depiction (photos or sketches).\nPlease visit http://val.cds.iisc.ac.in/sketchfix to access our SketchFix-160 dataset and additional material."}, {"heading": "II. RELATED WORK", "text": "The study of eye-fixations on images is a very active research area in the inter-related fields of neuroscience and computer vision [1]\u2013[5], [22]. However, the broad image category containing sketch-like depictions has not received as much attention presumably due to lack of usable image content with which to correlate the fixation data. Within the literature available, there are four broad categories of studies involving eye-gaze tracking and sketch-like depictions. The first category of studies use gaze tracking to understand how people copy and draw line-drawings and simple shapes [23], [24]. The second use gaze tracking to study differences between perception of photographic image content and corresponding line-drawing representations [25], [26]. In the third category, gaze tracking is used to characterize semantic plausibility of objects in line-drawings of scenes [27]. The fourth category of studies use gaze tracking to explore sketch-like depictions of objects [7]\u2013[9].\nSaliency-related studies typically involve photos of scenes. Although objects make the scenes meaningful in many instances [28], [29], their role has been studied in a limited context. To address this, the role of category-level and semanticlevel information has been examined on a large database of objects by Xu et al. [13]. Additionally, in their survey paper, Frintop et al. [30] summarize prevailing theories of the relationship between attention and object recognition.\nRecently, Ali and Itti [16] constructed a large-scale dataset CAT2000 consisting of eye-fixation data for 20 categories of scenes. One of the 20 categories, \u2018Sketches\u2019, contains fixations for sketch images originally belonging to the same sketch dataset we have used [14]. However, the number of sketches in CAT2000 is quite small (200) compared to what we have studied (3904).\nOur work belongs to the fourth category mentioned above (viz. using gaze tracking to explore sketch-like depictions of objects). However, we analyze a much larger pool of object sketches. To the best of our knowledge, we are also the first to analyze the sketches in terms of categories and that too, across a relatively large number of categories."}, {"heading": "III. DATA GATHERING PROTOCOL", "text": "For our analysis, we used 3904 sketches spread across 160 object categories studied in the work of Sarvadevabhatla and Babu [31]. In their work, the authors use sketches correctly classified by a deep-feature classifier to construct sparsified yet recognizable versions of the sketches which they term category-epitomes. In our study, we utilize the original, full sketches from the dataset of Eitz et al. [14] used to construct epitomic versions. Our choice of sketch data was motivated by two factors: (a) enable collection and analysis of eyefixation data for freehand sketches across a large number of object categories while keeping the burden of data collection manageable (b) enable eye-fixation based analysis of epitomelike sparsified representations in future.\nEquipment and Setup: All subjects were comfortably seated at a viewing distance of 60 cm from a 19-inch LCD monitor. The stimulus images (sketches) were effectively displayed at a resolution of 1024\u00d7 1024 and centered within the display. The stimuli were all the same size. Eye tracking was performed non-invasively via real-time video feed provided by an iView XTM Hi-Speed system. The equipment was operated in monocular mode at a sampling rate of 500Hz. The subject pool consisted of 36 people of both genders aged between 18 and 45 years. All subjects displayed normal or corrected-tonormal vision.\nFor our experiments, we used 3940 sketches spread across 160 object categories. There were 24 sketches for each category and the mean length of fixation sequences was 9. The eye fixation data was collected in two regimes which we refer to as \u2018Unprimed\u2019 and \u2018Primed\u2019.\nUnprimed Regime: The total set of 3904 images (spread over 160 object categories) was first divided randomly into 16 groups of 244 images each. Each subject viewed sketches from a single group. These groups were shown to 20 subjects. In total, the 20\u00d7244 = 4880 image viewings in the unprimed regime resulted in a total of 42943 eye-fixations. The sketches in each group were shown to the corresponding subject in a randomized fashion. In this regime, each sketch in a group was displayed for 3 seconds2 followed by a gray screen display\n2In our pilot studies, we found that a duration above 3 seconds between stimuli resulted in pilot subjects reporting eye strain and inability to fully complete viewing our pre-requisite number of sketches per participant. With a view to minimize participant burden, we chose a presentation time of 3 seconds.\nlasting 1 second, before the next sketch was shown (see Fig. 2a).\nPrimed Regime: As with unprimed regime, the images were again divided into groups. However, the images were chosen only from 13 selected categories for which semantic object-part annotations exist [18]. Thus, the effective total number of images was 283. The images were divided into 4 groups (3 groups with 71 images each and 1 with 70 images). These groups were shown to 16 subjects. Effectively, each unique image was seen by 4 subjects. In this regime, the distinguishing feature was a cue screen (which preceded the sketch) displaying the sketch\u2019s category as a text string representing the category label as provided by the creators of the freehand sketch dataset [14]. All the categories were coarsegrained. For sketch of a dog, the category label provided was dog \u2013 thus, it was neither too coarse (\u2018animal\u2019) nor too finegrained (\u2018Labrador\u2019). Timing considerations were determined by feedback from pilot studies3. The cue, stimulus image and a gray screen were displayed (in that order) for a period of 2, 3 and 2 seconds respectively (see Fig. 2b).\nThe participants were instructed to observe and freely view the images. The participants were informed about the duration for which image/gray screen would be visible. Depending on the regime, the participants were informed whether or not the object category cue would be provided. To minimize visual strain, the subjects were allowed a total of 5 breaks in the unprimed regime and 2 in the primed version. The eye tracking device was recalibrated when resuming after each break to ensure good eye tracking accuracy. All of these aspects were informed to the participants before commencing the data collection procedure. The subjects were provided details of the study and informed consent was obtained from them prior to their session.\nThe regimes mentioned above are motivated by the possibility that the \u2018Unprimed Regime\u2019 necessitates visual exploration for sketch understanding whereas the \u2018Primed Regime\u2019 only\n3 For word presentation, feedback from pilot studies indicated that 1 second to be too small for the viewer to read the category label satisfactorily and 3 seconds to be far too long.\ninvolves visual verification of the category label provided during the study. This distinction is crucial. In fact, we shall soon see that primed gaze sequences are more structured (Section V) and help validate the consistency of implicit object part visitation (Section IX) for sketches."}, {"heading": "IV. CONSTRUCTING THE FIXATION MAP FOR A SKETCH", "text": "Fixation maps summarize the fixation information averaged over multiple subjects viewing the same image (sketch) and are used as ground-truth for evaluating saliency prediction methods [1], [3], [15], [16]. We construct fixation maps for sketches based on the approach described by O\u2019Connell et al. [2] which we summarize next.\nFor a given sketch S, let {(xf , yf )}, f = 1, 2, . . . N denote the set of fixation locations (i.e. the combined set of all fixations aggregated over all the subjects who viewed the sketch). Let {tf} denote the set of corresponding fixation durations. The fixation map is initialized as the sum of impulse functions centered at each fixation location and weighted by the fixation duration. This map is then convolved by a Gaussian kernel whose standard deviation is set to 1\u25e6 of visual angle (\u223c 36 pixels in our case) to approximate the uncertainty in eye-tracker\u2019s fixation measurements.\nF \u2032(x, y) = 1\nN\u2211\nf=1\ntf\nN\u2211\nf=1\ntf exp (\u2212(xf \u2212 x)2 \u2212 (yf \u2212 y)2\n\u03c32\n)\n(1) Typical approaches normalize F \u2032 with respect to the maximum value or to lie between 0 and 1 [1], [16]. In contrast, O\u2019Connell et al. [2] suggest standardizing F \u2032 to obtain a zero mean, unit standard deviation map F . Letting F\u0304 \u2032 and \u03c3F \u2032 denote the mean and standard deviation over the spatial locations of the stimulus region respectively, we have:\nF (x, y) = F \u2032(x, y)\u2212 F\u0304 \u2032\n\u03c3F \u2032 (2)\nThus, we obtain the standardized fixation map F corresponding to the sketch S. Next, we shall see how a fixation-\nmap based similarity measure can be used to analyze the level of agreement between fixation sequences of subjects viewing a given sketch.\nV. INTER-OBSERVER CONGRUENCY (IOC) Suppose a given sketch S is viewed by N subjects s1, s2, . . . sN resulting in N fixation sequences F1, F2, . . . FN . To measure the degree of agreement between the subjects, we compute the Inter-observer congruency (IOC) by computing the similarity D(Fi, F[1:N ]\\i) between fixations of subject si and the set of fixations corresponding to the rest of the subjects (F[1:N ]\\i). The IOC value for sketch S is computed as the average of similarity values over all the subjects, i.e.\nIOCS = 1\nN\nN\u2211\nj=1\nD(Fj , F[1:N ]\\j). For our experiments, we\nuse shuffled AUC (sAUC) as the similarity measure D. To compute sAUC, the set of fixations from rest of the subjects (i.e. F[1:N ]\\i) are utilized to construct a \u2018saliency map\u2019 using the procedure described in Section IV. This saliency map is thresholded and used as a binary classifier to predict saliency of fixations Fi from the reference subject. The resulting true and false positive rates at various threshold settings are used to compute the final similarity. Please refer to Zhang et al. [32] for details of sAUC computation.\nFor each category, we first compute IOC score for each sketch. Next, we compute the median across IOC scores of all the sketches within the category. We compute two groups of IOC scores. The first group comprises of IOC scores obtained using only the fixation sequences collected in the primed regime (Section III). The second group comprises of IOC scores across all the sequences regardless of regime (primed and unprimed). To verify the statistical significance of IOC scores (i.e. whether they exhibit above-chance similarity), we also compute IOC between each subject\u2019s fixation sequence and randomly generated sequences. We find the median of the resulting IOCs across sketches and refer to the same as random median IOC of the category.\nA high value for IOC indicates consistency among the fixation sequences and is commonly observed for natural images, particularly with a central object [1]. The results for the first group of IOC scores (primed regime) can be seen in Figure 3. Across the categories, the median IOCs (in red) are seen to be high and well-separated from the corresponding random median IOCs (in blue). This trend is repeated for the second group of IOC scores (Figure 4) across all the 160 categories, albeit with slightly decreased median IOCs and relatively smaller separation from the random median IOCs compared to the primed regime. The high median IOCs indicates the overall reliability of our SketchFix-160 dataset.\nIn the next section, we move beyond per-sketch fixations and analyze fixations at a category level. To enable this analysis, the data from fixation sequences of all the sketches from a given category is combined into a category-specific spatial map. We start by describing the construction of this category-level map."}, {"heading": "VI. FIXATION MAPS : A CATEGORY-LEVEL PERSPECTIVE", "text": "To obtain a category-level perspective of fixations, the category-specific fixation maps Uc are computed by averaging the standardized fixation maps of sketches within a category (Equation 2). Let nc denote the number of fixation maps in category c. We have:\nUc = 1\nnc\nnc\u2211\ni=1\nFi (3)\nAdditionally, to compensate for biases common to all the categories (e.g. center-bias), marginalized versions of the category maps are computed by subtracting each category\u2019s map from the average over all the category-level maps, i.e.\nMc = Uc \u2212 1\nC\nC\u2211\nj=1\nUj (4)\nIn the above procedure, standardization helps highlight locations which are statistically fixated more than average\n(\u201chotspots\u201d) or less than average across instances4. Next, we shall visualize the category-level maps created in this manner.\nTrends in category-level maps: An analysis of the multiplicity and spatial distribution of highly-fixated regions (\u201chotspots\u201d) in category-level maps reveals certain interesting groupings among the categories. Some of the categories have a sharp, centrally dominant hotspot. Visually, the sketches of these categories tend to depicted symmetrically and compactly (See bottom row of panel 1\u00a9 in Figure 5).\nIn some of the categories, the location of the hotspot is offcenter. Among such categories, three groups were observed. In the first group (panel 2\u00a9 in Figure 5), the hotspot is the location associated with activity or potential for activity (e.g. the glowing end of cigarette, candle or the gong of a bell). Categories in the second off-central hotspot group tend to be, borrowing the terminology of Yun et al. [33], \u201cfunctional\u201d categories (i.e. crafted for a specific purpose). For these categories (panel 3\u00a9 in Figure 5), hotspots tend to be on the portion of the object which is typically associated the most with the underlying category\u2019s functionality (e.g. the taps of the bathtub, the detonation pin in the grenade, the earpads of headphone etc.). The third group corresponds to \u201cbiological\u201d categories (panel 4\u00a9 in Figure 5) wherein the hotspot is predominantly located on the head or face of the biological entity. This phenomenon has been repeatedly observed for regular image datasets as well [1], [16].\nSome of the categories tend to have multiple hotspots (panel 5\u00a9 in Figure 5). Interestingly, most of the vehicular categories (airplane, bicycle, bus, car) seem to exhibit this multi-hotspot pattern.\nThe groupings mentioned above are not disjoint \u2013 for instance, a category in \u2018Offcenter\u2019 group could also be a member of the \u2018Functional\u2019 group (e.g. cigarette). Nev-\n4Instances can refer to fixation locations within a single sketch or within a category\nertheless, the hotspot-based structural grouping of categories demonstrates that eye-fixations on sketches are not merely a random after-effect of the underlying objects\u2019 gestalt. In fact, the statistical structure of the fixation sequences is sometimes sufficient for predicting the underlying category, as we shall see next."}, {"heading": "VII. OBJECT CATEGORIZATION: HOW WELL CAN WE PREDICT CATEGORY FROM FIXATIONS ALONE?", "text": "The high level of IOC observed in Section V suggests a reliable consistency among fixation sequences and raises an intriguing question : Given a fixation sequence, is it possible to predict the category of the sketch it corresponds to, just from fixation locations alone ? In this regard, a good prediction performance would enable finer, object-part based analyses which rely on a correct prediction of the underlying category (See Section IX).\nTo answer the question posed above, we follow the procedure of O\u2019Connell et al. [2]. We hold out the fixation data of one subject for testing and utilize the fixation data from the rest of the subjects (Leave-One-Subject-Out method) to build category-wise fixation maps as described in Section VI. Suppose the subject whose data has been earmarked for testing has viewed M sketches. Let {(x(j)f , y (j) f )}, f = 1, 2, . . . Nj denote the set of fixation locations for the j-th sketch (j = 1, 2, . . .M ). Let {t(j)f } denote the set of corresponding fixation durations. For a given category c, the prediction score is computed from the corresponding marginalized fixation map Mc (Equation 4) as:\nGc(j) =\n\u2211Nj f=1 t (j) f Mc ( x (j) f , y (j) f ) \u2211Nj\nf=1 t (j) f\n(5)\nThe category prediction for the j-th sketch is then computed as c\u2217(j) = argmax\nc Gc(j)\nThe prediction procedure is repeated for all the M sketches viewed by the test subject. The resulting predictions are aggregated category-wise. The overall prediction rates are obtained by averaging the category-wise predictions across the test subjects. The average category-wise prediction rates of the Leave-One-Subject-Out procedure described above can be viewed in Figure 6 for the primed regime. The performance for most categories well exceeds chance (reciprocal of number of primed categories). To determine the effect of durations, we also obtained prediction rates for the condition where duration is ignored (by setting all durations to 1). We found that inclusion of duration information results in a slight performance improvement overall. The inclusion of duration in saliency map construction provided approximately a 5% average improvement in category prediction accuracy for primed regime (Cohens d = 0.54, medium effect). Specifically, the median prediction rate across the categories was 32% (ignoring duration) and 37.2% (including duration). To place these results in perspective, the best prediction rates obtained by O\u2019Connell et al. [2] averages around 35% (only 6 (scene) categories, 22 subjects, 216 photographic images). Given the disproportionate lack of content compared to photographic images, it is quite remarkable that eye-fixation sequences on sketches exhibit a predictable regularity on par with images.\nIn Figure 6, category-level prediction rates for \u2018car\u2019 and \u2018plant\u2019 are good since the corresponding sketches exhibit limited variations in terms of depiction viewpoint and appearance causing consistent spread in fixations compared to other categories \u2013 we refer the interested reader to view the sketch dataset of Eitz et al. [14]. Conversely, the prediction rates for certain categories (\u2018dog\u2019, \u2018sheep\u2019,\u2018train\u2019) are poor since the corresponding sketches exhibit large viewpoint and appearance changes. Also, including fixation durations in saliency map computation does not help \u2018car\u2019 and \u2018plant\u2019 categories since their maps have multiple hotspots (see Figure 5) which lowers the discriminative capability induced by fixation duration."}, {"heading": "A. Ablative Experiments", "text": "We also performed a series of ablative experiments over (a) the regimes \u2013 primed and unprimed5 (b) utilizing the regimes alternately for training/testing and (c) inclusion of duration in fixation map creation. We do not include the results of these experiments in Figure 6 to retain clarity. However, we discovered the following trends: \u2022 Models trained on primed fixations predict better and\nfairly above chance for test fixation sequences regardless of regime (p < 0.005 for primed regime and p < 0.05 for unprimed regime, sign-test) \u2022 Quite a number of categories are misclassified when models are trained on unprimed regime fixation sequences. We believe this is due to confusion arising out an increased number of classes in unprimed regime (160 vs 13), the similarity among categories belonging to the same \u2018fixation map\u2019 group (Section 6), lack of\n5We used fixation data of only those categories and sketches of \u2018unprimed setting\u2019 which were also utilized in the \u2018primed setting\u2019, thus making the comparisons fair.\ncategory cue as in the primed regime and the fact that the number of fixation sequences per category are smaller (38) compared to the primed regime (48). \u2022 Inclusion of duration in fixation maps did not improve prediction performance in most of the ablative combinations."}, {"heading": "VIII. FIXATIONS AND SEMANTIC OBJECT-PARTS", "text": "The category prediction results in the previous section further reinforce the possibility that the fixations on sketches are not mere gestalt but are indicators of a deeper phenomenon. One hypothesis for this deeper phenomenon is the following: In the primed regime (i.e. when object category is made known), subjects fixate to regions corresponding to \u2018signature\u2019 semantic object-parts in a consistent manner, with the fixation visitation order reflecting the relative importance of the parts.\nTo verify this hypothesis, we utilized object-part contour annotations available for the sketches viewed in primed regime [18]. As a general approach, we assigned fixations to the corresponding object-part regions. This assignment of fixations to part regions entails a certain degree of ambiguity due to overlapping part annotations. To minimize this, we used a multi-stage spatial analysis algorithm to determine the assignment of fixations to part regions.\nMeasuring similarity between part visitation sequences: Assigning fixations to object part contours enables us to treat fixation sequences as part-name token sequences6. In general, suppose the i-th part token sequence for a given category C is given by P(i) = {P (i)1 , P (i) 2 , . . . P (i) ni }, i = 1, 2, . . . nC where ni denotes the number of fixations in the i-th fixation sequence and nC denotes the number of fixation sequences corresponding to category C. To determine similarity between pairs of part-name token sequences, we used the Needleman-Wunsch algorithm [34] with a 0 \u2212 1 cost model (Cost(P,Q) = 0 if Part P = Part Q, 1 otherwise) and a gap penalty of 0. The resulting similarity measure lies between 0 and 1. The median pairwise similarities for primed regime categories can be seen in Figure 7 (red-colored plot) and are fairly high (between 0.57 and 0.73).\nSuppose, for a given category C, the similarity between two fixation sequences fCi and f C j is s C i,j . To verify the statistical reliability of our similarity estimation method, we first generate 100 random fixation label sequences whose length matches that of sequence fCj . We then compute the similarity of each of the random sequences with fCi . Suppose the mean similarity value is \u00b5Ci,random and the corresponding standard deviation is \u03c3Ci,random. We compute the z-score (i.e. sCi,j \u2212 \u00b5Ci,random\n\u03c3Ci,random ).\nWe repeat this for all possible fixation sequence pairs of category C and compute the median pairwise random z-score for each category. The z-scores (blue-colored plot in Figure 7) indicate that the distribution of pairwise similarities between part-name token sequences is well separated from the random counterparts \u2013 most of the category-wise similarities exhibit a separation of at least two standard deviations from the mean similarity of random sequences."}, {"heading": "IX. A COMPUTATIONAL MODEL FOR FIXATION-BASED OBJECT PART PREDICTION", "text": "Building upon the encouraging trends in the similarity of eye-fixation sequences at fixation (Section V) and part-label level (Section VIII), we ask : Given a fixation sequence, is it possible to predict the object-part labels of each fixation in the sequence?. Such predictions could enable applications such as object-part contour annotations of freehand sketches [35] from fixations alone7, in analogy with the object-bounding-boxesfrom-eye-fixations approaches described in Papadopoulos et al. [36] and Yun et al. [33]. In addition, the additional part presence information (available for sketches annotated via fixations) can be utilized for refining results of sketch-based image retrieval approaches [37]. The consistency in fixation sequences at sketch and category level certainly suggests that the ordinal position and spatial location of a fixation within a fixation sequence has a high chance of being correlated with the object-part within whose contour the fixation falls. These observations motivate our computational model for fixationbased object part-label prediction, which we describe next.\n6For example, a part token sequence from the airplane category could look like {fuselage,window,window,wing,fuselage,windshield,engine}.\n7This would be considerably less burdensome than annotating object part contours by hand.\nSuppose the object category is C. We divide the entire set of fixation sequences within C randomly into training and test sets. Note that in our current context, we interpret sequences to mean part-label sequences8. Suppose PC = {P1,P2, . . .PnC} represents the part-labels for category C."}, {"heading": "A. Computational Models", "text": "To model the eye fixation object-part visitation sequence, we use a Hidden Markov Model (HMM) \u03bb. Using terminology relevant to HMMs, let us denote the observation sequence as f1:T = (f1, f2, . . . ft, . . . fT ) where ft \u2208 Rd (i.e. continuous observations) and the corresponding sequence of \u03bb\u2019s hidden states as y1:T . The HMM is typically specified by three components \u03bb = (A,B, \u03c0). \u2022 A is the state transition matrix such that Aij = p(yt+1 = j|yt = i)\n\u2022 B is the observation model, i.e. Bt = p(ft|yt) \u2022 \u03c0 is the probability vector of initial states, i.e. \u03c0i = p(y1 = i)\nLet F = (f1, f2, . . . fM ) represent the sequence of fixation features wherein fi denotes features corresponding to the ith fixation. We treat F as observations arising from a hidden sequence L = (l1, l2, . . . lM ) of states. In general, the number of hidden states is a hyper-parameter. But in our case, a natural choice for hidden states is the set of part-labels PC, i.e. li \u2208 PC, 1 \u2264 i \u2264M .\n1) Objective: Our objective can be stated as: For each lt , what is the most likely instantiation given the entire observation sequence F ?\nl\u2217t = argmax lt\u2208PC p(lt|F) (6)\nThis is referred to as pointwise maximum a posteriori (PMAP) estimate. Essentially, PMAP maximizes the posterior probability p(li|A) of each individual hidden state given the entire observation sequence [38]. Note that the objective above is different from conventional Viterbi decoding, i.e. What is the most likely sequence of states given the entire observation sequence F ? L = (L1, L2, . . . LM ) = argmax\n(l1,l2,...lM )\np(l1, l2, . . . ...lM |F) (7)\nOur choice of PMAP is motivated by the fact that PMAP maximizes the expected number of correctly estimated states and hence provides better state estimates overall compared to Viterbi decoding [39].\n2) Fully-supervised HMM training: Note that in our case, the observations F and the corresponding hidden state instantiations y1:T are simultaneously available for training the HMM. Thus, the training of HMM is fully supervised. In such a scenario, the maximum-likelihood training of HMMs reduces to a counting process wherein the observation, transition and initial state models can be modeled and estimated independently [40].\nObservation model: Let Q = Pi, 1 \u2264 i \u2264 nC. For a given training sequence (of part-labels), we first determine\n8By mapping each fixation to the object part within whose boundary the fixation falls, we obtain the part-label sequence.\nthe subset of labels which equal Q. Suppose the j-th fixation label of a given training sequence equals Q. We compute the corresponding ordinal factor rj = NF\u2212j+1NF for the fixation where NF denotes the maximum length among all fixation sequences of the gaze data-set. The ordinal factor captures the notion that the earlier the part-label occurs in a fixation sequence, the more its relative importance within fixation sequences. This choice of ordinal factor also has the effect of normalizing for different fixation sequence lengths since the factor is always 1 for the first fixation. The feature vector aj for the fixation is constructed as fj = (rj , xj , yj) where (xj , yj) denotes the normalized coordinates of the fixation.\nNext, we construct a normalized non-parametric distribution KQ over fjs across all occurrences of the part-label Q in the training data. In particular, we model the distribution via Kernel Density Estimation [41] with the bandwidth automatically selected using single-dimensional likelihood-based search [42]. We repeat this procedure for each Q to obtain our HMM\u2019s observation model Bt = p(ft|yt = Q).\nTransition and Initial state models: In our case, the state transition probabilities Aij and initial state distribution \u03c0i where 1 \u2264 i, j \u2264 nC can be obtained by normalizing the appropriate part-label frequency counts from the training set sequences."}, {"heading": "B. Experimental Setup", "text": "For our experiments, we used 60% of the category\u2019s fixation sequences selected at random to construct the per-part training models. To ensure sufficient training data, we performed data augmentation. For each reference training sequence, we generated 50 augmented sequences wherein the fixation position was randomly perturbed. During this process, we ensured that for each reference fixation, the perturbation resulted in less than 1 degree of visual angle position deviation. For each test sequence, we computed the proportion of correctly predicted part-labels (sequence prediction accuracy). The category-wise part-visitation sequence prediction accuracy was averaged over 10 trials9. To verify that the model\u2019s performance was better than chance, we computed predictions using randomly assigned part-labels to fixations of a sequence."}, {"heading": "C. Alternate models", "text": "In this section, we provide a brief overview of the alternate models we explored for the part-prediction computation model.\n1) DTW: In the DTW model, we utilize the spatial locations of fixations. For each sequence of test fixations, we find the training fixation sequence with the optimal alignment distance d\u2217. In determining the optimality criteria, we use Euclidean metric as the base distance between fixation locations across test and train sequences. Having determined the training sequence with the smallest distance d\u2217, we determine partlabel predictions for the test sequence as follows: Suppose p and q contain the indices of test and \u2018optimal-matching\u2019 train sequences respectively, as determined by the DTW procedure.\n9In each trial, the training set was selected randomly.\nInitially, all of test fixations p are considered unmatched. Each pi is initially assigned the DTW match qj . Additionally, pi is flagged as \u2018provisionally matched\u2019. If pi is not matched to any other point in q, then the \u2018provisional match\u2019 becomes the final match. However, if pi participates in a subsequent match with qk, k > j, then it is matched to qk provided the corresponding fixation location is closer (in Euclidean distance) to the test fixation indexed by pi. Once all the matchings are obtained, the part-label prediction for each pi is obtained as the partlabel corresponding to the training fixation indexed by qj .\n2) RNN: Recurrent Neural Networks (RNN)s have become a popular choice for sequence analysis in recent times, particularly for their ability to capture long-range interactions with the sequence. We evaluated different RNNs across a large variety of parameter (dropout, optimizer) and architecture (number of hidden layers) choices. The result in Figure 8 reflects the best performance out of the aforementioned choices. We also experimented with LSTMs, again with a similar variety of choices, but the results weren\u2019t too different from those obtained using RNNs.\n3) HMM-Viterbi: As mentioned previously, conventional Viterbi decoding procedure utilizes the following objective i.e. What is the most likely sequence of states given the entire observation sequence F ?\nL = (L1, L2, . . . LM ) = argmax (l1,l2,...lM ) p(l1, l2, . . . ...lM |F) (8)"}, {"heading": "D. Results for our computational model", "text": "We compared our HMM-based computation model with alternative models which are commonly used for sequence-based prediction tasks. In particular, we performed comparative evaluation with HMM Viterbi decoding (HMM-Viterbi), Dynamic Time Warping (DTW) and Recurrent Neural Networks (RNN), as described in Section IX-C.\nFor the evaluation procedure, the experimental setup is the same (Section IX-B). To verify that our model\u2019s performance was better than chance, we computed predictions using randomly assigned part-labels to fixations of a sequence. All the aforementioned models perform better than random. However, our PMAP-based model\u2019s performance is superior to the alternative models (see Figure 8) and distinctly better than random part-label assignment model \u2013 on average, at least 61% of the sequence\u2019s labels are correctly predicted by our model across object categories.\nBy nature of its computations, DTW aims to stitch together optimal local alignments into a final optimal alignment. DTWbased matching typically penalizes long-range matches. Moreover, the final part-predictions are deterministically based on a \u2018single\u2019 best alignment. In contrast, the other methods are probabilistic in nature and can capture long-range interactions. Of these, we initially expected RNNs to outperform the HMM-based methods since they are not constrained by the Markov assumption. However, the results show otherwise. We believe one reason could be the small amount of data (even after augmentation) on a per-category basis and lack of variety therein. Another reason could be that our PMAP method maximizes the number of correctly predicted labels in\nexpectation, which is not the case with vanilla RNNs. It would be interesting to explore PMAP-like formulations for RNNs and train them to maximize the number of correctly predicted parts. The work of Jiang et al. [22], related to learning an explicit prediction model for eye-fixation sequences, would be a worthy alternative to explore as well."}, {"heading": "X. DISCUSSION", "text": "The motivation for studying eye fixations on objects arises from the works of Einhauser et al. [43] and Nuthmann et al. [44] who posit that humans more likely than not, represent and understand visual content in terms of objects (although contradictory results have been reported with non-fixation based approaches [45]). Our observations (Section VI) indicate a category-level regularity in the eye-fixation patterns on sketch objects under a free-viewing paradigm. This is in line with similar studies on photographic images [2].\nfMRI-based studies have shown that the neural response of the visual system to images of a scene and line drawings depicting the scene category is virtually the same [46]. But is it also possible that eye fixation mechanisms for photos and sketches are similar? What could be the reasons they are similar or dissimilar? In our study, we have not delved into these aspects for reasons of scope. Current theories and computational models of saliency rely on features specific to photo images such as color-contrast and edge-contrast [47]. Such saliency-based explanations seem insufficient to explain our findings since such features do not exist for freehand line sketches. Therefore, at least from the viewpoint of saliency, we believe that our eye-gaze data lays the ground for uncovering connections between eye fixation patterns on objects in photographic images and their sketched versions. Such connections can help understand depiction invariant attributes and aspects of visual object representations. Combined with data from similar studies on other modalities [13], our sketch gaze data can contribute towards new insights into cross-modal (photo, brush art, line drawing) object representations [21]. Further investigation into our discovery of these interesting patterns by the community could lead to more general computational\nmodels of human visual saliency which can explain fixations regardless of depiction (photos or sketches).\nOur experiments for predicting object categories (Section VI) and semantic object parts (Section IX) can be viewed as an attempt to uncover higher-level semantics from eye fixations. For example, Subramanian et al. [48] utilize eye gaze patterns to analyze semantics, albeit those related to social and affective scenes. Part-based representations of objects, semantic or otherwise, are well supported by multiple studies in neuroscience and computer vision [49]\u2013[51]. Specifically, Taylor et al. [52] suggest that humans tap into generic concepts of objects, including linguistic propositions such as semantic object-parts, when analyzing a scene. Furthermore, studies by Palmer [53] have shown that when parts correspond to a \u2018good\u2019 segmentation of a figure (e.g. named-part contour annotations), the speed and accuracy of responses related to queries on figure attributes improves significantly. Combined, these observations lend support for our use of semantic objectparts as a lens to view fixation patterns.\nThe relationship between part-based object representations and eye fixations has been examined via imagery-based studies. In such studies, subjects are typically presented with a stimulus which is subsequently removed. The subjects are then asked to imagine or recall aspects of the stimulus previously presented to them. These studies [54] suggest that the eye fixations are an external manifestation which aim to verify an implicit, internal part-based representation for the object. Therefore, the consistency of eye fixation order would mirror the consistency of the representation. In this respect, the strong degree of within-category similarity among partvisitation sequences (Section VIII) for sketches is remarkable. In fact, this suggests that eye fixations on objects consistently aim to verify a part-based representation even though, unlike imagery studies, the stimulus (sketch object) has not been seen before and possibly has a different degree of detail and image quality10. On a related note, Xu et al. [13] perform a study which evaluates the ability of eye gaze statistics in predicting attributes of objects such as shape and size.\nIn addition, other studies suggest that the order in which the fixations happen reflects not only consistency, but the relative importance among the parts [12]. We use this observation and build upon encouraging results from part-visitation sequence similarities (Section VIII) to show that the underlying (and often visually absent) semantic-part can be predicted from the knowledge of the corresponding fixation\u2019s position within the fixation sequence and image alone ( Section IX). Our partprediction model shows promise in this regard and we believe improved models (e.g. exploiting part-whole object semantics) can enable object-part contour annotations of freehand sketches [35] as mentioned previously (Section IX). On a deeper level, this ability to perform part-prediction extends the consistency of fixations previously seen at category (Section VI) and sketch level (Section V) to meaningful albeit implicit sub-regions (parts) of the object sketch as well.\nThe importance of fixation duration has been repeatedly\n10The internal part-based model for the object could have been constructed from a photographic image.\nindicated in eye-fixation and saliency literature [2], [48], [55]. In fact, Henderson [56] argues that a complete model of scene perception needs to actively take duration into account, in addition to fixation location. However, duration is typically not considered essential in most saliency prediction approaches [57]. In our case, we found that duration seemed to make a difference for the better only for predicting fixation sequence category in primed regime (Section VIII). Analysis of our fixation data reveals a negative correlation between the average fixation duration and number of fixations per sketch across categories (\u22120.81 in the primed regime and \u22120.62 in the unprimed version). Given the fixed per-sketch time budget during the study, we posit that subjects possibly trade off between number of fixations and time spent per fixation, thereby rendering duration information irrelevant for fixationrelated processes such as fixation map generation.\nOur motivations for studying sketches under the \u2018primed\u2019 and \u2018unprimed\u2019 regimes were to determine whether the act of priming (a) changes the fixation sequences in a noticeable manner and (b) helps validate the consistency of implicit object part visitation via fixations. Our analysis reveals that priming helps in creation of better quality models for category prediction and fixation map generation (as attested by Figure 3)."}, {"heading": "XI. CONCLUSION", "text": "In this paper, we have presented the first large-category scale exploration of eye-fixation data on freehand sketches. As a result of our study, an eye fixation database for 3904 handdrawn sketches across 160 visual object categories has been obtained for the benefit of the community. Analysis of data from our user study has shown that eye-fixations on freehand sketches are not mere gestalt \u2013 they are strongly conditioned by the categorical aspect of sketch content (object). In fact, this conditioning is strong enough for the object category to be predicted from the fixation sequence alone. Even more dramatically, our results show that the sequencing of fixations corresponds to an implicit sequence of parts that constitute the object although the parts themselves may not be delineated in the sketch stroke data. In our work, we have also shown how the consistency in visitations can be used to build object-specific computational models capable of predicting the semantic object parts which underlie fixations.\nMore broadly, our sketch object eye-fixations data lays the ground for uncovering connections between eye fixation patterns on objects in photographic images [36] and their sketched versions. Such connections can help understand depiction invariant aspects of visual object representations and with data from similar studies on other modalities, provide new insights into cross-modal (photo, brush art, line drawing) object representations [58].\nTo the best of our knowledge, current theories and computational models of saliency, which implicitly assume photographs or real-world scenes as input, seem insufficient to explain our findings. Therefore, further investigation into our discovery of these interesting patterns by the community could lead to more general computational models of human visual\nsaliency which can explain fixations regardless of depiction (photos or sketches)."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank Prof. Veni Madhavan (Indian Institute of Science), for access to SMI Eye Tracker equipment. The authors would also like to thank NVIDIA for their donation of Tesla K40 GPU."}], "references": [{"title": "Learning to predict where humans look", "author": ["T. Judd", "K. Ehinger", "F. Durand", "A. Torralba"], "venue": "IEEE International Conference on Computer Vision, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Dissociation of salience-driven and content-driven spatial attention to scene category with predictive decoding of gaze patterns", "author": ["T.P. O\u2019Connell", "D.B. Walther"], "venue": "Journal of Vision, vol. 15, no. 5, p. 20, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting eye fixations using convolutional neural networks", "author": ["N. Liu", "J. Han", "D. Zhang", "S. Wen", "T. Liu"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Prediction of search targets from fixations in open-world settings", "author": ["H. Sattar", "S. M\u00fcller", "M. Fritz", "A. Bulling"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of scores, datasets, and models in visual saliency prediction", "author": ["A. Borji", "H. Tavakoli", "D. Sihite", "L. Itti"], "venue": "IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Centreof-gravity fixations in visual search: When looking at nothing helps to find something", "author": ["D. Venini", "R.W. Remington", "G. Horstmann", "S.I. Becker"], "venue": "Journal of Ophthalmology, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Eye movement control during visual object processing: effects of initial fixation position and semantic constraint.", "author": ["J.M. Henderson"], "venue": "Canadian Journal of Experimental Psychology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "The influence of image characteristics on image recognition: a comparison of photographs and line drawings", "author": ["S. Heuer"], "venue": "Aphasiology, vol. 30, no. 8, pp. 943\u2013961, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Parallel object activation and attentional gating of information: Evidence from eye movements in the multiple object naming paradigm.", "author": ["E.R. Schotter", "V.S. Ferreira", "K. Rayner"], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Cognitive determinants of fixation location during picture viewing.", "author": ["G.R. Loftus", "N.H. Mackworth"], "venue": "Journal of Experimental Psychology: Human perception and performance,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1978}, {"title": "The effects of semantic consistency on eye movements during complex scene viewing.", "author": ["J.M. Henderson", "P.A. Weeks Jr.", "A. Hollingworth"], "venue": "Journal of experimental psychology: Human perception and performance,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "The relation between gaze behavior and categorization: Does where we look determine what we see?", "author": ["M.O. Hartendorp", "S. Van der Stigchel", "I. Hooge", "J. Mostert", "T. de Boer", "A. Postma"], "venue": "Journal of Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Predicting human gaze beyond pixels", "author": ["J. Xu", "M. Jiang", "S. Wang", "M.S. Kankanhalli", "Q. Zhao"], "venue": "Journal of vision, vol. 14, no. 1, pp. 28\u201328, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "How do humans sketch objects?", "author": ["M. Eitz", "J. Hays", "M. Alexa"], "venue": "ACM Trans. Graph. (Proc. SIGGRAPH),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations", "author": ["S.S.S. Kruthiventi", "K. Ayush", "R. Venkatesh Babu"], "venue": "CoRR, vol. abs/1510.02927, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Cat2000: A large scale fixation dataset for boosting saliency research", "author": ["A. Borji", "L. Itti"], "venue": "CoRR, vol. abs/1505.03581, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Saliency unified: A deep architecture for simultaneous eye fixation prediction and salient object segmentation", "author": ["S.S. Kruthiventi", "V. Gudisa", "J.H. Dholakiya", "R. Venkatesh Babu"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Analyzing structural characteristics of object category representations from their semanticpart distributions", "author": ["R.K. Sarvadevabhatla", "R. Venkatesh Babu"], "venue": "ACM Multimedia Conference, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Where do people draw lines?", "author": ["F. Cole", "A. Golovinskiy", "A. Limpaecher", "H.S. Barros", "A. Finkelstein", "T. Funkhouser", "S. Rusinkiewicz"], "venue": "in ACM Transactions on Graphics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Impact of product design representation on customer judgment", "author": ["T.N. Reid", "E.F. MacDonald", "P. Du"], "venue": "Journal of Mechanical Design, vol. 135, no. 9, p. 091008, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Swiden: Convolutional neural networks for depiction invariant object recognition", "author": ["R.K. Sarvadevabhatla", "S. Surya", "S.S.S. Kruthiventi", "R. Venkatesh Babu"], "venue": "ACM Multimedia Conference, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to predict sequences of human visual fixations", "author": ["M. Jiang", "X. Boix", "G. Roig", "J. Xu", "L. Van Gool", "Q. Zhao"], "venue": "IEEE transactions on neural networks and learning systems, vol. 27, no. 6, pp. 1241\u20131252, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Visuomotor characterization of eye movements in a drawing task", "author": ["R. Coen-Cagli", "P. Coraggio", "P. Napoletano", "O. Schwartz", "M. Ferraro", "G. Boccignone"], "venue": "Vision Research, vol. 49, no. 8, pp. 810 \u2013 818, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Segmentation and accuracy in copying and drawing: Experts and beginners", "author": ["J. Tchalenko"], "venue": "Vision Research, vol. 49, no. 8, pp. 791 \u2013 800, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Chapter 29 - absence of scene context effects in object detection and eye gaze capture", "author": ["L. Gareze", "J.M. Findlay"], "venue": "Eye Movements. Oxford: Elsevier, 2007, pp. 617 \u2013 637.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Looking at anything that is green when hearing frog: How object surface colour and stored object colour knowledge influence language-mediated overt attention", "author": ["F. Huettig", "G.T. Altmann"], "venue": "The Quarterly Journal of Experimental Psychology, vol. 64, no. 1, pp. 122\u2013145, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Attentional orienting and scene semantics", "author": ["P. De Graef", "J. Lauwereyns", "K. Verfaillie"], "venue": "Psychological Reports, no. 268, p. 2, 2000.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Fixations on objects in natural scenes: dissociating importance from salience", "author": ["B.M. t Hart", "H.C.E.F. Schmidt", "C. Roth", "W. Einhauser"], "venue": "Frontiers in psychology, vol. 4, p. 455, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Object-based saccadic selection during scene perception: Evidence from viewing position effects", "author": ["M. Pajak", "A. Nuthmann"], "venue": "Journal of vision, vol. 13, no. 5, pp. 2\u20132, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Computational visual attention systems and their cognitive foundations: A survey", "author": ["S. Frintrop", "E. Rome", "H.I. Christensen"], "venue": "ACM Transactions on Applied Perception (TAP), vol. 7, no. 1, p. 6, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Eye of the dragon: Exploring discriminatively minimalist sketch-based abstractions for object categories", "author": ["R.K. Sarvadevabhatla", "R. Venkatesh Babu"], "venue": "ACM Multimedia Conference, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Sun: A bayesian framework for saliency using natural statistics", "author": ["L. Zhang", "M.H. Tong", "T.K. Marks", "H. Shan", "G.W. Cottrell"], "venue": "Journal of Vision, vol. 8, no. 7, p. 32, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Studying relationships between human gaze, description, and computer vision", "author": ["K. Yun", "Y. Peng", "D. Samaras", "G.J. Zelinsky", "T. Berg"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 739\u2013746.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "A general method applicable to the search for similarities in the amino acid sequence of two proteins", "author": ["S.B. Needleman", "C.D. Wunsch"], "venue": "Journal of Molecular Biology, vol. 48, no. 3, pp. 443 \u2013 453, 1970.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1970}, {"title": "Data-driven segmentation and labeling of freehand sketches", "author": ["Z. Huang", "H. Fu", "R.W.H. Lau"], "venue": "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2014), 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Training object class detectors from eye tracking data", "author": ["D.P. Papadopoulos", "A.D.F. Clarke", "F. Keller", "V. Ferrari"], "venue": "European Conference on Computer Vision, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Intra-category sketchbased image retrieval by matching deformable part models", "author": ["Y. Li", "T. Hospedales", "Y.-Z. Song", "S. Gong"], "venue": "British Machine Vision Conference, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal decoding of linear codes for minimizing symbol error rate (corresp.)", "author": ["L. Bahl", "J. Cocke", "F. Jelinek", "J. Raviv"], "venue": "IEEE Transactions on Information Theory, vol. 20, no. 2, pp. 284\u2013287, 1974.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1974}, {"title": "Bridging viterbi and posterior decoding: a generalized risk approach to hidden path inference based on hidden markov models", "author": ["J. Lember", "A.A. Koloydenko"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1\u201358, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence labeling: Generative and discriminative approaches", "author": ["H. Erdogan"], "venue": "iCMLA 2010 Tutorial. http://www.icmla-conference.org/ icmla10/CFP Tutorial files/hakan.pdf.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Introduction to Statistical Pattern Recognition (2Nd Ed.)", "author": ["K. Fukunaga"], "venue": "San Diego, CA, USA: Academic Press Professional,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1990}, {"title": "Algorithms for maximumlikelihood bandwidth selection in kernel density estimators", "author": ["J.M. Leiva-Murillo", "A. Art\u00e9s-Rodr\u0131\u0301guez"], "venue": "Pattern Recognition Letters, vol. 33, no. 13, pp. 1717\u20131724, 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Objects predict fixations better than early saliency", "author": ["W. Einhauser", "M. Spain", "P. Perona"], "venue": "Journal of Vision, vol. 8, no. 14, pp. 1\u201326, 2008.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Object-based attentional selection in scene viewing", "author": ["A. Nuthmann", "J.M. Henderson"], "venue": "Journal of vision, vol. 10, no. 8, p. 20, 2010.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual scenes are categorized by function.", "author": ["M.R. Greene", "C. Baldassano", "A. Esteva", "D.M. Beck", "L. Fei-Fei"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "Simple line drawings suffice for functional mri decoding of natural scene categories", "author": ["D.B. Walther", "B. Chai", "E. Caddigan", "D.M. Beck", "L. Fei-Fei"], "venue": "Proceedings of the National Academy of Sciences, vol. 108, no. 23, pp. 9661\u20139666, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Saliency filters: Contrast based filtering for salient region detection", "author": ["F. Perazzi", "P. Kr\u00e4henb\u00fchl", "Y. Pritch", "A. Hornung"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2012.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Can computers learn from humans to see better?: inferring scene semantics from viewers\u2019 eye movements", "author": ["R. Subramanian", "V. Yanulevskaya", "N. Sebe"], "venue": "ACM Multimedia Conference, 2011.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2011}, {"title": "13-part-based representations of visual shape and implications for visual cognition", "author": ["M. Singh", "D.D. Hoffman"], "venue": "Advances in psychology, vol. 130, pp. 401\u2013459, 2001.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2001}, {"title": "Object representations in ventral and dorsal visual streams: fmri repetition effects depend on attention and part\u2013whole configuration", "author": ["V. Thoma", "R.N. Henson"], "venue": "Neuroimage, vol. 57, no. 2, pp. 513\u2013525, 2011.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Cviu special issue on parts and attributes: Mid-level representation for object recognition, scene classification and object detection", "author": ["T. Darrell", "V. Ferrari", "F. Jurie", "V. Lepetit"], "venue": "Computer Vision and Image Understanding, vol. 138, pp. 85 \u2013, 2015.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2015}, {"title": "An algebra for the analysis of object encoding", "author": ["C.W. Tyler", "L.T. Likova"], "venue": "NeuroImage, vol. 50, no. 3, pp. 1243\u20131250, 2010.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Hierarchical structure in perceptual representation", "author": ["S.E. Palmer"], "venue": "Cognitive Psychology, vol. 9, no. 4, pp. 441 \u2013 474, 1977.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1977}, {"title": "Scrutinizing visual images: The role of gaze in mental imagery and memory", "author": ["B. Laeng", "I.M. Bloem", "S. DAscenzo", "L. Tommasi"], "venue": "Cognition, vol. 131, no. 2, pp. 263\u2013283, 2014.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Fixation location and fixation duration as indices of cognitive processing", "author": ["D.E. Irwin"], "venue": "The interface of language, vision, and action: Eye movements and the visual world, pp. 105\u2013134, 2004.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2004}, {"title": "Regarding scenes", "author": ["J.M. Henderson"], "venue": "Current directions in psychological science, vol. 16, no. 4, pp. 219\u2013222, 2007.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2007}, {"title": "Quantifying the contribution of low-level saliency to human eye movements in dynamic scenes", "author": ["L. Itti"], "venue": "Visual Cognition, vol. 12, pp. 1093\u2013 1123, 2005.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2005}, {"title": "The cross-depiction problem: Computer vision algorithms for recognising objects in artwork and in photographs", "author": ["H. Cai", "Q. Wu", "T. Corradi", "P. Hall"], "venue": "CoRR, vol. abs/1505.00110, 2015.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Multiple studies [1]\u2013[5] have demonstrated that this fixation mechanism is bottom-up, predominantly driven by image content and richness of detail (color, texture etc.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "Multiple studies [1]\u2013[5] have demonstrated that this fixation mechanism is bottom-up, predominantly driven by image content and richness of detail (color, texture etc.", "startOffset": 21, "endOffset": 24}, {"referenceID": 5, "context": "nothing\u201d phenomenon [6]\u2013[9], wherein the eye fixations on the same stimulus by multiple subjects fall on empty regions, yet exhibit enough regularity to make gaze-based inferences.", "startOffset": 20, "endOffset": 23}, {"referenceID": 8, "context": "nothing\u201d phenomenon [6]\u2013[9], wherein the eye fixations on the same stimulus by multiple subjects fall on empty regions, yet exhibit enough regularity to make gaze-based inferences.", "startOffset": 24, "endOffset": 27}, {"referenceID": 9, "context": "semantics [10] and the regularity in rest of the fixations is a statistical anomaly.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "However, a more intriguing explanation is that these empty region fixations aim to implicitly verify the overall consistency of the scene content depicted in the sketch [11], [12].", "startOffset": 169, "endOffset": 173}, {"referenceID": 11, "context": "However, a more intriguing explanation is that these empty region fixations aim to implicitly verify the overall consistency of the scene content depicted in the sketch [11], [12].", "startOffset": 175, "endOffset": 179}, {"referenceID": 13, "context": "[14] used in our eye-fixation user study.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "predicted from gaze patterns alone [2], [8].", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "predicted from gaze patterns alone [2], [8].", "startOffset": 40, "endOffset": 43}, {"referenceID": 12, "context": "Do the gaze patterns exhibit correlation with any object-level attributes such as semantic-parts? [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "To examine these questions and related issues, we conducted an eye fixation study on a large database containing handdrawn sketches of objects across 160 categories [14].", "startOffset": 165, "endOffset": 169}, {"referenceID": 14, "context": "(3904 compared to the previous largest set of 200) can be used in benchmarking generalized1 saliency prediction models [15]\u2013[17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 16, "context": "(3904 compared to the previous largest set of 200) can be used in benchmarking generalized1 saliency prediction models [15]\u2013[17].", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "\u2022 We map eye fixation sequences to semantic object-part label sequences by utilizing part contour annotations of sketches [18].", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "underlying category in 3D model renderings [19], art displays,", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "[20] use eye-gaze data to interpret customer selections among sketch-like consumer product renderings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "the complexity of the study design [20].", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "Combined with data from similar studies on other modalities [13], our sketch gaze data can contribute towards new insights into cross-modal (photo, brush art, line drawing)", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "object representations [21].", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "The study of eye-fixations on images is a very active research area in the inter-related fields of neuroscience and computer vision [1]\u2013[5], [22].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "The study of eye-fixations on images is a very active research area in the inter-related fields of neuroscience and computer vision [1]\u2013[5], [22].", "startOffset": 136, "endOffset": 139}, {"referenceID": 21, "context": "The study of eye-fixations on images is a very active research area in the inter-related fields of neuroscience and computer vision [1]\u2013[5], [22].", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "The first category of studies use gaze tracking to understand how people copy and draw line-drawings and simple shapes [23], [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 23, "context": "The first category of studies use gaze tracking to understand how people copy and draw line-drawings and simple shapes [23], [24].", "startOffset": 125, "endOffset": 129}, {"referenceID": 24, "context": "perception of photographic image content and corresponding line-drawing representations [25], [26].", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "perception of photographic image content and corresponding line-drawing representations [25], [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 26, "context": "In the third category, gaze tracking is used to characterize semantic plausibility of objects in line-drawings of scenes [27].", "startOffset": 121, "endOffset": 125}, {"referenceID": 6, "context": "The fourth category of studies use gaze tracking to explore sketch-like depictions of objects [7]\u2013[9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "The fourth category of studies use gaze tracking to explore sketch-like depictions of objects [7]\u2013[9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 27, "context": "Although objects make the scenes meaningful in many instances [28], [29], their role has been studied in a limited con-", "startOffset": 62, "endOffset": 66}, {"referenceID": 28, "context": "Although objects make the scenes meaningful in many instances [28], [29], their role has been studied in a limited con-", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] summarize prevailing theories of the relationship between attention and object recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Recently, Ali and Itti [16] constructed a large-scale dataset", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "for sketch images originally belonging to the same sketch dataset we have used [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 30, "context": "For our analysis, we used 3904 sketches spread across 160 object categories studied in the work of Sarvadevabhatla and Babu [31].", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "[14] used to construct epitomic versions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "However, the images were chosen only from 13 selected categories for which semantic object-part annotations exist [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "sketch) displaying the sketch\u2019s category as a text string representing the category label as provided by the creators of the freehand sketch dataset [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "Fixation maps summarize the fixation information averaged over multiple subjects viewing the same image (sketch) and are used as ground-truth for evaluating saliency prediction methods [1], [3], [15], [16].", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "Fixation maps summarize the fixation information averaged over multiple subjects viewing the same image (sketch) and are used as ground-truth for evaluating saliency prediction methods [1], [3], [15], [16].", "startOffset": 190, "endOffset": 193}, {"referenceID": 14, "context": "Fixation maps summarize the fixation information averaged over multiple subjects viewing the same image (sketch) and are used as ground-truth for evaluating saliency prediction methods [1], [3], [15], [16].", "startOffset": 195, "endOffset": 199}, {"referenceID": 15, "context": "Fixation maps summarize the fixation information averaged over multiple subjects viewing the same image (sketch) and are used as ground-truth for evaluating saliency prediction methods [1], [3], [15], [16].", "startOffset": 201, "endOffset": 205}, {"referenceID": 1, "context": "[2] which we summarize next.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "(1) Typical approaches normalize F \u2032 with respect to the maximum value or to lie between 0 and 1 [1], [16].", "startOffset": 97, "endOffset": 100}, {"referenceID": 15, "context": "(1) Typical approaches normalize F \u2032 with respect to the maximum value or to lie between 0 and 1 [1], [16].", "startOffset": 102, "endOffset": 106}, {"referenceID": 1, "context": "[2] suggest standardizing F \u2032 to obtain a zero mean, unit standard deviation map F .", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[32]", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "A high value for IOC indicates consistency among the fixation sequences and is commonly observed for natural images, particularly with a central object [1].", "startOffset": 152, "endOffset": 155}, {"referenceID": 32, "context": "[33], \u201cfunctional\u201d categories (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "observed for regular image datasets as well [1], [16].", "startOffset": 44, "endOffset": 47}, {"referenceID": 15, "context": "observed for regular image datasets as well [1], [16].", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] averages around 35% (only 6 (scene) categories, 22 subjects, 216 photographic images).", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "To verify this hypothesis, we utilized object-part contour annotations available for the sketches viewed in primed regime [18].", "startOffset": 122, "endOffset": 126}, {"referenceID": 33, "context": "To determine similarity between pairs of part-name token sequences, we used the Needleman-Wunsch algorithm [34] with a 0 \u2212 1 cost model (Cost(P,Q) = 0 if Part P = Part Q, 1 otherwise) and a gap penalty of 0.", "startOffset": 107, "endOffset": 111}, {"referenceID": 34, "context": "Such predictions could enable applications such as object-part contour annotations of freehand sketches [35] from fixations alone7, in analogy with the object-bounding-boxesfrom-eye-fixations approaches described in Papadopoulos et", "startOffset": 104, "endOffset": 108}, {"referenceID": 35, "context": "[36] and Yun et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "In addition, the additional part presence information (available for sketches annotated via fixations) can be utilized for refining results of sketch-based image retrieval approaches [37].", "startOffset": 183, "endOffset": 187}, {"referenceID": 37, "context": "Essentially, PMAP maximizes the posterior probability p(li|A) of each individual hidden state given the entire observation sequence [38].", "startOffset": 132, "endOffset": 136}, {"referenceID": 38, "context": "Our choice of PMAP is motivated by the fact that PMAP maximizes the expected number of correctly estimated states and hence provides better state estimates overall compared to Viterbi decoding [39].", "startOffset": 193, "endOffset": 197}, {"referenceID": 39, "context": "In such a scenario, the maximum-likelihood training of HMMs reduces to a counting process wherein the observation, transition and initial state models can be modeled and estimated independently [40].", "startOffset": 194, "endOffset": 198}, {"referenceID": 40, "context": "In particular, we model the distribution via Kernel Density Estimation [41] with the bandwidth automatically selected using single-dimensional likelihood-based search [42].", "startOffset": 71, "endOffset": 75}, {"referenceID": 41, "context": "In particular, we model the distribution via Kernel Density Estimation [41] with the bandwidth automatically selected using single-dimensional likelihood-based search [42].", "startOffset": 167, "endOffset": 171}, {"referenceID": 21, "context": "[22], related to learning an explicit prediction model for eye-fixation sequences, would", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] and Nuthmann et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44] who posit that humans more likely than not, represent and understand visual content in terms of objects (although", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "contradictory results have been reported with non-fixation based approaches [45]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "This is in line with similar studies on photographic images [2].", "startOffset": 60, "endOffset": 63}, {"referenceID": 45, "context": "fMRI-based studies have shown that the neural response of the visual system to images of a scene and line drawings depicting the scene category is virtually the same [46].", "startOffset": 166, "endOffset": 170}, {"referenceID": 46, "context": "Current theories and computational models of saliency rely on features specific to photo images such as color-contrast and edge-contrast [47].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "Combined with data from similar studies on other modalities [13], our sketch gaze data", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "can contribute towards new insights into cross-modal (photo, brush art, line drawing) object representations [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 47, "context": "[48] utilize eye gaze", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "Part-based representations of objects, semantic or otherwise, are well supported by multiple studies in neuroscience and computer vision [49]\u2013[51].", "startOffset": 137, "endOffset": 141}, {"referenceID": 50, "context": "Part-based representations of objects, semantic or otherwise, are well supported by multiple studies in neuroscience and computer vision [49]\u2013[51].", "startOffset": 142, "endOffset": 146}, {"referenceID": 51, "context": "[52] suggest that humans tap into generic concepts", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "Furthermore, studies by Palmer [53] have shown that when parts correspond to a \u2018good\u2019 segmentation of a figure (e.", "startOffset": 31, "endOffset": 35}, {"referenceID": 53, "context": "These studies [54] suggest that the eye fixations are an external manifestation which aim to verify an implicit, internal part-based representation for the object.", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "[13] perform a study which evaluates the ability of eye gaze statistics in predicting attributes of objects such as shape and size.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "importance among the parts [12].", "startOffset": 27, "endOffset": 31}, {"referenceID": 34, "context": "exploiting part-whole object semantics) can enable object-part contour annotations of freehand sketches [35] as mentioned previously (Section IX).", "startOffset": 104, "endOffset": 108}, {"referenceID": 1, "context": "indicated in eye-fixation and saliency literature [2], [48], [55].", "startOffset": 50, "endOffset": 53}, {"referenceID": 47, "context": "indicated in eye-fixation and saliency literature [2], [48], [55].", "startOffset": 55, "endOffset": 59}, {"referenceID": 54, "context": "indicated in eye-fixation and saliency literature [2], [48], [55].", "startOffset": 61, "endOffset": 65}, {"referenceID": 55, "context": "In fact, Henderson [56] argues that a complete model of", "startOffset": 19, "endOffset": 23}, {"referenceID": 56, "context": "However, duration is typically not considered essential in most saliency prediction approaches [57].", "startOffset": 95, "endOffset": 99}, {"referenceID": 35, "context": "More broadly, our sketch object eye-fixations data lays the ground for uncovering connections between eye fixation patterns on objects in photographic images [36] and their sketched versions.", "startOffset": 158, "endOffset": 162}, {"referenceID": 57, "context": "new insights into cross-modal (photo, brush art, line drawing) object representations [58].", "startOffset": 86, "endOffset": 90}], "year": 2017, "abstractText": "The study of eye gaze fixations on photographic images is an active research area. In contrast, the image subcategory of freehand sketches has not received as much attention for such studies. In this paper, we analyze the results of a freeviewing gaze fixation study conducted on 3904 freehand sketches distributed across 160 object categories. Our analysis shows that fixation sequences exhibit marked consistency within a sketch, across sketches of a category and even across suitably grouped sets of categories. This multi-level consistency is remarkable given the variability in depiction and extreme image content sparsity that characterizes hand-drawn object sketches. In our paper, we show that the multi-level consistency in the fixation data can be exploited to (a) predict a test sketch\u2019s category given only its fixation sequence and (b) build a computational model which predicts part-labels underlying fixations on objects. We hope that our findings motivate the community to deem sketch-like representations worthy of gaze-based studies vis-avis photographic images.", "creator": "LaTeX with hyperref package"}}}