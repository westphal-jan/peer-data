{"id": "1703.06914", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Applying Deep Machine Learning for psycho-demographic profiling of Internet users using O.C.E.A.N. model of personality", "abstract": "in less modern era, each internet user leaves enormous amounts of auxiliary digital residuals ( footprints ) by using a variety of on - line services. all this data is already collected and stored for many years. in recent works, accuracy was demonstrated that it's possible to apply simple machine learning methods to analyze irrelevant digital footprints and safely create psychological profiles of individuals. however, while more works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied to practical studies. we have sensed that using advanced deep machine learning methods vastly considerably increase the accuracy of predictions. we started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural computational. how we compared understanding power of studied models again made conclusions about its performance. finally, we made hypotheses how prediction accuracy april be further improved. as result of this activity, researchers provide full source modeling used in the experiments for all interested researchers and practitioners in corresponding github repository. we believe that eliminating deep machine learning for psychological simulations may have having enormous impact reducing functioning society ( for good or worse ) and providing explicit knowledge code further our research we hope we intensify further research by some wider circle of scholars.", "histories": [["v1", "Tue, 7 Mar 2017 09:27:21 GMT  (814kb,D)", "https://arxiv.org/abs/1703.06914v1", "arXiv admin note: text overlap witharXiv:1207.0580by other authors"], ["v2", "Wed, 5 Jul 2017 12:16:24 GMT  (814kb,D)", "http://arxiv.org/abs/1703.06914v2", null]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1207.0580by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.CY", "authors": ["iaroslav omelianenko"], "accepted": false, "id": "1703.06914"}, "pdf": {"name": "1703.06914.pdf", "metadata": {"source": "CRF", "title": "Applying Deep Machine Learning for psycho-demographic profiling of Internet users using O.C.E.A.N. model of personality", "authors": ["Iaroslav Omelianenko"], "emails": ["yaric@newground.com.ua"], "sections": [{"heading": null, "text": "1. Introduction\nBy using various on-line services, modern Internet user leaves an enormous amount of digital tracks in the form of server logs, user-generated content, etc. All these information bits meticulously saved by on-line service providers create the vast amount of digital footprints for almost every Internet user. In recent research [Lambiotte, R., and Kosinski, M., 2014], it was demonstrated that by applying simple machine learning methods it was possible to find statistical correlations between digital footprints and psycho-demographic profile of individuals. The considered psycho-demographic profile comprise of psychometric scores based on five-factor O.C.E.A.N. model of personality [Goldberg et. al, 2006] and demographic scores such as Age, Gender and the Political Views. The O.C.E.A.N. is an abbreviation for Openness (Conservative and Traditional - Liberal and Artistic), Conscientiousness (Impulsive and Spontaneous - Organized and Hard Working), Extroversion (Contemplative - Engaged with outside world), Agreeableness (Competitive - Team working and Trusting), and Neuroticism (Laid back and Relaxed - Easily Stressed and Emotional).\nIn this work we decided to test whether applying advanced machine learning methods to analyze digital footprints of Internet users can outperform results of previous research conducted by M. Kosinski: Mining Big Data to Extract Patterns and Predict Real-Life Outcomes[Kosinski et. al, 2016]. For our experiments we used data corpus comprising of\nar X\niv :1\n70 3.\n06 91\n4v 2\n[ cs\n.L G\n] 5\nJ ul\n2 01\npsycho-demographic scores of individuals and their digital footprints in form of Facebook likes. The data corpus used in experiments kindly provided by M. Kosinski through corresponding web site: http://dataminingtutorial.com.\nWe started our experiments with building simple machine learning models based on linear/logistic regression methods as proposed by M. Kosinski in [Kosinski et. al, 2016]. By training and execution of simple models we estimated basic predictive performance of machine learning methods against available data set. Then we continued our experiments with advanced machine learning methods based on shallow and deep neural network architectures.\nThe full source code of our experiments provided in form of GitHub repository: https://github.com/NewGround-LLC/ psistats\nThe source code is written in R programming language [R Core Team, 2015] which is highly optimized for statistical data processing and allows to apply advanced deep machine learning algorithms by bridging with Google Brain\u2019s TensorFlow framework [Google Brain Team, 2015].\nThis paper is organized as follows: In Section 2, we describe data corpus structure, and necessary data preprocessing steps to be applied. It is followed in Section 3 by details about how to build and run simple prediction models based on linear and logistic regression with results of their execution. In Section 4, we provide details how to create and execute advanced prediction models based on artificial neural networks. Finally, in Section 6 we compare the performance of different machine learning methods studied in this work and draw conclusions about the predictive power of studied machine learning models.\n2. Data Corpus Preparation\nIn this section, we consider the creation of input data corpus from the publicly available data set, and it\u2019s preprocessing to allow further analysis by selected machine learning algorithms.\n2.1. Data Set Description\nThe data set kindly provided by M. Kosinski and used in this work contains psycho-demographic profiles of nu = 110 728 Facebook users and nL = 1 580 284 of associated Facebook likes. For simplicity and manageability, the sample is limited to U.S. users [Kosinski et. al, 2016]. The following three files can be downloaded from corresponding web site - http://dataminingtutorial.com:\n1. users.csv: contains psycho-demographic user profiles. It has nu = 110 728 rows (excluding the row holding column names) and nine columns: anonymised user ID, gender (\"0\" for male and \"1\" for female), age, political views (\"0\" for Democrat and \"1\" for Republican), and scores of five-factor model of personality [Goldberg et. al, 2006].\n2. likes.csv: contains anonymized IDs and names of nL = 1 580 284 Facebook (FB) Likes. It has two columns: ID and name.\n3. users-likes.csv: contains the associations between users and their FB Likes, stored as user-Like pairs. It has nuL = 10 612 326 rows and two columns: user ID and Like ID. An existence of a user-Like pair implies that a given user had the corresponding Like on their profile.\n2.2. Data pre-processing\nThe raw data preprocessing is an important step in machine learning analysis which will significantly reduce the time needed for analysis and result in better prediction power of created machine learning models.\nThe detailed description of data corpus preprocessing steps applied during this research given hereafter.\nConstruction of sparse users-likes matrix and matrix trimming\nTo use provided data corpus in machine learning analysis it should be transformed first into optimal format. Taking into account properties of provided data corpus (user can like specific topic only once and most users has generated small amount of likes) its natural to present it as sparse matrix where most of data points is zero (the resulting matrix density is about 0,006% - see Table 1). The sparse matrix data structure is optimized to perform numeric operations on sparse data and considerably reduce computational costs compared to the dense matrix used for same data set.\nAfter users-likes sparse matrix creation, it was trimmed by removing rare data points. As a result, the significantly reduced data corpus was created, imposing even lower demands on computational resources and more useful for manual analysis to extract specific patterns. The descriptive statistics of users-likes matrix before and after trimming present in Table 1.\nThe users-likes matrix can be constructed from provided three comma-separated files with the help of accompanying script written in R language: src/preprocessing.R. To use this script make sure that input_data_dir variable in the src/config.R points to the root directory where sample data corpus in the form of .CSV files were unpacked.\nTo start preprocessing and trimming, run the following command from terminal in the project\u2019s root directory:\n$ R s c r i p t ./ s r c /preprocess ing . R \\\\ \u2212u 150 \u2212 l 50\nwhere: -u is the minimum number of users per like uL, and -l is the minimum number of likes per user Lu to keep in resulting matrix.\nThe values for the minimum number of users per like uL and the minimum number of likes per user Lu was selected based on recommendations given in [Kosinski et. al, 2016]. We have experimented with other set of parameters as well (uL = 20 and Lu = 2), but accuracy of trained prediction models degraded as result.\nData imputation of missed values\nThe raw data corpus has missed values in column with \u2019Political\u2019 dependent variable data. Before building the prediction model for this dependent variable, it is advisable to impute missed values. In this work, we applied multivariate imputation using LDA method with number of multiple imputations equals to m = 5 to fill missed values as described in [van Buuren, Groothuis-Oudshoorn, 2011].\nThe data imputation performed by the same src/preprocessing.R script, as part of users-likes matrix creation routine. The summary statistics for data imputation applied to political variable, presented in Table 2.\nof the total variance that is attributable to the missing data \u03bb = B+ B m\nT )."}, {"heading": "Dimensionality reduction with SVD", "text": "After two previous steps, the resulting users-likes sparse matrix still has a considerable number of features per data sample: 8 523 of feature columns. To make it even more maintainable, we considered applying singular value decomposition [Golub, G. H., and Reinsch, C. 1970], representing eigendecomposition-based methods, projecting a set of data points into a set of dimensions. As mentioned in [Kosinski et. al, 2016], reducing the dimensionality of data corpus has number of advantages:\n\u2022 With reduced features space we can use fewer number of data samples, as it is required by most of the machine learning analysis algorithms that number of data samples exceeds the number of features (input variables) \u2022 It will reduce risk of overfitting and increase statistical power of results \u2022 It will remove multicollinearity and redundancy in data corpus by grouping related features (variables) in single\ndimension \u2022 It will significantly reduce required computational power and memory requirements \u2022 And finally it makes it easier to analyze data by hand over small set of dimensions as opposite to hundreds or thousands of\nseparate features\nTo apply SVD analysis against generated users-likes matrix run the following command from project\u2019s root directory:\n$ R s c r i p t ./ s r c /svd_varimax . R \\\\ \u2212\u2212svd_dimensions 50 \u2212\u2212apply_varimax true\nwhere: \u2013svd_dimensions is the number of SVD dimensions for projection, and \u2013apply_varimax is the flag to indicate whether varimax rotation should be applied afterwards."}, {"heading": "Factor rotation analysis", "text": "The factor rotation analysis methodology can be used to further simplify SVD dimensions and increase their interpretability by mapping the original multidimensional space into a new, rotated space. Rotation approaches can be orthogonal (i.e., producing uncorrelated dimensions) or oblique (i.e., allowing for correlations between rotated dimensions).\nIn this work during data preprocessing we applied one of the most popular orthogonal rotations - varimax. It minimizes both the number of dimensions related to each variable and the number of variables related to each dimension, thus improving the interpretability of the data by human analysts.\nFor more details on rotation techniques, see [Abdi, H., 2003].\n3. Regression analysis\nThere is an abundance of methods developed to build prediction machine learning models suitable for analysis of large data sets. They ranging from sophisticated methods such as Deep Machine Learning [Goodfellow et al., 2016], probabilistic graphical models, or support vector machines [Cortes & Vapnik, 1995], to much simpler, such as linear and logistic regressions [Yan, Su, 2009].\nStarting with simple methods is a common practice allowing the creation of good baseline prediction model with minimal computational efforts. The results obtained from these models can be used later to debug and estimate the quality of results obtained from advanced models.\n3.1. Regression model description and model-specific data preprocessing\nIn our data corpus, we have eight dependent variables with psycho-demographic scores of individuals to be predicted. Among those variables, six have continuous values, and two has categorical values. To build the prediction model for variables with continuous values we applied linear regression analysis and for variables with categorical values - logistic regression analysis.\nHereafter we describe the rationale for selection of appropriate regression analysis methods as well as a description of model-specific data preprocessing needed."}, {"heading": "Linear regression analysis", "text": "The linear regression is an approach for modeling the relationship between continuous scalar dependent variable y and one or more explanatory (or independent) variables denoted X. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression\n[David A. Freedman, 2009]. In linear regression, the relationships are modeled using linear predictor function y = \u0398TX whose unknown model parameters \u0398 estimated from the input data. Such models are called linear models [Hilary L. Seal, 1967].\nWe used linear regression to build prediction models for analysis of six continuous dependent variables in given data corpus: Age, Openness, Conscientiousness, Extroversion, Agreeableness, and Neuroticism"}, {"heading": "Logistic regression analysis", "text": "The logistic regression is a regression model where the dependent variable is categorical [David A. Freedman, 2009]. It measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function \u03c3(x) = 11+e\u2212x , which is the cumulative logistic distribution [Rodriguez, G., 2007].\nWe considered only specialized binary logistic regression because categorical dependent variables found in our data corpus (Gender and Political Views) are binominal, i.e. have only two possible types, \"0\" and \"1\"."}, {"heading": "Cross-Validation", "text": "We applied k-fold cross-validation to help with avoiding model overfitting when evaluating accuracy scores of prediction models. In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k \u2212 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged to produce a single estimation. The advantage of this method is that all observations are used for both training and validation, and each observation is used for validation exactly once. The 10-fold cross-validation is commonly used, but in general, k remains an unfixed parameter [Kohavi, Ron, 1995]."}, {"heading": "Dimensionality reduction", "text": "To reduce the number of features (input variables) in the data corpus was applied singular value decomposition (SVD) with subsequent varimax factor rotation analysis. The number of the varimax-rotated singular value decomposition dimensions (K) has a considerable impact on the accuracy of model predictions. To find an optimal number of SVD dimensions, we performed analysis of relationships between K and accuracy of model predictions by creating series of regression models for different values of K. Then we plotted prediction accuracy of regression models against chosen number of K SVD dimensions. Typically the prediction accuracy grows rapidly within lower ranges of K and may start decreasing once the number of dimensions becomes large. Selecting value of K that marks the end of a rapid growth of prediction accuracy values usually offers decent interpretability of the input data topics. In general, the larger K values often results in better predictive power when preprocessed data corpus further analyzed with specific machine learning algorithm [Zhang, Marron, Shen,& Zhu, 2007]. See Figure 1 for results of our experiments.\nTo start SVD analysis run the following command from terminal in the project\u2019s root directory:\n$ R s c r i p t ./ s r c / a n a l y s i s . R\nThe resulting plots will be saved as \"Rplots.pdf\" file in the project root and include two plots:\n\u2022 the plot with relationships between the accuracy of prediction models for each dependent variable and the number of the varimax-rotated SVD dimensions used for dimensionality reduction (Figure 1). With this plot,\nit\u2019s easy to visually find an optimal number of K SVD dimensions to maximize predicting power of regression model per particular dependent variable. \u2022 the heat map of correlations between scores of digital footprints of individuals projected on specific number of varimax-rotated SVD dimensions and each dependent variable (Figure 2). This plot can be used to find most correlated dependent variables visually. Later, it will be shown that predictive models for dependent variables with higher correlation have better prediction accuracy.\n3.2. Implementing simple prediction models and its accuracy evaluation\nThe given data corpus has eight dependent variables for which to build prediction models. The simple machine learning methods such as regression analysis mostly applied to estimate single dependent variable. But in case when multiple dependent variables need to be estimated the specialized methods of multivariate regression analysis can be used. Taking into account that our dependent variables have different types (continuous and nominal) which require different regression analysis methods to be applied, we decided to build separate regression models per each dependent variable. The metric to evaluate accuracy of prediction model is related to the regression method used in the model. In this research we have considered following metrics:\n\u2022 the accuracy of prediction model applied to the continuous dependent variable will be measured as Pearson product-moment correlation [Gain, 1951] \u2022 the accuracy of prediction model applied to the bi-nominal dependent variable will be measured as area under the receiver-operating characteristic curve coefficient (AUC) [Sing, Sander, Beerenwinkel, Lengauer, 2005]\nBefore executing models make sure that data corpus already preprocessed as described in Subsection: \"Construction of sparse users-likes matrix and matrix trimming\"\nWhen data corpus is ready, the following command can be executed to start linear/logistic regression models building and its predictive performance evaluation (run command from terminal in the project\u2019s root directory):\n$ R s c r i p t ./ s r c / r e g r e s s i o n _ a n a l y s i s . R\nThe results will be saved into the file \u201dout/pred_accuracy_regr.txt\u201d. The prediction accuracy of regression models for data corpus trimmed to contain 150 users-per-like and 50 likes-per-user and varimax-rotated against K = 50 SVD dimensions presented in Table 3.\nFrom the Table 3 it can be seen that prediction accuracy of linear/logistic regression models differs per each dependent variable. Furthermore for most variables the accuracy is too low to be applied in real-life predictions. The most accurate predictions made for Gender, Age, and Political view with Openness following after. That correlates well with our previous analysis of SVD correlations heat map (see Figure 2). In general only prediction model for Gender is accurate enough to be useful in real-life applications. Thus, simple linear/logistic regression models can not be used to accurately estimate psycho-demographic profiles of Internet users based only on their Facebook likes.\nIn following sections, we will test if applying advanced deep machine learning methods can improve prediction accuracy any further.\n4. Fully Connected Feed Forward Artificial Neural Networks\nIn this work, we considered multilayer fully connected feed-forward neural networks (NN) for building simple (shallow) and deep machine learning NN models. The fully connected NN characterized by interconnectedness of all units of one layer with all units of the layer before it in the graph. The feed-forward NN is not allowed to have cycles from latter layers back to the earlier.\nHereafter we will describe Artificial NN architectures evaluated and prediction accuracy results obtained.\n4.1. The Shallow Feed Forward Artificial NN evaluation\nA Shallow Neural Network (SNN) is an artificial neural network with one hidden layer of units (neurons) between input and output layers. Its hidden units (neurons) take inputs from input units (columns of input data matrix) and feeds into the output units, where linear or categorical analysis performed. To mimic biological neuron, hidden units in the neural network apply specific non-linear activation functions. One of the popular activation functions is ReLU non-linearity that we considered as activation function for the units of hidden layers in studied network architectures [Nair, Hinton, 2010]. It improves information disentangling and linear separability producing efficient variable size representation of model\u2019s data. Furthermore ReLU activation is computationally cheaper: there is no need for computing the exponential function as in case of sigmoid activation [Glorot, Bordes, Bengio, 2011].\nTo reduce overfitting was applied dropout regularization with drop-probability 0.5, which means that each hidden unit if randomly omitted from the network with the specified probability. This helps to break the rare dependencies that can occur in the training data [Hinton, G. et al., 2012].\nThe NN architecture was build using Google Brain\u2019s TensorFlow library - an open source software library for numerical computation using data flow graphs. [Google Brain Team, 2015] Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The resulting two layer (one hidden layer) ANN\u2019s architecture graph depicted in Figure 5\nAs the loss function to be optimized was selected Mean Squared Error (MSE) with Adam optimizer (Adaptive Moment Estimation) to estimate it\u2019s minimum. The Adam optimizer was selected for it\u2019s proven advantages some of them are that the magnitudes of parameter updates are invariant to rescaling of the gradient, its step sizes are approximately bounded by the step size hyper-parameter, it does not require a stationary objective, it works with sparse gradients, and it naturally performs a form of step size annealing [Kingma, Ba, 2014]. The batch size was selected to be 100. We also tested training with batch size 10 but found no statistically relevant prediction accuracy difference between runs with either batch size but reducing batch size considerably increased training time of NN models.\nIt was found that optimal number of SVD dimensions for Shallow ANN is K = 128, with number of units in the hidden layer 512, and learning rate \u03b3 = 0.0001. See Table 4.\nThe optimal learning rate was selected by comparing ratio of survived hidden units with non zero ReLU activation and monitoring loss function over iterations plot (Figure 3). With presented hyper-parameters it was achieved maximum ratio 0.57 of zero ReLU activations for largest learning rate value, which is acceptable taking into account tendency of ReLU to saturate at zero during gradient back propagation stage when strong gradients applied, due to high learning rates [Nair, Hinton, 2010]. The maximal number of iterations (50 000) and correspondingly number of training epochs was selected based on loss function plot (Figure 3). With series of experiments it was selected as optimal the learning rate value \u03b3 = 0.0001, which gives smooth loss function, stable acceptable number of \"dead\" neurons after ReLU activation (ReLU zero activations ratio is about 0.5), and best prediction scores among runs (see Table 5).\nThe accompanying launch script provided to conduct experiments under Unix:\n$ ./ eval_mlp_1 . sh u l _ s v d _ m a t r i x _ f i l e\nwhere: ul_svd_matrix_file the path to the preprocessed users-likes matrix with dimension of feature columns reduced as described in Construction of sparse users-likes matrix and matrix trimming\nThe source code of shallow ANN implementation used for experiment can be found in src/mlp.R of accompanying\nGitHub repository.\n4.2. Feed Forward Deep Learning Networks Architecture Evaluation\nA Deep Neural Network (DNN) is an artificial neural network with multiple hidden layers of units between the input and the output layers. The first hidden layer will take inputs from each of the input units and the subsequent hidden layer will take inputs from the outputs of previous hidden layer\u2019s units [Christopher M. Bishop, 1995]. Similar to shallow, deep neural network can model complex non-linear relationships. But added extra layers enable composition of features from lower layers, giving the potential of modeling complex data with fewer units than a similarly performing shallow network [Bengio, Yoshua, 2009]. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer [LeCun, Bengio, Hinton, 2015].\nAs with shallow ANNs, many issues can arise with training of the deep neural networks, with two most common problems - are overfitting and computation time [Tetko, Livingstone, Luik, 1995].\nHereafter we will consider DNN architectures studied in this research."}, {"heading": "The three-layer Deep Learning Network Architecture Evaluation", "text": "Our experiments with deep learning networks we started with simple DNN architecture comprising of two hidden layers with ReLU activation and dropout after each hidden layer with keep probability of 0.5. The experimental network graph depicted in Figure 7.\nWe started with learning rate \u03b3 = 0.0001 which resulted in best prediction performance for shallow ANN and attempted series of experiments to estimate optimal value of K SVD dimensions. The optimal prediction accuracy of DNN model was achieved with K = 256 SVD dimensions and two hidden layers comprising of [512, 256] units correspondingly. Similar prediction accuracy can be achieved with K = 1024 SVD dimensions and [2048, 1024] units per layer with learning rate \u03b3 = 10\u22125. But we have not considered later set of hyper-parameters due to its extra computational overhead while giving statistically same results as former set. The results of experiments presented in Table 6.\nAfter finding optimal values of K SVD dimensions and number of units per hidden layer, we have conducted series of experiments to determine optimal initial learning rate value for found hyper-parameters. It was experimentally confirmed that initial learning rate value \u03b3 = 0.0001 is the optimal one. See Table 7\nIn our experiments, we applied exponential learning rate decay with the number of steps before decay 10 000 and decay rate of 0.96. Such scheme has positive effect on network convergence speed due to the learning rate annealing effect, which gives a system the ability to escape from poor local minima to which it might have been initialized [Kirkpatrick et al., 1983]. We selected batch size 100 as optimal for this experiment.\nThe accompanying launch script provided to conduct experiments under Unix:\n$ ./ eval_dnn . sh u l _ s v d _ m a t r i x _ f i l e\nwhere: ul_svd_matrix_file is the path to the preprocessed users-likes matrix with dimension of feature columns reduced as described in Construction of sparse users-likes matrix and matrix trimming\nThe source code of DNN with two hidden layers implementation used for experiment can be found in src/dnn.R of accompanying GitHub repository."}, {"heading": "The four-layer Deep Learning Network Architecture Evaluation", "text": "This architecture comprise of three hidden layers with ReLU activation and one output linear layer. All network layers are fully connected and network architecture is feed-forward as in all previous NN experiments. The experimental network graph depicted in Figure 8.\nWe have tested two dropout regularization schemes: (a) dropout applied after each hidden layer with keep-probability 0.5; (b) dropout applied after each second hidden layer with keep probability calculated by formula: pd = i2n , (where: n - number of dropouts, i - current dropout index).\nIt was found that former scheme gives better results than the last one. Thus for final evaluation run, we applied dropout regularization after each hidden layer.\nBased on our previous experiments with more shallow networks we decided to start with following hyper-parameters: learning rate \u03b3 = 0.0001, learning rate decay step 10 000 with decay rate 0.96, K = 128 SVD dimensions, and hidden layers configuration - [256, 128, 128].\nTo find the optimal number of K SDV dimensions and hidden layers configurations we have conducted series of experiments trying various combinations. The heuristic applied to select the number of units per hidden layer is rather naive and assumes that with dropout probability of 0.5 half of the units will be saturated to zero at ReLU activation. Thus we decided to have the number of units in the first hidden layer to be twice as much as the number of features in input data (K). The results of experiments present in Table 8\nDespite the fact that best accuracy was achieved with K = 1024 SDV dimensions and the number of units in hidden layers [2048, 1024, 1024] respectively, it was detected slight model overfitting during training/validation with these hyper-parameters applied. So, we have decided to stop increasing K and number of hidden units as it will give no further gain in prediction accuracy against validation data set and even may lead to worsening of validation accuracy with greater overfitting level. (See Figure 6)\nThe accompanying launch script provided to conduct experiments under Unix:\n$ ./ eval_3dnn . sh u l _ s v d _ m a t r i x _ f i l e\nwhere: ul_svd_matrix_file is the path to the preprocessed users-likes matrix with dimension of feature columns reduced as described in Construction of sparse users-likes matrix and matrix trimming\nThe source code of DNN with three hidden layers implementation used for experiments can be found in src/3dnn.R of accompanying GitHub repository.\n5. Future Work\nWith conducted experiments, we have found that prediction accuracy differs considerably among machine learning methods studied and best results was achieved by using advanced methods based on neural networks architectures. At the same time, the prediction accuracy per individual dependent variable also differs per particular prediction model and selected set of hyper-parameters. From experimental results it can be seen that specific combination of NN architecture with given set of hyper-parameters are best suited for one dependent variable but worsened predictive power for some of the others.\nIn future studies, it is interesting to investigate this dependency and build separate NN models per each dependent variable as it was done in case of simple machine learning methods (see Section: \u2019Regression analysis\u2019)\nAlso, it seems promising to apply methodology described in [Ba, Caruana, 2014] which provide evidence that shallow networks are capable of learning the same functions as deep learning networks, and often with the same number of parameters as the deep learning networks. In [Ba, Caruana, 2014] it was shown that with wide shallow networks it\u2019s possible to reach the state-of-the-art performance of deep models and reduce training time by the factor of 10 using parallel computational resources (GPU).\n6. Conclusion\nFrom our experiments we found that weak correlation exists between most of O.C.E.A.N. psychometric scores of individuals and collected Facebook likes associated with them. Either simple or advanced machine learning algorithms that we have tested provided poor prediction accuracy for almost all O.C.E.A.N. personality traits. It seems not feasible yet to use machine learning models to accurately estimate psychometric profile of an individual based only on Facebook likes. But we believe that by complementing Facebook likes of user with additional data points, it is possible to greatly improve accuracy of machine learning prediction models for psychometric profile estimation.\nAt the same time, we have found a strong correlation with demographic traits of individuals such as Age, Gender, and the Political Views with their Facebook activity (likes). Our experiments confirmed that its possible to use advanced machine learning methods to build the correct demographic profile of an individual based only on collected Facebook likes.\nAmong all studied machine learning prediction models the best overall accuracy was achieved with Shallow Neural Network architecture. We hypothesize that this may be the result of its ability to learn best parameters space function within an optimal number of SVD dimensions applied to the input data set (users-likes sparse matrix). Adding extra hidden layers either leads to model overfitting when the number of SVD dimensions is too high, or underfitting when the number of SVD dimensions is too low. Also, it\u2019s interesting to notice that performance of shallow networks and deep learning networks with two hidden layers are comparable, while with introducing of third and more hidden\nlayers it drops significantly. Thus we can conclude that no further improvements can be gained with extra hidden layers introduced. See Table 9.\nGathered experimental data confirms that advanced machine learning methods based on variety of studied artificial neural networks architectures outperform simple machine learning methods (linear and logistic regression) described in previous research conducted by M. Kosinski [Kosinski et. al, 2016]. We believe that further prediction accuracy improvements can be achieved by building separate advanced machine learning models per dependent variable, which is the subject of our future research activities.\nA. The SNN evaluation plots\nThe following pages provide plots and diagrams related to evaluation of two layer (shallow) feed forward artificial neural network with one fully connected hidden layer and linear output layer.\nB. Deep NN evaluation plots\nThe following pages provide plots and diagrams related to evaluation of studied deep neural networks.\nReferences\n[Kosinski et. al, 2016] Michal Kosinski, Yilun Wang, Himabindu Lakkaraju, and Jure Leskovec, (2016). Mining Big Data to Extract Patterns and Predict Real-Life Outcomes. Psychological Methods 2016, Vol. 21, No. 4, 493-506. DOI: 10.1037/met0000105\n[Lambiotte, R., and Kosinski, M., 2014] Lambiotte, R., and Kosinski, M., (2014). Tracking the digital footprints of personality. Proceedings of the Institute of Electrical and Electronics Engineers, 102, 1934-1939. DOI: 10.1109/JPROC.2014.2359054\n[Goldberg et. al, 2006] Goldberg, L. R., Johnson, J. A., Eber, H. W., Hogan, R., Ashton, M. C., Cloninger, C. R., and Gough, H. G. (2006). The International Personality Item Pool and the future of public-domain personality measures. Journal of Research in Personality, 40, 84-96. DOI: 10.1016/j.jrp.2005.08.007\n[Golub, G. H., and Reinsch, C. 1970] Golub, G. H., and Reinsch, C. (1970). Singular value decomposition and least squares solutions. Numerische Mathematik, 14, 403-420. DOI: 10.1007/BF02163027\n[Abdi, H., 2003] Abdi, H. (2003). Factor rotations in factor analyses. In M. Lewis-Beck, A. E. Bryman, & T. F. Liao (Eds.), The SAGE encyclopedia of social science research methods (pp. 792-795). Thousand Oaks, CA: SAGE.\n[Goodfellow et al., 2016] Ian Goodfellow and Yoshua Bengio and Aaron Courville, (2016). Deep learning. Manuscript in preparation. Retrieved from http://www.deeplearningbook.org/\n[Daphne Koller, 2012] Daphne Koller, (2010-2012) Probabilistic Graphical Models. Stanford University. Retrieved from http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course= ProbabilisticGraphicalModels\n[Cortes & Vapnik, 1995] Cortes, C. & Vapnik, V. (1995). Support-vector networks. Machine Learning. 20 (3):273-297. DOI: 10.1007/BF00994018\n[Yan, Su, 2009] Xin Yan, Xiao Gang Su, (2009), Linear Regression Analysis: Theory and Computing, World Scientific, pp. 1-2, ISBN 9789812834119\n[David A. Freedman, 2009] David A. Freedman (2009). Statistical Models: Theory and Practice. Cambridge University Press, p. 26.\n[Hilary L. Seal, 1967] Hilary L. Seal (1967). The historical development of the Gauss linear model. Biometrika. 54 (1/2): 1-24. DOI: 10.1093/biomet/54.1-2.1\n[Rodriguez, G., 2007] Rodriguez, G. (2007). Lecture Notes on Generalized Linear Models. pp. Chapter 3, page 45. Retrieved from http://data.princeton.edu/wws509/notes/\n[Sing, Sander, Beerenwinkel, Lengauer, 2005] Sing, T., Sander, O., Beerenwinkel, N., & Lengauer, T. (2005). ROCR: Visualizing classifier performance in R. Bioinformatics, 21, 3940-3941. DOI: 10.1093/bioinformatics/bti623\n[Gain, 1951] Gain, A. K. (1951). The frequency distribution of the product moment correlation coefficient in random samples of any size draw from non-normal universes. Biometrika. 38: 219-247. DOI: 10.1093/biomet/38.1-2.219\n[Kohavi, Ron, 1995] Kohavi, Ron (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence. San Mateo, CA: Morgan Kaufmann. 2 (12): 1137-1143. CiteSeerX 10.1.1.48.529\n[Zhang, Marron, Shen,& Zhu, 2007] Zhang, L., Marron, J., Shen, H., & Zhu, Z. (2007). Singular value decomposition and its visualization. Journal of Computational and Graphical Statistics, 16, 833-854.\n[van Buuren, Groothuis-Oudshoorn, 2011] Stef van Buuren and Karin Groothuis-Oudshoorn (2011). mice: Multivariate Imputation by Chained Equations in R. Journal of Statistical Software 45 (3) American Statistical Association. Retrieved from http://doc.utwente.nl/78938/\n[Rubin DB, 1987] Rubin DB (1987). Multiple Imputation for Nonresponse in Surveys. John Wiley & Sons, New York.\n[Christopher M. Bishop, 1995] Christopher M. Bishop (1995). Neural Networks for Pattern Recognition, Oxford University Press, Inc. New York, NY, USA c\u00a91995 ISBN:0198538642\n[Bengio, Yoshua, 2009] Bengio, Yoshua (2009). Learning Deep Architectures for AI. Foundations and Trends in Machine Learning. 2 (1): 1-127. DOI: 10.1561/2200000006\n[LeCun, Bengio, Hinton, 2015] LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey (2015). Deep learning. Nature. 521: 436\u00e2A\u0306S\u0327444. DOI:10.1038/nature14539\n[Tetko, Livingstone, Luik, 1995] Tetko, I. V.; Livingstone, D. J.; Luik, A. I. (1995). Neural network studies. 1. Comparison of Overfitting and Overtraining. J. Chem. Inf. Comput. Sci. 35 (5): 826-833. DOI: 10.1021/ci00027a006\n[Hinton, G. et al., 2012] Hinton, Geoffrey E.; Srivastava, Nitish; Krizhevsky, Alex; Sutskever, Ilya; Salakhutdinov, Ruslan R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580v1\n[Nair, Hinton, 2010] Vinod Nair and Geoffrey Hinton (2010). Rectified linear units improve restricted Boltzmann machines. ICML. Retrieved from http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_ NairH10.pdf\n[Glorot, Bordes, Bengio, 2011] Xavier Glorot, Antoine Bordes, Yoshua Bengio (2011). Deep Sparse Rectifier Neural Networks. JMLR W&CP 15:315-323 Retrieved from http://jmlr.org/proceedings/papers/v15/glorot11a. html\n[Kingma, Ba, 2014] Diederik P. Kingma; Lei Jimmy Ba (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980\n[Ba, Caruana, 2014] Lei Jimmy Ba, Rich Caruana (2014). Do Deep Nets Really Need to be Deep? arXiv preprint arXiv:1312.6184\n[Kirkpatrick et al., 1983] S. Kirkpatrick, C. D. Gelatt Jr., M. P. Vecchi (1983). Optimization by Simulated Annealing. Science, 13 May 1983: Vol. 220, Issue 4598, pp. 671-680 DOI: 10.1126/science.220.4598.671\n[R Core Team, 2015] R Core Team, (2015). R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from http://www.R-project.org/\n[Google Brain Team, 2015] Google Brain Team, (2015). TensorFlow is an open source software library for numerical computation using data flow graphs. Retrieved from https://www.tensorflow.org"}], "references": [{"title": "Mining Big Data to Extract Patterns and Predict Real-Life Outcomes", "author": ["Kosinski et"], "venue": "Psychological Methods 2016,", "citeRegEx": "et.,? \\Q2016\\E", "shortCiteRegEx": "et.", "year": 2016}, {"title": "Tracking the digital footprints of personality", "author": ["R. Lambiotte", "M. Kosinski", "R. 2014] Lambiotte"], "venue": "Proceedings of the Institute of Electrical and Electronics Engineers,", "citeRegEx": "Lambiotte et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lambiotte et al\\.", "year": 2014}, {"title": "The International Personality Item Pool and the future of public-domain personality measures", "author": ["L.R. Goldberg", "J.A. Johnson", "H.W. Eber", "R. Hogan", "M.C. Ashton", "C.R. Cloninger", "H.G. Gough"], "venue": "[Goldberg", "citeRegEx": "Goldberg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2006}, {"title": "Singular value decomposition and least squares solutions", "author": ["G.H. Golub", "Reinsch", "G.H.C. 1970] Golub", "C. Reinsch"], "venue": "Numerische Mathematik,", "citeRegEx": "Golub et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1970}, {"title": "Factor rotations in factor analyses", "author": ["H. Abdi"], "venue": null, "citeRegEx": "Abdi,? \\Q2003\\E", "shortCiteRegEx": "Abdi", "year": 2003}, {"title": "Deep learning. Manuscript in preparation", "author": ["Goodfellow et al", "2016] Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Probabilistic Graphical Models. Stanford University. Retrieved from http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course= ProbabilisticGraphicalModels", "author": ["Daphne Koller"], "venue": null, "citeRegEx": "Koller,? \\Q2012\\E", "shortCiteRegEx": "Koller", "year": 2012}, {"title": "Linear Regression Analysis: Theory and Computing, World Scientific, pp", "author": ["Yan", "Su", "2009] Xin Yan", "Xiao Gang Su"], "venue": null, "citeRegEx": "Yan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2009}, {"title": "Statistical Models: Theory and Practice", "author": ["David A. Freedman", "2009] David A. Freedman"], "venue": null, "citeRegEx": "Freedman and Freedman,? \\Q2009\\E", "shortCiteRegEx": "Freedman and Freedman", "year": 2009}, {"title": "ROCR: Visualizing classifier performance", "author": ["Sing", "Sander", "Beerenwinkel", "Lengauer", "T. 2005] Sing", "O. Sander", "N. Beerenwinkel", "T. Lengauer"], "venue": "in R. Bioinformatics,", "citeRegEx": "Sing et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sing et al\\.", "year": 2005}, {"title": "The frequency distribution of the product moment correlation coefficient in random samples of any size draw from non-normal universes", "author": ["A.K. Gain"], "venue": "[Gain,", "citeRegEx": "Gain,? \\Q1951\\E", "shortCiteRegEx": "Gain", "year": 1951}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["Kohavi", "Ron"], "venue": "Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Kohavi and Ron,? \\Q1995\\E", "shortCiteRegEx": "Kohavi and Ron", "year": 1995}, {"title": "Singular value decomposition and its visualization", "author": ["Zhang", "Marron", "Shen", "Zhu", "L. 2007] Zhang", "J. Marron", "H. Shen", "Z. Zhu"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Zhang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2007}, {"title": "mice: Multivariate Imputation by Chained Equations in R", "author": ["van Buuren", "Groothuis-Oudshoorn", "2011] Stef van Buuren", "Karin"], "venue": "Groothuis-Oudshoorn", "citeRegEx": "Buuren et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Buuren et al\\.", "year": 2011}, {"title": "Neural Networks for Pattern Recognition, Oxford University Press, Inc", "author": ["Christopher M. Bishop", "1995] Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop and Bishop,? \\Q1995\\E", "shortCiteRegEx": "Bishop and Bishop", "year": 1995}, {"title": "Learning Deep Architectures for AI", "author": ["Bengio", "Yoshua"], "venue": "Foundations and Trends in Machine Learning", "citeRegEx": "Bengio and Yoshua,? \\Q2009\\E", "shortCiteRegEx": "Bengio and Yoshua", "year": 2009}, {"title": "Neural network studies", "author": ["Tetko", "Livingstone", "Luik", "1995] Tetko", "I. V", "D.J. Livingstone", "A.I. Luik"], "venue": "J. Chem. Inf. Comput. Sci", "citeRegEx": "V. et al\\.,? \\Q1995\\E", "shortCiteRegEx": "V. et al\\.", "year": 1995}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "G. et al", "2012] Hinton", "Geoffrey E", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov"], "venue": null, "citeRegEx": "E. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "E. et al\\.", "year": 2012}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Nair", "Hinton", "2010] Vinod Nair", "Geoffrey Hinton"], "venue": "ICML. Retrieved from http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Deep Sparse Rectifier Neural Networks. JMLR W&CP 15:315-323 Retrieved from http://jmlr.org/proceedings/papers/v15/glorot11a", "author": ["Glorot", "Bordes", "Bengio", "2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": null, "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba", "2014] Diederik P. Kingma", "Lei Jimmy Ba"], "venue": null, "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Do Deep Nets Really Need to be Deep", "author": ["Ba", "Caruana", "2014] Lei Jimmy Ba", "Rich Caruana"], "venue": null, "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Optimization by Simulated Annealing", "author": ["Kirkpatrick et al", "1983] S. Kirkpatrick", "C.D. Gelatt Jr.", "M.P. Vecchi"], "venue": "Science, 13 May 1983:", "citeRegEx": "al. et al\\.,? \\Q1983\\E", "shortCiteRegEx": "al. et al\\.", "year": 1983}], "referenceMentions": [{"referenceID": 10, "context": "In this research we have considered following metrics: \u2022 the accuracy of prediction model applied to the continuous dependent variable will be measured as Pearson product-moment correlation [Gain, 1951] \u2022 the accuracy of prediction model applied to the bi-nominal dependent variable will be measured as area under the receiver-operating characteristic curve coefficient (AUC) [Sing, Sander, Beerenwinkel, Lengauer, 2005] Before executing models make sure that data corpus already preprocessed as described in Subsection: \"Construction of sparse users-likes matrix and matrix trimming\" When data corpus is ready, the following command can be executed to start linear/logistic regression models building and its predictive performance evaluation (run command from terminal in the project\u2019s root directory): $ R s c r i p t .", "startOffset": 190, "endOffset": 202}], "year": 2017, "abstractText": "In the modern era, each Internet user leaves enormous amounts of auxiliary digital residuals (footprints) by using a variety of on-line services. All this data is already collected and stored for many years. In recent works, it was demonstrated that it\u2019s possible to apply simple machine learning methods to analyze collected digital footprints and to create psycho-demographic profiles of individuals. However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks. Then we compared prediction power of studied models and made conclusions about its performance. Finally, we made hypotheses how prediction accuracy can be further improved. As result of this work, we provide full source code used in the experiments for all interested researchers and practitioners in corresponding GitHub repository. We believe that applying deep machine learning for psycho-demographic profiling may have an enormous impact on the society (for good or worse) and provides means for Artificial Intelligence (AI) systems to better understand humans by creating their psychological profiles. Thus AI agents may achieve the human-like ability to participate in conversation (communication) flow by anticipating human opponents\u2019 reactions, expectations, and behavior. By providing full source code of our research we hope to intensify further research in the area by the wider circle of scholars.", "creator": "LaTeX with hyperref package"}}}