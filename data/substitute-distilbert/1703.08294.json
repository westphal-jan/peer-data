{"id": "1703.08294", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Multi-Level Discovery of Deep Options", "abstract": "augmenting an agents's control with new higher - level behaviors called options can greatly reduce the sample complexity over reinforcement learning, but manually picking options is infeasible in high - dimensional and abstract state spaces. while longitudinal work has proposed several techniques enabling task configuration discovery, they don't scale to multi - level environments and to expressive representations describing as deep integration. experiments present discovery of deep options ( ddo ), a policy - gradient methodology who avoids parametrized options from a set of demonstration trajectories, and allows be used recursively to discover additional levels of the hierarchy. total scalability determining our approach to multi - level hierarchies stems from the decoupling of low - level option discovery from high - level extra - item policy learning, facilitated by under - parametrization of the high level. we demonstrate that using the discovered options to augment the action objectives of deep q - network agents can accelerate decisions by guiding exploration in tasks \u2013 random actions are unlikely to reach valuable states. we show that ddo is effective in adding options effectively accelerate learning in 4 out of 5 atari ram environments chosen in our investigations. we also show that ddo can discover structure in robot - assisted surgical videos and kinematics that match expert annotation with 72 % accuracy.", "histories": [["v1", "Fri, 24 Mar 2017 06:35:46 GMT  (1163kb,D)", "http://arxiv.org/abs/1703.08294v1", null], ["v2", "Thu, 5 Oct 2017 07:33:58 GMT  (1122kb,D)", "http://arxiv.org/abs/1703.08294v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["roy fox", "sanjay krishnan", "ion stoica", "ken goldberg"], "accepted": false, "id": "1703.08294"}, "pdf": {"name": "1703.08294.pdf", "metadata": {"source": "CRF", "title": "Multi-Level Discovery of Deep Options", "authors": ["Roy Fox", "Sanjay Krishnan", "Ion Stoica", "Ken Goldberg"], "emails": ["ROYF@BERKELEY.EDU", "SANJAYKRISHNAN@BERKELEY.EDU", "ISTOICA@BERKELEY.EDU", "GOLDBERG@BERKELEY.EDU"], "sections": [{"heading": null, "text": "Augmenting an agent\u2019s control with useful higher-level behaviors called options can greatly reduce the sample complexity of reinforcement learning, but manually designing options is infeasible in high-dimensional and abstract state spaces. While recent work has proposed several techniques for automated option discovery, they do not scale to multi-level hierarchies and to expressive representations such as deep networks. We present Discovery of Deep Options (DDO), a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories, and can be used recursively to discover additional levels of the hierarchy. The scalability of our approach to multi-level hierarchies stems from the decoupling of low-level option discovery from high-level meta-control policy learning, facilitated by under-parametrization of the high level. We demonstrate that using the discovered options to augment the action space of Deep Q-Network agents can accelerate learning by guiding exploration in tasks where random actions are unlikely to reach valuable states. We show that DDO is effective in adding options that accelerate learning in 4 out of 5 Atari RAM environments chosen in our experiments. We also show that DDO can discover structure in robot-assisted surgical videos and kinematics that match expert annotation with 72% accuracy.\nPreliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute."}, {"heading": "1. Introduction", "text": "Control policies that perform long and intricate tasks often require high-dimensional parametrization. In principle, existing reinforcement learning (RL) algorithms can learn such policies, but often with prohibitive sample complexity. One approach is to augment the agent\u2019s controls with useful higher-level behaviors called options (Sutton et al., 1999), each consisting of a control policy for one region of the state space, and a termination condition recognizing leaving that region. This augmentation naturally defines a hierarchical structure of high-level meta-control policies that invoke lower-level options to solve sub-tasks. This leads to a \u201cdivide and conquer\u201d relationship between the levels, where each option can specialize in short-term planning over local state features, and the meta-control policy can specialize in long-term planning over slowly changing state features.\nTo apply RL in long-horizon tasks, such as driving and surgery, we need to consider deep hierarchies of deep options \u2014 policies that employ multi-level hierarchies of expressive options parametrized by deep neural networks. Since designing such structures manually is infeasible, we need algorithms to discover them. Despite recent results in option discovery, some proposed techniques do not generalize well to multi-level hierarchies (Bacon et al., 2016; Heess et al., 2016; Kulkarni et al., 2016), while others are inefficient for learning expressive representations (Bui et al., 2002; Daniel et al., 2012; Hamidi et al., 2015; Lakshminarayanan et al., 2016).\nWe introduce Discovery of Deep Options (DDO), an algorithm for efficiently discovering deep hierarchies of deep options. DDO is a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories (sequences of states and actions) provided either by a supervisor or by roll-outs of previously learned policies. These demonstrations need not be given by an optimal agent, but it is assumed that they are informative of the pre-\nar X\niv :1\n70 3.\n08 29\n4v 1\n[ cs\n.L G\n] 2\n4 M\nar 2\nferred actions to take in each visited state, and are not just random walks.\nThe basis for this algorithm is Hierarchical Behavioral Cloning (HBC), where the agent tries to learn a hierarchical policy that matches the demonstrated behavior. Given a set of trajectories, the algorithm discovers a fixed, predetermined number of options that are most likely to generate the observed trajectories. Since an option is represented by both a control policy and a termination condition, our algorithm simultaneously (1) infers option boundaries in demonstrations which segment trajectories into different control regimes, (2) infers the meta-control policy for selecting options as a mapping of segments to the option that likely generated them, and (3) learns a control policy for each option, which can be interpreted as a soft clustering where the centroids correspond to prototypical behaviors of the agent.\nThe algorithm repeats for each additional level of the hierarchy. The scalability of our approach to multi-level hierarchies stems from the decoupling of low-level option discovery from high-level meta-control policy learning. The true meta-control policy is often too richly parametrized, and must be simplified to facilitate low-level option discovery. Each higher level of the hierarchy can then be discovered separately, building on established lower-level options, without involving any unlearned higher levels. We show that DDO is effective in adding to the hierarchy more levels that can further accelerate learning.\nWe demonstrate that using the discovered options in Deep Q-Network agents can accelerate reinforcement learning by guiding exploration. Useful options are persistent control loops that lead to valuable states, from which exploration can continue. When these states are unlikely to be reached by random actions, using options allows an - greedy exploring agent to nevertheless reach these valuable states by suspending exploration until the option terminates."}, {"heading": "2. Related Work", "text": "The field of hierarchical reinforcement learning has a long history (Barto & Mahadevan, 2003; Parr, 1998; Sutton et al., 1999), and has also been applied in robotics (Konidaris et al., 2012; Krishnan et al., 2016; Sermanet et al., 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al., 2009; Solway et al., 2014; Whiten et al., 2006; Zacks et al., 2011). Early work in hierarchical control demonstrated the advantages of hierarchical structures by handcrafting hierarchical policies (Brooks, 1986) and by learning them given various manual specifications: state abstractions (Dayan & Hinton, 1992; Hengst, 2002; Kolter et al., 2007; Konidaris & Barto,\n2007), a set of waypoints (Kaelbling, 1993), low-level skills (Bacon & Precup, 2015; Huber & Grupen, 1997; Liaw et al., 2017), a set of finite-state meta-controllers (Parr & Russell, 1997), a set of subgoals (Dietterich, 2000; Sutton et al., 1999), or intrinsic reward (Kulkarni et al., 2016). Since then, the focus of research has shifted towards discovery of the hierarchical structure itself, by: trading off value with description length (Thrun & Schwartz, 1994), identifying transitional states (Lakshminarayanan et al., 2016; McGovern & Barto, 2001; Menache et al., 2002; S\u0327ims\u0327ek & Barto, 2004; Stolle, 2004), inference from demonstrations (Bui et al., 2002; Daniel et al., 2012; Krishnan et al., 2015; 2016), iteratively expanding the set of solvable initial states (Konidaris & Barto, 2009; Konidaris et al., 2012), policy gradient (Levy & Shimkin, 2011), trading off value with informational constraints (Florensa et al., 2017; Fox et al., 2016; Genewein et al., 2015; Jonsson & G\u00f3mez, 2016), active learning (Hamidi et al., 2015), or recently value-function approximation (Bacon et al., 2016; Heess et al., 2016; Sharma et al., 2017).\nMost previous work in hierarchy discovery focused on a 2-level hierarchy, and its extension to multi-level hierarchies is not always straightforward. Naively, the number of different contexts that the d highest levels can provide to lower levels grows exponentially with d. Discovery of multi-level hierarchies was shown to be possible via particle filters (Bui et al., 2002), however we are seeking more expressive representations. We show that this \u201ccurse of dimensionality\u201d of multi-level meta-control can be avoided if, during the option discovery phase, we make a modeling assumption that under-parametrizes the higher-level model \u2014 e.g. fixes the meta-control policy. This decouples the lower-level discovery from the levels above it. Empirically, we find that this assumption does not sacrifice the quality of the discovered options (Section 5.1).\nOur work is most related to (Daniel et al., 2016), who use a similar generative model, originally introduced by (Bui et al., 2002) as an Abstract Hidden Markov Model, and learn its parameters via the Expectation-Maximization (EM) algorithm. EM applies the same forward-backward E-step as our Expectation-Gradient algorithm (Section 4.2) to compute marginal posteriors, but uses them for a complete optimization M-step over the options and the metacontrol policies. This optimization is infeasible for expressive representations, as well as for multi-level hierarchies. Gradient-descent algorithms for value-function approximation with deep networks has been used to train hierarchical policies (Heess et al., 2016; Kulkarni et al., 2016), using a Universal Value Function Approximator (Schaul et al., 2015). These methods learn the entire hierarchy simultaneously, and may be inefficient for multi-level hierarchies."}, {"heading": "3. Preliminaries", "text": ""}, {"heading": "3.1. Markov Decision Processes", "text": "We consider a discrete-time discounted Markov Decision Process (MDP), described by a 6-tuple xS,A, p0, p, r, \u03b3y, where S denotes the state space,A the action space, p0ps0q the initial state distribution, ppst`1|st, atq the state transition distribution, rpst, atq P R the reward function, and \u03b3 P r0, 1q the discount factor. A policy \u03c0pat|stq defines a conditional probability distribution over actions given the state. A trajectory is defined as a sequence of states and actions \u03be \u201c ps0, a0, s1, . . . , sT q of a given length T . In a given MDP, a policy \u03c0 induces the distribution over trajectories:\nP\u03c0p\u03beq \u201c p0ps0q T\u00b41\u017a\nt\u201c0 \u03c0pat|stqppst`1|st, atq.\nThe return of a policy is its expected total discounted reward over trajectories\nV\u03c0 \u201c E\u03be\u201eP\u03c0 \u00ab T\u00b41\u00ff\nt\u201c0 \u03b3trpst, atq\nff ."}, {"heading": "3.2. Reinforcement Learning", "text": "In many real-world domains, we seek a policy \u03c0 that achieves high return despite the state transition distribution p being initially unknown. One successful approach to this problem is Q-learning (Sutton & Barto, 1998; Watkins & Dayan, 1992), a reinforcement learning algorithm for estimating the maximum return Qps, aq, which is the expected total discounted reward obtained by taking the action a in the state s, and thereafter following the optimal policy.\nQ-learning is off-policy, in that it samples a state si, action ai, reward ri \u201c rpsi, aiq and next state s1i \u201e pp\u00a8|si, aiq using some suboptimal exploration policy. Based on these samples, Q-learning then updates its estimate of Qpsi, aiq greedily towards the one-step backward estimate\nyi \u201c ri ` \u03b3max a1 Qps1i, a1q. When Q\u03b8 is represented by a Deep Q-Network (DQN) (Mnih et al., 2015), the update is a gradient step to reduce the mean square Bellman error over a batch of samples\nLp\u03b8q \u201c \u00b4Erpyi \u00b4Q\u03b8psi, aiqq2s \u03b8 \u00d0 \u03b8 ` \u03b1\u2207\u03b8Lp\u03b8q,\nwith learning rate \u03b1, and yi based on the fixed current parameter \u03b8\u00b4, independent of \u03b8."}, {"heading": "3.3. Imitation Learning", "text": "Imitation learning (IL) is an alternative to the reinforcement learning setting, in which the policy is learned from demonstrations of expert behavior rather than from the reward sig-\nnal of an environment. In the behavioral cloning (BC) setting, the goal is to match the expert policy as closely as possible.\nWe can formalize this setting as estimation of the parameters of a generative model. Suppose that a demonstration trajectory \u03be \u201c ps0, a0, s1, . . . , sT q is generated by an unknown policy \u03c0\u02da according to the following generative model:\nInitialize t \u201c 0, s0 \u201e p0 for t \u201c 0, . . . , T \u00b4 1 do\nDraw at \u201e \u03c0\u02dap\u00a8|stq Draw st`1 \u201e pp\u00a8|st, atq\nend for\nWe can define a parametrized set of policies \u03c0\u03b8 and find the parameters that maximize the log-likelihood\nLr\u03b8; \u03bes \u201c log p0ps0q ` T\u00b41\u00ff\nt\u201c0 logp\u03c0\u03b8pat|stqppst`1|st, atqq.\nIt is interesting to note that the dynamics factor out of this optimization problem, simplifying it to\narg max \u03b8P\u0398 Lr\u03b8; \u03bes \u201c arg max \u03b8P\u0398\nT\u00b41\u00ff t\u201c0 log \u03c0\u03b8pat|stq.\nFor differentiable parametrizations the gradient update is\n\u03b8 \u00d0 \u03b8 ` \u03b1 T\u00b41\u00ff\nt\u201c0 \u2207\u03b8 log \u03c0\u03b8pat|stq."}, {"heading": "3.4. The Options Framework", "text": "Control policies that perform long and intricate tasks often require very high-dimensional parameterization, which can take many trajectories to learn. The options framework is a hierarchical policy structure that can mitigate this sample complexity by breaking down the policy into simpler skills called options (Sutton et al., 1999). An option represent a lower-level control primitive that can be invoked by the meta-control policy at a higher-level of the hierarchy, in order to perform a certain subtask. Formally, an option h is described by a triplet xIh, \u03c0h, \u03c8hy, where Ih \u010e S denotes the initiation set, \u03c0hpat|stq the control policy, and \u03c8hpstq P r0, 1s the termination policy. When the process reaches a state s P Ih, the option h can be invoked to run the policy \u03c0h. After each action is taken and the next state s1 is reached, the option h terminates with probability \u03c8hps1q and returns control up the hierarchy to its invoking level.\nThe options framework enables multi-level hierarchies to be formed by allowing options to invoke other options. A higher-level meta-control policy is defined by augmenting its action spaceA with the setH of all lower-level options. In this paper we do not explicitly consider initiation sets. Instead, any higher-level policy \u03b7ph|sq should assign low\nprobability to a lower-level option h outside the region of state space in which h is effective."}, {"heading": "4. Discovery of Deep Options", "text": "In this section, we present Discovery of Deep Options (DDO), a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories (sequences of states and actions). The option parameters are inferred by fitting a generative model to the observed trajectories. These demonstrations need not be given by an optimal agent \u2014 the demonstrator can be a human who is not an expert, or a partially trained algorithmic controller. Our approach only assumes that the trajectories are informative of the preference of actions to take in each visited state, and that these preferences can be represented in a hierarchical structure (see Section C)."}, {"heading": "4.1. Imitation Learning for Option Discovery", "text": "We generalize standard IL to hierarchical control, by introducing Hierarchical Behavioral Cloning (HBC). In HBC, the meta-control signals that form the hierarchy are unobservable, latent variables of the generative model, that must be inferred.\nConsider a trajectory \u03be \u201c ps0, a0, s1, . . . , sT q that is generated by a two-level hierarchy. The low level implements a set H of options x\u03c0h, \u03c8hyhPH. The high level implements a meta-control policy \u03b7pht|stq that repeatedly chooses an option h given the current state, and runs it until termination. Our hierarchical generative model is:\nInitialize t \u201c 0, s0 \u201e p0, h0 \u201e \u03b7p\u00a8|s0q for t \u201c 0, . . . , T \u00b4 1 do\nDraw at \u201e \u03c0htp\u00a8|stq Draw st`1 \u201e pp\u00a8|st, atq Draw bt`1 \u201e Berp\u03c8htpst`1qq if bt`1 \u201c 0 then\nSet ht`1 \u00d0 ht else // bt`1 \u201c 1\nDraw ht`1 \u201e \u03b7p\u00a8|st`1q end if\nend for"}, {"heading": "4.2. Expectation-Gradient Algorithm", "text": "We denote by \u03b8 the vector of parameters for \u03c0h, \u03c8h and \u03b7. For example, \u03b8 can be the weights and biases of a feedforward network that computes these probabilities. This generic notation allows us the flexibility of a completely separate network for each option, \u03b8 \u201c p\u03b8hqhPH, or the efficiency of sharing some of the parameters between options, similarly to a Universal Value Function Approximator (Schaul et al., 2015).\nWe want to find the \u03b8 P \u0398 that maximizes the loglikelihood assigned to a given dataset of trajectories. The likelihood of a trajectory depends on the latent sequence \u03b6 \u201c ph0, b1, h1, . . . , hT q of meta-actions and termination indicators, and in order to use a gradient-based optimization method we rewrite the gradient as \u2207\u03b8Lr\u03b8; \u03bes \u201c \u2207\u03b8 logP\u03b8p\u03beq\n\u201c 1 P\u03b8\u00b4p\u03beq\n\u00ff\n\u03b6PHT`1\u02c6t0,1uT \u2207\u03b8 P\u03b8p\u03b6, \u03beq\n\u201c \u00ff\n\u03b6\nP\u03b8\u00b4p\u03b6, \u03beq P\u03b8\u00b4p\u03beq \u2207\u03b8 logP\u03b8p\u03b6, \u03beq\n\u201c E\u03b8\u00b4r\u2207\u03b8 logP\u03b8p\u03b6, \u03beq|\u03bes, which is the so-called Expectation-Gradient method (McLachlan & Krishnan, 2007; Salakhutdinov et al., 2003). \u03b8\u00b4 denotes the current parameter taken as fixed outside the gradient.\nApplying this to the generative model in the previous section, we get:\nP\u03b8p\u03b6, \u03beq \u201c p0ps0q\u03b7\u03b8ph0|s0q\n\u00a8 T\u00b41\u017a\nt\u201c0 \u03c0\u03b8;htpat|stqppst`1|st, atq\n\u00a8 P\u03b8pbt`1, ht`1|ht, st`1q, with P\u03b8pbt`1\u201c1, ht`1|ht, st`1q \u201c \u03c8\u03b8;htpst`1q\u03b7\u03b8pht`1|st`1q P\u03b8pbt`1\u201c0, ht`1|ht, st`1q \u201c p1\u00b4 \u03c8\u03b8;htpst`1qq\u03b4ht,ht`1 , \u03b4ht,ht`1 denotes the indicator that ht \u201c ht`1. Ignoring the terms that do not depend on \u03b8, we can simplify the gradient to:\n\u2207\u03b8Lr\u03b8; \u03bes \u201c E\u03b8\u00b4 \u00ab \u2207\u03b8 log \u03b7\u03b8ph0|s0q\n` T\u00b41\u00ff\nt\u201c0 p\u2207\u03b8 log \u03c0\u03b8;htpat|stq\n`\u2207\u03b8 logP\u03b8pbt`1, ht`1|ht, st`1qq \u02c7\u030c \u02c7\u030c \u02c7\u03be ff .\nThe log-likelihood gradient can therefore be computed as the sum of the log-probability gradients of the various parameterized networks, weighed by the marginal posteriors\nutphq \u201c P\u03b8\u00b4pht \u201c h|\u03beq vtphq \u201c P\u03b8\u00b4pht \u201c h, bt`1 \u201c 1|\u03beq wtphq \u201c P\u03b8\u00b4pbt`1 \u201c 1, ht`1 \u201c h|\u03beq.\nIn the Expectation-Gradient algorithm, the E-step computes u, v and w, and the G-step updates the parameter\nwith a gradient step, namely \u2207\u03b8Lr\u03b8; \u03bes \u201c \u00ff\nhPH\n\u02dc u0phq\u2207\u03b8 log \u03b7\u03b8ph|s0q\n` T\u00b41\u00ff\nt\u201c0\n\u02dc utphq\u2207\u03b8 log \u03c0\u03b8;hpat|stq\n` vtphq\u2207\u03b8 log\u03c8\u03b8;hpst`1q ` putphq \u00b4 vtphqq\u2207\u03b8 logp1\u00b4 \u03c8\u03b8;hpst`1qq ` wtphq\u2207\u03b8 log \u03b7\u03b8ph|st`1q \u00b8\u00b8 .\nThese equations lead to the natural iterative algorithm. In each iteration, the marginal posteriors u, v and w can be computed with a forward-backward message-passing algorithm similar to Baum-Welch (Baum, 1972), with time complexity Op|H|2T q. Importantly, this algorithm can be performed without any knowledge of the state dynamics. The details of this computation are given in the supplementary material. Then, the computed posteriors can be used in a gradient descent algorithm to update the parameters:\n\u03b8 \u00d0 \u03b8 ` \u03b1 \u00ff\ni\n\u2207\u03b8Lr\u03b8; \u03beis.\nThis update can be made stochastic using a single trajectory, uniformly chosen from the demonstration dataset, to perform each update.\nIntuitively, the algorithm attempts to jointly optimize three objectives:\n\u2022 in the E-step, to probabilistically infer the option boundaries where b \u201c 1 appears likely in v \u2014 this segments the trajectory into regimes where we expect h to change and employ different control laws;\n\u2022 in the E-step, to infer the option selection after a switch, given by w, and in the G-step to reduce the cross-entropy loss between that distribution, weighted by the probability of a switch, and the meta-control policy; and\n\u2022 in the G-step, to reduce the cross-entropy loss between the empirical action distribution, weighted by the probability for h, and the control policy for h.\nThis can be interpreted as a form of soft clustering. The data points are one-hot representations of each at in the space of distributions over actions. Each time-step t is assigned to option h with probability utphq, forming a soft clustering of data points. The G-step directly minimizes the KL-divergence of the control policy \u03c0h from the weighted centroid of the corresponding cluster."}, {"heading": "4.3. Deeper Hierarchies", "text": "Our ultimate goal is to use the algorithm presented here to discover a multi-level hierarchical structure \u2014 the key insight being that the problem is recursive in nature. A D-level hierarchy can be viewed as a 2-level hierarchy, in which the \u201chigh level\u201d has a pD \u00b4 1q-level hierarchical structure. The challenge is the coupling between the levels; namely, the value of a set of options is determined by its usefulness for meta-control (Fox et al., 2016), while the value of a meta-control policy depends on which options are available. This potentially leads to an exponential growth in the size of the latent variables required for inference. The available data may be insufficient to learn a policy so expressive.\nWe can avoid this problem by using a simplified parametrization for the intermediate meta-control policy \u03b7d used when discovering level-d options. In the extreme, we can fix a uniform meta-control policy that chooses each option with probability 1{|Hd|. Discovery of the entire hierarchy can now proceed recursively from the lowest level upward: level-d options can invoke already-discovered lowerlevel options; and are discovered in the context of a simplified level-d meta-control policy, decoupled from higherlevel complexity. One of the contributions of this work is to demonstrate that, perhaps counter-intuitively, this assumption does not sacrifice too much during option discovery. An informative meta-control policy would serve as a prior on the assignment of demonstration segments to the options that generated them, but with sufficient data this assignment can also be inferred from the low-level model, purely based on the likelihood of each segment to be generated by each option.\nWe use the following algorithm to iteratively discover a hierarchy of D-levels, each level d consisting of kd options:\nfor d \u201c 1, . . . , D \u00b4 1 do Initialize a set of optionsHd \u201c thd,1, . . . , hd,kdu DDO: train options x\u03c0h, \u03c8hyhPHd with \u03b7d fixed Augment action space A\u00d0 AYHd end for Use RL algorithm to train high-level policy"}, {"heading": "5. Experiments", "text": "We present an empirical study of DDO. Our results suggest that DDO can discover options that accelerates reinforcement learning. We explore two different scenarios: (Supervised) given a supervisor who demonstrates a few times how to perform a task, show that the discovered options are useful for accelerating reinforcement learning on the same task; (Exploration) apply reinforcement learning for T episodes, sample trajectories from the current\nbest policy, and augment the action space with the discovered options for the remaining T 1 episodes. The first set of experiments illustrates DDO on a series of GridWorld domains. Then, we show how the Expectation-Gradient can scale to more challenging Atari RAM domains. Finally, we show that DDO can be used to identify visuomotor primitives in surgical data."}, {"heading": "5.1. Four Rooms GridWorld", "text": "We study a simple four-room domain (Figure 1). On a 15\u02c611 grid, the agent can move in four directions; moving into a wall has no effect. To simulate environment noise, we replace the agent\u2019s action with a random one with probability 0.3. An observable apple is spawned in a random location in one of the rooms. Upon taking the apple, the agent gets a unit reward and the apple is re-spawned.\nWe use the following notation to describe the different ways we can parametrize option discovery: A a baseline of only using atomic actions; H1u discovering a single level of options where the higher-level is parametrized by a uniform distribution; H1s discovering a single level of options where the higher-level is parametrized by an multi-layer perceptron (MLP); H2u and H2s are the two-level counterparts of H1u and H1s, respectively. All of these discovered options are used in an RL phase to augment the action space of a high-level global policy.\nSupervised Setting. We use Value Iteration to compute the optimal policy, and then use this policy as a supervisor to generate 50 trajectories of length 1000. All policies, whether for control, meta-control or termination, are parametrized by a MLP, with a single two-node hidden layer, and tanh activation functions. The MLP\u2019s input consists of the full state (agent and apple locations), and the output is computed by a softmax function over the MLP output vector, which has length |A| for control policies and two for termination.\nThe options corresponding to H2u are visualized in Figure 1 by trajectories generated using each option from a random initial state until termination. At the first level, two of the discovered options move the agent between the upper and lower rooms, and two move it from one side to the other. At the second level, the discovered options aggregate these primitives into higher-level behaviors that move the agent from any initial location to a specific room.\nImpact of options and hierarchy depth. To evaluate the quality of the discovered options, we train a Deep QNetwork (DQN) with the same MLP architecture, and action space augmented by the options. The exploration is -greedy with \u201c 0.2 and the discount factor is \u03b3 \u201c 0.9. Figure 2 shows the average reward in 15 of the algorithm\u2019s runs in the Supervised and Exploration experimental settings. The results illustrate that augmenting the action space with options can significantly accelerate learning. Note that the options learned with a two-level hierarchy (H2u) provide significant benefits over the options learned only with a single-level hierarchy H1u. The hierarchical approaches achieve roughly the same average reward after 1000 episodes as A does after 5000 episodes.\nImpact of policy parametrization. To evaluate the effect of meta-control policy parametrization, we also compare the rewards during DQN reinforcement learning with options discovered with MLP meta-control policies (H1s and H2s). Our empirical results suggest that less expressive parametrization of the meta-control policy does not significantly hurt the performance, and in some cases can even provide a benefit (Figure 2). This is highly important, because the high sample complexity of jointly training all levels of a deep hierarchy necessitates simplifying the meta-\ncontrol policy \u2014 which would otherwise be represented by a one level shallower hierarchy. We conjecture that the reason for the improved performance of the less expressive model is that more complex parametrization of the metacontrol policy increases the prevalence of local optima in the inference problem, which may lead to worse options.\nExploration Setting. Finally, we demonstrate that options can also be useful when discovered from selfdemonstrations by a partially trained agent, rather than by an expert supervisor. We run the same DQN as above, with only atomic actions, for 2000 episodes. We then use the greedy policy for the learned Q-function to generate 100 trajectories. We reset the Q-function (except for the baseline A), and run DQN again with the augmented action space. Figure 2 illustrates that even when these options are not discovered from demonstrations of optimal behavior, they are useful in accelerating reinforcement learning. The reason is that options are policy fragments that have been discovered to lead to interesting states, and therefore benefit from not being interrupted by random exploration."}, {"heading": "5.2. Atari RAM Games", "text": "The RAM variant of the popular Atari Deep Reinforcement Learning domains considers a game-playing agent which is given not the screen, but rather the RAM state of the Atari machine. This RAM state is a 128-byte vector that completely determines the state of the game, and can be encoded in one-hot representation as s P R128\u02c6256. The RAM state-space illustrates the power of an automated option discovery framework, as it would be infeasible to manually code options without carefully understanding the game\u2019s memory structure. With a discovery algorithm, we have a general-purpose approach to learn in this environment.\nAll policies are parametrized with a deep network. There are three dense layers, each with tanh activations, and the output distribution is a softmax of the last layer, which has length |A| for control policies and two for termination. We use a single level of options, with the number of options\ntuned to optimize performance, and given in Figure 3.\nFor each game, we first run the DQN for 1000 episodes, and then generate 100 trajectories from the greedy policy, and use them to discover options with DDO. The DQN has the same architecture, using -greedy exploration for 1000 episodes with \u201c 0.05 and discount factor \u03b3 \u201c 0.85 (similar to the parameters used in (Sygnowski & Michalewski, 2016)). Finally, we augment the action space with the discovered options and rerun DQN for 4000 episodes. We compare this to the baseline of running DQN for 5000 episodes with actions only.\nFigure 3 plots the estimate value, averaged over 50 trials, of the learned policies for five Atari games: Atlantis, Pooyan, Gopher, Space Invaders, and Sea Quest. In four out of five games, we see a significant acceleration learning. The relative improvements are the largest for the three hardest domains: Gopher, Sea Quest, and Space Invaders. It is promising that DDO offers such an advantage where other methods struggle. Figure 4 shows four frames from one of the options discovered by DDO for the Atlantis game 1. The option appears to identify an incoming alien and determine when to fire the gun, terminating when the alien is destroyed. As in the GridWorld experiments, the options are policy fragments that have been discovered to lead to high-value states, and therefore benefit from not being interrupted by random exploration.\n1In the final version, we will provide a link to the videos of the learned options."}, {"heading": "5.3. Segmentation of Robotic-Assisted Surgery", "text": "In this section, we illustrate the wide applicability of the DDO framework by applying it to human demonstrations in a robotic domain. We apply DDO to long robotic trajectories (e.g. 3 minutes) demonstrating an intricate task, and discover options for useful subtasks, as well as segmentation of the demonstrations into semantic units. The JIGSAWS dataset consists of surgical robot trajectories of human surgeons performing training procedures (Gao, 2014). The dataset was captured using the da Vinci Surgical System from eight surgeons with different skill levels, performing five repetitions each of needle passing, suturing, and knot tying. This dataset consists of videos and kinematic data of the robot arms, and is annotated by experts identifying the activity occurring in each frame.\nEach policy network takes as input a three-channel RGB 200\u02c6200 image, downscaled from 640\u02c6480 in the dataset, applies three convolutional layers with ReLU activations followed by two fully-connected dense layers reducing to 64 and then eight real-valued components. An action is represented by 3D translations and the opening angles of the left and right arm grippers.\nWe investigate how well the segmentation provided by DDO corresponds to expert annotations, when applied to demonstrations of the three surgical tasks. Figure 5 shows a representative sample of 10 trajectories from each task, with each time step colored by the most likely option to be active at that time. Human boundary annotations are marked in \u02c6. We quantify the match between the manual and automatic annotation by the fraction of option boundaries that have exactly one human annotation in a 300 ms window around them. By this metric, DDO obtains 72% accuracy, while random guessing gets only 14%. These results suggest that DDO succeeds in learning some latent structure of the task."}, {"heading": "6. Discussion", "text": "In this paper we presented the DDO algorithm that discovers parametrized options from a set of demonstration trajectories, and can be used recursively to discover multi-level hierarchies. Our results demonstrate that the discovered options accelerate learning in RL problems.\nAn important point for discussion is setting the hyperparameters of DDO, namely, (1) the number of options at each level of the hierarchy, (2) how many RL episodes to wait before applying DDO, and (3) how to design the parametrization for the options. In our experiments, for (1), we tuned the number of options based on observing the RL agent\u2019s performance using those options. For (2), we ran the RL agent with only the actions, and observed the time step when its greedy policy started accumulating\nabove-random rewards. We empirically found that this was a good stopping point to sample the greedy policy and apply DDO. Finally, we selected the option parametrization to be similar in architecture to the Q-Networks used on the domains. More principled methods for selecting all of these hyperparameters merit further research.\nIn some of our experiments we train the meta-control policy during option discovery. When we subsequently train a higher-level policy using the discovered options to augment the action space, we initialize the high-level Q function to 0, rather than reusing any pre-training from the discovery stage. Our algorithm outperforms the baseline despite the handicap of this Q-function reset. A method for initializing theQ function from discovery-stage pre-trained values, without introducing too much bias, could accelerate learning further.\nEven with Q-function reset, options trained jointly with a meta-control policy accelerate the high-level learning stage more than options trained with a fixed uniform metacontrol policy. However, when more levels are added to the hierarchy beyond the lowest two, our experiments suggest that the relative advantage is reversed: learning is faster when using lowest-level options that were trained with a uniform meta-control policy at the first stage. This preliminary finding indicates that option discovery with a simplified meta-control policy may not hurt performance, and perhaps even help.\nOur derivation and algorithms apply to continuous as well as discrete action spaces, although in this paper we experimented mainly with the latter. A meta-control policy over a continuous action space augmented with a finite set of options can be represented by a hybrid network, that outputs parameters for the continuous action distribution, the discrete option distribution, and a bit to choose between them. The effectiveness of our approach in such domains remains an open question."}, {"heading": "Acknowledgements", "text": "This research was performed at the AUTOLAB at UC Berkeley in affiliation with the Berkeley AI Research (BAIR) Lab, the Real-Time Intelligent Secure Execution (RISE) Lab, and the CITRIS \u201cPeople and Robots\u201d (CPAR) Initiative. The authors were supported in part by the U.S. National Science Foundation under NRI Award IIS1227536: Multilateral Manipulation by Human-Robot Collaborative Systems and the Berkeley Deep Drive (BDD) Program, by DHS Award HSHQDC-16-3-00083, NSF CISE Expeditions Award CCF-1139158, and donations and gifts from Siemens, Google, Cisco, Autodesk, IBM, Ant Financial, Amazon Web Services, CapitalOne, Ericsson, GE, Huawei, Intel, Microsoft and VMware.\nAny opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Sponsors."}, {"heading": "A. Forward-Backward Algorithm", "text": "Despite the exponential domain size of the latent variable \u03b6, Expectation-Gradient for trajectories allows us to decompose the posterior P\u03b8p\u03b6|\u03beq and only concern ourselves with each marginal posterior separately. These marginal posteriors can be computed by a forward-backward dynamic programming algorithm, similar to Baum-Welch (Baum, 1972).\nOmitting the current parameter \u03b8\u00b4 and trajectory \u03be from our notation, we start by computing the likelihood of a trajectory prefix \u03c6tphq \u201c Pps0, a0, . . . , st, ht \u201c hq, using the forward recursion\n\u03c60phq \u201c p0ps0q\u03b7ph|s0q \u03c6t`1ph1q \u201c \u00ff\nhPH \u03bdtphq\u03c0hpat|stqppst`1|st, atqPph1|h, st`1q,\nwith Pph1|h, st`1q \u201c \u03c8hpst`1q\u03b7ph1|st`1q ` p1\u00b4 \u03c8hpst`1qq\u03b4h,h1 . We similarly compute the likelihood of a trajectory suffix \u03c9tphq \u201c Ppat, st`1, . . . , sT |st, ht \u201c hq, using the backward recursion \u03c9T phq \u201c 1 \u03c9tphq \u201c \u03c0hpat|stqppst`1|st, atq \u00ff\nh1PH Pph1|h, st`1q\u03c9t`1ph1q.\nWe can compute our target likelihood using any 0 \u010f t \u010f T Pp\u03beq \u201c \u00ff\nhPH Pp\u03be, ht \u201c hq \u201c\n\u00ff\nhPH \u03c6tphq\u03c9tphq.\nRecall that we define utphq \u201c Ppht \u201c h|\u03beq vtphq \u201c Ppht \u201c h, bt`1 \u201c 1|\u03beq wtphq \u201c Ppbt`1 \u201c 1, ht`1 \u201c h|\u03beq.\nThe marginal posteriors are\nutphq \u201c 1 Pp\u03beq\u03c6tphq\u03c9tphq\nvtphq \u201c 1 Pp\u03beq\u03c6tphq\u03c0hpat|stqppst`1|st, atq\u03c8hpst`1q\n\u00a8 \u00ff\nh1PH \u03b7ph1|st`1q\u03c9t`1ph1q\nwtph1q \u201c 1 Pp\u03beq\n\u00ff\nhPH \u03c6tphq\u03c0hpat|stqppst`1|st, atq\n\u00a8 \u03c8hpst`1q\u03b7ph1|st`1q\u03c9t`1ph1q. Note that the constant p0ps0q\u015bT\u00b41t\u201c0 ppst`1|st, atq is cancelled out in these normalizations. This allows us to omit these terms during the forward-backward algorithm, which can thus be applied without any knowledge of the dynamics."}, {"heading": "B. Supplemental Experiments", "text": "This section includes a number of simplified examples to convey intuition of why options can improve RL performance.\nGridWorld: Supervised\nIn the first scenario, we construct a 10x20 GridWorld environment with two rooms with a 30% probability of random action. We generate 10 demonstrations from randomly chosen start and end points situated in different rooms using Value Iteration. We apply the inference algorithm to learn 2 options (Figure 6). Not surprisingly, there is one option policy that exits the left room and moves to the right, and another that exits the right room and moves to the left. The termination probability is visualized in red. Figure 6 illustrates the flexibility of the proposed inference algorithm, namely, we can apply it to a tabular (fully parameterized) representation as well as a neural network representation (multi-layer perceptron MLP). We can see that the MLP generalizes to unseen states while still being expressive enough to model the geometry of the two rooms. This is especially true for the parameterization of the termination probabilities. The MLP learns to terminate the primitive in each of the rooms while the tabular representation does not make that generalization.\nGiven a new start and end goal, we can use reinforcement learning to find a policy. We add a reward function to the two-rooms environment which gives a reward of +1 to the agent if it reaches the goal in 20 time-steps. We evaluate the extent to which the options learned reduce exploration to find this policy. We use the MLP options described in the previous experiments. We use a Deep Q Network (DQN) as the RL algorithm. We compare three approaches: (1) DQN using the primitive actions, (2) DQN using only the options, and (3) DQN using an augmented action space of the primitive actions and options. Figure 7 plots the estimated value of each policy from 30 trials. The DQN augmented with action and options converges to the maximum reward in 100 episodes, whereas the DQN over just the primitive actions requires 4600. Figure 7b illustrates why this is likely the case. When we compare the optionsonly to the approach with both options and actions, we see that the options-only approach is able to quickly receive a positive reward but not always able to reach the goal. This early positive reward signal can greatly speed up the convergence.\nGridWorld: Demonstrations and Initialization\nNext, we discuss some additional details about the DDO algorithm. First, we explore the number and quality of demonstrations needed to learn viable options. As before,\nwe consider an MLP representation for the options. Figure 8a varies the environment noise in the GridWorld and plots the number of episodes to convergence (defined as a reward of 1.0 on 10 consecutive trials). For the value iteration demonstrator, a fairly small number of demonstrations is required to learn useful options. This number is relatively robust to environments with small amounts of stochasticity. On the other hand, Figure 8b plots \u201csupervisor\u201d noise in the demonstrations. We randomly corrupt a fraction of the taken actions from the supervisor. For this type of noise, a significantly larger number of demonstrations is required to see the same gains in performance as the infallable supervisor.\nAnother further point of discussion is the initialization of the DDO algorithm. As in clustering algorithms, one must initialize the model in a way that breaks any symmetry. The initialization approach can affect the solutions that arise. In\nparticular, we can use an approach to initialize where the state action tuples are first clustered and then initial models are trained on each cluster. This is similar to techniques used to initialize k-means. When we use this initialization for the tabular representation, this leads to a qualitatively different set of primitives for the two rooms environment than in the previous result (Figure 9). There is one option that leads the agent out of each room to the center and another that takes the agent from the center to each room. One could argue that this set of primitives is easier to plan with as the termination condition is far simpler than in the previous result.\nGridWorld: Exploration\nNext, we show how within a task options can be used to improve the convergence of an RL algorithm. We construct a 10x25 GridWorld environment with three rooms with a 30% probability of random action. The agent starts in the leftmost room, has to reach a point in the second room, and then progress to the third room. The agent receives a reward of 0.5 when it reaches the first goal, and receives a reward of 0.5 when it reaches the second goal only if it had reached the first goal. Figure 10 illustrates the domain and the results.\nWe apply RL with a DQN for 5000 episodes. This roughly corresponds to the agent reliably reaching the first goal. Then, we apply the DDO algorithm to learn two options. For future iterations, we augment the DQN with the learned options. Figure 10b illustrates how this can improve the convergence. The agent can use the options as a fixed subroutine that can take it to the first goal, and then from that point, it can try further actions. This focuses the exploration towards searching for the goal in the second room.\nState-Dependent: Four Rooms\nTo understand why the richer high-level meta policy potentially hurts in the presented Four-Room GridWorld example, we visualize the learned options in Figure 11. One of the learned options is an overly specific routine traversing the small corridor. This leads to a more complex high-level policy that composes three options to go to each room instead of two. However, we caution that this is not definitive evidence that simplifying the high-level policy improves option discovery. It suggests that a richer high-level policy potentially requires more regularization and more accurate initialization.\nAtari Neural Network Architecture\nThe neural network used in the Atari domain to represent options (control and termination policies) and meta-control policies, is depicted in Figure 12."}, {"heading": "C. Additional Considerations", "text": "Attempting to fit the hierarchical generative model to demonstrations makes an implicit assumption that there is useful structure to discover in the first place. Hierarchy as-\nsumes that the meta-control policy can identify, for each option h, a set of states where h is advantageous, by setting \u03b7ph|sq high; that the advantage of using the policy \u03c0h, when used in such a state, is likely to persist to the next state; and that the termination policy can identify states where h stops being advantageous, by setting \u03c8hpsq high. Under these assumptions, the meta-control policy benefits from only attending to slowly changing state features, while each option benefits from only attending to local state features.\nA key consideration is the expressive power of the parametrization of \u03c0h, \u03c8h and \u03b7, and its effect on the \u201clife expectancy\u201d of options, i.e. the expected time until one terminates. Any policy \u03c0pa|sq can be represented as a single low-level option that never terminates, if it is expressive enough. At the other extreme, by including single-action options for each a P A, having \u03c0hapa1|sq \u201c \u03b4a,a1 and immediate termination \u03c8hapsq \u201c 1, the meta-control policy can implement any policy directly with \u03b7pha|sq \u201c \u03c0pa|sq."}], "references": [{"title": "Learning with options: Just deliberate and relax", "author": ["Bacon", "Pierre-Luc", "Precup", "Doina"], "venue": "In NIPS Bounded Optimality and Rational Metareasoning Workshop,", "citeRegEx": "Bacon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2015}, {"title": "The option-critic architecture", "author": ["Bacon", "Pierre-Luc", "Harb", "Jean", "Precup", "Doina"], "venue": "arXiv preprint arXiv:1609.05140,", "citeRegEx": "Bacon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2016}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["Barto", "Andrew G", "Mahadevan", "Sridhar"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "Barto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barto et al\\.", "year": 2003}, {"title": "An equality and associated maximization technique in statistical estimation for probabilistic functions of markov processes", "author": ["Baum", "Leonard E"], "venue": "Inequalities, 3:1\u20138,", "citeRegEx": "Baum and E.,? \\Q1972\\E", "shortCiteRegEx": "Baum and E.", "year": 1972}, {"title": "Hierarchical models of behavior and prefrontal function", "author": ["Botvinick", "Matthew M"], "venue": "Trends in cognitive sciences,", "citeRegEx": "Botvinick and M.,? \\Q2008\\E", "shortCiteRegEx": "Botvinick and M.", "year": 2008}, {"title": "Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective", "author": ["Botvinick", "Matthew M", "Niv", "Yael", "Barto", "Andrew C"], "venue": null, "citeRegEx": "Botvinick et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Botvinick et al\\.", "year": 2009}, {"title": "A robust layered control system for a mobile robot", "author": ["Brooks", "Rodney"], "venue": "IEEE journal on robotics and automation,", "citeRegEx": "Brooks and Rodney.,? \\Q1986\\E", "shortCiteRegEx": "Brooks and Rodney.", "year": 1986}, {"title": "Policy recognition in the abstract hidden Markov model", "author": ["Bui", "Hung Hai", "Venkatesh", "Svetha", "West", "Geoff"], "venue": null, "citeRegEx": "Bui et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bui et al\\.", "year": 2002}, {"title": "Hierarchical relative entropy policy search", "author": ["Daniel", "Christian", "Neumann", "Gerhard", "Peters", "Jan"], "venue": "In AISTATS, pp", "citeRegEx": "Daniel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2012}, {"title": "Probabilistic inference for determining options in reinforcement learning", "author": ["Daniel", "Christian", "Van Hoof", "Herke", "Peters", "Jan", "Neumann", "Gerhard"], "venue": "Machine Learning,", "citeRegEx": "Daniel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2016}, {"title": "Feudal reinforcement learning", "author": ["Dayan", "Peter", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Dayan et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1992}, {"title": "Hierarchical reinforcement learning with the MAXQ value function", "author": ["Dietterich", "Thomas G"], "venue": "decomposition. JAIR,", "citeRegEx": "Dietterich and G.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich and G.", "year": 2000}, {"title": "Cstochastic neural networks for hierarchical reinforcement learning", "author": ["Florensa", "Carlos", "Duan", "Yan", "Abbeel", "Pieter"], "venue": "In ICLR,", "citeRegEx": "Florensa et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Florensa et al\\.", "year": 2017}, {"title": "Principled option learning in Markov decision processes", "author": ["Fox", "Roy", "Moshkovitz", "Michal", "Tishby", "Naftali"], "venue": "arXiv preprint arXiv:1609.05524,", "citeRegEx": "Fox et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2016}, {"title": "The JHU-ISI gesture and skill assessment dataset (jigsaws): A surgical activity working set for human motion modeling", "author": ["Gao", "Yixin"], "venue": "In Medical Image Computing and Computer-Assisted Intervention (MICCAI),", "citeRegEx": "Gao and Yixin,? \\Q2014\\E", "shortCiteRegEx": "Gao and Yixin", "year": 2014}, {"title": "Bounded rationality, abstraction, and hierarchical decision-making: An informationtheoretic optimality principle", "author": ["Genewein", "Tim", "Leibfried", "Felix", "Grau-Moya", "Jordi", "Braun", "Daniel Alexander"], "venue": "Frontiers in Robotics and AI,", "citeRegEx": "Genewein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Genewein et al\\.", "year": 2015}, {"title": "Active imitation learning of hierarchical policies", "author": ["Hamidi", "Mandana", "Tadepalli", "Prasad", "Goetschalckx", "Robby", "Fern", "Alan"], "venue": "In IJCAI, pp", "citeRegEx": "Hamidi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hamidi et al\\.", "year": 2015}, {"title": "Learning and transfer of modulated locomotor controllers", "author": ["Heess", "Nicolas", "Wayne", "Greg", "Tassa", "Yuval", "Lillicrap", "Timothy", "Riedmiller", "Martin", "Silver", "David"], "venue": "arXiv preprint arXiv:1610.05182,", "citeRegEx": "Heess et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2016}, {"title": "Discovering hierarchy in reinforcement learning with HEXQ", "author": ["Hengst", "Bernhard"], "venue": "In ICML,", "citeRegEx": "Hengst and Bernhard.,? \\Q2002\\E", "shortCiteRegEx": "Hengst and Bernhard.", "year": 2002}, {"title": "A feedback control structure for on-line learning tasks", "author": ["Huber", "Manfred", "Grupen", "Roderic A"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Huber et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Huber et al\\.", "year": 1997}, {"title": "Hierarchical linearlysolvable Markov decision problems", "author": ["Jonsson", "Anders", "G\u00f3mez", "Vicen\u00e7"], "venue": "arXiv preprint arXiv:1603.03267,", "citeRegEx": "Jonsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jonsson et al\\.", "year": 2016}, {"title": "Hierarchical learning in stochastic domains: Preliminary results", "author": ["Kaelbling", "Leslie Pack"], "venue": "In ICML, pp", "citeRegEx": "Kaelbling and Pack.,? \\Q1993\\E", "shortCiteRegEx": "Kaelbling and Pack.", "year": 1993}, {"title": "Hierarchical apprenticeship learning with application to quadruped locomotion", "author": ["Kolter", "J Zico", "Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Kolter et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kolter et al\\.", "year": 2007}, {"title": "Building portable options: Skill transfer in reinforcement learning", "author": ["Konidaris", "George", "Barto", "Andrew G"], "venue": "In IJCAI,", "citeRegEx": "Konidaris et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2007}, {"title": "Skill discovery in continuous reinforcement learning domains using skill chaining", "author": ["Konidaris", "George", "Barto", "Andrew G"], "venue": "In NIPS, pp", "citeRegEx": "Konidaris et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2009}, {"title": "Robot learning from demonstration by constructing skill trees", "author": ["Konidaris", "George", "Kuindersma", "Scott", "Grupen", "Roderic A", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Konidaris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2012}, {"title": "Transition state clustering: Unsupervised surgical trajectory segmentation for robot learning", "author": ["Krishnan", "Sanjay", "Garg", "Animesh", "Patil", "Sachin", "Lea", "Colin", "Hager", "Gregory", "Abbeel", "Pieter", "Goldberg", "Ken"], "venue": "ISRR,", "citeRegEx": "Krishnan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krishnan et al\\.", "year": 2015}, {"title": "SWIRL: A sequential windowed inverse reinforcement learning algorithm for robot tasks with delayed rewards", "author": ["Krishnan", "Sanjay", "Garg", "Animesh", "Liaw", "Richard", "Thananjeyan", "Brijen", "Miller", "Lauren", "Pokorny", "Florian T", "Goldberg", "Ken"], "venue": null, "citeRegEx": "Krishnan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishnan et al\\.", "year": 2016}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Kulkarni", "Tejas D", "Narasimhan", "Karthik", "Saeedi", "Ardavan", "Tenenbaum", "Josh"], "venue": "In NIPS,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Option discovery in hierarchical reinforcement learning using spatiotemporal clustering", "author": ["Lakshminarayanan", "Aravind S", "Krishnamurthy", "Ramnandan", "Kumar", "Peeyush", "Ravindran", "Balaraman"], "venue": "arXiv preprint arXiv:1605.05359,", "citeRegEx": "Lakshminarayanan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lakshminarayanan et al\\.", "year": 2016}, {"title": "Unified inter and intra options learning using policy gradient methods", "author": ["Levy", "Kfir Y", "Shimkin", "Nahum"], "venue": "In European Workshop on Reinforcement Learning,", "citeRegEx": "Levy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2011}, {"title": "Composing meta-policies for autonomous driving using hierarchical deep reinforcement", "author": ["Liaw", "Richard", "Krishnan", "Sanjay", "Garg", "Animesh", "Crankshaw", "Daniel", "Gonzalez", "Joseph E", "Goldberg", "Ken"], "venue": null, "citeRegEx": "Liaw et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Liaw et al\\.", "year": 2017}, {"title": "Automatic discovery of subgoals in reinforcement learning using diverse density", "author": ["McGovern", "Amy", "Barto", "Andrew G"], "venue": "In ICML, pp", "citeRegEx": "McGovern et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 2001}, {"title": "The EM algorithm and extensions, volume 382", "author": ["McLachlan", "Geoffrey", "Krishnan", "Thriyambakam"], "venue": null, "citeRegEx": "McLachlan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McLachlan et al\\.", "year": 2007}, {"title": "Q-cut\u2014 dynamic discovery of sub-goals in reinforcement learning", "author": ["Menache", "Ishai", "Mannor", "Shie", "Shimkin", "Nahum"], "venue": "In ECML,", "citeRegEx": "Menache et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["Parr", "Ronald", "Russell", "Stuart J"], "venue": "In NIPS, pp", "citeRegEx": "Parr et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Parr et al\\.", "year": 1997}, {"title": "Hierarchical control and learning for Markov decision processes", "author": ["Parr", "Ronald Edward"], "venue": "PhD thesis, UNIVERSITY of CALIFORNIA at BERKELEY,", "citeRegEx": "Parr and Edward.,? \\Q1998\\E", "shortCiteRegEx": "Parr and Edward.", "year": 1998}, {"title": "Optimization with EM and expectation-conjugategradient", "author": ["Salakhutdinov", "Ruslan", "Roweis", "Sam", "Ghahramani", "Zoubin"], "venue": "In ICML, pp", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2003}, {"title": "Universal value function approximators", "author": ["Schaul", "Tom", "Horgan", "Daniel", "Gregor", "Karol", "Silver", "David"], "venue": "In ICML, pp", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Unsupervised perceptual rewards for imitation learning", "author": ["Sermanet", "Pierre", "Xu", "Kelvin", "Levine", "Sergey"], "venue": "arXiv preprint arXiv:1612.06699,", "citeRegEx": "Sermanet et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2016}, {"title": "Learning to repeat: Fine grained action repetition for deep reinforcement learning", "author": ["Sharma", "Sahil", "Lakshminarayanan", "Aravind S", "Ravindran", "Balaraman"], "venue": "In ICLR,", "citeRegEx": "Sharma et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2017}, {"title": "Using relative novelty to identify useful temporal abstractions in reinforcement learning", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Barto", "Andrew G"], "venue": "In ICML,", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2004\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2004}, {"title": "Optimal behavioral hierarchy", "author": ["Solway", "Alec", "Diuk", "Carlos", "C\u00f3rdova", "Natalia", "Yee", "Debbie", "Barto", "Andrew G", "Niv", "Yael", "Botvinick", "Matthew M"], "venue": "PLOS Comput Biol,", "citeRegEx": "Solway et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Solway et al\\.", "year": 2014}, {"title": "Automated discovery of options in reinforcement learning", "author": ["Stolle", "Martin"], "venue": "PhD thesis, McGill University,", "citeRegEx": "Stolle and Martin.,? \\Q2004\\E", "shortCiteRegEx": "Stolle and Martin.", "year": 2004}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder P"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Learning from the memory of Atari 2600", "author": ["Sygnowski", "Jakub", "Michalewski", "Henryk"], "venue": "arXiv preprint arXiv:1605.01335,", "citeRegEx": "Sygnowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sygnowski et al\\.", "year": 2016}, {"title": "Finding structure in reinforcement learning", "author": ["Thrun", "Sebastian", "Schwartz", "Anton"], "venue": "In NIPS, pp", "citeRegEx": "Thrun et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1994}, {"title": "Imitation of hierarchical action structure by young children", "author": ["Whiten", "Andrew", "Flynn", "Emma", "Brown", "Katy", "Lee", "Tanya"], "venue": "Developmental science,", "citeRegEx": "Whiten et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Whiten et al\\.", "year": 2006}, {"title": "Prediction error associated with the perceptual segmentation of naturalistic events", "author": ["Zacks", "Jeffrey M", "Kurby", "Christopher A", "Eisenberg", "Michelle L", "Haroutunian", "Nayiri"], "venue": "Journal of Cognitive Neuroscience,", "citeRegEx": "Zacks et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zacks et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 46, "context": "One approach is to augment the agent\u2019s controls with useful higher-level behaviors called options (Sutton et al., 1999), each consisting of a control policy for one region of the state space, and a termination condition recognizing leaving that region.", "startOffset": 98, "endOffset": 119}, {"referenceID": 1, "context": "Despite recent results in option discovery, some proposed techniques do not generalize well to multi-level hierarchies (Bacon et al., 2016; Heess et al., 2016; Kulkarni et al., 2016), while others are inefficient for learning expressive representations (Bui et al.", "startOffset": 119, "endOffset": 182}, {"referenceID": 17, "context": "Despite recent results in option discovery, some proposed techniques do not generalize well to multi-level hierarchies (Bacon et al., 2016; Heess et al., 2016; Kulkarni et al., 2016), while others are inefficient for learning expressive representations (Bui et al.", "startOffset": 119, "endOffset": 182}, {"referenceID": 28, "context": "Despite recent results in option discovery, some proposed techniques do not generalize well to multi-level hierarchies (Bacon et al., 2016; Heess et al., 2016; Kulkarni et al., 2016), while others are inefficient for learning expressive representations (Bui et al.", "startOffset": 119, "endOffset": 182}, {"referenceID": 7, "context": ", 2016), while others are inefficient for learning expressive representations (Bui et al., 2002; Daniel et al., 2012; Hamidi et al., 2015; Lakshminarayanan et al., 2016).", "startOffset": 78, "endOffset": 169}, {"referenceID": 8, "context": ", 2016), while others are inefficient for learning expressive representations (Bui et al., 2002; Daniel et al., 2012; Hamidi et al., 2015; Lakshminarayanan et al., 2016).", "startOffset": 78, "endOffset": 169}, {"referenceID": 16, "context": ", 2016), while others are inefficient for learning expressive representations (Bui et al., 2002; Daniel et al., 2012; Hamidi et al., 2015; Lakshminarayanan et al., 2016).", "startOffset": 78, "endOffset": 169}, {"referenceID": 29, "context": ", 2016), while others are inefficient for learning expressive representations (Bui et al., 2002; Daniel et al., 2012; Hamidi et al., 2015; Lakshminarayanan et al., 2016).", "startOffset": 78, "endOffset": 169}, {"referenceID": 46, "context": "The field of hierarchical reinforcement learning has a long history (Barto & Mahadevan, 2003; Parr, 1998; Sutton et al., 1999), and has also been applied in robotics (Konidaris et al.", "startOffset": 68, "endOffset": 126}, {"referenceID": 25, "context": ", 1999), and has also been applied in robotics (Konidaris et al., 2012; Krishnan et al., 2016; Sermanet et al., 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al.", "startOffset": 47, "endOffset": 117}, {"referenceID": 27, "context": ", 1999), and has also been applied in robotics (Konidaris et al., 2012; Krishnan et al., 2016; Sermanet et al., 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al.", "startOffset": 47, "endOffset": 117}, {"referenceID": 40, "context": ", 1999), and has also been applied in robotics (Konidaris et al., 2012; Krishnan et al., 2016; Sermanet et al., 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al.", "startOffset": 47, "endOffset": 117}, {"referenceID": 5, "context": ", 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al., 2009; Solway et al., 2014; Whiten et al., 2006; Zacks et al., 2011).", "startOffset": 50, "endOffset": 153}, {"referenceID": 43, "context": ", 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al., 2009; Solway et al., 2014; Whiten et al., 2006; Zacks et al., 2011).", "startOffset": 50, "endOffset": 153}, {"referenceID": 49, "context": ", 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al., 2009; Solway et al., 2014; Whiten et al., 2006; Zacks et al., 2011).", "startOffset": 50, "endOffset": 153}, {"referenceID": 50, "context": ", 2016) and in the analysis of biological systems (Botvinick, 2008; Botvinick et al., 2009; Solway et al., 2014; Whiten et al., 2006; Zacks et al., 2011).", "startOffset": 50, "endOffset": 153}, {"referenceID": 22, "context": "Early work in hierarchical control demonstrated the advantages of hierarchical structures by handcrafting hierarchical policies (Brooks, 1986) and by learning them given various manual specifications: state abstractions (Dayan & Hinton, 1992; Hengst, 2002; Kolter et al., 2007; Konidaris & Barto, 2007), a set of waypoints (Kaelbling, 1993), low-level skills (Bacon & Precup, 2015; Huber & Grupen, 1997; Liaw et al.", "startOffset": 220, "endOffset": 302}, {"referenceID": 31, "context": ", 2007; Konidaris & Barto, 2007), a set of waypoints (Kaelbling, 1993), low-level skills (Bacon & Precup, 2015; Huber & Grupen, 1997; Liaw et al., 2017), a set of finite-state meta-controllers (Parr & Russell, 1997), a set of subgoals (Dietterich, 2000; Sutton et al.", "startOffset": 89, "endOffset": 152}, {"referenceID": 46, "context": ", 2017), a set of finite-state meta-controllers (Parr & Russell, 1997), a set of subgoals (Dietterich, 2000; Sutton et al., 1999), or intrinsic reward (Kulkarni et al.", "startOffset": 90, "endOffset": 129}, {"referenceID": 28, "context": ", 1999), or intrinsic reward (Kulkarni et al., 2016).", "startOffset": 29, "endOffset": 52}, {"referenceID": 29, "context": "Since then, the focus of research has shifted towards discovery of the hierarchical structure itself, by: trading off value with description length (Thrun & Schwartz, 1994), identifying transitional states (Lakshminarayanan et al., 2016; McGovern & Barto, 2001; Menache et al., 2002; \u015eim\u015fek & Barto, 2004; Stolle, 2004), inference from demonstrations (Bui et al.", "startOffset": 206, "endOffset": 319}, {"referenceID": 34, "context": "Since then, the focus of research has shifted towards discovery of the hierarchical structure itself, by: trading off value with description length (Thrun & Schwartz, 1994), identifying transitional states (Lakshminarayanan et al., 2016; McGovern & Barto, 2001; Menache et al., 2002; \u015eim\u015fek & Barto, 2004; Stolle, 2004), inference from demonstrations (Bui et al.", "startOffset": 206, "endOffset": 319}, {"referenceID": 7, "context": ", 2002; \u015eim\u015fek & Barto, 2004; Stolle, 2004), inference from demonstrations (Bui et al., 2002; Daniel et al., 2012; Krishnan et al., 2015; 2016), iteratively expanding the set of solvable initial states (Konidaris & Barto, 2009; Konidaris et al.", "startOffset": 75, "endOffset": 143}, {"referenceID": 8, "context": ", 2002; \u015eim\u015fek & Barto, 2004; Stolle, 2004), inference from demonstrations (Bui et al., 2002; Daniel et al., 2012; Krishnan et al., 2015; 2016), iteratively expanding the set of solvable initial states (Konidaris & Barto, 2009; Konidaris et al.", "startOffset": 75, "endOffset": 143}, {"referenceID": 26, "context": ", 2002; \u015eim\u015fek & Barto, 2004; Stolle, 2004), inference from demonstrations (Bui et al., 2002; Daniel et al., 2012; Krishnan et al., 2015; 2016), iteratively expanding the set of solvable initial states (Konidaris & Barto, 2009; Konidaris et al.", "startOffset": 75, "endOffset": 143}, {"referenceID": 25, "context": ", 2015; 2016), iteratively expanding the set of solvable initial states (Konidaris & Barto, 2009; Konidaris et al., 2012), policy gradient (Levy & Shimkin, 2011), trading off value with informational constraints (Florensa et al.", "startOffset": 72, "endOffset": 121}, {"referenceID": 12, "context": ", 2012), policy gradient (Levy & Shimkin, 2011), trading off value with informational constraints (Florensa et al., 2017; Fox et al., 2016; Genewein et al., 2015; Jonsson & G\u00f3mez, 2016), active learning (Hamidi et al.", "startOffset": 98, "endOffset": 185}, {"referenceID": 13, "context": ", 2012), policy gradient (Levy & Shimkin, 2011), trading off value with informational constraints (Florensa et al., 2017; Fox et al., 2016; Genewein et al., 2015; Jonsson & G\u00f3mez, 2016), active learning (Hamidi et al.", "startOffset": 98, "endOffset": 185}, {"referenceID": 15, "context": ", 2012), policy gradient (Levy & Shimkin, 2011), trading off value with informational constraints (Florensa et al., 2017; Fox et al., 2016; Genewein et al., 2015; Jonsson & G\u00f3mez, 2016), active learning (Hamidi et al.", "startOffset": 98, "endOffset": 185}, {"referenceID": 16, "context": ", 2015; Jonsson & G\u00f3mez, 2016), active learning (Hamidi et al., 2015), or recently value-function approximation (Bacon et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 1, "context": ", 2015), or recently value-function approximation (Bacon et al., 2016; Heess et al., 2016; Sharma et al., 2017).", "startOffset": 50, "endOffset": 111}, {"referenceID": 17, "context": ", 2015), or recently value-function approximation (Bacon et al., 2016; Heess et al., 2016; Sharma et al., 2017).", "startOffset": 50, "endOffset": 111}, {"referenceID": 41, "context": ", 2015), or recently value-function approximation (Bacon et al., 2016; Heess et al., 2016; Sharma et al., 2017).", "startOffset": 50, "endOffset": 111}, {"referenceID": 7, "context": "Discovery of multi-level hierarchies was shown to be possible via particle filters (Bui et al., 2002), however we are seeking more expressive representations.", "startOffset": 83, "endOffset": 101}, {"referenceID": 9, "context": "Our work is most related to (Daniel et al., 2016), who use a similar generative model, originally introduced by (Bui et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 7, "context": ", 2016), who use a similar generative model, originally introduced by (Bui et al., 2002) as an Abstract Hidden Markov Model, and learn its parameters via the Expectation-Maximization (EM) algorithm.", "startOffset": 70, "endOffset": 88}, {"referenceID": 17, "context": "Gradient-descent algorithms for value-function approximation with deep networks has been used to train hierarchical policies (Heess et al., 2016; Kulkarni et al., 2016), using a Universal Value Function Approximator (Schaul et al.", "startOffset": 125, "endOffset": 168}, {"referenceID": 28, "context": "Gradient-descent algorithms for value-function approximation with deep networks has been used to train hierarchical policies (Heess et al., 2016; Kulkarni et al., 2016), using a Universal Value Function Approximator (Schaul et al.", "startOffset": 125, "endOffset": 168}, {"referenceID": 39, "context": ", 2016), using a Universal Value Function Approximator (Schaul et al., 2015).", "startOffset": 55, "endOffset": 76}, {"referenceID": 35, "context": "When Q\u03b8 is represented by a Deep Q-Network (DQN) (Mnih et al., 2015), the update is a gradient step to reduce the mean square Bellman error over a batch of samples Lp\u03b8q \u201c  \u0301Erpyi  \u0301Q\u03b8psi, aiqqs \u03b8 \u00d0 \u03b8 ` \u03b1\u2207\u03b8Lp\u03b8q, with learning rate \u03b1, and yi based on the fixed current parameter \u03b8 \u0301, independent of \u03b8.", "startOffset": 49, "endOffset": 68}, {"referenceID": 46, "context": "The options framework is a hierarchical policy structure that can mitigate this sample complexity by breaking down the policy into simpler skills called options (Sutton et al., 1999).", "startOffset": 161, "endOffset": 182}, {"referenceID": 39, "context": "This generic notation allows us the flexibility of a completely separate network for each option, \u03b8 \u201c p\u03b8hqhPH, or the efficiency of sharing some of the parameters between options, similarly to a Universal Value Function Approximator (Schaul et al., 2015).", "startOffset": 233, "endOffset": 254}, {"referenceID": 38, "context": "which is the so-called Expectation-Gradient method (McLachlan & Krishnan, 2007; Salakhutdinov et al., 2003).", "startOffset": 51, "endOffset": 107}, {"referenceID": 13, "context": "The challenge is the coupling between the levels; namely, the value of a set of options is determined by its usefulness for meta-control (Fox et al., 2016), while the value of a meta-control policy depends on which options are available.", "startOffset": 137, "endOffset": 155}], "year": 2017, "abstractText": "Augmenting an agent\u2019s control with useful higher-level behaviors called options can greatly reduce the sample complexity of reinforcement learning, but manually designing options is infeasible in high-dimensional and abstract state spaces. While recent work has proposed several techniques for automated option discovery, they do not scale to multi-level hierarchies and to expressive representations such as deep networks. We present Discovery of Deep Options (DDO), a policy-gradient algorithm that discovers parametrized options from a set of demonstration trajectories, and can be used recursively to discover additional levels of the hierarchy. The scalability of our approach to multi-level hierarchies stems from the decoupling of low-level option discovery from high-level meta-control policy learning, facilitated by under-parametrization of the high level. We demonstrate that using the discovered options to augment the action space of Deep Q-Network agents can accelerate learning by guiding exploration in tasks where random actions are unlikely to reach valuable states. We show that DDO is effective in adding options that accelerate learning in 4 out of 5 Atari RAM environments chosen in our experiments. We also show that DDO can discover structure in robot-assisted surgical videos and kinematics that match expert annotation with 72% accuracy. Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute.", "creator": "LaTeX with hyperref package"}}}