{"id": "1703.08537", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Crowdsourcing Universal Part-Of-Speech Tags for Code-Switching", "abstract": "code - switching is the phenomenon by observing large speakers switch between larger languages during communication. the importance of developing language technologies for codeswitching data is realized, given the large populations that routinely code - switch. high - quality linguistic annotations are extremely valuable for any nlp task, and memory is often limited by what amount given high - quality labeled data. however, considerable such data exists for code - switching. in this paper, we describe crowd - sourcing universal part - of - speech tags for the miami codex corpus of spanish - english code - switched speech. we split such annotation pathway into three subtasks : an about which a class of tokens are labeled automatically, one in which questions are specifically designed to disambiguate a subset of high frequency words, and a more general cascaded approach for the remaining data toward which questions am displayed to the worker following a decision tree structure. each subtask initially extended and adapted for a larger setting and the universal tagset. voting quality of the annotation process follows measured using hidden spaces, annotated opaque gold labels. the overall agreement is gold standard labels and the majority vote is between 0. 95 and 0. 96 wearing just three labels and the average recall across part - of - speech tags is between 0. 87 and 0. 99, depending on the selection.", "histories": [["v1", "Fri, 24 Mar 2017 17:55:33 GMT  (18kb)", "http://arxiv.org/abs/1703.08537v1", "Submitted to Interspeech 2017"]], "COMMENTS": "Submitted to Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["victor soto", "julia hirschberg"], "accepted": false, "id": "1703.08537"}, "pdf": {"name": "1703.08537.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["vsoto@cs.columbia.edu,", "julia@cs.columbia.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n08 53\n7v 1\n[ cs\n.C L\n] 2\n4 M\nar 2\n01 7\ners switch between multiple languages during communication. The importance of developing language technologies for codeswitching data is immense, given the large populations that routinely code-switch. High-quality linguistic annotations are extremely valuable for any NLP task, and performance is often limited by the amount of high-quality labeled data. However, little such data exists for code-switching. In this paper, we describe crowd-sourcing universal part-of-speech tags for the Miami Bangor Corpus of Spanish-English code-switched speech. We split the annotation task into three subtasks: one in which a subset of tokens are labeled automatically, one in which questions are specifically designed to disambiguate a subset of high frequency words, and a more general cascaded approach for the remaining data in which questions are displayed to the worker following a decision tree structure. Each subtask is extended and adapted for a multilingual setting and the universal tagset. The quality of the annotation process is measured using hidden check questions annotated with gold labels. The overall agreement between gold standard labels and the majority vote is between 0.95 and 0.96 for just three labels and the average recall across part-of-speech tags is between 0.87 and 0.99, depending on the task. Index Terms: annotation, code-switching, crowdsourcing, part-of-speech tags, resources"}, {"heading": "1. Introduction & Previous Work", "text": "Linguistic Code-Switching (CS) occurs when a multilingual speaker switches languages during written or spoken communication. CS typically involves two or more languages or varieties of a language and it is classified as inter-sentential when it occurs between utterances or intra-sentential when it occurs within the utterance. For example a Spanish-English speaker might say \u201cEl teacher me dijo que Juanito is very good at math.\u201d (The teacher told me that Juanito is very good at math).\nCS can be observed in various linguistic levels of representation for different language pairs: phonological, morphological, lexical, syntactic, semantic, and discourse/pragmatic switching. However, very little code-switching annotated data exists for language id, part-of-speech tags, or other syntactic, morphological, or discourse phenomena from which researchers can train statistical models. In this paper, we present an annotation scheme for obtaining part-of-speech (POS) tags for code-switching using a combination of expert knowledge and crowdsourcing. POS tags have been proven to be valuable features for NLP tasks like parsing, information extraction\nThanks to Morgan Ulinksi and Thamar Solorio for their useful advice and support. This research was funded in part by NSF CNS 1205556 \u201dCI-ADDO-NEW: Collaborative Research: A Repository for Annotating Multilingual Code Switched Data\u201d and in part by a Google Faculty Research Award.\nand machine translation [1]. They are also routinely used in language modeling for speech recognition and in the front-end component of speech synthesis for training and generation of pitch accents and phrase boundaries from text [2, 3, 4, 5, 6].\nWith the advent of large scale machine learning approaches, the annotation of large datasets has become increasingly challenging and expensive. Linguistic annotations by domain experts are key to any language understanding task, but unfortunately they are also expensive and slow to obtain. One widely adopted solution is crowdsourcing. In crowdsourcing naive annotators submit annotations for the same items on crowdsourcing platforms such as Amazon Mechanical Turk (AMT) and Crowdflower (CF). These are then aggregated into a single label using a decision rule like majority vote. Crowdsourcing allows one to obtain annotations quickly at lower cost. It also raises some important questions about the validity and quality of the annotations, mainly: a) are aggregated labels by non-experts as good as labels by experts? b) what steps are necessary to ensure quality? and c) how do you explain complex tasks to non-experts to maximize output quality? [7].\nIn [8] the authors crowdsourced annotations in five different NLP tasks. To evaluate the quality of the new annotations they measured the agreement between the gold and crowdsourced labels. Furthermore, they showed that training a machine learning model on the crowdsourced labels yielded a high-performing model. [9] crowdsourced translation quality evaluations and found that by aggregating non-expert judgments it was possible to achieve the quality expected from experts. In [10] crowdsourcing was used to annotate sentiment in political snippets using multiple noisy labels. The authors showed that eliminating noisy annotators and ambiguous examples improved the quality of the annotations. [11] described a crowdsourced approach to obtaining Named Entity labels for Twitter data from a set of four labels using both AMT and CF. They found that a small fraction of workers completed most of the annotations and that those workers tended to score highest inter-annotator agreements. [12] proposes a two-step disambiguation task to extract prepositional phrase attachments from noisy blog data.\nThe task of crowdsourcing POS tags is challenging insofar as POS tagsets tend to be large and the task is intrinsically sequential. This means that workers need to be instructed about a large number of categories and they need to focus on more than the word to tag, making the task potentially longer, more difficult, and thus, more expensive. More importantly, even though broad differences between POS tags are not hard to grasp, more subtle differences tend to be critically important. An example would be deciding whether a word like \u201dup\u201d is being used as a preposition (\u201dHe lives up the street\u201d) or a particle (\u201dHe lived up to the expectations.\u201d)\nPrevious research has tackled the task of crowdsourcing POS tags. The authors in [13] collected five judgments per word in a task which consists of reading a short context where the word to be tagged occurs, and selecting the POS tag from\na drop-down menu. Using MACE [14] they obtained 82.6% accuracy and 83.7% when restricting the number of words to be tagged using dictionaries. In his M.S. thesis, Mainzer [15] proposed an interactive approach to crowdsourcing POS tags, where workers are assisted through a sequence of questions to help disambiguate the tags with minimal knowledge of linguistics. Workers following this approach for the Penn Treebank (PTB) Tagset [16] achieved 90% accuracy.\nIn this work, we propose to adapt the monolingual annotation scheme from [15] to crowdsource Universal POS tags in a code-switching setting for the Miami Bangor Corpus. Our main contributions are the following: finding mappings to the universal POS tagset, extending a monolingual annotation scheme to a code-switching setting, creating resources for the second language of the pair (Spanish) and creating a paradigm that others can adopt to annotate other code-switched language pairs."}, {"heading": "2. The Miami Bangor Corpus", "text": "The Miami Bangor corpus is a conversational speech corpus recorded from bilingual Spanish-English speakers living in Miami, FL. It includes 56 files of conversational speech from 84 speakers. The corpus consists of 242,475 words (transcribed) and 35 hours of recorded conversation. 63% of transcribed words are English, 34% Spanish, and 3% are undetermined. The manual transcripts include beginning and end times of utterances and per word language identification.\nThe original Bangor Miami corpus was automatically glossed and tagged with POS tags using the Bangor Autoglosser [17, 18]. The autoglosser finds the closest Englishlanguage gloss for each token in the corpus and assigns the tag or group of tags most common for that word in the annotated language. These tags have two main problems: they are unsupervised and the tagset used is uncommon and not specifically designed for multilingual text. To overcome these problems we decided to a) obtain new part-of-speech tags through in-lab annotation and crowdsourcing and b) to use the Universal Part-of-Speech Tagset [19]. The Universal POS tagset is ideal for annotating code-switching corpora because it was designed with the goal of being appropriate to any language. Furthermore, it is useful for crowdsourced annotations because it is much smaller than other widely-used tagsets. Comparing it to the PTB POS tagset [16, 20], which has a total of 45 tags, the Universal POS tagset has only 17: Adjective, Adposition, Adverb, Auxiliary Verb, Coordinating and Subordinating Conjunction, Determiner, Interjection, Noun, Numeral, Proper Noun, Pronoun, Particles, Punctuation, Symbol, Verb and Other. A detailed description of the tagset can be found in http://universaldependencies.org/u/pos/."}, {"heading": "3. Annotation Scheme", "text": "The annotation scheme we have developed consists of multiple tasks: each token is assigned to a tagging task depending on word identity, its language and whether it is present in one of three disjoint wordlists. The process combines a) manual annotation by computational linguists, b) automatic annotation based on knowledge distilled from the Penn TreeBank guidelines and the Universal Tagset guidelines, and c) and d) two language-specific crowdsourcing tasks, one for English and one for Spanish. The pseudocode of the annotation scheme is shown in Algorithm 1. Table 1 shows the number and percentage of tokens tagged in each annotation task (second and third column) and the percentage of tokens that was annotated by\nexperts in-lab, either because it was the manual task or because there was a tie in the crowdsourced task. In the next subsections we explain in detail each one of the annotation blocks. All the wordlists and sets of questions and answers mentioned but not included in the following sections are available in www.cs.columbia.edu/\u02dcvsoto/cspos_supplemental.pdf"}, {"heading": "3.1. Automatically tagged tokens", "text": "For English, the PTB Annotation guidelines [16] instructs annotators to tag a certain subset of words with a given POS tag. We follow those instructions by mapping the fixed PTB tag to a Universal tag. Moreover we expand this wordlist with a) English words that we found were always tagged with the same Universal tag in the Universal Dependencies Dataset and b) low-frequency words that we found only occur with a unique tag in the Bangor Corpus.\nSimilarly, for Spanish, we automatically tagged all the words tagged with a unique tag throughout the Universal Dependencies Dataset (e.g. conjunctions like \u2018aunque\u2019, \u2018e\u2019, \u2018o\u2019, \u2018y\u2019, etc.; adpositions like \u2018a\u2019, \u2018con\u2019, \u2018de\u2019, etc.; and some adverbs, pronouns and numerals) and low frequency words that only occurred with one tag throughout the Bangor corpus (e.g. \u2018aquella\u2019, \u2018tanta\u2019, \u2018bastantes\u2019, etc.).\nGiven the abundance of exclamations and interjections in conversational speech, we collected a list of frequent interjections in the corpus and tagged them automatically as INTJ. For example: \u2018ah\u2019, \u2018aha\u2019, \u2018argh\u2019, \u2018duh\u2019, \u2018oh\u2019, \u2018shh\u2019. Finally, tokens labeled as Named Entities or Proper Nouns in the original Miami Bangor Corpus were automatically tagged as PROPN."}, {"heading": "3.2. Manually tagged tokens", "text": "We identified a set of English and Spanish words that we found to be particularly challenging for naive workers to tag and which occurred in the dataset in such low frequency that we were able to have them tagged in the lab by computational linguists. Note\nthat a question specific to each one of these tokens could have been designed for crowdsourced annotations the way it was done for the words in section 3.3.1. The majority of these are tokens that needed to be disambiguated between adposition and adverb in English (e.g.\u2018above\u2019, \u2018across\u2019, \u2018below\u2019, \u2018between\u2019) and between determinant and pronoun in Spanish (e.g. \u2018algunos/as\u2019, \u2018cua\u0301ntos/as\u2019, \u2018muchos/as\u2019)."}, {"heading": "3.3. Crowdsourcing Universal Tags", "text": "We used crowdsourcing to obtain new gold labels for every word not manually or automatically labeled. We started with the two basic approaches discussed in [15] for disambiguating POS tags using crowdsourcing which we modified for a multilingual corpus. In the first task a question and a set of answers were designed to disambiguate the POS tag of a specific token. In the second task we defined two Question Trees (one for English and one for Spanish) that sequentially ask non-technical questions of the workers until the POS tag is disambiguated. These questions were designed so that the worker needs minimal knowledge of linguistics. All the knowledge needed, including definitions, is given as instructions or as examples in every set of questions and answers. Most of the answers contain examples illustrating the potential uses for the token in that answer.\nTwo judgments were collected from the pertinent crowdsourced task and a third one was computed from applying a mapping from the Bangor tagset to the Universal tagset The new gold standard was computed as the majority tag between the three POS tags."}, {"heading": "3.3.1. Token-specific questions (TSQ)", "text": "In this task, we designed a question and multiple answers specifically for particular word tokens. The worker was then asked to choose the answer that is the most true in his/her opinion. Below is the question we asked workers for the token \u2018can\u2019 (Note that users cannot see the POS tags when they select one of the answers):\nIn the context of the sentence, is \u2018can\u2019 a verb that takes the meaning of \u2018being able to\u2019 or \u2018know\u2019?\n\u2022 Yes. For example: \u2018I can speak Spanish.\u2019 (AUX)\n\u2022 No, it refers to a cylindrical container. For example:\n\u2018Pass me a can of beer.\u2019 (NOUN)\nWe began with the initial list of English words and the questions developed in [15] for English. However, we added additional token-specific questions for words that a) we thought would be especially challenging to label (e.g. \u2018as\u2019, \u2018off\u2019, \u2018on\u2019) and b) appear frequently throughout the corpus (e.g. \u2018anything\u2019, \u2018something\u2019, \u2018nothing\u2019).\nWe designed specific questions for a subset of Spanish words. Just as for English, we chose a subset of most frequent words that we thought would be especially challenging for annotation by workers like tokens that can be either adverbs or adpositions (e.g.\u2018como\u2019, \u2018cuando\u2019, \u2018donde\u2019) or determiners and pronouns (e.g. \u2018ese/a\u2019, \u2018este/a\u2019, \u2018la/lo\u2019) We modified many of the questions proposed in [15], to adapt them to a code-switching setting and to the universal POS tagset. For example, the token \u2018no\u2019 can be an Adverb and Interjection in Spanish, and also a Determiner in English. Also, some of our questions required workers to choose the most accurate translations for a token in a given context:\nIn the context of the sentence, would \u2018la\u2019 be translated in English as \u2018her\u2019 or \u2018the\u2019?\n\u2022 The (\u2018La nin\u0303a esta\u0301 corriendo\u2019 becomes \u2018The girl is run-\nning\u2019) (DET)\n\u2022 Her (\u2018La dije que parase\u2019 becomes \u2018I told her to stop\u2019)\n(PRON)"}, {"heading": "3.3.2. Annotations Following a Question Tree", "text": "In this task the worker is presented with a sequence of questions that follows a tree structure. Each answer selected by the user leads to the next question until a leaf node is reached, when the token is assigned a POS tag. We followed the basic tree structure proposed in [15], but needed to modify the trees considerably due again to the multilingual context. For example, the new Question Tree starts by first asking whether the token is an interjection or a proper noun. This is very important since any verb, adjective, adverb or noun can effectively be part of or itself be an interjection or proper noun. If the worker responds negatively, then they are asked to follow the rest of the tree. The resulting tree is slightly simpler than the one in [15]. This is mainly because we moved the Particle-Adverb-Adposition disambiguation from this task into the Token-Specific Questions task. On the other hand, we added question nodes designed to disambiguate between main verbs and auxiliary verbs. The following is an example of the annotation task following the English Question Tree:\nRead the sentence carefully: \u201cSabes porque I plan to move in August but I need to find a really good job.\u201d In the context of the sentence, is the word \u2018good\u2019:\n\u2022 A Proper Noun or part of a Proper Noun.\n\u2022 A single word used as an exclamation that expresses ac-\nknowledgement or an emotional reaction.\n\u2022 None of the above. X\nIn the context, \u2018good\u2019 is a:\n\u2022 Noun, because it names a thing, an animal, a place,\nevents or ideas.\n\u2022 Adjective, because it says something about the quality,\nquantity or the kind of noun or pronoun it refers to. X\n\u2022 Verb, because it is used to demonstrate an action or state\nof being.\n\u2022 Adverb, because it tells the how, where, when, when or\nthe degree at which something is done.\nCould \u2018good\u2019 be a noun or a verb?\n\u2022 It could be a Noun. For example, fun can be a noun as in\n... or an adjective as in...\n\u2022 It could be a Verb. For example, surprised can be a verb\nas in ... or an adjective as in ...\n\u2022 No, it\u2019s definitely an Adjective. X\nFor the Spanish portion of the corpus, we modified the English subtasks still further, adapting them according to the syntactic properties of Spanish. One of the key differences from the English tree concerns verbs in their infinitival form. Users that choose to tag a token as verb are then asked to confirm that the infinitival form is not a noun, and if it is not, to decide whether a verb is acting as main verb or as an auxiliary verb (as a compound verb or periphrasis)."}, {"heading": "3.3.3. Mapping Stage", "text": "We use the pre-annotated tag from the Bangor corpus as the third tag to aggregate using majority voting. To obtain it, we first cleaned the corpus of ambiguous tags, and then defined a mapping from the Bangor tagset to the Universal tagset. This mapping process was first published in [21]."}, {"heading": "4. Results", "text": "We assigned two judgments per token for each of our tasks. Before they were allowed to begin the tasks, workers were prescreened using a quiz of ten questions. If two or more questions were missed during the initial quiz, the worker was denied access to the task. Furthermore, workers were required to be certified for the Spanish language requirement in Crowdflower. Only workers from U.K., U.S.A., Spain, Mexico and Argentina were allowed access to the task. The tasks for the workers were designed to present 9 questions per page plus one test question used to assess workers\u2019 performance. When a worker reached an accuracy lower than 85% on these test questions, all their submitted judgments were discarded and the task made subsequently unavailable. Every set of 9+1 judgments was paid 5 cents (USD) for the Token-Specific Questions task and 6 cents for the Question Tree tasks.\nTable 2 shows the number of test questions for each task and of evaluation metrics to estimate the accuracy of the annotations obtained from the crowdsourcing workers. Taking into account all the judgments submitted for test questions, the majority voting tag had an accuracy of 0.97-0.98 depending on the task. These estimations are not expected to match the true accuracy we would get from the two judgments we obtained for the rest of non-test tokens, so we re-estimate the accuracy of the majority vote tag for every subset of one, two, three and four judgments collected, adding the initial Bangor tag. In this case we get an average accuracy ranging from 0.89-0.92 with just one token to 0.95-0.96 when using four tags. The best accuracy estimates for our POS tags are for the option of two crowdsourced tags and the Bangor tag, for which we obtained accuracies of 0.92 to 0.94. When looking at non-aggregated tags, the average accuracy per token of single judgments (SJ) were observed to be between 0.87 and 0.88. Measuring the agreement between single judgments and the majority vote (MV) per token, the average agreement value is between 0.87 and 0.89.\nWe examine the vote split for every non-test token to obtain a measure of confidence for the tags. We see that we consistently obtained full-confidence crowdsourced tags on at least 60% of the tokens for each of the tasks, reaching 70% for the Spanish Question Tree task. The option for which one of the\ncrowdsourced tags was different from the other two (marked as 2-1 Bangor) on the table occurred between 18% and 23% of the time depending on the task, whereas the split where the Bangor tag was different from the crowdsourced tags (marked as 2-1 CF) occurred only between 10.63 and 12.15% of the time. Finally the vote was split in three different categories only between 1.29% and 4.51% of the time. In those instances, the tie was broken by in-lab annotators. To further evaluate the perfor-\nmance of the annotation process by different tag categories, we examine the recall on the gold test questions. The recall across all tags and tasks is higher than 0.93 except for Interjections and Adjectives for the Spanish Question Tree and Adverbs for the English Question Tree. Looking at the failed test questions for Adverbs, it becomes apparent that workers had difficulty with adverbs of place that can also function as nouns, like: \u2018home\u2019, \u2018west\u2019, \u2018south\u2019, etc. For example \u2018home\u2019 in \u2018right when I got home\u2019 was tagged 24 times as a Noun, and only 5 as an Adverb."}, {"heading": "5. Conclusions", "text": "We have presented a new scheme for crowdsourcing Universal POS tagging of Spanish-English code-switched data derived from a monolingual process which also used a different tagset. Our scheme consists of four different tasks (one automatic, one manual, and two crowdsourced). Each word in the corpus is sent to only one task based upon curated wordlists. For the crowdsourced tokens, we have demonstrated that, taking the majority vote of one unsupervised tag and two crowdsourced judgments, we obtain highly accurate predictions. We have also shown high agreement on the predictions: between 95 and 99% of the tokens received two or more votes for the same tag. Looking at the performance of each POS tag, our predictions averaged between 0.88 and 0.93 recall depending on the task."}, {"heading": "6. References", "text": "[1] F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. M. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng et al., \u201cA smorgasbord of features for statistical machine translation.\u201d in HLTNAACL, 2004, pp. 161\u2013168.\n[2] P. Taylor, A. W. Black, and R. Caley, \u201cThe architecture of the festival speech synthesis system,\u201d 1998.\n[3] P. Taylor and A. W. Black, \u201cAssigning phrase breaks from partof-speech sequences,\u201d 1998.\n[4] H. Zen, K. Tokuda, and A. W. Black, \u201cStatistical parametric speech synthesis,\u201d Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009.\n[5] J. Hirschberg, \u201cAccent and discourse context: Assigning pitch accent in synthetic speech.\u201d in AAAI, vol. 90, 1990, pp. 952\u2013957.\n[6] O. Watts, J. Yamagishi, and S. King, \u201cUnsupervised continuousvalued word features for phrase-break prediction without a partof-speech tagger.\u201d in INTERSPEECH, 2011, pp. 2157\u20132160.\n[7] C. Callison-Burch and M. Dredze, \u201cCreating speech and language data with amazon\u2019s mechanical turk,\u201d in Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. Association for Computational Linguistics, 2010, pp. 1\u201312.\n[8] R. Snow, B. O\u2019Connor, D. Jurafsky, and A. Y. Ng, \u201cCheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks,\u201d in Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, 2008, pp. 254\u2013263.\n[9] C. Callison-Burch, \u201cFast, cheap, and creative: evaluating translation quality using amazon\u2019s mechanical turk,\u201d in Proceedings of the 2009 Conference on Empirical Methods in Natural Language\nProcessing: Volume 1-Volume 1. Association for Computational Linguistics, 2009, pp. 286\u2013295.\n[10] P.-Y. Hsueh, P. Melville, and V. Sindhwani, \u201cData quality from crowdsourcing: a study of annotation selection criteria,\u201d in Proceedings of the NAACL HLT 2009 workshop on active learning\nfor natural language processing. Association for Computational Linguistics, 2009, pp. 27\u201335.\n[11] T. Finin, W. Murnane, A. Karandikar, N. Keller, J. Martineau, and M. Dredze, \u201cAnnotating named entities in twitter data with crowdsourcing,\u201d in Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical\nTurk. Association for Computational Linguistics, 2010, pp. 80\u2013 88.\n[12] M. Jha, J. Andreas, K. Thadani, S. Rosenthal, and K. McKeown, \u201cCorpus creation for new genres: A crowdsourced approach to pp attachment,\u201d in Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechan-\nical Turk. Association for Computational Linguistics, 2010, pp. 13\u201320.\n[13] D. Hovy, B. Plank, and A. S\u00f8gaard, \u201cExperiments with crowdsourced re-annotation of a pos tagging data set.\u201d in ACL (2), 2014, pp. 377\u2013382.\n[14] D. Hovy, T. Berg-Kirkpatrick, A. Vaswani, and E. H. Hovy, \u201cLearning whom to trust with mace.\u201d in Proceedings of the NAACL HLT 2013. Association for Computational Linguistics, 2013, pp. 1120\u20131130.\n[15] J. E. Mainzer, \u201cLabeling parts of speech using untrained annotators on mechanical turk,\u201d Master\u2019s thesis, The Ohio State University, 2011.\n[16] B. Santorini, Part-of-speech tagging guidelines for the Penn Treebank Project (3rd revision), 3rd ed., LDC, UPenn, 1990, 2nd Printing.\n[17] K. Donnelly and M. Deuchar, \u201cThe bangor autoglosser: a multilingual tagger for conversational text,\u201d ITA11, Wrexham, Wales, 2011.\n[18] \u2014\u2014, \u201cUsing constraint grammar in the bangor autoglosser to disambiguate multilingual spoken text,\u201d in Constraint Grammar Applications: Proceedings of the NODALIDA 2011 Workshop, Riga,\nLatvia, 2011, pp. 17\u201325.\n[19] S. Petrov, D. Das, and R. McDonald, \u201cA universal part-of-speech tagset,\u201d arXiv preprint arXiv:1104.2086, 2011.\n[20] M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini, \u201cBuilding a large annotated corpus of english: The penn treebank,\u201d Computational linguistics, vol. 19, no. 2, pp. 313\u2013330, 1993.\n[21] F. AlGhamdi, G. Molina, M. Diab, T. Solorio, A. Hawwari, V. Soto, and J. Hirschberg, \u201cPart of speech tagging for code switched data,\u201d EMNLP 2016, p. 98, 2016."}], "references": [{"title": "A smorgasbord of features for statistical machine translation.", "author": ["F.J. Och", "D. Gildea", "S. Khudanpur", "A. Sarkar", "K. Yamada", "A.M. Fraser", "S. Kumar", "L. Shen", "D. Smith", "K. Eng"], "venue": "in HLT- NAACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "The architecture of the festival speech synthesis system", "author": ["P. Taylor", "A.W. Black", "R. Caley"], "venue": "1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Assigning phrase breaks from partof-speech sequences", "author": ["P. Taylor", "A.W. Black"], "venue": "1998.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A.W. Black"], "venue": "Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Accent and discourse context: Assigning pitch accent in synthetic speech.", "author": ["J. Hirschberg"], "venue": "in AAAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Unsupervised continuousvalued word features for phrase-break prediction without a partof-speech tagger.", "author": ["O. Watts", "J. Yamagishi", "S. King"], "venue": "INTERSPEECH,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Creating speech and language data with amazon\u2019s mechanical turk", "author": ["C. Callison-Burch", "M. Dredze"], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. Association for Computational Linguistics, 2010, pp. 1\u201312.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks", "author": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": "Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, 2008, pp. 254\u2013263.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast, cheap, and creative: evaluating translation quality using amazon\u2019s mechanical turk", "author": ["C. Callison-Burch"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1. Association for Computational Linguistics, 2009, pp. 286\u2013295.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Data quality from crowdsourcing: a study of annotation selection criteria", "author": ["P.-Y. Hsueh", "P. Melville", "V. Sindhwani"], "venue": "Proceedings of the NAACL HLT 2009 workshop on active learning for natural language processing. Association for Computational Linguistics, 2009, pp. 27\u201335.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Annotating named entities in twitter data with crowdsourcing", "author": ["T. Finin", "W. Murnane", "A. Karandikar", "N. Keller", "J. Martineau", "M. Dredze"], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. Association for Computational Linguistics, 2010, pp. 80\u2013 88.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Corpus creation for new genres: A crowdsourced approach to pp attachment", "author": ["M. Jha", "J. Andreas", "K. Thadani", "S. Rosenthal", "K. McKeown"], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. Association for Computational Linguistics, 2010, pp. 13\u201320.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning whom to trust with mace.", "author": ["D. Hovy", "T. Berg-Kirkpatrick", "A. Vaswani", "E.H. Hovy"], "venue": "Proceedings of the NAACL HLT", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Labeling parts of speech using untrained annotators on mechanical turk", "author": ["J.E. Mainzer"], "venue": "Master\u2019s thesis, The Ohio State University, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Part-of-speech tagging guidelines for the Penn Treebank Project (3rd revision)", "author": ["B. Santorini"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "The bangor autoglosser: a multilingual tagger for conversational text", "author": ["K. Donnelly", "M. Deuchar"], "venue": "ITA11, Wrexham, Wales, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Using constraint grammar in the bangor autoglosser to disambiguate multilingual spoken text", "author": ["\u2014\u2014"], "venue": "Constraint Grammar Applications: Proceedings of the NODALIDA 2011 Workshop, Riga, Latvia, 2011, pp. 17\u201325.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "A universal part-of-speech tagset", "author": ["S. Petrov", "D. Das", "R. McDonald"], "venue": "arXiv preprint arXiv:1104.2086, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, vol. 19, no. 2, pp. 313\u2013330, 1993.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Part of speech tagging for code switched data", "author": ["F. AlGhamdi", "G. Molina", "M. Diab", "T. Solorio", "A. Hawwari", "V. Soto", "J. Hirschberg"], "venue": "EMNLP 2016, p. 98, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "and machine translation [1].", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "They are also routinely used in language modeling for speech recognition and in the front-end component of speech synthesis for training and generation of pitch accents and phrase boundaries from text [2, 3, 4, 5, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 2, "context": "They are also routinely used in language modeling for speech recognition and in the front-end component of speech synthesis for training and generation of pitch accents and phrase boundaries from text [2, 3, 4, 5, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 3, "context": "They are also routinely used in language modeling for speech recognition and in the front-end component of speech synthesis for training and generation of pitch accents and phrase boundaries from text [2, 3, 4, 5, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 4, "context": "They are also routinely used in language modeling for speech recognition and in the front-end component of speech synthesis for training and generation of pitch accents and phrase boundaries from text [2, 3, 4, 5, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 5, "context": "They are also routinely used in language modeling for speech recognition and in the front-end component of speech synthesis for training and generation of pitch accents and phrase boundaries from text [2, 3, 4, 5, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 6, "context": "It also raises some important questions about the validity and quality of the annotations, mainly: a) are aggregated labels by non-experts as good as labels by experts? b) what steps are necessary to ensure quality? and c) how do you explain complex tasks to non-experts to maximize output quality? [7].", "startOffset": 299, "endOffset": 302}, {"referenceID": 7, "context": "In [8] the authors crowdsourced annotations in five different NLP tasks.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "[9] crowdsourced translation quality evaluations and found that by aggregating non-expert judgments it was possible to achieve the quality expected from experts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "In [10] crowdsourcing was used to annotate sentiment in political snippets using multiple noisy labels.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "[11] described a crowdsourced approach to obtaining Named Entity labels for Twitter data from a set of four labels using both AMT and CF.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] proposes a two-step disambiguation task to extract prepositional phrase attachments from noisy blog data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Using MACE [14] they obtained 82.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "thesis, Mainzer [15] proposed an interactive approach to crowdsourcing POS tags, where workers are assisted through a sequence of questions to help disambiguate the tags with minimal knowledge of linguistics.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "Workers following this approach for the Penn Treebank (PTB) Tagset [16] achieved 90% accuracy.", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "In this work, we propose to adapt the monolingual annotation scheme from [15] to crowdsource Universal POS tags in a code-switching setting for the Miami Bangor Corpus.", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "The original Bangor Miami corpus was automatically glossed and tagged with POS tags using the Bangor Autoglosser [17, 18].", "startOffset": 113, "endOffset": 121}, {"referenceID": 16, "context": "The original Bangor Miami corpus was automatically glossed and tagged with POS tags using the Bangor Autoglosser [17, 18].", "startOffset": 113, "endOffset": 121}, {"referenceID": 17, "context": "To overcome these problems we decided to a) obtain new part-of-speech tags through in-lab annotation and crowdsourcing and b) to use the Universal Part-of-Speech Tagset [19].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "Comparing it to the PTB POS tagset [16, 20], which has a total of 45 tags, the Universal POS tagset has only 17: Adjective, Adposition, Adverb, Auxiliary Verb, Coordinating and Subordinating Conjunction, Determiner, Interjection, Noun, Numeral, Proper Noun, Pronoun, Particles, Punctuation, Symbol, Verb and Other.", "startOffset": 35, "endOffset": 43}, {"referenceID": 18, "context": "Comparing it to the PTB POS tagset [16, 20], which has a total of 45 tags, the Universal POS tagset has only 17: Adjective, Adposition, Adverb, Auxiliary Verb, Coordinating and Subordinating Conjunction, Determiner, Interjection, Noun, Numeral, Proper Noun, Pronoun, Particles, Punctuation, Symbol, Verb and Other.", "startOffset": 35, "endOffset": 43}, {"referenceID": 14, "context": "For English, the PTB Annotation guidelines [16] instructs annotators to tag a certain subset of words with a given POS tag.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "We started with the two basic approaches discussed in [15] for disambiguating POS tags using crowdsourcing which we modified for a multilingual corpus.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "We began with the initial list of English words and the questions developed in [15] for English.", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "\u2018ese/a\u2019, \u2018este/a\u2019, \u2018la/lo\u2019) We modified many of the questions proposed in [15], to adapt them to a code-switching setting and to the universal POS tagset.", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "We followed the basic tree structure proposed in [15], but needed to modify the trees considerably due again to the multilingual context.", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": "The resulting tree is slightly simpler than the one in [15].", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "This mapping process was first published in [21].", "startOffset": 44, "endOffset": 48}], "year": 2017, "abstractText": "Code-switching is the phenomenon by which bilingual speakers switch between multiple languages during communication. The importance of developing language technologies for codeswitching data is immense, given the large populations that routinely code-switch. High-quality linguistic annotations are extremely valuable for any NLP task, and performance is often limited by the amount of high-quality labeled data. However, little such data exists for code-switching. In this paper, we describe crowd-sourcing universal part-of-speech tags for the Miami Bangor Corpus of Spanish-English code-switched speech. We split the annotation task into three subtasks: one in which a subset of tokens are labeled automatically, one in which questions are specifically designed to disambiguate a subset of high frequency words, and a more general cascaded approach for the remaining data in which questions are displayed to the worker following a decision tree structure. Each subtask is extended and adapted for a multilingual setting and the universal tagset. The quality of the annotation process is measured using hidden check questions annotated with gold labels. The overall agreement between gold standard labels and the majority vote is between 0.95 and 0.96 for just three labels and the average recall across part-of-speech tags is between 0.87 and 0.99, depending on the task.", "creator": "LaTeX with hyperref package"}}}