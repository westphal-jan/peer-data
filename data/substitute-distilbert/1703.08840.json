{"id": "1703.08840", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2017", "title": "Inferring The Latent Structure of Human Decision-Making from Raw Visual Inputs", "abstract": "the goal of imitation induction proves to match example object behavior, without access involving a reinforcement signal. expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. we introduce an extension to the generative adversarial imitation learning method that can infer the latent structure of human decision - taking in realistic unsupervised view. our method can not only imitate complex behaviors, but also achieve interpretable and meaningful representations. we demonstrate whereas innate approach is applicable to high - dimensional environments including raw visual inputs. in naive highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human - like driving behaviors however accurately anticipate human actions. exemplary skill surpasses various baselines in terms namely performance and functionality.", "histories": [["v1", "Sun, 26 Mar 2017 16:20:36 GMT  (8149kb,D)", "http://arxiv.org/abs/1703.08840v1", "10 pages, 6 figures"]], "COMMENTS": "10 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["yunzhu li", "jiaming song", "stefano ermon"], "accepted": true, "id": "1703.08840"}, "pdf": {"name": "1703.08840.pdf", "metadata": {"source": "META", "title": "Inferring The Latent Structure of Human Decision-Making  from Raw Visual Inputs", "authors": ["Yunzhu Li", "Jiaming Song", "Stefano Ermon"], "emails": ["<leo.liyunzhu@pku.edu.cn>,", "mon@cs.stanford.edu>."], "sections": [{"heading": "1. Introduction", "text": "A key limitation of reinforcement learning (RL) is that it involves the optimization of a predefined reward function or reinforcement signal (Levine & Koltun, 2013; Schulman et al., 2015a; Lillicrap et al., 2015; Schulman et al., 2015b; Silver et al., 2016; Tamar et al., 2016). Defining a reward function is straightforward in some cases, e.g., in games such as Go or chess. However, designing an appropriate reward function can be difficult in more complex and less well-specified environments, e.g., for autonomous driving where there is a need to balance safety and efficiency.\nImitation learning methods have the potential to close this gap by learning how to perform tasks directly from expert demonstrations, and has succeeded in a wide range of\n1Peking University, Beijing, China 2Stanford University, California, USA. Correspondence to: Yunzhu Li <leo.liyunzhu@pku.edu.cn>, Stefano Ermon <ermon@cs.stanford.edu>.\nproblems (Ziebart et al., 2008; Ratliff et al., 2009; Stadie et al., 2017). Among them, Generative Adversarial Imitation Learning (GAIL, Ho & Ermon (2016)) is a modelfree imitation learning method that is highly effective and scales to relatively high dimensional models. The training process of GAIL can be thought of as building a generative model, a stochastic policy that reacts to a fixed simulation environment, to produce behavior that matches the expert demonstrations. To determine a reasonable distance metric, a discriminator is jointly trained to distinguish expert trajectories from ones produced by the policy.\nIn imitation learning, example demonstrations are typically provided by human experts. These demonstrations can show significant variability. For example, they might be collected from multiple experts, each employing a different policy. External latent factors of variation that are not explicitly captured by the simulation environment can also significantly affect the observed behavior. For example, expert driving demonstrations might be collected from users with different skills and habits. The goal of this paper is to develop an imitation learning framework that is able to automatically discover and disentangle the latent factors of variation underlying human decision-making. Analogous to the goal of uncovering style, shape, and color in generative modeling of images (Chen et al., 2016), we aim to automatically learn concepts such as driver aggressiveness from human demonstrations.\nWe propose a new method for learning a latent variable generative model of trajectories in a dynamic environment that not only accurately reproduce expert behavior, but also learns a latent space that is semantically meaningful. Our approach is an extension of GAIL, where the objective is augmented with a mutual information term between the latent variables and the observed state-action pairs. We demonstrate an application in autonomous driving, where we learn to imitate complex driving behaviors while learning semantically meaningful structure, without any supervision beyond the expert trajectories. Remarkably, our method performs directly on raw visual inputs, using raw pixels as the only source of perceptual information.\nIn particular, the contributions of this paper are threefold:\nar X\niv :1\n70 3.\n08 84\n0v 1\n[ cs\n.L G\n] 2\n6 M\nar 2\n01 7\n1. We introduce a component which maximizes the mutual information between latent structure and trajectories, similar to InfoGAN (Chen et al., 2016) , resulting in a policy where low-level actions can be controlled through high-level latent variables.\n2. We extend GAIL to use raw pixels as input and produce human-like behaviors in complex highdimensional environments.\n3. We demonstrate an application to autonomous highway driving using the TORCS driving simulator (Wymann et al., 2000). We first demonstrate that the learned policy is able to navigate traffic without collisions. Then, we show that by specifying high-level latent variables, our model learns to reproduce different styles of human-like driving behavior."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. Preliminaries", "text": "We use the tuple (S,A, P, r , \u03c10, \u03b3) to define an infinitehorizon, discounted Markov decision process (MDP), where S represents the state space, A represents the action space, P : S \u00d7 A \u00d7 S \u2192 R denotes the transition probability distribution, r : S \u2192 R denotes the reward function, \u03c10 : S \u2192 R is the distribution of the initial state s0, and \u03b3 \u2208 (0, 1) is the discount factor. Let \u03c0 denote a stochastic policy \u03c0 : S \u00d7 A \u2192 [0, 1], and \u03c0E denote the expert policy to which we only have access to demonstrations. The expert demonstrations \u03c4E are a set of trajectories generated using policy \u03c0E , each of which consists of a sequence of state-action pairs."}, {"heading": "2.2. Imitation learning", "text": "The goal of imitation learning is to learn how to perform a task directly from expert demonstrations, without any access to the reinforcement signal r. Typically, there are two approaches to imitation learning: 1) behavior cloning (BC), which learns a policy through supervised learning over the state-action pairs from the expert trajectories; and 2) apprenticeship learning (AL), which assumes the expert policy is optimal under some unknown reward and learns a policy by recovering the reward and solving the corresponding planning problem. BC tends to have poor generalization properties due to compounding errors and covariate shift (Ross & Bagnell, 2010; Ross et al., 2011). AL, on the other hand, has the advantage of learning a reward function that can be used to score entire trajectories (Abbeel & Ng, 2004; Syed et al., 2008; Ho et al., 2016), but is typically expensive to run because it requires solving a reinforcement learning (RL) problem inside a learning loop."}, {"heading": "2.3. Generative Adversarial Imitation Learning", "text": "Recent work on AL has adopted a different approach by learning a policy without learning the reward function. In particular, Generative Adversarial Imitation Learning (GAIL, Ho & Ermon (2016) ) is a recent AL method inspired by Generative Adversarial Networks (GAN, Goodfellow et al. (2014)). In the GAIL framework, the agent imitates the behavior of an expert policy \u03c0E by matching the generated state-action distribution with the expert distribution, where the optimum is achieved when these two distributions match perfectly. However, measuring the similarity between high-dimensional distributions is complicated, so GAIL introduces a neural network to approximately minimize the Jensen-Shannon divergence. Intuitively, the neural network is a discriminator that tries to differentiate the two distributions. The formal GAIL objective is denoted as min\u03b8 max\u03c9 V (\u03b8, \u03c9), where V (\u03b8, \u03c9) is\nE\u03c0\u03b8 [logD\u03c9(s, a)] + E\u03c0E [log(1\u2212D\u03c9(s, a))]\u2212 \u03bbH(\u03c0\u03b8) (1)\nand \u03c0\u03b8 (usually a neural network parameterized by \u03b8) is the policy that we wish to imitate \u03c0E with, D\u03c9 is a discriminator network which tries to distinguish state-action pairs from the trajectories of \u03c0\u03b8 and \u03c0E , E\u03c0[f(s, a)] denotes the expectation of f over the state-action pairs generated by \u03c0, and H(\u03c0\u03b8) , E\u03c0\u03b8 [\u2212 log \u03c0\u03b8(a|s)] is the \u03b3-discounted causal entropy of the policy \u03c0\u03b8 (Bloem & Bambos, 2014). We assume that our policies are Gaussian distributions with fixed standard deviations, thusH(\u03c0\u03b8) is constant in our settings.\nInstead of directly learning a reward function, GAIL relies on the discriminator to guide \u03c0\u03b8 into imitating the expert policy. Optimization over the GAIL objective is performed by alternating between an Adam (Kingma & Ba, 2014) gradient step on \u03c9 to increase V (\u03b8, \u03c9) with respect toD, and a Trust Region Policy Optimization (TRPO, Schulman et al. (2015a)) step on \u03b8 to decrease V (\u03b8, \u03c9) with respect to \u03c0. GAIL is model-free: it requires a simulator to provide rollouts, but it does not need to construct a model for the environment."}, {"heading": "3. Visual InfoGAIL", "text": "Expert demonstrations are typically collected from human experts. The resulting trajectories can show significant variability among different individuals due to internal latent factors of variation, such as social values, character, or mood. In this section, we propose an approach that 1) can discover and disentangle salient latent factors underlying human decision-making without supervision, 2) learns policies that produce trajectories which correspond to these latent factors, and 3) uses visual inputs as the only external perceptual information.\nFormally, we assume that the expert policy is a mixture of experts, and we define the generative process of the expert trajectory \u03c4E as: s0 \u223c \u03c10, c \u223c p(c), \u03c0 \u223c p(\u03c0|c), at \u223c \u03c0(at|st), st+1 \u223c P (st+1|at, st) where c is a latent variable that selects a specific policy \u03c0 from the mixture of expert policies through p(\u03c0|c), and p(c) is the prior distribution of c. Similar to the settings in GAIL, we consider the apprenticeship learning problem as a dual of an occupancy measure matching problem, and treat the trajectory \u03c4 as a set of state-action pairs. The objective is to recover \u03c0(a|s, c) from the state-action pairs given the prior p(c)."}, {"heading": "3.1. Interpretable Imitation Learning", "text": "We model our parametrized policy as \u03c0\u03b8(a|s, c). To draw a connection between latent codes c and the behavior of \u03c0\u03b8, we utilize the information-theoretic regularization that there should be high mutual information between c and the state-action pairs in the generated trajectory. This concept is introduced by InfoGAN (Chen et al., 2016), where latent codes are utilized to discover the salient structured semantic features of the data distribution and guide the generating process. In particular, the regularization seeks to maximize the mutual information between latent codes and state-action pairs, denoted as I(c; s, a), which is hard to maximize directly as it requires access to the posterior. Hence we introduce a variational lower bound, LI(\u03c0\u03b8, Q), of the mutual information I(c; s, a)1:\nLI(\u03c0\u03b8, Q) = Ec\u223cp(c),a\u223c\u03c0\u03b8(\u00b7|s,c)[logQ(c|s, a)] +H(c) \u2264 I(c; s, a) (2)\nwhere Q(c|s, a) is an approximation of P (c|s, a) parameterized with weights \u03c8. The objective under this regularization, which we call Information Maximizing Generative Adversarial Imitation Learning (InfoGAIL), then becomes:\nmin \u03b8,\u03c8 max \u03c9 V (\u03b8, \u03c9)\u2212 \u03bb1LI(\u03c0\u03b8, Q\u03c8) (3)\nwhere \u03bb1 > 0 is the hyperparameter for information maximization regularization.\nBy introducing the latent code, InfoGAIL is able to identify the salient factors in the expert trajectories through mutual information maximization, and imitate the corresponding expert policy through generative adversarial training. This allows us to disentangle trajectories that may arise from a mixture of experts, such as different individuals performing the same task. We note that LI(\u03c0\u03b8, Q\u03c8) can be optimized through stochastic gradient methods, with \u03c0\u03b8 updated by TRPO, and Q\u03c8 updated by Adam.\n1Chen et al. (2016) presents a proof for the lower bound."}, {"heading": "3.2. Utilizing Raw Visual Inputs via Transfer Learning", "text": "In many real world applications, the state s is related to visual inputs, such as an image or a sequence of images; indeed, visual data is often inexpensive to obtain, highly informative, and heavily relied upon by people when performing tasks. Although our approach is general, we will focus on scenarios where the states s are represented by images. Despite recent successes in visual perception, this is a very challenging scenario as raw visual inputs are typically very high-dimensional. As a result, learning a policy mapping high-dimensional raw visual inputs to actions is particularly difficult. Intuitively, the policy will have to simultaneously learn how to identify meaningful visual features, and how to leverage them to achieve desired behavior.\nConvolutional neural networks (CNNs) have led to dramatic improvements across many computer vision tasks (Krizhevsky et al., 2012). Unfortunately, they require very large amounts of training data; therefore, using the trajectories to directly train the policy and image recognition will be expensive. Hence, methods to mitigate the high sample complexity problem are crucial to the success of InfoGAIL on raw visual inputs.\nHere, we take a transfer learning approach. Features extracted using a CNN pre-trained on ImageNet contain highlevel information about the input images, which can be adapted to new vision tasks via transfer learning (Yosinski et al., 2014). However, it is not yet clear whether these relatively high-level features can be directly applied to tasks where perception and action are tightly interconnected.\nWe show that it is indeed possible. We perform transfer learning by exploiting features from a pre-trained neural network that effectively convert raw images into relatively high-level information (Sharif Razavian et al., 2014). In particular, we use a Deep Residual Network (He et al., 2016) pre-trained on the ImageNet classification task (Russakovsky et al., 2015) to obtain the visual features used as inputs for the policy network."}, {"heading": "4. Improved Optimization", "text": "While GAIL is successful in tasks with low-dimensional inputs (in Ho & Ermon (2016), the largest observation has 376 continuous variables), few have explored tasks where the input dimension is very high (such as 110 \u00d7 200 \u00d7 3 pixels as in our experiments), even with pre-trained features from Residual Networks. In order to effectively learn a policy that relies solely on visual input, we make the following improvements over the original GAIL framework."}, {"heading": "4.1. Reward Augmentation", "text": "In complex and less well-specified environments, imitation learning methods have the potential to perform better than reinforcement learning methods since they do not require manual specification of an appropriate reward function. Assuming the expert is following an optimal policy under some unknown reward, and that the expert is optimal, the reward function learned from the expert trajectories should match the desired reinforcement signal. However, if the expert is performing sub-optimally, then reward functions recovered from the expert will be imperfect. Therefore, any policy trained under these learned rewards will be also suboptimal; in other words, the imitation learning agent\u2019s potential is bounded by the capabilities of the expert.\nIn many cases, it is very difficult to fully specify a suitable reward function for a given task, yet, it is relatively straightforward to come up with constraints that we would like to enforce over the policy. For example, even if we cannot design an appropriate reward function capturing the subtleties of autonomous driving, we know that an autonomous vehicle should avoid colliding with other objects.\nThis motivates the introduction of reward augmentation, a general framework to incorporate prior knowledge in imitation learning by providing additional incentives to the agent without interfering with the imitation learning process. We achieve this by specifying a surrogate state-based reward \u03b7(\u03c0\u03b8) = Es\u223c\u03c0\u03b8 [r(s)] that reflects our biases over the desired agent\u2019s behavior:\nmin \u03b8 max \u03c9\nV (\u03b8, \u03c9)\u2212 \u03bb0\u03b7(\u03c0\u03b8) (4)\nwhere \u03bb0 > 0 is a hyper-parameter. This approach can be seen as a hybrid between imitation and reinforcement learning, where part of the reinforcement signal for the policy optimization is coming from the surrogate reward and part from the discriminator, i.e., from mimicking the expert. The surrogate reward can also be thought of as side information provided to the generator. For example, in our autonomous driving experiment below we show that by providing the agent with a penalty if it collides with other cars, we are able to significantly reduce the collision rate of the policy.\nComputationally, this does not interfere with discriminator training, and for the policy gradient algorithm the surrogate reward is simply an additional gradient term for the TRPO update (Schulman et al., 2015a)."}, {"heading": "4.2. Wasserstein GAN", "text": "Wasserstein GAN (WGAN, Arjovsky et al. (2017)) is a recently proposed framework for generative adversarial training. Unlike its traditional GAN counterpart, the discriminator network in WGAN solves a regression problem instead\nof a classification problem by assigning scores to its inputs, and tries to maximize the score of real data while minimizing that of generated data. We extend the use of WGAN to the generative adversarial imitation learning framework, defining a new objective W (\u03b8, \u03c9):\nE\u03c0\u03b8 [D\u03c9(s, a)]\u2212 E\u03c0E [D\u03c9(s, a)] (5)\nExtending the analysis of (Arjovsky et al., 2017), it can be shown that if D is K-Lipschitz, the objective approximately minimizes the Earth-Mover (EM) distance between the distribution of trajectories from \u03c0\u03b8 and \u03c0E . This objective function suffers less from the vanishing gradient and mode collapse problems compared to traditional GANs. This is especially important in our setting, where we want to model complex distributions over trajectories that can potentially have a large number of modes. In Arjovsky et al. (2017), the K-Lipschitz property is satisfied by clipping weights in D\u03c9 to between [\u22120.01, 0.01], and using momentum-free optimization methods such as RMSProp (Tieleman & Hinton, 2012)."}, {"heading": "4.3. Variance Reduction", "text": "Policy gradients methods are notorious for suffering from high-variance gradients, since it is computationally expensive to obtain enough rollouts from the simulator to match the policy distribution. Therefore, we apply several variance-reduction techniques, such as replay buffer (Schaul et al., 2015) and baseline methods (Williams, 1992)."}, {"heading": "4.4. Algorithm", "text": "Apart from the baseline, we have three networks to update in the InfoGAIL framework: the discriminator network D\u03c9(s, a), the policy network \u03c0\u03b8(a|s, c), and the posterior estimator network Q\u03c8(c|s, a). We update D\u03c9 using RMSprop (as suggested in the original WGAN paper), and updateQ\u03c8 and \u03c0\u03b8 using Adam and TRPO respectively. The training procedure is presented in Algorithm 1. To speed up training, we initialize our policy from a behavior cloning policy, as in Ho & Ermon (2016).\nThe discriminator network D\u03c9 and the posterior approximation network Q\u03c8 are treated as distinct networks, as opposed to the InfoGAN approach where they share the same network parameters until the final output layer. This is because D\u03c9 requires weight clipping and momentumfree optimization methods, which is required by the current WGAN training framework. These changes would interfere with the training of an expressive Q\u03c8 if D\u03c9 and Q\u03c8 share the same network parameters.\nAlgorithm 1 InfoGAIL Input: Expert trajectories \u03c4E \u223c \u03c0E ; initial policy, discriminator and posterior parameters \u03b80, \u03c90, \u03c80; replay buffer B = \u2205; Output: Learned policy \u03c0\u03b8 for i = 0, 1, 2, ... do\nSample a batch of latent codes: ci \u223c P (c) Sample trajectories: \u03c4i \u223c \u03c0\u03b8i(ci), with the latent code fixed during each rollout. Update the replay buffer: B \u2190 B \u222a \u03c4i. Sample \u03c7i \u223c B and \u03c7E \u223c \u03c4E with same batch size. Update \u03c9i by ascending with gradients\n\u2206\u03c9 = E\u0302\u03c7i [\u2207\u03c9D\u03c9(s, a)]\u2212 E\u0302\u03c7E [\u2207\u03c9(D\u03c9(s, a))]\nClip the weights of \u03c9i to [\u22120.01, 0.01]. Update \u03c8i+1 by descending with gradients\n\u2206\u03c8 = \u2212\u03bb1E\u0302\u03c7i [\u2207\u03c8 logQ\u03c8(c|s, a)]\nTake a policy step from \u03b8i to \u03b8i+1, using the TRPO update rule with the following objective (without reward augmentation):\nE\u0302\u03c7i [D\u03c9i+1(s, a)]\u2212 \u03bb1LI(\u03c0\u03b8i , Q\u03c8i+1)\nor (with reward augmentation):\nE\u0302\u03c7i [D\u03c9i+1(s, a)]\u2212 \u03bb0\u03b7(\u03c0\u03b8i)\u2212 \u03bb1LI(\u03c0\u03b8i , Q\u03c8i+1)\nend for"}, {"heading": "5. Experiments", "text": "We demonstrate the performance of our method by applying it to a complex autonomous driving from visual inputs domain. By conducting experiments on a car racing simulator, we show that our learned policy \u03c0\u03b8 can 1) imitate human behavior using raw visual input using only a handful of expert demonstrations, 2) cluster human behaviors into different and semantically meaningful categories, and 3) reproduce different styles of human-like driving behaviors by setting the high-level latent variables."}, {"heading": "5.1. Environment Setup", "text": "The Open Racing Car Simulator (TORCS, Wymann et al. (2000)) is a popular simulator environment for research in autonomous vehicles. We packaged it into a client-server framework with APIs similar to OpenAI Gym (Brockman et al., 2016). Our framework produces a realistic dashboard view and driving related information, and communicates with the policy (client) through TCP packets, so that the policy can be written in languages other than C++. In particular, we implemented our policy using the TensorFlow Python API (Abadi et al., 2016). This framework and the code for reproducing the experiments are available at"}, {"heading": "128 fc 128 fc", "text": "https://github.com/YunzhuLi/InfoGAIL.\nAll of our experiments are conducted in the TORCS environment. The demonstrations are collected from human experts, by manually driving along the race track, and demonstrate typical behaviors like staying within lanes, avoiding collisions with other cars, and surpassing other cars. The policy accepts raw visual inputs as the only external inputs for the state, and produces a three-dimensional action that consists of steering, acceleration, and braking."}, {"heading": "5.2. Network Structure", "text": "In addition, our policy requires certain auxiliary information as internal input to serve as a short-term memory. These auxiliary information can be accessed along with the\nraw visual inputs. In our experiments, the auxiliary information for the policy at time t consists of the following: 1) velocity at time t, which is a three dimensional vector; 2) actions at time t\u22121 and t\u22122, which are both three dimensional vectors; 3) damage of the car, which is a real value. The auxiliary input has 10 dimensions in total.\nFor the policy network, input visual features are passed through two convolutional layers, and then combined with the auxiliary information vector and (in the case of InfoGAIL) the latent code c. The exact architecture for \u03c0\u03b8 is in Figure 1. Moreover, merging latent codes at the higher levels would have less effect over the actions, since the visual features have much larger dimensions. We parameterize the baseline as a network with the same architecture except for the final layer, which is just a one dimensional output indicates the expected accumulated future rewards.\nThe discriminator D\u03c9 accepts three elements as input: a resized image with lower resolution, the auxiliary information, and the current action. The output is a score for the WGAN training objective, which is supposed to be higher for expert state-action pairs, and lower for generated ones. Details for the architecture of D\u03c9 are shown in Figure 2. The posterior approximation network Q\u03c8 adopts the same architecture as the discriminator except that the output is a softmax over the discrete latent variables, or factored Gaussian over continuous latent variables."}, {"heading": "5.3. Inferring The Latent Structure of Human Decision-Making", "text": "In this experiment, we consider two subsets of human driving behaviors: turn, where the agent takes a turn using either the inside lane or the outside lane; and pass, where the agent passes another vehicle from either the left or the right. In both cases, the expert policy has two significant modes. Our goal is to have InfoGAIL capture the two modes from expert demonstrations.\nWe consider using a discrete latent code, which is a one-hot encoded vector with two possible states. For both settings, there are 80 expert trajectories in total, with 100 frames in each trajectory. The performance of a learned policy is quantified with two metrics: the average distance is determined by the distance traveled by the agent before a collision (and is bounded by the length of the simulation horizon), and accuracy is defined as the classification accuracy of the expert state-action pairs according to the latent code inferred with Q\u03c8 .\nThe average distance and sampled trajectories at different stages of training are shown in Figures 3 and 4 for turn and pass respectively. During the initial stages of training, the model does not distinguish the two modes and has a high chance of colliding, due to the limitations of behavior\ncloning (which we used to initialize the policy). As training progresses, trajectories provided by the learned policy begin to diverge. Towards the end of training, the two types of trajectories are clearly distinguishable, with only a few exceptions. In turn, [0, 1] corresponds to using the inside lane, while [1, 0] corresponds to the outside lane. In pass, the two kinds of latent codes are corresponding to passing from right and left separately. Meanwhile, the average distance of the rollouts steadily increases with more training.\nIn Figure 5, we show the visual input for the policy at certain time steps, during which the agent observes itself passing a corner or surpassing another vehicle from the side determined by the latent code. This behavior is visually similar to the corresponding expert behavior.\nLearning the two modes separately requires accurate inference of the latent code. To examine the accuracy of posterior inference, we select state-action pairs from the expert trajectories and obtain the corresponding latent code through Q\u03c8(c|s, a). See Table 1, although we did not explicitly provide labeling, our model is able to correctly distinguish over 81% of the state-action pairs in pass (and almost all the pairs in turn, which can be seen in Figure 3).\nFor comparison, we also visualize the trajectories of pass for the original GAIL objective in Figure 4, where there is no maximum mutual information regularization. GAIL learns the expert trajectories as a whole, and cannot distinguish the two modes in the expert policy.\nInterestingly, instead of learning two separate trajectories, GAIL tries to fit the left trajectory by swinging the car suddenly to the left after it has surpassed the other car from the right. We believe this reflects a limitation in the discriminators. Since D\u03c9(s, a) only requires state-action pairs as input, the policy is only required to match most of the stateaction pairs; matching each rollout with a particular expert trajectory is not necessary. InfoGAIL with discrete latent codes can alleviate this problem by forcing the model to learn separate trajectories."}, {"heading": "5.4. Ablation Experiments", "text": "We conduct a series of ablation experiments to demonstrate that our proposed techniques are indeed crucial for learning an effective policy. The experiments consider a long-term setting: our policy drives a car on the race track along with other cars, whereas the human expert provides trajectories by trying to drive as fast as possible without collision. Reward augmentation is performed by adding a reward that encourages the car to drive faster to the imitation learning objective. The performance of the policy is determined by the average distance. Therefore, a longer average rollout distance indicates a better policy.\nIn our ablation experiments, we remove parts of the\nimproved optimization methods in Section 4. InfoGAIL(Ours) includes all the optimization techniques; InfoGAIL\\WGAN switches the WGAN objective with the GAN objective; InfoGAIL\\RA removes the reward augmentation term from the objective; InfoGAIL\\RB removes the replay buffer and only samples from the most recent rollouts; Behavior Cloning is the behavior cloning method we use for initialization, and Human is the expert policy.\nTable 2 shows the average rollout distances of different policies. Our method is able to outperform the expert with the help of reward augmentation; policies without reward augmentation or WGANs perform slightly worse than the expert; removing the replay buffer causes the performance to deteriorate significantly due to increased variance in gradient estimation."}, {"heading": "6. Related work", "text": "There are two major paradigms for vision-based driving systems (Chen et al., 2015). Mediated perception is a two-step approach that first obtains scene information and then makes a driving decision (Aly, 2008; Lenz et al., 2011); behavior reflex, on the other hand, adopts a direct approach by mapping visual inputs to driving actions (Pomerleau, 1989; 1991). Many of the current autonomous\ndriving methods rely on the two-step approach, which requires hand-crafting features such as the detection of lane markings and cars (Geiger et al., 2013; Chen et al., 2015). Our approach, on the other hand, attempts to learn these features through end-to-end training. While mediated perception approaches are currently more prevalent, we believe that end-to-end learning methods are more scalable and may lead to better performance in the long run.\nBojarski et al. (2016) introduce an end-to-end imitation learning framework that learns to drive entirely from visual information, and test their approach on real-world scenarios. However, their method uses behavior cloning by performing supervised learning over the state-action pairs, which is well-known to generalize poorly to more sophisticated tasks, such as changing lanes or passing vehicles. With the use of GAIL, our method can learn to perform these sophisticated operations easily.\nMost imitation learning methods for end-to-end driving rely heavily on LIDAR-like inputs to obtain precise distance measurements (Ho et al., 2016; Kuefler et al., 2017). These inputs are not usually available to humans during driving. In particular, Kuefler et al. (2017) applies GAIL to the task of modeling human driving behavior on highways. Their policy is modeled using a recurrent neural network, which is supposed to maintain sufficient statistics of the past observations. In contrast, our policy requires only raw visual information as external input, which in practice is all the information humans need in order to drive.\nSermanet et al. (2016) have also introduced a pre-trained deep neural network to achieve better performance in imitation learning with relatively few demonstrations. Specifically, they introduce a pre-trained model to learn dense, incremental reward functions that are suitable for performing downstream reinforcement learning tasks, such as realworld robotic experiments. This is different from our approach, in that transfer learning is performed over the critic instead of the policy. Since pre-trained neural networks have displayed the potential to learn more sophisticated reward functions, it would be interesting to combine that reward with our approach through reward augmentation."}, {"heading": "7. Conclusion", "text": "In this paper, we present a method to imitate complex behaviors and at the same time identify salient latent factors of variation in human decision-making. Discovering these latent factors does not require direct supervision beyond expert demonstrations, and the whole process can be trained end-to-end with standard policy optimization algorithms. We also introduce several techniques to successfully perform end-to-end imitation learning using visual inputs, including transfer learning and reward augmentation. Our experimental results in the TORCS simulator clearly show that our methods can automatically distinguish certain behaviors in human driving, while learning a policy that can imitate and even outperform the expert behavior, with visual information as the sole external input. We hope\nthat our work can further inspire end-to-end learning approaches to autonomous driving under more realistic scenarios.\nAnother compelling direction for future work is to explore how current mediated perception approaches could be combined with direct imitation learning through reward augmentation. Scene information could provide us with more powerful reinforcement signals, which are crucial to training machines that are better at driving than humans."}, {"heading": "Acknowledgements", "text": "Toyota Research Institute (TRI) provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "Real time detection of lane markers in urban streets", "author": ["Aly", "Mohamed"], "venue": "In Intelligent Vehicles Symposium,", "citeRegEx": "Aly and Mohamed.,? \\Q2008\\E", "shortCiteRegEx": "Aly and Mohamed.", "year": 2008}, {"title": "Infinite time horizon maximum causal entropy inverse reinforcement learning", "author": ["Bloem", "Michael", "Bambos", "Nicholas"], "venue": "In Decision and Control (CDC),", "citeRegEx": "Bloem et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bloem et al\\.", "year": 2014}, {"title": "End to end learning for self-driving cars", "author": ["Bojarski", "Mariusz", "Del Testa", "Davide", "Dworakowski", "Daniel", "Firner", "Bernhard", "Flepp", "Beat", "Goyal", "Prasoon", "Jackel", "Lawrence D", "Monfort", "Mathew", "Muller", "Urs", "Zhang", "Jiakai"], "venue": "arXiv preprint arXiv:1604.07316,", "citeRegEx": "Bojarski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojarski et al\\.", "year": 2016}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["Chen", "Chenyi", "Seff", "Ari", "Kornhauser", "Alain", "Xiao", "Jianxiong"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Vision meets robotics: The kitti dataset", "author": ["Geiger", "Andreas", "Lenz", "Philip", "Stiller", "Christoph", "Urtasun", "Raquel"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Geiger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Geiger et al\\.", "year": 2013}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Generative adversarial imitation learning", "author": ["Ho", "Jonathan", "Ermon", "Stefano"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2016}, {"title": "Model-free imitation learning with policy optimization", "author": ["Ho", "Jonathan", "Gupta", "Jayesh K", "Ermon", "Stefano"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Ho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Imitating driver behavior with generative adversarial networks", "author": ["Kuefler", "Alex", "Morton", "Jeremy", "Wheeler", "Tim", "Kochenderfer", "Mykel"], "venue": "arXiv preprint arXiv:1701.06699,", "citeRegEx": "Kuefler et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kuefler et al\\.", "year": 2017}, {"title": "Sparse scene flow segmentation for moving object detection in urban environments", "author": ["Lenz", "Philip", "Ziegler", "Julius", "Geiger", "Andreas", "Roser", "Martin"], "venue": "In Intelligent Vehicles Symposium (IV), 2011 IEEE,", "citeRegEx": "Lenz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lenz et al\\.", "year": 2011}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Alvinn, an autonomous land vehicle in a neural network", "author": ["Pomerleau", "Dean A"], "venue": "Technical report,", "citeRegEx": "Pomerleau and A.,? \\Q1989\\E", "shortCiteRegEx": "Pomerleau and A.", "year": 1989}, {"title": "Efficient training of artificial neural networks for autonomous navigation", "author": ["Pomerleau", "Dean A"], "venue": "Neural Computation,", "citeRegEx": "Pomerleau and A.,? \\Q1991\\E", "shortCiteRegEx": "Pomerleau and A.", "year": 1991}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["Ratliff", "Nathan D", "Silver", "David", "Bagnell", "J Andrew"], "venue": "Autonomous Robots,", "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "Efficient reductions for imitation learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "Drew"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "Drew"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"], "venue": "In ICML, pp", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Unsupervised perceptual rewards for imitation learning", "author": ["Sermanet", "Pierre", "Xu", "Kelvin", "Levine", "Sergey"], "venue": "arXiv preprint arXiv:1612.06699,", "citeRegEx": "Sermanet et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2016}, {"title": "Cnn features off-theshelf: an astounding baseline for recognition", "author": ["Sharif Razavian", "Ali", "Azizpour", "Hossein", "Sullivan", "Josephine", "Carlsson", "Stefan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Third person imitation learning", "author": ["Stadie", "Bradly", "Abbeel", "Pieter", "Sutskever", "Ilya"], "venue": "In ICLR,", "citeRegEx": "Stadie et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2017}, {"title": "Apprenticeship learning using linear programming", "author": ["Syed", "Umar", "Bowling", "Michael", "Schapire", "Robert E"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Syed et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2008}, {"title": "Value iteration networks", "author": ["Tamar", "Aviv", "Levine", "Sergey", "Abbeel", "WU Pieter", "YI", "Thomas", "Garrett"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tamar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2016}, {"title": "Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural networks for machine learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Torcs, the open racing car simulator. Software available at http://torcs", "author": ["Wymann", "Bernhard", "Espi\u00e9", "Eric", "Guionneau", "Christophe", "Dimitrakakis", "Christos", "Coulom", "R\u00e9mi", "Sumner", "Andrew"], "venue": "sourceforge. net,", "citeRegEx": "Wymann et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Wymann et al\\.", "year": 2000}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Ziebart", "Brian D", "Maas", "Andrew L", "Bagnell", "J Andrew", "Dey", "Anind K"], "venue": "In AAAI,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": "A key limitation of reinforcement learning (RL) is that it involves the optimization of a predefined reward function or reinforcement signal (Levine & Koltun, 2013; Schulman et al., 2015a; Lillicrap et al., 2015; Schulman et al., 2015b; Silver et al., 2016; Tamar et al., 2016).", "startOffset": 141, "endOffset": 277}, {"referenceID": 27, "context": "A key limitation of reinforcement learning (RL) is that it involves the optimization of a predefined reward function or reinforcement signal (Levine & Koltun, 2013; Schulman et al., 2015a; Lillicrap et al., 2015; Schulman et al., 2015b; Silver et al., 2016; Tamar et al., 2016).", "startOffset": 141, "endOffset": 277}, {"referenceID": 32, "context": "problems (Ziebart et al., 2008; Ratliff et al., 2009; Stadie et al., 2017).", "startOffset": 9, "endOffset": 74}, {"referenceID": 17, "context": "problems (Ziebart et al., 2008; Ratliff et al., 2009; Stadie et al., 2017).", "startOffset": 9, "endOffset": 74}, {"referenceID": 25, "context": "problems (Ziebart et al., 2008; Ratliff et al., 2009; Stadie et al., 2017).", "startOffset": 9, "endOffset": 74}, {"referenceID": 17, "context": ", 2008; Ratliff et al., 2009; Stadie et al., 2017). Among them, Generative Adversarial Imitation Learning (GAIL, Ho & Ermon (2016)) is a modelfree imitation learning method that is highly effective and scales to relatively high dimensional models.", "startOffset": 8, "endOffset": 131}, {"referenceID": 30, "context": "We demonstrate an application to autonomous highway driving using the TORCS driving simulator (Wymann et al., 2000).", "startOffset": 94, "endOffset": 115}, {"referenceID": 19, "context": "BC tends to have poor generalization properties due to compounding errors and covariate shift (Ross & Bagnell, 2010; Ross et al., 2011).", "startOffset": 94, "endOffset": 135}, {"referenceID": 26, "context": "AL, on the other hand, has the advantage of learning a reward function that can be used to score entire trajectories (Abbeel & Ng, 2004; Syed et al., 2008; Ho et al., 2016), but is typically expensive to run because it requires solving a reinforcement learning (RL) problem inside a learning loop.", "startOffset": 117, "endOffset": 172}, {"referenceID": 8, "context": "AL, on the other hand, has the advantage of learning a reward function that can be used to score entire trajectories (Abbeel & Ng, 2004; Syed et al., 2008; Ho et al., 2016), but is typically expensive to run because it requires solving a reinforcement learning (RL) problem inside a learning loop.", "startOffset": 117, "endOffset": 172}, {"referenceID": 6, "context": "In particular, Generative Adversarial Imitation Learning (GAIL, Ho & Ermon (2016) ) is a recent AL method inspired by Generative Adversarial Networks (GAN, Goodfellow et al. (2014)).", "startOffset": 156, "endOffset": 181}, {"referenceID": 21, "context": "Optimization over the GAIL objective is performed by alternating between an Adam (Kingma & Ba, 2014) gradient step on \u03c9 to increase V (\u03b8, \u03c9) with respect toD, and a Trust Region Policy Optimization (TRPO, Schulman et al. (2015a)) step on \u03b8 to decrease V (\u03b8, \u03c9) with respect to \u03c0.", "startOffset": 205, "endOffset": 229}, {"referenceID": 11, "context": "Convolutional neural networks (CNNs) have led to dramatic improvements across many computer vision tasks (Krizhevsky et al., 2012).", "startOffset": 105, "endOffset": 130}, {"referenceID": 31, "context": "Features extracted using a CNN pre-trained on ImageNet contain highlevel information about the input images, which can be adapted to new vision tasks via transfer learning (Yosinski et al., 2014).", "startOffset": 172, "endOffset": 195}, {"referenceID": 7, "context": "In particular, we use a Deep Residual Network (He et al., 2016) pre-trained on the ImageNet classification task (Russakovsky et al.", "startOffset": 46, "endOffset": 63}, {"referenceID": 20, "context": "Therefore, we apply several variance-reduction techniques, such as replay buffer (Schaul et al., 2015) and baseline methods (Williams, 1992).", "startOffset": 81, "endOffset": 102}, {"referenceID": 30, "context": "The Open Racing Car Simulator (TORCS, Wymann et al. (2000)) is a popular simulator environment for research in autonomous vehicles.", "startOffset": 38, "endOffset": 59}, {"referenceID": 4, "context": "There are two major paradigms for vision-based driving systems (Chen et al., 2015).", "startOffset": 63, "endOffset": 82}, {"referenceID": 13, "context": "Mediated perception is a two-step approach that first obtains scene information and then makes a driving decision (Aly, 2008; Lenz et al., 2011); behavior reflex, on the other hand, adopts a direct approach by mapping visual inputs to driving actions (Pomerleau, 1989; 1991).", "startOffset": 114, "endOffset": 144}, {"referenceID": 5, "context": "driving methods rely on the two-step approach, which requires hand-crafting features such as the detection of lane markings and cars (Geiger et al., 2013; Chen et al., 2015).", "startOffset": 133, "endOffset": 173}, {"referenceID": 4, "context": "driving methods rely on the two-step approach, which requires hand-crafting features such as the detection of lane markings and cars (Geiger et al., 2013; Chen et al., 2015).", "startOffset": 133, "endOffset": 173}, {"referenceID": 8, "context": "Most imitation learning methods for end-to-end driving rely heavily on LIDAR-like inputs to obtain precise distance measurements (Ho et al., 2016; Kuefler et al., 2017).", "startOffset": 129, "endOffset": 168}, {"referenceID": 12, "context": "Most imitation learning methods for end-to-end driving rely heavily on LIDAR-like inputs to obtain precise distance measurements (Ho et al., 2016; Kuefler et al., 2017).", "startOffset": 129, "endOffset": 168}, {"referenceID": 8, "context": "Most imitation learning methods for end-to-end driving rely heavily on LIDAR-like inputs to obtain precise distance measurements (Ho et al., 2016; Kuefler et al., 2017). These inputs are not usually available to humans during driving. In particular, Kuefler et al. (2017) applies GAIL to the task of modeling human driving behavior on highways.", "startOffset": 130, "endOffset": 272}, {"referenceID": 8, "context": "Most imitation learning methods for end-to-end driving rely heavily on LIDAR-like inputs to obtain precise distance measurements (Ho et al., 2016; Kuefler et al., 2017). These inputs are not usually available to humans during driving. In particular, Kuefler et al. (2017) applies GAIL to the task of modeling human driving behavior on highways. Their policy is modeled using a recurrent neural network, which is supposed to maintain sufficient statistics of the past observations. In contrast, our policy requires only raw visual information as external input, which in practice is all the information humans need in order to drive. Sermanet et al. (2016) have also introduced a pre-trained deep neural network to achieve better performance in imitation learning with relatively few demonstrations.", "startOffset": 130, "endOffset": 656}], "year": 2017, "abstractText": "The goal of imitation learning is to match example expert behavior, without access to a reinforcement signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learning method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex behaviors, but also learn interpretable and meaningful representations. We demonstrate that the approach is applicable to high-dimensional environments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human-like driving behaviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.", "creator": "LaTeX with hyperref package"}}}