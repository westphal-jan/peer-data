{"id": "1703.09470", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "Learned Spectral Super-Resolution", "abstract": "we describe a novel method for blind, single - image spectral super - resolution. while conventional super - resolution aims to increase the spatial resolution of an input image, our goal is to mechanically shift the input, i. e., generate an image with the same pixels resolution, so hence greatly increased number of narrow ( hyper - spectral ) wave - length bands. modes like the spatial statistics of natural images develop rich contrasts, which one observer exploit as prior to producing high - frequency content from suitable low resolution image, the same is also true in the spectral domain : the materials and lighting conditions throughout the laser world induce systems exhibiting dense spectrum of wavelengths observed at a specified pixel. surprisingly, very little paper exists that attempts to follow this diagnosis and achieve blind spectral super - resolution from single slits. we suffer from the conjecture that, just like in the spatial domain, we can learn the statistics of natural image spectra, and with its help generate finely resolved hyper - spectral images from discrete input. technically, we follow the current best paradigm and implement a convolutional neural network ( cnn ), which is trained to carry out the end - to - end mapping from said entire rgb image to the corresponding hyperspectral image of equal size. we demonstrate spectral super - geometry both for conventional spectra sources and for multi - spectral signal data, outperforming the state - of - the - art.", "histories": [["v1", "Tue, 28 Mar 2017 09:17:38 GMT  (3427kb,D)", "http://arxiv.org/abs/1703.09470v1", "Submitted to ICCV 2017 (10 pages, 8 figures)"]], "COMMENTS": "Submitted to ICCV 2017 (10 pages, 8 figures)", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["silvano galliani", "charis lanaras", "dimitrios marmanis", "emmanuel baltsavias", "konrad schindler"], "accepted": false, "id": "1703.09470"}, "pdf": {"name": "1703.09470.pdf", "metadata": {"source": "CRF", "title": "Learned Spectral Super-Resolution", "authors": ["Silvano Galliani", "Charis Lanaras", "Dimitrios Marmanis", "Emmanuel Baltsavias", "Konrad Schindler"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Single-image super-resolution is a challenging computer vision problem with many interesting applications, e.g. in the fields of astronomy, medical imaging and law enforcement. The goal is to infer, from a single low-resolution image, the missing high frequency content that would be visible in a corresponding high resolution image. The problem itself is inherently ill-posed, extremely so for large upscaling factors. Still, several successful schemes have been designed [10]. The key is to exploit the high degree of struc-\nture in the visual world and design or learn a prior that constrains the solution accordingly.\nIndeed there is a large body of literature on single-image super-resolution, which is however largely limited to the spatial domain. Very few authors address the complementary problem, to increase the spectral resolution of the input image beyond the coarse RGB channels. The topic of this paper is single-image spectral super-resolution. We pose the obvious question whether we can also learn the spectral structure of the visual world, and use it as a prior to predict hyper-spectral images with finer spectral resolution from a standard RGB image.1 Note the trade-off between spatial\n1Or from some other image with similarly broad channels, e.g., a color infrared image.\nar X\niv :1\n70 3.\n09 47\n0v 1\n[ cs\n.C V\n] 2\n8 M\nar 2\nand spectral information, even at the sensor level: to obtain a reasonable signal-to-noise ratio, cameras can have small pixels and integrate over large spectral bands; or they can have fine spectral resolution, but integrate over large pixels.\nDepending on the available images and the application, it may be useful to increase the resolution in space or to obtain a finer quantisation of the visible spectrum. While in the spatial domain the restoration of missing high-frequency information reveals smaller objects and more accurate boundaries, high-frequency spectral information makes it easier to separate the spectral signatures of different objects and materials that have similar RGB color. The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].\nA related, but simpler problem has been studied by several authors, namely hyper-spectral super-resolution [2, 18, 24, 33]. There, one assumes that both a HS image of low spatial resolution and an RGB image with finer resolution are available, and the two are fused to get the best of both worlds. The desired output is thus the same as in our problem \u2014 but requires an additional input. Our work can be seen as an attempt to do away with the spatially coarse hyper-spectral image and learn a generic prior for hyperspectral signatures.\nThe problem is heavily under-constrained: for typical terrestrial applications, the goal is to generate, for each pixel, \u224830 spectral bands from the 3 input channels. The difference is even more extreme in aerial and satellite remote sensing, where the low-resolution image has at most 10 channels covering the visible and infrared range, whereas hyper-spectral images routinely have >200 bands over the same range. Still, there is evidence that blind spectral super-resolution is possible. For practical processing, hyper-spectral signatures are sometimes projected to a lower-dimensional subspace [4], indicating that there is a significant amount of correlation between their bands. Moreover, most scenes consist of a limited number of materials, distributed in characteristic patterns. Thus, there is hope that one can learn them from a suitable training set. Here, we do exactly that: we train a convolutional neural network (CNN) to predict the missing high-frequency detail of the colour spectrum observed at each pixel.\nThere are two main differences to spatial superresolution, which has also been tackled with CNNs. First, spatial super-resolution has the convenient property that training data can be obtained by downsampling existing images of the desired resolution, so training data is available for free in virtually unlimited quantities. This is not the case for our problem, because hyper-spectral cameras are not a ubiquitous consumer product, and training data is compara-\ntively rare. We nevertheless manage to obtain enough training data even if we are constrained to a more limited amount of image. In cases where the overall number of images is small we regularize the solution with an Euclidean penalized and additionally augment the training data by flipping and rotating input images. Second, and more importantly, the point spread functions of different cameras are rather similar and in general steep, whereas the spectral response (the \u201cspectral blur kernel\u201d) of the color channels can vary significantly from sensor to sensor. The latter means that an individual super-resolution has to be learned for each camera type."}, {"heading": "2. Related Work", "text": "Single-image super-resolution usually corresponds to spatially upsampling a single low-resolution RGB image to higher spatial resolution. This has been a popular topic for several years, and quite some literature exists. Early method attempted to devise clever upsampling functions, sometimes by manually analyzing the image statistics, while recently the trend has been to learn dictionaries of image patches, often in combination with a sparsity prior [36, 43, 47]. Lately, CNNs have boosted the performance of super-resolution, showing significant improvements [9, 20, 21]. They are also able to perform the task in real time [32]. Interestingly, the RMSE does not seem to be the best loss function to obtain visually convincing upsampling results. Other loss functions aiming for \u201cphotorealism\u201d better match human perception, although the actual intensity differences are higher [26].\nOn the other hand hyperspectral super-resolution uses as input a low resolution hyperspectral and an RGB image to create a high resolution hyperspectral output. There are two main schools. Some methods require only known spectral response of the RGB camera, but can correct for spatial mis-alignment known [1, 2, 18]. Others assume that also the registration between the two input images is perfectly known [24, 33, 38, 41, 45].\nOur work also has some relation to the problem of image colorization, where a grayscale image is spectrally upsampled to RGB, i.e., from one channel to three. There CNNs have also shown promising results [25, 48] by converting the input to a Lab colorspace and predicting the ab channels.\nAcquiring a hyperspectral image by using only an RGB camera has been attempted with the help of active lighting [7]. This can be achieved by using spectral filters in front of the illumination, with the main disadvantage that the method can only be used in the laboratory. A similar idea is to use tunable narrow-band filters and take multiple images, such that narrow spectral bands are recorded sequentially [13]. Taking a step further from tuning the hardware, Wug et al. [40] proposed the use of multiple RGB\nimages from different cameras, which are combined to obtain a single hyper-spectral image \u2014 effectively turning the differences between the camera\u2019s spectral responses into an advantage. All these solutions require dedicated hardware as well as a static scene.\nOn the contrary, attempts to reconstruct hyper-spectral information from a single RGB image are rare. Nguyen et al. [28] use a radial basis function network to model the mapping from RGB values to scene reflectance. They assume the camera\u2019s spectral response function is perfectly known (and, as a by-product, also estimate the spectral illumination). More recently, Arad et al. [3] proposed to learn sparse dictionary with K-SVD as hyper-spectral upsampling prior. Assuming the spectral response of the RGB camera is known, they then use Orthogonal Matching Pursuit (OMP) to reconstruct the hyper-spectral signal using from the RGB intensities. Closely related methods exist for spatial super-resolution [47] as well as hyper-spectral superresolution [1]. The two methods are closely related the only main technical difference is on which images are used to learn the dictionary. Zeyde et al. [47] employ low resolution hyperspectral image as prior while Akhtar et al. [1] compute their prior on a similar image contained inside their dataset.\nTo summarize, several constrained versions of the spectral super-resolution problem have been investigated. But we believe that our work is the first generic framework that requires only a single RGB image, no knowledge of the spectral response functions, can be used indoors and outdoors, and needs neither a static scene nor special filter hardware."}, {"heading": "3. Method", "text": "In our work we follow the current rend in computer vision research and learn the desired super-resolution mapping end-to-end with a (convolutonal) neural network. In the following we present the network architecture and give implementation details.\nSelecting a network architecture for deep learning is not straightforward for a novel application, where no prior studies point to suitable designs. It is however clear that, for our purposes, the output should have the same image size as the input (but more channels). We thus build on recent work in semantic segmentation. Our proposed network is a variant of the semantic segmentation architecture Tiramisu of Je\u0301gou et al. [17], which in turn is based on the Densenet [16] architecture for classification. As a first measure, we replace the loss function and use an Euclidean loss, since we face a regression-type problem: instead of class labels (respectively, class scores) our network shall predict the continuously varying intensities for all spectral bands. Additionally, since we are interested in the high fidelity representation of each pixel we replace the original deconvolution layer with subpixel upsampling as proposed by the superresolution work of Shi et al. [32].\nThe Tiramisu network has further interesting properties for our task. Skip connections, within and across Densenet blocks (see 2, 3) perform concatenation instead of summation of layers, as opposed to ResNet [14]. They greatly speed up the learning and alleviate the vanishing gradient problem. More importantly, its architecture is based on a multiscale paradigm which allows the network to learn the overall image structure, while keeping the image resolution constant. In the Tiramisu structure, each downscaling step is done with a convolutional layer of size 1 and max-pooling, while for each resolution level a single Densenet is used, with varying number of convolutional layers.\nOur network architecture, with a total of 56 layers, is depicted in Fig. 2 where, if otherwise specified, each convolution has size 3\u00d7 3.\nThe image gets down-scaled 5 times by a factor of 2,\nwith a 1 \u00d7 1 convolution followed by max-pooling. In it\u2019s own terminology, each Densenet block has a growth rate of 4 with 16 layers, which means 4 convolutional layers per block, each with 16 filters, see Fig. 3. For a more details about the Densenet/Tiramisu architecture, please refer to the original papers [16, 17].\nFor each image in the training dataset we randomly sample a set of patches of size 64 \u00d7 64 and directly feed them to the neural network. At test time, where the goal is to reconstruct the complete image, we tile the input into 64\u00d7 64 tiles, with 8 pixels overlap to avoid boundary artifacts."}, {"heading": "3.1. Relation to spectral unmixing", "text": "Often, hyper-spectral images are interpreted in terms of \u201cendmembers\u201d and \u201cabundances\u201d: the endmembers can be imagined as the pure spectra of the observed materials and form a natural basis. Observed pixel spectra are additive combinations of endmembers, with the abundances (proportions of different materials) as coefficients.\nDong et al. [9] showed how a shallow CNN for superresolution can be interpreted in terms of basis learning and reconstruction. In much the same manner, our CNN can be seen as an implicit, non-linear extension of the unmixing model, where the knowledge about the endmembers at both low and high spectral resolution is embedded in the convolution weights. The forward pass through the network can be thought of as first extracting the abundances from the input and then multiplying them with the learned endmembers to obtain the hyperspectral output image."}, {"heading": "3.2. Implementation details", "text": "The network, implemented with Keras [8], is trained from scratch, using the Adam optimizer [23] with Nesterov moment [34, 11]. We iterate it for 100 epochs with learning rate 0.002, then for another 200 epoch with learning rate 0.0002, the rest of the parameters follows those provided in the paper. We initialize our model with HeUniform [15], and apply 50% dropout in the convolutional layers to avoid overfitting. Moreover, we found it crucial to carefully tune the Euclidean regularization, probably due to the general lack of copious amount of images on the training set. We fix it to 10\u22126, higher values lead to overly smooth, less accurate solutions."}, {"heading": "4. Results", "text": "We evaluate our result on four different datasets. Where possible, we compare it with the other two methods [3, 28] that are also able to estimate an hyperspectral image from an single RGB. Note, both baselines need the spectral response of the RGB camera to be known. Hence, we feed it to them as additional input, contrary to our method. Despite the disadvantage, our CNN super-resolution is more accurate, see below.\nError computation We evaluate w.r.t. three different error metric over 8bit images (as far as they are available): root mean square error (RMSE), relative root mean square error (RMSERel), and the spectral angle mapper (SAM) [46], i.e., the average angular deviation between the estimated pixel spectra, measured in degrees. We would like to highlight how we measure RMSERel and RMSE as there\nGr ou\nnd tr\nut h\nRe fle\nct an ce Ng uy en e t a l. Re fle ct an ce Ou\nrs Di ffe re nc e Ou rs Di ffe re nc e Ng uy en e t a l.\n410 nm 490 nm 590 nm\n0\n0.1\n0.2\n0\n0.2\n0.4\n0.6\n0\n0.2\n0.4\n0.6\n0\n0.2\n0.4\n0.6\n0\n0.1\n0.2\nFigure 5. Qualitative comparison w.r.t. [28] over three non consecutive spectral bands.\nis not a common agreement on its computation. RMSE is obtained by computing it on 8bit and clipping values higher and lower than the allowed range. For RMSERel we normalized the predicted image by the mean of the ground truth."}, {"heading": "4.1. Training data", "text": "We follow the standard practice for quantitative evaluation and synthetically generate the input data, given the difficulties of capturing separate hyper-spectral and RGB images that are aligned and have comparable resolution and sharpness. I.e., the RGB image is emulated by integrating over hyper-spectral channels according to a predefined camera response function. We always use the response functions provided by the authors, to ensure the images are strictly the same and the comparisons are fair. If a dataset already provides a train/test split, we follow it. Otherwise, we run two-fold cross-validation: split the dataset in two, train on the first half to predict the second half and vice versa."}, {"heading": "4.2. ICVL dataset", "text": "The ICVL dataset has been released by Arad and BenShahar [3], together with their method. It contains 201 images acquired using a line scanner camera (Specim PS Kappa DX4 hyperspectral), mounted on a rotary stage for spatial scanning. The dataset contains a variety of scenes captured both indoors and outdoors, including man-made to natural objects. Images were originally captured with a spatial resolution of 1392\u00d71300 over 519 spectral bands (400-1,000nm ) but have been downsampled to 31 spectral channels from 400nm to 700nm at 10nm increments. We map the hyperspectral images to RGB using CIE 1964 color matching functions, like in the original paper.\nThere, a sparse dictionary is learned with K-SVD as hyper-spectral upsampling prior. However, they do not use a global train/test split, as we do. Rather, they divide the dataset into subsets of images that show the same type of scene (such as parks or indoor environments), hold out one test image per subset, and train on the remaining ones; thus learning a different prior for each test image that is specif-\nically tuned to the scene type. We prefer to keep the prior generic and use a single, global train/test split. We then predict their held-out images, but using the same network for all that fall into the first, respectively second half of our split. Even so, our results are competitive, see Table 1. We are not able to reproduce their method due to missing parameter or availability of code, instead we show the same figures presented in their paper in Fig. 4."}, {"heading": "4.3. NUS dataset", "text": "The NUS dataset [28] contains the spectral irradiance and spectral illumination (400-700 nm with step of 10 nm) for 66 outdoor and indoor scenes, captured under several different illuminations. In that dataset the authors already prescribe a train/test split. What their learning method does is to estimate both the reflectance and the illumination from an RGB image with known camera response function. In order to fairly evaluate their method, we run the authors\u2019 original code to estimate the reflectance, and convert it to radiance with the ground truth illumination. Of the three different camera response functions evaluated in their paper, we pick the one that gave them the best results (Canon 1D Mark III), to create the RGB images. Additionally we apply their ground truth illumination to our result to also compare reflectance. Also for this dataset our method obtain the best result in terms of RMSE, see Tables 2 and 3. In this case our SAM error was slightly worse, probably due to outlier on same channels which would increase considerably the error result."}, {"heading": "4.4. CAVE dataset", "text": "The CAVE dataset [44] is a popular hyper-spectral dataset. As opposed to all the other ones it is not captured\nwith a rotating line scanner. Instead, the hyper-spectral bands are recorded sequentially with a tunable filter. The main benefit is the elimination of possible noise when using a pushbroom scanner, while moving objects such as trees pose problems, because the bands are not correctly aligned. The dataset contains a variety of challenging objects to predict. The heterogeneity of the captured scenes makes it harder to learn a global prior for all scenes and challenges learning-based methods, like ours. Nevertheless, our method is competitive w.r.t. the number provided by [3], see Table 1."}, {"heading": "4.5. Satellite Data", "text": "We tested our method also on data captured from Hyperion satellite [31] a sensor on board the satellite EO-1. The satellite carries a hyperspectral line scanner that records 242 channels (from 0.4 to 2.5 m) at 30 m ground resolution, out of which 198 are calibrated and can be used. Our scenes are already cloud-free, have a size of \u2248256\u00d77000 pixels, and show the river Rhine in Western Europe. Note, like most satellite data the images are stored with an intensity range of 16 bits ber channel, and have an effective radiometric depth of\u22485000 different gray values. The input image is emulated by integrating the hyper-spectral bands into the channels of ALI, the 9-channel multispectral sensor on board the same satellite. As test bed, we use different acquisitions dates over (roughly) the same area. This is of course a favourable scenario for our method: since training and test data show the same region, the network can learn adapt to the specific structure present in the region, and potentially to some\ndegree even to the scene layout. Indeed, both the quantitative results in table 4 and the visual examples in Fig 6 validate the performance of our method over multiple visible and non-visible bands. While the training data is certainly favourable, it is not an unrealistic assumption that legacy hyper-spectral data for a given region is available. We find it quite remarkable that, according to the example, we are able to predict, with high accuracy, a finely resolved spectrum > 200 bands from a standard, multi-spectral satellite image."}, {"heading": "4.6. Denoising", "text": "An interesting property of our learned upsampling is that it can be used as a denoising method: downsampling the original images (as we do in our experiments) removes noise, but upsampling does not re-insert it. Indeed it is known that deep neural networks achieve state-of-the-art results in image denoising [42]. See the prediction in Fig. 7, note how the marked line artifacts in the ground truth get removed by our method. On the satellite data, which is in general much noisier, this effect gets very prominent. In most of the cases the predicted images for Hyperion are cleaner and more useful than the original \u201cground truth\u201d one. In Fig. 6 the difference images is dominated by the noise, while the \u201ctrue\u201d prediction error appears minimal. This claim is further supported by the fact that we were able to extract plausible spectral endmembers from the predicted hyperspectral images, which we found impossible for the originals."}, {"heading": "4.7. Hyperspectral Unmixing", "text": "We also check our reconstruction on satellite data, by performing hyperspectral unmixing [5] a process that separates material information (also called endmembers) and their location in the image (also called abundances). We take an of the shelf endmember extraction algorithm (VCA, [27]) to identify dominant spectral signatures in the images. Then, we perform a Fully Constrained Least Squares (FCLS) adjustment to extract the abundance maps, according to the Linear Mixing Model (LMM) [19]. The abundance maps show the presence of each endmember in each pixel and are constrained to be non-negative and sum to one. We select a subset of one image and extract 15 endmem-\nbers and their corresponding abundances, for three different cases of hyperspectral image: Our prediction, ground truth and ground truth denoised, by projecting the data points onto the first 15 principal components of the image (PCA\nprojection), see Fig 8. This kind of denoising method is suited for white noise as long as its variance is lower that that of the signal. Unfortunately, this is not enough to remove the noise from the abundance estimation, because the noise in this problem is not white, and so strong that apparently 15 principal components are insufficient to cover the underyling (obviously non-linear) subspace. As can be seen in Fig. 8 the ground truth itself cannot be used for hyperspectral unmixing as it is noisy. On the other hand, our method, only using 9 dimensions, is denoising the image as can be seen by the sharp abundance images, which clearly depict water, vegetation and urban areas, second row."}, {"heading": "5. Conclusions", "text": "We show that it is possible to do super resolution for image not only in the spatial domain but also in the spectral domain. Our method builds on a recent high-performance convolutional neural network, which was originally designed for semantic segmentation. Contrary to other work on spectral super-resolution, we train and predict directly the endto-end relation between an RGB image and its corresponding hyper-spectral image, without using any additional input, such as the spectral response function. We show the performance of our work on multiple indoor, outdoor and satellite datasets, where we compare favorably to other, less generic methods. We believe that our work may be useful for a number of applications that would benefit from higher spectral resolution, but where the recording conditions or the cost do not allow for routine use of hyper-spectral cameras."}], "references": [{"title": "Sparse spatio-spectral representation for hyperspectral image super-resolution", "author": ["N. Akhtar", "F. Shafait", "A. Mian"], "venue": "European Conference on Computer Vision (ECCV)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical beta process with gaussian process prior for hyperspectral image super resolution", "author": ["N. Akhtar", "F. Shafait", "A. Mian"], "venue": "European Conference on Computer Vision, pages 103\u2013120. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Sparse recovery of hyperspectral signal from natural rgb images", "author": ["B. Arad", "O. Ben-Shahar"], "venue": "European Conference on Computer Vision, pages 19\u201334. Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Hyperspectral remote sensing data analysis and future challenges", "author": ["J.M. Bioucas-Dias", "A. Plaza", "G. Camps-Valls", "P. Scheunders", "N.M. Nasrabadi", "J. Chanussot"], "venue": "IEEE, Geoscience and Remote Sensing Magazine, 1(2):6\u201336", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Hyperspectral unmixing overview: Geometrical", "author": ["J.M. Bioucas-Dias", "A. Plaza", "N. Dobigeon", "M. Parente", "Q. Du", "P. Gader", "J. Chanussot"], "venue": "statistical, and sparse regressionbased approaches. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 5(2):354\u2013 379", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Advances in hyperspectral image classification: Earth  monitoring with statistical learning methods", "author": ["G. Camps-Valls", "D. Tuia", "L. Bruzzone", "J.A. Benediktsson"], "venue": "IEEE Signal Processing Magazine, 31(1):45\u201354", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-spectral imaging by optimized wide band illumination", "author": ["C. Chi", "H. Yoo", "M. Ben-Ezra"], "venue": "International Journal of Computer Vision, 86(2-3):140", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/ keras", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a deep convolutional network for image super-resolution", "author": ["C. Dong", "C.C. Loy", "K. He", "X. Tang"], "venue": "European Conference on Computer Vision, pages 184\u2013199. Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization", "author": ["W. Dong", "L. Zhang", "G. Shi", "X. Wu"], "venue": "IEEE Transactions on Image Processing, 20(7):1838\u20131857", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Incorporating nesterov momentum into adam", "author": ["T. Dozat"], "venue": "Technical report, Stanford University, Tech. Rep., 2015.[Online]. Available: http://cs229. stanford. edu/proj2015/054 report. pdf", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multispectral camera and radiative transfer equation used to depict leonardo\u2019s sfumato in mona lisa", "author": ["M. Elias", "P. Cotte"], "venue": "Applied optics, 47(12):2146\u20132154", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Imaging spectroscopy using tunable filters: a review", "author": ["N. Gat"], "venue": "AeroSense 2000, pages 50\u201364. International Society for Optics and Photonics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "and L", "author": ["G. Huang", "Z. Liu", "K.Q. Weinberger"], "venue": "van der Maaten. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation", "author": ["S. J\u00e9gou", "M. Drozdzal", "D. Vazquez", "A. Romero", "Y. Bengio"], "venue": "arXiv preprint arXiv:1611.09326", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["R. Kawakami", "Y. Matsushita", "J. Wright"], "venue": "Ben-Ezra, Y.- W. Tai, and K. Ikeuchi. High-resolution hyperspectral imaging via matrix factorization. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 2329\u20132336. IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectral unmixing", "author": ["N. Keshava", "J.F. Mustard"], "venue": "IEEE signal processing magazine, 19(1):44\u201357", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Accurate image superresolution using very deep convolutional networks", "author": ["J. Kim", "J. Kwon Lee", "K. Mu Lee"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Deeply-recursive convolutional network for image super-resolution", "author": ["J. Kim", "J. Kwon Lee", "K. Mu Lee"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Visual enhancement of old documents with hyperspectral imaging", "author": ["S.J. Kim", "F. Deng", "M.S. Brown"], "venue": "Pattern Recognition, 44(7):1461\u20131469", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Hyperspectral super-resolution by coupled spectral unmixing", "author": ["C. Lanaras", "E. Baltsavias", "K. Schindler"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 3586\u20133594", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning representations for automatic colorization", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "European Conference on Computer Vision, pages 577\u2013593. Springer", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["C. Ledig", "L. Theis", "F. Husz\u00e1r", "J. Caballero", "A. Cunningham", "A. Acosta", "A. Aitken", "A. Tejani", "J. Totz", "Z. Wang"], "venue": "Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint arXiv:1609.04802", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Minimum volume simplex analysis: A fast algorithm to unmix hyperspectral data", "author": ["J. Li", "J.M. Bioucas-Dias"], "venue": "Geoscience and Remote Sensing Symposium, 2008. IGARSS 2008. IEEE International, volume 3, pages III\u2013250. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Trainingbased spectral reconstruction from a single rgb image", "author": ["R.M. Nguyen", "D.K. Prasad", "M.S. Brown"], "venue": "European Conference on Computer Vision, pages 186\u2013201. Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantitative hyperspectral imaging of historical documents: technique and applications", "author": ["R. Padoan", "T.A. Steemers", "M. Klein", "B. Aalderink", "G. De Bruin"], "venue": "Art Proceedings, pages 25\u201330", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Face recognition in hyperspectral images", "author": ["Z. Pan", "G. Healey", "M. Prasad", "B. Tromberg"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(12):1552\u20131560", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Hyperion", "author": ["J.S. Pearlman", "P.S. Barry", "C.C. Segal", "J. Shepanski", "D. Beiso", "S.L. Carman"], "venue": "a space-based imaging spectrometer. IEEE Transactions on Geoscience and Remote Sensing, 41(6):1160\u20131173", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}, {"title": "Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network", "author": ["W. Shi", "J. Caballero", "F. Huszar", "J. Totz", "A.P. Aitken", "R. Bishop", "D. Rueckert", "Z. Wang"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "A convex formulation for hyperspectral image superresolution via subspace-based regularization", "author": ["M. Sim\u00f5es", "J. Bioucas-Dias", "L.B. Almeida", "J. Chanussot"], "venue": "IEEE Transactions on Geoscience and Remote Sensing, 53(6):3373\u20133388", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton"], "venue": "ICML (3), 28:1139\u20131147", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmentation and classification of hyperspectral images using watershed transformation", "author": ["Y. Tarabalka", "J. Chanussot", "J.A. Benediktsson"], "venue": "Pattern Recognition, 43(7):2367\u20132379", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Anchored neighborhood regression for fast example-based super-resolution", "author": ["R. Timofte", "V. De Smet", "L. Van Gool"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Tracking via object reflectance using a hyperspectral video camera", "author": ["H. Van Nguyen", "A. Banerjee", "R. Chellappa"], "venue": "IEEE Computer Vision and Pattern Recognition Workshops (CVPRW), pages 44\u201351. IEEE", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast fusion of multi-band images based on solving a sylvester equation", "author": ["Q. Wei", "N. Dobigeon", "J.-Y. Tourneret"], "venue": "IEEE Transactions on Image Processing, 24(11):4109\u20134121", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Advanced applications of hyperspectral imaging technology for food quality and safety analysis and assessment: A reviewpart i: Fundamentals", "author": ["D. Wu", "D.-W. Sun"], "venue": "Innovative Food Science & Emerging Technologies, 19:1\u201314", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Do it yourself hyperspectral imaging with everyday digital cameras", "author": ["S. Wug Oh", "M.S. Brown", "M. Pollefeys", "S. Joo Kim"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2461\u20132469", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "A non-negative sparse promoting algorithm for high resolution hyperspectral imaging", "author": ["E. Wycoff", "T.-H. Chan", "K. Jia", "W.-K. Ma", "Y. Ma"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 1409\u20131413. IEEE", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["J. Xie", "L. Xu", "E. Chen"], "venue": "Advances in Neural Information Processing Systems, pages 341\u2013349", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Image superresolution via sparse representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "IEEE transactions on image processing, 19(11):2861\u20132873", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalized Assorted Pixel Camera: Post-Capture Control of Resolution, Dynamic Range and Spectrum", "author": ["F. Yasuma", "T. Mitsunaga", "D. Iso", "S. Nayar"], "venue": "Technical report,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Coupled nonnegative matrix factorization unmixing for hyperspectral and multispectral data fusion", "author": ["N. Yokoya", "T. Yairi", "A. Iwasaki"], "venue": "IEEE Transactions on Geoscience and Remote Sensing, 50(2):528\u2013537", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Discrimination among semi-arid landscape endmembers using the spectral angle mapper (sam", "author": ["R.H. Yuhas", "A.F. Goetz", "J.W. Boardman"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1992}, {"title": "On single image scale-up using sparse-representations", "author": ["R. Zeyde", "M. Elad", "M. Protter"], "venue": "International conference on curves and surfaces, pages 711\u2013730. Springer", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "European Conference on Computer Vision, pages 649\u2013666. Springer", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Still, several successful schemes have been designed [10].", "startOffset": 53, "endOffset": 57}, {"referenceID": 36, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 115, "endOffset": 119}, {"referenceID": 34, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 157, "endOffset": 161}, {"referenceID": 21, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 181, "endOffset": 189}, {"referenceID": 28, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 181, "endOffset": 189}, {"referenceID": 11, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 213, "endOffset": 217}, {"referenceID": 38, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 235, "endOffset": 239}, {"referenceID": 5, "context": "The extra information included in the recovered hyper-spectral (HS) image bands enables applications like tracking [37], segmentation [35], face recognition [30], document analysis [22, 29], analysis of paintings [12], food inspection [39] and image classification [6].", "startOffset": 265, "endOffset": 268}, {"referenceID": 1, "context": "A related, but simpler problem has been studied by several authors, namely hyper-spectral super-resolution [2, 18, 24, 33].", "startOffset": 107, "endOffset": 122}, {"referenceID": 17, "context": "A related, but simpler problem has been studied by several authors, namely hyper-spectral super-resolution [2, 18, 24, 33].", "startOffset": 107, "endOffset": 122}, {"referenceID": 23, "context": "A related, but simpler problem has been studied by several authors, namely hyper-spectral super-resolution [2, 18, 24, 33].", "startOffset": 107, "endOffset": 122}, {"referenceID": 32, "context": "A related, but simpler problem has been studied by several authors, namely hyper-spectral super-resolution [2, 18, 24, 33].", "startOffset": 107, "endOffset": 122}, {"referenceID": 3, "context": "For practical processing, hyper-spectral signatures are sometimes projected to a lower-dimensional subspace [4], indicating that there is a significant amount of correlation between their bands.", "startOffset": 108, "endOffset": 111}, {"referenceID": 35, "context": "Early method attempted to devise clever upsampling functions, sometimes by manually analyzing the image statistics, while recently the trend has been to learn dictionaries of image patches, often in combination with a sparsity prior [36, 43, 47].", "startOffset": 233, "endOffset": 245}, {"referenceID": 42, "context": "Early method attempted to devise clever upsampling functions, sometimes by manually analyzing the image statistics, while recently the trend has been to learn dictionaries of image patches, often in combination with a sparsity prior [36, 43, 47].", "startOffset": 233, "endOffset": 245}, {"referenceID": 46, "context": "Early method attempted to devise clever upsampling functions, sometimes by manually analyzing the image statistics, while recently the trend has been to learn dictionaries of image patches, often in combination with a sparsity prior [36, 43, 47].", "startOffset": 233, "endOffset": 245}, {"referenceID": 8, "context": "Lately, CNNs have boosted the performance of super-resolution, showing significant improvements [9, 20, 21].", "startOffset": 96, "endOffset": 107}, {"referenceID": 19, "context": "Lately, CNNs have boosted the performance of super-resolution, showing significant improvements [9, 20, 21].", "startOffset": 96, "endOffset": 107}, {"referenceID": 20, "context": "Lately, CNNs have boosted the performance of super-resolution, showing significant improvements [9, 20, 21].", "startOffset": 96, "endOffset": 107}, {"referenceID": 31, "context": "They are also able to perform the task in real time [32].", "startOffset": 52, "endOffset": 56}, {"referenceID": 25, "context": "Other loss functions aiming for \u201cphotorealism\u201d better match human perception, although the actual intensity differences are higher [26].", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": "Some methods require only known spectral response of the RGB camera, but can correct for spatial mis-alignment known [1, 2, 18].", "startOffset": 117, "endOffset": 127}, {"referenceID": 1, "context": "Some methods require only known spectral response of the RGB camera, but can correct for spatial mis-alignment known [1, 2, 18].", "startOffset": 117, "endOffset": 127}, {"referenceID": 17, "context": "Some methods require only known spectral response of the RGB camera, but can correct for spatial mis-alignment known [1, 2, 18].", "startOffset": 117, "endOffset": 127}, {"referenceID": 23, "context": "Others assume that also the registration between the two input images is perfectly known [24, 33, 38, 41, 45].", "startOffset": 89, "endOffset": 109}, {"referenceID": 32, "context": "Others assume that also the registration between the two input images is perfectly known [24, 33, 38, 41, 45].", "startOffset": 89, "endOffset": 109}, {"referenceID": 37, "context": "Others assume that also the registration between the two input images is perfectly known [24, 33, 38, 41, 45].", "startOffset": 89, "endOffset": 109}, {"referenceID": 40, "context": "Others assume that also the registration between the two input images is perfectly known [24, 33, 38, 41, 45].", "startOffset": 89, "endOffset": 109}, {"referenceID": 44, "context": "Others assume that also the registration between the two input images is perfectly known [24, 33, 38, 41, 45].", "startOffset": 89, "endOffset": 109}, {"referenceID": 24, "context": "There CNNs have also shown promising results [25, 48] by converting the input to a Lab colorspace and predicting the ab channels.", "startOffset": 45, "endOffset": 53}, {"referenceID": 47, "context": "There CNNs have also shown promising results [25, 48] by converting the input to a Lab colorspace and predicting the ab channels.", "startOffset": 45, "endOffset": 53}, {"referenceID": 6, "context": "Acquiring a hyperspectral image by using only an RGB camera has been attempted with the help of active lighting [7].", "startOffset": 112, "endOffset": 115}, {"referenceID": 12, "context": "A similar idea is to use tunable narrow-band filters and take multiple images, such that narrow spectral bands are recorded sequentially [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 39, "context": "[40] proposed the use of multiple RGB", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Note that, except for the first convolutions, the other blocks are made of a dense block as in [17]", "startOffset": 95, "endOffset": 99}, {"referenceID": 27, "context": "[28] use a radial basis function network to model the mapping from RGB values to scene reflectance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] proposed to learn sparse dictionary with K-SVD as hyper-spectral upsampling prior.", "startOffset": 0, "endOffset": 3}, {"referenceID": 46, "context": "Closely related methods exist for spatial super-resolution [47] as well as hyper-spectral superresolution [1].", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "Closely related methods exist for spatial super-resolution [47] as well as hyper-spectral superresolution [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 46, "context": "[47] employ low resolution hyperspectral image as prior while Akhtar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] compute their prior on a similar image contained inside their dataset.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17], which in turn is based on the Densenet [16] architecture for classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17], which in turn is based on the Densenet [16] architecture for classification.", "startOffset": 45, "endOffset": 49}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Skip connections, within and across Densenet blocks (see 2, 3) perform concatenation instead of summation of layers, as opposed to ResNet [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 2, "context": "[3]", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "For a more details about the Densenet/Tiramisu architecture, please refer to the original papers [16, 17].", "startOffset": 97, "endOffset": 105}, {"referenceID": 16, "context": "For a more details about the Densenet/Tiramisu architecture, please refer to the original papers [16, 17].", "startOffset": 97, "endOffset": 105}, {"referenceID": 8, "context": "[9] showed how a shallow CNN for superresolution can be interpreted in terms of basis learning and reconstruction.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The network, implemented with Keras [8], is trained from scratch, using the Adam optimizer [23] with Nesterov moment [34, 11].", "startOffset": 36, "endOffset": 39}, {"referenceID": 22, "context": "The network, implemented with Keras [8], is trained from scratch, using the Adam optimizer [23] with Nesterov moment [34, 11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 33, "context": "The network, implemented with Keras [8], is trained from scratch, using the Adam optimizer [23] with Nesterov moment [34, 11].", "startOffset": 117, "endOffset": 125}, {"referenceID": 10, "context": "The network, implemented with Keras [8], is trained from scratch, using the Adam optimizer [23] with Nesterov moment [34, 11].", "startOffset": 117, "endOffset": 125}, {"referenceID": 14, "context": "We initialize our model with HeUniform [15], and apply 50% dropout in the convolutional layers to avoid overfitting.", "startOffset": 39, "endOffset": 43}, {"referenceID": 2, "context": "Where possible, we compare it with the other two methods [3, 28] that are also able to estimate an hyperspectral image from an single RGB.", "startOffset": 57, "endOffset": 64}, {"referenceID": 27, "context": "Where possible, we compare it with the other two methods [3, 28] that are also able to estimate an hyperspectral image from an single RGB.", "startOffset": 57, "endOffset": 64}, {"referenceID": 45, "context": "three different error metric over 8bit images (as far as they are available): root mean square error (RMSE), relative root mean square error (RMSERel), and the spectral angle mapper (SAM) [46], i.", "startOffset": 188, "endOffset": 192}, {"referenceID": 27, "context": "[28] over three non consecutive spectral bands.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The ICVL dataset has been released by Arad and BenShahar [3], together with their method.", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "[3] on ICVL and CAVE dataset.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Ours Arad et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] RMSE 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[28] on NUS dataset RMSE RMSERel SAM Nguyen et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] 8.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Error evaluation on the reflectance using the same procedure as in [28] on NUS dataset RMSE RMSERel SAM Nguyen et al.", "startOffset": 67, "endOffset": 71}, {"referenceID": 27, "context": "[28] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The NUS dataset [28] contains the spectral irradiance and spectral illumination (400-700 nm with step of 10 nm) for 66 outdoor and indoor scenes, captured under several different illuminations.", "startOffset": 16, "endOffset": 20}, {"referenceID": 43, "context": "The CAVE dataset [44] is a popular hyper-spectral dataset.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "the number provided by [3], see Table 1.", "startOffset": 23, "endOffset": 26}, {"referenceID": 30, "context": "We tested our method also on data captured from Hyperion satellite [31] a sensor on board the satellite EO-1.", "startOffset": 67, "endOffset": 71}, {"referenceID": 41, "context": "Indeed it is known that deep neural networks achieve state-of-the-art results in image denoising [42].", "startOffset": 97, "endOffset": 101}, {"referenceID": 4, "context": "We also check our reconstruction on satellite data, by performing hyperspectral unmixing [5] a process that separates material information (also called endmembers) and their location in the image (also called abundances).", "startOffset": 89, "endOffset": 92}, {"referenceID": 26, "context": "We take an of the shelf endmember extraction algorithm (VCA, [27]) to identify dominant spectral signatures in the images.", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "Then, we perform a Fully Constrained Least Squares (FCLS) adjustment to extract the abundance maps, according to the Linear Mixing Model (LMM) [19].", "startOffset": 143, "endOffset": 147}], "year": 2017, "abstractText": "We describe a novel method for blind, single-image spectral super-resolution. While conventional superresolution aims to increase the spatial resolution of an input image, our goal is to spectrally enhance the input, i.e., generate an image with the same spatial resolution, but a greatly increased number of narrow (hyper-spectral) wavelength bands. Just like the spatial statistics of natural images has rich structure, which one can exploit as prior to predict high-frequency content from a low resolution image, the same is also true in the spectral domain: the materials and lighting conditions of the observed world induce structure in the spectrum of wavelengths observed at a given pixel. Surprisingly, very little work exists that attempts to use this diagnosis and achieve blind spectral super-resolution from single images. We start from the conjecture that, just like in the spatial domain, we can learn the statistics of natural image spectra, and with its help generate finely resolved hyper-spectral images from RGB input. Technically, we follow the current best practice and implement a convolutional neural network (CNN), which is trained to carry out the end-to-end mapping from an entire RGB image to the corresponding hyperspectral image of equal size. We demonstrate spectral super-resolution both for conventional RGB images and for multi-spectral satellite data, outperforming the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}