{"id": "1703.09938", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Grouped Convolutional Neural Networks for Multivariate Time Series", "abstract": "robust multivariate time series data is important for many applications such as automated control, fault collection and anomaly identification. one of the key challenges is can construct latent features automatically from dynamically changing model input. examining visual recognition tasks, convolutional behavior networks ( cnns ) have been successful to learn generalized binary extractors with numerical parameters over the spatial domain. however, when high - dimensional multivariate moment series include given, designing truly appropriate cnn brain structure becomes challenging because fewer kernels may need initially be extended through the full dimension of the input volume. to address this issue, we present linear structure learning algorithms for deep cnn models. our algorithms exploit the covariance structure over multiple time courses to partition input volume surrounding groups. the first algorithm learns the entire cnn structures explicitly by clustering individual input sequences. the second algorithm learns the group cnn structures implicitly from the error backpropagation. in experiments with two real - world partners, we demonstrate that our group cnns outperform existing cnn based regression methods.", "histories": [["v1", "Wed, 29 Mar 2017 09:05:40 GMT  (8196kb,D)", "https://arxiv.org/abs/1703.09938v1", null], ["v2", "Fri, 31 Mar 2017 05:18:33 GMT  (8196kb,D)", "http://arxiv.org/abs/1703.09938v2", null], ["v3", "Tue, 4 Apr 2017 06:05:50 GMT  (8196kb,D)", "http://arxiv.org/abs/1703.09938v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["subin yi", "janghoon ju", "man-ki yoon", "jaesik choi"], "accepted": false, "id": "1703.09938"}, "pdf": {"name": "1703.09938.pdf", "metadata": {"source": "CRF", "title": "Grouped Convolutional Neural Networks for Multivariate Time Series", "authors": ["Subin Yi", "Janghoon Ju", "Man-Ki Yoon", "Jaesik Choi"], "emails": ["<jaesik@unist.ac.kr>."], "sections": [{"heading": "1. Introduction", "text": "Advances in computing technology has made many complicated systems such as automobile, avionics, and industrial control systems more sophisticated and sensitive. Analyzing multiple variables that compose such systems accurately is therefore becoming more important for many applications such as automated control, fault diagnosis, and anomaly detection.\nIn complex systems, one of the key requirements is to maintain integrity of the sensor data so that it can be moni-\n1Ulsan National Institute of Science and Technology, Ulsan, 44919, Republic of Korea 2University of Illinois at UrbanaChampaign, Urbana, IL 61801. Correspondence to: Jaesik Choi <jaesik@unist.ac.kr>.\ntored and analyzed in a trusted manner. Previously, sensor integrity has been analyzed by feedback controls (Mo & Sinopoli, 2015; Pajic et al.) and nonparametric Bayesian methods (Krause et al., 2008). However, regression models based on control theory and nonparametric Bayesian are highly sensitive to the model parameters. Thus, finding the best model parameter for the regression models is challenging with high-dimensional multivariate sequences.\nArtificial neural network models also have been used to handle multivariate time series data. Autoencoders (Bourlard & Kamp, 1988; Zemel, 1994) train model parameters in an unsupervised manner by specifying the same input and output values. Recurrent neural networks (RNN) (Rumelhart et al.) and long-short term memory (LSTM) (Hochreiter & Schmidhuber, 1997) represent changes of time series data by learning recurrent transition function between time steps. Unfortunately, existing neural network models for time series data assume fully connected networks among time series under the Markov assumption. Thus, such models are often not precise enough to address high-dimensional multivariate regression problems.\nTo address this issue, we present two structure learning algorithms for deep convolutional neural networks (CNNs). Both of our algorithms partition input volume into groups by exploiting the covariance structure for multiple time series so that the input CNN kernels process only one of the grouped time series. Due to this partitioning of the input time series, we can avoid the CNN kernels being extended through the full dimension. In this reason, we denote the CNN models as Group CNN (G-CNN) which can exploit latent features from multiple time-series more efficiently by utilizing structural covariance of the input variables.\nThe first structure learning algorithm learns the CNN structure explicitly by clustering input sequences with spectral clustering (Luxburg, 2007). The second algorithm learns the CNN structures implicitly with the error backpropagation which will be explained in Section 3.2.\nOur model design principle is to reduce model parameters by sharing parameters when necessary. In multivariate time series regression tasks, our hypotheses on the parameter sharing scheme (or parameter tying) are as follow: (1) convolutions on a group of correlated signals are more robust to signal noises; and (2) convolutions operators on groups\nar X\niv :1\n70 3.\n09 93\n8v 3\n[ cs\n.L G\n] 4\nA pr\n2 01\n7\nof signals are more feasible to learn when a large number of time series is given. In experiments, we show that GCNN make the better predictive performance on challenging regression tasks compared to the existing CNN based regression models."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. Convolutional Neural Network", "text": "A convolutional neural network (CNN) is a multi-layer artificial neural network that has been successful recognizing visual patterns. The most common architecture of the CNNs is a stack of three types of multiple layers: convolutional layer, sub-sampling layer, and fully-connected layer. Conventionally, A CNN consists of alternate layers of convolutional layers and sub-sampling layers on the bottom and several fully-connected layers following them.\nFirst, an unit of a convolutional layer receives inputs from a set of neighboring nodes of the previous layer similarly with animal visual cortex cell. The local weights of convolutional layers are shared with the nodes in the same layer. Such local computations in the layer reduce the memory burden and improve the classification performance.\nNone-linear down-sapling layer, which is the second type of CNN layers, is another important characteristic of CNNs. The idea of local sub-sampling is that once a feature has been detected, its location itself is not as important as its relative location with other features. By reducing the dimensionality, it reduces the local sensitivity of the network and computational complexity (LeCun & Bengio, 1995; LeCun et al., 1998).\nThe last type of the layers is the fully-connected layer. It computes a full matrix calculation with all activations and nodes same as regular neural networks. After convolutional layers and sub-sampling layers extract features, fully-connected layers implements reasoning and gives the actual output. Then the model is trained in the way minimizing the error between the actual output of the model and the target output values by backpropagation method.\nCNN has been very effective for solving many computer\nvision problems such as classification (LeCun et al., 1998; Krizhevsky et al., 2012), object detection and semantic segmentation (Ren et al., 2015; Long et al., 2015). It has been also applied to other problems such as natural language processing (Kalchbrenner et al., 2014; Collobert & Weston, 2008). Recently, variants of CNN are applied to analyzing various kinds of time-series such as sensor values and EEG (electroencephalogram) signals (Yang et al., 2015; Ordez & Roggen, 2016)."}, {"heading": "2.2. Recurrent Convolutional Neural Network (RCNN)", "text": "Recurrent Convolutional Neural Network (RCNN) is a type of CNN with its convolutional layers being replaced with recurrent convolutional layers. It improves the expressive power of the convolutional layer by exploiting multiple convolutional layers that share the parameters. RCNN has been applied to not only the image processing problem (Liang & Hu, 2015; Pinheiro & Collobert, 2014) but also other tasks that require temporal analysis (Lai et al., 2015). RCNN can effectively extract invariant features in the temporal domain regarding the time-series data as a 2- dimensional data with one of the dimensions is one. with one of the dimensions is 1, RCNN can effectively extract invariant features in the temporal domain."}, {"heading": "2.2.1. RECURRENT CONVOLUTIONAL LAYER", "text": "Recurrent Convolutional Layer (RCL), which is the most representative building block of an RCNN, is the composition of l intermediate convolutional layers that shares the same parameters. The first convolutional layer of an RCL carries the convolution on the input x, resulting in the output \u03c3(W \u2217x) whereW is the convolutional filter, * is a convolution operator, and \u03c3(\u00b7) is an activation function. Then the next convolutional layer recursively processes the summation of the original input and the output of the previous layer, x + \u03c3(W \u2217 x), as an input. After some iterations of this process, an RCL gives the result of the final intermediate convolutional layer as its output.\nDuring the error backpropagation, the parameters are updated l times. In each update, the parameters are changed to fix the error made by itself from the previous layer.\nRCL can also be regarded as a skip-layer connection (He et al., 2015; Intrator & Intrator, 2001). Skip-layer connection represents connecting layers skipping intermediate layers as in Figure 1\u2019s RCL. The main motivation is that the deeper networks show a better performance in many cases but they are also harder to train in actual applications due to vanishing gradients and degradation problem (Bengio et al., 1994; Glorot & Bengio, 2010).\n(He et al., 2015) designed such layers with skip-layer connection, named as residual learning. The idea was that if\none can hypothesizes that multiple nonlinear layers can estimate an underlying mapping H(x), it is equivalent to estimating an residual function F (x) := H(x) \u2212 x. If the residual F (x) is approximately a zero mapping, H(x) is an optimal identity mapping."}, {"heading": "2.3. Spectral Clustering", "text": "The goal of clustering data points x1, ...,xN is to partition the data points into some groups such that the points in the same group are similar and points in different groups are dissimilar in a certain similarity measure sij between xi and xj . Spectral clustering is the clustering method that solves this problem from the graph-cut point of view.\nFrom the graph-cut point of view, data points are represented as a similarity graph G = (V,E). Let G be a weighted undirected graph with the vertex set V = {v1, ..., vN} where each vertex vi represents a data point xi and the weighted adjacency matrix W = {wij |i, j = 1, ..., N} where wij represents the similarity sij . Let the degree of a vertex vi \u2208 V be di = \u2211n j=1 wij and define a degree matrix D as the diagonal matrix with the degrees d1, ..., dN on the diagonal.\nThen, clustering can be reformulated to find a partition of the graph such that the edges between different groups have very low weights and the edges within a group have high weights.\nOne of the most intuitive way to solve this problem is to solve the min-cut problem (Shi & Malik, 2000). Min-cut problem is to choose a partition A1, ..., AK for a given number K that minimizes the equation (2.3.1) given as:\ncut(A1, ..., AK) := 1\n2 K\u2211 i=1 link(Ai, A\u0304i) (2.3.1)\ns.t. link(A,B) := \u2211\ni\u2208A,j\u2208B wij for disjoint A,B \u2282 A.\n(2.3.2)\nHere, 1/2 is introduce for normalizing as otherwise each edge will be counted twice. The algorithm of (Shi & Malik, 2000) explicitly requests the sets be large enough where the size of a subset A \u2282 V is measured by:\nvol(A) := \u2211 i\u2208A di (2.3.3)\nThen, find the following normalized cut, Ncut:\nNcut(A1, ..., AK) := 1\n2 K\u2211 i=1 link(Ai, A\u0304i) vol(AI)\n(2.3.4)\nThe denominator of the Ncut tries to balance the size of the clusters and the numerator finds the minimum cut of the\ngiven graph. Then to find the partition A1, ..., AK is same as to solve the following optimization problem:\nmin H\u2208RNxK\ntrace(HTLH) (2.3.5)\ns.t. Hij :=  1\u221a vol(Vj) , vi \u2208 Vj\n0, otherwise (2.3.6)\nL := D \u2212W (2.3.7)\nAs hTi Lhi=cut(Ai, A\u0304i)/vol(Ai), H TH=I , and hTi Dhi=1 where the indicator vector hj is the j-th column of the matrix H.\nUnfortunately, introducing the additional term to the mincut problem has proven to make the problem NP-hard (Wagner & Wagner, 1993) so (Luxburg, 2007; Bach & Jordan, 2004) solves the relaxed problem, which gives the solution H that consists of the eigenvectors corresponding to the K smallest eigenvalues of the matrix Lrw := D\u22121L or the K smallest generalized eigenvectors of Lu = \u03bbDu.\nGiven an NxN similarity matrix, the spectral clustering algorithm runs eigenvalue decomposition (EVD) on the graph Laplacian matrix and the eigenvectors corresponding to the K smallest eigenvalues are clustered by a clustering algorithm representing the graph vertices. The K eigenvectors are also the eigenvectors of the similarity matrix whereas corresponding K largest eigenvalues, which can be considered as an encoding of the graph similarity matrix."}, {"heading": "3. Grouped Time Series", "text": "In this section, we present two algorithms build group CNN structure. The first method builds the group structure explicitly from Spectral clustering. The second method build the group structure through the error backpropagation."}, {"heading": "3.1. Learning the Structure by Spectral Clustering", "text": "Our group CNN structure receives both the input variables X = [x1, ...,xN ] and their cluster information C = [c1, ..., cN ] where ci represents the membership of the variable xi. Unlike usual convolutional layers, the grouped\nconvolutional layers divide the input volume based on the cluster membership ci and performs the convolution operations over the input variables that belong to the same cluster as described in the Figure 2. Formally, the k-th group of the layer H is defined as:\nHk = \u03c3 (\u2211\ni\nW k \u00b7 xi + bk )\n(3.1.1)\nwhere (\u00b7) is the convolution operation, i \u2208 {j|j \u2208 {1, ..., N}, cj = k}, W k is the weight matrix, and bk is the bias vector of the k-th group.\nAs in the CNN models, the input variablesX are processed throughout multiple grouped convolutional layers and subsampling layers, flattened into one-dimensional layer followed by fully-connected layers, and produces the output y = {y1, ..., yP } (Figure 3).\nGiven the target output t = {t1, ..., tP }, we can also train this model using gradient descent solving the optimization problem:\nmin \u03b8 p\u2211 i=1 (yi \u2212 ti)2 (3.1.2)\nwith respect to the trainable parameter \u03b8 = {W, b}. The error is backpropagated to each group separately, training the CNN structure explicitly.\nThis model requires significantly less number of parameters compared to the vanilla CNN model. For example, to process 100 input variables producing 100 output channel, existing CNN model needs 100 kernels of size (width, height, 100). However, if the input variables consist of 5 clusters, each with 20 variables, it requires 5x20 kernels of size (width, height, 20), which is 5 times less than the vanilla model. It could make the CNN model more compact by eliminating redundant parameters."}, {"heading": "3.2. Neural Networks with Clustering Coefficient", "text": "Assuming that the input time series are correlated with each other, we group those variables explicitly to make use of such correlations as CNN utilizes local connectivity of an image. It can be considered to find the local connectivity and correlations within channels of CNN.\nGiven an input data X which consists of N variables where each variables are D dimensional real valued vectors, i.e. X = [x1, ...,xN ], we wish to group these variables into K clusters introducing a matrix U = [ui,j ; i \u2208 {1, ..., N}, j \u2208 {1, ...,K}] where ui,j \u2208 [0, 1], \u2211K j ui,j = 1 whose element ui,j is the clustering coefficient which represents the portion of the j-th cluster takes for the variable xi as in multinomial distribution. In this paper, we use boldface letters to represent D-dimensional real valued column vectors.Then, hj , the node that represents the j-th cluster is defined as :\nhj = \u03c3( N\u2211 i ui,jx T i \u00b7W1i,j + b1j ) (3.2.1)\nwhere W1i,j is the i-th row\u2019s j-th column of the weight matrix W 1 of size NxK, b1j is the bias, and \u03c3(\u00b7) is the activation function. By multiplying ui,j , variables can proportionally participate to each cluster.\nSuppose that an example of two layered neural network is given as shown in the Figure 4. The output node y is also defined as :\ny = \u03c3( K\u2211 j hTj \u00b7W2j,1 + b21). (3.2.2)\nGiven the true target value t, this network can be trained by gradient descent method solving the below optimization problem:\nmin W,U,B Err, (3.2.3)\nErr := 1\n2 (y \u2212 t)2 (3.2.4)\nAssuming linear activation function, gradient of the Err with respect to ui,j is :\n\u2202Err \u2202ui,j = \u2202Err \u2202y \u2202y \u2202ui,j\n= \u2202Err\n\u2202y\n\u2202y\n\u2202hj \u2202hj \u2202ui,j\n= \u2202Err \u2202y ( \u2202hTj \u2202ui,j + K\u2211 j\u2032 I{j\u2032 6= j} \u2202hTj\u2032 \u2202ui,j\u2032 \u2202ui,j\u2032 \u2202ui,j ) \u2202hj \u2202ui,j = (y \u2212 t)(KxTi \u00b7W1i,j \u2212 K\u2211 j xTi \u00b7W1i,j)(xTi \u00b7W1i,j)\n(3.2.5)\nwhere j\u2032 is the cluster out of the j-th cluster and I is an indicator function. Intuitively, the parameter update ui,j includes (K times of loss from the j-th cluster - loss from all clusters ; KxTi \u00b7W1i,j \u2212 \u2211K j x T i \u00b7W1i,j), ui,j value that gives smaller loss increase finding the optimal values while minimizing the error by the gradient descent method.\nTo implement the clustering coefficient to out model, we added a new layer which works as the Figure 5 on the bottom of the model, before the layer 1 of the Figure 3\n(b). This layer receives N input variables and computes channel-wise convolution throughout the variables using the same weight and bias in the group (group parameter sharing). This channel-wise convolution is repeated for K groups with different parameters for each groups. Therefore, the i-th channel of the k-th group is defined as:\nhKi = \u03c3(ui,kW k \u00b7 xi + bk) (3.2.6)\nThen the output is processed by the same process with the model from the Section 3.1 with explicit clustering."}, {"heading": "4. Related Work", "text": "Recently, deep learning methods are making good results in solving a variety of problems such as visual pattern recognition, signal processing, and others. Consequently there has been researches for applying those methods to analyzing complex multivariate systems.\nNeural networks that are composed of fully-connected layers only are not appropriate for handling sequential data since they need to process the whole sequence of input. More specifically, such networks are too inefficient in terms of both memory usage and learning efficiency.\nOne of the popular choices for processing time-series is a recurrent neural network (RNN). An RNN (Williams & Zipser, 1989; Funahashi & Nakamura, 1993) processes sequential data with recurrent connections to represent transition models over time. so that it can store temporal information within the network. RNN models have been successfully used for processing sequential data (Mozer, 1993; Pascanu et al., 2013; Koutnik et al., 2014). (Coulibaly & Baldwin, 2005) used dynamic RNN to forecast nonstationary hydrological time-series and (Malhotra et al., 2015) used stacked LSTM network as a predictor over a number of time steps and detected anomalies that has high prediction error in time series.\nConvolutional neural network (CNN) is also commonly used to analyze temporal data.(Abdel-Hamid et al., 2012) used CNN for speech recognition problem and (Zheng et al., 2014) proposed multi-channels deep convolutional neural network for multivariate time series classification.\nRecurrent Convolutional Neural Network (RCNN), which can be considered as a variant of a CNN, is recently proposed and shows state-of-the-art performance on classifying multiple time series (Pinheiro & Collobert, 2014; Liang & Hu, 2015; Lai et al., 2015). When a small number of time series is given, multiple signals can be handled individually in a straightforward manner by using polling operators or fully connected linear operators on signals. However, it is not clear how to model the covariance structure of large number of multiple sequences explicitly for deep neural network models."}, {"heading": "5. Experimental Results", "text": "In experiments, we compare the regression performance of several CNN-based models on two real-world highdimensional multivariate datasets, groundwater level data and drone flight data. Groundwater data and drone data respectively have 88 and 148 variables."}, {"heading": "5.1. Settings", "text": "To evaluate the regression performance, we picked one of the variables, say xp, from the dataset randomly and constructed the variable\u2019s values as a target at time t, y = xp(t), by seeing its correlated variables\u2019 values from time t\u2212T to twithout including variable, i.e. X = \u222ai,i6=p[xi(t\u2212 T ), ..., xi(t)].\nWe trained our models with 90% of the whole dataset and tested on the other 10%. Then, the regression performance were compared with other regression models: linear regression, ridge regression, CNN and RCNN. The regression performance was measured on the scale of the standardized root mean square error (SRMSE), which is defined as the equation 5.1.1 when t\u0304 is the mean value of the target vector t.\nSRMSE =\n\u221a \u03a3Nt=1(tt \u2212 yt)2/N\nSE (5.1.1)\nSE =\n\u221a 1\nN \u2211 i (ti \u2212 t\u0304)2 (5.1.2)"}, {"heading": "5.2. Datasets", "text": ""}, {"heading": "5.2.1. GROUNDWATER DATA", "text": "We used daily collected groundwater data provided by the United States Geological Survey (USGS)1. Dataset is composed of various parameters from the US territories and we used depth to water level from the regions other than Hawaii and Alaska. Regions where the data was collected over 28 years (1987-2015) were selected and those that have unrecorded periods longer than two months were excluded. Empty records shorter than two months were filled by interpolation. Final dataset contains records from 88 sites of 10,228 days."}, {"heading": "5.2.2. DRONE DATA", "text": "We used a quadcopteras our experimental platform to collect flight sensor data. Quadcopters are aerodynamically unstable and their actuators, i.e., the motors, must be controlled directly by an on-board computer for stable flight. We used the Pixhawk2 as the autopilot hardware for our quadcopter. It has on-board sensors such as inertial mea-\n1https://waterdata.usgs.gov 2https://pixhawk.org/\nsurement unit (IMU), compass, and barometer. We run the open-source PX4 autopilot software suite3 on the ARM Cortex M4F processor on the Pixhawk. It combines sensor data and flight commands to compute correct outputs to the motors, which then controls the vehicle\u2019s orientation and position.\nWe collected flight data of the quadcopter using PX4\u2019s autopilot logging facility. Each flight data is composed of time-stamped sensor and actuator measurements, flight set points (attitude, position), and other auxiliary information (radio input, battery status, etc.). We collected data sets by flying the quadcopter in an autonomous mode, in which it flies along a pre-defined path. We obtained three sets of logs by varying the path as shown in Figure 6. In total, we used 148 sensors of 12,654 time points excluding those that do not show any change during the flight and have missing values."}, {"heading": "5.3. Results", "text": "We built group CNN and group RCNN using both spectral clustering method (explicit) and the clustering coefficient method (coeff), and compared the performance with corresponding vanilla CNNs and vanilla RCNNs. The deep CNN model architecture are shown in Table 1. All the learning parameters such as the learning rate and weight\n3http://px4.io/\ninitialization parameters are matched. Every models were trained for 200 epochs and the best results were chosen.\nExperiment results are shown in the following tables, Table 2 and Table 3. In general, our group CNN models outperform in the groundwater dataset. RCNN with clustering coefficient model performs best with 0.754 SRMSE compared to 0.985 of vanilla RCNN model. Our group CNN models also tend to perform better than the vanilla CNN models in the drone flight dataset. RCNN with spectral\nclustering model performs best with 0.438 SRMSE compared to 0.464 SRMSE of vanilla CNN model. The values predicted by our models are shown in Figure 7."}, {"heading": "6. Conclusion", "text": "In this paper, we presented two structure learning algorithms for deep CNN models. Our algorithms exploited\nthe covariance structure over multiple time series to partition input volume into groups. The first algorithm learned the group CNN structures explicitly by clustering individual input sequences. The second algorithm learned the group CNN structures implicitly from the error backpropagation. In the experiments with two real-world datasets, we demonstrate that our group CNN models outperformed the existing CNN based regression methods."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Bo Liu at the Intelligent Robotics Laboratory, University of Illinois, for helping with collecting drone sensor data and the anonymous reviewers for their helpful and constructive comments. This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (Ministry of Science, ICT & Future Planning, MSIP) (No. 2014M2A8A2074096), and the POSCO grant (No. 2016X043)."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "H. Jiang", "G. Penn"], "venue": "In Proceedings of IEEE international conference on Acoustics, speech and signal processing,", "citeRegEx": "Abdel.Hamid et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abdel.Hamid et al\\.", "year": 2012}, {"title": "Learning spectral clustering", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "In Proceedings of Advances in neural information processing systems,", "citeRegEx": "Bach and Jordan,? \\Q2004\\E", "shortCiteRegEx": "Bach and Jordan", "year": 2004}, {"title": "Learning longterm dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "In Proceedings of IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Biological cybernetics,", "citeRegEx": "Bourlard and Kamp,? \\Q1988\\E", "shortCiteRegEx": "Bourlard and Kamp", "year": 1988}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceeding of the International Conference on Machine learning,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Nonstationary hydrological time series forecasting using nonlinear dynamic methods", "author": ["P. Coulibaly", "C.K. Baldwin"], "venue": "Hydrology, 307:164\u2013174,", "citeRegEx": "Coulibaly and Baldwin,? \\Q2005\\E", "shortCiteRegEx": "Coulibaly and Baldwin", "year": 2005}, {"title": "Approximation of dynamical systems by continuous time recurrent neural networks", "author": ["K.I. Funahashi", "Y. Nakamura"], "venue": "Neural networks,", "citeRegEx": "Funahashi and Nakamura,? \\Q1993\\E", "shortCiteRegEx": "Funahashi and Nakamura", "year": 1993}, {"title": "Understanding the difficulty of training deep feedforward neural networks. Aistats", "author": ["X. Glorot", "Y. Bengio"], "venue": null, "citeRegEx": "Glorot and Bengio,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Technical report,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Interpreting neural-network results: a simulation study", "author": ["O. Intrator", "N. Intrator"], "venue": "Computational statistics & data analysis,", "citeRegEx": "Intrator and Intrator,? \\Q2001\\E", "shortCiteRegEx": "Intrator and Intrator", "year": 2001}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "Machine Learning Research,", "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["S. Lai", "L. Xu", "K. Liu", "J. Zhao"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Lai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Convolutional networks for images, speech, and time-series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "LeCun and Bengio,? \\Q1995\\E", "shortCiteRegEx": "LeCun and Bengio", "year": 1995}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["M. Liang", "X. Hu"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Liang and Hu,? \\Q2015\\E", "shortCiteRegEx": "Liang and Hu", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "A tutorial on spectral clustering", "author": ["U.V. Luxburg"], "venue": "Statistics and computing,", "citeRegEx": "Luxburg,? \\Q2007\\E", "shortCiteRegEx": "Luxburg", "year": 2007}, {"title": "Long short term memory networks for anomaly detection in time series", "author": ["P. Malhotra", "L. Vig", "G. Shroff", "P. Agarwal"], "venue": "In Proceedings of European Symposium on Artificial Neural Networks, pp", "citeRegEx": "Malhotra et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malhotra et al\\.", "year": 2015}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Cirean", "J. Schmidhuber"], "venue": "In Proceedings of International Conference on Artificial Neural Networks,", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Secure estimation in the presence of integrity attacks", "author": ["Mo", "Yilin", "Sinopoli", "Bruno"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Mo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mo et al\\.", "year": 2015}, {"title": "Induction of multiscale temporal structure", "author": ["M.C. Mozer"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Mozer,? \\Q1993\\E", "shortCiteRegEx": "Mozer", "year": 1993}, {"title": "Deep convolutional and lstm recurrent neural networks for multimodal wearable activity", "author": ["F.J. Ordez", "D. Roggen"], "venue": "recognition. Sensors,", "citeRegEx": "Ordez and Roggen,? \\Q2016\\E", "shortCiteRegEx": "Ordez and Roggen", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "In Proceedings of the 30 th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Recurrent convolutional neural networks for scene labeling", "author": ["P. Pinheiro", "R. Collobert"], "venue": "In Proceedings of the 31 st International Conference on Machine Learning,", "citeRegEx": "Pinheiro and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Pinheiro and Collobert", "year": 2014}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "In Proceedings of Advances in neural information processing systems,", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "In Proceedings of IEEE Transactions on pattern analysis and machine intelligence,", "citeRegEx": "Shi and Malik,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik", "year": 2000}, {"title": "Between min cut and graph bisection", "author": ["D. Wagner", "F. Wagner"], "venue": null, "citeRegEx": "Wagner and Wagner,? \\Q1993\\E", "shortCiteRegEx": "Wagner and Wagner", "year": 1993}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["R.J. Williams", "D. Zipser"], "venue": null, "citeRegEx": "Williams and Zipser,? \\Q1989\\E", "shortCiteRegEx": "Williams and Zipser", "year": 1989}, {"title": "Deep convolutional neural networks on multichannel time series for human activity recognition", "author": ["J.B. Yang", "M.N. Nguyen", "P.P. San", "X.L. Li", "Krishnaswamy", "Sh"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Autoencoders, minimum description length and helmholtz free energy", "author": ["R.S. Zemel"], "venue": "Proceedings of the Neural Information Processing Systems Conference,", "citeRegEx": "Zemel,? \\Q1994\\E", "shortCiteRegEx": "Zemel", "year": 1994}], "referenceMentions": [{"referenceID": 11, "context": ") and nonparametric Bayesian methods (Krause et al., 2008).", "startOffset": 37, "endOffset": 58}, {"referenceID": 31, "context": "Autoencoders (Bourlard & Kamp, 1988; Zemel, 1994) train model parameters in an unsupervised manner by specifying the same input and output values.", "startOffset": 13, "endOffset": 49}, {"referenceID": 18, "context": "The first structure learning algorithm learns the CNN structure explicitly by clustering input sequences with spectral clustering (Luxburg, 2007).", "startOffset": 130, "endOffset": 145}, {"referenceID": 15, "context": "By reducing the dimensionality, it reduces the local sensitivity of the network and computational complexity (LeCun & Bengio, 1995; LeCun et al., 1998).", "startOffset": 109, "endOffset": 151}, {"referenceID": 15, "context": "CNN has been very effective for solving many computer vision problems such as classification (LeCun et al., 1998; Krizhevsky et al., 2012), object detection and semantic segmentation (Ren et al.", "startOffset": 93, "endOffset": 138}, {"referenceID": 12, "context": "CNN has been very effective for solving many computer vision problems such as classification (LeCun et al., 1998; Krizhevsky et al., 2012), object detection and semantic segmentation (Ren et al.", "startOffset": 93, "endOffset": 138}, {"referenceID": 26, "context": ", 2012), object detection and semantic segmentation (Ren et al., 2015; Long et al., 2015).", "startOffset": 52, "endOffset": 89}, {"referenceID": 17, "context": ", 2012), object detection and semantic segmentation (Ren et al., 2015; Long et al., 2015).", "startOffset": 52, "endOffset": 89}, {"referenceID": 10, "context": "It has been also applied to other problems such as natural language processing (Kalchbrenner et al., 2014; Collobert & Weston, 2008).", "startOffset": 79, "endOffset": 132}, {"referenceID": 30, "context": "Recently, variants of CNN are applied to analyzing various kinds of time-series such as sensor values and EEG (electroencephalogram) signals (Yang et al., 2015; Ordez & Roggen, 2016).", "startOffset": 141, "endOffset": 182}, {"referenceID": 13, "context": "RCNN has been applied to not only the image processing problem (Liang & Hu, 2015; Pinheiro & Collobert, 2014) but also other tasks that require temporal analysis (Lai et al., 2015).", "startOffset": 162, "endOffset": 180}, {"referenceID": 8, "context": "RCL can also be regarded as a skip-layer connection (He et al., 2015; Intrator & Intrator, 2001).", "startOffset": 52, "endOffset": 96}, {"referenceID": 2, "context": "The main motivation is that the deeper networks show a better performance in many cases but they are also harder to train in actual applications due to vanishing gradients and degradation problem (Bengio et al., 1994; Glorot & Bengio, 2010).", "startOffset": 196, "endOffset": 240}, {"referenceID": 8, "context": "(He et al., 2015) designed such layers with skip-layer connection, named as residual learning.", "startOffset": 0, "endOffset": 17}, {"referenceID": 18, "context": "Unfortunately, introducing the additional term to the mincut problem has proven to make the problem NP-hard (Wagner & Wagner, 1993) so (Luxburg, 2007; Bach & Jordan, 2004) solves the relaxed problem, which gives the solution H that consists of the eigenvectors corresponding to the K smallest eigenvalues of the matrix Lrw := D\u22121L or the K smallest generalized eigenvectors of Lu = \u03bbDu.", "startOffset": 135, "endOffset": 171}, {"referenceID": 22, "context": "RNN models have been successfully used for processing sequential data (Mozer, 1993; Pascanu et al., 2013; Koutnik et al., 2014).", "startOffset": 70, "endOffset": 127}, {"referenceID": 24, "context": "RNN models have been successfully used for processing sequential data (Mozer, 1993; Pascanu et al., 2013; Koutnik et al., 2014).", "startOffset": 70, "endOffset": 127}, {"referenceID": 19, "context": "(Coulibaly & Baldwin, 2005) used dynamic RNN to forecast nonstationary hydrological time-series and (Malhotra et al., 2015) used stacked LSTM network as a predictor over a number of time steps and detected anomalies that has high prediction error in time series.", "startOffset": 100, "endOffset": 123}, {"referenceID": 0, "context": "(Abdel-Hamid et al., 2012) used CNN for speech recognition problem and (Zheng et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 13, "context": "Recurrent Convolutional Neural Network (RCNN), which can be considered as a variant of a CNN, is recently proposed and shows state-of-the-art performance on classifying multiple time series (Pinheiro & Collobert, 2014; Liang & Hu, 2015; Lai et al., 2015).", "startOffset": 190, "endOffset": 254}], "year": 2017, "abstractText": "Analyzing multivariate time series data is important for many applications such as automated control, fault diagnosis and anomaly detection. One of the key challenges is to learn latent features automatically from dynamically changing multivariate input. In visual recognition tasks, convolutional neural networks (CNNs) have been successful to learn generalized feature extractors with shared parameters over the spatial domain. However, when high-dimensional multivariate time series is given, designing an appropriate CNN model structure becomes challenging because the kernels may need to be extended through the full dimension of the input volume. To address this issue, we present two structure learning algorithms for deep CNN models. Our algorithms exploit the covariance structure over multiple time series to partition input volume into groups. The first algorithm learns the group CNN structures explicitly by clustering individual input sequences. The second algorithm learns the group CNN structures implicitly from the error backpropagation. In experiments with two realworld datasets, we demonstrate that our group CNNs outperform existing CNN based regression methods.", "creator": "LaTeX with hyperref package"}}}