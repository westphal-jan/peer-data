{"id": "1704.00783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Brief Notes on Hard Takeoff, Value Alignment, and Coherent Extrapolated Volition", "abstract": "i make some careful observations about hard edges, value balance, and coherent extrapolated volition, concepts which have been central in analyses of superintelligent ai systems.", "histories": [["v1", "Mon, 3 Apr 2017 19:45:04 GMT  (5kb)", "http://arxiv.org/abs/1704.00783v1", "3 pages"]], "COMMENTS": "3 pages", "reviews": [], "SUBJECTS": "cs.AI cs.CY cs.LG", "authors": ["gopal p sarma"], "accepted": false, "id": "1704.00783"}, "pdf": {"name": "1704.00783.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Gopal P. Sarma"], "emails": ["gopal.sarma@emory.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n00 78\n3v 1\n[ cs\n.A I]\n3 A\npr 2\n01 7\nI make some basic observations about hard takeoff, value alignment, and coherent extrapolated\nvolition, concepts which have been central in analyses of superintelligent AI systems.\nI. On Hard Takeoff\nThe distinction between hard takeoff and soft takeoff has been used to describe different possible scenarios following the arrival of human-level artificial intelligence. The basic premise underlying these concepts is that software-based agents would have the ability to improve their own intelligence by analyzing and rewriting their source code, whereas biological organisms are significantly more restricted in their capacity for self-improvement [1, 2, 3, 4, 5].\nThere is no precise boundary between the two scenarios, but in broad strokes, a hard take refers to a transition from human level intelligence to superintelligence in a matter of minutes, hours or days. A soft takeoff refers to a scenario where this transition is much more gradual, perhaps taking many months or years. The practical importance of this qualitative distinction is that in a soft takeoff, there may opportunities for human intervention in the event that the initial AI systems have problematic design flaws.\nThe purpose of this brief note is simply to point out the following: takeoff speed refers to the rate of change of the agents\u2019 level of intelligence, and not our perceived changes in\n\u2217Email: gopal.sarma@emory.edu\nthe world around us. Because the notion of an \u201cintelligence explosion\u201d has been constructed in analogy to a physical explosion [6], it gives rise to an inaccurate mental picture in people\u2019s minds. If self-improving AI systems1 are thought to be the intellectual analogue of nuclear chain reactions, then the natural image of an intelligence explosion that this metaphor creates is a scenario in which massive, disruptive changes take place in the world that are difficult for individuals and for society to handle.\nHowever, the premise of intelligent agents with capacities in substantial excess of any human being, which are able to process the sum total of human knowledge in the form of books, video, and ongoing contemporary events implies that greater levels of intelligence in the AI systems will be accompanied by actions taken with a corresponding level of information, insight, and operational skill. Therefore, if the initial systems are designed correctly with respect to value alignment and goal structure stability, it is in fact a hard takeoff scenario which would be less disruptive\n1As this analysis neither requires nor implies that the driving force of change is a unitary agent, I have chosen to use the plural terms \u201csoftware-based agents,\u201d \u201cAI systems,\" or \u201csuperintelligent AI systems.\u201d It may very well be a collection of agents / systems possessing powerful AI capabilities in aggregate.\nthan a soft takeoff, not the other way around.\nI reiterate this claim for emphasis: The takeoff speed of an intelligence explosion refers to the rate of change of intelligence in the AI systems, and not our perceived changes in the world around us. Therefore, under the assumption of correctly design systems, a hard takeoff is preferable to a soft takeoff because the resultant changes that take place in the world will be executed with greater precision, thoughtfulness, and insight.\nII. On Value Alignment and Coherent Extrapolated Volition\nThe preceding argument relied on a key assumption, namely that the AI systems capable of self-improvement were designed correctly with respect to value alignment and goal structure stability. Value alignment refers to the construction of systems that take actions consistent with human values. Russell states 3 design principles which encapsulate the notion of value alignment [7]:\n1. The machine\u2019s purpose must be to maximize the realization of human values. In\nparticular, it has no purpose of its own and no innate desire to protect itself.\n2. The machine must be initially uncertain about what those human values are. The\nmachine may learn more about human values as it goes along, but it may never achieve complete certainty.\n3. The machine must be able to learn about\nhuman values by observing the choices that we humans make.\nA related notion is Yudkowsky\u2019s \u201ccoherent extrapolated volition\u201d [8, 9]. The basic premise of this proposal is that sophisticated AI systems will be capable of extrapolating and resolving disagreements between the value systems of individuals and groups, ultimately arriving at a goal structure that represents the collective desires of humanity. This process of iterated reflection is analogous to Rawls\u2019\n\u201creflective equilibrium\u201d [10].\nLike the metaphor of hard takeoff, the notions of value alignment and coherent extrapolated volition can also give rise to an inaccurate mental picture, namely, that the aligned goal structure would either require or result in all humans arriving at complete agreement on all issues. However, with adequate resources, it may very well be that value-aligned AI systems shape a world in which groups of individuals co-exist who disagree about object-level issues. Certainly we can point to many examples in contemporary human society where individuals maintain divergent preferences without conflict.\nThe purpose of this brief note is simply to point out the following: Implicit in any practical analysis of value alignment are the physical resources available to the AI systems. In particular, the construction of a human compatible goal structure does not mean that all human disagreements have been resolved. Rather, it means that a mutually satisfactory set of outcomes has been achieved, subject to resource constraints.\nIt may be impossible to arrive at a consensus goal structure without adequate resources. As a trivial example, if we have two individuals each of whom desires at least one apple, there is no disagreement if we have two apples. On the other hand, if there is only one apple, conflict may very well be inevitable in the absence of other factors with which to resolve the imbalance between specific individual desires and total available resources. In the context of superintelligent AI systems capable of exerting substantial influence on the world and shaping society on a global scale, adequate analysis of value alignment requires an understanding of the sum total of physical resources that the AI systems have at their disposal.\nAcknowledgements\nI would like to thank Eric Drexler and Anders Sandberg for valuable discussions and feed-\nback on the manuscript.\nReferences\n[1] N. Bostrom, Superintelligence: Paths, Dangers, Strategies. OUP Oxford, 2014.\n[2] I. J. Good, \u201cSpeculations Concerning the\nFirst Ultraintelligent Machine,\u201d Advances In Computers, vol. 6, no. 99, pp. 31\u201383, 1965.\n[3] D. Chalmers, \u201cThe Singularity: A Philo-\nsophical Analysis,\u201d Journal of Consciousness Studies, vol. 17, no. 9-10, pp. 7\u201365, 2010.\n[4] M. Shanahan, The Technological Singular-\nity. MIT Press, 2015.\n[5] C. Shulman and A. Sandberg, \u201cImplications of a software-limited singularity,\u201d in\nProceedings of the European Conference of Computing and Philosophy, 2010.\n[6] E. Yudkowsky, \u201cArtificial intelligence as a positive and negative factor in\nglobal risks,\u201d in Global Catastrophic Risks (N. Bostrom and M. Cirkovic, eds.), ch. 15, pp. 308\u2013345, Oxford: Oxford University Press, 2011.\n[7] S. Russell, \u201cShould We Fear Supersmart Robots?,\u201d Scientific American, vol. 314,\nno. 6, pp. 58\u201359, 2016.\n[8] E. Yudkowsky, \u201cCoherent Extrapolated\nVolition,\u201d Singularity Institute for Artificial Intelligence, 2004.\n[9] N. Tarleton, \u201cCoherent extrapolated voli-\ntion: A meta-level approach to machine ethics,\u201d Machine Intelligence Research Institute, 2010.\n[10] J. Rawls, A Theory of Justice. Belknapp,\n1971."}], "references": [{"title": "Superintelligence: Paths, Dangers, Strategies", "author": ["N. Bostrom"], "venue": "OUP Oxford,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Speculations Concerning the First Ultraintelligent Machine", "author": ["I.J. Good"], "venue": "Advances In Computers, vol. 6, no. 99, pp. 31\u201383, 1965.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1965}, {"title": "The Singularity: A Philosophical Analysis", "author": ["D. Chalmers"], "venue": "Journal of Consciousness Studies, vol. 17, no. 9-10, pp. 7\u201365, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "The Technological Singularity", "author": ["M. Shanahan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Implications of a software-limited singularity", "author": ["C. Shulman", "A. Sandberg"], "venue": "Proceedings of the European Conference of Computing and Philosophy, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Artificial intelligence as a positive and negative factor in global risks", "author": ["E. Yudkowsky"], "venue": "Global Catastrophic Risks (N. Bostrom and M. Cirkovic, eds.), ch. 15, pp. 308\u2013345, Oxford: Oxford University Press, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Should We Fear Supersmart Robots", "author": ["S. Russell"], "venue": "Scientific American, vol. 314, no. 6, pp. 58\u201359, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Coherent Extrapolated Volition", "author": ["E. Yudkowsky"], "venue": "Singularity Institute for Artificial Intelligence, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Coherent extrapolated volition: A meta-level approach to machine ethics", "author": ["N. Tarleton"], "venue": "Machine Intelligence Research Institute, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The basic premise underlying these concepts is that software-based agents would have the ability to improve their own intelligence by analyzing and rewriting their source code, whereas biological organisms are significantly more restricted in their capacity for self-improvement [1, 2, 3, 4, 5].", "startOffset": 279, "endOffset": 294}, {"referenceID": 1, "context": "The basic premise underlying these concepts is that software-based agents would have the ability to improve their own intelligence by analyzing and rewriting their source code, whereas biological organisms are significantly more restricted in their capacity for self-improvement [1, 2, 3, 4, 5].", "startOffset": 279, "endOffset": 294}, {"referenceID": 2, "context": "The basic premise underlying these concepts is that software-based agents would have the ability to improve their own intelligence by analyzing and rewriting their source code, whereas biological organisms are significantly more restricted in their capacity for self-improvement [1, 2, 3, 4, 5].", "startOffset": 279, "endOffset": 294}, {"referenceID": 3, "context": "The basic premise underlying these concepts is that software-based agents would have the ability to improve their own intelligence by analyzing and rewriting their source code, whereas biological organisms are significantly more restricted in their capacity for self-improvement [1, 2, 3, 4, 5].", "startOffset": 279, "endOffset": 294}, {"referenceID": 4, "context": "The basic premise underlying these concepts is that software-based agents would have the ability to improve their own intelligence by analyzing and rewriting their source code, whereas biological organisms are significantly more restricted in their capacity for self-improvement [1, 2, 3, 4, 5].", "startOffset": 279, "endOffset": 294}, {"referenceID": 5, "context": "Because the notion of an \u201cintelligence explosion\u201d has been constructed in analogy to a physical explosion [6], it gives rise to an inaccurate mental picture in people\u2019s minds.", "startOffset": 106, "endOffset": 109}, {"referenceID": 6, "context": "Russell states 3 design principles which encapsulate the notion of value alignment [7]:", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "A related notion is Yudkowsky\u2019s \u201ccoherent extrapolated volition\u201d [8, 9].", "startOffset": 65, "endOffset": 71}, {"referenceID": 8, "context": "A related notion is Yudkowsky\u2019s \u201ccoherent extrapolated volition\u201d [8, 9].", "startOffset": 65, "endOffset": 71}], "year": 2017, "abstractText": "I make some basic observations about hard takeoff, value alignment, and coherent extrapolated volition, concepts which have been central in analyses of superintelligent AI systems.", "creator": "LaTeX with hyperref package"}}}