{"id": "1704.01279", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders", "abstract": "generative models in vision techniques seen excellent progress due to algorithmic improvements and the success of advanced - quality image schemes. in this discipline, we offer contributions in both these principles to enable similar progress in audio modeling. first, we establish a powerful new graph - style statistical model that conditions an autoregressive problem on temporal codes learned via the raw audio waveform. second, we introduce nsynth, a large - scale and high - quality dataset of musical notes that is an order three magnitude larger than comparable public datasets. using nsynth, we demonstrate improved detection and quantitative effects of coherent wavenet autoencoder over long well - tuned spectral autoencoder baseline. finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, repeatedly interpolating in timbre to create new types of voices that are realistic and expressive.", "histories": [["v1", "Wed, 5 Apr 2017 06:34:22 GMT  (4465kb,D)", "http://arxiv.org/abs/1704.01279v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SD", "authors": ["jesse engel", "cinjon resnick", "adam roberts", "sander dieleman", "mohammad norouzi", "douglas eck", "karen simonyan"], "accepted": true, "id": "1704.01279"}, "pdf": {"name": "1704.01279.pdf", "metadata": {"source": "META", "title": "Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders", "authors": ["Jesse Engel", "Cinjon Resnick", "Adam Roberts", "Sander Dieleman", "Douglas Eck", "Karen Simonyan", "Mohammad Norouzi"], "emails": ["<jesseengel@google.com>."], "sections": [{"heading": "1. Introduction", "text": "Audio synthesis is important for a large range of applications including text-to-speech (TTS) systems and music generation. Audio generation algorithms, know as vocoders in TTS and synthesizers in music, respond to higher-level control signals to create fine-grained audio waveforms. Synthesizers have a long history of being hand-designed instruments, accepting control signals such as \u2018pitch\u2019, \u2018velocity\u2019, and filter parameters to shape the tone, timbre, and dynamics of a sound (Pinch et al., 2009). In spite of their limitations, or\n*Equal contribution 1Google Brain, Mountain View, California, USA. Work done while Cinjon Resnick was a member of the Google Brain Residency Program 2DeepMind, London, England. Correspondence to: Jesse Engel <jesseengel@google.com>.\nperhaps because of them, synthesizers have had a profound effect on the course of music and culture in the past half century (Punk, 2014).\nIn this paper, we outline a data-driven approach to audio synthesis. Rather than specifying a specific arrangement of oscillators or an algorithm for sample playback, such as in FM Synthesis or Granular Synthesis (Chowning, 1973; Xenakis, 1971), we show that it is possible to generate new types of expressive and realistic instrument sounds with a neural network model. Further, we show that this model can learn a semantically meaningful hidden representation that can be used as a high-level control signal for manipulating tone, timbre, and dynamics during playback.\nExplicitly, our two contributions to advance the state of generative audio modeling are:\n\u2022 A WaveNet-style autoencoder that learns temporal hidden codes to effectively capture longer term structure without external conditioning.\n\u2022 NSynth: a large-scale dataset for exploring neural audio synthesis of musical notes.\nThe primary motivation for our novel autoencoder structure follows from the recent advances in autoregressive models like WaveNet (van den Oord et al., 2016a) and SampleRNN (Mehri et al., 2016). They have proven to be effective at modeling short and medium scale (\u223c500ms) signals, but rely on external conditioning for longer-term dependencies. Our autoencoder removes the need for that external conditioning. It consists of a WaveNet-like encoder that infers hidden embeddings distributed in time and a WaveNet decoder that uses those embeddings to effectively reconstruct the original audio. This structure allows the size of an embedding to scale with the size of the input and encode over much longer time scales.\nRecent breakthroughs in generative modeling of images (Kingma & Welling, 2013; Goodfellow et al., 2014; van den Oord et al., 2016b) have been predicated on\nar X\niv :1\n70 4.\n01 27\n9v 1\n[ cs\n.L G\n] 5\nA pr\n2 01\n7\nthe availability of high-quality and large-scale datasets such as MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011), CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al., 2009). While generative models are notoriously hard to evaluate (Theis et al., 2015), these datasets provide a common test bed for consistent qualitative and quantitative evaluation, such as with the use of the Inception score (Salimans et al., 2016).\nWe recognized the need for an audio dataset that was as approachable as those in the image domain. Audio signals found in the wild contain multi-scale dependencies that prove particularly difficult to model (Raffel, 2016; Bertin-Mahieux et al., 2011; King et al., 2008; Thickstun et al., 2016), leading many previous efforts at data-driven audio synthesis to focus on more constrained domains such as texture synthesis or training small parametric models (Sarroff & Casey, 2014; McDermott et al., 2009).\nInspired by the large, high-quality image datasets, NSynth is an order of magnitude larger than comparable public datasets (Humphrey, 2016). It consists of \u223c300k four-second annotated notes sampled at 16kHz from \u223c1k harmonic musical instruments.\nAfter introducing the models and describing the dataset, we evaluate the performance of the WaveNet autoencoder over a baseline convolutional autoencoder model trained on spectrograms. We examine the tasks of reconstruction and interpolation, and analyze the\nlearned space of embeddings. For qualitative evaluation, download audio files for all examples mentioned in this paper here. Despite our best efforts to convey analysis in plots, listening to the samples is essential to understanding this paper and we strongly encourage the reader to listen along as they read."}, {"heading": "2. Models", "text": ""}, {"heading": "2.1. WaveNet Autoencoder", "text": "WaveNet (van den Oord et al., 2016a) is a powerful generative approach to probabilistic modeling of raw audio. In this section we describe our novel WaveNet autoencoder structure. The primary motivation for this approach is to attain consistent long-term structure without external conditioning. A secondary motivation is to use the learned encodings for applications such as meaningful audio interpolation.\nRecalling the original WaveNet architecture described in (van den Oord et al., 2016a), at each step a stack of dilated convolutions predicts the next sample of audio from a fixed-size input of prior sample values. The joint probability of the audio x is factorized as a product of conditional probabilities:\np(x) = N\u220f i=1 p(xi|x1, ..., xN\u22121)\nUnconditional generation from this model manifests as \u201cbabbling\u201d due to the lack of longer term structure (Listen: CAUTION, VERY LOUD! (ex1, ex2, ex3, ex4)). However, (van den Oord et al., 2016a) showed in the context of speech that long-range structure can be enforced by conditioning on temporally aligned linguistic features.\nOur autoencoder removes the need for that external conditioning. It works by taking raw audio waveform as input from which the encoder produces an embedding Z = f(x). Next, we causally shift the same input and feed it into the decoder, which reproduces the input waveform. The joint probablity is now:\np(x) = N\u220f i=1 p(xi|x1, ..., xN\u22121, f(x))\nWe could parameterize Z as a latent variable p(Z|x) that we would have to marginalize over (Gulrajani et al., 2016), but in practice we have found this to be less effective. As discussed in (Chen et al., 2016), this may be due to the decoder being so powerful that it can ignore the latent variables unless they encode a much larger context that\u2019s otherwise inaccessible.\nNote that the decoder could completely ignore the deterministic encoding and degenerate to a standard unconditioned WaveNet. However, because the encoding is a strong signal for the supervised output, the model learns to utilize it.\nDuring inference, the decoder autoregressively generates a single output sample at a time conditioned on an embedding and a starting palette of zeros. The embedding can be inferred deterministically from audio or drawn from other points in the embedding space, e.g. through interpolation or analogy (White, 2016).\nFigure 1b depicts the model architecture in more detail. The temporal encoder model is a 30-layer nonlinear residual network of dilated convolutions followed by 1x1 convolutions. Each convolution has 128 channels and precedes a ReLU nonlinearity. The output feed into another 1x1 convolution before downsampling with average pooling to get the encoding Z. We call it a \u2018temporal encoding\u2019 because the result is a sequence of hidden codes with separate dimensions for time and channel. The time resolution depends on the stride of the pooling. We tune the stride, keeping total size of the embedding constant (\u223c32x compression). In the trade-off between temporal resolution and embedding expressivity, we find a sweet spot at a stride of 512 (32ms) with 16 dimensions per timestep, yielding a 125x16 embedding for each NSynth note. We\nadditionally explore models that condition on global attributes by utilizing a one-hot pitch embedding.\nThe WaveNet decoder model is similar to that presented in (van den Oord et al., 2016a). We condition it by biasing every layer with a different linear projection of the temporal embeddings. Since the decoder does not downsample anywhere in the network, we upsample the temporal encodings to the original audio rate with nearest neighbor interpolation. As in the original design, we quantize our input audio using 8-bit mu-law encoding and predict each output step with a softmax over the resulting 256 values.\nThis WaveNet autoencoder is a deep and expressive network, but has the trade-off of being limited in temporal context to the chunk-size of the training audio. While this is sufficient for consistently encoding the identity of a sound and interpolating among many sounds, achieving larger context would be better and is an area of ongoing research."}, {"heading": "2.2. Baseline: Spectral Autoencoder", "text": "As a point of comparison, we set out to create a straightforward yet strong baseline for the our neural audio synthesis experiments. Inspired by image models (Vincent et al., 2010), we explore convolutional autoencoder structures with a bottleneck that forces the model to find a compressed representation for an entire note. Figure 1a shows a block diagram of our baseline architecture. The convolutional encoder and decoder are each 10 layers deep with 2x2 strides and 4x4 kernels. Every layer is followed by a leaky-ReLU (0.1) nonlinearity and batch normalization (Ioffe & Szegedy, 2015). The number of channels grows from 128 to 1024 before a linear fully-connected layer creates a single 19841 dimensional hidden vector (Z) to match that of the WaveNet autoencoder.\nGiven the simplicity of the architecture, we examined a range of input representations. Using the raw waveform as input with a mean-squared error (MSE) cost proved difficult to train and highlighted the inadequacy of the independent Gaussian assumption. Spectral representations such as the real and imaginary components of the Fast Fourier Transform (FFT) fared better, but suffered from low perceptual quality despite achieving low MSE cost. We found that training on the log magnitude of the power spectra, peak normalized to be between 0 and 1, correlated better with perceptual distortion.\nWe also explored several representations of phase, in-\n1This size was aligned with a WaveNet autoencoder that had a pooling stride of 1024 and a 62x32 embedding.\ncluding instantaneous frequency and circular normal cost functions (see Appendix), but in each case independently estimating phase and magnitude led to poor sample quality due to phase errors. We find a large improvement by estimating only the magnitude and using a well established iterative technique to reconstruct the phase (Griffin & Lim, 1984). To get the best results, we used a large FFT size (1024) relative to the hop size (256) and ran the algorithm for 1000 iterations. As a final heuristic, we weighted the MSE loss, starting at 10 for 0Hz and decreasing linearly to 1 at 4000Hz and above. At the expense of some precision in timbre, this created more phase coherence for the fundamentals of notes, where errors in the linear spectrum lead to a larger relative error in frequency."}, {"heading": "2.3. Training", "text": "We train all models with stochastic gradient descent with an Adam optimizer (Kingma & Ba, 2014). The baseline models commonly use a learning rate of 1e-4, while the WaveNet models use a schedule, starting at 2e-4 and descending to 6e-5, 2e-5, and 6e-6 at iterations 120k, 180k, and 240k respectively. The baseline models train asynchronously for 1800k iterations with a batch size of 8. The WaveNet models train synchronously for 250k iterations with a batch size of 32."}, {"heading": "3. The NSynth Dataset", "text": "To evaluate our WaveNet autoencoder model, we wanted an audio dataset that let us explore the learned embeddings. Musical notes are an ideal setting for this study as we hypothesize that the embeddings will capture structure such as pitch, dynamics, and timbre. While several smaller datasets currently exist (Goto et al., 2003; Romani Picas et al., 2015), deep networks train better on abundant, high-quality data, motivating the development of a new dataset."}, {"heading": "3.1. A Dataset of Musical Notes", "text": "NSynth consists of 306 043 musical notes, each with a unique pitch, timbre, and envelope. For 1006 instruments from commercial sample libraries, we generated four second, monophonic 16kHz audio snippets, referred to as notes, by ranging over every pitch of a standard MIDI piano (21-108) as well as five different velocities2 (25, 50, 75, 100, 127). The note was held for the first three seconds and allowed to decay for the final second. Some instruments are not capable of\n2MIDI velocity is similar to volume control and they have a direct relationship. For physical intuition, higher velocity corresponds to pressing a piano key harder.\nproducing all 88 pitches in this range, resulting in an average of 65.4 pitches per instrument. Furthermore, the commercial sample packs occasionally contain duplicate sounds across multiple velocities, leaving an average of 4.75 unique velocities per pitch."}, {"heading": "3.2. Annotations", "text": "We also annotated each of the notes with three additional pieces of information based on a combination of human evaluation and heuristic algorithms:\n\u2022 Source: The method of sound production for the note\u2019s instrument. This can be one of \u2018acoustic\u2019 or \u2018electronic\u2019 for instruments that were recorded from acoustic or electronic instruments, respectively, or \u2018synthetic\u2019 for synthesized instruments.\n\u2022 Family: The high-level family of which the note\u2019s instrument is a member. Each instrument is a member of exactly one family. See Appendix for the complete list.\n\u2022 Qualities: Sonic qualities of the note. See Appendix for the complete list of classes and their co-occurrences. Each note is annotated with zero or more qualities."}, {"heading": "3.2.1. Availability", "text": "The full NSynth dataset is available for download at https://magenta.tensorflow.org/datasets/nsynth as TFRecord files split into training and holdout sets. Each note is represented by a serialized TensorFlow Example protocol buffer containing the note and annotations. Details of the format can be found in the README."}, {"heading": "4. Evaluation", "text": "We evaluate and analyze our models on the tasks of note reconstruction, instrument interpolation, and pitch interpolation.\nAudio is notoriously hard to represent visually. Magnitude spectrograms capture many aspects of a signal for analytics, but two spectrograms that appear very similar to the eye can correspond to audio that sound drastically different due to phase differences. We have included supplemental audio examples of every plot and encourage the reader to listen along as they read.\nThat said, in our analysis we present examples as plots of the constant-q transform (CQT) (Brown, 1991), which is useful because it is shift invariant to changes in the fundamental frequency. In this way, the struc-\nture and envelope of the overtone series (higher harmonics) determines the dynamics and timbre of a note, regardless of its base frequency. However, due to the logarithmic binning of frequencies, transient noise-like impulses appear as rainbow \u201cpyramidal spikes\u201d rather than straight broadband lines. We display CQTs with a pitch range of 24-96 (C2-C8), hop size of 256, 40 bins per octave, and a filter scale of 0.8.\nAs phase plays such an essential part in sample quality, we have attempted to show both magnitude and phase on the same plot. The intensity of lines is proportional to the log magnitude of the power spectrum while the color is given by the derivative of the unrolled phase (\u2018instantaneous frequency\u2019) (Boashash, 1992). We display the derivative of the phase because it creates a solid continuous line for a harmonic of a consistent frequency. We can understand this because if the instantaneous frequency of a harmonic (fharm) and an FFT bin (fbin) are not exactly equal, each timestep will introduce a constant phase shift, \u2206\u03c6 = (fbin \u2212 fharm) hopsizesamplerate . We affectionately re-\nfer to these instantaneous frequency colored spectrograms as \u201dRainbowgrams\u201d due to their tendency to form rainbows as the instantaneous frequencies modulate up and down."}, {"heading": "4.1. Reconstruction", "text": "Figure 2 displays rainbowgrams for notes from 3 different instruments in the holdout set, where the original notes are on the first column and the model reconstructions are on the second and third columns. Each note has a similar structure with some noise on onset, a fundamental frequency with a series of harmonics, and a decay. For all the WaveNet models, there is a slight built-in distortion due to the compression of the mulaw encoding. It is a minor effect for many samples, but is more pronounced for lower frequencies. Using different representations without this distortion is an ongoing area of research.\nWhile each rainbowgram matches the general contour of the original note, we can hear a pronounced difference in sample quality that we can ascribe to certain\nfeatures. For the Glockenspiel, we can see that the WaveNet autoencoder reproduces the magnitude and phase of the fundamental (solid blue stripe, (A)), and also the noise on attack (vertical rainbow spike (B)). There is a slight error in the fundamental as it starts a little high and quickly descends to the correct pitch (C). In contrast, the baseline has a more percussive, multitonal sound, similar to a bell or gong. The fundamental is still present, but so are other frequencies, and the phases estimated from the Griffin-Lim procedure are noisy as indicated by the blurred horizontal rainbow texture (D).\nThe electric piano has a more clearly defined harmonic series (the horizontal rainbow solid lines, (E)) and a noise on the beginning and end of the note (vertical rainbow spikes, (F)). Listening to the sound, we hear that it is slightly distorted, which promotes these upper harmonics. Both the WaveNet autoencoder and the baseline produce rainbowgrams with similar shapes to the original, but with different types of phase artifacts. The WaveNet model has sufficient phase\nstructure to model the distortion, but has a slight wavering of the instantaneous frequency of some harmonics, as seen in the color change in harmonic stripes (G). In contrast, the baseline lacks the structure in phase to maintain the punchy character of the original note, and produces a duller sound that is slightly out of tune. This is represented in the less brightly colored harmonics due to phase noise (H).\nThe flugelhorn displays perhaps the starkest difference between the two models. The sound combines rich harmonics (many lines), non-tonal wind and lip noise (background color), and vibrato - oscillation of pitch that results in a corresponding rainbow of color in all of the harmonics. While the WaveNet autoencoder does not replicate the exact trace of the vibrato (I), it creates a very similar rainbowgram with oscillations in the instantaneous frequency at all levels synced across the harmonics (J). This results in a rich and natural sounding reconstruction with all three aspects of the original sound. The baseline, by comparison, is unable to model such structure. It creates a more or less\ncorrect harmonic series, but the phase has lots of random perturbations. Visually this shows up as colors which are faded and speckled with rainbow noise (K), which contrasts with the bright colors of the original and WaveNet examples. Acoustically, this manifests as an unappealing buzzing sound laid over an inexpressive and consistent series of harmonics. The WaveNet model also produces a few inaudible discontinuities visually evidenced by the vertical rainbow spikes (L)."}, {"heading": "4.1.1. Quantitative Comparison", "text": "Inspired by the use of the Inception Score for images (Salimans et al., 2016), we train a multi-task classification network to perform a quantitative comparison of the model reconstructions by predicting pitch and quality labels on the NSynth dataset (details in the Appendix). The network configuration is the same as the baseline encoder and testing is done on reconstructions of a randomly chosen subset of 4096 examples from the held-out set.\nThe results in Table 1 confirm our qualititive observation that the WaveNet reconstructions are of superior quality. The classifier is \u223c70% more successful at extracting pitch from the reconstructed WaveNet samples than the baseline and several points higher for predicting quality information, giving an accuracy roughly equal to the original audio."}, {"heading": "4.2. Interpolation in Timbre and Dynamics", "text": "Given the limited factors of variation in the dataset, we know that a successful embedding space (Z) should span the range of timbre and dynamics in its reconstructions. In Figure 3, we show reconstructions from linear interpolations (0.5:0.5) in the Z space among three different instruments and additionally compare these to interpolations in the original audio space. The latter are simple super-positions of the individual instruments\u2019 rainbowgrams. This is perceptually equivalent to the two instruments being played at the same time.\nIn contrast, we find that the generative models fuse aspects of the instruments. As we saw in Section 4.1, the WaveNet autoencoder models the data much more\nrealistically than the baseline, so it is no surprise that it also learns a manifold of codes that yield more perceptually interesting reconstructions.\nFor example, in the interpolated note between the bass and flute (Figure 3, column 2), we can hear and see that both the baseline and WaveNet models blend the harmonic structure of the two instruments while imposing the amplitude envelope of the bass note onto the upper harmonics of the flute note. However, the WaveNet model goes beyond this to create a dynamic mixing of the overtones in time, even jumping to a higher harmonic at the end of the note (A). This sound captures expressive aspects of the timbre and dynamics of both the bass and flute, but is distinctly separate from either original note. This contrasts with the interpolation in audio space, where the dynamics and timbre of the two notes is independent. The baseline model also introduces phase distortion similar to those in the reconstructions of the bass and flute.\nWe see this phenomenon again in the interpolation between flute and organ (Figure 3, column 4). Both models also seem to create new harmonic structure, rather than just overlay the original harmonics. The WaveNet model adds additional harmonics as well as a sub-harmonic to the original flute note, all while preserving phase relationships (B). The resulting sound has the breathiness of a flute, with the upper frequency modulation of an organ. By contrast, the lack of phase structure in the baseline leads to a new harmonic yet dull sound lacking a unique character.\nThe WaveNet model additionally has a tendency to exaggerate amplitude modulation behavior, while the baseline suppresses it. If we examine the original organ sound (Figure 3, column 5), we can see a subtle modulation signified by the blue harmonics periodically fading to black (C). The baseline model misses this behavior completely as it is washed out. Conversely, the WaveNet model amplifies the behavior, adding in new harmonics not present in the original note and modulating all the harmonics. This is seen in the figure by four vertical black stripes that align with the four modulations of the original signal (D)."}, {"heading": "4.3. Entanglement of Pitch and Timbre", "text": "By conditioning on pitch during training, we hypothesize that we should be able to generate multiple pitches from a single Z vector that preserve the identity of timbre and dynamics. Our initial attempts were unsuccessful, as it seems our models had learned to ignore the conditioning variable. We investigate this further with classification and correlation studies."}, {"heading": "4.3.1. Pitch Classification from Z", "text": "One way to study the entanglement of pitch and Z is to consider the pitch classification accuracy from embeddings. If training with pitch conditioning disentangles the representation of pitch and timbre, then we would expect a linear pitch classifier trained on the embeddings to drop in accuracy. To test this, we train a series of baseline autoencoder models with different embedding sizes, both with and without pitch conditioning. For each model, we then train a logistic regression pitch classifier on its embeddings and test on a random sample of 4096 held-out embeddings.\nThe first two rows of Table 2 demonstrate that the\nbaseline and WaveNet models decrease in classification accuracy by 13-30% when adding pitch conditioning during training. This is indicative a reduced presence of pitch information in the latent code and thus a decoupling of pitch and timbre information. Further, as the total embedding size decreases below 512, the accuracy drop becomes much more pronounced, reaching a 75% relative decrease. This is likely due to the greater expressivity of larger embeddings, where there is less to be gained from utilizing the pitch conditioning. However, as the embedding size decreases, so too does reconstruction quality. This is more pronounced for the WaveNet models, which have farther to fall in terms of sample quality.\nAs a proof of principle, we find that for a baseline model with an embedding size of 128, we are able to successfully balance reconstruction quality and response to conditioning. Figure 4 demonstrates two octaves of a C major chord created from a single embedding of an electric piano note, but conditioned on different pitches. The resulting harmonic structure of the original note is only partially preserved across the range. As we shift the pitch upwards, a sub-harmonic emerges (A) such that the pitch +12 note is similar to\nthe original except that the harmonics of the octave are accentuated in amplitude. This aligns with our pitch classification results, where we find that pitches are most commonly confused with those one octave away (see Appendix). These errors can account for as much as 20% absolute classification error."}, {"heading": "4.3.2. Z Correlation across Pitch", "text": "We can gain further insight into the relationship between timbre and pitch by examining the correlation of WaveNet embeddings among pitches for a given instrument. Figure 5 shows correlations for several instruments across their entire 88 note range at velocity 127. We see that each instrument has a unique partitioning into two or more registers over which notes of different pitches have similar embeddings. Even the average over all instruments shows a broad distinction between high and low registers. On reflection, this is unsurprising as the timbre and dynamics of an instrument can vary dramatically across its range."}, {"heading": "4.4. Generalization of Temporal Encodings", "text": "The WaveNet autoencoder model has some unique properties that allow it to generalize to situations not in the dataset. Since the model learns embeddings that bias an autoregressive decoder, they effectively act as a \u201ddriving function\u201d for a nonlinear oscillator / infinite impulse response filter. This is made clear by Figure 6, where the embeddings follow a magnitude contour similar to that of the rainbowgrams of their corresponding sounds in Figures 2 and 3.\nFurther, much like a spectrogram, the embeddings only capture a local context. This lets them gener-\nalize in time. The model has only ever seen single notes with sound that lasts for up to three seconds, and yet Figure 7 demonstrates that it can successfully reconstruct both a whole series of notes, as well as notes played for longer than three seconds. While the WaveNet autoencoder adds more harmonics to the original timbre of the organ instrument, it follows the fundamental frequency as it plays up two octaves of a C major arpeggio, back down a G dominant arrpeggio, and holds for several seconds on the base note. The fact that it has never seen a transition between two notes is clear, as the fundamental frequency actually glissandos smoothly between new notes."}, {"heading": "5. Conclusion and Future Directions", "text": "In this paper, we have introduced a WaveNet autoencoder model that captures long term structure without the need for external conditioning and demonstrated its effectiveness on the new NSynth dataset for generative modeling of audio.\nThe WaveNet autoencoder that we describe is a powerful representation for which there remain multiple avenues of exploration. It builds upon the fine-grained local understanding of the original WaveNet work and provides access to a useful hidden space. However, due to memory constraints, it is unable to fully capture global context. Overcoming this limitation is an important open problem.\nNSynth was inspired by image recognition datasets that have been core to recent progress in deep learning. Similar to how many image datasets focus on a single object per example, NSynth hones in on a\nsingle note. Indeed, much modern music production employs such a factorization, using MIDI for note sequences and software synthesizers for timbre. Noteto-note dependencies can be partly restored by passing sequence-level timbre and dynamics information to the note-level synthesizer. While not perfect, this factorization is based on the physics of many instruments and is surprisingly effective.\nWe encourage the broader community to use NSynth as a benchmark and entry point into audio machine learning. We also view NSynth as a building block for future datasets and envision a high-quality multi-note dataset for tasks like generation and transcription that involve learning complex language-like dependencies."}, {"heading": "A. Phase Representation for the Baseline Model", "text": "We explored several audio representations for our baseline model. Each representation uses an MSE cost and always includes the magnitude of the STFT spectrogram. We found that training on the peak-normalized log magnitude of the power spectra correlated better with perceptual distortion. When using phase in teh objective, we regress on the phase angle. We can assume a circular normal distribution (Bishop, 2006) for the phase with a log likelihood loss proportional to cos(\u03c0 \u2217 (x \u2212 x\u0302)). Figure 8 shows CQT spectrograms of reconstructions of a trumpet sound from models trained on each input representation. We also include audio of each reconstruction, which is essential listening to hear the improvement of the perceptual weighting."}, {"heading": "B. Description of Quality Tags", "text": "We provide quality annotations for the 10 different note qualities described below. None of the tags are mutually exclusive by definition except for Bright and Dark. However, it is possible for a note to be neither Bright nor Dark.\n\u2022 Bright: A large amount of high frequency content and strong upper harmonics.\n\u2022 Dark: A distinct lack of high frequency content, giving a muted and bassy sound. Also sometimes described as \u2019Warm\u2019.\n\u2022 Distortion: Waveshaping that produces a distinctive crunchy sound and presence of many harmonics. Sometimes paired with non-harmonic noise.\n\u2022 Fast Decay: Amplitude envelope of all harmonics decays substantially before the \u2019note-off\u2019 point at 3 seconds.\n\u2022 Long Release: Amplitude envelope decays slowly after the \u2019note-off\u2019 point, sometimes still present at the end of the sample at 4 seconds.\n\u2022 Multiphonic: Presence of overtone frequencies related to more than one fundamental frequency.\n\u2022 Non-Linear Envelope: Modulation of the sound with a distinct envelope behavior different than the monotonic decrease of the note. Can\nalso include filter envelopes as well as dynamic envelopes.\n\u2022 Percussive: A loud non-harmonic sound at note onset.\n\u2022 Reverb: Room acoustics that were not able to be removed from the original sample.\n\u2022 Tempo-Synced: Rhythmic modulation of the sound to a fixed tempo."}, {"heading": "C. Details of Pitch and Quality Classifier", "text": "We train a multi-task classification model to do pitch and quality tag classification on the entire NSynth dataset. We use the the encoder structure from the baseline model with the exception that there is no bottleneck (see Figure 10). We use a softmax-crossentropy loss for the pitch labels as they are mutually exclusive and a sigmoid-crossentropy loss for the quality tags as they are not. Note that since the architecture uses only magnitude spectra, it cannot take advantage of the improved phase coherence of the WaveNet samples."}], "references": [{"title": "The million song dataset", "author": ["Bertin-Mahieux", "Thierry", "Ellis", "Daniel PW", "Whitman", "Brian", "Lamere", "Paul"], "venue": "In ISMIR,", "citeRegEx": "Bertin.Mahieux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bertin.Mahieux et al\\.", "year": 2011}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "Estimating and interpreting the instantaneous frequency of a signal", "author": ["Boashash", "Boualem"], "venue": "i. fundamentals. Proceedings of the IEEE,", "citeRegEx": "Boashash and Boualem.,? \\Q1992\\E", "shortCiteRegEx": "Boashash and Boualem.", "year": 1992}, {"title": "Calculation of a constant q spectral transform", "author": ["Brown", "Judith C"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "Brown and C.,? \\Q1991\\E", "shortCiteRegEx": "Brown and C.", "year": 1991}, {"title": "Variational lossy autoencoder", "author": ["Chen", "Xi", "Kingma", "Diederik P", "Salimans", "Tim", "Duan", "Yan", "Dhariwal", "Prafulla", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": "CoRR, abs/1611.02731,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "The synthesis of complex audio spectra by means of frequency modulation", "author": ["Chowning", "John M"], "venue": "Journal of the audio engineering society,", "citeRegEx": "Chowning and M.,? \\Q1973\\E", "shortCiteRegEx": "Chowning and M.", "year": 1973}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"], "venue": "In CVPR09,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Rwc music database: Music genre database and musical instrument sound database", "author": ["Goto", "Masataka", "Hashiguchi", "Hiroki", "Nishimura", "Takuichi", "Oka", "Ryuichi"], "venue": null, "citeRegEx": "Goto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Goto et al\\.", "year": 2003}, {"title": "Signal estimation from modified short-time fourier transform", "author": ["Griffin", "Daniel", "Lim", "Jae"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Griffin et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Griffin et al\\.", "year": 1984}, {"title": "Pixelvae: A latent variable model for natural images", "author": ["Gulrajani", "Ishaan", "Kumar", "Kundan", "Ahmed", "Faruk", "Taiga", "Adrien Ali", "Visin", "Francesco", "V\u00e1zquez", "David", "Courville", "Aaron C"], "venue": "CoRR, abs/1611.05013,", "citeRegEx": "Gulrajani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulrajani et al\\.", "year": 2016}, {"title": "Minst, a collection of musical sound datasets, 2016", "author": ["Humphrey", "Eric J"], "venue": "URL https://github.com/ ejhumphrey/minst-dataset/", "citeRegEx": "Humphrey and J.,? \\Q2016\\E", "shortCiteRegEx": "Humphrey and J.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "shift. CoRR,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "The blizzard challenge", "author": ["King", "Simon", "Clark", "Robert AJ", "Mayo", "Catherine", "Karaiskos", "Vasilis"], "venue": null, "citeRegEx": "King et al\\.,? \\Q2008\\E", "shortCiteRegEx": "King et al\\.", "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Sound texture synthesis via filter statistics", "author": ["McDermott", "Josh H", "Oxenham", "Andrew J", "Simoncelli", "Eero P"], "venue": "In Applications of Signal Processing to Audio and Acoustics,", "citeRegEx": "McDermott et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McDermott et al\\.", "year": 2009}, {"title": "Samplernn: An unconditional end-to-end neural audio generation model", "author": ["Mehri", "Soroush", "Kumar", "Kundan", "Gulrajani", "Ishaan", "Rithesh", "Jain", "Shubham", "Sotelo", "Jose", "Courville", "Aaron C", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Mehri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mehri et al\\.", "year": 2016}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Analog days: The invention and impact of the Moog synthesizer", "author": ["Pinch", "Trevor J", "Trocco", "Frank", "TJ"], "venue": null, "citeRegEx": "Pinch et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pinch et al\\.", "year": 2009}, {"title": "Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching", "author": ["Raffel", "Colin"], "venue": "PhD thesis, COLUMBIA UNIVERSITY,", "citeRegEx": "Raffel and Colin.,? \\Q2016\\E", "shortCiteRegEx": "Raffel and Colin.", "year": 2016}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian J", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": "CoRR, abs/1606.03498,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Musical audio synthesis using autoencoding neural nets", "author": ["Sarroff", "Andy M", "Casey", "Michael A"], "venue": "In ICMC,", "citeRegEx": "Sarroff et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sarroff et al\\.", "year": 2014}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Learning features of music from scratch", "author": ["Thickstun", "John", "Harchaoui", "Zaid", "Kakade", "Sham"], "venue": "In preprint, https: // arxiv. org/ abs/ 1611", "citeRegEx": "Thickstun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Thickstun et al\\.", "year": 2016}, {"title": "Wavenet: A generative model for raw audio", "author": ["van den Oord", "A\u00e4ron", "Dieleman", "Sander", "Zen", "Heiga", "Simonyan", "Karen", "Vinyals", "Oriol", "Graves", "Alex", "Kalchbrenner", "Nal", "Senior", "Andrew W", "Kavukcuoglu", "Koray"], "venue": null, "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks. CoRR, abs/1601.06759, 2016b", "author": ["van den Oord", "A\u00e4ron", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "URL http: //arxiv.org/abs/1601.06759", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Sampling generative networks: Notes on a few effective techniques", "author": ["White", "Tom"], "venue": "CoRR, abs/1609.04468,", "citeRegEx": "White and Tom.,? \\Q2016\\E", "shortCiteRegEx": "White and Tom.", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "Synthesizers have a long history of being hand-designed instruments, accepting control signals such as \u2018pitch\u2019, \u2018velocity\u2019, and filter parameters to shape the tone, timbre, and dynamics of a sound (Pinch et al., 2009).", "startOffset": 197, "endOffset": 217}, {"referenceID": 19, "context": ", 2016a) and SampleRNN (Mehri et al., 2016).", "startOffset": 23, "endOffset": 43}, {"referenceID": 7, "context": "Recent breakthroughs in generative modeling of images (Kingma & Welling, 2013; Goodfellow et al., 2014; van den Oord et al., 2016b) have been predicated on ar X iv :1 70 4.", "startOffset": 54, "endOffset": 131}, {"referenceID": 17, "context": "the availability of high-quality and large-scale datasets such as MNIST (LeCun et al., 1998), SVHN (Netzer et al.", "startOffset": 72, "endOffset": 92}, {"referenceID": 20, "context": ", 1998), SVHN (Netzer et al., 2011), CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 6, "context": ", 2011), CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al., 2009).", "startOffset": 56, "endOffset": 75}, {"referenceID": 25, "context": "While generative models are notoriously hard to evaluate (Theis et al., 2015), these datasets provide a common test bed for consistent qualitative and quantitative evaluation, such as with the use of the Inception score (Salimans et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 23, "context": ", 2015), these datasets provide a common test bed for consistent qualitative and quantitative evaluation, such as with the use of the Inception score (Salimans et al., 2016).", "startOffset": 150, "endOffset": 173}, {"referenceID": 0, "context": "Audio signals found in the wild contain multi-scale dependencies that prove particularly difficult to model (Raffel, 2016; Bertin-Mahieux et al., 2011; King et al., 2008; Thickstun et al., 2016), leading many previous efforts at data-driven audio synthesis to focus on more constrained domains such as texture synthesis or training small parametric models (Sarroff & Casey, 2014; McDermott et al.", "startOffset": 108, "endOffset": 194}, {"referenceID": 13, "context": "Audio signals found in the wild contain multi-scale dependencies that prove particularly difficult to model (Raffel, 2016; Bertin-Mahieux et al., 2011; King et al., 2008; Thickstun et al., 2016), leading many previous efforts at data-driven audio synthesis to focus on more constrained domains such as texture synthesis or training small parametric models (Sarroff & Casey, 2014; McDermott et al.", "startOffset": 108, "endOffset": 194}, {"referenceID": 26, "context": "Audio signals found in the wild contain multi-scale dependencies that prove particularly difficult to model (Raffel, 2016; Bertin-Mahieux et al., 2011; King et al., 2008; Thickstun et al., 2016), leading many previous efforts at data-driven audio synthesis to focus on more constrained domains such as texture synthesis or training small parametric models (Sarroff & Casey, 2014; McDermott et al.", "startOffset": 108, "endOffset": 194}, {"referenceID": 18, "context": ", 2016), leading many previous efforts at data-driven audio synthesis to focus on more constrained domains such as texture synthesis or training small parametric models (Sarroff & Casey, 2014; McDermott et al., 2009).", "startOffset": 169, "endOffset": 216}, {"referenceID": 10, "context": "We could parameterize Z as a latent variable p(Z|x) that we would have to marginalize over (Gulrajani et al., 2016), but in practice we have found this to be less effective.", "startOffset": 91, "endOffset": 115}, {"referenceID": 4, "context": "As discussed in (Chen et al., 2016), this may be due to the decoder being so powerful that it can ignore the latent variables unless they encode a much larger context that\u2019s otherwise inaccessible.", "startOffset": 16, "endOffset": 35}, {"referenceID": 8, "context": "While several smaller datasets currently exist (Goto et al., 2003; Romani Picas et al., 2015), deep networks train better on abundant, high-quality data, motivating the development of a new dataset.", "startOffset": 47, "endOffset": 93}, {"referenceID": 23, "context": "Inspired by the use of the Inception Score for images (Salimans et al., 2016), we train a multi-task classification network to perform a quantitative comparison of the model reconstructions by predicting pitch and quality labels on the NSynth dataset (details in the Appendix).", "startOffset": 54, "endOffset": 77}], "year": 2017, "abstractText": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.", "creator": "LaTeX with hyperref package"}}}