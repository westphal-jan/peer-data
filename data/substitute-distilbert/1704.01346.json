{"id": "1704.01346", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "CompiLIG at SemEval-2017 Task 1: Cross-Language Plagiarism Detection Methods for Semantic Textual Similarity", "abstract": "we present the submitted systems for semantic textual similarity ( sts ) track 4 at semeval - 2017. given a pair of spanish - themed sentences, each system must estimate their structure similarity by a score between 0 and 5. in our submission, we use syntax - shifting, dictionary - producing, context - based, and mt - based methods. we also combine these methods in unsupervised and experimental way. our best run took 1st on track 4a with numerical slope of 83. 02 % with recent annotations.", "histories": [["v1", "Wed, 5 Apr 2017 10:07:22 GMT  (23kb)", "http://arxiv.org/abs/1704.01346v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jeremy ferrero", "frederic agnes", "laurent besacier", "didier schwab"], "accepted": false, "id": "1704.01346"}, "pdf": {"name": "1704.01346.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Didier Schwab"], "emails": ["jeremy.ferrero@imag.fr", "frederic@compilatio.net", "laurent.besacier@imag.fr", "didier.schwab@imag.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n01 34\n6v 1\n[ cs\n.C L\n] 5\nA pr\nWe present our submitted systems for Semantic Textual Similarity (STS) Track 4 at SemEval-2017. Given a pair of SpanishEnglish sentences, each system must estimate their semantic similarity by a score between 0 and 5. In our submission, we use syntax-based, dictionary-based, context-based, and MT-based methods. We also combine these methods in unsupervised and supervised way. Our best run ranked 1st on track 4a with a correlation of 83.02% with human annotations."}, {"heading": "1 Introduction", "text": "CompiLIG is a collaboration between Compilatio1 - a company particularly interested in crosslanguage plagiarism detection - and LIG research group on natural language processing (GETALP). Cross-language semantic textual similarity detection is an important step for cross-language plagiarism detection, and evaluation campaigns in this new domain are rare. For the first time, SemEval STS task (Agirre et al., 2016) was extended with a Spanish-English cross-lingual sub-task in 2016. This year, sub-task was renewed under track 4 (divided in two sub-corpora: track 4a and track 4b).\nGiven a sentence in Spanish and a sentence in English, the objective is to compute their semantic textual similarity according to a score from 0\n1 www.compilatio.net\nto 5, where 0 means no similarity and 5 means full semantic similarity. The evaluation metric is a Pearson correlation coefficient between the submitted scores and the gold standard scores from human annotators. Last year, among 26 submissions from 10 teams, the method that achieved the best performance (Brychcin and Svoboda, 2016) was a supervised system (SVM regression with RBF kernel) based on word alignment algorithm presented in Sultan et al. (2015).\nOur submission in 2017 is based on crosslanguage plagiarism detection methods combined with the best performing STS detection method published in 2016. CompiLIG team participated to SemEval STS for the first time in 2017. The methods proposed are syntax-based, dictionary-based, context-based, and MT-based. They show additive value when combined. The submitted runs consist in (1) our best single unsupervised approach (2) an unsupervised combination of best approaches (3) a fine-tuned combination of best approaches. The best of our three runs ranked 1st with a correlation of 83.02% with human annotations on track 4a among all submitted systems (51 submissions from 20 teams for this track). Correlation results of all participants (including ours) on track 4b were much lower and we try to explain why (and question the validity of track 4b) in the last part of this paper."}, {"heading": "2 Cross-Language Textual Similarity Detection Methods", "text": "2.1 Cross-Language Character N-Gram\n(CL-CnG)\nCL-CnG aims to measure the syntactical similarity between two texts. It is based on Mcnamee and Mayfield (2004) work used in information retrieval. It compares two texts under their n-grams vectors representation. The main advantage of this kind of method is that it does not require any translation between source and target text.\nAfter some tests on previous year\u2019s dataset to find the best n, we decide to use the Potthast et al. (2011)\u2019s CL-C3G implementation. Let Sx and Sy two sentences in two different languages. First, the alphabet of these sentences is normalized to the ensemble \u2211\n= {a \u2212 z, 0 \u2212 9, }, so only spaces and alphanumeric characters are kept. Any other diacritic or symbol is deleted and the whole text is lower-cased. The texts are then segmented into 3-grams (sequences of 3 contiguous characters) and transformed into tf.idf vectors of character 3-grams. We directly build our idf model on the evaluation data. We use a double normalization K (with K = 0.5) as tf (Manning et al., 2008) and a inverse document frequency smooth as idf. Finally, a cosine similarity is computed between the vectors of source and target sentences."}, {"heading": "2.2 Cross-Language Conceptual Thesaurus-based Similarity (CL-CTS)", "text": "CL-CTS (Gupta et al., 2012; Pataki, 2012) aims to measure the semantic similarity between two vectors of concepts. The model consists in representing texts as bag of words (or concepts) to compare them. The method also does not require explicit translation since the matching is performed using internal connections in the used \u201contology\u201d.\nLet S a sentence of length n, the n words of the\nsentence are represented by wi as:\nS = {w1, w2, w3, ..., wn} (1)\nSx and Sy are two sentences in two different languages. A bag of words S\u2032 from each sentence S is built, by filtering stop words and by using a function that returns for a given word all its possible translations. These translations are jointly given by a linked lexical resource, DBNary (Se\u0301rasset, 2015), and by cross-lingual word em-\nbeddings. More precisely, we use the top 10 closest words in the embeddings model and all the available translations from DBNary to build the bag of words of a word. We use the MultiVec (Berard et al., 2016) toolkit for computing and managing word embeddings. The corpora used to build the embeddings are Europarl and Wikipedia sub-corpus, part of the dataset of Ferrero et al. (2016)2. For training our embeddings, we use CBOW model with a vector size of 100, a window size of 5, a negative sampling parameter of 5, and an alpha of 0.02.\nSo, the sets of words S\u2032x and S \u2032 y are the conceptual representations in the same language of Sx and Sy respectively. To calculate the similarity between Sx and Sy, we use a syntactically and frequentially weighted augmentation of the Jaccard distance, defined as:\nJ(Sx, Sy) = \u2126(S\u2032x) + \u2126(S \u2032 y)\n\u2126(Sx) + \u2126(Sy) (2)\nwhere Sx and Sy are the input sentences (also represented as sets of words), and \u2126 is the sum of the weights of the words of a set, defined as:\n\u2126(S) =\nn\u2211\ni=1 , wi\u2208S\n\u03d5(wi) (3)\nwhere wi is the i th word of the bag S, and \u03d5 is\nthe weight of word in the Jaccard distance:\n\u03d5(w) = pos weight(w)1\u2212\u03b1 . idf(w)\u03b1 (4)\nwhere pos weight is the function which gives the weight for each universal part-of-speech tag of a word, idf is the function which gives the inverse document frequency of a word, and . is the scalar product. Equation (4) is a way to syntactically (pos weight) and frequentially (idf ) weight the contribution of a word to the Jaccard distance (both contributions being controlled with the \u03b1 parameter). We assume that for one word, we have its part-of-speech within its original sentence, and its inverse document frequency. We use TreeTagger (Schmid, 1994) for POS tagging, and we normalize the tags with Universal Tagset of Petrov et al. (2012). Then, we assign a weight for each of the 12 universal POS tags. The 12 POS weights and the value \u03b1 are optimized with Condor (Berghen and Bersini, 2005) in the same\n2 https://github.com/FerreroJeremy/Cross-Language-Datase\nway as in Ferrero et al. (2017). Condor applies a Newton's method with a trust region algorithm to determinate the weights that optimize a desired output score. No re-tuning of these hyperparameters for SemEval task was performed."}, {"heading": "2.3 Cross-Language Word Embedding-based Similarity", "text": "CL-WES (Ferrero et al., 2017) consists in a cosine similarity on distributed representations of sentences, which are obtained by the weighted sum of each word vector in a sentence. As in previous section, each word vector is syntactically and frequentially weighted.\nIf Sx and Sy are two sentences in two different languages, then CL-WES builds their (bilingual) common representation vectors Vx and Vy and applies a cosine similarity between them. A distributed representation V of a sentence S is calculated as follows:\nV =\nn\u2211\ni=1 , wi\u2208S\n(vector(wi) . \u03d5(wi)) (5)\nwhere wi is the i th word of the sentence S, vector is the function which gives the word embedding vector of a word, \u03d5 is the same that in formula (4), and . is the scalar product. We make this method publicly available through MultiVec3 (Berard et al., 2016) toolkit."}, {"heading": "2.4 Translation + Monolingual Word Alignment (T+WA)", "text": "The last method used is a two-step process. First, we translate the Spanish sentence into English with Google Translate (i.e. we are bringing the two sentences in the same language). Then, we align both utterances. We reuse the monolingual aligner4 of Sultan et al. (2015) with the improvement of Brychcin and Svoboda (2016), who won the cross-lingual sub-task in 2016 (Agirre et al., 2016). Because this improvement has not been released by the initial authors, we propose to share our re-implementation on GitHub5.\nIf Sx and Sy are two sentences in the same language, then we try to measure their similarity with the following formula:\nJ(Sx, Sy) = \u03c9(Ax) + \u03c9(Ay)\n\u03c9(Sx) + \u03c9(Sy) (6)\n3 https://github.com/eske/multivec 4https://github.com/ma-sultan/monolingual-word-aligner 5 https://github.com/FerreroJeremy/monolingual-word-aligner\nwhere Sx and Sy are the input sentences (represented as sets of words), Ax and Ay are the sets of aligned words for Sx and Sy respectively, and \u03c9 is a frequency weight of a set of words, defined as:\n\u03c9(A) =\nn\u2211\ni=1 , wi\u2208A\nidf(wi) (7)\nwhere idf is the function which gives the in-\nverse document frequency of a word."}, {"heading": "2.5 System Combination", "text": "These methods are syntax-, dictionary-, contextand MT- based, and are thus potentially complementary. That is why we also combine them in unsupervised and supervised fashion. Our unsupervised fusion is an average of the outputs of each method. For supervised fusion, we recast fusion as a regression problem and we experiment all available methods inWeka 3.8.0 (Hall et al., 2009)."}, {"heading": "3 Results on SemEval-2016 Dataset", "text": "Table 1 reports the results of the proposed systems on SemEval-2016 STS cross-lingual evaluation dataset. The dataset, the annotation and the evaluation systems were presented in the SemEval-2016 STS task description paper (Agirre et al., 2016), so we do not re-detail them here. The lines in bold represent the methods that obtain the best mean score in each category of system (best method alone, unsupervised and supervised fusion). The scores for the supervised systems are obtained with 10-folds cross-validation."}, {"heading": "4 Runs Submitted to SemEval-2017", "text": "First, it is important to mention that our outputs are linearly re-scaled to a real-valued space [0 ; 5].\nRun 1: Best Method Alone. Our first run is only based on the best method alone during our tests (see Table 1), i.e. Cross-Language Conceptual Thesaurus-based Similarity (CL-CTS) model, as described in section 2.2.\nRun 2: Fusion by Average. Our second run is a fusion by average on three methods: CL-C3G, CL-CTS and T+WA, all described in section 2.\nRun 3: M5\u2032 Model Tree. Unlike the two precedent runs, the third run is a supervised system. We have selected the system that obtained the best score during our tests on SemEval-2016 evaluation dataset (see Table 1), which is the M5\u2032 m del tree (Wang and Witten, 1997) (called\nM5P in Weka 3.8.0 (Hall et al., 2009)). Model trees have a conventional decision tree structure but use linear regression functions at the leaves instead of discrete class labels. The first implementation of model trees, M5, was proposed by Quinlan (1992) and the approach was refined and improved in a system called M5\u2032 by Wang and Witten (1997). To learn the model, we use all the methods described in section 2 as features."}, {"heading": "5 Results of the 2017 evaluation and Discussion", "text": "Dataset, annotation and evaluation systems are presented in SemEval-2017 STS task description paper (Agirre et al., 2017). We can see in Table 2 that our systems work well on SNLI6 (Bowman et al., 2015) (track 4a), on which we ranked 1st with more than 83% of correlation with human annotations. Conversely, correlations on theWMT corpus (track 4b) are strangely low. This difference is notable on the scores of all participating teams (Agirre et al., 2017)7. This might be explained by the fact that WMT was annotated by\n6 http://nlp.stanford.edu/projects/snli/\n7The best score for this track is 34%, while for the other tracks it is around 85%.\nonly one annotator, while the SNLI corpus was annotated by many.\nTo investigate deeper on this issue, we manually annotated 60 random pairs of each sub-corpus (120 annotated pairs among 500). These annotations provide a second annotator reference. We can see in Table 3 that, on SNLI corpus (4a), our methods behave the same way for both annotations (a difference of about 1.3%). However, the difference in correlation is huge between our annotations and SemEval gold standard on the WMT corpus (4b): 30% on average. The Pearson correlation between our annotated pairs and the related gold standard is 85.76% for the SNLI corpus and 29.16% for the WMT corpus. These results question the validity of the WMT corpus (4b) for semantic textual similarity detection."}, {"heading": "6 Conclusion", "text": "We described our submission to SemEval-2017 Semantic Textual Similarity task on track 4 (SpEn cross-lingual sub-task). Our best results were achieved by a M5\u2032 model tree combination of various textual similarity detection techniques. This approach worked well on the SNLI corpus (4a - finishes 1st with more than 83% of correlation with human annotations), which corresponds to a real cross-language plagiarism detection scenario. We also questioned WMT corpus (4b) validity providing our own manual annotations and showing low correlations with those of SemEval."}], "references": [{"title": "SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation", "author": ["Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Rada Mihalcea", "Janyce Wiebe."], "venue": "Proceedings of the 10th International", "citeRegEx": "Agirre et al\\.,? 2016", "shortCiteRegEx": "Agirre et al\\.", "year": 2016}, {"title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "I\u00f1igo LopezGazpio", "Lucia Specia."], "venue": "Proceedings of the 11th InternationalWorkshop on Semantic", "citeRegEx": "Agirre et al\\.,? 2017", "shortCiteRegEx": "Agirre et al\\.", "year": 2017}, {"title": "MultiVec: a Multilingual and Multilevel Representation Learning Toolkit for NLP", "author": ["Alexandre Berard", "Christophe Servan", "Olivier Pietquin", "Laurent Besacier."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation", "citeRegEx": "Berard et al\\.,? 2016", "shortCiteRegEx": "Berard et al\\.", "year": 2016}, {"title": "CONDOR, a new parallel, constrained extension of Powell\u2019s UOBYQA algorithm: Experimental results and comparison with the Journal of Computational and Applied", "author": ["Frank Vanden Berghen", "Hugues Bersini"], "venue": null, "citeRegEx": "Berghen and Bersini.,? \\Q2005\\E", "shortCiteRegEx": "Berghen and Bersini.", "year": 2005}, {"title": "A Large Annotated Corpus for Learning Natural Language Inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "UWB at SemEval-2016 Task 1: Semantic textual similarity using lexical, syntactic, and semantic information", "author": ["Tomas Brychcin", "Lukas Svoboda."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016). San Diego, CA, USA, pages 588\u2013594.", "citeRegEx": "Brychcin and Svoboda.,? 2016", "shortCiteRegEx": "Brychcin and Svoboda.", "year": 2016}, {"title": "A Multilingual, Multi-style and Multi-granularity Dataset for Cross-language Textual Similarity Detection", "author": ["J\u00e9r\u00e9my Ferrero", "Fr\u00e9d\u00e9ric Agn\u00e8s", "Laurent Besacier", "Didier Schwab."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation", "citeRegEx": "Ferrero et al\\.,? 2016", "shortCiteRegEx": "Ferrero et al\\.", "year": 2016}, {"title": "Using Word Embedding for Cross-Language Plagiarism Detection", "author": ["J\u00e9r\u00e9my Ferrero", "Laurent Besacier", "Didier Schwab", "Fr\u00e9d\u00e9ric Agn\u00e8s."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Ferrero et al\\.,? 2017", "shortCiteRegEx": "Ferrero et al\\.", "year": 2017}, {"title": "DLS@CU: Sentence similarity from word alignment and semantic vector composition", "author": ["Md Arafat Sultan", "Steven Bethard", "Tamara Sumner."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Sultan et al\\.,? 2015", "shortCiteRegEx": "Sultan et al\\.", "year": 2015}, {"title": "Induction of model trees for predicting continuous classes", "author": ["Yong Wang", "Ian H. Witten."], "venue": "Proceedings of the poster papers of the European Conference on Machine Learning. Prague, Czech Republic, pages 128\u2013137.", "citeRegEx": "Wang and Witten.,? 1997", "shortCiteRegEx": "Wang and Witten.", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "For the first time, SemEval STS task (Agirre et al., 2016) was extended with", "startOffset": 37, "endOffset": 58}, {"referenceID": 5, "context": "Last year, among 26 submissions from 10 teams, the method that achieved the best performance (Brychcin and Svoboda, 2016) was a supervised system (SVM regression with RBF kernel) based on word alignment algorithm presented in Sultan et al.", "startOffset": 93, "endOffset": 121}, {"referenceID": 5, "context": "Last year, among 26 submissions from 10 teams, the method that achieved the best performance (Brychcin and Svoboda, 2016) was a supervised system (SVM regression with RBF kernel) based on word alignment algorithm presented in Sultan et al. (2015).", "startOffset": 94, "endOffset": 247}, {"referenceID": 2, "context": "We use the MultiVec (Berard et al., 2016) toolkit for computing and managing word embeddings.", "startOffset": 20, "endOffset": 41}, {"referenceID": 2, "context": "We use the MultiVec (Berard et al., 2016) toolkit for computing and managing word embeddings. The corpora used to build the embeddings are Europarl and Wikipedia sub-corpus, part of the dataset of Ferrero et al. (2016). For training our embeddings, we use CBOW model with a vector size of 100, a window size of 5, a negative sampling parameter of 5, and an alpha of 0.", "startOffset": 21, "endOffset": 219}, {"referenceID": 3, "context": "The 12 POS weights and the value \u03b1 are optimized with Condor (Berghen and Bersini, 2005) in the same", "startOffset": 61, "endOffset": 88}, {"referenceID": 6, "context": "way as in Ferrero et al. (2017). Condor applies a Newton's method with a trust region algorithm to determinate the weights that optimize a desired output score.", "startOffset": 10, "endOffset": 32}, {"referenceID": 7, "context": "CL-WES (Ferrero et al., 2017) consists in a cosine similarity on distributed representations of sentences, which are obtained by the weighted sum of each word vector in a sentence.", "startOffset": 7, "endOffset": 29}, {"referenceID": 2, "context": "We make this method publicly available through MultiVec (Berard et al., 2016) toolkit.", "startOffset": 56, "endOffset": 77}, {"referenceID": 0, "context": "(2015) with the improvement of Brychcin and Svoboda (2016), who won the cross-lingual sub-task in 2016 (Agirre et al., 2016).", "startOffset": 103, "endOffset": 124}, {"referenceID": 5, "context": "We reuse the monolingual aligner of Sultan et al. (2015) with the improvement of Brychcin and Svoboda (2016), who won the cross-lingual sub-task in 2016 (Agirre et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 3, "context": "(2015) with the improvement of Brychcin and Svoboda (2016), who won the cross-lingual sub-task in 2016 (Agirre et al.", "startOffset": 31, "endOffset": 59}, {"referenceID": 0, "context": "The dataset, the annotation and the evaluation systems were presented in the SemEval-2016 STS task description paper (Agirre et al., 2016), so we do not re-detail them here.", "startOffset": 117, "endOffset": 138}, {"referenceID": 9, "context": "We have selected the system that obtained the best score during our tests on SemEval-2016 evaluation dataset (see Table 1), which is the M5 m del tree (Wang and Witten, 1997) (called", "startOffset": 151, "endOffset": 174}, {"referenceID": 9, "context": "The first implementation of model trees, M5, was proposed by Quinlan (1992) and the approach was refined and improved in a system called M5 by Wang and Witten (1997). To learn the model, we use all the methods described in section 2 as features.", "startOffset": 143, "endOffset": 166}, {"referenceID": 1, "context": "Dataset, annotation and evaluation systems are presented in SemEval-2017 STS task description paper (Agirre et al., 2017).", "startOffset": 100, "endOffset": 121}, {"referenceID": 4, "context": "We can see in Table 2 that our systems work well on SNLI (Bowman et al., 2015) (track 4a), on which we ranked 1 with more than 83% of correlation with human annotations.", "startOffset": 57, "endOffset": 78}, {"referenceID": 1, "context": "difference is notable on the scores of all participating teams (Agirre et al., 2017).", "startOffset": 63, "endOffset": 84}], "year": 2017, "abstractText": "We present our submitted systems for Semantic Textual Similarity (STS) Track 4 at SemEval-2017. Given a pair of SpanishEnglish sentences, each system must estimate their semantic similarity by a score between 0 and 5. In our submission, we use syntax-based, dictionary-based, context-based, and MT-based methods. We also combine these methods in unsupervised and supervised way. Our best run ranked 1 on track 4a with a correlation of 83.02% with human annotations.", "creator": "LaTeX with hyperref package"}}}