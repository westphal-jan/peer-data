{"id": "1704.01415", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2017", "title": "Multi-Label Learning with Global and Local Label Correlation", "abstract": "it is well - known that exploiting label comparison is important to multi - label cases. existing approaches currently argue that the label correlations are global than shared by all instances ; or that the label correlations are local and shared only by network data system. in fact, in less real - world applications, both cases may occur that some label bundles are globally applicable and some are shared only in a local group of instances. moreover, attention is also at usual case that only partial correlation are observed, which makes the exploitation of the label correlations much more difficult. that continues, it is hard to estimate the label correlations when many labels are absent. in : paper, we start a new multi - label approach glocal dealing explores both seemingly full - label and simply missing - label cases, exploiting global and local label correlations simultaneously, through learning a latent label representation and optimizing label manifolds. the extensive experimental studies validate the effectiveness of our approach on both full - label and missing - trace data.", "histories": [["v1", "Tue, 4 Apr 2017 12:50:25 GMT  (413kb,D)", "http://arxiv.org/abs/1704.01415v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["yue zhu", "james t kwok", "zhi-hua zhou"], "accepted": false, "id": "1704.01415"}, "pdf": {"name": "1704.01415.pdf", "metadata": {"source": "CRF", "title": "Multi-Label Learning with Global and Local Label Correlation", "authors": ["Yue Zhu", "James T. Kwok", "Zhi-Hua Zhou"], "emails": ["zhouzh}@lamda.nju.edu.cn", "jamesk@cse.ust.hk"], "sections": [{"heading": null, "text": "It is well-known that exploiting label correlations is important to multi-label learning. Existing approaches either assume that the label correlations are global and shared by all instances; or that the label correlations are local and shared only by a data subset. In fact, in the real-world applications, both cases may occur that some label correlations are globally applicable and some are shared only in a local group of instances. Moreover, it is also a usual case that only partial labels are observed, which makes the exploitation of the label correlations much more difficult. That is, it is hard to estimate the label correlations when many labels are absent. In this paper, we propose a new multi-label approach GLOCAL dealing with both the full-label and the missinglabel cases, exploiting global and local label correlations simultaneously, through learning a latent label representation and optimizing label manifolds. The extensive experimental studies validate the effectiveness of our approach on both full-label and missing-label data. Key words: Global and local label correlation, label manifold, missing labels, multi-label learning."}, {"heading": "1. Introduction", "text": "In real-world classification applications, an instance is often associated with more than one class labels. For example, a scene image can be annotated with several tags [3], a document may\n\u2217Corresponding author.\nPreprint submitted for review April 6, 2017\nar X\niv :1\n70 4.\n01 41\n5v 1\n[ cs\n.L G\n] 4\nA pr\nbelong to multiple topics [18], and a piece of music may be associated with different genres [17]. Thus, multi-label learning has attracted a lot of attention in recent years [24].\nCurrent studies on multi-label learning try to incorporate label correlations of different orders [24]. However, existing approaches mostly focus on global label correlations shared by all instances [7, 10, 15]. For example, labels \u201cfish\u201d and \u201cocean\u201d are highly correlated, and so are \u201cstock\u201d and \u201cfinance\u201d. On the other hand, certain label correlations are only shared by a local data subset [9]. For example, \u201capple\u201d is related to \u201cfruit\u201d in gourmet magazines, but is related to \u201cdigital devices\u201d in technology magazines. Previous studies focus on exploiting either global or local label correlations. However, considering both of them is obviously more beneficial and desirable.\nAnother problem with label correlations is that they are usually difficult to specify manually. As label correlations may vary in different contexts and there is no unified measure for specifying appropriate correlations, they are usually estimated from the observed data. Some approaches learn the label hierarchies by hierarchical clustering [14] or Bayesian network structure learning [23]. However, the hierarchical structure may not exist in some applications. For example, labels such as \u201cdesert\u201d, \u201cmountains\u201d, \u201csea\u201d, \u201csunset\u201d and \u201ctrees\u201d do not have any natural hierarchical correlations, and label hierarchies may not be useful. Others estimate label correlations by the co-occurrence of labels in training data [13]. However, it may cause overfitting. Moreover, co-occurrence is less meaningful for labels with very few positive instances.\nIn multi-label learning, some labels may be missing from the training set. For example, human labelers may ignore object classes they do not know or of little interest. Recently, multi-label learning with missing labels has become a hot topic. Xu et al. [21] and Yu et al. [22] considered using the low-rank structure on the instance-label mapping. A more direct approach to model the label dependency approximates the label matrix as a product of two low-rank matrices [8]. This leads to simpler recovery of the missing labels, and produces a latent representation of the label matrix.\nIn the missing label cases, estimation of label correlation becomes even more difficult, as the observed label distribution is different from the true one. As a result, the aforementioned methods (based on hierarchical clustering and co-occurrence, for example) will produce biased estimates of label correlations.\nIn this paper, we propose a new approach called \u201cMulti-Label Learning with GLObal and loCAL Correlation\u201d (GLOCAL), which simultaneously recovers the missing labels, trains the linear classifiers and exploits both global and local label correlations. It learns a latent label representation. Classifier outputs are encouraged to be similar on highly positively correlated labels, and dissimilar on highly negatively correlated labels. We do not assume the presence of external knowledge sources specifying the label correlations. Instead, these correlations are learned simultaneously with the latent label representations and instance-label mapping.\nThe rest of the paper is organized as follows. In Section 2, related works of multi-label learning with label correlations are introduced. In Section 3, the problem formulation and the GLOCAL approach are proposed. Experimental results are presented in Section 4. Finally, Section 5 concludes the work.\nNotations For a matrix A, A> denotes its transpose, tr(A) is its trace, \u2016A\u2016F is its Frobenius norm, and diag(A) returns a vector containing the diagonal elements of A. For two matrices A and B, A\u25e6B denotes the Hadamard (element-wise) product. For a vector c, \u2016c\u20162 is its `2-norm, and Diag(c) returns a diagonal matrix with c on the diagonal."}, {"heading": "2. Related Work", "text": "Multi-label learning has been widely studied in recent years. Based on the degree of label correlations used, it can be divided into three categories [24]: (i) first-order; (ii) second-order; and (iii) high-order. For the first-order strategy, label correlations are not considered, and the multi-label problem is transformed into multiple independent binary classification problems. For example, BR [3] trains a classifier for each label independently. For the second-order strategy, pairwise label relations are considered. For example, CLR [7] transforms the multi-label learning problem into the pairwise label ranking problem. For the high-order strategy, all other labels\u2019 influences imposed on each label are taken into account. For example, CC [15] transforms the multi-label learning problem into a chain of binary classification problems, with the ground-truth labels encoded into the features.\nMost previous studies focus on global label correlations. However, MLLOC [9] demonstrates that sometimes label correlations may only be shared by a local data subset. Specifically, it enhances\nthe feature representation of each instance by embedding a code into the feature space, which encodes the influence of labels of an instance to the local label correlations. This has some limitations. First, when the dimensionality of the feature space is large, the code is less discriminative and will be dominated by the original features. Second, MLLOC considers only the local label correlations, but not the global ones. Third, MLLOC cannot learn with missing labels.\nIn some real-world applications, labels are partially observed, and multi-label learning with missing labels has attracted much attention. MAXIDE [21] is based on fast low-rank matrix completion, and has strong theoretical guarantees. However, it only works in the transductive setting. Moreover, a label correlation matrix has to be specified manually. LEML [22] also relies on a low-rank structure, and works in an inductive setting. However, it only implicitly uses global label correlations. ML-LRC [20] adopts a low-rank structure to capture global label correlations, and addresses the missing labels by introducing a supplementary label matrix. However, only global label correlations are taken into account. Obviously, it would be more desirable to learn both global and local label correlations simultaneously.\nManifold regularization [1] exploits instance similarity by forcing the predicted values on similar instances to be similar. A similar idea can be adapted to the label manifold, and so predicted values for correlated labels should be similar. However, the Laplacian matrix is based on some label similarity or correlation matrix, which can be hard to specify as discussed in Section 1."}, {"heading": "3. The Proposed Approach", "text": "In multi-label learning, an instance can be associated with multiple class labels. Let C = {c1, . . . , cl} be the class label set of l labels. We denote the feature vector of an instance by x \u2208 X \u2286 Rd, and denote the ground-truth label vector by y\u0303 \u2208 Y \u2286 {\u22121, 1}l, where [y\u0303]j = 1 if x is with class label cj , and \u22121 otherwise. As mentioned in Section 1, instances in the training data may be partially labeled, i.e., some labels may be missing. We adopt the general setting that both positive and negative labels can be missing [8, 21, 22]. The observed label vector is denoted y, where [y]j = 0 if class label cj is not labeled (i.e. it is missing), and [y]j = [y\u0303]j otherwise. Given the training data D = {(xi,yi)}ni=1, our goal is to learn a mapping function \u03a8 : X \u2192 Y.\nIn this paper, we propose the GLOCAL algorithm, which learns and exploits both global and local label correlations via label manifolds. To recover the missing labels, learning of the latent label representation and classifier training are performed simultaneously."}, {"heading": "3.1. Basic Model", "text": "Let Y\u0303 = [y\u03031, . . . , y\u0303n] \u2208 {\u22121, 1}l\u00d7n be the ground-truth label matrix, where each y\u0303i is the label vector for instance i. As discussed in Section 1, Y\u0303 is low-rank. Let its rank be k < l. Thus, Y\u0303 can be written as the low-rank decomposition UV , where U \u2208 Rl\u00d7k and V \u2208 Rk\u00d7n. Intuitively, V represents the latent labels that are more compact and more semantically abstract than the original labels, while matrix U projects the original labels to the latent label space.\nIn general, the labels are only partially observed. Let the observed label matrix be Y = [y1, . . . ,yn] \u2208 {\u22121, 0, 1}l\u00d7n, and \u2126 be the set containing indices of the observed labels in Y (i.e., indices of the nonzero elements in Y ). We focus on minimizing the reconstruction error on the observed labels, i.e., \u2016\u03a0\u2126(Y \u2212UV )\u20162F , where [\u03a0\u2126(A)]ij = Aij if (i, j) \u2208 \u2126, and 0 otherwise. Moreover, we use a linear mapping W \u2208 Rd\u00d7k to map instances to the latent labels. This W is learned by minimizing \u2016V \u2212W>X\u20162F , where X = [x1, . . . ,xn] \u2208 Rd\u00d7n is the instance matrix. Combining these two, we obtain the following optimization problem:\nmin U,V,W\n\u2016\u03a0\u2126(Y \u2212UV )\u20162F + \u03bb\u2016V \u2212W>X\u20162F + \u03bb2R(U,V,W ), (1)\nwhere R(U,V,W ) is a regularizer and \u03bb, \u03bb2 are tradeoff parameters. While the square loss has been used in Eqn (1), it can be replaced by any differentiable loss function. The prediction on x is sign(f(x)), where f(x) = UW>x. Let f = [f1, \u00b7 \u00b7 \u00b7 , fl]>, thus fj(x) denotes the predictive value on j-th label for x. We concatenate all f(x), \u2200x \u2208 X, denoted by F0, thus F0 = [f(x1), \u00b7 \u00b7 \u00b7 ,f(xn)] = UW>X."}, {"heading": "3.2. Global and Local Manifold Regularizers", "text": "Exploiting label correlations is an essential ingredient in multi-label learning. Here, we use label correlations to regularize the model. Intuitively, the more positively correlated two labels are, the closer are the corresponding classifier outputs, and vice versa. Let S0 = [Sij ] \u2208 Rl\u00d7l be the\nglobal label correlation matrix. The manifold regularizer \u2211\ni,j Sij\u2016fi,:\u2212fj,:\u201622 should have a small\nvalue [12]. Here, fi,:, the ith row of F0, is the vector of classifier outputs for the ith label on the n samples. Let D0 be the diagonal matrix with diagonal S01, where 1 is the vector of ones. The manifold regularizer can be equivalently written as tr(F>0 L0F0) [11], where L0 = D0\u2212S0 is the Laplacian matrix of S0.\nAs discussed in Section 1, label correlations may vary from one local region to another. Assume that the data X is partitioned into g groups {X1, . . . ,Xg}, where Xm \u2208 Rd\u00d7nm has size nm. This partitioning can be obtained by domain knowledge (e.g., gene pathways [16] and networks [4] in bioinformatics applications) or clustering. Let Ym be the label submatrix in Y corresponding to Xm, and Sm \u2208 Rl\u00d7l be the local label correlation matrix of group m. Similar to global label correlation, to encourage the classifier outputs to be similar on the positively correlated labels and dissimilar on the negatively correlated ones, we minimize tr(F>mLmFm), where Lm is the Laplacian matrix of Sm and Fm = UW >Xm is the classifier output matrix for group m.\nCombining global and local label correlations with Eqn. (1), we have the following optimization problem:\nmin U,V,W\n\u2016\u03a0\u2126(Y\u2212UV )\u20162F+\u03bb\u2016V\u2212W>X\u20162F +\u03bb2R(U ,V ,W )+\u03bb3tr(F>0 L0F0)+ g\u2211\nm=1\n\u03bb4tr(F > mLmFm),\n(2)\nwhere \u03bb, \u03bb2, \u03bb3, \u03bb4 are tradeoff parameters.\nIntuitively, a large local group contributes more to the global label correlations. In particular, the following Lemma shows that when the cosine similarity is used to compute Sij , we have\nS0 = \u2211g m=1 nm n Sm.\nLemma 1 Let [S0]ij = yi,:y\n> j,:\n\u2016yi,:\u2016\u2016yj,:\u2016 and [Sm]ij = ym,i,:y\n> m,j,:\n\u2016ym,i,:\u2016\u2016ym,j,:\u2016 , where yi,: is the ith row of Y , and ym,i,: is the ith row of Ym. Then, S0 = \u2211g m=1 nm n Sm.\nIn general, when the global label correlation matrix is a linear combination of the local label correlation matrices, the following Proposition shows that the global label Laplacian matrix is also a linear combination of the local label Laplacian matrices with the same combination coefficients.\nProposition 1 If S0 = \u2211g m=1 \u03b2mSm, then L0 = \u2211g m=1 \u03b2mLm.\nUsing Lemma 1 and Proposition 1, Eqn. (2) can then be rewritten as follows:\nmin U,V,W\n\u2016\u03a0\u2126(Y \u2212UV )\u20162F +\u03bb\u2016V \u2212W>X\u20162F +\u03bb2R(U,V,W )\n+ g\u2211 m=1 ( \u03bb3nm n tr(F>0 LmF0) + \u03bb4tr(F > mLmFm) ) . (3)\nThe success of label manifold regularization hinges on a good correlation matrix (or equivalently, a good Laplacian matrix). In multi-label learning, one rudimentary approach is to compute the correlation coefficient between two labels by cosine distance [19]. However, this can be noisy since some labels may only have very few positive instances in the training data. When labels can be missing, this computation may even become misleading, since the label distribution of observed labels may be much different from that of the ground-truth label distribution due to the missing labels.\nIn this paper, instead of specifying any correlation metric or label correlation matrix, we learn the Laplacian matrices directly. Note that the Laplacian matrices are symmetric positive definite. Thus, for m \u2208 {1, . . . , g}, we decompose Lm as ZmZ>m, where Zm\u2208Rl\u00d7k. For simplicity, k is set to the dimensionality of the latent representation V . As a result, learning the Laplacian matrices is transformed to learning Z \u2261 {Z1, . . . ,Zg}. Note that optimization w.r.t. Zm may lead to the trivial solution Zm = 0. To avoid this problem, we add the constraint that the diagonal entries in ZmZ > m are 1, for m \u2208 {1, \u00b7 \u00b7 \u00b7 , g}. This constraint also enables us to obtain a normalized Laplacian matrix [5] of Lm.\nLet J = [Jij ] be the indicator matrix with Jij = 1 if (i, j) \u2208 \u2126, and 0 otherwise. \u03a0\u2126(Y \u2212UV ) can be rewritten as the Hadamard product J \u25e6 (Y \u2212 UV ). Combining the decomposition of Laplacian matrices and the diagonal constraints of Zm, we obtain the optimization problem as:\nmin U,V,W,Z\n\u2016J \u25e6(Y \u2212UV )\u20162F +\u03bb\u2016V \u2212W>X\u20162F +\u03bb2R(U,V,W )\n+ g\u2211 m=1 (\u03bb3nm n tr ( F>0 ZmZ > mF0 ) +\u03bb4tr(F > mZmZ > mFm) ) s.t. diag(ZmZ > m) = 1,m \u2208 {1, 2, . . . , g}. (4)\nMoreover, we will use R(U,V,W ) = \u2016U\u20162F + \u2016V \u20162F + \u2016W \u20162F .\nAlgorithm 1 GLOCAL. Input: data matrix X, label matrix Y , observation indicator matrix J , and the group partition Output: U,W,Z.\n1: initialize U ,V ,W ,Z; 2: repeat 3: for m = 1, . . . , g 4: update Zm by solving (5);//Fix V ,U ,W , update Zm 5: end for 6: update V by solving (6); //Fix U ,W ,Z, update V 7: update U by solving (7); //Fix V ,W ,Z, update U 8: update W by solving (8); //Fix U ,V ,Z, update W 9: until convergence or maximum number of iterations;\n10: output U ,W , and Z \u2261 {Z1, . . . ,Zg}."}, {"heading": "3.3. Learning by Alternating Minimization", "text": "Problem (4) can be solved by alternating minimization (Algorithm 1). In each iteration, we update one of the variables in {Z,U ,V ,W } with gradient descent, and leave the others fixed. Specifically, the MANOPT toolbox [2] is utilized to implement gradient descent with line search on the Euclidean space for the update of U ,V ,W , and on the manifolds for the update of Z."}, {"heading": "3.3.1. Updating Zm", "text": "With U ,V ,W fixed, problem (4) reduces to\nmin Zm \u03bb3nm n\ntr ( F>0 ZmZ > mF0 ) + \u03bb4tr ( F>mZmZ > mFm ) s.t. diag(ZmZ > m) = 1, (5)\nfor each m \u2208 {1, . . . , g}. Due to the constraint diag(ZmZ>m) = 1, it has no closed-form solution, and we will solve it with projected gradient descent. The gradient of the objective w.r.t. Zm is\n\u2207Zm = \u03bb3nm n UW>XX>WU>Zm+\u03bb4UW >XmX > mWU >Zm.\nTo satisfy the constraint diag(ZmZ > m) = 1, we project each row of Zm onto the unit norm ball after each update:\nzm,j \u2190 zm,j/\u2016zm,j\u2016,\nwhere zm,j is the jth row of Zm."}, {"heading": "3.3.2. Updating V", "text": "With Zm\u2019s and U ,W fixed, problem (4) reduces to\nmin V \u2016J \u25e6 (Y \u2212UV )\u20162F + \u03bb\u2016V \u2212W>X\u20162F + \u03bb2\u2016V \u20162F . (6)\nNotice that each column of V is independent to each other, and thus V can be solved columnby-column. Let ji and vi be ith column of J and V , respectively. The optimization problem for vi can be written as:\nmin vi \u2016Diag(ji)yi \u2212Diag(ji)Uvi\u20162 + \u03bb\u2016vi \u2212W>xi\u20162 + \u03bb2\u2016vi\u20162.\nSetting the gradient w.r.t. vi to 0, we obtain the following closed-form solution of vi:\nvi = ( U>Diag(ji)U + (\u03bb+ \u03bb2)I )\u22121( \u03bbW>xi + U >Diag(ji)yi ) .\nThis involves computing a matrix inverse for each i. If this is expensive, we can use gradient descent instead. The gradient of the objective in (6) w.r.t. V is\n\u2207V = U> (J \u25e6 (UV \u2212 Y )) + \u03bb(V \u2212W>X) + \u03bb2V ."}, {"heading": "3.3.3. Updating U", "text": "With Zm\u2019s and V ,W fixed, problem (4) reduces to\nmin U \u2016J \u25e6 (Y \u2212UV )\u20162F + \u03bb2\u2016U\u20162F + g\u2211 m=1 (\u03bb3nm n tr(F>0 ZmZ > mF0)+\u03bb4tr(F > mZmZ > mFm) ) . (7)\nAgain, we use gradient descent, and the gradient w.r.t. U is:\n\u2207U =(J \u25e6 (UV \u2212 Y ))V > + \u03bb2U+ g\u2211\nm=1\nZiZ > i U (\u03bb3nm n W>XmX > mW +\u03bb4W >XX>W ) ."}, {"heading": "3.3.4. Updating W", "text": "With Zm\u2019s and U ,V fixed, problem (4) reduces to\nmin W\n\u03bb\u2016V \u2212W>X\u20162F + \u03bb2\u2016W \u20162F + g\u2211\nm=1\n( \u03bb3nm n tr(F>0 ZmZ > mF0)+\u03bb4tr(F > mZmZ > mFm) ) . (8)\nThe gradient w.r.t. W is:\n\u2207W= \u03bbX ( X>W \u2212 V > ) + \u03bb2W + g\u2211 m=1 (\u03bb3nm n XX> + \u03bb4XmX > m ) WU>ZmZ > mU ."}, {"heading": "4. Experiments", "text": "In this section, extensive experiments are performed on text and image datasets. Performance on both the full-label and missing-label cases are discussed."}, {"heading": "4.1. Setup", "text": ""}, {"heading": "4.1.1. Data sets", "text": "On text, eleven Yahoo datasets1 (Arts, Business, Computers, Education, Entertainment, Health, Recreation, Reference, Science, Social and Society) and the Enron dataset2 are used. On images, the Corel5k3 and Image3 datasets are used. In the sequel, each dataset is denoted by its first three letters.4 Detailed information of the datasets are shown in Table 1. For each dataset, we randomly select 60% of the instances for training, and the rest for testing."}, {"heading": "4.1.2. Baselines", "text": "In the GLOCAL algorithm, we use the kmeans clustering algorithm to partition the data into local groups. The solution of Eqn. (1) is used to warm-start U ,V and W . The Zm\u2019s are randomly initialized. GLOCAL is compared with the following state-of-the-art multi-label learning algorithms:\n1http://www.kecl.ntt.co.jp/as/members/ueda/yahoo.tar 2http://mulan.sourceforge.net/datasets-mlc.html 3http://cse.seu.edu.cn/people/zhangml/files/Image.rar 4\u201cSociety\u201d is denoted \u201cSoci\u201d, so as to distinguish it from \u201cSocial\u201d.\n1. BR [3], which trains a binary linear SVM (using the LIBLINEAR package [6]) for each label\nindependently;\n2. MLLOC [9], which exploits local label correlations by encoding them into the instance\u2019s feature\nrepresentation;\n3. LEML [22], which learns a linear instance-to-label mapping with low-rank structure, and\nimplicitly takes advantage of global label correlation;\n4. ML-LRC [20], which learns and exploits low-rank global label correlations for multi-label\nclassification with missing labels.\nNote that BR does not take label correlation into account. MLLOC considers only local label correlations; LEML implicitly uses global label correlations, whereas ML-LRC models global label correlation directly. On the ability to handle missing labels, BR and MLLOC can only learn with full labels.\nFor simplicity, we set \u03bb = 1 in GLOCAL. The other parameters, as well as those of the baseline methods, are selected via 5-fold cross-validation on the training set. All the algorithms are implemented in Matlab (with some C++ code for LEML)."}, {"heading": "4.1.3. Performance Evaluation", "text": "Let p be the number of test instances, C+i ,C \u2212 i be the sets of positive and negative labels associated with the ith instance; and Z+j ,Z \u2212 j be the sets of positive and negative instances belonging to the jth label. Given input x, let rankf (x, y) be the rank of label y in the predicted label ranking (sorted in descending order). For performance evaluation, we use the following popular metrics in multi-label learning [24]:\n1. Ranking loss (Rkl): This is the fraction that a negative label is ranked higher than a positive\nlabel. For instance i, define Qi = {(j\u2032, j\u2032\u2032) | fj\u2032(xi) \u2264 fj\u2032\u2032(xi), (j\u2032, j\u2032\u2032) \u2208 C+i \u00d7C \u2212 i }. Then,\nRkl = 1\np p\u2211 i=1 |Qi| |C+i ||C \u2212 i | .\n2. Average AUC (Auc): This is the fraction that a positive instance is ranked higher than a\nnegative instance, averaged over all labels. Specifically, for label j, define Q\u0303j = {(i\u2032, i\u2032\u2032) | fj(xi\u2032) \u2265 fj(xi\u2032\u2032), (xi\u2032 ,xi\u2032\u2032) \u2208 Z+j \u00d7Z \u2212 j }. Then,\nAuc = 1\nl l\u2211 j=1 |Q\u0303j | |Z+j ||Z \u2212 j | .\n3. Coverage (Cvg): This counts how many steps are needed to move down the predicted label\nranking so as to cover all the positive labels of the instances.\nCvg = 1\np p\u2211 i=1 max{rankf (xi, j) | j \u2208 C+i } \u2212 1.\n4. Average precision (Ap): This is the average fraction of positive labels ranked higher than a\nparticular positive label. For instance i, define Q\u0302i,c = {j | rankf (xi, j) \u2264 rankf (xi, c), j \u2208 C+i }. Then,\nAp = 1\np p\u2211 i=1 1 |C+i | \u2211 c\u2208C+i |Q\u0302i,c| rankf (xi, c) .\nFor Auc and Ap, the higher the better; whereas for Rkl and Cvg, the lower the better. To reduce statistical variability, results are averaged over 10 independent repetitions."}, {"heading": "4.2. Learning with Full Labels", "text": "In this experiment, all elements in the training label matrix are observed. Performance on the test data is shown in Table 2. As expected, BR is the worst , since it treats each label independently without considering label correlations. MLLOC only considers local label correlations and LEML only makes use of the low-rank structure. Though ML-LRC takes advantage of both the low-rank structure and label correlations, only global label correlations are considered. As a result, GLOCAL is the best overall, as it models both global and local label correlations.\nTo show the example correlations learned by GLOCAL, we use two local groups extracted from the Image dataset. Figure 1 shows that local label correlation does vary from group to group, and is different from global correlation. For group 1, \u201csunset\u201d is highly correlated with \u201cdesert\u201d and \u201csea\u201d (Figure 1(c)). This can also be seen from the images in Figure 1(a). Moreover,\n\u201ctrees\u201d sometimes co-occurs with \u201cdeserts\u201d (first and last images in Figure 1(a)). However, in group 2 (Figure 1(d)), \u201cmountain\u201d and \u201csea\u201d often occur together and \u201ctrees\u201d occurs less often with \u201cdesert\u201d (Figure 1(b)). Figure 1(e) shows the learned global label correlation: \u201csea\u201d and \u201csunset\u201d, \u201cmountain\u201d and \u201ctrees\u201d are positively correlated, whereas \u201cdesert\u201d and \u201csea\u201d, \u201cdesert\u201d and \u201ctrees\u201d are negatively correlated. All these correlations are consistent with intuition.\nTo further validate the effectiveness of global and local label correlations, we study two degenerate versions of GLOCAL: (i) GLObal, which uses only global label correlations; and (ii) loCAL, which\nuses only local label correlations. Note that the local groups obtained by clustering are not of equal sizes. For some datasets, the largest cluster contains more than 40% of instances, while some small ones contain fewer than 5% each. Global correlation is then dominated by the local correlation matrix of the largest cluster (Proposition 1), making the performance difference on the whole test set obscure. Hence, we focus on the performance of the small clusters. As can be seen from Table 3, using only global or local correlation may be good enough on some data sets (such as Health). On the other hand, considering both types of correlation as in GLOCAL achieves comparable or even better performance."}, {"heading": "4.3. Learning with Missing Labels", "text": "In this experiment, we randomly sample \u03c1% of the elements in the label matrix as observed, and the rest as missing. Note that BR and MLLOC can only handle datasets with full labels. Hence,\nwe first use MAXIDE [21], a matrix completion algorithm for transductive multi-label learning, to fill in the missing labels before they can be applied. We use MBR for MAXIDE+BR, and MMLLOC for MAXIDE+MLLOC.\nTables 4 and 5 show the results on the training and test data, respectively.5\nAs can be seen, performance increases with more observed entries in general. Overall, GLOCAL performs best at different \u03c1\u2019s, as it simultaneously considers both global and local label correlations with label manifold regularization. In contrast, MBR and MMLLOC handle label recovery and learning separately. Moreover, MMLLOC takes only local label correlation, and MBR does not consider label correlations. As a result, they perform much worse than GLOCAL. Though LEML and ML-LRC perform learning with missing label recovery together, they consider only global correlation, and are thus often worse than GLOCAL."}, {"heading": "4.4. Convergence", "text": "In this section, we empirically study the convergence of GLOCAL. Figure 2 shows the objective value w.r.t. the number of iterations for the full-label case. Because of the lack of space, results are only shown on the Arts, Business, Enron and Image datasets. As can be seen, the objective converges quickly in a few iterations. A similar phenomenon can be observed on the other datasets.\nTable 6 shows the timing results on learning with missing labels (with \u03c1 = 70). GLOCAL and LEML train a classifier for all the labels jointly, and also can take advantage of the low-rank structure of either the model or label matrix during training. Thus, they are the fastest. However, GLOCAL has\n5To fit the tables on one page, we do not report the standard deviation. MBR, which performs worst, is also not\nshown.\nto be warm-started by Eqn. (1), and requires an additional clustering step to obtain local groups of the instances. Hence, it is slower than LEML. However, as have been observed in previous sections, GLOCAL outperforms LEML in terms of label recovery. ML-LRC uses a low-rank label correlation matrix. However, it does not reduce the size of the label matrix or model involved in each iteration, and so is slower than GLOCAL. MBR and MMLLOC require training a classifier for each label, and also an additional step to recover the missing labels. Thus, they are often the slowest, especially when the number of class labels is large. Similar results can be observed with \u03c1 = 30, which are not reported here."}, {"heading": "4.5. Sensitivity to Parameters", "text": "In this experiment, we study the influence of parameters, including the number of clusters g, regularization parameters \u03bb3 and \u03bb4 (corresponding to the manifold regularizer for global and local label correlations, respectively), regularization parameter \u03bb2 for the Frobenius norm regularizer, and dimensionality k of the latent representation. We vary one parameter, while keeping the others fixed at their best setting."}, {"heading": "4.5.1. Varying the Number of Clusters g", "text": "Figure 3 shows the influence on the Enron dataset. When there is only one cluster, no local label correlation is considered. With more clusters, performance improves as more local label\ncorrelations are taken into account. When too many clusters are used, very few instances are placed in each cluster, and the local label correlations cannot be reliably estimated. Thus, the performance starts to deteriorate."}, {"heading": "4.5.2. Influence of Label Manifold Regularizers (\u03bb3 and \u03bb4)", "text": "A larger \u03bb3 means higher importance of global label correlation, whereas a larger \u03bb4 means higher importance of local label correlation. Figures 4 and 5 show their effects on the Enron dataset. When \u03bb3 = 0, only local label correlations are considered, and the performance is poor. With increasing \u03bb3, performance improves. However, when \u03bb3 is very large, performance deteriorates as the global label correlations dominate. A similar phenomenon can be observed for \u03bb4."}, {"heading": "4.5.3. Varying the Latent Representation Dimensionality k", "text": "Figure 6 shows the effect of varying k on the Enron dataset. As can be seen, when k is too small, the latent representation cannot capture enough information. With increasing k, performance improves. When k is too large, the low-rank structure is not fully utilized, and performance starts to get worse."}, {"heading": "4.5.4. Influence of \u03bb2", "text": "Figure 7 shows the effect of varying \u03bb2 on the Enron dataset. As can be seen, GLOCAL is not sensitive to this parameter."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed a new multi-label correlation learning approach GLOCAL, which simultaneously recovers the missing labels, trains the classifier and exploits both global and local label correlations, through learning a latent label representation and optimizing the label manifolds. Compared with the previous work, it is the first to exploit both global and local label correlations, which directly learns the Laplacian matrix without requiring any other prior knowledge\non label correlations. As a result, the classifier outputs and label correlations best match each other, both globally and locally. Moreover, GLOCAL provides a unified solution for both full-label and missing-label multi-label learning. Experimental results show that our approach outperforms the state-of-the-art multi-label learning approaches on learning with both full labels and missing labels. In our work, we handle the case that label correlations are symmetric. In many situations, correlations can be asymmetric. For example, \u201cmountain\u201d are highly correlated to \u201ctree\u201d, since it is very common that a mountain has trees in it. However, \u201ctree\u201d may be less correlated to \u201cmountain\u201d, because trees can be found not only in mountains, but often in the streets, parks, etc. So it is desirable to study the asymmetric label correlations in our future work."}, {"heading": "Acknowledgment", "text": "This research was supported by NSFC (61333014), 111 Project (B14020), and the Collaborative Innovation Center of Novel Software Technology and Industrialization."}], "references": [{"title": "Manifold regularization: a geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Manopt, a Matlab toolbox for optimization on manifolds", "author": ["N. Boumal", "B. Mishra", "P.-A. Absil", "R. Sepulchre"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning multi-label scene classification", "author": ["M. Boutell", "J. Luo", "X. Shen", "C. Brown"], "venue": "Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Network-based classification of breast cancer metastasis", "author": ["H.-Y. Chuang", "E. Lee", "Y.-T. Liu", "D. Lee", "T. Ideker"], "venue": "Molecular Systems Biology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Spectral graph theory, volume 92", "author": ["F. Chung"], "venue": "American Mathematical Soc.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Multilabel classification via calibrated label ranking", "author": ["J. F\u00fcrnkranz", "E. H\u00fcllermeier", "E. Men\u0107\u0131a", "K. Brinker"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Transduction with matrix completion: Three birds with one stone", "author": ["A. Goldberg", "B. Recht", "J. Xu", "R. Nowak", "X. Zhu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Multi-label learning by exploiting label correlations locally", "author": ["S.-J. Huang", "Z.-H. Zhou"], "venue": "In Proceedings of the 26th AAAI Conference on Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Extracting shared subspace for multi-label classification", "author": ["S. Ji", "L. Tang", "S. Yu", "J. Ye"], "venue": "In Proceedings of the 14th International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Non-negative laplacian embedding", "author": ["D. Luo", "C. Ding", "H. Huang", "T. Li"], "venue": "In Proceedings of the 9th IEEE International Conference on Data Mining,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Laplacian Support Vector Machines Trained in the Primal", "author": ["S. Melacci", "M. Belkin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Submodular multi-label learning", "author": ["J. Petterson", "T. Caetano"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Automatically learning document taxonomies for hierarchical classification", "author": ["K. Punera", "S. Rajan", "J. Ghosh"], "venue": "Proceedings of the 14th International Conference on World Wide Web,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Classifier chains for multi-label classification", "author": ["J. Read", "B. Pfahringer", "G. Holmes", "E. Frank"], "venue": "Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles", "author": ["A. Subramanian", "P. Tamayo", "V. Mootha", "S.M.B. Ebert", "M. Gillette", "A. Paulovich", "S. Pomeroy", "T. Golub", "E. Lander"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Semantic annotation and retrieval of music and sound effects", "author": ["D. Turnbull", "L. Barrington", "D. Torres", "C. Lanckriet"], "venue": "IEEE Transactions on Audio, Speech and Language Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Parametric mixture models for multi-labeled text", "author": ["N. Ueda", "K. Saito"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Image annotation using multi-label correlated green\u2019s function", "author": ["H. Wang", "H. Huang", "C. Ding"], "venue": "In Proceedings of the 12th International Conference on Computer", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Learning low-rank label correlations for multi-label classification with missing labels", "author": ["L. Xu", "Z. Wang", "Z. Shen", "Y. Wang", "E. Chen"], "venue": "In Proceedings of the 14th IEEE International Conference on Data Mining,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Speedup matrix completion with side information: Application to multi-label learning", "author": ["M. Xu", "R. Jin", "Z.-H. Zhou"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Large-scale multi-label learning with missing labels", "author": ["H.-F. Yu", "P. Jain", "P. Kar", "I. Dhillon"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Multi-label learning by exploiting label dependency", "author": ["M.-L. Zhang", "K. Zhang"], "venue": "In Proceedings of the 16th International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A review on multi-label learning algorithms", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "For example, a scene image can be annotated with several tags [3], a document may", "startOffset": 62, "endOffset": 65}, {"referenceID": 17, "context": "belong to multiple topics [18], and a piece of music may be associated with different genres [17].", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "belong to multiple topics [18], and a piece of music may be associated with different genres [17].", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "Thus, multi-label learning has attracted a lot of attention in recent years [24].", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Current studies on multi-label learning try to incorporate label correlations of different orders [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 6, "context": "However, existing approaches mostly focus on global label correlations shared by all instances [7, 10, 15].", "startOffset": 95, "endOffset": 106}, {"referenceID": 9, "context": "However, existing approaches mostly focus on global label correlations shared by all instances [7, 10, 15].", "startOffset": 95, "endOffset": 106}, {"referenceID": 14, "context": "However, existing approaches mostly focus on global label correlations shared by all instances [7, 10, 15].", "startOffset": 95, "endOffset": 106}, {"referenceID": 8, "context": "On the other hand, certain label correlations are only shared by a local data subset [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 13, "context": "Some approaches learn the label hierarchies by hierarchical clustering [14] or Bayesian network structure learning [23].", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "Some approaches learn the label hierarchies by hierarchical clustering [14] or Bayesian network structure learning [23].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "Others estimate label correlations by the co-occurrence of labels in training data [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "[21] and Yu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] considered using the low-rank structure on the instance-label mapping.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "A more direct approach to model the label dependency approximates the label matrix as a product of two low-rank matrices [8].", "startOffset": 121, "endOffset": 124}, {"referenceID": 23, "context": "Based on the degree of label correlations used, it can be divided into three categories [24]: (i) first-order; (ii) second-order; and (iii) high-order.", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "For example, BR [3] trains a classifier for each label independently.", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "For example, CLR [7] transforms the multi-label learning problem into the pairwise label ranking problem.", "startOffset": 17, "endOffset": 20}, {"referenceID": 14, "context": "For example, CC [15] transforms the multi-label learning problem into a chain of binary classification problems, with the ground-truth labels encoded into the features.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "However, MLLOC [9] demonstrates that sometimes label correlations may only be shared by a local data subset.", "startOffset": 15, "endOffset": 18}, {"referenceID": 20, "context": "MAXIDE [21] is based on fast low-rank matrix completion, and has strong theoretical guarantees.", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "LEML [22] also relies on a low-rank structure, and works in an inductive setting.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "ML-LRC [20] adopts a low-rank structure to capture global label correlations, and addresses the missing labels by introducing a supplementary label matrix.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "Manifold regularization [1] exploits instance similarity by forcing the predicted values on similar instances to be similar.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "We adopt the general setting that both positive and negative labels can be missing [8, 21, 22].", "startOffset": 83, "endOffset": 94}, {"referenceID": 20, "context": "We adopt the general setting that both positive and negative labels can be missing [8, 21, 22].", "startOffset": 83, "endOffset": 94}, {"referenceID": 21, "context": "We adopt the general setting that both positive and negative labels can be missing [8, 21, 22].", "startOffset": 83, "endOffset": 94}, {"referenceID": 11, "context": "The manifold regularizer \u2211 i,j Sij\u2016fi,:\u2212fj,:\u20162 should have a small value [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "The manifold regularizer can be equivalently written as tr(F> 0 L0F0) [11], where L0 = D0\u2212S0 is the Laplacian matrix of S0.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": ", gene pathways [16] and networks [4] in bioinformatics applications) or clustering.", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": ", gene pathways [16] and networks [4] in bioinformatics applications) or clustering.", "startOffset": 34, "endOffset": 37}, {"referenceID": 18, "context": "In multi-label learning, one rudimentary approach is to compute the correlation coefficient between two labels by cosine distance [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 4, "context": "This constraint also enables us to obtain a normalized Laplacian matrix [5] of Lm.", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "Specifically, the MANOPT toolbox [2] is utilized to implement gradient descent with line search on the Euclidean space for the update of U ,V ,W , and on the manifolds for the update of Z.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "BR [3], which trains a binary linear SVM (using the LIBLINEAR package [6]) for each label independently; 2.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "BR [3], which trains a binary linear SVM (using the LIBLINEAR package [6]) for each label independently; 2.", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "MLLOC [9], which exploits local label correlations by encoding them into the instance\u2019s feature representation; 3.", "startOffset": 6, "endOffset": 9}, {"referenceID": 21, "context": "LEML [22], which learns a linear instance-to-label mapping with low-rank structure, and implicitly takes advantage of global label correlation; 4.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "ML-LRC [20], which learns and exploits low-rank global label correlations for multi-label classification with missing labels.", "startOffset": 7, "endOffset": 11}, {"referenceID": 23, "context": "For performance evaluation, we use the following popular metrics in multi-label learning [24]:", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "we first use MAXIDE [21], a matrix completion algorithm for transductive multi-label learning, to fill in the missing labels before they can be applied.", "startOffset": 20, "endOffset": 24}], "year": 2017, "abstractText": "It is well-known that exploiting label correlations is important to multi-label learning. Existing approaches either assume that the label correlations are global and shared by all instances; or that the label correlations are local and shared only by a data subset. In fact, in the real-world applications, both cases may occur that some label correlations are globally applicable and some are shared only in a local group of instances. Moreover, it is also a usual case that only partial labels are observed, which makes the exploitation of the label correlations much more difficult. That is, it is hard to estimate the label correlations when many labels are absent. In this paper, we propose a new multi-label approach GLOCAL dealing with both the full-label and the missinglabel cases, exploiting global and local label correlations simultaneously, through learning a latent label representation and optimizing label manifolds. The extensive experimental studies validate the effectiveness of our approach on both full-label and missing-label data.", "creator": "TeX"}}}