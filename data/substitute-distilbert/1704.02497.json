{"id": "1704.02497", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Apr-2017", "title": "On the Linearity of Semantic Change: Investigating Meaning Variation via Dynamic Graph Models", "abstract": "terms represent two graph models of semantic change. the first is a time - series model that relates embedding vectors from one time period to embedding polynomials across previous one periods. in the second, ourselves use one graph for each word : nodes supporting this graph correspond to time points comparable edge weights to the similarity of the description's meaning across two key points. we apply our two models to corpora for three different periods. we find that logical change is linear in two senses. firstly, today's embedding vectors ( = meaning ) of words can be derived as specific combinations of scaling vectors of their neighbors in previous time periods. secondly, self - similarity of words decays linearly in time. we consider both findings as new laws / hypotheses of semantic change.", "histories": [["v1", "Sat, 8 Apr 2017 13:56:55 GMT  (40kb)", "http://arxiv.org/abs/1704.02497v1", "Published at ACL 2016, Berlin (short papers)"]], "COMMENTS": "Published at ACL 2016, Berlin (short papers)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["steffen eger", "alexander mehler"], "accepted": true, "id": "1704.02497"}, "pdf": {"name": "1704.02497.pdf", "metadata": {"source": "CRF", "title": "On the Linearity of Semantic Change: Investigating Meaning Variation via Dynamic Graph Models", "authors": ["Steffen Eger"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n02 49\n7v 1\n[ cs\n.C L\n] 8\nA pr\n2 01\n7\nWe consider two graph models of semantic change. The first is a time-series model that relates embedding vectors from one time period to embedding vectors of previous time periods. In the second, we construct one graph for each word: nodes in this graph correspond to time points and edge weights to the similarity of the word\u2019s meaning across two time points. We apply our two models to corpora across three different languages. We find that semantic change is linear in two senses. Firstly, today\u2019s embedding vectors (= meaning) of words can be derived as linear combinations of embedding vectors of their neighbors in previous time periods. Secondly, self-similarity of words decays linearly in time. We consider both findings as new laws/hypotheses of semantic change."}, {"heading": "1 Introduction", "text": "Meaning is not uniform, neither across space, nor across time. Across space, different languages tend to exhibit different polysemous associations for corresponding terms (Eger et al., 2015; Kulkarni et al., 2015b). Across time, several wellknown examples of meaning change in English have been documented. For example, the word gay\u2019s meaning has shifted, during the 1970s, from an adjectival meaning of cheerful at the beginning of the 20th century to its present meaning of homosexual (Kulkarni et al., 2015a). Similarly, technological progress has led to semantic broadening of terms such as transmission, mouse, or apple.\nIn this work, we consider two graph models of semantic change. Our first model is a dynamic model in that the underlying paradigm is a (time-)series of graphs. Each node in the series of graphs corresponds to one word, associated with which is a semantic embedding vector. We then ask how the embedding vectors in one time period (graph) can be predicted from the embedding vectors of neighbor words in previous time periods. In particular, we postulate that there is a linear functional relationship that couples a word\u2019s today\u2019s meaning with its neighbor\u2019s meanings in the past. When estimating the coefficients of this model, we find that the linear form appears indeed very plausible. This functional form then allows us to address further questions, such as negative relationships between words \u2014 which indicate semantic differentiation over time \u2014 as well as projections into the future. We call our second graph model time-indexed self-similarity graphs. In these graphs, each node corresponds to a time point and the link between two time points indicates the semantic similarity of a specific word across the two time points under consideration. The analysis of these graphs reveals that most words obey a law of linear semantic \u2018decay\u2019: semantic self-similarity decreases linearly over time.\nIn our work, we capture semantics by means of word embeddings derived from context-predicting neural network architectures, which have become the state-of-the-art in distributional semantics modeling (Baroni et al., 2014). Our approach and results are partly independent of this representation, however, in that we take a structuralist approach: we derive new, \u2018second-order embeddings\u2019 by modeling the meaning of words by\nmeans of their semantic similarity relations to all other words in the vocabulary (de Saussure, 1916; Rieger, 2003). Thus, future research may in principle substitute the deep-learning architectures for semantics considered here by any other method capable of producing semantic similarity values between lexical units.\nThis work is structured as follows. In \u00a72, we discuss related work. In \u00a73.1 and 3.2, respectively, we formally introduce the two graph models outlined. In \u00a74, we detail our experiments and in \u00a75, we conclude."}, {"heading": "2 Related work", "text": "Broadly speaking, one can distinguish two recent NLP approaches to meaning change analysis. On the one hand, coarse-grained trend analyses compare the semantics of a word in one time period with the meaning of the word in the preceding time period (Jatowt and Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more finegrained analyses typically sense-label word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Senselabeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the \u2018law of differentiation\u2019) (Xu and Kemp, 2015) and papers that intend to detect corresponding terms across time (words with similar meanings/roles in two time periods but potentially different lexical forms) (Zhang et al., 2015)."}, {"heading": "3 Graph models", "text": "Let V = {w1, . . . , w|V |} be the common vocabulary (intersection) of all words in all time periods t \u2208 T . Here, T is a set of time indices. Denote an embedding of a word wi at time period t as wi(t) \u2208 R d. Since embeddings wi(s),wi(t) for two different time periods s, t are generally not comparable, as they may lie in different coordinate systems, we consider the vectors w\u0303i(t) =\n( sim(wi(t),w1(t)), . . . , sim(wi(t),w|V |(t)) ) , (1)\neach of which lies in R|V | and where sim is a similarity function such as the cosine. We note that our structuralist definition of w\u0303i(t) is not unproblematic, since the vectors w1(t), . . . ,w|V |(t) tend to be different across t, by our very postulate, so that there is non-identity of these \u2018reference points\u2019 over time. However, as we may assume that the meanings of at least a few words are stable over time, we strongly expect the vectors w\u0303i(t) to be suitable for our task of analysis of meaning changes.1 For the remainder of this work, for convenience, we do not distinguish, in terms of notation, between wi(t) and w\u0303i(t)."}, {"heading": "3.1 A linear model of semantic change", "text": "We postulate, and subsequently test, the following model of meaning dynamics which describes meaning change over time for words wi:\nwi(t) =\np \u2211\nn=1\n\u2211\nwj\u2208V \u2229N(wi)\n\u03b1nwjwj(t\u2212 n) (2)\nwhere \u03b1nwj \u2208 R, for n = 1, . . . , p, are coefficients of meaning vectors wj(t\u2212 n) and p \u2265 1 is the order of the model. The set N(wi) \u2286 V denotes a set of \u2018neighbors\u2019 of word wi. 2 This model says that the meaning of a word wi at some time t is determined by reference to the meanings of its \u2018neighbors\u2019 in previous time periods, and that the underlying functional relationship is linear.\n1An alternative to our second-order embeddings is to project vectors from different time periods in a common space (Mikolov et al., 2013a; Faruqui and Dyer, 2014), which requires to find corresponding terms across time. Further, one could also consider a \u2018core\u2019 vocabulary of semantically stable words, e.g., in the spirit of Swadesh (1952), instead of using all vocabulary words as reference.\n2We also constrain the vectors wi(t), for all wi \u2208 V , to contain non-zero entries only for words in N(wi).\nWe remark that the model described by Eq. (2) is a time-series model, and, in particular, a vector-autoregressive (VAR) model with special structure. The model may also be seen in the socio-economic context of so-called \u201copinion dynamic models\u201d (Golub and Jackson, 2010; Acemoglu and Ozdaglar, 2011; Eger, 2016). There it is assumed that agents are situated in network structures and continuously update their opinions/beliefs/actions according to their ties with other agents. Model (2) substitutes multi-dimensional embedding vectors for one-dimensional opinions."}, {"heading": "3.2 Time-indexed self-similarity graphs", "text": "We track meaning change by considering a fully connected graph G(w) for each word w in V . The nodes of G(w) are the time indices T , and there is an undirected link between any two s, t \u2208 T whose weight is given by sim(w(s),w(t)). We call the graphs G(w) time-indexed self-similarity (TISS) graphs because they indicate the (semantic) similarity of a given word with itself across different time periods. Particular interest may lie in weak links in these graphs as they indicate low similarity between two different time periods, i.e., semantic change across time."}, {"heading": "4 Experiments", "text": "Data As corpus for English, we use the Corpus of Historical American (COHA).3 This covers texts from the time period 1810 to 2000. We extract two slices: the years 1900-2000 and 1810- 2000. For both slices, each time period t is one decade, e.g., T = {1810, 1820, 1830, . . .}.4 For each slice, we only keep words associated to the word classes nouns, adjectives, and verbs. For computational and estimation purposes, we also only consider words that occur at least 100 times in each time period. To induce word embeddings w \u2208 Rd for each word w \u2208 V , we use word2vec (Mikolov et al., 2013b) with default parametrizations. We do so for each time period t \u2208 T independently. We then use these embeddings to derive the new embeddings as in Eq. (1). Throughout, we use cosine similarity as\n3http://corpus.byu.edu/coha/. 4Each time period contains texts that were written in that\ndecade.\nsim measure. For German, we consider a proprietary dataset of the German newspaper SZ5 for which T = {1994, 1995, . . . , 2003}. We lemmatize and POS tag the data and likewise only consider nouns, verbs and adjectives, making the same frequency constraints as in English. Finally, we use the PL (Migne, 1855) as data set for Latin. Here, T = {300, 400, . . . , 1300}. We use the same preprocessing, frequency, and word class constraints as for English and German.\nThroughout, our datasets are well-balanced in terms of size. For example, the English COHA datasets contain about 24M-30M tokens for each decade from 1900 to 2000, where the decades 1990 and 2000 contain slighly more data than the earlier decades. The pre-1900 decades contain 18- 24M tokens, with only the decades 1810 and 1820 containing very little data (1M and 7M tokens, respectively). The corpora are also balanced by genre."}, {"heading": "4.1 TISS graphs", "text": "We start with investigating the TISS graphs. Let Dt0 represent how semantically similar a word is across two time periods, on average, when the distance between time periods is t0: Dt0 = 1 C \u2211 w\u2208V \u2211 |s\u2212t|=t0 sim(w(s),w(t)), where C = |V | \u00b7 |{(s, t) | |s\u2212 t| = t0}| is a normalizer. Figure 1 plots the values Dt0 for the time slice from 1810 to 2000, for the English data. We notice a clear trend: self-similarity of a word tends to (almost perfectly) linearly decrease with time distance. In fact, Table 1 below indicates that this trend holds across all our corpora, i.e., for different time scales and different languages: the linear \u2018decay\u2019 model fits theDt0 curves very well, with adjustedR 2 values substantially above 90% and consistently and significantly negative coefficients. We believe that this finding may be considered a new statistical law of semantic change.\nThe values Dt0 as a function of t0 are averages over all words. Thus, it might be possible that the average word\u2019s meaning decays linearly in time, while the semantic behavior, over time, of a large fraction of words follows different trends. To investigate this, we consider the distribution of Dt0(w) = 1 C\u2032 \u2211 |s\u2212t|=t0 sim(w(s),w(t)) over fixed words w. Here C \u2032 = |{(s, t) | |s\u2212 t| = t0}|.\n5http://www.sueddeutsche.de/\nWe consider the regression models\nDt0(w) = \u03b1 \u00b7 t0 + const.\nfor each word w independently and assess the distribution of coefficients \u03b1 as well as the goodnessof-fit values. Figure 2 shows \u2014 exemplarily for the English 1900-2000 COHA data \u2014 that the coefficients \u03b1 are negative for almost all words. In fact, the distribution is left-skewed with a mean of around \u22120.4%. Moreover, the linear model is always a good to very good fit of the data in that R2 values are centered around 85% and rarely fall below 75%. We find similar patterns for all other datasets considered here. This shows that not only the average word\u2019s meaning decays linearly, but almost all words\u2019 (whose frequency mass exceeds a particular threshold) semantics behaves this way.\nNext, we use our TISS graphs for the task of finding words that have undergone meaning change. To this end, we sort the graphs G(w) by the ratios RG(w) = maxlink minlink , where maxlink denotes maximal weight of a link in graph G(w) and minlink is the minimal weight of a link in\ngraph G(w). We note that weak links may indicate semantic change, but the stated ratio requires that \u2018weakness\u2019 is seen relative to the strongest semantic links in the TISS graphs. Table 2 presents selected words that have highest values RG(w). 6 We omit a fine-grained semantic change analysis,\n6The top ten words with the lowest valuesRG(w) are one, write, have, who, come, only, even, know, hat, fact.\nwhich could be conducted via the methods outlined in \u00a72, but notice a few cases. \u2018Terrific\u2019 has a large semantic discrepancy between the 1900s and the 1970s, when the word probably (had) changed from a negative to a more positive meaning. The largest discrepancy for \u2018web\u2019 is between the 1940s and the 2000s, when it probably came to be massively used in the context of the Internet. The high RG(w) value for w = \u2018axis\u2019 derives from comparing its use in the 1900s with its use in the 1940s, when it probably came to be used in the context of Nazi Germany and its allies. We notice that the presented method can account for gradual, accumulating change, which is not possible for models that compare two succeeding time points such as the model of Kulkarni et al. (2015a)."}, {"heading": "4.2 Meaning dynamics network models", "text": "Finally, we estimate meaning dynamics models as in Eq. (2), i.e., we estimate the coefficients \u03b1nwj from our data sources. We let the neighbors N(w) of a wordw as in Eq. (2) be the union (w.r.t. t) over sets Nt(w;n) denoting the n \u2265 1 semantically most similar words (estimated by cosine similarity on the original word2vec vectors) of word w in time period t \u2208 T .7 In Table 3, we indicate two measures: adjusted R2, which indicates the goodness-of-fit of a model, and prediction error. By prediction error, we measure the average Euclidean distance between the true semantic vector of a word in the final time period tN vs. the predicted semantic vector, via the linear model in Eq. (2), estimated on the data excluding the final period. The indicated prediction error is the average over all words. We note the following: R2 values are high (typically above 95%), indicating that the linear semantic change model we have suggested fits the data well. Moreover, R2 values slightly increase between order p = 1 and p = 2; however, for prediction error this trend is reversed.8 We also include a strong baseline that claims that word meanings do not change in the final period tN but are the same as in tN\u22121. We note that the order p = 1 model consistently improves\n7We exclude word w from Nt(w;n). We found that including w did not improve performance results.\n8We experimented with orders p \u2265 3, but found them to be inadequate. Typically, coefficients for lagged-3 variables are either zero or model predictions are way off, possibly indicating multi-collinearity.\nupon this baseline, by as much as 18%, depending upon parameter settings.\nNegative relationships Another very interesting aspect of the model in Eq. (2) is that it allows for detecting words wj whose embeddings have negative coefficients \u03b1wj for a target word wi (we consider p = 1 in the remainder). Such negative coefficients may be seen as instantiations of the \u2018law of differentiation\u2019: the two words\u2019 meanings move, over time, in opposite directions in semantic space. We find significantly negative relationships between the following words, among others: summit \u2194 foot, boy \u2194 woman, vow \u2194 belief, negro \u2194 black. Instead of a detailed analysis, we mention that the Wikipedia entry for the last pair indicates that the meanings of \u2018negro\u2019 and \u2018black\u2019 switched roles between the early and late 20th century. While \u2018negro\u2019 was once the \u201cneutral\u201d term for the colored population in the US, it acquired negative connotations after the 1960s; and vice versa for \u2018black\u2019."}, {"heading": "5 Concluding remarks", "text": "We suggested two novel models of semantic change. First, TISS graphs allow for detecting gradual, non-consecutive meaning change. They enable to detect statistical trends of a possibly general nature. Second, our time-series models allow for investigating negative trends in meaning change (\u2018law of differentiation\u2019) as well as forecasting into the future, which we leave for future work. Both models hint at a linear behavior of semantic change, which deserves further investigation. We note that this linearity concerns the core vocabulary of languages (in our case, words that occurred at least 100 times in each time period), and, in the case of TISS graphs, is an average result; particular words may have drastic, non-linear meaning changes across time (e.g.,\nproper names referring to entirely different entities). However, our analysis also finds that most core words\u2019 meanings decay linearly in time."}], "references": [{"title": "Opinion dynamics and learning in social networks", "author": ["Acemoglu", "Ozdaglar2011] Daron Acemoglu", "Asuman Ozdaglar"], "venue": "Dynamic Games and Applications,", "citeRegEx": "Acemoglu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Acemoglu et al\\.", "year": 2011}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "GeorgianaDinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Chen et al.2014] Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Towards semantic language classification: Inducing and clustering semantic association networks from europarl", "author": ["Eger et al.2015] Steffen Eger", "Niko Schenk", "Alexander Mehler"], "venue": "In Proceedings of the Fourth Joint Conference on Lexi-", "citeRegEx": "Eger et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Eger et al\\.", "year": 2015}, {"title": "Opinion dynamics and wisdom under out-group discrimination", "author": ["Steffen Eger"], "venue": "Mathematical Social Sciences,", "citeRegEx": "Eger.,? \\Q2016\\E", "shortCiteRegEx": "Eger.", "year": 2016}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Dyer2014] Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of EACL", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Nave learning in social networks and the wisdom of crowds", "author": ["Golub", "Jackson2010] Benjamin Golub", "Matthew O. Jackson"], "venue": "American Economic Journal: Microeconomics,", "citeRegEx": "Golub et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Golub et al\\.", "year": 2010}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "A framework for analyzing semantic change of words across time", "author": ["Jatowt", "Duh2014] Adam Jatowt", "Kevin Duh"], "venue": "In Proceedings of the Joint JCDL/TPDL Digital Libraries Conference", "citeRegEx": "Jatowt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jatowt et al\\.", "year": 2014}, {"title": "Statistically significant detection of linguistic change", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "2015b. Freshman or fresher? quantifying the geographic variation of internet", "author": ["Bryan Perozzi", "Steven Skiena"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "Word sense induction for novel sense detection", "author": ["Lau et al.2012] Jey Han Lau", "Paul Cook", "Diana McCarthy", "David Newman", "Timothy Baldwin"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association", "citeRegEx": "Lau et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lau et al\\.", "year": 2012}, {"title": "Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "TomasMikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "TomasMikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "That\u2019s sick dude!: Automatic identification of word sense change across different timescales", "author": ["Mitra et al.2014] Sunny Mitra", "Ritwik Mitra", "Martin Riedl", "Chris Biemann", "Animesh Mukherjee", "Pawan Goyal"], "venue": "In Proceedings of the 52nd Annual", "citeRegEx": "Mitra et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mitra et al\\.", "year": 2014}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "In Proceedings of the 2014 Conference on Empirical", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Investigation of word senses over time using linguistic corpora", "author": ["Thomas Bartz", "Katharina Morik", "Angelika Strrer"], "venue": "In Pavel Krl and Vclav Matouek,", "citeRegEx": "Plitz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plitz et al\\.", "year": 2015}, {"title": "Semiotic cognitive information processing: Learning to understand discourse. A systemic model of meaning constitution", "author": ["Burghard B. Rieger"], "venue": null, "citeRegEx": "Rieger.,? \\Q2003\\E", "shortCiteRegEx": "Rieger.", "year": 2003}, {"title": "Towards tracking semantic change by visual analytics", "author": ["Annette Hautli", "Thomas Mayer", "Miriam Butt", "Daniel A. Keim", "Frans Plank"], "venue": "In The 49th Annual Meeting of the Association", "citeRegEx": "Rohrdantz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rohrdantz et al\\.", "year": 2011}, {"title": "Lexicostatistic dating of prehistoric ethnic contacts", "author": ["Morris Swadesh"], "venue": "Proceedings of the American Philosophical Society,", "citeRegEx": "Swadesh.,? \\Q1952\\E", "shortCiteRegEx": "Swadesh.", "year": 1952}, {"title": "A computational evaluation of two laws of semantic change", "author": ["Xu", "Kemp2015] Y. Xu", "C. Kemp"], "venue": "In Proceedings of the 37th Annual Conference of the Cognitive Science Society", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Omnia mutantur, nihil interit: Connecting past with present by finding corresponding terms across time", "author": ["Zhang et al.2015] Yating Zhang", "Adam Jatowt", "Sourav S. Bhowmick", "Katsumi Tanaka"], "venue": "In Proceedings of the 53rd Annual Meeting", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "tend to exhibit different polysemous associations for corresponding terms (Eger et al., 2015; Kulkarni et al., 2015b).", "startOffset": 74, "endOffset": 117}, {"referenceID": 1, "context": "tics modeling (Baroni et al., 2014).", "startOffset": 14, "endOffset": 35}, {"referenceID": 17, "context": "means of their semantic similarity relations to all other words in the vocabulary (de Saussure, 1916; Rieger, 2003).", "startOffset": 82, "endOffset": 115}, {"referenceID": 18, "context": "In contrast, more finegrained analyses typically sense-label word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015).", "startOffset": 161, "endOffset": 245}, {"referenceID": 14, "context": "In contrast, more finegrained analyses typically sense-label word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015).", "startOffset": 161, "endOffset": 245}, {"referenceID": 16, "context": "In contrast, more finegrained analyses typically sense-label word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015).", "startOffset": 161, "endOffset": 245}, {"referenceID": 21, "context": "In contrast, more finegrained analyses typically sense-label word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015).", "startOffset": 161, "endOffset": 245}, {"referenceID": 7, "context": "Senselabeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al.", "startOffset": 76, "endOffset": 141}, {"referenceID": 2, "context": "Senselabeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al.", "startOffset": 76, "endOffset": 141}, {"referenceID": 15, "context": "Senselabeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al.", "startOffset": 76, "endOffset": 141}, {"referenceID": 18, "context": ", 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012).", "startOffset": 133, "endOffset": 175}, {"referenceID": 11, "context": ", 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012).", "startOffset": 133, "endOffset": 175}, {"referenceID": 21, "context": "time (words with similar meanings/roles in two time periods but potentially different lexical forms) (Zhang et al., 2015).", "startOffset": 101, "endOffset": 121}, {"referenceID": 13, "context": "An alternative to our second-order embeddings is to project vectors from different time periods in a common space (Mikolov et al., 2013a; Faruqui and Dyer, 2014), which requires to find corresponding terms across time. Further, one could also consider a \u2018core\u2019 vocabulary of semantically stable words, e.g., in the spirit of Swadesh (1952), instead of using all vocabulary words as reference.", "startOffset": 115, "endOffset": 340}, {"referenceID": 4, "context": "The model may also be seen in the socio-economic context of so-called \u201copinion dynamic models\u201d (Golub and Jackson, 2010; Acemoglu and Ozdaglar, 2011; Eger, 2016).", "startOffset": 95, "endOffset": 161}, {"referenceID": 9, "context": "We notice that the presented method can account for gradual, accumulating change, which is not possible for models that compare two succeeding time points such as the model of Kulkarni et al. (2015a).", "startOffset": 176, "endOffset": 200}], "year": 2016, "abstractText": "We consider two graph models of semantic change. The first is a time-series model that relates embedding vectors from one time period to embedding vectors of previous time periods. In the second, we construct one graph for each word: nodes in this graph correspond to time points and edge weights to the similarity of the word\u2019s meaning across two time points. We apply our two models to corpora across three different languages. We find that semantic change is linear in two senses. Firstly, today\u2019s embedding vectors (= meaning) of words can be derived as linear combinations of embedding vectors of their neighbors in previous time periods. Secondly, self-similarity of words decays linearly in time. We consider both findings as new laws/hypotheses of semantic change.", "creator": "gnuplot 4.6 patchlevel 4"}}}