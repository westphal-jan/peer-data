{"id": "1704.02681", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Pyramid Vector Quantization for Deep Learning", "abstract": "this paper explores the use of pyramid vector quantization ( ml ) to reduce the computational cost for a variety of neural networks ( nns ) while, essentially the same time, choosing inverse products that describe them. also is based on the fact that the clifford product between an n dimensional vector of real numbers and inverse n dimensional pvq polynomial can be calculated with only additions and subtractions and one multiplication. this starts further since tensor products, commonly used in nns, can be re - conduced generating a dot product or a set of dot products. practically, it already stressed that any nn architecture that appears based on an operation that can be re - conduced to a dot product can benefit from the techniques described here.", "histories": [["v1", "Mon, 10 Apr 2017 01:17:43 GMT  (238kb)", "http://arxiv.org/abs/1704.02681v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["vincenzo liguori"], "accepted": false, "id": "1704.02681"}, "pdf": {"name": "1704.02681.pdf", "metadata": {"source": "CRF", "title": "Pyramid Vector Quantization for Deep Learning", "authors": ["Vincenzo Liguori"], "emails": ["enzo@ocean-logic.com"], "sections": [{"heading": null, "text": "Pyramid Vector Quantization for Deep LearningVincenzo Liguori Ocean Logic Pty Ltd, Australia Email: enzo@ocean-logic.com Abstract\u2014This paper explores the use of Pyramid Vector\nQuantization (PVQ) to reduce the computational cost for a variety of neural networks (NNs) while, at the same time, compressing the weights that describe them. This is based on the fact that the dot product between an N dimensional vector of real numbers and an N dimensional PVQ vector can be calculated with only additions and subtractions and one multiplication. This is advantageous since tensor products, commonly used in NNs, can be re-conduced to a dot product or a set of dot products. Finally, it is stressed that any NN architecture that is based on an operation that can be re-conduced to a dot product can benefit from the techniques described here.\nIndex Terms\u2014 Machine vision, Convolutional Neural Networks, Deep Learning, Vector Quantization.\nI. INTRODUCTION n the past decade, biologically inspired artificial neural networks (NNs) have re-gained popularity thanks to the availability of ever increasing computing power required during their learning phase. Convolutional Neural Networks (CNNs)[1] are the current prevailing implementation of artificial neural networks. The efficacy of CNNs is also well known in fields from machine vision to pattern recognition and many others. I\nTheir success in multiple fields has resulted in a wide search of methods for their efficient computation, especially at inference time, as well as methods to reduce the number of bits required to describe them. Such methods have relied on pruning or simplifying a network [2] as well as quantization of the weights (in [3] to 16 bits). Quantization has been pushed to the extreme case of binary activation functions and binary (+1,-1) weights [4][5][6]. Although the basic idea of binarized weights NNs is not new [7], in their modern incarnations they have reached a level of sophistication that has practical implication for the real world use of NNs.\nReducing the computational cost at inference time has also applications in embedded systems and low power, real time hardware implementation.\nThis paper will explore how PVQ, a vector quantization technique, can be used to quantize NNs weights. This simplifies the computation of NNs as well as compresses their description.\nSpecifically, PVQ will be applied to NNs weights and show a small degradation in performance in exchange for a more compact network. Also, since dot products with PVQ vectors can be calculated only with addition and a single multiplication, a substantial reduction of the computational cost at inference time can be achieved. In most practical cases, the single multiplication can also be eliminated."}, {"heading": "II. PYRAMID VECTOR QUANTIZATION", "text": "A pyramid vector quantizer[8] (PVQ) is based on the cubic lattice points that lie on the surface of an N-dimensional pyramid. Unlike better known forms of vector quantization that require complex iterative procedures in order to find the optimal quantized vector, it has a simple encoding algorithm.\nGiven an integer K, any point on the surface of an Ndimensional pyramid y\u0302 is such that\n\u2211 i=0\nN\u22121\n|y\u0302 i|=K\n(1) with y\u0302 i integers. The pair of integers N and K, together with (1), completely define the surface of an N-dimensional pyramid indicated here with P(N ,K ) .\nIn this work a particular type of PVQ will be used, known as product PVQ. Here a vector y\u20d7\u2208\u211dN is approximated by its norm r=\u2016y\u20d7\u20162 (also referred to as \u201dradius\u201d or \u201clength\u201d of the vector) and a direction in N-dimensional space given by the vector that passes between the origin and a point y\u0302 on the surface of the N-dimensional pyramid:\nr y\u0302 \u2016 y\u0302\u20162\n(2) Note that the direction in N-dimensional space is effectively vector quantized. Null vectors are represented by r=0 . The radius r can also be quantized with a scalar quantizer. The vector y\u0302 needs to be normalized as it does not lie on the unit hyper-sphere. Given N, increasing K increases the number of quantized directions in N-dimensional space and, hence, the quality of the approximation.\nThe paper[8] also includes simple algorithms to calculate the number of points N p(N , K ) on the surface of the Ndimensional pyramid. It also provides algorithms to map any point on said surface to an integer 0\u2264i<N p(N ,K ) and vice-versa. Such mapping provides a much more compact representation of a surface point then a direct bit representation. For example, for N=8 and K=4, each component y\u0302 i would naively need 4 bits (including the sign), for a total of 8x4=32 bits for the whole vector y\u0302 . However, because of the constraint (1), N p(8,4)=2816 and, therefore, less than 12 bits are required to map any y\u0302\u2208P(8,4) .\nThe mapping of y\u0302 to an integer is not essential to the vector quantization of y\u20d7 but it can be useful in those applications where a quantized vector needs to be stored in a\nmore compact way. In this work, \u201cPVQ encoding\u201d or simply \u201cencoding\u201d a vector y\u20d7 will mean finding its closest approximation (2). \u201cMapping a vector to an integer\u201d will refer to the process that associates a PVQ vector y\u0302\u2208P(N ,K ) to an integer\n0\u2264i<N p(N , K) . Similarly for its opposite. PVQ is suitable for quantizing Laplacian sources and, in fact, it is used to quantize transformed images (whose samples can be modeled with a source that is Laplacian or approximately so) for their compression.\nThe computational cost of PVQ encoding is not very high. In any case, in this paper, PVQ vectors will be considered precalculated constants. In other words, PVQ encoding will be understood to be performed offline and not during the inference of a NN."}, {"heading": "III. DOT PRODUCT", "text": "We will now look at the dot product between a PVQ vector (2) y\u0302\u2208P(N , K ) with radius r and an N-dimensional vector x\u20d7\u2208\u211dN :\nr y\u0302 \u2016 y\u0302\u20162 \u22c5\u20d7x= r \u2016 y\u0302\u20162 \u2211 i=0\nN\u22121\ny\u0302 i x i\n(3)\nThe author has shown in [9] that \u2211 i=0\nN\u22121\ny\u0302 i xi can be\ncalculated with exactly K-1 additions and/or subtractions and no multiplications for each possible y\u0302\u2208P(N , K ) . In this case, since we consider all PVQ vector as calculated offline,\nthe scaling factor \u03c1= r\n\u2016y\u0302\u20162 \u22650 can also be pre-calculated\nand considered as a single value. In this case the dot product between a PVQ approximated vector (2) and a vector x\u20d7\u2208\u211dN takes K-1 addition/subtractions and one multiplication by the factor \u03c1 ."}, {"heading": "IV. NEURAL NETWORKS", "text": "Detailed description of NNs is beyond the scope of this article. Suffice to say that the output y of an artificial neuron can be modeled as :\ny=f (\u2211 i=0\nN\u22121\nwi xi+b)=f (w\u20d7\u22c5\u20d7x+b)\n(4) Where f () is a non-linear function (also known as activation function), w\u20d7=(w0,\u22ef,w i ,\u22ef,wN\u22121)\u2208\u211d N and\nb\u2208\u211d are constants (known as weights and bias, respectively) and x\u20d7=(x0,\u22ef, x i ,\u22ef, xN\u22121)\u2208\u211d\nN are the inputs to the neuron. If we now concatenate w\u20d7 and b to form w\u20d7 '=(w\u20d7 , b)\u2208\u211dN+1 and x\u20d7 and 1 to form x\u20d7 '=( x\u20d7 ,1)\u2208\u211dN+1 then (4) can be expressed as :\ny=f (w\u20d7 '\u22c5\u20d7x ') (5)\nEquation (5) is the one that will be used in the rest of the paper for the model of an artificial neuron.\nNNs have many different architectures. For example, in fully connected networks, each neuron is connected to all available inputs (or to all previous neurons in case of multilayer) whereas CNNs can be considered a subset of fully connected NNs. In fact, each neuron is only connected to a subset of the available input and the values of some weights are shared. In CNNs, groups of input images are mapped to groups of output images using a tensor product. The latter, once the tensor is flattened, is effectively re-conduced to a dot product. There are also other architectures, but a common theme is a dot product as in (5).\nGiven the ubiquity of the dot product in NNs and the advantage of performing the dot product with a PVQ vector, the following is proposed:\n1. Train a NNs as usual 2. Perform PVQ on groups of its original weights 3. Test the NN with the new weights for loss of\naccuracy Note that the amount of quantization can be tuned by varying the parameter K in the PVQ encoding process. A few iterations at steps 2) and 3) might be necessary to optimize the trade off between accuracy and inference performance.\nThis means that the weights w\u20d7 ' in (5) are substituted by PVQ(w\u20d7 ' )=\u03c1 w\u0302 ' resulting in:\ny=f (\u03c1w\u0302 '\u22c5\u20d7x ' ) (6)\nThe advantage of this procedure is that every dot product on N-dimensional vectors that used to take N multiplications and N-1 additions can now be performed with K-1 additions and one multiplication. NNs with weights that are PVQ vectors will be referred to as PVQ nets.\nAt this point it is legitimate to ask what kind of accuracy can be expected from such approximation. We already know that PVQ performs well when the source is Laplacian or approximately so. This means in practice that PVQ should perform well for weight distributions that have a high frequency around 0 and drop off quickly for higher values (in absolute value). This is indeed what the author has observed in his experiments. It also appear to be the case for published work like [3] (see fig. 7). According to the same paper, as well as some insights of the author, L1 and L2 regularizations, applied to both the activities and the weights during NN training can help to sparsify the weights as well as improve the statistical properties that help PVQ encoding.\nIn practice, the author has observed that, for CNNs, for all convolutional layers (except the first), using N\u2243K in the PVQ processing of the weights results in a drop of accuracy of a few %. For the first layer less quantization is necessary with K equal to 1.5x to 3x N. Fully connected layers seem to be\nmore resilient to PVQ with the ratio N K as high as 2 to 5. It\nis important to realize that, with N\u2243K , the N multiplications are reduced to one and only N-1 additions remain.\nFinally, training a NN can be formulated as an optimization problem with the weights w\u20d7 '\u2208\u211dn as the variables to be optimized. An alternative approach would be to restrict the search space for the same optimization problem directly on the surface of the hyper-pyramid P(N,K). In other words, instead of a continuous optimization problem with variables w\u20d7 '\u2208\u211dn followed by PVQ encoding of the weights, we\nwould have a mixed optimization problem with \u03c1\u2208\u211d and w\u0302 '\u2208PVQ(N , K ) as variables. This alternative\napproach is worth noting but it will not be discussed any further in this paper. A hybrid optimization technique is also possible:\n1. Train a NNs as usual 2. Perform PVQ on groups of its original weights 3. Continue training as the mixed optimization problem\ndescribed above Some preliminary experiments seem to indicate that step 3) acts as a refining and improving step. Yet another possible algorithm can be defined as Kannealing. The PVQ parameter K defines the level of quantization with larger K indicating lower quantization noise.\nThe mixed optimization problem is started with a high value for K. This is gradually lowered (\u201cannealed\u201d) to the target K as the optimization proceeds."}, {"heading": "V. FURTHER OPTIMIZATIONS", "text": "Let\u2019s consider a set of M neurons, each with its set of\nweights w\u20d7 i ' and inputs x\u20d7 i ' : y i=f (w\u20d7 i '\u22c5\u20d7xi ') with 0\u2264i<M\n(7) The dimensionality of each set of weights and inputs does not need to be the same (i.e. dim( w\u20d7i ')\u2260dim (w\u20d7 j ' ) ). We already know that, if the statistics of the weights are favorable, we can approximate the set of neurons by PVQ encoding the weights and substituting them to the original. This means by PVQ( w\u20d7i ' )=\u03c1iw\u0302 i ' resulting in:\ny i=f (\u03c1i w\u0302 i '\u22c5\u20d7x i ') (8)\nLet W\u20d7=(w\u20d70 ' ,\u22ef, w\u20d7 i ' ,\u22ef, w\u20d7 'M\u22121) and X\u20d7=( x\u20d70 ' ,\u22ef, x\u20d7 i ' ,\u22ef, x\u20d7 'M\u22121) be built by concatenating the aforementioned weights and inputs. Let\u2019s also PVQ encode W\u20d7 : PVQ(W\u20d7 )=\u03c1W\u0302=\u03c1( w\u03020 ' ' ,\u22ef, w\u0302i ' ' ,\u22ef, w\u0302 ' 'M\u22121)\n(9) Note that, in general, w\u0302 i '\u2260w\u0302 i ' ' . Let\u2019s now consider\nthe dot product:\n\u03c1W\u0302\u22c5\u20d7X=\u03c1(w\u03020 ' '\u22c5\u20d7x0+\u22ef+ w\u0302i ' '\u22c5\u20d7xi+\u22ef+ w\u0302 ' 'M\u22121\u22c5\u20d7xM\u22121) (10)\nLeaving aside the fact that this particular dot product has no particular meaning, we can stress once again that, as for any dot product with a PVQ vector, it can be calculated with K-1 additions or subtractions and one multiplication. This is important because it shows that the total number of additions necessary to calculate all the partial dot products w\u0302 i ' '\u22c5\u20d7x i will be \u2264K\u22121 . More importantly (and we shall see why soon), if we PVQ all the weights w\u20d7 i ' concatenated together as W\u20d7 , then the scaling factor \u03c1 will be a single scalar instead of M different ones had we PVQ encoded each w\u20d7 i ' separately as in (9). Now (9) can be re-written as:\ny i=f (\u03c1w\u0302 i ' '\u22c5\u20d7x i ' ) (11)\nAnd, for any activation function for which f (\u03c1 x)=\u03c1 f (x )\n(12) such as ReLU (very useful in practice), we have:\ny i=\u03c1 f (w\u0302 i ' '\u22c5\u20d7x i ' ) (13)\nIn other words, if PVQ encode the weights of a group of neurons together, then, with (12) true, the scaling factor \u03c1 can \u201cpass through\u201d the non-linearity.\nThis is very important as NNs are built in stacked layers. If we PVQ encode all the weights for a particular layer together, then, with (13) true, we have:\n\u2022 All the calculations for that layer, before the activation function can be done with at most K-1 addition and subtractions \u2022 The output for all the neurons for that layer will be scaled by the same \u03c1 since \u03c1 can \u201cpass through\u201d the activation function as in (14) Let\u2019s focus on the second point. For many NNs, the outputs of one layer become the inputs of the next. If all the outputs of one layer were scaled by the same factor, then the next layer will also see all its inputs scaled by the same factor. Let\u2019s see what happens when we also PVQ the weights of the next layer and assume that all the inputs are scaled by the same value, with (12) true: z i=f ((\u03c11w\u0302 i)\u22c5(\u03c10 x\u20d7 i))=f (\u03c11\u03c10 w\u0302 i\u22c5\u20d7xi)=\u03c11\u03c10 f (w\u0302 i\u22c5\u20d7x i)\n(14) Here \u03c11 is the scaling factor resulting from the PVQ encoding of the current layer weights while \u03c10 is the one from PVQ encoding of previous layer weights (and \u201cpassed through\u201d the activation function (12)). This can be repeated for any number of layers.\nSo, if we apply PVQ encoding to each layer of a NN, with a suitable activation function (12), the scaling factor \u03c1 for each PVQ vector dot product can be propagated through the network, layer by layer, up to the outputs of the network which\nwill have a final scaling factor \u03c1=\u220f i=0\nL\u22121\n\u03c1i for an L-layer\nNN. In other words, only the outputs of such network will have to be scaled by \u03c1 . This is an important result because:\n\u2022 The number of outputs in a NN is, in general, smaller and often much smaller than the number of outputs of all the neurons in the network. Therefore the impact of multiplications is limited to the outputs only. \u2022 In many cases the output of the NN uses one hot encoding and no activation is applied. In this case the output of the NN is given by the argmax function that is not influenced by a positive scaling factor. Therefore, in this case, the latter can be completely eliminated.\n\u2022 All the layers of the network can be calculated with only addition and subtractions (because they have been subject to PVQ encoding). \u2022 For many useful NNs, the input values are integers (i.e. 8 bit pixels) and this means that all the layers can be calculated by only addition and subtraction of integer values. These are referred to as integer PVQ nets. This is also true if the network contains Maxpool layers because, remembering that \u03c1\u22650 :\nMax (\u03c1 y0 ,\u2026,\u03c1 yN\u22121)=\u03c1Max ( y0 ,\u2026 , y N\u22121) (15) So, the scaling factor \u03c1 will also propagate through Maxpool layers. And this also applies to convolutional as well as fully connected layers.\nThis means that many existing NNs, can be converted to integer PVQ nets with a substantial computational advantage.\nYet another advantage of integer PVQ nets is that, being computed only with addition and subtractions of integer values, the precision required can be easily tracked through all the layers. In fact, for deep networks, the full precision is probably not necessary and, at each layer, one can simply rescale the values by a power of 2 (i.e. with shift operations) in order to reduce the number of bits required.\nAnother type of activation function that leads to great computational advantage in PVQ nets is the one for which:\nf (\u03c1 x)=f (x ) (16)\nis true, at least when \u03c1\u22650 which is obviously the case in the case of PVQ vector scaling factors. In this case the scaling factor \u03c1 of the PVQ encoded weights is simply \u201dadsorbed\u201d by the activation function and all the properties previously described also apply, without the need to \u201cpropagate\u201d \u03c1 to the outputs as before.\nAn example of such activity function can be found in the binary version (with only +/-1 output values) of the sign(x ) function (which is ternary with -1, 0 and +1 as\npossible outputs):\nbsign(x)={+1 if x\u22650\u22121 if x<0 (17)\nIt is possible to train multi-layer NNs with (17) as activation function and continuous weights. We can then PVQ encode the weights at each layer. For each layer, the scaling factor \u03c1 is then eliminated because of the nature of the activation function (16). The calculations are now simplified to addition and subtractions of +1 and -1 values only (except for the first layer where, if the inputs are integers, it will still be computed with integer additions and subtractions). We will refer to these nets as binary PVQ nets.\nNote the difference from binarized networks such as [4] or [6]. In these all weights are either +1 or -1 whereas in binary PVQ nets a weight can be (theoretically) as large as +/-K. However, the total number of additions or subtractions in a layer is guaranteed to be \u2264K\u22121 . If in a particular layer of a binary PVQ net N=K, then the total number of binary additions and subtractions will be the same as in [4] but each individual weight doesn\u2019t need to be +/-1.\nThis is an important difference and it is worth making a simple example. Suppose we have a single neuron in a binary net with N=7 binary inputs and weights. Suppose the weights are (-1,1,1,1,-1,-1,1). The absolute value of each weight must be 1 and the dot product with the binary input will require 6 additions or subtractions. Suppose now we have a binary PVQ net with a single neuron and 7 inputs with N=K=7. A weight vector can have values different from +/-1 like (-2,1,0,0,0,2,2) or (0,0,-3,0,-2,2,0) (both respecting constraint (1)) but the dot product will still require 6 additions or subtractions."}, {"heading": "VI. WEIGHTS COMPRESSION", "text": "After PVQ encoding the weights can be losslessly compressed. In fact, as already mentioned, [8] describes an algorithm to map a point on a hyper-pyramid P(N,K) to an integer. Such integer can obviously be represented by a string of bits that completely and compactly describes the integer part of a PVQ vector. The scaling factor \u03c1 , if required, must be quantized separately.\nUnfortunately, the algorithm given in [8] is not very practical, especially when it comes to the inverse process (i.e. converting the integer number back to a PVQ vector). This is because it can involve multiple arithmetic operations on numbers thousands of bit long. However, an important advantage remains in mapping a PVQ vector to an integer, as mentioned in section II: unlike the methods that will be discussed below that result in a string of bits of unpredictable size, the method described in [8] requires\nlog2(N p(N ,K )) bits for any w\u0302\u2208P(N , K) . More conventional compression techniques can still be effective and much more practical. In fact, after PVQ encoding, the values of the elements of the vector quantized vector are not all equiprobable: 0 and +/-1 values being, for example, far more likely than any others. This means that a\nHuffan encoding scheme is a possibility, assigning a smaller codeword to the most likely values. The drawback is the creation of a potentially very large table. This is because if we want to Huffman encode PVQ encoded weights for a NN layer, generally speaking, we are talking about thousands of dimensions. Now, with N\u2243K and +/-K the largest (theoretical) value to Huffman encode, a very large table would berequired. A more practical scheme would consist in creating a Huffman table for each value whose absolute value is less than a certain value V plus an escape code. Any element of the PVQ encoded vector will then have its own Huffman code if its absolute value is less than V, otherwise the escape code is used followed by the full binary representation of all the other possible values (minus the ones covered by the Huffman code).\nGolomb exponential codes are also an interesting possibility: they do not need the storage of large tables and they are suitable for encoding values whose frequency decays rapidly with their increase in magnitude (just like the values of the elements of a PVQ encoded vector).\nFor fully connected layers in a NN where the ratio N K\ncan be as high as 5, run length encoding is a good fit as it allow less than one bit per weight for long runs of zeros. In a\nPVQ encoded vector with a ratio N K\u22435 , at least 4 5 of\nthe values are guaranteed to be zero. This is because, best case, there will be only K elements with absolute value equal to one leaving all the other elements necessarily set to zero (remember the constraint (1) that applies to all PVQ vectors).\nArithmetic encoding is also a possibility although the author is not particularly keen on this method given its difficulty to parallelize and/or randomly access a particular point in the compressed bitstream.\nThe similarity between the type of data obtained from image and video after transform and quantization and the PVQ encoded NN weights is an open invitation to re-use and adapt algorithms from the lossless compression stages of image and video compression standards such as JPEG, H.264 and others. These are designed to compress similar types of data where small values are more frequent."}, {"heading": "VII. EXPERIMENTS", "text": "The author performed a series of experiments by training some simple NN using tools such as Keras[10] and Tensorflow[11] and then by PVQ encoding the original weights a whole layer at the time.\nThe most accurate PVQ encoding algorithm known to the author has O(NK) complexity. Since even in relative small networks it might be required to PVQ encode vectors with a dimensionality of 1,000,000 or more, it became necessary for the author to create a GPU implementation using CUDA.\nAlso, the author is new to the field of NN training and his results are far from the state of the art. However, the important\npoint here is not the absolute accuracy of NN but, rather, the relatively small loss of accuracy that a NN suffers when PVQ encoding is applied to its layers. In other words, the important point here is the small drop in performance for a NN brought by PVQ encoding that is traded off for all the computational, storage and bandwidth advantages discussed.\nThe purpose of the experiments mentioned here is to try to ascertain, for some specific NN, what kind of PVQ encoding is possible if one is willing to tolerate an overall loss of accuracy of a few %. PVQ encoding is applied to the weights of each layer of a NN separately. Specifically, for each layer, the following steps are taken:\n\u2022 Extract all the weights and biases from the given layer. \u2022 The weights (in the form of a tensor or a matrix) are flattened and then concatenated with the biases to form a single vector of dimensionality N. \u2022 PVQ encoding is applied to the N-dimensional vector with a given quantization parameter K, resulting a scalar \u03c1 and an integer vector w\u0302\u2208P(N , K) . \u2022 The vector \u03c1 w\u0302 will then be split into its weights and biases components.\n\u2022 The weights vector so obtained will now be reassembled into its original matrix/tensor shape and biases. \u2022 The original weights and biases for the given layer will be now replaced with the new quantized ones. Note that no further refinement or mixed optimization (as mentioned at the end of section IV) was performed to\nrefine or improve on PVQ encoding.\nTable 1 shows the anatomy of the NN taken from the Keras examples for the MINIST dataset [12] (indicated with A for future reference). Different layers are indicated by IN for input, FC for fully connected, CONV for convolutional, MAX for maxpool, DRP for dropout. The activation function used is ReLU.\nNote that PVQ encoding with the procedure described above is only applied to layers that contain weights such as CONV and FC. For these layers, N indicates the dimensionality of the flattened vector (including biases) and the quantization parameter K is expressed as ratio with N.\nFor the NN shown in Table 1, the testing accuracy went\nfrom 98.27% before PVQ encoding to 95.33%. after.\nTable 2 shows the anatomy of the NN taken from the Keras examples for the CIFFAR10 dataset [13] (indicated with B). Again, PVQ encoding is only applied to layers that contain weights. For the NN shown in Table 2, the testing accuracy went from 78.46% before PVQ encoding to 73.21%. after.\nA few observations: \u2022 After PVQ encoding, all the networks are greatly\nsimplified and compressed, especially for fully connected layers. \u2022 As seen also in [4][6], the first layer is the hardest to quantize. Fully connected layers seem to be the most compressible. \u2022 Since the inputs are integers and the activation function is a ReLU, after PVQ encoding, these are essentially integer PVQ nets. Thus their inference only requires integer additions and subtractions. More experiments were performed on these networks by changing the activation function to (17). Training of NNs is based on optimization of an objective function with variations of the gradient descent method. The latter implies the existence of the derivative of the objective function which would be essentially zero if one were to use (17) as activation function. In order to overcome this problem, Geoff Hinton in [14] suggests to use a \u201cStraight Through Estimator\u201d (STE) which, essentially, consists of imposing a derivative for (17) by definition:\nd d x bsign(x )=1\n(18) Unfortunately Keras does not support user defined activation functions and its derivatives. Therefore the author had to convert the two NNs above to Tensorflow.\nAfter converting to Tensorflow, changing the activation function to (17) and its pseudo-derivative (18) was difficult\nTable 3 shows the same NN shown in Table 1 with the ReLU activation function changed to (17) (indicated with C). The testing accuracy for this NN went from 94.14% before PVQ encoding to 91.28%. after.\nTable 4 shows the same NN shown in Table 2 with the ReLU activation function changed to (17). When converted to Tensorflow, even with ReLU as activation function, this NN never reached the same level of accuracy reached with Keras (with Tensorflow backend): 70.74%. With the activation function changed to (17), testing accuracy was 61.62% before and 58.54% after PVQ encoding. Some considerations:\n\u2022 These are only few of the possible K assignments during PVQ encoding: others might lead to better approximation for the same level of compression. \u2022 After PVQ encoding, these NNs are binary PVQ nets and all layers (except the first whose inputs are integers) can be calculated with additions and subtractions of binary values only. \u2022 For layers where N/K~=1, the number of addition and subtractions of binary values is the same as in a binarized net as in [4][6]. However, for layers where N/K>1, the number of additions and subtractions of binary values is smaller than a binarized net. \u2022 For both NNs dropout was not used as it resulted in worse results. It is possible that binarized nets are self-regularizing: training and testing accuracy were always close.\n\u2022 As stressed before, the important point is the relatively small drop in accuracy compared to the gain in performance during inference. No direct experiments on further, lossless compression of the weights have been performed. However, weights statistics after PVQ encoding have been collected and shown below. The experimental results strongly support the discussion in section VI as the weight statistics show their high compressibility.\nTable 5 shows the distribution for the weights of NN A after PVQ encoding, layer by layer. Note that all the weights are integer values as the \u03c1 factor from PVQ encoding is not used for these NNs. Now, to get an idea of how compressible the NN PVQ weights are, let us consider a simple scheme that uses exponential Golomb codes for each value. Exponential Golomb codes will use 1 bit for 0 values, 3 bits for \u00b12..3, 5 bits for \u00b14..7, etc. So, the average for FC0 in NN A will be 0.8119+3*0.1771+5*0.011+7*0.000052=~1.4 bits/weight.\nHowever, due to the large number of zeros, it is likely that a better result can be obtained with a simple run-length encoding scheme.\nTable 6 shows the distribution for the weights of NN B after PVQ encoding. Again, using a simple exponential Golomb coding scheme as outlined above on, say, CONV1 of NN B, one gets ~2.8 bits/weight. Results on the C and D NNs follow."}, {"heading": "VIII. HARDWARE CONSIDERATIONS", "text": "PVQ encoding of NN weights brings many advantages to their hardware implementation such as reducing computational as well as bandwidth and storage cost. This is especially true for binary PVQ nets. Table 5 in [6] shows the advantage in hardware implementation in reducing operations from floating point to integer. The underlining assumption in this section is that PVQ encoding of weights happens offline. Thus, the number and position of zero coefficients in a PVQ vector are known in advance and they can be excluded from any calculation.\nFig.1 shows a couple of possible serial architectures for the dot product in PVQ nets. There\u2019s also an INIT signal to clear the Acc content for the first product.\nThe one on the left uses a multiplier to multiply a PVQ weight w\u0302 i with an input x i . The result is then accumulated in Acc. This is the classical way of calculating a\ndot product in hardware and, with the assumption made above of w\u0302 i\u22600 , it will take K clock cycles at most.\nThe architecture on the right, exploits the properties of a PVQ vector and it works by adding x i to Acc w\u0302 i times if w\u0302 i>0 and subtracting it |w\u0302i| times if w\u0302 i<0 (the control signal for the add/sub is not shown). It will take exactly K clock cycles, regardless of the PVQ encoded weights. This architecture would appear to be the best because of the lack of the multiplier. This is generally the case. However, we know from the experiments that, even with N\u2243K , up to 1/3 for the PVQ weights is zero. This allows to calculate the dot product in less cycles in the architecture with the multiplier whereas the other one will always take K cycles. This can be an advantage in some cases, especially if we consider that the PVQ weights w\u0302 i are small in magnitude resulting in a simple multiplier (see weight statistics in the previous section). The advantage is even more pronounced in the case of fully connected layers with even more w\u0302 i=0 . Finally we notice that a useful activation function as ReLU can be easily implemented at the output of either architecture in Fig.1. In fact, assuming that Acc represents numbers in two\u2019s complement, all that is needed is the sign bit of Acc to force the output to zero if negative. This can easily be done with AND gates controlled from the sign bit.\nWe can now examine two architectures in Fig. 2 that are suitable for binary PVQ nets. They assume that their input x i is binary with 0 indicating x i=1 and 1 for x i=\u22121 . The architecture on the left will accumulate PVQ weights with the sign inverted if x i=\u22121 (the add/sub is controlled by x i ). This will take K cycles at most.\nThe architecture on the right is an up/down counter that can be set to zero by INIT signal. The counter will increment when U/D input is zero, decrement otherwise. If the PVQ weight w\u0302 i>0 then 0 will be input w\u0302 i times, otherwise 1 will be input |w\u0302i| times. The XOR gate will perform a sign product with x i : if the sign is the same the counter will be incremented, decremented otherwise. In other words, the counter will add or subtract w\u0302 i to its value, depending on the sign of x i . This will take exactly K cycles.\nSimilarly to the other architectures, there is a trade off between complexity and speed.\nWe also note that the activation function (17) needs no circuit for its implementation: it\u2019s simply the sign bit of the Acc/counters.\nAlso, in FPGA implementations, we can exploit the LUT architecture to pack multiple sums of products as shown in Fig.3.\nDue to the binary nature of the inputs in binary PVQ nets, multiple sums of product can be pre-calculated in LUTs. Modern FPGA have 6 input LUTs that are user programmable (even dynamically, during the computation) and can pack 6 partial sums of product as a bitslice. The number of LUTs will depend on the required precision of the output. Other FPGA families have abundant small memories that can be used for a similar purpose with an even larger number of binary inputs. The partial sums still need to be added up to form the final dot product but a lot of the computation is taken care by the LUT.\nAll the architectural elements described can be clearly parallelized. Some parallel architectures, as suggested in section IV in [9] can be more practical for binary PVQ nets. For example, the crossbar there described is clearly simpler for binary value and it can be implemented with Clos networks [15] either in full or in a time-multiplexing fashion.\nFinally, for binary PVQ nets, the Maxpool non-linearity is simply implemented with:\nMax (x0 ,\u2026, xN\u22121)=x0 AND x1\u2026AND x N\u22121 (20) where x0 ,\u2026, xN\u22121 are binary values with the\nconvention described above to represent +/-1 values."}, {"heading": "IX. CONCLUSION AND FUTURE WORK", "text": "This paper has shown how to use PVQ to vector quantize NN weights and then use the properties of PVQ encoded vectors to simplify the NN inference. In particular:\n\u2022 PVQ encoding the weights substantially reduces the number of bits required per weight. \u2022 If the NN is built with activation functions like ReLU and non linearity such as Maxpool, then it can be inferred with addition and subtraction only. \u2022 If the NN is built with activation functions like bsign(x), then it can be mostly inferred with addition and subtractions of binary values. \u2022 This reduction in computational cost and storage is very important for low power implementations in hardware for embedded system. Future work will focus on what can be done during training to improve results after PVQ encoding as well as postquantization optimization steps. Hardware implementations of PVQ nets will also be created."}], "references": [{"title": "Gradient-Based Learning Applied to Document Recognition\u201dProceedings of the IEEE 86", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "W.J.Dally \u201cLearning both Weightsand Connections for Efficient Neural Networks", "author": ["S. Han", "J. Pool", "J. Tran"], "venue": "arXiv : 1506.02626v3,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A highly Scalable Restricted Boltzmann Machine FPGA Implementation", "author": ["S.K. Kim", "L.C. McAfee", "P.L. McMahon", "K. Olukotun"], "venue": "International Conference on Field Programmable Logic and Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "XNOR-Net: ImageNet Classification", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "Using Binary Convolutional Neural Networks\u201d, arXiv:1603.05279v3,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "On the capacity of neural networks with binary weights", "author": ["I. Kocher", "R. Monasson"], "venue": "J. Phys. A: Math. Gen", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "A Pyramid Vector Quantizer", "author": ["T.R. Fischer"], "venue": "IEEE Transactions on Information Theory, Vol. IT-32,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1986}, {"title": "Vector Quantization for Machine Vision", "author": ["V. Liguori"], "venue": "arXiv: 1603.09037,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Neural networks for machine learning", "author": ["Geoffrey Hinton"], "venue": "Coursera, video lectures,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "A study of non-blocking switching networks\",Bell System", "author": ["C. Charles"], "venue": "Technical Journal", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1953}], "referenceMentions": [{"referenceID": 0, "context": "Convolutional Neural Networks (CNNs)[1] are the current prevailing implementation of artificial neural networks.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "Such methods have relied on pruning or simplifying a network [2] as well as quantization of the weights (in [3] to 16 bits).", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "Such methods have relied on pruning or simplifying a network [2] as well as quantization of the weights (in [3] to 16 bits).", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "Quantization has been pushed to the extreme case of binary activation functions and binary (+1,-1) weights [4][5][6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "Although the basic idea of binarized weights NNs is not new [7], in their modern incarnations they have reached a level of sophistication that has practical implication for the real world use of NNs.", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "A pyramid vector quantizer[8] (PVQ) is based on the cubic lattice points that lie on the surface of an N-dimensional pyramid.", "startOffset": 26, "endOffset": 29}, {"referenceID": 5, "context": "The paper[8] also includes simple algorithms to calculate the number of points N p(N , K ) on the surface of the Ndimensional pyramid.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "The author has shown in [9] that \u2211", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "It also appear to be the case for published work like [3] (see fig.", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Note the difference from binarized networks such as [4] or [6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 3, "context": "If in a particular layer of a binary PVQ net N=K, then the total number of binary additions and subtractions will be the same as in [4] but each individual weight doesn\u2019t need to be +/-1.", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": "In fact, as already mentioned, [8] describes an algorithm to map a point on a hyper-pyramid P(N,K) to an integer.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "Unfortunately, the algorithm given in [8] is not very practical, especially when it comes to the inverse process (i.", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "However, an important advantage remains in mapping a PVQ vector to an integer, as mentioned in section II: unlike the methods that will be discussed below that result in a string of bits of unpredictable size, the method described in [8] requires log2(N p(N ,K )) bits for any \u0175\u2208P(N , K) .", "startOffset": 234, "endOffset": 237}, {"referenceID": 3, "context": "\u2022 As seen also in [4][6], the first layer is the hardest to quantize.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "In order to overcome this problem, Geoff Hinton in [14] suggests to use a \u201cStraight Through Estimator\u201d (STE) which, essentially, consists of imposing a derivative for (17) by definition: d d x bsign(x )=1 (18) Unfortunately Keras does not support user defined activation functions and its derivatives.", "startOffset": 51, "endOffset": 55}, {"referenceID": 3, "context": "\u2022 For layers where N/K~=1, the number of addition and subtractions of binary values is the same as in a binarized net as in [4][6].", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "Some parallel architectures, as suggested in section IV in [9] can be more practical for binary PVQ nets.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "For example, the crossbar there described is clearly simpler for binary value and it can be implemented with Clos networks [15] either in full or in a time-multiplexing fashion.", "startOffset": 123, "endOffset": 127}], "year": 2017, "abstractText": "This paper explores the use of Pyramid Vector Quantization (PVQ) to reduce the computational cost for a variety of neural networks (NNs) while, at the same time, compressing the weights that describe them. This is based on the fact that the dot product between an N dimensional vector of real numbers and an N dimensional PVQ vector can be calculated with only additions and subtractions and one multiplication. This is advantageous since tensor products, commonly used in NNs, can be re-conduced to a dot product or a set of dot products. Finally, it is stressed that any NN architecture that is based on an operation that can be re-conduced to a dot product can benefit from the techniques described here.", "creator": "Writer"}}}