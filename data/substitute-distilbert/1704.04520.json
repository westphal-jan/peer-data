{"id": "1704.04520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Neural Machine Translation Model with a Large Vocabulary Selected by Branching Entropy", "abstract": "neural machine division ( nmt ), starting new approach to machine translation, has achieved promising results comparable meeting those of traditional approaches such as statistical machine translation ( smt ). despite its recent success, nmt cannot handle a larger vocabulary because initial training complexity and decoding complexity proportionally increase with the number of target words. this problem becomes even more realistic but translating vocabulary documents, which contain many technical terms, are observed infrequently. in this paper, we hope to select phrases that contain out - of - way words using the statistical approach of branching entropy. this allows the typical nmt system to be applied to a translation task of any language pair without any language - sensitive knowledge about technical term identification. the selected phrases are uniformly replaced with tokens during translate and post - translated, different phrase translation table of information. evaluation on japanese - to - chinese and chinese - to - japanese patent sentence translation proved the effectiveness : phrases selected with branching entropy, where the proposed nmt system performs a substantial improvement over current baseline nmt grammar without our proposed technique.", "histories": [["v1", "Fri, 14 Apr 2017 19:35:14 GMT  (1458kb,D)", "https://arxiv.org/abs/1704.04520v1", "EMNLP 2017, under review"], ["v2", "Sun, 14 May 2017 11:41:39 GMT  (1914kb,D)", "http://arxiv.org/abs/1704.04520v2", "EMNLP 2017 and MTSummit 2017, under review"], ["v3", "Tue, 6 Jun 2017 05:58:48 GMT  (1914kb,D)", "http://arxiv.org/abs/1704.04520v3", "EMNLP 2017 and MTSummit 2017, under review. arXiv admin note: text overlap witharXiv:1704.04521"], ["v4", "Thu, 20 Jul 2017 01:59:10 GMT  (5100kb,D)", "http://arxiv.org/abs/1704.04520v4", "ACL 2017 rejected"], ["v5", "Wed, 26 Jul 2017 08:26:54 GMT  (4759kb,D)", "http://arxiv.org/abs/1704.04520v5", "MT summit 2017 poster"], ["v6", "Wed, 6 Sep 2017 04:06:07 GMT  (4759kb,D)", "http://arxiv.org/abs/1704.04520v6", "MT summit 2017 poster"]], "COMMENTS": "EMNLP 2017, under review", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zi long", "ryuichiro kimura", "takehito utsuro", "tomoharu mitsuhashi", "mikio yamamoto"], "accepted": false, "id": "1704.04520"}, "pdf": {"name": "1704.04520.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation Model with a Large Vocabulary Selected by Branching Entropy", "authors": ["Zi Long", "Ryuichiro Kimura", "Takehito Utsuro", "Tomoharu Mitsuhashi", "Mikio Yamamoto"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Bahdanau et al., 2015; Cho et al., 2014; Jean et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a,b; Sutskever et al., 2014). An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation. The entire neural network is jointly trained to maximize the conditional probability of the correct translation of a source sentence with a bilingual corpus. Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabu-\nar X\niv :1\n70 4.\n04 52\n0v 6\n[ cs\nlaries. This is because the training complexity and decoding complexity proportionally increase with the number of target words. Words that are out of vocabulary are represented by a single \u201c\u3008unk\u3009\u201d token in translations, as illustrated in Figure 1. The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms.\nThere have been a number of related studies that address the vocabulary limitation of NMT systems. Jean et al. (2014) provided an efficient approximation to the softmax function to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of the out-of-vocabulary token in the target sentence with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed replacing outof-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data. Sennrich et al. (2016) introduced an effective approach based on encoding rare and out-of-vocabulary words as sequences of subword units. Luong and Manning (2016) provided a character-level and word-level hybrid NMT model to achieve an open vocabulary, and Costa-Jussa\u0300 and Fonollosa (2016) proposed an NMT system that uses character-based embeddings.\nHowever, these previous approaches have limitations when translating patent sentences. This is because their methods only focus on addressing the problem of out-of-vocabulary words even though the words are parts of technical terms. It is obvious that a technical term should be considered as one word that comprises components that always have different meanings and translations when they are used alone. An example is shown in Figure 1, where the Japanese word \u201c \u201d(bridge) should be translated to Chinese word \u201c \u201d when included in technical term \u201cbridge interface\u201d; however, it is always translated as \u201c \u201d.\nTo address this problem, Long et al. (2016) proposed extracting compound nouns as technical terms and replacing them with tokens. These compound nouns then are post-translated with the phrase translation table of the statistical machine translation (SMT) system. However, in their work on Japanese-to-Chinese patent translation, Japanese compound nouns are identified using several heuristic rules that use specific linguistic knowledge based on part-of-speech tags of morphological analysis of Japanese language, and thus, the NMT system has limited application to the translation task of other language pairs. In this paper, based on the approach of training an NMT model on a bilingual corpus wherein technical term pairs are replaced with tokens as in Long et al. (2016), we aim to select phrase pairs using the statistical approach of branching entropy; this allows the proposed technique to be applied to the translation task on any language pair without needing specific language knowledge to formulate the rules for technical term identification. Based on the results of our experiments on many pairs of languages:\nJapanese-to-Chinese, Chinese-to-Japanese, Japanese-to-English and English-to-Japanese, the proposed NMT model achieves a substantial improvement over a baseline NMT model without our proposed technique. Our proposed NMT model achieves an improvement of 1.2 BLEU points over a baseline NMT model when translating Japanese sentences into Chinese, and an improvement of 1.7 BLEU points when translating Chinese sentences into Japanese. Our proposed NMT model achieves an improvement of 1.1 BLEU points over a baseline NMT model when translating Japanese sentences into English, and an improvement of 1.4 BLEU points when translating English sentences into Japanese. Moreover, the number of translation error of under-translations1 by the the baseline NMT model without our proposed technique reduces to around half by the proposed NMT model."}, {"heading": "2 Neural Machine Translation", "text": "NMT uses a single neural network trained jointly to maximize the translation performance (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al., 2014). Given a source sentence x = (x1, . . . , xN ) and target sentence y = (y1, . . . , yM ), an NMT model uses a neural network to parameterize the conditional distributions\np(yz | y<z,x)\nfor 1 \u2264 z \u2264M . Consequently, it becomes possible to compute and maximize the log probability of the target sentence given the source sentence as\nlog p(y | x) = M\u2211 l=1 log p(yz|y<z,x)\nIn this paper, we use an NMT model similar to that used by Bahdanau et al. (2015), which consists of an encoder of a bidirectional long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and another LSTM as decoder. In the model of Bahdanau et al. (2015), the encoder consists of forward and backward LSTMs. The forward LSTM reads the source sentence as it is ordered (from x1 to xN ) and calculates a sequence of forward hidden states, while the backward LSTM reads the source sentence in the reverse order (from xN to x1) , resulting in a sequence of backward hidden states. The decoder then predicts target words using not only a recurrent hidden state and the previously predicted word but also a context vector as followings:\np(yz | y<z,x) = g(yz\u22121, sz\u22121, cz)\nwhere sz\u22121 is an LSTM hidden state of decoder, and cz is a context vector computed from both of the forward hidden states and backward hidden states, for 1 \u2264 z \u2264M ."}, {"heading": "3 Phrase Pair Selection using Branching Entropy", "text": "Branching entropy has been applied to the procedure of text segmentation (e.g., (Jin and TanakaIshii, 2006)) and key phrases extraction (e.g., (Chen et al., 2010)). In this work, we use the left/right branching entropy to detect the boundaries of phrases, and thus select phrase pairs automatically.\n1 It is known that NMT models tend to have the problem of the under-translation. Tu et al. (2016) proposed coverage-based NMT which considers the problem of the under-translation."}, {"heading": "3.1 Branching Entropy", "text": "The left branching entropy and right branching entropy of a phrase w are respectively defined as\nHl(w) = \u2212 \u2211\nv\u2208Vwl\npl(v) log2 pl(v)\nHr(w) = \u2212 \u2211\nv\u2208Vwr\npr(v) log2 pr(v)\nwhere w is the phrase of interest (e.g., \u201c / \u201d in the Japanese sentence shown in Figure 1, which means \u201cbridge interface\u201d), Vwl is a set of words that are adjacent to the left of w (e.g., \u201c \u201d in Figure 1, which is a Japanese particle) and Vwr is a set of words that are adjacent to the right of w (e.g., \u201c388\u201d in Figure 1). The probabilities pl(v) and pr(v) are respectively computed as\npl(v) = fv,w fw\npr(v) = fw,v fw\nwhere fw is the frequency count of phrase w, and fv,w and fw,v are the frequency counts of sequence \u201cv,w\u201d and sequence \u201cw,v\u201d respectively. According to the definition of branching entropy, when a phrase w is a technical term that is always used as a compound word, both its left branching entropy Hl(w) and right branching entropy Hr(w) have high values because many different words, such as particles and numbers, can be adjacent to the phrase. However, the left/right branching entropy of substrings of w have low values because words contained in w are always adjacent to each other."}, {"heading": "3.2 Selecting Phrase Pairs", "text": "Given a parallel sentence pair \u3008Ss, St\u3009, all n-grams phrases of source sentence Ss and target sentence St are extracted and aligned using phrase translation table and word alignment of SMT according to the approaches described in Long et al. (2016). Next, phrase translation pair \u3008ts, tt\u3009 obtained from \u3008Ss, St\u3009 that satisfies all the following conditions is selected as a phrase pair and is extracted:\n(1) Either ts or tt contains at least one out-of-vocabulary word.2\n(2) Neither ts nor tt contains predetermined stop words.\n(3) Entropies Hl(ts), Hl(tt), Hr(ts) and Hr(tt) are larger than a lower bound, while the left/right branching entropy of the substrings of ts and tt are lower than or equal to the lower bound.\nHere, the maximum length of a phrase as well as the lower bound of the branching entropy are tuned with the validation set.3 All the selected source-target phrase pairs are then used in the\n2 One of the major focus of this paper is the comparison between the proposed method and Luong et al. (2015b). Since Luong et al. (2015b) proposed to pre-process and post-translate only out-of-vocabulary words, we focus only on compound terms which include at least one out-of-vocabulary words.\n3 Throughout the evaluations on patent translation of both language pairs of Japanese-Chinese and Japanese-English, the maximum length of the extracted phrases is tuned as 7. The lower bounds of the branching entropy are tuned as 5 for patent translation of the language pair of Japanese-Chinese, and 8 for patent translation of the language pair of Japanese-English. We also tune the number of stop words using the validation set, and use the 200 most-frequent Japanese morphemes and Chinese words as stop words for the language pair of Japanese-Chinese, use the 100 mostfrequent Japanese morphemes and English words as stop words for the language pair of Japanese-English.\nnext section as phrase pairs.4"}, {"heading": "4 NMT with a Large Phrase Vocabulary", "text": "In this work, the NMT model is trained on a bilingual corpus in which phrase pairs are replaced with tokens. The NMT system is then used as a decoder to translate the source sentences and replace the tokens with phrases translated using SMT."}, {"heading": "4.1 NMT Training after Replacing Phrase Pairs with Tokens", "text": "Figure 2 illustrates the procedure for training the model with parallel patent sentence pairs in which phrase pairs are replaced with phrase token pairs \u3008T s1 , T t1\u3009, \u3008T s2 , T t2\u3009, and so on.\nIn the step 1 of Figure 2, source-target phrase pairs that contain at least one out-ofvocabulary word are selected from the training set using the branching entropy approach described in Section 3.2. As shown in the step 2 of Figure 2, in each of the parallel patent sentence pairs, occurrences of phrase pairs \u3008ts1, tt1\u3009, \u3008ts2, tt2\u3009, . . ., \u3008tsk, ttk\u3009 are then replaced with token pairs \u3008T s1 , T t1\u3009, \u3008T s2 , T t2\u3009, . . ., \u3008T sk , T tk\u3009. Phrase pairs \u3008ts1, tt1\u3009, \u3008ts2, tt2\u3009, . . ., \u3008tsk, ttk\u3009 are numbered in the order of occurrence of the source phrases ts1 (i = 1, 2, . . . , k) in each source sentence Ss. Here note that in all the parallel sentence pairs \u3008Ss, St\u3009, the tokens pairs \u3008T s1 , T t1\u3009, \u3008T s2 , T t2\u3009, . . . that are identical throughout all the parallel sentence pairs are used in this procedure. Therefore, for example, in all the source patent sentences Ss, the phrase ts1 which appears earlier than other phrases in Ss is replaced with T s1 . We then train the NMT model on a bilingual corpus, in which the phrase pairs are replaced by token pairs \u3008T si , T ti \u3009 (i = 1, 2, . . .), and obtain an NMT model in which the phrases are represented as tokens.5\n4 We sampled 200 Japanese-Chinese sentence pairs, manually annotated compounds and evaluated the approach of phrase extraction with the branching entropy. Based on the result, (a) 25% of them are correct, (b) 20% subsume correct compounds as their substrings, (c) 18% are substrings of correct compounds, (d) 22% subsume substrings of correct compounds but other than (b) nor (c), and (e) the remaining 15% are error strings such as functional compounds and fragmental strings consisting of numerical expressions.\n5 We treat the NMT system as a black box, and the strategy we present in this paper could be applied to any NMT system (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al.,"}, {"heading": "4.2 NMT Decoding and SMT Phrase Translation", "text": "Figure 3 illustrates the procedure for producing target translations by decoding the input source sentence using the method proposed in this paper.\nIn the step 1 of Figure 3, when given an input source sentence, we first generate its translation by decoding of SMT translation model. Next, as shown in the step 2 of Figure 3, we automatically extract the phrase pairs by branching entropy according to the procedure of Section 3.2, where the input sentence and its SMT translation are considered as a pair of parallel sentence. Phrase pairs that contains at least one out-of-vocabulary word are extracted and are replaced with phrase token pairs \u3008T si , T ti \u3009 (i = 1, 2, . . .). Consequently, we have an input sentence in which the tokens \u201cT si \u201d (i = 1, 2, . . .) represent the positions of the phrases and a list of SMT phrase translations of extracted Japanese phrases. Next, as shown in the step 3 of Figure 3, the source Japanese sentence with tokens is translated using the NMT model trained according to the procedure described in Section 4.1. Finally, in the step 4, we replace the tokens \u201cT ti \u201d (i = 1, 2, . . .) of the target sentence translation with the phrase translations of the SMT."}, {"heading": "5 Evaluation", "text": ""}, {"heading": "5.1 Patent Documents", "text": "Japanese-Chinese parallel patent documents were collected from the Japanese patent documents published by the Japanese Patent Office (JPO) during 2004-2012 and the Chinese patent documents published by the State Intellectual Property Office of the People\u2019s Republic of China (SIPO) during 2005-2010. From the collected documents, we extracted 312,492 patent families, and the method of Utiyama and Isahara (2007) was applied6 to the text of the extracted patent families to align the Japanese and Chinese sentences. The Japanese sentences were segmented into a sequence of morphemes using the Japanese morphological analyzer MeCab7 with\n2014). 6 Herein, we used a Japanese-Chinese translation lexicon comprising around 170,000 Chinese entries. 7 http://mecab.sourceforge.net/\nthe morpheme lexicon IPAdic,8 and the Chinese sentences were segmented into a sequence of words using the Chinese morphological analyzer Stanford Word Segment (Tseng et al., 2005) trained using the Chinese Penn Treebank. In this study, Japanese-Chinese parallel patent sentence pairs were ordered in descending order of sentence-alignment score and we used the topmost 2.8M pairs, whose Japanese sentences contain fewer than 40 morphemes and Chinese sentences contain fewer than 40 words.9\nJapanese-English patent documents are provided in the NTCIR-7 workshop (Fujii et al., 2008), which are collected from the 10 years of unexamined Japanese patent applications published by the Japanese Patent Office (JPO) and the 10 years patent grant data published by the U.S. Patent & Trademark Office (USPTO) in 1993-2000. The numbers of documents are approximately 3,500,000 for Japanese and 1,300,000 for English. From these document sets, patent families are automatically extracted and the fields of \u201cBackground of the Invention\u201d and \u201cDetailed Description of the Preferred Embodiments\u201d are selected. Then, the method of Utiyama and Isahara (2007) is applied to the text of those fields, and Japanese and English sentences are aligned. The Japanese sentences were segmented into a sequence of morphemes using the Japanese morphological analyzer MeCab with the morpheme lexicon IPAdic. Similar to the case of Japanese-Chinese patent documents, in this study, out of the provided 1.8M Japanese-English parallel sentences, 1.1M parallel sentences whose Japanese sentences contain fewer than 40 morphemes and English sentences contain fewer than 40 words are used."}, {"heading": "5.2 Training and Test Sets", "text": "We evaluated the effectiveness of the proposed NMT model at translating parallel patent sentences described in Section 5.1. Among the selected parallel sentence pairs, we randomly extracted 1,000 sentence pairs for the test set and 1,000 sentence pairs for the validation set; the remaining sentence pairs were used for the training set. Table 1 shows statistics of the datasets.\nAccording to the procedure of Section 3.2, from the Japanese-Chinese sentence pairs of the training set, we collected 426,551 occurrences of Japanese-Chinese phrase pairs, which\n8 http://sourceforge.jp/projects/ipadic/ 9 It is expected that the proposed NMT model can improve the baseline NMT without the proposed technique when translating longer sentences that contain more than 40 morphemes / words. It is because the approach of replacing phrases with tokens also shortens the input sentences, expected to contribute to solving the weakness of NMT model when translating long sentences.\nare 254,794 types of phrase pairs with 171,757 unique types of Japanese phrases and 129,071 unique types of Chinese phrases. Within the total 1,000 Japanese patent sentences in the Japanese-Chinese test set, 121 occurrences of Japanese phrases were extracted, which correspond to 120 types. With the total 1,000 Chinese patent sentences in the Japanese-Chinese test set, 130 occurrences of Chinese phrases were extracted, which correspond to 130 types.\nFrom the Japanese-English sentence pairs of the training set, we collected 70,943 occurrences of Japanese-English phrase pairs, which are 61,017 types of phrase pairs with unique 57,675 types of Japanese phrases and 58,549 unique types of English phrases. Within the total 1,000 Japanese patent sentences in the Japanese-English test set, 59 occurrences of Japanese phrases were extracted, which correspond to 59 types. With the total 1,000 English patent sentences in the Japanese-English test set, 61 occurrences of English phrases were extracted, which correspond to 61 types."}, {"heading": "5.3 Training Details", "text": "For the training of the SMT model, including the word alignment and the phrase translation table, we used Moses (Koehn et al., 2007), a toolkit for phrase-based SMT models. We trained the SMT model on the training set and tuned it with the validation set.\nFor the training of the NMT model, our training procedure and hyperparameter choices were similar to those of Bahdanau et al. (2015). The encoder consists of forward and backward deep LSTM neural networks each consisting of three layers, with 512 cells in each layer. The decoder is a three-layer deep LSTM with 512 cells in each layer. Both the source vocabulary and the target vocabulary are limited to the 40K most-frequently used morphemes / words in the training set. The size of the word embedding was set to 512. We ensured that all sentences in a minibatch were roughly the same length. Further training details are given below: (1) We set the size of a minibatch to 128. (2) All of the LSTM\u2019s parameter were initialized with a uniform distribution ranging between -0.06 and 0.06. (3) We used the stochastic gradient descent, beginning at a fixed learning rate of 1. We trained our model for a total of 10 epochs, and we began to halve the learning rate every epoch after the first seven epochs. (4) Similar to Sutskever et al. (2014), we rescaled the normalized gradient to ensure that its norm does not exceed 5. We trained the NMT model on the training set. The training time was around two days when using the described parameters on a 1-GPU machine.\nWe compute the branching entropy using the frequency statistics from the training set."}, {"heading": "5.4 Evaluation Results", "text": "In this work, we calculated automatic evaluation scores for the translation results using a popular metrics called BLEU (Papineni et al., 2002). As shown in Table 2, we report the evaluation scores, using the translations by Moses (Koehn et al., 2007) as the baseline SMT and the scores using the translations produced by the baseline NMT system without our proposed approach as the baseline NMT. As shown in Table 2, the BLEU score obtained by the proposed NMT\nmodel is clearly higher than those of the baselines. Here, as described in Section 3, the lower bounds of branching entropy for phrase pair selection are tuned as 5 throughout the evaluation of language pair of Japanese-Chinese, and tuned as 8 throughout the evaluation of language pair of Japanese-English, respectively. When compared with the baseline SMT, the performance gains of the proposed system are approximately 5.2 BLEU points when translating Japanese into Chinese and 7.1 BLEU when translating Chinese into Japanese. When compared with the baseline SMT, the performance gains of the proposed system are approximately 10.0 BLEU points when translating Japanese into English and 10.8 BLEU when translating English into Japanese. When compared with the result of the baseline NMT, the proposed NMT model achieved performance gains of 1.2 BLEU points on the task of translating Japanese into Chinese and 1.7 BLEU points on the task of translating Chinese into Japanese. When compared with the result of the baseline NMT, the proposed NMT model achieved performance gains of 0.4 BLEU points on the task of translating Japanese into English and 1.4 BLEU points on the task of translating English into Japanese.\nFurthermore, we quantitatively compared our study with the work of Luong et al. (2015b). Table 2 compares the NMT model with the PosUnk model, which is the best model proposed by Luong et al. (2015b). The proposed NMT model achieves performance gains of 0.8 BLEU points when translating Japanese into Chinese, and performance gains of 1.3 BLEU points when translating Chinese into Japanese. The proposed NMT model achieves performance gains of 0.2 BLEU points when translating Japanese into English, and performance gains of 1.0 BLEU points when translating English into Japanese.\nWe also compared our study with the work of Long et al. (2016). As reported in Long et al. (2017), when translating Japanese into Chinese, the BLEU of the NMT system of Long et al. (2016) in which all the selected compound nouns are replaced with tokens is 58.6, the BLEU of the NMT system in which only compound nouns that contain out-of-vocabulary words are selected and replaced with tokens is 57.4, while the BLEU of the proposed NMT system of this paper is 57.7. Out of all the selected compound nouns of Long et al. (2016), around 22% contain out-of-vocabulary words, of which around 36% share substrings with the phrases selected by branching entropy. The remaining 78% compound nouns do not contain out-of-vocabulary words and are considered to contribute to the improvement of BLEU points compared with the proposed method. Based on this analysis, as one of our important future work, we revise the\nprocedure in Section 3.2 of selecting phrases by branching entropy and then incorporate those in-vocabulary compound nouns into the set of the phrases selected by the branching entropy.\nIn this study, we also conducted two types of human evaluations according to the work of Nakazawa et al. (2015): pairwise evaluation and JPO adequacy evaluation. In the pairwise evaluation, we compared each translation produced by the baseline NMT with that produced by the proposed NMT model as well as the NMT model with PosUnk model, and judged which translation is better or whether they have comparable quality. The score of the pairwise evaluation is defined as below:\nscore = 100\u00d7 W \u2212 L W + L+ T\nwhere W, L, and T are the numbers of translations that are better than, worse than, and comparable to the baseline NMT, respectively. The score of pairwise evaluation ranges from \u2212100 to 100. In the JPO adequacy evaluation, Chinese translations are evaluated according to the\nquality evaluation criterion for translated patent documents proposed by the Japanese Patent Office (JPO).10 The JPO adequacy criterion judges whether or not the technical factors and their relationships included in Japanese patent sentences are correctly translated into Chinese. The Chinese translations are then scored according to the percentage of correctly translated information, where a score of 5 means all of those information are translated correctly, while a score of 1 means that most of those information are not translated correctly. The score of the JPO adequacy evaluation is defined as the average over all the test sentences. In contrast to the study conducted by Nakazawa et al. (2015), we randomly selected 200 sentence pairs from the test set for human evaluation, and both human evaluations were conducted using only one judgement. Table 3 and Table 4 shows the results of the human evaluation for the baseline SMT, baseline NMT, NMT model with PosUnk model, and the proposed NMT model. We observe that the proposed model achieves the best performance for both the pairwise and JPO adequacy evaluations when we replace the tokens with SMT phrase translations after decoding the source sentence with the tokens.\nFor the test set, we also counted the numbers of the untranslated words of input sentences. As shown in Table 5, the number of untranslated words by the baseline NMT reduced to around 50% in the cases of ja\u2192 ch and ch\u2192 ja by the proposed NMT model, and reduced to around 60% in the cases of ja\u2192 en and en\u2192 ja.11 12 This is mainly because part of untranslated source words are out-of-vocabulary, and thus are untranslated by the baseline NMT. The proposed system extracts those out-of-vocabulary words as a part of phrases and replaces those phrases with tokens before the decoding of NMT. Those phrases are then translated by SMT and inserted in the output translation, which ensures that those out-of-vocabulary words are translated.\n10 https://www.jpo.go.jp/shiryou/toushin/chousa/pdf/tokkyohonyaku_hyouka/01. pdf (in Japanese) 11 Although we omit the detail of the evaluation results of untranslated words of the NMT model with PosUnk model (Luong et al., 2015b) in Table 5, the number of the untranslated words of the NMT model with PosUnk model is almost the same as that of the baseline NMT, which is much more than that of the proposed NMT model.\n12 Following the result of an additional evaluation where having approximately similar size of the training parallel sentences between the language pairs of Japanese-to-Chinese/Chinese-to-Japanese and Japanese-to-English/English-toJapanese, we concluded that the primary reason why the numbers of untranslated morphemes / words tend to be much larger in the case of the language pair of Japanese-to-English/English-to-Japanese than in the case of the language pair of Japanese-to-Chinese/Chinese-to-Japanese is simply the matter of a language specific issue.\nFigure 4 compares an example of correct translation produced by the proposed system with one produced by the baseline NMT. In this example, the translation is a translation error because the Japanese word \u201c (Bridgman)\u201d is an out-of-vocabulary word and is erroneously translated into the \u201c\u3008unk\u3009\u201d token. The proposed NMT model correctly translated the Japanese sentence into Chinese, where the out-of-vocabulary word \u201c \u201d is correctly selected by the approach of branching entropy as a part of the Japanese phrase \u201c (vertical Bridgman method)\u201d. The selected Japanese phrase is then translated by the phrase translation table of SMT. Figure 5 shows another example of correct translation produced by the proposed system with one produced by the baseline NMT. As shown in Figure 5, the translation produced by baseline NMT is a translation error because the out-of-vocabulary Chinese word \u201c (band pattern)\u201d is an untranslated word and its translation is not contained in the output translation of the baseline NMT. The proposed NMT model correctly translated the Chinese word into Japanese because the Chinese word \u201c (band pattern)\u201dis selected as a part of Chinese phrase \u201c (typical band pattern)\u201d with branching entropy and then is translated by SMT. Moreover, Figure 6 and Figure 7 compare examples of correct translations produced by the proposed system with those produced by the baseline NMT when translating patent sentences in both directions of Japanese-to-English and English-to-Japanese."}, {"heading": "6 Conclusion", "text": "This paper proposed selecting phrases that contain out-of-vocabulary words using the branching entropy. These selected phrases are then replaced with tokens and post-translated using an SMT phrase translation. Compared with the method of Long et al. (2016), the contribution of the proposed NMT model is that it can be used on any language pair without language-specific knowledge for technical terms selection. We observed that the proposed NMT model performed much better than the baseline NMT system in all of the language pairs: Japanese-to-Chinese/Chineseto-Japanese and Japanese-to-English/English-to-Japanese. One of our important future tasks is to compare the translation performance of the proposed NMT model with that based on subword units (e.g. Sennrich et al. (2016)). Another future task is to improve the performance of the present study by incorporating the in-vocabulary non-compositional phrases, whose translations cannot be obtained by translating their constituent words. It is expected to achieve a better translation performance by translating those kinds of phrases using a phrase-based SMT instead of using NMT."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "Proc. 3rd ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Automatic key term extraction from spoken course lectures using branching entropy and prosodic/semantic features", "author": ["Y. Chen", "Y. Huang", "S. Kong", "L. Lee"], "venue": "Proc. 2010 IEEE SLT Workshop, pages 265\u2013270.", "citeRegEx": "Chen et al\\.,? 2010", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merri\u00ebnboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proc. EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Character-based neural machine translation", "author": ["M.R. Costa-Juss\u00e0", "J.A.R. Fonollosa"], "venue": "Proc. 54th ACL, pages 357\u2013361.", "citeRegEx": "Costa.Juss\u00e0 and Fonollosa,? 2016", "shortCiteRegEx": "Costa.Juss\u00e0 and Fonollosa", "year": 2016}, {"title": "Toward the evaluation of machine translation using patent information", "author": ["A. Fujii", "M. Utiyama", "M. Yamamoto", "T. Utsuro"], "venue": "Proc. 8th AMTA, pages 97\u2013106.", "citeRegEx": "Fujii et al\\.,? 2008", "shortCiteRegEx": "Fujii et al\\.", "year": 2008}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "Y. Bengio", "R. Memisevic"], "venue": "Proc. 28th NIPS, pages 1\u201310.", "citeRegEx": "Jean et al\\.,? 2014", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Unsupervised segmentation of Chinese text by use of branching entropy", "author": ["Z. Jin", "K. Tanaka-Ishii"], "venue": "Proc. COLING/ACL 2006, pages 428\u2013435.", "citeRegEx": "Jin and Tanaka.Ishii,? 2006", "shortCiteRegEx": "Jin and Tanaka.Ishii", "year": 2006}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "Proc. EMNLP, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proc. 45th ACL, Companion Volume, pages 177\u2013180.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Towards zero unknown word in neural machine translation", "author": ["X. Li", "J. Zhang", "C. Zong"], "venue": "Proc. 25th IJCAI, pages 2852\u20132858.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Translation of patent sentences with a large vocabulary of technical terms using neural machine translation", "author": ["Z. Long", "T. Utsuro", "T. Mitsuhashi", "M. Yamamoto"], "venue": "Proc. 3rd WAT, pages 47\u201357.", "citeRegEx": "Long et al\\.,? 2016", "shortCiteRegEx": "Long et al\\.", "year": 2016}, {"title": "Neural machine translation model with a large vocabulary selected by branching entropy", "author": ["Z. Long", "T. Utsuro", "T. Mitsuhashi", "M. Yamamoto"], "venue": "https://arxiv.org/ abs/1704.04520v4. Online; accessed 24-July-2017.", "citeRegEx": "Long et al\\.,? 2017", "shortCiteRegEx": "Long et al\\.", "year": 2017}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["M. Luong", "C.D. Manning"], "venue": "Proc. 54th ACL, pages 1054\u20131063.", "citeRegEx": "Luong and Manning,? 2016", "shortCiteRegEx": "Luong and Manning", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M. Luong", "H. Pham", "C.D. Manning"], "venue": "Proc. EMNLP, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["M. Luong", "I. Sutskever", "O. Vinyals", "Q.V. Le", "W. Zaremba"], "venue": "Proc. 53rd ACL, pages 11\u201319.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Overview of the 2nd workshop on Asian translation", "author": ["T. Nakazawa", "H. Mino", "I. Goto", "G. Neubig", "S. Kurohashi", "E. Sumita"], "venue": "Proc. 2nd WAT, pages 1\u201328.", "citeRegEx": "Nakazawa et al\\.,? 2015", "shortCiteRegEx": "Nakazawa et al\\.", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J."], "venue": "Proc. 40th ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "Proc. 54th ACL, pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural machine translation", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Proc. 27th NIPS, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A conditional random field word segmenter for Sighan bakeoff 2005", "author": ["H. Tseng", "P. Chang", "G. Andrew", "D. Jurafsky", "C. Manning"], "venue": "Proc. 4th SIGHAN Workshop on Chinese Language Processing, pages 168\u2013171.", "citeRegEx": "Tseng et al\\.,? 2005", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "Modeling coverage for neural machine translation", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li"], "venue": "Proc. ACL 2016, pages 76\u201385.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "A Japanese-English patent parallel corpus", "author": ["M. Utiyama", "H. Isahara"], "venue": "Proc. MT Summit XI, pages 475\u2013482.", "citeRegEx": "Utiyama and Isahara,? 2007", "shortCiteRegEx": "Utiyama and Isahara", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Bahdanau et al., 2015; Cho et al., 2014; Jean et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a,b; Sutskever et al., 2014).", "startOffset": 112, "endOffset": 251}, {"referenceID": 2, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Bahdanau et al., 2015; Cho et al., 2014; Jean et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a,b; Sutskever et al., 2014).", "startOffset": 112, "endOffset": 251}, {"referenceID": 6, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Bahdanau et al., 2015; Cho et al., 2014; Jean et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a,b; Sutskever et al., 2014).", "startOffset": 112, "endOffset": 251}, {"referenceID": 8, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Bahdanau et al., 2015; Cho et al., 2014; Jean et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a,b; Sutskever et al., 2014).", "startOffset": 112, "endOffset": 251}, {"referenceID": 19, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Bahdanau et al., 2015; Cho et al., 2014; Jean et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a,b; Sutskever et al., 2014).", "startOffset": 112, "endOffset": 251}, {"referenceID": 5, "context": "Jean et al. (2014) provided an efficient approximation to the softmax function to accommodate a very large vocabulary in an NMT system.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Jean et al. (2014) provided an efficient approximation to the softmax function to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of the out-of-vocabulary token in the target sentence with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy.", "startOffset": 0, "endOffset": 157}, {"referenceID": 5, "context": "Jean et al. (2014) provided an efficient approximation to the softmax function to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of the out-of-vocabulary token in the target sentence with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed replacing outof-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data.", "startOffset": 0, "endOffset": 431}, {"referenceID": 5, "context": "Jean et al. (2014) provided an efficient approximation to the softmax function to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of the out-of-vocabulary token in the target sentence with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed replacing outof-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data. Sennrich et al. (2016) introduced an effective approach based on encoding rare and out-of-vocabulary words as sequences of subword units.", "startOffset": 0, "endOffset": 587}, {"referenceID": 5, "context": "Jean et al. (2014) provided an efficient approximation to the softmax function to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of the out-of-vocabulary token in the target sentence with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed replacing outof-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data. Sennrich et al. (2016) introduced an effective approach based on encoding rare and out-of-vocabulary words as sequences of subword units. Luong and Manning (2016) provided a character-level and word-level hybrid NMT model to achieve an open vocabulary, and Costa-Juss\u00e0 and Fonollosa (2016) proposed an NMT system that uses character-based embeddings.", "startOffset": 0, "endOffset": 727}, {"referenceID": 3, "context": "Luong and Manning (2016) provided a character-level and word-level hybrid NMT model to achieve an open vocabulary, and Costa-Juss\u00e0 and Fonollosa (2016) proposed an NMT system that uses character-based embeddings.", "startOffset": 119, "endOffset": 152}, {"referenceID": 3, "context": "Luong and Manning (2016) provided a character-level and word-level hybrid NMT model to achieve an open vocabulary, and Costa-Juss\u00e0 and Fonollosa (2016) proposed an NMT system that uses character-based embeddings. However, these previous approaches have limitations when translating patent sentences. This is because their methods only focus on addressing the problem of out-of-vocabulary words even though the words are parts of technical terms. It is obvious that a technical term should be considered as one word that comprises components that always have different meanings and translations when they are used alone. An example is shown in Figure 1, where the Japanese word \u201c \u201d(bridge) should be translated to Chinese word \u201c \u201d when included in technical term \u201cbridge interface\u201d; however, it is always translated as \u201c \u201d. To address this problem, Long et al. (2016) proposed extracting compound nouns as technical terms and replacing them with tokens.", "startOffset": 119, "endOffset": 867}, {"referenceID": 3, "context": "Luong and Manning (2016) provided a character-level and word-level hybrid NMT model to achieve an open vocabulary, and Costa-Juss\u00e0 and Fonollosa (2016) proposed an NMT system that uses character-based embeddings. However, these previous approaches have limitations when translating patent sentences. This is because their methods only focus on addressing the problem of out-of-vocabulary words even though the words are parts of technical terms. It is obvious that a technical term should be considered as one word that comprises components that always have different meanings and translations when they are used alone. An example is shown in Figure 1, where the Japanese word \u201c \u201d(bridge) should be translated to Chinese word \u201c \u201d when included in technical term \u201cbridge interface\u201d; however, it is always translated as \u201c \u201d. To address this problem, Long et al. (2016) proposed extracting compound nouns as technical terms and replacing them with tokens. These compound nouns then are post-translated with the phrase translation table of the statistical machine translation (SMT) system. However, in their work on Japanese-to-Chinese patent translation, Japanese compound nouns are identified using several heuristic rules that use specific linguistic knowledge based on part-of-speech tags of morphological analysis of Japanese language, and thus, the NMT system has limited application to the translation task of other language pairs. In this paper, based on the approach of training an NMT model on a bilingual corpus wherein technical term pairs are replaced with tokens as in Long et al. (2016), we aim to select phrase pairs using the statistical approach of branching entropy; this allows the proposed technique to be applied to the translation task on any language pair without needing specific language knowledge to formulate the rules for technical term identification.", "startOffset": 119, "endOffset": 1598}, {"referenceID": 0, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al., 2014).", "startOffset": 89, "endOffset": 207}, {"referenceID": 2, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al., 2014).", "startOffset": 89, "endOffset": 207}, {"referenceID": 8, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al., 2014).", "startOffset": 89, "endOffset": 207}, {"referenceID": 14, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al., 2014).", "startOffset": 89, "endOffset": 207}, {"referenceID": 19, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Luong et al., 2015a; Sutskever et al., 2014).", "startOffset": 89, "endOffset": 207}, {"referenceID": 5, "context": "(2015), which consists of an encoder of a bidirectional long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and another LSTM as decoder.", "startOffset": 86, "endOffset": 120}, {"referenceID": 0, "context": "In this paper, we use an NMT model similar to that used by Bahdanau et al. (2015), which consists of an encoder of a bidirectional long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and another LSTM as decoder.", "startOffset": 59, "endOffset": 82}, {"referenceID": 0, "context": "In this paper, we use an NMT model similar to that used by Bahdanau et al. (2015), which consists of an encoder of a bidirectional long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and another LSTM as decoder. In the model of Bahdanau et al. (2015), the encoder consists of forward and backward LSTMs.", "startOffset": 59, "endOffset": 264}, {"referenceID": 1, "context": ", (Chen et al., 2010)).", "startOffset": 2, "endOffset": 21}, {"referenceID": 21, "context": "Tu et al. (2016) proposed coverage-based NMT which considers the problem of the under-translation.", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "Given a parallel sentence pair \u3008Ss, St\u3009, all n-grams phrases of source sentence Ss and target sentence St are extracted and aligned using phrase translation table and word alignment of SMT according to the approaches described in Long et al. (2016). Next, phrase translation pair \u3008ts, tt\u3009 obtained from \u3008Ss, St\u3009 that satisfies all the following conditions is selected as a phrase pair and is extracted:", "startOffset": 230, "endOffset": 249}, {"referenceID": 14, "context": "2 One of the major focus of this paper is the comparison between the proposed method and Luong et al. (2015b). Since Luong et al.", "startOffset": 89, "endOffset": 110}, {"referenceID": 14, "context": "2 One of the major focus of this paper is the comparison between the proposed method and Luong et al. (2015b). Since Luong et al. (2015b) proposed to pre-process and post-translate only out-of-vocabulary words, we focus only on", "startOffset": 89, "endOffset": 138}, {"referenceID": 22, "context": "From the collected documents, we extracted 312,492 patent families, and the method of Utiyama and Isahara (2007) was applied6 to the text of the extracted patent families to align the Japanese and Chinese sentences.", "startOffset": 86, "endOffset": 113}, {"referenceID": 9, "context": "Baseline SMT (Koehn et al., 2007) 52.", "startOffset": 13, "endOffset": 33}, {"referenceID": 15, "context": "9 (Luong et al., 2015b) NMT with phrase translation by SMT (phrase 57.", "startOffset": 2, "endOffset": 23}, {"referenceID": 20, "context": "the morpheme lexicon IPAdic,8 and the Chinese sentences were segmented into a sequence of words using the Chinese morphological analyzer Stanford Word Segment (Tseng et al., 2005) trained using the Chinese Penn Treebank.", "startOffset": 159, "endOffset": 179}, {"referenceID": 4, "context": "9 Japanese-English patent documents are provided in the NTCIR-7 workshop (Fujii et al., 2008), which are collected from the 10 years of unexamined Japanese patent applications published by the Japanese Patent Office (JPO) and the 10 years patent grant data published by the U.", "startOffset": 73, "endOffset": 93}, {"referenceID": 4, "context": "9 Japanese-English patent documents are provided in the NTCIR-7 workshop (Fujii et al., 2008), which are collected from the 10 years of unexamined Japanese patent applications published by the Japanese Patent Office (JPO) and the 10 years patent grant data published by the U.S. Patent & Trademark Office (USPTO) in 1993-2000. The numbers of documents are approximately 3,500,000 for Japanese and 1,300,000 for English. From these document sets, patent families are automatically extracted and the fields of \u201cBackground of the Invention\u201d and \u201cDetailed Description of the Preferred Embodiments\u201d are selected. Then, the method of Utiyama and Isahara (2007) is applied to the text of those fields, and Japanese and English sentences are aligned.", "startOffset": 74, "endOffset": 655}, {"referenceID": 15, "context": "5 (Luong et al., 2015b) NMT with phrase translation by SMT (phrase 14.", "startOffset": 2, "endOffset": 23}, {"referenceID": 9, "context": "For the training of the SMT model, including the word alignment and the phrase translation table, we used Moses (Koehn et al., 2007), a toolkit for phrase-based SMT models.", "startOffset": 112, "endOffset": 132}, {"referenceID": 0, "context": "For the training of the NMT model, our training procedure and hyperparameter choices were similar to those of Bahdanau et al. (2015). The encoder consists of forward and backward deep LSTM neural networks each consisting of three layers, with 512 cells in each layer.", "startOffset": 110, "endOffset": 133}, {"referenceID": 0, "context": "For the training of the NMT model, our training procedure and hyperparameter choices were similar to those of Bahdanau et al. (2015). The encoder consists of forward and backward deep LSTM neural networks each consisting of three layers, with 512 cells in each layer. The decoder is a three-layer deep LSTM with 512 cells in each layer. Both the source vocabulary and the target vocabulary are limited to the 40K most-frequently used morphemes / words in the training set. The size of the word embedding was set to 512. We ensured that all sentences in a minibatch were roughly the same length. Further training details are given below: (1) We set the size of a minibatch to 128. (2) All of the LSTM\u2019s parameter were initialized with a uniform distribution ranging between -0.06 and 0.06. (3) We used the stochastic gradient descent, beginning at a fixed learning rate of 1. We trained our model for a total of 10 epochs, and we began to halve the learning rate every epoch after the first seven epochs. (4) Similar to Sutskever et al. (2014), we rescaled the normalized gradient to ensure that its norm does not exceed 5.", "startOffset": 110, "endOffset": 1043}, {"referenceID": 17, "context": "In this work, we calculated automatic evaluation scores for the translation results using a popular metrics called BLEU (Papineni et al., 2002).", "startOffset": 120, "endOffset": 143}, {"referenceID": 9, "context": "As shown in Table 2, we report the evaluation scores, using the translations by Moses (Koehn et al., 2007) as the baseline SMT and the scores using the translations produced by the baseline NMT system without our proposed approach as the baseline NMT.", "startOffset": 86, "endOffset": 106}, {"referenceID": 9, "context": "Baseline SMT (Koehn et al., 2007) 3.", "startOffset": 13, "endOffset": 33}, {"referenceID": 15, "context": "2 (Luong et al., 2015b) NMT with phrase translation by SMT (phrase 4.", "startOffset": 2, "endOffset": 23}, {"referenceID": 12, "context": "Furthermore, we quantitatively compared our study with the work of Luong et al. (2015b). Table 2 compares the NMT model with the PosUnk model, which is the best model proposed by Luong et al.", "startOffset": 67, "endOffset": 88}, {"referenceID": 12, "context": "Furthermore, we quantitatively compared our study with the work of Luong et al. (2015b). Table 2 compares the NMT model with the PosUnk model, which is the best model proposed by Luong et al. (2015b). The proposed NMT model achieves performance gains of 0.", "startOffset": 67, "endOffset": 200}, {"referenceID": 11, "context": "We also compared our study with the work of Long et al. (2016). As reported in Long et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 11, "context": "We also compared our study with the work of Long et al. (2016). As reported in Long et al. (2017), when translating Japanese into Chinese, the BLEU of the NMT system of Long et al.", "startOffset": 44, "endOffset": 98}, {"referenceID": 11, "context": "We also compared our study with the work of Long et al. (2016). As reported in Long et al. (2017), when translating Japanese into Chinese, the BLEU of the NMT system of Long et al. (2016) in which all the selected compound nouns are replaced with tokens is 58.", "startOffset": 44, "endOffset": 188}, {"referenceID": 11, "context": "We also compared our study with the work of Long et al. (2016). As reported in Long et al. (2017), when translating Japanese into Chinese, the BLEU of the NMT system of Long et al. (2016) in which all the selected compound nouns are replaced with tokens is 58.6, the BLEU of the NMT system in which only compound nouns that contain out-of-vocabulary words are selected and replaced with tokens is 57.4, while the BLEU of the proposed NMT system of this paper is 57.7. Out of all the selected compound nouns of Long et al. (2016), around 22% contain out-of-vocabulary words, of which around 36% share substrings with the phrases selected by branching entropy.", "startOffset": 44, "endOffset": 529}, {"referenceID": 16, "context": "In this study, we also conducted two types of human evaluations according to the work of Nakazawa et al. (2015): pairwise evaluation and JPO adequacy evaluation.", "startOffset": 89, "endOffset": 112}, {"referenceID": 16, "context": "In contrast to the study conducted by Nakazawa et al. (2015), we randomly selected 200 sentence pairs from the test set for human evaluation, and both human evaluations were conducted using only one judgement.", "startOffset": 38, "endOffset": 61}, {"referenceID": 15, "context": "model (Luong et al., 2015b) in Table 5, the number of the untranslated words of the NMT model with PosUnk model is almost the same as that of the baseline NMT, which is much more than that of the proposed NMT model.", "startOffset": 6, "endOffset": 27}, {"referenceID": 11, "context": "Compared with the method of Long et al. (2016), the contribution of the proposed NMT model is that it can be used on any language pair without language-specific knowledge for technical terms selection.", "startOffset": 28, "endOffset": 47}, {"referenceID": 11, "context": "Compared with the method of Long et al. (2016), the contribution of the proposed NMT model is that it can be used on any language pair without language-specific knowledge for technical terms selection. We observed that the proposed NMT model performed much better than the baseline NMT system in all of the language pairs: Japanese-to-Chinese/Chineseto-Japanese and Japanese-to-English/English-to-Japanese. One of our important future tasks is to compare the translation performance of the proposed NMT model with that based on subword units (e.g. Sennrich et al. (2016)).", "startOffset": 28, "endOffset": 571}], "year": 2017, "abstractText": "Neural machine translation (NMT), a new approach to machine translation, has achieved promising results comparable to those of traditional approaches such as statistical machine translation (SMT). Despite its recent success, NMT cannot handle a larger vocabulary because the training complexity and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently. In this paper, we propose to select phrases that contain out-of-vocabulary words using the statistical approach of branching entropy. This allows the proposed NMT system to be applied to a translation task of any language pair without any language-specific knowledge about technical term identification. The selected phrases are then replaced with tokens during training and post-translated by the phrase translation table of SMT. Evaluation on Japanese-to-Chinese, Chinese-to-Japanese, Japanese-to-English and English-to-Japanese patent sentence translation proved the effectiveness of phrases selected with branching entropy, where the proposed NMT model achieves a substantial improvement over a baseline NMT model without our proposed technique. Moreover, the number of translation errors of under-translation by the baseline NMT model without our proposed technique reduces to around half by the proposed NMT model.", "creator": "LaTeX with hyperref package"}}}