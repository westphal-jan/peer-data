{"id": "1704.05179", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine", "abstract": "we publicly release a new large - scale dataset, titled searchqa, for machine comprehension, or question - answering. unlike recently released python, let be deepmind cnn / messenger and squad, the proposed searchqa was considered to reflect a full cycle of experimental question - answering. that is, we start not from an existing article and generate its question - answer pair, yet start from own existing question - answer pair, run from j! s, and augment it with text snippets retrieved by google. following this approach, we built searchqa, which consists of more than 140k question - answer pairs with each pair having 49. 6 snippets on average. each question - answer - context portion of the searchqa comes with additional meta - data such even matching snippet'07 url, which we believe will be valuable resources for future collaboration. we offer human evaluation as well as linear response baseline methods, one simple word sorting and the other deep learning based, on the searchqa. we show that there is a meaningful gap shaping the human and machine performances. this suggesting that the proposed dataset could well serve as a benchmark for question - answering.", "histories": [["v1", "Tue, 18 Apr 2017 02:42:17 GMT  (154kb,D)", "https://arxiv.org/abs/1704.05179v1", null], ["v2", "Thu, 4 May 2017 14:07:21 GMT  (154kb,D)", "http://arxiv.org/abs/1704.05179v2", null], ["v3", "Sun, 11 Jun 2017 11:51:06 GMT  (154kb,D)", "http://arxiv.org/abs/1704.05179v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["matthew dunn", "levent sagun", "mike higgins", "v ugur guney", "volkan cirik", "kyunghyun cho"], "accepted": false, "id": "1704.05179"}, "pdf": {"name": "1704.05179.pdf", "metadata": {"source": "CRF", "title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine", "authors": ["Matt Dunn", "Levent Sagun", "Mike Higgins", "Volkan Cirik"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "One of the driving forces behind the recent success of deep learning in challenging tasks, such as object recognition (Krizhevsky et al., 2012), speech recognition (Xiong et al., 2016) and machine translation (Bahdanau et al., 2014), has been the increasing availability of large-scale annotated data.\nThis observation has also led to the interest in building a large-scale annotated dataset for question-answering. In 2015, Bordes et al. (2015) released a large-scale dataset of 100k open-world question-answer pairs constructed from Freebase, and Hermann et al. (2015) released two datasets, each consisting of closed-world question-answer pairs automatically generated from news articles. The latter was followed by Hill et al. (2015), Rajpurkar et al. (2016) and Onishi et al. (2016), each of which has released a set of large-scale closedworld question-answer pairs focused on a specific aspect of question-answering.\nLet us first take a step back, and ask what a full end-to-end pipeline for question-answering would look like. A general question-answering system would be able to answer a question about any domain, based on the world knowledge. This system would consist of three stages. A given question is read and reformulated in the first stage, followed by information retrieval via a search engine. An answer is then synthesized based on the query and a set of retrieved documents.\nWe notice a gap between the existing closedworld question-answering data sets and this conceptual picture of a general question-answering system. The general question-answering system must deal with a noisy set of retrieved documents, which likely consist of many irrelevant documents as well as semantically and syntactically illformed documents. On the other hand, most of the existing closed-world question-answering datasets were constructed in a way that the context provided for each question is guaranteed relevant and well-written. This guarantee comes from the fact that each question-answer-context tuple was generated starting from the context from which the question and answer were extracted.\nIn this paper, we build a new closed-world question-answering dataset that narrows this gap.\nar X\niv :1\n70 4.\n05 17\n9v 3\n[ cs\n.C L\n] 1\n1 Ju\nn 20\n17\nUnlike most of the existing work, we start by building a set of question-answer pairs from Jeopardy!. We augment each question-answer pair, which does not have any context attached to it, by querying Google with the question. This process enables us to retrieve a realistic set of relevant/irrelevant documents, or more specifically their snippets. We filter out those questions whose answers could not be found within the retrieved snippets and those with less than forty web pages returned by Google. We end up with 140k+ question-answer pairs, and in total 6.9M snippets.1\nWe evaluate this new dataset, to which we refer as SearchQA, with a variant of recently proposed attention sum reader (Kadlec et al., 2016) and with human volunteers. The evaluation shows that the proposed SearchQA is a challenging task both for humans and machines but there is still a significant gap between them. This suggests that the new dataset would be a valuable resource for further research and advance our ability to build a better automated question-answering system."}, {"heading": "2 SearchQA", "text": "Collection A major goal of the new dataset is to build and provide to the public a machine comprehension dataset that better reflects a noisy information retrieval system. In order to achieve this goal, we need to introduce a natural, realistic noise to the context of each question-answer pair. We use a production-level search engine \u2013Google\u2013 for this purpose.\nWe crawled the entire set of question-answer pairs from J! Archive2 which has archived all the question-answer pairs from the popular television show Jeopardy!. We used the question from each pair to query Google in order to retrieve a set of relevant web page snippets. The relevancy in this case was fully determined by an unknown, but inproduction, algorithm underlying Google\u2019s search engine, making it much closer to a realistic scenario of question-answering.\nCleaning Because we do not have any control over the internals of Google search engine, we extensively cleaned up the entire set of questionanswer-context tuples. First, we removed any snippet returned that included the air-date of the Jeopardy! episode, the exact copy of the question,\n1 The dataset can be found at https://github.com/ nyu-dl/SearchQA.\n2http://j-archive.com\nor a term \u201cJeopardy!\u201d, \u201cquiz\u201d or \u201ctrivia\u201d, to ensure that the answer could not be found trivially by a process of word/phrase matching. Furthermore, we manually checked any URL, from which these removed snippets were taken, that occurs more than 50 times and removed any that explicitly contains Jeopardy! question-answer pairs.\nAmong the remaining question-answer-context tuples, we removed any tuple whose context did not include the answer. This was done mainly for computational efficiency in building a questionanswering system using the proposed dataset. We kept only those tuples whose answers were three or less words long.\nBasic Statistics After all these processes, we have ended up with 140,461 question-answer pairs. Each pair is coupled with a set of 49.6\u00b12.10 snippets on average. Each snippet is 37.3\u00b111.7 tokens long on average. Answers are on average 1.47\u00b10.58 tokens long. There are 1,257,327 unique tokens.\nMeta-Data We collected for each questionanswer-context tuple additional metadata from Jeopardy! and returned by Google. More specifically, from Jeopardy! we have the category, dollar value, show number and air date for each question. From Google, we have the URL, title and a set of related links (often none) for each snippet. Although we do not use them in this paper, these items are included in the public release of SearchQA and may be used in the future. An example of one question-answer pair with just one snippet is presented in Fig. 1.\nTraining, Validation and Test Sets In order to maximize its reusability and reproducibility, we provide a predefined split of the dataset into training, validation and test sets. One of the most important aspects in question-answering is whether a question-answering machine would generalize to unseen questions from the future. We thus ensure that these three sets consist of questionanswer pairs from non-overlapping years, and that\nthe validation and test question-answer pairs are from years later than the training set\u2019s pairs. The training, validation and test sets consist of 99,820, 13,393 and 27,248 examples, respectively. Among these, examples with unigram answers are respectively 55,648, 8,672 and 17,056."}, {"heading": "3 Related Work", "text": "Open-World Question-Answering An openworld question-answering dataset consists of a set of question-answer pairs and the knowledge database. It does not come with an explicit link between each question-answer pair and any specific entry in the knowledge database. A representative example of such a dataset is SimpleQA by (Bordes et al., 2015). SimpleQA consists of 100k questionanswer pairs, and uses Freebase as a knowledge database. The major limitation of this dataset is that all the questions are simple in that all of them are in the form of (subject, relationship, ?).\nClosed-World Question-Answering Although we use open-world snippets, the final SearchQA is a closed-world question-answering dataset since each question can be answered entirely based on the associated snippets. One family of such datasets includes Children\u2019s Book dataset (Hill et al., 2015), CNN and DailyMail (Hermann et al., 2015). Each question-answer-context tuple in these datasets was constructed by first selecting the context article and then creating a questionanswer pair, where the question is a sentence with a missing word and the answer is the missing word. This family differs from SearchQA in two aspects. First, in SearchQA we start from a question-answer pair, and, second, our question is not necessarily of a fill-in-a-word type.\nAnother family is an extension of the former family of datasets. This family includes SQuAD (Rajpurkar et al., 2016) and NEWSQA (Trischler et al., 2016). Unlike the first family, answers in this family are often multiword phrases, and they do not necessarily appear as they are in the corresponding context. In contrast, in SearchQA we ensure that all multi-word phrase answers appear in their corresponding context. Answers, often as well as questions, are thus often crowd-sourced in this family of datasets. Nonetheless, each tuple in these datasets was however also constructed starting from a corresponding context article, making them less realistic than the proposed SearchQA.\nMS MARCO (Nguyen et al., 2016)\u2013the most recently released dataset to our knowledge\u2013 is perhaps most similar to the proposed SearchQA. Nguyen et al. (2016) selected a subset of actual user-generated queries to Microsoft Bing that correspond to questions. These questions are augmented with a manually selected subset of snippets returned by Bing. The question is then answered by a human. Two major differences between MS MARCO and SearchQA are the choice of questions and search engine. We believe the comparison between MS MARCO and the proposed SearchQA would be valuable for expanding our understanding on how the choice of search engines as well as types of questions impact question-answering systems in the future."}, {"heading": "4 Experiments and Results", "text": "As a part of our release of SearchQA, we provide a set of baseline performances against which other researchers may compare their future approaches. Unlike most of the previous datasets, SearchQA augments each question-answer pair with a noisy, real context retrieved from the largest search engine in the world. This implies that the human performance is not necessarily the upper-bound but we nevertheless provide it as a guideline."}, {"heading": "4.1 Human Evaluation", "text": "We designed a web interface that displays a query and retrieved snippets and lets a user select an answer by clicking words on the screen. A user is given up to 40 minutes to answer as many questions as possible. We randomly select questionanswer-context pairs from the test set.\nWe recruited thirteen volunteers from the master\u2019s program in the Center for Data Science at NYU. They were uniform-randomly split into two groups. The first group was presented with questions that have single-word (unigram) answers only, and the other group with questions that have either single-word or multi-word (n-gram) answers. On average, each participant answers 47.23 questions with the standard deviation of 30.42.\nWe report the average and standard deviation of the accuracy achieved by the volunteers in Table 1. We notice the significant gap between the accuracies by the first and second groups, suggesting that the difficulty of question-answering grows as the length of the answer increases. Also, according to the F1 scores, we observe a large gap between the ASR and humans. This suggests the potential for the proposed SearchQA as a benchmark for advancing question-answering research. Overall, we found the performance of human volunteers much lower than expected and suspect the following underlying reasons. First, snippets are noisy, as they are often excerpts not full sentences. Second, human volunteers may have become exhausted over the trial. We leave more detailed analysis of the performance of human subjects on the proposed SearchQA for the future."}, {"heading": "4.2 Machine Baselines", "text": "TF-IDF Max An interesting property of the proposed SearchQA is that the context of each question-answer pair was retrieved by Google with the question as a query. This implies that the information about the question itself may be implicitly embedded in the snippets. We therefore test a naive strategy (TF-IDF Max) of selecting the word with the highest TF-IDF score in the context as an answer. Note that this can only be used for the questions with a unigram answer.\nAttention Sum Reader Attention sum reader (ASR, Kadlec et al., 2016) is a variant of a pointer network (Vinyals et al., 2015) that was specifically constructed to solve a cloze-style questionanswering task. ASR consists of two encoding recurrent networks. The first network encodes a given context c, which is the concatenation of all the snippets in the case of SearchQA, into a set of hidden vectors {hcj}, and the second network encodes a question q into a single vector hq. The dot product between each hidden vector from the context and the question vector is exponentiated to form word scores \u03b2j = exp(hq>hcj). ASR then pulls these word scores by summing the scores of the same word, resulting in a set of unique word scores \u03b2\u2032i = \u2211 j\u2208Di \u03b2j , where Di indicates where the word i appears in the context. These uniqueword scores are normalized, and we obtain an answer distribution p(i|c, q) = \u03b2\u2032i/ \u2211 i\u2032 \u03b2 \u2032 i\u2032 . The ASR is trained to maximize this (log-)probability of the correct answer word in the context.\nThis vanilla ASR only works with a unigram answer and is not suitable for an n-gram answer. We avoid this issue by introducing another recurrent network which encodes the previous answer words (a\u03021, . . . , a\u0302l\u22121) into a vector ha. This vector is added to the question vectors, i.e., hq \u2190 hq+ha. During training, we use the correct previou answer words, while we let the model, called n-gram ASR, predict one answer at a time until it predicts \u3008answer\u3009. This special token, appended to the context, indicates the end of the answer.\nWe try both the vanilla and n-gram ASR\u2019s on the unigram-answer-only subset and on the whole set, respectively. We use recurrent networks with 100 gated recurrent units (GRU, Cho et al., 2014) for both unigram and n-gram models, respectively. We use Adam (Kingma and Ba, 2014) and dropout (Srivastava et al., 2014) for training.\nResult We report the results in Table 2. We see that the attention sum reader is below human evaluation, albeit by a rather small margin. Also, TFIDF Max scores are not on par when compared to ASR which is perhaps not surprising. Given the unstructured nature of SearchQA, we believe improvements on the benchmarks presented are crucial for developing a real-world Q&A system."}, {"heading": "5 Conclusion", "text": "We constructed a new dataset for questionanswering research, called SearchQA. It was built using an in-production, commercial search engine. It closely reflects the full pipeline of a (hypothetical) general question-answering system, which consists of information retrieval and answer synthesis. We conducted human evaluation as well as machine evaluation. Using the latest technique, ASR, we show that there is a meaningful gap between humans and machines, which suggests the potential of SearchQA as a benchmark\ntask for question-answering research. We release SearchQA publicly, including our own implementation of ASR and n-gram ASR in PyTorch.3"}, {"heading": "Acknowledgments", "text": "KC thanks support by Google, NVIDIA, eBay and Facebook. MD conducted this work as a part of DS-GA 1010: Independent Study in Data Science at the Center for Data Science, New York University."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1506.02075 .", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Conference on Empirical Methods in", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1693\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1511.02301 .", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1603.01547 .", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems. pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "arXiv preprint arXiv:1611.09268 .", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Who did what: A large-scale person-centered cloze dataset", "author": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester."], "venue": "arXiv preprint arXiv:1608.05457 .", "citeRegEx": "Onishi et al\\.,? 2016", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.05250 .", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "NewsQA: A machine comprehension dataset", "author": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1611.09830 .", "citeRegEx": "Trischler et al\\.,? 2016", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Advances in Neural Information Processing Systems. pages 2692\u20132700.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Achieving human parity in conversational speech recognition", "author": ["Wayne Xiong", "Jasha Droppo", "Xuedong Huang", "Frank Seide", "Mike Seltzer", "Andreas Stolcke", "Dong Yu", "Geoffrey Zweig."], "venue": "arXiv preprint arXiv:1610.05256 .", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "One of the driving forces behind the recent success of deep learning in challenging tasks, such as object recognition (Krizhevsky et al., 2012), speech recognition (Xiong et al.", "startOffset": 118, "endOffset": 143}, {"referenceID": 14, "context": ", 2012), speech recognition (Xiong et al., 2016) and machine translation (Bahdanau et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 0, "context": ", 2016) and machine translation (Bahdanau et al., 2014), has been the increasing availability of large-scale annotated data.", "startOffset": 32, "endOffset": 55}, {"referenceID": 0, "context": ", 2016) and machine translation (Bahdanau et al., 2014), has been the increasing availability of large-scale annotated data. This observation has also led to the interest in building a large-scale annotated dataset for question-answering. In 2015, Bordes et al. (2015) released a large-scale dataset of 100k open-world question-answer pairs constructed from Freebase, and Hermann et al.", "startOffset": 33, "endOffset": 269}, {"referenceID": 0, "context": ", 2016) and machine translation (Bahdanau et al., 2014), has been the increasing availability of large-scale annotated data. This observation has also led to the interest in building a large-scale annotated dataset for question-answering. In 2015, Bordes et al. (2015) released a large-scale dataset of 100k open-world question-answer pairs constructed from Freebase, and Hermann et al. (2015) released two datasets,", "startOffset": 33, "endOffset": 394}, {"referenceID": 4, "context": "The latter was followed by Hill et al. (2015), Rajpurkar et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 4, "context": "The latter was followed by Hill et al. (2015), Rajpurkar et al. (2016) and Onishi et al.", "startOffset": 27, "endOffset": 71}, {"referenceID": 4, "context": "The latter was followed by Hill et al. (2015), Rajpurkar et al. (2016) and Onishi et al. (2016), each of which has released a set of large-scale closed-", "startOffset": 27, "endOffset": 96}, {"referenceID": 5, "context": "We evaluate this new dataset, to which we refer as SearchQA, with a variant of recently proposed attention sum reader (Kadlec et al., 2016) and with human volunteers.", "startOffset": 118, "endOffset": 139}, {"referenceID": 1, "context": "example of such a dataset is SimpleQA by (Bordes et al., 2015).", "startOffset": 41, "endOffset": 62}, {"referenceID": 4, "context": "One family of such datasets includes Children\u2019s Book dataset (Hill et al., 2015), CNN and DailyMail (Hermann et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 3, "context": ", 2015), CNN and DailyMail (Hermann et al., 2015).", "startOffset": 27, "endOffset": 49}, {"referenceID": 10, "context": "This family includes SQuAD (Rajpurkar et al., 2016) and NEWSQA (Trischler et al.", "startOffset": 27, "endOffset": 51}, {"referenceID": 12, "context": ", 2016) and NEWSQA (Trischler et al., 2016).", "startOffset": 19, "endOffset": 43}, {"referenceID": 8, "context": "MS MARCO (Nguyen et al., 2016)\u2013the most recently released dataset to our knowledge\u2013 is perhaps most similar to the proposed SearchQA.", "startOffset": 9, "endOffset": 30}, {"referenceID": 8, "context": "MS MARCO (Nguyen et al., 2016)\u2013the most recently released dataset to our knowledge\u2013 is perhaps most similar to the proposed SearchQA. Nguyen et al. (2016) selected a subset of actual user-generated queries to Microsoft Bing that correspond to questions.", "startOffset": 10, "endOffset": 155}, {"referenceID": 13, "context": ", 2016) is a variant of a pointer network (Vinyals et al., 2015) that was specifically constructed to solve a cloze-style questionanswering task.", "startOffset": 42, "endOffset": 64}, {"referenceID": 6, "context": "We use Adam (Kingma and Ba, 2014) and dropout (Srivastava et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 11, "context": "We use Adam (Kingma and Ba, 2014) and dropout (Srivastava et al., 2014) for training.", "startOffset": 46, "endOffset": 71}], "year": 2017, "abstractText": "We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet\u2019s URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.", "creator": "LaTeX with hyperref package"}}}