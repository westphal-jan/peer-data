{"id": "1704.05426", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "abstract": "tan paper introduces the multi - stat natural language inference ( sonar ) corpus, a dataset designed for use in the development and evaluation of flexible structured models for sentence understanding. in addition if being descriptive of the largest samples available for the task of nli, at 433k examples, this corpus improves upon available resources in sample coverage : ct offers data from ten distinct genres of written and spoken english - - making it possible to evaluate systems on nearly certainly full complexity of the language - - and it offers an explicit setting for the evaluation of cross - genre domain adaptation.", "histories": [["v1", "Tue, 18 Apr 2017 17:10:13 GMT  (86kb,D)", "http://arxiv.org/abs/1704.05426v1", "10 pages, 2 figures, 5 tables, submitted to EMNLP 2017"], ["v2", "Thu, 18 May 2017 16:40:41 GMT  (86kb,D)", "http://arxiv.org/abs/1704.05426v2", "10 pages, 2 figures, 5 tables, submitted to EMNLP 2017. v2 corrects a misreported accuracy number for the CBOW model in the 'matched' setting"], ["v3", "Tue, 5 Sep 2017 18:25:11 GMT  (91kb,D)", "http://arxiv.org/abs/1704.05426v3", "12 pages, 2 figures, 7 tables. v2 corrects a misreported accuracy number for the CBOW model in the 'matched' setting. v3 adds a discussion of the difficulty of the corpus to the analysis section"]], "COMMENTS": "10 pages, 2 figures, 5 tables, submitted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["adina williams", "nikita nangia", "samuel r bowman"], "accepted": false, "id": "1704.05426"}, "pdf": {"name": "1704.05426.pdf", "metadata": {"source": "CRF", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "authors": ["Adina Williams", "Nikita Nangia", "Samuel R. Bowman"], "emails": ["adinawilliams@nyu.edu", "nikitanangia@nyu.edu", "bowman@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Many of the most actively-studied problems in NLP, including question answering, translation, and dialog, depend in large part on natural language understanding (NLU) for success. While there has been a great deal of work that uses representation learning techniques to make progress on these applied NLU problems directly, in order for a representation learning model to fully succeed at one of these problems, it must simultaneously succeed at both NLU and at one or more additional hard machine learning problems like structured prediction or memory access. This makes it difficult to accurately judge the degree to which current models extract reasonable representations of language meaning in these settings.\nThe task of natural language inference (NLI) is uniquely well-positioned to serve as a benchmark task for research on NLU. In this task, also known\nas recognizing textual entailment (RTE; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentences\u2014like one of those in Figure 1\u2014 and asked to judge the relationship between their meanings by picking a label from a small set: typically entailment, neutral, and contradiction. Succeeding at NLI requires a model to fully capture sentence meaning (i.e., lexical and compositional semantics). This requires a model to handle phenomena like lexical entailment, quantification, coreference, tense, belief, modality, (reasoning about should and must, for example), and lexical and syntactic ambiguity.\nBecause this task of natural language inference is so simply defined and yet so demanding, we argue that any model or learning technique that is capable of high-quality NLU at the sentence level can be made to succeed at NLI with minimal additional effort, and that any model or learning technique that lacks this ability is destined to fail on at least some typical examples of NLI.\nAs the only large, human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a standard benchmark for sentence understanding and spurring work on core representation learning techniques for NLU such as attention (Wang and Jiang, 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016).\nHowever, it falls short of the promise laid out above. The sentences in SNLI are derived from image captions and are limited to descriptions of concrete visual scenes, rendering the sentences\nar X\niv :1\n70 4.\n05 42\n6v 1\n[ cs\n.C L\n] 1\n8 A\npr 2\n01 7\nshort and simple, and making the handling many key phenomena like tense, belief, and modality irrelevant to task performance. Because of these two factors it is not sufficiently demanding to serve as an effective benchmark for NLU, with the best current model performance (Chen et al., 2017) falling within a few percentage points of human accuracy, and limited room left for fine-grained comparisons between models.\nThis paper introduces a new dataset, the MultiGenre NLI Corpus (MultiNLI), whose chief purpose is to remedy these limitations by making it possible to run large-scale NLI evaluations that capture the full complexity of English. Its size (433k pairs) and mode of collection are modeled closely on SNLI, but unlike that corpus, MultiNLI\nrepresents both written and spoken speech and a range of styles, degrees of formality, and topics.\nOur chief motivation in creating this corpus is to provide a benchmark for ambitious machine learning research on the core problems of NLU, but we are additionally interested in constructing the corpus in a way that facilitates work on domain adaptation and cross-domain transfer learning. In many application areas outside NLU, artificial neural network techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014). However, attempts to bring this kind of general purpose representation learning to NLU\nhave seen only very limited successes (see, for example, Mou et al., 2016a). Nearly all successful applications of representation learning to problems in NLU have involved models that are trained on data that closely resembles the target evaluation data, both in task and style. This fact limits the usefulness of these tools for problems involving styles of language not represented in large annotated training sets.\nWith this in mind, we construct MultiNLI so as to explicitly evaluate models both on the quality of their representations of text in any of the training genres, and on their ability to derive reasonable representations outside those genres. In particular, the corpus consists of sentences derived from ten different sources of text, reflecting ten different genres of written and spoken English. All of the sources are present in test and development sets, but only five are included in the training set. Models thus can be evaluated on both the matched test examples, which are derived from the same sources as those in the training set, and on the mismatched examples, which do not closely resemble any seen at training time.\nIn the remainder of this paper, we introduce the methods and data sources that we use to collect the corpus, present statistics over the resulting data, and introduce and analyze a number of machine learning models that are meant to provide baselines for further work on the corpus."}, {"heading": "2 The Corpus", "text": ""}, {"heading": "2.1 Data Collection", "text": "The basic collection methodology for MultiNLI is similar to that of SNLI: We create each sentence pair by selecting a premise sentence from a preexisting text source and asking a human annotator to compose a novel sentence to pair with it as a hypothesis. This section discusses the sources of our premise sentences, our collection method for hypothesis sentences, and our validation (relabeling) strategy.\nPremise Text Sources The MultiNLI premise sentences are derived from ten sources of freely available text which are meant to cover a maximally broad range of genres of American English. Nine of these sources come from the second release of the Open American National Corpus (OANC; Fillmore et al., 1998; Macleod, 2000; Ide and Suderman, 2006, downloaded 12/2016):\n\u2022 FACE-TO-FACE: transcriptions of two-sided in-person conversations from the Charlotte, NC area in the early 2000s, taken from the the Charlotte Narrative and Conversation Collection.\n\u2022 TELEPHONE: transcriptions of two-sided, conversations held in 1990\u20131991 by speakers of both sexes from several major dialect regions, taken from the University of Pennsylvania\u2019s Linguistic Data Consortium Switchboard corpus.\n\u2022 9/11: report on the September 11th 2001 terrorist attacks, released on July 22, 2004 by the National Commission on Terrorist Attacks Upon the United States.\n\u2022 TRAVEL: travel guides discussing vacation and traveling abroad, released in the early 2000s by Berlitz Publishing.\n\u2022 LETTERS: letters promoting fundraising for non-profit organizations written in late 1990s\u2013early 2000s, collected by The Indiana Center for Intercultural Communication of Philanthropic Fundraising Discourse.\n\u2022 OUP: five non-fiction works published by Oxford University Press on the textile industry and child development.\n\u2022 SLATE: articles on popular culture, written between 1996\u20132000, taken from the archives of Slate Magazine.\n\u2022 VERBATIM: articles from a quarterly magazine containing short posts about language and linguistics for non-specialists, written between 1990 and 1996, taken from the Verbatim archives.\n\u2022 GOVERNMENT: reports, speeches, letters, and press releases from public domain government websites.\nFor our tenth genre, FICTION, we compile several freely available works of contemporary fiction written between 1912 and 2010. This section of our source corpus consists of eight modern works of short to mid-length fiction spanning crime, mystery, humor, western, adventure, science fiction, and fantasy. The authors of these works include Isaac Asimov, Agatha Christie, Ben Essex\n(Elliott Gesswell), Nick Name (Piotr Kowalczyk), Andre Norton, Lester del Ray, and Mike Shea.\nWe construct premise sentences from these source texts with minimal preprocessing; we select only unique the sentences within genres, exclude very short sentences (under eight characters) and manually remove certain types of nonnarrative writing, such as mathematical formulae, bibliographic references, and lists.\nDespite the fact that SNLI is collected in largely the same way as MultiNLI, no SNLI examples are included in the distributed MultiNLI corpus. SNLI consists only of sentences derived from image captions from the Flickr30k corpus (Young et al., 2014), and thus can be treated as a large additional CAPTIONS genre.\nHypothesis Collection To collect a sentence pair using this method, we present a crowdworker with a sentence from a source text and ask them to compose three novel sentences (the hypotheses): one which is necessarily true or appropriate in the same situations as the premise (to be paired with the premise under the label entailment), one which is necessarily false or inappropriate whenever the premise is true (contradiction), and one where neither condition applies (neutral). This method of data collection ensures that each class will be represented equally in the raw corpus.\nWe tailor the prompts that surround each premise sentence during hypothesis collection to fit the genre of that premise sentence. We pilot these prompts prior to data collection to ensure that the instructions are clear and that they yield hypothesis sentences that fit the intended meanings of the three classes. Each prompt provides examples for the three requested hypothesis sentences (corresponding to the three labels) that are specific to the relevant genre. There are five unique prompts in total: one for written non-fiction genres (SLATE, OUP, GOVERNMENT, VERBATIM, TRAVEL, Figure 1), one for spoken genres (TELEPHONE, FACE-TO-FACE), one for each of the less formal written genres (FICTION, LETTERS), and a specialized one for 9/11, tailored to fit its potentially emotional content.\nBelow the instructions, we present three text fields\u2014one for each of entailment, contradiction, and neutral\u2014followed by a fourth field for reporting issues, and a link to the frequently asked questions (FAQ) page. We provide one FAQ page tailored to each prompt. FAQs are modeled on their\nSNLI counterparts and include additional curated examples, answers to genre specific questions that arose in our pilot phase (e.g., annotators need not introduce um for spoken genres, despite their presence in the premise), and information about logistical concerns like payment.\nFor both hypothesis collection and validation, we present prompts to a selected pool of workers using the Hybrid crowdsourcing platform. This differs from the collection of SNLI, which was\ndone over Amazon Mechanical Turk. 387 workers contributed.\nValidation We also perform an additional round of annotation on our test and development examples to ensure that their labels are accurate. The validation phase follows the procedure used in SICK (Marelli et al., 2014b) and SNLI: Workers are presented with pairs of sentences and asked to supply a single label (entailment, contradiction, neutral) for the pair. Each pair is relabeled by four workers, yielding a total of five labels per example. Instructions are very similar to those given in Figure 1, and a single FAQ, modeled after validation FAQ from SNLI, is provided for reference. In order to encourage thoughtful labeling decisions, we manually label one percent of the validation examples and offer a $1 bonus each time a worker selects a label that matches ours.\nFor each validated sentence pair, we assign a gold label representing a majority vote between the initial label assigned to the pair and the four additional labels assigned by validation annotators. A small number of examples did not receive a three-vote consensus on any one label. These examples are included in the distributed corpus, but are marked with the gold label \u2018-\u2019, and should not be used in standard evaluations. Table 2 shows summary statistics capturing the results of validation, alongside corresponding figures for SNLI."}, {"heading": "2.2 The Resulting Corpus", "text": "Table 1 shows one randomly chosen validated example from the development set of each of the genres along with its assigned label. Hypothesis sentences tend to rely heavily on knowledge about the world, and don\u2019t tend to correspond closely with their premises in sentence structure. These syntactic differences suggest that alignment-based NLI models (like, for example, that of MacCartney, 2009) are unlikely to suc-\nceed. Hypotheses tend to be fluent and correctly spelled, consisting of full sentences, fragments, noun phrases, bare prepositional phrases, and verb phrases. Hypothesis-internal punctuation is often omitted.\nThe current version of the corpus is available at nyu.edu/projects/bowman/ multinli/. The corpus is freely available for typical machine learning uses, and may be modified and redistributed. The majority of the corpus is released under the OANC\u2019s license, which allows all content to be freely used, modified, and shared under permissive terms. The data in the FICTION section falls under several permissive licenses; Seven Swords is available under a Creative Commons Share-Alike 3.0 Unported License, and with the explicit permission of the author, Living History and Password Incorrect are available under Creative Commons Attribution 3.0 Unported Licenses, and the remaining works of fiction are in the public domain in the United States (but be licensed differently elsewhere).\nThe corpus is available in two formats, tab separated text and JSON Lines (jsonl), following SNLI. Both formats have the following fields for each example:\n\u2022 gold label: The label to be used for classification. In examples which are rejected during the validation process, the value of this field will be \u2018-\u2019. These should be ignored in typical evaluations.\n\u2022 sentence1: The premise sentence for the pair, extracted from one of the sources described above.\n\u2022 sentence2: The hypothesis sentence for\nthe pair, composed by a crowd worker as a companion sentence for sentence1.\n\u2022 sentence{1,2} parse: Each sentence as parsed by the Stanford PCFG Parser (3.5.2 Klein and Manning, 2003).\n\u2022 sentence{1,2} binary parse: The above parses in unlabeled binary-branching format.\n\u2022 promptID: A unique identifier for each premise sentence. Note that a single premise will typically appear in three different sentence pairs, each with a different hypothesis and label.\n\u2022 pairID: A unique identifier for each example.\n\u2022 genre: The genre of the source text from which sentence1 was drawn.\n\u2022 label[1]: The label assigned during the creation of the sentence pair. In rare cases this may be different from gold label, if a consensus of annotators during the validation phase chose a different label.\n\u2022 label[2...5]: The four labels assigned by individual annotators to each development and test example during validation. For training examples, these fields will not be filled.\nPartition The distributed corpus comes with an explicit training/testing/development split. The testing and development sets contain 2,000 randomly determined examples from each of the genres, resulting in a total of 20k examples per set. There is no overlap in premise sentences between the three sections.\nStatistics Premise sentences in MultiNLI tend to be longer than their hypothesis sentences (see Figure 2), and much longer than premise sentences in SNLI. The longest premise was 401 words and the longest hypothesis 70 words. Table 3 contains additional statistics. We observe that the two spoken genres differ greatly from one another in apparent structure: The parser assigns far more \u2018S\u2019-labeled root nodes (indicating full sentences) to premises in FACE-TO-FACE than to those in TELEPHONE. We speculate that there might be fewer full sentences in the TELEPHONE genre because conversational turn-taking is harder without visual contact, resulting in a high incidence of simultaneous speech and making transcription difficult."}, {"heading": "3 Baselines", "text": "To test the difficulty of the NLI task on this corpus, we experiment with three neural network models that are intended to be maximally comparable with strong published work on SNLI.\nThe first two models are built to produce a sin-\ngle vector representing each sentence and compute label predictions based on the two resulting vectors. To do this, they concatenate the two representations, their difference, and their elementwise product (following Mou et al., 2016b), and pass the result to a single tanh layer followed by a three-way softmax classifier. The first such model is a simple continuous bag of words (CBOW) model in which each sentence is represented as the sum of the embedding representations of its words. The second uses the average of the states of a bidirectional LSTM RNN (BiLSTM; Hochreiter and Schmidhuber, 1997) over the words to compute representations.\nIn addition to these two baselines, we also implement and evaluate Chen et al.\u2019s (2017) Enhanced Sequential Inference Model (ESIM), which represents the state of the art on SNLI at the time of writing. We use the base ESIM without ensembling with a TreeLSTM (as is done in one published variant of the model).\nAll three models are initialized with 300D reference GloVe vectors (840B token version; Pennington et al., 2014). Out-of-vocabulary (OOV) words are initialized randomly, and all word embeddings are fine-tuned during training. The models use 300D hidden states, as in most prior work on SNLI. We use Dropout (Srivastava et al., 2014) for regularization. For ESIM, we follow the paper and use a dropout rate of 0.5. For the CBOW and BiLSTM models, we tune Dropout on the SNLI development set and find that a drop rate of 0.1 works well. We use the Adam (Kingma and Ba,\n2015) optimizer with the default parameters. We train models on SNLI, on MultiNLI, and on a mixture of both corpora. In the mixed setting, we use the full MultiNLI training set but downsample SNLI by randomly selecting 15% of the SNLI training set at each epoch. This ensures that each available genre is seen with roughly equal frequency during training. Table 4 shows the results.\nWe also train a separate CBOW model on each individual genre to establish the degree to which simple models already allow for effective transfer across genres. When training on SNLI, a single random sample of 15% of the original training set is used. Table 5 shows these results.1"}, {"heading": "4 Discussion", "text": "Data Collection In data collection for NLI, different annotator decisions about the coreference between entities and events across the two sentences in a pair can lead to very different assignments of pairs to labels (de Marneffe et al., 2008; Marelli et al., 2014a; Bowman et al., 2015). Drawing an example from Bowman et al., the pair \u201ca boat sank in the Pacific Ocean\u201d and \u201ca boat sank in the Atlantic Ocean\u201d can be labeled either contradiction or neutral depending on (among other things) whether the two mentions of boats are assumed to refer to the same entity in the world.\nThis uncertainty can present a serious problem for inter-annotator agreement, since it is not clear\n1For the CBOW models trained on individual genres, we use a dropout rate of 0.2.\nthat it is possible to define an explicit set of rules around coreference that would be easily intelligible to an untrained annotator (or any non-expert). Bowman et al. attempt to avoid this problem by using an annotation prompt that is highly dependent on the concreteness of image descriptions, but there is no reason to suspect a priori that any similar prompt and annotation strategy can work for the much more abstract writing that is found in, for example, government documents. We are surprised to find that this is not a major issue. Through relatively straightforward trial-and-error, and discussion with our annotators, we are able to design prompts for more abstract genres that yield inter-annotator agreement scores nearly identical to those of SNLI (see Table 2).\nDifficulty As expected, the increased range of linguistic phenomena in MultiNLI and the longer average sentence length conspire to make MultiNLI dramatically more difficult to model than SNLI. Our three baseline models perform better on SNLI than on MultiNLI by about fifteen percentage points when trained on the respective datasets. When the models are trained only on SNLI, all three achieve accuracy above 80% on the SNLI test set. However, when trained on MultiNLI, only ESIM surpasses 70% accuracy on either of MultiNLI\u2019s test sets. When the models are trained on MultiNLI and downsampled SNLI, their performance improves significantly on SNLI as expected, but performance on the MultiNLI test sets does not significantly change. Between MultiNLI\u2019s difficulty and its relatively high interannotator agreement, it presents a problem with substantial headroom for future work.\nWe note that models trained on MultiNLI have relatively poor performance on SNLI. This may be because SNLI consists largely of presentational structures like \u201ca black car starts up in front of a crowd of people\u201d, while MultiNLI has very few.\nCross-Genre Similarity Table 5 provides us with an indirect measure of similarity between genres. For each genre represented in the training set, the model that performs best was trained on that genre. We see some correlation in performance among the genres we\u2019d expect to be similar. For example, for the FACE-TO-FACE genre, the model trained on TELEPHONE (the other spoken genre) attains the best accuracy. While SLATE seems to be a more difficult genre and performance on it is relatively poor, the model trained on SLATE achieves best accuracy on 9/11 and VERBATIM. The SLATE model also gets relatively high accuracy on the TRAVEL and GOVERNMENT genres, second only to the models trained on these respective genres. This may be because sentences in SLATE cover a wide range of topics, making it harder to do well on, but also forcing models trained on it be more broadly capable.\nWe also observe that our models perform similarly on both the matched and mismatched test sets of MultiNLI. Any difference in performance between the sets owing to train\u2013test genre mismatch is likely to be obscured by the substantial uncontrolled variation in difficulty between genres. We expect that as models are developed that can better fit the training genres of MultiNLI, issues of genre mismatch will become more conspicuous."}, {"heading": "5 Conclusion", "text": "Natural language inference makes it easy to judge the degree to which neural network models for sentence understanding capture the full meanings for natural language sentences. Existing NLI datasets like SNLI have facilitated substantial advances in modeling, but have limited headroom and coverage of the full diversity of meanings expressed in English. This paper presents a new dataset that offers dramatically greater difficulty and diversity, and also serves as a benchmark for the study of cross-genre domain adaptation."}, {"heading": "Acknowledgments", "text": "This work was made possible by a Google Faculty Research Award to Sam Bowman and Angeliki Lazaridou. We also thank George Dahl, the organizers of the RepEval 2016 and RepEval 2017 workshops, Andrew Drozdov, and our other colleagues at NYU for their help and advice."}], "references": [{"title": "Recognising textual entailment with logical inference", "author": ["Johan Bos", "Katja Markert."], "venue": "Proc. EMNLP.", "citeRegEx": "Bos and Markert.,? 2005", "shortCiteRegEx": "Bos and Markert.", "year": 2005}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts."], "venue": "Proc. ACL.", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Enhanced LSTM for natural language inference", "author": ["Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen."], "venue": "Proc. ACL.", "citeRegEx": "Chen et al\\.,? 2017", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Entailment, intensionality and text understanding", "author": ["Cleo Condoravdi", "Dick Crouch", "Valeria de Paiva", "Reinhard Stolle", "Daniel G. Bobrow."], "venue": "Proc. NAACL.", "citeRegEx": "Condoravdi et al\\.,? 2003", "shortCiteRegEx": "Condoravdi et al\\.", "year": 2003}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Machine learning challenges. Evaluating predictive uncertainty, visual object classification, and recognising textual entailment, Springer.", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Finding contradictions in text", "author": ["Marie-Catherine de Marneffe", "Anna N. Rafferty", "Christopher D. Manning."], "venue": "Proc. ACL.", "citeRegEx": "Marneffe et al\\.,? 2008", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "DeCAF: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell."], "venue": "Proc. ICML.", "citeRegEx": "Donahue et al\\.,? 2014", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "An American national corpus: A proposal", "author": ["Charles Fillmore", "Nancy Ide", "Daniel Jurafsky", "Catherine Macleod."], "venue": "Proc. LREC.", "citeRegEx": "Fillmore et al\\.,? 1998", "shortCiteRegEx": "Fillmore et al\\.", "year": 1998}, {"title": "A natural logic inference system", "author": ["Yaroslav Fyodorov", "Yoad Winter", "Nissim Francez."], "venue": "Proceedings of the 2nd Workshop on Inference in Computational Semantics.", "citeRegEx": "Fyodorov et al\\.,? 2000", "shortCiteRegEx": "Fyodorov et al\\.", "year": 2000}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8).", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Integrating linguistic resources: The national corpus model", "author": ["Nancy Ide", "Keith Suderman."], "venue": "Proc. LREC.", "citeRegEx": "Ide and Suderman.,? 2006", "shortCiteRegEx": "Ide and Suderman.", "year": 2006}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proc. ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proc. ACL. https://doi.org/10.3115/1075096.1075150.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in Neural Information Processing Systems 25.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Natural Language Inference", "author": ["Bill MacCartney."], "venue": "Ph.D. thesis, Stanford University.", "citeRegEx": "MacCartney.,? 2009", "shortCiteRegEx": "MacCartney.", "year": 2009}, {"title": "An extended model of natural logic", "author": ["Bill MacCartney", "Christopher D Manning."], "venue": "Proceedings of the of the Eighth International Conference on Computational Semantics.", "citeRegEx": "MacCartney and Manning.,? 2009", "shortCiteRegEx": "MacCartney and Manning.", "year": 2009}, {"title": "The american national corpus: Standardized resources for american english", "author": ["Catherine Ide-Nancy Grishman Ralph Macleod."], "venue": "Proc. LREC.", "citeRegEx": "Macleod.,? 2000", "shortCiteRegEx": "Macleod.", "year": 2000}, {"title": "SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella bernardi", "Roberto Zamparelli."], "venue": "Proc. LREC.", "citeRegEx": "Marelli et al\\.,? 2014b", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "How transferable are neural networks in nlp applications? In Proc", "author": ["Lili Mou", "Zhao Meng", "Rui Yan", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin."], "venue": "EMNLP.", "citeRegEx": "Mou et al\\.,? 2016a", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["Lili Mou", "Men Rui", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin."], "venue": "Proc. ACL.", "citeRegEx": "Mou et al\\.,? 2016b", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Neural semantic encoders", "author": ["Tsendsuren Munkhdalai", "Hong Yu."], "venue": "Proc. EACL.", "citeRegEx": "Munkhdalai and Yu.,? 2017", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2017}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "JMLR 15.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Learning natural language inference with LSTM", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "Proc. NAACL.", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "TACL 2.", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D. Zeiler", "Rob Fergus."], "venue": "Proc. ECCV .", "citeRegEx": "Zeiler and Fergus.,? 2014", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "In this task, also known as recognizing textual entailment (RTE; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentences\u2014like one of those in Figure 1\u2014 and asked to judge the relationship between their meanings by picking a label from a small set: typically entailment, neutral, and contradiction.", "startOffset": 59, "endOffset": 185}, {"referenceID": 4, "context": "In this task, also known as recognizing textual entailment (RTE; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentences\u2014like one of those in Figure 1\u2014 and asked to judge the relationship between their meanings by picking a label from a small set: typically entailment, neutral, and contradiction.", "startOffset": 59, "endOffset": 185}, {"referenceID": 0, "context": "In this task, also known as recognizing textual entailment (RTE; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentences\u2014like one of those in Figure 1\u2014 and asked to judge the relationship between their meanings by picking a label from a small set: typically entailment, neutral, and contradiction.", "startOffset": 59, "endOffset": 185}, {"referenceID": 5, "context": "In this task, also known as recognizing textual entailment (RTE; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentences\u2014like one of those in Figure 1\u2014 and asked to judge the relationship between their meanings by picking a label from a small set: typically entailment, neutral, and contradiction.", "startOffset": 59, "endOffset": 185}, {"referenceID": 16, "context": "In this task, also known as recognizing textual entailment (RTE; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentences\u2014like one of those in Figure 1\u2014 and asked to judge the relationship between their meanings by picking a label from a small set: typically entailment, neutral, and contradiction.", "startOffset": 59, "endOffset": 185}, {"referenceID": 1, "context": "As the only large, human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a standard benchmark for sentence understanding and spurring work on core representation learning techniques for NLU such as attention (Wang and Jiang, 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al.", "startOffset": 95, "endOffset": 122}, {"referenceID": 25, "context": ", 2015) has enabled a good deal of progress on NLU, serving as a standard benchmark for sentence understanding and spurring work on core representation learning techniques for NLU such as attention (Wang and Jiang, 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al.", "startOffset": 198, "endOffset": 220}, {"referenceID": 22, "context": ", 2015) has enabled a good deal of progress on NLU, serving as a standard benchmark for sentence understanding and spurring work on core representation learning techniques for NLU such as attention (Wang and Jiang, 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al.", "startOffset": 229, "endOffset": 254}, {"referenceID": 21, "context": ", 2015) has enabled a good deal of progress on NLU, serving as a standard benchmark for sentence understanding and spurring work on core representation learning techniques for NLU such as attention (Wang and Jiang, 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016).", "startOffset": 287, "endOffset": 327}, {"referenceID": 2, "context": ", 2015) has enabled a good deal of progress on NLU, serving as a standard benchmark for sentence understanding and spurring work on core representation learning techniques for NLU such as attention (Wang and Jiang, 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016).", "startOffset": 287, "endOffset": 327}, {"referenceID": 3, "context": "Because of these two factors it is not sufficiently demanding to serve as an effective benchmark for NLU, with the best current model performance (Chen et al., 2017) falling within a few percentage points of human accuracy, and limited room left for fine-grained comparisons between models.", "startOffset": 146, "endOffset": 165}, {"referenceID": 14, "context": "In many application areas outside NLU, artificial neural network techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014).", "startOffset": 239, "endOffset": 311}, {"referenceID": 27, "context": "In many application areas outside NLU, artificial neural network techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014).", "startOffset": 239, "endOffset": 311}, {"referenceID": 7, "context": "In many application areas outside NLU, artificial neural network techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014).", "startOffset": 239, "endOffset": 311}, {"referenceID": 26, "context": "SNLI consists only of sentences derived from image captions from the Flickr30k corpus (Young et al., 2014), and thus can be treated as a large additional CAPTIONS genre.", "startOffset": 86, "endOffset": 106}, {"referenceID": 19, "context": "The validation phase follows the procedure used in SICK (Marelli et al., 2014b) and SNLI: Workers are presented with pairs of sentences and asked to supply a single label (entailment, contradiction, neutral) for the pair.", "startOffset": 56, "endOffset": 79}, {"referenceID": 10, "context": "The second uses the average of the states of a bidirectional LSTM RNN (BiLSTM; Hochreiter and Schmidhuber, 1997) over the words to compute representations.", "startOffset": 70, "endOffset": 112}, {"referenceID": 3, "context": "In addition to these two baselines, we also implement and evaluate Chen et al.\u2019s (2017) Enhanced Sequential Inference Model (ESIM), which represents the state of the art on SNLI at the time of writing.", "startOffset": 67, "endOffset": 88}, {"referenceID": 23, "context": "All three models are initialized with 300D reference GloVe vectors (840B token version; Pennington et al., 2014).", "startOffset": 67, "endOffset": 112}, {"referenceID": 24, "context": "We use Dropout (Srivastava et al., 2014) for regularization.", "startOffset": 15, "endOffset": 40}, {"referenceID": 12, "context": "We use the Adam (Kingma and Ba, 2015) optimizer with the default parameters.", "startOffset": 16, "endOffset": 37}, {"referenceID": 1, "context": "Data Collection In data collection for NLI, different annotator decisions about the coreference between entities and events across the two sentences in a pair can lead to very different assignments of pairs to labels (de Marneffe et al., 2008; Marelli et al., 2014a; Bowman et al., 2015).", "startOffset": 217, "endOffset": 287}], "year": 2017, "abstractText": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. In addition to being one of the largest corpora available for the task of NLI, at 433k examples, this corpus improves upon available resources in its coverage: it offers data from ten distinct genres of written and spoken English\u2014making it possible to evaluate systems on nearly the full complexity of the language\u2014and it offers an explicit setting for the evaluation of crossgenre domain adaptation.", "creator": "TeX"}}}