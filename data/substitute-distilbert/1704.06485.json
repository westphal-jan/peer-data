{"id": "1704.06485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Attend to You: Personalized Image Captioning with Context Sequence Memory Networks", "abstract": "we address crucial issues of image captioning, previously'not been discussed yet in previous research. for a query image, we aim to generate a descriptive sentence, accounting for prior knowledge such as the user's active vocabularies in previous documents. as applications seek personalized image captioning, research tackle two post automation tasks : hashtag prediction and post generation, on typing newly received instagram address, consisting of 1. 1m posts numbering 6. 3k users. we propose a novel captioning model named context sequence memory network ( csmn ). its unique updates over previous memory network models include ( i ) exploiting memory as a repository for multiple types of context information, ( ii ) appending previously generated strings into memory to capture long - term information without suffering from the vanishing loop problem, and ( iii ) adopting cnn memory structure to critically analyze nearby ordered memory slots for better context understanding. with comprehensive evaluation function tracking studies via amazon mechanical turk, we show the effectiveness requires maintaining three leading features of csmn and its performance enhancement for personalized filter filtering over fine - of - some - art captioning models.", "histories": [["v1", "Fri, 21 Apr 2017 11:29:07 GMT  (3876kb,D)", "http://arxiv.org/abs/1704.06485v1", "Accepted paper at CVPR 2017"], ["v2", "Tue, 25 Apr 2017 23:30:43 GMT  (3879kb,D)", "http://arxiv.org/abs/1704.06485v2", "Accepted paper at CVPR 2017"]], "COMMENTS": "Accepted paper at CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["cesc chunseong park", "byeongchang kim", "gunhee kim"], "accepted": false, "id": "1704.06485"}, "pdf": {"name": "1704.06485.pdf", "metadata": {"source": "META", "title": "Attend to you: Personalized Image Captioning with Context Sequence Memory Networks", "authors": ["Cesc Chunseong Park", "Byeongchang Kim", "Gunhee Kim"], "emails": ["cspark@lunit.io", "byeongchang.kim@vision.snu.ac.kr", "gunhee@snu.ac.kr"], "sections": [{"heading": "1. Introduction", "text": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33]. As this task is often regarded as one of frontierAI problems, it has been actively studied in recent vision and language research. It requires an algorithm not only to understand the image content in depth beyond category or attribute levels, but also to connect its interpretation with a language model to create a natural sentence.\nThis work addresses personalization issues of image\n\u2217This work was done while the author was at Seoul National University. Codes: https://github.com/cesc-park/attend2u.\ncaptioning, which have not been discussed in previous research. We aim to generate a descriptive sentence for an image, accounting for prior knowledge such as the user\u2019s active vocabularies or writing styles in previous documents. Potentially, personalized image captioning is applicable to a wide range of automation services in photo-sharing social networks. For example, in Instagram or Facebook, users instantly take and share pictures as posts using mobile phones. One bottleneck to complete an image post is to craft hashtags or associated text description using their own words. Indeed, crafting text is more cumbersome than taking a picture for general users; photo-taking can be done with only a single tab on the screen of a smartphone, whereas text writing requires more time and mental energy for selecting suitable keywords and completing a sentence to describe theme, sentiment, and context of the image.\nIn this paper, as examples of personalized image captioning, we focus on two post automation tasks: hashtag\nar X\niv :1\n70 4.\n06 48\n5v 1\n[ cs\n.C V\n] 2\n1 A\npr 2\nprediction and post generation. Figure 1 shows an Instagram post example. The hashtag prediction automatically predicts a list of hashtags for the image, while the post generation creates a sentence consisting of normal words, emojis, and even hashtags. Personalization is key to success in these two tasks, because text in social networks is not simple description of image content, but the user\u2019s own story and experience about the image with his or her favorite vocabularies and expressions.\nTo achieve personalized image captioning tasks, we propose a memory network model named as context sequence memory network (CSMN). Our model is inspired by memory networks [5, 24, 29], which explicitly include memory components to which neural networks read and write data for capturing long-term information. Our major updates over previous memory network models are three-fold.\nFirst, we propose to use the memory as a context repository of prior knowledge for personalized image captioning. Since the topics of social network posts are too broad and users\u2019 writing styles are too diverse, it is crucial to leverage prior knowledge about the authors or metadata around the images. Our memory retains such multiple types of context information to promote more focused prediction, including users\u2019 active vocabularies and various image descriptors.\nSecond, we design the memory to sequentially store all of the words that the model generates. It leads two important advantages. First, it enables the model to selectively attend, at every step, on the most informative previous words and their combination with other context information in the memory. Second, our model does not suffer from the vanishing gradient problem. Most captioning models are equipped with RNN-based encoders (e.g. [3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history. Thus, RNNs and their variants often fails to capture longterm dependencies, which could worsen if one wants to use prior knowledge together. On the other hand, our statebased sequence generation explicitly retains all information in the memory to predict next words. By using teacherforced learning [30], our model has a markov property at training time; predicting a previous word yt\u22121 has no effect on predicting a next word yt, which depends on only the current memory state. Thus, the gradients from the current time step prediction yt are not propagated through the time.\nThird, we propose to exploit a CNN to jointly represent nearby ordered memory slots for better context understanding. Original memory networks [24, 29] leverage time embedding to model the memory order. Still its representation power is low, since it cannot represent the correlations between multiple memory slots, for which we exploit convolution layers that lead much stronger representation power.\nFor evaluation, we collect a new personalized image\ncaptioning dataset, comprising 1.1M Instagram posts from 6.3K users. Instagram is a great source for personalized captioning, because posts mostly include personal pictures with long hashtag lists and characteristic text with a wide range of topics. For each picture post, we consider the body text or a list of hashtags as groundtruth captions.\nOur experimental results demonstrate that aforementioned three unique features of our CSMN model indeed improve captioning performance, especially for personalization purpose. We also validate that our CSMN significantly outperforms several state-of-the-art captioning models with the decoders of RNNs or LSTMs (e.g. [27, 28, 31]). We evaluate with quantitative language metrics (e.g. BLEU [21], CIDEr [26], METEOR [14], and ROUGE [15]) and user studies via Amazon Mechanical Turk.\nWe summarize contribution of this work as follows. (1) To the best of our knowledge, we propose a first personalized image captioning approach. We introduce two practical post automation tasks that benefit from personalized captioning: post generation and hashtag prediction.\n(2) We propose a novel memory network model named CSMN for personalized captioning. The unique updates of CSMN include (i) exploiting memory as a repository for multiple context information, (ii) appending previously generated words into memory to capture long-term information without, and (iii) adopting CNN memory structure to jointly represent nearby ordered memory slots.\n(3) For evaluation of personalized image captioning, we introduce a novel Instagram dataset. We make the code and data publicly available.\n(4) With quantitative evaluation and user studies via AMT, we demonstrate the effectiveness of three novel features of CSMN and its performance superiority over stateof-the-art captioning models, including [27, 28, 31]."}, {"heading": "2. Related work", "text": "Image Captioning. In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few. Many proposed captioning models exploit RNN-based decoders to generate a sequence of words from encoded representation of input images. For example, long-term recurrent convolutional networks [3] are one of earliest model to use RNNs for modeling the relations between sequential inputs and outputs. You et al. [33] exploit semantic attention to combine top-down and bottom-up strategies to extract richer information from images, and couples it with an LSTM decoder. Compared to such recent progress of image captioning research, it is novel to replace an RNN-based decoder with a sequence memory. Moreover, no previous work has tackled the personalization issue, which is the key objective of this work. We also introduce post completion and hashtag prediction as solid and practical applications of image captioning.\nPersonalization in Vision and Language Research. There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23]. Especially, Denton et al. [2] develop a CNN model that predicts hashtags from image content and user information. However, this work does not formulate the hashtag prediction as image captioning, and not address the post completion. In computer vision, Yao et al. [32] propose a domain adaptation approach to classify user-specific human gestures. Almaev et al. [1] adopt a transfer learning framework to detect person-specific facial action unit detection. In NLP, Mirkin et al. [18] enhance machine translation performance by exploiting personal traits. Polozov et al. [23] generate personalized mathematical word problem for a given tutor/student specification by logic programming. Compared to these papers, our problem setup is novel in that personalization issues in image captioning have not been discussed yet.\nNeural Networks with Memory. Various memory network models have been proposed to enable neural networks to store variables and data over long timescales. Neural Turing Machines [5] use external memory to solve algorithmic problems such as sorting and copying. Later, this architecture is extended to Differential Neural Computer (DNC) [6] to solve more complicated algorithmic problems such as finding shortest path and graph traversal. Weston et al. [29] propose one of the earliest memory network models for natural language question answering (QA), and later Sukhbaatar et al. [24] modify the network to be trainable in an end-to-end manner. Kumar et al. [13] and Milleret al. [17] address language QA tasks proposing novel memory networks such as dynamic networks with episodic memory in [13] and key-value memory networks in [17]. Compared to previous memory networks, our CSMN has three novel features as discussed in section 1."}, {"heading": "3. Dataset", "text": "We introduce our newly collected Instagram dataset, whose key statistics are outlined in Table 1. We make separate datasets for post completion and hashtag prediction."}, {"heading": "3.1. Collection of Instagram Posts", "text": "We collect image posts from Instagram, which is one of the fastest growing photo-sharing social networks. As a post crawler, we use the built-in hashtag search function provided by Instagram APIs. We select 270 search key-\nwords, which consist of the 10 most common hashtags for each of 27 general categories of Pinterest (e.g. design, food, style). We use the Pinterest categories because they are well-defined topics to obtain image posts of diverse users. We totally collect 3,455,021 raw posts from 17,813 users.\nNext we process a series of filtering. We first apply language filtering to include only English posts; we exclude the posts where more than 20% of words are not in English based on the dictionary en.us dict of PyEnchant. We then remove the posts that embed hyperlinks in the body text because they are likely to be advertisement. Finally, if users have more than max(15, 0.15\u00d7#user posts) non-English or advertisement posts, we remove all of their posts.\nNext we apply filtering rules for the lengths of captions and hashtags. We limit maximum number of posts per user to 1,000, not to make the dataset biased to a small number of dominant users. We also limit minimum number of posts per user to 50, to be sufficiently large to discover users\u2019 writing patterns from posts. We also filter out the posts if their lengths are too short or too long. We set 15 as maximum post length because we observe that lengthy posts tend to include irrelevant stuff to the associated pictures. We set 3 as minimum post length because too short posts are likely to include only an exclamation (e.g. great!) or a short reply (e.g. thanks to everyone!). We use the same rule for hashtag dataset. We observe that lengthy lists of hashtags more than 15 are often too much redundant (e.g. #fashionable, #fashionblog, #fashionista, #fashionistas, #fashionlover, #fashionlovers). Finally, we obtain about 721,176 posts for captions and 518,116 posts for hashtags."}, {"heading": "3.2. Preprocessing", "text": "We separately build a vocabulary dictionary V for each of the two tasks, by choosing the most frequent V words in our dataset. For instance, the dictionary for hashtag prediction includes only most frequent hashtags as vocabularies. We set V to 40K for post completion and 60K for hash prediction after thorough tests. Before building the dictionary, we first remove any urls, unicodes except emojis, and special characters. We then lowercase words and change user names to a @username token."}, {"heading": "4. The Context Sequence Memory Network", "text": "Figure 2 illustrates the proposed context sequence memory network (CSMN) model. The input is a query image Iq of a specific user, and the output is a sequence of words: {yt} = y1, . . . , yT , each of which is a symbol coming from the dictionary V . That is, {yt} corresponds to a list of hashtags in hashtag prediction, and a post sentence in post generation. Optional input is the context information to be added to memory, such as active vocabularies of a given user. Since both tasks can be formulated as word sequence prediction for a given image, we exploit the same CSMN\nmodel while changing only the dictionary. Especially, we too regard hashtag prediction as sequence prediction instead of prediction of a bag of orderless tag words. Since hashtags in a post tend to have strong co-occurrence relations, it is better to take previous hashtags into account to predict a next one. It will be validated by our experimental results."}, {"heading": "4.1. Construction of Context Memory", "text": "As in Figure 2(a), we construct the memory to store three types of context information: (i) image memory for representation of a query image, (ii) user context memory for TF-IDF weighted D frequent words from the query user\u2019s previous posts, and (iii) word output memory for previously generated words. Following [29], each input to the memory is embedded into input and output memory representation, for which we use superscript a and c, respectively.\nImage Memory. We represent images using ResNet101 [7] pretrained on the ImageNet 2012 dataset. We test two different descriptions: (7 \u00d7 7) feature maps of res5c layer, and pool5 feature vectors. The res5c feature map denoted by Ir5c \u2208 R2,048\u00d77\u00d77 is useful if a model exploits spatial attention; otherwise, the pool5 feature Ip5 \u2208 R2,048 is used as a feature vector of the image. Hence, the pool5 is inserted into a single memory cell, while the res5c feature map occupies 49 cells, on which the memory attention later can focus on different regions of an (7\u00d7 7) image grid. We will compare these two descriptors in the experiments.\nThe image memory vector mim \u2208 R1,024 for the res5c feature is represented by\nmaim,j = ReLU(W a imI r5c j + b a im), (1) mcim,j = ReLU(W c imI r5c j + b c im), (2)\nfor j = 1, . . . , 49. The parameters to learn include Wa,cim \u2208 R1,024\u00d72,048 and ba,cim \u2208 R1,024. The ReLU indicates an element-wise ReLU activation [19]. For the pool5, we use\nm a/c im,j = ReLU(W a/c im I p5 j + b a/c im ). (3)\nfor j = 1. In Eq.(3), we simply present two equations for input and output memory as a single one using superscript a/c. Without loss of generality, we below derive the formulation assuming that we use the res5c feature.\nUser Context Memory. In a personalized setting where the author of a query image is identifiable, we define {ui}Di=1 by selectingDmost frequent words from the user\u2019s previous posts. We input {ui}Di=1 into the user context memory in a decreasing order of scores, in order to exploit CNN later effectively. This context memory improves the model\u2019s performance by focusing more on the user\u2019s writing style of active vocabularies or hashtags. To build {ui}Di=1, we compute TF-IDF scores and select top-D words for a given user. Using TF-IDF scores means that we do not include too general terms that many users commonly use, because they are not helpful for personalization. Finally, the user context memory vector ma/cus \u2208 R1,024 becomes\nuaj = W a euj ,u c j = W c euj ;yj ; j \u2208 1, . . . , D (4)\nm a/c us,j = ReLU(Wh[u a/c j ] + bh), (5)\nwhere uj is a one-hot vector for j-th active word. Parameters include Wa/ce \u2208 R512\u00d7V and Wh \u2208 R1,024\u00d7512. We use the same Wh for both input and output memory, while we learn separate word embedding matrices Wa/ce .\nWord Output Memory. As shown in Figure 2(c), we insert a series of previously generated words y1, . . . , yt\u22121 into the word output memory, which is represented as\noaj = W a eyj ,o c j = W c eyj ; j \u2208 1, . . . , t\u2212 1 (6)\nm a/c ot,j = ReLU(Wh[o a/c j ] + bh). (7)\nwhere yj is a one-hot vector for j-th previous word. We use the same word embeddings Wa/ce and parameters Wh,bh with user context memory in Eq.(4). We update ma/cot,j for every iteration whenever a new word is generated.\nFinally, we concatenate the input and memory presentation of all memory types: Ma/ct = [m a/c im,1\u2295\u00b7 \u00b7 \u00b7\u2295m a/c im,49\u2295 m a/c us,1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 m a/c us,D \u2295 m a/c ot,1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 m a/c ot,t\u22121]. We use m to denote the memory size, which is the sum of sizes of three memory types: m = mim + mus + mot."}, {"heading": "4.2. State-Based Sequence Generation", "text": "RNNs and their variants have been widely used for sequence generation via recurrent connections throughout time. However, our approach does not involve any RNN module, but sequentially store all of previously generated words into the memory. It enables to predict each output word by selectively attending on the combinations of all previous words, image regions, and user context.\nWe now discuss how to predict a word yt at time step t based on the memory state (see Figure 2(b)). Letting onehot vector of previous word to yt\u22121, we first generate an input vector qt at time t to our memory network as\nqt = ReLU(Wqxt + bq), where xt = Wbeyt\u22121. (8)\nwhere Wbe \u2208 R512\u00d7V and Wq \u2208 R1,024\u00d7512 are learned. Next qt is fed into the attention model of context memory:\npt = softmax(Mat qt), Mot(\u2217, i) = pt \u25e6Mct(\u2217, i). (9)\nWe compute how well the input vector qt matches with each cell of memory Mat by a matrix multiplication followed by a softmax. That is, pt \u2208 Rm indicates the compatibility of qt overmmemory cells. Another interpretation is that pt indicates which part of input memory is important for input qt at current time step (i.e. to which part of memory the attention turns at time t [31]). Next we rescale each column of the output memory presentationMct \u2208 Rm\u00d71,024 by element-wise multiplication (denoted by \u25e6) with pt \u2208 Rm. As a result, we obtain the attended output memory representation Mot, which are decomposed into three memory types as Mot = [moim,1:49 \u2295m a/c us,1:D \u2295m a/c ot,1:t\u22121].\nMemory CNNs. We then apply a CNN to the attended output of memory Mot. As will be shown in our experiments, using a CNN significantly boosts the captioning performance. It is mainly due to that the CNN allows us to obtain a set of powerful representations by fusing multiple heterogeneous cells with different filters.\nWe define a set of three filters whose depth is 300 by changing window sizes h = [3, 4, 5]. We separately apply a single convolutional layer and max-pooling layer to each memory type. For h = [3, 4, 5],\nchim,t = maxpool(ReLU(w h im \u2217moim,1:49 + bhim)) (10)\nwhere \u2217 indicates the convolutional operation. Parameters include biases bhim \u2208 R49\u00d7300 and filters whim \u2208 R[3,4,5]\u00d71,024\u00d7300. Via max-pooling, each chim,t is reduced\nfrom (300\u00d7 [47, 46, 45]) to (300\u00d7 [1, 1, 1]). Finally, we obtain cim,t by concatenating chim,t from h = 3 to 5. We repeat the convolution and maxpooling operation of Eq.(13) to the other memory types as well. As a result, we obtain ct = [cim,t \u2295 cus,t \u2295 cot,t], whose dimension is 2, 700 = 3\u00d7 3\u00d7 300.\nNext we compute the output word probability st \u2208 RV : ht = ReLU(Woct + bo), (11) st = softmax(Wfht). (12)\nWe obtain the hidden state ht by Eq.(11) with a weight matrix Wo \u2208 R2,700\u00d72,700 and a bias bo \u2208 R2,700. We then compute the output probability st over vocabularies V by a softmax layer in Eq.(12).\nFinally, we select the word that attains the highest probability yt = argmaxs\u2208V(st). Unless the output word yt is the EOS token, we repeat generating a next word by feeding yt into the word output memory in Eq.(6) and the input of Eq.(8) at time step t + 1. As a simple post-processing only for hashtag prediction, we remove duplicate output hashtags. In summary, this inference is greedy in the sense that the model creates the best sequence by a sequential search for the best word at each time step."}, {"heading": "4.3. Training", "text": "To train our model, we adopt teacher forced learning that provide the correct memory state to predict next words. We use the softmax cross-entropy loss as the cost function for every time step predictions, which minimizes the negative log likelihood from the estimated yt to its corresponding target word yGT,t. We randomly initialize all the parameters with a uniform unit scaling of 1.0 factor: [\u00b1 \u221a 3/dim].\nWe apply mini-batch stochastic gradient descent. We select the Adam optimizer [11] with \u03b22 = 0.9, \u03b22 = 0.999 and \u03f5 = 1e \u2212 08. To speed up training, we use four GPUs for data parallelism, and set a batch size as 200 for each GPU. We earn the best results the initial learning rate is set as 0.001 for all the models. At every 5 epochs, we divide a learning rate by 1.2 to gradually decrease it. We train our models up to 20 epochs."}, {"heading": "5. Experiments", "text": "We compare the performance of our approach with other state-of-the-art models via quantitative measures and Amazon Mechanical Turk (AMT) studies."}, {"heading": "5.1. Experimental Setting", "text": "We use the image of a test post as a query and associated hashtags and text description as groundtruth (GT). For evaluation metrics of hashtag prediction, we compute the F1-score as a balanced average metric between precision and recall between predicted hashtag sets and GT\n18650018_@_56299423158975190\nsets: 2(1/precision+1/recall)\u22121. For evaluation measures of post generation, we compute the language similarity between predicted sentences and GTs. We exploit BLEU [21], CIDEr [26], METEOR [14], and ROUGE-r [15] scores. In all measures, higher scores indicate better performance.\nWe randomly split the dataset into 90% for training, 5K posts for test and the rest for validation. We divide the dataset by users so that training and test users are disjoint, in order to correctly measure the prediction power of methods. If users\u2019s posts exist both in training and test sets, then prediction can be often trivial by simply retrieving their closest posts in the training set.\nWhile some benchmark datasets for image captioning (e.g. Flickr30K [34] and MS COCO [16]) have multiple GTs (e.g. 5 sentences per image in the MS COCO), our dataset has only one GT post text and hashtag list per test example. Hence, the absolute metric values in this work may be lower than those in these benchmark datasets."}, {"heading": "5.2. Baselines", "text": "As baselines, we select multiple nearest neighbor approaches, one language generation algorithm, two state-ofthe-art image captioning methods, and multiple variants of our model. As straightforward baselines, we first test the 1-nearest search by images, denoted by (1NN-Im); for a query image, we find its closest training image using the \u21132 distance on ResNet pool5 descriptors, and return its text as prediction. Second, we test the 1-nearest search by users, denoted by (1NN-Usr); we find the nearest user whose 60 active vocabularies are overlapped the most with those of the query user, and then randomly select one post of the\nnearest user. The third nearest neighbor variant denoted by (1NN-UsrIm) is to find the 1-nearest image among the nearest user\u2019s images, and return its text as prediction.\nAs a language-only method, we use the sequenceto-sequence model by Vinyals et al. [27], denoted by (seq2seq). It is a recurrent neural network with three hidden LSTM layers, and originally applied to the language translation. This baseline takes 60 active words of the query user in a decreasing order of TF-IDF weights, and predicts captions. Since this baseline does not use an image for text generation, this comparison quantifies how much the image is important to predict hashtags or text.\nWe also compare with the two state-of-the-art image captioning methods with no personalization. The first baseline is (ShowTell) of [28], which is a multi-modal CNN and LSTM model. The second baseline is the attention-based captioning model of [31] denoted by (AttendTell).\nWe compare different variants of our method (CSMN-*). To validate the contribution of each component, we exclude one of key components from our model as follows: (i) without the memory CNN in section 4.2 denoted by (-NoCNN-), (ii) without user context memory denoted by (-NoUC-), and (iii) without feedback of previously generated words to output memory by (-NoWO-). That is, the (-NoCNN-) quantifies the performance improvement by the use of the memory CNN. The (-NoUC-) is the model without personalization; that is, it does not use the information about query users, such as their D active vocabularies. Finally, the (-NoWO-) is the model without sequential prediction. For hashtag prediction, the (-NoWO-) indicates the performance of separate tag generation instead of sequen-\nMethods B-1 B-2 B-3 B-4 METEOR CIDEr ROUGE-L (seq2seq) [27] 0.050 0.012 0.003 0.000 0.024 0.034 0.065\n(ShowTell)\u2217 [28] 0.055 0.019 0.007 0.003 0.038 0.004 0.081 (AttendTell)\u2217 [31] 0.106 0.015 0.000 0.000 0.026 0.049 0.140\n(1NN-Im)\u2217 0.071 0.020 0.007 0.004 0.032 0.059 0.069 (1NN-Usr) 0.063 0.014 0.002 0.000 0.028 0.025 0.059 (1NN-UsrIm) 0.106 0.032 0.011 0.005 0.046 0.084 0.104 (CSMN-NoCNN-P5) 0.086 0.037 0.015 0.000 0.037 0.103 0.122 (CSMN-NoUC-P5)\u2217 0.079 0.032 0.015 0.008 0.037 0.133 0.120 (CSMN-NoWO-P5) 0.090 0.040 0.016 0.006 0.037 0.119 0.116\n(CSMN-R5C) 0.097 0.034 0.013 0.006 0.040 0.107 0.110 (CSMN-P5) 0.171 0.068 0.029 0.013 0.064 0.214 0.177 (CSMN-W20-P5) 0.116 0.041 0.018 0.007 0.044 0.119 0.123 (CSMN-W100-P5) 0.109 0.037 0.015 0.007 0.042 0.109 0.112\nTable 2. Evaluation of post generation between different methods for the Instagram dataset. As performance measures, we use language similarity metrics (BLEU, CIDEr, METEOR, ROUGE-L). The methods with [\u2217] use no personalization.\nMethods F1 score (seq2seq) [27] 0.132 0.085 (ShowTell)\u2217 [28] 0.028 0.011 (AttendTell)\u2217 [31] 0.020 0.014\n(1NN-Im)\u2217 0.049 0.110 (1NN-Usr) 0.054 0.173 (1NN-UsrIm) 0.109 0.380 (CSMN-NoCNN-P5) 0.135 0.310 (CSMN-NoUC-P5)\u2217 0.111 0.076 (CSMN-NoWO-P5) 0.117 0.244\n(CSMN-R5C) 0.192 0.340 (CSMN-P5) 0.230 0.390 (CSMN-W20-P5) 0.147 0.349 (CSMN-W80-P5) 0.135 0.341\nTable 3. Evaluation of hashtag prediction. We show test results for split by users in the left and split by posts in the right.\ntial prediction of our original proposal. We also test two different image descriptors in section 4.1: (7\u00d77) res5c feature maps and pool5 feature vectors, denoted by (-R5C) and (-P5) respectively. Finally, we also evaluate the effects on the sizes of user context memory: (-W20-) and (-W80-) or (-W100-)."}, {"heading": "5.3. Quantitative Results", "text": "Table 2 and 3 summarize the quantitative results of post generation and hashtag prediction, respectively. Since algorithms show similar patterns in both tasks, we below analyze the experimental results together.\nFirst of all, according to most metrics in both tasks, our approach (CSMN-*) significantly outperforms baselines. We can divide the algorithms into two groups with or without personalization; the latter includes (ShowTell), (AttendTell), (1NN-Im), and (CSMN-NoUC-P5), while the former comprises the other methods. Our (CSMN-NoUC-P5) ranks the first among the methods with no personalization, while the (CSMN-P5) achieves the best overall. Interestingly, using a pool5 feature vector as image description that occupies only a single memory slot leads better performance than using (7 \u00d7 7) res5c feature maps with 49 slots. It is mainly due to that attention learning quickly becomes harder with a larger dimension of image representation. Another reason could be that users do not tent to discuss in the level of details about individual (7\u00d77) image grids, and thus a holistic view of the image content is sufficient for prediction of users\u2019 text.\nWe summarize other interesting observations as follows. First, among the baselines, the simple nearest neighbor approach (1NN-UsrIm) turns out to be the strongest candidate. Second, our approach becomes significantly worsen, if we remove one of key components, such as memory CNN, personalization, and sequential prediction. Third, among the tested memory sizes of user context, the best performance is obtained with 60. With larger memory sizes, attention learning becomes harder. Moreover, we choose the size of 60 based on the statistics of our dataset; with a too\nlarger size, there are many empty slots, which also make attention learning difficult. Finally, we observe that post generation is more challenging than hashtag prediction. It is due to that the expression space of post generation is much larger because the post text includes any combinations of words, emojis, and symbols.\nGiven that Instagram provides a function of hashtag automation from previous posts when writing a new post, we test another dataset split for hashtag prediction. That is, we divide dataset by posts so that each user\u2019s posts are included in both training and test sets. We call this as split by posts, while the original split as split by users. We observe that, due to the automation function, many posts in our training and test set have almost identical hashtags. This setting is highly favorable for (1NN-UsrIm), which returns the text of the closest training image of the query user. Table 3 shows the results of split by users in the left and split by posts in the right. Interestingly, our (CSMN-P5) works better than (1NN-UsrIm) even in the setting of split by posts, although its performance margin (i.e. 0.01 in F1 score) is not as significant as in the split by users (i.e. 0.121)."}, {"heading": "5.4. User Studies via Amazon Mechanical Turk", "text": "We perform AMT tests to observe general users\u2019 preferences between different algorithms for the two post automation tasks. For each task, we randomly sample 100 test examples. At test, we show a query image and three randomly sampled complete posts of the query user as a personalization clue, and two text descriptions generated by our method and one baseline in a random order. We ask turkers to choose more relevant one among the two. We obtain answers from three different turkers for each query. We select the variant (CSMN-P5) as a representative of our method, because of its best quantitative performance. We compare with three baselines by selecting the best method in each group of 1NNs, image captioning, and language-only methods: (1NN-UsrIm), (ShowTell), and (seq2seq).\nTable 4 summarize the results of AMT tests, which validate that human annotators significantly prefer our results\nto those of baselines. Among the baselines, (1NN-UsrIm) is preferred the most, given that the performance gap with our approach is the smallest. These results coincide with those of quantitative evaluation in Table 2 and 3."}, {"heading": "5.5. Qualitative Results", "text": "Figure 3 illustrates selected examples of post generation. In each set, we show a query image, GT, and generated text description by our method and baselines. In many of Instagram examples, GT comments are hard to correctly predict, because they are extremely diverse, subjective, and private conversation over a variety of topics. Nonetheless, most of predicted text descriptions are relevant to the query images. Moreover, our CSMN model is able to appropriately use normal words, emojis, and even mentions to other users (anonymized by @username). Figure 4 shows examples of hashtag prediction. We observe that our hashtag prediction is robust even with a variety of topics, including profiles, food, fashion, and interior design.\nFigure 5 shows examples of how much hashtag and post prediction vary according to different users for the same query images. Although predicted results change in terms\nof used words, they are relevant and meaningful for the query images. Figure 6 illustrates the variation of text predictions according to query images for the same user. We first select a user whose most of posts are about design, and then obtain prediction by changing the query images. For design-related query images, the CSMN predicts relevant hashtags for the design topic (Figure 6(a)). For the query images of substantially different topics, our CSMN is also resilient to predict relevant hashtags (Figure 6(b))."}, {"heading": "6. Conclusions", "text": "We proposed the context sequence memory networks (CSMN) as a first personalized image captioning approach. We addressed two post automation tasks: hashtag prediction and post generation. With quantitative evaluation and AMT user studies on nearly collected Instagram dataset, we showed that our CSMN approach outperformed other stateof-the-art captioning models. There are several promising future directions that go beyond this work. First, we can extend the CSMNmodel for another interesting related task such as post commenting that generates a thread of replies for a given post. Second, since we dealt with only Instagram posts in this work, we can explore data in other social networks such as Flickr, Pinterest, or Tumblr, which have different post types, metadata, and text and hashtag usages.\nAcknowledgements. This research is partially supported by Hancom and Basic Science Research Program through National Research Foundation of Korea (2015R1C1A1A02036562). Gunhee Kim is the corresponding author."}, {"heading": "A. Why using memory CNN can be helpful?", "text": "While conventional memory networks cannot model the structural ordering unless time embedding is added to the model, we propose to exploit the memory CNN to model the structural ordering with much stronger representation power. More specifically, for the word output memory, the CNN is useful to represent the sequential order of generated words. For the user context memory, the CNN can correctly capture the importance order of the context words, given that we feed the user\u2019s frequent words into the context memory in a decreasing order of TF-IDF weighted scores (rather than putting them in a random order).\nFor example, suppose that a user can have active words related to fashion, street and landscape in the user context memory. If the fashion-related words are at the top of the user context memory, and the street is located between fashion-related words, this user can be modeled to be interested in street fashion. On the other hand, if the landscape-related words are at the top of the memory, and the street is located between landscape-related words, this user can be modeled to be interested in landscape since the meaning of street can be interpreted similarly as landscape. Without the memory CNN, it can be difficult to distinguish between these two different uses of street."}, {"heading": "B. Details of Dataset Collection", "text": "27 general categories of Pinterest. For dataset collection, we select 270 search keywords by gathering the 10 most common hashtags for each of the following 27 general categories of Pinterest: celebrities, design, education, food, drink, gardening, hair, health, fitness, history, humor, decor, outdoor, illustration, quotes, product, sports, technology, travel, wedding, tour, car, football, animal, pet, fashion and worldcup.\nDataset statistics. We show the cumulative distribution functions (CDFs) of the words for captions and hashtags in Figure 7. Based on the statistics, we set the vocabulary size for captions as 40K and for hashtags as 60K. For captions, top 40K most frequent words take 97.3% of all the word occurrences, indicating sufficient coverage for the vocabulary usage in the Instagram. On the other hand, hashtags in Instagram show extremely diverse neologism. The dictionary of 60K vocabularies cover only 84.31%, but we set 60K because the increase of the CDF is very slow even with increasing the dictionary size.\nWe also show some statistics of the active vocabularies per user for captions and hashtags in Table 5. We set the sizes of user context memory based on these values. If the memory size is too small, then the memory cannot correctly capture the users\u2019 writing style. On the other hand, if the memory size is too large, the learning becomes hard and performance decreases. We set the sizes so that they\nroughly correspond to the sum of mean and standard deviation values)."}, {"heading": "C. Model Details", "text": "In the main draft, we describe our model assuming that we use the res5c features Ir5c \u2208 R2,048\u00d77\u00d77 for image representation. Here we discuss some formulation changes when we use the pool5 features Ip5 \u2208 R2,048.\nFor memory CNNs, we represent its output of memory for res5c features as follows (i.e. Eq.(10) of the main draft).\nchim,t = maxpool(ReLU(w h im \u2217moim,1:49 + bhim)) (13)\nWhen we use pool5 features, Eq.(13) is changed to\nchim,t = w h imm o im,1 + b h im. (14)\nwhere bhim \u2208 R1,800, wimh \u2208 R1,024\u00d71,800, and chim,t \u2208 R1,800. After experiments, we found out that adding ReLu on Eq.(14) slightly improves performance.\nFinally, the memory output concatenation for res5c features, which is represented by\nct = [cim,t \u2295 cus,t \u2295 cot,t], (15)\nis also changed to, when we use pool5 features,\nct = cim,t + [cus,t \u2295 cot,t]. (16)\nThe dimension of ct is changed from 2, 700 = 3 \u00d7 3 \u00d7 300 for rec5c features to 1, 800 = 2 \u00d7 3 \u00d7 300 for pool5 features."}, {"heading": "D. Training Details", "text": "Although our model can take variable-length sequences as input, to speed up training, it is better for a minibatch to consist of sentences with the same length. Therefore, we randomly group training samples to a set of minibatches, each of which have the same length as possible. We then randomly shuffle a batch order so that short and long minibatches are mixed. It empirically leads a better training as a curriculum learning in [35] proposes."}], "references": [{"title": "Learning to Transfer: Transferring Latent Task Structures and Its Application to Person-specific Facial Action Unit Detection", "author": ["T. Almaev", "B. Martinez", "M. Valstar"], "venue": "ICCV,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "User Conditional Hashtag Prediction for Images", "author": ["E. Denton", "J. Weston", "M. Paluri", "L. Bourdev", "R. Fergus"], "venue": "KDD,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "From Captions to Visual Concepts and Back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Dollar", "J. Gao", "X. He", "M. Mitchell", "J. Platt", "L. Zitnick", "G. Zweig"], "venue": "CVPR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Turing Machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv:1410.5401,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid Computing Using a Neural Network with Dynamic", "author": ["A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska-Barwi\u0144ska", "S.G. Colmenarejo", "E. Grefenstette", "T. Ramalho", "J. Agapiou"], "venue": "External Memory. Nature,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Unconstrained Realtime Facial Performance Capture", "author": ["P.-L. Hsieh", "C. Ma", "J. Yu", "H. Li"], "venue": "CVPR,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Personalized Handwriting Recognition via Biased Regularization", "author": ["W. Kienzle", "K. Chellapilla"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "ADAM: AMethod For Stochastic Optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "ICLR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal Neural Language Models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "ICML,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask me Anything: Dynamic Memory Networks for Natural Language Processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments", "author": ["S.B.A. Lavie"], "venue": "ACL,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author": ["C.-Y. Lin"], "venue": "WAS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Microsoft COCO: Common Objects in Context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "ECCV,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Key-Value Memory Networks for Directly Reading Documents", "author": ["A. Miller", "A. Fisch", "J. Dodge", "A.-H. Karimi", "A. Bordes", "J. Weston"], "venue": "EMNLP,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Motivating Personality-aware Machine Translation", "author": ["S. Mirkin", "S. Nowson", "C. Brun", "J. Perez"], "venue": "EMNLP,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Im2Text: Describing Images Using 1 Million Captioned Photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "NIPS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "ACL,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Expressing an Image Stream with a Sequence of Natural Sentences", "author": ["C.C. Park", "G. Kim"], "venue": "NIPS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Personalized Mathematical Word Problem Generation", "author": ["O. Polozov", "E. ORourke", "A.M. Smith", "L. Zettlemoyer", "S. Gulwani", "Z. Popovic"], "venue": "IJCAI,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to- End Memory Networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "NIPS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q. Le"], "venue": "NIPS,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "CIDEr: Consensus-based Image Description Evaluation", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "CVPR,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Grammar as a Foreign Language", "author": ["O. Vinyals", "\u0141. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "NIPS,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "IEEE TPMAI, 39(4):652\u2013663,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Memory Networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "ICLR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks", "author": ["R.J. Williams", "D. Zipser"], "venue": "NIPS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1989}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Gesture Recognition Portfolios for Personalization", "author": ["A. Yao", "L. Van Gool", "P. Kohli"], "venue": "CVPR,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Image Captioning with Semantic Attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference over Event Descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "TACL,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to Execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv:1410.4615,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 3, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 8, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 11, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 19, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 21, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 27, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 30, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 32, "context": "Image captioning is a task of automatically generating a descriptive sentence of an image [3, 4, 9, 12, 20, 22, 28, 31, 33].", "startOffset": 90, "endOffset": 123}, {"referenceID": 4, "context": "Our model is inspired by memory networks [5, 24, 29], which explicitly include memory components to which neural networks read and write data for capturing long-term information.", "startOffset": 41, "endOffset": 52}, {"referenceID": 23, "context": "Our model is inspired by memory networks [5, 24, 29], which explicitly include memory components to which neural networks read and write data for capturing long-term information.", "startOffset": 41, "endOffset": 52}, {"referenceID": 28, "context": "Our model is inspired by memory networks [5, 24, 29], which explicitly include memory components to which neural networks read and write data for capturing long-term information.", "startOffset": 41, "endOffset": 52}, {"referenceID": 2, "context": "[3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history.", "startOffset": 0, "endOffset": 23}, {"referenceID": 21, "context": "[3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history.", "startOffset": 0, "endOffset": 23}, {"referenceID": 24, "context": "[3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history.", "startOffset": 0, "endOffset": 23}, {"referenceID": 27, "context": "[3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history.", "startOffset": 0, "endOffset": 23}, {"referenceID": 30, "context": "[3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history.", "startOffset": 0, "endOffset": 23}, {"referenceID": 32, "context": "[3, 22, 25, 28, 31, 33]), which predict a word at every time step, based on only a current input and a single or a few hidden states as an implicit summary of all previous history.", "startOffset": 0, "endOffset": 23}, {"referenceID": 29, "context": "By using teacherforced learning [30], our model has a markov property at training time; predicting a previous word yt\u22121 has no effect on predicting a next word yt, which depends on only the current memory state.", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "Original memory networks [24, 29] leverage time embedding to model the memory order.", "startOffset": 25, "endOffset": 33}, {"referenceID": 28, "context": "Original memory networks [24, 29] leverage time embedding to model the memory order.", "startOffset": 25, "endOffset": 33}, {"referenceID": 26, "context": "[27, 28, 31]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 27, "context": "[27, 28, 31]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 30, "context": "[27, 28, 31]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 20, "context": "BLEU [21], CIDEr [26], METEOR [14], and ROUGE [15]) and user studies via Amazon Mechanical Turk.", "startOffset": 5, "endOffset": 9}, {"referenceID": 25, "context": "BLEU [21], CIDEr [26], METEOR [14], and ROUGE [15]) and user studies via Amazon Mechanical Turk.", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": "BLEU [21], CIDEr [26], METEOR [14], and ROUGE [15]) and user studies via Amazon Mechanical Turk.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": "BLEU [21], CIDEr [26], METEOR [14], and ROUGE [15]) and user studies via Amazon Mechanical Turk.", "startOffset": 46, "endOffset": 50}, {"referenceID": 26, "context": "(4) With quantitative evaluation and user studies via AMT, we demonstrate the effectiveness of three novel features of CSMN and its performance superiority over stateof-the-art captioning models, including [27, 28, 31].", "startOffset": 206, "endOffset": 218}, {"referenceID": 27, "context": "(4) With quantitative evaluation and user studies via AMT, we demonstrate the effectiveness of three novel features of CSMN and its performance superiority over stateof-the-art captioning models, including [27, 28, 31].", "startOffset": 206, "endOffset": 218}, {"referenceID": 30, "context": "(4) With quantitative evaluation and user studies via AMT, we demonstrate the effectiveness of three novel features of CSMN and its performance superiority over stateof-the-art captioning models, including [27, 28, 31].", "startOffset": 206, "endOffset": 218}, {"referenceID": 2, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 3, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 8, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 11, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 19, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 21, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 27, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 30, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 32, "context": "In recent years, much work has been published on image captioning, including [3, 4, 9, 12, 20, 22, 28, 31, 33], to name a few.", "startOffset": 77, "endOffset": 110}, {"referenceID": 2, "context": "For example, long-term recurrent convolutional networks [3] are one of earliest model to use RNNs for modeling the relations between sequential inputs and outputs.", "startOffset": 56, "endOffset": 59}, {"referenceID": 32, "context": "[33] exploit semantic attention to combine top-down and bottom-up strategies to extract richer information from images, and couples it with an LSTM decoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23].", "startOffset": 102, "endOffset": 123}, {"referenceID": 7, "context": "There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23].", "startOffset": 102, "endOffset": 123}, {"referenceID": 0, "context": "There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23].", "startOffset": 102, "endOffset": 123}, {"referenceID": 31, "context": "There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23].", "startOffset": 102, "endOffset": 123}, {"referenceID": 9, "context": "There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23].", "startOffset": 102, "endOffset": 123}, {"referenceID": 22, "context": "There have been many studies about personalization in computer vision and natural language processing [2, 8, 1, 32, 10, 23].", "startOffset": 102, "endOffset": 123}, {"referenceID": 1, "context": "[2] develop a CNN model that predicts hashtags from image content and user information.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[32] propose a domain adaptation approach to classify user-specific human gestures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] adopt a transfer learning framework to detect person-specific facial action unit detection.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] enhance machine translation performance by exploiting personal traits.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] generate personalized mathematical word problem for a given tutor/student specification by logic programming.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Neural Turing Machines [5] use external memory to solve algorithmic problems such as sorting and copying.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "Later, this architecture is extended to Differential Neural Computer (DNC) [6] to solve more complicated algorithmic problems such as finding shortest path and graph traversal.", "startOffset": 75, "endOffset": 78}, {"referenceID": 28, "context": "[29] propose one of the earliest memory network models for natural language question answering (QA), and later Sukhbaatar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] modify the network to be trainable in an end-to-end manner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] and Milleret al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] address language QA tasks proposing novel memory networks such as dynamic networks with episodic memory in [13] and key-value memory networks in [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[17] address language QA tasks proposing novel memory networks such as dynamic networks with episodic memory in [13] and key-value memory networks in [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "[17] address language QA tasks proposing novel memory networks such as dynamic networks with episodic memory in [13] and key-value memory networks in [17].", "startOffset": 150, "endOffset": 154}, {"referenceID": 28, "context": "Following [29], each input to the memory is embedded into input and output memory representation, for which we use superscript a and c, respectively.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "We represent images using ResNet101 [7] pretrained on the ImageNet 2012 dataset.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "The ReLU indicates an element-wise ReLU activation [19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 30, "context": "to which part of memory the attention turns at time t [31]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "We define a set of three filters whose depth is 300 by changing window sizes h = [3, 4, 5].", "startOffset": 81, "endOffset": 90}, {"referenceID": 3, "context": "We define a set of three filters whose depth is 300 by changing window sizes h = [3, 4, 5].", "startOffset": 81, "endOffset": 90}, {"referenceID": 4, "context": "We define a set of three filters whose depth is 300 by changing window sizes h = [3, 4, 5].", "startOffset": 81, "endOffset": 90}, {"referenceID": 2, "context": "For h = [3, 4, 5],", "startOffset": 8, "endOffset": 17}, {"referenceID": 3, "context": "For h = [3, 4, 5],", "startOffset": 8, "endOffset": 17}, {"referenceID": 4, "context": "For h = [3, 4, 5],", "startOffset": 8, "endOffset": 17}, {"referenceID": 2, "context": "Parameters include biases bim \u2208 R49\u00d7300 and filters w im \u2208 R[3,4,5]\u00d71,024\u00d7300.", "startOffset": 60, "endOffset": 67}, {"referenceID": 3, "context": "Parameters include biases bim \u2208 R49\u00d7300 and filters w im \u2208 R[3,4,5]\u00d71,024\u00d7300.", "startOffset": 60, "endOffset": 67}, {"referenceID": 4, "context": "Parameters include biases bim \u2208 R49\u00d7300 and filters w im \u2208 R[3,4,5]\u00d71,024\u00d7300.", "startOffset": 60, "endOffset": 67}, {"referenceID": 0, "context": "Via max-pooling, each cim,t is reduced from (300\u00d7 [47, 46, 45]) to (300\u00d7 [1, 1, 1]).", "startOffset": 73, "endOffset": 82}, {"referenceID": 0, "context": "Via max-pooling, each cim,t is reduced from (300\u00d7 [47, 46, 45]) to (300\u00d7 [1, 1, 1]).", "startOffset": 73, "endOffset": 82}, {"referenceID": 0, "context": "Via max-pooling, each cim,t is reduced from (300\u00d7 [47, 46, 45]) to (300\u00d7 [1, 1, 1]).", "startOffset": 73, "endOffset": 82}, {"referenceID": 10, "context": "We select the Adam optimizer [11] with \u03b22 = 0.", "startOffset": 29, "endOffset": 33}, {"referenceID": 20, "context": "We exploit BLEU [21], CIDEr [26], METEOR [14], and ROUGE-r [15] scores.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "We exploit BLEU [21], CIDEr [26], METEOR [14], and ROUGE-r [15] scores.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "We exploit BLEU [21], CIDEr [26], METEOR [14], and ROUGE-r [15] scores.", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "We exploit BLEU [21], CIDEr [26], METEOR [14], and ROUGE-r [15] scores.", "startOffset": 59, "endOffset": 63}, {"referenceID": 33, "context": "Flickr30K [34] and MS COCO [16]) have multiple GTs (e.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "Flickr30K [34] and MS COCO [16]) have multiple GTs (e.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "[27], denoted by (seq2seq).", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The first baseline is (ShowTell) of [28], which is a multi-modal CNN and LSTM model.", "startOffset": 36, "endOffset": 40}, {"referenceID": 30, "context": "The second baseline is the attention-based captioning model of [31] denoted by (AttendTell).", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "(seq2seq) [27] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 27, "context": "065 (ShowTell)\u2217 [28] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": "081 (AttendTell)\u2217 [31] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 26, "context": "(seq2seq) [27] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 27, "context": "085 (ShowTell)\u2217 [28] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": "011 (AttendTell)\u2217 [31] 0.", "startOffset": 18, "endOffset": 22}], "year": 2017, "abstractText": "We address personalization issues of image captioning, which have not been discussed yet in previous research. For a query image, we aim to generate a descriptive sentence, accounting for prior knowledge such as the user\u2019s active vocabularies in previous documents. As applications of personalized image captioning, we tackle two post automation tasks: hashtag prediction and post generation, on our newly collected Instagram dataset, consisting of 1.1M posts from 6.3K users. We propose a novel captioning model named Context Sequence Memory Network (CSMN). Its unique updates over previous memory network models include (i) exploiting memory as a repository for multiple types of context information, (ii) appending previously generated words into memory to capture long-term information without suffering from the vanishing gradient problem, and (iii) adopting CNN memory structure to jointly represent nearby ordered memory slots for better context understanding. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the effectiveness of the three novel features of CSMN and its performance enhancement for personalized image captioning over state-of-the-art captioning models.", "creator": "LaTeX with hyperref package"}}}