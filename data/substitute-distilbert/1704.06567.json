{"id": "1704.06567", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Attention Strategies for Multi-Source Sequence-to-Sequence Learning", "abstract": "modeling attention in neural multi - source sequence - to - context learning remains a relatively unexplored difficulty, despite its usefulness in tasks that satisfy independent source languages or modalities. we propose two novel approaches to combine the outputs of attention mechanisms over four source sequence, flat and hierarchical. we compare the possible methods with existing techniques and present results of systematic evaluation of those methods on the classical multimodal translation and automatic post - editing tasks. we show that the proposed methods achieve consistently results satisfying each tasks.", "histories": [["v1", "Fri, 21 Apr 2017 14:39:27 GMT  (186kb,D)", "http://arxiv.org/abs/1704.06567v1", "7 pages; Accepted to ACL 2017"]], "COMMENTS": "7 pages; Accepted to ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["jindrich libovick\u00fd", "jindrich helcl"], "accepted": true, "id": "1704.06567"}, "pdf": {"name": "1704.06567.pdf", "metadata": {"source": "CRF", "title": "Attention Strategies for Multi-Source Sequence-to-Sequence Learning", "authors": ["Jind\u0159ich Libovick\u00fd"], "emails": ["helcl}@ufal.mff.cuni.cz"], "sections": [{"heading": "1 Introduction", "text": "Sequence-to-sequence (S2S) learning with attention mechanism recently became the most successful paradigm with state-of-the-art results in machine translation (MT) (Bahdanau et al., 2014; Sennrich et al., 2016a), image captioning (Xu et al., 2015; Lu et al., 2016), text summarization (Rush et al., 2015) and other NLP tasks.\nAll of the above applications of S2S learning make use of a single encoder. Depending on the modality, it can be either a recurrent neural network (RNN) for textual input data, or a convolutional network for images.\nIn this work, we focus on a special case of S2S learning with multiple input sequences of possibly different modalities and a single output-generating recurrent decoder. We explore various strategies the decoder can employ to attend to the hidden states of the individual encoders.\nThe existing approaches to this problem do not explicitly model different importance of the inputs to the decoder (Firat et al., 2016; Zoph and Knight,\n2016). In multimodal MT (MMT), where an image and its caption are on the input, we might expect the caption to be the primary source of information, whereas the image itself would only play a role in output disambiguation. In automatic post-editing (APE), where a sentence in a source language and its automatically generated translation are on the input, we might want to attend to the source text only in case the model decides that there is an error in the translation.\nWe propose two interpretable attention strategies that take into account the roles of the individual source sequences explicitly\u2014flat and hierarchical attention combination.\nThis paper is organized as follows: In Section 2, we review the attention mechanism in single-source S2S learning. Section 3 introduces new attention combination strategies. In Section 4, we evaluate the proposed models on the MMT and APE tasks. We summarize the related work in Section 5, and conclude in Section 6."}, {"heading": "2 Attentive S2S Learning", "text": "The attention mechanism in S2S learning allows an RNN decoder to directly access information about the input each time before it emits a symbol. Inspired by content-based addressing in Neural Turing Machines (Graves et al., 2014), the attention mechanism estimates a probability distribution over the encoder hidden states in each decoding step. This distribution is used for computing the context vector\u2014the weighted average of the encoder hidden states\u2014as an additional input to the decoder.\nThe standard attention model as described by Bahdanau et al. (2014) defines the attention energies eij , attention distribution \u03b1ij , and the con-\nar X\niv :1\n70 4.\n06 56\n7v 1\n[ cs\n.C L\n] 2\n1 A\npr 2\n01 7\ntext vector ci in i-th decoder step as:\neij = v > a tanh(Wasi + Uahj), (1)\n\u03b1ij = exp(eij)\u2211Tx k=1 exp(eik) , (2)\nci = Tx\u2211 j=1 \u03b1ijhj . (3)\nThe trainable parameters Wa and Ua are projection matrices that transform the decoder and encoder states si and hj into a common vector space and va is a weight vector over the dimensions of this space. Tx denotes the length of the input sequence. For the sake of clarity, bias terms (applied every time a vector is linearly projected using a weight matrix) are omitted.\nRecently, Lu et al. (2016) introduced sentinel gate, an extension of the attentive RNN decoder with LSTM units (Hochreiter and Schmidhuber, 1997). We adapt the extension for gated recurrent units (GRU) (Cho et al., 2014), which we use in our experiments:\n\u03c8i = \u03c3(Wyyi +Wssi\u22121) (4)\nwhere Wy and Ws are trainable parameters, yi is the embedded decoder input, and si\u22121 is the previous decoder state.\nAnalogically to Equation 1, we compute a scalar energy term for the sentinel:\ne\u03c8i = v > a tanh ( Wasi + U (\u03c8) a (\u03c8i si) ) (5)\nwhere Wa, U (\u03c8) a are the projection matrices, va is the weight vector, and \u03c8i si is the sentinel vector. Note that the sentinel energy term does not depend on any hidden state of any encoder. The sentinel vector is projected to the same vector space as the encoder state hj in Equation 1. The term e\u03c8i is added as an extra attention energy term to Equation 2 and the sentinel vector \u03c8i si is used as the corresponding vector in the summation in Equation 3.\nThis technique should allow the decoder to choose whether to attend to the encoder or to focus on its own state and act more like a language model. This can be beneficial if the encoder does not contain much relevant information for the current decoding step."}, {"heading": "3 Attention Combination", "text": "In S2S models with multiple encoders, the decoder needs to be able to combine the attention information collected from the encoders.\nA widely adopted technique for combining multiple attention models in a decoder is concatenation of the context vectors c(1)i , . . . , c (N) i (Zoph and Knight, 2016; Firat et al., 2016). As mentioned in Section 1, this setting forces the model to attend to each encoder independently and lets the attention combination to be resolved implicitly in the subsequent network layers.\nIn this section, we propose two alternative strategies of combining attentions from multiple encoders. We either let the decoder learn the \u03b1i distribution jointly over all encoder hidden states (flat attention combination) or factorize the distribution over individual encoders (hierarchical combination).\nBoth of the alternatives allow us to explicitly compute distribution over the encoders and thus interpret how much attention is paid to each encoder at every decoding step."}, {"heading": "3.1 Flat Attention Combination", "text": "Flat attention combination projects the hidden states of all encoders into a shared space and then computes an arbitrary distribution over the projections. The difference between the concatenation of the context vectors and the flat attention combination is that the \u03b1i coefficients are computed jointly for all encoders:\n\u03b1 (k) ij = exp(e (k) ij )\u2211N\nn=1 \u2211T (n)x m=1 exp ( e (n) im ) (6) where T (n)x is the length of the input sequence of the n-th encoder and e(k)ij is the attention energy of the j-th state of the k-th encoder in the i-th decoding step. These attention energies are computed as in Equation 1. The parameters va andWa are shared among the encoders, and Ua is different for each encoder and serves as an encoder-specific projection of hidden states into a common vector space.\nThe states of the individual encoders occupy different vector spaces and can have a different dimensionality, therefore the context vector cannot be computed as their weighted sum. We project\nthem into a single space using linear projections:\nci = N\u2211 k=1 T (k) x\u2211 j=1 \u03b1 (k) ij U (k) c h (k) j (7)\nwhere U (k)c are additional trainable parameters. The matrices U (k)c project the hidden states into a common vector space. This raises a question whether this space can be the same as the one that is projected into in the energy computation using matrices U (k)a in Equation 1, i.e., whether U (k) c = U (k) a . In our experiments, we explore both options. We also try both adding and not adding the sentinel \u03b1(\u03c8)i U (\u03c8) c (\u03c8i si) to the context vector."}, {"heading": "3.2 Hierarchical Attention Combination", "text": "The hierarchical attention combination model computes every context vector independently, similarly to the concatenation approach. Instead of concatenation, a second attention mechanism is constructed over the context vectors.\nWe divide the computation of the attention distribution into two steps: First, we compute the context vector for each encoder independently using Equation 3. Second, we project the context vectors (and optionally the sentinel) into a common space (Equation 8), we compute another distribution over the projected context vectors (Equation 9) and their corresponding weighted average (Equation 10):\ne (k) i = v > b tanh(Wbsi + U (k) b c (k) i ), (8)\n\u03b2 (k) i = exp(e (k) i )\u2211N\nn=1 exp(e (n) i )\n, (9)\nci = N\u2211 k=1 \u03b2 (k) i U (k) c c (k) i (10)\nwhere c(k)i is the context vector of the k-th encoder, additional trainable parameters vb and Wb are shared for all encoders, and U (k)b and U (k) c are encoder-specific projection matrices, that can be set equal and shared, similarly to the case of flat attention combination."}, {"heading": "4 Experiments", "text": "We evaluate the attention combination strategies presented in Section 3 on the tasks of multimodal translation (Section 4.1) and automatic post-editing (Section 4.2).\nThe models were implemented using the Neural Monkey sequence-to-sequence learning toolkit (Helcl and Libovicky\u0301, 2017).1 In both setups, we process the textual input with bidirectional GRU network (Cho et al., 2014) with 300 units in the hidden state in each direction and 300 units in embeddings. For the attention projection space, we use 500 hidden units. We optimize the network to minimize the output cross-entropy using the Adam algorithm (Kingma and Ba, 2014) with learning rate 10\u22124."}, {"heading": "4.1 Multimodal Translation", "text": "The goal of multimodal translation (Specia et al., 2016) is to generate target-language image captions given both the image and its caption in the source language.\nWe train and evaluate the model on the Multi30k dataset (Elliott et al., 2016). It consists of 29,000 training instances (images together with English captions and their German translations), 1,014 validation instances, and 1,000 test instances. The results are evaluated using the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011).\nIn our model, the visual input is processed with a pre-trained VGG 16 network (Simonyan and Zisserman, 2014) without further fine-tuning. Attention distribution over the visual input is computed from the last convolutional layer of the network.\n1http://github.com/ufal/neuralmonkey\nThe decoder is an RNN with 500 conditional GRU units (Firat and Cho, 2016) in the recurrent layer. We use byte-pair encoding (Sennrich et al., 2016b) with a vocabulary of 20,000 subword units shared between the textual encoder and the decoder.\nThe results of our experiments in multimodal MT are shown in Table 1. We achieved the best results using the hierarchical attention combination without the sentinel mechanism, which also showed the fastest convergence. The flat combination strategy achieves similar results eventually. Sharing the projections for energy and context vector computation does not improve over the concatenation baseline and slows the training almost prohibitively. Multimodal models were not able to surpass the textual baseline (BLEU 33.0).\nUsing the conditional GRU units brought an improvement of about 1.5 BLEU points on average, with the exception of the concatenation scenario where the performance dropped by almost 5 BLEU points. We hypothesize this is caused by the fact the model has to learn the implicit attention combination on multiple places \u2013 once in the output projection and three times inside the conditional GRU unit (Firat and Cho, 2016, Equations 10-12). We thus report the scores of the introduced attention combination techniques trained with conditional GRU units and compare them with the concatenation baseline trained with plain GRU units."}, {"heading": "4.2 Automatic MT Post-editing", "text": "Automatic post-editing is a task of improving an automatically generated translation given the source sentence where the translation system is treated as a black box.\nWe used the data from the WMT16 APE Task (Bojar et al., 2016; Turchi et al., 2016), which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain. Each triplet contains an English source sentence, an automatically generated German translation of the source sentence, and a manually post-edited German sentence as a reference. In case of this dataset, the MT outputs are almost perfect in and only little effort was required to post-edit the sentences. The results are evaluated using the humantargeted error rate (HTER) (Snover et al., 2006) and BLEU score (Papineni et al., 2002).\nFollowing Libovicky\u0301 et al. (2016), we encode the target sentence as a sequence of edit operations\ntransforming the MT output into the reference. By this technique, we prevent the model from paraphrasing the input sentences. The decoder is a GRU network with 300 hidden units. Unlike in the MMT setup (Section 4.1), we do not use the conditional GRU because it is prone to overfitting on the small dataset we work with.\nThe models were able to slightly, but significantly improve over the baseline \u2013 leaving the MT output as is (HTER 24.8). The differences between the attention combination strategies are not significant."}, {"heading": "5 Related Work", "text": "Attempts to use S2S models for APE are relatively rare (Bojar et al., 2016). Niehues et al. (2016) concatenate both inputs into one long sequence, which forces the encoder to be able to work with both source and target language. Their attention is then similar to our flat combination strategy; however, it can only be used for sequential data.\nThe best system from the WMT\u201916 competition (Junczys-Dowmunt and Grundkiewicz, 2016) trains two separate S2S models, one translating from MT output to post-edited targets and the second one from source sentences to post-edited targets. The decoders average their output distributions similarly to decoder ensembling. The biggest source of improvement in this state-of-theart posteditor came from additional training data generation, rather than from changes in the network architecture.\nCaglayan et al. (2016) used an architecture very similar to ours for multimodal translation. They\nSource: a man sleeping in a green room on a couch . Reference: ein Mann schla\u0308ft in einem gru\u0308nen Raum auf einem Sofa . Output with attention:\nmade a strong assumption that the network can be trained in such a way that the hidden states of the encoder and the convolutional network occupy the same vector space and thus sum the context vectors from both modalities. In this way, their multimodal MT system (BLEU 27.82) remained far bellow the text-only setup (BLEU 32.50).\nNew state-of-the-art results on the Multi30k dataset were achieved very recently by Calixto et al. (2017). The best-performing architecture uses the last fully-connected layer of VGG-19 network (Simonyan and Zisserman, 2014) as decoder initialization and only attends to the text encoder hidden states. With a stronger monomodal baseline (BLEU 33.7), their multimodal model achieved a BLEU score of 37.1. Similarly to Niehues et al. (2016) in the APE task, even further improvement was achieved by synthetically extending the dataset."}, {"heading": "6 Conclusions", "text": "We introduced two new strategies of combining attention in a multi-source sequence-to-sequence\nsetup. Both methods are based on computing a joint distribution over hidden states of all encoders.\nWe conducted experiments with the proposed strategies on multimodal translation and automatic post-editing tasks, and we showed that the flat and hierarchical attention combination can be applied to these tasks with maintaining competitive score to previously used techniques.\nUnlike the simple context vector concatenation, the introduced combination strategies can be used with the conditional GRU units in the decoder. On top of that, the hierarchical combination strategy exhibits faster learning than than the other strategies."}, {"heading": "Acknowledgments", "text": "We would like to thank Ondr\u030cej Dus\u030cek, Rudolf Rosa, Pavel Pecina, and Ondr\u030cej Bojar for a fruitful discussions and comments on the draft of the paper.\nThis research has been funded by the Czech Science Foundation grant no. P103/12/G084, the EU grant no. H2020-ICT-2014-1-645452 (QT21), and Charles University grant no. 52315/2014 and SVV project no. 260 453. This work has been using language resources developed and/or stored and/or distributed by the LINDAT-Clarin project of the Ministry of Education of the Czech Republic (project LM2010013)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Findings of the 2016 conference on machine translation (WMT16)", "author": ["bino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri"], "venue": "In Proceedings of the First Conference on Machine Translation (WMT). Volume", "citeRegEx": "bino et al\\.,? \\Q2016\\E", "shortCiteRegEx": "bino et al\\.", "year": 2016}, {"title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "Proceedings of the First Conference on Ma-", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "CUNI system for WMT16 automatic post-editing and multimodal translation tasks", "author": ["Jind\u0159ich Libovick\u00fd", "Jind\u0159ich Helcl", "Marek Tlust\u00fd", "Ond\u0159ej Bojar", "Pavel Pecina."], "venue": "Proceedings of the First Conference on Machine", "citeRegEx": "Libovick\u00fd et al\\.,? 2016", "shortCiteRegEx": "Libovick\u00fd et al\\.", "year": 2016}, {"title": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning", "author": ["Jiasen Lu", "Caiming Xiong", "Devi Parikh", "Richard Socher."], "venue": "CoRR abs/1612.01887. http://arxiv.org/abs/1612.01887.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Pre-translation for neural machine translation", "author": ["Jan Niehues", "Eunah Cho", "Thanh-Le Ha", "Alex Waibel."], "venue": "CoRR abs/1610.05243. http://arxiv.org/abs/1610.05243.", "citeRegEx": "Niehues et al\\.,? 2016", "shortCiteRegEx": "Niehues et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Edinburgh neural machine translation systems for WMT 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "CoRR abs/1409.1556. http://arxiv.org/abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "Proceedings of association for machine translation in the Americas. volume 200.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "A shared task on multimodal machine translation and crosslingual image description", "author": ["Lucia Specia", "Stella Frank", "Khalil Sima\u2019an", "Desmond Elliott"], "venue": "In Proceedings of the First Conference on Machine Translation", "citeRegEx": "Specia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "WMT16 APE shared task data", "author": ["Marco Turchi", "Rajen Chatterjee", "Matteo Negri."], "venue": "LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics, Charles University in Prague. http://hdl.handle.net/11372/LRT-1632.", "citeRegEx": "Turchi et al\\.,? 2016", "shortCiteRegEx": "Turchi et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "David Blei", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Sequence-to-sequence (S2S) learning with attention mechanism recently became the most successful paradigm with state-of-the-art results in machine translation (MT) (Bahdanau et al., 2014; Sennrich et al., 2016a), image captioning (Xu et al.", "startOffset": 164, "endOffset": 211}, {"referenceID": 9, "context": "Sequence-to-sequence (S2S) learning with attention mechanism recently became the most successful paradigm with state-of-the-art results in machine translation (MT) (Bahdanau et al., 2014; Sennrich et al., 2016a), image captioning (Xu et al.", "startOffset": 164, "endOffset": 211}, {"referenceID": 15, "context": ", 2016a), image captioning (Xu et al., 2015; Lu et al., 2016), text summarization (Rush et al.", "startOffset": 27, "endOffset": 61}, {"referenceID": 5, "context": ", 2016a), image captioning (Xu et al., 2015; Lu et al., 2016), text summarization (Rush et al.", "startOffset": 27, "endOffset": 61}, {"referenceID": 8, "context": ", 2016), text summarization (Rush et al., 2015) and other NLP tasks.", "startOffset": 28, "endOffset": 47}, {"referenceID": 16, "context": "The existing approaches to this problem do not explicitly model different importance of the inputs to the decoder (Firat et al., 2016; Zoph and Knight, 2016).", "startOffset": 114, "endOffset": 157}, {"referenceID": 0, "context": "The standard attention model as described by Bahdanau et al. (2014) defines the attention energies eij , attention distribution \u03b1ij , and the conar X iv :1 70 4.", "startOffset": 45, "endOffset": 68}, {"referenceID": 5, "context": "Recently, Lu et al. (2016) introduced sentinel gate, an extension of the attentive RNN decoder with LSTM units (Hochreiter and Schmidhuber, 1997).", "startOffset": 10, "endOffset": 27}, {"referenceID": 16, "context": ", c (N) i (Zoph and Knight, 2016; Firat et al., 2016).", "startOffset": 10, "endOffset": 53}, {"referenceID": 3, "context": "We optimize the network to minimize the output cross-entropy using the Adam algorithm (Kingma and Ba, 2014) with learning rate 10\u22124.", "startOffset": 86, "endOffset": 107}, {"referenceID": 13, "context": "The goal of multimodal translation (Specia et al., 2016) is to generate target-language image captions given both the image and its caption in the source language.", "startOffset": 35, "endOffset": 56}, {"referenceID": 7, "context": "The results are evaluated using the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011).", "startOffset": 41, "endOffset": 64}, {"referenceID": 11, "context": "In our model, the visual input is processed with a pre-trained VGG 16 network (Simonyan and Zisserman, 2014) without further fine-tuning.", "startOffset": 78, "endOffset": 108}, {"referenceID": 10, "context": "We use byte-pair encoding (Sennrich et al., 2016b) with a vocabulary of 20,000 subword units shared between the textual encoder and the decoder.", "startOffset": 26, "endOffset": 50}, {"referenceID": 14, "context": "We used the data from the WMT16 APE Task (Bojar et al., 2016; Turchi et al., 2016), which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain.", "startOffset": 41, "endOffset": 82}, {"referenceID": 12, "context": "The results are evaluated using the humantargeted error rate (HTER) (Snover et al., 2006) and BLEU score (Papineni et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 7, "context": ", 2006) and BLEU score (Papineni et al., 2002).", "startOffset": 23, "endOffset": 46}, {"referenceID": 4, "context": "Following Libovick\u00fd et al. (2016), we encode the target sentence as a sequence of edit operations sh ar e se nt .", "startOffset": 10, "endOffset": 34}, {"referenceID": 6, "context": "Niehues et al. (2016) concatenate both inputs into one long sequence, which forces the encoder to be able to work with both source and target language.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "The best system from the WMT\u201916 competition (Junczys-Dowmunt and Grundkiewicz, 2016) trains two separate S2S models, one translating from MT output to post-edited targets and the second one from source sentences to post-edited targets.", "startOffset": 44, "endOffset": 84}, {"referenceID": 11, "context": "The best-performing architecture uses the last fully-connected layer of VGG-19 network (Simonyan and Zisserman, 2014) as decoder initialization and only attends to the text encoder hidden states.", "startOffset": 87, "endOffset": 117}, {"referenceID": 6, "context": "Similarly to Niehues et al. (2016) in the APE task, even further improvement was achieved by synthetically extending the dataset.", "startOffset": 13, "endOffset": 35}], "year": 2017, "abstractText": "Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical. We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the proposed methods achieve competitive results on both tasks.", "creator": "LaTeX with hyperref package"}}}