{"id": "1704.06880", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Misspecified Linear Bandits", "abstract": "we consider uncertainty problem finding online learning in misspecified linear stochastic multi - armed bandit problems. regret research for edge - of - the - eye linear bandit algorithms such as optimism in the face control uncertainty linear bandit ( oful ) hold back the assumption that the arms expected rewards are perfectly linear in their features. it is, however, of bias to investigate the impact of potential misspecification in linear bandit models, where the expected rewards are perturbed away under the linear subspace determined by the arms features. although oful has recently been shown but be robust to generating small deviations from linearity, we show that any linear bandit algorithm that enjoys optimal regret performance in the perfectly linear setting ( e. ex., oful ) must suffer exponential regret under a sparse variation perturbation of the posterior model. in an attempt might overcome this negative result, we define \u0430 natural class of reconstruction models characterized including a probability - sparse fraction from linearity. we argue that the oful algorithm can arise to achieve sublinear regret even under models that have non - contiguous deviation. we additionally develop a novel bandit algorithm, comprising a hypothesis test for linearity followed by their decision to use either the oful or upper confidence bound ( ucb ) algorithm. for perfectly linear bandit models, the conjecture provably exhibits ofuls algorithm regret recovery, while for misspecified models satisfying the minimal - sparse deviation property, the algorithm avoids the incomplete regret phenomenon and falls back on ucbs sublinear pattern scaling. numerical experiments on objective data, and on recommendation data from the public yahoo! learning to rank challenge dataset, empirically support bias findings.", "histories": [["v1", "Sun, 23 Apr 2017 04:37:57 GMT  (599kb,D)", "http://arxiv.org/abs/1704.06880v1", "Thirty-First AAAI Conference on Artificial Intelligence, 2017"]], "COMMENTS": "Thirty-First AAAI Conference on Artificial Intelligence, 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["avishek ghosh", "sayak ray chowdhury", "aditya gopalan"], "accepted": true, "id": "1704.06880"}, "pdf": {"name": "1704.06880.pdf", "metadata": {"source": "CRF", "title": "Misspecified Linear Bandits", "authors": ["Avishek Ghosh", "Sayak Ray Chowdhury", "Aditya Gopalan"], "emails": ["ghosh@berkeley.edu", "srchowdhury@ece.iisc.ernet.in", "aditya@ece.iisc.ernet.in"], "sections": [{"heading": "1 Introduction", "text": "Stochastic multi-armed bandits have been used with significant success to model sequential decision making and optimization problems under uncertainty, due to their succinct expression of the exploration-exploitation tradeoff. Regret is one of the most widely studied performance measures for bandit problems, and it is well-known that the optimal regret that can be achieved in an iid stochastic bandit instance with N actions, [0, 1]-bounded rewards and T rounds, without any additional information about the reward distribution, is1 O\u0303( \u221a NT ). This is achieved, for instance, by the cele-\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n1The notation O\u0303 hides polylogarithmic factors.\nbrated Upper Confidence Bound (UCB) algorithm of (Auer, Cesa-Bianchi, and Fischer 2002).\nThe (polynomial) dependence of the regret in a standard stochastic bandit on the number of actions N can be rather prohibitive in settings with a very large number (and potentially infinite) of actions. Under the assumption that the rewards from playing arms are linear functions of known features or context vectors, linear bandit algorithms such as LinUCB (Li et al. 2010), Optimism in the Face of Uncertainty Linear bandit (OFUL) (Abbasi-Yadkori, Pa\u0301l, and Szepesva\u0301ri 2011) and Thompson sampling (Thompson 1933) give regret O\u0303(d \u221a T ) where d is the feature dimension. This is particularly attractive in practice where the feature dimension d N (for instance, news article recommendation data typically has d of the order of hundreds while N is 2 or 3 orders higher). The framework also extends to the more general contextual linear bandit model, where the features for arms are allowed to vary with time (Chu et al. 2011; Agrawal and Goyal 2013).\nThe design, and attractiveness, of linear bandit algorithms hinges on the assumption that the expected reward from playing arms are linear in their features, i.e., under a fixed ordering of the arms, the vector of expected rewards from all arms belongs to a known linear subspace, spanned by the arms\u2019 features. However, real-world environments may not necessarily conform perfectly to this linear reward model and in fact in most cases, have large deviation (Section 7 presents a case study using a real-world dataset to this effect). One possible reason for this is that features are often designed with careful domain expertise without explicit regard for linearity with respect to the utilities of actions. Another situation where linearity ma be violated is when there is feature noise or uncertainty (Hainmueller and Hazlett 2014) \u2013 even a small amount of noise in the assumed features shifts the expected reward vector out of the linear subspace. When the rewards need not be perfectly linear in terms of the features in hand, it becomes important to study how robust or fragile strategies for linear bandits can be to such misspecification.\nThe specific questions we address are: (a) With features available for arms with respect to which the arms\u2019 rewards need not necessarily be linear, how do deviations from linearity impact the performance of state-of-the-art linear bandit algorithms? (b) Is it possible to design bandit algorithms\nar X\niv :1\n70 4.\n06 88\n0v 1\n[ cs\n.L G\n] 2\n3 A\npr 2\n01 7\nthat control for deviations from linearity and still enjoy \u2018best-of-both-worlds\u2019 regret performance, i.e., regret that is sublinear in T and depends only on the feature dimension when the model is linear (or near-linear), and that falls back on the number of arms (as for UCB) when all bets are off (i.e., the model is far from linear)?\nOverview of results. The paper makes the following contributions:\n1. We first prove a negative result about the robustness of linear bandit algorithms to sparse deviations from linearity (Theorem 1): Any linear bandit algorithm that enjoys optimal regret guarantees on perfectly linear bandit problem instances (i.e., O(d \u221a T ) regret in dimension d), such\nas OFUL and LinUCB, must suffer linear regret on some misspecified linear bandit model. Furthermore our constructive argument shows that it is possible to find a misspecified model that differs only sparsely from a perfectly linear model \u2013 in fact, by a perturbation of the expected reward of only a single arm. We also rule out the possibility of using a state-of-the art bandit algorithm OFUL for handling instances with large non-sparse deviation (Theorem 2).\n2. Towards overcoming this negative result, we propose and analyze a novel bandit algorithm (Algorithm 1) (abbreviated RLB in Table 1), which is not only robust to nonsparse deviations from linearity but also retains the orderwise optimal regret performance in the standard linear bandit model. The algorithm provably achieves OFUL\u2019s O\u0303(d \u221a T ) regret2 in the ideal linear case, and UCB\u2019s\nO\u0303( \u221a NT ) regret for a broad class of reward models which are not linear but are well-separated from the feature subspace in a non-sparse sense, which we characterize (Theorem 3). The algorithm is comprised of a hypothesis test, followed by a decision to employ either OFUL or UCB. Numerical experiments on both synthetic as well as on the public Yahoo! Learning to Rank Challenge data 3, lend support to our theoretical results.\nRelated work. Many strategies have been devised and studied for stochastic multi-armed bandits for the general setting without structure \u2013 UCB (Auer, Cesa-Bianchi, and Fischer 2002), -greedy (Cesa-Bianchi and Fischer 1998),\n2Note that we concern ourselves with studying the gapindependent (worse-case over problem instances) regret; a similar exercise can be carried out in terms of the reward gap parameter.\n3 https://webscope.sandbox.yahoo.com/catalog.php?datatype=c\nBoltzmann exploration (Sutton and Barto 1998), BayesUCB (Kaufmann, Garivier, and Cappe 2012), MOSS (Audibert and Bubeck 2009) and Thompson sampling (Thompson 1933; Agrawal and Goyal 2012; Kaufmann, Korda, and Munos 2012), to name a few. Linear stochastic bandits have been extensively investigated (Rusmevichientong and Tsitsiklis 2010; Dani, Hayes, and Kakade 2008; AbbasiYadkori, Pa\u0301l, and Szepesva\u0301ri 2011) under the well-specified or perfectly linear reward model, achieving (near) optimal problem-independent regret of O\u0303(d \u221a T ) if the features are of dimension d (note that the number of arms can in principle unbounded). Researchers have also considered extensions of linear-bandit algorithms for the case of rewards following a generalized linear model with a known, nonlinear link function (Filippi et al. 2008).\nIn contrast to the abundance of work on linear bandits, very little work, to the best of our knowledge, has dealt with the impact of misspecification on stochastic decision making with partial (bandit) feedback. A notable study is that of (Besbes and Zeevi 2015) who study misspecified models in a specific dynamic pricing setting. Working in a specialized 2-parameter linear reward setting, they arrive at the conclusion that, within a small range of perturbations of the model away from linearity, one can preserve the sublinear regret of a standard bandit algorithm. There has been significant work, in a different vein, on the effect of model misspecification for the classical linear regression problem (i.e., estimation) in statistics where the metric is overall distortion and not explicitly maximum reward \u2013 see for instance the work of (White 1981) and related references. Very recently (Gopalan, Maillard, and Zaki 2016) provides some results for the linear bandit algorithm OFUL when the devation fron linearity is small. We expect to contribute towards filling a much-needed gap in the study of sensitivity properties in linearly parameterized bandit decision-making in this work."}, {"heading": "2 Setup & Preliminaries", "text": "Consider a multi-armed bandit problem with N arms, and a d-dimensional (d N ) context or feature vector xi \u2208 Rd associated with each arm i, i = 1, . . . , N . An arm i, upon playing, yields a stochastic and independent reward with expectation \u00b5i. Let \u00b5\u2217 = maxi \u00b5i be the best expected reward, and let X be the matrix having the feature vectors for each arm as its columns: X = [x1 | x2 | . . . | xN ] \u2208 Rd\u00d7N , with X T assumed to have full column rank. Define \u00b5 = [\u00b51 \u00b52 . . . \u00b5N ]\nT \u2208 RN to be the expected reward vector. At each time instant t = 1, 2, . . ., the learner chooses any one of the N arms and observes the reward collected from that arm. The action set for the player isA = {1, 2, . . . , N}. The regret after T rounds is defined to be the quantity R(T ) = T\u00b5\u2217\u2212 \u2211T t=1 \u00b5At . The goal of the player is to maximize the net reward, or equivalently, minimize the regret, over the course of T rounds. (If the learner has exact knowledge of \u03b8\u2217 and beforehand, the optimal choice is to play a best possible arm i\u2217 = arg maxi \u00b5i at all time instances.)\nUnder a perfectly linear model, the observed reward Yt at time t is modeled as the random variable, Yt = \u3008xAt , \u03b8\u2217\u3009+ \u03b7t = \u00b5At + \u03b7t, where At is the action chosen at time t, \u03b8 \u2217 \u2208\nRd is the unknown parameter vector, \u3008., .\u3009 denotes the inner product in Rd and \u03b7t is zero-mean stochastic noise assumed to be conditionally R-sub-Gaussian given At. Thus, under a perfectly linear model, the mean reward for each arm is a linear function of its features: there exists a unique \u03b8\u2217 \u2208 Rd such that \u00b5i = xTi \u03b8\u2217 \u2200i \u2208 A (the uniqueness property follows from the full column rank of X T ).\nConsider now the case where a linear model for \u00b5 with respect to the features X may not be valid, resulting in a deviation from linearity or a misspecified linear bandit model. We model the reward in this case by\nYt = \u3008xAt , \u03b8\u3009+ At + \u03b7t = \u00b5At + At + \u03b7t, where \u03b8 \u2208 Rd is a choice of weights, and := [ 1 2 . . . N ]\nT \u2208 RN denotes the deviation in the expected rewards of arms. Note that (a) the model remains perfectly linear if4 \u2208 span(X T ) \u2286 RN ), and (b) choice of \u03b8 satisfying the equation above is not unique if \u00b5 is separated from the subspace span(X T ), i.e., min\u03b8\u2208Rd \u2016 X T \u03b8 \u2212 \u00b5 \u20162> 0."}, {"heading": "3 Lower Bound for Linear Bandit Algorithms under Large Sparse Deviation", "text": "In this section, we present our first key result \u2013 a general lower bound on regret of any \u2018optimal\u2019 linear bandit algorithm on misspecified problem instances. Specifically, we show that any linear bandit algorithm that enjoys the optimal O(d \u221a T ) regret scaling, for linearly parameterized models of dimension d, must in fact suffer linear regret under a misspecified model in which only one arm has a mismatched expected reward. Theorem 1. Let A be an algorithm for the linear bandit problem, whose expected regret is O\u0303(d \u221a T ) on any linear problem instance with feature dimension d, time horizon T and expected rewards bounded in absolute value by 1. There exists an instance of a sparsely perturbed linear bandit, with the expected reward of one arm having been perturbed, for which A suffers linear, i.e., \u2126(T ), expected regret.\nThe formal proof of Theorem 1 is deferred to the appendix, but we present the main ideas in the following.\nProof sketch. The argument starts by considering a perfectly linear bandit instance with order of \u221a T arms in dimension d. It follows from the regret hypothesis that number of suboptimal arm plays must be O( \u221a T ). By a pigeonhole argument, since there are order of \u221a T suboptimal arms, there must exist a suboptimal arm that is played no more than O(1) times in expectation. Markov\u2019s inequality then gives that the event that both a) this suboptimal arm is played at most O(1) times and b) overall regret is O(d \u221a T ), occurs with probability at least a constant, say 1/3. Having isolated a suboptimal arm that is played very rarely by the algorithm (note that the choice of such an arm may very well depend on the algorithm), the argument proceeds by adding a perturbation to this suboptimal arm\u2019s reward to make it the best arm in the problem instance. A\n4For a matrix M , span(M ) denotes the subspace spanned by the columns of M .\nchange-of-measure argument is now used to reason that in the perturbed instance, the probability of the algorithm playing the arm in question does not change significantly as it was anyway played only a constant number of times in the pure linear model. But this must imply that the expected regret is linear due to neglecting the optimal arm in the perturbed problem instance."}, {"heading": "4 Performance of OFUL Under Deviation", "text": "A state-of-the-art algorithm for the linear bandit problem is OFUL. We study the performance of OFUL5 for various cases of deviations (suitably \u201csmall\u201d and \u201clarge\u201d). Specifically, we argue that OFUL is robust to small deviations, but for large deviations, the performance of OFUL is very poor leading to a linear regret scaling. The findings motivate us to propose a more robust algorithm to tackle linear bandit problems with significantly large deviations.\nAt time t \u2265 1, based on previous actions and observations upto t \u2212 1, OFUL solves a regularized linear least squares problem to estimate the unknown parameter \u03b8\u2217 \u2208 Rd and constructs a high-confidence ellipsoid around the estimate using concentration-of-measure properties of the sampled rewards. Using the confidence set, the high probability regret of OFUL is O(d \u221a T )."}, {"heading": "4.1 OFUL with Small Deviation", "text": "When the deviation from linearity is considerably small, it can be shown that OFUL performs similar to the perfect linear model in terms of regret scaling (see (Gopalan, Maillard, and Zaki 2016, Theorem 3) for details and a formal quantification of \u201csmall\u201d deviation). Assuming ||\u03b8\u2217||2 \u2264 S, ||xi||2 \u2264 L and |\u00b5i| \u2264 1 for all i \u2208 A, with probability at least 1 \u2212 \u03b4\u0303 (\u03b4\u0303 > 0), the cumulative regret upto time T of OFUL is given by,\nROFUL(T ) \u2264 8\u03c1\u2032 \u221a Td log ( 1 + TL2\n\u03bbd\n)( \u03bb1/2S\n+R \u221a 2 log 1\n\u03b4\u0303 + d log\n( 1 + TL2\n\u03bbd )) where \u03c1\u2032 is a geometric constant that measures the \u201cdistortion\u201d in the arms\u2019 actual rewards with respect to (linear) approximation and \u03bb is a regularization parameter.\nRemark: OFUL retains O(d \u221a T ) regret scaling even in\nthe presence of \u201csmall\u201d deviation."}, {"heading": "4.2 OFUL with Large Sparse Deviation", "text": "The regret of OFUL under pure linear bandit instance is O(d \u221a T ). Therefore from Theorem 1, the cumulative expected regret under large sparse deviation will be \u2126(T ).\n5We consider the OFUL algorithm in this work chiefly because it is known to be the most competitive in terms of regret scaling. It is conceivable that similar results can be shown for other, related, bandit strategies as well, such as ConfidenceBall (Dani, Hayes, and Kakade 2008), UncertaintyEllipsoid (Rusmevichientong and Tsitsiklis 2010), etc."}, {"heading": "4.3 OFUL with Large Non-sparse Deviation", "text": "We need to identify a natural class of structured large deviations that we dub non-sparse. We impose the following structure in terms of sparsity on the expected rewards \u00b5. Recall from Section 2 that X denotes the context matrix, \u00b5 the mean reward vector, \u03b8 a choice of weights, and the deviation from mean \u00b5; thus, \u00b5 = X T \u03b8 + .\nDefinition 1 (Non-sparse deviation). Given a feature set Xf = {x1, ..., xN} \u2282 Rd and constants l > 0, \u03b2 \u2208 [0, 1], an expected reward vector \u00b5 \u2208 RN is said to have the (l, \u03b2) deviation property if,\nP ( |xTid+1 [X f i1,...,id ]\u22121[\u00b5i1,...,id ]\u2212 \u00b5id+1 | \u2265 l ) \u2265 1\u2212 \u03b2\nfor all {i1, i2, . . . , id, id+1} \u2286 {1, 2, . . . , N}, such that {xi1 , xi2 , . . . , xid} linearly independent, where X f i1,...,id = [xTi1 , . . . , x T id ]T and \u00b5i1,...,id = [\u00b5i1 , . . . , \u00b5id ] T . The randomness is over the choice of d+ 1 arms.\nIn other words, the deviation of reward \u00b5 is (l, \u03b2) nonsparse if, whenever one uses any d linearly independent features, with their corresponding rewards, to regress a (d+1)th unknown reward linearly, then the magnitude of error is at least l > 0 (bounded away from 0) with probability at least 1\u2212 \u03b2. Typically, \u03b2 is positive and close to 0.\nFor example, consider the problem instance of Theorem 1, i.e., only one arm is perturbed away from linearity. This is an example of sparse deviation. If the perturbed arm is picked as one of d+1 arms in Definition 1, l will be a large positive number, but when the perturbed arm is missed, l will be 0, which is inconsistent with Definition 1. Also, \u03b2 can be chosen such that the probability of missing the perturbed arm is strictly greater than \u03b2.\nWe now argue, by counterexample (Theorem 2), that the regret of OFUL with large non-sparse deviation is \u2126(T ).\nTheorem 2. Consider a linear bandit problem with A = {1, 2}, context matrix X = [1 2], mean reward vector \u00b5 = [\u00b51 \u00b52]\nT with \u00b52 > \u00b51 and \u00b52 6= 2\u00b51. The deviation vector = [ 1 2]\nT is such that | i| > c (c > 0) for i = {1, 2} (with respect to Definition 1, l = c and \u03b2 = 0). There exists a problem instance for which the expected regret of OFUL until time T , E(ROFUL) = \u2126(T ).\nThe description of problem instance with the formal proof of theorem is deferred to the supplementary material.\nSummary: OFUL is robust to \u201csmall\u201d deviation (irrespective of sparsity) but incurs linear regret under large deviation (for both sparse and non-sparse). Theorem 1 shows the futility of designing any linear bandit algorithm under sparse deviation. However the quest is still valid if the deviation is large but non-sparse. We will investigate this issue in rest of the paper. It is clear that under large deviation, context vectors do not contribute in reducing regret and thus a rational player should discard contexts under such circumstances. The player may choose any standard algorithm for basic multi-armed bandits (UCB for instance)."}, {"heading": "5 A Linear Bandit Algorithm Robust to Large, Non-sparse Deviations", "text": "This section accomplishes the objective of developing a new algorithm that maintains the sublinear regret property in a model with non-sparse, large deviations. Non-sparse deviations can be seen to naturally arise in the presence of stochastic measurement or estimation noise; e.g., let xi and x\u0304i be the measured and original context vector respectively for arm i with xi = x\u0304i + \u03b6i. \u03b6Ti \u03b8 can be modeled as a Gaussian random variable with mean, E(\u03b6Ti \u03b8) = i. Substituting, we get, \u00b5 = X T \u03b8 + . It is possible to find suitable (l, \u03b2) pair (Definition 1) for this model and thus \u00b5 is non-sparse. The associated feature vectors corresponding to the mean reward vector satisfying Definition 1, are called \u201cuniformly perturbed features\u201d.\nWe now define 2 hypotheses \u2013H0 andH1, corresponding intuitively to \u201clinear\u201d and \u201cnot linear\u201d \u2013 on (X , \u00b5), which will be used to quantify the performance of the algorithm developed in this section. We say that hypothesis H0 holds if the separation of \u00b5 from span(X T ), i.e., the quantity min\u03b8\u2208Rd \u2016 X T \u03b8 \u2212 \u00b5 \u20162, is 0, i.e., the model is perfectly linear. On the other hand, we say that hypothesis H1 holds if the separation is greater than 0 and \u00b5 satisfies the (l1, \u03b2) deviation property of Definition 1 with l1 > 0.\nRemark: The definition of H0 be generalized to handle small deviations in the \u2016 \u00b7 \u20162 norm with distortion parameter \u03c1\u2032 \u2265 1, in the sense of (Gopalan, Maillard, and Zaki 2016, Theorem 3)."}, {"heading": "5.1 A Robust Linear Bandit (RLB) Algorithm", "text": "The sequence of actions for the proposed novel bandit algorithm, namely Robust Linear Bandit (RLB) is summarized in Algorithm 1, mainly consisting of three steps. First, RLB executes an initial sampling phase, in which d+1 arms out of N are sampled. Based on these samples, it constructs a confidence ellipsoid for \u03b8\u2217 in the next phase. Finally, based on experimentation on the (d+ 1)-th arm, it decides to play either OFUL or UCB for the remainder of the horizon. We will illustrate the necessity of non-sparse deviation as follows: consider a problem instance with = (0, . . . , 0, c, 0, . . . , 0), |c| 0. As N d, with high probability, the deviated arm can be missed in the sampling phase and according to Algorithm 1, the learner learns that the model is linear and decides to play OFUL which according to Theorem 1 incurs \u2126(T ) regret.\nStep 1: Sampling of d+ 1 arms\nFor non-sparse deviation, the choice of d + 1 among N arms may be arbitrary. Without loss of generality, we sample the arms indexed {1, 2, . . . , d+ 1}, k times each (resulting (d + 1) \u00d7 k (:= \u03c4 ) sampling instances). From Hoeffding\u2019s inequality, the sample mean estimate of d + 1-th arm, \u00b5\u0302d+1, satisfies P(|\u00b5\u0302d+1\u2212\u00b5d+1| > rs) \u2264 exp(\u22122r2sk). With \u03b4s := exp(\u22122r2sk), the confidence interval around \u00b5d+1 will be [\u00b5d+1 \u2212 rs, \u00b5d+1 + rs] with probability at least 1\u2212 \u03b4s.\nStep 2: Construction of Confidence Ellipsoid Based on the samples of first d arms, RLB constructs a confidence ellipsoid for \u03b8\u2217 assumingH0 is true. UnderH0,\ny (j) i = \u3008xi, \u03b8 \u2217\u3009+ \u03b7i,j \u2200i \u2208 {1, 2, \u00b7, d}, 1 \u2264 j \u2264 k.\nIn this setup, we re-define the reward vector Y = [y\n(1) 1 , \u00b7, y (k) 1 , y (1) 2 , \u00b7, y (k) 2 , . . . , y (k) d ] T , feature-matrix X = [xT1 , x T 1 , \u00b7, xT1 , xT2 , xT2 , . . . , xTd ]T and noise vector \u03b7 = [\u03b71, \u03b72, . . . , \u03b7kd] T with Y = X\u03b8\u2217 + \u03b7. Let \u03b8\u0302 be the solution of `2 regularized least square, i.e., \u03b8\u0302 = (XTX + \u03bbI)\u22121XTY, where \u03bb > 0 is the regularization parameter.\nUsing the same line of argument as in (Abbasi-Yadkori, Pa\u0301l, and Szepesva\u0301ri 2011), it can be shown that for any \u03b4\u0304 > 0, with probability at least 1\u2212 \u03b4\u0304, \u03b8\u2217 lies in the set,\nC = { \u03b8 \u2208 Rd : \u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2225\u2225\u2225 V\u0304 \u2264 R\n\u00d7 \u221a 2 log( det(V\u0304 )1/2 det(\u03bbI)\u22121/2\n\u03b4\u0304 ) + \u03bb1/2S } V\u0304 = \u03bbI + k \u2211d i=1 xix T i = X TX + \u03bbI , \u2016\u03b8\u2217\u20162 \u2264 S.\nStep 3: Hypothesis test for non-sparse deviation We project the confidence ellipsoid onto the context of d+1th arm. The projection, \u3008xd+1, \u03b8\u3009 , \u03b8 \u2208 C will result in an interval, Ie, centered at xTd+1\u03b8\u0302 (Lemma 2). We compare Ie with the interval obtained from sampling d+ 1-th arm, Is. If H0 is true, Lemma 3 states that Ie and Is overlap with high probability. Similarly, from Lemma 4, underH1, Ie will not intersect with Is with high probability, i.e., probability of choosing H1 when H0 is true (and vice versa), is significantly low. 6 Based on this experiment, the player adopts the following decision rule: if Ie \u2229 Is 6= \u03c6, declareH0 and play OFUL, otherwise declareH1 and play UCB.\nAlgorithm 1 Robust Linear Bandit (RLB) 1: Sample the first d arms k times each. 2: Compute the `2-regularized least square estimate (\u03b8\u0302)\nbased on d\u00d7 k samples assumingH0. 3: Construct a confidence ellipsoid C such that with high\nprobability, \u03b8\u2217 \u2208 C. 4: Project the ellipsoid onto the context of d+ 1 th arm to\nobtain interval Ie. 5: Sample d + 1 th arm k times, obtain mean estimate, \u00b5\u0302d+1, and confidence interval Is. 6: If Ie \u2229 Is 6= \u03c6, declare H0 and play OFUL for the remaining time instants, otherwise play UCB."}, {"heading": "6 Regret Analysis", "text": "The objective of RLB is to learn the gap from linearity and play accordingly to obtain regret of Table 1. For zero deviation, RLB exploits linear reward structure and incur a regret\n6Owing to space constraints, Lemma 2, 3 and 4, with their proofs are moved to supplementary material.\nof O(d \u221a T ). For large non-sparse deviation, RLB discards the contexts and avoids linear regret. During the initial sampling phase upto \u03c4 , regret will scale linearly as each step is either forced exploration or exploitation, i.e., Rs(\u03c4) = O(\u03c4). After that, based on the player\u2019s decision, either OFUL or UCB is played. ForH0, we use regret of OFUL as given in (Abbasi-Yadkori, Pa\u0301l, and Szepesva\u0301ri 2011). With N arms and time T , (Bubeck and Cesa-Bianchi 2012) provided O( \u221a NT log T ) regret for standard UCB. Also, (Audibert and Bubeck 2009), gave an algorithm MOSS, inspired by UCB which incurs a regret upper bound of 49 \u221a NT ."}, {"heading": "6.1 Regret of Algorithm 1", "text": "From Lemma (3) it can be seen that, if H0 is true, OFUL and UCB are played with a probability of 1 \u2212 \u03b41(k, \u03bb) and \u03b41(k, \u03bb) respectively and accordingly regret is accumulated. By an appropriate choice of k and \u03bb, \u03b41(k, \u03bb) can be made arbitrarily close to 0. Similarly, under H1, corresponding probabilities are \u03b42(k, \u03bb) +\u03b2 and 1\u2212 \u03b42(k, \u03bb)\u2212\u03b2 respectively (Lemma 4). \u03b2 comes from the definition of non-sparse deviation. Therefore, under non-sparse deviation, probability of playing OFUL and incurring linear regret is \u03b42(k, \u03bb)+\u03b2, which can be pushed to arbitrarily small value by proper choice of k and \u03bb as typically, \u03b2 is very small and close to 0. \u03c4 can be choosen as log(T ), a sublinear function of T . Now we are in a position to state our main result - an upper bound on regret of RLB. Theorem 3 (Regret guarantees for RLB). The expected regret of RLB in T time steps satisfies the following: (a) Under hypothesisH0,\nE(RRLB(T )) \u2264 c1((d+ 1)k) + 4[(1\u2212 \u03b41(k, \u03bb)) \u00d7 \u221a (T \u2212 log T )d log(1 + (T \u2212 log T )L 2\n\u03bbd )(\u03bb1/2S\n+ R \u221a 2 log 1\n\u03b4 + d log(1 + (T \u2212 log T )L2 \u03bbd ))]\n+ 49\u03b41(k, \u03bb) \u221a N(T \u2212 log T )\n(b) UnderH1, E(RRLB(T )) \u2264 c1((d+ 1)k) + 49(1\u2212 \u03b42(k, \u03bb)\u2212 \u03b2) \u00d7 \u221a N(T \u2212 log T ) + c2(\u03b42(k, \u03bb) + \u03b2)(T \u2212 log T )\nwhere, a total of d + 1 arms are sampled k times each, \u03bb is regularization parameter, \u03b4, L, c1, c2 are constants and,\n\u03b41(k, \u03bb) := exp(\u2212 k(rs \u221a log k + rp( \u221a log k \u2212 1))2\n2R2 )\n\u03b42(k, \u03bb) := exp(\u2212 k(l1 \u2212 rp\n\u221a log k \u2212 rs \u221a log k)2\n2R2 )\nwith 2rs and 2rp being the length of the intervals Is and Ie respectively and l1 comes from the definition ofH1.\nImplication. We see that if k increases, \u03b41(k, \u03bb) and \u03b42(k, \u03bb) goes to 0 exponentially. Under H1 and a given (l, \u03b2) pair, for RLB to decide in favor of H1 and hence ensuring sub-linear regret with probability greater than 1 \u2212 \u03b42(k, \u03bb) \u2212 \u03b2, we need, \u221a log k(rp + rs) < l1, (shown in the proof of Lemma 4). Since rs and rp are both O(1/ \u221a k), k satisfies, k/ log k > b/l21 for some constant b\n(> 0). Simulations show that a considerably small \u03bb also pushes \u03b41(k, \u03bb) and \u03b42(k, \u03bb) close to 0. Therefore, with H0, RRLB(T ) = O(log T ) + O(d \u221a T \u2212 log T ). Similarly,\nfor H1, RRLB(T ) = O(log T ) + O( \u221a N(T \u2212 log T )), as shown in Table 1."}, {"heading": "7 Simulation Results", "text": ""}, {"heading": "7.1 Synthetic Data", "text": "In this setup, we assume, N = 1000, d = 20 and k = 50. \u03bb and R are taken as 0.001 and 0.1 respectively. Context vectors and mean rewards are generated at random (in the range [0, 1]). All high probability events are simulated with an error probability of 0.001. The simulation is run for 1000 instances and cumulative regret is shown in Figure 1.\nUnder H0, RLB predicts correctly with a probability of false alarm 0.0001. Figure 1 shows the regret performance of RLB. In the sampling phase, regret is linear and thus greater than the perturbed OFUL and UCB algorithm. After the sampling phase, regret of RLB closely follows regret of OFUL with probability 0.9999. The false alarm probabillity can be further pushed if the value of k is increased. If we allow time horizon T to be very large, the deviation in terms of regret between UCB and RLB will be significantly large.\nThe same experiment is carried for H1 with | i| > 2 for all i \u2208 {1, 2, . . . , N}, and RLB verdicts in favor of UCB with an error (miss detection) of 0.0001. Figure 1 shows the variation of regret with time. Further, if k is increased, the error decreases but the regret from sampling phase increases."}, {"heading": "7.2 Yahoo! Learning to Rank Data", "text": "The performance of RLB is evaluated on the Yahoo! dataset \u201cLearning to Rank Challenge\u201d (Chapelle and Chang 2011). Specifically, we use the file set2.test.txt. The dataset consists of query document instance pairs with 103174 rows and 702 columns. The first column lists rating given by user (which we take as reward) with entries {0, 1, 2, 3, 4} and the second column captures user id. We treat the rest 700 columns as context vector corresponding to each user. We select 20, 000 rows and 50 columns at random (similar results were found for several random selections). We cluster the data usingK-means clustering withK = 500. Each cluster can be treated as a bandit arm with mean reward equal to\nthe empirical mean of the individual rating in the cluster and context (or feature) vector equals to the centroid of the cluster. Thus, we have a bandit setting with N = 500, d = 50.\nTo show that the obtained data does not fall in H0 (i.e., linear model), we fit a linear regression model. It is observed that, average value of residuals (error) is 0.15 (with a maximum value of 0.67), where average mean reward is 1.13. Therefore, we conclude that the data falls underH1. We run OFUL, UCB and RLB on the dataset and regret performance is shown in Figure 2. We consider the following cases:\n1. k = 70: we conclude that all arms are sufficiently sampled and thus RLB avoids high regret of OFUL and plays UCB. But RLB suffers high regret upto 3570 rounds.\n2. k = 10: arms are not properly sampled, leading to an increase in the radius Is and violating the lower bound on k. Owing to this, RLB plays OFUL and incurs high regret.\nWe carry out the same experiment with K = 800, i.e., N = 800, d = 50 and the observations are similar (Figure 2(b)). For a reasonable value of k (50 in this case), RLB properly identifies the optimal algorithm (UCB) to play, but with very low k (10), RLB suffers the high regret of OFUL. We omit the errorbars as over 1000 instances, regret values for different algorithms remain almost the same."}, {"heading": "8 Conclusion and Future work", "text": "We addressed the problem of adapting to misspecification in linear bandits. We showed that a state-of-the art linear bandit algorithm like OFUL is not always robust to deviations away from linearity. To overcome this, we have proposed a robust bandit algorithm and provided a formal regret upper bound. Experiments on both synthetic and real world datasets support our reasoning that (a) feature-reward maps can often be far from linear in practice, and (b) employing a strategy that is aware of potential deviation from linearity and tests for it suitably does lead to performance gains. Moving forward, it would be interesting to explore other non-linearity structures than sparse deviations as was studied here, and to derive information-theoretic regret lower bounds for the class of general bandit problems with given feature sets. It is also intriguing to investigate the performance of Bayesianinspired algorithms like Thompson Sampling on linear bandits in presence of deviations."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the DST INSPIRE faculty grant IFA13-ENG-69. The authors are grateful to anonymous reviewers for providing useful comments."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["P\u00e1l Abbasi-Yadkori", "D. P\u00e1l", "C. Szepesv\u00e1ri"], "venue": "In Proc. NIPS", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["Agrawal", "S. Goyal 2012] Agrawal", "N. Goyal"], "venue": "Journal of Machine Learning Research - Proceedings Track", "citeRegEx": "Agrawal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2012}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["Agrawal", "S. Goyal 2013] Agrawal", "N. Goyal"], "venue": null, "citeRegEx": "Agrawal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2013}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Audibert", "Bubeck 2009] Audibert", "J.-Y", "S. Bubeck"], "venue": "COLT", "citeRegEx": "Audibert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2009}, {"title": "Finite-time analysis of the multiarmed bandit problem. Machine Learning 47(2):235\u2013256", "author": ["Cesa-Bianchi Auer", "P. Fischer 2002] Auer", "N. CesaBianchi", "P. Fischer"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "On the (surprising) sufficiency of linear models for dynamic pricing with demand learning. Management Science 61(4):723\u2013739", "author": ["Besbes", "O. Zeevi 2015] Besbes", "A. Zeevi"], "venue": null, "citeRegEx": "Besbes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Besbes et al\\.", "year": 2015}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["Bubeck", "S. Cesa-Bianchi 2012] Bubeck", "N. CesaBianchi"], "venue": "arXiv preprint arXiv:1204.5721", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Finite-time regret bounds for the multiarmed bandit problem", "author": ["Cesa-Bianchi", "N. Fischer 1998] Cesa-Bianchi", "P. Fischer"], "venue": "In In 5th International Conference on Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1998}, {"title": "Yahoo! learning to rank challenge overview", "author": ["Chapelle", "O. Chang 2011] Chapelle", "Y. Chang"], "venue": "In Yahoo! Learning to Rank Challenge,", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Contextual bandits with linear payoff functions", "author": ["Chu"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Chu,? \\Q2011\\E", "shortCiteRegEx": "Chu", "year": 2011}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Hayes Dani", "V. Kakade 2008] Dani", "T.P. Hayes", "S.M. Kakade"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "A near optimal policy for channel allocation in cognitive radio", "author": ["Filippi"], "venue": "Recent Advances in Reinforcement Learning,", "citeRegEx": "Filippi,? \\Q2008\\E", "shortCiteRegEx": "Filippi", "year": 2008}, {"title": "Low-rank bandits with latent mixtures", "author": ["Maillard Gopalan", "A. Zaki 2016] Gopalan", "O.-A. Maillard", "M. Zaki"], "venue": "arXiv preprint arXiv:1609.01508", "citeRegEx": "Gopalan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2016}, {"title": "Kernel regularized least squares: Reducing misspecification bias with a flexible and interpretable machine learning approach. Political Analysis 22(2):143\u2013168", "author": ["Hainmueller", "J. Hazlett 2014] Hainmueller", "C. Hazlett"], "venue": null, "citeRegEx": "Hainmueller et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hainmueller et al\\.", "year": 2014}, {"title": "On bayesian upper confidence bounds for bandit problems", "author": ["Garivier Kaufmann", "E. Cappe 2012] Kaufmann", "A. Garivier", "O. Cappe"], "venue": "In AISTATS", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Thompson sampling: an asymptotically optimal finite-time analysis", "author": ["Korda Kaufmann", "E. Munos 2012] Kaufmann", "N. Korda", "R. Munos"], "venue": "In ALT", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Li"], "venue": null, "citeRegEx": "Li,? \\Q2010\\E", "shortCiteRegEx": "Li", "year": 2010}, {"title": "Linearly parameterized bandits. Mathematics of Operations Research 35(2):395\u2013411", "author": ["Rusmevichientong", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "Rusmevichientong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rusmevichientong et al\\.", "year": 2010}, {"title": "Reinforcement learning: An introduction, volume 1. MIT press Cambridge", "author": ["Sutton", "R.S. Barto 1998] Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson 1933] Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}], "referenceMentions": [{"referenceID": 19, "context": "2010), Optimism in the Face of Uncertainty Linear bandit (OFUL) (Abbasi-Yadkori, P\u00e1l, and Szepesv\u00e1ri 2011) and Thompson sampling (Thompson 1933) give regret \u00d5(d \u221a T ) where d is the feature dimension.", "startOffset": 129, "endOffset": 144}, {"referenceID": 19, "context": "php?datatype=c Boltzmann exploration (Sutton and Barto 1998), BayesUCB (Kaufmann, Garivier, and Cappe 2012), MOSS (Audibert and Bubeck 2009) and Thompson sampling (Thompson 1933; Agrawal and Goyal 2012; Kaufmann, Korda, and Munos 2012), to name a few.", "startOffset": 163, "endOffset": 235}], "year": 2017, "abstractText": "We consider the problem of online learning in misspecified linear stochastic multi-armed bandit problems. Regret guarantees for state-of-the-art linear bandit algorithms such as Optimism in the Face of Uncertainty Linear bandit (OFUL) hold under the assumption that the arms expected rewards are perfectly linear in their features. It is, however, of interest to investigate the impact of potential misspecification in linear bandit models, where the expected rewards are perturbed away from the linear subspace determined by the arms features. Although OFUL has recently been shown to be robust to relatively small deviations from linearity, we show that any linear bandit algorithm that enjoys optimal regret performance in the perfectly linear setting (e.g., OFUL) must suffer linear regret under a sparse additive perturbation of the linear model. In an attempt to overcome this negative result, we define a natural class of bandit models characterized by a non-sparse deviation from linearity. We argue that the OFUL algorithm can fail to achieve sublinear regret even under models that have non-sparse deviation. We finally develop a novel bandit algorithm, comprising a hypothesis test for linearity followed by a decision to use either the OFUL or Upper Confidence Bound (UCB) algorithm. For perfectly linear bandit models, the algorithm provably exhibits OFULs favorable regret performance, while for misspecified models satisfying the non-sparse deviation property, the algorithm avoids the linear regret phenomenon and falls back on UCBs sublinear regret scaling. Numerical experiments on synthetic data, and on recommendation data from the public Yahoo! Learning to Rank Challenge dataset, empirically support our findings.", "creator": "LaTeX with hyperref package"}}}