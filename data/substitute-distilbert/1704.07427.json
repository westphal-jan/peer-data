{"id": "1704.07427", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Recognizing Descriptive Wikipedia Categories for Historical Figures", "abstract": "wikipedia is a useful knowledge library that benefits many applications in info processing and knowledge representation. ones important feature of wikipedia is that of categories. wikipedia pages are indexed different categories according to their contents as ada - annotated labels which can be used in information navigation, ad hoc search filters, entity ranking and tag submission. however, important pages only usually assigned too many categories, which makes it difficult to recognize the most important ones that give the best descriptions.", "histories": [["v1", "Mon, 24 Apr 2017 19:28:52 GMT  (703kb,D)", "http://arxiv.org/abs/1704.07427v1", "9 pages, 6 tables, 5 figures"]], "COMMENTS": "9 pages, 6 tables, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yanqing chen", "steven skiena"], "accepted": false, "id": "1704.07427"}, "pdf": {"name": "1704.07427.pdf", "metadata": {"source": "CRF", "title": "Recognizing Descriptive Wikipedia Categories for Historical Figures", "authors": ["Yanqing Chen", "Steven Skiena"], "emails": ["cyanqing@cs.stonybrook.edu", "skiena@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "In this paper, we propose an approach to recognize the most descriptive Wikipedia categories. We observe that historical figures in a precise category presumably are mutually similar and such categorical coherence could be evaluated via texts or Wikipedia links of corresponding members in the category. We rank descriptive level of Wikipedia categories according to their coherence and our ranking yield an overall agreement of 88.27% compared with human wisdom.\nI. INTRODUCTION\nWikipedia is a useful knowledge source that benefits many applications in language processing and knowledge representation. An important feature of Wikipedia is that of categories. Wikipedia pages are assigned different categories according to their contents as human-annotated labels which can be used in information retrieval, ad hoc search improvements, entity ranking and tag recommendations.\nHowever, important pages are usually assigned too many categories. Figure 1 shows the distribution of categories for historical figures on Wikipedia.\nOn the other hand, most of these categories are not descriptive enough. For instance, Barack Obama is listed in almost fifty different Wikipedia categories, including \u201c 20th-century American writers\u201d, \u201c1961 births\u201d, \u201cAmerican Nobel laureates\u201d, \u201c Grammy Award winners\u201d and \u201cHarvard Law School alumni\u201d. These categories are undoubtedly correct but are sometimes trivial. We believe that \u201cPresidents of the United States\u201d most accurately captures his historical/cultural significance.\nOne approach might be based on keywords, e.g. recognizing that \u201cPresident\u201d is more important distinction than other titles. However, this appears quite challenging. Being President of a small organization is generally less impressive. Example of Jonathan Edwards shows that even \u201cPresident of Princeton University\u201d might not be dominant. Thus, evaluating the magnitude of such accomplishments seems difficult and subjective.\nInstead, we propose an approach to recognize the most descriptive Wikipedia categories based on categorical coherence. We observe that the figures in a precise category presumably are mutually similar: the presidents of the United States presumably more closely resemble each other than American writers or Grammy Award recipients. Such categorical coherence could be evaluated via similarities in texts or Wikipedia links of corresponding members in the category and we believe an interesting category requires coherence to make it interesting. We rank descriptive level of Wikipedia categories according to their coherence, which yield an overall agreement of 88.27% compared with human wisdom.\nSpecifically, we make following contributions in this paper:\n\u2022 We create vector representations for people with Wikipedia pages using LDA topic modeling and Deepwalk. These representations position each person as a point in a high-dimensional space, facilitating similarity comparisons. Deepwalk representations trained on Wikipedia links are proved more valuable in our\nar X\niv :1\n70 4.\n07 42\n7v 1\n[ cs\n.C L\n] 2\n4 A\npr 2\n01 7\nexperiment than LDA topic modeling which is based on the text content.\n\u2022 We present a new approach to identifying the most salient categories associated with Wikipedia entities, based on the use of vector representations. We evaluate using different distance measures and coherence criteria to show what is best at quantifying descriptive level of Wikipedia categories.\n\u2022 Through human wisdom collected from Crowdflower, we verify that our notion of category appropriateness generally jibes with that of human reviewers. Indeed, we have fashioned a game app testing how often users agree with our algorithmically-chosen categorization.\nThe rest of this paper proceeds as follows: In Section 2 we review related work. Section 3 demonstrates the procedure of collecting human reviews. In Section 4 we describe Chinese menu choices in constructing our models. In Section 5 we show experiment results and corresponding analysis."}, {"heading": "II. RELATED WORK", "text": "Entity ranking gains popularity since better rankings boost the performances of search engines, resulting in faster and more precise information retrieval. Wikipedia seems to be a good playground. The problem of ranking web pages could be easily reduced to Wikipedia entity ranking, plus that Wikipedia has a large collection of entities of different types [2] and Wikipedia contains valuable texts, human annotated tags, enriched links and a great structure to analyze ranking effectiveness. Certain ranking can serve as a pivot for extensibility or analysis [3], [4] or be used to answer queries in named entity recognition [5]. Additionally, retrieving real-life knowledge of reputations, fames and historical significance from entity ranking is also valuable [1].\nTraditional ranking algorithms on Wikipedia basically consider two parts. One part focuses on information provided by raw text, including length of pages, word occurrences and topic distributions. LDA is among the most valuable approaches in such tasks [6]. Topics from LDA highly agree with real tags when finding most important feature words of a page [7]. The other part of ranking criteria relies heavily on links. Representatives include PageRank [8] and HITS [9]. PageRank is a link analysis algorithm that assigns high numerical weighting to pages that are referred to by many other pages and the structure of weight distribution conclude the importance of web pages. HITS defines hubs to be pages that have links pointing to many authority pages, serving as another important criteria in ranking. Recent work of Deepwalk [10] uses truncated random walks to learn latent representations by encoding social relations in a continuous vector space, which can be easily exploited by statistical models.\nOn the other hand, human annotated tags for pages benefits many related tasks. On Wikipedia, performance of entity ranking is improved by utilizing Wikipedia categories [11]. Links and topic difficulty prediction together with category information greatly boost the performance of entity ranking [12]. Additionally, Wikipedia categories can be used to boost search results in an ad hoc way [13]. Researchers also found that it is possible to analyze consistent semantic relationships\nin the tags [14] and corresponding latent representations of raw text would help tag recommendations [7]. Given these facts, we believe a reversed application to rank Wikipedia categories based on latent representations of pages would help summarize the content in the text thus provides better and more precise descriptions of the Wikipedia pages."}, {"heading": "III. COLLECTING HUMAN WISDOM", "text": "Our experiment mainly focus on Wikipedia pages of historical figures. It is clear that not all categories are created with equal descriptive power, most of them are correct but provide only trivial information. For instance, categories like \u201c1961 births\u201d and \u201cLiving people\u201d record certain status of facts of a person but are usually meaningless; categories like \u201cHarvard Law School alumni\u201d provide better idea of a person but usually that is not enough to summarize a single aspect of one\u2019s life. We here define descriptive power of a category to be the ability to construct analogous connections between people within same categories. The easier we can distinguish a person as well as other people with a single category tag, the more descriptive power we have on that category. We expect to find categories like \u201cPresidents of the United States\u201d to be listed at the top of our ranking of descriptive power as these categories most accurately captures one\u2019s historical/cultural significance.\nWe start a project on Crowdflower, a leading peoplepowered data enrichment platform, to collect human reviews of the most descriptive Wikipedia categories on the top-500 most famous people according to \u201cranking of significance\u201d described in [1].\nFor each Wikipedia person we manually select 4 categories with good descriptive power plus 1 random category to make up a question with 5 choices. Example of questions are shown in Figure 2.\nEach answer makes a clarification that one choice is dominating the other four. If we collect multiple answers for a single question, categories with higher descriptive power will have more votes, thus the distribution indicate overall descriptive power of each Wikipedia category. For each question we collect answers from 20 volunteers. In total we gathered 10,000 answers from 176 volunteers, covering 1076 distinct Wikipedia categories.\nAs another part of data collection, we extracted raw texts from all valid English Wikipedia pages and created adjacent lists for them. To avoid unexpected information from Wikipedia categories, we carefully removed all links / texts that appear in the reference of a main Wikipedia page. This procedure helps us collect texts / links of 4,517,721 pages. We also extracted corresponding Wikipedia categories for future uses."}, {"heading": "IV. CHINESE MENU OF RANKING MODEL", "text": "In this section we will describe how we calculate categorical coherence. The procedure involves 4 major steps: Generating feature vectors using latent representations from text or links; Measuring distances in vector space; Definition of close neighbors; Calculating categorical coherence based on observations of close neighbors. We will have multiple choices for each step in the experiment and we expect to find the best combinations to solve this task."}, {"heading": "A. Generating feature vectors", "text": "We focus on converting text / links of Wikipedia pages into vector representations. We experimented on 2 methods in our tests.\n1) LDA model: LDA model provides probability distribution of possible topics. It is based on co-occurrence of words and has an advantage that the output of each topic would be easy for human beings to read and understand. We train LDA model for the all available pages in our data collection with 5000 topics, converting each Wikipedia page in the corpus into a probability distribution of possible topics. Figure 3 shows an example of top related topics for some historical figures. Pages have higher probability to fall into same topics should be considered more similar thus close in high-dimensional vector space.\n2) Deepwalk model: Deepwalk is an online algorithm that creates statistical representation of graph by learning via random walks in the graph. Walks are considered as sentences metaphor and generate latent dimensions according to adjacent list. With a hierarchical Softmax layer these latent dimensions will be finally converted into vector representations. In our\nexperiment, we propose that Wikipedia pages sharing more common links will sit closer in Deepwalk embedding space since random walks in corresponding pages visit through very similar paths. Groups with large fraction of links between corresponding Wikipedia people will indicate stable relationships on similarity as random walks have lower chances to step out of the group. We use the package described in [10] with 128 output dimensions to train Deepwalk embedding on adjacent lists of all Wikipedia pages."}, {"heading": "B. Measuring distances", "text": "Feature vectors are usually considered as points in highdimensional space. Thus, distance between vectors may following the definition of Manhattan distance (L1 normalization) or Euclidean distance (L1 normalization). Plus, Cosine similarity are frequently used to measure similarities between feature vectors.\nAdditionally, since LDA represents probability distributions, we include Kullback\u2013Leibler divergence and Jensen\u2013Shannon divergence in our distance metrics."}, {"heading": "C. Definition of close neighbors", "text": "Distance between feature vectors indicate similarity between corresponding Wikipedia people. However, such representations are not linear \u2013 pairs with twice the distance do not necessarily mean half the similarity. On the other hand, observation shows that only pairs within a certain range show strong signals of similarity. Such ranges are not pre-defined and usually it is correlated with the density of feature vectors in a certain area of the embedding space.\nWe here propose two approaches to define close neighbors. One is to limit the count of close neighbors, considering only the closest K neighbors in vector space to be \u201cclose\u201d neighbors. Since relationship of close neighbors is not always reversible under this definition, this strategy will usually create asymmetric results. The other is to pick close neighbors by distance, marking all neighbors within a certain distance of D as close. With this strategy, if a point in space is semiisolated then nothing would be considered similar to it. Both two approaches are reasonable and will be considered as a hyper-parameter in our experiment."}, {"heading": "D. Ranking criteria", "text": "We propose two methods to quantify how descriptive a Wikipedia category is.\n1) Conductance: The \u201cconductance\u201d of a category is defined as the ratio of close neighbors inside the category and those outside the category. This is a simple and direct measurement. Category with more close neighbors inside seems to be more stable since close neighbors are sharing similarities with \u201cmembers inside this category\u201d. However, this method does not consider the size of the category well. For larger categories with more corresponding people, it is even harder to keep all close neighbors inside the category / to guarantee\ncategory members are close, thus reducing the \u201cconductance\u201d of the category.\nLet Ccat be the conductance of a category, then we have: Ccat = \u2211\nX\u2208cat Y \u2208cat\n(X,Y )\u2208closeneighbors\n/ \u2211\nX\u2208cat (X,Y )\u2208closeneighbors\nFigure 4 shows an example of calculating conductance.\n2) Surprise level: The other measurement is named \u201csurprise level\u201d. This metric focuses on balancing the effect of both size of the category and the probability of inside-category pairs become close neighbors. We assume that the probability of \u201ca pair is inside a category\u201d is independent from the probability of \u201ca pair shares close-neighbor relationship\u201d, then for an entity A in the category the probability of its close neighbor B belongs to the category will be:\nPcat =\n\u2211 X\u2208cat\u2211\nX\nIf we randomly pick CA close neighbors of entity A, the probability of having K close neighbors in the group will be:\nP (cat,K,CA) = ( CA K ) (Pcat K \u2217 (1\u2013Pcat)CA\u2013K)\nWe then use the real number of close neighbors CA and close neighbors that are in the category GA from observations in vector space and the surprise level of the category from observer A will be defined as:\nScat,A = \u2211\nX\u2265GA\nP (cat,X,CA)\nWe then average the surprise level for all observers in the category to get the categorical surprise level. Example in Figure 4 will result in following values:\nPcat = 4\n8 = 0.5\nScat,A = Scat,C =\n( 2\n2\n) Pcat = 0.25\nScat,B = Scat,D =\n( 3\n2\n) Pcat + ( 3\n3\n) Pcat = 0.5\nScat = 0.25 + 0.5 + 0.25 + 0.5\n4 = 0.375"}, {"heading": "E. Chinese menu combination", "text": "In our experiment, we will conduct a grid search with following choices:\n\u2022 Feature vectors: (LDA, Deepwalk) \u2022 Distance function: (L1, L2, Cosine, KL (for LDA) and\nJS (for LDA))\n\u2022 Close neighbors: (Count and Distance). For count strategy we will test using parameters of 5, 10, 25, 50 and 100. For distance strategy we sort pair-wise distances and choose thresholds to keep an average of 5, 10, 25, 50 and 100 close neighbors correspondingly.\n\u2022 Measurement: (Conductance and Surprise level)"}, {"heading": "V. EVALUATION RESULTS AND ANALYSIS", "text": "We will rank all Wikipedia categories in our data collection using our Chinese menu combination model. However, final quality of our ranking will be evaluated according to the agreement with human annotations, i.e. the ranks of 1067 categories that appear in at least 1 question in human annotations."}, {"heading": "A. Basic evaluation", "text": "A single answer in our collection votes for the most descriptive categories among 5 possible choices, thus each answer actually indicates 4 comparisons between the voted one and the remaining 4. If the voted answer ranks top in our ranking compared with other 4 choices, then our ranking get 1 point for making 4 correct judges; if the voted answer ranks second then we miss one comparison and score 0.75 point ans so on. For each answer, we will gain (5 \u2013 i) * 0.25 point towards the final score where i is the relative rank among 5 choices in our ranking. Rough overall accuracy will be calculated as:\nAccrough = \u2211 i Score for answeri |Answers|\nWhere the total number of answers is 10,000 (500 questions * 20 answers each question)."}, {"heading": "B. Maximum possible accuracy?", "text": "Human reviews are not perfect and there are possible conflicts between answers. Observations on the data collection show that some categories are highly correlative and confusing as they often co-appear and they share similar descriptive power. Table I lists top 10 most confusing category pairs.\nSince the existence of confusing category pairs, human reviews cannot perfectly reach the maximum possible accuracy of 100% and the descriptive power will reflect in the distribution of voted answers. Here we propose an improved overall accuracy which consider the imperfectness of human reviews. We construct a graph where nodes are Wikipedia categories and directed edges show that people votes more to one answer if both appear in the same question. We run a topological sort algorithm to figure out that the best possible \u201ccheating\u201d score is 8456.25, which means based on the gold standard from Crowdflower human reviews, about 15.5% answers will never be correct.\nThe improved overall accuracy is now set to:\nAccimproved = \u2211 i Score for answeri |BestCheatingScore|\nC. Influence of feature vectors\nWe first discuss the influence of choosing different feature vectors in Table II.\nDeepwalk basically outperforms LDA. Deepwalk actually making 1.5% more answers correct and points gained from agreement with the 1st vote from human reviews are also higher. Distance distribution both vectors are stable since the gap between each threshold (5, 10, 25, 50, 100) is increasing reasonably, indicating that points in high-dimensional spaces\nare not too dense. Deepwalk also win considering average performances among all Chinese menus parameters, with higher accuracy and lower standard deviation.\nD. Influence of distance measurement\nTable III shows statistics of how different distance measurements change the performance.\nFrom our observations there are no big gaps discovered between L1 normalization and L2 normalization for best / average performances. Cosine similarity is working well on average cases but it does not win to produce the best performances. KL divergence and JS divergence, though specifically\ndesigned for probability distributions, do not yield better performances in our tasks. This phenomena probably indicates the redundancy in feature vectors so that no matter which normalization function was chosen, there will be a factor that preserve the property of making similar points close enough.\nE. Influence of defining close neighbors\nFigure 5 shows 10 different definitions of close neighbors, with corresponding performances, including 5 using count as threshold and 5 using distance as threshold\nIt is clear that judging close neighbors using only the value of distance is worse. The reason is that density of local surrounding of semi-isolated points (e.g. person with few introduction text and Wikipedia links) is much lower than those frequently mentioned historical figures. Setting threshold to be a certain diameter will create uneven distribution of close neighbors, thus lower quality and stability of similarity measurement.\nOn the other hand, both count and distance threshold shows a peak within the range of 25 to 50 average close neighbors, which is approximately 0.005% to 0.01% of the whole data collection (Total number of people\u2019s pages on Wikipedia about 557,596). Neither increasing nor decreasing this value will give better performances. We believe such criteria would still work even under more sophisticated circumstance, e.g. considering most famous 50,000 people since points in the embedding space are usually evenly distributed.\nF. Influence of ranking criteria\nTable IV lists comparisons between \u201cConductance\u201d and \u201cSurprise level\u201d. One interesting observation is, the best performance of all conductance measurement happens when creating an average of 100 close neighbors for each point in feature\nvector space. However, we discovered that Surprise level measurement still outperforms Conductance even with much fewer close neighbors, indicating that the size of category should be carefully examined during the procedure."}, {"heading": "G. Error analysis", "text": "We try to make a deep analysis based on the best ranking, which is generated using Deepwalk embedding, L2 normalization, each person having 25 close neighbors and measured via Surprise level. We list all categories with more votes but less surprise level in our data collection. Table V shows the top 10 of them:\nAs we can see, 3 out of 10 categories (Prime Ministers of the United Kingdom, Presidents of the United States, First Ladies of the United States) are political leaders whose titles are so recognizable that they bestowed enough to distinguish this person from the others. However, such categories have a long history \u2013 it is quite possible that there are less similarity between U.S. presidents in 1800s and U.S. presidents after 2000 except the title itself and Deepwalk did not find supporting evidences from Wikipedia links. \u201cThe Beatles Members\u201d plays a special role since the size of the category is too small while the descriptive power is unbelievably large. The\nremaining categories are rough and generalized, sometimes with confusions, which makes it hard to process."}, {"heading": "H. Sample Results", "text": "Here we listed top 100 Wikipedia categories discovered by our algorithms in Table VI using best Deepwalk model with 50 close neighbors, L2 normalization and Surprise level (represented in logarithm as S-Level)."}, {"heading": "VI. CONCLUSION", "text": "In our work we proposed models to rank Wikipedia categories according to their descriptive power. We experimented two models to convert texts of links in Wikipedia to feature vectors and tried different definition of closeness that reflects similarities between entities. We then calculate descriptive power based on categorical coherence, which reflects how well a category keeps its inside members close from latent representations of feature vectors. Our models naturally extend to analyzing pages in different languages, and also to extend to other classes of entities like locations (i.e. cities and countries) and organizations (companies and universities) and we are able to identify similar individuals for suggesting friends in social networks, or even matching algorithms pairing up roommates or those seeking romantic partners.\nWe collected human reviews indicating descriptive power Wikipedia categories from Crowdflower. We tested our models on approximately 600,000 historical figures pages from Wikipedia. Deepwalk model trained using Wikipedia links yielded the best overall accuracy of 88.27% in our evaluation, showing a good recovery of importance of Wikipedia categories.\nThe final target of our application is not only focusing on identifying similarities on Wikipedia. Considering possible applications of finding similar people via their personal webpage or resume (which focus on text) or their social network friends list (using graph structures), we are glad to see that our models can be applied to various types of text and graphs. Such utility could help improve algorithms of online recommending systems.\nWe provide sorted list of Wikipedia categories that best summarize the historical/cultural significance of people, which can be used to better explain similarity between historical figures or serve as a pivot to improve search results. We collect knowledge to understand the importance of these particular strong or overrepresented features in our analysis.\nFuture work includes parameterizing our methods so we can capture different tradeoffs between different models as topic-based analysis and links analysis focus on different aspects of the pages."}], "references": [{"title": "Who\u2019s Bigger?: Where Historical Figures Really Rank", "author": ["S. Skiena", "C.B. Ward"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Entity ranking using wikipedia as a pivot", "author": ["R. Kaptein", "P. Serdyukov", "A. De Vries", "J. Kamps"], "venue": "Proceedings of the 19th ACM international conference on Information and knowledge management. ACM, 2010, pp. 69\u201378.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Ranking of wikipedia articles in search engines revisited: Fair ranking for reasonable quality?", "author": ["D. Lewandowski", "U. Spree"], "venue": "Journal of the American Society for Information Science and technology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Entity ranking in wikipedia", "author": ["A.-M. Vercoustre", "J.A. Thom", "J. Pehcevski"], "venue": "Proceedings of the 2008 ACM symposium on Applied computing. ACM, 2008, pp. 1101\u20131106.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Latent dirichlet allocation for tag recommendation", "author": ["R. Krestel", "P. Fankhauser", "W. Nejdl"], "venue": "Proceedings of the third ACM conference on Recommender systems. ACM, 2009, pp. 61\u201368.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "The anatomy of a large-scale hypertextual web  search engine", "author": ["S. Brin", "L. Page"], "venue": "Computer networks and ISDN systems, vol. 30, no. 1, pp. 107\u2013117, 1998.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Authoritative sources in a hyperlinked environment", "author": ["J.M. Kleinberg"], "venue": "Journal of the ACM (JACM), vol. 46, no. 5, pp. 604\u2013632, 1999.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201914. New York, NY, USA: ACM, 2014, pp. 701\u2013710. [Online]. Available: http://doi.acm.org/10.1145/2623330.2623732", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Using wikipedia categories and links in entity ranking", "author": ["A.-M. Vercoustre", "J. Pehcevski", "J.A. Thom"], "venue": "Focused Access to XML Documents. Springer, 2008, pp. 321\u2013335.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Entity ranking in wikipedia: utilising categories, links and topic difficulty prediction", "author": ["J. Pehcevski", "J.A. Thom", "A.-M. Vercoustre", "V. Naumovski"], "venue": "Information retrieval, vol. 13, no. 5, pp. 568\u2013600, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Using wikipedia categories for ad hoc search", "author": ["R. Kaptein", "M. Koolen", "J. Kamps"], "venue": "Proceedings of the 32nd international ACM SIGIR  conference on Research and development in information retrieval. ACM, 2009, pp. 824\u2013825.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Extracting semantics relationships between wikipedia categories.", "author": ["S. Chernov", "T. Iofciu", "W. Nejdl", "X. Zhou"], "venue": "SemWiki, vol", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "We use \u201cranking of significance\u201d described in [1] to decide the most famous people on Wikipedia.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "Certain ranking can serve as a pivot for extensibility or analysis [3], [4] or be used to answer queries in named entity recognition [5].", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "Certain ranking can serve as a pivot for extensibility or analysis [3], [4] or be used to answer queries in named entity recognition [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Certain ranking can serve as a pivot for extensibility or analysis [3], [4] or be used to answer queries in named entity recognition [5].", "startOffset": 133, "endOffset": 136}, {"referenceID": 0, "context": "Additionally, retrieving real-life knowledge of reputations, fames and historical significance from entity ranking is also valuable [1].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "LDA is among the most valuable approaches in such tasks [6].", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "Topics from LDA highly agree with real tags when finding most important feature words of a page [7].", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": "Representatives include PageRank [8] and HITS [9].", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "Representatives include PageRank [8] and HITS [9].", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "Recent work of Deepwalk [10] uses truncated random walks to learn latent representations by encoding social relations in a continuous vector space, which can be easily exploited by statistical models.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "On Wikipedia, performance of entity ranking is improved by utilizing Wikipedia categories [11].", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "Links and topic difficulty prediction together with category information greatly boost the performance of entity ranking [12].", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "Additionally, Wikipedia categories can be used to boost search results in an ad hoc way [13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "Researchers also found that it is possible to analyze consistent semantic relationships in the tags [14] and corresponding latent representations of raw text would help tag recommendations [7].", "startOffset": 100, "endOffset": 104}, {"referenceID": 5, "context": "Researchers also found that it is possible to analyze consistent semantic relationships in the tags [14] and corresponding latent representations of raw text would help tag recommendations [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 0, "context": "We start a project on Crowdflower, a leading peoplepowered data enrichment platform, to collect human reviews of the most descriptive Wikipedia categories on the top-500 most famous people according to \u201cranking of significance\u201d described in [1].", "startOffset": 241, "endOffset": 244}, {"referenceID": 8, "context": "We use the package described in [10] with 128 output dimensions to train Deepwalk embedding on adjacent lists of all Wikipedia pages.", "startOffset": 32, "endOffset": 36}], "year": 2017, "abstractText": "Wikipedia is a useful knowledge source that benefits many applications in language processing and knowledge representation. An important feature of Wikipedia is that of categories. Wikipedia pages are assigned different categories according to their contents as human-annotated labels which can be used in information retrieval, ad hoc search improvements, entity ranking and tag recommendations. However, important pages are usually assigned too many categories, which makes it difficult to recognize the most important ones that give the best descriptions. In this paper, we propose an approach to recognize the most descriptive Wikipedia categories. We observe that historical figures in a precise category presumably are mutually similar and such categorical coherence could be evaluated via texts or Wikipedia links of corresponding members in the category. We rank descriptive level of Wikipedia categories according to their coherence and our ranking yield an overall agreement of 88.27% compared with human wisdom.", "creator": "LaTeX with hyperref package"}}}