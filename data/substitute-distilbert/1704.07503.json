{"id": "1704.07503", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Learning of Human-like Algebraic Reasoning Using Deep Feedforward Neural Networks", "abstract": "there is so wide fusion between symbolic methods and deep learning. in this research, we explore that possibility of adapting deep learning to improve the reasoning. briefly, in a reasoning system, a deep feedforward neural network is used let guide rewriting processes concerning learning from algebraic reasoning examples produced by humans. to enable implicit neural network to recognise patterns of algebraic figures with non - deterministic sizes, reduced partial trees are used among represent the expressions. also, to represent both top - down and bottom - up information of the expressions, a centralisation technique is used to improve the reduced partial trees. besides, symbolic association vectors and chemical application records are used into improve analysis rewriting procedure. experimental results reveal that the algebraic reasoning examples can be accurately learnt only if the feedforward neural network has enough hidden layers. also, the centralisation technique, the symbolic association vectors plus the lattice application records can reduce error rates of reasoning. in particular, the various approaches have led to 2. 08 % error rate of reasoning on a dataset of linear arrays, differentials and integrals.", "histories": [["v1", "Tue, 25 Apr 2017 01:10:09 GMT  (870kb,D)", "http://arxiv.org/abs/1704.07503v1", "8 pages, 7 figures"]], "COMMENTS": "8 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.LO", "authors": ["cheng-hao cai", "dengfeng ke", "yanyan xu", "kaile su"], "accepted": false, "id": "1704.07503"}, "pdf": {"name": "1704.07503.pdf", "metadata": {"source": "CRF", "title": "Learning of Human-like Algebraic Reasoning Using Deep Feedforward Neural Networks", "authors": ["Cheng-Hao Cai", "Dengfeng Ke", "Yanyan Xu", "Kaile Su"], "emails": ["chenghao.cai@outlook.com,", "dengfeng.ke@nlpr.ia.ac.cn,", "xuyanyan@bjfu.edu.cn,", "k.su@griffith.edu.au"], "sections": [{"heading": "1 Introduction", "text": "It is challenging to integrate symbolic reasoning and deep learning in effective ways [Garcez et al., 2015]. In the field of symbolic reasoning, much work has been done on using formal methods to model reliable reasoning processes [Chang and Lee, 1973]. For instance, algebraic reasoning can be modelled by using first-order predicate logics or even higher-order logics, but these logics are usually designed by experienced experts, because it is challenging for machines to learn these logics from data automatically [Bundy and Welham, 1981; Nipkow et al., 2002]. On the other hand, recent approaches on deep learning have revealed that deep neural networks are powerful tools for learning from data [Lecun et al., 2015], especially for learning speech features\n\u2217Corresponding Author.\n[Mohamed et al., 2012] and image features [Sun et al., 2015]. However, not much work has been done on using deep neural networks to learn formal symbolic logics. To close the gap between symbolic reasoning and deep learning, this research explores the possibility of using deep feedforward neural networks to learn logics of rewriting in algebraic reasoning. In other words, we try to teach neural networks to solve mathematical problems, such as finding the solution of an equation and calculating the differential or integral of an expression, by using a rewriting system.\nRewriting is an important technique in symbolic reasoning. Its core concept is to simply reasoning process by using equivalence relations between different expressions [Bundy, 1983]. Usually, rewriting is based on a tree-manipulating system, as many algebraic expressions can be represented by using tree structures, and the manipulation of symbols in the expressions is equivalent to the manipulation of nodes, leaves and sub-trees on the trees [Rosen, 1973]. To manipulate symbols, a rewriting system usually uses one way matching, which is a restricted application of unification, to find a desired pattern from an expression and then replaces the pattern with another equivalent pattern [Bundy, 1983]. In order to reduce the search space, rewriting systems are expected to be Church-Rosser, which means that they should be terminating and locally confluent [Rosen, 1973; Huet, 1980]. Thus, very careful designs and analyses are needed: A design can start from small systems, because proving termination and local confluence of a smaller system is usually easier than proving those of a larger system [Bundy and Welham, 1981]. Some previous work has focused on this aspect: The Knuth-Bendix completion algorithm can be used to solve the problem of local confluence [Knuth and Bendix, 1983], and Huet [1981] has provided a proof of correctness for this algorithm. Also, dependency pairs [Arts and Giesl, 2000] and semantic labelling [Zantema, 1995] can solve the problem of termination for some systems. After multiple small systems have been designed, they can be combined into a whole system, because the direct sum of two Church-Rosser systems holds the same property [Toyama, 1987].\nDeep neural networks have been used in many fields of artificial intelligence, including speech recognition [Mohamed et al., 2012], human face recognition [Sun et al., 2015], natural language understanding [Sarikaya et al., 2014], reinforcement learning for playing video games [Mnih\nar X\niv :1\n70 4.\n07 50\n3v 1\n[ cs\n.A I]\n2 5\nA pr\n2 01\n7\net al., 2015] and Monte Carlo tree search for playing Go [Silver et al., 2016]. Recently, some researchers are trying to extend them to reasoning tasks. For instance, Irving et al. [2016] have proposed DeepMath for automated theorem proving with deep neural networks. Also, Serafini and Garcez [2016] have proposed logic tensor networks to combine deep learning with logical reasoning. In addition, Garnelo et al. [2016] have explored deep symbolic reinforcement learning.\nIn this research, we use deep feedforward neural networks [Lecun et al., 2015] to guide rewriting processes. This technique is called human-like rewriting, as it is adapted from standard rewriting and can simulate human\u2019s behaviours of using rewrite rules after learning from algebraic reasoning schemes. The following sections provide detailed discussions about this technique: Section 2 introduces the core method of human-like rewriting. Section 3 discusses algebraic reasoning schemes briefly. Section 4 provides three methods for system improvement. Section 5 provides experimental results of the core method and the improvement methods. Section 6 is for conclusions."}, {"heading": "2 Human-like Rewriting", "text": "Rewriting is an inference technique for replacing expressions or subexpressions with equivalent ones [Bundy, 1983]. For instance1, given two rules of the Peano axioms:\nx+ 0\u21d2 x (1)\nx+ S(y)\u21d2 S(x+ y) (2) S(0) + S(S(0)) can be rewritten via:\nS(0) + S(S(0))\ufe38 \ufe37\ufe37 \ufe38 by (2) \u21d2 S( S(0) + S(0)\ufe38 \ufe37\ufe37 \ufe38 by (2) ) \u21d2 S(S( S(0) + 0\ufe38 \ufe37\ufe37 \ufe38 by (1) ))\n\u21d2 S(S(S(0)))\n(3)\nMore detailed discussions about the Peano axioms can be found from [Pillay, 1981]. Generally, rewriting requires a source expression s and a set of rewrite rules \u03c4 . Let l \u21d2 r denote a rewrite rule in \u03c4 , t a subexpression of s, and \u03b8 the most general unifier of one way matching from l and t. A single rewriting step of inference can be formed as:\ns(t) (l\u21d2 r) \u2208 \u03c4 l[\u03b8] \u2261 t s(r[\u03b8])\n(4)\nIt is noticeable that \u03b8 is only applied to l, but not to t. The reason is that one way matching, which is a restricted application of unification, requires that all substitutions in a unifier are only applied to the left-hand side of a unification pair. Standard rewriting is to repeat the above step until\n1We use the mathematical convention that a word is a constant if its first letter is in upper case, and it is a variable if its first letter is in lower case.\nno rule can be applied to the expression further. It requires the set of rewrite rules \u03c4 to be Church-Rosser, which means that \u03c4 should be terminating and locally confluent. This requirement restricts the application of rewriting in many\nfields. For instance, the chain rule in calculus D(f)\nD(x) \u21d2\nD(f) D(u) \u00b7 D(u) D(x) , which is very important for computing derivatives, will result in non-termination:\nD(Sin(X))\nD(X)\n\u21d2 D(Sin(X)) D(u1) \u00b7 D(u1) D(X)\n\u21d2 D(Sin(X)) D(u2) \u00b7 D(u2) D(u1) \u00b7 D(u1) D(X)\n\u21d2 D(Sin(X)) D(u3) \u00b7 D(u3) D(u2) \u00b7 D(u2) D(u1) \u00b7 D(u1) D(X)\n\u21d2 \u00b7 \u00b7 \u00b7\n(5)\nThe above process means that it is challenging to use the chain rule in standard rewriting. Similarly, a commutativity rule x \u25e6 y \u21d2 y \u25e6 x, where \u25e6 is an addition, a multiplication, a logical conjunction, a logical disjunction or another binary operation satisfying commutativity, is difficult to be used in standard rewriting. If termination is not guaranteed, it will be difficult to check local confluence, as local confluence requires a completely developed search tree, but non-termination means that the search tree is infinite and cannot be completely developed. More detailed discussion about standard rewriting and Church-Rosser can be found from [Bundy, 1983].\nHuman-like rewriting is adapted from standard rewriting. It uses a deep feedforward neural network [Lecun et al., 2015] to guide rewriting processes. The neural network has learnt from some rewriting examples produced by humans, so that it can, to some extent, simulate human\u2019s ways of using rewrite rules: Firstly, non-terminating rules are used to rewrite expressions. Secondly, local confluence is not checked. Lastly, experiences of rewriting can be learnt and can guide future rewriting processes.\nTo train the feedforward neural network, input data and target data are required. An input can be generated via the following steps: Firstly, an expression is transformed to a parsing tree [Huth and Ryan, 2004] with position annotations. A position annotation is a unique label < p1, p2, \u00b7 \u00b7 \u00b7 , pN > indicating a position on a tree, where each pi is the order of a branch. Then the tree is reduced to a set of partial trees with a predefined maximum depth d. Next, the partial trees are expanded to perfect k-ary trees with the depth d and a predefined breadth k. In particular, empty positions on the prefect k-ary trees are filled by Empty. After that, the perfect k-ary trees are transformed to lists via in-order traversal. Detailed discussions about perfect k-ary trees and in-order traversal can be found from [Cormen et al., 2001]. Finally, the lists with their position annotations are transformed to a set of one-hot representations [Turian et\nal., 2010]. In particular, Empty is transformed to a zero block. Figure 1 provides an example for the above procedure. This representation is called a reduced partial tree (RPT) representation of the expression. A target is the one-hot representation [Turian et al., 2010] of a rewrite rule name with a position annotation for applying the rule.\nIt is noticeable that the input of the neural network is a set of vectors, and the number of vectors is non-deterministic, as it depends on the structure of the expression. However, the target is a single vector. Thus, the dimension of the input will disagree with the dimension of the target if a conventional feedforward neural network structure is used. To solve this problem, we replace its Softmax layer with an averaged Softmax layer. Let xj,i denote the ith element of the jth input vector, P the number of the input vectors, u an averaged input vector, ui the ith element of u, W a weight matrix, b a bias\nvector, Softmax the standard Softmax function [Bishop, 2006], and y the output vector. The averaged Softmax layer is defined as:\nui = 1\nP P\u2211 j=1 xj,i (6)\ny = Softmax(W \u00b7 u+ b) (7) It is noticeable that the output is a single vector regardless of the number of the input vectors.\nThe feedforward neural network is trained by using the back-propagation algorithm with the cross-entropy error function [Hecht-Nielsen, 1988; Bishop, 2006]. After training, the neural network can be used to guide a rewriting procedure: Given the RPT representation of an expression, the neural network uses forward computation to get an output vector, and the position of the maximum element indicates the name of a rewrite rule and a possible position for the application of the rule."}, {"heading": "3 Algebraic Reasoning Schemes", "text": "The learning of the neural network is based on a set of algebraic reasoning schemes. Generally, an algebraic reasoning scheme consists of a question, an answer and some intermediate reasoning steps. The question is an expression indicating the starting point of reasoning. The answer is an expression indicating the goal of reasoning. Each intermediate reasoning step is a record consisting of: \u2022 A source expression; \u2022 The name of a rewrite rule; \u2022 A position annotation for applying the rewrite rule; \u2022 A target expression.\nIn particular, the source expression of the first reasoning step is the question, and the target expression of the final reasoning step is the answer. Also, for each reasoning step, the target expression will be the source expression of the next step if the \u201cnext step\u201d exists. By applying all intermediate reasoning steps, the question can be rewritten to the answer deterministically.\nIn this research, algebraic reasoning schemes are developed via a rewriting system in SWI-Prolog [Wielemaker et al., 2012]. The rewriting system is based on Rule (4), and it uses breadth-first search to find intermediate reasoning steps from a question to an answer. Like most rewriting systems and automated theorem proving systems2, its ability of reasoning is restricted by the problem of combinatorial explosion: The number of possible ways of reasoning can grow rapidly when the question becomes more complex [Bundy, 1983]. Therefore, a full algebraic reasoning scheme of a complex question is usually difficult to be generated automatically, and guidance from humans is required. In other words, if the system fails to develop the scheme, we will apply rewrite rules manually until the remaining part of the scheme can be developed automatically, or we will\n2A practical example is the \u201cby auto\u201d function of Isabelle/HOL [Nipkow et al., 2002]. It is often difficult to prove a complex theorem automatically, so that experts\u2019 guidance is often required.\nprovide some subgoals for the system to reduce the search space. After algebraic reasoning schemes are developed, their intermediate reasoning steps are used to train the neural network: For each step, the RPT representation of the source expression is the input of the neural network, and the onehot representation of the rewrite rule name and the position annotation is the target of the neural network, as discussed by Section 2."}, {"heading": "4 Methods for System Improvement", "text": ""}, {"heading": "4.1 Centralised RPT Representation", "text": "The RPT representation discussed before is a top-down representation of an expression: A functor in the expression is a node, and arguments dominated by the functor are child nodes or leaves of the node. However, it does not record bottom-up information about the expression. For instance, in Figure 1, the partial tree labelled < 1, 1 > does not record any information about its parent node \u201c=\u201d.\nA centralised RPT (C-RPT) representation can represent\nboth top-down and bottom-up information of an expression: Firstly, every node on a tree considers itself as the centre of the tree and grows an additional branch to its parent node (if it exists), so that the tree becomes a directed graph. This step is called \u201ccentralisation\u201d. Then the graph is reduced to a set of partial trees and expanded to a set of perfect k-ary trees. In particular, each additional branch is defined as the kth branch of its parent node, and all empty positions dominated by the parent node are filled by Empty. Detailed discussions about perfect k-ary trees and directed graphs can be found from [Cormen et al., 2001]. Figure 2 provides an example for the above steps. Finally, these perfect k-ary trees are transformed to lists and further represented as a set of vectors, as discussed by Section 2."}, {"heading": "4.2 Symbolic Association Vector", "text": "Consider the following rewrite rule:\nx\u00d7 x\u21d2 x2 (8)\nThe application of this rule requires that two arguments of \u201c\u00d7\u201d are the same. If this pattern exists in an expression, it will be a useful hint for selecting rules. In such case, the use of a symbolic association vector (SAV) can provide useful information for the neural network: Assume that H is the list representation of a perfect k-ary tree (which has been discussed by Section 2) with a length L. S is defined as an L\u00d7 L matrix which satisfies:\nSi,j = { 1, if Hi = Hj and i 6= j; 0, otherwise.\n(9)\nAfter the matrix is produced, it can be reshaped to a vector and be a part of an input vector of the neural network."}, {"heading": "4.3 Rule Application Record", "text": "Previous applications of rewrite rules can provide hints for current and future applications. In this research, we use rule application records (RAR) to record the previous applications of rewrite rules: Let Qi denote the ith element of an RAR Q, rulei the name of the previous ith rewrite rule, and posi the position annotation for applying the rule. Qi is defined as:\nQi \u2261 < rulei, posi > (10)\nUsually, the RAR only records the last N applications of rewrite rules, where N is a predefined length of Q. To enable the neural network to read the RAR, it needs to be transformed to a one-hot representation [Turian et al., 2010]. A drawback of RARs is that they cannot be used in the first N steps of rewriting, as they record exactly N previous applications of rewrite rules."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Datasets and Evaluation Metrics", "text": "A dataset of algebraic reasoning schemes is used to train and test models. This dataset contains 400 schemes about linear equations, differentials and integrals and 80 rewrite rules, and these schemes consist of 6,067 intermediate reasoning steps totally. We shuffle the intermediate steps and then divide\nthem into a training set and a test set randomly: The training set contains 5,067 examples, and the test set contains 1,000 examples. After training a model with the training set, an error rate of reasoning on the test set is used to evaluate the model, and it can be computed by:\nError Rate = NError\nNTotal \u00d7 100% (11)\nwhere NError is the number of cases when the model fails to indicate an expected application of rewrite rules, and NTotal is the number of examples in the test set."}, {"heading": "5.2 Using RPT Representations and Neural Networks", "text": "In this part, we evaluate the core method of human-like rewriting: All expressions in the dataset are represented by using the RPT representations. The breadth of an RPT is set to 2, because the expressions in the dataset are unary or binary. The depth of an RPT is set to 1, 2 or 3. Also, feedforward neural networks [Lecun et al., 2015] with 1, 3 and 5 hidden layers are used to learn from the dataset. The number of units in each hidden layer is set to 1,024, and their activation functions are rectified linear units (ReLU) [Glorot et al., 2011]. The output layer of each neural network is an averaged Softmax layer. The neural networks are trained via the back-propagation algorithm with the crossentropy error function [Hecht-Nielsen, 1988; Bishop, 2006]. When training models, learning rates are decided by the Newbob+/Train strategy [Wiesler et al., 2014]: The initial learning rate is set to 0.01, and the learning rate is halved when the average improvement of the cross-entropy loss on the training set is smaller than 0.1. The training process stops when the improvement is smaller than 0.01.\nFigure 3 provides learning curves of the models, where \u201cFNNn\u201d means that the neural network has n hidden layers, and \u201cRPTm\u201d means that the depth of RPTs is m. To aid the readability, the curves of \u201cFNN1\u201d, \u201cFNN3\u201d and \u201cFNN5\u201d are in blue, red and green respectively, and the curves of \u201cRPT1\u201d, \u201cRPT2\u201d and \u201cRPT3\u201d are displayed by using dotted lines, dashed lines and solid lines respectively. By comparing the curves with the same colour, it is noticeable that more hidden\nlayers can bring about significantly better performance of learning. On the other hand, if the neural network only has a single hidden layer, the learning will stop early, while the cross-entropy loss is very high. Also, by comparing the curves with the same type of line, it is noticeable that a deeper RPT often brings about better performance of learning, but an exception is the curve of the \u201cFNN5 + RPT2\u201d model.\nTable 1 reveals performance of the trained models on the test set. In this table, results in \u201cFNNn\u201d rows and \u201cRPTm\u201d columns correspond to the \u201cFNNn+RPTm\u201d models in Figure 3. It is noticeable that the error rates of reasoning decrease significantly when the numbers of hidden layers increase. Also, the error rates of reasoning often decrease when the depths of RPTs increase, but an exception occurs in the case of \u201cFNN5 + RPT2\u201d. We believe that the reason why the exception occurs is that the learning rate strategy results in early stop of training. In addition, the error rate of the FNN5 + RPT3 model is the best among all results."}, {"heading": "5.3 Using Improvement Methods", "text": "In Section 5.2, we have found that the neural networks with 5 hidden layers have better performance than those with 1 or 3 hidden layers on the task of human-like rewriting. Based on the neural networks with 5 hidden layers, we apply the three improvement methods to these models.\nFigure 4 shows learning curves of models improved by CRPTs, SAVs and RARs. Also, learning curves of the baseline RPTm models are displayed by using dashed lines, where m is the depth of RPTs. Learning curves of the C-RPT models are displayed by Figure 4(a). A comparison between two lines in the same colour reveals that the C-RPT representation can improve the model when m is fixed. Also, the C-RPT2 curve is very close to the RPT3 curve during the last 6 epochs, which reveals that there might be a trade-off between using C-RPTs and increasing the depth of RPTs. The best learning curve is the C-RPT3 curve, as its cross-entropy loss is always the lowest during all epochs. Figure 4(b) provides learning curves of the RPT models with the SAV method. It is noticeable that SAVs have two effects: The first is that they can bring about lower cross-entropy losses. The second is that they can reduce the costs of learning time, as each RPTm + SAV model uses fewer epochs to finish learning than its counterpart. Figure 4(c) shows learning curves of the RPT models with the RAR method. This figure reveals that RARs always improve the models. In particular, even the RPT1 + RAR model has better learning performance than the RPT3 model. Also, the RPT1 + RAR model and the RPT3 + RAR model use less epochs to be trained, which means that RARs may reduce the time consumption of learning. Figure 4(d) provides learning curves of the models with all\nimprovement methods. A glance at the figure reveals that these models have better performance of learning than the baseline models. Also, they require less epochs to be trained than their counterparts. In addition, the final cross-entropy loss of the C-RPT2 + SAV + RAR model is the lowest among all results.\nTable 2 shows error rates of reasoning on the test set after using the improvement methods. It is noticeable that: Firstly, the C-RPTm models have lower error rates than the baseline RPTmmodels, especially whenm = 2. Secondly, the RPTm + SAV models have lower error rates than the baseline RPTm model when m is 2 or 3, but this is not the case for the RPT1 + SAV model. Thirdly, the RARs can reduce the error rates significantly. Finally, the error rates can be reduced further when the three improvement methods are used together. In\nparticular, the C-RPT2 + SAV + RAR model reaches the best error rate (4.6%) among all models."}, {"heading": "6 Conclusions and Future Work", "text": "Deep feedforward neural networks are able to guide rewriting processes after learning from algebraic reasoning schemes. The use of deep structures is necessary, because the behaviours of rewriting can be accurately modelled only if the neural networks have enough hidden layers. Also, it has been shown that the RPT representation is effective for the neural networks to model algebraic expressions, and it can be improved by using the C-RPT representation, the SAV method and the RAR method. Based on these techniques, human-like rewriting can solve many problems about linear equations, differentials and integrals. In the future, we will try to use human-like rewriting to deal with more complex tasks of mathematical reasoning and extend it to more general first-order logics and higher-order logics."}, {"heading": "Acknowledgments", "text": "This work is supported by the Fundamental Research Funds for the Central Universities (No. 2016JX06) and the National Natural Science Foundation of China (No. 61472369)."}], "references": [{"title": "Comput", "author": ["Thomas Arts", "J\u00fcrgen Giesl. Termination of term rewriting using dependency pairs. Theor"], "venue": "Sci., 236(1-2):133\u2013178,", "citeRegEx": "Arts and Giesl. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher M Bishop"], "venue": "Springer-Verlag New York, Inc.,", "citeRegEx": "Bishop. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Artif", "author": ["Alan Bundy", "Bob Welham. Using meta-level inference for selective application of multiple rewrite rule sets in algebraic manipulation"], "venue": "Intell., 16(2):189\u2013212,", "citeRegEx": "Bundy and Welham. 1981", "shortCiteRegEx": null, "year": 1981}, {"title": "The computer modelling of mathematical reasoning", "author": ["Alan Bundy"], "venue": "Academic Press,", "citeRegEx": "Bundy. 1983", "shortCiteRegEx": null, "year": 1983}, {"title": "Computer science classics", "author": ["Chin-Liang Chang", "Richard C.T. Lee. Symbolic logic", "mechanical theorem proving"], "venue": "Academic Press,", "citeRegEx": "Chang and Lee. 1973", "shortCiteRegEx": null, "year": 1973}, {"title": "Introduction to algorithms (second edition)", "author": ["T H Cormen", "C E Leiserson", "R L Rivest", "C. Stein"], "venue": "page 1297C1305", "citeRegEx": "Cormen et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Neural-symbolic learning and reasoning: Contributions and challenges", "author": ["Garcez et al", "2015] Artur D \u2019Avila Garcez", "Tarek R Besold", "Luc De Raedt", "Peter Fldiak", "Pascal Hitzler", "Thomas Icard", "Kai Uwe Khnberger", "Luis C Lamb", "Risto Miikkulainen", "Daniel L Silver"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "CoRR", "author": ["Marta Garnelo", "Kai Arulkumaran", "Murray Shanahan. Towards deep symbolic reinforcement learning"], "venue": "abs/1609.05518,", "citeRegEx": "Garnelo et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot et al", "2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Neural Networks", "author": ["Robert Hecht-Nielsen. Theory of the backpropagation neural network"], "venue": "1(Supplement-1):445\u2013448,", "citeRegEx": "Hecht.Nielsen. 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "Confluent reductions: Abstract properties and applications to term rewriting systems: Abstract properties and applications to term rewriting systems", "author": ["Grard Huet"], "venue": "Journal of the Acm, 27(4):797\u2013821,", "citeRegEx": "Huet. 1980", "shortCiteRegEx": null, "year": 1980}, {"title": "Syst", "author": ["G\u00e9rard P. Huet. A complete proof of correctness of the knuth-bendix completion algorithm. J. Comput"], "venue": "Sci., 23(1):11\u201321,", "citeRegEx": "Huet. 1981", "shortCiteRegEx": null, "year": 1981}, {"title": "Logic in computer science - modelling and reasoning about systems (2", "author": ["Michael Huth", "Mark Dermot Ryan"], "venue": "ed.). Cambridge University Press,", "citeRegEx": "Huth and Ryan. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Deepmath - deep sequence models for premise selection", "author": ["Irving et al", "2016] Geoffrey Irving", "Christian Szegedy", "Alexander A. Alemi", "Niklas E\u00e9n", "Fran\u00e7ois Chollet", "Josef Urban"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Simple word problems in universal algebras", "author": ["Knuth", "Bendix", "1983] Donald E. Knuth", "Peter B. Bendix"], "venue": null, "citeRegEx": "Knuth et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Knuth et al\\.", "year": 1983}, {"title": "Nature", "author": ["Yann Lecun", "Yoshua Bengio", "Geoffrey Hinton. Deep learning"], "venue": "521(7553):436\u2013 444,", "citeRegEx": "Lecun et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih et al", "2015] Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Acoustic modeling using deep belief networks", "author": ["Mohamed", "George E. Dahl", "Geoffrey E. Hinton"], "venue": "IEEE Trans. Audio, Speech & Language Processing,", "citeRegEx": "Mohamed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2012}, {"title": "Isabelle/HOL - A Proof Assistant for Higher-Order Logic", "author": ["Tobias Nipkow", "Lawrence C. Paulson", "Markus Wenzel"], "venue": "volume 2283 of Lecture Notes in Computer Science. Springer,", "citeRegEx": "Nipkow et al.. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Journal of Symbolic Logic", "author": ["Anand Pillay. Models of peano arithmetic"], "venue": "67(3):1265\u20131273,", "citeRegEx": "Pillay. 1981", "shortCiteRegEx": null, "year": 1981}, {"title": "In Acm Symposium on Theory of Computing", "author": ["Barry K. Rosen. Tree-manipulating systems", "church-rosser theorems"], "venue": "pages 117\u2013127,", "citeRegEx": "Rosen. 1973", "shortCiteRegEx": null, "year": 1973}, {"title": "IEEE/ACM Trans", "author": ["Ruhi Sarikaya", "Geoffrey E. Hinton", "Anoop Deoras. Application of deep belief networks for natural language understanding"], "venue": "Audio, Speech & Language Processing, 22(4):778\u2013784,", "citeRegEx": "Sarikaya et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "d\u2019Avila Garcez", "author": ["Luciano Serafini", "Artur S"], "venue": "Logic tensor networks: Deep learning and logical reasoning from data and knowledge. CoRR, abs/1606.04422,", "citeRegEx": "Serafini and d.Avila Garcez. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": "search. Nature,", "citeRegEx": "Sutskever et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2016}, {"title": "Deepid3: Face recognition with very deep neural networks", "author": ["Yi Sun", "Ding Liang", "Xiaogang Wang", "Xiaoou Tang"], "venue": "CoRR, abs/1502.00873,", "citeRegEx": "Sun et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "ACM", "author": ["J Yoshihito Toyama. On the church-rosser property for the direct sum of term rewriting systems."], "venue": "34(1):128\u2013143,", "citeRegEx": "Toyama. 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al", "2010] Joseph P. Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "ACL", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "TPLP", "author": ["Jan Wielemaker", "Tom Schrijvers", "Markus Triska", "Torbj\u00f6rn Lager. Swi-prolog"], "venue": "12(1-2):67\u201396,", "citeRegEx": "Wielemaker et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Mean-normalized stochastic gradient for large-scale deep learning", "author": ["Wiesler et al", "2014] Simon Wiesler", "Alexander Richard", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Fundam", "author": ["Hans Zantema. Termination of term rewriting by semantic labelling"], "venue": "Inform., 24(1/2):89\u2013105,", "citeRegEx": "Zantema. 1995", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 4, "context": "In the field of symbolic reasoning, much work has been done on using formal methods to model reliable reasoning processes [Chang and Lee, 1973].", "startOffset": 122, "endOffset": 143}, {"referenceID": 2, "context": "For instance, algebraic reasoning can be modelled by using first-order predicate logics or even higher-order logics, but these logics are usually designed by experienced experts, because it is challenging for machines to learn these logics from data automatically [Bundy and Welham, 1981; Nipkow et al., 2002].", "startOffset": 264, "endOffset": 309}, {"referenceID": 18, "context": "For instance, algebraic reasoning can be modelled by using first-order predicate logics or even higher-order logics, but these logics are usually designed by experienced experts, because it is challenging for machines to learn these logics from data automatically [Bundy and Welham, 1981; Nipkow et al., 2002].", "startOffset": 264, "endOffset": 309}, {"referenceID": 15, "context": "On the other hand, recent approaches on deep learning have revealed that deep neural networks are powerful tools for learning from data [Lecun et al., 2015], especially for learning speech features \u2217Corresponding Author.", "startOffset": 136, "endOffset": 156}, {"referenceID": 17, "context": "[Mohamed et al., 2012] and image features [Sun et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 24, "context": ", 2012] and image features [Sun et al., 2015].", "startOffset": 27, "endOffset": 45}, {"referenceID": 3, "context": "Its core concept is to simply reasoning process by using equivalence relations between different expressions [Bundy, 1983].", "startOffset": 109, "endOffset": 122}, {"referenceID": 20, "context": "Usually, rewriting is based on a tree-manipulating system, as many algebraic expressions can be represented by using tree structures, and the manipulation of symbols in the expressions is equivalent to the manipulation of nodes, leaves and sub-trees on the trees [Rosen, 1973].", "startOffset": 263, "endOffset": 276}, {"referenceID": 3, "context": "To manipulate symbols, a rewriting system usually uses one way matching, which is a restricted application of unification, to find a desired pattern from an expression and then replaces the pattern with another equivalent pattern [Bundy, 1983].", "startOffset": 230, "endOffset": 243}, {"referenceID": 20, "context": "In order to reduce the search space, rewriting systems are expected to be Church-Rosser, which means that they should be terminating and locally confluent [Rosen, 1973; Huet, 1980].", "startOffset": 155, "endOffset": 180}, {"referenceID": 10, "context": "In order to reduce the search space, rewriting systems are expected to be Church-Rosser, which means that they should be terminating and locally confluent [Rosen, 1973; Huet, 1980].", "startOffset": 155, "endOffset": 180}, {"referenceID": 2, "context": "Thus, very careful designs and analyses are needed: A design can start from small systems, because proving termination and local confluence of a smaller system is usually easier than proving those of a larger system [Bundy and Welham, 1981].", "startOffset": 216, "endOffset": 240}, {"referenceID": 0, "context": "Also, dependency pairs [Arts and Giesl, 2000] and semantic labelling [Zantema, 1995] can solve the problem of termination for some systems.", "startOffset": 23, "endOffset": 45}, {"referenceID": 29, "context": "Also, dependency pairs [Arts and Giesl, 2000] and semantic labelling [Zantema, 1995] can solve the problem of termination for some systems.", "startOffset": 69, "endOffset": 84}, {"referenceID": 25, "context": "After multiple small systems have been designed, they can be combined into a whole system, because the direct sum of two Church-Rosser systems holds the same property [Toyama, 1987].", "startOffset": 167, "endOffset": 181}, {"referenceID": 17, "context": "Deep neural networks have been used in many fields of artificial intelligence, including speech recognition [Mohamed et al., 2012], human face recognition [Sun et al.", "startOffset": 108, "endOffset": 130}, {"referenceID": 24, "context": ", 2012], human face recognition [Sun et al., 2015], natural language understanding [Sarikaya et al.", "startOffset": 32, "endOffset": 50}, {"referenceID": 21, "context": ", 2015], natural language understanding [Sarikaya et al., 2014], reinforcement learning for playing video games [Mnih ar X iv :1 70 4.", "startOffset": 40, "endOffset": 63}, {"referenceID": 15, "context": "In this research, we use deep feedforward neural networks [Lecun et al., 2015] to guide rewriting processes.", "startOffset": 58, "endOffset": 78}, {"referenceID": 3, "context": "2 Human-like Rewriting Rewriting is an inference technique for replacing expressions or subexpressions with equivalent ones [Bundy, 1983].", "startOffset": 124, "endOffset": 137}, {"referenceID": 19, "context": "More detailed discussions about the Peano axioms can be found from [Pillay, 1981].", "startOffset": 67, "endOffset": 81}, {"referenceID": 3, "context": "More detailed discussion about standard rewriting and Church-Rosser can be found from [Bundy, 1983].", "startOffset": 86, "endOffset": 99}, {"referenceID": 15, "context": "It uses a deep feedforward neural network [Lecun et al., 2015] to guide rewriting processes.", "startOffset": 42, "endOffset": 62}, {"referenceID": 12, "context": "An input can be generated via the following steps: Firstly, an expression is transformed to a parsing tree [Huth and Ryan, 2004] with position annotations.", "startOffset": 107, "endOffset": 128}, {"referenceID": 5, "context": "Detailed discussions about perfect k-ary trees and in-order traversal can be found from [Cormen et al., 2001].", "startOffset": 88, "endOffset": 109}, {"referenceID": 1, "context": "Let xj,i denote the ith element of the jth input vector, P the number of the input vectors, u an averaged input vector, ui the ith element of u, W a weight matrix, b a bias vector, Softmax the standard Softmax function [Bishop, 2006], and y the output vector.", "startOffset": 219, "endOffset": 233}, {"referenceID": 9, "context": "The feedforward neural network is trained by using the back-propagation algorithm with the cross-entropy error function [Hecht-Nielsen, 1988; Bishop, 2006].", "startOffset": 120, "endOffset": 155}, {"referenceID": 1, "context": "The feedforward neural network is trained by using the back-propagation algorithm with the cross-entropy error function [Hecht-Nielsen, 1988; Bishop, 2006].", "startOffset": 120, "endOffset": 155}, {"referenceID": 27, "context": "In this research, algebraic reasoning schemes are developed via a rewriting system in SWI-Prolog [Wielemaker et al., 2012].", "startOffset": 97, "endOffset": 122}, {"referenceID": 3, "context": "Like most rewriting systems and automated theorem proving systems2, its ability of reasoning is restricted by the problem of combinatorial explosion: The number of possible ways of reasoning can grow rapidly when the question becomes more complex [Bundy, 1983].", "startOffset": 247, "endOffset": 260}, {"referenceID": 18, "context": "A practical example is the \u201cby auto\u201d function of Isabelle/HOL [Nipkow et al., 2002].", "startOffset": 62, "endOffset": 83}, {"referenceID": 5, "context": "Detailed discussions about perfect k-ary trees and directed graphs can be found from [Cormen et al., 2001].", "startOffset": 85, "endOffset": 106}, {"referenceID": 15, "context": "Also, feedforward neural networks [Lecun et al., 2015] with 1, 3 and 5 hidden layers are used to learn from the dataset.", "startOffset": 34, "endOffset": 54}, {"referenceID": 9, "context": "The neural networks are trained via the back-propagation algorithm with the crossentropy error function [Hecht-Nielsen, 1988; Bishop, 2006].", "startOffset": 104, "endOffset": 139}, {"referenceID": 1, "context": "The neural networks are trained via the back-propagation algorithm with the crossentropy error function [Hecht-Nielsen, 1988; Bishop, 2006].", "startOffset": 104, "endOffset": 139}], "year": 2017, "abstractText": "There is a wide gap between symbolic reasoning and deep learning. In this research, we explore the possibility of using deep learning to improve symbolic reasoning. Briefly, in a reasoning system, a deep feedforward neural network is used to guide rewriting processes after learning from algebraic reasoning examples produced by humans. To enable the neural network to recognise patterns of algebraic expressions with non-deterministic sizes, reduced partial trees are used to represent the expressions. Also, to represent both top-down and bottom-up information of the expressions, a centralisation technique is used to improve the reduced partial trees. Besides, symbolic association vectors and rule application records are used to improve the rewriting processes. Experimental results reveal that the algebraic reasoning examples can be accurately learnt only if the feedforward neural network has enough hidden layers. Also, the centralisation technique, the symbolic association vectors and the rule application records can reduce error rates of reasoning. In particular, the above approaches have led to 4.6% error rate of reasoning on a dataset of linear equations, differentials and integrals.", "creator": "TeX"}}}