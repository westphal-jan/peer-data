{"id": "1704.07616", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Joint POS Tagging and Dependency Parsing with Transition-based Neural Networks", "abstract": "while part - of - detail ( pos ) coordination and dependency parsing are observed to very closely related, existing work on joint modeling with manually crafted feature templates departing from the feature sparsity and incompleteness problems. in this paper, we propose an approach to joint pos tagging and dependency parsing ; transition - based neural networks. three neural network based classifiers jointly designed to resolve moderate / reduce, tagging, and labeling conflicts. studies show that our knowledge significantly exaggerated previous methods for joint pos tagging and dependency parsing across a variety, natural types.", "histories": [["v1", "Tue, 25 Apr 2017 10:22:57 GMT  (403kb,D)", "http://arxiv.org/abs/1704.07616v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["liner yang", "meishan zhang", "yang liu", "nan yu", "maosong sun", "guohong fu"], "accepted": false, "id": "1704.07616"}, "pdf": {"name": "1704.07616.pdf", "metadata": {"source": "CRF", "title": "Joint POS Tagging and Dependency Parsing with Transition-based Neural Networks", "authors": ["Liner Yang", "Meishan Zhang", "Yang Liu", "Nan Yu", "Maosong Sun", "Guohong Fu"], "emails": ["mason.zms}@gmail.com,", "liuyang2011@tsinghua.edu.cn,", "yunan.hlju@gmail.com,", "sms@tsinghua.edu.cn,", "ghfu@hlju.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Part-of-speech (POS) tagging [Collins, 2002; Toutanova et al., 2003; dos Santos and Zadrozny, 2014; Huang et al., 2015] and dependency parsing [McDonald et al., 2005; Nivre et al., 2006; Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016] are two fundamental tasks for understanding natural languages. While POS tagging aims to assign parts of speech to words in a text to indicate their word categories, the goal of dependency parsing is to analyze the syntactic structure of sentences by establishing relationships between words.\nIt is widely accepted that POS tagging and dependency parsing are closely related. On one hand, POS tagging often requires long-distance syntactic information for resolving tagging ambiguity [Sun et al., 2013]. Hatori et al. [2011] indicate that the disambiguation between POS tags \u201cDEG\u201d (a genitive marker) and \u201cDEC\u201d (a complementizer) for a Chinese word de often depends on global context. On the other hand, as a pre-processing step, POS tagging directly influences the accuracy of dependency parsing significantly. For example, determining the head word of a two-word phrase \u201cclosed door\u201d directly depends on the POS tag of \u201cclosed\u201d (adjective or verb in past tense). Li et al. [2011] report that dependency accuracy drops by around 6% on Chinese when automatic POS tagging results instead of ground-truth tags are used.\nTherefore, joint POS tagging and dependency parsing has attracted intensive attention in the NLP community. Previous\nwork has focused on jointly modeling POS tagging and dependency parsing using linear models that combine both tagging and parsing features [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Li et al., 2012; Zhang et al., 2012]. Allowing lexicality and syntax to interact in a unified framework, joint POS tagging and dependency parsing improves both tagging and parsing performance over independent modeling significantly [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012].\nHowever, existing work on joint POS tagging and dependency parsing suffers from the feature sparsity and incompleteness problems. Chen and Manning [2014] indicate that lexicalized indicator features indispensable for discriminative dependency parsing are usually highly sparse. The situation in joint POS tagging and dependency parsing is much severer because tagging and parsing features are concatenated in joint models [Li et al., 2011]. Moreover, due to the complexity of tagging and parsing natural languages, it is hard for manually-designed features to cover all regularities. As a result, the incompleteness of feature design is considered as an unavoidable issue in conventional discriminative models [Chen and Manning, 2014].\nIn this paper, we propose an approach to joint POS tagging and dependency parsing with neural networks by extending from a transition-based dependency parsing model. Three neural network based classifiers are designed to resolve the conflicts of transition actions, respectively for shift/reduce (dependency parse tree skeletons), tagging (POS tagging), and labeling (dependency label) disambiguations. Experiments show that our approach significantly outperforms previous methods for joint POS tagging and dependency parsing on on three treebanks across eight natural languages.\nar X\niv :1\n70 4.\n07 61\n6v 1\n[ cs\n.C L\n] 2\n5 A\npr 2\n01 7"}, {"heading": "2 Approach", "text": ""}, {"heading": "2.1 Problem Statement", "text": "As shown in Figure 1, given an English sentence \u201cHe won the game\u201d, the corresponding tag sequence is \u201cPRP VBD DT NN\u201d. These tags indicate the part of speech of each word: \u201cHe\u201d is a personal pronoun, \u201cwon\u201d is a verb in past tense, \u201cthe\u201d is a determiner, and \u201cgame\u201d is a noun.\nFigure 1 also shows a dependency tree, which is a collection of dependency arcs. The leftmost arc between the first two words indicates that \u201cwon\u201d is a head word, \u201cHe\u201d is a modifier, and the syntactic label \u201cnsubj\u201d suggests that \u201cHe\u201d is a nominal subject.\nMore formally, given a natural language sentence x = x1, . . . , xN , we denote its corresponding POS tag sequence as t = t1, . . . , tN , where t \u2208 T is a POS tag and T is a set of all possible tags. A dependency tree is denoted by d = {\u3008h,m, l\u3009|0 < h \u2264 N, 0 < m \u2264 N, l \u2208 L}. We use \u3008h,m, l\u3009 to represent a dependency arc, where xh is a head word, xm is a modifier, and l is syntactic label. We use L to denote the set of all possible syntactic labels. The dependency tree in Figure 1 consists of three arcs: \u30082, 1, nsubj\u3009, \u30082, 4, dobj\u3009, and \u30084, 3, det\u3009.\nTherefore, the goal of our work is to generate a tag sequence t and a dependency tree d for a given sentence x."}, {"heading": "2.2 Transition System", "text": "In this work, we leverage a transition-based approach [Nivre, 2008] to joint POS tagging and dependency parsing, which uses classifiers to predict individual actions of shift-reduce algorithms.\nWe define a configuration as a quadruple c = \u3008S,B, T,D\u3009, where\n1. S: a stack that is a disjoint sublist of words,\n2. B: a buffer that is a sublist of words to be processed,\n3. T : a tag sequence that stores the result of POS tagging,\n4. D: a dependency arc set that stores the result of dependency parsing.\nAs shown in Table 1, we define five categories of actions for the transition between configurations: 1\n1While it is possible to integrate two actions into one action (e.g., combining SHIFT and TAGt into SHIFTt) [Bohnet and Nivre, 2012],\n1. SHIFT: move the leftmost word from the buffer B to the stack S ;\n2. LEFT: combine the top two items on the stack, xm and xh, replace them with xh as the head, and add an unlabeled dependency arc \u3008h,m,\u22a5\u3009 to D;\n3. RIGHT: combine the top two items on the stack, xh and xm, replace them with xh as the head, and add an unlabeled dependency arc \u3008h,m,\u22a5\u3009 to D;\n4. TAGt: assign a POS tag t to the last added word if the previous action is SHIFT (i.e., |T | = N \u2212 |B| \u2212 1);\n5. LABELl: assign a syntactic label l to the last generated dependency arc if the previous action is LEFT or RIGHT (i.e., D\u22121.l = \u22a5).\nwhere N is the length of the input sentence. We follow Bohnet and Nivre [2012] to use\u22a5 to denote an undefined syntactic label. D\u22121.l represents the syntactic label of the last added dependency arc. Note that the first three actions are used to determine the skeletons of dependency trees, which can be applied on condition that all words removed from the buffer are tagged (i.e., |T | = N \u2212 |B|), and all generated dependency arcs are labeled (i.e., D\u22121.l 6= \u22a5).\nTable 2 demonstrates the process of joint tagging and dependency parsing for the example in Figure 1. The initial configuration at step 0 is c0 = \u3008\u2205, {x1, x2, x3, x4}, \u2205, \u2205\u3009. In step 1, the action SHIFT moves the leftmost word x1 (i.e., \u201cHe\u201d) from the buffer B to the stack S. Then, the action TAGPRP assigns a POS tag \u201cPRP\u201d to the last shifted word \u201cHe\u201d. In this way, the configuration keeps changing by applying various actions until the terminal configuration (i.e., the stack contains only one item, the buffer is empty, all words are tagged, and all arcs are labeled) is generated."}, {"heading": "2.3 Modeling", "text": "Given a sentence x with N words, tag sequence t and dependency tree d corresponds to a unique sequence of actionconfiguration pairs {\u3008ci, ai\u3009}4N\u22122i=1 , as shown in Table 2 2.\nwe find that separating tag and label actions (i.e., TAGt and LABELl) from structural actions (i.e., SHIFT, LEFT, and RIGHT) leads to significant improvements over using combined actions.\n2We follow Chen and Manning [2014] to map a parse to a unique sequence of action-configuration pairs by using the \u201cshortest stack\u201d strategy.\nNote that the number of SHIFT actions is N , LEFT or RIGHT is N \u2212 1, TAGt is N , and LABELl is N \u2212 1, where SHIFT and TAGt have the same number as words, LEFT/RIGHT and LABEL have the same number as dependency arcs.\nAs a result, the probabilistic model for transition-based joint POS tagging and dependency parsing is defined as\nP (t,d|x;\u03b8) = 4N\u22122\u220f i=1 P (ai|ci\u22121;\u03b8)\u00d7 P (ci|ci\u22121, ai) (1)\nTherefore, we only need to focus on the action probability conditioned on the previous configuration.\nIn our transition system, there are three types of conflicts:\n1. Tag conflict among all possible POS tags {TAGt|t \u2208 T }, 2. Shift/reduce conflict between SHIFT, LEFT, and RIGHT.\nFor example, at step 5 in Table 2, both SHIFT and LEFT can be applied,\n3. Label conflict among all possible syntactic labels {LABELl|l \u2208 L}.\nTo resolve these conflicts, we develop three corresponding neural network based classifiers. Note that the separation of structural actions from tagging and labeling actions results in three small classifiers with fewer classes (i.e., |T | classes for the tag classifier, 3 for the shift/reduce classifier, and |L| for the label classifier) rather than one big classifier with much more classes (i.e., |T |+ 2|L|).\nBasic Features We use xn to denote the vector representation of the n-th word xn. In our experiments, we follow Kiperwasser and Goldberg [2016] to learnxn using bidirectional LSTM whose inputs are concatenations of randomly initialized word embeddings with additional pre-trained embeddings as well as character-based representations [dos Santos and Zadrozny, 2014; Ballesteros et al., 2015]. We use tn to denote the vector representation of the n-th POS tag tn, which can be learned using a unidirectional LSTM based on randomly initialized tag embeddings. Note that the bidirectional LSTM\nfeature representations for words are computed before joint POS tagging and dependency parsing while the unidirectional LSTM feature representations for tags are calculated during the search on the fly.\nTag Classification Resolving the tag conflict is a |T |-class classification problem. Instead of using conventional feature templates that are highly sparse and inevitably incomplete, we leverage a neural network based classifier. To determine the POS tag of the last word added to the stack, which is represented as xS0 , the input layer consists of the following representations:\n1. xS1 : the word representation of the second item in the stack,\n2. tS1 : the tag representation of the second item in the stack,\n3. xB\u22122 : the word representation of the second last item removed from the buffer,\n4. tB\u22122 : the tag representation of the second last item removed from the buffer,\n5. xS0 : the word representation of the first item in the stack,\n6. xB0 : the word representation of the first item in the buffer.\nwhere, xB\u22122 , xS0 , xB0 are window-based features that have been widely adopted in previous work [Huang et al., 2015] and tB\u22122 models the previous tag which has been widely used implicitly by markov assumption in CRF models. Note that xB\u22122 , xS0 , xB0 are sequential words and xS1 is not necessarily identical to xB\u22122 due to the RIGHT action.\nWe expect that these representations can provide useful contextual information for resolving the tagging ambiguity. Note that the tagging classifier is capable of exploiting syntactic information encoded in xS1 and tS1 .\nAs shown in Figure 2(a), the hidden layer is calculated as\nhS0tag = W (1) tag[xS1 ; tS1 ;xB\u22122 ; tB\u22122 ;xS0 ;xB0 ] (2)\nThen, the probability for tagging xS0 as t is computed at the softmax layer:\nPtag(a|c;\u03b8) = softmax ( W (2) tagh S0 tag ) (3)\nwhere a \u2208 {TAGt|t \u2208 T }. Shift/Reduce Classification Resolving the shift/reduce conflict is a 3-class classification problem. As shown in Figure 2(b), we also use a neural classifier, in which the hidden layer is given by: 3\nhparse = W (1) parse[xS2 ; tS2 ;xS1 ; tS1 ;xS0 ; tS0 ;xB0 ] (4)\nwhere S2 denotes the third item in the stack. Note that the shift/reduce classifier is capable of exploiting lexical information encoded in tS2 , tS1 , and tS0 .\nTherefore, the shift/reduce classification probability is computed as\nPparse(a|c;\u03b8) = softmax ( W(2)parsehparse ) (5)\nwhere a \u2208 {SHIFT, LEFT,RIGHT}. Label Classification Resolving the label conflict is a |L|-class classification problem. As shown in Figure 2(c), the corresponding neural classifier takes the word and tag representations of the first two items in the stack as input:\nhlabel = W (1) label[xS1 ; tS1 ;xS0 ; tS0 ] (6)\n3Although it is possible to use hidden states in the tag classifier (e.g., hS0tag) to replace tag representations tS0 as suggested by Zhang and Weiss [2016], we find that it results in degenerate tagging and parsing results as compared with Eq. (4).\nClearly, labeling a dependency arc also depends on tag representations tS1 and tS0 .\nThe label classification probability is computed as Plabel(a|c;\u03b8) = softmax ( W (2) labelhlabel ) (7)\nwhere a \u2208 {LABELl|l \u2208 L}."}, {"heading": "2.4 Training and Parsing", "text": "Given a set of training examples {\u3008x(k), t(k),d(k)\u3009}Kk=1, the training objective is to minimize the cross-entropy loss plus a `2-regularization term:\n\u03b8\u0302 = argmin \u03b8\n{ \u2212 K\u2211 k=1 logP (t(k),d(k)|x(k);\u03b8) + \u03bb 2 ||\u03b8||2 } (8)\nIn parsing, we follow Chen and Manning [2014] to perform greedy decoding. The most probable tag sequence and dependency tree corresponds to a sequence of action-configuration pairs with the highest probability: {\u3008c\u0302i, a\u0302i\u3009}4N\u22122i=1 , where\na\u0302i = argmax a\nP (a|c\u0302i\u22121; \u03b8\u0302) (9)\nand c\u0302i is obtained by applying a\u0302i to c\u0302i\u22121."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Setup", "text": "Datasets and Evaluation We evaluate our approach on three datasets: the English Penn Treebank (PTB) with annotated phrase-structure trees of English, the Chinese Penn Treebank (CTB) version 5.1 with annotated phrase-structure trees of Chinese, and the Universal Dependency Treebank (UD) version 1.2 4 with annotated dependency trees across a number of natural languages.\nWe use the standard splitting method to divide the PTB dataset into training, development and test sections, and convert the phrase-structure trees into dependency trees by the Stanford dependency converter v3.3.0 [de Marneffe et al., 2006]. For the CTB5.1 dataset, we follow previous work [Hatori et al., 2011; Bohnet and Nivre, 2012] to split the dataset into training, development and test sections , and use the Penn2Malt tool with the head-finding rule of [Zhang and Clark, 2008] to convert the phrase-structure trees into dependencies. For the UD dataset, we follow Ammar et al. [2016], using the same subset of seven languages including German (de), English (en), Spanish (es), French (fr), Italian (it), Portuguese (pt) and Swedish (sv) and using the same data splitting method.\nFor POS tagging, we use the standard tagging accuracy (POS) based on words as the major evaluation metric. For dependency parsing, we use two metrics, namely unlabeled attachment score (UAS) and labeled attachment score (LAS), where UAS denotes the ratio of the correctly-headed words with respect to the total words, which considers only the head of a word, and LAS takes into account the dependency label as well, and is emlpoyed as the major metric to evaluate dependency parsing.\n4http://universaldependencies.org\nHyper-parameters and Training Details We tune all hyper-parameters in our models according the development results. Concretely, the dimension sizes of word, tag and character embeddings are 150, 50 and 50, respectively. We use the same pre-trained word embeddings for PTB and CTB5.1 as Chris et al. [2015] 5, and do not use any pre-trained embeddings for UD, and the dimension size of the hidden states in neural classifiers is 300.\nWe exploit the Adam optimizer [Kingma and Ba, 2015] to update model parameters during training, setting the hyperparameters \u03b21 and \u03b22 both to 0.9. Gradient clipping [Pascanu et al., 2013] by a max norm 5.0 is used to avoid gradient exploding. To avoid overfitting, we use `2-regularization by a parameter 10\u22128 as well as the dropout technique [Srivastava et al., 2014] with a drop rate of 0.25. Since the arc-standard algorithm can only handle the projective trees, we apply a projectivization step to the training sets of the UD dataset."}, {"heading": "3.2 Main Results", "text": "Table 3 shows the final results of our models on PTB and CTB5.1. We include the pipeline performances as well. Our joint model brings significant improvements on both POS tagging (POS) and dependency parsing (LAS) compared with the pipeline model (the p-value is below 10\u22125 using pairwise t-test). In addition, we compare our joint model with the baseline parsing model using gold-standard POS tags, which can be treated as the oracle performances of our joint model. Although the joint model gives improved performances over the pipeline model, it still has large spaces to reach the oracle\n5We thank the authors very much for sharing their data with us.\nperformances, which demonstrates the effectiveness of POS tags in dependency parsing.\nWe compare our model with previous work as well. On the one hand, we compare our joint model with previous joint models. As shown in Table 3, our neural joint model shows the highest results for both PTB and CTB5.1, obtaining much higher performances in dependency parsing, which demonstrates the effectiveness of the neural features. On the other hand, we compare our baseline model with state-of-the-art transition-based dependency parsing models. Typically, the PTB results are reported by using auto POS tags and the CTB5.1 results are reported by using gold-standard POS tags, respectively. Our baseline model produces strong enough results for both PTB and CTB5.1.\nTable 4 shows the final results on the UD dataset. Joint models also achieves significantly better results in comparison with the pipeline models (p-value below 10\u22125), which is similar to our finding on CTB5.1. Besides, our joint model achieves the best-reported results among the transition-based models, even by using a greedy manner for decoding, which can be attributed to the effective exploration of the interaction between the tagging and parsing in our joint model, while no previous work has studied it under the neural setting to our knowledge. The work of Zhang and Weiss [2016] resembles our work most, which improve a feed-forward dependency parser by using POS tags in a pipeline way by stackpropagation. While our joint model benefits from the use of LSTM, and in addition, we find that directly using the resulting tags rather than the penultimate hidden representations of a tag classifier leads to better results."}, {"heading": "3.3 Discussion", "text": "To investigate the effect of POS tagging on dependency parsing, we conduct analysis on the CTB5.1 dataset to illustrate the effectiveness of the joint model. Here we examine in detail to see the benefits from the interaction between tagging and parsing in our joint model. First, we can remove the tag representations from parsing in Eq. (4) and Eq. (6):\nh\u0303parse = W\u0303 (1) parse[xS2 ;xS1 ;xS0 ;xB0 ] (10)\nh\u0303label = W\u0303 (1) label[xS1 ;xS0 ] (11)\nSimilarly, we can also remove the syntactic information from tagging in Eq. (2) to investigate the effect of dependency parsing on POS tagging:\nh\u0303S0tag = W\u0303 (1) tag[xB\u22122 ; tB\u22122 ;xS0 ;xB0 ] (12)\nTable 5 gives the tagging and parsing results on the CTB 5.1 development set. We observe that disabling the interactions between tagging and parsing significantly deteriorates both tagging and parsing quality.\nAn interesting finding is that providing lexical information to parsing (\u201ctag \u2192 parse\u201d) leads to more benefits than providing syntactic information to tagging (\u201ctag\u2190 parse\u201d). This is because tagging ambiguity is mostly local while dependency parsing heavily depends on POS tags to predict syntactic structures.\nNote that enabling \u201ctag \u2192 parse\u201d only also improves the tagging accuracy itself. One possible reason is that tagging and parsing is still connected via the sharing of word embeddings and bidirectional LSTM hidden states although the connection at hidden layer in classifiers is explicitly disabled."}, {"heading": "4 Related Work", "text": "Our work is closely related to two lines of research: (1) joint POS tagging and dependency parsing using feature templates, and (2) neural dependency parsing."}, {"heading": "4.1 Joint Modeling with Feature Templates", "text": "Most previous endeavors on joint POS tagging and dependency parsing have focused on developing linear models with feature templates [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012]. They introduce transition systems that can perform POS tagging and dependency parsing in a joint search space.\nOur transition system differs from previous work in the separation of structural, tagging, and labeling actions. This results in three small classifiers with fewer classes (i.e., |T | classes for the tag classifier, 3 for the shift/reduce classifier, and |L| for the label classifier) rather than one big classifier with much more classes (i.e., |T |+ 2|L|).\nMore importantly, we use continuous representations instead of discrete indicator features to build the classifiers. As indicated by Chen and Manning [2014], lexicalized indicator features crucial for improving parsing accuracy are highly sparse and often incomplete. Alternatively, we resort to neural networks to learn representations from data to circumvent the sparsity and incompleteness problems. Another benefit of using neural networks is that there is no need to compose individual features to obtain more complex features like conventional discriminative dependency parsing [Dyer et al., 2015]."}, {"heading": "4.2 Neural POS Tagging and Dependency Parsing", "text": "Our work is also inspired by recent advances in applying neural networks to POS tagging [Huang et al., 2015] and dependency parsing [Chen and Manning, 2014; Dyer et al., 2015; Ballesteros et al., 2015; Alberti et al., 2015; Ammar et al., 2016; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Wang and Chang, 2016; Cheng et al., 2016; Dozat and Manning, 2017].\nAmong them, our work bears the most resemblance to [Zhang and Weiss, 2016], which propose stack-propagation to integrate a tagging model into a neural parser. They propose a stacked pipeline of models and utilize POS tags as a regularizer of learned representations. While Zhang and Weiss [2016] use the hidden layer of the tagger network as the input for the parser, we are interested in enabling tagging and parsing to benefit each other in a joint search space. As a result, the tagger is able to resolve long-distance tagging ambiguity by exploiting syntactic information. Meanwhile, the error propagation problem the parser faces can be alleviated due to the cascaded error reduction by joint modeling."}, {"heading": "5 Conclusion", "text": "We have presented an approach to joint part-of-speech tagging and dependency parsing using transition-based neural networks. Based on a five-action transition system, we develop three classifiers to resolve structural, tagging, and labeling conflicts. As our approach allows lexicality and syntax to interact with each other in the joint search process, it improves over previous work on joint POS tagging and dependency parsing on three treebanks across a variety of natural languages. Our code is released at http://github. com/lineryang/joint-parser."}], "references": [{"title": "Improved transition-based parsing and tagging with neural networks", "author": ["Chris Alberti", "David Weiss", "Greg Coppola", "Slav Petrov"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Alberti et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "one parser", "author": ["Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith. Many languages"], "venue": "TACL,", "citeRegEx": "Ammar et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliasei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": "Proceedings of ACL,", "citeRegEx": "Andor et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah Smith"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Ballesteros et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A transition-based system for joint part-of-speech tagging and labeled non-projective parsing", "author": ["Bernd Bohnet", "Joakim Nivre"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Bohnet and Nivre. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Chen and Manning. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Bi-directional attention with agreement for dependency parsing", "author": ["Hao Cheng", "Hao Fang", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Cheng et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Collins. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine de Marneffe", "Bill MacCartney", "Christopher D. Manning"], "venue": "Proceedings of LREC,", "citeRegEx": "de Marneffe et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "Zadrozny", "Bianca Zadrozny"], "venue": "In Proceedings of ICML,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["Timothy Dozat", "Christopher D. Manning"], "venue": "Proceedings of ICLR,", "citeRegEx": "Dozat and Manning. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Transitionbased depdnency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Mattews", "A. Smith", "Noah"], "venue": "Proceedings of ACL,", "citeRegEx": "Dyer et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Jun\u2019ichi Tsujii", "author": ["Jun Hatori", "Takuya Matsuzaki", "Yusuke Miyao"], "venue": "Incremental joint pos tagging and dependency parsing in chinese. In Proceedings of IJCNLP,", "citeRegEx": "Hatori et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv,", "citeRegEx": "Huang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "Proceedings of ICLR,", "citeRegEx": "Kingma and Ba. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg"], "venue": "TACL,", "citeRegEx": "Kiperwasser and Goldberg. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint models for chinese pos tagging and dependency parsing", "author": ["Zhenghua Li", "Min Zhang", "Wanxiang Che", "Ting Liu", "Wenliang Chen", "Haizhou Li"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Li et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A separately passive-aggressive training algorithm for joint pos tagging and dependency parsing", "author": ["Zhenghua Li", "Min Zhang", "Wanxiang Che", "Ting Liu"], "venue": "Proceedings of COLING,", "citeRegEx": "Li et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira"], "venue": "Proceedings of ACL,", "citeRegEx": "McDonald et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Labeled pseudoprojective dependency parsing with support vector machines", "author": ["Joakim Nivre", "Johan Hall", "Jens Nilsson", "Gulsen Eryigit", "Svetoslav Marinov"], "venue": "Proceedings of CoNLL,", "citeRegEx": "Nivre et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Algorithms for deterministic incremental depdendency parsing", "author": ["Joakim Nivre"], "venue": "Computational Linguistics,", "citeRegEx": "Nivre. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Proceedings of ICML", "citeRegEx": "Pascanu et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhtdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Capturing long-distance dependencies in sequence models: A case study of chinese part-of-speech tagging", "author": ["Weiwei Sun", "Xiaochang Peng", "Xiaojun Wan"], "venue": "Proceedings of IJCNLP,", "citeRegEx": "Sun et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "Proceedings of NAACL,", "citeRegEx": "Toutanova et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Graph-based dependency parsing with bidirectional lstm", "author": ["Wenhui Wang", "Baobao Chang"], "venue": "Proceedings of ACL,", "citeRegEx": "Wang and Chang. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing", "author": ["Yue Zhang", "Stephen Clark"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Zhang and Clark. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Stack-propagation: Improved representation learning for syntax", "author": ["Yuan Zhang", "David Weiss"], "venue": "Proceedings of ACL,", "citeRegEx": "Zhang and Weiss. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Stacking heterogeneous joint models of Chinese POS tagging and dependency parsing", "author": ["Meishan Zhang", "Wanxiang Che", "Ting Liu", "Zhenghua Li"], "venue": "Proceedings of COLING,", "citeRegEx": "Zhang et al.. 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "Part-of-speech (POS) tagging [Collins, 2002; Toutanova et al., 2003; dos Santos and Zadrozny, 2014; Huang et al., 2015] and dependency parsing [McDonald et al.", "startOffset": 29, "endOffset": 119}, {"referenceID": 24, "context": "Part-of-speech (POS) tagging [Collins, 2002; Toutanova et al., 2003; dos Santos and Zadrozny, 2014; Huang et al., 2015] and dependency parsing [McDonald et al.", "startOffset": 29, "endOffset": 119}, {"referenceID": 13, "context": "Part-of-speech (POS) tagging [Collins, 2002; Toutanova et al., 2003; dos Santos and Zadrozny, 2014; Huang et al., 2015] and dependency parsing [McDonald et al.", "startOffset": 29, "endOffset": 119}, {"referenceID": 18, "context": ", 2015] and dependency parsing [McDonald et al., 2005; Nivre et al., 2006; Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016] are two fundamental tasks for understanding natural languages.", "startOffset": 31, "endOffset": 149}, {"referenceID": 19, "context": ", 2015] and dependency parsing [McDonald et al., 2005; Nivre et al., 2006; Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016] are two fundamental tasks for understanding natural languages.", "startOffset": 31, "endOffset": 149}, {"referenceID": 5, "context": ", 2015] and dependency parsing [McDonald et al., 2005; Nivre et al., 2006; Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016] are two fundamental tasks for understanding natural languages.", "startOffset": 31, "endOffset": 149}, {"referenceID": 11, "context": ", 2015] and dependency parsing [McDonald et al., 2005; Nivre et al., 2006; Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016] are two fundamental tasks for understanding natural languages.", "startOffset": 31, "endOffset": 149}, {"referenceID": 15, "context": ", 2015] and dependency parsing [McDonald et al., 2005; Nivre et al., 2006; Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016] are two fundamental tasks for understanding natural languages.", "startOffset": 31, "endOffset": 149}, {"referenceID": 23, "context": "On one hand, POS tagging often requires long-distance syntactic information for resolving tagging ambiguity [Sun et al., 2013].", "startOffset": 108, "endOffset": 126}, {"referenceID": 16, "context": "work has focused on jointly modeling POS tagging and dependency parsing using linear models that combine both tagging and parsing features [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Li et al., 2012; Zhang et al., 2012].", "startOffset": 139, "endOffset": 238}, {"referenceID": 12, "context": "work has focused on jointly modeling POS tagging and dependency parsing using linear models that combine both tagging and parsing features [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Li et al., 2012; Zhang et al., 2012].", "startOffset": 139, "endOffset": 238}, {"referenceID": 4, "context": "work has focused on jointly modeling POS tagging and dependency parsing using linear models that combine both tagging and parsing features [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Li et al., 2012; Zhang et al., 2012].", "startOffset": 139, "endOffset": 238}, {"referenceID": 17, "context": "work has focused on jointly modeling POS tagging and dependency parsing using linear models that combine both tagging and parsing features [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Li et al., 2012; Zhang et al., 2012].", "startOffset": 139, "endOffset": 238}, {"referenceID": 28, "context": "work has focused on jointly modeling POS tagging and dependency parsing using linear models that combine both tagging and parsing features [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Li et al., 2012; Zhang et al., 2012].", "startOffset": 139, "endOffset": 238}, {"referenceID": 16, "context": "Allowing lexicality and syntax to interact in a unified framework, joint POS tagging and dependency parsing improves both tagging and parsing performance over independent modeling significantly [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012].", "startOffset": 194, "endOffset": 256}, {"referenceID": 12, "context": "Allowing lexicality and syntax to interact in a unified framework, joint POS tagging and dependency parsing improves both tagging and parsing performance over independent modeling significantly [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012].", "startOffset": 194, "endOffset": 256}, {"referenceID": 4, "context": "Allowing lexicality and syntax to interact in a unified framework, joint POS tagging and dependency parsing improves both tagging and parsing performance over independent modeling significantly [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012].", "startOffset": 194, "endOffset": 256}, {"referenceID": 16, "context": "The situation in joint POS tagging and dependency parsing is much severer because tagging and parsing features are concatenated in joint models [Li et al., 2011].", "startOffset": 144, "endOffset": 161}, {"referenceID": 5, "context": "As a result, the incompleteness of feature design is considered as an unavoidable issue in conventional discriminative models [Chen and Manning, 2014].", "startOffset": 126, "endOffset": 150}, {"referenceID": 20, "context": "2 Transition System In this work, we leverage a transition-based approach [Nivre, 2008] to joint POS tagging and dependency parsing, which uses classifiers to predict individual actions of shift-reduce algorithms.", "startOffset": 74, "endOffset": 87}, {"referenceID": 4, "context": ", combining SHIFT and TAGt into SHIFTt) [Bohnet and Nivre, 2012], 1.", "startOffset": 40, "endOffset": 64}, {"referenceID": 3, "context": "In our experiments, we follow Kiperwasser and Goldberg [2016] to learnxn using bidirectional LSTM whose inputs are concatenations of randomly initialized word embeddings with additional pre-trained embeddings as well as character-based representations [dos Santos and Zadrozny, 2014; Ballesteros et al., 2015].", "startOffset": 252, "endOffset": 309}, {"referenceID": 13, "context": "where, xB\u22122 , xS0 , xB0 are window-based features that have been widely adopted in previous work [Huang et al., 2015] and tB\u22122 models the previous tag which has been widely used implicitly by markov assumption in CRF models.", "startOffset": 97, "endOffset": 117}, {"referenceID": 8, "context": "0 [de Marneffe et al., 2006].", "startOffset": 2, "endOffset": 28}, {"referenceID": 12, "context": "1 dataset, we follow previous work [Hatori et al., 2011; Bohnet and Nivre, 2012] to split the dataset into training, development and test sections , and use the Penn2Malt tool with the head-finding rule of [Zhang and Clark, 2008] to convert the phrase-structure trees into dependencies.", "startOffset": 35, "endOffset": 80}, {"referenceID": 4, "context": "1 dataset, we follow previous work [Hatori et al., 2011; Bohnet and Nivre, 2012] to split the dataset into training, development and test sections , and use the Penn2Malt tool with the head-finding rule of [Zhang and Clark, 2008] to convert the phrase-structure trees into dependencies.", "startOffset": 35, "endOffset": 80}, {"referenceID": 26, "context": ", 2011; Bohnet and Nivre, 2012] to split the dataset into training, development and test sections , and use the Penn2Malt tool with the head-finding rule of [Zhang and Clark, 2008] to convert the phrase-structure trees into dependencies.", "startOffset": 157, "endOffset": 180}, {"referenceID": 14, "context": "We exploit the Adam optimizer [Kingma and Ba, 2015] to update model parameters during training, setting the hyperparameters \u03b21 and \u03b22 both to 0.", "startOffset": 30, "endOffset": 51}, {"referenceID": 21, "context": "Gradient clipping [Pascanu et al., 2013] by a max norm 5.", "startOffset": 18, "endOffset": 40}, {"referenceID": 22, "context": "To avoid overfitting, we use `2-regularization by a parameter 10\u22128 as well as the dropout technique [Srivastava et al., 2014] with a drop rate of 0.", "startOffset": 100, "endOffset": 125}, {"referenceID": 16, "context": "Most previous endeavors on joint POS tagging and dependency parsing have focused on developing linear models with feature templates [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012].", "startOffset": 132, "endOffset": 194}, {"referenceID": 12, "context": "Most previous endeavors on joint POS tagging and dependency parsing have focused on developing linear models with feature templates [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012].", "startOffset": 132, "endOffset": 194}, {"referenceID": 4, "context": "Most previous endeavors on joint POS tagging and dependency parsing have focused on developing linear models with feature templates [Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012].", "startOffset": 132, "endOffset": 194}, {"referenceID": 11, "context": "Another benefit of using neural networks is that there is no need to compose individual features to obtain more complex features like conventional discriminative dependency parsing [Dyer et al., 2015].", "startOffset": 181, "endOffset": 200}, {"referenceID": 13, "context": "Our work is also inspired by recent advances in applying neural networks to POS tagging [Huang et al., 2015] and dependency parsing [Chen and Manning, 2014; Dyer et al.", "startOffset": 88, "endOffset": 108}, {"referenceID": 5, "context": ", 2015] and dependency parsing [Chen and Manning, 2014; Dyer et al., 2015; Ballesteros et al., 2015; Alberti et al., 2015; Ammar et al., 2016; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Wang and Chang, 2016; Cheng et al., 2016; Dozat and Manning, 2017].", "startOffset": 31, "endOffset": 261}, {"referenceID": 11, "context": ", 2015] and dependency parsing [Chen and Manning, 2014; Dyer et al., 2015; Ballesteros et al., 2015; Alberti et al., 2015; Ammar et al., 2016; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Wang and Chang, 2016; Cheng et al., 2016; Dozat and Manning, 2017].", "startOffset": 31, "endOffset": 261}, {"referenceID": 3, "context": ", 2015] and dependency parsing [Chen and Manning, 2014; Dyer et al., 2015; Ballesteros et al., 2015; Alberti et al., 2015; Ammar et al., 2016; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Wang and Chang, 2016; Cheng et al., 2016; Dozat and Manning, 2017].", "startOffset": 31, "endOffset": 261}, {"referenceID": 0, "context": ", 2015] and dependency parsing [Chen and Manning, 2014; Dyer et al., 2015; Ballesteros et al., 2015; Alberti et al., 2015; Ammar et al., 2016; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Wang and Chang, 2016; Cheng et al., 2016; Dozat and Manning, 2017].", "startOffset": 31, "endOffset": 261}, {"referenceID": 1, "context": ", 2015] and dependency parsing [Chen and Manning, 2014; Dyer et al., 2015; Ballesteros et al., 2015; Alberti et al., 2015; Ammar et al., 2016; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Wang and Chang, 2016; Cheng et al., 2016; Dozat and Manning, 2017].", "startOffset": 31, "endOffset": 261}, {"referenceID": 15, "context": ", 2015] and dependency parsing [Chen and Manning, 2014; Dyer et al., 2015; Ballesteros et al., 2015; Alberti et al., 2015; Ammar et al., 2016; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Wang and Chang, 2016; Cheng et al., 2016; Dozat and Manning, 2017].", "startOffset": 31, "endOffset": 261}, {"referenceID": 2, "context": ", 2015] and dependency parsing [Chen and Manning, 2014; Dyer et al., 2015; Ballesteros et al., 2015; Alberti et al., 2015; Ammar et al., 2016; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Wang and Chang, 2016; Cheng et al., 2016; Dozat and Manning, 2017].", "startOffset": 31, "endOffset": 261}, {"referenceID": 25, "context": ", 2015] and dependency parsing [Chen and Manning, 2014; Dyer et al., 2015; Ballesteros et al., 2015; Alberti et al., 2015; Ammar et al., 2016; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Wang and Chang, 2016; Cheng et al., 2016; Dozat and Manning, 2017].", "startOffset": 31, "endOffset": 261}, {"referenceID": 6, "context": ", 2015] and dependency parsing [Chen and Manning, 2014; Dyer et al., 2015; Ballesteros et al., 2015; Alberti et al., 2015; Ammar et al., 2016; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Wang and Chang, 2016; Cheng et al., 2016; Dozat and Manning, 2017].", "startOffset": 31, "endOffset": 261}, {"referenceID": 10, "context": ", 2015] and dependency parsing [Chen and Manning, 2014; Dyer et al., 2015; Ballesteros et al., 2015; Alberti et al., 2015; Ammar et al., 2016; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Wang and Chang, 2016; Cheng et al., 2016; Dozat and Manning, 2017].", "startOffset": 31, "endOffset": 261}, {"referenceID": 27, "context": "Among them, our work bears the most resemblance to [Zhang and Weiss, 2016], which propose stack-propagation to integrate a tagging model into a neural parser.", "startOffset": 51, "endOffset": 74}], "year": 2017, "abstractText": "While part-of-speech (POS) tagging and dependency parsing are observed to be closely related, existing work on joint modeling with manually crafted feature templates suffers from the feature sparsity and incompleteness problems. In this paper, we propose an approach to joint POS tagging and dependency parsing using transitionbased neural networks. Three neural network based classifiers are designed to resolve shift/reduce, tagging, and labeling conflicts. Experiments show that our approach significantly outperforms previous methods for joint POS tagging and dependency parsing across a variety of natural languages.", "creator": "LaTeX with hyperref package"}}}