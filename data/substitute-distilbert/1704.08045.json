{"id": "1704.08045", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "The Loss Surface of Deep and Wide Neural Networks", "abstract": "while the optimization problem developing deep neural networks is highly non - convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. assumption has been argued that this is the case as all local minima are preferred to being normally optimal. we show that this believes ( almost ) true, in fact almost all local minima are globally optimal, for a fully connected graphs with squared loss and analytic activation techniques given that the number of n units of one layer composing the network is larger than his number of training points hiding the network structure from this layer on corresponding pyramidal.", "histories": [["v1", "Wed, 26 Apr 2017 10:24:54 GMT  (68kb)", "https://arxiv.org/abs/1704.08045v1", "14 pages"], ["v2", "Mon, 12 Jun 2017 19:43:39 GMT  (82kb)", "http://arxiv.org/abs/1704.08045v2", "ICML 2017. Main results now hold for larger classes of loss functions"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE stat.ML", "authors": ["quynh nguyen", "matthias hein"], "accepted": true, "id": "1704.08045"}, "pdf": {"name": "1704.08045.pdf", "metadata": {"source": "CRF", "title": "The Loss Surface of Deep and Wide Neural Networks", "authors": ["Quynh Nguyen", "Matthias Hein"], "emails": ["<quynh@cs.uni-saarland.de>."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n08 04\n5v 2\n[ cs\n.L G\n] 1\n2 Ju\nn 20\n17\nneural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal."}, {"heading": "1. Introduction", "text": "The application of deep learning (LeCun et al., 2015) has in recent years lead to a dramatic boost in performance in many areas such as computer vision, speech recognition or natural language processing. Despite this huge empirical success, the theoretical understanding of deep learning is still limited. In this paper we address the non-convex optimization problem of training a feedforward neural network. This problem turns out to be very difficult as there can be exponentially many distinct local minima (Auer et al., 1996; Safran & Shamir, 2016). It has been shown that the training of a network with a single neuron with a variety of activation functions turns out to be NP-hard (Sima, 2002).\nIn practice local search techniques like stochastic gradient descent or variants are used for training deep neural networks. Surprisingly, it has been observed (Dauphin et al., 2014; Goodfellow et al., 2015) that in the training of stateof-the-art feedforward neural networks with sparse connectivity like convolutional neural networks (LeCun et al., 1990; Krizhevsky et al., 2012) or fully connected ones\n1Department of Mathematics and Computer Science, Saarland University, Germany. Correspondence to: Quynh Nguyen <quynh@cs.uni-saarland.de>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\none does not encounter problems with suboptimal local minima. However, as the authors admit themselves in (Goodfellow et al., 2015), the reason for this might be that there is a connection between the fact that these networks have good performance and that they are easy to train.\nOn the theoretical side there have been several interesting developments recently, see e.g. (Brutzkus & Globerson, 2017; Lee et al., 2016; Poggio & Liao, 2017; Rister & Rubin, 2017; Soudry & Hoffer, 2017; Zhou & Feng, 2017). For some class of networks one can show that one can train them globally optimal efficiently. However, it turns out that these approaches are either not practical (Janzamin et al., 2016; Haeffele & Vidal, 2015; Soltanolkotabi, 2017) as they require e.g. knowledge about the data generating measure, or they modify the neural network structure and objective (Gautier et al., 2016). One class of networks which are simpler to analyze are deep linear networks for which it has been shown that every local minimum is a global minimum (Baldi & Hornik, 1988; Kawaguchi, 2016). While this is a highly non-trivial result as the optimization problem is non-convex, deep linear networks are not interesting in practice as one efficiently just learns a linear function. In order to characterize the loss surface for general networks, an interesting approach has been taken by (Choromanska et al., 2015a). By randomizing the nonlinear part of a feedforward network with ReLU activation function and making some additional simplifying assumptions, they can relate it to a certain spin glass model which one can analyze. In this model the objective of local minima is close to the global optimum and the number of bad local minima decreases quickly with the distance to the global optimum. This is a very interesting result but is based on a number of unrealistic assumptions (Choromanska et al., 2015b). It has recently been shown (Kawaguchi, 2016) that if some of these assumptions are dropped one basically recovers the result of the linear case, but the model is still unrealistic.\nIn this paper we analyze the case of overspecified neural networks, that is the network is larger than what is required to achieve minimum training error. Under overspecification (Safran & Shamir, 2016) have recently analyzed under which conditions it is possible to generate an initialization so that it is in principle possible to reach the global optimum with descent methods. However, they can only deal\nwith one hidden layer networks and have to make strong assumptions on the data such as linear independence or cluster structure. In this paper overspecification means that there exists a very wide layer, where the number of hidden units is larger than the number of training points. For this case, we can show that a large class of local minima is globally optimal. In fact, we will argue that almost every critical point is globally optimal. Our results generalize previous work of (Yu & Chen, 1995), who have analyzed a similar setting for one hidden layer networks, to networks of arbitrary depth. Moreover, it extends results of (Gori & Tesi, 1992; Frasconi et al., 1997) who have shown that for certain deep feedforward neural networks almost all local minima are globally optimal whenever the training data is linearly independent. While it is clear that our assumption on the number of hidden units is quite strong, there are several recent neural network structures which contain a quite wide hidden layer relative to the number of training points e.g. in (Lin et al., 2016) they have 50,000 training samples and the network has one hidden layer with 10,000 hidden units and (Ba & Caruana, 2014) have 1.1 million training samples and a layer with 400,000 hidden units. We refer to (Ciresan et al., 2010; Neyshabur et al., 2015; Vincent et al., 2010; Caruana et al., 2001) for other examples where the number of hidden units of one layer is on the order of the number of training samples. We conjecture that for these kind of wide networks it still holds that almost all local minima are globally optimal. The reason is that one can expect linear separability of the training data in the wide layer. We provide supporting evidence for this conjecture by showing that basically every critical point for which the training data is linearly separable in the wide layer is globally optimal. Moreover, we want to emphasize that all of our results hold for neural networks used in practice. There are no simplifying assumptions as in previous work."}, {"heading": "2. Feedforward Neural Networks and Backpropagation", "text": "We are mainly concerned with multi-class problems but our results also apply to multivariate regression problems. Let N be the number of training samples and denote by X = [x1, . . . , xN ] T \u2208 RN\u00d7d, Y = [y1, . . . , yN ] T \u2208 R N\u00d7m the input resp. output matrix for the training data (xi, yi) N i=1, where d is the input dimension and m the number of classes. We consider fully-connected feedforward networks with L layers, indexed from 0, 1, 2, . . . , L, which correspond to the input layer, 1st hidden layer, etc, and output layer. The network structure is determined by the weight matrices (Wk) L k=1 \u2208 W := R\nd\u00d7n1 \u00d7 . . . \u00d7 R\nnk\u22121\u00d7nk\u00d7. . .\u00d7RnL\u22121\u00d7m; where nk is the number of hidden units of layer k (for consistency, we set n0 = d, nL = m), and the bias vectors (bk) L k=1 \u2208 B := R\nn1 \u00d7 . . .\u00d7RnL . We denote by P = W \u00d7 B the space of all possible pa-\nrameters of the network. In this paper, [a] denotes the set of integers {1, 2, . . . , a} and [a, b] the set of integers from a to b. The activation function \u03c3 : R \u2192 R is assumed at least to be continuously differentiable, that is \u03c3 \u2208 C1(R). In this paper, we assume that all the functions are applied componentwise. Let fk, gk : R d \u2192 Rnk be the mappings from the input space to the feature space at layer k, which are defined as\nf0(x) = x, fk(x) = \u03c3(gk(x)), gk(x) = W T k fk\u22121(x) + bk\nfor every k \u2208 [L], x \u2208 Rd. In the following, let Fk = [fk(x1), fk(x2), . . . , fk(xN )]\nT \u2208 RN\u00d7nk and Gk = [gk(x1), gk(x2), . . . , gk(xN )]\nT \u2208 RN\u00d7nk be the matrices that store the feature vectors of layer k after and before applying the activation function. One can easily check that\nF1 = \u03c3(XW1 + 1Nb T 1 ), Fk = \u03c3(Fk\u22121Wk + 1Nb T k ), for k \u2208 [2, L].\nIn this paper we analyze the behavior of the loss of the network without any form of regularization, that is the final objective \u03a6 : P \u2192 R of the network is defined as\n\u03a6 (\n(Wk, bk) L k=1\n)\n=\nN\u2211\ni=1\nm\u2211\nj=1\nl(fLj(xi)\u2212 yij) (1)\nwhere l : R \u2192 R is assumed to be a continuously differentiable loss function, that is l \u2208 C1(R). The prototype loss which we consider in this paper is the squared loss, l(\u03b1) = \u03b12, which is one of the standard loss functions in the neural network literature. We assume throughout this paper that the minimum of (1) is attained.\nThe idea of backpropagation is the core of our theoretical analysis. Lemma 2.1 below shows well-known relations for feed-forward neural networks, which are used throughout the paper. The derivative of the loss w.r.t. the value of unit j at layer k evaluated at a single training sample xi is denoted as \u03b4kj(xi) =\n\u2202\u03a6 \u2202gkj(xi) . We arrange these vectors\nfor all training samples into a single matrix\u2206k, defined as\n\u2206k = [\u03b4k:(x1), . . . , \u03b4k:(xN )] T \u2208 RN\u00d7nk .\nIn the following we use the Hadamard product \u25e6, which for A,B \u2208 Rm\u00d7n is defined as A \u25e6 B \u2208 Rm\u00d7n with (A \u25e6 B)ij = AijBij .\nLemma 2.1 Let \u03c3, l \u2208 C1(R). Then it holds\n1. \u2206k =\n{\nl\u2032(FL \u2212 Y ) \u25e6 \u03c3 \u2032(GL), k = L (\u2206k+1W T k+1) \u25e6 \u03c3 \u2032(Gk), k \u2208 [L\u2212 1]\n2. \u2207Wk\u03a6 =\n{\nXT\u22061, k = 1 FTk\u22121\u2206k, k \u2208 [2, L]\n3. \u2207bk\u03a6 = \u2206 T k 1N \u2200 k \u2208 [L]\nProof:\n1. By definition, it holds for every i \u2208 [N ], j \u2208 [nL] that\n(\u2206L)ij = \u03b4Lj(xi)\n= \u2202\u03a6\n\u2202gLj(xi)\n= l\u2032(fLj(xi)\u2212 yij)\u03c3 \u2032(gLj(xi)) = l\u2032((FL)ij \u2212 Yij)\u03c3 \u2032((GL)ij)\nand hence,\u2206L = l \u2032(FL \u2212 Y ) \u25e6 \u03c3 \u2032(GL).\nFor every k \u2208 [L \u2212 1], the chain rule yields for every i \u2208 [N ], j \u2208 [nk] that\n(\u2206k)ij = \u03b4kj(xi)\n= \u2202\u03a6\n\u2202gkj(xi)\n=\nnk+1\u2211\nl=1\n\u2202\u03a6\n\u2202g(k+1)l(xi)\n\u2202g(k+1)l(xi)\n\u2202gkj(xi)\n=\nnk+1\u2211\nl=1\n\u03b4(k+1)l(xi)(Wk+1)jl\u03c3 \u2032(gkj(xi))\n=\nnk+1\u2211\nl=1\n(\u2206(k+1))il(Wk+1) T lj\u03c3 \u2032((Gk)ij)\nand hence\u2206k = (\u2206k+1W T k+1) \u25e6 \u03c3 \u2032(Gk).\n2. For every r \u2208 [d], s \u2208 [n1] it holds\n\u2202\u03a6\n\u2202(W1)rs =\nN\u2211\ni=1\n\u2202\u03a6\n\u2202g1s(xi)\n\u2202g1s(xi) \u2202(W1)rs\n=\nN\u2211\ni=1\n\u03b41s(xi)xir =\nN\u2211\ni=1\n(XT )ri(\u22061)is\n= ( XT\u22061 )\nrs\nand hence\u2207W1\u03a6 = X T\u22061.\nFor every k \u2208 [2, L], r \u2208 [nk\u22121], s \u2208 [nk], one obtains\n\u2202\u03a6\n\u2202(Wk)rs =\nN\u2211\ni=1\n\u2202\u03a6\n\u2202gks(xi)\n\u2202gks(xi) \u2202(Wk)rs\n=\nN\u2211\ni=1\n\u03b4ks(xi)f(k\u22121)r(xi) =\nN\u2211\ni=1\n(FTk\u22121)ri(\u2206k)is\n= ( FTk\u22121\u2206k )\nrs\nand hence\u2207Wk\u03a6 = F T k\u22121\u2206k.\n3. For every k \u2208 [1, L], s \u2208 [nk] it holds\n\u2202\u03a6\n\u2202(bk)s =\nN\u2211\ni=1\n\u2202\u03a6\n\u2202gks(xi)\n\u2202gks(xi)\n\u2202(bk)s\n=\nN\u2211\ni=1\n\u03b4ks(xi) = ( \u2206Tk 1N )\ns\nand hence\u2207bk\u03a6 = \u2206 T k 1N .\n\u2737\nNote that Lemma 2.1 does not apply to non-differentiable activation functions like the ReLU function, \u03c3ReLU(x) = max{0, x}. However, it is known that one can approximate this activation function arbitrarily well by a smooth function e.g. \u03c3\u03b1(x) = 1 \u03b1 log(1 + e\n\u03b1x) (a.k.a. softplus) satisfies lim\u03b1\u2192\u221e \u03c3\u03b1(x) = \u03c3ReLU(x) for any x \u2208 R."}, {"heading": "3. Main Result", "text": "We first discuss some prior work and present then our main result together with extensive discussion. For improved readability we postpone the proof of the main result to the next section which contains several intermediate results which are of independent interest."}, {"heading": "3.1. Previous Work", "text": "Our work can be seen as a generalization of the work of (Gori & Tesi, 1992; Yu & Chen, 1995). While (Yu & Chen, 1995) has shown that for a one-hidden layer network, that if n1 = N \u2212 1, then every local minimum is a global minimum, the work of (Gori & Tesi, 1992) considered also multi-layer networks. For the convenience of the reader, we first restate Theorem 1 of (Gori & Tesi, 1992) using our previously introduced notation. The critical points of a continuously differentiable function f : R\nd \u2192 R are the points where the gradient vanishes, that is \u2207f(x) = 0. Note that this is a necessary condition for a local minimum.\nTheorem 3.1 (Gori & Tesi, 1992) Let \u03a6 : P \u2192 R be defined as in (1) with least squares loss l(a) = a2. Assume \u03c3 : R \u2192 [d, d\u0304] to be continuously differentiable with strictly positive derivative and\nlim a\u2192\u221e\n\u03c3\u2032(a)\nd\u0304\u2212\u03c3(a) > 0, lima\u2192\u221e\n\u2212\u03c3\u2032\u2032(a) d\u0304\u2212\u03c3(a) > 0\nlima\u2192\u2212\u221e \u03c3\u2032(a)\n\u03c3(a)\u2212d > 0, lima\u2192\u2212\u221e\n\u03c3\u2032\u2032(a)\n\u03c3(a)\u2212d > 0\nThen every critical point (Wl, bl) L l=1 of \u03a6 which satisfies the conditions\n1. rank(Wl) = nl for all l \u2208 [2, L],\n2. [X,1N ] T\u22061 = 0 implies\u22061 = 0\nis a global minimum.\nWhile this result is already for general multi-layer networks, the condition \u201c[X,1N ] T\u22061 = 0 implies\u22061 = 0\u201d is the main caveat. It is already noted in (Gori & Tesi, 1992), that \u201cit is quite hard to understand its practical meaning\u201d as it requires prior knowledge of\u22061 at every critical point. Note that this is almost impossible as \u22061 depends on all the weights of the network. For a particular case, when the training samples (biases added) are linearly independent, i.e. rank([X,1N ]) = N , the condition holds automatically. This case is discussed in the following Theorem 3.4, where we consider a more general class of loss and activation functions."}, {"heading": "3.2. First Main Result and Discussion", "text": "A function f : Rd \u2192 R is real analytic if the corresponding multivariate Taylor series converges to f(x) on an open subset of Rd (Krantz & Parks, 2002). All results in this section are proven under the following assumptions on the loss/activation function and training data.\nAssumptions 3.2 1. There are no identical training\nsamples, i.e. xi 6= xj for all i 6= j,\n2. \u03c3 is analytic on R, strictly monotonically increasing and\n(a) \u03c3 is bounded or\n(b) there are positive \u03c11, \u03c12, \u03c13, \u03c14, s.t. |\u03c3(t)| \u2264 \u03c11e \u03c12t for t < 0 and |\u03c3(t)| \u2264 \u03c13t+ \u03c14 for t \u2265 0\n3. l \u2208 C2(R) and if l\u2032(a) = 0 then a is a global minimum\nThese conditions are not always necessary to prove some of the intermediate results presented below, but we decided to provide the proof under the above strong assumptions for better readability. For instance, all of our results also hold for strictly monotonically decreasing activation functions. Note that the above conditions are not restrictive as many standard activation functions satisfy them.\nLemma 3.3 The sigmoid activation function \u03c31(t) = 1 1+e\u2212t , the tangent hyperbolic \u03c32(t) = tanh(t) and the softplus function \u03c33(t) = 1 \u03b1 log(1 + e\n\u03b1t) for \u03b1 > 0 satisfy Assumption 3.2.\nProof: Note that \u03c32(t) = 2\n1+e\u22122t \u2212 1. Moreover, it is\nwell known that \u03c6(t) = 11+t is real-analytic on R+ = {t \u2208 R | t \u2265 0}. The exponential function is analytic with values in (0,\u221e). As composition of real-analytic function is realanalytic (see Prop 1.4.2 in (Krantz & Parks, 2002)), we get that \u03c31 and \u03c32 are real-analytic. Similarly, since log(1 + t)\nis real-analytic on (\u22121,\u221e) and the composition with the exponential function is real-analytic, we get that \u03c33 is a real-analytic function.\nFinally, we note that \u03c31,\u03c32, \u03c33 are strictly monotonically increasing. Since \u03c31,\u03c32 are bounded, they both satisfy Assumption 3.2. For \u03c33, we note that 1 + e\n\u03b1t \u2264 2e\u03b1t for t \u2265 0, and thus it holds for every t \u2265 0 that\n0 \u2264 \u03c33(t) = 1\n\u03b1 log(1 + e\u03b1t)\n\u2264 1\n\u03b1 log(2e\u03b1t)\n= log(2)\n\u03b1 + t,\nand with log(1+x) \u2264 x for x > \u22121 it holds log(1+e\u03b1t) \u2264 e\u03b1t for every t \u2208 R. In particular\n0 \u2264 \u03c33(t) \u2264 e\u03b1t\n\u03b1 \u2200t < 0\nwhich implies that \u03c33 satisfies Assumption 3.2 for \u03c11 = 1/\u03b1, \u03c12 = \u03b1, \u03c13 = 1, \u03c14 = log(2)/\u03b1. \u2737\nThe conditions on l are satisfied for any twice continuously differentiable convex loss function. A typical example is the squared loss l(a) = a2 or the PseudoHuber loss (Hartley & Zisserman, 2004) given as l\u03b4(a) = 2\u03b42( \u221a\n1 + a2/\u03b42 \u2212 1) which approximates a2 for small a and is linear with slope 2\u03b4 for large a. But also non-convex loss functions satisfy this requirement, for instance:\n1. Blake-Zisserman: l(a) = \u2212 log(exp(\u2212a2) + \u03b4) for \u03b4 > 0. For small a, this curve approximates a2, whereas for large a the asymptotic value is \u2212 log(\u03b4).\n2. Corrupted-Gaussian:\nl(a) = \u2212 log ( \u03b1 exp(\u2212a2)+(1\u2212\u03b1) exp(\u2212a2/w2)/w )\nfor \u03b1 \u2208 [0, 1], w > 0. This function computes the negative log-likehood of a gaussian mixture model.\n3. Cauchy: l(a) = \u03b42 log(1 + a2/\u03b42) for \u03b4 6= 0. This curve approximates a2 for small a and the value of \u03b4 determines for what range of a this approximation is close.\nWe refer to (Hartley & Zisserman, 2004) (p.617-p.619) for more examples and discussion on robust loss functions.\nAs a motivation for our main result, we first analyze the case when the training samples are linearly independent, which requiresN \u2264 d+1. It can be seen as a generalization of Corollary 1 in (Gori & Tesi, 1992).\nTheorem 3.4 Let \u03a6 : P \u2192 R be defined as in (1) and let the Assumptions 3.2 hold. If the training samples are linearly independent, that is rank([X,1N ]) = N , then every critical point (Wl, bl) L l=1 of \u03a6 for which the weight matrices (Wl) L l=2 have full column rank, that is rank(Wl) = nl for l \u2208 [2, L], is a global minimum.\nProof: The proof is based on induction. At a critical point it holds \u2207W1\u03a6 = X T\u22061 = 0 and \u2207b1\u03a6 = \u2206 T 1 1N = 0 thus [X,1N ] T\u22061 = 0. By assumption, the data matrix [X,1N ] T \u2208 R(d+1)\u00d7N has full column rank, this implies \u22061 = 0. Using induction, let us assume that \u2206k = 0 for some 1 \u2264 k \u2264 L \u2212 1, then by Lemma 2.1, we have \u2206k = (\u2206k+1W T k+1)\u25e6\u03c3 \u2032(Gk) = 0. As by assumption \u03c3 \u2032 is strictly positive, this is equivalent to \u2206k+1W T k+1 = 0 resp. Wk+1\u2206 T k+1 = 0. As by assumptionWk+1 has full column rank, it follows \u2206k+1 = 0. Finally, we get \u2206L = 0. With Lemma 2.1 we thus get l\u2032(FL \u2212 Y ) \u25e6 \u03c3\n\u2032(GL) = 0 which implies with the same argument as above l\u2032(FL \u2212 Y ) = 0. From our Assumption 3.2, it holds that if l\u2032(a) = 0 then a is a global minimum of l. Thus each individual entry of (FL \u2212 Y ) must represent a global minimum of l. This combined with (1) implies that the critical point must be a global minimum of \u03a6. \u2737\nTheorem 3.4 implies that the weight matrices of potential saddle points or suboptimal local minima need to have low rank for one particular layer. Note however that the set of low rank weight matrices in W has measure zero. At the moment we cannot prove that suboptimal low rank local minima cannot exist. However, it seems implausible that such suboptimal low rank local minima exist as every neighborhood of such points contains full rank matrices which increase the expressiveness of the network. Thus it should be possible to use this degree of freedom to further reduce the loss, which contradicts the definition of a local minimum. Thus we conjecture that all local minima are indeed globally optimal.\nThe main restriction in the assumptions of Theorem 3.4 is the linear independence of the training samples as it requires N \u2264 d + 1, which is very restrictive in practice.\nWe prove in this section a similar guarantee in our main Theorem 3.8 by implicitly transporting this condition to some higher layer. A similar guarantee has been proven by (Yu & Chen, 1995) for a single hidden layer network, whereas we consider general multi-layer networks. The main ingredient of the proof of our main result is the observation in the following lemma.\nLemma 3.5 Let \u03a6 : P \u2192 R be defined as in (1) and let the Assumptions 3.2 hold. Let (Wl, bl) L l=1 \u2208 P be given. Assume there is some k \u2208 [L\u2212 1] s.t. the following holds\n1. rank([Fk,1N ]) = N\n2. rank(Wl) = nl, l \u2208 [k + 2, L] 3. \u2207Wk+1\u03a6 ( (Wl, bl) L l=1 ) = 0\n\u2207bk+1\u03a6 ( (Wl, bl) L l=1 ) = 0\nthen (Wl, bl) L l=1 is a global minimum.\nProof: By Lemma 2.1 it holds that\n\u2207Wk+1\u03a6 = F T k \u2206k+1 = 0, \u2207bk+1\u03a6 = \u2206 T k+11N = 0,\nwhich implies [Fk,1N ] T\u2206k+1 = 0. By our assumption, rank([Fk,1N ]) = N it holds that \u2206k+1 = 0. Since rank(Wl) = nl, l \u2208 [k + 2, L], we can apply a similar induction argument as in the proof of Theorem 3.4, to arrive at\u2206L = 0 and thus a global minimum. \u2737\nThe first condition of Lemma 3.5 can be seen as a generalization of the requirement of linearly independent training inputs in Theorem 3.4 to a condition of linear independence of the feature vectors at a hidden layer. Lemma 3.5 suggests that if we want to make statements about the global optimality of critical points, it is sufficient to know when and which critical points fulfill these conditions. The third condition is trivially satisfied by a critical point and the requirement of full column rank of the weight matrices is similar to Theorem 3.4. However, the first one may not be fulfilled since rank([Fk,1N ]) is dependent not only on\nthe weights but also on the architecture. The main difficulty of the proof of our following main theorem is to prove that this first condition holds under the rather simple requirement that nk \u2265 N \u2212 1 for a subset of all critical points.\nBut before we state the theorem we have to discuss a particular notion of non-degenerate critical point.\nDefinition 3.6 (Block Hessian) Let f : D \u2192 R be a twice-continuously differentiable function defined on some open domainD \u2286 Rn. The Hessian w.r.t. a subset of variables S \u2286 {x1, . . . , xn} is denoted as\u2207 2 Sf(x) \u2208 R\n|S|\u00d7|S|. When |S| = n, we write\u22072f(x) \u2208 Rn\u00d7n to denote the full Hessian matrix.\nWe use this to introduce a slightly more general notion of non-degenerate critical point.\nDefinition 3.7 (Non-degenerate critical point) Let f : D \u2192 R be a twice-continuously differentiable function defined on some open domain D \u2286 Rn. Let x \u2208 D be a critical point, i.e. \u2207f(x) = 0, then\n\u2022 x is non-degenerate for a subset of variables S \u2286 {x1, . . . , xn} if \u2207 2 Sf(x) is non-singular.\n\u2022 x is non-degenerate if \u22072f(x) is non-singular.\nNote that a non-degenerate critical point might not be nondegenerate for a subset of variables, and vice versa, if it is non-degenerate on a subset of variables it does not necessarily imply non-degeneracyon the whole set. For instance,\n\u22072f(x) = 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 , \u22072f(y) = 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0\nClearly, det\u22072f(x) = 0 but det\u22072{x1,x2}f(x) 6= 0, and det\u22072f(y) 6= 0 but det\u22072{y3,y4}f(y) = 0. The concept of non-degeneracy on a subset of variables is crucial for the following statement of our main result.\nTheorem 3.8 Let \u03a6 : P \u2192 R be defined as in (1) and let the Assumptions 3.2 hold. Suppose nk \u2265 N \u2212 1 for some k \u2208 [L \u2212 1]. Then every critical point (W \u2217l , b \u2217 l ) L l=1 of \u03a6 which satisfies the following conditions\n1. (W \u2217l , b \u2217 l ) L l=1 is non-degenerate on {(Wl, bl) | l \u2208 I},\nfor some subset I \u2286 {k + 1, . . . , L} satisfying {k + 1} \u2208 I,\n2. (W \u2217l ) L l=k+2 has full column rank, that is, rank(W \u2217 l ) =\nnl for l \u2208 [k + 2, L],\nis a global minimum of \u03a6.\nFirst of all we note that the full column rank condition of (Wl) L l=k+2 in Theorem 3.4, and 3.8 implicitly requires that nk+1 \u2265 nk+2 \u2265 . . . \u2265 nL. This means the network needs to have a pyramidal structure from layer k + 2 to L. It is interesting to note that most modern neural network architectures have a pyramidal structure from some layer, typically the first hidden layer, on. Thus this is not a restrictive requirement. Indeed, one can even argue that Theorem 3.8 gives an implicit justification as it hints on the fact that such networks are easy to train if one layer is sufficiently wide.\nNote that Theorem 3.8 does not require fully nondegenerate critical points but non-degeneracy is only needed for some subset of variables that includes layer k + 1. As a consequence of Theorem 3.8, we get directly a stronger result for non-degenerate local minima.\nCorollary 3.9 Let \u03a6 : P \u2192 R be defined as in (1) and let the Assumptions 3.2 hold. Suppose nk \u2265 N \u2212 1 for some k \u2208 [L \u2212 1]. Then every non-degenerate local minimum (W \u2217l , b \u2217 l ) L l=1 of \u03a6 for which (W \u2217 l ) L l=k+2 has full column rank, that is rank(W \u2217l ) = nl, is a global minimum of \u03a6.\nProof: The Hessian at a non-degenerate local minimum is positive definite and every principal submatrix of a positive definite matrix is again positive definite, in particular for the subset of variables (Wl, bl) L l=k+1. Then application of Theorem 3.8 yields the result. \u2737\nLet us discuss the implications of these results. First, note that Theorem 3.8 is slightly weaker than Theorem 3.4 as it requires also non-degeneracy wrt to a set of variables including layer k + 1. Moreover, similar to Theorem 3.4 it does not exclude the possibility of suboptimal local minima of low rank in the layers \u201cabove\u201d layer k + 1. On the other hand it makes also very strong statements. In fact, if nk \u2265 N \u2212 1 for some k \u2208 [L \u2212 1] then even degenerate saddle points/local maxima are excluded as long as they are non-degenerate with respect to any subset of parameters of upper layers that include layer k + 1 and the rank condition holds. Thus given that the weight matrices of the upper layers have full column rank , there is not much room left for degenerate saddle points/local maxima. Moreover, for a one-hidden-layer network for which n1 \u2265 N \u2212 1, every non-degenerate critical point with respect to the output layer parameters is a global minimum, as the full rank condition is not active for one-hidden layer networks.\nConcerning the non-degeneracy condition of main Theorem 3.8, one might ask how likely it is to encounter degenerate points of a smooth function. This is answered by an application of Sard\u2019s/Morse theorem in (Milnor, 1965).\nTheorem 3.10 (A. Morse, p.11) If f : U \u2282 Rd \u2192 R is twice continuously differentiable. Then for almost all w \u2208\nR d with respect to the Lebesgue measure it holds that f \u2032 defined as f \u2032(x) = f(x) + \u3008w, x\u3009 has only non-degenerate critical points.\nNote that the theorem would still hold if one would draw w uniformly at random from the set {z \u2208 Rd | \u2016z\u20162 \u2264 \u01eb} for any \u01eb > 0. Thus almost every linear perturbation f \u2032 of a function f will lead to the fact all of its critical points are non-degenerate. Thus, this result indicates that exact degenerate points might be rare. Note however that in practice the Hessian at critical points can be close to singular (at least up to numerical precision), which might affect the training of neural networks negatively (Sagun et al., 2016).\nAs we argued for Theorem 3.4 our main Theorem 3.8 does not exclude the possibility of suboptimal degenerate local minima or suboptimal local minima of low rank. However, we conjecture that the second case cannot happen as every neighborhood of the local minima contains full rank matrices which increase the expressiveness of the network and this additional flexibility can be used to reduce the loss which contradicts the definition of a local minimum.\nAs mentioned in the introduction the condition nk \u2265 N\u22121 looks at first sight very strong. However, as mentioned in the introduction, in practice often networks are used where one hidden layer is rather wide, that is nk is on the order of N (typically it is the first layer of the network). As the condition of Theorem 3.8 is sufficient and not necessary, one can expect out of continuity reasons that the loss surface of networks where the condition is approximately true, is still rather well behaved, in the sense that still most local minima are indeed globally optimal and the suboptimal ones are not far away from the globally optimal ones."}, {"heading": "4. Proof of Main Result", "text": "For better readability, we first prove our main Theorem 3.8 for a special case where I is the whole set of upper layers, i.e. I = {k + 1, . . . , L} , and then show how to extend the proof to the general case where I \u2286 {k + 1, . . . , L} . Our proof strategy is as follows. We first show that the output of each layer are real analytic functions of network parameters. Then we prove that there exists a set of parameters such that rank([Fk,1N ]) = N. Using properties of real analytic functions, we conclude that the set of parameters where rank([Fk,1N ]) < N has measure zero. Then with the non-degeneracy condition, we can apply the implicit-function theorem to conclude that even if rank([Fk,1N ]) = N is not true at a critical point, then still in any neighborhood of it there exists a point where the conditions of Lemma 3.5 are true and the loss is minimal. By continuity of \u03a6, this implies that the loss must also be minimal at the critical point.\nWe introduce some notation frequently used in the proofs.\nLet B(x, r) = {z \u2208 Rd | \u2016x\u2212 z\u20162 < r} be the open ball in Rd of radius r around x.\nLemma 4.1 If the Assumptions 3.2 hold, then the output of each layer fl for every l \u2208 [L] are real analytic functions of the network parameters on P .\nProof: Any linear function is real analytic and the set of real analytic functions is closed under addition, multiplication and composition, see e.g. Prop. 2.2.2 and Prop. 2.2.8 in (Krantz & Parks, 2002). As we assume that the activation function is real analytic, we get that all the output functions of the neural network fk are real analytic functions of the parameters as compositions of real analytic functions. \u2737\nThe concept of real analytic functions is important in our proofs as these functions can never be \u201cconstant\u201d in a set of the parameter space which has positive measure unless they are constant everywhere. This is captured by the following lemma.\nLemma 4.2 (Nguyen, 2015; Mityagin, 2015) If f : Rn \u2192 R is a real analytic function which is not identically zero then the set {x \u2208 Rn | f(x) = 0} has Lebesgue measure zero.\nIn the next lemma we show that there exist network parameters such that rank([Fk,1N ]) = N holds if nk \u2265 N \u2212 1. Note that this is only possible due to the fact that one uses non-linear activation functions. For deep linear networks, it is not possible for Fk to achieve maximum rank if the layers below it are not sufficiently wide. To see this, one considers Fk = Fk\u22121Wk + 1Nb T k for a linear network, then rank(Fk) \u2264 min{rank(Fk\u22121), rank(Wk)} + 1 since the addition of a rank-one term does not increase the rank of a matrix by more than one. By using induction, one gets rank(Fk) \u2264 rank(Wl) + k \u2212 l+ 1 for every l \u2208 [k].\nThe existence of network parameters where rank([Fk,1N ]) = N together with the previous lemma will then be used to show that the set of network parameters where rank([Fk,1N ]) < N has measure zero.\nLemma 4.3 If the Assumptions 3.2 hold and nk \u2265 N \u2212 1 for some k \u2208 [L \u2212 1], then there exists at least one set of parameters (Wl, bl) k l=1 such that rank([Fk,1N ]) = N.\nProof: We first show by induction that there always exists a set of parameters (Wl, bl) k\u22121 l=1 s.t. Fk\u22121 has distinct rows. Indeed, we have F1 = \u03c3(XW1 + 1Nb T 1 ). The set of (W1, b1) that makes F1 to have distinct rows is characterized by\n\u03c3(WT1 xi + b1) 6= \u03c3(W T 1 xj + b1), \u2200i 6= j.\nNote, that \u03c3 is strictly monotonic and thus bijective on its domain. Thus this is equivalent to\nWT1 (xi \u2212 xj) 6= 0, \u2200i 6= j.\nLet us denote the first column of W1 by a, then the existence of a for which\naT (xi \u2212 xj) 6= 0, \u2200i 6= j, (2)\nwould imply the result. Note that by assumption xi 6= xj for all i 6= j. Then the set {a \u2208 Rd | aT (xi \u2212 xj) = 0} is a hyperplane, which has measure zero and thus the set where condition (2) fails corresponds to the union of N(N\u22121)\n2 hy-\nperplaneswhich again has measure zero. Thus there always exists a vector a such that condition (2) is satisfied and thus there exists (W1, b1) such that the rows of F1 are distinct. Now, assume that Fp\u22121 has distinct rows for some p \u2265 1, then by the same argument as above we need to construct Wp such that\nWTp\n( fp\u22121(xi)\u2212 fp\u22121(xj) ) 6= 0, \u2200i 6= j.\nBy construction fp\u22121(xi) 6= fp\u22121(xj) and thus with the same argument as above we can choose Wp such that this condition holds. As a result, there exists a set of parameters (Wl, bl) k\u22121 l=1 so that Fk\u22121 has distinct rows.\nNow, given that Fk\u22121 has distinct rows, we show how to construct (Wk, bk) in such a way that [Fk,1N ] \u2208 R\nN\u00d7(nk+1) has full row rank. Since nk \u2265 N \u2212 1, it is sufficient to make the first N \u2212 1 columns of Fk together with the all-ones vector become linearly independent. In particular, let Fk = [A,B] where A \u2208 R\nN\u00d7(N\u22121) and B \u2208 RN\u00d7(nk\u2212N+1) be the matrices containing outputs of the first (N \u2212 1) hidden units and last (nk \u2212 N + 1) hidden units of layer k respectively. Let Wk = [w1, . . . , wN\u22121, wN , . . . , wnk ] \u2208 R nk\u22121\u00d7nk and bk = [v1, . . . , vN\u22121, vN , . . . , vnk ] \u2208 R nk . Let Z = Fk\u22121 = [z1, . . . , zN ] T \u2208 RN\u00d7nk\u22121 with zi 6= zj for every i 6= j. By definition of Fk, it holds Aij = \u03c3(z T i wj + vj) for i \u2208 [N ], j \u2208 [N \u2212 1]. As mentioned above, we just need to show there exists (wj , vj) N\u22121 j=1 so that rank([1N , A]) = N because then it will follow immediately that rank([Fk,1N ]) = N. Pick any a \u2208 R nk\u22121 satisfying potentially after reordering w.l.o.g. \u3008a, z1\u3009 < \u3008a, z2\u3009 < . . . < \u3008a, zN \u3009. By the discussion above such a vector always exists since the complementary set is contained in \u22c3\ni6=j {a \u2208 R nk\u22121 | \u3008zi \u2212 zj , a\u3009 = 0} which has\nmeasure zero.\nWe first prove the result for the case where \u03c3 is bounded. Since \u03c3 is bounded and strictly monotonically increasing, there exist two finite values \u03b3, \u00b5 \u2208 R with \u00b5 < \u03b3 s.t.\nlim \u03b1\u2192\u2212\u221e \u03c3(\u03b1) = \u00b5 and lim \u03b1\u2192+\u221e \u03c3(\u03b1) = \u03b3.\nMoreover, since \u03c3 is strictly monotonically increasing it holds for every \u03b2 \u2208 R, \u03c3(\u03b2) > \u00b5. Pick some \u03b2 \u2208 R. For \u03b1 \u2208 R, we define wj = \u2212\u03b1a, vj = \u03b1z T j a + \u03b2 for every j \u2208 [N \u2212 1]. Note that the matrix A changes as we vary \u03b1. Thus, we consider a family of matrices A(\u03b1) defined as A(\u03b1)ij = \u03c3(z T i wj + vj) = \u03c3(\u03b1(zj \u2212 zi)\nT a+ \u03b2). Then it holds for every i \u2208 [N ], j \u2208 [N \u2212 1]\nlim \u03b1\u2192+\u221e A(\u03b1)ij =\n \n\n\u03b3 j > i\n\u03c3(\u03b2) j = i\n\u00b5 j < i\nLet E(\u03b1) = [1N , A(\u03b1)] then it holds\nlim \u03b1\u2192+\u221e E(\u03b1)ij =\n  \n\n1 j = 1\n\u00b5 j \u2208 [2, N ], i \u2265 j\n\u03c3(\u03b2) j = i+ 1\n\u03b3 else\nLet E\u0302(\u03b1) be a modified matrix where one subtracts every row i by row (i\u2212 1) of E(\u03b1), in particular, let\nE\u0302(\u03b1)ij =\n{\nE(\u03b1)ij i = 1, j \u2208 [N ] E(\u03b1)ij \u2212 E(\u03b1)i\u22121,j i > 1, j \u2208 [N ] (3)\nthen it holds\nlim \u03b1\u2192+\u221e E\u0302(\u03b1)ij =\n \n\n1 i = j = 1\n\u00b5\u2212 \u03c3(\u03b2) < 0 i = j > 1\n0 i > j\nWe do not show the values of other entries as what matters is that the limit, lim \u03b1\u2192+\u221e E\u0302(\u03b1), is an upper triangular matrix. Thus, the determinant is equal to the product of its diagonal entries which is non-zero. Note that the determinant of E\u0302(\u03b1) is the same as that of E(\u03b1) as subtraction of some row from some other row does not change the determinant, and thus we get that lim \u03b1\u2192+\u221e E(\u03b1) has full rank N . As the determinant of E(\u03b1) is a polynomial of its entries and thus continuous in \u03b1, there exists \u03b10 \u2208 R s.t. for every \u03b1 \u2265 \u03b10 it holds rank(E(\u03b1)) = rank([1N , A(\u03b1)]) = N. Moreover, since A is chosen as the first (N \u2212 1) columns of Fk, one can always choose the weights of the first (N \u2212 1) hidden units of layer k so that rank([Fk,1N ]) = N .\nIn the case where the activation function fulfills |\u03c3(t)| \u2264 \u03c11e\n\u03c12t for t < 0 and |\u03c3(t)| \u2264 \u03c13t + \u03c14 for t \u2265 0 we consider directly the determinant of the matrix E(\u03b1). In particular, let us pick some \u03b2 \u2208 R such that \u03c3(\u03b2) 6= 0. We consider the family of matrices A(\u03b1) defined as A(\u03b1)ij = \u03c3(z T i wj + vj) = \u03c3(\u03b1(zj \u2212 zi)\nTa + \u03b2) where wj = \u2212\u03b1a, vj = \u03b1z T j a + \u03b2 for every j \u2208 [N \u2212 1]. Let E(\u03b1) = [A(\u03b1),1N ]. Note that the all-ones vector is now\nsituated at the last column of E(\u03b1) instead of first column as before. This column re-ordering does not change the rank of E(\u03b1). By the Leibniz-formula one has\ndet(E(\u03b1)) = \u2211\n\u03c0\u2208SN\nsign(\u03c0)\nN\u22121\u220f\nj=1\nE(\u03b1)\u03c0(j)j ,\nwhere SN is the set of all N ! permutations of the set {1, . . . , N} and we used the fact that the last column of E(\u03b1) is equal to the all ones vector. Define the permutation \u03b3 as \u03b3(j) = j for j \u2208 [N ]. Then we have\ndet(E(\u03b1))\n= sign(\u03b3)\u03c3(\u03b2)N\u22121 + \u2211\n\u03c0\u2208SN\\{\u03b3}\nsign(\u03c0)\nN\u22121\u220f\nj=1\nE(\u03b1)\u03c0(j)j .\nThe idea now is to show that \u220fN\u22121\nj=1 E(\u03b1)\u03c0(j)j goes to zero for every permutation \u03c0 6= \u03b3 as \u03b1 goes to infinity. And since the whole summation goes to zero while \u03c3(\u03b2) 6= 0, the determinant would be non-zero as desired. With that, we first note that for any permutation \u03c0 6= \u03b3 there has to be at least one component \u03c0(j) where \u03c0(j) > j, in which case, \u03b4j = (zj \u2212 z\u03c0(j))\nTa < 0 and thus for sufficiently large \u03b1, it holds \u03b1\u03b4j + \u03b2 < 0. Thus\n|E(\u03b1)\u03c0(j)j | = |\u03c3(\u03b1(zj\u2212z\u03c0(j)) Ta+\u03b2)| \u2264 \u03c11e \u03c12\u03b2e\u2212\u03b1\u03c12|\u03b4j |."}, {"heading": "If \u03c0(j) = j thenE(\u03b1)\u03c0(j)j = \u03c3(\u03b2). In cases where \u03c0(j) <", "text": "j(j 6= N) it holds that \u03b4j = (zj \u2212 z\u03c0(j))\nT a > 0 and thus for sufficiently large \u03b1, it holds \u03b1\u03b4j + \u03b2 > 0 and we have\n|E(\u03b1)\u03c0(j)j | = |\u03c3(\u03b1(zj\u2212z\u03c0(j)) Ta+\u03b2)| \u2264 \u03c13\u03b4j\u03b1+\u03c13\u03b2+\u03c14.\nSo far, we have shown that |E(\u03b1)\u03c0(j)j | can always be upper-bounded by an exponential function resp. affine function of \u03b1 when \u03c0(j) > j resp. \u03c0(j) < j or it is just a constant when \u03c0(j) = j. The above observations imply that there exist positive constants P,Q,R, S, T such that it holds for every \u03c0 \u2208 SN \\ {\u03b3} ,\n\u2223 \u2223 \u2223 N\u22121\u220f\nj=1\nE(\u03b1)\u03c0(j)j \u2223 \u2223 \u2223 \u2264 R(P\u03b1+Q)Se\u2212\u03b1T .\nAs \u03b1 \u2192 \u221e the upper bound goes to zero. As there are only finitely many such terms, we get\nlim \u03b1\u2192\u221e\ndet(E(\u03b1)) = sign(\u03b3)\u03c3(\u03b2)N\u22121 6= 0,\nand thus with the same argument as before we can argue that there exists a finite \u03b10 for which E(\u03b1) has full rank.\n\u2737\nNow we combine the previous lemma with Lemma 4.2 to conclude the following.\nLemma 4.4 If the Assumptions 3.2 hold and nk \u2265 N \u2212 1 for some k \u2208 [L \u2212 1] then the set S := {(\nWl, bl )k\nl=1\n\u2223 \u2223 \u2223 rank([Fk,1N ]) < N } has Lebesgue mea-\nsure zero.\nProof: Let Ek = [Fk,1N ] \u2208 R N\u00d7(nk+1). Note that with Lemma 4.1 the output Fk of layer k is an analytic function of the network parameters on P . The set of low rank matrices Ek can be characterized by a system of equations such that the\n( nk+1 N ) determinants of all N \u00d7 N submatri-\nces of Ek are zero. As the determinant is a polynomial in the entries of the matrix and thus an analytic function of the entries and composition of analytic functions are again analytic, we conclude that each determinant is an analytic function of the network parameters of the first k layers. By Lemma 4.3 there exists at least one set of network parameters of the first k layers such that one of these determinant functions is not identically zero and thus by Lemma 4.2 the set of network parameters where this determinant is zero has measure zero. But as all submatrices need to have low rank in order that rank([Fk,1N ]) < N , it follows that the set of network parameters where rank([Fk,1N ]) < N has measure zero. \u2737\nWe conclude that for nk \u2265 N \u2212 1 even if there are network parameters such that rank([Fk,1N ]) < N , then every neighborhood of these parameters contains network parameters such that rank([Fk,1N ]) = N.\nCorollary 4.5 If the Assumptions 3.2 hold and nk \u2265 N\u22121 for some k \u2208 [L \u2212 1], then for any given (W 0l , b 0 l ) k l=1 and for every \u01eb > 0, there exists at least one ( Wl, bl )k\nl=1 \u2208\nB ((\nW 0l , b 0 l\n)k l=1 , \u01eb ) s.t. rank([Fk,1N ]) = N.\nProof: Let S := {( Wl, bl )k\nl=1\n\u2223 \u2223 \u2223 rank([Fk,1N ]) < N } .\nThe ball B (( Wl, bl )k l=1 , \u01eb ) has positive Lebesgue measure while S has measure zero due to Lemma 4.4. Thus, for every ( Wl, bl )k\nl=1 \u2208 B (( W 0l , b 0 l )k l=1 , \u01eb ) \\ S it holds\nrank([Fk,1N ]) = N. \u2737\nThe final proof of our main Theorem 3.8 is heavily based on the implicit function theorem, see e.g. (Marsden, 1974).\nTheorem 4.6 Let \u03a8 : Rs \u00d7 Rt \u2192 Rt be a continuously differentiable function. Suppose (u0, v0) \u2208 R\ns \u00d7 Rt and \u03a8(u0, v0) = 0. If the Jacobian matrix w.r.t. v,\nJv\u03a8(u0, v0) =\n\n \n\u2202\u03a81 \u2202v1 \u00b7 \u00b7 \u00b7 \u2202\u03a81\u2202vt ...\n... \u2202\u03a8t \u2202v1 \u00b7 \u00b7 \u00b7 \u2202\u03a8t\u2202vt\n\n  \u2208 R t\u00d7t\nis non-singular at (u0, v0), then there is an open ball B(u0, \u01eb) for some \u01eb > 0 and a unique function \u03b1 :\nB(u0, \u01eb) \u2192 R t such that \u03a8(u, \u03b1(u)) = 0 for all u \u2208 B(u0, \u01eb). Furthermore, \u03b1 is continuously differentiable.\nWith all the intermediate results proven above, we are finally ready for the proof of the main result.\nProof of Theorem 3.8 for case I = {k + 1, . . . , L} Let us divide the set of all parameters of the network into two subsets where one corresponds to all parameters of all layers up to k, for that we denote u = [vec(W1) T , bT1 , . . . , vec(Wk) T , bTk ] T , and the other corresponds to the remaining parameters, for that we denote v = [vec(Wk+1) T , bTk+1, . . . , vec(WL) T , bTL] T . By abuse of notation, we write \u03a6(u, v) to denote \u03a6 (\n(Wl, bl) L l=1\n)\n.\nLet s = dim(u), t = dim(v) and (u\u2217, v\u2217) \u2208 Rs\u00d7Rt be the corresponding vectors for the critical point (W \u2217l , b \u2217 l ) L l=1. Let \u03a8 : Rs \u00d7 Rt \u2192 Rt be a map defined as \u03a8(u, v) = \u2207v\u03a6(u, v) \u2208 R\nt, which is the gradient mapping of \u03a6 w.r.t. all parameters of the upper layers from (k + 1) to L. Since the gradient vanishes at a critical point, it holds that \u03a8(u\u2217, v\u2217) = \u2207v\u03a6(u\n\u2217, v\u2217) = 0. The Jacobian of \u03a8 w.r.t. v is the principal submatrix of the Hessian of \u03a6 w.r.t. v, that is, Jv\u03a8(u, v) = \u2207 2 v\u03a6(u, v) \u2208 R\nt\u00d7t. As the critical point is assumed to be non-degenerate with respect to v, it holds that Jv\u03a8(u \u2217, v\u2217) = \u22072v\u03a6(u\n\u2217, v\u2217) is nonsingular. Moreover, \u03a8 is continuously differentiable since \u03a6 \u2208 C2(P) due to Assumption 3.2. Therefore, \u03a8 and (u\u2217, v\u2217) satisfy the conditions of the implicit function theorem 4.6. Thus there exists an open ball B(u\u2217, \u03b41) \u2282 R s for some \u03b41 > 0 and a continuously differentiable function \u03b1 : B(u\u2217, \u03b41) \u2192 R t such that\n{\n\u03a8(u, \u03b1(u)) = 0, \u2200u \u2208 B(u\u2217, \u03b41) \u03b1(u\u2217) = v\u2217\nBy assumption we have rank(W \u2217l ) = nl, l \u2208 [k+2, L], that is the weight matrices of the \u201cupper\u201d layers have full column rank. Note that (W \u2217l ) L l=k+2 corresponds to the weight matrix part of v\u2217 where one leaves out W \u2217k+1. Thus there exists a sufficiently small \u01eb such that for any v \u2208 B(v\u2217, \u01eb), the weight matrix part (Wl) L l=k+2 of v has full column rank. In particular, this, combined with the continuity of \u03b1, implies that for a potentially smaller 0 < \u03b42 \u2264 \u03b41, it holds for all u \u2208 B(u\u2217, \u03b42) that\n\u03a8(u, \u03b1(u)) = 0, \u03b1(u\u2217) = v\u2217,\nand that the weight matrix part (Wl) L l=k+2 of \u03b1(u) \u2208 R t has full column rank.\nNow, by Corollary 4.5 for any 0 < \u03b43 \u2264 \u03b42 there exists a u\u0303 \u2208 B(u\u2217, \u03b43) such that the generated output matrix F\u0303k at layer k of the corresponding network parameters of u\u0303 satisfies rank([F\u0303k,1N ]) = N. Moreover, it holds for v\u0303 = \u03b1(u\u0303) that \u03a8(u\u0303, v\u0303) = 0 and the weight matrix part (W\u0303l) L l=k+2 of\nv\u0303 has full column rank. Assume (u\u0303, v\u0303) corresponds to the following representation\n{\nu\u0303 = [vec(W\u03031) T , b\u0303T1 , . . . , vec(W\u0303k) T , b\u0303Tk ] T \u2208 Rs v\u0303 = [vec(W\u0303k+1) T , b\u0303Tk+1, . . . , vec(W\u0303L) T , b\u0303TL] T \u2208 Rt\nWe obtain the following\n \n \n\u03a8(u\u0303, v\u0303) = 0 \u21d2 \u2207Wk+1\u03a6 ( (W\u0303l, b\u0303l) k l=1 ) = 0 \u03a8(u\u0303, v\u0303) = 0 \u21d2 \u2207bk+1\u03a6 ( (W\u0303l, b\u0303l) k l=1 ) = 0\nrank(W\u0303l) = nl, \u2200 l \u2208 [k + 2, L]\nrank([F\u0303k,1N ]) = N\nThus, Lemma 3.5 implies that (W\u0303l, b\u0303l) L l=1 is a global minimum of \u03a6. Let p\u2217 = \u03a6 (\n(W\u0303l, b\u0303l) L l=1\n)\n= \u03a6(u\u0303, v\u0303). Note that\nthis construction can be done for any \u03b43 \u2208 (0, \u03b42]. In particular, let (\u03b3r) \u221e r=1 be a strictly monotonically decreasing sequence such that \u03b31 = \u03b43 and limr\u2192\u221e \u03b3r = 0. By Corollary 4.5 and the previous argument, we can choose for any \u03b3r > 0 a point u\u0303r \u2208 B(u \u2217, \u03b3r) such that v\u0303r = \u03b1(u\u0303r) has full rank and \u03a6(u\u0303r, v\u0303r) = p \u2217. Moreover, as limr\u2192\u221e \u03b3r = 0, it follows that limr\u2192\u221e u\u0303r = u \u2217 and as \u03b1 is a continuous function, it holds with v\u0303r = \u03b1(u\u0303r) that limr\u2192\u221e v\u0303r = limr\u2192\u221e \u03b1(u\u0303r) = \u03b1(limr\u2192\u221e u\u0303r) = \u03b1(u\n\u2217) = v\u2217. Thus we get limr\u2192\u221e(u\u0303r, v\u0303r) = (u\n\u2217, v\u2217) and as \u03a6 is a continuous function it holds\nlim r\u2192\u221e\n\u03a6 ( (u\u0303r, v\u0303r) ) = \u03a6(u\u2217, v\u2217) = p\u2217,\nas \u03a6 attains the global minimum for the whole sequence (u\u0303r, v\u0303r).\nProof of Theorem 3.8 for general case In the general case I \u2286 {k + 1, . . . , L}, the previous proof can be easily adapted. The idea is that we fix all layers in {k + 1, . . . , L} \\ I. In particular, let\n{\nu = [vec(W1) T , bT1 , . . . , vec(Wk) T , bTk ] T v = [vec(WI(1)) T , bTI(1), . . . , vec(WI(|I|)) T , bTI(|I|)] T .\nLet s = dim(u), t = dim(v) and (u\u2217, v\u2217) \u2208 Rs\u00d7Rt be the corresponding vectors at (W \u2217l , b \u2217 l ) L l=1. Let \u03a8 : R s \u00d7 Rt \u2192 R t be a map defined as\u03a8(u, v) = \u2207v\u03a6 ( (Wl, bl) L l=1 ) with \u03a8(u\u2217, v\u2217) = \u2207v\u03a6 ( (W \u2217l , b \u2217 l ) L l=1 ) = 0.\nThe only difference is that all the layers from {k + 1, . . . , L} \\ I are hold fixed. They are not contained in the arguments of \u03a8, thus will not be involved in our perturbation analysis. In this way, the full rank property of the weight matrices of these layers are preserved, which is needed to obtain the global minimum."}, {"heading": "5. Relaxing the Condition on the Number of Hidden Units", "text": "We have seen that nk \u2265 N \u2212 1 is a sufficient condition which leads to a rather simple structure of the critical points, in the sense that all local minima which have full rank in the layers k + 2 to L and for which the Hessian is non-degenerate on any subset of upper layers that includes layer k+1 are automatically globally optimal. This suggests that suboptimal locally optimal points are either completely absent or relatively rare. We have motivated before that networks with a certain wide layer are used in practice, which shows that the condition nk \u2265 N \u2212 1 is not completely unrealistic. On the other hand we want to discuss in this section how it could be potentially relaxed. The following result will provide some intuition about the case nk < N \u2212 1, but will not be as strong as our main result 3.8 which makes statements about a large class of critical points. The main idea is that with the condition nk \u2265 N \u2212 1 the data is linearly separable at layer k. As modern neural networks are expressive enough to represent any function, see (Zhang et al., 2017) for an interesting discussion on this, one can expect that in some layer the training data becomes linearly separable. We prove that any critical point, for which the \u201clearned\u201d network outputs at any layer are linearly separable (see Definition 5.1) is a global minimum of the training error.\nDefinition 5.1 (Linearly separable vectors) A set of vectors (xi) N i=1 \u2208 R d from m classes (Cj) m j=1 is called linearly separable if there exist m vectors (aj) m j=1 \u2208 R d and m scalars (bj) m j=1 \u2208 R so that a T j xi + bj > 0 for xi \u2208 Cj and aTj xi + bj < 0 for xi /\u2208 Cj for every i \u2208 [N ], j \u2208 [m].\nIn this section, we use a slightly different loss function than in the previous section. The reason is that the standard least squares loss is not necessarily small when the data is linearly separable. Let C1, . . . , Cm denote m classes. We consider the objective function \u03a6 : P \u2192 R from (1)\n\u03a6 (\n(Wl, bl) L l=1\n) = N\u2211\ni=1\nm\u2211\nj=1\nl ( fLj(xi)\u2212 yij ) (4)\nwhere the loss function now takes the new form\nl ( fLj(xi)\u2212 yij ) =\n{\nl1 ( fLj(xi)\u2212 yij ) xi \u2208 Cj l2 ( fLj(xi)\u2212 yij ) xi /\u2208 Cj\nwhere l1, l2 penalize the deviation from the label encoding for the true class resp. wrong classes. We assume that the minimum of \u03a6 is attained over P . Note that \u03a6 is bounded from below by zero as l1 and l2 are non-negative loss functions. The results of this section are made under the following assumptions on the activation and loss function.\nAssumptions 5.2 1. \u03c3 \u2208 C1(R) and strictly monotonically increasing.\n2. l1 : R \u2192 R+, l1 \u2208 C 1, l1(a) = 0 \u21d4 a \u2265 0, l \u2032 1(a) =\n0 \u21d4 a \u2265 0 and l\u20321(a) < 0 \u2200 a < 0\n3. l2 : R \u2192 R+, l2 \u2208 C 1, l2(a) = 0 \u21d4 a \u2264 0, l \u2032 2(a) =\n0 \u21d4 a \u2264 0 and l\u20322(a) > 0 \u2200 a > 0\nIn classification tasks, this loss function encourages higher values for the true class and lower values for wrong classes. An example of the loss function that satisfies Assumption 5.2 is given as (see Figure 2):\nl1(a) =\n{\na2 a \u2264 0 0 a \u2265 0 l2(a) =\n{\n0 a \u2264 0 a2 a \u2265 0\nNote that for a {+1,\u22121}-label encoding, +1 for the true class and \u22121 for all wrong classes, one can rewrite (4) as\n\u03a6 (\n(Wl, bl) L l=1\n)\n=\nN\u2211\ni=1\nm\u2211\nj=1\nmax{0, 1\u2212 yijfLj(xi)} 2,\nwhich is similar to the truncated squared loss (also called squared hinge loss) used in the SVM for binary classification. Since \u03c3 and l are continuously differentiable, all the results from Lemma 2.1 still hold.\nOur main result in this section is stated as follows.\nTheorem 5.3 Let \u03a6 : P \u2192 R+ be defined as in (4) and let the Assumptions 5.2 hold. Then it follows:\n1. Every critical point of \u03a6 for which the feature vectors contained in the rows of Fk are linearly separable and all the weight matrices (Wl) L l=k+2 have full column\nrank is a global minimum.\n2. If the training inputs are linearly separable then every\ncritical point of \u03a6 for which all the weight matrices (Wl) L l=2 have full column rank is a global minimum.\nProof:\n1. Let F\u0303k = [Fk,1N ]. Since Fk contains linearly separable feature vectors, there exists m vectors\nh1, . . . , hm \u2208 R nk+1 s.t. \u3008hj , (F\u0303k)i:\u3009 > 0 for xi \u2208 Cj and \u3008hj , (F\u0303k)i:\u3009 < 0 for xi /\u2208 Cj . Let H = [h1, . . . , hm] \u2208 R (nk+1)\u00d7m, one obtains\n(HT F\u0303k T )ji = \u3008hj , (F\u0303k)i:\u3009\n{\n> 0 xi \u2208 Cj < 0 xi /\u2208 Cj .\nOn the other hand,\n(\u2206L)ij = \u03b4Lj(xi)\n= \u2202\u03a6\n\u2202gLj(xi)\n=\n{\nl\u20321(fLj(xi)\u2212 yij)\u03c3 \u2032(gLj(xi)) xi \u2208 Cj l\u20322(fLj(xi)\u2212 yij)\u03c3 \u2032(gLj(xi)) xi /\u2208 Cj\nWe show thatHT F\u0303k T \u2206L = 0 if and only if\u2206L = 0. Indeed, if \u2206L = 0 the implication is trivial. For the other direction, assume that HT F\u0303k T \u2206L = 0. Then it holds for every j \u2208 [m] that 0 = (HT F\u0303k T \u2206L)jj = \u2211N i=1(H T F\u0303k T )ji(\u2206L)ij . In particular,\nN\u2211\ni=1\n(HT F\u0303k T )ji(\u2206L)ij\n= \u2211\ni\nxi\u2208Cj\n\u2329\n(F\u0303k)i,:, hj\n\u232a\nl\u20321(fLj(xi)\u2212 yij)\u03c3 \u2032(gLj(xi))\n+ \u2211\ni\nxi /\u2208Cj\n\u2329\n(F\u0303k)i,:, hj\n\u232a\nl\u20322(fLj(xi)\u2212 yij)\u03c3 \u2032(gLj(xi)) \u2264 0\nNote that under the assumptions on the loss and activation function and since the features are separable, the terms in both sums are non-positive and thus the sum can only vanish if all terms vanish which implies\n{\nl\u20321 ( fLj(xi)\u2212 yij ) = 0 xi \u2208 Cj l\u20322 ( fLj(xi)\u2212 yij ) = 0 xi /\u2208 Cj \u2200i \u2208 [N ], j \u2208 [m]\n(5)\nwhich yields\u2206L = 0.\nBack to the main proof, the idea is to prove that \u2206L = 0 at the given critical point. Let us assume for the sake of contradiction that \u2206L 6= 0. For every l \u2208 [L], il \u2208 [nl] define \u03a3\nl il = diag(\u03c3\u2032(glil(x1)), . . . , \u03c3 \u2032(glil(xN ))). Since the cumulative product \u220fL\u22121\nl=k+1 \u03a3 l il is aN\u00d7N diagonalma-\ntrix which contains only positive entries in its diagonal, it does not change the sign pattern of\u2206L, and thus it holds with, HT F\u0303k T \u2206L = 0 if and only if \u2206L = 0, for every (ik+1, . . . , iL\u22121) \u2208 [nk+1] \u00d7 . . . \u00d7 [nL\u22121]\nthat\n0 6= HT F\u0303k T ( L\u22121\u220f\nl=k+1\n\u03a3lil\n)\n\u2206L (6)\n0 6= HT F\u0303k T ( L\u22121\u220f\nl=k+1\n\u03a3lil\n)\n\u2206LW T L (rank(WL) = nL),\nwhere the last inequality is implied by (6) as WL has full column rank nL = m. Since the above product of matrices is a non-zero matrix, there must exist a non-zero column, say p \u2208 [nL\u22121], then\n0 6= HT F\u0303k T ( L\u22121\u220f\nl=k+1\n\u03a3lil\n)(\n\u2206LW T L\n)\n:p\nSince iL\u22121 is arbitrary, pick iL\u22121 = p one obtains\n0 6= HT F\u0303k T ( L\u22122\u220f\nl=k+1\n\u03a3lil\n)\n\u03a3L\u22121p\n(\n\u2206LW T L\n)\n:p \ufe38 \ufe37\ufe37 \ufe38\n(\u2206L\u22121):p\n(7)\nMoreover, it holds for every i \u2208 [N ] (\n\u03a3L\u22121p ( \u2206LW T L )\n:p\n)\ni\n= \u03c3\u2032(g(L\u22121)p(xi))\nnL\u2211\nj=1\n\u03b4Lj(xi)(WL)pj\n= \u03b4(L\u22121)p(xi) = ( \u2206L\u22121 )\nip\nand thus from (7),\n0 6= HT F\u0303k T ( L\u22122\u220f\nl=k+1\n\u03a3lil\n)\n(\u2206L\u22121):p\n\u21d2 0 6= HT F\u0303k T ( L\u22122\u220f\nl=k+1\n\u03a3lil\n)\n\u2206L\u22121\nCompared to (6), we have reduced the product from \u220fL\u22121\nl=k+1 to \u220fL\u22122 l=k+1, By induction, one can easily\nshow that\n0 6= HT F\u0303k T \u03a3k+1ik+1\u2206k+2\nand hence 0 6= HT F\u0303k T \u2206k+1, which implies 0 6= F\u0303k T \u2206k+1 = [(\u2207Wk+1 ) T\u03a6,\u2207bk+1\u03a6] T . However, this is a contradiction to the fact that we assumed that (Wl, bl) L l=1 is a critical point. Thus it follows that it has to hold \u2206L = 0. As \u2206L = 0 it holds (5) which implies {\nfLj(xi) \u2265 yij xi \u2208 Cj fLj(xi) \u2264 yij xi /\u2208 Cj \u2200 i \u2208 [N ], j \u2208 [m].\nThis in turn implies \u03a6 (\n(Wl, bl) L l=1\n)\n= 0. Thus the\ncritical point (Wl, bl) L l=1 is a global minimum.\n2. This can be seen as a special case of the first statement.\nIn particular, assume one has a zero-layer which coincides with the training inputs, namely F0 = X , then the result follows immediately.\n\u2737\nNote that the second statement of Theorem 5.3 can be considered as a special case of the first statement. In the case where L = 2 and training inputs are linearly separable, the second statement of our Theorem 5.3 recovers the similar result of (Gori & Tesi, 1992; Frasconi et al., 1997) for onehidden layer networks.\nEven though the assumptions of Theorem 3.4 and Theorem 5.3 are different in terms of class of activation and loss functions, their results are related. In fact, it is well known that if a set of vectors is linearly independent then they are linearly separable, see e.g. p.340 (Barber, 2012). Thus Theorem 5.3 can be seen as a direct generalization of Theorem 3.4. The caveat, which is also the main difference to Theorem 3.8, is that Theorem 5.3 makes only statements for all the critical points for which the problem has become separable at some layer, whereas there is no such condition in Theorem 3.8. However, we still think that the result is of practical relevance, as one can expect for a sufficiently large network that stochastic gradient descent will lead to a network structure where the data becomes separable at a particular layer. When this happens all the associated critical points are globally optimal. It is an interesting question for further research if one can show directly under some architecture condition that the network outputs become linearly separable at some layer for any local minimum and thus every local minimum is a global minimum."}, {"heading": "6. Discussion", "text": "Our results show that the loss surface becomes wellbehaved when there is a wide layer in the network. Implicitly, such a wide layer is often present in convolutional neural networks used in computer vision. It is thus an interesting future research question how and if our result can be generalized to neural networks with sparse connectivity. We think that the results presented in this paper are a significant addition to the recent understanding why deep learning works so efficiently. In particular, since in this paper we are directly working with the neural networks used in practice without any modifications or simplifications."}, {"heading": "Acknowledgment", "text": "The authors acknowledge support by the ERC starting grant NOLEPRO 307793."}], "references": [{"title": "Exponentially many local minima for single neurons", "author": ["P. Auer", "M. Herbster", "M.K. Warmuth"], "venue": "In NIPS,", "citeRegEx": "Auer et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1996}, {"title": "Do deep nets really need to be deep", "author": ["J. Ba", "R. Caruana"], "venue": "In NIPS,", "citeRegEx": "Ba and Caruana,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caruana", "year": 2014}, {"title": "Neural networks and principle component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural Networks,", "citeRegEx": "Baldi and Hornik,? \\Q1988\\E", "shortCiteRegEx": "Baldi and Hornik", "year": 1988}, {"title": "Bayesian Reasoning and Machine Learning", "author": ["D. Barber"], "venue": null, "citeRegEx": "Barber,? \\Q2012\\E", "shortCiteRegEx": "Barber", "year": 2012}, {"title": "Globally optimal gradient descent for a convnet with gaussian inputs, 2017", "author": ["A. Brutzkus", "A. Globerson"], "venue": null, "citeRegEx": "Brutzkus and Globerson,? \\Q2017\\E", "shortCiteRegEx": "Brutzkus and Globerson", "year": 2017}, {"title": "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping", "author": ["R. Caruana", "S. Lawrence", "L. Giles"], "venue": "In NIPS,", "citeRegEx": "Caruana et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Caruana et al\\.", "year": 2001}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Hena", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Open problem: The landscape of the loss surfaces of multilayer networks. JMLR, 2015b", "author": ["A. Choromanska", "Y. LeCun", "G.B. Arous"], "venue": null, "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Deep, big, simple neural nets for handwritten digit recognition", "author": ["D.C. Ciresan", "U. Meier", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Ciresan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2010}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Successes and failures of backpropagation: A theoretical investigation", "author": ["P. Frasconi", "M. Gori", "A. Tesi"], "venue": "Progress in Neural Networks: Architecture,", "citeRegEx": "Frasconi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Frasconi et al\\.", "year": 1997}, {"title": "Globally optimal training of generalized polynomial neural networks with nonlinear spectral methods", "author": ["A. Gautier", "Q. Nguyen", "M. Hein"], "venue": null, "citeRegEx": "Gautier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gautier et al\\.", "year": 2016}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["I.J. Goodfellow", "O. Vinyals", "A.M. Saxe"], "venue": "In ICLR,", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "On the problem of local minima in backpropagation", "author": ["M. Gori", "A. Tesi"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gori and Tesi,? \\Q1992\\E", "shortCiteRegEx": "Gori and Tesi", "year": 1992}, {"title": "Global optimality in tensor factorization, deep learning, and beyond", "author": ["B.D. Haeffele", "R. Vidal"], "venue": null, "citeRegEx": "Haeffele and Vidal,? \\Q2015\\E", "shortCiteRegEx": "Haeffele and Vidal", "year": 2015}, {"title": "Multiple view geometry in computer vision", "author": ["R. Hartley", "A. Zisserman"], "venue": null, "citeRegEx": "Hartley and Zisserman,? \\Q2004\\E", "shortCiteRegEx": "Hartley and Zisserman", "year": 2004}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": null, "citeRegEx": "Janzamin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2016}, {"title": "Deep learning without poor local minima", "author": ["K. Kawaguchi"], "venue": "In NIPS,", "citeRegEx": "Kawaguchi,? \\Q2016\\E", "shortCiteRegEx": "Kawaguchi", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "In NIPS,", "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Gradient descent only converges to minimizers", "author": ["J.D. Lee", "M. Simchowitz", "M.I. Jordan", "B. Recht"], "venue": "In COLT,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "How far can we go without convolution", "author": ["Z. Lin", "R. Memisevic", "K. Konda"], "venue": "Improving fully-connected networks. preprint,", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Elementary classical analysis", "author": ["J.E. Marsden"], "venue": "W.H.Freeman and Company,", "citeRegEx": "Marsden,? \\Q1974\\E", "shortCiteRegEx": "Marsden", "year": 1974}, {"title": "Lectures on H-Cobordism Theorem", "author": ["J. Milnor"], "venue": null, "citeRegEx": "Milnor,? \\Q1965\\E", "shortCiteRegEx": "Milnor", "year": 1965}, {"title": "The zero set of a real analytic function", "author": ["B. Mityagin"], "venue": null, "citeRegEx": "Mityagin,? \\Q2015\\E", "shortCiteRegEx": "Mityagin", "year": 2015}, {"title": "Pathsgd: Path-normalized optimization in deep neural networks", "author": ["B. Neyshabur", "R.R. Salakhutdinov", "N. Srebro"], "venue": "In NIPS,", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "Complex powers of analytic functions and meromorphic renormalization in qft", "author": ["V.D. Nguyen"], "venue": null, "citeRegEx": "Nguyen,? \\Q2015\\E", "shortCiteRegEx": "Nguyen", "year": 2015}, {"title": "Theory ii: Landscape of the empirical risk in deep learning, 2017", "author": ["T. Poggio", "Q. Liao"], "venue": null, "citeRegEx": "Poggio and Liao,? \\Q2017\\E", "shortCiteRegEx": "Poggio and Liao", "year": 2017}, {"title": "Piecewise convexity of artificial neural networks, 2017", "author": ["B. Rister", "D.L. Rubin"], "venue": null, "citeRegEx": "Rister and Rubin,? \\Q2017\\E", "shortCiteRegEx": "Rister and Rubin", "year": 2017}, {"title": "On the quality of the initial basin in overspecified networks", "author": ["I. Safran", "O. Shamir"], "venue": "In ICML,", "citeRegEx": "Safran and Shamir,? \\Q2016\\E", "shortCiteRegEx": "Safran and Shamir", "year": 2016}, {"title": "Singularity of the hessian in deep learning, 2016", "author": ["L. Sagun", "L. Bottou", "Y. LeCun"], "venue": null, "citeRegEx": "Sagun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2016}, {"title": "Training a single sigmoidal neuron is hard", "author": ["J. Sima"], "venue": "Neural Computation,", "citeRegEx": "Sima,? \\Q2002\\E", "shortCiteRegEx": "Sima", "year": 2002}, {"title": "Learning relus via gradient descent, 2017", "author": ["M. Soltanolkotabi"], "venue": null, "citeRegEx": "Soltanolkotabi,? \\Q2017\\E", "shortCiteRegEx": "Soltanolkotabi", "year": 2017}, {"title": "Exponentially vanishing suboptimal local minima in multilayer neural networks, 2017", "author": ["D. Soudry", "E. Hoffer"], "venue": null, "citeRegEx": "Soudry and Hoffer,? \\Q2017\\E", "shortCiteRegEx": "Soudry and Hoffer", "year": 2017}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. andManzagol"], "venue": "JLMR, 11:3371\u20133408,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "On the local minima free condition of backpropagation learning", "author": ["X. Yu", "G. Chen"], "venue": "IEEE Transaction on Neural Networks,", "citeRegEx": "Yu and Chen,? \\Q1995\\E", "shortCiteRegEx": "Yu and Chen", "year": 1995}, {"title": "Understanding deep learning requires re-thinking generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "Vinyals", "Oriol"], "venue": "In ICLR,", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "The landscape of deep learning algorithms, 2017", "author": ["P. Zhou", "J. Feng"], "venue": null, "citeRegEx": "Zhou and Feng,? \\Q2017\\E", "shortCiteRegEx": "Zhou and Feng", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "This problem turns out to be very difficult as there can be exponentially many distinct local minima (Auer et al., 1996; Safran & Shamir, 2016).", "startOffset": 101, "endOffset": 143}, {"referenceID": 31, "context": "It has been shown that the training of a network with a single neuron with a variety of activation functions turns out to be NP-hard (Sima, 2002).", "startOffset": 133, "endOffset": 145}, {"referenceID": 9, "context": "Surprisingly, it has been observed (Dauphin et al., 2014; Goodfellow et al., 2015) that in the training of stateof-the-art feedforward neural networks with sparse connectivity like convolutional neural networks (LeCun et al.", "startOffset": 35, "endOffset": 82}, {"referenceID": 12, "context": "Surprisingly, it has been observed (Dauphin et al., 2014; Goodfellow et al., 2015) that in the training of stateof-the-art feedforward neural networks with sparse connectivity like convolutional neural networks (LeCun et al.", "startOffset": 35, "endOffset": 82}, {"referenceID": 19, "context": ", 2015) that in the training of stateof-the-art feedforward neural networks with sparse connectivity like convolutional neural networks (LeCun et al., 1990; Krizhevsky et al., 2012) or fully connected ones Department of Mathematics and Computer Science, Saarland University, Germany.", "startOffset": 136, "endOffset": 181}, {"referenceID": 18, "context": ", 2015) that in the training of stateof-the-art feedforward neural networks with sparse connectivity like convolutional neural networks (LeCun et al., 1990; Krizhevsky et al., 2012) or fully connected ones Department of Mathematics and Computer Science, Saarland University, Germany.", "startOffset": 136, "endOffset": 181}, {"referenceID": 12, "context": "However, as the authors admit themselves in (Goodfellow et al., 2015), the reason for this might be that there is a connection between the fact that these networks have good performance and that they are easy to train.", "startOffset": 44, "endOffset": 69}, {"referenceID": 20, "context": "(Brutzkus & Globerson, 2017; Lee et al., 2016; Poggio & Liao, 2017; Rister & Rubin, 2017; Soudry & Hoffer, 2017; Zhou & Feng, 2017).", "startOffset": 0, "endOffset": 131}, {"referenceID": 16, "context": "However, it turns out that these approaches are either not practical (Janzamin et al., 2016; Haeffele & Vidal, 2015; Soltanolkotabi, 2017) as they require e.", "startOffset": 69, "endOffset": 138}, {"referenceID": 32, "context": "However, it turns out that these approaches are either not practical (Janzamin et al., 2016; Haeffele & Vidal, 2015; Soltanolkotabi, 2017) as they require e.", "startOffset": 69, "endOffset": 138}, {"referenceID": 11, "context": "knowledge about the data generating measure, or they modify the neural network structure and objective (Gautier et al., 2016).", "startOffset": 103, "endOffset": 125}, {"referenceID": 17, "context": "One class of networks which are simpler to analyze are deep linear networks for which it has been shown that every local minimum is a global minimum (Baldi & Hornik, 1988; Kawaguchi, 2016).", "startOffset": 149, "endOffset": 188}, {"referenceID": 17, "context": "It has recently been shown (Kawaguchi, 2016) that if some of these assumptions are dropped one basically recovers the result of the linear case, but the model is still unrealistic.", "startOffset": 27, "endOffset": 44}, {"referenceID": 10, "context": "Moreover, it extends results of (Gori & Tesi, 1992; Frasconi et al., 1997) who have shown that for certain deep feedforward neural networks almost all local minima are globally optimal whenever the training data is linearly independent.", "startOffset": 32, "endOffset": 74}, {"referenceID": 21, "context": "in (Lin et al., 2016) they have 50,000 training samples and the network has one hidden layer with 10,000 hidden units and (Ba & Caruana, 2014) have 1.", "startOffset": 3, "endOffset": 21}, {"referenceID": 8, "context": "We refer to (Ciresan et al., 2010; Neyshabur et al., 2015; Vincent et al., 2010; Caruana et al., 2001) for other examples where the number of hidden units of one layer is on the order of the number of training samples.", "startOffset": 12, "endOffset": 102}, {"referenceID": 25, "context": "We refer to (Ciresan et al., 2010; Neyshabur et al., 2015; Vincent et al., 2010; Caruana et al., 2001) for other examples where the number of hidden units of one layer is on the order of the number of training samples.", "startOffset": 12, "endOffset": 102}, {"referenceID": 34, "context": "We refer to (Ciresan et al., 2010; Neyshabur et al., 2015; Vincent et al., 2010; Caruana et al., 2001) for other examples where the number of hidden units of one layer is on the order of the number of training samples.", "startOffset": 12, "endOffset": 102}, {"referenceID": 5, "context": "We refer to (Ciresan et al., 2010; Neyshabur et al., 2015; Vincent et al., 2010; Caruana et al., 2001) for other examples where the number of hidden units of one layer is on the order of the number of training samples.", "startOffset": 12, "endOffset": 102}, {"referenceID": 23, "context": "This is answered by an application of Sard\u2019s/Morse theorem in (Milnor, 1965).", "startOffset": 62, "endOffset": 76}, {"referenceID": 30, "context": "Note however that in practice the Hessian at critical points can be close to singular (at least up to numerical precision), which might affect the training of neural networks negatively (Sagun et al., 2016).", "startOffset": 186, "endOffset": 206}, {"referenceID": 26, "context": "2 (Nguyen, 2015; Mityagin, 2015) If f : R \u2192 R is a real analytic function which is not identically zero then the set {x \u2208 R | f(x) = 0} has Lebesgue measure zero.", "startOffset": 2, "endOffset": 32}, {"referenceID": 24, "context": "2 (Nguyen, 2015; Mityagin, 2015) If f : R \u2192 R is a real analytic function which is not identically zero then the set {x \u2208 R | f(x) = 0} has Lebesgue measure zero.", "startOffset": 2, "endOffset": 32}, {"referenceID": 22, "context": "(Marsden, 1974).", "startOffset": 0, "endOffset": 15}, {"referenceID": 36, "context": "As modern neural networks are expressive enough to represent any function, see (Zhang et al., 2017) for an interesting discussion on this, one can expect that in some layer the training data becomes linearly separable.", "startOffset": 79, "endOffset": 99}, {"referenceID": 10, "context": "3 recovers the similar result of (Gori & Tesi, 1992; Frasconi et al., 1997) for onehidden layer networks.", "startOffset": 33, "endOffset": 75}, {"referenceID": 3, "context": "340 (Barber, 2012).", "startOffset": 4, "endOffset": 18}], "year": 2017, "abstractText": "While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.", "creator": "LaTeX with hyperref package"}}}