{"id": "1704.08224", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "Punny Captions: Witty Wordplay in Image Descriptions", "abstract": "wit is a quintessential term celebrating rich socio - human appreciation, particularly is often grounded in geographically specific situation ( e. o., spoken comment in relates to that event ). in this work, we attempt to build computational models that can produce witty descriptions for a given image. inspired by a cognitive account of humor appreciation, we employ semantic wordplay, specifically puns. we compare our approach against meaningful baseline approaches via human studies. in a turing test style evaluation, people find our model's description for an image to be wittier than a human'en witty description 55 % of the time!", "histories": [["v1", "Wed, 26 Apr 2017 17:22:53 GMT  (7835kb,D)", "http://arxiv.org/abs/1704.08224v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["arjun chandrasekaran", "devi parikh", "mohit bansal"], "accepted": false, "id": "1704.08224"}, "pdf": {"name": "1704.08224.pdf", "metadata": {"source": "CRF", "title": "Punny Captions: Witty Wordplay in Image Descriptions", "authors": ["Arjun Chandrasekaran", "Devi Parikh", "Mohit Bansal"], "emails": ["parikh}@gatech.edu", "mbansal@cs.unc.edu"], "sections": [{"heading": "1 Introduction", "text": "\u201cWit is the sudden marriage of ideas which before their union were not perceived to have any relation.\u201d\n\u2013 Mark Twain\nWit is integral to inter-human interactions. Witty remarks are often contextual, i.e., grounded in a specific situation. Developing computational models that can understand and emulate subtleties of human expression such as contextual humor, is an important step towards making human-AI interaction more natural and more engaging (Yu et al., 2016). For instance, witty chatbots could help relieve stress and increase user engagement by being more personable, human-like, and trustworthy. Bots could automatically post witty comments (or suggest witty remarks) in response to a friend\u2019s post on social media, chat, or messaging. In this work, we attempt to tackle the challenging task\n\u2217Part of this work was done when AC was an intern with MB at TTI-Chicago and a student at Virginia Tech.\nof producing a witty (pun-based) remark about a given (possibly boring) image. Our approach is inspired by Suls (1972)\u2019s two-stage cognitive account of humor appreciation. According to Suls, a perceiver experiences humor when a stimulus1 causes an incongruity, followed by resolution. We attempt to introduce an incongruity by using an unexpected word (pun) in the description of the image. Consider the words (poll, knight) used in descriptions in Fig. 1a. The expectations of a perceiver regarding the image (night, traffic light, street, etc.) are disconfirmed by the description, which mentions \u2018a poll on the city street\u2019. The incongruity is resolved when the perceiver reinterprets the image description with the original pun word associated with the image (e.g., pole, night, in Fig. 1a). This incongruity followed by resolu-\n1The stimulus can be a joke or a captioned cartoon.\nar X\niv :1\n70 4.\n08 22\n4v 1\n[ cs\n.C L\n] 2\n6 A\npr 2\n01 7\ntion may be perceived as witty2. We build two computational models based on our approach to produce witty descriptions for an image. The first model generates witty descriptions for an image by modifying an image captioning model to include the specified pun word in the description during inference. The second model retrieves sentences that are relevant to the image, and also contain a pun, from a large corpus of stories (Zhu et al., 2015). We evaluate the wittiness of these image descriptions via human studies. We compare the top-ranked captions from our models against 3 meaningful, qualitatively different baselines. In a Turing-test style evaluation, we compare head-to-head our top-ranked generated description for an image against a humanwritten witty description that also uses puns.\nOur paper makes the following contributions: To the best of our knowledge, this is the first work that tackles the challenging problem of producing a witty natural language remark in an everyday (boring) context. We present two novel models to produce witty (pun-based) captions for a novel (likely boring) image. Our models rely on linguistic wordplay. They use an unexpected pun in an image description during inference/retrieval. Thus, they do not require to be trained with witty captions. Humans vote the descriptions from the top-ranked generated captions \u2018wittier\u2019 than a description decoded via regular inference, a humanwitty caption that is mismatched for the given image, and a \u2018punny\u2019 description that is ambiguous, and plausibly witty (see Sec 4). In a Turing teststyle evaluation, our model\u2019s best description for an image is found to be wittier than witty humanwritten caption 55% of the time3."}, {"heading": "2 Related Work", "text": "Theories of verbal humor. General theory of verbal humor (Attardo and Raskin, 1991) characterizes linguistic stimuli that induce humor. As Binsted (1996) notes, however, implementing computational models of this theory requires severely restricting its assumptions. Puns. A few works have studied the mechanisms due to which puns induce humor. Pepicello and Green (1984) categorize riddles based on the type\n2The perceiver fails to appreciate wit if the process of \u2018solving\u2019 (resolution) is trivial (the joke is obvious) or too complex (does not \u2018get\u2019 the joke).\n3Please note that in a Turing test, a machine-approach would equal a human at 50%.\nof linguistic ambiguity that they exploit \u2013 phonological, morphological or syntactic. Zwicky and Zwicky (1986) detail the difference between perfect and imperfect puns, i.e., words that are pronounced exactly the same (homophonic) and those pronounced differently (heterophonic). Miller and Gurevych (2015) and Miller (2016) develop methods to better detect and identify puns. Generating textual humor. JAPE (Binsted and Ritchie, 1997) is a pun-based riddle generating program. Similar to our work, they leverage phonological ambiguity. While our task involves producing free-form responses to a novel stimulus, JAPE only produces stand-alone \u201ccanned\u201d jokes. Also similar to our work is HAHAcronym (Stock and Strapparava, 2005), which generates a funny expansion for a given stimulus (acronym). Unlike our work, HAHAcronym is constrained to a single modality (text), and is limited to producing sets of words. In comparison, our approach is applicable to situational humor in real world human interactions since we generate free form naturallanguage sentences in response to visual stimuli. Petrovic and Matthews (2013) develop an unsupervised model that produces \u201cI like my X like I like my Y, Z\u201d jokes. Generating multi-modal humor. Wang and Wen (2015) predict a meme\u2019s text based on a given funny image. Similarly Shahaf et al. (2015) and Radev et al. (2015) learn to rank cartoon captions based on their funniness. Unlike the typically boring context (images) in our task, memes and cartoons involve a context (images) that are already funny or atypical4. Chandrasekaran et al. (2016) modify an abstract scene to make it more funny. Their task is restricted to altering the input (visual) modality, while our task is to generate witty natural language remarks for a novel image."}, {"heading": "3 Approach", "text": "The lack of large corpora of witty remarks and the relationship of wit with surprise, add to the challenge of learning to be witty via purely data driven methods. Our approach employs linguistic wordplay to overcome some of these challenges. We now describe our pipeline and models in detail. Tags. The first step towards producing a contextually witty remark is to identify concepts that are\n4E.g., \u201cLOL-cats\u201d (involving funny cat photos), \u201cBiebermemes\u201d (involving modified pictures of a Justin Bieber) and abstract cartoons that involve talking animals.\nrelevant to the context (input image). In some cases, an image is directly associated with relevant concepts, e.g., tags posted on social media. We consider the general case where such tags are unavailable, and automatically extract tags associated with an image. First, we recognize objects in the image using an image classifier. We utilize the top K5 predictions from a state-of-theart Inception-ResNet-v2 model trained for image classification on ImageNet (Deng et al., 2009).\nSecond, we consider the words from a (boring) description of an image, generated by the Show-and-Tell (Vinyals et al., 2016) image captioning model. The architecture uses an Inceptionv3 CNN encoder (Szegedy et al., 2016) and LSTM decoder, and is trained on the COCO dataset (Lin et al., 2014). As shown in Fig. 2, we combine object labels from the classifier with words from the caption (ignoring stopwords) to produce a set of tags associated with an image. Puns. We construct a list of heterographic homopohones by mining the web. To increase coverage, we use a model from automatic speech recognition research (Jyothi and Livescu, 2014) that predicts the edit distance between two words based on articulatory features. We consider only pairs of words with an edit distance of 0 and manually eliminate false positives. Our list of puns has a total of 1067 unique words (931 from the web and 136 from the speech recognition model). Pun vocabulary. We utilize our pun list to filter puns for a given image, i.e., we identify words associated with an image that have phonologically identical counterparts. The result is a pun vocabu-\n5In our experiments, we use K = 5.\nlary for each image (see Fig. 2). Generating punny image captions. Based on an image and its pun vocabulary6, we generate witty descriptions (shown in gray in Fig. 2). We use the Show-and-Tell architecture (described above), which decodes the words of a caption conditioned on an image. At specific time-steps during inference (shown in orange in Fig. 2), we force the model to produce a phonological counterpart of a pun word that is associated with the image (in our example, \u2018sell\u2019 or \u2018sighed\u2019). We achieve this by limiting the vocabulary of the decoder to contain only the counterparts of the image-puns at that time-step. At time-steps that follow, the decoder produces a new word, conditioned on all previously decoded words. Thus, the decoder attempts to produce sentences that flow well based on previously uttered words. A downside to this is that introducing puns at later time-steps results in less grammatical sentences. We overcome this by training two models that decode an image description in forward (start to end) and reverse (end to start) directions, depicted as \u2018fRNN\u2019 and \u2018rRNN\u2019 in Fig. 2 respectively. The forward RNN and reverse RNN generate sentences in which the pun appears in each of the first T and last T positions, respectively7. This results in a pool of candidate witty captions. In Fig. 2, a pun is chosen to be decoded in the 2nd and 4th time-steps during inference of the forward and reverse RNN respectively. Retrieving punny image captions. We attempt to leverage the usage of natural, human-written sentences which are relevant (yet unexpected) in the\n6On average, we find that 2 in 5 images have puns associated with them, i.e., have non-empty Pun vocabularies.\n7In our experiments we choose T=5 and a beam size of 6.\ngiven context. Concretely, we attempt to retrieve natural language sentences from a combination of the Book Corpus (Zhu et al., 2015) and corpora from the NLTK toolkit (Loper and Bird, 2002). We have two constraints on the retrieved sentence. First, to introduce an incongruity, the retrieved sentence must contain the counterpart of the pun word that is associated with the image. Second, to ensure contextual relevance, the retrieved sentence must have support in the image, i.e., it must contain at least one image tag. This results in a pool of candidate captions that are perfectly grammatical, a little unexpected, yet relevant to the given context (image). Ranking. We rank captions in the pools of candidates from both models (generation and retrieval), according to log-probability score from the image captioning model. This results in top-ranked descriptions that are both relevant to the image and grammatically correct. We then perform nonmaximal suppression, i.e., eliminate captions that are similar8 to a higher-ranked caption to reduce the pool to a smaller, more diverse set. The top 3 ranked captions from each model are the \u2018best\u2019 captions. The generation model produces better descriptions among our two models. Data. We produce witty descriptions for images from the validation set of COCO that have puns associated with them. We evaluate them via human studies on a random subset of 100 images."}, {"heading": "4 Experiments", "text": "Baselines. We compare the wittiness of descriptions generated by our model against 3 qualitatively different baselines. Regular inference generates a fluent caption relevant to the image, but is not attempting to be witty. Witty mismatch is a human-written witty caption, but for a different image from the one being evaluated. This baseline results in a witty caption, but does not attempt to be relevant to the image. Ambiguous is a \u2018punny\u2019 caption where a pun word in the boring (regular) caption is replaced by its counterpart. This caption is likely to contain content that is relevant to the image but it also contains a pun that is not relevant to the image or to the rest of the caption. Annotations. We asked people on AMT to vote for the wittier among the a given pair of de-\n8Two sentences are similar if the cosine similarity between the average of the Word2Vec (Mikolov et al., 2013) representations of words in each sentence is \u2265 0.8.\nscriptions for an image. We compared head-tohead, each of the 4 output captions from our models (3 high scoring, 1 low scoring, described in Sec. 3), against each of the 3 baseline captions. We choose the majority among 9 votes for each relative choice. Metric. In Fig. 3, we report performance of our model using the Recall@K metric. For each K = 1, 2, 3, we compute the percentage of images for which at least one of the K \u2018best\u2019 descriptions from our model outperformed a baseline. As described earlier, our \u2018best\u2019 captions are the top 3 captions, sorted by their log. probability according to our image captioning model. Quantitative results. As we see in Fig. 3, the image descriptions from our generation approach are voted wittier than all baselines (>50%) even at K = 1. As K increases, the recall steadily increases. This indicates that generated captions are witty in the context of the image, and are wittier than a naive approach that introduces ambiguity. The retrieved captions on the other hand are neither witty nor relevant for the image \u2013 they are less witty than Regular inference and Ambiguous descriptions. Further, in a head-to-head comparison, we find the generated captions to be wittier than the retrieved captions (see Fig. 3).\nWe also validate our choice of ranking captions based on the image captioning model score. We observe that a \u2018bad\u2019 caption, i.e., one that is ranked lower by our model, is significantly less witty than the top 3 output captions. Human-written witty captions. We ask people on AMT to write a witty description for an image\nusing a (given) pun associated with it. We then ask a different set of people to compare this description against the top ranked description from our model, in a Turing-test style evaluation. De-\nscriptions from our model are wittier than humans 55% of the time.\nQualitative analysis. We observe that the generation model uses interesting \u2018techniques\u2019 to pro-\nduce witty captions. For instance, in Fig. 4a, it employs alliteration using the original pun (bear) and its counterpart (bare). In another example, the caption in Fig 4b makes sense for both the original pun associated with the image (board), and its phonological counterpart (bored). In a few cases, such as in Fig. 4f, the model naively replaces a pun associated with the image (meat) with its counterpart (meet) in a description.\nThe retrieved sentence often contains words or phrases that are irrelevant to the context of the image, as we see in Fig. 4b. This is a likely reason for why a retrieved sentence containing a pun is perceived as less witty when compared with witty descriptions generated for the image."}, {"heading": "5 Discussion", "text": "Since wit involves unexpectedness, the objective of describing an image in a witty manner often results in a trade-off between the description being witty and the description being relevant to the image. It may be interesting to study how the perceived wittiness of an image description varies as it includes more creative elements and becomes less relevant to the image. Another interesting factor that can influence perceived humor is presentation. For instance, the text in cartoons and memes are funny in their characteristic, informal font but may seem boring in other, more \u201cserious\u201d font.\nProducing a description for an image that is perceived as witty is challenging because the description must achieve the fine balance between lending itself to easy resolution by the perceiver while not being impossible or too trivial. There are other challenges, however. For instance, automatic image recognition and captioning models, despite the great strides of advancement in recent times, are still imperfect. In our approach, these are cascading sources of error which could adversely affect the perceived wittiness of an image description.\nIn this work, we only consider the use of words that are perfect puns. Future work can extend our approach to explore the use of phrase-based and imperfect puns to create alternate interpretations of a sentence.\nOur approach has no constraints on the modality of the input stimulus. It can be extended to generate witty responses to input stimuli of different modalities, e.g., text (to generate witty dialogue) or video (to generate witty video-descriptions)."}, {"heading": "6 Conclusion", "text": "We presented novel computational models to address the challenging task of producing contextually witty descriptions for a given image. We leverage linguistic wordplay, specifically puns in both retrieval and generation style models. We evaluate our models against meaningful baseline approaches via human studies. In a Turing test style evaluation, annotators find image descriptions from our generation model to be wittier than a human\u2019s witty description 55% of the time!"}, {"heading": "Acknowledgements", "text": "We thank Shubham Toshniwal for his help with the automatic speech recognition model. This work was supported in part by: NSF CAREER, ARO YIP, Google GFRA, ARL grant W911NF15-2-0080, ONR grant N00014-16-1-2713, ONR YIP, Paul G. Allen Family Foundation award, and a Sloan Fellowship to DP; and NVIDIA GPU donations, Google GFRA, IBM Faculty Award, and Bloomberg Data Science Research Grant to MB."}], "references": [{"title": "Script theory revis (it) ed: Joke similarity and joke representation model", "author": ["Salvatore Attardo", "Victor Raskin."], "venue": "Humor-International Journal of Humor Research 4(3-4):293\u2013348.", "citeRegEx": "Attardo and Raskin.,? 1991", "shortCiteRegEx": "Attardo and Raskin.", "year": 1991}, {"title": "Machine humour: An implemented model of puns", "author": ["Kim Binsted."], "venue": "PhD Thesis, University of Edinburgh .", "citeRegEx": "Binsted.,? 1996", "shortCiteRegEx": "Binsted.", "year": 1996}, {"title": "Computational rules for generating punning riddles", "author": ["Kim Binsted", "Graeme Ritchie."], "venue": "Humor: International Journal of Humor Research .", "citeRegEx": "Binsted and Ritchie.,? 1997", "shortCiteRegEx": "Binsted and Ritchie.", "year": 1997}, {"title": "We are humor beings: Understanding and predicting visual humor", "author": ["Arjun Chandrasekaran", "Ashwin Kalyan", "Stanislaw Antol", "Mohit Bansal", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "CVPR.", "citeRegEx": "Chandrasekaran et al\\.,? 2016", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei."], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, pages 248\u2013255.", "citeRegEx": "Deng et al\\.,? 2009", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Revisiting word neighborhoods for speech recognition", "author": ["Preethi Jyothi", "Karen Livescu."], "venue": "ACL 2014 page 1.", "citeRegEx": "Jyothi and Livescu.,? 2014", "shortCiteRegEx": "Jyothi and Livescu.", "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "European Conference on Computer Vision. Springer, pages 740\u2013755.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Nltk: The natural language toolkit", "author": ["Edward Loper", "Steven Bird."], "venue": "Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics - Volume 1. Association for", "citeRegEx": "Loper and Bird.,? 2002", "shortCiteRegEx": "Loper and Bird.", "year": 2002}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Adjusting Sense Representations for Word Sense Disambiguation and Automatic Pun Interpretation", "author": ["Tristan Miller."], "venue": "Ph.D. thesis, tuprints.", "citeRegEx": "Miller.,? 2016", "shortCiteRegEx": "Miller.", "year": 2016}, {"title": "Automatic disambiguation of english puns", "author": ["Tristan Miller", "Iryna Gurevych."], "venue": "ACL (1). pages 719\u2013729.", "citeRegEx": "Miller and Gurevych.,? 2015", "shortCiteRegEx": "Miller and Gurevych.", "year": 2015}, {"title": "Language of riddles: new perspectives", "author": ["William J Pepicello", "Thomas A Green."], "venue": "The Ohio State University Press.", "citeRegEx": "Pepicello and Green.,? 1984", "shortCiteRegEx": "Pepicello and Green.", "year": 1984}, {"title": "Unsupervised joke generation from big data", "author": ["Sasa Petrovic", "David Matthews."], "venue": "ACL.", "citeRegEx": "Petrovic and Matthews.,? 2013", "shortCiteRegEx": "Petrovic and Matthews.", "year": 2013}, {"title": "Humor in collective discourse: Unsupervised funniness detection", "author": ["Dragomir Radev", "Amanda Stent", "Joel Tetreault", "Aasish Pappu", "Aikaterini Iliakopoulou", "Agustin Chanfreau", "Paloma de Juan", "Jordi Vallmitjana", "Alejandro Jaimes", "Rahul Jha"], "venue": null, "citeRegEx": "Radev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2015}, {"title": "Inside jokes: Identifying humorous cartoon captions", "author": ["Dafna Shahaf", "Eric Horvitz", "Robert Mankoff."], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, pages 1065\u20131074.", "citeRegEx": "Shahaf et al\\.,? 2015", "shortCiteRegEx": "Shahaf et al\\.", "year": 2015}, {"title": "HAHAcronym: A computational humor system", "author": ["Oliviero Stock", "Carlo Strapparava."], "venue": "ACL.", "citeRegEx": "Stock and Strapparava.,? 2005", "shortCiteRegEx": "Stock and Strapparava.", "year": 2005}, {"title": "A two-stage model for the appreciation of jokes and cartoons: An informationprocessing analysis", "author": ["Jerry M Suls."], "venue": "The Psychology of Humor: Theoretical Perspectives and Empirical Issues .", "citeRegEx": "Suls.,? 1972", "shortCiteRegEx": "Suls.", "year": 1972}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 2818\u20132826.", "citeRegEx": "Szegedy et al\\.,? 2016", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "IEEE transactions on pattern analysis and machine intelligence .", "citeRegEx": "Vinyals et al\\.,? 2016", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "I can has cheezburger? a nonparanormal approach", "author": ["William Yang Wang", "Miaomiao Wen"], "venue": null, "citeRegEx": "Wang and Wen.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Wen.", "year": 2015}, {"title": "A wizard-of-oz study on a non-task-oriented dialog systems that reacts to user engagement", "author": ["Zhou Yu", "Leah Nicolich-Henkin", "Alan W Black", "Alex I Rudnicky."], "venue": "17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. page 55.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Proceedings of the IEEE In-", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}, {"title": "Imperfect puns, markedness, and phonological similarity: With fronds like these, who needs anemones", "author": ["Arnold Zwicky", "Elizabeth Zwicky."], "venue": "Folia Linguistica 20(3-4):493\u2013503.", "citeRegEx": "Zwicky and Zwicky.,? 1986", "shortCiteRegEx": "Zwicky and Zwicky.", "year": 1986}], "referenceMentions": [{"referenceID": 20, "context": "Developing computational models that can understand and emulate subtleties of human expression such as contextual humor, is an important step towards making human-AI interaction more natural and more engaging (Yu et al., 2016).", "startOffset": 209, "endOffset": 226}, {"referenceID": 16, "context": "Our approach is inspired by Suls (1972)\u2019s two-stage cognitive account of humor appreciation.", "startOffset": 28, "endOffset": 40}, {"referenceID": 21, "context": "The second model retrieves sentences that are relevant to the image, and also contain a pun, from a large corpus of stories (Zhu et al., 2015).", "startOffset": 124, "endOffset": 142}, {"referenceID": 0, "context": "General theory of verbal humor (Attardo and Raskin, 1991) characterizes linguistic stimuli that induce humor.", "startOffset": 31, "endOffset": 57}, {"referenceID": 0, "context": "General theory of verbal humor (Attardo and Raskin, 1991) characterizes linguistic stimuli that induce humor. As Binsted (1996) notes, however, implementing computational models of this theory requires severely restricting its assumptions.", "startOffset": 32, "endOffset": 128}, {"referenceID": 0, "context": "General theory of verbal humor (Attardo and Raskin, 1991) characterizes linguistic stimuli that induce humor. As Binsted (1996) notes, however, implementing computational models of this theory requires severely restricting its assumptions. Puns. A few works have studied the mechanisms due to which puns induce humor. Pepicello and Green (1984) categorize riddles based on the type", "startOffset": 32, "endOffset": 345}, {"referenceID": 20, "context": "Zwicky and Zwicky (1986) detail the difference between perfect and imperfect puns, i.", "startOffset": 0, "endOffset": 25}, {"referenceID": 9, "context": "Miller and Gurevych (2015) and Miller (2016) develop methods to better detect and identify puns.", "startOffset": 0, "endOffset": 27}, {"referenceID": 9, "context": "Miller and Gurevych (2015) and Miller (2016) develop methods to better detect and identify puns.", "startOffset": 0, "endOffset": 45}, {"referenceID": 2, "context": "JAPE (Binsted and Ritchie, 1997) is a pun-based riddle generating program.", "startOffset": 5, "endOffset": 32}, {"referenceID": 15, "context": "Also similar to our work is HAHAcronym (Stock and Strapparava, 2005), which generates a funny expansion for a given stimulus (acronym).", "startOffset": 39, "endOffset": 68}, {"referenceID": 1, "context": "JAPE (Binsted and Ritchie, 1997) is a pun-based riddle generating program. Similar to our work, they leverage phonological ambiguity. While our task involves producing free-form responses to a novel stimulus, JAPE only produces stand-alone \u201ccanned\u201d jokes. Also similar to our work is HAHAcronym (Stock and Strapparava, 2005), which generates a funny expansion for a given stimulus (acronym). Unlike our work, HAHAcronym is constrained to a single modality (text), and is limited to producing sets of words. In comparison, our approach is applicable to situational humor in real world human interactions since we generate free form naturallanguage sentences in response to visual stimuli. Petrovic and Matthews (2013) develop an unsupervised model that produces \u201cI like my X like I like my Y, Z\u201d jokes.", "startOffset": 6, "endOffset": 717}, {"referenceID": 16, "context": "Wang and Wen (2015) predict a meme\u2019s text based on a given funny image.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Similarly Shahaf et al. (2015) and Radev et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 12, "context": "(2015) and Radev et al. (2015) learn to rank cartoon captions based on their funniness.", "startOffset": 11, "endOffset": 31}, {"referenceID": 3, "context": "Chandrasekaran et al. (2016) modify an abstract scene to make it more funny.", "startOffset": 0, "endOffset": 29}, {"referenceID": 4, "context": "We utilize the top K5 predictions from a state-of-theart Inception-ResNet-v2 model trained for image classification on ImageNet (Deng et al., 2009).", "startOffset": 128, "endOffset": 147}, {"referenceID": 18, "context": "Second, we consider the words from a (boring) description of an image, generated by the Show-and-Tell (Vinyals et al., 2016) image captioning model.", "startOffset": 102, "endOffset": 124}, {"referenceID": 17, "context": "The architecture uses an Inceptionv3 CNN encoder (Szegedy et al., 2016) and LSTM decoder, and is trained on the COCO dataset (Lin et al.", "startOffset": 49, "endOffset": 71}, {"referenceID": 6, "context": ", 2016) and LSTM decoder, and is trained on the COCO dataset (Lin et al., 2014).", "startOffset": 61, "endOffset": 79}, {"referenceID": 5, "context": "To increase coverage, we use a model from automatic speech recognition research (Jyothi and Livescu, 2014) that predicts the edit distance between two words based on articulatory features.", "startOffset": 80, "endOffset": 106}, {"referenceID": 21, "context": "Concretely, we attempt to retrieve natural language sentences from a combination of the Book Corpus (Zhu et al., 2015) and corpora from the NLTK toolkit (Loper and Bird, 2002).", "startOffset": 100, "endOffset": 118}, {"referenceID": 7, "context": ", 2015) and corpora from the NLTK toolkit (Loper and Bird, 2002).", "startOffset": 42, "endOffset": 64}, {"referenceID": 8, "context": "Two sentences are similar if the cosine similarity between the average of the Word2Vec (Mikolov et al., 2013) representations of words in each sentence is \u2265 0.", "startOffset": 87, "endOffset": 109}], "year": 2017, "abstractText": "Wit is a quintessential form of rich interhuman interaction, and is often grounded in a specific situation (e.g., a comment in response to an event). In this work, we attempt to build computational models that can produce witty descriptions for a given image. Inspired by a cognitive account of humor appreciation, we employ linguistic wordplay, specifically puns. We compare our approach against meaningful baseline approaches via human studies. In a Turing test style evaluation, people find our model\u2019s description for an image to be wittier than a human\u2019s witty description 55% of the time!", "creator": "LaTeX with hyperref package"}}}