{"id": "1705.01197", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "Analyzing Knowledge Transfer in Deep Q-Networks for Autonomously Handling Multiple Intersections", "abstract": "we analyze how storing knowledge to autonomously handle one code of intersection, styled as euclidean deep q - network, translates to other types of intersections ( tasks ). we view intersection handling as a deep reinforcement learning problem, which approximates the choice of q function as a deep neural network. using random traffic simulator, we show that directly copying a network trained for one type of intersection to another type of intersection decreases one success rate. we also show that when a network that is pre - trained on task a and then is fine - regulated on a task b, the resulting network not only performs better on the task b than an network exclusively trained on task a, but also retained knowledge performing the task a. finally, we examine a neural loop setting, where we train a single network on five core types of intersections sequentially and showing that the resulting tangle exhibited subsequent forgetting of knowledge via previous tasks. previous result suggests a need for a long - term memory component to correct knowledge.", "histories": [["v1", "Tue, 2 May 2017 23:05:56 GMT  (3281kb,D)", "http://arxiv.org/abs/1705.01197v1", "Submitted to IEEE International Conference on Intelligent Transportation Systems (ITSC 2017)"]], "COMMENTS": "Submitted to IEEE International Conference on Intelligent Transportation Systems (ITSC 2017)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["david isele", "akansel cosgun", "kikuo fujimura"], "accepted": false, "id": "1705.01197"}, "pdf": {"name": "1705.01197.pdf", "metadata": {"source": "CRF", "title": "Analyzing Knowledge Transfer in Deep Q-Networks for Autonomously Handling Multiple Intersections", "authors": ["David Isele", "Akansel Cosgun", "Kikuo Fujimura"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nCar companies has been increasing their R&D spending on Automated Driving (AD) technology in recent years, for good cause: AD promises to greatly reduce accidentrelated fatalities and increase productivity of the society as a whole. Although AD technology has made important strides over the last couple of years, current technology is still not ready for large scale roll-out. Urban environments especially pose significant challenges for AD, due to the unpredictable nature of pedestrians and vehicles in city traffic. Handling intersections safely and efficiently is one of the most challenging problems for Urban AD.\nRule-based methods provide a predictable method to handle intersections. However, rule-based intersection handling approaches don\u2019t scale well because it becomes increasingly harder to design hand-crafted rules as scene complexity increases. Moreover, the algorithm designer has to come up with hand-crafted rules and parameters for different types of intersections. By different intersection types, we mean single or multi-lane right, left turns and forward passing.\nOur goal for this research is to explore a machine learning based method that generalizes to various types of intersections. Machine learning, and particularly deep learning is a growing field that had tremendous impact on applications such as computer vision, speech recognition and language translation and it is increasingly being used for decision making. We model the AD vehicle as a learning agent, which learns from positive (successful passing) and negative experiences (collisions) in a reinforcement learning framework.\n1David Isele is with The University of Pennsylvania, Philadelphia, PA, USA 2Akansel Cosgun and Kikuo Fujimura are with the Honda Research Institute, Mountain View, CA, USA\nIn this paper we focus on how the knowledge for one type of intersection, represented as a Deep Q-Network (DQN), translates to other types of intersections (tasks). First we look at direct copy: how well a network trained for Task A performs in Task B. Second, we analyze how the performance of a network initialized from Task A and fine tuned in Task B compares to a randomly initialized network exclusively trained on Task B. Third, we investigate reverse transfer: if a network pre-trained for Task A and fine-tuned to Task B, preserves knowledge for Task A. Finally, we explore training a network for five tasks sequentially as a lifelong learning scenario.\nThis paper is organized as follows. After providing a brief literature survey in Section II, we present the problem formulation as a DQN in Section III, before examining various knowledge sharing strategies in Section IV. After explaining the experimental setup in Section V, then present our results in Section VI before concluding in Section VII.\nar X\niv :1\n70 5.\n01 19\n7v 1\n[ cs\n.L G\n] 2\nM ay\n2 01\n7"}, {"heading": "II. RELATED WORK", "text": "Recently there has been an increased interest in using machine learning techniques to control autonomous vehicles. In imitation learning, the policy is learned from a human driver [1]. Online planners based on partially observable Monte Carlo Planning (POMCP) have been shown to handle intersections [2] if the existence of an accurate generative model is available, and Markov Decision Processes (MDP) have been used offline to address the intersection problem [3], [4]. Additionally, machine learning has been used to optimize comfort from a set of safe trajectories [5].\nMachine learning has greatly benefited from training on large amounts of data. This helps a system learn general representations and prevents over fitting based on incidental correlations in the sampled data. In the absence of huge datasets, training on multiple related tasks can give similar improvement gains [6]. A large breadth of research has investigated transferring knowledge from one system to another both in machine learning in general [7], and reinforcement learning specifically [8].\nThe training time and sample complexity of deep networks make transfer methods particularly appealing [9], and has prompted in depth investigation to help understand its behavior [10]. Recent work in deep reinforcement learning has looked at combining networks from different tasks to share information [11], [12]. And efforts have been made to enable a unified framework for learning multiple tasks through changes in architecture design [13] and modified objective functions [14] to address known problems like catastrophic forgetting [15].\nIII. INTERSECTION HANDLING USING DEEP Q-NETWORKS\nWe view intersection handling as a reinforcement learning problem, and use a Deep Q-Network (DQN) to learn the state action value Q-function. We assume the AD vehicle is at the intersection, the path is known to it, and the network is tasked with choosing between two actions: wait or go, for every time step. Once the agent decides to go, it follows an intelligent driver model for keeping distance with the vehicles in front."}, {"heading": "A. Reinforcement Learning", "text": "In reinforcement learning, an agent in state s takes an action a according to the policy \u03c0\u03b8 parameterized by \u03b8 . The agent transitions to the state s\u2032, and receives a reward r. This collection is defined as an experience e = (s,a,r,s\u2032).\nThis is typically formulated as a Markov Decision Process (MDP) \u3008S,A,P,R,\u03b3\u3009, where S is the set of states, A is the set of actions that the agent may execute, P : S\u00d7A\u2192 S is the state transition function, R : S\u00d7A\u00d7S\u2192R is the reward function, and \u03b3 \u2208 (0,1] is a discount factor that adds preference for earlier rewards and provides stability in the case of infinite time horizons. MDPs follow the Markov assumption that the probability of transitioning to a new state given the current state and action is independent of all previous states and actions p(st+1|st ,at , . . . ,s0,a0) = p(st+1|st ,at).\nThe goal at any time step t is to maximize the future discounted return Rt = \u2211Tk=t \u03b3k\u2212trk. In order to optimize the expected return we use Q-learning [16]."}, {"heading": "B. Q-learning", "text": "Q-learning defines an optimal action-value function Q\u2217(s,a) as the maximum expected return that is achievable following any policy given a state s and action a, Q\u2217(s,a) = max\u03c0E[Rt |st = s,at = a,\u03c0].\nThis follows the dynamic programming properties of the Bellman equation, which state that if the values Q\u2217(s\u2032,a\u2032) are known for all a\u2032 then the optimal strategy is to select a\u2032 that maximizes the expected value of r+ \u03b3Q\u2217(s\u2032,a\u2032):\nQ\u2217(s,a) = E[r+ \u03b3 max a\u2032 Q\u2217(s\u2032,a\u2032)|s,a] . (1)\nIn Deep Q-learning [17], the optimal value function is approximated with a neural network Q\u2217(s,a) \u2248 Q(s,a;\u03b8). The parameters \u03b8 are learned by using the Bellman equation as an iterative update Qi+1(s,a) =E[r+\u03b3 maxa\u2032Qi(s\u2032,a\u2032)|s,a] and minimizing the error between the expected return and the state-action value predicted by the network. This gives the loss for an individual experience in a deep Q-network (DQN)\nL(ei,\u03b8) = (\nri + \u03b3 max a\u2032i\nQ(s\u2032i,a \u2032 i;\u03b8)\u2212Q(si,ai;\u03b8)\n)2 . (2)\nIn practice, Q(st+1,at+1;\u03b8) is a poor estimate early on, which can make learning slow since many updates are required to propagate the reward to the appropriate preceding states and actions. One way to make learning more efficient is to use n-step return[18] E[Rt |st = s,a]\u2248 rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7+ \u03b3n\u22121rt+n\u22121 + \u03b3n maxat+n Q(st+n,at+n;\u03b8).\nDuring learning, an \u03b5-greedy policy is followed by selecting a random action with probability \u03b5 to promote exploration and otherwise greedily selecting the best action maxaQ(s,a;\u03b8) according to the current network. In order to improve the effectiveness of the random exploration we make use of dynamic frame skipping. Frequently the same repeated actions is required over several time steps. It was recently shown that allowing an agent to select actions over extended time periods improves the learning time of an agent [19]. For example, rather than having to explore through trial and error and build up over a series of learning steps that eight time steps is the appropriate amount of time an agent should wait for a car to pass, the agent need only discover that a \u201dwait eight steps\u201d action is appropriate. Dynamic frame skipping can viewed as a simplified version of options [20] which is recently starting to be explored by the Deep RL community. [21], [22], [23]."}, {"heading": "C. Deep Neural Network setup", "text": "The DQN uses a convolutional neural network with two convolution layers, and one fully connected layer. The first convolutional layer has 32 6\u00d7 6 filters with stride two, the second convolution layer has 64 3\u00d7 3 filters with stride 2. The fully connected layer has 100 nodes. All layers use leaky ReLU activation functions [24]. The final linear output layer has five outputs: a single go action, and a wait action at\nfour time scales (1, 2, 4, and 8 time steps). The network is optimized using the RMSProp algorithm [25].\nOur experience replay buffers have an allotment of 1,000 experiences. At each learning iteration we samples a batch of 60 experiences. Since the experience replay buffer imposes off-policy learning, we are able to calculate the return for each state-action pair in the trajectory prior to adding each step into the replay buffer. This allows us to train directly on the n-step return and forgo the added complexity of using target networks [26].\nThe state space of the DQN is represented as a 18\u00d7 26 grid in global coordinates. The epsilon governing random exploration was 0.05. For the reward we used +1 for successfully navigating the intersection, \u22121 for a collision, and \u22120.01 step cost."}, {"heading": "IV. KNOWLEDGE TRANSFER", "text": "We are interested in sharing knowledge between different driving tasks. By sharing knowledge from different tasks we can reduce learning time and create more general and capable systems. Ideally knowledge sharing can be extended to involve a system that continues to learn after it has been deployed [27] and can enable a system to accurately predict appropriate behavior in novel situations [28]. We examine the behavior of various knowledge sharing strategies in the autonomous driving domain."}, {"heading": "A. Direct copy", "text": "To demonstrate the extent of transfer and show the difference between tasks, we train a network on a single source task for 25,000 iterations. The unmodified network is then evaluated on every other task. We repeat this process, using each different task as a source task."}, {"heading": "B. Fine tuning", "text": "Starting with a network trained for 10,000 iterations on a source task, we then fine tune a network for an additional 25,000 iterations on second target task. We use 10,000 iterations because it demonstrates substantial learning, but is suboptimal in order to emphasize the possible benefits gained from transfer. Fine tuning demonstrates the jumpstart and asymptotic performance as described by Taylor and Stone[8]."}, {"heading": "C. Reverse transfer", "text": "After a network has been fine tuned on the target task, we evaluate the performance of that network on the source task. If training on a later task improves the performance of an earlier task this is known as reverse transfer. It is known that\nneural networks often forget earlier tasks in what is called catastrophic forgetting [29], [30], [15].\nIn the case of forgetting, retention describes the amount of previous knowledge retained by the network after training on a new task. This value is difficult to define formally since it must exclude any relevant knowledge for source tasks obtained from training on the target task, and additionally retention might include aspects that are not quantifiable such of weight configurations in the network. For example a network might exhibit catastrophic forgetting but in fact have retained a weight configuration that greatly reduces the training time needed to retrain the source task. Because of the difficulty of defining retention we define the empirical retention as the difference between the direct copy and fine tuned direct copy of the same network."}, {"heading": "D. Lifelong Learning", "text": "Lifelong learning is the process of learning multiple tasks sequentially where the goal is to optimize the performance on every task [27], [31]. The combination of information from all previous tasks can be used to jumpstart learning a new task. In a reciprocal fashion, learning a new task can potentially refine existing knowledge for previous tasks. By having a single system that handles all tasks, the system is able to handle a broader set of problems and will likely generalize better to new problems.\nWe examine how a deep Q-network performs when learning a sequence of tasks. The order in which tasks are encountered does impact learning, and several groups have investigated the effects of ordering [32], [33]. For our experiments we use a task ordering that demonstrates forgetting and hold it fixed for all experiments.\nWe are interested in how each tasks performance changes over time. We test at regular intervals with testing run as a separate procedure that does not have an impact on the replay buffer or learning process of the network."}, {"heading": "V. EXPERIMENTAL SETUP", "text": "Experiments were run using the Sumo simulator [34], which is an open source traffic simulation package. This package allows users to model road networks, road signs, traffic lights, a variety of vehicles (including public transportation), and pedestrians to simulate traffic conditions in different types of scenarios. Importantly for the purpose of testing and evaluation of autonomous vehicle systems, Sumo provides tools that facilitate online interaction and vehicle control. For any traffic scenario, users can have control over a vehicle\u2019s position, velocity, acceleration, steering direction\nand can simulate motion using basic kinematics models. Traffic scenarios like multi-lane intersections can be setup by defining the road network (lanes and intersections) along with specifications that control traffic conditions. To simulate traffic, users have control over the types of vehicles, road paths, vehicle density, and departure times. Traffic cars follow IDM to control their motion. In Sumo, randomness is simulated by varying the speed distribution of the vehicles and by using parameters that control driver imperfection (based on the Krauss stochastic driving model [35]). The simulator runs based on a predefined time interval which controls the length of every step.\nWe ran experiments using five different intersection scenarios: Right, Left, Left2, Forward and a Challenge. Each of these scenarios is depicted in Figure 2. The Right scenario involves making a right turn, the Forward scenario involves crossing the intersection, the Left scenario involves making a left turn, the Left2 scenario involves making a left turn across two lanes, and the Challenge scenario involves crossing a six lane intersection.\nThe Sumo traffic simulator is configured so that each lane has a 45 miles per hour (20 m/s) max speed. The car begins from a stopped position. Each time step is equal to 0.2 seconds. The max number of steps per trial is capped 100 steps which is equivalent to 20 seconds. The traffic density is set by the probability that a vehicle will be emitted randomly per second. We use depart probability of 0.2 for each lane for all tasks.\nNavigating intersections involves multiple conflicting objectives. We evaluate four metrics in order to collect our statistics. The metrics are as follows: \u2022 Percentage of successes: the percentage of the runs the\ncar successfully reached the goal. This metric takes into both collisions and time-outs. \u2022 Percentage of collisions: a measure of the safety of the method. \u2022 Average time: how long it takes a successful trial to run to completion. \u2022 Average braking time: the amount of time other cars in the simulator are braking, this can be seen as a measure of how disruptive the autonomous car is to traffic.\nWhile there are multiple metrics, we focus on the percentage of success, which is the metric used in all our plots. All state representations ignores occlusion, assuming all cars are always visible."}, {"heading": "VI. RESULTS", "text": "Direct Copy: Table I shows the results when a network is trained on one task and applied to another. In no instance does a network trained on a different task do better than a network trained on the matching task, but we do see that several tasks achieve similar performance with transfer. Particularly we see that the single lane tasks (right,left, and forward) are related, they are consistently the top performers in all single lane tasks. Additionally the more challenging multi-lane settings (left2 and challenge) appear connected,\nthe Left2 network does substantially better than either of the single lane tasks on the Challenge task. Fine Tuning: Figure 4 shows fine tuning results. We see that in nearly all cases, pre-training with a different network gives a significant advantage in jumpstart [8] and in several cases there is an asymptotic benefit as well. When the fine tuned networks are re-applied to the source task the performance looks similar to direct copy, as shown in Figure 5. Reverse Transfer: While the performance on the source task drops after fine tuning, we see a trend of positive improvement compared to direct copy. This indicates that some information was retained by the network. Figure 3 shows the retention for each task pair, showing the percentage gain resulting from the initialization. The Left2 and Challenge tasks have less overlap with other tasks in the state space, so it is possible that more aspects of the initialization are left unchanged, which might explain why there is the\nlargest amount of retention for these tasks. This hypothesis is supported by the fact that training on the Right task exhibits the most retention, since these two tasks have the least overlap. Lifelong learning: The results for the lifelong learning experiment are shown in Figure 6. Every task initially benefits from learning on the first task (Forward), although the performance in the Left2 and Challenge settings benefit less. In some cases we see that training on a different task helps up to a point and then further training hurts other tasks. For example, after training on approximately 5000\ntrials of Forward setting, the Right task performance starts to decrease.\nOverall, we see an affinity between both the single lane tasks (Left, Right, and Forward) and the multi-lane tasks. When training on the Challenge task starts, Left2 benefits, but the single lane tasks exhibit catastrophic forgetting. Training on the Left task helps the other single lane tasks, but Challenge decreases in performance.\nHowever the results are not consistent across the grouping\nof single lane tasks. Training on the Right task has a much more detrimental effect on the multi-lane tasks than either Forward or Left. We suspect this is because right turns can ignore one of the lanes of traffic which matters to all other tasks. Overall the negative effects of catastrophic forgetting negate many of the positive effects of transfer."}, {"heading": "VII. CONCLUSION", "text": "In this paper we view the AD vehicle as a learning agent in a reinforcement learning setting, and analyze how the knowledge for handling one type of intersection, represented as a Deep Q-Network, translates to other types of intersections. We investigated and compared four different transfer methods between different intersections (tasks): direct copy, fine tuning, reverse transfer and lifelong learning. Our results have several conclusions. First, we found the success rates were consistently low when a network is trained on Task A but directly tested on Task B. Second, a network that is initialized with the network of a Task A and then finetuned on Task B generally performed better than a randomly initialized network that is trained on Task B. Third, when a network that is initialized with Task A, fine-tuned on Task B, and is tested back on Task A, it performed better than a network directly copied from Task B to Task A. Finally, we examine a lifelong learning domain, where we train a single network to handle all five intersection scenarios and show that the resulting network exhibited catastrophic forgetting of previous task knowledge.\nAs future work, we will conduct research on the concept of a long-term memory and investigate how to effectively preserve previous task knowledge for lifelong learning."}], "references": [{"title": "End to end learning for self-driving cars", "author": ["M. Bojarski", "D. Del Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang"], "venue": "arXiv preprint arXiv:1604.07316, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Belief state planning for navigating urban intersections", "author": ["M. Bouton", "A. Cosgun", "M.J. Kochenderfer"], "venue": "IEEE Intelligent Vehicles Symposium (IV), 2017.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Probabilistic decisionmaking under uncertainty for autonomous driving using continuous pomdps", "author": ["S. Brechtel", "T. Gindele", "R. Dillmann"], "venue": "Intelligent Transportation Systems (ITSC), 2014 IEEE 17th International Conference on. IEEE, 2014, pp. 392\u2013399.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Intention-aware autonomous driving decision-making in an uncontrolled intersection", "author": ["W. Song", "G. Xiong", "H. Chen"], "venue": "Mathematical Problems in Engineering, vol. 2016, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Safe, multiagent, reinforcement learning for autonomous driving", "author": ["S. Shalev-Shwartz", "S. Shammah", "A. Shashua"], "venue": "arXiv preprint arXiv:1610.03295, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Multitask Learning", "author": ["R. Caruana"], "venue": "Machine Learning, vol. 28, pp. 41\u2013 75, 1997.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "A Survey on Transfer Learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "Journal of Machine Learning Research, vol. 10, no. Jul, pp. 1633\u20131685, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "CNN Features off-the-shelf: an Astounding Baseline for Recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 512\u2013 519, Mar. 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "How transferable are features in deep neural networks ?", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Progressive neural networks", "author": ["A.A. Rusu", "N.C. Rabinowitz", "G. Desjardins", "H. Soyer", "J. Kirkpatrick", "K. Kavukcuoglu", "R. Pascanu", "R. Hadsell"], "venue": "arXiv preprint arXiv:1606.04671, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Knowledge transfer for deep reinforcement learning with hierarchical experience replay", "author": ["H. Yin", "S.J. Pan"], "venue": "AAAI Conference on Artificial Intelligence (AAAI), 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Compete to compute", "author": ["R.K. Srivastava", "J. Masci", "S. Kazerounian", "F. Gomez", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, 2013, pp. 2310\u20132318.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Overcoming catastrophic forgetting in neural networks", "author": ["J. Kirkpatrick", "R. Pascanu", "N. Rabinowitz", "J. Veness", "G. Desjardins", "A.A. Rusu", "K. Milan", "J. Quan", "T. Ramalho", "A. Grabska-Barwinska"], "venue": "arXiv preprint arXiv:1612.00796, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks", "author": ["I.J. Goodfellow", "M. Mirza", "D. Xiao", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1312.6211, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 279\u2013292, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Incremental multi-step q-learning", "author": ["J. Peng", "R.J. Williams"], "venue": "Machine learning, vol. 22, no. 1-3, pp. 283\u2013290, 1996.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Dynamic action repetition for deep reinforcement learning", "author": ["A. Srinivas", "S. Sharma", "B. Ravindran"], "venue": "AAAI Conference on Artificial Intelligence (AAAI), 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.05397, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["C. Tessler", "S. Givony", "T. Zahavy", "D.J. Mankowitz", "S. Mannor"], "venue": "arXiv preprint arXiv:1604.07255, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["T.D. Kulkarni", "K. Narasimhan", "A. Saeedi", "J. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems, 2016, pp. 3675\u20133683.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "Proc. ICML, vol. 30, no. 1, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Lecture 6.5-rmsprop, coursera: Neural networks for machine learning", "author": ["T. Tieleman", "G. Hinton"], "venue": "University of Toronto, Tech. Rep, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Is learning the n-th thing any easier than learning the first?", "author": ["S. Thrun"], "venue": "Advances in neural information processing systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1996}, {"title": "Using task features for zeroshot knowledge transfer in lifelong learning", "author": ["D. Isele", "M. Rostami", "E. Eaton"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem", "author": ["M. McCloskey", "N.J. Cohen"], "venue": "Psychology of learning and motivation, vol. 24, pp. 109\u2013165, 1989.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1989}, {"title": "Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions", "author": ["R. Ratcliff"], "venue": "Psychological review, vol. 97, no. 2, pp. 285\u2013308, 1990.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1990}, {"title": "ELLA: An efficient lifelong learning algorithm", "author": ["P. Ruvolo", "E. Eaton"], "venue": "Proceedings of the International Conference on Machine Learning, vol. 28, pp. 507\u2013515, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "International Conference on Machine Learning, pp. 41\u201348, 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Active task selection for lifelong machine learning.", "author": ["P. Ruvolo", "E. Eaton"], "venue": "AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Recent development and applications of SUMO\u2013simulation of urban mobility", "author": ["D. Krajzewicz", "J. Erdmann", "M. Behrisch", "L. Bieker"], "venue": "International Journal on Advances in Systems and Measurements (IARIA), vol. 5, no. 3\u20134, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Microscopic modeling of traffic flow: Investigation of collision free vehicle dynamics", "author": ["S. Krauss"], "venue": "Ph.D. dissertation, Deutsches Zentrum fuer Luft-und Raumfahrt, 1998.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "driver [1].", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "Online planners based on partially observable Monte Carlo Planning (POMCP) have been shown to handle intersections [2] if the existence of an accurate generative model is available, and Markov Decision Processes (MDP) have been used offline to address the intersection problem [3], [4].", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "Online planners based on partially observable Monte Carlo Planning (POMCP) have been shown to handle intersections [2] if the existence of an accurate generative model is available, and Markov Decision Processes (MDP) have been used offline to address the intersection problem [3], [4].", "startOffset": 277, "endOffset": 280}, {"referenceID": 3, "context": "Online planners based on partially observable Monte Carlo Planning (POMCP) have been shown to handle intersections [2] if the existence of an accurate generative model is available, and Markov Decision Processes (MDP) have been used offline to address the intersection problem [3], [4].", "startOffset": 282, "endOffset": 285}, {"referenceID": 4, "context": "Additionally, machine learning has been used to optimize comfort from a set of safe trajectories [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "In the absence of huge datasets, training on multiple related tasks can give similar improvement gains [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "A large breadth of research has investigated transferring knowledge from one system to another both in machine learning in general [7], and reinforcement learning specifically [8].", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "A large breadth of research has investigated transferring knowledge from one system to another both in machine learning in general [7], and reinforcement learning specifically [8].", "startOffset": 176, "endOffset": 179}, {"referenceID": 8, "context": "The training time and sample complexity of deep networks make transfer methods particularly appealing [9], and has prompted in depth investigation to help understand its behavior [10].", "startOffset": 102, "endOffset": 105}, {"referenceID": 9, "context": "The training time and sample complexity of deep networks make transfer methods particularly appealing [9], and has prompted in depth investigation to help understand its behavior [10].", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "Recent work in deep reinforcement learning has looked at combining networks from different tasks to share information [11], [12].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "Recent work in deep reinforcement learning has looked at combining networks from different tasks to share information [11], [12].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "And efforts have been made to enable a unified framework for learning multiple tasks through changes in architecture design [13] and modified objective functions [14] to address known problems like catastrophic forgetting [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "And efforts have been made to enable a unified framework for learning multiple tasks through changes in architecture design [13] and modified objective functions [14] to address known problems like catastrophic forgetting [15].", "startOffset": 162, "endOffset": 166}, {"referenceID": 14, "context": "And efforts have been made to enable a unified framework for learning multiple tasks through changes in architecture design [13] and modified objective functions [14] to address known problems like catastrophic forgetting [15].", "startOffset": 222, "endOffset": 226}, {"referenceID": 15, "context": "In order to optimize the expected return we use Q-learning [16].", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "In Deep Q-learning [17], the optimal value function is approximated with a neural network Q\u2217(s,a) \u2248 Q(s,a;\u03b8).", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "One way to make learning more efficient is to use n-step return[18] E[Rt |st = s,a]\u2248 rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7+ \u03b3rt+n\u22121 + \u03b3n maxat+n Q(st+n,at+n;\u03b8).", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "It was recently shown that allowing an agent to select actions over extended time periods improves the learning time of an agent [19].", "startOffset": 129, "endOffset": 133}, {"referenceID": 19, "context": "can viewed as a simplified version of options [20] which is recently starting to be explored by the Deep RL community.", "startOffset": 46, "endOffset": 50}, {"referenceID": 20, "context": "[21], [22], [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[21], [22], [23].", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "[21], [22], [23].", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "All layers use leaky ReLU activation functions [24].", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "The network is optimized using the RMSProp algorithm [25].", "startOffset": 53, "endOffset": 57}, {"referenceID": 25, "context": "This allows us to train directly on the n-step return and forgo the added complexity of using target networks [26].", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "Ideally knowledge sharing can be extended to involve a system that continues to learn after it has been deployed [27] and can enable a system to accurately predict appropriate behavior in novel situations [28].", "startOffset": 113, "endOffset": 117}, {"referenceID": 27, "context": "Ideally knowledge sharing can be extended to involve a system that continues to learn after it has been deployed [27] and can enable a system to accurately predict appropriate behavior in novel situations [28].", "startOffset": 205, "endOffset": 209}, {"referenceID": 7, "context": "Fine tuning demonstrates the jumpstart and asymptotic performance as described by Taylor and Stone[8].", "startOffset": 98, "endOffset": 101}, {"referenceID": 28, "context": "It is known that neural networks often forget earlier tasks in what is called catastrophic forgetting [29], [30], [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 29, "context": "It is known that neural networks often forget earlier tasks in what is called catastrophic forgetting [29], [30], [15].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "It is known that neural networks often forget earlier tasks in what is called catastrophic forgetting [29], [30], [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "Lifelong learning is the process of learning multiple tasks sequentially where the goal is to optimize the performance on every task [27], [31].", "startOffset": 133, "endOffset": 137}, {"referenceID": 30, "context": "Lifelong learning is the process of learning multiple tasks sequentially where the goal is to optimize the performance on every task [27], [31].", "startOffset": 139, "endOffset": 143}, {"referenceID": 31, "context": "investigated the effects of ordering [32], [33].", "startOffset": 37, "endOffset": 41}, {"referenceID": 32, "context": "investigated the effects of ordering [32], [33].", "startOffset": 43, "endOffset": 47}, {"referenceID": 33, "context": "Experiments were run using the Sumo simulator [34], which is an open source traffic simulation package.", "startOffset": 46, "endOffset": 50}, {"referenceID": 34, "context": "simulated by varying the speed distribution of the vehicles and by using parameters that control driver imperfection (based on the Krauss stochastic driving model [35]).", "startOffset": 163, "endOffset": 167}, {"referenceID": 7, "context": "We see that in nearly all cases, pre-training with a different network gives a significant advantage in jumpstart [8] and in several cases there is an asymptotic benefit as well.", "startOffset": 114, "endOffset": 117}], "year": 2017, "abstractText": "We analyze how the knowledge to autonomously handle one type of intersection, represented as a Deep QNetwork, translates to other types of intersections (tasks). We view intersection handling as a deep reinforcement learning problem, which approximates the state action Q function as a deep neural network. Using a traffic simulator, we show that directly copying a network trained for one type of intersection to another type of intersection decreases the success rate. We also show that when a network that is pre-trained on Task A and then is fine-tuned on a Task B, the resulting network not only performs better on the Task B than an network exclusively trained on Task A, but also retained knowledge on the Task A. Finally, we examine a lifelong learning setting, where we train a single network on five different types of intersections sequentially and show that the resulting network exhibited catastrophic forgetting of knowledge on previous tasks. This result suggests a need for a long-term memory component to preserve knowledge.", "creator": "LaTeX with hyperref package"}}}