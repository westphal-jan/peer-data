{"id": "1705.02314", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Building Morphological Chains for Agglutinative Languages", "abstract": "in this paper, we build morphological chains for agglutinative languages by utilizing a log - linear model for determining morphological segmentation task. phylogenetic model is based on the unsupervised morphological segmentation tree called morphochains. we extend as log linear model by projecting the candidate space recursively to cover more split points for agglutinative languages such as turkish, whereas in the original latter structures are generated by considering only binary segmentation of each word. parallel proceeds show that we improve the state - of - art sample scores by 12 % having a e - measure of 75 % and we improve the english scores after 3 % through a f - out of 74 %. eventually, the process outperforms onto morphochains and other well - known unsupervised morphological segmentation systems. the results indicate that candidate generation plays an important role in such an elementary log - linear model that is learned using contrastive estimation with negative samples.", "histories": [["v1", "Fri, 5 May 2017 17:30:50 GMT  (291kb,D)", "http://arxiv.org/abs/1705.02314v1", "10 pages, accepted and presented at the CICLing 2017 (18th International Conference on Intelligent Text Processing and Computational Linguistics)"]], "COMMENTS": "10 pages, accepted and presented at the CICLing 2017 (18th International Conference on Intelligent Text Processing and Computational Linguistics)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["serkan ozen", "burcu can"], "accepted": false, "id": "1705.02314"}, "pdf": {"name": "1705.02314.pdf", "metadata": {"source": "CRF", "title": "Building Morphological Chains for Agglutinative Languages", "authors": ["Serkan Ozen", "Burcu Can"], "emails": ["serkan1ozen@gmail.com", "burcucan@cs.hacettepe.edu.tr"], "sections": [{"heading": null, "text": "Keywords: unsupervised learning, morphological segmentation, morphology, loglinear models, contrastive estimation"}, {"heading": "1 Introduction", "text": "Unsupervised morphological segmentation has been one of the fundamental tasks in natural language processing. Segmentation of words is required normally as a preprocessing task in many natural language processing applications, such as machine translation, question answering, sentiment analysis and so on. One of the main reasons to perform morphological segmentation before applying any natural language processing task is the out-of-vocabulary (OOV) problem. The number of different word forms can be theoretically infinite in agglutinative languages [2].\nMorphological analysis is also required for some natural language processing tasks. In a full morphological analysis, morphemes are tagged according to their syntactic roles in addition to finding the morpheme boundaries. For example, in order to distinguish the word that is inflected with the negation suffix ma (or me depending in the vowel harmony) from another word that has a derivational suffix ma (or me) in Turkish\nar X\niv :1\n70 5.\n02 31\n4v 1\n[ cs\n.C L\n] 5\nrequires a full morphological analysis. Here, we aim to perform morphological segmentation rather than a full morphological analysis. Thus we only aim to find the morpheme segmentation points of each word.\nIn this paper, we propose an improvement to the MorphoChains segmentation system [1] by extending the candidate space used in contrastive estimation, thereby covering also agglutinative languages for multiple split points. Normally, log-linear models are supervised. However, using contrastive estimation by shifting the probability mass from the unobserved data (and possibly that are impossible to observe in data) to observed data enables unsupervised learning. Unobserved data is generated with negative sampling using the observed data through some transformations on the observed data (such as transpose, deletion, insertion etc.).\nIn this paper, rather than extending the probability mass assigned for unobserved data, we target the probability mass assigned for the observed data. For that purpose, we generate more segmentation points (i.e. candidates) for each observed word to extend the observed space.\nUnsupervised models seem to be a good alternative for discovering both orthographic and semantic features of words. We also adopt both orthographic features and semantic features in this paper as proposed in the original model.\nWe perform all experiments on publicly available Turkish, English and German datasets provided by Morpho Challenge 2010 [3]. The evaluation method will be the same with the one used in MorphoChains segmentation system [1].\nThe paper is organized as follows: Section 2 addresses the related work on unsupervised morphological segmentation, section 3 describes the extended log-linear model, section 4 explains the improvements performed on the original log-linear model, section 5 presents the experiments and scores for English, Turkish and German along with a discussion over the scores, and finally section 6 concludes the paper with the potential future work."}, {"heading": "2 Related Work", "text": "Morphological segmentation is one of the oldest natural language processing tasks that has been excessively studied.\nThe oldest works have been usually based on deterministic methods. One of the earliest works is Linguistica that is proposed by Goldsmith [4]. The model is based on Minimum Description Length (MDL) principle, which is deterministic. Linguistica employs morphological structures called signatures in order to represent words. Signatures reflect the internal structure of words. Words with similar morphological structure reside in the same signature. For example, {order, walk}-{ing, s} make a signature that covers words such as walking, ordering, walks, orders, and {paper, pen}-{s} make another signature that covers papers, pens.\nProbabilistic methods have also been used in unsupervised morphological segmentation. Creutz and Lagus [5] introduce another well-known unsupervised morphological segmentation Morfessor Baseline, the first member of the Morfessor family. One of the versions is based on MDL principle and the other one is based on Maximum Likelihood (ML) estimate. In another member of the same family, Creutz and Lagus [6], suggest\nusing priors by converting the model into a Maximum a Posteriori model, thereby introducing another member of the same family, called Morfessor Categories MAP (Maximum A-posterior). Morfessor has been one of the main reference segmentation systems to compare with most of the unsupervised segmentation systems. In this paper, we also compare our extended model with Morfessor Baseline and Morfessor CatMAP.\nNon-parametric Bayesian methods have also been used in segmentation task. Goldwater et al. [7] present a framework that generates power-laws by using word frequencies. Pitman-Yor Process [8] (the two parameter extension of a Dirichlet Process) is used as a stochastic process in their framework. Snyder and Barzilay [9] use Dirichlet Process, the simplified version of the Pitman-Yor Process, to induce morpheme boundaries on a bilingual aligned corpus simultaneously by finding the cross-lingual morpheme relations. Lee et al. [10] address the connection between syntax and morphology in a statistical model. Syntactic knowledge is incorporated in their morphological segmentation system. Their results show that using syntactic information helps in morphological segmentation.\nSome of the systems not only attempt to perform morphological segmentation, but also aim to learn hidden structures behind words. Chan [11] applies Latent Dirichlet Allocation (LDA) to learn morphological paradigms as latent classes. The model assumes that correct segmentations of words are known but morphological paradigms are to be learned. Chan discovers that the final morphological paradigms can be matched with syntactic tags (such as noun, verb etc.). Can and Manandhar [12] obtain syntactic categories from a context distributional clustering algorithm [13] and learn paradigms by using the the pairs of syntactic categories that have common stems.\nSimilar to MorphoChains system, log-linear models have also been utilized in morphological segmentation. Poon et al. [14] suggest using bi-gram morpheme contexts in a log linear model similar to the current study in this paper. In addition to morpheme contexts, Minimum Description Length-inspired (MDL) prior information is also used in their model to keep the lexicon and corpus size small.\nIn the recent years, deep neural networks are used for learning morphology. Cao and Rei [15] propose a model where word embeddings and segmentation are learned simultaneously. Soricut and Och [16] learn the morphological transformations between words using a high dimensional vector space (i.e. word-embedding space).\nIn this paper, 200-dimensional neural word embeddings obtained from word2vec [17] are also used to capture semantic similarities between words that are derived from each other."}, {"heading": "3 Model", "text": ""}, {"heading": "3.1 Model Definition", "text": "In this paper, we extend the MorphoChains[1] segmentation system where each word and its morphological roots are represented as a chain structure. For example, {walking, walk} and {undoable, doable, do} make morphological chains. In the morphological chain, each word appears in a parent-child relation. Here, walk is the parent of walking; doable is the parent of undoable, and do is the parent of doable.\nIn the MorphoChains system, a log-linear model is used to extract the chain structure in an unannotated corpus. The model has a feature vector \u03c6: W\u00d7 Z\u2192Rd and a corresponding weight vector \u03b8 \u2208 Rd, where W denotes words and Z denotes candidates. A candidate is a potential parent set of a word. For example, the word doors has the following candidates: (door, suffix), (doo, suffix), (do, suffix), (rs, prefix), (ors,prefix), (oors, prefix). Every word and candidate pair has a feature vector associated with it.\nProbability of a word-candidate pair (w,z) is modeled as:\nP (w, z) = e\u03b8\u00b7\u03c6(w,z) (1)\nwhere w is a word and z is a candidate of w. Thus, the conditional probability of a candidate given its word is computed by:\nP (z|w) = e \u03b8\u00b7\u03c6(w,z)\u2211\nz\u2032\u2208C(w) e \u03b8\u00b7\u03c6(w,z) (2)\nwhere C(w) corresponds to the candidates of w. The log-linear model proposed in the original paper uses features and their weights in order to learn the underlying segmentation of words. These features are described in the following section."}, {"heading": "3.2 Features", "text": "Features play a key role in a log-linear model as they represent both orthographic and semantic properties of word-candidate pairs. The features in the model are as follows:\nSemantic Similarity is applied by the cosine similarity of a word-parent pair. The cosine similarity is computed by using the word embeddings obtained from word2vec [17]. The paper indicates that morphologically related word-parent pairs tend to have high cosine similarity. For example, (fly, flying) pair will have higher cosine similarity when compared to (flyi, flying) and this will favor fly to be the parent of flying rather than having flyi as the parent.\nAffixes are automatically generated as a list of most frequent affixes in the corpus. In order to build the affix list, each word having a higher frequency than a manually set threshold is analyzed through its potential suffixes and prefixes. All potential suffixes are added into the affix list. If another word in the corpus having the same suffix is met, then the frequency of the suffix is incremented.\nAffix correlation shows how related two affixes are in terms of the rate of their common stems. For example (ing, ed) suffix pair is expected to have a high correlation since many verbs in English can take both of the suffixes. If two affixes share common stems, then they are called neighbor suffixes. Again the same pair (ing, ed) are called neighbors since they share many common stems. For example, regarding the word (laughing) and the suffix (-ing), since another word (laughed) exists in the corpus, the parent-candidate pair (laugh, laughing) gets a feature stating that (-ing) is most probably a suffix which in turn favors (laugh) to be a strong candidate for (laughing).\nPresence in the wordlist represents whether the parent is seen in the corpus or not. This provides a bias on the likelihood of a parent to be a valid word. This feature assumes that the language is concatenative.\nTransformation features are used for stem changes during affixation. There are three types of transformation features, namely repeat, delete, modify. For example, (running, run) word-candidate pair has a repeat feature set to 1 due to the repetition of n at the end of the word, and (deleting, delete) pair has a delete feature set to 1 due to the deletion of the letter e at the end of the word.\nStop features help to identify whether a parent is the root or not. One of the key features to handle this is the highest cosine similarity between a word and its parents. For example, for the word flying, fly is more likely to be the base word than fl because cosine similarity between flying and fly is higher than the cosine similarity between fl and flying."}, {"heading": "4 Improvements to the Model", "text": "The model is learned in an unsupervised setting my maximizing the likelihood of observed words in a given corpus. The likelihood of the model for a given unannotated word list D is given as follows in the original paper:\nL(\u03b8,D) = \u220f w\u2217\u2208D P (w\u2217)\n= \u220f w\u2217\u2208D \u2211 z\u2208C(w\u2217) P (w\u2217, z) (3) = \u220f w\u2217\u2208D [ \u2211 z\u2208C(w\u2217) e \u03b8\u00b7\u03c6(w\u2217,z)\u2211 w\u2208\u03a3\u2217 \u2211 z\u2208C(w) e \u03b8\u00b7\u03c6(w,z) ] (4)\n(5)\nwhere \u03a3\u2217 denotes the alphabet, which is problematic to calculate for all possible unobserved data for a given language. Contrastive estimation is used to apply negative sampling and replace the normalization term with the neighbors of each word. This process creates a large space of unobserved data from which the probability mass will be shifted to observed space and therefore the likelihood will be normalized through the unobserved data.\nHere, we have noticed that although candidates play an important role in the model, they are generated by only binary segmentation of each word. For example, the Turkish word kitap+lar+dan (from the books) will never have the suffix lar in any of its candidates. This holds true for any word with more than one suffix. With this intuition, we aim to increase the candidate space generated from each word by including all possible segmentations of each word, therefore introducing the suffixes in the middle of words as candidates as well.\nIn our approach, in order to generate all possible candidates of a word, each binary segmentation of the word is proposed as a candidate. For each candidate stem obtained from the binary segmentation, candidate segmentations are generated again with a binary segmentation. Therefore, a left-recursion is applied for each word in all levels of the binary segmentation in order to generate candidates. In other words, the process is repeated recursively for each candidate stem in each iteration.\nFunction RCG (word, candidateList) Data: word Result: candidateList for i=word.length-1;i \u2265word.length-4 and i \u22650; i=i-1 do\nparent\u2190word.substring(0, i); if 2 * parent.length \u2265word.length and word.length >2 then\ncandidateList\u2190 parent; AddSuffixFeature(parent); RCG(parent, candidateList);\nend parent\u2190word.substring(i,word.length); if 2 * parent.length \u2265word.length then\ncandidateList\u2190 parent; AddPrefixFeature(parent);\nend end\nAlgorithm 1: RCG (Recursive candidate generation) algorithm\nWe restrict some candidate generations with some heuristics. Each candidate has a maximum suffix length of 4. The recursion continues as long as the base word\u2019s length is greater than 2. Another heuristic in the original model is that twice of parent\u2019s length must be greater than or equal to the twice of the child word\u2019s length. For example, candidates of the word cars will be car, ca, rs, ars. Words (c, s) are detained from being candidates since they do not meet any of the heuristics.\nAn example is given in Figure 1. The candidates of the word kitapc\u0327\u0131lar (means bookshops) is generated recursively. As can be seen in the figure, base word has at most 4 child nodes that corresponds to the first level candidates3 (i.e. having a suffix with maximum 4 letters).\nThis recursive generation creates a large candidate space that also enlarges the observed space. The recursive procedure is given in Algorithm 1.\nThe model is learned by optimizing the feature weights according to the model likelihood given in Equation 3. Gradient-descent algorithm is used for the optimization similar to the original model. Once the model is learned, the prediction is performed for a novel word through the optimized weights for each feature, where again a recursive segmentation is applied."}, {"heading": "5 Results", "text": "We use the publicly available datasets provided by Morpho Challenge [3] for both training and testing. The training sets contain 878K words, 617K words, and 2M for English, Turkish, and German respectively. The test sets contain 2200 words, 2500 words, and 785 words for English, Turkish, and German respectively that are also obtained from Morpho Challenge gold standard datasets by aggregating the gold sets in Morpho Challenge 2005-2010.\n3 We chose 4 because of the fact that the longest suffix in Turkish language is 4, e.g -iyor.\nWe also use large datasets for training the neural word embedding model, word2vec [17] in order to build the neural word embeddings for the semantic similarity feature. All neural word embeddings are 200-dimensional. The corpora size is given in Table 1.\nExperiments and evaluation are held as in the original paper. Segmentation points in the results are compared to those given in gold segmentation data, and Precision, Recall and F-1 measure values are calculated accordingly.\nWe compare our Turkish and English results with MorphoChains-O [1] (original MorphoChains system), Morfessor Baseline [5], Morfessor CatMAP [6] and Lee Segmenter [10]. All models are trained on the same train and test sets. Recursive candidate generation notably improves the scores with 12% on Turkish with a final F-measure of 72%, whereas the original MorphoChains system has a F-measure of 60%. The same\nimprovement also applies in English, having a F-measure of 74% with 3% improvement compared to the original MorphoChains system which has a F-measure of 71%.\nWe compare the recursive MorphoChains system with the original MorphoChains system and Morfessor Baseline on also German. Morfessor Baseline outperforms two other models with a F-measure of 54%. The German results are better in the original model with a F-measure of 38%, whereas the recursive model gives 25%. This is possibly because of the morphological structure of the German language. German is not an agglutinative language and the left-recursion applied in the candidate generation will generate more erroneous candidate suffixes. This is also because of the common compounds in German language. All results for English, Turkish and German are given in Table 2.\nResults suggest that enlarging the candidate space will also enlarge the neighborhood size. Since contrastive estimation performs better on larger datasets, enlarging the size of the candidate space improves the precision scores because of the improved sub-word counts. For example, for the word kitapc\u0327\u0131lar, the recursive candidate space will contain many valid candidates which in turn will help learning correct weights for correct candidates.\nSome examples to correct and incorrect segmentations are given in Table 3. In the original MorphoChains system, words are prone to be oversegmented, especially in Turkish. In the recursive MorphoChains system, more words are segmented correctly by overcoming the oversegmentation problem in the original model.\nAll this information can let us claim that increasing the candidate space in log-linear models improves the segmentation results especially in agglutinative languages such as Turkish. Enlarging the unobserved word space has been studied before via negative sampling. However, enlarging the obsverved space has not been studied before to our\nknowledge. In this paper, we show how it affects to enlarge the observed space in such a log-linear model. The results show that its affect is noticeably high."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we extend the unsupervised morphological segmentation system called MorphoChains [1]. We adopt the original log-linear model that uses contrastive estimation with negative sampling and aim to enlarge the observed space from which probability mass will be shifted.\nWe enlarge the observed candidate space by generating candidates recursively, whereas in the original model candidates are generated through binary segmentations of each word. Therefore, for each word the number of candidates is equal to the number of letters in each word. The recursion provides generating candidates that extract the suffixes in the middle of the word and this increases the probability assigned to these suffixes. However, in the original model only the probability of suffixes at the end of the words are increased with their occurrence counts in the corpus.\nWe aim to try different optimization algorithms in the original log-linear model as a future goal. We believe that using a better optimization technique will also improve the results further."}, {"heading": "Acknowledgments", "text": "This research is supported by the Scientific and Technological Research Council of Turkey (TUBITAK) with the project number EEEAG-115E464 and we are grateful to TUBITAK for their financial support."}], "references": [{"title": "An unsupervised method for uncovering morphological chains", "author": ["K. Narasimhan", "R. Barzilay", "T. Jaakkola"], "venue": "Transactions of the Association for Computational Linguistics 3", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Finite state morphology and left to right phonology", "author": ["J. Hankamer"], "venue": "Proceedings of the West Coast Conference on Formal Linguistics. Volume 5.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1986}, {"title": "Morpho challenge 2010", "author": ["M. Kurimo", "K. Lagus", "S. Virpioja", "V. Turunen"], "venue": "http:// research.ics.tkk.fi/events/morphochallenge2010/", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised learning of the morphology of a natural language", "author": ["J. Goldsmith"], "venue": "Computational Linguistics 27(2)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Unsupervised discovery of morphemes", "author": ["M. Creutz", "K. Lagus"], "venue": "Proceedings of the ACL02 workshop on morphological and phonological learning, Association for Computational Linguistics", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Inducing the morphological lexicon of a natural language from unannotated text", "author": ["M. Creutz", "K. Lagus"], "venue": "Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR05.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Interpolating between types and tokens by estimating power-law generators", "author": ["S. Goldwater", "M. Johnson", "T.L. Griffiths"], "venue": "Advances in Neural Information Processing Systems 18. MIT Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Generalized weighted Chinese restaurant processes for species sampling mixture models", "author": ["H. Ishwaran", "L.F. James"], "venue": "Statistica Sinica 13", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["B. Snyder", "R. Barzilay"], "venue": "ACL.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling syntactic context improves morphological segmentation", "author": ["Y.K. Lee", "A. Haghighi", "R. Barzilay"], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, Association for Computational Linguistics", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning probabilistic paradigms for morphology in a latent class model", "author": ["E. Chan"], "venue": "Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology and Morphology, Association for Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Clustering morphological paradigms using syntactic categories", "author": ["B. Can", "S. Manandhar"], "venue": "Multilingual Information Access Evaluation I. Text Retrieval Experiments: 10th Workshop of the Cross-Language Evaluation Forum, CLEF 2009, Corfu, Greece, September 30 - October 2, 2009, Revised Selected Papers, Berlin, Heidelberg, Springer Berlin Heidelberg", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Inducing syntactic categories by context distribution clustering", "author": ["A. Clark"], "venue": "Proceedings of the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning - Volume 7. ConLL \u201900, Stroudsburg, PA, USA, Association for Computational Linguistics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Unsupervised morphological segmentation with loglinear models", "author": ["H. Poon", "C. Cherry", "K. Toutanova"], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Association for Computational Linguistics", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "A joint model for word embedding and word morphology", "author": ["K. Cao", "M. Rei"], "venue": "CoRR abs/1606.02601", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised morphology induction using word embedddings", "author": ["R. Soricut", "F. Och"], "venue": "Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, Association for Computational Linguistics", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The model is based on the unsupervised morphological segmentation system called MorphoChains [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "The number of different word forms can be theoretically infinite in agglutinative languages [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "In this paper, we propose an improvement to the MorphoChains segmentation system [1] by extending the candidate space used in contrastive estimation, thereby covering also agglutinative languages for multiple split points.", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "We perform all experiments on publicly available Turkish, English and German datasets provided by Morpho Challenge 2010 [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 0, "context": "The evaluation method will be the same with the one used in MorphoChains segmentation system [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "One of the earliest works is Linguistica that is proposed by Goldsmith [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "Creutz and Lagus [5] introduce another well-known unsupervised morphological segmentation Morfessor Baseline, the first member of the Morfessor family.", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "In another member of the same family, Creutz and Lagus [6], suggest", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "[7] present a framework that generates power-laws by using word frequencies.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Pitman-Yor Process [8] (the two parameter extension of a Dirichlet Process) is used as a stochastic process in their framework.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "Snyder and Barzilay [9] use Dirichlet Process, the simplified version of the Pitman-Yor Process, to induce morpheme boundaries on a bilingual aligned corpus simultaneously by finding the cross-lingual morpheme relations.", "startOffset": 20, "endOffset": 23}, {"referenceID": 9, "context": "[10] address the connection between syntax and morphology in a statistical model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Chan [11] applies Latent Dirichlet Allocation (LDA) to learn morphological paradigms as latent classes.", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "Can and Manandhar [12] obtain syntactic categories from a context distributional clustering algorithm [13] and learn paradigms by using the the pairs of syntactic categories that have common stems.", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "Can and Manandhar [12] obtain syntactic categories from a context distributional clustering algorithm [13] and learn paradigms by using the the pairs of syntactic categories that have common stems.", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "[14] suggest using bi-gram morpheme contexts in a log linear model similar to the current study in this paper.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Cao and Rei [15] propose a model where word embeddings and segmentation are learned simultaneously.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "Soricut and Och [16] learn the morphological transformations between words using a high dimensional vector space (i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "In this paper, 200-dimensional neural word embeddings obtained from word2vec [17] are also used to capture semantic similarities between words that are derived from each other.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "In this paper, we extend the MorphoChains[1] segmentation system where each word and its morphological roots are represented as a chain structure.", "startOffset": 41, "endOffset": 44}, {"referenceID": 16, "context": "The cosine similarity is computed by using the word embeddings obtained from word2vec [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "We use the publicly available datasets provided by Morpho Challenge [3] for both training and testing.", "startOffset": 68, "endOffset": 71}, {"referenceID": 16, "context": "We also use large datasets for training the neural word embedding model, word2vec [17] in order to build the neural word embeddings for the semantic similarity feature.", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "We compare our Turkish and English results with MorphoChains-O [1] (original MorphoChains system), Morfessor Baseline [5], Morfessor CatMAP [6] and Lee Segmenter [10].", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "We compare our Turkish and English results with MorphoChains-O [1] (original MorphoChains system), Morfessor Baseline [5], Morfessor CatMAP [6] and Lee Segmenter [10].", "startOffset": 118, "endOffset": 121}, {"referenceID": 5, "context": "We compare our Turkish and English results with MorphoChains-O [1] (original MorphoChains system), Morfessor Baseline [5], Morfessor CatMAP [6] and Lee Segmenter [10].", "startOffset": 140, "endOffset": 143}, {"referenceID": 9, "context": "We compare our Turkish and English results with MorphoChains-O [1] (original MorphoChains system), Morfessor Baseline [5], Morfessor CatMAP [6] and Lee Segmenter [10].", "startOffset": 162, "endOffset": 166}, {"referenceID": 0, "context": "In this paper, we extend the unsupervised morphological segmentation system called MorphoChains [1].", "startOffset": 96, "endOffset": 99}], "year": 2017, "abstractText": "In this paper, we build morphological chains for agglutinative languages by using a log linear model for the morphological segmentation task. The model is based on the unsupervised morphological segmentation system called MorphoChains [1]. We extend MorphoChains log linear model by expanding the candidate space recursively to cover more split points for agglutinative languages such as Turkish, whereas in the original model candidates are generated by considering only binary segmentation of each word. The results show that we improve the state-of-art Turkish scores by 12% having a F-measure of 72% and we improve the English scores by 3% having a F-measure of 74%. Eventually, the system outperforms both MorphoChains and other well-known unsupervised morphological segmentation systems. The results indicate that candidate generation plays an important role in such an unsupervised log-linear model that is learned using contrastive estimation with negative samples.", "creator": "LaTeX with hyperref package"}}}