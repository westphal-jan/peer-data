{"id": "1705.02772", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2017", "title": "Scene Text Eraser", "abstract": "the character information in natural scene images contains various personal information, such up telephone numbers, home addresses, etc. it is a high risk of leakage the information if they are deleted. in this paper, kim proposed a dynamic text erasing method to properly hide the information via an inpainting collaborative neural network ( cnn ) model. the input is a scene text object, likewise the output is reported to be text erased image nor all the character regions filled up the colors of the surrounding reader pixels. this work is accomplished by a cnn model through convolution to deconvolution information interconnection process. the training samples and the projected inpainting images are considered as teaching signals for training. to evaluate the text erasing technologies, the output images are detected by a novel scene text targeting method. nevertheless, utilizing same measurement on text detection is utilized for testing the images in benchmark dataset icdar2013. compared with direct text detection way, the scene text erasing process demonstrates what drastically decrease on the citation, recall and f - score. that proves the effectiveness while proposed strategies for erasing the text entering natural user images.", "histories": [["v1", "Mon, 8 May 2017 08:28:34 GMT  (1457kb,D)", "http://arxiv.org/abs/1705.02772v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["toshiki nakamura", "anna zhu", "keiji yanai", "seiichi uchida"], "accepted": false, "id": "1705.02772"}, "pdf": {"name": "1705.02772.pdf", "metadata": {"source": "CRF", "title": "Scene Text Eraser", "authors": ["Toshiki Nakamura", "Anna Zhu", "Keiji Yanai", "Seiichi Uchida"], "emails": ["nakamura@human.ait.kyushu-u.ac.jp", "uchida@human.ait.kyushu-u.ac.jp", "annakkk@live.com", "yanai@cs.uec.ac.jp"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nNowadays, personal private information such as telephone numbers, ID number, home addresses, car numbers [1], etc. have become the special identity of person. Those important information may be incidentally captured, and appear in natural scene images. If published on the internet, it is a high risk to be collocated automatically by machines and criminals for illegal usage. To prevent the leakage of personal information, especially the text in scene images, information hidden technology is in great demand. Different from scene text detection [2], the text hidden technics do not extract the whole text lines. That means perfect detection accuracy is not required. Only the characters or parts of them are removed and the process parts should not be distinct from the background. The example is shown in Fig. 1.\nThe goal is to erase the text regions and make them hard to be detected. The simple image processing like blurring through Gaussian filter [5] is only valid to text with specific shape and stroke. However, scene text has various appearance [3], such as color, font, size, orientation, etc. Additionally, in the background, lots of clutters exist and effect the text and nontext judgement. Those challenges make the task difficult to solve. In this paper, we propose a novel method that erases the scene text via an inpainting deep neural network (DNN).\nThe problem is converted as image transformation refereing to transforming images from a source image space to a target image space. In our case, it only needs input images and the output are text erased images with non-text regions remain original. The inpainting DNN is considered as the eraser. It composes of Convolutional neural networks (CNN) in front and deconvolutional neural networks (DeCNN) [4] subsequently to recover the image resolution. The CNN is used to represent the feature of the image [6]. If only the features on the top are used for transformation, some details may be lost. To tackle this problem, interconnection between the deconvolutional layers and the convolutional layers which have the same size is built, and then the result is inputted to next deconvolutional layer. This model is trained in end-to-end fashion. We use inpainting [7] and dilation process to obtain the ground truth for training.\nA text detection method [8] then detects the text regions in the text erased images and the performance is evaluated in the same manner [9] including the precision, recall and fscore. Compared with the detection result on original ICDAR 2013 images, all the measurements decrease drastically on text erased images. That demonstrates effectiveness of our proposed method.\nThe major contributions of our work are claimed as follows: \u2022 We propose to use scene text eraser to hide text in the\nimages without text detection. The concept is novel for preventing the leakage of private text-based information. And this scene text eraser can remove the text naturally and effectively. \u2022 The scene text eraser is implemented in image transformation way. The convolution-to-deconvolution structure adds the summation process among layers for better image quality. \u2022 The dealation and inpainting process are applied to label ground truth automatically and accurately.\nThe rest of the paper is structured as follows: A selection of related work is reviewed in Sect. II. Sect. III presents\nar X\niv :1\n70 5.\n02 77\n2v 1\n[ cs\n.C V\n] 8\nM ay\n2 01\n7\nour proposed method in detail. In Sect. IV, we give the experimental results which include the details of databases and the experimental setup. Finally, Sect. V gives a summarization and conclusion of this paper."}, {"heading": "II. RELATED WORK", "text": "Two strategies can be used for scene text erasing. One follws text detection pipelines [10], [11] that extract the text regions and then erase them by post-process. The other shares the idea of image transformation [12], [13] that considers the output image as a different style in which the text are removed and the other parts keep original.\nGenerally, the text detection methods detect text through either connected component analysis (CCA)-based procedure or sliding window-based procedure. The CCA-based methods [14], [15] involves character candidates extraction, character/non-character classification, and text grouping. The sliding window-based methods [16], [17] extract regional textual features, such as HoG, LBP [18], CNN etc, from the regions which are scanned discretely from the image space by multi-scale and multi-ratio, and then scores the regions by inputting the features to a pertained text/non-text classification engine. Regions with high text scores are grounded to text regions eventually. Sometimes, image pre-processing or postprocessing techniques are required and added in the two pipelines. For text erasing, further process is required, for instance, how to fill the text regions by background color.\nIn recent years, many classic problems can be framed as image transformation tasks [13], where a system receives some input image and transforms it into an output image. Examples from image processing include denoising, superresolution, and colorization, where the input is a degraded image (noisy, low-resolution, or grayscale) and the output is a high-quality color image. Examples from computer vision include semantic segmentation and depth estimation, where the input is a color image and the output image encodes semantic or geometric information about the scene. The related algorithms, either transfer the tone (color, contrast, saturation, etc.) of an image, preserving its patterns and details, or distort the texture uniformly of an image to create style. Scene text erasing can also be treated as a style transferring. Due to the richness of features that a deep CNN can possess, this task used to train a feedforward DNN in a supervised manner for transferring. Examples include the Ref [19] that automatically converts complex rough sketches to line drawings, Ref [20] converts the image style, Ref [21] performs color conversion on black and white images, etc. In this paper, we think out using image transform technology to hide the characters in the image by DNN with a special structure."}, {"heading": "III. PROPOSED METHOD", "text": "The flowchart of the proposed method is shown in Fig. 2. Since the purpose of text erasing is not the same as accurate text detection task, a single scale sliding window is applied to the original input images. The sliding stride is half of the\nCNN\nSliding window by 32 pixels\n64\u00d764 Output image\n32\u00d732\nOriginal image\n64\u00d764 Input image\nResult image\nErasing Process\nFig. 2. The proposed method for scene text erasing.\nwindow size. We cut the whole images into 64 \u00d7 64 patches and then input them into a pre-trained DNN. The size of each output result patches is also 64 \u00d7 64. To overcome the ambiguousness in the overlap regions, only the center part with 32 \u00d7 32 pixels of the output is considered valid and put back to original location. After this process, a single text hidden image is generated."}, {"heading": "A. The structure of the scene text eraser", "text": "A feedforward DNN composed with half convolution part and half deconvolution part is used as eraser in our approach. The architecture of the DNN is shown in Fig. 3.\nThe convolution part contains four convolutional layers. The filter size of each convolutional layer is 4 \u00d7 4. The stride step and padding size is set to 2 and 1, respectively. Therefore, in each layer, the size of the feature maps reduces half comparing with the previous ones. The deconvolution part has the same structure but replaces the convolutional layers to deconvolution layer. The size of the filter, stride step and padding size is exactly the same as in convolution part. Thus, with the layer going deeper, the size of the feature maps is double increased. Due to the reduction of the image by convolution and the enlargement of the image by deconvolution, the output image has the same size as the original image.\nHowever, if we only use a linear structure, in which the image size reduction or enlargement operations are performed isolated, lots of information on the original image may be lost. Because in the convolution part, only part of the information in the input image is stored, and the output image size is reduced. And in the deconvolution part, only the stored information is used to recover the image content. It results in information losing and low resolution of the output image.\nTo tackle this problem, we used skip connection technique [22] which is effective for restoring images with less deterioration. The skip connection sums the feature maps in\ndifferent layer and inputs them to the next layer. Since the feature maps in convolution layers have more detailed information, such as the position information of objects, etc. By adding up the feautres of the previous layer for image recovering, it is possible to complement some image information that is lost by the reduction and enlargement procedure. And this process is expected to prevent the resolution being lowered.\nAs shown in Fig. 3, the skip connection is performed by adding a summation layer after each deconvolution layer. It is expressed in Eq. 1 by adding up the features X1 in deconvolution layer and features X2 in convolution layer element-wisely, and then inputing them to the next deconvolution layer. This summation layer requires the input from different layers have the same size. So the convolution to deconvolution structure is symmetry.\nF (X1, X2) = max(0, X1 +X2). (1)\nRectified Linear Unit (ReLU) [24] is followed after each layer. Normalization is performed as well. Thus, the output result in each layer is rendered nonnegative. The lose function for back propagation uses mean square error [23] as expressed in Eq. 2. N is the total training samples. Xi represents the output through the skip connection DNN model and Yi is the text removed ground truth. We implement and train the network on Caffe. The stochastic gradient descent (SGD) with learning rate of 10\u22124 is used in training phase.\nL(w) = 1\nN N\u2211 i=1 \u2016F (Xi, w)\u2212 Yi\u20162 (2)"}, {"heading": "B. Training", "text": "Since in our method, the patch images are input for DNN, we need to collect the training samples on patch level. The aim of the system is to hide the text information in natural scene images. So, for positive samples, the input are scene text images and the ground truth are the same images with text removed. For negative samples, the input and ground truth are the same background images. To automatically generate the training samples, the image inpainting process is performed. It is a technique to fill up defects in the images and make them inconspicuous. Specially, it is frequently used for restoring images when noises exist. For our case, the text is considered as defects and filled by the surrounding background color after inpainting.\nFig. 4 shows the details of the processing. Given the character ground truth in pixel level (Fig. 4(b)), inpainting process is applied on the original scene text image. The character ground truth is the basement as shown in Fig. 4(c)) and we can get the processing result in Fig. 4(d). The pixels on character strokes are inpainted by the surrounding background color. To make the boundaries between character and background more inconspicuous in the image, additional dilation process is implemented on the basement images before image inpainting. Fig. 4(e) and 4(g) is the dilate results by performing dilation once and three times, respectively. And Fig. 4(f) and 4(h) is\nthe final generation images by dilation and inpainting process sequentially.\nTo collect the patch level training samples, the sliding window with the size setting to 64 \u00d7 64 pixels is used. The batch formation is performed as well. The pair of input and output images are cropped from the same position in the original images and the corresponding inpainting images. The character ground truth is the guidance to classify the patches to positive or negative samples. In character ground truth images, if the corresponding cropped region contains any text, it is classified as text sample. Otherwise, that is background sample. Examples of the training data are shown in Fig. 5.\nWith this process, the training samples can be collected and classified automatically."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Dataset", "text": "In the experiment, a Flickr image dataset which contains more than 3000 scene images and the benchmark dataset ICDAR 2013 [9] which contains 229 images used for training. Most of the images in this dataset have signboards and billboards with text attached on. The font, color and position of characters and the background is various which is benefit for training the model. To evaluate the performance, the dataset ICDAR 2013 that is different from images used in training is tested."}, {"heading": "B. Qualitative Evaluation", "text": "Fig. 6 shows some text erased image by employing our proposed method. In Fig. 6(a), text can be successfully erased, even they are in complicated background, such as the the glass, the trees, etc. However, our proposed model fails for some cased as shown in Fig. 6(b). Our work only uses one scale sliding window to get the subregion. The captured parts in the character whose size is much larger than the window size might be considered as background. So the output of the DNN has no changes in that subregion. This results in the bad erasing performance on images with large size characters. Comparing results from differently trained DNN, the one that is trained with three times dilatation and inpainting ground truth gets the best performance. As shown in Fig. 6, text in the images of the last column are mostly erased and can not be distinguished by human. Since the dilate operation turns more pixels on the character boundary to be considered as part of the character, the text erasing result looks smoothing and natural."}, {"heading": "C. Quantitative Evaluation", "text": "To evaluate the scene text erasing performance, a modified text detection method [8] is used to detect the text in images after erasing process. It is an object proposal based deep neural network that predicts discrete regions with different aspect ratios and scales from multiple feature maps. To make it adapted for text detection, we select six aspect ratios: 0.7, 1, 2, 3, 5, 7 for designing the default boxes. The scales on the prediction layers range from 0.06 to 0.85. In total, 38124 regions estimated. Most of them are non-text regions. Only the detections with text probability higher than 0.7 are remained as text. We test this text detection method on scene text erased images in ICDAR 2013 and compare the results with original scene text images. They are named by the generation ways as below:\n\u2022 Original images dataset: focused scene text images in ICDAR 2013. \u2022 Erased0 images dataset: Scene text erased images of ICDAR 2013 by network trained with inpainting ground truth. \u2022 Erased1 images dataset: Scene text erased images of ICDAR 2013 by network trained with one time dilatation and inpainting ground truth. \u2022 Erased3 images dataset: Scene text erased images of ICDAR 2013 by network trained with three times dilatation and inpainting ground truth.\nWe follow the text detection performance measurement by compute the precision, recall and f-score under two protocols, the DetEval [25] and the ICDAR 2013 evaluation [9]. Precision represents the proportion of detected text regions to all detected regions. Recall is the proportion of detected text regions to ground truth text regions. f-score is a trade-off between precision and recall rate by computing their harmonic mean. Table I demonstrates the results. After text erasing, the recall of text detection decreases more than 70%. That demonstrates less text regions are detected. The precision decreases about 30% representing that the non-text regions\u2019 proportion becomes higher in all the detected regions. Compared with the text detection results on original images, the three text erased image datasets have worse performance. The overall measurement f-score drops drastically after text erasing in the images. Inversely, it proves the effectiveness of the proposed method. As explained above, by adding the dilate operation for training samples, the text can be erased more smoothly and naturally. Without the shape boundaries between background and text regions, the erased text regions are much difficult to be detected. Examples of text detection results are displayed in Fig. 7. The proposed text eraser can distinguish the text regions and non-text regions well. From the results, we can see that most text regions go through exserting process and are hidden afterwards.\nIn this work, we only used single scale sliding windowbased method to perform text erasing in images. It has some weakness for erasing large size text. In our future work, a real end-to-end system will be employed. The input is a complete\nscene text image, and the output is the text erased image. For training, the full images and the corresponding inpaining images will be the training samples instead of using cropped image patches. Additionally, we will propose new evaluation method to measure the character erased performance but not only by text detection evaluation."}, {"heading": "V. CONCLUSION", "text": "To protect privacy of the text based information in natural scene images, we proposed a novel scene text eraser. It used the image transform method which transferred the scene text images to text erased images via an inpainting deep neural network. This network process the image patches, which are cropped by sliding window, from convolution to deconvolution. To improve the resolution of output images and conserve more information of the non-text part in the original images, we used skip connection to sum the feature maps in both deconvolutional layers and specified convolutional layers. For model training, the dilate and inpainting technologies are applied subsequently to generate the training samples. A text detection method evaluated the text erasing performance on ICDAR 2013 dataset. The precision, recall and f-score dropped drastically after erasing the text in images. It proved the effectiveness of this text eraser. In our future work, we will develop this model in end-to-end fashion and think out new evaluation method to better measure the performance of scene text eraser."}, {"heading": "VI. ACKNOWLEDGMENTS", "text": "The pictures of left bottom of Fig.5(a) and left top of Fig.5(b) are taken from Flickr under the copyright license. The authors would like to thank the contributors of those pictures. Left bottom of Fig.5(a) and Left top of Fig.5(b) : alykat"}], "references": [{"title": "Selective concealment of characters for privacy protection,", "author": ["K. Inai", "M. Palsson", "V. Frinken", "Y. Feng", "S. Uchida"], "venue": "in Pattern Recognition (ICPR),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Text detection and recognition in imagery: A survey,", "author": ["Q D.D. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Reading text in the wild with convolutional neural networks,", "author": ["M J", "K S", "A V", "A Z"], "venue": "International Journal of Computer Vision, vol. 116,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning deconvolution network for semantic segmentation,", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Carreiraperpinan, \u201dFast nonparametric clustering with gaussian blurring mean-shift.\u201d,International", "author": ["A. M"], "venue": "Conference on Machine Learning ,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Multiple convolutional neural network for feature extraction,", "author": ["G. Yang", "H. Jing"], "venue": "Proceedings of the IEEE International Conference on Image Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Ssd: Single shot multibox detector,", "author": ["W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S. Reed", "C.-Y. Fu", "A.C. Berg"], "venue": "European Conference on Computer Vision. Springer,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Scene text detection and recognition: Recent advances and future trends,", "author": ["Y Z. Y", "C Y", "X B"], "venue": "Frontiers of Computer Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Robust text detection in natural scene images,", "author": ["X.-C. Yin", "X. Yin", "K. Huang", "H.-W. Hao"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Perceptual losses for realtime style transfer and super-resolution,", "author": ["J. Johnson", "A. Alahi", "L. Fei-Fei"], "venue": "European Conference on Computer Vision. Springer,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Robust scene text detection with convolution neural network induced mser trees,", "author": ["W. Huang", "Y. Qiao", "X. Tang"], "venue": "European Conference on Computer Vision. Springer,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Detecting text in natural scenes with stroke width transform,", "author": ["B. Epshtein", "E. Ofek", "Y. Wexler"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "End-to-end text recognition with convolutional neural networks,", "author": ["T. Wang", "D.J. Wu", "A. Coates", "A.Y. Ng"], "venue": "in Pattern Recognition (ICPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Scene text localization and recognition with oriented stroke detection,", "author": ["L. Neumann", "J. Matas"], "venue": "Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Pedestrian detection based on hog-lbp feature,", "author": ["G. Gan", "J. Cheng"], "venue": "Computational Intelligence and Security (CIS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Learning to simplify: fully convolutional networks for rough sketch cleanup,", "author": ["E. Simo-Serra", "S. Iizuka", "K. Sasaki", "H. Ishikawa"], "venue": "ACM Transactions on Graphics (TOG), vol. 35,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Image style transfer using convolutional neural networks,", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification,", "author": ["S. Iizuka", "E. Simo-Serra", "H. Ishikawa"], "venue": "ACM Transactions on Graphics (TOG), vol. 35,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation,", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Image restoration using convolutional auto-encoders with symmetric skip connections,", "author": ["X.-J. Mao", "C. Shen", "Y.-B. Yang"], "venue": "arXiv preprint arXiv:1606.08921,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Rectified linear units improve restricted boltzmann machines,", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Bichot et al., \u201dEvaluation of video activity localizations integrating quality and quantity measurements,", "author": ["C. Wolf", "E. Lombardi", "J. Mille", "O. Celiktutan", "M. Jiu", "E. Dogan", "G. Eren", "M. Baccouche", "E. Dellandrea", "C.-E"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Nowadays, personal private information such as telephone numbers, ID number, home addresses, car numbers [1], etc.", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "Different from scene text detection [2], the text hidden technics do not extract the whole text lines.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "The simple image processing like blurring through Gaussian filter [5] is only valid to text with specific shape and stroke.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "However, scene text has various appearance [3], such as color, font, size, orientation, etc.", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "It composes of Convolutional neural networks (CNN) in front and deconvolutional neural networks (DeCNN) [4] subsequently to recover the image resolution.", "startOffset": 104, "endOffset": 107}, {"referenceID": 5, "context": "The CNN is used to represent the feature of the image [6].", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "A text detection method [8] then detects the text regions in the text erased images and the performance is evaluated in the same manner [9] including the precision, recall and fscore.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "One follws text detection pipelines [10], [11] that extract the text regions and then erase them by post-process.", "startOffset": 36, "endOffset": 40}, {"referenceID": 8, "context": "One follws text detection pipelines [10], [11] that extract the text regions and then erase them by post-process.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "The other shares the idea of image transformation [12], [13] that considers the output image as a different style in which the text are removed and the other parts keep original.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "The CCA-based methods [14], [15] involves character candidates extraction, character/non-character classification, and text grouping.", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "The CCA-based methods [14], [15] involves character candidates extraction, character/non-character classification, and text grouping.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "The sliding window-based methods [16], [17] extract regional textual features, such as HoG, LBP [18], CNN etc, from the regions which are scanned discretely from the image space by multi-scale and multi-ratio, and then scores the regions by inputting the features to a pertained text/non-text classification engine.", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "The sliding window-based methods [16], [17] extract regional textual features, such as HoG, LBP [18], CNN etc, from the regions which are scanned discretely from the image space by multi-scale and multi-ratio, and then scores the regions by inputting the features to a pertained text/non-text classification engine.", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "The sliding window-based methods [16], [17] extract regional textual features, such as HoG, LBP [18], CNN etc, from the regions which are scanned discretely from the image space by multi-scale and multi-ratio, and then scores the regions by inputting the features to a pertained text/non-text classification engine.", "startOffset": 96, "endOffset": 100}, {"referenceID": 9, "context": "In recent years, many classic problems can be framed as image transformation tasks [13], where a system receives some input image and transforms it into an output image.", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "Examples include the Ref [19] that automatically converts complex rough sketches to line drawings, Ref [20] converts the image style, Ref [21] performs color conversion on black and white images, etc.", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "Examples include the Ref [19] that automatically converts complex rough sketches to line drawings, Ref [20] converts the image style, Ref [21] performs color conversion on black and white images, etc.", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "Examples include the Ref [19] that automatically converts complex rough sketches to line drawings, Ref [20] converts the image style, Ref [21] performs color conversion on black and white images, etc.", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "To tackle this problem, we used skip connection technique [22] which is effective for restoring images with less deterioration.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "Rectified Linear Unit (ReLU) [24] is followed after each layer.", "startOffset": 29, "endOffset": 33}, {"referenceID": 19, "context": "The lose function for back propagation uses mean square error [23] as expressed in Eq.", "startOffset": 62, "endOffset": 66}, {"referenceID": 6, "context": "To evaluate the scene text erasing performance, a modified text detection method [8] is used to detect the text in images after erasing process.", "startOffset": 81, "endOffset": 84}, {"referenceID": 21, "context": "We follow the text detection performance measurement by compute the precision, recall and f-score under two protocols, the DetEval [25] and the ICDAR 2013 evaluation [9].", "startOffset": 131, "endOffset": 135}], "year": 2017, "abstractText": "The character information in natural scene images contains various personal information, such as telephone numbers, home addresses, etc. It is a high risk of leakage the information if they are published. In this paper, we proposed a scene text erasing method to properly hide the information via an inpainting convolutional neural network (CNN) model. The input is a scene text image, and the output is expected to be text erased image with all the character regions filled up the colors of the surrounding background pixels. This work is accomplished by a CNN model through convolution to deconvolution with interconnection process. The training samples and the corresponding inpainting images are considered as teaching signals for training. To evaluate the text erasing performance, the output images are detected by a novel scene text detection method. Subsequently, the same measurement on text detection is utilized for testing the images in benchmark dataset ICDAR2013. Compared with direct text detection way, the scene text erasing process demonstrates a drastically decrease on the precision, recall and f-score. That proves the effectiveness of proposed method for erasing the text in natural scene images.", "creator": "LaTeX with hyperref package"}}}