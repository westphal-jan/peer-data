{"id": "1705.05108", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Kernel Truncated Regression Representation for Robust Subspace Clustering", "abstract": "subspace clustering aims to group data points over clustered clusters of which each corresponds to one subspace. most existing subspace clustering methods assume, the data could be entirely represented with each other in the input space. in practice, however, this assumption is hard to be satisfied. to achieve nonlinear subspace clustering, we propose a novel method which consists of the following three steps : 1 ) projecting the data into a hidden space thy respect the data can receive linearly reconstructed neighboring each other ; 2 ) calculating the globally linear reconstruction coefficients in the kernel space ; r ) translating the similarity coefficients to achieve robustness and wide - diagonality, and then achieving clustering by solving a graph laplacian problem. our approximation has the advantages of a closed - form solution and capacity of concurrent data points that overlap in nonlinear subspaces. the first advantage makes our method efficient driver handling unit - scale data sets, and to second one enables the dynamic method to address the nonlinear subspace clustering challenge. extensive experiments of five long - domain datasets demonstrate the effectiveness and software efficiency enabled the proposed method in comparison with ten state - of - the - art approaches regarding four evaluation metrics.", "histories": [["v1", "Mon, 15 May 2017 08:16:34 GMT  (1414kb)", "https://arxiv.org/abs/1705.05108v1", null], ["v2", "Tue, 23 May 2017 15:33:56 GMT  (1414kb)", "http://arxiv.org/abs/1705.05108v2", "12 pages"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["liangli zhen", "dezhong peng", "xin yao"], "accepted": false, "id": "1705.05108"}, "pdf": {"name": "1705.05108.pdf", "metadata": {"source": "CRF", "title": "Kernel Truncated Regression Representation for Robust Subspace Clustering", "authors": ["Liangli Zhen", "Dezhong Peng", "Xin Yao"], "emails": ["llzhen@outlook.com).", "(pengdz@scu.edu.cn).", "x.yao@cs.bham.ac.uk)."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n05 10\n8v 2\n[ cs\n.C V\n] 2\n3 M\nay 2\n01 7\nIndex Terms\u2014Kernel truncated regression; nonlinear subspace clustering; spectral clustering; kernel techniques.\nI. INTRODUCTION\nSubspace clustering is one of the most popular techniques for data analysis, which has attracted increasing interests from numerous areas, such as computer vision, image analysis, and signal processing [1]. With the assumption of highdimensional data lying in a union of low-dimensional subspaces, subspace clustering aims to seek a set of subspaces to fit a given data set and perform clustering based on the identified subspaces.\nDuring past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320]. In recent years, spectral clustering-based approaches have achieved the state-of-the-art in subspace clustering, of which the key is finding a blockdiagonal affinity matrix, where the element of the matrix denotes the similarity between two data points and the block-\nLiangli Zhen is with the Machine Intelligence Laboratory, College of Computer Science, Sichuan University, Chengdu 610065, China, and the CERCIA, School of Computer Science, University of Birmingham, Birmingham B15 2TT, UK. (e-mail: llzhen@outlook.com). Dezhong Peng is with the Machine Intelligence Laboratory, College of Computer Science, Sichuan University, Chengdu 610065, China. (pengdz@scu.edu.cn). Xin Yao is with the CERCIA, School of Computer Science, University of Birmingham, Birmingham B15 2TT, UK. (e-mail: x.yao@cs.bham.ac.uk).\ndiagonal structure means that only the similarity among intracluster data points is nonzero.\nTo obtain a block-diagonal affinity matrix, most recent spectral clustering-based approaches measure the similarity using so-called self-expression, i.e. representing each data point as a linear combination of the whole data set and then using the representation coefficients to build the affinity matrix. The major difference of those methods is the constraints enforced on the representation coefficients. For example, sparse subspace clustering (SSC) [11] assumes that each data point can be linearly represented by a few of other points. To achieve this end, SSC adopts the \u21131-norm constraint. Low-rank representation (LRR) [12] encourages the coefficient matrix to be low rank, such that it can capture the global structures of data. To obtain low rankness, LRR enforces the nuclearnorm constraint on the coefficients. Different from SSC and LRR, truncated regression representation (TRR) [17, 19] takes Frobenius norm instead of \u21131- and nuclear-norm, which has shown promising performance in many real-world applications. Like most existing subspace clustering algorithms [11\u2013 13, 21], the major disadvantage of TRR is that it may not give a satisfactory clustering result when data points cannot be linearly represented with each other. In fact, many real-world data are sampled from multiple nonlinear subspaces, which brings challenges towards TRR and limits its applications in practice.\nTo group the data drawn from multiple nonlinear subspaces, in this paper, we propose a novel nonlinear subspace clustering method, termed kernel truncated regression representation (KTRR). Our basic idea is based on the following assumption, i.e. there exists a projection space in which the data can be linearly represented. To illustrate this simple but effective idea, we give a toy example in Fig. 1. The proposed method consists of the following steps: 1) projecting the input into another space via an implicit nonlinear transformation; 2) calculating the global self-expression of the whole data set in the projection space in which the data can be linearly reconstructed; 3) eliminating the effect of errors such as Gaussian noise by zeroing trivial coefficients; 4) constructing a Laplacian graph using the obtained coefficients; 5) solving a generalized Eigen-decomposition problem and obtain clustering with kmeans. The contributions and novelty of this work could be summarized as follows:\n\u2022 We propose a novel method which can cluster the data\npoints drawn from multiple nonlinear subspaces. To the best of our knowledge, this is the first nonlinear extension of TRR and one of the first several nonlinear clustering approaches.\n2\n\u2022 We develop a closed-form solution to our method. This\nmakes our method very efficient, and useful for largescale data sets. \u2022 Different from most existing subspace clustering methods\nlike SSC and LRR, KTRR achieves robustness by eliminating the impact of noises in the projection space instead of input space. In other words, KTRR does not require the prior on the structure of errors, which is more competitive to handle corrupted subspaces. Extensive experimental results show that our method significantly outperforms ten other state-of-the-art subspace clustering algorithms regarding accuracy, robustness, and computational cost.\nThe rest of this paper is organized as follows. Section II discusses some related work. Section III presents the kernel truncated regression and the new robust subspace clustering method. Section IV provides experimental results to illustrate the effectiveness and the efficiency of the proposed algorithm. Section V concludes the paper.\nNotations: In this paper, unless specified otherwise, lowercase bold letters represent column vectors, upper-case bold letters represent matrices, and the entries of matrices are denoted with subscripts. For instance, v is a column vector, vi is its ith entry. M is a matrix, Mij is the entry in the ith row, jth column, and mj denotes the jth column of M. Moreover, MT represents the transpose of M, M\u22121 denotes the inverse matrix of M, and I stands for the identity matrix. Table I summarizes some notations used throughout the paper.\nII. RELATED WORK\nDuring past decades, some spectral clustering-based methods have been proposed to achieve subspace clustering in many applications such as image clustering [21], motion segmentation [22], and gene expression analysis [23]. The key of these methods is to obtain a block-diagonal similarity matrix of which nonzero elements are only located on the connections of the points from the same subspace. There are two common strategies to compute the similarity matrix, i.e., pairwise distance-based strategy and linear representationbased strategy [16]. Pairwise distance-based strategy computes the similarity between two points according to their pairwise relationship, e.g., the original spectral clustering method adopts the Euclidean distance with Heat Kernel to calculate the similarity, i.e.,\ns(xi,xj) = e \u2212\n\u2016xi\u2212xj\u2016 2\n2\u03c32 (1)\nwhere s(xi,xj) denotes the similarity between the data point xi and data point xj , and the parameter \u03c3 controls the width of the neighborhoods.\nAlternatively, linear representation-based approaches assume that each data point could be represented as a linear combination of some points from the intra-subspace. Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.\nFor given a data matrixX = [x1,x2, . . .xn] \u2208 R m\u00d7n, these methods linearly represent X and obtain the coefficient matrix C \u2208 Rn\u00d7n in a self-expression manner by solving\nmin\u211c(C) s.t. X = XC, diag(C) = 0, (2)\nwhere diag(C) = 0 avoids the trivial solution which uses the data point to represent itself by enforcing the diagonal elements of C to be zeros. \u211c(C) denotes the adopted prior structured regularization on C and the major difference among most existing subspace clustering methods is the choice of \u211c(C). For example, SSC [11] enforces the sparsity on C by adopting \u21131-norm via \u211c(C) = \u2016C\u20161, LRR [12] obtains low rankness by using nuclear norm with \u211c(C) = \u2016C\u2016\u2217. To further achieving robustness, (2) is extended as follows:\nmin\u211c(C) + \u2118(E) s.t. X = XC+E, diag(C) = 0, (3)\nwhere E stands for the errors induced by the noise and corruption, \u2118(E) measures the impact of the errors. Generally, the \u2118(\u00b7) = \u2016E\u2016F and \u2118(\u00b7) = \u2016E\u20161 are used to describe the Gaussian noise and Laplacian noise, respectively. \u2016\u00b7\u2016F denotes the Frobenius norm\nDue to the assumption on linear reconstruction, those methods failed to achieve nonlinear subspaces clustering. To address this challenging issue, some recent works have been proposed [15, 24], however, the methods have the following two disadvantages: 1) the methods are computationally inefficient since they involve solving \u21131- or nuclear-norm minimization problem; 2) Like SSC and LRR, the methods\n3 need the prior on the errors existed in the data sets to get the correct mathematical formulation. If the prior is inconsistent with the real situation, the methods could achieve inferior performance. To solve these issues, we propose a nonlinear subspace clustering method which is complementary to existing approaches. Noticed that, Peng at al. recently proposed to achieve nonlinearity with deep structures [27, 28], which are first works to leverage deep learning and subspace clustering. However, this has been beyond of the scope of this paper.\nIII. THE PROPOSED SUBSPACE CLUSTERING METHOD\nThis section will give the details of our proposed method, which consists of three steps: 1) calculating the kernel truncated regression representation over the whole data set. 2) eliminating the effectiveness of possible errors such as noises from the representation and then building a graph Laplacian. 3) obtaining clustering by performing the k-means algorithm on leading eigenvectors of the graph Laplacian. Moreover, we also give the computational complexity of the proposed method."}, {"heading": "A. Kernel Truncated Regression Representation", "text": "For a given data set {xi} n i=1, where xi \u2208 R m, we define a matrix X = [x1,x2, . . .xn]. Let \u03c6: R m \u2192 H be a nonlinear mapping which transforms the input into a kernel spaceH, and \u03c6(Xi) = [\u03c6(x1), . . . , \u03c6(xi\u22121),0, \u03c6(xi+1), . . . , \u03c6(xn)]. After mapping X into a kernel space, the corresponding {\u03c6(xi)} n i=1 is generally believed lying in linear subspaces [15, 24]. Based on this basic idea, we propose to formulate the objective function of our KTRR as follows:\nmin ci\n1 2 \u2016\u03c6(xi)\u2212 \u03c6(Xi)ci\u2016 2 2 + \u03bb 2 \u2016ci\u2016 2 2, (4)\nwhere the first term is the reconstruction error in the kernel space, the second term serves as an \u21132-norm regularization, and \u03bb is a positive real number, which controls the strength of the \u21132-norm regularization term.\nFor each transformed data representation \u03c6(xi), solving the optimization problem (4), it gives that\nci = ( \u03c6(Xi) T\u03c6(Xi) + \u03bbI )\u22121 \u03c6(Xi)\nT\u03c6(xi), (i = 1, \u00b7 \u00b7 \u00b7 , n). (5)\nNote that it requires O(n4 + mn2) for solving the above problems of n data points with dimensionality of m.\nTo solve (4) more efficiently, we rewrite it as\nmin ci\n1 2 \u2016\u03c6(xi)\u2212 \u03c6(X)ci\u2016 2 2 + \u03bb 2 \u2016ci\u2016 2 2, s.t. e T i ci = 0,\n(6)\nwhere ei is a column vector with all zero elements except the i-th element is 1, and the constraint eTi ci = 0 eliminates the trivial solution of writing a transformed point as a linear combination of itself.\nUsing Lagrangian method, we obtain that\nL(ci) = 1\n2 \u2016\u03c6(xi)\u2212 \u03c6(X)ci\u2016\n2 2 +\n\u03bb 2 \u2016ci\u2016 2 2 + \u03b8e T i ci, (7)\nwhere \u03b8 is the Lagrangian multiplier. Clearly,\n\u2202L(ci)\n\u2202ci = (\u03c6(X)T \u03c6(X) + \u03bbI)ci \u2212 \u03c6(X) T \u03c6(xi) + \u03b8ei. (8)\nLet \u2202L(ci) \u2202ci = 0, we get\nci = (\u03c6(X) T \u03c6(X) + \u03bbI)\u22121(\u03c6(X)T \u03c6(xi)\u2212 \u03b8ei). (9)\nMultiplying ei on both sides of (9), and since e T i ci = 0, it\nholds that\n\u03b8 = eTi (\u03c6(X) T \u03c6(X) + \u03bbI)\u22121\u03c6(X)T \u03c6(xi)\neTi (\u03c6(X) T \u03c6(X) + \u03bbI)\u22121ei\n. (10)\nSubstituting (10) into (9), the optimal solution is given as\nci = qi \u2212P eTi qiei\neTi Pei , (11)\nwhere qi = P(\u03c6(X) T \u03c6(xi)), and P = (\u03c6(X) T\u03c6(X)+\u03bbI)\u22121. One can find that the solution to (11) does not require \u03c6(xi) to be explicitly computed, i.e. we will only need their dot products. Therefore, we can employ kernel functions for computing these dot products without explicitly performing the mapping \u03c6. For some choices of a kernel \u03ba(xi,xj): R\nm\u00d7Rm \u2192 R, [29] has shown that \u03ba can get the dot product in the kernel space H induced by the mapping \u03c6. We can combine all the dot products as a matrix K \u2208 R N\u00d7N whose elements are calculated as\nKij = \u03c6(xi) T\u03c6(xj) = [\u03c6(X) T\u03c6(X)]ij = \u03ba(xi,xj), (12)\nwhere \u03c6(X) = [\u03c6(x1), \u03c6(x2), . . . \u03c6(xn)]. The matrix K is the kernel matrix, which is a symmetric and positive semidefinite matrix. Accordingly, (11) can be rewritten as\nc\u2217i = vi \u2212U eTi viei\neTi Uei , (13)\nwhere vi = Uki, and U = (K+ \u03bbI) \u22121.\nIt is notable that only one pseudo-inverse operation is needed for solving the representation problems of all data points. The computational complexity of calculating the optimal solutions in (13) has decreased to O(n3 +mn2) for n data points with m dimensions.\nIt has been proved that, under certain condition, the coefficients over intra-subspace data points are larger than those over inter-subspace data points [17]. After representing the data set by the kernel matrix via (13), we handle the errors by performing a hard thresholding operator T\u03b7(\u00b7) over ci, where T\u03b7(\u00b7) keeps \u03b7 largest entries in ci and sets other entries as zeros like TRR, i.e.,\nT\u03b7(ci) = [T\u03b7(C1i), T\u03b7(C2i), . . . , T\u03b7(Cni)] T (14)\nand\nT\u03b7(Cji) =\n{\nCji, if Cji \u2208 \u2126i; 0, otherwise, (15)\nwhere \u2126i consists of \u03b7 largest elements of ci. Typically, the optimal \u03b7 equals to the dimensionality of corresponding kernel\n4 subspace. In this manner, it avoids to model the impact of the noises into the optimization problem explicitly and does not need the prior knowledge about the errors."}, {"heading": "B. KTRR for Robust Subspace Clustering", "text": "In this section, we present the method to achieve subspace clustering by incorporating the KTRR into spectral clustering framework [8].\nFor a given data set X which consists of n data points in R m, we assume that these points should be lying in a union of L low-dimensional nonlinear subspaces. We propose to project the data points into another space, in which the mapped points can be linearly represented by these mapped points from the intra-subspace. From (11), we find that the representation coefficients does not require the projection function in explicit form, but are only needed in dot products. We can induce a kernel function to calculate these dot products, and obtain the representation coefficients via (13).\nMoreover, the existence of the errors in the input data set leads to some error connections among the data points from different subspaces. We propose to remove these errors through a hard thresholding on each column vector of the coefficient matrix C via (14).\nAs we claimed before, these representation coefficients can be seen as the similarities among the input data points. The similarity between two intra-subspace data points is large, and that between two inter-subspace data points is zero or very close to zero. Such that we can build a similarity matrix W based on the obtained coefficient matrix C as\nW = |CT |+ |C|. (16)\nThis is a symmetric similarity matrix which is suitable for integrating into the spectral clustering framework.\nThen, we compute the normalized Laplacian matrix [8]\nL = I\u2212D\u2212 1 2WD\u2212 1 2 , (17)\nwhere D is a diagonal matrix with Dii = \u2211n\nj=1 Wij . The\nmatrix L is positive semi-definite and has an eigenvalue equals 0 with eigenvector D 1\n2 1 [9], where 1 = [1, 1, . . . , 1]T \u2208 Rn. Next, we calculate the first L eigenvectors y1,y2, . . . ,yL of L, which corresponding to its first L smallest nonzero eigenvalues, and construct a matrix Y = [y1,y2, . . . ,yL] \u2208 R n\u00d7L.\nFinally, we apply the k-means clustering method on the matrix Y, by treating each row vector as a point, to get the clustering membership. The proposed subspace clustering algorithm is summarized in Algorithm 1."}, {"heading": "C. Computational Complexity Analysis", "text": "Given a data matrix X \u2208 Rm\u00d7n, the KTRR takes O(mn2) to compute the kernel matrix K. Then it takes O(n3) to obtain the matrixU, and O(mn2) to calculate all the solutions in (13) with the matrices U and K. Finally, it requires O(\u03b7log\u03b7) to find \u03b7 largest coefficients in each column of the representation matrix C. Putting these steps together, we get the computational complexity of KTRR as O(mn2+n3). This computational complexity is the same as that of TRR, and is\nAlgorithm 1 Learning kernel truncated regression representation for robust subspace clustering Input: A given data set X \u2208 Rm\u00d7n, the tradeoff parameter \u03bb, thresholding parameter \u03b7, and the number of subspaces\nL.\nOutput: The clustering labels of the input data points.\n1: Calculate the kernel matrix K and the matrix U in (13)\nand store them.\n2: For each point xi \u2208 R m, calculate its linear representation\ncoefficients in the kernel space ci \u2208 R n via (13).\n3: Remove the trivial coefficients from ci by performing hard\nthresholding operator T\u03b7(ci), i.e., keeping \u03b7 largest entries in ci and zeroing all other elements.\n4: Construct a symmetric similarity matrix via (16). 5: Calculate the normalised Laplacian matrix L via (17). 6: Compute the eigenvector matrix Y \u2208 Rn\u00d7L that consists of the first L normalized eigenvectors of L corresponding\nto its L smallest nonzero eigenvalues.\n7: Perform k-means clustering algorithm over the rows of Y\nto get the clustering membership.\nconsiderably less than that of KSSC (O(mn2 + tn3))[30], KLRR(O(t(rX + r)n\n2)[24], where t denotes the total number of iterations for the corresponding algorithm, rX is the rank of X, and r is the rank for partial SVD at each iteration of KLRR.\nIV. EXPERIMENTAL RESULTS AND ANALYSIS\nIn this section, we experimentally evaluate the performance of the proposed method. We consider the results in terms of three aspects: 1) accuracy, 2) robustness, and 3) computational cost. Robustness is evaluated by conducting experiments using samples with two different types of corruptions, i.e., Gaussian noises and random pixel corruption."}, {"heading": "A. Databases", "text": "Five popular image databases are used in our experiments, including Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], Columbia Object Image Library (COIL 100) [33], USPS [34], and MNIST [35]. We give the details of these databases as follows:\n\u2022 The ExYaleB database contains 2414 frontal face images of 38 subjects and around 64 near frontal images under different illuminations per individual, where each image\nis manually cropped and normalized to the size of 32\u00d732 pixels [36]. \u2022 The COIL 20 and COIL 100 databases contain 20 and\n100 objects respectively. The images of each object were taken 5 degrees apart as the object is rotated on a turntable and each object has 72 images. The size of each image is 32\u00d7 32 pixels, with 256 grey levels per pixel [36]. \u2022 The USPS handwritten digit database1 includes ten\nclasses (0\u22129 digit characters) and 11000 samples in total.\n1The USPS database and MNIST database used in this paper are download from http://www.cad.zju.edu.cn/home/dengcai/Data/MLData.html.\n5 We use a popular subset contains 9298 handwritten digit images for the experiments, and all of these images are normalized to the size of 16\u00d716 pixels. In the experiment, we select 200 samples of each subject from the database randomly by following the strategy in [10].\n\u2022 The MNIST handwritten digit database includes ten\nclasses (0 \u2212 9 digit characters) and 60000 samples in total. We use first 10000 handwritten digit images of the training subset to conduct the experiments, and all of these images are normalized to the size of 28 \u00d7 28 pixels. In the experiment, we also select 200 samples of each subject from the database randomly to evaluate the performance of different algorithms.\nThe details of these real-world databases are summarized in\nTable II"}, {"heading": "B. Baselines and Evaluation Metrics", "text": "We compare KTRR2 with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with \u21131-norm [12], low-rank representation (LRR2) with \u211321-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].\nFor a fair comparison, we use the same spectral clustering framework [8] with different similarity matrices obtained by the tested algorithms. Like [24], for all kernel-based algorithms, we adopt the commonly used Gaussian kernel on all datasets and use the default bandwidth parameter which is set to the mean of the distances between all the samples.\nFour popular metrics are adopted to evaluate the subspace clustering quality, i.e., accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40]. The values of above four metrics are higher if the method works better. The values of these four metrics are equal to 1 indicates the predict results is perfectly matching with the ground truth, whereas 0 indicates totally mismatch.\n2The source code of our proposed method are available at https://www.dropbox.com/s/8vj1k1b184w2ksv/KTRR.zip?dl=0.\nC. Visualisation of Representation and Similarity Matrices\nBefore evaluating the clustering performance of the proposed method, we illustrate the visualization results of the KTRR coefficients matrix and the obtained similarity matrix. We get the result by using the first 128 facial images in the ExYaleB database, first 64 samples of which belong to the first subject, and the other 64 samples belong to the second subject. We set the parameters as \u03bb = 5, \u03b7 = 4. The representation matrix C in (13) and the constructed similarity matrix W are shown in Fig. 2(a) and Fig. 2(b) respectively.\nFrom Fig. 2(a), we can see that the upper-left part and the bottom-right part are illuminated from the upper-right part and the bottom-left part, but there still exist some non-zero elements in the upper-right part and the bottom-left part. That is to say, the connections among the same subject are much stronger than that among different subjects, while there are many trivial connections among the samples from different subjects since the samples from various subjects are facial images, which have some common characteristics.\nAs we know, an ideal similarity matrix for the spectral clustering algorithm is a block diagonal matrix, i.e., the connections should only exist among the data points from the same cluster [12, 15, 17, 24, 26], such that a hard thresholding operation has been executed. From the result of similarity matrix W in Fig. 2(b), we find that:\n\u2022 Our method reveals the latent structure of data though\nthese images belong to the two subjects. There exist only a few bright spots in the upper-right part and the bottomleft part of the obtained similarity matrix, i.e., the trivial connections among the samples from different subjects have been mostly removed by using the thresholding processing; \u2022 Almost all of the bright spots lie in the diagonal blocks\nof the similarity matrix, i.e., the strong connections exist among the samples from the same subject; \u2022 The obtained similarity matrix is a symmetric matrix\nwhich can be directly used for subspace clustering under the framework of spectral clustering [8]."}, {"heading": "D. Clustering on Clean Images", "text": "In this experiment, we compare the KTRR method with other ten state-of-the-art approaches on four different benchmark databases, i.e., Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], USPS [34], MNIST [35]. For each dataset, we perform each algorithm 10 runs, in each run the k-means clustering step are repeated 500 times, and report the mean and the standard deviation of the used metrics. The clustering quality on above four databases are shown in Table III - Table VI. The better means for each database are highlighted in boldface. To have statistically sound conclusions, the Wilcoxon\u2019s rank sum test [41] at a 0.05 significance level is adopted to test the significance of the differences between the results obtained by the proposed method and all other algorithms. From the results, we can obtain the following conclusions.\n(1) Evaluation on the ExYaleB facial database:\n6 (a) (b)\nFig. 2. The visualization of the representation matrix and the similarity matrix on 128 facial images which from first 2 subjects in ExYaleB database. (a) The representation matrix in (13). (b) The similarity matrix obtained by our algorithm. The experiment was carried out on the first two subjects of ExYaleB. The top rows and the right columns illustrate some images of these two subjects. The dotted lines split each matrix into four parts. The upper-left part: the similarity relationship among the 64 images of the first subject. The bottom-right part: the similarity relationship among the 64 images of the second subject. The upper-right part and the bottom-left part: the similarity relationship among the images from different subjects. From the connections, it is easy to find that the upper-left part and the bottom-right part are illuminated from the upper-right part and the bottom-left part, which means that our method reflects the correct relationship among the samples from different subjects.\n7 \u2022 All linear and non-linear representation methods, i.e.,\nKTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8]. \u2022 All the linear representation methods, i.e., TRR [17],\nLRR [12], and SSC [11], are inferior to their kernel-based extensions, i.e., KTRR, KLRR [24], and KSSC [15]. It means that the non-linear representation methods are more suitable to model the ExYaleB facial images. \u2022 The KTRR algorithm achieves the best results in the\ntests and gains a significant improvement over TRR. The means of Accuracy, NMI , ARI , and Fscore of KTRR are about 17%, 17%, 26% and 24% higher than that of the TRR, 32%, 28%, 41%, and 40% higher than that of the KLRR.\n(2) Evaluation on the COIL 20 database:\n\u2022 All the linear representation methods, i.e., TRR [17],\nLRR [12], and SSC [11], are still inferior to their kernel-based extensions, i.e., KTRR, KLRR [24], and KSSC [15]. Their non-linear versions obtain the Accuracy improvements of 6.13%, 0.68%, and 10%, respectively. \u2022 The KTRR algorithm gets the Accuracy of 90.25%, which is better than all other tested methods. Specifically,\nthe Accuracy of KTRR is about 6.13% higher than that of the second best method TRR, and 21.86% higher than that of the third best method KLRR. \u2022 The KLRR, LatLRR and two types of LRR methods are\nall inferior to the standard spectral method.\n(3) Evaluation on the USPS handwriting database:\n\u2022 All the linear representation methods, i.e., TRR [17],\nLRR [12], and SSC [11], are inferior to their kernel-based extensions, i.e., KTRR, KLRR [24], and KSSC [15]. The performance improvement is considerable, e.g., the Accuracy of KSSC is about 44% higher than that of SSC. \u2022 SSC is inferior to LRR, while its kernel-based extension\nKLRR outperforms the kernel-based extension of LRR. The implicit transformation on the USPS images makes the mapped data points to be much better represented with each other in a sparse representation form. \u2022 The KTRR algorithm achieves the best results in the tests.\nThe Accuracy of KTRR is about 21% higher than that of the TRR, 10% higher than that of the KLRR, and 6% higher than that of the KSSC. The performance indices of KTRR on NMI , ARI , and Fscore are also greater than other tested methods.\n(4) Evaluation on the MNIST handwriting database:\n\u2022 All the linear representation methods, i.e., TRR [17],\nLRR [12], and SSC [11], are inferior to their kernel-based extensions, i.e., KTRR, KLRR [24], and KSSC [15]. Especially, LRR results in poor performance on this database, while its kernel-based version, KLRR, obtains much better clustering quality regarding Accuracy, NMI , ARI , and Fscore. \u2022 The KTRR, KLRR, SMCE and LSA algorithms achieve\nthe best clustering results on the MNIST handwriting\nimages compared with other methods. However, the performances of all the test methods are not well. \u2022 The proposed method KTRR achieves the best clustering\nresult and obtains a significant improvement of 31.78% at Accuracy on TRR. The indexes NMI , ARI , and Fscore of KTRR are also higher than all other tested methods."}, {"heading": "E. Clustering on Corrupted Images", "text": "To evaluate the robustness of the proposed method, we conduct the experiments on the first 10 subjects of COIL20 database and ExYaleB database respectively. All used images are corrupted by additive white Gaussian noises or random pixel corruptions. Some corrupted image samples under different levels of noises are as shown in Fig. 3. Actually, for the additional Gaussian noises, we add the noises with SNR equals 10, 20, 30, 40, 50 dB; For the random pixel corruptions, we adopt the pepper & salt noises with the ratios of affected pixels be 5%, 10%, 15%, 20%, 25%. The clustering quality of the compared methods on the two databases with additional Gaussian noises is shown in Fig. 4, from which we can get the following observations:\n\u2022 Most of these spectral-based methods are relatively robust\nto the additional Gaussian noises. While the performance of LRR1, LRR2, and LatLRR are sharply deteriorated on these two databases. The main reason may be that the additional Gaussian noises have destroyed the underlie the low-rank structure of the representation matrix. \u2022 The accuracy of all tested methods on COIL20 database\nare higher than that on ExYaleB database. It is consistent with the result of that on clean images. \u2022 The proposed KTRR is considerably more robust than\nother methods for additional Gaussian noises. Specifically, KTRR obtains the Accuracy around 80% under SNR = 10dB, which are much higher than all other tested algorithms, especially SC, LSA, LRR1, and LRR2.\nThe clustering quality of the compared methods on the images with randomly corruptions is shown in Fig. 5, from which we obtain that:\n\u2022 All the investigated methods perform not as well as the\ncase with white Gaussian noise. The result is consistent with a widely-accepted conclusion that non-additive corruptions are more challenging than additive ones in pattern recognition; \u2022 All of the test algorithms perform much better on COIL20\ndatabase than on ExYaleB database. The Accuracy of all algorithms are lower than 40% on ExYaleB under 20% and 25% of corrupted pixels. From the Fig. 3, we find that most pixel values of the images from COIL20 database are close to 0 or 255. This leads to some of the corruptions to be useless and weakens the impact to the final clustering results. \u2022 The KTRR algorithm is robust to the random pixel\ncorruptions. It achieves the best results under the ratio of affected pixels equals 5% to 15% on the test two databases. It obtains the Accuracy around 60% under the ratio of affected pixels equals 25% on COIL20 database,\n8 (a) (b)\nwhich is a very challenging situation that we can see in Fig. 3. However, the Accuracy of KTRR drops severely with the increase of the ratios of corrupted pixels, and lower than that of SSC under 20% and 25% of corrupted pixels. The KTRR should be improved to handle the images with salt & pepper corruptions."}, {"heading": "F. Computational Time", "text": "To investigate the efficiency of KTRR, we compare its computational time with that of other 10 approaches on the clean images of four databases. Our hardware configuration comprises of a 2.4-GHz CPU and a 16 GB RAM. The time cost for building similarity graph (t1) and the whole time cost\nfor clustering (t2) are recorded to evaluate the efficiency of compared methods.\nTable VII shows the time cost of different methods with the parameters which achieve their best results. We can see that:\n\u2022 The standard SC [8] is the fastest since its similarity graph\nis computing via the pairwise kernel distances among the input samples, while the KSSC [15] is the most timeconsuming method. \u2022 The time cost of the proposed method is very close to\nthat of its linear version TRR [17]. Specifically, the TRR method is faster than KTRR on ExYaleB and USPS databases, while it is slower than KTRR on MNIST database. They have similar time cost on COIL database.\n9\nTABLE VII COMPUTATIONAL TIME (SECONDS) COMPARISONS OF DIFFERENT METHODS ON THE EXYALEB, COIL 20, USPS, AND MNIST DATABASES. THE t1 AND t2 DENOTE THE TIME COST ON THE SIMILARITY GRAPH CONSTRUCTION PROCESS AND THE TIME COST ON THE WHOLE CLUSTERING PROCESS OF EACH METHOD RESPECTIVELY. THE BEST MEAN RESULTS IN DIFFERENT METRICS ARE IN BOLD.\nDatabases KTRR TRR KLRR KSSC LatLRR LRR1 LRR2 SSC SMCE LSA SC ExYaleB t1 22.96 23.71 45.82 5512.68 772.44 248.94 270.65 2301.75 10.15 198.48 0.33 t2 47.62 48.8 71.26 5543.4 806.43 286.91 311.34 2313.31 45.18 229.26 124.45 COIL 20 t1 6.50 6.54 16.11 1466.12 579.01 430.23 454.87 121.76 5.76 61.14 0.15 t2 11.66 12.75 25.55 1472.07 584.46 436.59 460.07 126.88 10.12 66.01 7.61 USPS t1 16.92 11.95 29.41 2752.97 50.05 43.34 49.10 62.25 67.44 108.67 0.14 t2 27.82 22.74 39.93 2763.19 58.98 51.88 58.90 98.79 76.50 120.46 11.73 MNIST t1 22.56 22.43 34.20 5742.89 246.35 155.70 172.32 112.13 16.78 142.71 1.09 t2 33.96 32.35 44.64 5753.42 270.19 167.25 186.82 153.23 25.52 154.64 14.65\nThe Wilcoxon\u2019s rank sum test [41] at a 0.05 significance level shows there is no significant difference between the time costs of the KTRR and TRR on the similarity graph construction and the whole clustering process on the tested four databases. \u2022 The KTRR and TRR [17] algorithms are much faster\nthan KSSC, SSC, KLRR, and LRR methods. The results are consist with the fact that the theocratical computation complexities of KTRR and TRR are much lower than that of KSSC, SSC, KLRR, and LRR methods. The KTRR and TRR [17] algorithms both have analytical solutions, and only one pseudo-inverse operation is required for solving the representation problems of all data points for KTRR and TRR algorithms."}, {"heading": "G. Clustering Performance with Varying Number of Subjects", "text": "In this subsection, we investigate the clustering performance of the proposed method with a different number of subjects on COIL 100 image database. The experiments are carried out on the first t classes of the database, where t increases from 10 to 100 with an interval of 10. The clustering results are shown in Fig. 6.\nFrom the results, we can see that:\n\u2022 In general, with the number of subjects increase, the\nclustering performance is decreased since the clustering difficulty is increasing with the number of subjects growth. \u2022 With increasing number of subjects, the NMI of KTRR\nis changed slightly, varying from 100% to 90%. The\npossible reason is that the NMI is robust to the data distribution (increasing subject number) [19]. \u2022 The proposed method obtains satisfactory performance on\nCOIL 100 database. It achieves perfect clustering result for t = 10, and gets the satisfactory performance at t = 100 with Accuracy, NMI , ARI , and Fscore be around 74%, 90%, 68%, and 68%, respectively."}, {"heading": "H. Parameter Analysis", "text": "The KTRR has two parameters, the tradeoff parameter \u03bb and the thresholding parameter \u03b7. The selection of the values of the parameters depends on the data distribution. A bigger \u03bb is suitable for highly corrupted databases, and \u03b7 corresponds to the dimensionality of the corresponding subspace for the mapped data points.\nTo evaluate the impact of \u03bb and k, we conduct the experiment on the ExYaleB and COIL20 databases. We set the \u03bb from 10\u22125 to 102, and \u03b7 from 1 to 50, the results are shown in Fig. 7 and Fig. 8 .\nFrom the results, we get the following observations:\n\u2022 The proposed method achieves the best clustering perfor-\nmance with \u03bb and \u03b7 as 0.1 and 5 on ExYaleB database, and 10 and 4 on COIL20 database, respectively. \u2022 The proposed method can obtain satisfactory performance\nwith \u03bb from 0.1 to 1 on ExYaleB database, where the Accuracy, NMI , ARI , and Fscore are more than 85%, 90%, 75%, and 75%, respectively, and with \u03bb from 0.2 to 100 on COIL20 database, where the Accuracy, NMI , ARI , and Fscore are more than 80%, 90%, 80%, and 80%. The performance of KTRR is not sensitive to the parameter of \u03bb, which makes KTRR be suitable for the real applications. \u2022 The clustering quality with \u03b7 from 3 to 10 on ExYaleB and COIL20 databases are much better than other cases. It\nmeans that the thresholding process is helpful to improve the performance of KTRR, and the dimensionality of underlying subspaces of the ExYaleB and COIL20 databases in the hidden space belongs the scope from 3 to 10."}, {"heading": "I. Different Kernel Functions", "text": "The commonly used kernel functions are polynomial kernels, radial basis functions, and sigmoid kernels. To investigate the performance of the proposed method using different kernels, we study six different kernel functions. The results on\n10\nUSPS and MINIST databases are shown in Table VIII, from which we can get the following observations:\n\u2022 The kernel function \u03ba(xi,xj) = e \u2212\n\u2016xi\u2212xj\u2016 2\n\u03c32 achieves the\nbest performance on USPS database. While the kernel function \u03ba(xi,xj) = (x T i xj) 2 obtains the best performance on MNIST database.\n\u2022 The kernel function \u03ba(xi,xj) = (x T i xj) 3 outperforms\n\u03ba(xi,xj) = (x T i xj) 2 on USPS database, which is different to that on MNIST database. It is mainly caused by the fact that the images from USPS database lie in much higher nonlinear subspaces than that from MNIST database, and the former function induced a much more nonlinear mapping. \u2022 The selection of different kernels results in a great\ndifference in the subspace clustering performance both on USPS database and MNIST database.\nV. CONCLUSION\nIn this paper, we have incorporated the kernel technique into TRR method to achieve robust nonlinear subspace clustering. It does not need the prior knowledge about the structure of errors in the input data and remedies the drawback of the existing TRR method that it cannot deal with the data points from nonlinear subspaces. Moreover, through the theoretical analysis of our proposed mathematical model, we find that the developed optimization problem can be solved analytically, and the closed-form solution is only dependent on the kernel matrix. Theses advantages make our proposed method useful in many real-world applications. Comprehensive experiments\n11\non four real-world image databases have demonstrated the effectiveness and the efficiency of the proposed method.\nIn the future, we plan to conduct a systematical investigation on the selection of optimal kernel for our proposed method and study how to determine the number of nonlinear subspaces automatically.\nACKNOWLEDGMENT\nThis work was supported in part by the National Natural Science Foundation of China under grants 61432012, 61329302, and the Engineering and Physical Sciences Research Council (EPSRC) of U.K. under grant EP/J017515/1.\nREFERENCES\n[1] L. Parsons, E. Haque, and H. Liu, \u201cSubspace clustering for high dimensional data: a review,\u201d ACM SIGKDD Explorations Newsletter, vol. 6, no. 1, pp. 90\u2013105, 2004. [2] P. S. Bradley and O. L. Mangasarian, \u201ck-plane clustering,\u201d Journal of Global Optimization, vol. 16, no. 1, pp. 23\u201332, 2000. [3] S. R. Rao, R. Tron, R. Vidal, and Y. Ma, \u201cMotion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories,\u201d in Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008, pp. 1\u20138. [4] H. Derksen, Y. Ma, W. Hong, and J. Wright, \u201cSegmentation of multivariate mixed data via lossy coding and compression,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 9, p. 15461562, 2007. [5] J. P. Costeira and T. Kanade, \u201cA multibody factorization method for independently moving objects,\u201d International Journal of Computer Vision, vol. 29, no. 3, pp. 159\u2013179, 1998. [6] C. W. Gear, \u201cMultibody grouping from motion images,\u201d International Journal of Computer Vision, vol. 29, no. 2, pp. 133\u2013 150, 1998. [7] R. Vidal, Y. Ma, and S. Sastry, \u201cGeneralized principal component analysis,\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 12, pp. 1945\u20131959, 2005. [8] A. Y. Ng, M. I. Jordan, and Y. Weiss, \u201cOn spectral clustering: Analysis and an algorithm,\u201d in Advances in Neural Information Processing Systems, vol. 14, 2002, Conference Proceedings, pp. 849\u2013856. [9] U. Von Luxburg, \u201cA tutorial on spectral clustering,\u201d Statistics and Computing, vol. 17, no. 4, pp. 395\u2013416, 2007. [10] B. Cheng, J. Yang, S. Yan, Y. Fu, and T. S. Huang, \u201cLearning with \u21131-graph for image analysis,\u201d IEEE Transactions on Image Processing, vol. 19, no. 4, pp. 858\u2013866, April 2010. [11] E. Elhamifar and R. Vidal, \u201cSparse subspace clustering: Algorithm, theory, and applications,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 11, pp. 2765\u2013 2781, 2013. [12] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, \u201cRobust recovery of subspace structures by low-rank representation,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 171\u2013184, 2013. [13] C. Y. Lu, H. Min, Z. Q. Zhao, L. Zhu, D. S. Huang, and S. C. Yan, \u201cRobust and efficient subspace segmentation via least squares regression,\u201d in Proc. of 12th Eur. Conf. Comput. Vis., Florence, Italy, Oct. 2012, pp. 347\u2013360. [14] C. Lu, J. Tang, M. Lin, L. Lin, S. Yan, and Z. Lin, \u201cCorrentropy induced l2 graph for robust subspace clustering,\u201d in 2013 IEEE International Conference on Computer Vision, Dec 2013, pp. 1801\u20131808. [15] V. M. Patel and R. Vidal, \u201cKernel sparse subspace clustering,\u201d in Image Processing (ICIP), 2014 IEEE International Conference on. IEEE, 2014, Conference Proceedings, pp. 2849\u20132853.\n[16] L. Zhen, Z. Yi, X. Peng, and D. Peng, \u201cLocally linear representation for image clustering,\u201d Electronics Letters, vol. 50, no. 13, pp. 942\u2013943, 2014. [17] X. Peng, Z. Yi, and H. Tang, \u201cRobust subspace clustering via thresholding ridge regression,\u201d in The Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015, Conference Proceedings, pp. 3827\u20133833. [18] H. Liu, T. Liu, J. Wu, D. Tao, and Y. Fu, \u201cSpectral ensemble clustering,\u201d in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp. 715\u2013724. [19] X. Peng, H. Tang, L. Zhang, Z. Yi, and S. Xiao, \u201cA unified framework for representation-based subspace clustering of outof-sample and large-scale data,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 27, no. 12, pp. 2499\u20132512, Dec 2016. [20] X. Peng, C. Lu, Y. Zhang, and H. Tang, \u201cConnections between nuclear norm and frobenius norm based representation,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. PP, no. 99, pp. 1\u20137, 2016. [21] E. Elhamifar and R. Vidal, \u201cSparse subspace clustering,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, 2009, Conference Proceedings, pp. 2790\u20132797. [22] S. R. Rao, R. Tron, R. Vidal, and Y. Ma, \u201cMotion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories,\u201d in Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008, pp. 1\u20138. [23] Z. Yu, H.-S. Wong, and H. Wang, \u201cGraph-based consensus clustering for class discovery from gene expression data,\u201d Bioinformatics, vol. 23, no. 21, pp. 2888\u20132896, 2007. [24] S. Xiao, M. Tan, D. Xu, and Z. Y. Dong, \u201cRobust kernel lowrank representation,\u201d IEEE Transactions on Neural Networks and Learning Systems, 2015. [25] G. Liu and S. Yan, \u201cLatent low-rank representation for subspace segmentation and feature extraction,\u201d in 2011 International Conference on Computer Vision, Nov 2011, pp. 1615\u20131622. [26] E. Elhamifar and R. Vidal, \u201cSparse manifold clustering and embedding,\u201d in Advances in Neural Information Processing Systems, 2011, Conference Proceedings, pp. 55\u201363. [27] X. Peng, S. Xiao, J. Feng, W. Yau, and Z. Yi, \u201cDeep subspace clustering with sparsity prior,\u201d in Proceedings of the 25 International Joint Conference on Artificial Intelligence, New York, NY, USA, 9-15 July 2016, pp. 1925\u20131931. [Online]. Available: http://www.ijcai.org/Abstract/16/275 [28] X. Peng, J. Feng, J. Lu, W.-Y. Yau, and Z. Yi, \u201cCascade subspace clustering,\u201d in Proceedings of the 31th AAAI Conference on Artificial Intelligence. SFO, USA: AAAI, Feb. 2017, pp. 2478\u20132484. [29] J. Shawe-Taylor and N. Cristianini, Kernel methods for pattern analysis. Cambridge university press, 2004. [30] V. M. Patel and R. Vidal, \u201cKernel sparse subspace clustering,\u201d in 2014 IEEE International Conference on Image Processing (ICIP). IEEE, 2014, pp. 2849\u20132853. [31] K.-C. Lee, J. Ho, and D. J. Kriegman, \u201cAcquiring linear subspaces for face recognition under variable lighting,\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 5, pp. 684\u2013698, 2005. [32] S. K. N. S. A. Nene and H. Murase, \u201cColumbia object image library (coil-20),\u201d Report, 1996. [33] \u2014\u2014, \u201cColumbia object image library (coil-100),\u201d Report, 1996. [34] J. J. Hull, \u201cA database for handwritten text recognition re-\nsearch,\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 16, no. 5, pp. 550\u2013554, 1994. [35] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \u201cGradient-based learning applied to document recognition,\u201d Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998. [36] D. Cai, X. He, J. Han, and T. S. Huang, \u201cGraph regularized nonnegative matrix factorization for data representation,\u201d Pat-\n12\ntern Analysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 8, pp. 1548\u20131560, 2011. [37] J. Yan and M. Pollefeys, \u201cA general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate,\u201d in European conference on computer vision. Springer, 2006, pp. 94\u2013106. [38] X. Zheng, D. Cai, X. He, W.-Y. Ma, and X. Lin, \u201cLocality preserving clustering for image database,\u201d in Proceedings of the 12th annual ACM international conference on Multimedia. ACM, Conference Proceedings, pp. 885\u2013891. [39] L. Hubert and P. Arabie, \u201cComparing partitions,\u201d Journal of classification, vol. 2, no. 1, pp. 193\u2013218, 1985. [40] C. Goutte and E. Gaussier, A probabilistic interpretation of precision, recall and F-score, with implication for evaluation. Springer, 2005, pp. 345\u2013359. [41] J. D. Gibbons and S. Chakraborti, Nonparametric statistical inference. Springer, 2011."}], "references": [{"title": "Subspace clustering for high dimensional data: a review", "author": ["L. Parsons", "E. Haque", "H. Liu"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 6, no. 1, pp. 90\u2013105, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "k-plane clustering", "author": ["P.S. Bradley", "O.L. Mangasarian"], "venue": "Journal of Global Optimization, vol. 16, no. 1, pp. 23\u201332, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Motion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories", "author": ["S.R. Rao", "R. Tron", "R. Vidal", "Y. Ma"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008, pp. 1\u20138.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Segmentation of multivariate mixed data via lossy coding and compression", "author": ["H. Derksen", "Y. Ma", "W. Hong", "J. Wright"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 9, p. 15461562, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "A multibody factorization method for independently moving objects", "author": ["J.P. Costeira", "T. Kanade"], "venue": "International Journal of Computer Vision, vol. 29, no. 3, pp. 159\u2013179, 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Multibody grouping from motion images", "author": ["C.W. Gear"], "venue": "International Journal of Computer Vision, vol. 29, no. 2, pp. 133\u2013 150, 1998.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Generalized principal component analysis", "author": ["R. Vidal", "Y. Ma", "S. Sastry"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 12, pp. 1945\u20131959, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1945}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems, vol. 14, 2002, Conference Proceedings, pp. 849\u2013856.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and Computing, vol. 17, no. 4, pp. 395\u2013416, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning with l-graph for image analysis", "author": ["B. Cheng", "J. Yang", "S. Yan", "Y. Fu", "T.S. Huang"], "venue": "IEEE Transactions on Image Processing, vol. 19, no. 4, pp. 858\u2013866, April 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 11, pp. 2765\u2013 2781, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Yu", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 171\u2013184, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust and efficient subspace segmentation via least squares regression", "author": ["C.Y. Lu", "H. Min", "Z.Q. Zhao", "L. Zhu", "D.S. Huang", "S.C. Yan"], "venue": "Proc. of 12th Eur. Conf. Comput. Vis., Florence, Italy, Oct. 2012, pp. 347\u2013360.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Correntropy induced l2 graph for robust subspace clustering", "author": ["C. Lu", "J. Tang", "M. Lin", "L. Lin", "S. Yan", "Z. Lin"], "venue": "2013 IEEE International Conference on Computer Vision, Dec 2013, pp. 1801\u20131808.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel sparse subspace clustering", "author": ["V.M. Patel", "R. Vidal"], "venue": "Image Processing (ICIP), 2014 IEEE International Conference on. IEEE, 2014, Conference Proceedings, pp. 2849\u20132853.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Locally linear representation for image clustering", "author": ["L. Zhen", "Z. Yi", "X. Peng", "D. Peng"], "venue": "Electronics Letters, vol. 50, no. 13, pp. 942\u2013943, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust subspace clustering via thresholding ridge regression", "author": ["X. Peng", "Z. Yi", "H. Tang"], "venue": "The Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015, Conference Proceedings, pp. 3827\u20133833.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral ensemble clustering", "author": ["H. Liu", "T. Liu", "J. Wu", "D. Tao", "Y. Fu"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp. 715\u2013724.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A unified framework for representation-based subspace clustering of outof-sample and large-scale data", "author": ["X. Peng", "H. Tang", "L. Zhang", "Z. Yi", "S. Xiao"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 27, no. 12, pp. 2499\u20132512, Dec 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Connections between nuclear norm and frobenius norm based representation", "author": ["X. Peng", "C. Lu", "Y. Zhang", "H. Tang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. PP, no. 99, pp. 1\u20137, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2009, Conference Proceedings, pp. 2790\u20132797.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Motion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories", "author": ["S.R. Rao", "R. Tron", "R. Vidal", "Y. Ma"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008, pp. 1\u20138.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Graph-based consensus clustering for class discovery from gene expression data", "author": ["Z. Yu", "H.-S. Wong", "H. Wang"], "venue": "Bioinformatics, vol. 23, no. 21, pp. 2888\u20132896, 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust kernel lowrank representation", "author": ["S. Xiao", "M. Tan", "D. Xu", "Z.Y. Dong"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Latent low-rank representation for subspace segmentation and feature extraction", "author": ["G. Liu", "S. Yan"], "venue": "2011 International Conference on Computer Vision, Nov 2011, pp. 1615\u20131622.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse manifold clustering and embedding", "author": ["E. Elhamifar", "R. Vidal"], "venue": "Advances in Neural Information Processing Systems, 2011, Conference Proceedings, pp. 55\u201363.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep subspace clustering with sparsity prior", "author": ["X. Peng", "S. Xiao", "J. Feng", "W. Yau", "Z. Yi"], "venue": "Proceedings of the 25 International Joint Conference on Artificial Intelligence, New York, NY, USA, 9-15 July 2016, pp. 1925\u20131931. [Online]. Available: http://www.ijcai.org/Abstract/16/275", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Cascade subspace clustering", "author": ["X. Peng", "J. Feng", "J. Lu", "W.-Y. Yau", "Z. Yi"], "venue": "Proceedings of the 31th AAAI Conference on Artificial Intelligence. SFO, USA: AAAI, Feb. 2017, pp. 2478\u20132484.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}, {"title": "Kernel methods for pattern analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge university press,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Kernel sparse subspace clustering", "author": ["V.M. Patel", "R. Vidal"], "venue": "2014 IEEE International Conference on Image Processing (ICIP). IEEE, 2014, pp. 2849\u20132853.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K.-C. Lee", "J. Ho", "D.J. Kriegman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 5, pp. 684\u2013698, 2005.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Columbia object image library (coil-20)", "author": ["S.K.N.S.A. Nene", "H. Murase"], "venue": "Report, 1996.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "Columbia object image library (coil-100)", "author": ["\u2014\u2014"], "venue": "Report, 1996.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1996}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 16, no. 5, pp. 550\u2013554, 1994.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T.S. Huang"], "venue": "Pat-  12 tern Analysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 8, pp. 1548\u20131560, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate", "author": ["J. Yan", "M. Pollefeys"], "venue": "European conference on computer vision. Springer, 2006, pp. 94\u2013106.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Locality preserving clustering for image database", "author": ["X. Zheng", "D. Cai", "X. He", "W.-Y. Ma", "X. Lin"], "venue": "Proceedings of the 12th annual ACM international conference on Multimedia. ACM, Conference Proceedings, pp. 885\u2013891.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 0}, {"title": "Comparing partitions", "author": ["L. Hubert", "P. Arabie"], "venue": "Journal of classification, vol. 2, no. 1, pp. 193\u2013218, 1985.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1985}, {"title": "A probabilistic interpretation of precision, recall and F-score, with implication for evaluation", "author": ["C. Goutte", "E. Gaussier"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Subspace clustering is one of the most popular techniques for data analysis, which has attracted increasing interests from numerous areas, such as computer vision, image analysis, and signal processing [1].", "startOffset": 202, "endOffset": 205}, {"referenceID": 1, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 183, "endOffset": 189}, {"referenceID": 3, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 183, "endOffset": 189}, {"referenceID": 4, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 215, "endOffset": 220}, {"referenceID": 5, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 215, "endOffset": 220}, {"referenceID": 6, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 215, "endOffset": 220}, {"referenceID": 7, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 8, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 9, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 10, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 11, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 12, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 13, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 14, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 15, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 16, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 17, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 18, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 19, "context": "During past decades, many subspace clustering methods have been proposed, which can be roughly classified into four categories: 1) iterative approaches [2]; 2) statistical approaches [3, 4]; 3) algebraic approaches [5\u20137]; and 4) spectral clustering-based approaches [8\u201320].", "startOffset": 266, "endOffset": 272}, {"referenceID": 10, "context": "For example, sparse subspace clustering (SSC) [11] assumes that each data point can be linearly represented by a few of other points.", "startOffset": 46, "endOffset": 50}, {"referenceID": 11, "context": "Low-rank representation (LRR) [12] encourages the coefficient matrix to be low rank, such that it can capture the global structures of data.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "Different from SSC and LRR, truncated regression representation (TRR) [17, 19] takes Frobenius norm instead of l1- and nuclear-norm, which has shown promising performance in many real-world applications.", "startOffset": 70, "endOffset": 78}, {"referenceID": 18, "context": "Different from SSC and LRR, truncated regression representation (TRR) [17, 19] takes Frobenius norm instead of l1- and nuclear-norm, which has shown promising performance in many real-world applications.", "startOffset": 70, "endOffset": 78}, {"referenceID": 12, "context": "Like most existing subspace clustering algorithms [11\u2013 13, 21], the major disadvantage of TRR is that it may not give a satisfactory clustering result when data points cannot be linearly represented with each other.", "startOffset": 50, "endOffset": 62}, {"referenceID": 20, "context": "Like most existing subspace clustering algorithms [11\u2013 13, 21], the major disadvantage of TRR is that it may not give a satisfactory clustering result when data points cannot be linearly represented with each other.", "startOffset": 50, "endOffset": 62}, {"referenceID": 20, "context": "During past decades, some spectral clustering-based methods have been proposed to achieve subspace clustering in many applications such as image clustering [21], motion segmentation [22], and gene expression analysis [23].", "startOffset": 156, "endOffset": 160}, {"referenceID": 21, "context": "During past decades, some spectral clustering-based methods have been proposed to achieve subspace clustering in many applications such as image clustering [21], motion segmentation [22], and gene expression analysis [23].", "startOffset": 182, "endOffset": 186}, {"referenceID": 22, "context": "During past decades, some spectral clustering-based methods have been proposed to achieve subspace clustering in many applications such as image clustering [21], motion segmentation [22], and gene expression analysis [23].", "startOffset": 217, "endOffset": 221}, {"referenceID": 15, "context": ", pairwise distance-based strategy and linear representationbased strategy [16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.", "startOffset": 164, "endOffset": 187}, {"referenceID": 11, "context": "Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.", "startOffset": 164, "endOffset": 187}, {"referenceID": 12, "context": "Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.", "startOffset": 164, "endOffset": 187}, {"referenceID": 14, "context": "Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.", "startOffset": 164, "endOffset": 187}, {"referenceID": 18, "context": "Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.", "startOffset": 164, "endOffset": 187}, {"referenceID": 25, "context": "Based on this assumption, the linear representation coefficient can be used as a measurement of similarity and has achieved state of the art in subspace clustering [11\u201313, 15, 19, 24\u2013 26] since it encodes the global structure of the whole data set into similarity.", "startOffset": 164, "endOffset": 187}, {"referenceID": 10, "context": "For example, SSC [11] enforces the sparsity on C by adopting l1-norm via R(C) = \u2016C\u20161, LRR [12] obtains low rankness by using nuclear norm with R(C) = \u2016C\u2016\u2217.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "For example, SSC [11] enforces the sparsity on C by adopting l1-norm via R(C) = \u2016C\u20161, LRR [12] obtains low rankness by using nuclear norm with R(C) = \u2016C\u2016\u2217.", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "To address this challenging issue, some recent works have been proposed [15, 24], however, the methods have the following two disadvantages: 1) the methods are computationally inefficient since they involve solving l1- or nuclear-norm minimization problem; 2) Like SSC and LRR, the methods", "startOffset": 72, "endOffset": 80}, {"referenceID": 23, "context": "To address this challenging issue, some recent works have been proposed [15, 24], however, the methods have the following two disadvantages: 1) the methods are computationally inefficient since they involve solving l1- or nuclear-norm minimization problem; 2) Like SSC and LRR, the methods", "startOffset": 72, "endOffset": 80}, {"referenceID": 26, "context": "recently proposed to achieve nonlinearity with deep structures [27, 28], which are first works to leverage deep learning and subspace clustering.", "startOffset": 63, "endOffset": 71}, {"referenceID": 27, "context": "recently proposed to achieve nonlinearity with deep structures [27, 28], which are first works to leverage deep learning and subspace clustering.", "startOffset": 63, "endOffset": 71}, {"referenceID": 14, "context": "After mapping X into a kernel space, the corresponding {\u03c6(xi)} n i=1 is generally believed lying in linear subspaces [15, 24].", "startOffset": 117, "endOffset": 125}, {"referenceID": 23, "context": "After mapping X into a kernel space, the corresponding {\u03c6(xi)} n i=1 is generally believed lying in linear subspaces [15, 24].", "startOffset": 117, "endOffset": 125}, {"referenceID": 28, "context": "For some choices of a kernel \u03ba(xi,xj): R \u00d7R \u2192 R, [29] has shown that \u03ba can get the dot product in the kernel space H induced by the mapping \u03c6.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "It has been proved that, under certain condition, the coefficients over intra-subspace data points are larger than those over inter-subspace data points [17].", "startOffset": 153, "endOffset": 157}, {"referenceID": 7, "context": "In this section, we present the method to achieve subspace clustering by incorporating the KTRR into spectral clustering framework [8].", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "Then, we compute the normalized Laplacian matrix [8]", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "2 1 [9], where 1 = [1, 1, .", "startOffset": 4, "endOffset": 7}, {"referenceID": 29, "context": "considerably less than that of KSSC (O(mn + tn))[30], KLRR(O(t(rX + r)n )[24], where t denotes the total number of iterations for the corresponding algorithm, rX is the rank of X, and r is the rank for partial SVD at each iteration of KLRR.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "considerably less than that of KSSC (O(mn + tn))[30], KLRR(O(t(rX + r)n )[24], where t denotes the total number of iterations for the corresponding algorithm, rX is the rank of X, and r is the rank for partial SVD at each iteration of KLRR.", "startOffset": 73, "endOffset": 77}, {"referenceID": 30, "context": "Five popular image databases are used in our experiments, including Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], Columbia Object Image Library (COIL 100) [33], USPS [34], and MNIST [35].", "startOffset": 103, "endOffset": 107}, {"referenceID": 31, "context": "Five popular image databases are used in our experiments, including Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], Columbia Object Image Library (COIL 100) [33], USPS [34], and MNIST [35].", "startOffset": 149, "endOffset": 153}, {"referenceID": 32, "context": "Five popular image databases are used in our experiments, including Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], Columbia Object Image Library (COIL 100) [33], USPS [34], and MNIST [35].", "startOffset": 196, "endOffset": 200}, {"referenceID": 33, "context": "Five popular image databases are used in our experiments, including Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], Columbia Object Image Library (COIL 100) [33], USPS [34], and MNIST [35].", "startOffset": 207, "endOffset": 211}, {"referenceID": 34, "context": "Five popular image databases are used in our experiments, including Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], Columbia Object Image Library (COIL 100) [33], USPS [34], and MNIST [35].", "startOffset": 223, "endOffset": 227}, {"referenceID": 35, "context": "\u2022 The ExYaleB database contains 2414 frontal face images of 38 subjects and around 64 near frontal images under different illuminations per individual, where each image is manually cropped and normalized to the size of 32\u00d732 pixels [36].", "startOffset": 232, "endOffset": 236}, {"referenceID": 35, "context": "The size of each image is 32\u00d7 32 pixels, with 256 grey levels per pixel [36].", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "In the experiment, we select 200 samples of each subject from the database randomly by following the strategy in [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 164, "endOffset": 168}, {"referenceID": 29, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 211, "endOffset": 215}, {"referenceID": 24, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 256, "endOffset": 260}, {"referenceID": 11, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 306, "endOffset": 310}, {"referenceID": 11, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 357, "endOffset": 361}, {"referenceID": 10, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 396, "endOffset": 400}, {"referenceID": 25, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 450, "endOffset": 454}, {"referenceID": 36, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 486, "endOffset": 490}, {"referenceID": 7, "context": "We compare KTRR with 10 state-of-art subspace clustering algorithms including truncated regression representation (TRR) [17], kernel low-rank representation (KLRR) [24], kernel sparse subspace clustering (KSSC) [30], Latent lowrank representation (LatLRR) [25], low-rank representation (LRR1) with l1-norm [12], low-rank representation (LRR2) with l21-norm [12], sparse subspace clustering (SSC) [11], sparse manifold clustering and embedding (SMCE) [26], local subspace analysis (LSA) [37], and standard spectral clustering (SC) [8].", "startOffset": 530, "endOffset": 533}, {"referenceID": 7, "context": "For a fair comparison, we use the same spectral clustering framework [8] with different similarity matrices obtained by the tested algorithms.", "startOffset": 69, "endOffset": 72}, {"referenceID": 23, "context": "Like [24], for all kernel-based algorithms, we adopt the commonly used Gaussian kernel on all datasets and use the default bandwidth parameter which is set to the mean of the distances between all the samples.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": ", accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40].", "startOffset": 16, "endOffset": 24}, {"referenceID": 37, "context": ", accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40].", "startOffset": 16, "endOffset": 24}, {"referenceID": 9, "context": ", accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40].", "startOffset": 62, "endOffset": 70}, {"referenceID": 37, "context": ", accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40].", "startOffset": 62, "endOffset": 70}, {"referenceID": 38, "context": ", accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40].", "startOffset": 102, "endOffset": 106}, {"referenceID": 39, "context": ", accuracy (AC) [10, 38], normalized mutual information (NMI) [10, 38], the adjusted rand index (ARI) [39], and Fscore [40].", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": ", the connections should only exist among the data points from the same cluster [12, 15, 17, 24, 26], such that a hard thresholding operation has been executed.", "startOffset": 80, "endOffset": 100}, {"referenceID": 14, "context": ", the connections should only exist among the data points from the same cluster [12, 15, 17, 24, 26], such that a hard thresholding operation has been executed.", "startOffset": 80, "endOffset": 100}, {"referenceID": 16, "context": ", the connections should only exist among the data points from the same cluster [12, 15, 17, 24, 26], such that a hard thresholding operation has been executed.", "startOffset": 80, "endOffset": 100}, {"referenceID": 23, "context": ", the connections should only exist among the data points from the same cluster [12, 15, 17, 24, 26], such that a hard thresholding operation has been executed.", "startOffset": 80, "endOffset": 100}, {"referenceID": 25, "context": ", the connections should only exist among the data points from the same cluster [12, 15, 17, 24, 26], such that a hard thresholding operation has been executed.", "startOffset": 80, "endOffset": 100}, {"referenceID": 7, "context": ", the strong connections exist among the samples from the same subject; \u2022 The obtained similarity matrix is a symmetric matrix which can be directly used for subspace clustering under the framework of spectral clustering [8].", "startOffset": 221, "endOffset": 224}, {"referenceID": 30, "context": ", Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], USPS [34], MNIST [35].", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": ", Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], USPS [34], MNIST [35].", "startOffset": 83, "endOffset": 87}, {"referenceID": 33, "context": ", Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], USPS [34], MNIST [35].", "startOffset": 94, "endOffset": 98}, {"referenceID": 34, "context": ", Extended Yale Database B (ExYaleB) [31], Columbia Object Image Library (COIL 20) [32], USPS [34], MNIST [35].", "startOffset": 106, "endOffset": 110}, {"referenceID": 16, "context": ", KTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8].", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": ", KTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8].", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": ", KTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8].", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": ", KTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8].", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": ", KTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8].", "startOffset": 54, "endOffset": 58}, {"referenceID": 7, "context": ", KTRR, TRR [17], KLRR [24], KSSC [15], LRR [12], SSC [11], outperform the standard spectral clustering method [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 16, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": ", TRR [17], LRR [12], and SSC [11], are still inferior to their kernel-based extensions, i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": ", TRR [17], LRR [12], and SSC [11], are still inferior to their kernel-based extensions, i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": ", TRR [17], LRR [12], and SSC [11], are still inferior to their kernel-based extensions, i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": ", TRR [17], LRR [12], and SSC [11], are inferior to their kernel-based extensions, i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": ", KTRR, KLRR [24], and KSSC [15].", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "\u2022 The standard SC [8] is the fastest since its similarity graph is computing via the pairwise kernel distances among the input samples, while the KSSC [15] is the most timeconsuming method.", "startOffset": 18, "endOffset": 21}, {"referenceID": 14, "context": "\u2022 The standard SC [8] is the fastest since its similarity graph is computing via the pairwise kernel distances among the input samples, while the KSSC [15] is the most timeconsuming method.", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "\u2022 The time cost of the proposed method is very close to that of its linear version TRR [17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "\u2022 The KTRR and TRR [17] algorithms are much faster than KSSC, SSC, KLRR, and LRR methods.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "The KTRR and TRR [17] algorithms both have analytical solutions, and only one pseudo-inverse operation is required for solving the representation problems of all data points for KTRR and TRR algorithms.", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "The possible reason is that the NMI is robust to the data distribution (increasing subject number) [19].", "startOffset": 99, "endOffset": 103}], "year": 2017, "abstractText": "Subspace clustering aims to group data points into multiple clusters of which each corresponds to one subspace. Most existing subspace clustering methods assume that the data could be linearly represented with each other in the input space. In practice, however, this assumption is hard to be satisfied. To achieve nonlinear subspace clustering, we propose a novel method which consists of the following three steps: 1) projecting the data into a hidden space in which the data can be linearly reconstructed from each other; 2) calculating the globally linear reconstruction coefficients in the kernel space; 3) truncating the trivial coefficients to achieve robustness and block-diagonality, and then achieving clustering by solving a graph Laplacian problem. Our method has the advantages of a closed-form solution and capacity of clustering data points that lie in nonlinear subspaces. The first advantage makes our method efficient in handling large-scale data sets, and the second one enables the proposed method to address the nonlinear subspace clustering challenge. Extensive experiments on five realworld datasets demonstrate the effectiveness and the efficiency of the proposed method in comparison with ten state-of-the-art approaches regarding four evaluation metrics.", "creator": "LaTeX with hyperref package"}}}