{"id": "1705.05633", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "Social Media-based Substance Use Prediction", "abstract": "in this paper, we demonstrate where the state - of - the - art machine learning and text mining techniques can react used to build effective social media - based substance use detection systems. showing a substance use item truth is difficult to build on a large scale, to maximize system performance, we introduced different feature learning methods to take advantage of a colossal amount of accumulated social media data. they also demonstrate the benefit of using random - view unsupervised feature maps to combine heterogeneous user information such as facebook ` \" likes \" and \" status? \" to enhance system performance. modeled on our evaluation, our best models achieved 86 % auc for predicting tobacco use, 26 % for alcohol driving and 84 % for drug use, all scores which significantly outperformed existing methods. our investigation has also uncovered interesting relations between a user's social media behavior ( e. g., word usage ) affect substance use.", "histories": [["v1", "Tue, 16 May 2017 10:37:52 GMT  (78kb,D)", "http://arxiv.org/abs/1705.05633v1", null], ["v2", "Wed, 31 May 2017 19:38:14 GMT  (78kb,D)", "http://arxiv.org/abs/1705.05633v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SI", "authors": ["tao ding", "warren k bickel", "shimei pan"], "accepted": false, "id": "1705.05633"}, "pdf": {"name": "1705.05633.pdf", "metadata": {"source": "CRF", "title": "Social Media-based Substance Use Prediction", "authors": ["Tao Ding", "Warren K. Bickel", "Shimei Pan"], "emails": ["taoding01@umbc.edu", "shimei@umbc.edu", "wkbickel@vtc.vt.edu"], "sections": [{"heading": "1 Introduction", "text": "A substance use disorder (SUD) is defined as a condition in which recurrent use of substances such as alcohol, drugs and tobacco causes clinically and functionally significant impairment in an individual\u2019s daily life (SAMHSA, 2015). According to the 2014 National Survey on Drug Use and Health, 1 in 10 Americans age 12 and older had a substance use disorder. Substance use also costs Americans more than $700 billion a year in increased health care costs, crimes and lost productivity (NIDA, 2015).\nThese days, people also spend a significant amount of time on social media such as Twitter,\nFacebook and Instagram to interact with friends and families, exchange ideas and thoughts, provide status updates and organize events and activities. The ubiquity and widespread use of social media underlines the needs to explore its intersection with substance use and its potential as a scalable and cost-effective solution for screening and preventing substance misuse and abuse.\nIn this research, we employ the state-of-the-art machine learning and text mining algorithms to build automated substance use prediction systems, which can be used to identify people who are at risk of SUD. Moreover, by analyzing rich human behavior data on social media, we can also gain insight into patterns of use and risk factors associated with substance use. The main contributions of this work include:\n1. We have explored a comprehensive set of single-view feature learning methods to take advantage of a large amount of unsupervised social media data. Our results have shown significant improvement over baseline systems that only use supervised training data.\n2. We have explored several multi-view learning algorithms to take advantage of heterogeneous user data such as Facebook \u201clikes\u201d and \u201cstatus updates\u201d. Our results have demonstrated significant improvement over baselines that only use a single data type.\n3. We have uncovered new insight into the relationship between a person\u2019s social media activities and substance use such as the relationship between word usage and SUD."}, {"heading": "2 Related Work", "text": "Substance use disorder (SUD) encompasses a complex pattern of behaviors. Many studies have\nar X\niv :1\n70 5.\n05 63\n3v 1\n[ cs\n.C L\n] 1\n6 M\nay 2\n01 7\nbeen conducted to discover factors interacting with SUD. A growing number of studies have confirmed a strong association between personal traits and substance use. For example, (Campbell et al., 2014) found that smokers have significantly higher openness to experience and lower conscientiousness, a personality trait related to a tendency to show self-discipline, act dutifully, and aim for achievement. (Cook et al., 1998) examined the links between alcohol consumption and personality and found that alcohol use is correlated positively with sociability and extraversion. (Terracciano et al., 2008) conducted a study involving 1102 participants and found a link between drug use and low conscientiousness. (Carroll et al., 2009) revealed risk factors related to addiction such as age, sex, impulsivity, sweet-liking, novelty reactivity, proclivity for exercise, and environmental impoverishment. Additionally, addiction is also linked to environmental and social factors such as neighborhood environment (Crum et al., 1996), family environment (Cadoret et al., 1986; Brent, 1995) and social norms (Botvin, 2000; Oetting and Beauvais, 1987).\nTraditionally, in behavior science research, data are collected from surveys or interviews with a limited number of people. The advent of social media makes a large volume of diverse user data available to researchers, which makes it possible to study substance use based on online user behaviors in a natural setting. Typical data from social media include demographics (age, gender etc.), status updates (text posts etc.), social networks (follower and following graph etc.) and likes (thumb up/down etc.). Recently, social media analytics has increasingly become a powerful tool to help understand the traits and behaviors of millions of social media users such as personal traits (Golbeck et al., 2011; Volkova and Bachrach, 2015; Youyou et al., 2015; Kilic\u0327 and Pan, 2016), brand preferences (Yang et al., 2015), communities and events (Sayyadi et al., 2009), influenza trend (Aramaki et al., 2011) and crime (Li et al., 2012). So far, however, there has been limited work that directly applies large scale social media analytics to automatically predict SUD. Among the work known to us, (Zhou et al., 2016) identified common drug consumption behaviors with regard to the time of day and week. They also discovered common interests shared by drug users such as celebrities (e.g, Chris Tucker)\nand comedians (e.g., cheechandchong). In addition, (Kosinski et al., 2013) automatically predicted SUD based on social media likes. Since their dataset is very similar to ours, we will use the Kosinski model as one of our baselines."}, {"heading": "3 Dataset", "text": "The data for the study was collected from 2007 to 2012 as a part of the myPersonality project (Kosinski et al., 2015). myPersonality was a popular Facebook application that offered to its users psychometric tests and feedback on their scores. The data were gathered with an explicit opt-in consent for reuse for research purposes. Our study uses three separate datasets: Facebook status updates (a.k.a. posts), likes and SUD status.\nThe status update dataset contains 22 million textual posts authored by 153,000 users. The average posts per user is 143 and the average words per user is 1730. We removed users who only have non-English posts and those who have written less than 500 words. Our final status update dataset includes 106,509 users with 21 million posts. After filtering out low frequency words (those appear less than 50 times in our corpus), the vocabulary size of the status update dataset is 73,935.\nThe likes data are used by users to express positive sentiment toward various targets such as products, movies, books, expressions, websites and people (they are called Like Entities or LEs). Previous studies have demonstrated that social media likes speak volumes about who we are. In addition to directly signaling interests and preferences, social media likes are indicative of ethnicity, intelligence and personality (Kosinski et al., 2013). The like dataset includes the likes of 11 million Facebook users. Overall, there are 9.9 million unique LEs and 1.8 billion user-like pairs. The average likes per user is 161 and the average Likes each LE received is 182. We filter out users as well as LEs who have a small number of likes. The filtering threshold for users is 50 and is 800 for LEs. After the filtering, our like dataset contains 5,138,857 users and 253,980 unique LEs.\nThe SUD dataset contains a total of 13,557 participants (?). Users were asked to answer questions like \u201cDo you smoke?\u201d, with answers \u201cdaily or more\u201d, \u201cless than daily\u201d or \u201cnever\u201d. They also completed the Cigarette Dependence Scale (CDS5) (Etter et al., 2003), Alcohol Use Question-\nnaire(AUQ) (Townshend and Duka, 2005) and the Assessment of Substance Misuse Questionnaire (ASMA) (Willner, 2000). Based on these assessments, the participants were divided into groups for each SUD type. For example, based on the assessment of tobacco use, a person is catogorized as \u201cdaily or more\u201d (group 3), \u201cless than daily\u201d (group 2), or \u201cnever\u201d (group 1). The validity of the grouping was confirmed by the CDS-5 scores of the groups. Similarly, based on the assessment of alcohol use, participants were categorized as \u201cweekly or more\u201d (group 3), \u201cless than once a week\u201d (group 2) or \u201cnever\u201d (group 1). Finally, based on the assessment of drug use, a person is assigned to \u201cweekly or more\u201d (group 3), \u201cless than once a week\u201d (group 2), or \u201cnever\u201d (group 1). Among all the SUD participants, 37% of them are males and 63% are females. Their average age is 23 years old.\nSince the like, status update and SUD datasets are only partially overlapping, their intersections are usually much smaller. Table 1 summarizes the sizes and usage of these datasets. Table 2 shows additional details of the SUD dataset including the distributions of each SUD class.\nIn summary, among all the datasets we have, the unsupervised like dataset is the largest (5 million+ people). We also have a significant amount of unsupervised status update data (100k+ users). In contrast, the supervised datasets which have the SUD ground truth are pretty small, ranging from 896 for the intersection of the likes, status updates and SUD (LikeStatusSUD in Table 1) to 3508, which is the intersection of the likes and SUD (LikesSUD in Table 1). Thus, the main focuses of this research include (1) employing unsupervised feature learning to take advantage of a large amount of unsupervised data (2) employing multiview learning to combine heterogeneous user data for better prediction."}, {"heading": "4 Single-View Post Embedding (SPE)", "text": "The main purpose of this study is to demonstrate the usefulness of employing unsupervised feature learning to learn a dense vector representation of a user\u2019s Facebook posts to take advantage of a large amount of unsupervised data. Since we only use Facebook status updates (a.k.a. posts) in this study, we call the process Single-view user Post Embedding (SPE)."}, {"heading": "4.1 Feature Learning Methods", "text": "Since each user is associated with a sequence of textual posts, we have explored the following methods to learn a SPE for the user.\nSingular Value Decomposition (SVD) is a mathematical technique that is frequently used for dimension reduction (De Lathauwer et al., 2000). Given any m \u2217 n matrix A, the algorithm will find matrices U , V and W such that A = UWV T . Here U is an orthonormal m\u2217n matrix, W is a diagonal n \u2217n metrix and V is an orthonormal n \u2217n matrix. Dimensionality reduction is done by computing R = U \u2217 Wr where Wr neglects all but the r largest singular values in the diagonal matrix W . In our study, the m is the number of users, n is the number of unique words in the vocabulary. Aij = k where k is how many times wordj appears in useri\u2019s posts.\nLatent Dirichlet Allocation (LDA) is a generative graphical model that allows sets of documents to be explained by unobserved latent topics (Blei et al., 2003). For each document, LDA outputs a multinomial distribution over a set of latent topics. For each topic, LDA also outputs a multinomial distribution over the vocabulary.\nTo learn an SPE for each user based on all his/her posts, we have tried several methods (1) UserLDA: it treats all the posts from each user as one big document and trains an LDA model to drive the topic distribution for this document. The per-document topic distribution is then used as the\nSPE for this user. (2) PostLDA Doc: it treats each post as a separate document and trains an LDA model to derive a topic distribution for each post. To derive the SPE for each user, we aggregate all the per-post topic distribution vectors from the same user by averaging them. (3) PostLDA Word: instead of using the average of post-based topic distribution vectors, we used a word-based aggregation method suggested by (Schwartz et al., 2013):\np(topic|user) = \u2211\nw\u2208voc P (topic|w) \u2217 p(w|user)\nwhere voc represents the vocabulary, p(w|user) is the probability that word w appears in the posts of user and p(topic|w) is the topic distribution of a word w, which is available internally in an LDA model. For UserLDA model, all the hyper parameters were set to default values. For PostLDA, since Facebook posts are usually short and have a small number of topics in each post, we set the hyper parameter \u03b1 to 0.3, as suggested by (Schwartz et al., 2013)\nDocument Embedding with Distributed Memory (D-DM) Given a document, D-DM simultaneously learns a vector representation for each word and a vector for the entire document (Le and Mikolov, 2014). During training, the document vector and one or more word vectors are aggregated to predict a target word in the context. To learn a SPE for each user, we have explored two methods (1) User-D-DM: it treats all the posts by the same user as one document and trains a document vector to represent the user. (2) Post-D-DM: it treats each post as a document and train a D-DM to learn a vector for each post. To derive the SPE for a user, we aggregate all the post vectors from the same person using \u201caverage\u201d.\nDocument Embedding with Distributed Bag of Words (D-DBOW) D-DBOW learns a global document vector to predict words randomly sampled from the document. Unlike D-DM, D-DBOW only learns a vector for the entire document. It does not learn vectors for individual words. Nei-\nther does it use a local context window since the words for prediction are randomly sampled from the entire document. Similar to D-DM, to derive the SPE for a user, we used two methods (1) UserD-DBOW and (2) Post-D-DBOW."}, {"heading": "4.2 SUD Prediction with SPE", "text": "In our experiments, to search for the best model, we systematically varied the output SPE dimension from 50, 100, 300, to 500. We used the Gensim implementation of SVD, LDA, D-DM and DDBOW in our experiments. For D-DM, the context window size was set to 5.\nWe compared our models with two baselines that use only supervised learning (1) a unigram model which uses unigrams as the predicting features. Since we have a large number of unigrams, we performed supervised feature selection to lower the total number of input features. Finally since all our SUD variables have three values, we employed SVM in 3-way classifications. (2) a LIWC model which uses human engineered LIWC features for SUD prediction. LIWC is a psycholinguistic lexicon (Pennebaker et al., 2015) that has been frequently used in text-based human behavior prediction. Since the number of LIWC features is relatively small, no feature selection was performed. Here, we only used the Status dataset in Table 1 as the training data for SPE learning and the StatusSUD dataset for supervised SUD prediction .\nWe evaluate the performance of our models using 10-fold cross validation. The evaluation re-\nsults shown in Table 3 are based on weighted ROC AUC of the best models. Among all the feature learning methods for Facebook status updates, User-D-DBOW performed the best. It significantly outperformed all the baseline systems that only rely on supervised training (p < 0.01 based on t-tests). It also significantly outperformed all the traditional feature learning methods such as LDA and SVD (p < 0.01 based on t-tests). Moreover, in terms of whether to treat all the posts by the same user as one big document or separate documents, LDA prefers one post one document (models with a \u201cpost\u201d prefix) while all the document vector-based methods prefer one user one document (models with a \u201cUser\u201d prefix). Moreover, to use post-level LDA to derive the SPE for a user, the document-based aggregation method (PostLDA Doc) performed better than the wordbased method (PostLDA Word)."}, {"heading": "5 Single-View Like Embedding (SLE)", "text": "In addition to textual posts, each user account is also associated with likes. Since the like dataset is very sparse (e.g., among the millions of unique likes on Facebook, each user only has a small number of likes), we conduct experiments to learn a dense vector representation for all the likes by a user. We call this process Single-view user Like Embedding (SLE)."}, {"heading": "5.1 Feature Learning Methods", "text": "The input to SLE is simply a set of LEs liked by a user. Each LE is represented by its id. To map such a representation to a dense user like vector, we have tried multiple methods.\nSingular Value Decomposition (SVD) is similar to the one used in SPE except Aij = 1 if useri likes LEj . Otherwise, it is 0. Here A is a m \u2217 n matrix wherem is the number of users and n is the number of unique LEs in the like dataset.\nLatent Dirichlet Allocation (LDA). To apply LDA to the like data, each individual LE is treated as a word token and all the LEs liked by the same person form a document. The order of the LEs in the document is random. For each user, LDA outputs a multinomial distribution over a set of latent \u201cLike Topics\u201d. For example, a \u201cLike Topic\u201d about \u201chip hop music\u201d may include famous hip hop songs and musicians.\nAutoencoder (AE) is a neural network-based\nmethod for self-taught learning (Hinton and Salakhutdinov, 2006). It learns an identity function so that the output is as close to the input as possible. Although an identity function seems a trivial function to learn, by placing additional constraints (e.g,, to make the number of neurons in the hidden layer much smaller than that of the input), we can still uncover structures in the data. Architecturally, the AE we used has one input layer, one hidden layer and one output layer. For each user, we construct a training instance (X,Y ) where the input vector X and output vector Y are the same. The size of X and Y is the total number of unique LEs in our dataset. Xi and Yi equal to 1 if the user likes LEi. Otherwise they are 0.\nDocument Vector with Distributed Memory (D-DM) We also applied D-DM to the like data. Given all the likes of a user, D-DM learns a vector representation for each LE as well as a document vector for all the LEs from the same user. We use the learned document vector as the output SLE.\nDocument Vector with Distributed Bag of Words (D-DBOW) Similarly, we applied DDBOW to the like dataset. Since D-DBOW does not use a local context window and the words for prediction are randomly sampled from the entire document, it is more appropriate for the like dataset than D-DM. where the positions of LEs do matter."}, {"heading": "5.2 SUD Prediction with SLE", "text": "Similarly, we systematically varied the output SLE dimension from 50, 100, 300, to 500 in order to search for the best model. We used Keras with Theano backend to implement AE. For D-DM, the context window size was set to 20.\nWe used SVM to perform 3-way classification. We compared our results with a unigram baseline. We also compared our results with the Kosinski model (Kosinski et al., 2013) that was trained on the same like dataset. However, its results were based on two-way classification, a simpler task than 3-way classification. All the results are based on weighted ROC AUC.\nAs shown in Table 4, among all the SLE methods, the D-DBOW model performed the best. It significantly outperformed the unigram baseline that does not use any unsupervised data (p < 0.01 based on t-tests). It also significantly outperformed all the traditional feature learning method such as SVD and LDA (The Kosinski model used\nSVD for feature learning) (p < 0.01 based on ttests). Between the two document vector-based methods D-DM and D-DBOW, D-DBOW outperformed D-DM. We think this is due to the fact that D-DBOW does not use local context window, thus is not sensitive to the positions of LEs in a document. Since LE positions are randomly decided, D-DBOW seems to be a better fit for the like data."}, {"heading": "6 Multi-View User Embedding (MUE)", "text": "The main purpose of this study is to demonstrate the usefulness of combining heterogeneous user data such as likes and posts to learn a dense vector representation for each user. Since we employ unsupervised multi-view feature learning to combine these data, we call this process Multi-view User Embedding (MUE)."}, {"heading": "6.1 Feature Learning Methods", "text": "We have explored two multi-view learning algorithms: CCA and DCCA.\nCanonical Correlation Analysis (CCA) CCA is a statistical method for exploring the relationships between two multivariate sets of variables (vectors) (Hardoon et al., 2004). Given two vectors X1 and X2), CCA tries to find w1X1, w2X2 that are maximally correlated:\n(w\u22171, w \u2217 2) = argmax\nw1,w2 corr(w\n\u2032 1X1, w \u2032 2X2) (1)\n= argmax w1,w2\nw \u2032 1 \u2211 12w2\u221a\nw \u2032 1 \u2211 11w1w \u2032 2 \u2211 22w2 (2)\nwhere (X1, X2) denote random vectors with covariances ( \u2211 11, \u2211 22) and cross-covariance \u2211 12. CCA has been used frequently in unsupervised data analysis (Sargin et al., 2006; Chaudhuri et al., 2009; Kumar and Daume\u0301, 2011; Sharma et al., 2012).\nDeep Canonical Correlation Analysis (DCCA) DCCA aims to lean highly correlated deep archi-\ntectures, which can be a non-linear extension of CCA (Andrew et al., 2013). The intuition is to find a maximally correlated representation of the two views by passing them through multiple stacked layers of nonlinear transformation (Andrew et al., 2013). Typically, there are three steps to train DCCA: (1) using a denoising autoencoder to pretrain each single view. In our experiments, we pretrain each single view using SPE or SLE. (2) computing the gradient of the correlation of top-level representation. (3) tuning parameters using back propagation to optimize the total correlation."}, {"heading": "6.2 SUD Prediction With MUE", "text": "The input to MUE are the two single views obtained earlier (i.e. SPE or SLE). Here, we choose the outputs from D-DBOW since it consistently outperformed all the other methods in learning SPEs and SLEs. We have run CCA and DCCA in two settings (1) balanced setting in which the SPE and SLE dimensions are always the same (2) imbalanced setting in which the dimension of SPE may be different from that of SLE. Since we varied the output dimensions of SPE and SLE from 50, 100, 300, to 500 systematically, the input dimension to MUE under the balanced setting are 100, 200, 600 and 1000. When running CCA and DCCA under the imbalanced setting, we only chose the best SPE (with 50 dimensions) and the best SLE (with 300 dimensions). We also varied the number of MUE output dimensions systematically from 20,50,100,200,300,400,500 to 1000 (up to the total input MUE dimensions). We used the LikeStatus dataset in Table 1 as the training data for multi-view unsupervised feature learning. For MUE-based supervised SUD predication, we used the LikeStatusSUD data. In our experiments, we use the a variant of CCA called wGCCA implemented by (Benton et al., 2016) where we set the weights for both views equal 1. We used the DCCA implementation by (Andrew et al., 2013) which uses Keras and Theano as the deep learning platform 2. We also varied the number of hidden layers from 1 to 3 to tune the performance.\nWe compared our multi-view learning results with 3 baselines: BestSPE and BestSLE are the best single view models. We also used a 3rd baseline called Unigram combine, which simply concatenates all the post and like unigrams to-\n1https://github.com/abenton/wgcca 2https://github.com/VahidooX/DeepCCA\ngether and then applies supervised feature selection before uses the remaining features in a SVMbased classification. As shown in Table 5, both wGCCA and DCCA significantly outperformed the unigram-based baseline (p < 0.01 based on ttest). The difference between the best multi-view models (wGCCA balanced for Alcohol and drug, wGCCA imbalanced for drugs) and the best single view models are also significant (p < 0.01). wGCCA also performed significantly better than DCCA on our tasks (p < 0.01 based on t-tests)."}, {"heading": "7 Social Media and Substance Use", "text": "In addition to building models that predict SUD, we are also interested in understanding the relationship between a person\u2019s social media activities and substance use behavior. Since many of the SPES and SLEs are not easily interpretable, in this section, we focus on the LIWC features from status updates and the LDA topics from both Likes and status updates. Since the SUD ground truth is an ordinal variable and the LIWC/LDA features are numerical, we used Spearman\u2019s rank correlation analysis to identify features that are most significantly correlated with SUD. Figure 1 shows the LIWC features that are significantly correlated with at least one type of SUD (p < 0.05). The color red represents a positive correlation while blue represents a negative correlation. In addition, the saturation of the color indicates the significance of the correlation. The darker the color\nis, the more significant the correlation is. As shown in Figure 1, swear words such as \u201cfuck\u201d and \u201cshit\u201d, sexual words such as \u201chorny\u201d and \u201csex\u201d, words related to biological process such as \u201cblood\u201d and \u201cpain\u201d are positively correlated with all three types of SUD. In addition, words related to money such as \u201ccash\u201d and \u201cmoney\u201d, words related to body such as \u201chands\u201d and \u201clegs\u201d, words related to ingestion such as \u201ceat\u201d and \u201cdrink\u201d are positively correlated with both alcohol and drug use; words related to motion such as \u201ccar\u201d and \u201cgo\u201d are positively correlated with both alcohol and tobacco use. In addition, female references such as \u201cgirl\u201d and \u201cwoman\u201d, prepositions, space reference words such as \u201cup\u201d and \u201cdown\u201d are positively correlated with alcohol use, while words related to anger such as \u201chate\u201d and \u201ckill\u201d, words related to health such as \u201cclinic\u201d and \u201cpill\u201d are positively correlated with drug use.\nIn terms of LIWC features that are negatively correlated with SUD, words associated with the past such as \u201cdid\u201d and \u201cago\u201d are negatively correlated to both tobacco and drug use; assent words such as \u201cok\u201d, \u201cyes\u201d and \u201cagree\u201d are negatively correlated to both alcohol and tobacco use. In addition, male references such as \u201cboy\u201d and \u201cman\u201d, words related to reward such as \u201cprize\u201d and \u201cbenefit\u201d, words related to positive emotions such as \u201cnice\u201d and \u201csweet\u201d, first person pronouns (plural) such as \u201cwe\u201d and \u201cour\u201d are negatively correlated to drug use. Moreover, impersonal pronouns such as \u201cit\u201d, differentiation words such as \u201cbut\u201d and \u201celse\u201d, and work-related words such as \u201cjob\u201d and \u201cwork\u201d are negatively correlated with alcohol use. Surprisingly, risk related words such as \u201cdanger\u201d, words related to sadness, death and negative emotions are also negatively correlated with alcohol use.\nThere are a few surprising correlations in our\nresults. For example, female references such as \u201cgirl\u201d and \u201cwoman\u201d are positively related to alcohol use while male references such as \u201cman\u201d and \u201cboy\u201d are negatively related to drug use. To interpret this, previous research has shown (Schwartz et al., 2013) that female references actually are used more often by male authors and vice versa. Thus, our findings suggest that males are more likely to use alcohol while females are less likely to use drugs.\nWe have also used Spearman\u2019s correlation analysis to identify SUD-related \u201cLike Topics\u201d and \u201cStatus update Topics\u201d learned by LDA. Since the number of significant topics is quite large, in Table 6, we only show a few samples. From the table, we can see based on a user\u2019s status updates, \u201cswear topics\u201d (T1, T9) are positively correlated with both tobacco and drug use, which is consistent with our LIWC findings. The \u201cnight life topic\u201d (T5) is positively related to alcohol use. In addition, school related topics (T2, T6) are negatively correlated with tobacco and alcohol use. Positive familyrelated activities (T10) are negatively correlated with drug use. In addition, based on the LDA topics learned from \u201clike\u201d, a preference for rock music (T3,T11) is positive correlated with tobacco and drug use. A preference for movies such as \u201cV For Vendetta\u201d and \u201cBoondock Saints\u201d (T7) is positively correlated with alcohol use, while having a hobby (T12), liking cartoons and shows favored by kids (T8) or liking movies and brands favored by girls (T4) are negatively correlated with drug, alcohol and tobacco use respectively."}, {"heading": "8 Discussion and Future Work", "text": "Currently, our multi-view unsupervised features learning methods only learn from the intersection of the like and status update data, which is much smaller than either the like or the status update data. Similarly, MUE-based supervised prediction used only the intersection of all three datasets which is very small (only contains 896 users). Thus, it would be useful if a future multi-view feature learning algorithm is capable of using all the available data (e.g., the union of all the supervised and unsupervised training data). Moreover, our best SPE model only has 50 dimensions while our best SLE model has 300 dimensions. This might be because the supervised training data used by SPE is almost three times smaller than that for SLE . But surprisingly, SPE-based models performed better than SLE-based models. We expect that with more training data, the performance of SPE-based methods can be further improved."}, {"heading": "9 Conclusion", "text": "We believe social media is a promising platform for both studying SUD-related human behaviors as well as engaging the public for substance abuse prevention and screening. In this study, we have focused on four main tasks (1) employing unsupervised features learning to take advantage of a large amount of unsupervised social media user data (2) employing multi-view feature learning to combine heterogeneous user information such as \u201dlikes\u201d and \u201dstatus updates\u201d to learn a compre-\nhensive user representation (3) building SUD prediction models based on learned user features (4) employing correlation analysis to obtain humaninterpretable results. Our investigation has not only produced models with the state-of-the-art prediction performance (e.g., for all three types of SUD, our models achieved over 80% prediction accuracy based on AUC) , but also demonstrated the benefits of incorporating unsupervised heterogeneous user data for SUD prediction."}], "references": [{"title": "Deep canonical correlation analysis", "author": ["Galen Andrew", "Raman Arora", "Jeff A Bilmes", "Karen Livescu."], "venue": "ICML (3). pages 1247\u20131255.", "citeRegEx": "Andrew et al\\.,? 2013", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Twitter catches the flu: detecting influenza epidemics using twitter", "author": ["Eiji Aramaki", "Sachiko Maskawa", "Mizuki Morita."], "venue": "Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguis-", "citeRegEx": "Aramaki et al\\.,? 2011", "shortCiteRegEx": "Aramaki et al\\.", "year": 2011}, {"title": "Learning multiview embeddings of twitter users", "author": ["Adrian Benton", "Raman Arora", "Mark Dredze."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. volume 2, pages 14\u201319.", "citeRegEx": "Benton et al\\.,? 2016", "shortCiteRegEx": "Benton et al\\.", "year": 2016}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "Journal of machine Learning research 3(Jan):993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Preventing drug abuse in schools: Social and competence enhancement approaches targeting individual-level etiologic factors", "author": ["Gilbert J Botvin."], "venue": "Addictive behaviors 25(6):887\u2013897.", "citeRegEx": "Botvin.,? 2000", "shortCiteRegEx": "Botvin.", "year": 2000}, {"title": "Risk factors for adolescent suicide and suicidal behavior: mental and substance abuse disorders, family environmental factors, and life stress", "author": ["David A Brent."], "venue": "Suicide and Life-Threatening Behavior 25(s1):52\u201363.", "citeRegEx": "Brent.,? 1995", "shortCiteRegEx": "Brent.", "year": 1995}, {"title": "An adoption study of genetic and environmental factors in drug abuse. Archives of general psychiatry", "author": ["Remi J Cadoret", "Ed Troughton", "Thomas W O\u2019Gorman", "Ellen Heywood"], "venue": null, "citeRegEx": "Cadoret et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Cadoret et al\\.", "year": 1986}, {"title": "Personality and smoking behaviour of nonsmokers, previous smokers, and habitual smokers", "author": ["S Campbell", "L Henry", "J Hammelman", "M Pignatore."], "venue": "J Addict Research & Therapy 5:191.", "citeRegEx": "Campbell et al\\.,? 2014", "shortCiteRegEx": "Campbell et al\\.", "year": 2014}, {"title": "Modeling risk factors for nicotine and other drug abuse in the preclinical laboratory", "author": ["Marilyn E Carroll", "Justin J Anker", "Jennifer L Perry."], "venue": "Drug and alcohol dependence 104:S70\u2013S78.", "citeRegEx": "Carroll et al\\.,? 2009", "shortCiteRegEx": "Carroll et al\\.", "year": 2009}, {"title": "Personality correlates of alcohol consumption", "author": ["Mark Cook", "Alison Young", "Dean Taylor", "Anthony P Bedford."], "venue": "Personality and Individual Differences 24(5):641\u2013647.", "citeRegEx": "Cook et al\\.,? 1998", "shortCiteRegEx": "Cook et al\\.", "year": 1998}, {"title": "Neighborhood environment and opportunity to use cocaine and other drugs in late childhood and early adolescence", "author": ["Rosa M Crum", "Marsha Lillie-Blanton", "James C Anthony."], "venue": "Drug and alcohol dependence 43(3):155\u2013161.", "citeRegEx": "Crum et al\\.,? 1996", "shortCiteRegEx": "Crum et al\\.", "year": 1996}, {"title": "A multilinear singular value decomposition", "author": ["Lieven De Lathauwer", "Bart De Moor", "Joos Vandewalle."], "venue": "SIAM journal on Matrix Analysis and Applications 21(4):1253\u20131278.", "citeRegEx": "Lathauwer et al\\.,? 2000", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2000}, {"title": "A self-administered questionnaire to measure dependence on cigarettes: the cigarette dependence scale", "author": ["Jean-Francois Etter", "Jacques Le Houezec", "Thomas V Perneger."], "venue": "Neuropsychopharmacology 28(2):359.", "citeRegEx": "Etter et al\\.,? 2003", "shortCiteRegEx": "Etter et al\\.", "year": 2003}, {"title": "Predicting personality with social media", "author": ["Jennifer Golbeck", "Cristina Robles", "Karen Turner."], "venue": "CHI\u201911 extended abstracts on human factors in computing systems. ACM, pages 253\u2013262.", "citeRegEx": "Golbeck et al\\.,? 2011", "shortCiteRegEx": "Golbeck et al\\.", "year": 2011}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["David R Hardoon", "Sandor Szedmak", "John ShaweTaylor."], "venue": "Neural computation 16(12):2639\u20132664.", "citeRegEx": "Hardoon et al\\.,? 2004", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov."], "venue": "science 313(5786):504\u2013507.", "citeRegEx": "Hinton and Salakhutdinov.,? 2006", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Analyzing and preventing bias in text-based personal trait prediction algorithms", "author": ["I\u015fil Do\u011fa Yakut Kili\u00e7", "Shimei Pan."], "venue": "Tools with Artificial Intelligence (ICTAI), 2016 IEEE 28th International Conference on. IEEE, pages 1060\u20131067.", "citeRegEx": "Kili\u00e7 and Pan.,? 2016", "shortCiteRegEx": "Kili\u00e7 and Pan.", "year": 2016}, {"title": "Facebook as a research tool for the social sciences: Opportunities, challenges, ethical considerations, and practical guidelines", "author": ["Michal Kosinski", "Sandra C Matz", "Samuel D Gosling", "Vesselin Popov", "David Stillwell."], "venue": "American Psychologist", "citeRegEx": "Kosinski et al\\.,? 2015", "shortCiteRegEx": "Kosinski et al\\.", "year": 2015}, {"title": "Private traits and attributes are predictable from digital records of human behavior", "author": ["Michal Kosinski", "David Stillwell", "Thore Graepel."], "venue": "Proceedings of the National Academy of Sciences 110(15):5802\u20135805.", "citeRegEx": "Kosinski et al\\.,? 2013", "shortCiteRegEx": "Kosinski et al\\.", "year": 2013}, {"title": "A co-training approach for multi-view spectral clustering", "author": ["Abhishek Kumar", "Hal Daum\u00e9."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 393\u2013400.", "citeRegEx": "Kumar and Daum\u00e9.,? 2011", "shortCiteRegEx": "Kumar and Daum\u00e9.", "year": 2011}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "ICML. volume 14, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Tedas: A twitter-based event detection and analysis system", "author": ["Rui Li", "Kin Hou Lei", "Ravi Khadiwala", "Kevin Chen-Chuan Chang."], "venue": "2012 IEEE 28th International Conference on Data Engineering. IEEE, pages 1273\u20131276.", "citeRegEx": "Li et al\\.,? 2012", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Drugs, brains, and behavior: The science of addiction", "author": ["NIDA."], "venue": "https://www.drugabuse.gov/publications/drugsbrains-behavior-science-addiction/introduction.", "citeRegEx": "NIDA.,? 2015", "shortCiteRegEx": "NIDA.", "year": 2015}, {"title": "Common elements in youth drug abuse: Peer clusters and other psychosocial factors", "author": ["ER Oetting", "Fred Beauvais."], "venue": "Journal of Drug Issues 17(2):133\u2013151.", "citeRegEx": "Oetting and Beauvais.,? 1987", "shortCiteRegEx": "Oetting and Beauvais.", "year": 1987}, {"title": "The development and psychometric properties of liwc2015", "author": ["James W Pennebaker", "Ryan L Boyd", "Kayla Jordan", "Kate Blackburn."], "venue": "Technical report.", "citeRegEx": "Pennebaker et al\\.,? 2015", "shortCiteRegEx": "Pennebaker et al\\.", "year": 2015}, {"title": "Substance use disorders", "author": ["SAMHSA."], "venue": "https://www.samhsa.gov/disorders/substance-use.", "citeRegEx": "SAMHSA.,? 2015", "shortCiteRegEx": "SAMHSA.", "year": 2015}, {"title": "Multimodal speaker identification using canonical correlation analysis", "author": ["Mehmet Emre Sargin", "Engin Erzin", "Y\u00fccel Yemez", "A Murat Tekalp."], "venue": "Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE Interna-", "citeRegEx": "Sargin et al\\.,? 2006", "shortCiteRegEx": "Sargin et al\\.", "year": 2006}, {"title": "Event detection and tracking in social streams", "author": ["Hassan Sayyadi", "Matthew Hurst", "Alexey Maykov."], "venue": "Icwsm.", "citeRegEx": "Sayyadi et al\\.,? 2009", "shortCiteRegEx": "Sayyadi et al\\.", "year": 2009}, {"title": "Personality, gender, and age in the language", "author": ["H Andrew Schwartz", "Johannes C Eichstaedt", "Margaret L Kern", "Lukasz Dziurzynski", "Stephanie M Ramones", "Megha Agrawal", "Achal Shah", "Michal Kosinski", "David Stillwell", "Martin EP Seligman"], "venue": null, "citeRegEx": "Schwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schwartz et al\\.", "year": 2013}, {"title": "Generalized multiview analysis: A discriminative latent space", "author": ["Abhishek Sharma", "Abhishek Kumar", "Hal Daume", "David W Jacobs."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, pages 2160\u20132167.", "citeRegEx": "Sharma et al\\.,? 2012", "shortCiteRegEx": "Sharma et al\\.", "year": 2012}, {"title": "Five-factor model personality profiles of drug users", "author": ["Antonio Terracciano", "Corinna E L\u00f6ckenhoff", "Rosa M Crum", "O Joseph Bienvenu", "Paul T Costa."], "venue": "Bmc Psychiatry 8(1):22.", "citeRegEx": "Terracciano et al\\.,? 2008", "shortCiteRegEx": "Terracciano et al\\.", "year": 2008}, {"title": "Binge drinking, cognitive performance and mood in a population of young social drinkers", "author": ["Julia M Townshend", "Theodora Duka."], "venue": "Alcoholism: Clinical and Experimental Research 29(3):317\u2013325.", "citeRegEx": "Townshend and Duka.,? 2005", "shortCiteRegEx": "Townshend and Duka.", "year": 2005}, {"title": "On predicting sociodemographic traits and emotions from communications in social networks and their implications to online self-disclosure", "author": ["Svitlana Volkova", "Yoram Bachrach."], "venue": "Cyberpsychology, Behavior, and Social Networking 18(12):726\u2013736.", "citeRegEx": "Volkova and Bachrach.,? 2015", "shortCiteRegEx": "Volkova and Bachrach.", "year": 2015}, {"title": "Further validation and development of a screening instrument for the assessment of substance misuse in adolescents", "author": ["Paul Willner."], "venue": "Addiction 95(11):1691\u20131698.", "citeRegEx": "Willner.,? 2000", "shortCiteRegEx": "Willner.", "year": 2000}, {"title": "Using personal traits for brand preference prediction", "author": ["Chao Yang", "Shimei Pan", "Jalal Mahmud", "Huahai Yang", "Padmini Srinivasan."], "venue": "EMNLP. pages 86\u201396.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Computer-based personality judgments are more accurate than those made by humans", "author": ["Wu Youyou", "Michal Kosinski", "David Stillwell."], "venue": "Proceedings of the National Academy of Sciences 112(4):1036\u20131040.", "citeRegEx": "Youyou et al\\.,? 2015", "shortCiteRegEx": "Youyou et al\\.", "year": 2015}, {"title": "Understanding illicit drug use behaviors by mining social media", "author": ["Yiheng Zhou", "Numair Sani", "Chia-Kuei Lee", "Jiebo Luo."], "venue": "arXiv preprint arXiv:1604.07096 .", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 25, "context": "A substance use disorder (SUD) is defined as a condition in which recurrent use of substances such as alcohol, drugs and tobacco causes clinically and functionally significant impairment in an individual\u2019s daily life (SAMHSA, 2015).", "startOffset": 217, "endOffset": 231}, {"referenceID": 22, "context": "Substance use also costs Americans more than $700 billion a year in increased health care costs, crimes and lost productivity (NIDA, 2015).", "startOffset": 126, "endOffset": 138}, {"referenceID": 7, "context": "For example, (Campbell et al., 2014) found that smokers have significantly higher openness to experience and lower conscientiousness, a personality trait related to a tendency to show self-discipline, act dutifully, and aim for achievement.", "startOffset": 13, "endOffset": 36}, {"referenceID": 9, "context": "(Cook et al., 1998) examined the links between alcohol consumption and personality and found that alcohol use is correlated positively with sociability and extraversion.", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "(Terracciano et al., 2008) conducted a study involving 1102 participants and found a link between drug use and low conscientiousness.", "startOffset": 0, "endOffset": 26}, {"referenceID": 8, "context": "(Carroll et al., 2009) revealed risk factors related to addiction such as age, sex, impulsivity, sweet-liking, nov-", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Additionally, addiction is also linked to environmental and social factors such as neighborhood environment (Crum et al., 1996), family environment (Cadoret et al.", "startOffset": 108, "endOffset": 127}, {"referenceID": 4, "context": "Brent, 1995) and social norms (Botvin, 2000; Oetting and Beauvais, 1987).", "startOffset": 30, "endOffset": 72}, {"referenceID": 23, "context": "Brent, 1995) and social norms (Botvin, 2000; Oetting and Beauvais, 1987).", "startOffset": 30, "endOffset": 72}, {"referenceID": 34, "context": ", 2015; Kili\u00e7 and Pan, 2016), brand preferences (Yang et al., 2015), communities and events (Sayyadi et al.", "startOffset": 48, "endOffset": 67}, {"referenceID": 27, "context": ", 2015), communities and events (Sayyadi et al., 2009), influenza trend (Aramaki et al.", "startOffset": 32, "endOffset": 54}, {"referenceID": 1, "context": ", 2009), influenza trend (Aramaki et al., 2011) and crime (Li et al.", "startOffset": 25, "endOffset": 47}, {"referenceID": 21, "context": ", 2011) and crime (Li et al., 2012).", "startOffset": 18, "endOffset": 35}, {"referenceID": 36, "context": "Among the work known to us, (Zhou et al., 2016) identified common drug consumption behaviors with regard to the time of day and week.", "startOffset": 28, "endOffset": 47}, {"referenceID": 18, "context": "In addition, (Kosinski et al., 2013) automatically pre-", "startOffset": 13, "endOffset": 36}, {"referenceID": 17, "context": "The data for the study was collected from 2007 to 2012 as a part of the myPersonality project (Kosinski et al., 2015).", "startOffset": 94, "endOffset": 117}, {"referenceID": 18, "context": "In addition to directly signaling interests and preferences, social media likes are indicative of ethnicity, intelligence and personality (Kosinski et al., 2013).", "startOffset": 138, "endOffset": 161}, {"referenceID": 12, "context": "They also completed the Cigarette Dependence Scale (CDS5) (Etter et al., 2003), Alcohol Use Question-", "startOffset": 58, "endOffset": 78}, {"referenceID": 31, "context": "naire(AUQ) (Townshend and Duka, 2005) and the Assessment of Substance Misuse Questionnaire (ASMA) (Willner, 2000).", "startOffset": 11, "endOffset": 37}, {"referenceID": 33, "context": "naire(AUQ) (Townshend and Duka, 2005) and the Assessment of Substance Misuse Questionnaire (ASMA) (Willner, 2000).", "startOffset": 98, "endOffset": 113}, {"referenceID": 3, "context": "Latent Dirichlet Allocation (LDA) is a generative graphical model that allows sets of documents to be explained by unobserved latent topics (Blei et al., 2003).", "startOffset": 140, "endOffset": 159}, {"referenceID": 28, "context": "(3) PostLDA Word: instead of using the average of post-based topic distribution vectors, we used a word-based aggregation method suggested by (Schwartz et al., 2013):", "startOffset": 142, "endOffset": 165}, {"referenceID": 28, "context": "3, as suggested by (Schwartz et al., 2013)", "startOffset": 19, "endOffset": 42}, {"referenceID": 20, "context": "word and a vector for the entire document (Le and Mikolov, 2014).", "startOffset": 42, "endOffset": 64}, {"referenceID": 24, "context": "LIWC is a psycholinguistic lexicon (Pennebaker et al., 2015) that has been frequently used in text-based human behavior prediction.", "startOffset": 35, "endOffset": 60}, {"referenceID": 15, "context": "Autoencoder (AE) is a neural network-based method for self-taught learning (Hinton and Salakhutdinov, 2006).", "startOffset": 75, "endOffset": 107}, {"referenceID": 18, "context": "We also compared our results with the Kosinski model (Kosinski et al., 2013) that was trained on the same like dataset.", "startOffset": 53, "endOffset": 76}, {"referenceID": 14, "context": "Canonical Correlation Analysis (CCA) CCA is a statistical method for exploring the relationships between two multivariate sets of variables (vectors) (Hardoon et al., 2004).", "startOffset": 150, "endOffset": 172}, {"referenceID": 26, "context": "CCA has been used frequently in unsupervised data analysis (Sargin et al., 2006; Chaudhuri et al., 2009; Kumar and Daum\u00e9, 2011; Sharma et al., 2012).", "startOffset": 59, "endOffset": 148}, {"referenceID": 19, "context": "CCA has been used frequently in unsupervised data analysis (Sargin et al., 2006; Chaudhuri et al., 2009; Kumar and Daum\u00e9, 2011; Sharma et al., 2012).", "startOffset": 59, "endOffset": 148}, {"referenceID": 29, "context": "CCA has been used frequently in unsupervised data analysis (Sargin et al., 2006; Chaudhuri et al., 2009; Kumar and Daum\u00e9, 2011; Sharma et al., 2012).", "startOffset": 59, "endOffset": 148}, {"referenceID": 0, "context": "CCA (Andrew et al., 2013).", "startOffset": 4, "endOffset": 25}, {"referenceID": 0, "context": "The intuition is to find a maximally correlated representation of the two views by passing them through multiple stacked layers of nonlinear transformation (Andrew et al., 2013).", "startOffset": 156, "endOffset": 177}, {"referenceID": 2, "context": "In our experiments, we use the a variant of CCA called wGCCA implemented by (Benton et al., 2016) where we set the weights for both views equal 1.", "startOffset": 76, "endOffset": 97}, {"referenceID": 0, "context": "We used the DCCA implementation by (Andrew et al., 2013) which uses Keras and Theano as the deep learning platform 2.", "startOffset": 35, "endOffset": 56}], "year": 2017, "abstractText": "In this paper, we demonstrate how the state-of-the-art machine learning and text mining techniques can be used to build effective social media-based substance use detection systems. Since a substance use ground truth is difficult to obtain on a large scale, to maximize system performance, we explore different feature learning methods to take advantage of a large amount of unsupervised social media data. We also demonstrate the benefit of using multi-view unsupervised feature learning to combine heterogeneous user information such as Facebook \u201clikes\u201d and \u201cstatus updates\u201d to enhance system performance. Based on our evaluation, our best models achieved 86% AUC for predicting tobacco use, 81% for alcohol use and 84% for drug use, all of which significantly outperformed existing methods. Our investigation has also uncovered interesting relations between a user\u2019s social media behavior (e.g., word usage) and substance use.", "creator": "LaTeX with hyperref package"}}}