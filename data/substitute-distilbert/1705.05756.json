{"id": "1705.05756", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "All-relevant feature selection using multidimensional filters with exhaustive search", "abstract": "this paper describes zero property requiring identification of the informative variables in the information system with discrete decision variables. it is expressed specifically towards discovery of the variables that are non - informative are considered alone, but are informative when continuous synergistic interactions between multiple variables are considered. to unknown end, maximal mutual entropy of all possible k - norms of variables with decision variable is computed. then, relating each variable : maximal information gain due to interactions amongst other variables is obtained. for pseudo - coding variables this quantity conforms to the well known statistical distributions. regression allows for discerning truly informative variables from non - informative ones. for demonstration of the approach, the method is applied to continuous dimension datasets that involve complex multidimensional interactions between variables. it is capable of identifying most important informative variables, even in the case when spatial impact of the analysis is smaller than the true dimensionality of the problem. what is more, and high sensitivity of the algorithm allows for avoidance of the influence of nuisance variables on the response variable.", "histories": [["v1", "Tue, 16 May 2017 15:11:10 GMT  (1364kb,D)", "http://arxiv.org/abs/1705.05756v1", "27 pages, 11 figures, 3 tables"]], "COMMENTS": "27 pages, 11 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["krzysztof mnich", "witold r rudnicki"], "accepted": false, "id": "1705.05756"}, "pdf": {"name": "1705.05756.pdf", "metadata": {"source": "CRF", "title": "All-relevant feature selection using multidimensional filters with exhaustive search", "authors": ["Krzysztof Mnich"], "emails": ["k.mnich@uwb.edu.pl", "w.rudnicki@uwb.edu.pl"], "sections": [{"heading": null, "text": "Keywords: All-relevant feature selection, mutual information, multi-dimensional filters"}, {"heading": "1 Introduction", "text": "Modern datasets often contain very large number of variables, most of which are irrelevant for the phenomena under investigation. There are three main approaches to deal with such datasets. One can either select most relevant variables before modelling (filtering), use the dimensionality reduction techniques [45, 36, 40, 23] or apply modelling methodology that performs variable selection as a part of model building. The last option can be exercised in two flavours. Once can either use the modelling method with an embedded feature selection, or apply the wrapper method [12]. The well-known examples of the first approach are lasso [41] or elastic network [52] within domain of linear models. The wrappers can be based on various machine learning algorithms, such as SVM [7] \u2013 as in the SVM-RFE algorithm [13], or random forest [6] \u2013 as in the Boruta algorithm [28].\nar X\niv :1\n70 5.\n05 75\n6v 1\n[ cs\n.A I]\nThe application of feature selection is not limited to building predictive models. One of its common applications is discovery of genes associated with diseases, either within the context of genome wide association studies (GWAS)[17], or analysis of gene expression profiles [1]. The lists of genes revealed with statistical models are further analysed for their biological meaning and verified by wet-lab experiments. The number of variables analysed in gene expression studies can be as large as over fifty thousands [14], however the number is dwarfed by the GWAS where over ten millions of variables can be analysed [9]. One should stress, that in both types of experiments the number of subjects is much lower than the number of variables. With the number of variables that high the computational cost of wrapper and embedded methods is so high that the initial filtering is de-facto standard approach, even if other methods are used later for building final models or construction of the sets of marker genes [20, 30, 14]."}, {"heading": "1.1 Short review of filtering methods", "text": "Numerous filtering approaches that have been proposed in the literature fall in two general classes. In the first one a simple measure of association between the decision variable and descriptive variables is computed, and the variables are ranked according this measure. The association can be measured using for example t-test for population means, signal to noise ratio, correlations between decision and descriptors, as well as various measures based on information entropy [8, 39]. The inclusion to the relevant set is decided by application of the statistical test using either FWER [18], or FDR [4] method. In many cases the set is heuristically truncated to top N features, when selection of N is dictated by convenience [21]. The simple univariate filtering may overlook the variables that affect the investigated phenomenon only in interaction with other features. The number of overlooked but important features may be even higher when truncation is used, since some features that are not very relevant by themselves, may become very relevant when considered in conjunction with other features.\nThe second class of filters uses both the measure of dependence between predictors and dependences within the set of predictor variables to obtain a small set of non-redundant variables that can be used to build predictive models. In some cases, the optimal subset is obtained in a two-step procedure, where in the first step the ranking is obtained and then the selection of non-redundant optimal feature set is performed [51]. The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47]. Recently, the unifying approach based on the Hilbert-Schmidt Independence Criterion (HSIC) [11] has been proposed [37]. It uses nonlinear kernels transforming the feature space and correlations to establish dependence between variables. It has been shown, that certain selection of kernels are equivalent to the well established methods."}, {"heading": "1.2 Case for all-relevant feature selection", "text": "The relevant variables, which are identified in the filtering, can be further used for building predictive models for phenomena under investigation. However, one should not confuse predictive ability with understanding the mechanisms. Statistical and machine learning models may achieve very high predictive power without understanding the underlying phenomena. To obtain such a model, it is merely sufficient when descriptive variables and decision variables are influenced by some unknown hidden driver mechanism.\nOn the other hand, the list of relevant variables can be useful for the researchers working in the field to formulate hypotheses, design experiments and uncover the mechanisms\ngoverning phenomena under scrutiny. We would like to argue that this list should be as complete as possible; at best it should contain all statistically relevant variables.\nThere are at least three distinct reasons for that, which can be illustrated by the following example set in the context of gene selection. Let us assume the hypothetical situation when simultaneous but modest increase of activity of three genes a, b and c is required to represses activity of gene D, which is causing the phenotypic effect under scrutiny. In parallel the increased activity of these genes triggers the activity of the fourth gene E, whose expression is changed by orders of magnitude, but is unrelated to phenotypic effect. In this example the expression levels of genes a, b and c are the driver variables and expression levels of genes D and E are effector variables. The expression level of gene \u201dD\u201d is the decision variable, whereas the all remaining variables are descriptors.\nIn this example, the mechanisms driving the effects under scrutiny are complex and weakly demonstrated, whereas the effects are strong and easy to see. The simple analysis of statistical strength of the effect will discover the strong association between E and D, which may be sufficient to build the good predictive model. At the same time it may fail to discover any of the driver variables.\nSecondly, the measurements are inherently noisy and this influences the measures of relevance. An example of this effect is presented in the B.2. For our token example, the subtle effects in driver variables a, b and c may be hidden in the noise, whereas the amplified effects in the effector variable E can be clearly visible.\nFinally, the heuristic procedures that are used to build the optimal set may result in the selection of effector variables that are optimal for model building and drop the driver variables. In our token example the optimal set consist of variable E.\nThe procedure that reveals relevance of all genes a, b, c along with easily discovered relevance of D would give the researcher whole information and could possibly lead to discovery of the driver mechanism.\nThe univariate algorithms are simple and efficient and their result is an easy to understand list of variables which have a statistically significant connection to the decision variable. Unfortunately, they omit the variables that are relevant only in the interactions with other variables. Hence, in our token example, such algorithms would discover the variable E only. Such variables can be identified by the heuristic multi-dimensional methods, however, in most cases only the subset of all relevant variables is reported - in our example the variable E.\nThe example of the algorithm that both returns the full list of relevant variables and is able to discover variables that are involved in complex non-linear interactions is Relief-f [26]. Unfortunately, it is biased against weaker and correlated variables [35]. This problem may be specially important for example when multiple effector variables are strongly correlated with one driver variable, which is in turn correlated with the decision variable. The bias against highly correlated variables may result in removing an entire set of highly correlated variables from the final result. The bias against weaker predictor variables may lead to diminishing the estimate of their relevance to the point where they are non-separable from the irrelevant ones."}, {"heading": "1.3 Current study", "text": "The goal of the current study is to introduce a rigorous approach for the problem of the all-relevant variable selection, which also includes variables that are relevant due to nontrivial interactions with other variables, variables that are highly correlated and those that are weakly connected with the decision variable.\nTo this end, we propose to use multidimensional exhaustive analysis of the mutual information between decision variable and descriptor variables. Such analysis requires both large number of experimental samples and massive computations that were not\nfeasible until recently. However, rapid development of experimental technology as well as the growth of the computing power, especially using GPU for computation, allows for exhaustive search of pairs and triplets of variables in the context of GWAS [10, 19] as well as gene expression. Currently, even higher-dimensional interactions can be investigated in reasonable time for smaller number of variables.\nNonetheless, the technological developments have not been matched by a methodological advances that are required to identify the synergistic interactions of multiple variables. In particular, it has been shown [27, 19] that the standard measure of synergy can give misleading results in higher-dimensional cases. The attempts to propose different measures proved unsatisfactory \u2013 the tests reported significant multivariate interactions when there are none, or were unable to discover any interactions when they are doubtlessly present (see the next section for examples).\nThe current study introduces a method of identifying variables for which synergistic interactions with other variables increase information about the response variable. It is based on the information theory and allows for precise estimates of the statistical importance of findings. The method is tested using synthetic dataset with complex relations between descriptive variables, and with four various 3-dimensional response functions."}, {"heading": "2 Some previous approaches to the multivariate exhaustive", "text": "search"}, {"heading": "2.1 Regression-based approach", "text": "The regression-based tests are widely used for detection of the multivariate interactions [42, 43, 22]. An example of the regression model of interaction between 2 variables X1, X2, and the response variable Y is a linear regression:\nY = \u03b10 + \u03b11X1 + \u03b12X2 + \u03b112X1X2 (1)\nIn the presence of the interaction, the empirically obtained value of \u03b112 differs from 0 in a statistically significant amount.\nThere are, however, some limits of the regression test for interactions. First, such a test is limited to detection of monotonous interactions only. This is not a serious issue in the case of continuous decision variables, since there are usually some monotonous components of interactions. The problem appears for discrete response variables and also possibly for continuous variable with strongly non-monotonous interactions. For discrete response variables, it has been shown by VanderWeele in [43], that the positive value of \u03b112 is equivalent to a very strong probabilistic condition. Therefore, the regressionbased test still can ignore some important interactions. Moreover, the test becomes more complicated and less reliable, when more variables are considered. Then the regression model contains all the mixed terms \u2013 e.g. the model, that describes interactions between 3 explanatory variables consists of 8 terms. The errors of coefficients grow rapidly, which reduces the sensitivity of the test in more than 2 dimensions."}, {"heading": "2.2 Information measure of synergy", "text": "Several approaches have been proposed for directly searching for synergies, using information theory measure [46, 19, 2]. The most known synergy measure, that uses information theory, is the interaction information of the variable set \u03bd = {Y,X1, . . . , Xk}:\nIint(\u03bd) = \u2212 \u2211 \u03c4\u2286\u03bd (\u22121)|\u03bd|\u2212|\u03c4 |H(\u03c4) (2)\nwhere \u2211\n\u03c4\u2286\u03bd denotes summation over all subsets of \u03bd, and H(\u03c4) is an entropy of the subset \u03c4 [2]. The function becomes positive in presence of k-dimensional synergy, while less-dimensional interactions don\u2019t contribute the positive terms.\nFor a pair of variables X1, X2 the interaction information reads:\nIint(Y,X1, X2) =\u2212H(Y,X1, X2) +H(Y,X1) +H(Y,X2) +H(X1, X2)\n\u2212H(Y )\u2212H(X1)\u2212H(X2) (3)\nThe interaction information is, however, difficult to use for reliable statistic tests. The basic problem is connected with the fact, that the interaction information indicates not only synergy, but also redundancy of the set of variables. The correlations between features add negative terms to Iint(Y, {X}). As a result,\n\u2022 its distribution under the null hypothesis (i.e. in the absence of synergies) is not uniquely defined. It depends on the correlations between the explanatory variables;\n\u2022 in the extreme cases of statistically dependent variables, the interaction information can be zero or negative in spite of existing interactions. The unexpected behaviour of Iint had been reported for subsets of 3 or more variables [19, 2]. The authors proposed some improvements that were claimed to fix the problem in these cases. However, the issue seems to be inherent, since the interaction information test may happen to fail even for 2 explanatory variables, see Appendix B.1 for the illustrative example."}, {"heading": "2.3 Multivariate test of association", "text": "The problems with synergy measures led some researchers to examine an association between the response variable and the entire k-tuple of variables treated as a single feature. In such a case the features that are not relevant alone but show synergistic relations with other features can be identified as relevant. However, there is a significant risk of false positive results, since most k-tuples containing at least one feature, which is correlated with the response variable, would be reported as relevant. In the case of highdimensional datasets with numerous relevant variables, the number of reported k-tuples can be extremly large [16, 10]. One could possibly report only those k-tuples, where none of the variables has been recognised as relevant in the lower-dimensional search, but it is difficult to implement efficiently. What is more, such an approach would also overlook k\u2212tuples with variables that are relevant in the univariate test, but are invloved in strong synergistic relations."}, {"heading": "3 Searching for relevant variables", "text": "The experiences described above inclined us to focus our attention on the variables and propose the method that identifies informative variables, including those that are not recognised as informative by a single-dimensional analysis. Once all the relevant variables are identified, the search of synergies will be much simpler. In particular, all the variables, which are identified as relevant in multi-dimensional analysis and not in the one-dimensional one, are relevant due to synergies.\nIdentification of all informative variables can be conveniently described using the notion of weak relevance, introduced by Kohavi and John in [24].\nDefinition 3.1 weak relevance Let F be a set of descriptors for the response variable Y.\nLet SX be a set of all subsets S of F that do not contain X. The X is weakly relevant if for any of S \u2208 SX the following relation is true:\np(Y = y|X = x, S = s) 6= p(Y = y|S = s) (4)\nHence, we can say that a variable X is weakly relevant if there exists such a subset of variables S, that by adding variable X to this subset increases information on the decision variable Y . In the same paper authors proposed also a higher level of relevance \u2013 the strong relevance. A a variable X is strongly relevant if for all possible subsets of variables S, adding variable X to this subset increases information on the decision variable Y .\nDefinition 3.2 strong relevance Let F be a set of descriptors for the response variable Y. Let SX be a set of all subsets S of F that do not contain X. The X is strongly relevant if for all of S \u2208 SX the following relation is true:\np(Y = y|X = x, S = s) 6= p(Y = y|S = s) (5)\nOne may note, that these two notions are not sufficient to fully reflect all relationships between descriptive variables and decision variable. In particular, the weak relevance may reflect either correlation or synergy between descriptors. To demonstrate the first case let\u2019s assume that two variables X1 and X2 are strongly correlated and inclusion of either of to the subset S increases information on decision variable Y . However, when X1 is already present then adding X2 does not increase the information on Y and vice-versa. The X1 and X2 are both weakly relevant. For demonstration of synergy let\u2019s assume that X1 and X2 are both binary variables and that decision variable is also a binary variable. The decision variable depends on both variables in a nonlinerar fashion: when X1 = X2 the probability of Y = 1 is increased by 10%. Both variables are not correlated and adopt value 0 or 1 with equal probability. When either variable is present in the dataset, then adding another increases the information on the decision variable. On the other hand, when both variables are absent, then adding either one does not increase the information. Again, by definition of weak relevance, both X1 and X2 are weakly relevant.\nIn some cases these definitions lead to counterintuive results. Consider the information system consisting of three descriptive variables X1, X2, X3 and decision variable Y , where X1 = Y , X2 is a pure random noise, and X3 = X1 +X2. It is easy to see, that a subset S consisting of {X2, X3} contains all information on Y , hence adding X1 cannot add any information. On the other hand, adding any variable to X1 also cannot add information on Y , hence all three variables are weakly relevant, despite that X3 is clearly redundant and X2 by itself has no information on decision variable.\nYu and Liu [50] proposed to split weak-relevance into two classes. In their approach variables can be weakly relevant non-redundant or weakly relevant redundant. The redundant variables are defined using the notion of a Markov blanket [25].\nDefinition 3.3 Markov blanket Let F be a set of descriptors for the response variable Y . Given the variable X, let MX \u2208 F , X /\u2208MX . MX is a Markov blanket for X if\np(F \u2212X \u2212M,Y |X,MX) = p(F \u2212X \u2212M,Y |MX) (6)\nDefinition 3.4 Redundant variable A variable X \u2208 G is redundant in G when there exists a Markov blanket for X in G.\nWith this definition the minimal-optimal feature selection problem becomes a problem of finding all non-redundant variables. The notion of weakly-relevant and redundant variables helps to deal with the last example, since the X2 and X3 variables are clearly redundant. Unfortunately, it is not sufficient to help with other limitations.\nThe definitions of weak and strong relevance are absolute in the sense that they require examination of the all possible subsets S of the feature set F , disjoint with X, either to exclude the weak relevance or to prove the strong relevance of the variable X. An exhaustive search of all possible combinations is in most cases not possible due to limited computational resources and limited dataset.\nNevertheless, despite all these shortcomings, the distinction between weak and strong relevance is very useful concept, that explicitly demonstrates that relevance is not a unique property, but can be graded. In the context of practical applications weaker and more specific definitions of weak and strong relevance may be more appropriate than very general definitons proposed by Kohavi and John. In particular, we propose to introduce two notions, k-weak relevance and k-strong relevance that better reflect our limited ability to test exhaustively possible combinations of variables. The variable X is k-weakly relevant if its relevance can be established by analysing all( n k ) subsets of k variables that include variable X. More formally:\nDefinition 3.5 k-Weak Relevance Let F be a set of descriptors for the response variable Y. Let Sk\u22121;X be a set of all subsets S of F with cardinality (k \u2212 1) that do not contain X. The X is k-weakly relevant if for any of S \u2208 Sk\u22121;X the following relation is true:\np(Y = y|X = x, S = s) 6= p(Y = y|S = s) (7)\nSimilarly, the variable X is k-strongly relevant if its it strongly relevant in all ( n k ) subsets of k variables that include variable X. More formally:\nDefinition 3.6 k-Strong Relevance Let F be a set of descriptors for the response variable Y. Let Sk\u22121;X be a set of all subsets S of F with cardinality (k \u2212 1) that do not contain X. The X is k-strongly relevant if for all of S \u2208 Sk\u22121;X the following relation is true:\np(Y = y|X = x, S = s) 6= p(Y = y|S = s) (8)\nOne may note, that the standard univariate filtering is equivalent to finding all 1- weakly relevant variables. In this case the subset S in the definition 8 is simply the empty set \u2205.\nSelection of the particular value of k depends on the available computational resources and size of the dataset. One should note, that even with infinite computational resources the analysis in very high dimensions can be applied only for sufficiently large samples, since good representation of the probability density is necessary to obtain reliable results. Therefore in practice the dimensionality of the analysis may be limited both by the computational resources, when the number of variables is high, or by the sample size, when the number of objects is small. The presented method of feature selection is designed as an initial step of investigation. The analysis of the redundancy and synergies between variables is not performed here and left for the next step of the analysis.\nTo obtain a list of all variables, that exhibit the statistically significant influence on a response variable we use the conditional mutual information between Y and X given the subset S:\nI(Y ;X|S) = [H(X,S)\u2212H(Y,X, S)]\u2212 [H(S)\u2212H(Y, S)] (9)\nwhere H(X) denotes the information entropy of the variable.\nI(Y ;X|S) can be interpreted directly as amount of information about the response variable Y , that is contributed by the variable X to the subset S. It is always non-negative and becomes zero, when the variable contributes no information to the subset.\nNote that in the univariate case, i.e. if the subset S is empty, I(Y ;X|S) reduces to the mutual information of Y and X, commonly used to test the statistical association between the variables.\nI(Y ;X|\u2205) = I(Y ;X)\nThe conditional mutual information has been used in the context of minimal-optimal feature selection, see for example [33, 16, 32, 44], however it has not been used for identification of the synergistic relevant variables."}, {"heading": "3.1 The statistics of conditional mutual information", "text": "Computation of conditional mutual information is straightforward and efficient for discrete variables. In the case of continuous variables there are two possibilities. One can either build an estimate of a multidimensional probablity density function, or one can discretize variables. In the current study we use the quantile discretisation, which method is both simple and robust. To avoid overfitting, the discretisation is not well aligned with the test functions - the descriptive variables are discretised into three equipotent categories, whereas the decision variable is based on power of two periodicity.\nLet the response variable Y , the tested variableX and the variables {Si}, i = 1, . . . , k\u2212 1, be the discrete ones with number of categories CY , CX , {CSi}, respectively. The analysed dataset contains values of the variables for N objects. The obvious estimate of the probability, that the variable takes on a particular value, is\np\u0302x = nx N\nwhere nx is the number of objects in the dataset, for which the variable X takes on the value x. Hence, the estimate of information entropy reads:\nH\u0302(X) = \u2212 \u2211 x p\u0302x log p\u0302x\nand the estimate of the conditional mutual information is equal to:\nI\u0302(Y ;X|S) =  \u2211 y,x,{si} p\u0302yx{si} log p\u0302yx{si} \u2212 \u2211 x,{si} p\u0302x{si} log p\u0302x{si}  \u2212\n\u2211 y,{si} p\u0302y{si} log p\u0302y{si} \u2212 \u2211 {si} p\u0302{si} log p\u0302{si}  (10)\nwhere\np\u0302yx{si} = nyx{si} N , nyx{si} is the number of objects for which\nY = y, X = x, Si = si,\np\u0302x{si} = nx{si} N , nx{si} = \u2211 y nyx{si},\np\u0302y{si} = ny{si} N , ny{si} = \u2211 x nyx{si},\np\u0302{si} = n{si} N , n{si} = \u2211 y,x nyx{si}.\nIf X contributes no information about the response variable to the subset S \u2261 {Si} and the sample size N is sufficiently large, then 2NI\u0302(Y ;X|S) follows the well-known \u03c72 distribution, see Appendix A.1 for details:\n2NI\u0302(Y ;X|S) \u223c \u03c72(df), (11)\ndf = (CY \u2212 1)(CX \u2212 1) k\u22121\u220f i=1 CSi (12)\nSome examples of behaviour of conditional mutual information for various cases of variable associations are shown in Fig. 9, 10 in the Appendix B.2.\n3.2 The minimum p-value analysis\nFor each value of I\u0302(Y ;X|S), one can compute the associated p-value and determine the probability, that the variable X contributes no information to the subset S. To check the k-weak relevance of the variable, we should determine whether there exists any subset S of k \u2212 1 features, in context of which X is relevant. To this end we find the minimum p-value over all the subsets for each variable:\npmin(X) = min S\u2282F\n[ p\u03c72 ( I\u0302(Y ;X|S) )] . (13)\nIf the variable X is irrelevant, pmin(X) proves to follow the exponential distribution, for details, see Appendix A.2:\nP (pmin(X) < v) = 1\u2212 e\u2212\u03b3v. (14)\nThe parameter \u03b3 can be estimated based on the test results for all the variables, most of which are irrelevant. This allows us to calculate the probability of being irrelevant (the p-value) for each variable X. The result can be used directly to decide, whether the variable should be selected as relevant, via standard FWER (e.g. Bonferroni correction) or FDC (like Benjamini-Hochberg [4]) methods."}, {"heading": "4 The algorithm", "text": "The theory described above leads to the algorithm of multivariate feature selection. It consists of the following steps:\n\u2022 If the response variable and the explanatory variables are continuous, they are discretised first.\n\u2022 For each k-tuple of variables Sk the contingency table is built, that contains number of objects, for which the variables take on particular values. This step is the most expensive computationally, however, it is simple, so it can be performed in parallel, for example using GPU.\n\u2022 For each variable X \u2208 Sk the conditional mutual information between the response variable Y and X given the rest of the k-tuple Sk\u22121 is calculated. Then, the associated \u03c72 p-value is calculated.\n\u2022 For each variable the minimum p-value over all the k-tuples is recorded as pmin(X).\nIn a special case, when all the explanatory variables have the same number of categories (e.g. they are all the discretised continuous variables), the last two steps can be simplified. In this case, the minimum p-value corresponds to the maximum value of I(Y ;X|Sk\u22121). Instead of computing all the p-values, it is enough to record the maximum I(Y ;X|Sk\u22121), then calculate the associated p-value once for a variable.\n\u2022 Based on the statistics of pmin over all the variables, the parameter \u03b3 in Eq. (14) is estimated (see Appendix A.2).\n\u2022 The eventual p-value is calculated for each variable.\n\u2022 Finally, either Bonferroni-Holm FWER method, or Benjamini-Hochberg FDC method is used to decide which variables should be selected as relevant."}, {"heading": "5 Tests on Synthetic Datasets", "text": "To explore the performance of the method, we have generated a complex artificial dataset, containing 351 descriptive variables, defined for 5000 objects. There are two general classes of variables in this dataset. The first one consists of variables which are drawn from a (\u22121, 1) uniform distribution, with possible admixture of random noise. The second is obtained as linear combination of primary variables, also with possible admixture of random noise. The random noise is drawn from the (\u22120.15, 0.15) uniform distribution.\nMore specifically, the dataset consists of the following groups of variables:\n1. 3 base variables drawn from the uniform distribution. The response variable is a function of these variables only.\n2. 3 base variables with 15% random noise added.\n3. 20 linear combinations of the base variables (belonging to group 1) with random coefficients.\n4. 20 linear combinations of the base variables (belonging to group 1) and nuisance variables (which are henceforth refferred to as group 5) with random coefficients and 15% random noise added.\n5. 5 nuisance variables drawn from the uniform distribution, see Appendix B.3.1.\n6. 100 random variables.\n7. 200 linear combinations of 10 variables from group 6 with 15% additional random noise.\nThe variables from the groups 1-5 are expected to be weakly relevant, while groups 6-7 contain irrelevant ones. Obviously, the most important variables belong to the groups 1 and 2. The set simulates real datasets, where the pure signal is usually not available, and there are many irrelevant variables that are correlated with each other.\nThe response variable Y is a binary function of first 3 variables. We tested 4 example functions:\n\u2022 Y = (X1 \u00b7X2 \u00b7X3 < 0)\n\u2022 Y = (sin(2\u03c0X1) \u00b7 sin(2\u03c0X2) \u00b7 sin(2\u03c0X3) < 0)\n\u2022 Y = (X21 +X22 +X23 > 0.9)\n\u2022 Y random, independent of X1, X2, X3 2-dimensional versions of the functions are shown in Fig. 1.\nThe methodology presented in the current study is general and applies both to continuous and discrete variables, but for simplicity and efficiency it is restricted here to the discrete case. Therefore, the descriptive variables were discretised to conduct the tests. In all cases the variables were discretised into 3 equipotent monotonic categories. The discretisation was chosen with three criteria in mind. Firstly, it does not give unfair advantage to the base functions, since the discretisation is not particularly well fitted to the response functions under consideration. Secondly, it assures that the representation of each category in multidimensional cases is sufficiently large to minimise variance due to sampling. Finally, the single predefined split gives insight in the influence of the split on the results, at least in the case of the base variables. Statistical tests for discovery of explanatory variables were performed for each response variable, using FDR procedure with \u03b1 = 0.1 with one-, two- and three-dimensonal analyses.\nThe results of a feature selection procedure are often used to build predictive models of the phenomena under scrutiny, with the underlying assumption, that the most relevant (most informative) variables should be used for model building. In the case of the current study we have tested, whether variables selected by multidimensional feature selection procedure lead to better models than those obtained using univariate analysis. To this end, the variables selected by the procedure were used to build a predictive models using Random Forest classifier [6] implemented in R package randomForest [34, 29]. The procedure was performed either with three highest scoring variables or with all variables deemed relevant by the algorithm. As a reference we used predictive models built using following subsets of relevant variables:"}, {"heading": "6 Results", "text": "The three response functions that were generated for the same set of descriptive variables represent a wide range of difficulties - the easy problem in the case of sphere, intermediate in the case of exclusive or, and the 3D checkerboard is the most difficult, as can be seen in the Table 1, where the number of variables identified as relevant is reported.\nIt can be seen, that while the original problems are formulated in three dimensions, the presence of linear combinations of variables can reduce the apparent dimensionality of the problem."}, {"heading": "6.1 An easy problem - a 3D sphere", "text": "In particular, in the case of the spherical decision function, most of the informative variables are easily identified even in one dimension. Extension of the analysis to higher\ndimensions does not lead to large changes of the information gain, see Figure 2. Additionally, the relative ranking of the groups of variables is mostly preserved. The ranking of variables is concordant with the intuition - the pure base variables are scored higher than noisy base variables that are in turn scored higher than linear combinations of pure variables. The noisy linear combinations are below pure linear combinations, and the nuisance variables are scored lowest among informative variables. Nevertheless, increasing the dimensionality of analysis allows for higher sensitivity in the case of noisy linear combination and nuisance variables. In particular, transition from the 1- to 2-dimensional analysis improves the ranking of noisy linear combinations and nuisance variables, see Table 2 and Fig. 2. One should note that nuisance variables are informative only due to interactions with the noisy linear combinations, hence they are non-informative in one dimension and the increase of their ranking in higher dimensional analysis is expected."}, {"heading": "6.2 Intermediate difficulty - a 3D XOR", "text": "Three dimensional XOR is an example of a problem for which the multi-dimensional analysis and all-relevant feature selection is required for full understanding of the system under scrutiny. When only 3 base variables are taken into account, it is an example of pure 3-dimensional synergy, where no variable is informative unless all 3 are analysed together. However, the introduction of linear combinations of the base variables to the descriptive variables significantly affects the analysis.\nThe one-dimensional analysis cannot discover importance of the base variables. On the other hand, half of the pure linear combinations of base variables along with 2 noisy ones are deemed relevant, see Table 1. Hence, the univariate approach to this problem reveals set of twelve variables that can be further used for building predictive models or to design marker sets. However, this set does not contain any of the original variables that were used to generate the problem, neither pure nor noisy ones. Therefore, any reasoning, directed towards elucidation of the mechanism for generation of the response variable, based on the relevant variables identified by the univariate analysis would be seriously flawed.\nIncreasing the dimensionality of analysis to 2D is sufficient to identify nearly all rele-\nvant variables, only 2 out of 5 nuisance variables were not discovered. Nevertheless, the ranking of the variables does not reflect the true relevance of the variables. In particular, five highest scoring variables belong to the pure linear combination class, and the least informative pure base variable has the rank 25. Therefore, any analysis based on the highest scoring variables only, would also likely miss some of the truly relevant variables. Finally, the 3-dimensional analysis reveals all relevant variables, with slightly improved ranks of the base variables.\nIt is interesting to see how the relative ranking of different classes of variables is affected by the dimensionality of the analysis, see Table 2. In the one-dimensional analysis the highest scoring variables are linear combinations of base variables, with non-noisy versions consistently scored higher. The ranks of base variables are essentially randomly dispersed between 25 and 297. In two dimensions the relative ranking between groups of variables is radically changed. The top ranks still belong to non-noisy linear combinations of base variables. However, the base variables themselves, both pure and distorted, appear much higher in the ranking, well within the range spanned by the linear combinations of pure base variables. Interestingly, the noisy combinations score significantly worse, with large gap between information gain of the pure and noisy combinations. Nonetheless, all noisy combinations were also identified as relevant, along with 3 of 5 nuisance variables. The ranking of base variables is even slightly better in the 3-dimensional analysis. Yet, the highest rank for the base variable is only 5, and the lowest one is as far as 20.\nThe graphical summary presented in the Figure 3 shows the results of three stages for 1-, 2- and 3-dimensional analysis. The analysis in 1-, 2- and 3-dimensions are shown in the 1st, 2nd and 3rd row, respectively. The first column in the figure shows the gain of information on the decision variable due to knowledge of the variable under scrutiny, for all variables. The second column shows the P-P plot for the maximal information gain (2D and 3D case) or standard chi-squared distribution in 1D. The last column contains plots of the expected value of false discovery rate computed using Benjamini-Hochberg method.\nParticularly interesting is the excellent concordance between the theoretical and experimental distribution for the irrelevant variables on the P-P plots. In 1D p-values are computed for the chi-squared with 2 degrees of freedom, whereas for 2D and 3D they are computed from the extreme value distribution, with the distribution parameter estimated from data. The fit is also reasonably good, although not perfect in 3D. We suspect\nthat this is due to larger number of bins what can increase variance of the underlying chi-squared distribution. The final step of the algorithm, namely calling the relevant variables, is illustrated in the third column of the Figure 3. One can observe qualitative difference between the 1D on one side and 2D and 3D on the other. In the former, the FDR stays at zero for few variables, and then very rapidly approaches one, whereas for two latter cases the first horizontal segment is much longer and the following increase towards one is much more gradual. This reflects the differences between distribution of ranks in these cases, see Figure 4. In the 1D case, only few truly relevant variables score high, whereas the distribution of ranks for the remaining is random. In 2D and 3D, nearly all relevant variables are ranked higher than the irrelevant ones."}, {"heading": "6.3 Hard problem - 3D checkerboard", "text": "The third example of the informative response function generated using the same descriptor set is a 3-dimensional product of sinus functions that generates 3-dimensional 4\u00d74\u00d74 checkerboard-like pattern. This function is another example of pure synergy when analysed for base descriptors only, with more complex distribution of the response function. What is more, three equipotent splits of descriptors don\u2019t align well with variation of the decision function what makes the analysis harder. This mismatch results in much smaller information gains attributed to informative variables than in the previous example, and lower sensitivity, in particular in lower dimensions. Only two linear combinations of base variables were detected as relevant in 1-dimensional analysis. The sensitivity of the 2- dimensional analysis is higher, resulting in discovery of all of the pure linear combinations along with two of base variables. Finally, the 3-dimensional analysis revealed all the base variables, both pure and distorted, all pure linear combinations and some of the noisy linear combinations, see Figure 5. Furthermore, that even in 3D base variables were ranked lower than all but one linear combinations, see Figure 4. The analysis based on highest scoring variables could miss these most important variables."}, {"heading": "6.4 Random decision function", "text": "The final test is performed on the random response variable that is not dependent on any descriptors. This test is performed to check whether the procedure is not too aggressive and able to call random variables as relevant if there are no true relevant variables. The test is passed, since no variable has been called relevant neither in 1D, nor 2D nor 3D analysis. Additionally, the rankings of variables are completely random."}, {"heading": "6.5 Classification using identified relevant variables", "text": "The results of feature selection were further used for building predictive Random Forest models to test whether by extending the search for informative variables into higher dimension can potentially improve the modelling results. In particular, we were interested in the problem, where, on the one hand a significant fraction of relevant variables were not discovered using 1-dimensional analysis, but on the other hand several relevant variables were found. Therefore, we concentrated on the 3D XOR, since two other problems are either too easy (3D sphere) or too hard (3D checkerboard).\nIt turns out, that the 1-dimensional analysis reveals too few variables for building high quality models, either using all or 3-best variables only, see Table 3. However, both 2- and 3-dimensional analysis find subset of variables sufficient to build high quality models, with quality comparable to those built using all informative variables. Interestingly, the models build on three highest scoring variables are significantly worse than models based on all relevant variables, with the error level roughly six times higher. It is worthwile to note, that the models built using just three base variables are significantly better than models built using all informative variables. This last observation shows that in some cases identification of the causal variables is possible.\nThe results of this analysis show that in the presence of synergistic interactions between variables the extension of the analysis into multiple dimensions allows to find variables involved in these interactions and to improve predictive models."}, {"heading": "7 Conclusions", "text": "In the current study we have explored the possibility of identification of relevant variables, when the response variable is a complex multidimensional function of the descriptive features with nonlinear and synergistic interactions between variables. To this end, we have proposed a rigorous methodology for identification of all the relevant variables by the exhaustive multidimensional analysis using information theory. Based on the solid statistics, the method allows either for the false discovery control using BenjaminiHochberg procedure, or family-wise error rate, for example using Bonferoni correction. We have tested the methodology using the complex multidimensional datasets where response variable was a 3-dimensional nonlinear function of 3 descriptors and descriptor set contained also linear combinations of base variables, noisy copies of relevant variables noisy combinations of base variables, nuisance variables as well as the random noise. We have demonstrated that univariate analysis fails to identify most of the relevant variables in truly multidimensional cases and requires multidimensional analysis.\nThe tests on artificial data confirmed that the new approach is able to identify all or nearly all the weakly relevant variables (in the sense of Kohavi-John [24]), when the dimensionality of the analysis matches the true dimensionality of the problem. The exhaustive search of all possible combinations of variables limits the practical dimensionality of the analysis due to the quick growth of the number of combinations with increasing dimensionality. Nevertheless, the presence of linear combinations of variables effectively lowers the dimensionality of analysis required for discovering the truly important variables. In the case of 3D sphere and 3D XOR, all base variables and all combination variables, as well as most of the nuisance variables were identified already in the 2-dimensional analysis. Only the discovery of all subtle nuisance effects required full 3-dimensional analysis concordant with the true dimensionality of the problem.\nInterestingly, in the two more difficult problems the highest scoring variables are not those that were used for the generation of the response variable, independent of the dimensionality of the analysis. What is more, in some cases the base variables scored significantly below their linear combinations. This result suggests that when the goal of the feature selection is identification of causal variables, then the all-relevant feature selection approach should be applied.\nAcknowledgement The research was funded by the by the Polish National Science Centre, grant DEC-\n2013/09/B/ST6/01550."}, {"heading": "A Statistical toolbox", "text": "A.1 The probability distribution of the conditional mutual information\nThe estimate of the conditional mutual information (10) can be rewritten in the form:\nI\u0302(Y ;X|S) = 1 N \u2211 y,x,{si} nyx{si} log p\u0302yx{si}p\u0302{si} p\u0302x{si}p\u0302y{si} (15)\nwhich allow us to identify the expression:\np\u03020yx{si} = p\u0302y{si}p\u0302x{si}\np\u0302{si}\nas a probability estimate under the null hypothesis. The null hypothesis states, that the variable X contributes no information about Y to the subset S. p\u03020yx{si} is a well-defined probability estimate, since \u2211\ny,x,{si}\np\u03020yx{si} = 1\nHence, NI(Y ;X|S) can be interpreted as a likelihood ratio between the null-hypothesis distribution and the estimate distribution of Y , X, S. It has been proved by Wilks [48], that\n2 \u2211 i ni log p\u0302i p\u03020i\n(where natural logarithm is used), follows the \u03c72 distribution for N \u2212\u2192 \u221e. In practice, the distribution is close to the asymptotic if the contingency table is not sparse, i.e. there are some objects for each combination of values of the variables.\nThe number of degrees of freedom of the \u03c72 distribution can be calculated as\ndf = dfest \u2212 df0 (16)\nwhere dfest and df0 denote the number of degrees of freedom for the estimated model and the null-hypothesis model, respectively. The df is computed using a number of classes of the response variable, the variable under scrutiny and of all variables in the subset Si using the following formula:\ndf = (CY \u2212 1) (CX \u2212 1) \u220f i CSi (17)\nA.2 The statistics of pmin(X)\nFor each variable X multiple statistic tests are performed, over all possible subsets Sk\u22121. For these variables that contribute no information about the response variable Y , each test leads to a \u03c72-distributed quantity.\nIf the tests were mutually independent, then the minimum p-value over all the tests would follow the distribution:\nP (pmin(X) < v) = 1\u2212 (1\u2212 v)n \u2248 1\u2212 e\u2212nv (18)\nfor a sufficiently big number of tests n.\nIn our case, the tests are not independent \u2013 especially for dimensions higher than 2, where many subsets contain common variables. However, the number of tests is large enough to produce the exponential distribution, although the value of the distribution parameter is not known a priori; it can be significantly smaller than the number of tests.\nP (pmin(X) < v) \u2248 1\u2212 e\u2212\u03b3v, \u03b3 < n (19)\nThe distribution parameter \u03b3 can be estimated a posteriori. Assuming that the investigated dataset contains non-negligible number of irrelevant variables. For those variables the pmin follows the same exponential distribution end they can be used for the estimation of the distribution parameter. The number of irrelevant variables should be sufficient for that in most cases. If it is not the case, one can extend the dataset using the noninformative contrast variables [38, 28].\nThe maximum likelihood estimate of \u03b3 is the reciprocal of the average pmin:\n\u03b3\u0302 = 1\n< pmin >\nThe average should be taken over the variables, that fit the exponential distribution \u2013 the exceptionally small values of pmin are supposed to correspond to relevant variables and should be ignored.\nOne additional precaution is necessary. One can expect the average value of pmin to be of the order of the reciprocal of the variable number, so even a handful of incorrect values close to 1 can change it dramatically. Thus, the procedure described above is vulnerable to numerical artifacts that can produce some pathologically big values of pmin. To avoid that effect and assure numerical stability in the presence of numerical artifacts, the outlying large p-values should be ignored. The number of ignored points can be chosen to minimize the weighted square error of the estimated coefficient \u03b3. An illustration of the procedure is shown in Fig. 7."}, {"heading": "B Examples", "text": "B.1 Correlated synergistic variables\nConsider two correlated binary variables X1, X2, and the response variable Y = X1 \u2227X2 i.e. Y = 1 if both X1 and X2 = 1, otherwise Y = 0. The correlations between X1 and X2 as well as a decision variable are shown in the following table\nX1 X1 0 1 Y 0 1\nX2 0 1 3\n1 6 X2 0 0 0\n1 16 1 3 1 0 1\nThe dependence between Y and X1, X2 is a classical example of the epistasis in genetics, as defined by Bateson in 1909 [3]. However, the interaction information Iint(Y,X1, X2) = 0 due to the mutual dependence between X1 and X2.\nB.2 Statistical strength vs. importance of variables\nIn the example dataset the response variable Y is a function of two variables X1, X2, defined as: {Y = 1 if X1 > 0.5 and X2 > 0.5; Y = 0 otherwise}. The descriptor set consists of X1, X2 and additionally their twenty linear combinations fifty random variables. The result of the univariate \u03c72 test is the same for all of the 22 relevant variables (result not shown). When 50% noise is added to each variable, what simulates the real data (left panel of Fig. 8), X1 and X2 are no longer the statistically strongest variables. Only one of them lies among the 10 strongest ones (right panel of Fig. 8). A procedure that restricts search to a few statistically strongest features could ignore the most important ones.\nB.3 Multivariate interactions\nThe example datasets discussed in this section consist of 400 random samples of variables drawn from the same distribution.\nB.3.1 Relevant, redundant and nuisance variables\nThe first example ilustrates notions of relevant, redundant and nuisance variables. The variables are defined as follows:\nX1 and X2 are mutually independent random binary variables;\nX3 is a mixture of X1 and X2 i.e. X3 is equal to X1 for randomly chosen 50% of objects and to X2 for the remaining 50%.\nY = X1 + random noise (random noise is added to for clarity of P-P plots \u2013 otherwise the relevant variables would be represented by vertical lines at 0).\nFig. 9 shows P-P plots of the uni-variate mutual information tests for each variable and the conditional mutual information (in both directions) for each pair of variables. In this set, the X1 and X3are both informative (top left panel), however, X3 is redundant in pair with X1 (bottom left panel). Variable X2 is not informative, neither alone (top left) nor in pair with X1 (top right). However, it is informative, when considered in pair with X3 (bottom right). The mutual information between Y and the irrelevant variable X2 fits the theoretical \u03c72 distribution, unlike those for the relevant X1 and the mixture X3.\nSimilarly, when the pair (X1, X3) is considered, the variable X3 contains less information about the response variable than X1. It is redundant in this subset, so the conditional mutual information I\u0302(Y,X3|X1) follows the \u03c72 distribution. Interestingly, in the pair (X3, X2), both X3 and X2 are informative. X2 is an example of a nuisance variable. Such variable has no direct association with the response variable, however it modifies the value of the truly relevant variable. The so called \u201dlab\u201d, \u201dtechnician\u201d or \u201dhouse\u201d effects fall into this category. While measurement of some variable theoretically should be independent on the person performing the experiment or laboratory where it is performed, nevertheless, such effects happen in practice. In such a case, the information\non the true variables is not available, only the values modified by the known, or unknown nuisance factors. If the pure variable X1 is not available, the knowledge of X2 can be used to refine the signal of X3. Although not directly associated with the response variable, such a variable may be useful for model building. Such effects may be undetectable by a univariate test.\nB.3.2 Pure synergy of independent variables\nIn this example, the variables are defined as follows:\nX1 and X2 are mutually independent random binary variables;\nY = (X1 = X2) + random noise i.e. Y = 1 if X1 and X2 are equal to one another, otherwise Y = 0.\nThe test results are presented in Fig. 10. The univariate test shows that none of the variables proves relevant alone. However, the test of conditional mutual information for the pair of variables discovers the relevance of both X1 and X2. This is a classic example of synergy.\nB.3.3 Epistasis of correlated variables\nThis example explores the problem described in Appendix B.1.\nX1 and X2 are correlated with one another;\nY = (X1 \u2227X2) + random noise i.e. Y = 1 if both X1 and X2 are equal 1.\nThe relevance of the variables X1, X2 can be detected by the univariate test, and the result of the conditional mutual information test is even stronger (see Fig. 11)."}], "references": [{"title": "Distinct types of diffuse large b-cell lymphoma identified by gene expression", "author": ["Ash A Alizadeh", "Michael B Eisen", "R Eric Davis", "Chi Ma", "Izidore S Lossos", "Andreas Rosenwald", "Jennifer C Boldrick", "Hajeer Sabet", "Truc Tran", "Xin Yu"], "venue": "profiling. Nature,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "The large-sample distribution of the likelihood ratio for testing composite hypotheses", "author": ["D. Anastassiou"], "venue": "Mol. Syst. Biol.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Mendel\u2019s principles of heredity, by W", "author": ["William Bateson", "Gregor Mendel"], "venue": "Bateson.  Cambridge [Eng.]University Press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1909}, {"title": "Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing", "author": ["Yoav Benjamini", "Yosef Hochberg"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Statistical dependence measure for feature selection in microarray datasets", "author": ["Ver\u00f3nica Bol\u00f3n-Canedo", "Sohan Seth", "Noelia S\u00e1nchez-Marono", "Amparo Alonso- Betanzos", "Jose C Principe"], "venue": "ESANN,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Comparison of discrimination methods for the classification of tumors using gene expression data", "author": ["Sandrine Dudoit", "Jane Fridlyand", "Terence P Speed"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "A large genome-wide association study of age-related macular degeneration highlights contributions of rare and common variants", "author": ["Lars G Fritsche", "Wilmar Igl", "Jessica N Cooke Bailey", "Felix Grassmann", "Sebanti Sengupta", "Jennifer L Bragg-Gresham", "Kathryn P Burdon", "Scott J Hebbring", "Cindy Wen", "Mathias Gorski"], "venue": "Nature genetics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Gwis - model-free, fast and exhaustive search for epistatic interactions in case-control gwas", "author": ["Benjamin Goudey", "David Rawlinson", "Qiao Wang", "Fan Shi", "Herman Ferra", "Richard M. Campbell", "Linda Stern", "Michael T. Inouye", "Cheng Soon Ong", "Adam Kowalczyk"], "venue": "BMC Genomics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Measuring statistical dependence with hilbert-schmidt norms", "author": ["Arthur Gretton", "Olivier Bousquet", "Alex Smola", "Bernhard Sch\u00f6lkopf"], "venue": "In International conference on algorithmic learning theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "Journal of machine learning research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["Isabelle Guyon", "Jason Weston", "Stephen Barnhill", "Vladimir Vapnik"], "venue": "Machine learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Clinical utility of microarray-based gene expression profiling in the diagnosis and subclassification of leukemia: report from the international microarray innovations in leukemia study group", "author": ["Torsten Haferlach", "Alexander Kohlmann", "Lothar Wieczorek", "Giuseppe Basso", "Geertruy Te Kronnie", "Marie-Christine B\u00e9n\u00e9", "John De Vos", "Jesus M Hern\u00e1ndez", "Wolf- Karsten Hofmann", "Ken I Mills"], "venue": "Journal of Clinical Oncology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Correlation-based feature selection of discrete and numeric class machine learning", "author": ["Mark A Hall"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Intersnp: genome-wide interaction analysis guided by a priori information", "author": ["Christine Herold", "Michael Steffens", "Felix F Brockschmidt", "Max P Baur", "Tim Becker"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Genome-wide association studies for common diseases and complex traits", "author": ["Joel N Hirschhorn", "Mark J Daly"], "venue": "Nature Reviews Genetics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "A sharper bonferroni procedure for multiple tests of significance", "author": ["Yosef Hochberg"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1988}, {"title": "An information-gain approach to detecting three-way epistatic interactions in genetic association studies", "author": ["Ting Hu", "Yuanzhu Chen", "Jeff W Kiralis", "Ryan L Collins", "Christian Wejse", "Giorgio Sirugo", "Scott M Williams", "Jason H Moore"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Filter versus wrapper gene selection approaches in dna microarray domains", "author": ["I\u00f1aki Inza", "Pedro Larra\u00f1aga", "Rosa Blanco", "Antonio J Cerrolaza"], "venue": "Artificial intelligence in medicine,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Feature selection and classification for microarray data analysis: Evolutionary methods for identifying predictive genes", "author": ["Thanyaluk Jirapech-Umpai", "Stuart Aitken"], "venue": "BMC bioinformatics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Glide: Gpu-based linear regression for detection of epistasis", "author": ["Tony Kam-Thong", "C-A Azencott", "Lawrence Cayton", "Benno P\u00fctz", "Andr\u00e9 Altmann", "Nazanin Karbalai", "Philipp G S\u00e4mann", "Bernhard Sch\u00f6lkopf", "Bertram M\u00fcller-Myhsok", "Karsten M Borgwardt"], "venue": "Human heredity,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Subsystem identification through dimensionality reduction of large-scale gene expression data", "author": ["Philip M Kim", "Bruce Tidor"], "venue": "Genome research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Wrappers for feature subset selection", "author": ["Ron Kohavi", "George H. John"], "venue": "Artif. Intell.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "Estimating attributes: analysis and extensions of relief", "author": ["Igor Kononenko"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}, {"title": "Information of interactions in complex systems", "author": ["Klaus Krippendorff"], "venue": "International Journal of General Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Boruta A System for Feature Selection", "author": ["Miron B. Kursa", "Aleksander Jankowski", "Witold R. Rudnicki"], "venue": "Fundamenta Informaticae,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Classification and regression by randomforest", "author": ["Andy Liaw", "Matthew Wiener"], "venue": "R News,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "Criteria for the use of omics-based predictors in clinical trials: explanation and elaboration", "author": ["Lisa M McShane", "Margaret M Cavenagh", "Tracy G Lively", "David A Eberhard", "William L Bigbee", "P Mickey Williams", "Jill P Mesirov", "Mei-Yin C Polley", "Kelly Y Kim", "James V Tricoli"], "venue": "BMC medicine,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Informationtheoretic feature selection in microarray data using variable complementarity", "author": ["Patrick Emmanuel Meyer", "Colas Schretter", "Gianluca Bontempi"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Gene subset selection in microarray data using entropic filtering for cancer classification", "author": ["F\u00e9lix F Gonz\u00e1lez Navarro", "Ll\u00fa\u0131s A Belanche Mu\u00f1oz"], "venue": "Expert Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "A Language and Environment for Statistical Computing", "author": ["R R Core Team"], "venue": "R Foundation for Statistical Computing, Vienna,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Theoretical and empirical analysis of relieff and rrelieff", "author": ["Marko Robnik-\u0160ikonja", "Igor Kononenko"], "venue": "Machine learning,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T Roweis", "Lawrence K Saul"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "Feature selection via dependence maximization", "author": ["Le Song", "Alex Smola", "Arthur Gretton", "Justin Bedo", "Karsten Borgwardt"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}, {"title": "Ranking a Random Feature for Variable and Feature Selection", "author": ["Herve Stoppiglia", "Gerard Dreyfus", "Remi Dubois", "Yacine Oussar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Rankgene: identification of diagnostic genes based on expression", "author": ["Yang Su", "TM Murali", "Vladimir Pavlovic", "Michael Schaffer", "Simon Kasif"], "venue": "data. Bioinformatics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B Tenenbaum", "Vin De Silva", "John C Langford"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2000}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1996}, {"title": "Travelling the world of gene\u2013gene interactions", "author": ["Kristel Van Steen"], "venue": "Briefings in bioinformatics, page bbr012,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "Epistatic interactions", "author": ["Tyler J VanderWeele"], "venue": "Statistical Applications in Genetics and Molecular Biology,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "A review of feature selection methods based on mutual information", "author": ["Jorge R Vergara", "Pablo A Est\u00e9vez"], "venue": "Neural computing and applications,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Singular value decomposition and principal component analysis. In A practical approach to microarray data analysis, pages 91\u2013109", "author": ["Michael E Wall", "Andreas Rechtsteiner", "Luis M Rocha"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2003}, {"title": "Boost: A fast approach to detecting gene-gene interactions in genome-wide case-control studies", "author": ["Xiang Wan", "Can Yang", "Qiang Yang", "Hong Xue", "Xiaodan Fan", "Nelson L.S. Tang", "Weichuan Yu"], "venue": "CoRR, abs/1001.5130,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2010}, {"title": "Selecting feature subset for high dimensional data via the propositional foil rules", "author": ["Guangtao Wang", "Qinbao Song", "Baowen Xu", "Yuming Zhou"], "venue": "Pattern Recognition,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "The large-sample distribution of the likelihood ratio for testing composite hypotheses", "author": ["S.S. Wilks"], "venue": "Ann. Math. Statist., 9(1):60\u201362,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1938}, {"title": "Feature selection for high-dimensional data: A fast correlationbased filter solution", "author": ["Lei Yu", "Huan Liu"], "venue": "In ICML,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2003}, {"title": "Efficient feature selection via analysis of relevance and redundancy", "author": ["Lei Yu", "Huan Liu"], "venue": "Journal of machine learning research,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2004}, {"title": "Searching for interacting features", "author": ["Zheng Zhao", "Huan Liu"], "venue": "In ijcai,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2007}, {"title": "Regularization and variable selection via the elastic net", "author": ["Hui Zou", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology),", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2005}], "referenceMentions": [{"referenceID": 41, "context": "One can either select most relevant variables before modelling (filtering), use the dimensionality reduction techniques [45, 36, 40, 23] or apply modelling methodology that performs variable selection as a part of model building.", "startOffset": 120, "endOffset": 136}, {"referenceID": 32, "context": "One can either select most relevant variables before modelling (filtering), use the dimensionality reduction techniques [45, 36, 40, 23] or apply modelling methodology that performs variable selection as a part of model building.", "startOffset": 120, "endOffset": 136}, {"referenceID": 36, "context": "One can either select most relevant variables before modelling (filtering), use the dimensionality reduction techniques [45, 36, 40, 23] or apply modelling methodology that performs variable selection as a part of model building.", "startOffset": 120, "endOffset": 136}, {"referenceID": 20, "context": "One can either select most relevant variables before modelling (filtering), use the dimensionality reduction techniques [45, 36, 40, 23] or apply modelling methodology that performs variable selection as a part of model building.", "startOffset": 120, "endOffset": 136}, {"referenceID": 9, "context": "Once can either use the modelling method with an embedded feature selection, or apply the wrapper method [12].", "startOffset": 105, "endOffset": 109}, {"referenceID": 37, "context": "The well-known examples of the first approach are lasso [41] or elastic network [52] within domain of linear models.", "startOffset": 56, "endOffset": 60}, {"referenceID": 48, "context": "The well-known examples of the first approach are lasso [41] or elastic network [52] within domain of linear models.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "The wrappers can be based on various machine learning algorithms, such as SVM [7] \u2013 as in the SVM-RFE algorithm [13], or random forest [6] \u2013 as in the Boruta algorithm [28].", "startOffset": 112, "endOffset": 116}, {"referenceID": 24, "context": "The wrappers can be based on various machine learning algorithms, such as SVM [7] \u2013 as in the SVM-RFE algorithm [13], or random forest [6] \u2013 as in the Boruta algorithm [28].", "startOffset": 168, "endOffset": 172}, {"referenceID": 14, "context": "One of its common applications is discovery of genes associated with diseases, either within the context of genome wide association studies (GWAS)[17], or analysis of gene expression profiles [1].", "startOffset": 146, "endOffset": 150}, {"referenceID": 0, "context": "One of its common applications is discovery of genes associated with diseases, either within the context of genome wide association studies (GWAS)[17], or analysis of gene expression profiles [1].", "startOffset": 192, "endOffset": 195}, {"referenceID": 11, "context": "The number of variables analysed in gene expression studies can be as large as over fifty thousands [14], however the number is dwarfed by the GWAS where over ten millions of variables can be analysed [9].", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "The number of variables analysed in gene expression studies can be as large as over fifty thousands [14], however the number is dwarfed by the GWAS where over ten millions of variables can be analysed [9].", "startOffset": 201, "endOffset": 204}, {"referenceID": 17, "context": "With the number of variables that high the computational cost of wrapper and embedded methods is so high that the initial filtering is de-facto standard approach, even if other methods are used later for building final models or construction of the sets of marker genes [20, 30, 14].", "startOffset": 270, "endOffset": 282}, {"referenceID": 26, "context": "With the number of variables that high the computational cost of wrapper and embedded methods is so high that the initial filtering is de-facto standard approach, even if other methods are used later for building final models or construction of the sets of marker genes [20, 30, 14].", "startOffset": 270, "endOffset": 282}, {"referenceID": 11, "context": "With the number of variables that high the computational cost of wrapper and embedded methods is so high that the initial filtering is de-facto standard approach, even if other methods are used later for building final models or construction of the sets of marker genes [20, 30, 14].", "startOffset": 270, "endOffset": 282}, {"referenceID": 5, "context": "The association can be measured using for example t-test for population means, signal to noise ratio, correlations between decision and descriptors, as well as various measures based on information entropy [8, 39].", "startOffset": 206, "endOffset": 213}, {"referenceID": 35, "context": "The association can be measured using for example t-test for population means, signal to noise ratio, correlations between decision and descriptors, as well as various measures based on information entropy [8, 39].", "startOffset": 206, "endOffset": 213}, {"referenceID": 15, "context": "The inclusion to the relevant set is decided by application of the statistical test using either FWER [18], or FDR [4] method.", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "The inclusion to the relevant set is decided by application of the statistical test using either FWER [18], or FDR [4] method.", "startOffset": 115, "endOffset": 118}, {"referenceID": 18, "context": "In many cases the set is heuristically truncated to top N features, when selection of N is dictated by convenience [21].", "startOffset": 115, "endOffset": 119}, {"referenceID": 47, "context": "In some cases, the optimal subset is obtained in a two-step procedure, where in the first step the ranking is obtained and then the selection of non-redundant optimal feature set is performed [51].", "startOffset": 192, "endOffset": 196}, {"referenceID": 12, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 110, "endOffset": 118}, {"referenceID": 45, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 110, "endOffset": 118}, {"referenceID": 47, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 161, "endOffset": 169}, {"referenceID": 28, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 161, "endOffset": 169}, {"referenceID": 4, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 211, "endOffset": 214}, {"referenceID": 22, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 276, "endOffset": 292}, {"referenceID": 29, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 276, "endOffset": 292}, {"referenceID": 27, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 276, "endOffset": 292}, {"referenceID": 43, "context": "The interdependence between variables is examined using simple measures such as correlations between features [15, 49], measures based on the information theory [51, 32], monotonous dependence between variables [5] and various measures of dependency, relevance and redundancy [26, 33, 31, 47].", "startOffset": 276, "endOffset": 292}, {"referenceID": 8, "context": "Recently, the unifying approach based on the Hilbert-Schmidt Independence Criterion (HSIC) [11] has been proposed [37].", "startOffset": 91, "endOffset": 95}, {"referenceID": 33, "context": "Recently, the unifying approach based on the Hilbert-Schmidt Independence Criterion (HSIC) [11] has been proposed [37].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "The example of the algorithm that both returns the full list of relevant variables and is able to discover variables that are involved in complex non-linear interactions is Relief-f [26].", "startOffset": 182, "endOffset": 186}, {"referenceID": 31, "context": "Unfortunately, it is biased against weaker and correlated variables [35].", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "However, rapid development of experimental technology as well as the growth of the computing power, especially using GPU for computation, allows for exhaustive search of pairs and triplets of variables in the context of GWAS [10, 19] as well as gene expression.", "startOffset": 225, "endOffset": 233}, {"referenceID": 16, "context": "However, rapid development of experimental technology as well as the growth of the computing power, especially using GPU for computation, allows for exhaustive search of pairs and triplets of variables in the context of GWAS [10, 19] as well as gene expression.", "startOffset": 225, "endOffset": 233}, {"referenceID": 23, "context": "In particular, it has been shown [27, 19] that the standard measure of synergy can give misleading results in higher-dimensional cases.", "startOffset": 33, "endOffset": 41}, {"referenceID": 16, "context": "In particular, it has been shown [27, 19] that the standard measure of synergy can give misleading results in higher-dimensional cases.", "startOffset": 33, "endOffset": 41}, {"referenceID": 38, "context": "1 Regression-based approach The regression-based tests are widely used for detection of the multivariate interactions [42, 43, 22].", "startOffset": 118, "endOffset": 130}, {"referenceID": 39, "context": "1 Regression-based approach The regression-based tests are widely used for detection of the multivariate interactions [42, 43, 22].", "startOffset": 118, "endOffset": 130}, {"referenceID": 19, "context": "1 Regression-based approach The regression-based tests are widely used for detection of the multivariate interactions [42, 43, 22].", "startOffset": 118, "endOffset": 130}, {"referenceID": 39, "context": "For discrete response variables, it has been shown by VanderWeele in [43], that the positive value of \u03b112 is equivalent to a very strong probabilistic condition.", "startOffset": 69, "endOffset": 73}, {"referenceID": 42, "context": "2 Information measure of synergy Several approaches have been proposed for directly searching for synergies, using information theory measure [46, 19, 2].", "startOffset": 142, "endOffset": 153}, {"referenceID": 16, "context": "2 Information measure of synergy Several approaches have been proposed for directly searching for synergies, using information theory measure [46, 19, 2].", "startOffset": 142, "endOffset": 153}, {"referenceID": 1, "context": "2 Information measure of synergy Several approaches have been proposed for directly searching for synergies, using information theory measure [46, 19, 2].", "startOffset": 142, "endOffset": 153}, {"referenceID": 1, "context": "where \u2211 \u03c4\u2286\u03bd denotes summation over all subsets of \u03bd, and H(\u03c4) is an entropy of the subset \u03c4 [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 16, "context": "The unexpected behaviour of Iint had been reported for subsets of 3 or more variables [19, 2].", "startOffset": 86, "endOffset": 93}, {"referenceID": 1, "context": "The unexpected behaviour of Iint had been reported for subsets of 3 or more variables [19, 2].", "startOffset": 86, "endOffset": 93}, {"referenceID": 13, "context": "In the case of highdimensional datasets with numerous relevant variables, the number of reported k-tuples can be extremly large [16, 10].", "startOffset": 128, "endOffset": 136}, {"referenceID": 7, "context": "In the case of highdimensional datasets with numerous relevant variables, the number of reported k-tuples can be extremly large [16, 10].", "startOffset": 128, "endOffset": 136}, {"referenceID": 21, "context": "Identification of all informative variables can be conveniently described using the notion of weak relevance, introduced by Kohavi and John in [24].", "startOffset": 143, "endOffset": 147}, {"referenceID": 46, "context": "Yu and Liu [50] proposed to split weak-relevance into two classes.", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "I(Y ;X|\u2205) = I(Y ;X) The conditional mutual information has been used in the context of minimal-optimal feature selection, see for example [33, 16, 32, 44], however it has not been used for identification of the synergistic relevant variables.", "startOffset": 138, "endOffset": 154}, {"referenceID": 13, "context": "I(Y ;X|\u2205) = I(Y ;X) The conditional mutual information has been used in the context of minimal-optimal feature selection, see for example [33, 16, 32, 44], however it has not been used for identification of the synergistic relevant variables.", "startOffset": 138, "endOffset": 154}, {"referenceID": 28, "context": "I(Y ;X|\u2205) = I(Y ;X) The conditional mutual information has been used in the context of minimal-optimal feature selection, see for example [33, 16, 32, 44], however it has not been used for identification of the synergistic relevant variables.", "startOffset": 138, "endOffset": 154}, {"referenceID": 40, "context": "I(Y ;X|\u2205) = I(Y ;X) The conditional mutual information has been used in the context of minimal-optimal feature selection, see for example [33, 16, 32, 44], however it has not been used for identification of the synergistic relevant variables.", "startOffset": 138, "endOffset": 154}, {"referenceID": 3, "context": "Bonferroni correction) or FDC (like Benjamini-Hochberg [4]) methods.", "startOffset": 55, "endOffset": 58}, {"referenceID": 30, "context": "To this end, the variables selected by the procedure were used to build a predictive models using Random Forest classifier [6] implemented in R package randomForest [34, 29].", "startOffset": 165, "endOffset": 173}, {"referenceID": 25, "context": "To this end, the variables selected by the procedure were used to build a predictive models using Random Forest classifier [6] implemented in R package randomForest [34, 29].", "startOffset": 165, "endOffset": 173}, {"referenceID": 21, "context": "The tests on artificial data confirmed that the new approach is able to identify all or nearly all the weakly relevant variables (in the sense of Kohavi-John [24]), when the dimensionality of the analysis matches the true dimensionality of the problem.", "startOffset": 158, "endOffset": 162}, {"referenceID": 44, "context": "It has been proved by Wilks [48], that 2 \u2211", "startOffset": 28, "endOffset": 32}, {"referenceID": 34, "context": "If it is not the case, one can extend the dataset using the noninformative contrast variables [38, 28].", "startOffset": 94, "endOffset": 102}, {"referenceID": 24, "context": "If it is not the case, one can extend the dataset using the noninformative contrast variables [38, 28].", "startOffset": 94, "endOffset": 102}, {"referenceID": 2, "context": "The dependence between Y and X1, X2 is a classical example of the epistasis in genetics, as defined by Bateson in 1909 [3].", "startOffset": 119, "endOffset": 122}], "year": 2017, "abstractText": "This paper describes a method for identification of the informative variables in the information system with discrete decision variables. It is targeted specifically towards discovery of the variables that are non-informative when considered alone, but are informative when the synergistic interactions between multiple variables are considered. To this end, the mutual entropy of all possible k-tuples of variables with decision variable is computed. Then, for each variable the maximal information gain due to interactions with other variables is obtained. For non-informative variables this quantity conforms to the well known statistical distributions. This allows for discerning truly informative variables from non-informative ones. For demonstration of the approach, the method is applied to several synthetic datasets that involve complex multidimensional interactions between variables. It is capable of identifying most important informative variables, even in the case when the dimensionality of the analysis is smaller than the true dimensionality of the problem. What is more, the high sensitivity of the algorithm allows for detection of the influence of nuisance variables on the response variable.", "creator": "LaTeX with hyperref package"}}}