{"id": "1705.07867", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "SmartPaste: Learning to Adapt Source Code", "abstract": "deep neural networks have usually shown to succeed at this range of assembly language tasks such as machine translation mode text summarization. while tasks on source code ( ie, formal languages ) have traditionally considered recently, most work in this area does not attempt to capitalize on the unique opportunities attained by widely known syntax and structure. this implementation segment, we introduce smartpaste, a particular task that requires to use such information. memory task is symbolic variant of the program repair problem that requires to adapt a quick ( pasted ) snippet of code to complicated, arbitrary source code. as first solutions, we design a set of deep neural models that learn to represent the context of variable variable location and variable usage in, data flow - sensitive way. our evaluation suggests that our models can learn to solve the smartpaste task in many cases, achieving 58. 6 % accuracy, while learning meaningful representation of variable usages.", "histories": [["v1", "Mon, 22 May 2017 17:16:06 GMT  (34kb)", "http://arxiv.org/abs/1705.07867v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.SE", "authors": ["miltiadis allamanis", "marc brockschmidt"], "accepted": false, "id": "1705.07867"}, "pdf": {"name": "1705.07867.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["t-mialla@microsoft.com", "mabrocks@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n07 86\n7v 1\n[ cs\n.L G\n] 2\n2 M\nay 2\nDeep Neural Networks have been shown to succeed at a range of natural language tasks such as machine translation and text summarization. While tasks on source code (i.e., formal languages) have been considered recently, most work in this area does not attempt to capitalize on the unique opportunities offered by its known syntax and structure. In this work, we introduce SMARTPASTE, a first task that requires to use such information. The task is a variant of the program repair problem that requires to adapt a given (pasted) snippet of code to surrounding, existing source code. As first solutions, we design a set of deep neural models that learn to represent the context of each variable location and variable usage in a data flow-sensitive way. Our evaluation suggests that our models can learn to solve the SMARTPASTE task in many cases, achieving 58.6% accuracy, while learning meaningful representation of variable usages."}, {"heading": "1 Introduction", "text": "The advent of large repositories of source code as well as scalable machine learning methods naturally leads to the idea of \u201cbig code\u201d, i.e., largely unsupervised methods that support software engineers by generalizing from existing source code. Currently, existing machine learning models of source code capture its shallow, textual structure, e.g. as a sequence of tokens [3, 10], as parse trees [7, 11], or as a flat dependency networks of variables [16]. Such models miss out on the opportunity to capitalize on the rich and well-defined semantics of source code. In this work, we take a step to alleviate this by taking advantage of two additional elements of source code: data flow and execution paths. Our key insight is that exposing these semantics explicitly as input to a machine learning model lessens the requirements on amounts of training data, model capacity and training regime and allows us to solve tasks that are beyond the current state of the art.\nTo show how this information can be used, we introduce the SMARTPASTE structured prediction task, in which a larger, existing piece of source code is extended by a new snippet of code and the variables used in the pasted code need to be aligned with the variables used in the context. This task can be seen as a constrained code synthesis task and simultaneously as a useful machine learning-based software engineering tool. To achieve high accuracy on SMARTPASTE, we need to learn representations of program semantics. First an approximation of the semantic role of a variable (e.g., \u201cis it a counter?\u201d, \u201cis it a filename?\u201d) needs to be learned. Second, an approximation of variable usage semantics (e.g., \u201ca filename is needed here\u201d) is required. \u201cFilling the blank element(s)\u201d is related to methods for learning distributed representations of natural language words, such as Word2Vec [12] and GLoVe [14]. However, in our setting, we can learn from a much richer structure, such as data flow information. Thus, SMARTPASTE can be seen as a first step towards learning distributed representations of variable usages in an unsupervised manner. We expect such representations to be valuable in a wide range of tasks, such as code completion (\u201cthis is the variable you are looking for\u201d), bug finding (\u201cthis is not the variable you are looking for\u201d), and summarization (\u201csuch variables are usually called filePath\u201d).\nTo summarize, our contributions are: (i) We define the SMARTPASTE task as a challenge for machine learning modeling of source code, that requires to learn (some) semantics of programs (cf. section 2). (ii) We present five models for solving the SMARTPASTE task by modeling it as a probability distribution over graph structures which represent code\u2019s data flow (cf. section 3). (iii) We evaluate our models on a large dataset of 4.8 million lines of real-world source code, showing that our best model achieves accuracy of 58.6% in the SMARTPASTE task while learning useful vector representations of variables and their usages (cf. section 4)."}, {"heading": "2 The SMARTPASTE Task", "text": "We consider a task beyond standard source code completion in which we want to insert a snippet of code into an existing program and adapt variable identifiers in the snippet to fit the target program (Figure 1). This is a common scenario in software development [4], when developers copy a piece of code from a website (e.g. StackOverflow) or from an existing project into a new context. Furthermore, pasting code is a common source of software bugs [15], with more than 40% of Linux porting bugs caused by the inconsistent renaming of identifiers.\nWhile similar to standard code completion, this task differs in a number of important aspects. First, only variable identifiers need to be filled in, whereas many code completion systems focus on a broader task (e.g. predicting every next token in code). Second, several identifiers need to be filled in at the same time and thus all choices need to be made synchronously, reflecting interdependencies. This amounts to the structured prediction problem of inferring a graph structure (cf. Figure 1b).\nTask Description. We view a source code file as a sequence of tokens t0 . . . tN = T . The source code contains a set of variables v0, v1 \u00b7 \u00b7 \u00b7 \u2208 V \u2286 T . To simplify the presentation, we assume that the source snippet to be pasted has already been inserted at the target location, and all identifiers in it have been replaced by a set P of fresh placeholder identifiers (see Figure 1 for an example).\nThus, our input is a sequence of tokens t0 . . . tN with {t\u03bb1 , . . . , t\u03bbK} = P , and our aim is to find the \u201ccorrect\u201d assignment \u03b1 : P \u2192 V of variables to placeholders. For training and evaluation purposes, a correct solution is one that simply matches the ground truth, but note that in practice, several possible assignments could be considered correct."}, {"heading": "3 Models", "text": "In the following, we discuss a sequence of models designed for the SMARTPASTE task, integrating more and more known semantics of the underlying programming language. All models share the concepts of a context representation c(t) of a token t and a usage representation u(t, v) of the usage of a variable v at token t. The models differ in the definitions c(t) and u(t, v), but all finally try to maximize the inner product of c(t) and u(t, v) for the correct variable assignment v at t.\nNotation We use Vt \u2282 V to refer to the set of all variables in scope at the location of t, i.e., those variables that can be legally used at t. Furthermore, we use Up(t, v) \u2208 T \u222a {\u22a5} to denote the last occurrence of variable v before t in T (resp. Un(t, v) for the next occurrence), where \u22a5 is used when no previous (resp. next) such token exists. To denote all uses of a variable v \u2208 V, we use U(v) \u2282 T . To capture the flow of data through a program, we furthermore introduce the notation Dp(t, v) \u2286 T , which denotes the set of tokens at which v was possibly last used in an execution of the program (i.e. either read from or written to). Similarly, Dn(t, v) denotes the tokens at which v is next used. Note that Dp(t, v) (resp. Dn(t, v)) is a set of tokens and extends the notion of Up(t, v) (resp. Un(t, v)) which refers to a single token. Furthermore Dp(t, v) may include tokens appearing after t (resp. Dn(t, v) may include tokens appearing before t) in the case of loops, as it happens for variable i in \u03bb11 and \u03bb6 in Figure 1. Dp(t, v) and Dn(t, v) for the snippet in Figure 1 are depicted in Figure 2.\nLeveraging Variable Type Information We assume a statically typed language and that the source code can be compiled, and thus each variable has a (known) type \u03c4(v). To use it, we define a learnable embedding function r(\u03c4) for known types and additionally define an \u201cUNKTYPE\u201d for all unknown/unrepresented types. We also leverage the rich type hierarchy that is available in many object-oriented languages. For this, we map a variable\u2019s type \u03c4(v) to the set of its supertypes, i.e. \u03c4\u2217(v) = {\u03c4 : \u03c4(v) implements type \u03c4}\u222a{\u03c4(v)}. We then compute the type representation r\u2217(v) of a variable v as the element-wise maximum of {r(\u03c4) : \u03c4 \u2208 \u03c4\u2217(v)}. We chose the maximum here, as it is a natural pooling operation for representing partial ordering relations (such as type lattices). Using all types in \u03c4\u2217(v) allows us to generalize to unseen types that implement common supertypes or interfaces. For example, List<K> has multiple concrete types (e.g. List<int>, List<string>). Nevertheless, these types implement a common interface (IList) and share common characteristics. During training, we randomly select a non-empty subset of \u03c4\u2217(v) which ensures training of all known types in the lattice. This acts both like a dropout mechanism and allows us to learn a good representation for types that only have a single known subtype in the training data.\nContext Representations To fill in placeholders, we need to be able to learn how they are used. Intuitively, usage is defined by the source code surrounding the placeholder, as it describes what operations are performed on it. Consequently, we define the notion of a context of a token tk as the sequences of C tokens before and after tk (we use C = 3). We use a learnable function f that embeds each token t separately into a vector f(t) and finally compute the context representation c(t) using two learnable functions gp and gn to combine the token representations as follows.\nc(tk) = Wc \u00b7 [g p (f(tk\u2212C), . . . , f(tk\u22121)) , g n (f(tk+1), . . . , f(tk+C))]\nHere, Wc is a simple (unbiased) linear layer. Note that we process the representation of preceding and succeeding tokens separately, as the semantics of tokens strongly depends on their position relative to t. In this work, we experiment with a log-bilinear model [13] and a GRU [8] for g. Our embedding function f(t) integrates type information as follows. If t is a variable (i.e. t \u2208 v) it assigns r\u2217(v) to f(t). For each non-variable tokens t, it returns a learned embedding rt.\nUsage Representations We learn a vector representation u(t, v) as an approximation of the semantics of a variable v at position t by considering how it has been used before and after t. Here, we consider two possible choices of representing usages, namely the lexical usage representation and the data flow usage representation of a variable.\nFirst, we view source code as a simple sequence of tokens. We define the lexical usage representation uL(t, v) of a variable v at placeholder t using up to L (fixed to 14 during training1) usages of v around t in lexical order. For this, we use our learnable context representation c, and define a sequence of preceding (resp. succeeding) usages of a variable recursively as follows.\nUL p (L, t, v) =\n{\nUL p (L\u2212 1, t\u2032, v) \u25e6 c(t\u2032) if L > 0 \u2227 t\u2032 = Up(t, v) 6= \u22a5 \u01eb otherwise\nUL n (L, t, v) =\n{\nc(t\u2032) \u25e6 UL n (L \u2212 1, t\u2032, v) if L > 0 \u2227 t\u2032 = Un(t, v) 6= \u22a5 \u01eb otherwise\n1We set L = 14 to capture the 98th percentile of our training data and also allow efficient batching with padding instead of choosing the maximum L in the data.\nHere, \u25e6 is sequence composition and \u01eb is the empty sequence. Then, we can define uL(t, v) = h(UL\np (L, t, v),UL n (L, t, v)), i.e. the combination of the representations of the surrounding contexts. We will discuss two choices of h below, namely averaging and a RNN-based model. Note that c(t) is not included in either UL\np (L, t, v) or UL n (L, t, v)\nOur second method for computing u(t, v) takes the flow of data into account. Instead of using lexically preceding (resp. succeeding) contexts, we consider the data flow relation to identify relevant contexts. Unlike before, there may be several predecessors (resp. successors) of a variable use in the data flow relationship, e.g. to reflect a conditional operation on a variable. Thus, we define a tree of D preceding contexts UD\np (D, t, v), re-using our context representation c, as a limited unrolling2 of\nthe data flow graph as follows.\nUD p (D, t, v) =\n{\n{(c(t\u20320),U D p (D \u2212 1, t\u20320, v)), . . . , (c(t \u2032 d),U D p (D \u2212 1, t\u2032d, v))} ifD > 0 \u2227 Dp(t, v) = {t \u2032 0, . . . , t \u2032 d} \u2205 otherwise\nThe tree ofD succeeding contexts UD n (D, t, v) is defined analogously. For example, Figure 2 shows UD p (2, \u03bb8, \u00b7) and UDn (2, \u03bb8, \u00b7) for all variables in scope at \u03bb8 of Figure 1. We then compute a representation for the trees using a recursive neural network, whose results are then combined with an unbiased linear layer to obtain uD(t, v). Again, c(t) is not in either UD\np (D, t, v) or UD n (D, t, v).\nNote that in this way of computing the context, lexically distant variables uses (e.g. before a long conditional block or a loop) can be taken into account when computing a representation."}, {"heading": "3.1 Learning to Paste", "text": "Using the context representation c(t) of the placeholder t and the usage representations u(t, v) of all variables v \u2208 V we can now formulate the probability of a single placeholder t being filled by a variable v as the inner product of the two vectors:\np(T [replace t by v]) \u221d (c(t))T \u00b7 u(t, v)\nWhen considering more than one placeholder, we aim to find an assignment \u03b1 : P \u2192 V such that it maximizes the probability of the code obtained by replacing all placeholders t \u2208 P according to \u03b1 at the same time, i.e.,\nargmax \u03b1 p(T [replace all t \u2208 P by \u03b1(t)]). (1)\nAs in all structured prediction models, training the model directly on Equation 1 is computationally intractable because the normalization constant requires to compute exponentially (up to |V||P|) many different assignments. Thus, during training, we choose to train on a single usage, i.e. max\u03b8 p(T [replace t by \u03b1(t) and all others are fixed to ground truth]) where \u03b8 are all the parameters of the trained model. However, this objective is still computationally expensive since it requires to compute u(t, v) for all v of the variably-sized |V| per placeholder. To circumvent this problem and allow efficient batching, we approximate the normalization constant by using all variables in the current minibatch and train using maximum likelihood.\n2D = 15 during training to covers the 98th percentile in our training data and allows us to batch.\nAt test time, we need to fill in several placeholders in a given snippet of inserted code. To solve this structured prediction problem, we resort to iterative conditional modes (ICM), where starting from a random allocation \u03b1, iteratively for each placeholder t, we pick the variable v\u2217 = argmaxv\u2208Vt p(T [replace t by v]) until the assignment map \u03b1 converges or we reach a maximum number of iterations. To recover from local optima, we restart the search a few times; selecting the allocation with the highest probability. Note that Up(t, v), Un(t, v), Dp(t, v), Dn(t, v) and thus u(t, v) change during ICM, as the underlying source code is updated to reflect the last chosen assignment \u03b1.\nModel Zoo We evaluate 5 different models in this work, based on different choices for the implementation of u(t, v) and c(t).\n\u2022 LOC is a baseline using only local type information, i.e. u(t, v) = r\u2217(v).\n\u2022 AVGG averages over the (variable length) context representations of the lexical context, i.e.\nu(t, v) = r\u2217(v) + 1 \u2223\n\u2223UL p (L, t, v)\n\u2223 \u2223+ |UL n (L, t, v)|\n(\n\u2211\ni\n(UL p (L, t, v))i +\n\u2211\ni\n(UL n (L, t, v))i\n)\n.\n\u2022 GRUG uses a combination of the outputs of two GRUs to process the representations of the lexical context, i.e.\nu(t, v) = Wgru \u00b7 [ RNN p GRU(U L p (L, t, v)),RNNnGRU(U L n (L, t, v)) ] ,\nwhereWgru is a learned (unbiased) linear layer. Note that the two RNNs have different learned parameters. The initial state of the RNNGRU is set to r \u2217(v).\n\u2022 GRUD uses two TreeGRU models (akin to TreeLSTM of Tai et al. [19], but using a GRU cell) over the tree structures UD\np (D, t, v) and UD n (D, t, v), where we pool the representations com-\nputed for child nodes using an element-wise maximum operation elmax. The state of leafs of the data flow tree are again initialized with the type embedding of v, and thus, we have\nqp(D, t, v) =\n{\nelmaxt\u2032\u2208Dp(t,v)(GRU(c(t \u2032),qp(D \u2212 1, t\u2032, v))) ifD > 0 \u2227 Dp(t, v) 6= \u2205 r\u2217(v) otherwise.\nAnalogously, we define qn(D, t, v) and combine them to obtain\nu(t, v) = WD \u00b7 [qp(D, t, v),qn(D, t, v)] ,\nwhereWD is a learned (unbiased) linear layer.\n\u2022 HD is a hybrid between AVGG and GRUD, which uses another linear layer to combine their usage representations into a single representation of the correct dimensionality."}, {"heading": "4 Evaluation", "text": "Dataset We collected a dataset for the SMARTPASTE task from open source C# projects on GitHub. To select projects, we picked the top-starred (non-fork) projects in GitHub. We then filtered out projects that we could not (easily) compile in full using Roslyn3, as we require a compilation to extract precise type information for the code (including those types present in external libraries). Our final dataset contains 27 projects from a diverse set of domains (compilers, databases, . . . ) with about 4.8 million non-empty lines of code. A full table is shown in Appendix D.\nWe then created SMARTPASTE examples by selecting snippets of up to 80 syntax tokens (in practice, this means snippets are about 10 statements long) from the source files of a project that are either children of a single AST node (e.g. a block or a for loop) or are a contiguous sequence of statements. We then replace all variables in the pasted snippet by placeholders. The task is then to infer the variables that were replaced by placeholders.\nFrom our dataset, we selected two projects as our validation set. From the rest of the projects, we selected five projects for UNSEENPROJTEST to allow testing on projects with completely unknown structure and types. We split the remaining 20 projects into train/validation/test sets in the proportion 60-5-35, splitting along files (i.e., all examples from one source file are in the same set). We call the test set obtained like this SEENPROJTEST.\n3http://roslyn.io"}, {"heading": "4.1 Quantitative Evaluation", "text": "As a structured prediction problem, there are multiple measures of performance on the task. In the first part of Table 1, we report metrics when considering one placeholder at a time, i.e. as if we are pasting a single variable identifier. Accuracy reports the percent of correct predictions, MRR reports the mean reciprocal rank of each prediction. We also measure type correctness, i.e. the percent of single-placeholder suggestions that yielded a suggestion of the correct type. In a similar fashion, we present the results when pasting a full snippet. Now, we perform structured prediction over all the placeholders within each snippet, so we can now further compute exact match metrics over all the placeholders. All the models reported here are using a log-bilinear model for computing the context representation c(tk). Using a GRU for computing c(tk) yielded slightly worse results for all models. We believe that this is due to optimization issues caused by the increased depth of the network.\nOur results in Table 1 show that LOC \u2014 as expected \u2014 performs worse than all other models, indicating that our other models learn valuable information from the provided usage contexts. Somewhat surprisingly, our relatively simple AVGG already performs well. On the other hand, GRUD performs worse than models not taking the flow of data into account. We investigated this behavior more closely and found that the lexical context models can often profit from observing the use of variables in other branches of a conditional statement (i.e., peek at the then case when handling a snippet in the else branch). Consequently, HD, which combines data flow information with all usages always achieves high performance using both kinds of information. Finally, to evaluate the need for type information, we run the experiments removing all type information. This\u2014 on average \u2014 resulted to an 8% reduced performance on the SMARTPASTE task on all models.\nSame-Type Decisions So far, we considered the SMARTPASTE task where for each placeholder the neural networks consider all variables in scope. However, if we assume that we know the desired type of the placeholder, we can limit the set of suggestions. The last set of metrics in Table 1 evaluate this scenario, i.e. the suggestion performance within placeholders that have two or more type-correct possible suggestions. In our dataset, there are on average 5.4 (median 2) same-type variables in-scope per placeholder used in this evaluation. All networks (except LOC) achieve high precision-recall with a high AUC. This implies that our networks do not just learn typing information. Furthermore, for 10% recall our best model achieves a precision of 99.1%. First, this suggests that these models can be used as a high-precision method for detecting bugs caused by copy-pasting or porting that the code\u2019s existing type system would fail to catch. Additionally, this indicates that our model have learned a probabilistic refinement of the existing type system, i.e., that they can distinguish counters from other int variables; file names from other strings; etc.\nGeneralization to new projects Generalizing across a diverse set of source code projects with different domains is an important challenge in machine learning. We repeat the evaluation using the UNSEENPROJTEST set stemming from projects that have no files in the training set. The right side of Table 1 shows that our models still achieve good performance, although it is slightly lower compared to SEENPROJTEST, especially when matching variable types. This is expected since the type lattice is mostly unknown in UNSEENPROJTEST. We believe that some of the most important\nissues when transferring to new domains is the fact that projects have significantly different type hierarchies and that the vocabulary used (e.g. by method names) is very different from the training projects."}, {"heading": "4.2 Qualitative Evaluation", "text": "We show an example of the SMARTPASTE task in Figure 3, where we can observe that the model learns to discriminate both among variables with different types (elements of type IHTMLElement is not confused with string variables) as well as assigning more fine-grained semantics (url and path are treated separately) as implied by the results for our same-type scenario above.\nIn Figure 4, we show placeholders that have highly similar usage context representations u(t, v). Qualitatively, Figure 4 and the visualizations in Appendix B suggest that the learned representations can be used as a learned similarity metric for variable usage semantics. These representations learn protocols and conventions such as \u201cafter accessing X, we should access Y\u201d or the need to conditionally check a property, as shown in Figure 4.\nWe observed a range of common problems. Most notably, variables that are declared but not explicitly initialized (e.g. as a method parameter) cause the usage representation to be uninformative, grouping all such declarations into the same representation. The root cause is the limited information available in the context representations. Local optima in ICM and UNK tokens also are common."}, {"heading": "5 Related Work", "text": "Our work builds upon the recent field of using machine learning for source code artifacts. Recent research has lead to language models of code that try to model the whole code. Bhoopchand et al. [6], Hindle et al. [10] model the code as a sequence of tokens, while Maddison and Tarlow [11], Raychev et al. [17] model the syntax tree structure of code. All the work on language models of code find that predicting variable and method identifiers is one of biggest challenges in the task. We are not aware of any models that attempt to use data flow information for variables.\nface. Note that the local context if( ? .HasValue) is not used when computingu(t, v) but data flow information of the other usages is used (marked in yellow). In this example, the model learns a common representation for Nullables that are assigned and then conditionally used by accessing the .HasValue property. The formatting of the snippets has been changed for space saving. More examples can be found in Appendix A.\nClosest to our work is the work of Allamanis et al. [2] who learn distributed representations of variables using all their usages to predict their names. However, they do not use data flow information and only consider semantically equivalent renaming of variables (\u03b1-renaming). Finally, the work of Raychev et al. [16] is also relevant, as it uses a dependency network between variables. However, all variable usages are deterministically known beforehand (as the code is complete and remains unmodified), as in Allamanis et al. [1, 2].\nOur work is remotely related to work on program synthesis using sketches [18] and automated code transplantation [5]. However, these approaches require a set of specifications (e.g. input-output examples, test suites) to complete the gaps, rather than statistics learned from big code. These approaches can be thought as complementary to ours, since we learn to statistically complete the gaps without any need for specifications, by learning common dataflow structure from code.\nOur problem has also similarities with coreference resolution in NLP and methods for the structured prediction of graphs and \u2014 more commonly \u2014 trees. However, given the different characteristics of the problems, such as the existence of exact execution path information, we are not aware of any work that would be directly relevant. Somewhat similar to our work, is the work of Clark and Manning [9], who create a neural model that learns to rank pairs of clusters of mentions to either merge them into a single co-reference entity or keep them apart."}, {"heading": "6 Discussion & Conclusions", "text": "Although source code is well understood and studied within other disciplines such as programming language research, it is a relatively new domain for deep learning. It presents novel opportunities compared to textual or perceptual data, as its (local) semantics are well-defined and rich additional information can be extracted using well-known, efficient program analyses. On the other hand, integrating this wealth of structured information poses an interesting challenge. Our SMARTPASTE task exposes these opportunities, going beyond more simple tasks such as code completion. We consider it as a first proxy for the core challenge of learning the meaning of source code, as it requires to probabilistically refine standard information included in type systems.\nWe see a wealth of opportunities in the research area. To improve on our performance on the SMARTPASTE task, we want to extend our models to additionally take identifier names into account, which\nare obviously rich in information. Similarly, we are interested in exploring more advanced tasks such as bug finding, automatic code reviewing, etc."}, {"heading": "Acknowledgments", "text": "We would like to thank Alex Gaunt for his valuable comments and suggestions."}, {"heading": "A Per Placeholder Suggestion Samples", "text": "Below we list a set of sample same-type decisions made when considering one placeholder at a time. Some code comments and formatting have been altered for typesetting reasons. The ground truth choice is underlined.\nSample 1\nprivate static DataTable CreateDataTable(int cols, string colNamePrefix) {\nvar table = new DataTable(); for (int i = 1; i <= cols; i++) {\ntable.Columns.Add(new DataColumn() { ColumnName = colNamePrefix + #1 ,\nDefaultValue = #2 }); } table.Rows.Add(table.NewRow()); return table;\n}\n#1 i: 84%, cols: 16%\n#2 i: 53%, cols: 47%\nSample 2\npublic void A_VectorClock_must_not_happen_before_itself() {\nvar clock1 = VectorClock.Create(); var clock2 = VectorClock.Create();\n( #1 != #2 ).Should().BeFalse(); }\n#1 clock1: 44%, clock2: 56%\n#2 clock1: 9%, clock2: 91%\nSample 3\npublic MergeHub(int perProducerBufferSize) {\nif ( #1 <= 0)\nthrow new ArgumentException(\"Buffer size must be positive\", nameof( #2 ));\n_perProducerBufferSize = perProducerBufferSize; DemandThreshold = perProducerBufferSize/2 + perProducerBufferSize%2; Shape = new SourceShape<T>(Out);\n}\n#1 perProducerBufferSize: 100%, _perProducerBufferSize: 2e-4, DemandThreshold:\n1e-6 #2 perProducerBufferSize: 100%, _perProducerBufferSize: 3e-3, DemandThreshold:\n2e-3\nSample 4\npublic Task UpdateRuntimeStatistics(SiloAddress siloAddress, SiloRuntimeStatistics siloStats) { if (logger.IsVerbose)\nlogger.Verbose(\"UpdateRuntimeStatistics from {0}\", siloAddress); if (this.siloStatusOracle.GetApproximateSiloStatus(siloAddress)\n!= SiloStatus.Active) return Task.CompletedTask;\nSiloRuntimeStatistics old; // Take only if newer. if (periodicStats.TryGetValue(siloAddress, out old)\n&& old.DateTime > siloStats.DateTime) return Task.CompletedTask;\n#1 [siloAddress] = #2 ;\nNotifyAllStatisticsChangeEventsSubscribers(siloAddress, #3 ); return Task.CompletedTask;\n}\n#1 periodicStats: 94%, PeriodicStats: 6%\n#2 siloStats: 89%, old: 11%\n#3 old: 54%, siloStats: 46%\nSample 5\npublic override BoundNode VisitLocal(BoundLocal node) {\nLocalSymbol localSymbol = node.LocalSymbol; CheckAssigned(localSymbol, node.Syntax);\nif (localSymbol.IsFixed &&\n(this. #1 .MethodKind == MethodKind.AnonymousFunction ||\nthis. #2 .MethodKind == MethodKind.LocalFunction) &&\n#3 .Contains(localSymbol)) {\nDiagnostics.Add(ErrorCode.ERR_FixedLocalInLambda, new SourceLocation(node.Syntax), localSymbol);\n} return null;\n}\n#1 currentMethodOrLambda: 100%, topLevelMethod: 4e-3\n#2 currentMethodOrLambda: 100%, topLevelMethod: 2e-4\n#3 _writtenVariables: 60%,_capturedVariables: 40%\nSample 6\nprivate IDbContextServices InitializeServices() {\nif ( #1 ) {\nthrow new InvalidOperationException(CoreStrings.RecursiveOnConfiguring); }\n...\n#1 _initializing: 73%, _disposed: 27%\nSample 7\npublic static IMutableForeignKey GetOrAddForeignKey( [NotNull] this IMutableEntityType entityType, [NotNull] IReadOnlyList<IMutableProperty> properties, [NotNull] IMutableKey principalKey, [NotNull] IMutableEntityType principalEntityType) {\nCheck.NotNull( #1 , nameof( #2 ));\nreturn #3 .FindForeignKey(properties, principalKey, #4 )\n?? #5 .AddForeignKey(properties, principalKey, #6 ); }\n#1 entityType: 100%, principalEntityType: 4e-3\n#2 entityType: 100%, principalEntityType: 3e-4\n#3 entityType: 78%, principalEntityType: 22%\n#4 principalEntityType: 100%, entityType: 2e-3\n#5 principalEntityType: 60%, entityType: 30%\n#6 entityType: 99%, principalEntityType: 1%\nSample 8\npublic string URL {\nget {\nif ( #1 == null) {\n// Read the URL into a string Stream stream = (Stream)m_dataObject.GetData(DataFormatsEx.URLFormat); StreamReader reader = new StreamReader(stream);\nusing (reader) {\n#2 = reader.ReadToEnd().Trim((char)0); }\n}\nreturn #3 ; }\n}\n#1 m_url: 90%, m_title: 5%, URL: 4%, Title: 1%\n#2 m_url: 84%, Title:13%, m_title: 1%, URL: 1%,\n#3 m_url: 99%, m_title: 4e-3, URL: 3e-3, Title:6e-4\nSample 9\ninternal static byte[] UrlEncodeToBytes(byte[] bytes, int offset, int count) {\nif (bytes == null) throw new ArgumentNullException(\"bytes\");\nint blen = bytes.Length;\nif ( #1 == 0) return ArrayCache.Empty<byte>();\nif ( #2 < 0 || #3 >= #4 ) throw new ArgumentOutOfRangeException(\"offset\");\n...\n#1 blen: 85%, offset: 9%, count: 6%\n#2 offset: 43%, blen: 36%, count: 21%\n#3 offset: 76%, blen: 13%, count: 11%\n#4 count: 60%, offset: 31%, blen: 10%\nSample 10\nprivate static List<UsingDirectiveSyntax> AddUsingDirectives( CompilationUnitSyntax root, IList<UsingDirectiveSyntax> usingDirectives) { // We need to try and not place the using inside of a directive if possible. var usings = new List<UsingDirectiveSyntax>(); var endOfList = root.Usings.Count - 1; var startOfLastDirective = -1; var endOfLastDirective = -1;\nfor (var i = 0; #1 < root.Usings.Count; #2 ++) {\nif (root.Usings[ #3 ].GetLeadingTrivia() .Any(trivia => trivia.IsKind(SyntaxKind.IfDirectiveTrivia))) {\n#4 = #5 ; }\nif (root.Usings[ #6 ].GetLeadingTrivia() .Any(trivia => trivia.IsKind(SyntaxKind.EndIfDirectiveTrivia))) {\n#7 = #8 ; }\n} ...\n#1 i: 98%, endOfList: 1%, startOfLastDirective: 2e-3, endOfLastDirective: 3e-3\n#2 i: 99%, endOfList: 2e-3, startOfLastDirective: 6e-3, endOfLastDirective: 1e-3\n#3 i: 100%, endOfList: 1e-4, startOfLastDirective: 3e-4%, endOfLastDirective: 5e-5 #4 endOfLastDirective: 58%, startOfLastDirective: 30%, endOfList: 1%, i: 3e-3\n#5 endOfLastDirective: 77%, startOfLastDirective: 12%, i: 6%, endOfList: 5%\n#6 i: 100%, endOfList: 1e-3, startOfLastDirective: 2e-4, endOfLastDirective: 5e-4\n#7 endOfLastDirective: 52%, startOfLastDirective: 37%, endOfList: 1%, i: 3e-3,\n#8 i: 53%, endOfLastDirective: 27%, startOfLastDirective: 13%, endOfList: 7%"}, {"heading": "B Nearest Neighbor of Usage Representations", "text": "Here we show pairs of nearest neighbors based on the cosine similarity of the learned representations\nu(t, v). Each placeholder t is marked as ? and all usages of v are marked in yellow (i.e.\nvariableName ). Although names of variables are shown for convenience, they are not used (only their types \u2014 if known \u2014 is used). This is a set of hand-picked examples showing good and bad examples. A brief description follows after each pair.\nSample 1\npublic void SetDateTime(string year, string month, string day) {\nstring time = \"\"; if (year.Contains(\":\")) {\n? = year; year = DateTime.Now.Year.ToString(); TimeInfo = true;\n}\nDateTime = DateTime.Parse(string.Format(\"{0}/{1}/{2} {3}\", year,\nmonth, day, time )); DateTime = DateTime.ToLocalTime();\n}\npublic void MakeMultiDirectory(string dirName) {\nstring path = \"\";\nstring[] dirs = dirName.Split(\u2019/\u2019); foreach (string dir in dirs) {\nif (!string.IsNullOrEmpty(dir)) {\n? = URLHelpers.CombineURL(path, dir);\nMakeDirectory(URLHelpers.CombineURL(Options.Account.FTPAddress, path ));\n} }\nWriteOutput(\"MakeMultiDirectory: \" + dirName); }\n\u22b2 Usage context where a string has been initialized to blank but may be reassigned before it is used.\nSample 2\n... FtpWebRequest request = (FtpWebRequest)WebRequest.Create(url);\n? .Proxy = Options.ProxySettings;\nrequest .Method = WebRequestMethods.Ftp.ListDirectory;\nrequest .Credentials = new NetworkCredential(Options.Account.Username,\nOptions.Account.Password);\nrequest .KeepAlive = false;\nrequest .Timeout = 10000;\nrequest .UsePassive = !Options.Account.IsActive;\nusing (WebResponse response = request .GetResponse()) {\n...\n... FtpWebRequest request = (FtpWebRequest)WebRequest.Create(url);\n? .Proxy = Options.ProxySettings;\nrequest .Method = WebRequestMethods.Ftp.RemoveDirectory;\nrequest .Credentials = new NetworkCredential(Options.Account.Username,\nOptions.Account.Password);\nrequest .KeepAlive = false;\nrequest .GetResponse();\n...\n\u22b2 Similar protocols when using an object.\nSample 3\n...\nvar addMethod = @event.AddMethod;\nAssert.Equal(voidType, ? .ReturnType);\nAssert.True( addMethod .ReturnsVoid);\nAssert.Equal(1, addMethod .ParameterCount);\nAssert.Equal(eventType, addMethod .ParameterTypes.Single()); ...\n...\nvar removeMethod = @event.RemoveMethod;\nAssert.Equal(voidType, ? .ReturnType);\nAssert.True( removeMethod .ReturnsVoid);\nAssert.Equal(1, removeMethod .ParameterCount);\nAssert.Equal(eventType, removeMethod .ParameterTypes.Single()); ...\n\u22b2 These two placeholders have \u2014 by definition \u2014 identical representations.\nSample 4\n...\nint index = flpHotkeys.Controls.GetChildIndex(Selected);\nint newIndex;\nif ( ? == 0) {\nnewIndex = flpHotkeys.Controls.Count - 1; } else {\nnewIndex = index - 1; }\nflpHotkeys.Controls.SetChildIndex(Selected, newIndex);\nmanager.Hotkeys.Move( index , newIndex); ...\n... if (Selected != null && flpHotkeys.Controls.Count > 1) {\nint index = flpHotkeys.Controls.GetChildIndex(Selected);\nint newIndex;\nif ( ? == flpHotkeys.Controls.Count - 1) {\nnewIndex = 0; } else {\nnewIndex = index + 1; }\nflpHotkeys.Controls.SetChildIndex(Selected, newIndex);\nmanager.Hotkeys.Move( index , newIndex); ...\nSample 5\nint index = flpHotkeys.Controls.GetChildIndex(Selected);\nint newIndex ; if (index == 0) {\n? = flpHotkeys.Controls.Count - 1; } else {\nnewIndex = index - 1; }\nflpHotkeys.Controls.SetChildIndex(Selected, newIndex );\nmanager.Hotkeys.Move(index, newIndex );\nint index = flpHotkeys.Controls.GetChildIndex(Selected);\nint newIndex ; if (index == 0) {\nnewIndex = flpHotkeys.Controls.Count - 1; } else {\n? = index - 1; }\nflpHotkeys.Controls.SetChildIndex(Selected, newIndex );\nmanager.Hotkeys.Move(index, newIndex );\n\u22b2Because of the dataflow, these two placeholders (one in each branch of the if-else) have identical representations in the dataflow model, and have very similar representations in other models.\nSample 6\n_generatedCodeAnalysisFlagsOpt = generatedCodeAnalysisFlagsOpt;\n... context.RegisterCompilationStartAction(this.OnCompilationStart);\nif ( ? .HasValue) {\n// Configure analysis on generated code.\ncontext.ConfigureGeneratedCodeAnalysis( _generatedCodeAnalysisFlagsOpt .Value);\n} ...\n...\nvar symbolAndProjectId = await definition.TryRehydrateAsync(\n_solution, _cancellationToken).ConfigureAwait(false);\nif (! ? .HasValue) {\nreturn; }\nlock (_gate) {\n_definitionMap[definition] = symbolAndProjectId .Value;\n} ...\n\u22b2 Our model learns a similar representation for the placeholder between the locations where a Nullable variable is assigned and used, which corresponds to a check on the .HasValue property.\nSample 7\nvar analyzers = new DiagnosticAnalyzer[] { new ConcurrentAnalyzer(typeNames) }; var expected = new DiagnosticDescription[typeCount];\nfor (int i = 0; ? < typeCount; i ++) {\nvar typeName = $\"C{ i + 1}\";\nexpected[ i ] = Diagnostic(ConcurrentAnalyzer.Descriptor.Id, typeName) .WithArguments(typeName)\n.WithLocation( i + 2, 7); }\nvar builder = new StringBuilder(); var typeCount = 100; var typeNames = new string[typeCount];\nfor (int i = 1; ? <= typeCount; i ++) {\nvar typeName = $\"C{ i }\";\ntypeNames[ i - 1] = typeName; builder.Append($\"\\r\\nclass {typeName} {{ }}\");\n}\n\u22b2 The model learns \u2014 unsurprisingly\u2014 a very similar representation of the loop control variable i at the location of the bound check. Generalizing over varying loops.\nSample 8\n...\nif (! ? ) {\nif (disposeManagedResources) {\n_resizerControl.SizerModeChanged += new SizerModeEventHandler(resizerControl_SizerModeChanged); _resizerControl.Resized -= new EventHandler(resizerControl_Resized); _dragDropController.Dispose();\n}\n_disposed = true;\n} ...\n...\nif (! ? ) {\n_enableRealTimeWordCount = Settings.GetBoolean(SHOWWORDCOUNT, false);\n_enableRealTimeWordCountInit = true;\n} return _enableRealTimeWordCount; ...\n\u22b2 Similar representations for booleans that will be assigned to true within a branch.\nSample 9\n...\nSmartContentSelection selection = EditorContext.Selection as SmartContentSelection;\nif ( ? != null) {\nreturn selection .HTMLElement.sourceIndex == HTMLElement.sourceIndex; } else {\nreturn false; } ...\n... foreach (LiveClipboardFormat format in formats) {\nContentSourceInfo contentSource = FindContentSourceForLiveClipboardFormat(format);\nif ( ? != null)\nreturn contentSource ; } ...\n\u22b2 Representation for elements that will be returned but only one one path.\nSample 10\n...\nRectangle elementRect = ElementRectangle; _resizerControl.VirtualLocation = new Point(\nelementRect .X - ResizerControl.SIZERS_PADDING,\n? .Y - ResizerControl.SIZERS_PADDING); ...\n... Rectangle rect = CalculateElementRectangleRelativeToBody(HTMLElement); IHTMLElement body = (HTMLElement.document as IHTMLDocument2).body;\n_dragBufferControl.VirtualSize = new Size(body.offsetWidth, body.offsetHeight);\n_dragBufferControl.VirtualLocation = new Point(- rect .X, - ? .Y); _dragBufferControl.Visible = true; ...\n\u22b2 Protocol of accesses for Rectangle objects. After X has been accessed then the variable has the same representation (implied that Y is highly likely to be accessed next)\nSample 11\n... if ( parameters != null)\n{\nfor (int i = 0; i < parameters .Length; i += 2)\n{\nstring name = ? [i]; string val = parameters [i + 1];\nif (!cullMissingValues || (val != null && val != string.Empty)) Add(name, val);\n} } ...\n...\nstring[] refParams = value.Split(commaSeparator);\nif ( refParams .Length != 2 || string.IsNullOrEmpty( refParams [0])\n|| string.IsNullOrEmpty( refParams [1])) throw new ArgumentException(\"Reference path is invalid.\");\nModuleName = ? [0];\nResourceId = int.Parse( refParams [1]);\nreferencePath = value; ...\n\u22b2 Similar representation for first array access after bound checks.\nSample 12\n...\npublic static string GetHostName(string url ) {\nif (!IsUrl( ? )) return null;\nreturn new Uri( url ).Host; } ...\n...\npublic static bool IsUrlLinkable(string url ) {\nif (UrlHelper.IsUrl( ? )) {\nUri uri = new Uri( url ); foreach (string scheme in NonlinkableSchemes)\nif (uri.Scheme == scheme) return false;\n} return true;\n} ...\n\u22b2 Similar representations because of learned pattern when parsing URIs.\nSample 13\n... private void WriteEntry(string message, string category , string stackTrace)\n{ // Obtain the DateTime the message reached us. DateTime dateTime = DateTime.Now;\n// Default the message, as needed. if (message == null || message.Length == 0)\nmessage = \"[No Message]\";\n// Default the category, as needed. if ( category == null || category .Length == 0)\n? = \"None\";\nint seqNum = Interlocked.Increment(ref sequenceNumber);\nDebugLogEntry logEntry = new DebugLogEntry(facility, processId, seqNum,\ndateTime, message, category , stackTrace);\n...\n... private void WriteEntry(string message , string category, string stackTrace)\n{ // Obtain the DateTime the message reached us. DateTime dateTime = DateTime.Now;\n// Default the message, as needed. if ( message == null || message .Length == 0)\n? = \"[No Message]\";\n// Default the category, as needed. if (category == null || category.Length == 0)\ncategory = \"None\";\nint seqNum = Interlocked.Increment(ref sequenceNumber);\nDebugLogEntry logEntry = new DebugLogEntry(facility, processId, seqNum, dateTime, message , category, stackTrace);\n...\n\u22b2 The variables message and category (in the same snippet of code) have similar representations. This is a source of confusion for our models.\nSample 14\n...\nforeach (string file in files) {\nstring[] chunks = ? .Split(INTERNAL_EXTERNAL_SEPARATOR); switch (chunks[0]) {\n...\n... Uri uri = new Uri(url);\nforeach (string scheme in NonlinkableSchemes)\nif (uri.Scheme == ? ) return false;\n...\n\u22b2 Limited context (e.g. only declaration) causes variables to have similar usage representations. In the examples file and scheme are defined and used only once. This is a common source of confusion for our models."}, {"heading": "C Full Snippet Pasting Samples", "text": "Below we present some of the suggestions when using the full SMARTPASTE structured prediction. The variables shown at each placeholder correspond to the ground truth. Underlined tokens represent UNK tokens. The top three allocations are shown as well as the ground truth (if it is not in the top 3 suggestions). Red placeholders are the placeholders that need to be filled in when pasting. All other placeholders are marked in superscript next to the relevant variable.\nSample 1\n... charsLeft\u03bb1 = 0; while (p\u03bb2.IsRightOf(selection\u03bb3.Start)) {\ncharsLeft\u03bb4++;\np\u03bb5.MoveUnit(_MOVEUNIT_ACTION.MOVEUNIT_PREVCHAR); } ...\n\u03bb1 charsLeft: 87%, movesRight: 8%, p: 5%\n\u03bb2 p: 96%, selection: 4%, bounds: 1e-3\n\u03bb3 selection: 89%, bounds: 1%, p: 8e-3\n\u03bb4 movesRight: 66%, charsLeft: 16%, p: 1%\n\u03bb5 p: 83%, selection: 11%, bounds: 6%\nSample 2\n... HttpWebResponse response \u03bb0 = null; XmlDocument xmlDocument \u03bb1 = new XmlDocument(); try {\nusing (Blog blog \u03bb3 = new Blog(_blogId\u03bb4))\nresponse\u03bb5 = blog\u03bb6.SendAuthenticatedHttpRequest(notificationUrl\u03bb7, 10000);\n// parse the results\nxmlDocument\u03bb8.Load(response\u03bb9.GetResponseStream()); } catch (Exception) {\nthrow; } finally {\nif (response\u03bb10 != null)\nresponse\u03bb11.Close(); } ...\n\u03bb4 _hostBlogId: 12%, BlogId: 10%, _buttonId: 10%, _blogId: 1%\n\u03bb5 response: 86%, xmlDocument: 5%, notificationUrl: 3%\n\u03bb6 xmlDocument: 84%, blog: 12%, response: 2%\n\u03bb7 NotificationPollingTime: 95%, CONTENT_DISPLAY_SIZE: 2%, notificationUrl: 1%\n\u03bb8 xmlDocument: 100%, response: 9e-4, _buttonDescription: 4e-4\n\u03bb9 response: 65%, xmlDocument: 30%, _hostBlogId: 4%\n\u03bb10 response: 90%, _blogId: 3%, CurrentImage: 9e-3\n\u03bb11 response: 98%, _settingKey: 1%, xmlDocument: 9e-3\nSample 3\n... protected override void Dispose(bool disposing \u03bb1) {\nif (disposing\u03bb2) {\nif (components\u03bb3 != null)\ncomponents\u03bb4.Dispose(); }\nbase.Dispose(disposing\u03bb5); } ...\n\u03bb2 disposing: 100%, commandIdentifier: 4e-4, components: 1e-4\n\u03bb3 components: 100%, disposing: 3e-5, commandIdentifier: 2e-5\n\u03bb4 components: 100%, disposing: 9e-7, CommandIdentifier: 6e-9\n\u03bb5 disposing: 100%, components: 3e-5, CommandIdentifier: 2e-5\nSample 4\n... tmpRange \u03bb1.Start.MoveAdjacentToElement(startStopParent \u03bb2, _ELEMENT_ADJACENCY.ELEM_ADJ_BeforeBegin); if (tmpRange \u03bb3.IsEmptyOfContent()) {\ntmpRange \u03bb4.Start.MoveToPointer(selection \u03bb5.End); IHTMLElement endStopParent \u03bb6 = tmpRange \u03bb7.Start.GetParentElement(stopFilter \u03bb8);\nif (endStopParent\u03bb9 != null && startStopParent\u03bb10.sourceIndex == endStopParent\u03bb11.sourceIndex) {\ntmpRange\u03bb12.Start\n.MoveAdjacentToElement(endStopParent\u03bb13, _ELEMENT_ADJACENCY.ELEM_ADJ_BeforeEnd);\nif (tmpRange\u03bb14.IsEmptyOfContent()) {\ntmpRange\u03bb15.MoveToElement(endStopParent\u03bb16, true); if (maximumBounds\u03bb17.InRange(tmpRange\u03bb18) && !(endStopParent\u03bb19 is IHTMLTableCell)) {\ndeleteParentBlock\u03bb20 = true; }\n} }\n} ...\n\u03bb9 startStopParent: 97%, styleTagId: 1%, tmpRange: 1%, endStopParent: 3e-3\n\u03bb10 startStopParent: 100%, tmpRange: 2e-4, maximumBounds: 3e-5\n\u03bb11 startStopParent: 100%, styleTagId: 2e-3, endStopParent: 1e-3\n\u03bb12 tmpRange: 99%, selection: 9e-3, startStopParent: 2e-3\n\u03bb13 startStopParent: 96%, tmpRange: 2%, endStopParent: 1%\n\u03bb14 tmpRange: 98%, selection: 1%, maximumBounds: 1%\n\u03bb15 tmpRange: 98%, selection: 2%, maximumBounds: 4e-3\n\u03bb16 startStopParent: 43%, styleTagId: 29%, endStopParent: 21%\n\u03bb17 tmpRange: 70%, selection: 14%, maximumBounds: 8%\n\u03bb18 styleTagId: 84%, tmpRange: 5%, selection: 5%\n\u03bb19 startStopParent: 98%, endStopParent: 1%, styleTagId: 9e-3\n\u03bb20 deleteParentBlock: 90%, startStopParent: 4%, selection: 3%\nSample 5\n... public static void GetImageFormat(string srcFileName \u03bb1, out string extension \u03bb2, out ImageFormat imageFormat \u03bb3) {\nextension\u03bb4 = Path.GetExtension(srcFileName\u03bb5) .ToLower(CultureInfo.InvariantCulture); if (extension\u03bb6 == \".jpg\" || extension\u03bb7 == \".jpeg\") {\nimageFormat\u03bb8 = ImageFormat.Jpeg;\nextension\u03bb9 = \".jpg\"; } else if (extension\u03bb10 == \".gif\") {\nimageFormat\u03bb11 = ImageFormat.Gif; } else {\nimageFormat\u03bb12 = ImageFormat.Png;\nextension\u03bb13 = \".png\"; }\n} ...\n\u03bb4 extension: 64%, imageFormat: 36%, JPEG_QUALITY: 1e-4\n\u03bb5 extension: 98%, srcFileName: 1%, imageFormat: 1e-3\n\u03bb6 extension: 97%, imageFormat: 1%, srcFileName: 3e-4\n\u03bb7 extension: 75%, JPG: 4%, GIF: 4%\n\u03bb8 imageFormat: 100%, extension: 1e-5, JPEG_QUALITY: 2e-6\n\u03bb9 extension: 93%, imageFormat: 2%, JPEG: 9e-3\n\u03bb10 extension: 52%, imageFormat: 15%, ICO: 6%, JPG: 6%, GIF: 6%\n\u03bb11 imageFormat: 100%, extension: 4e-4, JPEG_QUALITY: 1e-5\n\u03bb12 imageFormat: 99%, JPEG_QUALITY: 4e-3, extension: 2e-3\n\u03bb13 extension: 66%, JPG: 6%, ICO: 6%, GIF: 6%, BMP: 6%\nSample 6\n... BitmapData destBitmapData \u03bb1 = scaledBitmap \u03bb2.LockBits(\nnew Rectangle(0, 0, destWidth \u03bb3, destHeight \u03bb4),\nImageLockMode.WriteOnly, scaledBitmap \u03bb5.PixelFormat); try {\nbyte* s0 \u03bb6 = (byte*)sourceBitmapData\u03bb7 .Scan0.ToPointer(); int sourceStride \u03bb8 = sourceBitmapData\u03bb9.Stride; byte* d0 \u03bb10 = (byte*)destBitmapData\u03bb11.Scan0.ToPointer(); int destStride \u03bb12 = destBitmapData\u03bb13.Stride;\nfor (int y \u03bb14 = 0; y \u03bb15 < destHeight \u03bb16; y \u03bb17++) {\nbyte* d \u03bb18 = d0 \u03bb19 + y \u03bb20 * destStride \u03bb21; byte* sRow \u03bb22 = s0 \u03bb23 + ((int)(y \u03bb24 * yRatio \u03bb25)\n+ yOffset \u03bb26) * sourceStride \u03bb27 + xOffset \u03bb28; ...\n\u03bb7 sourceBitmapData: 72%, destBitmapData: 28%, bitmap: 2e-6\n\u03bb9 sourceBitmapData: 90%, destBitmapData: 10%, bitmap: 1e-4\n\u03bb11 sourceBitmapData: 75%, destBitmapData: 25%, s0: 1e-5\n\u03bb13 sourceBitmapData: 83%, destBitmapData: 17%, bitmap: 3e-4\nSample 7\n... private static Stream GetStreamForUrl(string url \u03bb1, string pageUrl \u03bb2, IHTMLElement element \u03bb3) {\nif (UrlHelper.IsFileUrl(url \u03bb4)) {\nstring path \u03bb5 = new Uri(url\u03bb6).LocalPath; if (File.Exists(path\u03bb7)) {\nreturn File.OpenRead(path\u03bb8); } else {\nif (ApplicationDiagnostics.AutomationMode)\nTrace.WriteLine(\"File \" + url\u03bb9 + \" not found\"); else\nTrace.Fail(\"File \" + url\u03bb10 + \" not found\"); return null;\n} } else if (UrlHelper.IsUrlDownloadable(url \u03bb11)) {\nreturn HttpRequestHelper.SafeDownloadFile(url \u03bb12); } else {\n...\n\u03bb6 url: 96%, element: 2%, pageUrl: 1%\n\u03bb7 path: 86%, url: 14%, element: 1e-3\n\u03bb8 path: 99%, url: 1%, pageUrl: 4e-5\n\u03bb9 path: 97%, url: 2%, pagrUrl: 4e-3%\n\u03bb10 path: 67%, url: 24%, pageUrl: 5%\nSample 8\n... public static void ApplyAlphaShift(Bitmap bitmap \u03bb1, double alphaPercentage \u03bb2) {\nfor (int y \u03bb3 = 0; y \u03bb4 < bitmap \u03bb5.Height; y \u03bb6++) {\nfor (int x \u03bb7 = 0; x \u03bb8 < bitmap \u03bb9.Width; x \u03bb10++) {\nColor c \u03bb11 = bitmap.GetPixel(x \u03bb12, y \u03bb13); if (c\u03bb14.A > 0) //never make transparent pixels non-transparent {\nint newAlphaValue \u03bb15 = (int)(c\u03bb16.A * alphaPercentage\u03bb17); //value must be between 0 and 255 newAlphaValue\u03bb18 = Math.Max(0, Math.Min(255, newAlphaValue\u03bb19));\nbitmap\u03bb20.SetPixel(x\u03bb21, y\u03bb22, Color.FromArgb(newAlphaValue\u03bb23, c\u03bb24)); } else\nbitmap\u03bb25.SetPixel(x\u03bb26, y\u03bb27, c\u03bb28); }\n} } ...\n\u03bb14 alphaPercentage: 52%, bitmap: 32%, c: 13%\n\u03bb16 bitmap: 67%, alphaPercentage: 27%, c: 4%\n\u03bb17 alphaPercentage: 85%, c: 6%, JPEQ_QUALITY: 3%\n\u03bb18 newAlphaValue: 51%, bitmap: 24%, alphaPercentage: 11%\n\u03bb19 newAlphaValue: 86%, y: 4%, alphaPercentage: 3%\n\u03bb20 bitmap: 100%, c: 4e-3, JPEG_QUALITY: 3e-4\n\u03bb21 bitmap: 98%, x: 9e-2, c: 8e-3\n\u03bb22 c: 50%, bitmap: 49%, newAlphaValue: 2e-3, y: 3e-8\n\u03bb23 alphaPercentage: 42%, JPEG_QUALITY: 40%, bitmap: 10%, newAlphaValue: 3%\n\u03bb24 newAlphaValue: 60%, alphaPercentage: 25%, c: 5%\n\u03bb25 bitmap: 100%, c: 8e-4, alphaPercentage: 3e-4\n\u03bb26 bitmap: 88%, x: 9%, c: 2%\n\u03bb27 c: 79%, bitmap: 18%, JPEG_QUALITY: 1%, y: 3e-3\n\u03bb28 c: 82%, y: 6%, x: 4%\nSample 9\n... string s \u03bb1 = (string)Value\u03bb2; byte[] data \u03bb3;\nGuid g \u03bb4; if (s\u03bb5.Length == 0) {\ndata\u03bb6 = CollectionUtils.ArrayEmpty<byte>(); } else if (ConvertUtils.TryConvertGuid(s\u03bb7, out g\u03bb8)) {\ndata\u03bb9 = g\u03bb10.ToByteArray(); } else {\ndata\u03bb11 = Convert.FromBase64String(s\u03bb12); }\nSetToken(JsonToken.Bytes, data \u03bb13, false); return data \u03bb15; ...\n\u03bb2 t: 58%, Value: 12%, _tokenType: %\n\u03bb5 TokenType: 44%, QuoteChar: 43%, _currentPosition: 4%, s: 9e-3\n\u03bb6 _tokenType: 74%, data: 20%, _currentState: 5%\n\u03bb7 QuoteChar: 31%, ValueType: 26%, Path: 9%, s: 3e-4\n\u03bb8 g: 100%, data: 6e-5, t: 3e-5\n\u03bb9 data: 99%, _tokenType: 5e-3, ValueType: 9e-4\n\u03bb10 g: 99%, data: 6e-3, _currentState: 2e-3\n\u03bb11 data: 66%, _tokenType: 31%, _currentState: 6e-3\n\u03bb12 s: 74%, Value: 20%, t: 3%"}, {"heading": "D Dataset", "text": "The collected dataset and its characteristics are listed in Table 2."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Deep Neural Networks have been shown to succeed at a range of natural lan-<lb>guage tasks such as machine translation and text summarization. While tasks on<lb>source code (i.e., formal languages) have been considered recently, most work in<lb>this area does not attempt to capitalize on the unique opportunities offered by its<lb>known syntax and structure. In this work, we introduce SMARTPASTE, a first task<lb>that requires to use such information. The task is a variant of the program repair<lb>problem that requires to adapt a given (pasted) snippet of code to surrounding,<lb>existing source code. As first solutions, we design a set of deep neural models<lb>that learn to represent the context of each variable location and variable usage<lb>in a data flow-sensitive way. Our evaluation suggests that our models can learn<lb>to solve the SMARTPASTE task in many cases, achieving 58.6% accuracy, while<lb>learning meaningful representation of variable usages.", "creator": "LaTeX with hyperref package"}}}