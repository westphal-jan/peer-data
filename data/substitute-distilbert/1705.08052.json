{"id": "1705.08052", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Compressing Recurrent Neural Network with Tensor Train", "abstract": "recurrent neural network ( rnn ) are a popular domain for modeling temporal and sequential tasks and achieve many state - of - the - art tests on various complex problems. however, most of my state - of - the - situation rnns allow millions of dimensions and require huge computational resources for training into predicting new data. this paper proposes an alternative rnn model to reduce the number of parameters significantly faster representing the weight parameters based called tensor train ( tt ) format. in this paper, we implement the bt - format representation for several scheduling problems such such simple rnn and gated recurrent unit ( gru ). we compare and evaluate our proposed rnn model with uncompressed rnn model on sequence classification and sequence prediction tasks. our proposed rnns with tt - format are able to preserve the performance while reducing the risk of rnn parameters infinitely much to 40 times smaller.", "histories": [["v1", "Tue, 23 May 2017 01:14:22 GMT  (418kb,D)", "http://arxiv.org/abs/1705.08052v1", "Accepted at IJCNN 2017"]], "COMMENTS": "Accepted at IJCNN 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andros tjandra", "sakriani sakti", "satoshi nakamura"], "accepted": false, "id": "1705.08052"}, "pdf": {"name": "1705.08052.pdf", "metadata": {"source": "CRF", "title": "Compressing Recurrent Neural Network with Tensor Train", "authors": ["Andros Tjandra", "Sakriani Sakti", "Satoshi Nakamura"], "emails": ["andros.tjandra.ai6@is.naist.jp,", "ssakti@is.naist.jp,", "s-nakamura@is.naist.jp"], "sections": [{"heading": null, "text": "I. Introduction\nTemporal and sequential modeling are important subjects in machine learning. RNNs architecture has recently become a popular choice for modeling temporal and sequential tasks. Although RNNs have been researched for about two decades [1], [2], their recent resurgence reflects improvements in computer hardware and the growth of available datasets. Many state-of-the-arts in speech recognition [3], [4] and machine translation [5]\u2013[7] has been achieved by RNNs.\nHowever, most RNN models are computationally expensive and have a huge number of parameters. Since RNNs are constructed by multiple linear transformations followed by nonlinear transformations, we need multiple high-dimensional dense matrices as parameters. In time-steps, we need to apply multiple linear transformations between our dense matrix with high-dimensional input and previous hidden states. Especially for state-of-the-art models on speech recognition [4] and machine translation [5], such huge models can only be implemented in high-end cluster environments because they need massive computation power and millions of parameters. This limitation hinders the creation of efficient RNN models that are fast enough for massive real-time inference or small enough to be implemented in low-end devices like mobile phones [8] or embedded systems with limited memory.\nTo bridge the gap between high-performance state-of-the-art model with efficient computational and memory costs, there is a trade-off between high accuracy model and fast efficient model. A number of researchers have done notable work to minimize the accuracy loss and maximize the model efficiency. Hinton et al. [9] and Ba et al. [10] successfully compressed\na large deep neural network into a smaller neural network by training the latter on the transformed softmax outputs from the former. Distilling knowledge from larger neural networks has also been successfully applied to recurrent neural network architecture by [11]. Denil et al. [12] utilized low-rank matrix decomposition of the weight matrices. A recent study by Novikov et al. [13] replaced the dense weight matrices with Tensor Train (TT) format [14] inside convolutional neural network (CNN) model. With the TT-format, they significantly compress the number of parameters and kept the model accuracy degradation to a minimum. However, to the best of our knowledge, no study has focused on compressing more complex neural networks such as RNNs with tensor-based representation.\nIn this work, we propose TT-RNN, which is an RNN architecture based on TT-format. We apply TT-format to reformulate two different RNNs: a simple RNN and a GRU RNN. Our proposed RNN architectures are evaluated using two different tasks: sequence classification and sequence prediction. In Section II, we briefly review RNN. In Section III, we describe the details of our proposed TT-RNN architecture. In Section IV, we describe the tasks and datasets, followed by the experimental results. We present related works in Section V. Finally, we conclude our result in Section VI.\nII. Recurrent Neural Network"}, {"heading": "A. Simple Recurrent Neural Network", "text": "An RNN is a kind of neural network architecture that models sequential and temporal dependencies [15]. Typically, we define input sequence x = (x1, ..., xT ), hidden vector sequence h = (h1, ..., hT ) and output vector sequence y = (y1, ..., yT ). As illustrated in Fig. 1, a simple RNN at time t is can be formulated as:\nht = f (Wxhxt + Whhht\u22121 + bh) (1) yt = g(Whyht + by). (2)\nwhere Wxh represents the weight parameters between the input and hidden layer, Whh represents the weight parameters between the hidden and hidden layer, Why represents the weight parameters between the hidden and output layer, and bh and by represent bias vectors for the hidden and output layers. Functions f (\u00b7) and g(\u00b7) are nonlinear activation functions, such as sigmoid or tanh.\nar X\niv :1\n70 5.\n08 05\n2v 1\n[ cs\n.L G\n] 2\n3 M\nay 2\n01 7"}, {"heading": "B. Gated Recurrent Neural Network", "text": "Simple RNNs cannot easily be used for modeling datasets with long sequences and long-term dependency because the gradient can easily vanish or explode [16], [17]. This problem is caused by the effect of bounded activation functions and their derivatives. Therefore, training a simple RNN is more complicated than training a feedforward neural network. Some researches addressed the difficulties of training simple RNNs. For example, Le et al. [18] replaced the activation function that causes the vanishing gradient with a rectifier linear (ReLU) function. With an unbounded activation function and identity weight initialization, they optimized a simple RNN for long-term dependency modeling. Martens et al. [19] used a second-order Hessian-free (HF) optimization method rather than the first-order method such as gradient descent. However, estimation of the second-order gradient requires extra computational steps. Modifying the internal structure from RNN by introducing gating mechanism also helps RNNs solve the vanishing and exploding gradient problems. The additional gating layers control the information flow from the previous states and the current input [2]. Several versions of gated RNNs have been designed to overcome the weakness of simple RNNs by introducing gating units, such as Long-Short Term Memory (LSTM) RNN and GRU RNN. In the following subsections, we explain both in more detail.\n1) Long-Short Term Memory RNN: The LSTM RNN was proposed by Hochreiter et al. [2]. LSTM is a gated RNN with three gating layers and memory cells, utilizes the gating layers to control the current memory states by retaining the valuable information and forgetting the unneeded information. The memory cells store the internal information across time steps. As illustrated in Fig. 2, the LSTM hidden layer values at time t are defined by the following equations [20]:\nit = \u03c3(Wxixt + Whiht\u22121 + Wcict\u22121 + bi) ft = \u03c3(Wx f xt + Wh f ht\u22121 + Wc f ct\u22121 + b f ) ct = ft ct\u22121 + it tanh(Wxcxt + Whcht\u22121 + bc) ot = \u03c3(Wxoxt + Whoht\u22121 + Wcoct + bo) ht = ot tanh(ct)\nwhere \u03c3(\u00b7) is sigmoid activation function and it, ft, ot and ct are respectively the input gates, the forget gates, the output gates and the memory cells. The input gates retain the candidate\nmemory cell values that are useful for the current memory cell and the forget gates retain the previous memory cell values that are useful for the current memory cell. The output gates retain the memory cell values that are useful for the output and the next time-step hidden layer computation.\n2) Gated Recurrent Unit RNN: The GRU RNN was proposed by Cho et al. [21] as an alternative to LSTM. There are several key differences between GRU and LSTM. First, a GRU does not have memory cells [22]. Second, instead of three gating layers, it only has two: reset gates and update gates. As illustrated in Fig. 3, the GRU hidden layer at time t is defined by the following equations [21]:\nrt = \u03c3(Wxr xt + Whrht\u22121 + br) (3) zt = \u03c3(Wxzxt + Whzht\u22121 + bz) (4) h\u0303t = f (Wxhxt + Whh(rt ht\u22121) + bh) (5) ht = (1 \u2212 zt) ht\u22121 + zt h\u0303t (6)\nwhere \u03c3(\u00b7) is a sigmoid activation function, f (\u00b7) is a tanh activation function, rt, zt are the reset and update gates, h\u0303t is the candidate hidden layer values, and ht is the hidden layer values at time-t. The reset gates control the previous hidden layer values that are useful for the current candidate hidden layer. The update gates decide whether to keep the previous hidden layer values or replace the current hidden layer values with the candidate hidden layer values. GRU can match LSTM\u2019s performance and its convergence speed sometimes surpasses LSTM, despite having one fewer gating layer [22].\nIn this section, we provided the formulation and the details for several RNNs. As we can see, most of the RNNs consist of many dense matrices that represents a large number of weight parameters that are required to represent all of the RNN models. In the next section, we present an alternative RNN model that significantly reduces the number of parameters and simultaneously preserves the performance.\nIII. Proposed Tensor Train based RNN\nIn this section, we describe our proposed approach to compress RNN using Tensor Train (TT) format representation. We start with the description of Tensor Train [14] and then represent the linear transformation operation in the TT-format\n[13]. After that, we describe the details of our approach for TTRNN including a simple RNN and more sophisticated RNN with gating units. Applying the TT-format to represent the weight parameters in RNN presents more difficulties compared to the standard feedforward NN. To tackle this problem, we also propose a local initialization trick in the last subsection."}, {"heading": "A. Tensor Train (TT) format", "text": "Before defining Tensor Train (TT) format, we will explain the notations which we borrow from [13], [14] that will be used in later sections. In general cases, one-dimensional arrays are called vectors, two-dimensional arrays are called matrices, and all higher multidimensional arrays are commonly called tensors.\nWe represent vectors with lower case letters (e.g., b), matrices with upper case letters (e.g., W) and tensors with calligraphic upper case letters (e.g., W). Each element from the vectors, matrices and tensors is represented explicitly using indexing in every dimension. For example, b(i) is the i-th element from vector b, W(p, q) is the element of the p-th row and the q-th column from matrix W, W( j1, .., jd) is the element at index ( j1, .., jd) of tensor W and d is the order of tensorW. Based on previous description [13], we assume that d-dimensional array (tensor) W is represented in TT-format [14] if for each k \u2208 {1, .., d} and for each possible value of the k-th dimension index jk \u2208 {1, .., nk} there exists a matrix Gk[ jk] such that all elements of W can be computed as the following equation :\nW( j1, j2, .., jd\u22121, jd) = G1[ j1] \u00b7G2[ j2]...Gd\u22121[ jd\u22121] \u00b7Gd[ jd]. (7)\nFor all matrices Gk[ jk] related to the same dimension k, they must be represented with size rk\u22121 \u00d7 rk, where r0 and rd must be equal to 1 to retain the final matrix multiplication result as a scalar. In TT-format, we define a sequence of rank {rk}dk=0 and we call them TT-rank from tensorW. The set of matrices Gk = {Gk[ jk]}nkjk=1 where the matrices are spanned in the same index are called TT-core. We can describe Eq.7 in detail by\nenumerating the index qk\u22121 \u2208 {1, .., rk\u22121} and qk \u2208 {1, .., rk} in matrix Gk[ jk] across all k \u2208 {1, .., d}:\nW( j1, j2, .., jd\u22121, jd) =\u2211 q0,..,qd G1[ j1](q0, q1)..Gd[ jd](qd\u22121, qd). (8)\nBy factoring the original tensor W into multiple TTcores {Gk}dk=1, we can compress the number of elements needed to represent the original tensor size from \u220fd k=1 nk to\u2211d\nk=1 nkrk\u22121rk."}, {"heading": "B. Representing Linear Transformation using TT-format", "text": "Almost all of the parts of neural networks are composed of linear transformations:\ny = Wx + b, (9)\nwhere W \u2208 RM\u00d7N is the weight matrix and b \u2208 RM is the bias vector. In most cases, matrix W has many more parameters than bias b. Therefore, we can utilize the TT-format for optimizing our neural networks by replacing weight matrix W with tensor W in TT-format [13].\nWe represent the TT-format for matrix W \u2208 RM\u00d7N where M = \u220fd k=1 mk and N = \u220fd k=1 nk as tensor W by defining bijective functions fi : Z+ \u2192 Zd+ and f j : Z+ \u2192 Zd+. Function fi maps each row p \u2208 {1, ..,M} into fi(p) = [i1(p), .., id(p)] and f j map each column q \u2208 {1, ..,N} into f j(q) = [ j1(q), .., jd(q)]. After defining such bijective functions, we can access the value from matrix W(p, q) in tensorW with the index vectors generated by fi(p) and f j(q). We transform Eq.7 to represent matrix W in the TT-format:\nW(p, q) = W(fi(p), f j(q)) (10) = W ([i1(p), .., id(p)] , [ j1(q), .., jd(q)]) (11) = G1 [ i1(p), j1(q) ] ..Gd [ id(p), jd(q) ] (12)\nwhere for each k \u2208 {1, .., d}:\nGk[ik(p), jk(q)] \u2208 Rrk\u22121\u00d7rk\nik(p) \u2208 {1, ..,mk} jk(q) \u2208 {1, .., nk}.\nTo represent the linear transformation in Eq.9 with Eq.1012, we need to reshape the vector input x into tensor X and bias vector b into tensor B with order d to match our tensor\nW. The following equation calculates a similar operation with y(p) = W(p, :)x + b(p) where we map row index p to vector [i1(p), .., id(p)] and enumerate all possible mappings for all columns in matrix W:\nY (i1(p), .., id(p)) = \u2211\nj1,.., jd\nG1[i1(p), j1]..Gd[id(p), jd]\u00b7\nX ( j1, .., jd) + B (i1(p), .., id(p)) (13) We can control the shape of TT-cores {Gk}di=1 by choosing factor M as {mk}dk=1 and N as {nk}dk=1 as long as the number of factors is equal between M and N. We can also define TTrank {rk}dk=0 and treat them as a hyper-parameter. In general, if we use a smaller TT-rank, we will get more efficient models but this action restricts our model to learn more complex representation. If we use a larger TT-rank, we get more flexibility to express our weight parameters but we sacrifice model efficiency. Table I compares the forward and backward propagation times and the memory complexity between the fully connected layer and the TT-layer in Big-O notation [13]. We compare the fully connected layer with matrix W \u2208 RM\u00d7N versus the TT-layer with tensorW with TT-rank {rk}dk=0. In the table, m denotes max({mk}dk=1), and r denotes max({rk}dk=0)."}, {"heading": "C. Compressing Simple RNN with TT-format", "text": "We represent a simple RNN in TT-format and call this model TT-SRNN for the rest of this paper. From Section II-A, we focus our attention on two dense weight matrices: (Wxh,Whh). Previously, we defined Wxh \u2208 RM\u00d7N as inputto-hidden parameters and Whh \u2208 RM\u00d7M as hidden-to-hidden parameters.\nFirst, we factorize matrix shape M into \u220fd\nk=1 mk and N into\u220fd k=1 nk. Next, we determine TT-rank {rk}dk=0 for our model and substitute Wxh with tensor Wxh and Whh with tensor Whh. Tensor Wxh is represented by set of TT-cores {Gxhk }dk=1 where \u2200k \u2208 {1, .., d}, Gxhk \u2208 Rmk\u00d7nk\u00d7rk\u22121\u00d7rk , and tensor Whh is represented by set of TT-cores {Ghhk }dk=1 where \u2200k \u2208 {1, .., d}, Ghhk \u2208 Rmk\u00d7mk\u00d7rk\u22121\u00d7rk . We define bijective functions fxi and fhi to access row p from Wxh and Whh in the set of TT-cores. We rewrite our simple RNN formulation to calculate ht in Eq.1:\naxht (p) = \u2211\nj1,.., jd\nWxh(fxi (p), [ j1, .., jd]) \u00b7 Xt ( j1, .., jd) (14)\nahht (p) = \u2211\nj1,.., jd\nWhh(fhi (p), [ j1, .., jd]) \u00b7 Ht\u22121 ( j1, .., jd)(15)\naxht = [ axht (1), .., a xh t (M) ] (16)\nahht = [ ahht (1), .., a hh t (M) ] (17)\nht = f (axht + a hh t + bh), (18)\nwhere X is the tensor representation of input xt and Ht\u22121 is the tensor representation of previous hidden states ht\u22121."}, {"heading": "D. Compressing GRU RNN with TT-format", "text": "In this section, we apply TT-format to represent a gated RNN. Among several RNN architectures with gating mechanism, we choose GRU to be reformulated in TT-format because it has less complex formulation and similar performance as LSTM. We call this model TT-GRU for the rest of this paper. In Section II-B2, we focus on the following six dense weight matrices: (Wxr, Whr, Wxz, Whz, Wxh, and Whh). Weight matrices Wxr, Wxz, Wxh \u2208 RM\u00d7N are parameters for projecting the input layer to the reset gate, the update gate, the candidate hidden layer, and Whr, Whz, Whh \u2208 RM\u00d7M are respectively parameters for projecting previous hidden layer into the reset gate, the update gate and candidate hidden layer.\nWe factorize M = \u220fd k=1 mk, N = \u220fd\nk=1 nk and set TTrank as {rk}dk=0. All weight matrices (Wxr, Whr, Wxz, Whz, Wxh, Whh) are substituted with tensors (Wxr, Whr, Wxz, Whz, Wxh, Whh) in TT-format. Tensors Wxr, Wxz, Wxh are represented by a set of TT-cores ({Gxrk }dk=1, {G xz k }dk=1, {Gxhk }dk=1) where \u2200k \u2208 {1, .., d}, (Gxrk ,G xz k ,Gxhk \u2208 Rmk\u00d7nk\u00d7rk\u22121\u00d7rk ). Tensor Whr,Whz,Whh are represented by a set of TT-cores ({Ghrk }dk=1, {Ghzk }dk=1, {Ghhk }dk=1) where \u2200k \u2208 {1, .., d}, (Ghrk ,G hz k ,Ghhk \u2208 Rmk\u00d7mk\u00d7rk\u22121\u00d7rk ). We define bijective function fxi to access row p from Wxr,Wxz,Wxh and function fhi to access row p from Whr,Whz,Whh in the set of TT-cores. We rewrite the GRU formulation to calculate rt in Eq.3:\naxrt (p) = \u2211\nj1,.., jd\nWxr(fxi (p), [ j1, .., jd]) \u00b7 Xt ( j1, .., jd)\nahrt (p) = \u2211\nj1,.., jd\nWhr(fhi (p), [ j1, .., jd]) \u00b7 Ht\u22121 ( j1, .., jd)\naxrt = [ axrt (1), .., a xr t (M) ] ahrt = [ ahrt (1), .., a hr t (M)\n] rt = \u03c3(axrt + a hr t + br). (19)\nNext, we rewrite the GRU formulation to calculate zt in Eq.4:\naxzt (p) = \u2211\nj1,.., jd\nWxz(fxi (p), [ j1, .., jd]) \u00b7 Xt ( j1, .., jd)\nahzt (p) = \u2211\nj1,.., jd\nWhz(fhi (p), [ j1, .., jd]) \u00b7 Ht\u22121 ( j1, .., jd)\naxzt = [ axzt (1), .., a xz t (M) ] ahzt = [ ahzt (1), .., a hz t (M)\n] zt = \u03c3(axzt + a hz t + bz). (20)\nFinally, we rewrite the GRU formulation to calculate h\u0303t in\nEq.5: axht (p) = \u2211\nj1,.., jd\nWxh(fxi (p), [ j1, .., jd]) \u00b7 Xt ( j1, .., jd)\nahht (p) = \u2211\nj1,.., jd\nWhh(fhi (p), [ j1, .., jd]) \u00b7\n(Rt ( j1, .., jd) \u00b7 Ht\u22121 ( j1, .., jd)) axht = [ axht (1), .., a xh t (M) ] ahht = [ ahht (1), .., a hh t (M)\n] h\u0303t = f (axht + a hh t + bh). (21)\nAfter rt, zt and h\u0303t are calculated, we calculate ht on Eq.6 with standard operations like element-wise sum and multiplication.\nIn practice, we could assign a different d for each weight tensor as long as the input data dimension can also be factorized into the d values. We could also put different TT-rank for each tensor and treat them as our model hyper-parameter. However, to simplify our implementation we use the same TT-rank for both the input and hidden projection weight tensors. We also use the same factorizations M = \u220fd k=1 mk\nand N = \u220fd\nk=1 nk for all weight tensors in TT-SRNN and TTGRU.\nWe do not substitute bias vector b into tensor B because the number of bias parameters is insignificant compared to the number of parameters in matrix W. In terms of performance, the element-wise sum operation for bias vector b is also insignificant compared to the matrix multiplication between a weight matrix and the input layer or the previous hidden layer.\nE. Initialization for TT-cores Parameters\nWeight initialization is one critical detail for training deep neural networks. Especially for our RNN with TT-format that has many mini-tensors and several multiplications, the TTRNN will have a longer matrix multiplication chain than a standard RNN, and the hidden layer value will quickly saturate [23]. Therefore, we need to carefully choose the initialization method to help our proposed model start in a stable condition. In our implementation, we follow Glorot initialization [23] to keep the same variance of weights gradients across layers and time-steps to avoid the vanishing gradient problem. We initialize all the TT-cores as follows:\n\u2200k \u2208 {1, .., d}, Gk \u223c N(0, \u03c3k),\nwhere \u03c3k =\n\u221a 2\n(nk \u00b7 rk) + (mk \u00b7 rk\u22121)\nBy choosing a good initialization, our neural network will converge faster and obtain better local minima. Based on our preliminary experiments, we get better starting loss at the first several epochs compared to the randomly initialized model with the same \u03c3k on Gaussian distribution for all TT-cores.\nIV. Experiments\nIn this section, we evaluate our proposed RNN model with TT-formats (TT-SRNN and TT-GRU) and compare them to\nbaseline RNNs (a simple RNN and GRU). We conducted the experiments on sequence classification tasks, where each input sequence was assigned a single class, and sequence prediction tasks, where we predicted the next time-step based on previous information [24]. We used MNIST dataset for the sequence classification task and polyphonic music datasets for the sequence prediction task. For both tasks, we used local Glorot initialization trick from Section III-E for all the TT-cores weight parameters on the TT-SRNN and TT-GRU models. We used Adam algorithm [25] to optimize our model parameters.\nFor reports on both tasks, we simplified the model description as follows: RNN-HF where F denotes the number of hidden units (e.g., RNN-H256 means RNN with 256 hidden units) and TT-SRNN-HF-R where denotes the TT-rank (e.g., TT-SRNN-H10x10-R3 means TT-SRNN with hidden units 10x10 in TT-format and TT-rank 3). We used a grid search to determine the best number of hidden layer units for both models and the shape of TT-format based on the validation set performance."}, {"heading": "A. Sequence Classification on Sequential MNIST", "text": "We evaluated our proposed model TT-SRNN and TT-GRU for classification task using the MNIST dataset [18]. The MNIST dataset consists of 28 x 28 grayscale images from ten classes (digits 0-9). The MNIST dataset has a training set with 50000 images, a development set with 10000 images, and a test set with 10000 images. We have three different ways to represent the MNIST dataset in our experiments.\nFor the first experiment, we fed each row starting at the top row and ending at the bottom row, which means we fed a vector with 28 values at each time-step and a total of 28 time-steps to represent an image. We used the latest hidden layer activation as our image representation and put a softmax layer to classify the digits. This task\u2019s difficulty is medium for a simple RNN and an easy task for gated RNN. Our baseline models consists of RNN and GRU with 256 hidden units. For our proposed model, we use TT-SRNN and TT-GRU with 10 \u00d7 10 shapes and ranks (3, 5). For all the models, we use a projection layer with 32 hidden units before we feed the input to our RNN. The projection layer is used to embed the pixel representation into richer feature representation. We show the result on Table II. We repeated all of the experiments five times with different weight parameters initializations. Both the baseline and proposed models converged with good accuracy in several epochs and we achieved similar accuracy with a compression rate up to 80 times.\nIn our second experiment, we fed each pixel starting at the top left corner and ending at the bottom right corner, which means we fed a pixel at one time-step and in total we needed 784 time-steps to represent an image. This task is very challenging even for an RNN with gating mechanism because the RNN needs to model very long sequences [18]. As in the first task, we fed the softmax layer using the latest hidden layer values. For this very long-dependency task, we only benchmarked the gated RNN variants (GRU and TT-\nGRU). For our proposed model, we used TT-GRU with output shapes (10, 10) and three different TT-ranks (3, 5, 7). For all the models, we use a projection layer with 32 hidden units before we feed the input to our RNN. Fig. 5 compares the validation set cost for each epoch. We can observe that the TT-GRU able to converge as fast as the baseline GRU model. In table III, our proposed model matched the baseline model with TT-rank 5 and reduced the parameters 43 times smaller compared to the baseline model.\nIn the last experiment, we used the most difficult task [18] to push the limits of the gated RNN model. We shuffled the MNIST pixel-by-pixel and applied the same shuffled index to all the samples, and fed them one-by-one in a similar way as in the previous experiment. For the baseline and proposed model, we used the same configuration as in the previous experiment. Fig. 6 compares the validation set cost for each epoch. In Table IV, we show that our proposed models was able to match the baseline models with TT-rank 5 and reduced the RNN parameters to 43 times smaller."}, {"heading": "B. Sequence Prediction on Polyphonic Music", "text": "For the sequential modeling tasks, we used four polyphonic music datasets [26]: Piano-midi.de, Nottingham, MuseData, and JSB Chorales. All of these datasets have 88 binary values per time-step, and each consists of at least seven hours of polyphonic music. Our baseline models are a simple RNN with 512 hidden units and a GRU RNN with 512 hidden units. Our proposed models are TT-SRNN and TT-GRU with 8\u00d74\u00d78\u00d74 output shapes and TT-ranks (3, 5). Before we fed our input into the RNN, we projected them using hidden layer with 256 hidden units. In the polyphonic modeling task, we measured two different metrics: negative log-likelihood (NLL) and accuracy (ACC). To calculate the accuracy, we followed the evaluation metric proposed by [27] where ACC = T P/(T P + FP + FN). We only used true positive (TP), false positive (FP), false negative (FN) and ignored the true negative (TN) because most of the notes were turned off in the dataset. Table V lists all of the results of our experiments on the baseline and proposed models. We repeat all experiments five times with different weight parameters initialization.\nThe table shows that all of these models have similar performances based on the negative log-likelihood and the accuracy in the test set. Our proposed models was able to reduce the number of parameter with significant compression ratio and preserved the performance at the same time.\nV. RelatedWork\nCompressing parameters on neural network architecture has become an interesting topic over the past several years due to the increased complexity of neural networks. The number of parameters and processing times has also grown tremendously\nalong with their performance. A number of researchers comes up with many different ways to tackle this problem.\nBa et al. [10] and Hinton et al. [9] \u201cdistilled\u201d the knowledge from a deep neural network into a shallow neural network. First, they trained a state-of-the-art model with a deep and complex neural network using the original dataset and hard label as the target. After that, they reused the trained deep neural network by extracting output from the softmax layer and used them as the output target for a shallow neural network. By training the shallow network with a soft target, they achieved a better performance than the model trained using hard target labels. Recently, Tang et al. [11] utilized a similar approach for training RNN with a trained DNN. However, they had to train two different neural networks and built different structures to transfer the knowledge from bigger models.\nFrom the probabilistic perspective, Graves et al. [28] proposed a variational inference method for learning the mean and variance of Gaussian distribution for each weight parameter. They reformulated the variational inference as the optimization of a Minimum Description Length [29]. By modeling each weight parameter, they learned the importance of each weight in regard to the model. After the training process was finished, they pruned the parameters by removing the weight that has a high probability to be zero. However, they still needed large matrix multiplication and represented their model in dense weight matrix, and thus the algorithmic and memory complexity remained the same as in the original model.\nAnother approach to tackle the compression problem by a technical perspective is to limit the precision for weight parameters. Gupta et al. [30] and Courbariaux et al. [31] minimized the performance loss while using fewer bits (e.g., 16 bits) to represent floating points. Courbariaux et al. [32] proposed BinaryConnect to constrain the weight possible values to -1 or +1. Most of these ideas can be easily applied with our proposed model since several deep-learning frameworks have built-in low-precision floating point options [33], [34].\nModel compression using low-rank matrix has also been reported [12], [35]. Both of these works showed that many weight parameters are significantly redundant, and by representing them as low-rank matrices, they reduced the number of parameters with only a small drop in accuracy. Recently, Lu et al. [36] used low-rank matrix ideas to reduce the number of parameters in an RNN. Novikov et al. [13] utilized TT-format to represent weight matrices on feedforward neural networks.\nFrom their empirical evaluation on DNN-based architecture, the feedforward layer represented by the TT-format has a far better compression ratio and smaller accuracy loss compared to the low-rank matrix approach.\nTo the best of our knowledge, there are only a few research about compression on RNN models, and none of these works have utilized tensor-based format to represent the weight matrices for RNN models. In this work, we presented an RNN model by using TT-format weight to re-parameterize the weight matrices. We also compared the performance to standard uncompressed RNNs with a greater number of parameters. We expect our model could minimize the number of parameters and preserved the performance simultaneously.\nVI. Conclusion\nIn this paper, we presented an efficient and compact RNN model using TT-format representation. Using TT-format, we represented dense weight matrices inside the RNN layer with multiple low-rank tensors. Our proposed TT-SRNN and TTGRU significantly compressed the number of parameters while simultaneously retaining the model performance and accuracy. We evaluated our model with sequence classification and sequence prediction tasks. On sequence classification, with very long dependency tasks, our proposed RNNs model reduced the RNN parameters up to 40 times smaller compared to the original models without losing any accuracy. On the sequence prediction task, we evaluated our model with multiple music datasets, and our proposed RNNs reduced the RNN parameters up to 80 times smaller while preserving the performance.\nAcknowledgment\nPart of this work was supported by Microsoft CORE 10 Project as well as JSPS KAKENHI Grant Numbers 24240032 and 26870371.\nReferences\n[1] J. L. Elman, \u201cFinding structure in time,\u201d Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990. [2] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997. [3] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., \u201cDeep speech: Scaling up end-to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014. [4] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos et al., \u201cDeep speech 2: End-to-end speech recognition in English and Mandarin,\u201d arXiv preprint arXiv:1512.02595, 2015.\n[5] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey et al., \u201cGoogle\u2019s neural machine translation system: Bridging the gap between human and machine translation,\u201d arXiv preprint arXiv:1609.08144, 2016. [6] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014. [7] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in Advances in neural information processing systems, 2014, pp. 3104\u20133112. [8] M. Schuster, \u201cSpeech recognition for mobile devices at Google,\u201d in Pacific Rim International Conference on Artificial Intelligence. Springer, 2010, pp. 8\u201310. [9] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d arXiv preprint arXiv:1503.02531, 2015. [10] J. Ba and R. Caruana, \u201cDo deep nets really need to be deep?\u201d in Advances in neural information processing systems, 2014, pp. 2654\u2013 2662. [11] Z. Tang, D. Wang, and Z. Zhang, \u201cRecurrent neural network training with dark knowledge transfer,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5900\u20135904. [12] M. Denil, B. Shakibi, L. Dinh, N. de Freitas et al., \u201cPredicting parameters in deep learning,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 2148\u20132156. [13] A. Novikov, D. Podoprikhin, A. Osokin, and D. P. Vetrov, \u201cTensorizing neural networks,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 442\u2013450. [14] I. V. Oseledets, \u201cTensor-train decomposition,\u201d SIAM Journal on Scientific Computing, vol. 33, no. 5, pp. 2295\u20132317, 2011. [Online]. Available: http://dx.doi.org/10.1137/090752286 [15] A. Graves, A.-r. Mohamed, and G. Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649. [16] Y. Bengio, P. Simard, and P. Frasconi, \u201cLearning long-term dependencies with gradient descent is difficult,\u201d Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 157\u2013166, 1994. [17] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber, \u201cGradient flow in recurrent nets: the difficulty of learning long-term dependencies,\u201d 2001. [18] Q. V. Le, N. Jaitly, and G. E. Hinton, \u201cA simple way to initialize recurrent networks of rectified linear units,\u201d arXiv preprint arXiv:1504.00941, 2015. [19] J. Martens and I. Sutskever, \u201cLearning recurrent neural networks with Hessian-free optimization,\u201d in Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1033\u20131040. [20] A. Graves, N. Jaitly, and A.-r. Mohamed, \u201cHybrid speech recognition with deep bidirectional LSTM,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278. [21] K. Cho, B. Van Merrie\u0308nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using RNN encoder-decoder for statistical machine translation,\u201d arXiv preprint arXiv:1406.1078, 2014. [22] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d arXiv preprint arXiv:1412.3555, 2014. [23] X. Glorot and Y. Bengio, \u201cUnderstanding the difficulty of training deep feedforward neural networks,\u201d in In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics, 2010. [24] A. Graves et al., Supervised sequence labelling with recurrent neural networks. Springer, 2012, vol. 385. [25] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014. [26] N. Boulanger-lewandowski, Y. Bengio, and P. Vincent, \u201cModeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription,\u201d in Proceedings of the 29th International Conference on Machine Learning (ICML-12), J. Langford and J. Pineau, Eds. New York, NY, USA: ACM, 2012, pp. 1159\u20131166. [Online]. Available: http://icml.cc/2012/papers/590.pdf [27] M. Bay, A. F. Ehmann, and J. S. Downie, \u201cEvaluation of multiplef0 estimation and tracking systems.\u201d in 2009 International Society for Music Information Retrieval Conference (ISMIR), 2009, pp. 315\u2013320. [28] A. Graves, \u201cPractical variational inference for neural networks,\u201d in Advances in Neural Information Processing Systems, 2011, pp. 2348\u2013 2356. [29] G. E. Hinton and D. Van Camp, \u201cKeeping the neural networks simple by minimizing the description length of the weights,\u201d in Proceedings of the sixth annual conference on Computational learning theory. ACM, 1993, pp. 5\u201313. [30] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, \u201cDeep learning with limited numerical precision,\u201d in Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, 2015, pp. 1737\u20131746. [31] M. Courbariaux, J.-P. David, and Y. Bengio, \u201cTraining deep neural networks with low precision multiplications,\u201d arXiv preprint arXiv:1412.7024, 2014. [32] M. Courbariaux, Y. Bengio, and J.-P. David, \u201cBinaryConnect: Training deep neural networks with binary weights during propagations,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 3123\u2013 3131. [33] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane\u0301, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vie\u0301gas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, \u201cTensorFlow: Large-scale machine learning on heterogeneous systems,\u201d 2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/ [34] Theano Development Team, \u201cTheano: A Python framework for fast computation of mathematical expressions,\u201d arXiv e-prints, vol. abs/1605.02688, May 2016. [Online]. Available: http://arxiv.org/abs/ 1605.02688 [35] T. N. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhadran, \u201cLow-rank matrix factorization for deep neural network training with high-dimensional output targets,\u201d in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6655\u20136659. [36] Z. Lu, V. Sindhwani, and T. N. Sainath, \u201cLearning compact recurrent neural networks,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5960\u20135964."}], "references": [{"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech 2: End-to-end speech recognition in English and Mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi", "W. Macherey", "M. Krikun", "Y. Cao", "Q. Gao", "K. Macherey"], "venue": "arXiv preprint arXiv:1609.08144, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition for mobile devices at Google", "author": ["M. Schuster"], "venue": "Pacific Rim International Conference on Artificial Intelligence. Springer, 2010, pp. 8\u201310.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Do deep nets really need to be deep?", "author": ["J. Ba", "R. Caruana"], "venue": "Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Recurrent neural network training with dark knowledge transfer", "author": ["Z. Tang", "D. Wang", "Z. Zhang"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5900\u20135904.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2148\u20132156.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D.P. Vetrov"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 442\u2013450.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Tensor-train decomposition", "author": ["I.V. Oseledets"], "venue": "SIAM Journal on Scientific Computing, vol. 33, no. 5, pp. 2295\u20132317, 2011. [Online]. Available: http://dx.doi.org/10.1137/090752286", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "2001.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning recurrent neural networks with Hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1033\u20131040.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A.-r. Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["A. Graves"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-lewandowski", "Y. Bengio", "P. Vincent"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12), J. Langford and J. Pineau, Eds. New York, NY, USA: ACM, 2012, pp. 1159\u20131166. [Online]. Available: http://icml.cc/2012/papers/590.pdf", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluation of multiplef0 estimation and tracking systems.", "author": ["M. Bay", "A.F. Ehmann", "J.S. Downie"], "venue": "in 2009 International Society for Music Information Retrieval Conference (ISMIR),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Practical variational inference for neural networks", "author": ["A. Graves"], "venue": "Advances in Neural Information Processing Systems, 2011, pp. 2348\u2013 2356.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["G.E. Hinton", "D. Van Camp"], "venue": "Proceedings of the sixth annual conference on Computational learning theory. ACM, 1993, pp. 5\u201313.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1993}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, 2015, pp. 1737\u20131746.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Training deep neural networks with low precision multiplications", "author": ["M. Courbariaux", "J.-P. David", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.7024, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "BinaryConnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 3123\u2013 3131.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, vol. abs/1605.02688, May 2016. [Online]. Available: http://arxiv.org/abs/ 1605.02688", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6655\u20136659.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning compact recurrent neural networks", "author": ["Z. Lu", "V. Sindhwani", "T.N. Sainath"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5960\u20135964.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Although RNNs have been researched for about two decades [1], [2], their recent resurgence reflects improvements in computer hardware and the growth of available datasets.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "Although RNNs have been researched for about two decades [1], [2], their recent resurgence reflects improvements in computer hardware and the growth of available datasets.", "startOffset": 62, "endOffset": 65}, {"referenceID": 2, "context": "Many state-of-the-arts in speech recognition [3], [4] and machine translation [5]\u2013[7] has been achieved by RNNs.", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "Many state-of-the-arts in speech recognition [3], [4] and machine translation [5]\u2013[7] has been achieved by RNNs.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "Many state-of-the-arts in speech recognition [3], [4] and machine translation [5]\u2013[7] has been achieved by RNNs.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Many state-of-the-arts in speech recognition [3], [4] and machine translation [5]\u2013[7] has been achieved by RNNs.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "Especially for state-of-the-art models on speech recognition [4] and machine translation [5], such huge models can only be implemented in high-end cluster environments because they need massive computation power and millions of parameters.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Especially for state-of-the-art models on speech recognition [4] and machine translation [5], such huge models can only be implemented in high-end cluster environments because they need massive computation power and millions of parameters.", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "This limitation hinders the creation of efficient RNN models that are fast enough for massive real-time inference or small enough to be implemented in low-end devices like mobile phones [8] or embedded systems with limited memory.", "startOffset": 186, "endOffset": 189}, {"referenceID": 8, "context": "[9] and Ba et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] successfully compressed a large deep neural network into a smaller neural network by training the latter on the transformed softmax outputs from the former.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Distilling knowledge from larger neural networks has also been successfully applied to recurrent neural network architecture by [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "[12] utilized low-rank matrix decomposition of the weight matrices.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] replaced the dense weight matrices with Tensor Train (TT) format [14] inside convolutional neural network (CNN) model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13] replaced the dense weight matrices with Tensor Train (TT) format [14] inside convolutional neural network (CNN) model.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "An RNN is a kind of neural network architecture that models sequential and temporal dependencies [15].", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "Simple RNNs cannot easily be used for modeling datasets with long sequences and long-term dependency because the gradient can easily vanish or explode [16], [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Simple RNNs cannot easily be used for modeling datasets with long sequences and long-term dependency because the gradient can easily vanish or explode [16], [17].", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "[18] replaced the activation function that causes the vanishing gradient with a rectifier linear (ReLU) function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] used a second-order Hessian-free (HF) optimization method rather than the first-order method such as gradient descent.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The additional gating layers control the information flow from the previous states and the current input [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "2, the LSTM hidden layer values at time t are defined by the following equations [20]:", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "[21] as an alternative to LSTM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "First, a GRU does not have memory cells [22].", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "3, the GRU hidden layer at time t is defined by the following equations [21]:", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "GRU can match LSTM\u2019s performance and its convergence speed sometimes surpasses LSTM, despite having one fewer gating layer [22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "We start with the description of Tensor Train [14] and then represent the linear transformation operation in the TT-format", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Before defining Tensor Train (TT) format, we will explain the notations which we borrow from [13], [14] that will be used in later sections.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Before defining Tensor Train (TT) format, we will explain the notations which we borrow from [13], [14] that will be used in later sections.", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "Based on previous description [13], we assume that d-dimensional array (tensor) W is represented in TT-format [14] if for each k \u2208 {1, .", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "Based on previous description [13], we assume that d-dimensional array (tensor) W is represented in TT-format [14] if for each k \u2208 {1, .", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "Therefore, we can utilize the TT-format for optimizing our neural networks by replacing weight matrix W with tensor W in TT-format [13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "Table I compares the forward and backward propagation times and the memory complexity between the fully connected layer and the TT-layer in Big-O notation [13].", "startOffset": 155, "endOffset": 159}, {"referenceID": 22, "context": "Especially for our RNN with TT-format that has many mini-tensors and several multiplications, the TTRNN will have a longer matrix multiplication chain than a standard RNN, and the hidden layer value will quickly saturate [23].", "startOffset": 221, "endOffset": 225}, {"referenceID": 22, "context": "In our implementation, we follow Glorot initialization [23] to keep the same variance of weights gradients across layers and time-steps to avoid the vanishing gradient problem.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "We conducted the experiments on sequence classification tasks, where each input sequence was assigned a single class, and sequence prediction tasks, where we predicted the next time-step based on previous information [24].", "startOffset": 217, "endOffset": 221}, {"referenceID": 24, "context": "We used Adam algorithm [25] to optimize our model parameters.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "We evaluated our proposed model TT-SRNN and TT-GRU for classification task using the MNIST dataset [18].", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "This task is very challenging even for an RNN with gating mechanism because the RNN needs to model very long sequences [18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "In the last experiment, we used the most difficult task [18] to push the limits of the gated RNN model.", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "For the sequential modeling tasks, we used four polyphonic music datasets [26]: Piano-midi.", "startOffset": 74, "endOffset": 78}, {"referenceID": 26, "context": "To calculate the accuracy, we followed the evaluation metric proposed by [27] where ACC = T P/(T P + FP + FN).", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "[10] and Hinton et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] \u201cdistilled\u201d the knowledge from a deep neural network into a shallow neural network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] utilized a similar approach for training RNN with a trained DNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] proposed a variational inference method for learning the mean and variance of Gaussian distribution for each weight parameter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "They reformulated the variational inference as the optimization of a Minimum Description Length [29].", "startOffset": 96, "endOffset": 100}, {"referenceID": 29, "context": "[30] and Courbariaux et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] minimized the performance loss while using fewer bits (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] proposed BinaryConnect to constrain the weight possible values to -1 or +1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Most of these ideas can be easily applied with our proposed model since several deep-learning frameworks have built-in low-precision floating point options [33], [34].", "startOffset": 156, "endOffset": 160}, {"referenceID": 33, "context": "Most of these ideas can be easily applied with our proposed model since several deep-learning frameworks have built-in low-precision floating point options [33], [34].", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "Model compression using low-rank matrix has also been reported [12], [35].", "startOffset": 63, "endOffset": 67}, {"referenceID": 34, "context": "Model compression using low-rank matrix has also been reported [12], [35].", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "[36] used low-rank matrix ideas to reduce the number of parameters in an RNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] utilized TT-format to represent weight matrices on feedforward neural networks.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Recurrent Neural Network (RNN) are a popular choice for modeling temporal and sequential tasks and achieve many state-of-the-art performance on various complex problems. However, most of the state-of-the-art RNNs have millions of parameters and require many computational resources for training and predicting new data. This paper proposes an alternative RNN model to reduce the number of parameters significantly by representing the weight parameters based on Tensor Train (TT) format. In this paper, we implement the TT-format representation for several RNN architectures such as simple RNN and Gated Recurrent Unit (GRU). We compare and evaluate our proposed RNN model with uncompressed RNN model on sequence classification and sequence prediction tasks. Our proposed RNNs with TT-format are able to preserve the performance while reducing the number of RNN parameters significantly up to 40 times smaller.", "creator": "LaTeX with hyperref package"}}}