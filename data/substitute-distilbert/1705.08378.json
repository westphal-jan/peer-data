{"id": "1705.08378", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Detecting Adversarial Examples in Deep Networks with Adaptive Noise Reduction", "abstract": "deep reinforcement patterns ( dnns ) play the key role in many applications. unsurprisingly, they also became a potential attack vectors of adversaries. some efforts have demonstrated dnn classifiers can be fooled by the adversarial example, which is formed via introducing some perturbations beyond an original sample. accordingly, some powerful defense techniques were proposed against adversarial examples. however, existing retaliation techniques require modifying the target accurately or depend on outdated prior knowledge of attack techniques to realistic degrees. in this paper, we propose a straightforward method for detecting adversarial image examples. it doesn't require relevant prior knowledge of attack techniques and can spontaneously directly deployed into unmodified off - the - shelf dnn models. specifically, we consider the coupling to images as a kind of noise and creates two classical template processing structures, scalar detection and smoothing spatial filter, to reduce its effect. the resulting two - dimensional entropy inequality employed as a metric let explain various adaptive noise reduction for different frames of images. as a result, the adversarial example can be effectively detected by comparing the classification results using a given sample and its denoised version. versions of adversarial examples against some state - of - the - art dnn models are used to evaluate the proposed method, which are crafted with different attack techniques. the experiment shows exactly our detection method theoretically obtain an overall recall of 93. 73 % beyond an overall precision of 95. 45 % without referring to any prior knowledge of attack techniques.", "histories": [["v1", "Tue, 23 May 2017 15:50:32 GMT  (1440kb,D)", "https://arxiv.org/abs/1705.08378v1", null], ["v2", "Mon, 19 Jun 2017 02:28:53 GMT  (1440kb,D)", "http://arxiv.org/abs/1705.08378v2", null], ["v3", "Tue, 20 Jun 2017 01:15:17 GMT  (1440kb,D)", "http://arxiv.org/abs/1705.08378v3", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["bin liang", "hongcheng li", "miaoqiang su", "xirong li", "wenchang shi", "xiaofeng wang"], "accepted": false, "id": "1705.08378"}, "pdf": {"name": "1705.08378.pdf", "metadata": {"source": "META", "title": "Detecting Adversarial Examples in Deep Networks with Adaptive Noise Reduction", "authors": ["Bin Liang", "Hongcheng Li", "Miaoqiang Su", "Xirong Li", "Wenchang Shi", "Xiaofeng Wang"], "emails": ["liangb@ruc.edu.cn", "owenlee@ruc.edu.cn", "sumiaoqiang@ruc.edu.cn", "xirong@ruc.edu.cn", "wenchang@ruc.edu.cn", "xw7@indiana.edu"], "sections": [{"heading": null, "text": "KEYWORDS Adversarial Examples, Deep Neural Network, Detection"}, {"heading": "1 INTRODUCTION", "text": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67]. DNNs have exhibited very impressive performance in these tasks, especially in the image classification [60]. Some DNN-based classifiers achieved even higher performance than human [58, 59]. Meanwhile, their robustness has also raised concerns.\nSome recent studies [26, 49, 61] demonstrate that DNN-based image classifiers can be fooled by adversarial examples, which are well-crafted to cause a trained model to misclassify the instances it is given. As shown in Figure 1, an adversarial image can be generated by adding some imperceptible perturbations into a given\nimage [26]. Consequently, a famous DNN classifier GoogLeNet [60] will misclassify the resultant image, while a human observer can still correctly classify it and without noticing the existence of the introduced perturbations. These studies demonstrate that the adversaries could potentially use the crafted image to inflict serious damages. As shown in [52], a stop sign, after being crafted, will be incorrectly classified as a yield sign. As a result, a self-driving car equipped with the DNN classifier may behave dangerously.\nSome techniques have been proposed to defend adversarial examples in DNNs [26, 34, 53, 54]. Most of them require modifying the target classifier model. For example, the adversarial training is a straightforward defense technique which uses as many adversarial samples as possible during training process as a kind of regularization [26, 34, 53]. This can make it harder for attackers to generate new adversarial examples. Papernot et al.s [54] introduced a defense technique named defensive distillation to adversarial sample. Two networks were trained as a distillation, where the first network produced probability vectors to label the original dataset, while the other was trained using the newly labeled dataset. As a result, the effectiveness of adversarial examples can be substantially reduced. Several very recent studies [20, 27, 29, 65] focus on detecting adversarial examples directly. Similarly, these techniques also require modifying the model or acquiring sufficient adversarial examples, such as training new sub-models [20], retraining a revised model as a detector using known adversarial examples [29], performing a statistical test on a large group of adversarial and benign examples [27], or training the key detection parameter using a number of adversarial examples and their corresponding benign ones [65].\nUnfortunately, retraining an existing model or changing its architecture will introduce expensive training cost. Generating appropriate adversarial examples for training or statistical testing is also of high cost and depends on a very comprehensive prior knowledge of various potential adversarial techniques. Even worse, the attacker\nar X\niv :1\n70 5.\n08 37\n8v 3\n[ cs\n.C R\n] 2\n0 Ju\nn 20\n17\ncan craft adversarial examples with the technique unknown to the defender. In this case, the adversarial example has a good chance to evade the classification. Moreover, training a classifier with an emerging attack technique would take some time. There always is a window for attackers to craft effectual adversarial examples. Furthermore, most of existing defense techniques are model-specific. To apply a defense technique to different models, they need to be rebuilt or retrained individually. The security enhancement to a model cannot be directly applied to other ones.\nTo address the aforementioned challenges, we present in this paper a new technique capable of effectively capturing adversarial examples, even in the absence of prior knowledge about potential attacks.\nOur approach is based upon the observation that to make the adversarial change imperceptible, the perturbation incurred by the adversarial examples typically need to be confined within a small range. This is important, since otherwise, the example will be easily identified by human. Consequently, the information introduced by the perturbations should also be less than that of the original image. In the proposed method, the perturbation is regarded as a kind of noise and the noise reduction techniques are leveraged to reduce its adversarial effect. If the effect is downgraded properly, the denoised adversarial example will be classified as a new class that is different with the adversarial target. On the other hand, for the legitimate sample, the same denoising operation will most likely just slightly changes the image\u2019s semantics, keeping it still within its original category. Intuitively, all the adversarial perturbation is added later on to the image and therefore tends to less tolerant of the noise reduction process than the original image information. The information remaining in a denoised benign sample can be still enough for the classifier to correctly identify its class. In fact, some studies [2, 25, 40] have shown that the state-of-the-art classifier is invariant to different input transformations, such as translation, rotation, scale and etc. To this end, the adversarial example can be effectively detected by inspecting whether the classification of a sample is changed after it is denoised.\nTwo classical image processing techniques, scalar quantization and smoothing spatial filter, are leveraged to reduce the effect of perturbations. However, it is obviously inappropriate to denoise all samples in the same way. The quantization or smoothing suitable for a high-resolution image sample may be too excessive for a lowresolution one. To improve the generality of our method, an adaptive noise reduction is enforced by utilizing the two-dimensional (2-D) entropy of the sample. Specifically, as illustrated in Figure 2, the key component of our detection method is a filter. When feeding a sample f (x, y) to the target classifier, it will be denoised by the filter to generate a filtered sample f \u2019(x, y). The sample is first quantized with an appropriate interval size, which is determined by computing the 2-D entropy of the sample. We also use the entropy to decide whether the quantized sample needs to be smoothed. Only when the entropy is larger than a threshold, will it be smoothed by a spatial smoothing filter. As demonstrated in Section 4.1, introducing the 2-D entropy can essentially improve the generality and performance of the proposed method. Finally, if the denoised version of a sample is classified as a different class to the original sample, it is identified as an adversarial example.\nWeemploy some state-of-the-art DNNmodels and popular datasets, such as GoogLeNet [60], CaffeNet [32], ImageNet [31] andMNIST [4] to evaluate the effectiveness of the proposed method. Three upto-date attack techniques, i.e., FGSM [26], DeepFool [49], and CW attacks [13], are used to craft adversarial examples. In total, there are 9,162 effectual adversarial examples generated against the models. The experiment shows that the proposed method can achieve an overall recall of 93.73% and an overall precision of 95.47% for detecting the adversarial examples.\nIn summary, our three main contributions are the following.\n\u2022 We model the perturbation of the DNN adversarial samples as image noise and introduce classical image processing techniques to reduce its effect. This allows us to effectively detect adversarial samples without prior knowledge of attack techniques. \u2022 We employ the 2-D entropy to automatically adjust the detection strategy for a specific sample. This makes the proposed method capable of detecting different kinds of adversarial examples without requiring tuning its parameters, and can be directly integrated into unmodified target models. \u2022 Using state-of-the-art DNN models, we demonstrate that the proposed method can effectively detect the adversarial examples generated by different attack techniques with a high recall and precision1.\nThe rest of the paper is organized as follows. In Section 2, we present some essential background knowledge, including a brief introduction to deep neural networks and three up-to-date attack techniques which are used in our evaluation. In Section 3, the proposedmethod is described at length. In Section 4, we evaluate the effectiveness of our method via detecting the adversarial examples crafted by three attack techniques. Some potential problems and limitations are discussed in Section 5. We review the related work in Section 6 and conclude in Section 7.\n1The source code of our detection method, along with the experiment data, is all available at https://github.com/OwenSec/DeepDetector."}, {"heading": "2 BACKGROUND", "text": "In this section, we provide some preliminaries on DNNs and the attack techniques used to craft adversarial examples."}, {"heading": "2.1 Deep Neural Networks", "text": "As illustrated in Figure 3, a DNN consists of a succession of neural layers. Each neural layer serves as a parametric function to model the new representation obtained from the previous layer. Gradually from the low layers to the high layers, the network can efficiently realize feature extractions. A weight vector, indicating the activation of each neuron, is assigned for each neural layer. It is updated during training phase with the backpropagation algorithm. Generally, the features imported into the low layers are the raw data describing the basic original properties of the problem instances. After multiple layers abstraction, the features extracted from high layers possess more semantic information of the input.\nAccording to the type of output expected from the network, DNN can be fallen into twomain categories: supervised learning and unsupervised learning. The former is mainly used for classification. The network is trained with labeled dataset to learn some connections between inputs and outputs [15, 17, 23, 36]. The latter is often used for feature extraction [48] and network pre-training [19], which is trained with unlabeled dataset. In this paper, we focus on the DNNs used as classifiers. As shown in Figure 3, the DNN classifier outputs a vector p indicating the predication confidence of each predefined class j (j\u22081...n). The target of the attackers is to make the network output an incorrect predication for the input provided by them."}, {"heading": "2.2 Crafting Adversarial Example", "text": "Szegedy et al. [61] first made the intriguing discovery that various machine learning models, including DNNs [36, 39], are vulnerable to adversarial samples. In general, for a given sample x and a trained model C, the attacker aims to craft an adversarial example x\u2217 = x + \u2206x by adding a perturbation \u2206x to x, such that C(x*) , C(x).\nIn most of the cases, the attacker wants the target model misclassify the resultant image, while a human observer can still correctly classify it and without noticing the existence of the introduced perturbation. In practice, the adversarial examples can be\ngenerated straightforwardly [26] or with an optimization procedure [13, 49, 61]. In this paper, we choose the following three upto-date attack techniques to perform detection experiments. They can produce imperceptible perturbations.\nFast Gradient Sign Method. Goodfellow et al. [26] proposed a straightforward strategy named fast gradient sign method (FGSM) to craft adversarial samples against GoogLeNet [60]. The method is easy to implement and can compute adversarial perturbations very efficiently. Let c be the true class of x and J (C, x, c) be the cost function used to train the DNN C. The perturbation is computed as the sign of the model\u2019s cost function gradient, i.e.\n\u2206x = \u03b5 si\u0434n(\u25bdx J (C, x, c)) (1)\nwhere \u03b5 (range from 0.0 to 1.0) is set to be small enough to make \u2206x undetectable. Choosing a small \u03b5 can produce a well-disguised adversarial example. The change to the original image is difficult to be spotted by a human. As shown in Figure 1, using a very small \u03b5 (1/255) can also get a valid adversarial example. For a human observer, the difference from the original image is insensible. On the contrary, a large \u03b5 is likely to introduce noticeable perturbations but can get more adversarial examples when the original images are simple (e.g., handwritten digits).\nIn the classical FGSM algorithm, all input pixels are applied either a positive or negative change in the same degree according to the direction (sign) of corresponding cost gradients. However, as illustrated in Figure 4, we found that only manipulating the 30,000 (19.92%) input pixels with the highest positive or negative gradient magnitude can also generate an effectual adversarial sample using the same \u03b5 . The result implies that we can\u2019t assume the perturbation follows some kind of distribution.\nDeepFool. Moosavi-Dezfooli et al. [49] devised the DeepFool algorithm to find very small perturbations that are sufficient to change the classification result. The original image x0 is manipulated iteratively. At each iteration of the algorithm, the perturbation vector for xi that reaches the decision boundary is computed, and the current estimate is updated. The algorithm stops until the predicted class of xi changes. DeepFool is implemented as an optimization procedure which can yield a good approximation of the minimal\nperturbation. Moosavi-Dezfooli et al. performed some attack experiments against several DNN image classifiers, such as CaffeNet [32] and GoogLeNet [60], and so on. The experiments demonstrated that DeepFool can lead to a smaller perturbation, but which is still effective to trick the target models.\nCW Attacks. Carlini and Wagner [13] also employed an optimization algorithm to seek as small as possible perturbations. Three powerful attacks (CW attacks for short) are designed for the L0, L2, and L\u221e distance metrics. Using some public datasets, such as MNIST [4] and ImageNet [31], Carlini and Wagner trained some deep network models to evaluate their attack methods. As demonstrated in [13], CW attacks can find closer adversarial examples than the other attack techniques and never fail to find an adversarial example. For example, CW L0 and L2 attacks can find adversarial examples at least 2 times lower distortion than FGSM. Besides, Carlini and Wagner also illustrated their attacks can effectively break the defensive distillation [54]."}, {"heading": "3 METHODOLOGY", "text": ""}, {"heading": "3.1 Overview", "text": "The basic idea behind our method is to regard the perturbation as a kind of noise and introduce image processing techniques to reduce its adversarial effect as far as possible.\nGenerally, as described in Section 2, an adversarial sample is crafted by superimposing some perturbations on the original image. In this sense, the perturbation introduced in the adversarial sample is an additive noise item \u03b7(x, y), and the adversarial sample can be considered as a degraded image g(x, y) of the original image f (x, y) as follows. g(x, y) = f(x, y) + \u03b7(x, y) (2) where x and y are spatial coordinates, and f, g and\u03b7 are the functions mapping a pixel of coordinates (x, y) to its intensity.\nFor example, the perturbation of an FGSM adversarial sample is actually a random additive noise whose amplitude is \u03b5 . In fact, it is the noise that makes the sample misclassified. Ideally, if we can reconstruct the original image f (x, y) from an adversarial sample g(x, y), adversarial samples can be detected immediately. However, it is very difficult, if not impossible, to achieve this due to lack of the necessary knowledge about the noise term \u03b7(x, y). Instead, we seek to reconstruct the original image in the sense of classification. Namely, we want to convert g(x, y) to a new image f \u2019(x, y) such that its predicted class C(f \u2019(x, y)) is the same as C(f (x, y))\nNaturally, we hope that the classifier can correctly identify a benign sample after the conversion. If so, the adversarial example can be effectively detected by checking whether the classification of a sample is changed. If the classification is changed, the sample will be identified as a potential adversarial sample. Otherwise, it is considered benign. Fortunately, the state-of-the-art DNN image classifiers can tolerate a certain degree of distortion, although they are weak when facing adversarial samples. Goodfellow et al. [25] found that the features learned by deep networks are invariant to different input transformations, such as translation, rotation, scale and etc. LeCun et al. [2, 40] also demonstrated LeNet-5 classifier is robust to translation, scale, rotation, squeezing, and stroke width. Take the image shown in Figure 5(a) as an example, it is classified as Zebra by GoogLeNet with 99.97% confidence. We get several\nprocessed samples with some classical image processing methods, including graying, resizing, compressing and blurring. We can see that the obtained samples are still correctly classified with high confidences as shown in Figure 5(b) \u223c (e) respectively.\nAs mentioned in Section 1, the noise reduction techniques are leveraged to reduce the effect of the perturbation. Based on the above discussion, we have reasons to believe that although some details of interest in the sample may be removed too, the classifiers can output a correct classification for a denoised image. In image processing, there are a number of noise reduction techniques. Some of them are based on the prior knowledge of the noise. For example, Lee filtering [42], a very effective algorithm to filter noise. Nonetheless, this algorithm requires prior knowledge about the noise such as the underlying distribution, which is unavailable in our context. Besides, as demonstrated in Figure 4, the perturbations can be a kind of completely random noise, there is not a predictable distribution about them. For this reason, two straightforward techniques that require no prior knowledge, namely scalar quantization and smoothing spatial filter, are adopted to detect adversarial examples.\nFor scalar quantization, the size of intervals is a key parameter. In principle, using large intervals can more effectively reduce the effect of the perturbation but introduce more distortions at the same time, and the \"business\" of an image is damaged more heavily. This may result in a misclassification for a quantized benign sample, and produces a false positive. On the contrary, a small step may bring a number of false negatives due to inadequate noise reduction.\nWe utilize the entropy of image to determine the parameter. The image entropy is a quantity which is used to measure the amount of information possessed by an image. Commonly, the higher the entropy of an image is, the richer its semantics often is. Consequently, for an image with higher entropy, more information is required for the classifier to correctly identify its class. Based on the intuition, to avoid excessively eliminating the information of a sample, a small interval size will be applied to the high-entropy samples when quantizing them. Accordingly, the low-entropy samples will be assigned with a large interval size.\nSmoothing a sample will blur its details and often decrease its information. However, for a very simple image (with a low entropy), e.g., a handwritten digit, the smoothing may excessively eliminate its details, which are important to the classification task. Namely, the low-entropy image can\u2019t tolerate the blurring well from the perspective of the classification. To this end, we use the entropy to decide whether the sample needs to be smoothed."}, {"heading": "3.2 Computing Entropy", "text": "The conventional image entropy (1-D entropy) only concerns the concentration of the pixel values distribution. In order to catch the spatial correlation among the pixels, we employ two-dimensional entropy (2-D entropy) to measure the information of an image.\nWithout loss of generality, for an M \u00d7 N image with 256 pixel levels (0\u223c255), the average pixel value of the neighborhood is first calculated for each pixel. In this study, we adopt the averaging filter mask shown in Figure 10 to calculate the average pixel value of the neighborhood. This forms a pair (i, j), the pixel value i and the average of the neighborhood j. The frequency of the pair is denoted as f i, j, and a joint probability mass function pi, j is calculated as equation (3). On the basis, the 2-D entropy of the image can be computed as equation (4).\npi, j = fi, j / (M \u00d7 N) i, j = 0, 1, ..., 255 (3)\nH2D = 255\u2211 i=0 255\u2211 j=0 pi, jlo\u04342(pi, j) (4)\nFor a RGB color image, its 2-D entropy is the average of the 2-D entropies of its three color planes, which are computed individually."}, {"heading": "3.3 Scalar Quantization", "text": "Quantization is the process of representing a large (possibly infinite) set of values with a smaller (finite) one, e.g., mapping the real numbers to the integers. In image processing, quantization is often employed as a lossy compression technique by mapping a range of pixel intensities to a single representing one. In other words, reducing the number of colors of an image to cut its file size.\nScalar quantization is the most practical and straightforward approach to quantize an image. In scalar quantization, all inputs within a specified interval are mapped to a common value (called codeword), and the inputs in a different interval will be mapped to a different codeword. There are two types of scalar quantization techniques, uniform quantization and non-uniform quantization [24]. In uniform quantization, the input will be separated into the same size intervals, and in non-uniform quantization they are usually of different sizes chosen with an optimization algorithm to minimize the distortion [24]. In practice, we can set the interval size according to the probability density function (PDF) of pixel values. The intervals for frequent pixel values can be set smaller, and larger for infrequent pixel values. Figure 6 illustrates the examples of the two kinds of quantization.\nImages are meant to be viewed by the human and the human eyes can tolerate some distortions, such as the color reduction introduced by the lossy compression. In practice, the state-of-the-art DNN-based image classifiers are trained and classify samples from the view of human observers. Accordingly, these trained classifiers can also tolerate the color reduction to some extent. Namely, for a\nbenign sample, its classification is likely to be preserved for its quantized version. As shown in Figure 7, GoogLeNet can still correctly classify the scalar quantized samples with high confidences.\nMore importantly, the quantization technique cannot only be leveraged to compress the size of an image but also to reduce the noise in it. For an adversarial sample g(x, y) generated from f (x, y), the change to pixel values brought by the perturbation can be blurred with an appropriate quantization. As a result, the classification result of its quantized version, C(g\u2019(x, y)), is likely to be reverted to the original classification C(f (x, y)) and different with C(g(x, y)). We believe that the quantization technique can be leveraged to find potential adversarial samples by inspecting whether the classification result of a sample is changed after being quantized.\nIn practice, the perturbationmay distribute in all pixel values. For example, almost all pixels are added a perturbation in an adversarial image generated by the FGSM algorithm. If we adopt non-uniform quantization and choose a small interval for frequent pixel values, the effect of the perturbation in corresponding pixels may not be effectively reduced. Besides, finding appropriate non-uniform interval sizes will require more complex computation. To effectively downgrade the effect of the perturbation and achieve better performance, in this study, we adopt the uniform quantization technique to handle the sample.\nTo develop a scalar quantization, we first need to determine an appropriate interval size. For a given sample, an adaptive interval size will be applied to it according to its 2-D entropy computed as described in Section 3.2. As shown in Table 1, we determine the corresponding denoising strategies for different the 2-D entropies with a small-scale empirical study of different types of images\nTable 1: The denoising strategies for different 2-D entropies.\n2-D entropy Quantization Intervals Smoothing? >9.50 6 YES\n8.50 \u223c 9.50 4 NO <8.50 2 NO\nTable 2: The codebook of the 6-interval scalar quantization.\nin some popular datasets. They are determined by analyzing the relationship between the entropy of samples and the classification of their denoised versions in different denoising settings. Note that the empirical study only concerns the benign images that are easily available from many sources. As demonstrated in Section 4, the current strategy setting works well. In the future, we can perform a large-scale analysis to seek a possible better setting.\nThe image with a high 2-D entropy (larger than 9.50) often contains rich details, such as a photograph of an animal. According to the suggestion of Safe RGB Colors [24], we separate each color plane (R, G and B) into six intervals and set the step to 50. The colors representing a quantized sample are limited in 216 (63) safe RGB colors. All pixel values in an interval will be quantized to its left value. Our scalar quantization and codebook are illustrated in Figure 8 and Table 2. The quantization will perform the same quantization on the three color planes of a given sample respectively. As illustrated in Figure 9, after quantizing, the adversarial sample shown in Figure 1 is correctly classified as Panda by GoogLeNet with 98.92% confidence; and the benign sample is still classified as Panda with 99.71% confidence.\nFor the image with a low 2-D entropy (less than 8.50), such as a handwritten digit, it will be handled by an aggressive quantization\nwith only two intervals of the same size. The intensities within an interval are also mapped to its left value, i.e., 0 or 128. Other images are quantized with four intervals in the same way.\nA preliminary experiment on detecting FGSM adversarial samples shows that directly using the proposed quantization as the detection filter can achieve an average recall of 84.01% and an average precision of 84.99% for detecting adversarial samples. In essence, scalar quantization is a kind of point operation. Subsequently, we further reduce perturbation by introducing the neighborhood operation technique to achieve better detection performance."}, {"heading": "3.4 Spatial Smoothing Filter", "text": "The spatial smoothing filter is one of the most classical techniques for noise reduction. The idea behind it is to modify the value of the pixels in an image based on a local neighborhood of the pixels. As a result, the sharp transitions in pixel intensities, often brought by noise, are reduced in the target image. In linear smoothing filtering, the filtered image f \u2019(x, y) is the convolution of the original image f (x, y) with a filter mask w(x, y) as follows.\nf \u2032(x, y) = (w \u2217 f )(x, y) = a\u2211\ns=-a b\u2211 t=-b w (s, t) f (x \u2212 s, y \u2212 t) (5)\nThe filter mask determines the smoothing effect. Figure 10 presents a simple 5 \u00d7 5 averaging filter mask. With the mask, the intensity of a pixel is replaced with the standard average of the intensities of the pixels in its 5 \u00d7 5 neighborhood. After filtering, the target image is blurred and small details are removed from it. However, from the viewpoint of image classification, the objects of interest may be highlighted and easy to detect. In fact, to some extent, the state-of-the-art classifier does \"like\" the modification introduced by smoothing filtering. As shown in Figure 11, although the smoothed image is blurred by the filter, it is still correctly classified as Cab by GoogLeNet and surprisingly with a higher confidence (95.36%) than the original image (69.16%). However, as mentioned above, for a low-resolution image, the smoothing may be too excessive to preserve enough semantics information. We also use the 2-D entropy to determine whether the smoothing should be performed. As listed in Table 1, only the image whose 2-D entropy is larger than 9.50 is smoothed after being quantized.\nIn theory, adopting a filter mask with larger size will reduce noise more effectively but also blur the edges more heavily. However, the edges are often the desirable features for identifying an object of interest. As a tradeoff, we adopt a 5 \u00d7 5 filter mask to further reduce noise (perturbation) in quantized samples. Besides, in practice, some features of interest can be emphasized by giving more importance (weight) to some pixels in the mask at the expense of others [24]. For example, we can give bigger weights to the pixels at the center of the mask to reduce blurring in the smoothing process. In this study,\nwe believe the vertical and horizontal edges are most fundamental for identifying an object and adopt an aggressive way to preserve them. As shown in Figure 12, in the proposed filter, the pixel at the center and its vertical and horizontal neighbors are weighted by 1 and all others are weighted by 0.\nBy applying the smoothing filter to the quantized samples, some false positives and false negatives can be pruned. As illustrated in Figure 13, an adversarial sample crafted with FGSM is still misclassified as Flatworm with 79.62% confidence by GoogLeNet even after being quantized. However, its smoothed version is correctly classified as Zebra with 85.38% confidence. As a result, the adversarial sample can be detected successfully and a false negative will be avoided. On the other hand, as shown in Figure 14, a quantized begin sample is misclassified as Golden Fish and resulting in a false positive. Similarly, we can restore its correct classification Pineapple by using the smoothing filter."}, {"heading": "3.5 Detection Filter", "text": "Unfortunately, the smoothing technique may bring an excessive blurring to some samples and produce new false positives. To this end, we design a combination filter based on the two above techniques rather than simply concatenating them together.\nAs discussed in Section 2, the attacker often wants the perturbation introduced in the adversarial sample as small as possible to make it imperceptible. In other words, the perturbation to the pixel intensity is often limited in a small range. If the intensity of a pixel is blurred too much by the smoothing filter, the smoothing might be unnecessary. Based on the above intuition, our detection filter is defined by the following equation\nf \u2019(x, y) ={ fSQ (x, y), if | fSQ (x, y) \u2212 f(x, y) |\u2264| fSQ\u2212SF (x, y) \u2212 f(x, y) | fSQ\u2212SF (x, y), else\n(6) where f SQ (x, y) is the quantized original image and f SQ\u2212SF (x, y) is the smoothed quantized image. For a given pixel (a, b) of the input sample f (x, y), the output pixel value f \u2019(a, b) will be replaced with its quantization f SQ (a, b) when the distance between the quantization f SQ (a, b) and the original pixel value f (a, b) is smaller than the one between f SQ\u2212SF (a, b) and f (a, b); otherwise, it will be set to f SQ\u2212SF (a, b). As illustrated in Figure 15, there is a benign sample h(x, y) correctly identified as Pineapple (98.48%) by GoogLeNet. The non-optimized denoised version hSQ\u2212SF (x, y) is misclassified as Bee with a low but the highest confidence (8.81%) in prediction vector. If it is the ultimate output of our filter, a false positive will be produced. According to equation (6), the optimized denoised version h\u2019(x, y) is classified as Pineapple (88.70%) and the false positive is avoided.\nThe proposed method is transparent to the target model. In practice, the detection filter can be directly integrated with any off-the-shelf model as a sample preprocessor. The target model can be kept unchanged."}, {"heading": "4 EVALUATION", "text": "We evaluate the effectiveness of our method by applying it to detect adversarial examples crafted by the attack techniques described in Section 2.2. The recall rate and the precision rate are used to quantity the detection performance, which are defined as follows\nRecall = TP\nTP + FN (7)\nPrecision = TP\nTP + FP (8)\nwhere TP is the number of correctly detected adversarial examples (true positives), FN the number of adversarial samples that survive from our detection (false negatives), and FP the number of benign images that are detected as adversarial examples (false positives). The higher recall and precision rates indicate the better detection performance."}, {"heading": "4.1 Detecting FGSM Examples", "text": "Two off-the-shelf DNN models are employed to explore the effectiveness of the proposed detection method with respect to the FGSM attack. One is a GoogLeNet model trained with the ImageNet dataset [31], which has been taken as the attack target of FGSM in [26]. The other is a DNN model trained with the MNIST dataset, which is from an adversarial machine learning library [51] and trained for testing the FGSM attack. ImageNet [31] is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories, while MNIST [4] is a small-scalar dataset of simple gray handwritten digits.\nWe randomly choose four classes (Zebra, Panda, Cab, and Pineapple) of images from ImageNet to craft FGSM adversarial examples. For high-resolution images in ImageNet, we can use a small \u03b5 to craft effectual adversarial examples. In the experiment, \u03b5 is set to 1/255, this means the pixel value is manipulated by adding or decreasing 1. In total, 1,301 effectual adversarial examples are crafted. To get enough experiment samples for MNIST images, we use a comparatively large \u03b5(0.10) and craft 3,435 effectual adversarial examples from the 10,000 images in the MNIST test set.\nThe detection test set consists of all generated adversarial examples and their original images. As summarized in Table 3, the proposed method achieves an average recall of 90.32% with an average precision of 90.66% in detecting the 2,602 ImageNet samples; and for the 6,870 MNIST samples, the recall and precision rate are 90.95% and 97.81% respectively. Note that, the detection performance is obtained without using any prior knowledge about adversarial examples.\nThere is an obvious difference between the 2-D entropies of ImageNet and MNIST images. The ImageNet sample often has a high entropy, while that of the MNIST sample is generally lower. As mentioned above, we use the 2-D entropy to provide an adaptive noise filtering for a given sample. According the 2D-entropies of samples, in the detection experiment, the number of quantization intervals is automatically set to six or four for the most of ImageNet samples, and two or four for MNIST samples.\nTo demonstrate the effectiveness of introducing the 2-D entropy, we conduct an experiment to detect the samples with fixed parameters. The number of quantization intervals is deliberately set to two for all ImageNet samples, and six for MNIST samples. This results in an unacceptable detection performance. The average precision rate for ImageNet samples is dropped to 56.10%, i.e., almost all benign samples are incorrectly identified as adversarial. For MNIST samples, a great number of false negatives are produced such that the recall rate is downgraded to 64.13%. Figure 16 provides two examples. As shown in the first row, a benign ImageNet sample is incorrectly identified as an adversarial example when applying a 2-interval quantization to it. However, with a 6-interval quantization fitting in with its 2-D entropy (13.13), the false positive can be eliminated. Similarly, the MNIST adversarial example shown in the second row is missed when choosing an inappropriate interval size but can be detected with an adaptive setting."}, {"heading": "4.2 Detecting DeepFool Examples", "text": "Moosavi-Dezfooli et al. used two state-of-the-art CaffeNet and GoogLeNet models trained with ImageNet dataset to test their DeepFool attack, and the two models are available in [1].\nWe still choose the same four classes of images (i.e., Zebra, Panda, Cab, and Pineapple) to generate adversarial examples. By using the DeepFool algorithm provided in [1], 1,234 effectual adversarial examples are generated for the CaffeNet model and 1,032 for the GoogLeNet model. The generated examples and the corresponding original images make up our detection test set. The detection results of these samples are listed in Table 4. An average recall of 95.62% and an average precision of 91.12% is achieved for the samples targeting CaffeNet; 93.22% and 92.15% for the ones targeting GoogLeNet respectively.\nNote that although DeepFool can produce a smaller perturbation than FGSM, the proposed method is still effective and even achieves a higher detection accuracy on almost the same ImageNet samples. As illustrated in Figure 17, an effectual adversarial example is crafted from the original image shown in Figure 1withDeepFool, which can fool the GoogLeNet model into misclassifying it as Llama. Although\nthe introduced perturbation is obviously smaller, our method can still successfully detect it."}, {"heading": "4.3 Detecting CW Examples", "text": "Carlini and Wagner also use MNIST and ImageNet to train two DNN models as their attack targets. We directly download the two trained models from [3] for our evaluation. Considering CW L2 and L\u221e attacks don\u2019t result in observable perturbations, we choose them to generate adversarial examples.\nIn practice, using the two attacks is more expensive than other attack techniques. In our computer, generating an ImageNet adversarial example with L2 and L\u221e take about 30 minutes and 4.5 hours respectively. For this reason, we only picked the first 1,000 images in MNIST and 30 random images for each of the four ImageNet classes (listed in Table 3) as experiment dataset. Eventually, L2 attack successfully generated 991 effectual adversarial examples and 110 from the 120 ImageNet images; and L\u221e attack output 991 and 67 effectual examples from the two groups of images respectively. The generated examples and the corresponding original images make up our detection test set.\nAs listed in Table 5, the proposed method achieves very high recall and precision rates. For MNIST samples, there are only 10 false positives and 13 false negatives in 3,964 MNIST samples, and 9 and 0 in 354 ImageNet samples. By the way, detecting a sample with the proposed method only takes about 8 seconds. The introduced overhead is negligible compared with the time consumption of generating an adversarial example."}, {"heading": "CW MNIST L2 98.79% 99.49%", "text": ""}, {"heading": "4.4 Summary", "text": "All in all, 18,322 samples are used to evaluate our method in above experiments, half of them are adversarial and half are benign. We achieve an overall recall of 93.73% and an overall precision of 95.47% in detecting the adversarial examples generated by the three attack techniques."}, {"heading": "5 DISCUSSION AND LIMITATIONS", "text": "Robustness to Purposeful Attacks. If the adversaries are aware of the proposed method, they may try to develop a new attack technique to evade detection. However, if the perturbation to a pixel can survive from our scalar quantization, it must make the pixel value be mapped to a different interval. In other words, the amplitude of perturbation should be large enough. As a result, a perceptible modification will be introduced into the whole original image, and compromise the utility of the adversarial example. In many attack scenarios, a weird adversarial example is unacceptable, especially when the adversarial example is expected to fool the classifier and human observer at the same time. Besides, under the constraint of our filtering, it is not completely impossible to use an optimization procedure to compute an effectual adversarial example, but it would be difficult and expensive. We have reasons to believe that the proposed method can make it far more challenging to develop a new effective and practicable attack technique.\nFalse Positives and False Negatives. In principle, the performance of our detectionmethod is closely related to the classification capacity of target classifiers. Some false positives and false negatives are caused by the ambiguous images, which are essentially hard to classify for the target classifier.\nAs shown in Figure 18, an image consisting of various fruits is labeled as Pineapple, but GoogLeNet can tell that with only 19.55% confidence. This is really not a strong prediction. The sample is also considered as a Lemon with 10.85% confidence and a Jackfruit 9.43%. After being denoised by our filter, the image is misclassified as Lemon and results in a false positive. However, we think that neither the model nor the proposed detection filter is to blame\nSea anemone 19.76% Lemon 12.32%\nFig 8.17% Adversarial Sample\nSea anemone 21.83% Coral reef 14.65%\nJellyfish 9.29% Filtered Adversarial Sample\nFigure 19: A false negative caused by the ambiguous sample\nTwo Original Sample\nThree Adversarial Sample\nThree Filtered Adversarial\nSample\nFigure 20: A CW L0 adversarial example.\nfor the false positive, but the ambiguity within the image is. The confusing images cannot only result in false positives, but also false negatives. Take an adversarial example generated with FGSM as example, the image shown in Figure 19 is perturbed from Pineapple to Sea Anemone but only with 19.76% confidence. GoogLeNet gives a weak prediction for it. And our detection method also fails to detect this adversarial example and produces a false negative.\nThere are quite a few ambiguous images like the two examples in our test set, which brings down the precision rate as well as the recall rate. For the ambiguous samples, inspecting more predict classes might be necessary rather than just the top one with the highest confidence. We can compare the predict vectors to find a difference for detection as done in [65], if a trained threshold is available.\nThe above phenomenon also implies that the stealthy adversarial examples may can be generated by purposefully touching off an incorrect but weak prediction. We will further analyze the phenomenon and seek a new attack technique.\nPerceptible Perturbations. Some attack techniques, such as CW L0 [13], may introduce the large-amplitude perturbation. According to the L0 attack algorithm, the number of altered pixels is limited, but a pixel can be changed without any limitations. Consequently, as illustrated in Figure 20, the obtained adversarial example may present easy-to-notice distortions. It can be easily spotted by a human. However, it can still be exploited to launch an effective attack when the human interaction is no consideration. In principle, it is very difficult to properly reduce the effect of the heavy perturbation only with the filtering technique without compromising the semantics of the original image. To develop an effective technique to detect this kind of example is beyond this paper\u2019s scope but will be our future research.\nOther Image Processing Techniques. There are a number of other image processing techniques in addition to the ones adopted in our method. Some of them may can be leveraged to further improve our detection method, such as R\u00e9nyi entropy [14], image segmentation [24], etc. For example, we can segment an adversarial example into some regions according to the connectivity among\npixels to find such a region that possesses as much as possible information. From it, we have a good chance to restore the correct classification when the perturbation is isolated in other regions. In this way, the adversarial example shown in Figure 20 can be detected. In the future, we plan to investigate other image processing techniques to develop a more sophisticated detection method, especially for detecting the adversarial example with large-amplitude perturbations."}, {"heading": "6 RELATEDWORK", "text": "Many existing studies have paid much attention to the security of classifiers, and the arm race between adversaries and defenders will never end.\nAttacks on Traditional Classifiers. Many studies have investigated the security of traditional machine learning methods [5] and proposed some attack methods. Lowd and Meek conduct an attack that minimizes a cost function [45]. They further propose attacks against statistical spam filters that add the words indicative of nonspam emails to spam emails [46]. The same strategy is employed in [50]. In [47], a methodology, called reverse mimicry, is designed to evade structural PDF malware detection systems. The main idea is injecting malicious content into a legitimate PDF while introducing minimum differences within its structure. In [38], an online learning-based system for detection of PDF malware, PDFRATE, was used as a case to investigate the effectiveness of evasion attacks. The study reconstructs a similar classifier through training one of the publicly available datasets by a few deduced features, and then evades PDFRATE by insertion of dummy content into PDF files. In [6], an algorithm is proposed for evasion of classifiers with differentiable discriminant functions. The study empirically demonstrated that popular models such as SVMs and neural networks can be evaded with high probability even if the adversary can only learn limited knowledge. Liang et al. [44] demonstrated that client-side classifiers are also vulnerable to evasion attacks.\nXu et al. [66] presented a general approach to find evasive variants by stochastically manipulate a malicious sample seed. The experiment showed that the effectual variants can be automatically generated to against two PDF malware classifiers, i.e., PDFRATE and Hidost.\nFredrikson et al. [21, 64] developed a new form of model inversion attack which can infer sensitive features used in decision tree models and recover images from some facial recognition models by exploiting confidence values revealed by the target models. The proposed attack may cause serious privacy disclosure problems [21]. More model inversion attacks can be found in [64].\nDefenses for Traditional Classifiers. Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.\nGame-theoretical approaches [11, 12] model the interactions between the adversary and the classifier as a game. The adversary\u2019s goal is to evade detection by minimally manipulating the attack instances, while the classifier is retrained to correctly classify them.\nMCSs [7\u20139], as the name suggests, uses multiple classifiers rather than only one to improve classifier\u2019s robustness. The adversary who wants to effectively evade the classification has to fight with more than one classifier.\nKantchelian et al. [33] present family-based ensembles of classifiers. In particular, they trained an ensemble of classifiers, one for each family of malware. By combining classifications, it will be determined whether an unknown binary is malware, and if it is, which family it belongs to. What\u2019s more, they also demonstrate the importance of human operators in adversarial environments.\nIn [22], the method weight evenness via feature selection optimization is proposed. By appropriate feature selection, the weight of every feature is evenly distributed, thus the adversary has to manipulate a larger number of features to evade detection. In [35], the features are reweighted inversely proportional to their corresponding importance, making it difficult for the adversary to exploit the features.\nUnfortunately, these attack and defense techniques for traditional classifiers cannot be directly applied to DNNs. Along with the prevalence of DNNs, researchers have begun to pay close attention to the security of DNNs.\nAttacks on DNNs. Recently, researchers have begun to attack DNN-based classifiers through crafting adversarial samples. There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].\nKereliuk et al. [34] proposed a method to craft adversarial audio examples using the gradient information of the model\u2019s loss function. Through the application of minor perturbations to the input magnitude spectra, they can effectively craft an adversarial example. Text as discrete data is sensitive to perturbation. Liang et al.[43] proposed a method to craft adversarial text examples. Three perturbation strategies, namely insertion,modification, and removal, are designed to generate an adversarial sample for a given text. By computing the cost gradients, what should be inserted, modified or removed, where to insert and how to modify are determined effectively. By elaborately dressing a text sample, the adversary can modify the classification to any other classes while still keeps the meaning unchanged. Grosse et al. [28] presented a method to craft adversarial examples on neural networks for malware classification, by adapting the method originally proposed in [53].\nIn this paper, we focus on the detection of adversarial images. We believe that our method can be applied to detect adversarial examples for audio, which is also a kind of continuous data. But the proposed technique cannot be applied to discrete data, such as the adversarial text and malware. The new method need to be developed for effectively detecting them.\nNote that there are two recent studies focus on crafting adversarial examples in the physical world. Kurakin et al. [37] demonstrated that the adversarial images obtained from a cell-phone camera can still fool an ImageNet classifier. Sharif et al. [56] presented an attack method to fool facial biometric systems. They showed that with some well-crafted eyeglass frames, a subject can dodge recognition or impersonate others.\nBesides, Shokri et al. [57] developed a novel black-box membership inference attack against machine learning models, including DNN and non-DNN models. Given a data record, the attacker can determine whether it is in the target model\u2019s training dataset. For health-care datasets, such information leakage is unacceptable.\nImprove the Robustness of Deep Networks. The adversarial training [26, 34, 49, 53] is a straightforward defense technique to improve the robustness of target models. Retraining models by adding as many as possible adversarial samples can bring more challenges for attackers to find new adversarial samples.\nWang et al. [63] integrated a data transformation module right in front of a standard DNN to improve the model\u2019s resistance to adversarial examples. This data transformation module leverages non-parametric dimensionality reduction methods, and projects all the input samples into a new representation before passing the inputs to the target DNN in training and testing. Wang et al. [62] also proposed another method, named random feature nullification, for constructing adversary resistant DNNs. In particular, it randomly nullifies or masks features within input samples in both the training and testing phase. Such nullification makes a DNN model non-deterministic and then improves model\u2019s resistance to adversarial samples.\nThe proposed method is compatible with the above defense techniques. Defenders can still use our method in the enhanced model to get a better performance.\nDetection Techniques. Some studies also focus on detecting adversarial examples directly.\nXu et al. in a very recent study [65] proposed a method, called Feature Squeezing, to detect adversarial examples in a similar way as ours. They explore two approaches to squeeze the features of an image: reducing the color bit depth of each pixel and smoothing it using a spatial filter. Their system identifies the adversarial examples by measuring the disagreement among the prediction vectors of the original and squeezed examples. Their experiment illustrated high performance was achieved when detecting FGSM adversarial examples in a MNIST model. However, a predefined threshold is required for determining how much disagreement indicate the current sample is adversarial. In their experiment, half of the examples are used to train the threshold that can produce the best detection accuracy on training examples. This means the defender must have a sufficient number of adversarial examples generated with potential attack techniques. As a result, the method works well only when the attack technique is known but less effective when facing unknown attacks. Moreover, in principle, for different datasets, models or attacks, the thresholds need to be retrained to achieve acceptable performance. By introducing the 2-D entropy, we implement an adaptive detection method and can be directly applied to different models, datasets and attack techniques with the same setting and without requiring any prior knowledge of attacks.\nGrosse et al. [27] put forward a defense to detect adversarial examples using statistical tests. The method requires a sufficient large group of adversarial examples and benign examples to estimate their data distribution. However, the statistical test method cannot be directly applied to detect individual examples, making it less useful in practice. For this reason, Grosse et al. further propose a new method by adding an additional class (e.g., adversarial class) to\nthe model\u2019s output and retraining the model to classify adversarial examples as the new class.\nMetzen et al. [29] used a large number of adversarial examples to train a detector to identify unknown adversarial examples. A small \"detector\" subnetwork is trained on the binary classification task of distinguishing benign samples from adversarial perturbations.\nTo a large extent, the performance of the above two detection techniques [27, 29] also depends on how much effectual adversarial examples are available.\nFeinman et al. [20] devised two novel features to detect adversarial examples based on the idea that adversarial examples deviate the true data manifold. They introduced density estimates to measure the distance between an unknown input sample and a set of benign samples. The method is computationally expensive and may be less effective in detecting adversarial examples which are very close to benign samples."}, {"heading": "7 CONCLUSION", "text": "Many efforts have been paid to use various techniques to defend or detect the adversarial image examples in DNNs. However, the prior knowledge of attack techniques or the modifications to the target model is often required. This paper presents a straightforward and effective adversarial image examples detection method. The adversarial perturbations are regarded as a kind of noise and the proposed method is implemented as a filter to reduce their effect. The image 2-D entropy is used to automatically adjust the detection strategy for specific samples. Our method provides two important features (1) without requiring the prior knowledge about attacks and (2) can be directly integrated into unmodified models. The experiment shows that our method can achieve a high recall and precision in detecting the adversarial examples generated by the different attack techniques and targeting different models. Our method is also compatible with other defense techniques. A better performance can be achieved by combining them together.\nOur research demonstrated that the adversarial images can be effectively analyzed with classical image processing techniques. In the future, we will investigate more image processing techniques to find more effective and practicable detection techniques."}, {"heading": "8 ACKNOWLEDGMENTS", "text": "The authors would like to thank the anonymous reviewers for their insightful comments. The work is supported by XXX and YYY.\nACKNOWLEDGMENTS REFERENCES [1] A simple and accurate method to fool deep neural networks. https://github.com/\nlts4/deepfool. [2] LeNet-5, convolutional neural networks. http://yann.lecun.com/exdb/lenet/. [3] Robust evasion attacks against neural network to find adversarial examples.\nhttps://github.com/carlini/nn_robust_attacks. [4] The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist. [5] Marco Barreno, BlaineNelson, Russell Sears, AnthonyD Joseph, and J Doug Tygar.\nCan machine learning be secure? In Proceedings of the 2006 ACM Symposium on Information, computer and communications security, pages 16\u201325. ACM, 2006. [6] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim \u0160rndi\u0107, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 387\u2013402. Springer, 2013.\n[7] Battista Biggio, Giorgio Fumera, and Fabio Roli. Multiple classifier systems for adversarial classification tasks. In Proceedings of the International Workshop on Multiple Classifier Systems, pages 132\u2013141. Springer, 2009. [8] Battista Biggio, Giorgio Fumera, and Fabio Roli. Multiple classifier systems for robust classifier design in adversarial environments. International Journal of Machine Learning and Cybernetics, 1(1-4):27\u201341, 2010. [9] Battista Biggio, Giorgio Fumera, and Fabio Roli. Multiple classifier systems under attack. In Proceedings of the International Workshop on Multiple Classifier Systems, pages 74\u201383. Springer, 2010. [10] Battista Biggio, Giorgio Fumera, and Fabio Roli. Design of robust classifiers for adversarial environments. In Proceedings of the 2011 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pages 977\u2013982. IEEE, 2011. [11] Michael Br\u00fcckner, Christian Kanzow, and Tobias Scheffer. Static prediction games for adversarial learning problems. Journal of Machine Learning Research, 13(Sep):2617\u20132654, 2012. [12] Michael Br\u00fcckner and Tobias Scheffer. Stackelberg games for adversarial prediction problems. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 547\u2013555. ACM, 2011. [13] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. arXiv preprint arXiv:1608.04644, 2016. [14] Bin Chen and Jia-ju Zhang. On short interval expansion of r\u00e9nyi entropy. Journal of High Energy Physics, 11:164, 2013. [15] Dan Cire\u015fAn, Ueli Meier, Jonathan Masci, and J\u00fcrgen Schmidhuber. Multicolumn deep neural network for traffic sign classification. Neural Networks, 32:333\u2013338, 2012. [16] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM, 2008. [17] George E Dahl, Jack W Stokes, Li Deng, and Dong Yu. Large-scale malware classification using random projections and neural networks. In Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3422\u20133426. IEEE, 2013. [18] George E Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pretrained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 20(1):30\u201342, 2012. [19] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625\u2013660, 2010. [20] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410, 2017. [21] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, pages 1322\u20131333. ACM, 2015. [22] Amir Globerson and Sam Roweis. Nightmare at test time: robust learning by feature deletion. In Proceedings of the 23rd international conference on Machine learning, pages 353\u2013360. ACM, 2006. [23] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th international conference on Machine learning, pages 513\u2013520. ACM, 2011. [24] Rafael C Gonzalez and Richard E Woods. Digital image processing. Prentice Hall, 2002. [25] Ian Goodfellow, Honglak Lee, Quoc V Le, Andrew Saxe, and Andrew Y Ng. Measuring invariances in deep networks. In Proceedings of Advances in Neural Information Processing Systems, pages 646\u2013654, 2009. [26] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Proceedings of the 2015 International Conference on Learning Representations, 2015. [27] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017. [28] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. Adversarial perturbations against deep neural networks for malware classification. arXiv preprint arXiv:1606.04435, 2016. [29] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial perturbations. arXiv preprint arXiv:1702.04267, 2017. [30] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82\u201397, 2012. [31] Deng Jia, Dong Wei, Socher Richard, Li-Jia Li, Li Kai, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. IEEE, 2009. [32] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional\narchitecture for fast feature embedding. In Proceedings of the 22nd ACM international conference on Multimedia, pages 675\u2013678. ACM, 2014. [33] Alex Kantchelian, Sadia Afroz, Ling Huang, Aylin Caliskan Islam, Brad Miller, Michael Carl Tschantz, Rachel Greenstadt, Anthony D Joseph, and JD Tygar. Approaches to adversarial drift. In Proceedings of the 2013 ACM workshop on Artificial intelligence and security, pages 99\u2013110. ACM, 2013. [34] Corey Kereliuk, Bob L Sturm, and Jan Larsen. Deep learning and music adversaries. IEEE Transactions on Multimedia, 17(11):2059\u20132071, 2015. [35] Aleksander Ko\u0142cz and Choon Hui Teo. Feature weighting for improved classifier robustness. In Proceedings of the 6th Conference on Email and Anti-Spam, 2009. [36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Proceedings of Advances in Neural Information Processing Systems, pages 1097\u20131105, 2012. [37] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016. [38] Pavel Laskov et al. Practical evasion of a learning-based classifier: A case study. In Proceedings of the 2014 IEEE Symposium on Security and Privacy (S&P), pages 197\u2013211. IEEE, 2014. [39] Quoc V Le. Building high-level features using large scale unsupervised learning. In Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8595\u20138598. IEEE, 2013. [40] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541\u2013551, 1989. [41] Yann LeCun, Koray Kavukcuoglu, and Cl\u00e9ment Farabet. Convolutional networks and applications in vision. In Proceedings of the 2010 IEEE International Symposium on Circuits and Systems (ISCAS), pages 253\u2013256. IEEE, 2010. [42] Jong-Sen Lee. Digital image enhancement and noise filtering by use of local statistics. IEEE transactions on pattern analysis and machine intelligence, (2):165\u2013 168, 1980. [43] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. Deep text classification can be fooled. arXiv preprint arXiv:1704.08006, 2017. [44] Bin Liang, Miaoqiang Su, Wei You, Wenchang Shi, and Gang Yang. Cracking classifiers for evasion: A case study on the google\u2019s phishing pages filter. In Proceedings of the 25th International Conference on World Wide Web, pages 345\u2013 356. International World Wide Web Conferences Steering Committee, 2016. [45] Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 641\u2013647. ACM, 2005. [46] Daniel Lowd and Christopher Meek. Good word attacks on statistical spam filters. In Proceedings of the 2nd Conference on Email and Anti-Spam, 2005. [47] Davide Maiorca, Igino Corona, and Giorgio Giacinto. Looking at the bag is not enough to find the bomb: an evasion of structural methods for malicious pdf files detection. In Proceedings of the 8th ACM SIGSAC symposium on Information, computer and communications security, pages 119\u2013130. ACM, 2013. [48] Jonathan Masci, Ueli Meier, Dan Cire\u015fan, and J\u00fcrgen Schmidhuber. Stacked convolutional auto-encoders for hierarchical feature extraction. Artificial Neural Networks and Machine Learning, pages 52\u201359, 2011. [49] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2574\u20132582, 2016. [50] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Rubinstein, Udam Saini, Charles A Sutton, J Doug Tygar, and Kai Xia. Exploiting machine learning to subvert your spam filter. LEET, 8:1\u20139, 2008. [51] Nicolas Papernot, Ian Goodfellow, Ryan Sheatsley, Reuben Feinman, and Patrick McDaniel. cleverhans v1.0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768, 2016. [52] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pages 506\u2013519, 2017. [53] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In Proceedings of the 2016 IEEE European Symposium on Security and Privacy (EuroS&P), pages 372\u2013387. IEEE, 2016. [54] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Proceedings of the 2016 IEEE Symposium on Security and Privacy (S&P), pages 582\u2013597. IEEE, 2016. [55] Ricardo N Rodrigues, Lee Luan Ling, and Venu Govindaraju. Robustness of multimodal biometric fusion methods against spoof attacks. Journal of Visual Languages & Computing, 20(3):169\u2013179, 2009. [56] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages 1528\u20131540. ACM, 2016.\n[57] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In Proceedings of the 2017 IEEE Symposium on Security and Privacy (S&P). IEEE, 2017. [58] Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation by joint identification-verification. In Proceedings of Advances in Neural Information Processing Systems, pages 1988\u20131996, 2014. [59] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation from predicting 10,000 classes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1891\u20131898, 2014. [60] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139, 2015. [61] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In Proceedings of the 2014 International Conference on Learning Representations, 2014. [62] Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Alexander G Ororbia II, Xinyu Xing, C. Lee Giles, and Xue Liu. Adversary resistant deep neural networks with an application to malware detection. arXiv preprint arXiv:1610.01239, 2016. [63] Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Alexander G Ororbia II, Xinyu Xing, C Lee Giles, and Xue Liu. Learning adversary-resistant deep neural networks. arXiv preprint arXiv:1612.01401, 2016. [64] XiWu, Matthew Fredrikson, Somesh Jha, and Jeffrey F Naughton. Amethodology for formalizing model-inversion attacks. In Proceedings of the 2016 IEEE Computer Security Foundations Symposium (CSF), pages 355\u2013370. IEEE, 2016. [65] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155, 2017. [66] Weilin Xu, Yanjun Qi, and David Evans. Automatically evading classifiers. In Proceedings of the 2016 Network and Distributed Systems Symposium, 2016. [67] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Proceedings of Advances in Neural Information Processing Systems, pages 649\u2013657, 2015."}], "references": [{"title": "Can machine learning be secure", "author": ["Marco Barreno", "BlaineNelson", "Russell Sears", "AnthonyD Joseph", "J Doug Tygar"], "venue": "In Proceedings of the 2006 ACM Symposium on Information, computer and communications security,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Evasion attacks against machine learning at test time", "author": ["Battista Biggio", "Igino Corona", "Davide Maiorca", "Blaine Nelson", "Nedim \u0160rndi\u0107", "Pavel Laskov", "Giorgio Giacinto", "Fabio Roli"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Multiple classifier systems for adversarial classification tasks", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "In Proceedings of the International Workshop on Multiple Classifier Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Multiple classifier systems for robust classifier design in adversarial environments", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "International Journal of Machine Learning and Cybernetics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Multiple classifier systems under attack", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "In Proceedings of the International Workshop on Multiple Classifier Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Design of robust classifiers for adversarial environments", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "In Proceedings of the 2011 IEEE International Conference on Systems, Man, and Cybernetics (SMC),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Static prediction games for adversarial learning problems", "author": ["Michael Br\u00fcckner", "Christian Kanzow", "Tobias Scheffer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Stackelberg games for adversarial prediction problems", "author": ["Michael Br\u00fcckner", "Tobias Scheffer"], "venue": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Towards evaluating the robustness of neural networks", "author": ["Nicholas Carlini", "David Wagner"], "venue": "arXiv preprint arXiv:1608.04644,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "On short interval expansion of r\u00e9nyi entropy", "author": ["Bin Chen", "Jia-ju Zhang"], "venue": "Journal of High Energy Physics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Multicolumn deep neural network for traffic sign classification", "author": ["Dan Cire\u015fAn", "Ueli Meier", "Jonathan Masci", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Large-scale malware classification using random projections and neural networks", "author": ["George E Dahl", "Jack W Stokes", "Li Deng", "Dong Yu"], "venue": "In Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Detecting adversarial samples from artifacts", "author": ["Reuben Feinman", "Ryan R Curtin", "Saurabh Shintre", "Andrew B Gardner"], "venue": "arXiv preprint arXiv:1703.00410,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Model inversion attacks that exploit confidence information and basic countermeasures", "author": ["Matt Fredrikson", "Somesh Jha", "Thomas Ristenpart"], "venue": "In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["Amir Globerson", "Sam Roweis"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 28th international conference on Machine learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Digital image processing", "author": ["Rafael C Gonzalez", "Richard E Woods"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Measuring invariances in deep networks", "author": ["Ian Goodfellow", "Honglak Lee", "Quoc V Le", "Andrew Saxe", "Andrew Y Ng"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "In Proceedings of the 2015 International Conference on Learning Representations,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "On the (statistical) detection of adversarial examples", "author": ["Kathrin Grosse", "Praveen Manoharan", "Nicolas Papernot", "Michael Backes", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1702.06280,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["Kathrin Grosse", "Nicolas Papernot", "Praveen Manoharan", "Michael Backes", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1606.04435,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "On detecting adversarial perturbations", "author": ["Jan Hendrik Metzen", "Tim Genewein", "Volker Fischer", "Bastian Bischoff"], "venue": "arXiv preprint arXiv:1702.04267,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2017}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng Jia", "Dong Wei", "Socher Richard", "Li-Jia Li", "Li Kai", "Fei-Fei Li"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Caffe: Convolutional 12  architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the 22nd ACM international conference on Multimedia,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Approaches to adversarial drift", "author": ["Alex Kantchelian", "Sadia Afroz", "Ling Huang", "Aylin Caliskan Islam", "Brad Miller", "Michael Carl Tschantz", "Rachel Greenstadt", "Anthony D Joseph", "JD Tygar"], "venue": "In Proceedings of the 2013 ACM workshop on Artificial intelligence and security,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Deep learning and music adversaries", "author": ["Corey Kereliuk", "Bob L Sturm", "Jan Larsen"], "venue": "IEEE Transactions on Multimedia,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Feature weighting for improved classifier robustness", "author": ["Aleksander Ko\u0142cz", "Choon Hui Teo"], "venue": "In Proceedings of the 6th Conference on Email and Anti-Spam,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Adversarial examples in the physical world", "author": ["Alexey Kurakin", "Ian Goodfellow", "Samy Bengio"], "venue": "arXiv preprint arXiv:1607.02533,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Practical evasion of a learning-based classifier: A case study", "author": ["Pavel Laskov"], "venue": "In Proceedings of the 2014 IEEE Symposium on Security and Privacy (S&P),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V Le"], "venue": "In Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1989}, {"title": "Convolutional networks and applications in vision", "author": ["Yann LeCun", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In Proceedings of the 2010 IEEE International Symposium on Circuits and Systems (ISCAS),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "Digital image enhancement and noise filtering by use of local statistics", "author": ["Jong-Sen Lee"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1980}, {"title": "Deep text classification can be fooled", "author": ["Bin Liang", "Hongcheng Li", "Miaoqiang Su", "Pan Bian", "Xirong Li", "Wenchang Shi"], "venue": "arXiv preprint arXiv:1704.08006,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2017}, {"title": "Cracking classifiers for evasion: A case study on the google\u2019s phishing pages filter", "author": ["Bin Liang", "Miaoqiang Su", "Wei You", "Wenchang Shi", "Gang Yang"], "venue": "In Proceedings of the 25th International Conference on World Wide Web,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Adversarial learning", "author": ["Daniel Lowd", "Christopher Meek"], "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2005}, {"title": "Good word attacks on statistical spam filters", "author": ["Daniel Lowd", "Christopher Meek"], "venue": "In Proceedings of the 2nd Conference on Email and Anti-Spam,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2005}, {"title": "Looking at the bag is not enough to find the bomb: an evasion of structural methods for malicious pdf files detection", "author": ["Davide Maiorca", "Igino Corona", "Giorgio Giacinto"], "venue": "In Proceedings of the 8th ACM SIGSAC symposium on Information, computer and communications security,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["Jonathan Masci", "Ueli Meier", "Dan Cire\u015fan", "J\u00fcrgen Schmidhuber"], "venue": "Artificial Neural Networks and Machine Learning,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Exploiting machine learning to subvert your spam", "author": ["Blaine Nelson", "Marco Barreno", "Fuching Jack Chi", "Anthony D Joseph", "Benjamin IP Rubinstein", "Udam Saini", "Charles A Sutton", "J Doug Tygar", "Kai Xia"], "venue": "filter. LEET,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2008}, {"title": "cleverhans v1.0.0: an adversarial machine learning library", "author": ["Nicolas Papernot", "Ian Goodfellow", "Ryan Sheatsley", "Reuben Feinman", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1610.00768,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "Practical black-box attacks against machine learning", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2017}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "In Proceedings of the 2016 IEEE European Symposium on Security and Privacy (EuroS&P),", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": "In Proceedings of the 2016 IEEE Symposium on Security and Privacy (S&P),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2016}, {"title": "Robustness of multimodal biometric fusion methods against spoof attacks", "author": ["Ricardo N Rodrigues", "Lee Luan Ling", "Venu Govindaraju"], "venue": "Journal of Visual Languages & Computing,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["Mahmood Sharif", "Sruti Bhagavatula", "Lujo Bauer", "Michael K Reiter"], "venue": "In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "Membership inference attacks against machine learning models", "author": ["Reza Shokri", "Marco Stronati", "Congzheng Song", "Vitaly Shmatikov"], "venue": "In Proceedings of the 2017 IEEE Symposium on Security and Privacy (S&P)", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2017}, {"title": "Deep learning face representation by joint identification-verification", "author": ["Yi Sun", "Yuheng Chen", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2014}, {"title": "Deep learning face representation from predicting 10,000 classes", "author": ["Yi Sun", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "In Proceedings of the 2014 International Conference on Learning Representations,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2014}, {"title": "Adversary resistant deep neural networks with an application to malware detection", "author": ["Qinglong Wang", "Wenbo Guo", "Kaixuan Zhang", "Alexander G Ororbia II", "Xinyu Xing", "C. Lee Giles", "Xue Liu"], "venue": "arXiv preprint arXiv:1610.01239,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2016}, {"title": "Learning adversary-resistant deep neural networks", "author": ["Qinglong Wang", "Wenbo Guo", "Kaixuan Zhang", "Alexander G Ororbia II", "Xinyu Xing", "C Lee Giles", "Xue Liu"], "venue": "arXiv preprint arXiv:1612.01401,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2016}, {"title": "Amethodology for formalizing model-inversion attacks", "author": ["XiWu", "Matthew Fredrikson", "Somesh Jha", "Jeffrey F Naughton"], "venue": "In Proceedings of the 2016 IEEE Computer Security Foundations Symposium (CSF),", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2016}, {"title": "Feature squeezing: Detecting adversarial examples in deep neural networks", "author": ["Weilin Xu", "David Evans", "Yanjun Qi"], "venue": "arXiv preprint arXiv:1704.01155,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2017}, {"title": "Automatically evading classifiers", "author": ["Weilin Xu", "Yanjun Qi", "David Evans"], "venue": "In Proceedings of the 2016 Network and Distributed Systems Symposium,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2015}], "referenceMentions": [{"referenceID": 31, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67].", "startOffset": 98, "endOffset": 106}, {"referenceID": 36, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67].", "startOffset": 98, "endOffset": 106}, {"referenceID": 13, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67].", "startOffset": 127, "endOffset": 135}, {"referenceID": 25, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67].", "startOffset": 127, "endOffset": 135}, {"referenceID": 11, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67].", "startOffset": 169, "endOffset": 177}, {"referenceID": 62, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision [36, 41], speech recognition [18, 30], and natural language processing [16, 67].", "startOffset": 169, "endOffset": 177}, {"referenceID": 55, "context": "DNNs have exhibited very impressive performance in these tasks, especially in the image classification [60].", "startOffset": 103, "endOffset": 107}, {"referenceID": 53, "context": "Some DNN-based classifiers achieved even higher performance than human [58, 59].", "startOffset": 71, "endOffset": 79}, {"referenceID": 54, "context": "Some DNN-based classifiers achieved even higher performance than human [58, 59].", "startOffset": 71, "endOffset": 79}, {"referenceID": 21, "context": "Some recent studies [26, 49, 61] demonstrate that DNN-based image classifiers can be fooled by adversarial examples, which are well-crafted to cause a trained model to misclassify the instances it is given.", "startOffset": 20, "endOffset": 32}, {"referenceID": 44, "context": "Some recent studies [26, 49, 61] demonstrate that DNN-based image classifiers can be fooled by adversarial examples, which are well-crafted to cause a trained model to misclassify the instances it is given.", "startOffset": 20, "endOffset": 32}, {"referenceID": 56, "context": "Some recent studies [26, 49, 61] demonstrate that DNN-based image classifiers can be fooled by adversarial examples, which are well-crafted to cause a trained model to misclassify the instances it is given.", "startOffset": 20, "endOffset": 32}, {"referenceID": 21, "context": "presented in [26].", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "image [26].", "startOffset": 6, "endOffset": 10}, {"referenceID": 55, "context": "Consequently, a famous DNN classifier GoogLeNet [60] will misclassify the resultant image, while a human observer can still correctly classify it and without noticing the existence of the introduced perturbations.", "startOffset": 48, "endOffset": 52}, {"referenceID": 47, "context": "As shown in [52], a stop sign, after being crafted, will be incorrectly classified as a yield sign.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "Some techniques have been proposed to defend adversarial examples in DNNs [26, 34, 53, 54].", "startOffset": 74, "endOffset": 90}, {"referenceID": 29, "context": "Some techniques have been proposed to defend adversarial examples in DNNs [26, 34, 53, 54].", "startOffset": 74, "endOffset": 90}, {"referenceID": 48, "context": "Some techniques have been proposed to defend adversarial examples in DNNs [26, 34, 53, 54].", "startOffset": 74, "endOffset": 90}, {"referenceID": 49, "context": "Some techniques have been proposed to defend adversarial examples in DNNs [26, 34, 53, 54].", "startOffset": 74, "endOffset": 90}, {"referenceID": 21, "context": "For example, the adversarial training is a straightforward defense technique which uses as many adversarial samples as possible during training process as a kind of regularization [26, 34, 53].", "startOffset": 180, "endOffset": 192}, {"referenceID": 29, "context": "For example, the adversarial training is a straightforward defense technique which uses as many adversarial samples as possible during training process as a kind of regularization [26, 34, 53].", "startOffset": 180, "endOffset": 192}, {"referenceID": 48, "context": "For example, the adversarial training is a straightforward defense technique which uses as many adversarial samples as possible during training process as a kind of regularization [26, 34, 53].", "startOffset": 180, "endOffset": 192}, {"referenceID": 49, "context": "s [54] introduced a defense technique named defensive distillation to adversarial sample.", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "Several very recent studies [20, 27, 29, 65] focus on detecting adversarial examples directly.", "startOffset": 28, "endOffset": 44}, {"referenceID": 22, "context": "Several very recent studies [20, 27, 29, 65] focus on detecting adversarial examples directly.", "startOffset": 28, "endOffset": 44}, {"referenceID": 24, "context": "Several very recent studies [20, 27, 29, 65] focus on detecting adversarial examples directly.", "startOffset": 28, "endOffset": 44}, {"referenceID": 60, "context": "Several very recent studies [20, 27, 29, 65] focus on detecting adversarial examples directly.", "startOffset": 28, "endOffset": 44}, {"referenceID": 15, "context": "Similarly, these techniques also require modifying the model or acquiring sufficient adversarial examples, such as training new sub-models [20], retraining a revised model as a detector using known adversarial examples [29], performing a statistical test on a large group of adversarial and benign examples [27], or training the key detection parameter using a number of adversarial examples and their corresponding benign ones [65].", "startOffset": 139, "endOffset": 143}, {"referenceID": 24, "context": "Similarly, these techniques also require modifying the model or acquiring sufficient adversarial examples, such as training new sub-models [20], retraining a revised model as a detector using known adversarial examples [29], performing a statistical test on a large group of adversarial and benign examples [27], or training the key detection parameter using a number of adversarial examples and their corresponding benign ones [65].", "startOffset": 219, "endOffset": 223}, {"referenceID": 22, "context": "Similarly, these techniques also require modifying the model or acquiring sufficient adversarial examples, such as training new sub-models [20], retraining a revised model as a detector using known adversarial examples [29], performing a statistical test on a large group of adversarial and benign examples [27], or training the key detection parameter using a number of adversarial examples and their corresponding benign ones [65].", "startOffset": 307, "endOffset": 311}, {"referenceID": 60, "context": "Similarly, these techniques also require modifying the model or acquiring sufficient adversarial examples, such as training new sub-models [20], retraining a revised model as a detector using known adversarial examples [29], performing a statistical test on a large group of adversarial and benign examples [27], or training the key detection parameter using a number of adversarial examples and their corresponding benign ones [65].", "startOffset": 428, "endOffset": 432}, {"referenceID": 20, "context": "In fact, some studies [2, 25, 40] have shown that the state-of-the-art classifier is invariant to different input transformations, such as translation, rotation, scale and etc.", "startOffset": 22, "endOffset": 33}, {"referenceID": 35, "context": "In fact, some studies [2, 25, 40] have shown that the state-of-the-art classifier is invariant to different input transformations, such as translation, rotation, scale and etc.", "startOffset": 22, "endOffset": 33}, {"referenceID": 55, "context": "Weemploy some state-of-the-art DNNmodels and popular datasets, such as GoogLeNet [60], CaffeNet [32], ImageNet [31] andMNIST [4] to evaluate the effectiveness of the proposed method.", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": "Weemploy some state-of-the-art DNNmodels and popular datasets, such as GoogLeNet [60], CaffeNet [32], ImageNet [31] andMNIST [4] to evaluate the effectiveness of the proposed method.", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "Weemploy some state-of-the-art DNNmodels and popular datasets, such as GoogLeNet [60], CaffeNet [32], ImageNet [31] andMNIST [4] to evaluate the effectiveness of the proposed method.", "startOffset": 111, "endOffset": 115}, {"referenceID": 21, "context": ", FGSM [26], DeepFool [49], and CW attacks [13], are used to craft adversarial examples.", "startOffset": 7, "endOffset": 11}, {"referenceID": 44, "context": ", FGSM [26], DeepFool [49], and CW attacks [13], are used to craft adversarial examples.", "startOffset": 22, "endOffset": 26}, {"referenceID": 8, "context": ", FGSM [26], DeepFool [49], and CW attacks [13], are used to craft adversarial examples.", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "The network is trained with labeled dataset to learn some connections between inputs and outputs [15, 17, 23, 36].", "startOffset": 97, "endOffset": 113}, {"referenceID": 12, "context": "The network is trained with labeled dataset to learn some connections between inputs and outputs [15, 17, 23, 36].", "startOffset": 97, "endOffset": 113}, {"referenceID": 18, "context": "The network is trained with labeled dataset to learn some connections between inputs and outputs [15, 17, 23, 36].", "startOffset": 97, "endOffset": 113}, {"referenceID": 31, "context": "The network is trained with labeled dataset to learn some connections between inputs and outputs [15, 17, 23, 36].", "startOffset": 97, "endOffset": 113}, {"referenceID": 43, "context": "The latter is often used for feature extraction [48] and network pre-training [19], which is trained with unlabeled dataset.", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "The latter is often used for feature extraction [48] and network pre-training [19], which is trained with unlabeled dataset.", "startOffset": 78, "endOffset": 82}, {"referenceID": 56, "context": "[61] first made the intriguing discovery that various machine learning models, including DNNs [36, 39], are vulnerable to adversarial samples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[61] first made the intriguing discovery that various machine learning models, including DNNs [36, 39], are vulnerable to adversarial samples.", "startOffset": 94, "endOffset": 102}, {"referenceID": 34, "context": "[61] first made the intriguing discovery that various machine learning models, including DNNs [36, 39], are vulnerable to adversarial samples.", "startOffset": 94, "endOffset": 102}, {"referenceID": 21, "context": "generated straightforwardly [26] or with an optimization procedure [13, 49, 61].", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "generated straightforwardly [26] or with an optimization procedure [13, 49, 61].", "startOffset": 67, "endOffset": 79}, {"referenceID": 44, "context": "generated straightforwardly [26] or with an optimization procedure [13, 49, 61].", "startOffset": 67, "endOffset": 79}, {"referenceID": 56, "context": "generated straightforwardly [26] or with an optimization procedure [13, 49, 61].", "startOffset": 67, "endOffset": 79}, {"referenceID": 21, "context": "[26] proposed a straightforward strategy named fast gradient sign method (FGSM) to craft adversarial samples against GoogLeNet [60].", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[26] proposed a straightforward strategy named fast gradient sign method (FGSM) to craft adversarial samples against GoogLeNet [60].", "startOffset": 127, "endOffset": 131}, {"referenceID": 44, "context": "[49] devised the DeepFool algorithm to find very small perturbations that are sufficient to change the classification result.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "performed some attack experiments against several DNN image classifiers, such as CaffeNet [32] and GoogLeNet [60], and so on.", "startOffset": 90, "endOffset": 94}, {"referenceID": 55, "context": "performed some attack experiments against several DNN image classifiers, such as CaffeNet [32] and GoogLeNet [60], and so on.", "startOffset": 109, "endOffset": 113}, {"referenceID": 8, "context": "Carlini and Wagner [13] also employed an optimization algorithm to seek as small as possible perturbations.", "startOffset": 19, "endOffset": 23}, {"referenceID": 26, "context": "Using some public datasets, such as MNIST [4] and ImageNet [31], Carlini and Wagner trained some deep network models to evaluate their attack methods.", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "As demonstrated in [13], CW attacks can find closer adversarial examples than the other attack techniques and never fail to find an adversarial example.", "startOffset": 19, "endOffset": 23}, {"referenceID": 49, "context": "Besides, Carlini and Wagner also illustrated their attacks can effectively break the defensive distillation [54].", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "[25] found that the features learned by deep networks are invariant to different input transformations, such as translation, rotation, scale and etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[2, 40] also demonstrated LeNet-5 classifier is robust to translation, scale, rotation, squeezing, and stroke width.", "startOffset": 0, "endOffset": 7}, {"referenceID": 37, "context": "For example, Lee filtering [42], a very effective algorithm to filter noise.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "There are two types of scalar quantization techniques, uniform quantization and non-uniform quantization [24].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "In uniform quantization, the input will be separated into the same size intervals, and in non-uniform quantization they are usually of different sizes chosen with an optimization algorithm to minimize the distortion [24].", "startOffset": 216, "endOffset": 220}, {"referenceID": 44, "context": "Interval [0,49] [50,99] [100,149] [150,199] [200,249] [250,255]", "startOffset": 9, "endOffset": 15}, {"referenceID": 45, "context": "Interval [0,49] [50,99] [100,149] [150,199] [200,249] [250,255]", "startOffset": 16, "endOffset": 23}, {"referenceID": 19, "context": "According to the suggestion of Safe RGB Colors [24], we separate each color plane (R, G and B) into six intervals and set the step to 50.", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "Besides, in practice, some features of interest can be emphasized by giving more importance (weight) to some pixels in the mask at the expense of others [24].", "startOffset": 153, "endOffset": 157}, {"referenceID": 26, "context": "One is a GoogLeNet model trained with the ImageNet dataset [31], which has been taken as the attack target of FGSM in [26].", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "One is a GoogLeNet model trained with the ImageNet dataset [31], which has been taken as the attack target of FGSM in [26].", "startOffset": 118, "endOffset": 122}, {"referenceID": 46, "context": "The other is a DNN model trained with the MNIST dataset, which is from an adversarial machine learning library [51] and trained for testing the FGSM attack.", "startOffset": 111, "endOffset": 115}, {"referenceID": 26, "context": "ImageNet [31] is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories, while MNIST [4] is a small-scalar dataset of simple gray handwritten digits.", "startOffset": 9, "endOffset": 13}, {"referenceID": 60, "context": "We can compare the predict vectors to find a difference for detection as done in [65], if a trained threshold is available.", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "Some attack techniques, such as CW L0 [13], may introduce the large-amplitude perturbation.", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "Some of them may can be leveraged to further improve our detection method, such as R\u00e9nyi entropy [14], image segmentation [24], etc.", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "Some of them may can be leveraged to further improve our detection method, such as R\u00e9nyi entropy [14], image segmentation [24], etc.", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "Many studies have investigated the security of traditional machine learning methods [5] and proposed some attack methods.", "startOffset": 84, "endOffset": 87}, {"referenceID": 40, "context": "Lowd and Meek conduct an attack that minimizes a cost function [45].", "startOffset": 63, "endOffset": 67}, {"referenceID": 41, "context": "They further propose attacks against statistical spam filters that add the words indicative of nonspam emails to spam emails [46].", "startOffset": 125, "endOffset": 129}, {"referenceID": 45, "context": "The same strategy is employed in [50].", "startOffset": 33, "endOffset": 37}, {"referenceID": 42, "context": "In [47], a methodology, called reverse mimicry, is designed to evade structural PDF malware detection systems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "In [38], an online learning-based system for detection of PDF malware, PDFRATE, was used as a case to investigate the effectiveness of evasion attacks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "In [6], an algorithm is proposed for evasion of classifiers with differentiable discriminant functions.", "startOffset": 3, "endOffset": 6}, {"referenceID": 39, "context": "[44] demonstrated that client-side classifiers are also vulnerable to evasion attacks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[66] presented a general approach to find evasive variants by stochastically manipulate a malicious sample seed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[21, 64] developed a new form of model inversion attack which can infer sensitive features used in decision tree models and recover images from some facial recognition models by exploiting confidence values revealed by the target models.", "startOffset": 0, "endOffset": 8}, {"referenceID": 59, "context": "[21, 64] developed a new form of model inversion attack which can infer sensitive features used in decision tree models and recover images from some facial recognition models by exploiting confidence values revealed by the target models.", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "The proposed attack may cause serious privacy disclosure problems [21].", "startOffset": 66, "endOffset": 70}, {"referenceID": 59, "context": "More model inversion attacks can be found in [64].", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 91, "endOffset": 99}, {"referenceID": 7, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 91, "endOffset": 99}, {"referenceID": 5, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 124, "endOffset": 132}, {"referenceID": 50, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 124, "endOffset": 132}, {"referenceID": 2, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 243, "endOffset": 248}, {"referenceID": 3, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 243, "endOffset": 248}, {"referenceID": 4, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 243, "endOffset": 248}, {"referenceID": 17, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 321, "endOffset": 329}, {"referenceID": 30, "context": "Many countermeasures against evasion attacks have been proposed, such as using game theory [11, 12] or probabilistic models [10, 55] to predict attack strategy to construct more robust classifiers, employing multiple classifier systems (MCSs) [7\u20139] to increase the difficulty of evasion, and optimizing feature selection [22, 35] to make the features evenly distributed.", "startOffset": 321, "endOffset": 329}, {"referenceID": 6, "context": "Game-theoretical approaches [11, 12] model the interactions between the adversary and the classifier as a game.", "startOffset": 28, "endOffset": 36}, {"referenceID": 7, "context": "Game-theoretical approaches [11, 12] model the interactions between the adversary and the classifier as a game.", "startOffset": 28, "endOffset": 36}, {"referenceID": 2, "context": "MCSs [7\u20139], as the name suggests, uses multiple classifiers rather than only one to improve classifier\u2019s robustness.", "startOffset": 5, "endOffset": 10}, {"referenceID": 3, "context": "MCSs [7\u20139], as the name suggests, uses multiple classifiers rather than only one to improve classifier\u2019s robustness.", "startOffset": 5, "endOffset": 10}, {"referenceID": 4, "context": "MCSs [7\u20139], as the name suggests, uses multiple classifiers rather than only one to improve classifier\u2019s robustness.", "startOffset": 5, "endOffset": 10}, {"referenceID": 28, "context": "[33] present family-based ensembles of classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In [22], the method weight evenness via feature selection optimization is proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "In [35], the features are reweighted inversely proportional to their corresponding importance, making it difficult for the adversary to exploit the features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].", "startOffset": 121, "endOffset": 133}, {"referenceID": 44, "context": "There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].", "startOffset": 121, "endOffset": 133}, {"referenceID": 56, "context": "There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].", "startOffset": 121, "endOffset": 133}, {"referenceID": 29, "context": "There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].", "startOffset": 163, "endOffset": 167}, {"referenceID": 38, "context": "There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].", "startOffset": 189, "endOffset": 193}, {"referenceID": 23, "context": "There are various methods to generate adversarial samples against DNNs in various fields, not limited in computer vision [26, 49, 61], but also speech recognition [34], text classification [43] and malware detection [28].", "startOffset": 216, "endOffset": 220}, {"referenceID": 29, "context": "[34] proposed a method to craft adversarial audio examples using the gradient information of the model\u2019s loss function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[43] proposed a method to craft adversarial text examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[28] presented a method to craft adversarial examples on neural networks for malware classification, by adapting the method originally proposed in [53].", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[28] presented a method to craft adversarial examples on neural networks for malware classification, by adapting the method originally proposed in [53].", "startOffset": 147, "endOffset": 151}, {"referenceID": 32, "context": "[37] demonstrated that the adversarial images obtained from a cell-phone camera can still fool an ImageNet classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[56] presented an attack method to fool facial biometric systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[57] developed a novel black-box membership inference attack against machine learning models, including DNN and non-DNN models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The adversarial training [26, 34, 49, 53] is a straightforward defense technique to improve the robustness of target models.", "startOffset": 25, "endOffset": 41}, {"referenceID": 29, "context": "The adversarial training [26, 34, 49, 53] is a straightforward defense technique to improve the robustness of target models.", "startOffset": 25, "endOffset": 41}, {"referenceID": 44, "context": "The adversarial training [26, 34, 49, 53] is a straightforward defense technique to improve the robustness of target models.", "startOffset": 25, "endOffset": 41}, {"referenceID": 48, "context": "The adversarial training [26, 34, 49, 53] is a straightforward defense technique to improve the robustness of target models.", "startOffset": 25, "endOffset": 41}, {"referenceID": 58, "context": "[63] integrated a data transformation module right in front of a standard DNN to improve the model\u2019s resistance to adversarial examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[62] also proposed another method, named random feature nullification, for constructing adversary resistant DNNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "in a very recent study [65] proposed a method, called Feature Squeezing, to detect adversarial examples in a similar way as ours.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "[27] put forward a defense to detect adversarial examples using statistical tests.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[29] used a large number of adversarial examples to train a detector to identify unknown adversarial examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "To a large extent, the performance of the above two detection techniques [27, 29] also depends on how much effectual adversarial examples are available.", "startOffset": 73, "endOffset": 81}, {"referenceID": 24, "context": "To a large extent, the performance of the above two detection techniques [27, 29] also depends on how much effectual adversarial examples are available.", "startOffset": 73, "endOffset": 81}, {"referenceID": 15, "context": "[20] devised two novel features to detect adversarial examples based on the idea that adversarial examples deviate the true data manifold.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Deep neural networks (DNNs) play a key role in many applications. Unsurprisingly, they also became a potential attack target of adversaries. Some studies have demonstrated DNN classifiers can be fooled by the adversarial example, which is crafted via introducing some perturbations into an original sample. Accordingly, some powerful defense techniques were proposed against adversarial examples. However, existing defense techniques requiremodifying the target model or depend on the prior knowledge of attack techniques to different degrees. In this paper, we propose a straightforward method for detecting adversarial image examples. It doesn\u2019t require any prior knowledge of attack techniques and can be directly deployed into unmodified off-the-shelf DNN models. Specifically, we consider the perturbation to images as a kind of noise and introduce two classical image processing techniques, scalar quantization and smoothing spatial filter, to reduce its effect. The image two-dimensional entropy is employed as a metric to implement an adaptive noise reduction for different kinds of images. As a result, the adversarial example can be effectively detected by comparing the classification results of a given sample and its denoised version. Thousands of adversarial examples against some state-of-the-art DNN models are used to evaluate the proposed method, which are crafted with different attack techniques. The experiment shows that our detection method can achieve an overall recall of 93.73% and an overall precision of 95.47% without referring to any prior knowledge of attack techniques.", "creator": "LaTeX with hyperref package"}}}