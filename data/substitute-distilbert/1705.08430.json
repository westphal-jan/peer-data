{"id": "1705.08430", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues", "abstract": "in this work we derive a variant like the classic glivenko - cantelli theorem, which asserts uniform convergence of the optimal cumulative distribution function ( cdf ) to form cdf of the underlying distribution. consistent variant allows for tighter convergence bounds for extreme values of the parameters.", "histories": [["v1", "Tue, 23 May 2017 17:37:33 GMT  (18kb,D)", "https://arxiv.org/abs/1705.08430v1", null], ["v2", "Fri, 4 Aug 2017 12:40:36 GMT  (19kb,D)", "http://arxiv.org/abs/1705.08430v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["noga alon", "moshe babaioff", "yannai a gonczarowski", "yishay mansour", "shay moran", "amir yehudayoff"], "accepted": true, "id": "1705.08430"}, "pdf": {"name": "1705.08430.pdf", "metadata": {"source": "CRF", "title": "Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues", "authors": ["Noga Alon", "Moshe Babaioff", "Yannai A. Gonczarowski", "Yishay Mansour", "Shay Moran", "Amir Yehudayoff"], "emails": ["nogaa@tau.ac.il.", "moshe@microsoft.com.", "yannai@gonch.name.", "mansour@tau.ac.il.", "shaymoran1@gmail.com.", "amir.yehudayoff@gmail.com."], "sections": [{"heading": null, "text": "We apply our bound in the context of revenue learning, which is a well-studied problem in economics and algorithmic game theory. We derive sample-complexity bounds on the uniform convergence rate of the empirical revenues to the true revenues, assuming a bound on the kth moment of the valuations, for any (possibly fractional) k > 1.\nFor uniform convergence in the limit, we give a complete characterization and a zeroone law: if the first moment of the valuations is finite, then uniform convergence almost surely occurs; conversely, if the first moment is infinite, then uniform convergence almost never occurs."}, {"heading": "1 Introduction", "text": "A basic task in machine learning is to learn an unknown distribution \u00b5, given access to samples from it. A natural and widely studied criterion for learning a distribution is approximating its Cumulative Distribution Function (CDF). The seminal Glivenko-Cantelli Theorem (Glivenko, 1933; Cantelli, 1933) addresses this question when the distribution \u00b5 is over the real numbers. It determines the behavior of the empirical distribution function as the number of samples grows: let X1, X2, . . . be a sequence of i.i.d. random variables drawn from a distribution \u00b5 on R \u2217Tel Aviv University, Israel and Microsoft Research, Herzliya, Israel. nogaa@tau.ac.il. Research supported in part by an ISF grant and by a GIF grant. \u2020Microsoft Research, Herzliya, Israel. moshe@microsoft.com. \u2021The Hebrew University of Jerusalem, Israel and Microsoft Research, Herzliya, Israel. yannai@gonch.name. Yannai Gonczarowski is supported by the Adams Fellowship Program of the Israel Academy of Sciences and Humanities. His work is supported by ISF grant 1435/14 administered by the Israeli Academy of Sciences and by Israel-USA Bi-national Science Foundation (BSF) grant number 2014389. This project has received funding from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No 740282). \u00a7Tel Aviv University, Israel and Microsoft Research, Herzliya, Israel. mansour@tau.ac.il. This research was supported in part by The Israeli Centers of Research Excellence (I-CORE) program (Center No. 4/11), by a grant from the Israel Science Foundation, and by a grant from United States-Israel Binational Science Foundation (BSF). \u00b6University of California, San Diego, Simons Institute for the Theory of Computing, Berkeley, and Max Planck Institute for Informatics, Saarbru\u0308cken, Germany. shaymoran1@gmail.com. \u2016Technion \u2014 Israel Institute of Technology, Israel. amir.yehudayoff@gmail.com. Research supported by ISF grant 1162/15.\nar X\niv :1\n70 5.\n08 43\n0v 2\n[ cs\n.L G\n] 4\nA ug\n2 01\n7\nwith Cumulative Distribution Function (CDF) F , and let x1, x2, . . . be their realizations. The empirical distribution \u00b5n is\n\u00b5n , 1\nn n\u2211 i=1 \u03b4xi ,\nwhere \u03b4xi is the constant distribution supported on xi. Let Fn denote the CDF of \u00b5n, i.e., Fn(t) , 1 n \u00b7 \u2223\u2223{1 \u2264 i \u2264 n : xi \u2264 t}\u2223\u2223. The Glivenko-Cantelli Theorem formalizes the statement that \u00b5n converges to \u00b5 as n grows, by establishing that Fn(t) converges to F (t), uniformly over all t \u2208 R:\nTheorem 1.1 (Glivenko-Cantelli Theorem, 1933). Almost surely,\nlim n\u2192\u221e sup t \u2223\u2223Fn(t)\u2212 F (t)\u2223\u2223 = 0. Some twenty years after Glivenko and Cantelli discovered this theorem, Dvoretzky, Kiefer, and Wolfowitz (DKW) strengthened this result by giving an almost1 tight quantitative bound on the convergence rate. In 1990, Massart proved a tight inequality, confirming a conjecture due to Birnbaum and McCarty (1958):\nTheorem 1.2 (Massart, 1990). Pr [ supt \u2223\u2223Fn(t)\u2212F (t)\u2223\u2223 > ] \u2264 2 exp(\u22122n 2) for all > 0, n \u2208 N. The above theorems show that, with high probability, F and Fn are close up to some additive error. We would have liked to prove a stronger, multiplicative bound on the error:\n\u2200t : \u2223\u2223F (t)\u2212 Fn(t)\u2223\u2223 \u2264 \u00b7 F (t).\nHowever, for some distributions, the above event has probability 0, no matter how large n is. For example, assume that \u00b5 satisfies F (t) > 0 for all t. Since the empirical measure \u00b5n has finite support, there is t with Fn(t) = 0; for such a value of t, such a multiplicative approximation fails to hold.\nSo, the above multiplicative requirement is too strong to hold in general. A natural compromise is to consider a submultiplicative bound:\n\u2200t : \u2223\u2223F (t)\u2212 Fn(t)\u2223\u2223 \u2264 \u00b7 F (t)\u03b1,\nwhere 0 \u2264 \u03b1 < 1. When \u03b1 = 0, this is the additive bound studied in the context of the Glivenko-Cantelli Theorem. When \u03b1 = 1, this is the unattainable multiplicative bound. Our first main result shows that the case of \u03b1 < 1 is attainable:\nTheorem 1.3 (Submultiplicative Glivenko-Cantelli Theorem). Let > 0, \u03b4 > 0 and 0 \u2264 \u03b1 < 1. There exists n0( , \u03b4, \u03b1) such that for all n > n0, with probability 1\u2212 \u03b4:\n\u2200t : \u2223\u2223F (t)\u2212 Fn(t)\u2223\u2223 \u2264 \u00b7 F (t)\u03b1.\nIt is worth pointing out a central difference between Theorem 1.3 and other generalizations of the Glivenko-Cantelli Theorem: for example, the seminal work of Vapnik and Chervonenkis (1971) shows that for every class of events F of VC dimension d, there is n0 = n0( , \u03b4, d) such that for every n \u2265 n0, with probability 1\u2212 \u03b4 it holds that \u2200A \u2208 F :\n\u2223\u2223p(A)\u2212 pn(A)\u2223\u2223 \u2264 . This yields Glivenko-Cantelli by plugging F = { (\u2212\u221e, t] : t \u2208 R } , which has VC dimension 1. In contrast, the submultiplicative bound from Theorem 1.3 does not even extend to the VC dimension 1 class F = { {t} : t \u2208 R } . Indeed, pick any distribution p over R such that p ( {t} )\n= 0 for every t, and observe that for every sample x1, . . . , xn, it holds that pn ( {xi} ) \u2265 1/n, however p ( {xi} ) = 0,\nand therefore, as long as \u03b1 > 0, it is never the case that \u2223\u2223\u2223p({xi}) \u2212 pn({xi})\u2223\u2223\u2223 \u2264 p({xi})\u03b1. Theorem 1.3 is proven in Section 3, which also includes other extensions. Our second main result gives an explicit upper bound on n0( , \u03b4, \u03b1):\n1The inequality due to Dvoretzky et al. (1956) has a larger constant C in front of the exponent on the right hand side.\nTheorem 1.4 (Submultiplicative Glivenko-Cantelli Bound). Let , \u03b4 \u2264 1/4, and \u03b1 < 1. Then\nn0( , \u03b4, \u03b1) \u2264 max  ln ( 6/\u03b4 ) 2 2 ( \u03b43 ) \u2212 4\u03b1 1\u2212\u03b1 , (D + 1) ( 10 \u00b7 ln ( 12 \u00b7 D + 4 \u03b4(1\u2212 \u03b1) )) 4\u03b11\u2212\u03b1 , where D = ln(\n6/\u03b4) 2 2 ( \u03b4 6 \u00b7 ln ( 1+\u03b1 2\u03b1 ))\u2212 4\u03b11\u2212\u03b1 .\nNote that for fixed , \u03b4, when \u03b1 \u2192 0 the above bound approaches the familiar O ( ln(1/\u03b4) 2 ) bound by DKW and Massart for \u03b1 = 0. On the other hand, when \u03b1 \u2192 1 the above bound tends to \u221e, reflecting the fact that the multiplicative variant of Glivenko-Cantelli (\u03b1 = 1) does not hold. Theorem 1.4 is proven in Appendix A.\nNote that the dependency of the above bound on the confidence parameter \u03b4 is polynomial. This contrasts with standard uniform convergence rates, which, due to applications of concentration bounds such as Chernoff/Hoeffding, achieve logarithmic dependencies on \u03b4. These concentration bounds are not applicable in our setting when the CDF values are very small, and we use Markov\u2019s inequality instead. The following example shows that a polynomial dependency on \u03b4 is indeed necessary and is not due to a limitation of our proof.\nExample 1.5. For large n, consider n independent samples x1, . . . , xn from the uniform distribution over [0, 1], and set \u03b1 = 1/2 and = 1. The probability of the event\n\u2203i : xi \u2264 1/n3\nis roughly 1/n2: indeed, the complementary event has probability (1\u2212 1/n3)n \u2248 exp(\u22121/n2) \u2248 1 \u2212 1/n2. When this happens, we have: Fn(1/n3) \u2265 1/n >> 1/n3 + 1/n3/2 = F (1/n3) +[ F (1/n3) ]1/2 . Note that this happens with probability inverse polynomial in n (roughly 1/n2) and not inverse exponential.\nAn application to revenue learning. We demonstrate an application of our Submultiplicative Glivenko-Cantelli Theorem in the context of a widely studied problem in economics and algorithmic game theory: the problem of revenue learning. In the setting of this problem, a seller has to decide which price to post for a good she wishes to sell. Assume that each consumer draws her private valuation for the good from an unknown distribution \u00b5. We envision that a consumer with valuation v will buy the good at any price p \u2264 v, but not at any higher price. This implies that the expected revenue at price p is simply r(p) , p \u00b7 q(p), where q(p) , PrV\u223c\u00b5[V \u2265 p].\nIn the language of machine learning, this problem can be phrased as follows: the examples domain Z , R+ is the set of all valuations v. The hypothesis space H , R+ is the set of all prices p. The revenue (which is a gain, rather than loss) of a price p on a valuation v is the function p \u00b7 1{p\u2264v}.\nThe well-known revenue maximization problem is to find a price p\u2217 that maximizes the expected revenue, given a sample of valuations drawn i.i.d. from \u00b5. In this paper, we consider the more demanding revenue estimation problem: the problem of well-approximating r(p), simultaneously for all prices p, from a given sample of valuations. (This clearly also implies a good estimation of the maximum revenue and of a price that yields it.) More specifically, we address the following question: when do the empirical revenues, rn(p) , p \u00b7 qn(p), where qn(p) , PrV\u223c\u00b5n [V \u2265 p] = 1n \u00b7\n\u2223\u2223{1 \u2264 i \u2264 n : xi \u2265 t}\u2223\u2223, uniformly converge to the true revenues r(p)? More specifically, we would like to show that for some n0, for n \u2265 n0 we have with probability 1\u2212 \u03b4 that \u2223\u2223r(p)\u2212 rn(p)\u2223\u2223 \u2264 .\nThe revenue estimation problem is a basic instance of the more general problem of uniform convergence of empirical estimates. The main challenge in this instance is that the prices are unbounded (and so are the private valuations that are drawn from the distribution \u00b5).\nUnfortunately, there is no (upper) bound on n0 that is only a function of and \u03b4. Moreover, even if we add the expectation of valuations, i.e., E[V ] where V is distributed according to \u00b5, still there is no bound on n0 that is a function of only those three parameters (see Section 2.3 for an example). In contrast, when we consider higher moments of the distribution \u00b5, we are able to derive bounds on the value of n0. These bounds are based on our Submultiplicative Glivenko-Cantelli Bound. Specifically, assume that EV\u223c\u00b5[V 1+\u03b8] \u2264 C for some \u03b8 > 0 and C \u2265 1. Then, we show that for any , \u03b4 \u2208 (0, 1), we have\nPr [ \u2203v : \u2223\u2223r(v)\u2212 rn(v)\u2223\u2223 > ] \u2264 Pr[\u2203v : \u2223\u2223q(v)\u2212 qn(v)\u2223\u2223 > C 1 1+\u03b8 q(v) 1 1+\u03b8 ] .\nThis essentially reduces uniform convergence bounds to our Submultiplicative Glivenko-Cantelli variant. It then follows that there exists n0(C, \u03b8, , \u03b4) such that for any n \u2265 n0, with probability at least 1\u2212 \u03b4,\n\u2200v : \u2223\u2223rn(v)\u2212 r(v)\u2223\u2223 \u2264 .\nWe remark that when \u03b8 is large, our bound yields n0 \u2248 O ( ln(1/\u03b4) 2 ) , which recovers the standard sample complexity bounds obtainable via DKW and Massart. When \u03b8 \u2192 0, our bound diverges to infinity, reflecting the fact (discussed above) that there is no bound on n0 that depends only on , \u03b4, and E[V ]. Nevertheless, we find that E[V ] qualitatively determines whether uniform convergence occurs in the limit. Namely, we show that\n\u2022 If E\u00b5[V ] <\u221e, then almost surely limn\u2192\u221e supv \u2223\u2223r(v)\u2212 rn(v)\u2223\u2223 = 0,\n\u2022 Conversely, if E\u00b5[V ] =\u221e, then almost never limn\u2192\u221e supv \u2223\u2223r(v)\u2212 rn(v)\u2223\u2223 = 0."}, {"heading": "1.1 Related work", "text": "Generalizations of Glivenko-Cantelli. Various generalizations of the Glivenko-Cantelli Theorem were established. These include uniform convergence bounds for more general classes of functions as well as more general loss functions (for example, Vapnik and Chervonenkis, 1971; Vapnik, 1998; Koltchinskii and Panchenko, 2000; Bartlett and Mendelson, 2002). The results that concern unbounded loss functions are most relevant to this work (for example, Cortes et al., 2010, 2013; Vapnik, 1998). We next briefly discuss the relevant results from Cortes et al. (2013) in the context of this paper; more specifically, in the context of Theorem 1.3. To ease presentation, set \u03b1 in this theorem to be 1/2. Theorem 1.3 analyzes the event where the empirical quantile is bounded by2\nqn(p) \u2264 q(p) + \u221a q(p),\nqn(p) \u2265 q(p)\u2212 \u221a q(p).\nwhereas, Cortes et al. (2013) analyzes the event where it is bounded it by: qn(p) \u2264 O\u0303 ( q(p) + \u221a q(p)/n+ 1/n ) ,\nqn(p) \u2265 \u2126\u0303 ( q(p)\u2212 \u221a qn(p)/n\u2212 1/n ) 2For consistency with the canonical statement of the Glivenko-Cantelli theorem, we stated our submultiplicative variants of this theorem with regard to the CDFs Fn and F . However, these results also hold when replacing these CDFs with the respective quantiles (tail CDFs) qn and q. See Section 2.2 for details.\nThus, the main difference is the additive 1/n term in the bound from Cortes et al. (2013). In the context of uniform convergence of revenues, it is crucial to use the upper bound on the empirical quantile as we do, as it guarantees that large prices will not overfit, which is the main challenge in proving uniform convergence in this context. In particular, the upper bound from Cortes et al. (2013) does not provide any guarantee on the revenues of prices p >> n, as for such prices p \u00b7 1/n >> 1.\nIt is also worth pointing out that our lower bound on the empirical quantile implies that with high probability the quantile of the maximum sampled point is at least 1/n2 (or more generally, at least 1/n1/\u03b1 when \u03b1 6= 1/2), while the bound from Cortes et al. (2013) does not imply any non-trivial lower bound.\nAnother, more qualitative difference is that unlike the bounds in Cortes et al. (2013) that apply for general VC classes, our bound is tailored for the class of thresholds (corresponding to CDF/quantiles), and does not extend even to other classes of VC dimension 1 (see the discussion after Theorem 1.3).\nUniform convergence of revenues. The problem of revenue maximization is a central problem in economics and Algorithmic Game Theory (AGT). The seminal work of Myerson (1981) shows that given a valuation distribution for a single good, the revenue-maximizing selling mechanism for this good is a posted-price mechanism. In the recent years, there has been a growing interest in the case where the valuation distribution is unknown, but the seller observes samples drawn from it. Most papers in this direction assume that the distribution meets some tail condition that is considered \u201cnatural\u201d within the algorithmic game theory community, such as boundedness (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016; Gonczarowski and Nisan, 2017; Devanur et al., 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016), or such as a condition known as monotone hazard rate (Huang et al., 2015).4 These papers then go on to derive computation- or sample-complexity bounds on learning an optimal price (or an optimal selling mechanism from a given class) for a distribution that meets the assumed condition.\nA recurring theme in statistical learning theory is that learnability guarantees are derived via a, sometimes implicit, uniform convergence bound. However, this has not been the case in the context of revenue learning. Indeed, while some papers that studied bounded distributions (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016) did use uniform convergence bounds as part of their analysis, other papers, in particular those that considered unbounded distributions, had to bypass the usage of uniform convergence by more specialized arguments. This is due to the fact that many unbounded distributions do not satisfy any uniform convergence bound. As a concrete example, the (unbounded, Myerson-regular) equal revenue distribution5 has an infinite expectation and therefore, by our Theorem 2.3, satisfies no uniform convergence, even in the limit. Thus, it turns out that the works that studied the popular class of Myerson-regular distributions (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016) indeed could not have hoped to establish learnability via a uniform convergence argument. For instance, the way Dhangwatnotai et al. (2015) and Cole and Roughgarden (2014)\n3The analysis of Balcan et al. (2016) assumes a bound on the realized revenue (from any possible valuation profile) of any mechanism/auction in the class that they consider. For the class of posted-price mechanisms, this is equivalent to assuming a bound on the support of the valuation distribution. Indeed, for any valuation v, pricing at v gives realized revenue v (from the valuation v), and so unbounded valuations (together with the ability to post unbounded prices) imply unbounded realized revenues.\n4Both Myerson-regularity and monotone hazard rate are conditions on the second derivative of the revenue as a function of the quantile of the underlying distribution. In particular, they impose restrictions on the tail of the distribution.\n5This is a distribution that satisfies the special property that all prices have the same expected revenue.\nestablish learnability for Myerson-regular distributions is by considering the guarded ERM algorithm (an algorithm that chooses an empirical revenue maximizing price that is smaller than, say, the \u221a nth largest sampled price), and proving a uniform convergence bound, not for all prices, but only for prices that are, say, smaller than the \u221a nth largest sampled price, and then arguing that larger prices are likely to have a small empirical revenue, compared to the guarded empirical revenue maximizer. This means that the guarded ERM will output a good price, but it does not (and cannot) imply uniform convergence for all prices.\nWe complement the extensive literature surveyed above in a few ways. The first is generalizing the revenue maximization problem to a revenue estimation problem, where the goal is to uniformly estimate the revenue of all possible prices, when no bound on the possible valuations is given (or even exists). The problem of revenue estimation arises naturally when the seller has additional considerations when pricing her good, such as regulations that limit the price choice, bad publicity if the price is too high (or, conversely, damage to prestige if the price is too low), or willingness to suffer some revenue loss for better market penetration (which may translate to more revenue in the future). In such a case, the seller may wish to estimate the revenue loss due to posting a discounted (or inflated) price.\nThe second, and most important, contribution to the above literature is that we consider arbitrary distributions rather than very specific and limited classes of distributions (e.g., bounded, Myerson-regular, monotone hazard rate, etc.). Third, we derive finite sample bounds in the case that the expected valuation is bounded for some moment larger than 1. We further derive a zero-one law for uniform convergence in the limit that depends on the finiteness of the first moment. Technically, our bounds are based on an additive error rather than multiplicative ones, which are popular in the AGT community."}, {"heading": "1.2 Paper organization", "text": "The rest of the paper is organized as follows. We begin by presenting the application of our Submultiplicative Glivenko-Cantelli to revenue estimation in Section 2. In Section 3, we prove the Submultiplicative Glivenko-Cantelli variant, and discuss some extensions of it. Section 4 contains a discussion and possible directions of future work. Some of the proofs are deferred to the appendices."}, {"heading": "2 Uniform Convergence of Empirical Revenues", "text": "In this section we demonstrate an application of our Submultiplicative Glivenko-Cantelli variant by establishing uniform convergence bounds for a family of unbounded random variables in the context of revenue estimation."}, {"heading": "2.1 Model", "text": "Consider a good g that we wish to post a price for. Let V be a random variable that models the valuation of a random consumer for g. Technically, it is assumed that V is a nonnegative random variable, and we denote by \u00b5 its induced distribution over R+. A consumer who values g at a valuation v is willing to buy the good at any price p \u2264 v, but not at any higher price. This implies that the realized revenue to the seller from a (posted) price p is the random variable p \u00b7 1{p\u2264V }. The quantile of a value v \u2208 R+ is\nq(v) = q(v;\u00b5) , \u00b5 ( {x : x \u2265 v} ) .\nThis models the fraction of the consumers in the population that are willing to purchase the good if priced at v. The expected revenue from a (posted) price p \u2208 R+ is\nr(p) = r(p;\u00b5) , E \u00b5\n[ p \u00b7 1{p\u2264V } ] = p \u00b7 q(p).\nLet V1, V2, . . . be a sequence of i.i.d. valuations drawn from \u00b5, and let v1, v2, . . . be their realizations. The empirical quantile of a value v \u2208 R+ is\nqn(v) = q(v;\u00b5n) , 1n \u00b7 \u2223\u2223{1 \u2264 i \u2264 n : vi \u2265 v}\u2223\u2223.\nThe empirical revenue from a price p \u2208 R+ is\nrn(p) = r(p;\u00b5n) , E \u00b5n\n[ p \u00b7 1{p\u2264V } ] = p \u00b7 qn(p).\nThe revenue estimation error for a given sample of size n is\nn , sup p \u2223\u2223rn(p)\u2212 r(p)\u2223\u2223. It is worth highlighting the difference between revenue estimation and revenue maximization. Let p\u2217 be a price that maximizes the revenue, i.e., p\u2217 \u2208 arg supp r(p). The maximum revenue is r\u2217 = r(p\u2217). The goal in many works in revenue maximization is to find a price p\u0302 such that r\u2217 \u2212 r(p\u0302) \u2264 , or alternatively, to bound r\u2217/r(p\u0302).\nGiven a revenue-estimation error n, one can clearly maximize the revenue within an additive error of 2 n by simply posting a price p \u2217 n \u2208 arg maxp rn(p), thereby attaining revenue r\u2217n = r(p\u2217n). This follows since\nr\u2217n = r(p \u2217 n) \u2265 rn(p\u2217n)\u2212 n \u2265 rn(p\u2217)\u2212 n \u2265 r(p\u2217)\u2212 2 n = r\u2217 \u2212 2 n.\nTherefore, good revenue estimation implies good revenue maximization. We note that the converse does not hold. Namely, there are distributions for which revenue maximization is trivial but revenue estimation is impossible. One such case is the equal revenue distribution, where all values in the support of \u00b5 have the same expected revenue. For such distributions, the problem of revenue maximization becomes trivial, since any posted price is optimal. However, as follows from Theorem 2.3, since the expected revenue of such distributions is infinite, almost never do the empirical revenues uniformly converge to the true revenues."}, {"heading": "2.2 Quantitative bounds on the uniform convergence rate", "text": "Recall that we are interested in deriving sample bounds that would guarantee uniform convergence for the revenue estimation problem. We will show that given an upper bound on the kth moment of V for some k > 1, we can derive a finite sample bound. To this end we utilize our Submultiplicative Glivenko-Cantelli Bound (Theorem 1.4).\nWe also consider the case of k = 1, namely that E[V ] is bounded, and show that in this case there is still uniform convergence in the limit, but that there cannot be any guarantees on the convergence rate. Interestingly, it turns out that E[V ] < \u221e is not only sufficient but also necessary so that in the limit, the empirical revenues uniformly converge to the true revenues (see Section 2.3).\nWe begin by showing that bounds on the kth moment for k > 1 yield explicit bounds on the convergence rate. It is convenient to parametrize by setting k = 1 + \u03b8, where \u03b8 > 0.\nTheorem 2.1. Let EV\u223c\u00b5[V 1+\u03b8] \u2264 C for some \u03b8 > 0 and C \u2265 1, and let , \u03b4 \u2208 (0, 1). Set6\nn0 = O\u0303\n( ln(1/\u03b4)\n2 C\n2 1+\u03b8\n( 6 \u00b7 C 1 1+\u03b8\n\u03b4 ln ( 1 + \u03b8/2 ))4/\u03b8). (1) For any n \u2265 n0, with probability at least 1\u2212 \u03b4,\n\u2200v : \u2223\u2223rn(v)\u2212 r(v)\u2223\u2223 \u2264 .\n6The O\u0303 conceals low order terms.\nNote that when \u03b8 is large, this bound approaches the standard O ( ln(1/\u03b4) 2 ) sample complexity bound of the additive Glivenko-Cantelli. For example, if all moments are uniformly bounded, then the convergence is roughly as fast as in standard uniform convergence settings (e.g., VCdimension based bounds).\nThe proof of Theorem 2.1 follows from Theorem 1.4 and the next proposition, which reduces bounds on the uniform convergence rate of the empirical revenues to our Submultiplicative Glivenko-Cantelli.\nProposition 2.2. Let EV\u223c\u00b5[V 1+\u03b8] \u2264 C for some \u03b8 > 0 and C \u2265 1, and let , \u03b4 \u2208 (0, 1). Then,\nPr [ \u2203v : \u2223\u2223r(v)\u2212 rn(v)\u2223\u2223 > ] \u2264 Pr[\u2203v : \u2223\u2223q(v)\u2212 qn(v)\u2223\u2223 > C 1 1+\u03b8 q(v) 1 1+\u03b8 ] .\nThus, to prove Theorem 2.1, we first note that Theorem 1.4 (as well as Theorem 1.3) also holds when Fn and F are respectively replaced in the definition of n0 with qn and q (indeed, applying Theorem 1.4 to the measure \u00b5\u2032 defined by \u00b5\u2032(A) , \u00b5 ( {\u2212a | a \u2208 A} ) yields the required result with regard to the measure \u00b5). We then plug \u2190 C 1 1+\u03b8\nand \u03b1\u2190 11+\u03b8 into this variant of Theorem 1.4 to yield a bound on the right-hand side of the inequality in Proposition 2.2, whose application concludes the proof.\nProof of Proposition 2.2. By Markov\u2019s inequality:\nq(v) = Pr[V \u2265 v] = Pr[V 1+\u03b8 \u2265 v1+\u03b8] \u2264 C v1+\u03b8 . (2)\nNow, Pr [ \u2203v : \u2223\u2223r(v)\u2212 rn(v)\u2223\u2223 > ] = Pr[\u2203v : \u2223\u2223v \u00b7 q(v)\u2212 v \u00b7 qn(v)\u2223\u2223 > ] = Pr [ \u2203v :\n\u2223\u2223v \u00b7 q(v)\u2212 v \u00b7 qn(v)\u2223\u2223 > (v1+\u03b8 \u00b7 q(v)) 1 1+\u03b8 (v1+\u03b8 \u00b7 q(v)) 1 1+\u03b8 ] \u2264 Pr [ \u2203v :\n\u2223\u2223v \u00b7 q(v)\u2212 v \u00b7 qn(v)\u2223\u2223 > C 1 1+\u03b8 (v1+\u03b8 \u00b7 q(v)) 1 1+\u03b8 ] = Pr [ \u2203v :\n\u2223\u2223q(v)\u2212 qn(v)\u2223\u2223 > C 1 1+\u03b8 q(v) 1 1+\u03b8 ] .\nwhere the inequality follows from Equation (2)."}, {"heading": "2.3 A qualitative characterization of uniform convergence", "text": "The sample complexity bounds in Theorem 2.1 are meaningful as long as \u03b8 > 0, but deteriorate drastically as \u03b8 \u2192 0. Indeed, as the following example shows, there is no bound on the uniform convergence sample complexity that depends only on the first moment of V , i.e., its expectation.\nConsider a distribution \u03b7p so that with probability p we have V = 1/p and otherwise V = 0. Clearly, E[V ] = 1. However, we need to sample mp = O(1/p) valuations to see a single nonzero value. Therefore, there is no bound on the sample size mp as a function of the expectation, which is simply 1.\nWe can now consider the higher moments of \u03b7p. Consider the kth moment, for k = 1 + \u03b8 and \u03b8 > 0, so k > 1. For this moment, we have Ap,\u03b8 = E[V 1+\u03b8] = p\u03b8/(1+\u03b8), which implies that mp = O ( 1/(Ap,\u03b8) (1+\u03b8)/\u03b8 ) . This does allow us to bound mp as a function of \u03b8 and E[V 1+\u03b8], but for small \u03b8 we have a huge exponent of approximately 1/\u03b8. While the above examples show that there cannot be a bound on the sample size as a function of the expectation of the value, it turns out that there is a very tight connection between the first moment and uniform convergence:\nTheorem 2.3. The following dichotomy holds for a distribution \u00b5 on R+: 1. If E\u00b5[V ] <\u221e, then almost surely limn\u2192\u221e supv \u2223\u2223r(v)\u2212 rn(v)\u2223\u2223 = 0.\n2. If E\u00b5[V ] =\u221e, then almost never limn\u2192\u221e supv \u2223\u2223r(v)\u2212 rn(v)\u2223\u2223 = 0.\nThat is, the empirical revenues uniformly converge to the true revenues if and only if E\u00b5[V ] <\u221e.\nWe use the following basic fact in the Proof of Theorem 2.3:\nLemma 2.4. Let X be a nonnegative random variable. Then\n\u221e\u2211 n=1 Pr[X \u2265 n] \u2264 E[X] \u2264 \u221e\u2211 n=0 Pr[X \u2265 n].\nProof. Note that: \u221e\u2211 n=1 1{X\u2265n} = bXc \u2264 X \u2264 bXc+ 1 = \u221e\u2211 n=0 1{X\u2265n}.\nThe lemma follows by taking expectations. Proof of Theorem 2.3. We start by proving item 2. Let \u00b5 be a distribution such that E\u00b5 [ V ]\n= \u221e. If supv v \u00b7 q(v) =\u221e then for every realization v1, . . . , vn there is some v \u2265 max{v1, . . . , vn} such that v \u00b7 q(v) \u2265 1, but v \u00b7 qn(v) = 0. So, we may assume supv v \u00b7 q(v) <\u221e. Without loss of generality we may assume that supv v\u00b7q(v) = 1/2 by rescaling the distribution if needed. Consider the sequence of events E1, E2, . . . where En denotes the event that Vn \u2265 n. Since E\u00b5 [ V ]\n=\u221e, Lemma 2.4 implies that \u2211\u221e n=1 Pr[En] = \u221e. Thus, since these events are independent, the second Borel-Cantelli Lemma (Borel, 1909; Cantelli, 1917) implies that almost surely, infinitely many of them occur and so infinitely often\nVn \u00b7 qn(Vn) \u2265 1 \u2265 Vn \u00b7 q(Vn) + 12 .\nTherefore, the probability that v \u00b7 qn(v) uniformly converge to v \u00b7 q(v) is 0. Item 1 follows from the following monotone domination theorem:\nTheorem 2.5. Let F be a family of nonnegative monotone functions, and let F be an upper envelope7 for F . If E\u00b5[F ] <\u221e, then almost surely:\nlim n\u2192\u221e sup f\u2208F \u2223\u2223E \u00b5 [f ]\u2212 E \u00b5n [f ] \u2223\u2223 = 0.\nIndeed, item 1 follows by plugging F = { v \u00b7 1x\u2265v : v \u2208 R+ } , which is uniformly bounded by\nthe identity function F (x) = x. Now, by assumption E\u00b5[F ] <\u221e, and therefore, almost surely\nlim n\u2192\u221e sup v\u2208R+\n\u2223\u2223r(v)\u2212 rn(v)\u2223\u2223 = lim n\u2192\u221e sup f\u2208F \u2223\u2223E \u00b5 [f ]\u2212 E \u00b5n [f ] \u2223\u2223 = 0.\nTheorem 2.5 follows by known results in the theory of empirical processes (for example, with some work it can be proved using Theorem 2.4.3 from Vaart and Wellner (1996)). For completeness, we give a short and basic proof in Appendix B.\n7F is an upper envelope for F if F (v) \u2265 f(v) for every v \u2208 V and f \u2208 F ."}, {"heading": "3 Submultiplicative Glivenko-Cantelli", "text": "In this section \u00b5 is a fixed but otherwise arbitrary distribution, with CDF F and empirical CDF Fn.\nTheorem 1.3 is a corollary of the following lemma, which gives a quantitative bound on the confidence parameter \u03b4.\nLemma 3.1. Let n \u2208 N, > 0 and \u03b1, p, q \u2208 (0, 1). Assume that n \u2265 \u2212 1\n1\u2212\u03b1 and p \u2264 min{ 1 1\u2212\u03b1 , 1/e}. Then,\nPr [ \u2203t : \u2223\u2223F (t)\u2212 Fn(t)\u2223\u2223 > \u00b7 F (t)\u03b1] \u2264 q + \u2308 ln ln(nq ) ln(1+\u03b12\u03b1 ) \u2309 p 1\u2212\u03b1 2 + 2 exp ( \u22122n( p\u03b1)2 ) .\nNote that p and q appear only on the right-hand side, and therefore can be \u201ctuned\u201d in order to minimize the upper bound. Our proof of Lemma 3.1 uses Theorem 1.2. To better understand the parameters, we state the following corollary (whose first item is stronger than Theorem 1.3).\nCorollary 3.2. There are constants c1, c2 > 0 so that the following holds.\n1. If \u03b1(n) \u2264 1\u2212 c1 \u00b7 ln ln(n)ln(n) , then the probability of the event\n\u2200t : \u2223\u2223F (t)\u2212 Fn(t)\u2223\u2223 \u2264 \u00b7 F (t)\u03b1(n)\ntends to 1 as n tends to \u221e.\n2. If \u03b1(n) \u2265 1\u2212 c2 \u00b7 1ln(n) and \u00b5 is uniform over [0, 1], then the probability of the event\n\u2200t : \u2223\u2223F (t)\u2212 Fn(t)\u2223\u2223 \u2264 1\n10 \u00b7 F (t)\u03b1(n)\nis at most 1/2, for all n \u2265 2.\nWe leave as an open question to determine the behavior of these probabilities when \u03b1(n) \u2208 [ 1\u2212 c1 \u00b7 ln ln(n)\nln(n) , 1\u2212 c2 \u00b7\n1\nln(n)\n] .\nCorollary 3.2 is proven in Appendix C.\nProof of Lemma 3.1. Let , \u03b1, q, p, n be as in the statement of the lemma. We partition the event in question to three parts, depending on the value of t as follows. Partition R to\nI[0,q/n] = { t \u2208 R : 0 \u2264 F (t) \u2264 q\nn\n} , I(q/n,p) = { t \u2208 R : q\nn < F (t) \u2264 p } and\nI[p,1] = {t \u2208 R : p < F (t) \u2264 1} .\nThere are three corresponding events E[0,q/n], E(q/n,p) and E[p,1]; for example, E[0,q/n] is the event that \u2203t \u2208 I[0,q/n] : B(t) = 1, where B(t) is the indicator of \u2223\u2223F (t)\u2212 Fn(t)\u2223\u2223 > \u00b7 F (t)\u03b1. The following three claims bound from above the probabilities of these three events. The three claims and the union bound complete the proof of the theorem.\nClaim 3.3. Pr [ E[0,q/n] ] \u2264 q.\nProof. Let t \u2208 I[0,q/n] be so that B(t) = 1. For any t \u2208 I[0,q/n] we have that F (t) \u2264 q/n \u2264 1/n \u2264 1 1\u2212\u03b1 , where the last inequality is by our assumption on n and . This implies that\nF (t) \u2264 F (t)\u03b1. Since B(t) = 1 it must be the case that Fn(t) > F (t) + ( F (t) )\u03b1 \u2265 0, and therefore at least one sample xi satisfies xi \u2264 t \u2264 q/n. Now, by the union bound,\nPr [ E[0,q/n] ] \u2264 Pr [ \u2203i \u2208 [n] : xi \u2208 I[0,q/n] ] \u2264 n \u00b7 q\nn = q.\nClaim 3.4. Pr [ E(q/n,p) ] \u2264 \u2308 ln ln(n q )\nln( 1+\u03b1 2\u03b1 )\n\u2309 p 1\u2212\u03b1 2\n.\nProof. If q/n \u2265 p then this event is empty and its probability is 0. Therefore, assume that q/n < p, and that this event is not empty.\nFor all t \u2208 I(q/n,p) since F (t) \u2264 p \u2264 1\n1\u2212\u03b1 we have F (t) \u2212 F (t)\u03b1 \u2264 0. So, it suffices to consider the event\n\u2203t \u2208 I(q/n,p) : Fn(t) > F (t) + \u00b7 F (t)\u03b1.\nConsider the decreasing sequence of numbers p0, p1, . . . , pm defined by\npi = p\n( 1+\u03b1 2\u03b1 )i ,\nwhere m is such that pm < q/n \u2264 pm\u22121. Since p \u2264 1/e, we can bound m \u2264 \u2308 ln ln(n q )\nln( 1+\u03b1 2\u03b1 )\n\u2309 . Let\nti = inf { t \u2208 I(q/n,p) : F (t) \u2265 pi } .\nLet F\u2212n (t) = \u00b5n ( {x : x < t} ) . We claim that\n\u2203t \u2208 I(q/n,p) : B(t) = 1 =\u21d2 \u2203i < m : F\u2212n (ti) > \u00b7 p\u03b1i+1,\nIndeed, assume that t \u2208 I(q/n,p) satisfies Fn(t) > F (t) + \u00b7 F (t)\u03b1. Since pm < t < p0, there is some 0 \u2264 i \u2264 m\u22121 such that pi+1 \u2264 F (t) < pi. Note that ti+1 \u2264 t < ti. Indeed, ti+1 \u2264 t follows since pi+1 \u2264 F (t), and t < ti follows since F (ti) \u2265 pi (which is implied by right continuity of F ). Hence,\nF\u2212n (ti) \u2265 Fn(t) > F (t) + \u00b7 F (t)\u03b1 \u2265 pi+1 + \u00b7 p\u03b1i+1 \u2265 \u00b7 p\u03b1i+1.\nIt hence remains to upper bound the union of these events. Note that E [ F\u2212n (ti) ] = \u00b5 ( {x : x < ti} ) \u2264 pi.\nTherefore, by Markov\u2019s inequality:\nPr [ F\u2212n (ti) > \u00b7 p\u03b1i+1 ] \u2264 pi p\u03b1i+1 = 1 p ( 1\u2212\u03b1 2 )( 1+\u03b1 2\u03b1 )i .\nBy the union bound,\nPr [ \u2203i < m : Fn(ti) > pi+1 + \u00b7 p\u03b1i+1 ] \u2264 1 m\u22121\u2211 i=0 p ( 1\u2212\u03b1 2 )( 1+\u03b1 2\u03b1 )i \u2264 m p 1\u2212\u03b1 2 \u2264 \u2308 ln ln(nq ) ln(1+\u03b12\u03b1 ) \u2309 p 1\u2212\u03b1 2 .\nClaim 3.5. Pr [ E[p,1] ] \u2264 2 exp ( \u22122n( p\u03b1)2 ) .\nProof. For all t \u2208 I[p,1] we have F (t)\u03b1 \u2265 p\u03b1. The claim follows by Theorem 1.2.\nLemma 3.1 follows from combining Claims 3.3, 3.4, and 3.5."}, {"heading": "4 Discussion", "text": "Our main result is a submultiplicative variant of the Glivenko-Cantelli Theorem, which allows for tighter convergence bounds for extreme values of the CDF. We show that for the revenue learning setting our submultiplicative bound can be used to derive uniform convergence sample complexity bounds, assuming a finite bound on the kth moment of the valuations, for any (possibly fractional) k > 1. For uniform convergence in the limit, we give a complete characterization, where uniform convergence almost surely occurs if and only if the first moment is finite.\nIt would be interesting to find other applications of our submultiplicative bound in other settings. A potentially interesting direction is to consider unbounded loss functions (e.g., the squared-loss, or log-loss). Many works circumvent the unboundedness in such cases by ensuring (implicitly) that the losses are bounded, e.g., through restricting the inputs and the hypotheses. Our bound offers a different perspective of addressing this issue. In this paper we consider revenue learning, and replace the boundedness assumption by assuming bounds on higher moments. An interesting challenge is to prove uniform convergence bounds for other practically interesting settings. One such setting might be estimating the effect of outliers (which correspond to the extreme values of the loss).\nIn the context of revenue estimation, this work only considers the most na\u0308\u0131ve estimator, namely of estimating the revenues by the empirical revenues. One can envision other estimators, for example ones which regularize the extreme tail of the sample. Such estimators may have a potential of better guarantees or better convergence bounds. In the context of uniform convergence of selling mechanism revenues, this work only considers the basic class of postedprice mechanisms. While for one good and one valuation distribution, it is always possible to maximize revenue via a selling mechanism of this class, this is not the case in more complex auction environments. While in many more-complex environments, the revenue-maximizing mechanism/auction is still not understood well enough, for environments where it is understood (Cole and Roughgarden, 2014; Devanur et al., 2016; Gonczarowski and Nisan, 2017) (as well as for simple auction classes that do not necessarily contain a revenue-maximizing auction (Morgenstern and Roughgarden, 2016; Balcan et al., 2016)) it would also be interesting to study relaxations of the restrictive tail or boundedness assumptions currently common in the literature."}, {"heading": "A Proof of Theorem 1.4", "text": "Proof. Let , \u03b4 \u2264 1/4 and \u03b1 < 1. By Lemma 3.1,\nPr [ \u2203t : \u2223\u2223F (t)\u2212 Fn(t)\u2223\u2223 > \u00b7 F (t)\u03b1] \u2264 q + \u2308 ln ln(nq ) ln(1+\u03b12\u03b1 ) \u2309 p 1\u2212\u03b1 2 + 2 exp ( \u22122n( p\u03b1)2 ) (3)\nfor every q \u2264 1, n \u2265 \u2212 1 1\u2212\u03b1 and p \u2264 1\n1\u2212\u03b1 . Set q, p so that each of the first two summands in Equation 3 is at most \u03b4. Specifically,\nq = \u03b4, and\n1. if ln ln(n \u03b4 )\nln( 1+\u03b1 2\u03b1\n) \u2265 1 then set p =\n( \u03b4 \u00b7 ln ( 1+\u03b1 2\u03b1 ) 2 ln ln(n\n\u03b4 )\n) 2 1\u2212\u03b1\n, and\n2. if ln ln(n \u03b4 )\nln( 1+\u03b1 2\u03b1\n) < 1 then set p = ( \u03b4)\n2 1\u2212\u03b1 .\nNote that indeed the requirements n \u2265 \u2212 1 1\u2212\u03b1 and p \u2264 1\n1\u2212\u03b1 are satisfied by p and by the desired n (from the theorem statement).\nPlugging these p and q in the third summand in Equation 3 yields:\n2 exp \u22122n 2( \u03b4 \u00b7 ln(1+\u03b12\u03b1 ) 2 ln ln(n\u03b4 ) ) 4\u03b1 1\u2212\u03b1 \nwhen ln ln(n \u03b4 )\nln( 1+\u03b1 2\u03b1\n) \u2265 1, or\n2 exp ( \u22122n 2( \u03b4) 4\u03b1 1\u2212\u03b1 ) otherwise. In order for the above to be at most \u03b4, it suffices that\n2n 2 ( \u03b4 \u00b7 ln ( 1+\u03b1 2\u03b1 ) 2 ln ln(n\u03b4 ) ) 4\u03b1 1\u2212\u03b1 \u2265 ln(2/\u03b4)\nwhen ln ln(n \u03b4 )\nln( 1+\u03b1 2\u03b1\n) \u2265 1, or\n2n 2( \u03b4) 4\u03b1 1\u2212\u03b1 \u2265 ln(2/\u03b4)\notherwise. The second case implies an explicit bound of\nn \u2265 ln( 2/\u03b4)\n2 2 ( \u03b4)\u2212 4\u03b1 1\u2212\u03b1 . (4)\nTo get an explicit bound on n in the first case, we need to solve a recursion of the following type: find a lower bound on n so that the following inequality holds:\nn \u2265 D ( ln ln(E \u00b7 n) )F ,\nwhere D \u2265 0, E \u2265 4, F \u2265 0. (Here D = ln(2/\u03b4) 2 2 ( \u03b4 2 \u00b7 ln ( 1+\u03b1 2\u03b1 ))\u2212 4\u03b11\u2212\u03b1 , E = 1\u03b4 , F = 4\u03b1 1\u2212\u03b1 .) Setting\nn \u2265 (D + 1) ( 10 ( ln(D + 4) + ln(F + 4) + ln(E) ))F = (D + 1) ( 10 \u00b7 ln ( 4 \u00b7 D + 4\n\u03b4(1\u2212 \u03b1)\n)) 4\u03b11\u2212\u03b1 (5)\nsuffices. Therefore, the probability (i.e., the sum of all three summands of Equation 3) is bounded by 3\u03b4. Replacing \u03b4 by \u03b4/3 in Equations 4 and 5 (and in the definition of D) yields the desired bound on n0( , \u03b4, \u03b1)."}, {"heading": "B Proof of Theorem 2.5", "text": "Proof. Let > 0. Having E\u00b5[F ] <\u221e implies that there is v0 \u2208 R+ such that E\u00b5 [ 1{V\u2265v0}F ] < . Since we can write E \u00b5 [f ] = E \u00b5 [ f \u00b7 1{V\u2264v0} ] + E \u00b5 [ f \u00b7 1{V >v0}\n] it suffices to show that almost surely there exist n1 such that\n(\u2200n \u2265 n1) (\u2200f \u2208 F) : \u2223\u2223\u2223E \u00b5 [ f \u00b7 1{V\u2265v0} ] \u2212 E \u00b5n [ f \u00b7 1{V\u2265v0} ]\u2223\u2223\u2223 \u2264 2 (6) and that almost surely there exist n2 such that\n(\u2200n \u2265 n2) (\u2200f \u2208 F) : \u2223\u2223\u2223E \u00b5 [ f \u00b7 1{V <v0} ] \u2212 E \u00b5n [ f \u00b7 1{V\u2264v0} ]\u2223\u2223\u2223 \u2264 3 . (7) We begin by showing Equation (6): the law of large numbers implies that almost surely,\nthere exists n1 such that E\u00b5n [ 1{V\u2265v0}F ] < 2 , for every n \u2265 n1. Since every f \u2208 F satisfies\n0 \u2264 f \u2264 F , it follows that 0 \u2264 E\u00b5 [ 1{V\u2265v0}f ] < , and 0 \u2264 E\u00b5n [ 1{V\u2265v0}f ] < 2 for n \u2265 n1. This implies Equation (6). It remains to show Equation (7): set \u2032 = F (v0)+1 . The Glivenko-Cantelli Theorem implies that almost surely there exists n2 such that\n(\u2200n \u2265 n2) (\u2200v \u2208 R+) : \u2223\u2223q(v)\u2212 qn(v)\u2223\u2223 \u2264 \u2032.\nLet f \u2208 F . By monotonicity of f , it follows that there is a sequence 0 = a0, a1, . . . , aN = v0 such that f does not change by more than within each interval [ai, ai+1), (i.e., supx,y\u2208[ai,ai+1) \u2223\u2223f(x)\u2212 f(y)\n\u2223\u2223 < ). Consider the piecewise constant function f = f(a0) +\n\u2211 i ( f(ai+1)\u2212 f(ai) ) 1{V\u2265ai}.\nNote that f gets the value f(ai) on each interval [ai, ai+1). Thus, \u2223\u2223f(v)\u2212 f (v)\u2223\u2223 \u2264 for every\nv \u2264 v0. Therefore, \u2223\u2223E\u00b5[f1{V <v0}]\u2212E\u00b5[f 1{V <v0}]\u2223\u2223 \u2264 and \u2223\u2223E\u00b5n [f ]\u2212E\u00b5n [f ]\u2223\u2223 \u2264 . So, it suffices to show that (\u2200n \u2265 n2) : \u2223\u2223\u2223E \u00b5 [ f 1{V <v0} ] \u2212 E \u00b5n [ f 1{V <v0}\n]\u2223\u2223\u2223 \u2264 . Indeed, for n \u2265 n2:\u2223\u2223\u2223E\n\u00b5\n[ f 1{V <v0} ] \u2212 E \u00b5n [ f 1{V <v0} ]\u2223\u2223\u2223 \u2264\u2211 i ( f(ai+1)\u2212 f(ai) ) \u00b7 \u2223\u2223q(ai)\u2212 qn(ai)\u2223\u2223\n\u2264 \u2211 i ( f(ai+1)\u2212 f(ai) ) \u00b7 \u2032 (by definition of n2) \u2264 f(v0) \u00b7 \u2032 \u2264 . (by definition of \u2032)"}, {"heading": "C Proof of Corollary 3.2", "text": "Proof. We begin with the first item. Let \u03b4 > 0. It suffices to prove that Pr [ \u2200t : \u2223\u2223F (t)\u2212 Fn(t)\u2223\u2223 \u2264 \u00b7 F (t)\u03b1(n)] \u2264 3\u03b4\nfor a large enough n. To this end, we set q, p so that each of the first two summands in Lemma 3.1 is at most \u03b4. Specifically,\nq = \u03b4\nand\np =\n \u03b4 ln ( 1+\u03b1 2\u03b1 ) ln ln ( n \u03b4 ) + ln ( 1+\u03b1 2\u03b1 )  2 1\u2212\u03b1 .\nAs required by the premise of Lemma 3.1, p \u2264 1 1\u2212\u03b1 . (The other requirement, n \u2265 \u2212 1\n1\u2212\u03b1 , will be verified at the end of the proof.)\nPlugging these values for p, q, the last summand becomes\n2 exp ( \u22122n( p\u03b1)2 ) = 2 exp \u22122n 2  \u03b4 ln ( 1+\u03b1 2\u03b1 ) ln ln ( n \u03b4 ) + ln ( 1+\u03b1 2\u03b1 )  4\u03b1 1\u2212\u03b1  .\nWe need to verify that the above expression becomes less than \u03b4 for large n. Equivalently, that\nlim n\u2192\u221e n\n \u03b4 ln ( 1+\u03b1 2\u03b1 ) ln ln ( n \u03b4 ) + ln ( 1+\u03b1 2\u03b1 )  4\u03b1 1\u2212\u03b1 =\u221e.\nRewriting \u03b1 = 1\u2212 \u03b2 gives\nlim n\u2192\u221e n\n \u03b4 ln ( 1 + \u03b22\u22122\u03b2 ) ln ln ( n \u03b4 ) + ln ( 1 + \u03b22\u22122\u03b2 )  4\u22124\u03b2 \u03b2 =\u221e.\nSince we are focusing on small value of \u03b2 we can assume that \u03b2 \u2265 1/2. Using that x/2 \u2264 ln(1 + x) \u2264 x for x \u2208 [0, 1] and \u03b2 \u2264 1/2, it suffices that we show\nlim n\u2192\u221e n\n( \u03b4\u03b2/2\nln ln(n/\u03b4) + 1\n) 1 \u03b2\n=\u221e,\nor, by taking \u201cln\u201d, that\nlim n\u2192\u221e\n( lnn\u2212 1\n\u03b2\n( ln(1/ ) + ln(1/\u03b4) + ln(4/\u03b2) + ln ( ln ln(n/\u03b4) + 1 ))) =\u221e.\nTo this end, it suffices that 1/\u03b2 ln(1/\u03b2) \u2264 ln(n)/2, which holds for \u03b2 \u2265 c \u00b7 ln ln(n)/ln(n), where c is a sufficiently large constant.\nIt remains to check that the condition stated in Lemma 3.1, that n \u2265 \u2212 1 1\u2212\u03b1 = \u2212 1 \u03b2 , is\nsatisfied. Indeed, for a sufficiently large n\n\u2212 1 \u03b2 \u2264 1/ c\u00b7ln(n)/ln ln(n) = exp ( c ln(1/ ) \u00b7 ln(n)/ln ln(n) ) < exp ( ln(n) ) = n.\nFor the second item, let Y1 \u2264 Y2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 Yn denote the sequence obtained by sorting X1, X2, . . . , Xn. Note that it suffices to show that the probability that\nY1 < 1\n2n\nis at least 1/2: indeed, this event implies that\nFn\n( 1\n2n\n) \u2212 F ( 1\n2n ) \u2265 1 n \u2212 1 2n\n= 1\n2n \u2265 1 10 \u00b7 ( 1 2n )1\u2212 1 2 lnn\n(since n \u2265 2)\n= 1 10 \u00b7 F ( 1 2n )1\u2212 1 2 lnn ,\nwhich implies the conclusion with c2 = 1/2. Thus, it remains to show that with probability of at least 12 , we have Y1 < 1 2n :\nPr [ Y1 \u2265 1\n2n\n] = Pr [ \u2200i \u2264 n : Xi \u2265 1\n2n\n] = ( 1\u2212 1\n2n\n)n \u2265 1\n2 ."}], "references": [{"title": "Vitercik. Sample complexity of automated mechanism design", "author": ["M.-F. Balcan", "T. Sandholm"], "venue": "In Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Balcan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2016}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Les probabilit\u00e9s d\u00e9nombrables et leurs applications arithm\u00e9tiques", "author": ["\u00c9. Borel"], "venue": "Rendiconti del Circolo Matematico di Palermo (1884-1940),", "citeRegEx": "Borel.,? \\Q1909\\E", "shortCiteRegEx": "Borel.", "year": 1909}, {"title": "Sulla probabilit\u00e1 come limite della frequenza", "author": ["F.P. Cantelli"], "venue": "Atti Accad. Naz. Lincei,", "citeRegEx": "Cantelli.,? \\Q1917\\E", "shortCiteRegEx": "Cantelli.", "year": 1917}, {"title": "Sulla determinazione empirica delle leggi di probabilita", "author": ["F.P. Cantelli"], "venue": "Giornalle dell\u2019Istituto Italiano degli Attuari,", "citeRegEx": "Cantelli.,? \\Q1933\\E", "shortCiteRegEx": "Cantelli.", "year": 1933}, {"title": "The sample complexity of revenue maximization", "author": ["R. Cole", "T. Roughgarden"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Cole and Roughgarden.,? \\Q2014\\E", "shortCiteRegEx": "Cole and Roughgarden.", "year": 2014}, {"title": "Learning bounds for importance weighting", "author": ["C. Cortes", "Y. Mansour", "M. Mohri"], "venue": "In Proceedings of the 24th Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Relative deviation learning bounds and generalization with unbounded loss functions", "author": ["C. Cortes", "S. Greenberg", "M. Mohri"], "venue": "CoRR, abs/1310.5796,", "citeRegEx": "Cortes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2013}, {"title": "The sample complexity of auctions with side information", "author": ["N.R. Devanur", "Z. Huang", "C.-A. Psomas"], "venue": "In Proceedings of the 48th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Devanur et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Devanur et al\\.", "year": 2016}, {"title": "Revenue maximization with a single sample", "author": ["P. Dhangwatnotai", "T. Roughgarden", "Q. Yan"], "venue": "Games and Economic Behavior,", "citeRegEx": "Dhangwatnotai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dhangwatnotai et al\\.", "year": 2015}, {"title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator", "author": ["A. Dvoretzky", "J. Kiefer", "J. Wolfowitz"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Dvoretzky et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Dvoretzky et al\\.", "year": 1956}, {"title": "Sulla determinazione empirica delle leggi di probabilita", "author": ["V. Glivenko"], "venue": "Giornalle dell\u2019Istituto Italiano degli Attuari,", "citeRegEx": "Glivenko.,? \\Q1933\\E", "shortCiteRegEx": "Glivenko.", "year": 1933}, {"title": "Efficient empirical revenue maximization in single-parameter auction environments", "author": ["Y.A. Gonczarowski", "N. Nisan"], "venue": "In Proceedings of the 49th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Gonczarowski and Nisan.,? \\Q2017\\E", "shortCiteRegEx": "Gonczarowski and Nisan.", "year": 2017}, {"title": "Making the most of your samples", "author": ["Z. Huang", "Y. Mansour", "T. Roughgarden"], "venue": "In Proceedings of the 16th ACM Conference on Economics and Computation (EC),", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Rademacher Processes and Bounding the Risk of Function Learning, pages 443\u2013457", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": null, "citeRegEx": "Koltchinskii and Panchenko.,? \\Q2000\\E", "shortCiteRegEx": "Koltchinskii and Panchenko.", "year": 2000}, {"title": "The tight constant in the dvoretzky-kiefer-wolfowitz inequality", "author": ["P. Massart"], "venue": "The Annals of Probability,", "citeRegEx": "Massart.,? \\Q1990\\E", "shortCiteRegEx": "Massart.", "year": 1990}, {"title": "On the pseudo-dimension of nearly optimal auctions", "author": ["J. Morgenstern", "T. Roughgarden"], "venue": "In Proceedings of the 29th Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Morgenstern and Roughgarden.,? \\Q2015\\E", "shortCiteRegEx": "Morgenstern and Roughgarden.", "year": 2015}, {"title": "Learning simple auctions", "author": ["J. Morgenstern", "T. Roughgarden"], "venue": "In Proceedings of the 29th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Morgenstern and Roughgarden.,? \\Q2016\\E", "shortCiteRegEx": "Morgenstern and Roughgarden.", "year": 2016}, {"title": "Optimal auction design", "author": ["R. Myerson"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Myerson.,? \\Q1981\\E", "shortCiteRegEx": "Myerson.", "year": 1981}, {"title": "Ironing in the dark", "author": ["T. Roughgarden", "O. Schrijvers"], "venue": "In Proceedings of the 17th ACM Conference on Economics and Computation (EC),", "citeRegEx": "Roughgarden and Schrijvers.,? \\Q2016\\E", "shortCiteRegEx": "Roughgarden and Schrijvers.", "year": 2016}, {"title": "Weak convergence and empirical processes : with applications to statistics. Springer series in statistics", "author": ["A.W. v. d. Vaart", "J.A. Wellner"], "venue": null, "citeRegEx": "Vaart and Wellner.,? \\Q1996\\E", "shortCiteRegEx": "Vaart and Wellner.", "year": 1996}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V. Vapnik", "A. Chervonenkis"], "venue": "Theory Probab. Appl.,", "citeRegEx": "Vapnik and Chervonenkis.,? \\Q1971\\E", "shortCiteRegEx": "Vapnik and Chervonenkis.", "year": 1971}], "referenceMentions": [{"referenceID": 11, "context": "The seminal Glivenko-Cantelli Theorem (Glivenko, 1933; Cantelli, 1933) addresses this question when the distribution \u03bc is over the real numbers.", "startOffset": 38, "endOffset": 70}, {"referenceID": 4, "context": "The seminal Glivenko-Cantelli Theorem (Glivenko, 1933; Cantelli, 1933) addresses this question when the distribution \u03bc is over the real numbers.", "startOffset": 38, "endOffset": 70}, {"referenceID": 3, "context": "Some twenty years after Glivenko and Cantelli discovered this theorem, Dvoretzky, Kiefer, and Wolfowitz (DKW) strengthened this result by giving an almost1 tight quantitative bound on the convergence rate. In 1990, Massart proved a tight inequality, confirming a conjecture due to Birnbaum and McCarty (1958):", "startOffset": 37, "endOffset": 309}, {"referenceID": 15, "context": "2 (Massart, 1990).", "startOffset": 2, "endOffset": 17}, {"referenceID": 3, "context": "3 and other generalizations of the Glivenko-Cantelli Theorem: for example, the seminal work of Vapnik and Chervonenkis (1971) shows that for every class of events F of VC dimension d, there is n0 = n0( , \u03b4, d) such that for every n \u2265 n0, with probability 1\u2212 \u03b4 it holds that \u2200A \u2208 F : \u2223\u2223p(A)\u2212 pn(A)\u2223\u2223 \u2264 .", "startOffset": 44, "endOffset": 126}, {"referenceID": 3, "context": "3 and other generalizations of the Glivenko-Cantelli Theorem: for example, the seminal work of Vapnik and Chervonenkis (1971) shows that for every class of events F of VC dimension d, there is n0 = n0( , \u03b4, d) such that for every n \u2265 n0, with probability 1\u2212 \u03b4 it holds that \u2200A \u2208 F : \u2223\u2223p(A)\u2212 pn(A)\u2223\u2223 \u2264 . This yields Glivenko-Cantelli by plugging F = { (\u2212\u221e, t] : t \u2208 R } , which has VC dimension 1. In contrast, the submultiplicative bound from Theorem 1.3 does not even extend to the VC dimension 1 class F = { {t} : t \u2208 R } . Indeed, pick any distribution p over R such that p ( {t} ) = 0 for every t, and observe that for every sample x1, . . . , xn, it holds that pn ( {xi} ) \u2265 1/n, however p ( {xi} ) = 0, and therefore, as long as \u03b1 > 0, it is never the case that \u2223\u2223\u2223p({xi}) \u2212 pn({xi})\u2223\u2223\u2223 \u2264 p({xi})\u03b1. Theorem 1.3 is proven in Section 3, which also includes other extensions. Our second main result gives an explicit upper bound on n0( , \u03b4, \u03b1): The inequality due to Dvoretzky et al. (1956) has a larger constant C in front of the exponent on the right hand side.", "startOffset": 44, "endOffset": 994}, {"referenceID": 14, "context": "These include uniform convergence bounds for more general classes of functions as well as more general loss functions (for example, Vapnik and Chervonenkis, 1971; Vapnik, 1998; Koltchinskii and Panchenko, 2000; Bartlett and Mendelson, 2002).", "startOffset": 118, "endOffset": 240}, {"referenceID": 1, "context": "These include uniform convergence bounds for more general classes of functions as well as more general loss functions (for example, Vapnik and Chervonenkis, 1971; Vapnik, 1998; Koltchinskii and Panchenko, 2000; Bartlett and Mendelson, 2002).", "startOffset": 118, "endOffset": 240}, {"referenceID": 1, "context": "These include uniform convergence bounds for more general classes of functions as well as more general loss functions (for example, Vapnik and Chervonenkis, 1971; Vapnik, 1998; Koltchinskii and Panchenko, 2000; Bartlett and Mendelson, 2002). The results that concern unbounded loss functions are most relevant to this work (for example, Cortes et al., 2010, 2013; Vapnik, 1998). We next briefly discuss the relevant results from Cortes et al. (2013) in the context of this paper; more specifically, in the context of Theorem 1.", "startOffset": 211, "endOffset": 450}, {"referenceID": 6, "context": "whereas, Cortes et al. (2013) analyzes the event where it is bounded it by:", "startOffset": 9, "endOffset": 30}, {"referenceID": 6, "context": "Thus, the main difference is the additive 1/n term in the bound from Cortes et al. (2013). In the context of uniform convergence of revenues, it is crucial to use the upper bound on the empirical quantile as we do, as it guarantees that large prices will not overfit, which is the main challenge in proving uniform convergence in this context.", "startOffset": 69, "endOffset": 90}, {"referenceID": 6, "context": "Thus, the main difference is the additive 1/n term in the bound from Cortes et al. (2013). In the context of uniform convergence of revenues, it is crucial to use the upper bound on the empirical quantile as we do, as it guarantees that large prices will not overfit, which is the main challenge in proving uniform convergence in this context. In particular, the upper bound from Cortes et al. (2013) does not provide any guarantee on the revenues of prices p >> n, as for such prices p \u00b7 1/n >> 1.", "startOffset": 69, "endOffset": 401}, {"referenceID": 6, "context": "Thus, the main difference is the additive 1/n term in the bound from Cortes et al. (2013). In the context of uniform convergence of revenues, it is crucial to use the upper bound on the empirical quantile as we do, as it guarantees that large prices will not overfit, which is the main challenge in proving uniform convergence in this context. In particular, the upper bound from Cortes et al. (2013) does not provide any guarantee on the revenues of prices p >> n, as for such prices p \u00b7 1/n >> 1. It is also worth pointing out that our lower bound on the empirical quantile implies that with high probability the quantile of the maximum sampled point is at least 1/n2 (or more generally, at least 1/n1/\u03b1 when \u03b1 6= 1/2), while the bound from Cortes et al. (2013) does not imply any non-trivial lower bound.", "startOffset": 69, "endOffset": 764}, {"referenceID": 6, "context": "Thus, the main difference is the additive 1/n term in the bound from Cortes et al. (2013). In the context of uniform convergence of revenues, it is crucial to use the upper bound on the empirical quantile as we do, as it guarantees that large prices will not overfit, which is the main challenge in proving uniform convergence in this context. In particular, the upper bound from Cortes et al. (2013) does not provide any guarantee on the revenues of prices p >> n, as for such prices p \u00b7 1/n >> 1. It is also worth pointing out that our lower bound on the empirical quantile implies that with high probability the quantile of the maximum sampled point is at least 1/n2 (or more generally, at least 1/n1/\u03b1 when \u03b1 6= 1/2), while the bound from Cortes et al. (2013) does not imply any non-trivial lower bound. Another, more qualitative difference is that unlike the bounds in Cortes et al. (2013) that apply for general VC classes, our bound is tailored for the class of thresholds (corresponding to CDF/quantiles), and does not extend even to other classes of VC dimension 1 (see the discussion after Theorem 1.", "startOffset": 69, "endOffset": 895}, {"referenceID": 16, "context": "Most papers in this direction assume that the distribution meets some tail condition that is considered \u201cnatural\u201d within the algorithmic game theory community, such as boundedness (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016; Gonczarowski and Nisan, 2017; Devanur et al., 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al.", "startOffset": 180, "endOffset": 357}, {"referenceID": 19, "context": "Most papers in this direction assume that the distribution meets some tail condition that is considered \u201cnatural\u201d within the algorithmic game theory community, such as boundedness (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016; Gonczarowski and Nisan, 2017; Devanur et al., 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al.", "startOffset": 180, "endOffset": 357}, {"referenceID": 17, "context": "Most papers in this direction assume that the distribution meets some tail condition that is considered \u201cnatural\u201d within the algorithmic game theory community, such as boundedness (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016; Gonczarowski and Nisan, 2017; Devanur et al., 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al.", "startOffset": 180, "endOffset": 357}, {"referenceID": 0, "context": "Most papers in this direction assume that the distribution meets some tail condition that is considered \u201cnatural\u201d within the algorithmic game theory community, such as boundedness (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016; Gonczarowski and Nisan, 2017; Devanur et al., 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al.", "startOffset": 180, "endOffset": 357}, {"referenceID": 12, "context": "Most papers in this direction assume that the distribution meets some tail condition that is considered \u201cnatural\u201d within the algorithmic game theory community, such as boundedness (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016; Gonczarowski and Nisan, 2017; Devanur et al., 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al.", "startOffset": 180, "endOffset": 357}, {"referenceID": 8, "context": "Most papers in this direction assume that the distribution meets some tail condition that is considered \u201cnatural\u201d within the algorithmic game theory community, such as boundedness (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016; Gonczarowski and Nisan, 2017; Devanur et al., 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al.", "startOffset": 180, "endOffset": 357}, {"referenceID": 9, "context": ", 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016), or such as a condition known as monotone hazard rate (Huang et al.", "startOffset": 58, "endOffset": 156}, {"referenceID": 13, "context": ", 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016), or such as a condition known as monotone hazard rate (Huang et al.", "startOffset": 58, "endOffset": 156}, {"referenceID": 5, "context": ", 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016), or such as a condition known as monotone hazard rate (Huang et al.", "startOffset": 58, "endOffset": 156}, {"referenceID": 8, "context": ", 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016), or such as a condition known as monotone hazard rate (Huang et al.", "startOffset": 58, "endOffset": 156}, {"referenceID": 13, "context": ", 2016), or such as a condition known as monotone hazard rate (Huang et al., 2015).", "startOffset": 62, "endOffset": 82}, {"referenceID": 16, "context": "Indeed, while some papers that studied bounded distributions (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016) did use uniform convergence bounds as part of their analysis, other papers, in particular those that considered unbounded distributions, had to bypass the usage of uniform convergence by more specialized arguments.", "startOffset": 61, "endOffset": 186}, {"referenceID": 19, "context": "Indeed, while some papers that studied bounded distributions (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016) did use uniform convergence bounds as part of their analysis, other papers, in particular those that considered unbounded distributions, had to bypass the usage of uniform convergence by more specialized arguments.", "startOffset": 61, "endOffset": 186}, {"referenceID": 17, "context": "Indeed, while some papers that studied bounded distributions (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016) did use uniform convergence bounds as part of their analysis, other papers, in particular those that considered unbounded distributions, had to bypass the usage of uniform convergence by more specialized arguments.", "startOffset": 61, "endOffset": 186}, {"referenceID": 0, "context": "Indeed, while some papers that studied bounded distributions (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016) did use uniform convergence bounds as part of their analysis, other papers, in particular those that considered unbounded distributions, had to bypass the usage of uniform convergence by more specialized arguments.", "startOffset": 61, "endOffset": 186}, {"referenceID": 9, "context": "Thus, it turns out that the works that studied the popular class of Myerson-regular distributions (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016) indeed could not have hoped to establish learnability via a uniform convergence argument.", "startOffset": 98, "endOffset": 196}, {"referenceID": 13, "context": "Thus, it turns out that the works that studied the popular class of Myerson-regular distributions (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016) indeed could not have hoped to establish learnability via a uniform convergence argument.", "startOffset": 98, "endOffset": 196}, {"referenceID": 5, "context": "Thus, it turns out that the works that studied the popular class of Myerson-regular distributions (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016) indeed could not have hoped to establish learnability via a uniform convergence argument.", "startOffset": 98, "endOffset": 196}, {"referenceID": 8, "context": "Thus, it turns out that the works that studied the popular class of Myerson-regular distributions (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016) indeed could not have hoped to establish learnability via a uniform convergence argument.", "startOffset": 98, "endOffset": 196}, {"referenceID": 10, "context": "The seminal work of Myerson (1981) shows that given a valuation distribution for a single good, the revenue-maximizing selling mechanism for this good is a posted-price mechanism.", "startOffset": 20, "endOffset": 35}, {"referenceID": 0, "context": "Most papers in this direction assume that the distribution meets some tail condition that is considered \u201cnatural\u201d within the algorithmic game theory community, such as boundedness (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016; Gonczarowski and Nisan, 2017; Devanur et al., 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016), or such as a condition known as monotone hazard rate (Huang et al., 2015).4 These papers then go on to derive computation- or sample-complexity bounds on learning an optimal price (or an optimal selling mechanism from a given class) for a distribution that meets the assumed condition. A recurring theme in statistical learning theory is that learnability guarantees are derived via a, sometimes implicit, uniform convergence bound. However, this has not been the case in the context of revenue learning. Indeed, while some papers that studied bounded distributions (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016) did use uniform convergence bounds as part of their analysis, other papers, in particular those that considered unbounded distributions, had to bypass the usage of uniform convergence by more specialized arguments. This is due to the fact that many unbounded distributions do not satisfy any uniform convergence bound. As a concrete example, the (unbounded, Myerson-regular) equal revenue distribution5 has an infinite expectation and therefore, by our Theorem 2.3, satisfies no uniform convergence, even in the limit. Thus, it turns out that the works that studied the popular class of Myerson-regular distributions (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016) indeed could not have hoped to establish learnability via a uniform convergence argument. For instance, the way Dhangwatnotai et al. (2015) and Cole and Roughgarden (2014)", "startOffset": 285, "endOffset": 2056}, {"referenceID": 0, "context": "Most papers in this direction assume that the distribution meets some tail condition that is considered \u201cnatural\u201d within the algorithmic game theory community, such as boundedness (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016; Gonczarowski and Nisan, 2017; Devanur et al., 2016)3, such as a condition known as Myerson-regularity (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016), or such as a condition known as monotone hazard rate (Huang et al., 2015).4 These papers then go on to derive computation- or sample-complexity bounds on learning an optimal price (or an optimal selling mechanism from a given class) for a distribution that meets the assumed condition. A recurring theme in statistical learning theory is that learnability guarantees are derived via a, sometimes implicit, uniform convergence bound. However, this has not been the case in the context of revenue learning. Indeed, while some papers that studied bounded distributions (Morgenstern and Roughgarden, 2015; Roughgarden and Schrijvers, 2016; Morgenstern and Roughgarden, 2016; Balcan et al., 2016) did use uniform convergence bounds as part of their analysis, other papers, in particular those that considered unbounded distributions, had to bypass the usage of uniform convergence by more specialized arguments. This is due to the fact that many unbounded distributions do not satisfy any uniform convergence bound. As a concrete example, the (unbounded, Myerson-regular) equal revenue distribution5 has an infinite expectation and therefore, by our Theorem 2.3, satisfies no uniform convergence, even in the limit. Thus, it turns out that the works that studied the popular class of Myerson-regular distributions (Dhangwatnotai et al., 2015; Huang et al., 2015; Cole and Roughgarden, 2014; Devanur et al., 2016) indeed could not have hoped to establish learnability via a uniform convergence argument. For instance, the way Dhangwatnotai et al. (2015) and Cole and Roughgarden (2014)", "startOffset": 285, "endOffset": 2088}, {"referenceID": 0, "context": "The analysis of Balcan et al. (2016) assumes a bound on the realized revenue (from any possible valuation profile) of any mechanism/auction in the class that they consider.", "startOffset": 16, "endOffset": 37}, {"referenceID": 2, "context": "Thus, since these events are independent, the second Borel-Cantelli Lemma (Borel, 1909; Cantelli, 1917) implies that almost surely, infinitely many of them occur and so infinitely often", "startOffset": 74, "endOffset": 103}, {"referenceID": 3, "context": "Thus, since these events are independent, the second Borel-Cantelli Lemma (Borel, 1909; Cantelli, 1917) implies that almost surely, infinitely many of them occur and so infinitely often", "startOffset": 74, "endOffset": 103}, {"referenceID": 20, "context": "3 from Vaart and Wellner (1996)).", "startOffset": 7, "endOffset": 32}, {"referenceID": 5, "context": "While in many more-complex environments, the revenue-maximizing mechanism/auction is still not understood well enough, for environments where it is understood (Cole and Roughgarden, 2014; Devanur et al., 2016; Gonczarowski and Nisan, 2017) (as well as for simple auction classes that do not necessarily contain a revenue-maximizing auction (Morgenstern and Roughgarden, 2016; Balcan et al.", "startOffset": 159, "endOffset": 239}, {"referenceID": 8, "context": "While in many more-complex environments, the revenue-maximizing mechanism/auction is still not understood well enough, for environments where it is understood (Cole and Roughgarden, 2014; Devanur et al., 2016; Gonczarowski and Nisan, 2017) (as well as for simple auction classes that do not necessarily contain a revenue-maximizing auction (Morgenstern and Roughgarden, 2016; Balcan et al.", "startOffset": 159, "endOffset": 239}, {"referenceID": 12, "context": "While in many more-complex environments, the revenue-maximizing mechanism/auction is still not understood well enough, for environments where it is understood (Cole and Roughgarden, 2014; Devanur et al., 2016; Gonczarowski and Nisan, 2017) (as well as for simple auction classes that do not necessarily contain a revenue-maximizing auction (Morgenstern and Roughgarden, 2016; Balcan et al.", "startOffset": 159, "endOffset": 239}, {"referenceID": 17, "context": ", 2016; Gonczarowski and Nisan, 2017) (as well as for simple auction classes that do not necessarily contain a revenue-maximizing auction (Morgenstern and Roughgarden, 2016; Balcan et al., 2016)) it would also be interesting to study relaxations of the restrictive tail or boundedness assumptions currently common in the literature.", "startOffset": 138, "endOffset": 194}, {"referenceID": 0, "context": ", 2016; Gonczarowski and Nisan, 2017) (as well as for simple auction classes that do not necessarily contain a revenue-maximizing auction (Morgenstern and Roughgarden, 2016; Balcan et al., 2016)) it would also be interesting to study relaxations of the restrictive tail or boundedness assumptions currently common in the literature.", "startOffset": 138, "endOffset": 194}], "year": 2017, "abstractText": "In this work we derive a variant of the classic Glivenko-Cantelli Theorem, which asserts uniform convergence of the empirical Cumulative Distribution Function (CDF) to the CDF of the underlying distribution. Our variant allows for tighter convergence bounds for extreme values of the CDF. We apply our bound in the context of revenue learning, which is a well-studied problem in economics and algorithmic game theory. We derive sample-complexity bounds on the uniform convergence rate of the empirical revenues to the true revenues, assuming a bound on the kth moment of the valuations, for any (possibly fractional) k > 1. For uniform convergence in the limit, we give a complete characterization and a zeroone law: if the first moment of the valuations is finite, then uniform convergence almost surely occurs; conversely, if the first moment is infinite, then uniform convergence almost never occurs.", "creator": "LaTeX with hyperref package"}}}