{"id": "1705.08695", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Stochastic Sequential Neural Networks with Structured Inference", "abstract": "unsupervised delayed learning in reduced - dimensional time series data has attracted a lot of research successes. during example, experiments efficiently labelling high dimensional time series can more helpful in behavior understanding future medical diagnosis. recent advances in generative sequential modeling have suggested to combine recurrent neural networks with state space models ( e. g., hidden markov models ). this combination can model not offset the long term dependency in generic data, but also the uncertainty included in the hidden states. inheriting these advantages of stochastic vs sequential models, we propose a structured and stochastic coupled neural network, which models both the long - term dependencies via recurrent neural networks and the flexibility in the labeled and labels via discrete dependence intervals. for accurate diffusion efficient inference, we offer a bi - directional inference network by sampling the categorical segmentation and labels with the recent revised gumbel - softmax approximation and resort to the stochastic gradient vs bayes. we evaluate the proposed processes in a number of tasks, including speech typing, automatic segmentation and labeling in biological understanding, and sequential multi - objects recognition. survey results have demonstrated that our proposed model can achieve significant improvement over the state - of - the - art methods.", "histories": [["v1", "Wed, 24 May 2017 10:52:19 GMT  (2257kb,D)", "http://arxiv.org/abs/1705.08695v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["hao liu", "haoli bai", "lirong he", "zenglin xu"], "accepted": false, "id": "1705.08695"}, "pdf": {"name": "1705.08695.pdf", "metadata": {"source": "CRF", "title": "Stochastic Sequential Neural Networks with Structured Inference", "authors": ["Hao Liu", "Haoli Bai", "Lirong He", "Zenglin Xu"], "emails": ["liuhaosater@gmail.com", "haolibai@gmail.com", "he@std.uestc.edu.cn", "zenglin@gmail.com"], "sections": [{"heading": null, "text": "Keywords: Recurrent neural networks, Hidden Semi-Markov models, Sequential data"}, {"heading": "1. Introduction", "text": "Unsupervised structure learning in high-dimensional sequential data is an important research problem in a number of applications, such as machine translation, speech recognition, computational biology, and computational physiology Sutskever et al. (2014); Dai et al. (2017b). For example, in medical diagnosis, learning the segment boundaries and labeling of complicated physical signals is very useful for doctors to understand the underlying behavior or activity types.\nModels for sequential data analysis such as recurrent neural networks (RNNs)Rumelhart et al. (1988) and hidden Markov models (HMMs)Rabiner (1989) are widely used. Recent\nar X\niv :1\n70 5.\n08 69\n5v 1\n[ cs\n.L G\nliterature have investigated approaches of combining probabilistic generative models and recurrent neural networks for the sake of their complementary strengths in nonlinear representation learning and effective estimation of parameters Johnson et al. (2016); Dai et al. (2017b); Fraccaro et al. (2016). In many tasks, such as segmentation and labeling of natural scenes and physiological signals, the duration lengths and labels of segments are often interpretable and categorical distributed Jang et al. (2017). However, most of existing models are designed primarily for continuous situations and do not extend to discrete latent variables Johnson et al. (2016); Krishnan et al. (2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al. (2017b) utilizes discrete variables with informative information for label prediction of segmentation, the inference approach does not explicitly take advantage of structured information to exploit the bi-directional temporal information, and thus may lead to suboptimal performance, as verified in the experiment (See Table 5.3 in Section 5 for more details).\nTo address such issues, we propose the Stochastic Sequential Neural Network (SSNN) consisting of a generative network and an inference network. The generative network(as will shown in Figure 2(a)) shares the spirit of Hidden Semi-Markov Model (HSMM) Rabiner (1989) and Recurrent HSMM Dai et al. (2017b), and is composed with a continuous sequence (i.e., hidden states in RNN) as well as two discrete sequences (i.e., segmentation variables and labels in SSM). The inference network(as will shown in Figure 2(b)) can take the advantages of bi-directional temporal information by augmented variables, and efficiently approximate the categorical variables in segmentation and segment labels via the recently proposed Gumbel-Softmax approximation Jang et al. (2017); Maddison et al. (2016). Thus, SSNN can model the complex and long-range dependencies in sequential data, but also maintain the structure learning ability of SSMs with efficient inference.\nIn order to evaluate the performance of the proposed model, we compare our proposed model with the state-of-the-art neural models in a number of tasks, including automatic segmentation and labeling on datasets of speech modeling, and behavior modeling, medical diagnosis, and multi-object recognition. Experimental results in terms of both model fitting and labeling of learned segments have demonstrated the promising performance of the proposed model."}, {"heading": "2. Preliminaries", "text": "In this section, we present background related to generative sequential models. Specifically, we first introduce RNNs, followed by HMMs and HSMMs.\nRecurrent neural networks (RNNs), as a wide range of sequential models for time series, have been applied in a number of applications Sutskever et al. (2014). Here we introduce basic properties and notations of RNNs. Consider a sequence of temporal sequences of vectors x1:T = [x1,x2, ...,xT ] that depends on the inputs u1:T = [u1,u2, ...,uT ], where xt \u2208 Rm is the observation and ut \u2208 Rn is the input at the time step t, and T is the maximum time steps. RNN further introduces hidden states h1:T = [h1,h2, ...,hT ], where ht \u2208 Rh encodes the information before the time step t, and is determined by ht = f\u03b8(ht\u22121,ut), where f\u03b8(\u00b7) is a nonlinear function parameterized by a neural network.\nState space models such as hidden Markov models (HMMs) and hidden Semi-Markov models (HSMMs) are also widely-used methods in sequential learning Dai et al. (2017b); Chiappa et al. (2014); Dewar et al. (2012). In HMM, given an observed sequence x1:T , each xt is generated based on the hidden state zt \u2208 {1, 2, ...,K}, and p\u03b8(xt|zt) is the emission probability. We set p\u03b8(z1) as the distribution of the initial state, and p\u03b8(zt|zt\u22121) is the transition probability. We use z1:T = [z1, z2, ..., zT ]. Here \u03b8 includes all parameters necessary for these distributions. HSMM is a famous extension of HMM. Aside from the hidden state zt, HSMM further introduces time duration variables dt \u2208 {1, 2, ..,M}, where M is the maximum duration value for each xt. We set d1:T = [d1, d2, ..., dT ]. HSMM splits the sequence into L segments, allowing the flexibility to find the best segment representation. We set s1:L = [s1, s2, .., sL] as the beginning of the segments. A difference from HMM is that for segment i, the latent state zsi:si+dsi\u22121 is fixed in HMM. An illustration is given in Figure 1.\nThere are many variants of HSMMs such as the Hierarchical Dirichlet-Process HSMM (HDP-HSMM) Johnson and Willsky (2013) and subHSMM Johnson and Willsky (2014). The subHSMM and HDP-HSMM extend their HMM counterparts by allowing explicit modeling of state duration lengths with arbitrary distributions. While there are various types of HMM, the inference methods are mostly inefficient.\nAlthough HMMs and HSMMs can explicitly model uncertainty in the latent space and learn a interpretable representation through dt and zt, they are not good at capturing the long-range temporal dependencies when compared with RNNs."}, {"heading": "3. Model", "text": "In this section, we present our stochastic sequential neural network model. The notations and settings are generally consistent with HSMM Section 2, as also illustrated in Figure 1. For the simplicity of explanation, we present our model on a single sequence. It is straightforward to apply the model multiple sequences."}, {"heading": "3.1 Generative Model", "text": "In order to model the long-range temporal dependencies and the uncertainty in segmentation and labeling of time series, we aim to take advantages from RNN and HSMM, and learn categorical information and representation information from the observed data recurrently. As illustrated in Figure 2(a), we design an Stochastic Sequential Neural Network (SSNN) with one sequence of continuous latent variables modeling the recurrent hidden states, and two sequences of discrete variables denoting the segment duration and labels, respectively. The joint probability can be factorized as:\np\u03b8(x1:T , z1:T , d1:T ) = p\u03b8(x1:T |z1:T , d1:T ) \u00b7 p\u03b8(z1)p\u03b8(d1|z1) T\u220f t=2 p\u03b8(zt|zt\u22121, dt\u22121)p\u03b8(dt|zt, dt\u22121).\n(1)\nTo learn more interpretative latent labels, we follow the design in HSMM to set zt and dt as categorical random variables, The distribution of zt and dt is\np\u03b8(zt|zt\u22121, dt\u22121) = { I(zt = zt\u22121) if dt\u22121 > 1 p\u03b8(zt|zt\u22121) otherwise\n, (2)\np\u03b8(dt|zt, dt\u22121) = { I(dt = dt\u22121 \u2212 1) if dt\u22121 > 1 p\u03b8(dt|zt) otherwise , (3)\nwhere I(x) is the indicator function (whose value equals 1 if x is True, and otherwise 0). The transition probability p\u03b8(zt|zt\u22121) and p\u03b8(dt|zt), in implementation, can be achieved by learning a transition matrix.\nThe joint emission probability p\u03b8(x1:T |z1:T , d1:T ) can be further factorized into multiple segments. Specifically, for the i-th segment xsi:si+dsi\u22121 starting from si, the corresponding generative distribution is\np\u03b8(xsi:si+dsi\u22121|zsi , dsi) = si+dsi\u22121\u220f t=si p\u03b8(xt|xsi:t\u22121, zsi). = si+dsi\u22121\u220f t=si p\u03b8(xt|ht, zsi), (4)\nwhere ht is the latent deterministic variable in RNN. As mentioned earlier, ht can better model the complex dependency among segments, and capture past information of the observed sequence xt\u22121 as well as the previous state ht\u22121. We design ht = \u03c3(W (zsi ) x xt\u22121 +\nW (zsi ) h ht\u22121 + b (zsi ) h ), where \u03c3() is a tanh activation function, Wx \u2208 R K\u00d7h\u00d7m and Wh \u2208 RK\u00d7h\u00d7h are weight parameter, and bh \u2208 RK\u00d7h is the bias term. W (zsi ) x \u2208 Rh\u00d7m is the zsi-th slice of Wx, and it is similar for W (zsi ) h and b (zsi )\nh . Finally, the distribution of xt given ht and zsi is designed by a Normal distribution,\np\u03b8(xt|ht, zsi) = N (x;\u00b5,\u03c32), (5)\nwhere the mean satisfies \u00b5 = W (zsi ) \u00b5 ht+ b (zsi ) \u00b5 , and the covariance is a diagonal matrix with its log diagonal elements log\u03c32 = W (zsi ) \u03c3 ht +b (zsi ) \u03c3 . We use \u03b8 to include all the parameters in the generative model."}, {"heading": "3.2 Structured Inference", "text": "We are interested in maximizing the marginal log-likelihood log p(x), however, this is usually intractable since the complicated posterior distributions cannot be integrated out generally. Recent methods in Bayesian learning, such as the score function (or REINFORCE) Archer et al. (2015) and the Stochastic Gradient Variational Bayes (SGVB) Kingma and Welling (2013), are common black-box methods that lead to tractable solutions. We resort to the SGVB since it could efficiently learn the approximation with relatively low variances Kingma and Welling (2013), while the score function suffers from high variances and heavy computational costs.\nWe now focus on maximizing the evidence lower bound also known as ELBO,\nlog p\u03b8(x) \u2265 L(x1:T ; \u03b8, \u03c6) = Eq\u03c6(z1:T ,d1:T |x1:T )[log p\u03b8(x1:T , z1:T , d1:T )\u2212 log q\u03c6(z1:T , d1:T |x1:T )], (6)\nwhere q\u03c6(\u00b7) denotes the approximate posterior distribution, and \u03b8 and \u03c6 denote parameters for their corresponding distributions, repsectively."}, {"heading": "3.2.1 Bi-directional Inference", "text": "In order to find a more informative approximation to the posterior, we augment both random variables dt, zt with bi-directional information in the inference network. Such attempts have been explored in many previous work (Krishnan et al., 2016; Khan and Lin, 2017; Krishnan et al., 2015), however they mainly focus on continuous variables, and little attention is paid to the discrete variable. Specifically, we first learn a bi-directional deterministic variable h\u0302t = BiRNN(x1:t, xt:T ) , where BiRNN is a bi-directional RNN with each unit implemented as an LSTM Hochreiter and Schmidhuber (1997). Similar to Fraccaro et al. (2016), we further use a backward recurrent function It = g\u03c6I (It+1, [xt, h\u0302t]) to explicitly capture forward and backward information in the sequence via h\u0302t, where [xt, h\u0302t] is the concatenation of xt and h\u0302t.\nThe posterior approximation can be factorized as\nq\u03c6(z1:T , d1:T |x1:T ) = q\u03c6(z1|I1)q\u03c6(d1|z1, I1) T\u220f t=2 q\u03c6(zt|dt\u22121, It)q\u03c6(dt|dt\u22121, zt, It), (7)\nand the graphical model for the inference network is shown in Figure.2(b). We use \u03c6 to denotes all parameters in inference network.\nWe design the posterior distributions of dt and zt to be categorical distributions, respectively, as follows:\nq(zt|dt\u22121, It;\u03c6) = Cat(softmax(WTz It)), (8) q(dt|dt\u22121, zt, It;\u03c6) = Cat(softmax(WTd It)), (9)\nwhere Cat denotes the categorical distribution. Since the posterior distributions of zt and dt are conditioned on It, they depend on both the forward sequences (i.e., ht:T and xt:T ) and the backward sequences (i.e., h1:t\u22121 and x1:t\u22121), leading to a more informative approximation. However, the reparameterization tricks and their extensions (Chung et al., 2016) are not directly applicable due to the discrete random variables, i.e., dt and zt in our model. Thus we turn to the recently proposed Gumbel-Softmax reparameterization trick (Jang et al., 2017; Maddison et al., 2016), as shown in the following."}, {"heading": "3.2.2 Gumbel-Softmax Reparameterization", "text": "The Gumbel-Softmax reparameterization proposes an alternative to the back propagation through discrete random variables via the Gumbel-Softmax distribution, and circumvents the non-differentiable categorical distribution.\nTo use the Gumbel-Softmax reparameterization, we first map the discrete pair (zt, dt) to a N -dimensional vector \u03b3(t), and \u03b3(t) \u223c Cat(\u03c0(t)), where \u03c0(t) is a N -dimensional vector on the simplex and N = K \u00d7D. Then we use y(t) \u2208 RN to represent the Gumbel-Softmax distributed variable:\nyi(t) = exp((log(\u03c0i(t)) + gi)/\u03c4)\u2211k j=1 exp((log(\u03c0j(t)) + gj)/\u03c4)\nfor i = 1, ..., N, (10)\nwhere gi \u223c Gumbel(0, 1), and \u03c4 is the temperature that will be elaborated in the experiment. Via the Gumbel Softmax transformation, we set y(t) \u223c Concrete(\u03c0(t), \u03c4) according to (Maddison et al., 2016).\nNow we can sample y(t) from the Gumbel-Softmax posterior in replacement of the categorically distributed \u03b3(t), and use the back-propagation gradient with the ADAM Kingma and Ba (2014) optimizer to learn parameters \u03b8 and \u03c6.\nFor simplicity, we denote F (z, d) = log p\u03b8(x1:T , z1:T , d1:T ) \u2212 log q(z1:T , d1:T |x1:T ), and furthermore, F\u0303 (y, g) is the corresponding approximation term of F (z, d) after the GumbelSoftmax trick. The Gumbel-Softmax approximation of L(x1:T ; \u03b8, \u03c6) is: L(x1:T ; \u03b8, \u03c6) \u2248 Ey\u223cConcrete(\u03c0(t),\u03c4)[F\u0303 (y, g)] = Eg\u223c\u220fN Gumbel(0,1)[F\u0303 (y, g)]. (11) Hence the derivatives of the approximated ELBO w.r.t. the inference parameters \u03c6 can be approximated by the SGVB estimator:\n\u2202\n\u2202\u03c6 Eg\u223c\n\u220f N Gumbel(0,1) [F\u0303 (y, g)] = Eg\u223c \u220f N Gumbel(0,1)\n[ \u2202\n\u2202\u03c6\n( F\u0303 (y, g) )] \u2248 1 B B\u2211 b=1 \u2202 \u2202\u03c6 ( F\u0303 (yb, gb) ) , gb := (gb1, . . . , g b N ), (12)\nwhere yb, gb is the batch samples and B is the number of batches. The derivative w.r.t the generative parameters \u03b8 does not require the Gumbel-Softmax approximation, and can be directly estimated by the Monte Carlo estimator\n\u2202\n\u2202\u03b8 Eq\u03c6(z1:T ,d1:T )[F (z, d)] \u2248\n1\nB B\u2211 b=1 \u2202 \u2202\u03b8 ( F (zb, db) ) . (13)\nFinally, we summarize the inference algorithm in Algorithm 1.\nAlgorithm 1 Strucutured Inference Algorithm for SSNN\ninputs: Observed sequences {x(n)}Nn=1 Randomly initialized \u03c6(0) and \u03b8(0); Inference Model: q\u03c6(z1:T , d1:T |x1:T ); Generative Model: p\u03b8(x1:T , z1:T , d1:T ); outputs:Model parameters \u03b8 and \u03c6; for i = 1 to Iter do\n1. Sample sequences {x(n)}Mn=1 uniformly from dataset with a mini-batch size B. 2. Estimate and sample forward parameters using Eq.(1). 3. Evaluate the ELBO using Eq. (6). 4. Estimate the Monte Carlo approximation to \u2207\u03b8L using Eq. (13) 5. Estimate the SGVB approximation to \u2207\u03c6L using Eq.(12) with the Gumbel-Softmax approximation in Eq.(10); 6. Update \u03b8(i), \u03c6(i) using the ADAM.\nend for"}, {"heading": "4. Related Work", "text": "In this section, we review research work on generative sequential data modeling in terms of state space models and recurrent neural networks. In the following, we review some recent work on sequential latent variable model.\nIn terms of combining both and SSM and RNNs, the papers mostly close to our paper include Johnson et al. (2016); Krishnan et al. (2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series.\nIn parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner. Dai et al. (2017b) proposes to overcome the difficulty of learning discrete variables by optimizing their distribution instead of directly learning discrete variables.\nIn the aspect of optimization, Khan et al. (2015a,b) split the variational inference objective into a term to be linearized and a tractable concave term, which makes the resulting gradient easily to compute. Knowles and Minka (2011) proposes natural gradient descent with respect to natural parameters on each of the variational factors in turn. In Dai et al. (2017a), the discrete optimization is replaced by the maximization over the negative Helmholtz free energy. In contrast to linearizing intractable terms around the current iteration as used in the above approaches, we handle intractable terms via recognition networks and amortized inference(with the aid of Gumbel-Softmax reparameterization Jang et al. (2017); Maddison et al. (2016)) in this paper. That is, we use parametric function approximators to learn conditional evidence in a conjugate form."}, {"heading": "5. Experiment", "text": "In this section, we evaluate SSNN on several datasets across multiple scenarios. Specifically, we first evaluate its performance of finding complex structures and estimating data likelihood on a synthetic dataset and two speech datasets (TIMIT & Blizard). Then we test SSNN with learning segmentations and latent labels on Human activity Reyes-Ortiz et al. (2016) dataset, Drosophila dataset Kain et al. (2013) and PhysioNet Springer et al.\n(2016) Challenge dataset, and compare the results with HSMM and its variants. Finally we provide an additional challenging test on the multi-object recognition problem using the generated multi-MNIST dataset.\nAll models in the experiment use the Adam Kingma and Ba (2014) optimizer. Temperatures of Gumbel-Softmax were fixed throughout training. We implement the proposed model based on Theano Al-Rfou et al. (2016) and Block & Fuel Van Merrie\u0308nboer et al. (2015)."}, {"heading": "5.1 Synthetic Experiment", "text": "To validate that our method is able to model high dimensional data with complex dependency, we simulated a complex dynamic torque-controlled pendulum governed by a differential equation to generate non-Markovian observations from a dynamical system: ml2 d 2\u03c6(t) dt2\n= \u2212\u00b5d\u03c6(t)dt + mgl sin\u03c6(t) + u(t). For fair comparison with Karl et al. (2016), we set m = l = 1, \u00b5 = 0.5, and g = 9.81. We convert the generated ground-truth angles to image observations. The system can be fully described by angle and angular velocity.\nWe compare our method with Deep Variational Bayes Filter(DVBF-LL) Karl et al. (2016) and Deep Kalman Filters(DKF) Krishnan et al. (2015). The ordinary least square regression results are shown in Table 1. Our method is clearly better than DVBF-LL and DKF in predicting sin\u03c6, cos\u03c6 and d\u03c6dt . SSNN achieves a higher goodness-of-fit than other methods. The results indicate that generative model and inference network in SSNN are capable of capturing complex sequence dependency."}, {"heading": "5.2 Speech Modeling", "text": "We also test SSNN on the modeling of speech data, i.e., Blizzard and TIMIT datasets Prahallad et al. (2013). Blizzard records the English speech with 300 hours by a female speaker. TIMIT is a dataset with 6300 English sentences read by 630 speakers. For the TIMIT and Blizzard dataset, the sampling frequency is 16KHz and the raw audio signal is normalized using the global mean and standard deviation of the training set. Speech modeling on these two datasets has shown to be challenging since there\u2019s no good representation of the latent states Chung et al. (2015); Fabius and van Amersfoort (2014); Gu et al. (2015a); Gan et al. (2015); Sutskever et al. (2014).\nThe data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al. (2016), i.e. we report the average log-likelihood for half-second sequences on Blizzard, and report the average log-likelihood per sequence for the test set sequences on TIMIT. For the raw audio datasets, we use a fully factorized Gaussian output distribution.\nIn the experiment, We split the raw audio signals in the chunks of 2 seconds. The waveforms are divided into non-overlapping vectors with size 200. For Blizzard we split the data using 90% for training, 5% for validation and 5% for testing. For testing we report the average log-likelihood for each sequence with segment length 0.5s. For TIMIT we use the predefined test set for testing and split the rest of the data into 95% for training and 5% for validation.\nDuring training we use back-propagation through time (BPTT) for 1 second. For the first second we initialize hidden units with zeros and for the subsequent 3 chunks we use the previous hidden states as initialization. the temperature \u03c4 starts from a large value 0.1 and gradually anneals to 0.01.\nWe compare our method with the following methods. For RNN+VRNNs Chung et al. (2015), VRNN is tested with two different output distributions: a Gaussian distribution (VRNN-GAUSS), and a Gaussian Mixture Model (VRNN-GMM). We also compare to VRNN-I in which the latent variables in VRNN are constrained to be independent across time steps. For SRNN Fraccaro et al. (2016), we compare with the smoothing and filtering performance denoted as SRRR (smooth), SRNN (filt) and SRNN (smooth+ Resq) respectively. The results of VRNN-GMM, VRNN-Gauss and VRNN-I-Gauss are taken from Chung et al. (2015), and those of SRNN (smooth+Resq), SRNN (smooth) and SRNN (filt) are taken from Fraccaro et al. (2016). From Table 5.3 it can be observed that on both datasets SSNN outperforms the state of the art methods by a large margin, indicating its superior ability in speech modeling."}, {"heading": "5.3 Segmentation and Labeling of Time Series", "text": "To show the advantages of SSNN over HSMM and its variants when learning the segmentation and latent labels from sequences, we take experiments on Human activity Reyes-Ortiz et al. (2016), Drosophila dataset Kain et al. (2013) and PhysioNet Springer et al. (2016) Challenge dataset.Both Human Activity and Drosophila dataset are used for segmentation prediction.\nHuman activity consists of signals collected from waist-mounted smartphones with accelerometers and gyroscopes. Each volunteer is asked to perform 12 activities. There are 61 recorded sequences, and the maximum time steps T \u2248 3, 000. Each xt is a 6 dimensional vector.\nDrosophila dataset records the time series movement of fruit flies\u2019 legs. At each time step t, xt is a 45-dimension vector, which consists of the raw and some higher order features. the maximum time steps T \u2248 10, 000. In the experiment, we fix the \u03c4 at small value 0.0001.\nPhysioNet Challenge dataset records observation labeled with one of the four hidden states, i.e., Diastole, S1, Systole and S2. The experiment aims to exam SSNN on learning and predicting the labels. In the experiment, we find that annealing of temperature \u03c4 is important, we start from \u03c4 = 0.15 and anneal it gradually to 0.0001.\nModels Blizzard TIMIT VRNN-GMM \u2265 9107 \u2265 28982 VRNN-Gauss \u2265 9223 \u2265 28805 VRNN-I-Gauss \u2265 9223 \u226528805 SRNN(smooth+Resq) \u2265 11991 \u2265 60550 SRNN(smooth) \u226510991 \u226559269 SRNN(filt) \u226510846 50524 RNN-GMM 7413 26643 RNN-Gauss 3539 -1900 Our Method(SSNN) \u2265 13123 \u2265 64017\nTable 2: Average log-likelihood per sequence on the test sets. The higher the better.\nFigure 3: Visualization and comparison of SSNN and DRAW on multi-object recognition problems.\nSpecifically, we compare the predicted segments or latent labels with the ground truth, and report the mean and the standard deviation of the error rate for all methods. We use leave-one-sequence-out protocol to evaluate these methods, i.e., each time one sequence is held out for testing and the left sequences are for training. We set the truncation of max possible duration M to be 400 for all tasks. We also set the number of hidden states K to be the same as the ground truth.\nWe report the comparison with subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al. (2014) and rHSMM-dp Dai et al. (2017b).\nFor the HDP-HSMM and subHSMM, the observed sequences x1:T are generated by standard multivariate Gaussian distributions. The duration variable dt is from the Poisson distribution. We need to tune the concentration parameters \u03b1 and \u03b3. As for the hyper parameters, they can be learned automatically. For subHSMM, we tune the truncation threshold of the infinite HMM in the second level. For CRF-AE, we extend the original model to learn continuous data. We use mixture of Gaussian for the emission probability. For R-HSMM-dp, it is a version of R-HSMM with the exact MAP estimation via dynamic programming.\nExperimental results are shown in Table 5.3. It can be observed that SSNN achieves the lowest mean error rate, indicating the effectiveness of combining RNN with HSMM to collectively learn the segmentation and the latent states."}, {"heading": "5.4 Sequential Multi-objects Recognition", "text": "In order to further verify the ability of modeling complex spatial dependency, we test SSNN on the multiple objects recognition problem. This problem is interesting but hard, since it requires the model to capture the dependency of pixels in images and recognize the objects in images. Specifically, we construct a small image dataset including 3000 images, named as multi-MNIST. Each image contains three non-overlapping random MNIST digits with equal probability.\nOur goal is to sequentially recognize each digit in the image. In the experiment, we train our model with 2500 images and test on the rest 500 images.First we fix the maximum time steps T = 3 and feed the same image as input sequentially to SSNN. We interpret the latent variable dt as intensity and zt as the location variable in the training images. Then We train SSNN with random initialized parameters on 60,000 multi-MNIST images from scratch, i.e., without a curriculum or any form of supervision. All experiments were performed with a batch size of 64. The learning rate of model is 1\u00d7 10\u22125 and baselines were trained using a higher learning rate 1\u00d7 10\u22123. The LSTMs in the inference network had 256 cell units.\nWe compare the proposed model to DRAW Gregor et al. (2015) and visualize our learned latent representations in Figure 5.3. It can be observed that our model identifies the number and locations of digits correctly, while DRAW sometimes misses modes of data. The result shows that our method can accurately capture not only the number of objects but also locations."}, {"heading": "6. Conclusion", "text": "In order to learn the structures (e.g., the segmentation and labeling) of high-dimensional time series in a unsupervised way, we have proposed a Stochastic sequential neural network(SSNN) with structured inference. For better model interpretation, we further restrict the label and segmentation duration to be two sequences of discrete variables, respectively. In order to exploit forward and backward temporal information, we carefully design structured inference, and to overcome the difficulties of inferring discrete latent variables in deep neural networks, we resort to the recently proposed Gumbel-Softmax functions. The advantages of the proposed inference method in SSNN have been demonstrated in both synthetic and real-world sequential benchmarks."}, {"heading": "Acknowledgments", "text": "This paper was in part supported by Grants from the Natural Science Foundation of China (No. 61572111), the National High Technology Research and Development Program of China (863 Program) (No. 2015AA015408), a 985 Project of UESTC (No.A1098531023601041), and two Fundamental Research Funds for the Central Universities of China (Nos. ZYGX2016J078 and ZYGX2016Z003)."}], "references": [{"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi", "Christof Angermueller", "Dzmitry Bahdanau", "Nicolas Ballas", "Fr\u00e9d\u00e9ric Bastien", "Justin Bayer", "Anatoly Belikov", "Alexander Belopolsky"], "venue": "arXiv preprint arXiv:1605.02688,", "citeRegEx": "Al.Rfou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2016}, {"title": "Conditional random field autoencoders for unsupervised structured prediction", "author": ["Waleed Ammar", "Chris Dyer", "Noah A Smith"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Black box variational inference for state space models", "author": ["Evan Archer", "Il Memming Park", "Lars Buesing", "John Cunningham", "Liam Paninski"], "venue": "arXiv preprint arXiv:1511.07367,", "citeRegEx": "Archer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Archer et al\\.", "year": 2015}, {"title": "Learning stochastic recurrent networks", "author": ["Justin Bayer", "Christian Osendorfer"], "venue": "arXiv preprint arXiv:1411.7610,", "citeRegEx": "Bayer and Osendorfer.,? \\Q2014\\E", "shortCiteRegEx": "Bayer and Osendorfer.", "year": 2014}, {"title": "A recurrent latent variable model for sequential data", "author": ["Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Stochastic generative hashing", "author": ["Bo Dai", "Ruiqi Guo", "Sanjiv Kumar", "Niao He", "Le Song"], "venue": "arXiv preprint arXiv:1701.02815,", "citeRegEx": "Dai et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2017}, {"title": "Recurrent hidden semimarkov model", "author": ["Hanjun Dai", "Bo Dai", "Yan-Ming Zhang", "Shuang Li", "Le Song"], "venue": null, "citeRegEx": "Dai et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2017}, {"title": "Inference in hidden markov models with explicit state duration distributions", "author": ["Michael Dewar", "Chris Wiggins", "Frank Wood"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "Dewar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dewar et al\\.", "year": 2012}, {"title": "Variational recurrent auto-encoders", "author": ["Otto Fabius", "Joost R van Amersfoort"], "venue": "arXiv preprint arXiv:1412.6581,", "citeRegEx": "Fabius and Amersfoort.,? \\Q2014\\E", "shortCiteRegEx": "Fabius and Amersfoort.", "year": 2014}, {"title": "Bayesian nonparametric inference of switching dynamic linear models", "author": ["Emily Fox", "Erik B Sudderth", "Michael I Jordan", "Alan S Willsky"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Fox et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2011}, {"title": "Sequential neural models with stochastic layers", "author": ["Marco Fraccaro", "S\u00f8ren Kaae S\u00f8nderby", "Ulrich Paquet", "Ole Winther"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Fraccaro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fraccaro et al\\.", "year": 2016}, {"title": "Deep temporal sigmoid belief networks for sequence modeling", "author": ["Zhe Gan", "Chunyuan Li", "Ricardo Henao", "David E Carlson", "Lawrence Carin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Deep autoregressive networks", "author": ["Karol Gregor", "Ivo Danihelka", "Andriy Mnih", "Charles Blundell", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1310.8499,", "citeRegEx": "Gregor et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2013}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Neural adaptive sequential monte carlo", "author": ["Shixiang Gu", "Zoubin Ghahramani", "Richard E Turner"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Muprop: Unbiased backpropagation for stochastic neural networks", "author": ["Shixiang Gu", "Sergey Levine", "Ilya Sutskever", "Andriy Mnih"], "venue": "arXiv preprint arXiv:1511.05176,", "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Categorical reparameterization with gumbelsoftmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "venue": null, "citeRegEx": "Jang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jang et al\\.", "year": 2017}, {"title": "Stochastic variational inference for bayesian time series models", "author": ["Matthew Johnson", "Alan Willsky"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Johnson and Willsky.,? \\Q2014\\E", "shortCiteRegEx": "Johnson and Willsky.", "year": 2014}, {"title": "Composing graphical models with neural networks for structured representations and fast inference", "author": ["Matthew Johnson", "David K Duvenaud", "Alex Wiltschko", "Ryan P Adams", "Sandeep R Datta"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Bayesian nonparametric hidden semi-markov models", "author": ["Matthew J Johnson", "Alan S Willsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Johnson and Willsky.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Willsky.", "year": 2013}, {"title": "Leg-tracking and automated behavioural classification in drosophila", "author": ["Jamey Kain", "Chris Stokes", "Quentin Gaudry", "Xiangzhi Song", "James Foley", "Rachel Wilson", "Benjamin De Bivort"], "venue": "Nature communications,", "citeRegEx": "Kain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kain et al\\.", "year": 2013}, {"title": "Deep variational bayes filters: Unsupervised learning of state space models from raw data", "author": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "venue": "arXiv preprint arXiv:1605.06432,", "citeRegEx": "Karl et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karl et al\\.", "year": 2016}, {"title": "Kullback-leibler proximal variational inference", "author": ["Mohammad E Khan", "Pierre Baqu\u00e9", "Fran\u00e7ois Fleuret", "Pascal Fua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Khan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2015}, {"title": "Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models", "author": ["Mohammad Emtiyaz Khan", "Wu Lin"], "venue": "arXiv preprint arXiv:1703.04265,", "citeRegEx": "Khan and Lin.,? \\Q2017\\E", "shortCiteRegEx": "Khan and Lin.", "year": 2017}, {"title": "Faster stochastic variational inference using proximal-gradient methods with general divergence functions", "author": ["Mohammad Emtiyaz Khan", "Reza Babanezhad", "Wu Lin", "Mark Schmidt", "Masashi Sugiyama"], "venue": "arXiv preprint arXiv:1511.00146,", "citeRegEx": "Khan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Non-conjugate variational message passing for multinomial and binary regression", "author": ["David A Knowles", "Tom Minka"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Knowles and Minka.,? \\Q2011\\E", "shortCiteRegEx": "Knowles and Minka.", "year": 2011}, {"title": "Deep kalman filters", "author": ["Rahul G Krishnan", "Uri Shalit", "David Sontag"], "venue": "arXiv preprint arXiv:1511.05121,", "citeRegEx": "Krishnan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krishnan et al\\.", "year": 2015}, {"title": "Structured inference networks for nonlinear state space models", "author": ["Rahul G Krishnan", "Uri Shalit", "David Sontag"], "venue": "arXiv preprint arXiv:1609.09869,", "citeRegEx": "Krishnan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishnan et al\\.", "year": 2016}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Chris J Maddison", "Andriy Mnih", "Yee Whye Teh"], "venue": "arXiv preprint arXiv:1611.00712,", "citeRegEx": "Maddison et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2016}, {"title": "Variational inference for monte carlo objectives", "author": ["Andriy Mnih", "Danilo J Rezende"], "venue": "arXiv preprint arXiv:1602.06725,", "citeRegEx": "Mnih and Rezende.,? \\Q2016\\E", "shortCiteRegEx": "Mnih and Rezende.", "year": 2016}, {"title": "The blizzard challenge 2013\u2013indian language task", "author": ["Kishore Prahallad", "Anandaswarup Vadapalli", "Naresh Elluru", "G Mantena", "B Pulugundla", "P Bhaskararao", "HA Murthy", "S King", "V Karaiskos", "AW Black"], "venue": "In Blizzard Challenge Workshop,", "citeRegEx": "Prahallad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Prahallad et al\\.", "year": 2013}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rabiner.,? \\Q1989\\E", "shortCiteRegEx": "Rabiner.", "year": 1989}, {"title": "Transition-aware human activity recognition using smartphones", "author": ["Jorge-L Reyes-Ortiz", "Luca Oneto", "Albert Sama", "Xavier Parra", "Davide Anguita"], "venue": null, "citeRegEx": "Reyes.Ortiz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reyes.Ortiz et al\\.", "year": 2016}, {"title": "Discrete variational autoencoders", "author": ["Jason Tyler Rolfe"], "venue": "arXiv preprint arXiv:1609.02200,", "citeRegEx": "Rolfe.,? \\Q2016\\E", "shortCiteRegEx": "Rolfe.", "year": 2016}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Logistic regression-hsmm-based heart sound segmentation", "author": ["David B Springer", "Lionel Tarassenko", "Gari D Clifford"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "Springer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Springer et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1506.00619,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 36, "context": "Unsupervised structure learning in high-dimensional sequential data is an important research problem in a number of applications, such as machine translation, speech recognition, computational biology, and computational physiology Sutskever et al. (2014); Dai et al.", "startOffset": 231, "endOffset": 255}, {"referenceID": 6, "context": "(2014); Dai et al. (2017b). For example, in medical diagnosis, learning the segment boundaries and labeling of complicated physical signals is very useful for doctors to understand the underlying behavior or activity types.", "startOffset": 8, "endOffset": 27}, {"referenceID": 6, "context": "(2014); Dai et al. (2017b). For example, in medical diagnosis, learning the segment boundaries and labeling of complicated physical signals is very useful for doctors to understand the underlying behavior or activity types. Models for sequential data analysis such as recurrent neural networks (RNNs)Rumelhart et al. (1988) and hidden Markov models (HMMs)Rabiner (1989) are widely used.", "startOffset": 8, "endOffset": 324}, {"referenceID": 6, "context": "(2014); Dai et al. (2017b). For example, in medical diagnosis, learning the segment boundaries and labeling of complicated physical signals is very useful for doctors to understand the underlying behavior or activity types. Models for sequential data analysis such as recurrent neural networks (RNNs)Rumelhart et al. (1988) and hidden Markov models (HMMs)Rabiner (1989) are widely used.", "startOffset": 8, "endOffset": 370}, {"referenceID": 14, "context": "literature have investigated approaches of combining probabilistic generative models and recurrent neural networks for the sake of their complementary strengths in nonlinear representation learning and effective estimation of parameters Johnson et al. (2016); Dai et al.", "startOffset": 237, "endOffset": 259}, {"referenceID": 5, "context": "(2016); Dai et al. (2017b); Fraccaro et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 5, "context": "(2016); Dai et al. (2017b); Fraccaro et al. (2016). In many tasks, such as segmentation and labeling of natural scenes and physiological signals, the duration lengths and labels of segments are often interpretable and categorical distributed Jang et al.", "startOffset": 8, "endOffset": 51}, {"referenceID": 5, "context": "(2016); Dai et al. (2017b); Fraccaro et al. (2016). In many tasks, such as segmentation and labeling of natural scenes and physiological signals, the duration lengths and labels of segments are often interpretable and categorical distributed Jang et al. (2017). However, most of existing models are designed primarily for continuous situations and do not extend to discrete latent variables Johnson et al.", "startOffset": 8, "endOffset": 261}, {"referenceID": 5, "context": "(2016); Dai et al. (2017b); Fraccaro et al. (2016). In many tasks, such as segmentation and labeling of natural scenes and physiological signals, the duration lengths and labels of segments are often interpretable and categorical distributed Jang et al. (2017). However, most of existing models are designed primarily for continuous situations and do not extend to discrete latent variables Johnson et al. (2016); Krishnan et al.", "startOffset": 8, "endOffset": 413}, {"referenceID": 5, "context": "(2016); Dai et al. (2017b); Fraccaro et al. (2016). In many tasks, such as segmentation and labeling of natural scenes and physiological signals, the duration lengths and labels of segments are often interpretable and categorical distributed Jang et al. (2017). However, most of existing models are designed primarily for continuous situations and do not extend to discrete latent variables Johnson et al. (2016); Krishnan et al. (2015); Archer et al.", "startOffset": 8, "endOffset": 437}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks.", "startOffset": 8, "endOffset": 53}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework.", "startOffset": 8, "endOffset": 189}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework.", "startOffset": 8, "endOffset": 260}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density.", "startOffset": 8, "endOffset": 438}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al.", "startOffset": 8, "endOffset": 810}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al.", "startOffset": 8, "endOffset": 833}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al. (2017b) utilizes discrete variables with informative information for label prediction of segmentation, the inference approach does not explicitly take advantage of structured information to exploit the bi-directional temporal information, and thus may lead to suboptimal performance, as verified in the experiment (See Table 5.", "startOffset": 8, "endOffset": 874}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al. (2017b) utilizes discrete variables with informative information for label prediction of segmentation, the inference approach does not explicitly take advantage of structured information to exploit the bi-directional temporal information, and thus may lead to suboptimal performance, as verified in the experiment (See Table 5.3 in Section 5 for more details). To address such issues, we propose the Stochastic Sequential Neural Network (SSNN) consisting of a generative network and an inference network. The generative network(as will shown in Figure 2(a)) shares the spirit of Hidden Semi-Markov Model (HSMM) Rabiner (1989) and Recurrent HSMM Dai et al.", "startOffset": 8, "endOffset": 1492}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al. (2017b) utilizes discrete variables with informative information for label prediction of segmentation, the inference approach does not explicitly take advantage of structured information to exploit the bi-directional temporal information, and thus may lead to suboptimal performance, as verified in the experiment (See Table 5.3 in Section 5 for more details). To address such issues, we propose the Stochastic Sequential Neural Network (SSNN) consisting of a generative network and an inference network. The generative network(as will shown in Figure 2(a)) shares the spirit of Hidden Semi-Markov Model (HSMM) Rabiner (1989) and Recurrent HSMM Dai et al. (2017b), and is composed with a continuous sequence (i.", "startOffset": 8, "endOffset": 1530}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al. (2017b) utilizes discrete variables with informative information for label prediction of segmentation, the inference approach does not explicitly take advantage of structured information to exploit the bi-directional temporal information, and thus may lead to suboptimal performance, as verified in the experiment (See Table 5.3 in Section 5 for more details). To address such issues, we propose the Stochastic Sequential Neural Network (SSNN) consisting of a generative network and an inference network. The generative network(as will shown in Figure 2(a)) shares the spirit of Hidden Semi-Markov Model (HSMM) Rabiner (1989) and Recurrent HSMM Dai et al. (2017b), and is composed with a continuous sequence (i.e., hidden states in RNN) as well as two discrete sequences (i.e., segmentation variables and labels in SSM). The inference network(as will shown in Figure 2(b)) can take the advantages of bi-directional temporal information by augmented variables, and efficiently approximate the categorical variables in segmentation and segment labels via the recently proposed Gumbel-Softmax approximation Jang et al. (2017); Maddison et al.", "startOffset": 8, "endOffset": 1989}, {"referenceID": 2, "context": "(2015); Archer et al. (2015); Krishnan et al. (2016), probably due to the difficulty of inference for discrete variables in neural networks. For example, the work in Krishnan et al. (2015) considers combining variational autoencoders Kingma and Welling (2013) with continuous state-space models, aiming to capture nonlinear dynamics with control inputs and leading to an RNN-based variational framework. The work in Johnson et al. (2016) proposes a state space model with a general emission density. When composed with neural networks, state space models are natural to model discrete variables. While discrete variables can be more interpretable and helpful in many applications like medical analysis and behavior prediction, they are less considered as switching variables in previous work Fox et al. (2011); Johnson et al. (2016). Although the work in Dai et al. (2017b) utilizes discrete variables with informative information for label prediction of segmentation, the inference approach does not explicitly take advantage of structured information to exploit the bi-directional temporal information, and thus may lead to suboptimal performance, as verified in the experiment (See Table 5.3 in Section 5 for more details). To address such issues, we propose the Stochastic Sequential Neural Network (SSNN) consisting of a generative network and an inference network. The generative network(as will shown in Figure 2(a)) shares the spirit of Hidden Semi-Markov Model (HSMM) Rabiner (1989) and Recurrent HSMM Dai et al. (2017b), and is composed with a continuous sequence (i.e., hidden states in RNN) as well as two discrete sequences (i.e., segmentation variables and labels in SSM). The inference network(as will shown in Figure 2(b)) can take the advantages of bi-directional temporal information by augmented variables, and efficiently approximate the categorical variables in segmentation and segment labels via the recently proposed Gumbel-Softmax approximation Jang et al. (2017); Maddison et al. (2016). Thus, SSNN can model the complex and long-range dependencies in sequential data, but also maintain the structure learning ability of SSMs with efficient inference.", "startOffset": 8, "endOffset": 2013}, {"referenceID": 37, "context": "Recurrent neural networks (RNNs), as a wide range of sequential models for time series, have been applied in a number of applications Sutskever et al. (2014). Here we introduce basic properties and notations of RNNs.", "startOffset": 134, "endOffset": 158}, {"referenceID": 6, "context": "State space models such as hidden Markov models (HMMs) and hidden Semi-Markov models (HSMMs) are also widely-used methods in sequential learning Dai et al. (2017b); Chiappa et al.", "startOffset": 145, "endOffset": 164}, {"referenceID": 6, "context": "State space models such as hidden Markov models (HMMs) and hidden Semi-Markov models (HSMMs) are also widely-used methods in sequential learning Dai et al. (2017b); Chiappa et al. (2014); Dewar et al.", "startOffset": 145, "endOffset": 187}, {"referenceID": 6, "context": "State space models such as hidden Markov models (HMMs) and hidden Semi-Markov models (HSMMs) are also widely-used methods in sequential learning Dai et al. (2017b); Chiappa et al. (2014); Dewar et al. (2012). In HMM, given an observed sequence x1:T , each xt is generated based on the hidden state zt \u2208 {1, 2, .", "startOffset": 145, "endOffset": 208}, {"referenceID": 19, "context": "There are many variants of HSMMs such as the Hierarchical Dirichlet-Process HSMM (HDP-HSMM) Johnson and Willsky (2013) and subHSMM Johnson and Willsky (2014).", "startOffset": 92, "endOffset": 119}, {"referenceID": 19, "context": "There are many variants of HSMMs such as the Hierarchical Dirichlet-Process HSMM (HDP-HSMM) Johnson and Willsky (2013) and subHSMM Johnson and Willsky (2014). The subHSMM and HDP-HSMM extend their HMM counterparts by allowing explicit modeling of state duration lengths with arbitrary distributions.", "startOffset": 92, "endOffset": 158}, {"referenceID": 2, "context": "Recent methods in Bayesian learning, such as the score function (or REINFORCE) Archer et al. (2015) and the Stochastic Gradient Variational Bayes (SGVB) Kingma and Welling (2013), are common black-box methods that lead to tractable solutions.", "startOffset": 79, "endOffset": 100}, {"referenceID": 2, "context": "Recent methods in Bayesian learning, such as the score function (or REINFORCE) Archer et al. (2015) and the Stochastic Gradient Variational Bayes (SGVB) Kingma and Welling (2013), are common black-box methods that lead to tractable solutions.", "startOffset": 79, "endOffset": 179}, {"referenceID": 2, "context": "Recent methods in Bayesian learning, such as the score function (or REINFORCE) Archer et al. (2015) and the Stochastic Gradient Variational Bayes (SGVB) Kingma and Welling (2013), are common black-box methods that lead to tractable solutions. We resort to the SGVB since it could efficiently learn the approximation with relatively low variances Kingma and Welling (2013), while the score function suffers from high variances and heavy computational costs.", "startOffset": 79, "endOffset": 372}, {"referenceID": 31, "context": "Such attempts have been explored in many previous work (Krishnan et al., 2016; Khan and Lin, 2017; Krishnan et al., 2015), however they mainly focus on continuous variables, and little attention is paid to the discrete variable.", "startOffset": 55, "endOffset": 121}, {"referenceID": 25, "context": "Such attempts have been explored in many previous work (Krishnan et al., 2016; Khan and Lin, 2017; Krishnan et al., 2015), however they mainly focus on continuous variables, and little attention is paid to the discrete variable.", "startOffset": 55, "endOffset": 121}, {"referenceID": 30, "context": "Such attempts have been explored in many previous work (Krishnan et al., 2016; Khan and Lin, 2017; Krishnan et al., 2015), however they mainly focus on continuous variables, and little attention is paid to the discrete variable.", "startOffset": 55, "endOffset": 121}, {"referenceID": 16, "context": "Specifically, we first learn a bi-directional deterministic variable \u0125t = BiRNN(x1:t, xt:T ) , where BiRNN is a bi-directional RNN with each unit implemented as an LSTM Hochreiter and Schmidhuber (1997). Similar to Fraccaro et al.", "startOffset": 169, "endOffset": 203}, {"referenceID": 11, "context": "Similar to Fraccaro et al. (2016), we further use a backward recurrent function It = g\u03c6I (It+1, [xt, \u0125t]) to explicitly capture forward and backward information in the sequence via \u0125t, where [xt, \u0125t] is the concatenation of xt and \u0125t.", "startOffset": 11, "endOffset": 34}, {"referenceID": 5, "context": "However, the reparameterization tricks and their extensions (Chung et al., 2016) are not directly applicable due to the discrete random variables, i.", "startOffset": 60, "endOffset": 80}, {"referenceID": 18, "context": "Thus we turn to the recently proposed Gumbel-Softmax reparameterization trick (Jang et al., 2017; Maddison et al., 2016), as shown in the following.", "startOffset": 78, "endOffset": 120}, {"referenceID": 32, "context": "Thus we turn to the recently proposed Gumbel-Softmax reparameterization trick (Jang et al., 2017; Maddison et al., 2016), as shown in the following.", "startOffset": 78, "endOffset": 120}, {"referenceID": 32, "context": "Via the Gumbel Softmax transformation, we set y(t) \u223c Concrete(\u03c0(t), \u03c4) according to (Maddison et al., 2016).", "startOffset": 84, "endOffset": 107}, {"referenceID": 27, "context": "Now we can sample y(t) from the Gumbel-Softmax posterior in replacement of the categorically distributed \u03b3(t), and use the back-propagation gradient with the ADAM Kingma and Ba (2014) optimizer to learn parameters \u03b8 and \u03c6.", "startOffset": 163, "endOffset": 184}, {"referenceID": 10, "context": "In terms of combining both and SSM and RNNs, the papers mostly close to our paper include Johnson et al. (2016); Krishnan et al.", "startOffset": 90, "endOffset": 112}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al.", "startOffset": 14, "endOffset": 59}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems.", "startOffset": 14, "endOffset": 94}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering.", "startOffset": 14, "endOffset": 246}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning.", "startOffset": 14, "endOffset": 381}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference.", "startOffset": 14, "endOffset": 580}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables.", "startOffset": 14, "endOffset": 665}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network.", "startOffset": 14, "endOffset": 1161}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance.", "startOffset": 14, "endOffset": 1439}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients.", "startOffset": 14, "endOffset": 1612}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances.", "startOffset": 14, "endOffset": 1864}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner.", "startOffset": 14, "endOffset": 1970}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner. Dai et al. (2017b) proposes to overcome the difficulty of learning discrete variables by optimizing their distribution instead of directly learning discrete variables.", "startOffset": 14, "endOffset": 2128}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner. Dai et al. (2017b) proposes to overcome the difficulty of learning discrete variables by optimizing their distribution instead of directly learning discrete variables. In the aspect of optimization, Khan et al. (2015a,b) split the variational inference objective into a term to be linearized and a tractable concave term, which makes the resulting gradient easily to compute. Knowles and Minka (2011) proposes natural gradient descent with respect to natural parameters on each of the variational factors in turn.", "startOffset": 14, "endOffset": 2510}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner. Dai et al. (2017b) proposes to overcome the difficulty of learning discrete variables by optimizing their distribution instead of directly learning discrete variables. In the aspect of optimization, Khan et al. (2015a,b) split the variational inference objective into a term to be linearized and a tractable concave term, which makes the resulting gradient easily to compute. Knowles and Minka (2011) proposes natural gradient descent with respect to natural parameters on each of the variational factors in turn. In Dai et al. (2017a), the discrete optimization is replaced by the maximization over the negative Helmholtz free energy.", "startOffset": 14, "endOffset": 2645}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner. Dai et al. (2017b) proposes to overcome the difficulty of learning discrete variables by optimizing their distribution instead of directly learning discrete variables. In the aspect of optimization, Khan et al. (2015a,b) split the variational inference objective into a term to be linearized and a tractable concave term, which makes the resulting gradient easily to compute. Knowles and Minka (2011) proposes natural gradient descent with respect to natural parameters on each of the variational factors in turn. In Dai et al. (2017a), the discrete optimization is replaced by the maximization over the negative Helmholtz free energy. In contrast to linearizing intractable terms around the current iteration as used in the above approaches, we handle intractable terms via recognition networks and amortized inference(with the aid of Gumbel-Softmax reparameterization Jang et al. (2017); Maddison et al.", "startOffset": 14, "endOffset": 2998}, {"referenceID": 2, "context": "(2015, 2016); Archer et al. (2015); Fraccaro et al. (2016). In detail, Krishnan et al. (2015) combines variational auto-encoders with continuous state-space models, emphasizing the relationship to linear dynamical systems. Krishnan et al. (2016) lets inference network conditioned on both future and past hidden variables, which extends Deep Kalman Filtering. Archer et al. (2015) uses a structured Gaussian variational family to solve the problem of variational inference in general continuous state space models without considering parameter learning. And Johnson et al. (2016) can employ general emission density for structured inference. Fraccaro et al. (2016) extends state space models by combining recurrent neural networks with stochastic latent variables. Different from the above methods that require the hidden states of SSM be continuous, our paper utilizes discrete latent variables in the SSM part for better interpretablity, especially in applications of segmentation and labeling of high-dimensional time series. In parallel, some research also works on variational inference with discrete latent variables recently. Bayer and Osendorfer (2014) enhances recurrent neural networks with stochastic latent variables which they call stochastic neural network. For stochastic neural network the most applicable approach is the score function or REINFORCE approach, however it suffers from high variance. Mnih and Rezende (2016) proposes a gradient estimator for multi-sample objectives that use the mean of other samples to construct a baseline for each sample to decrease variance. Gu et al. (2015b) also models the baseline as a first-order Taylor expansion and overcomes back propagation through discrete sampling with a meanfield approximation, so it becomes practical to compute the baseline and derive the relevant gradients. Gregor et al. (2013) uses the first-order Taylor approximation as a baseline to reduce variances. In Discrete VAE Rolfe (2016), the sampling is autoregressive through each binary unit, which allows every discrete choice to be marginalized out in a tractable manner. Dai et al. (2017b) proposes to overcome the difficulty of learning discrete variables by optimizing their distribution instead of directly learning discrete variables. In the aspect of optimization, Khan et al. (2015a,b) split the variational inference objective into a term to be linearized and a tractable concave term, which makes the resulting gradient easily to compute. Knowles and Minka (2011) proposes natural gradient descent with respect to natural parameters on each of the variational factors in turn. In Dai et al. (2017a), the discrete optimization is replaced by the maximization over the negative Helmholtz free energy. In contrast to linearizing intractable terms around the current iteration as used in the above approaches, we handle intractable terms via recognition networks and amortized inference(with the aid of Gumbel-Softmax reparameterization Jang et al. (2017); Maddison et al. (2016)) in this paper.", "startOffset": 14, "endOffset": 3022}, {"referenceID": 35, "context": "Then we test SSNN with learning segmentations and latent labels on Human activity Reyes-Ortiz et al. (2016) dataset, Drosophila dataset Kain et al.", "startOffset": 82, "endOffset": 108}, {"referenceID": 22, "context": "(2016) dataset, Drosophila dataset Kain et al. (2013) and PhysioNet Springer et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 26, "context": "All models in the experiment use the Adam Kingma and Ba (2014) optimizer.", "startOffset": 42, "endOffset": 63}, {"referenceID": 0, "context": "We implement the proposed model based on Theano Al-Rfou et al. (2016) and Block & Fuel Van Merri\u00ebnboer et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 0, "context": "We implement the proposed model based on Theano Al-Rfou et al. (2016) and Block & Fuel Van Merri\u00ebnboer et al. (2015).", "startOffset": 48, "endOffset": 117}, {"referenceID": 23, "context": "For fair comparison with Karl et al. (2016), we set m = l = 1, \u03bc = 0.", "startOffset": 25, "endOffset": 44}, {"referenceID": 23, "context": "For fair comparison with Karl et al. (2016), we set m = l = 1, \u03bc = 0.5, and g = 9.81. We convert the generated ground-truth angles to image observations. The system can be fully described by angle and angular velocity. We compare our method with Deep Variational Bayes Filter(DVBF-LL) Karl et al. (2016) and Deep Kalman Filters(DKF) Krishnan et al.", "startOffset": 25, "endOffset": 304}, {"referenceID": 23, "context": "For fair comparison with Karl et al. (2016), we set m = l = 1, \u03bc = 0.5, and g = 9.81. We convert the generated ground-truth angles to image observations. The system can be fully described by angle and angular velocity. We compare our method with Deep Variational Bayes Filter(DVBF-LL) Karl et al. (2016) and Deep Kalman Filters(DKF) Krishnan et al. (2015). The ordinary least square regression results are shown in Table 1.", "startOffset": 25, "endOffset": 356}, {"referenceID": 29, "context": ", Blizzard and TIMIT datasets Prahallad et al. (2013). Blizzard records the English speech with 300 hours by a female speaker.", "startOffset": 30, "endOffset": 54}, {"referenceID": 4, "context": "Speech modeling on these two datasets has shown to be challenging since there\u2019s no good representation of the latent states Chung et al. (2015); Fabius and van Amersfoort (2014); Gu et al.", "startOffset": 124, "endOffset": 144}, {"referenceID": 4, "context": "Speech modeling on these two datasets has shown to be challenging since there\u2019s no good representation of the latent states Chung et al. (2015); Fabius and van Amersfoort (2014); Gu et al.", "startOffset": 124, "endOffset": 178}, {"referenceID": 4, "context": "Speech modeling on these two datasets has shown to be challenging since there\u2019s no good representation of the latent states Chung et al. (2015); Fabius and van Amersfoort (2014); Gu et al. (2015a); Gan et al.", "startOffset": 124, "endOffset": 197}, {"referenceID": 4, "context": "Speech modeling on these two datasets has shown to be challenging since there\u2019s no good representation of the latent states Chung et al. (2015); Fabius and van Amersfoort (2014); Gu et al. (2015a); Gan et al. (2015); Sutskever et al.", "startOffset": 124, "endOffset": 216}, {"referenceID": 4, "context": "Speech modeling on these two datasets has shown to be challenging since there\u2019s no good representation of the latent states Chung et al. (2015); Fabius and van Amersfoort (2014); Gu et al. (2015a); Gan et al. (2015); Sutskever et al. (2014).", "startOffset": 124, "endOffset": 241}, {"referenceID": 4, "context": "The data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al.", "startOffset": 87, "endOffset": 107}, {"referenceID": 4, "context": "The data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al. (2016), i.", "startOffset": 87, "endOffset": 131}, {"referenceID": 4, "context": "The data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al. (2016), i.e. we report the average log-likelihood for half-second sequences on Blizzard, and report the average log-likelihood per sequence for the test set sequences on TIMIT. For the raw audio datasets, we use a fully factorized Gaussian output distribution. In the experiment, We split the raw audio signals in the chunks of 2 seconds. The waveforms are divided into non-overlapping vectors with size 200. For Blizzard we split the data using 90% for training, 5% for validation and 5% for testing. For testing we report the average log-likelihood for each sequence with segment length 0.5s. For TIMIT we use the predefined test set for testing and split the rest of the data into 95% for training and 5% for validation. During training we use back-propagation through time (BPTT) for 1 second. For the first second we initialize hidden units with zeros and for the subsequent 3 chunks we use the previous hidden states as initialization. the temperature \u03c4 starts from a large value 0.1 and gradually anneals to 0.01. We compare our method with the following methods. For RNN+VRNNs Chung et al. (2015), VRNN is tested with two different output distributions: a Gaussian distribution (VRNN-GAUSS), and a Gaussian Mixture Model (VRNN-GMM).", "startOffset": 87, "endOffset": 1229}, {"referenceID": 4, "context": "The data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al. (2016), i.e. we report the average log-likelihood for half-second sequences on Blizzard, and report the average log-likelihood per sequence for the test set sequences on TIMIT. For the raw audio datasets, we use a fully factorized Gaussian output distribution. In the experiment, We split the raw audio signals in the chunks of 2 seconds. The waveforms are divided into non-overlapping vectors with size 200. For Blizzard we split the data using 90% for training, 5% for validation and 5% for testing. For testing we report the average log-likelihood for each sequence with segment length 0.5s. For TIMIT we use the predefined test set for testing and split the rest of the data into 95% for training and 5% for validation. During training we use back-propagation through time (BPTT) for 1 second. For the first second we initialize hidden units with zeros and for the subsequent 3 chunks we use the previous hidden states as initialization. the temperature \u03c4 starts from a large value 0.1 and gradually anneals to 0.01. We compare our method with the following methods. For RNN+VRNNs Chung et al. (2015), VRNN is tested with two different output distributions: a Gaussian distribution (VRNN-GAUSS), and a Gaussian Mixture Model (VRNN-GMM). We also compare to VRNN-I in which the latent variables in VRNN are constrained to be independent across time steps. For SRNN Fraccaro et al. (2016), we compare with the smoothing and filtering performance denoted as SRRR (smooth), SRNN (filt) and SRNN (smooth+ Resq) respectively.", "startOffset": 87, "endOffset": 1514}, {"referenceID": 4, "context": "The data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al. (2016), i.e. we report the average log-likelihood for half-second sequences on Blizzard, and report the average log-likelihood per sequence for the test set sequences on TIMIT. For the raw audio datasets, we use a fully factorized Gaussian output distribution. In the experiment, We split the raw audio signals in the chunks of 2 seconds. The waveforms are divided into non-overlapping vectors with size 200. For Blizzard we split the data using 90% for training, 5% for validation and 5% for testing. For testing we report the average log-likelihood for each sequence with segment length 0.5s. For TIMIT we use the predefined test set for testing and split the rest of the data into 95% for training and 5% for validation. During training we use back-propagation through time (BPTT) for 1 second. For the first second we initialize hidden units with zeros and for the subsequent 3 chunks we use the previous hidden states as initialization. the temperature \u03c4 starts from a large value 0.1 and gradually anneals to 0.01. We compare our method with the following methods. For RNN+VRNNs Chung et al. (2015), VRNN is tested with two different output distributions: a Gaussian distribution (VRNN-GAUSS), and a Gaussian Mixture Model (VRNN-GMM). We also compare to VRNN-I in which the latent variables in VRNN are constrained to be independent across time steps. For SRNN Fraccaro et al. (2016), we compare with the smoothing and filtering performance denoted as SRRR (smooth), SRNN (filt) and SRNN (smooth+ Resq) respectively. The results of VRNN-GMM, VRNN-Gauss and VRNN-I-Gauss are taken from Chung et al. (2015), and those of SRNN (smooth+Resq), SRNN (smooth) and SRNN (filt) are taken from Fraccaro et al.", "startOffset": 87, "endOffset": 1735}, {"referenceID": 4, "context": "The data preprocessing and the performance measures are identical to those reported in Chung et al. (2015); Fraccaro et al. (2016), i.e. we report the average log-likelihood for half-second sequences on Blizzard, and report the average log-likelihood per sequence for the test set sequences on TIMIT. For the raw audio datasets, we use a fully factorized Gaussian output distribution. In the experiment, We split the raw audio signals in the chunks of 2 seconds. The waveforms are divided into non-overlapping vectors with size 200. For Blizzard we split the data using 90% for training, 5% for validation and 5% for testing. For testing we report the average log-likelihood for each sequence with segment length 0.5s. For TIMIT we use the predefined test set for testing and split the rest of the data into 95% for training and 5% for validation. During training we use back-propagation through time (BPTT) for 1 second. For the first second we initialize hidden units with zeros and for the subsequent 3 chunks we use the previous hidden states as initialization. the temperature \u03c4 starts from a large value 0.1 and gradually anneals to 0.01. We compare our method with the following methods. For RNN+VRNNs Chung et al. (2015), VRNN is tested with two different output distributions: a Gaussian distribution (VRNN-GAUSS), and a Gaussian Mixture Model (VRNN-GMM). We also compare to VRNN-I in which the latent variables in VRNN are constrained to be independent across time steps. For SRNN Fraccaro et al. (2016), we compare with the smoothing and filtering performance denoted as SRRR (smooth), SRNN (filt) and SRNN (smooth+ Resq) respectively. The results of VRNN-GMM, VRNN-Gauss and VRNN-I-Gauss are taken from Chung et al. (2015), and those of SRNN (smooth+Resq), SRNN (smooth) and SRNN (filt) are taken from Fraccaro et al. (2016). From Table 5.", "startOffset": 87, "endOffset": 1837}, {"referenceID": 35, "context": "To show the advantages of SSNN over HSMM and its variants when learning the segmentation and latent labels from sequences, we take experiments on Human activity Reyes-Ortiz et al. (2016), Drosophila dataset Kain et al.", "startOffset": 161, "endOffset": 187}, {"referenceID": 22, "context": "(2016), Drosophila dataset Kain et al. (2013) and PhysioNet Springer et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 22, "context": "(2016), Drosophila dataset Kain et al. (2013) and PhysioNet Springer et al. (2016) Challenge dataset.", "startOffset": 27, "endOffset": 83}, {"referenceID": 16, "context": "The results of subHSMM, HDP-HSMM, CRF-AE and RHSMM-DP are taken from subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al.", "startOffset": 77, "endOffset": 104}, {"referenceID": 16, "context": "The results of subHSMM, HDP-HSMM, CRF-AE and RHSMM-DP are taken from subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al.", "startOffset": 77, "endOffset": 141}, {"referenceID": 1, "context": "The results of subHSMM, HDP-HSMM, CRF-AE and RHSMM-DP are taken from subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al. (2014) and rHSMM-dp Dai et al.", "startOffset": 149, "endOffset": 169}, {"referenceID": 1, "context": "The results of subHSMM, HDP-HSMM, CRF-AE and RHSMM-DP are taken from subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al. (2014) and rHSMM-dp Dai et al. (2017b).", "startOffset": 149, "endOffset": 201}, {"referenceID": 16, "context": "We report the comparison with subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al.", "startOffset": 38, "endOffset": 65}, {"referenceID": 16, "context": "We report the comparison with subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al.", "startOffset": 38, "endOffset": 102}, {"referenceID": 1, "context": "We report the comparison with subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al. (2014) and rHSMM-dp Dai et al.", "startOffset": 110, "endOffset": 130}, {"referenceID": 1, "context": "We report the comparison with subHSMM Johnson and Willsky (2014), HDP-HSMM Johnson and Willsky (2013), CRF-AE Ammar et al. (2014) and rHSMM-dp Dai et al. (2017b). For the HDP-HSMM and subHSMM, the observed sequences x1:T are generated by standard multivariate Gaussian distributions.", "startOffset": 110, "endOffset": 162}, {"referenceID": 13, "context": "We compare the proposed model to DRAW Gregor et al. (2015) and visualize our learned latent representations in Figure 5.", "startOffset": 38, "endOffset": 59}], "year": 2017, "abstractText": "Unsupervised structure learning in high-dimensional time series data has attracted a lot of research interests. For example, segmenting and labelling high dimensional time series can be helpful in behavior understanding and medical diagnosis. Recent advances in generative sequential modeling have suggested to combine recurrent neural networks with state space models (e.g., Hidden Markov Models). This combination can model not only the long term dependency in sequential data, but also the uncertainty included in the hidden states. Inheriting these advantages of stochastic neural sequential models, we propose a structured and stochastic sequential neural network, which models both the long-term dependencies via recurrent neural networks and the uncertainty in the segmentation and labels via discrete random variables. For accurate and efficient inference, we present a bi-directional inference network by reparamterizing the categorical segmentation and labels with the recent proposed Gumbel-Softmax approximation and resort to the Stochastic Gradient Variational Bayes. We evaluate the proposed model in a number of tasks, including speech modeling, automatic segmentation and labeling in behavior understanding, and sequential multi-objects recognition. Experimental results have demonstrated that our proposed model can achieve significant improvement over the state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}