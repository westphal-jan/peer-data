{"id": "1705.09436", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2017", "title": "Human Trajectory Prediction using Spatially aware Deep Attention Models", "abstract": "trajectory prediction of dynamic objects is a widely studied discipline in the field of artificial intelligence. pointing to a large number of applications like predicting abnormal events, navigation system for the blind, etc. there have been many approaches to attempt learning patterns of motion directly from data using a wide variety of techniques ranging from hand - crafted features to sophisticated deep learning models for unsupervised feature learning. all these approaches have been limited by problems introducing inefficient features in the field of smooth manipulated features, large error propagation across the visual trajectory and no information of static artefacts around the dynamic skating scene. we build an end to end feature learning model to learn the dynamic pattern of humans using typical navigational modules estimated from data using the much anticipated sequence to quote model coupled with a soft attention mechanism. we also propose a modified approach to model the static artefacts in animation scene and using these to predict the dynamic trajectories. the proposed pathways, tested on trajectories of pedestrians, consistently outperforms previously proposed state of the art properties on a variety their large scale data sets. we also show how our architecture can be naturally extended to handle three modes of movement ( say pedestrians, skaters, bikers and buses ) simultaneously.", "histories": [["v1", "Fri, 26 May 2017 05:37:36 GMT  (2530kb,D)", "http://arxiv.org/abs/1705.09436v1", "10 pages, 5 figures, Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017)"]], "COMMENTS": "10 pages, 5 figures, Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["daksh varshneya", "g srinivasaraghavan"], "accepted": false, "id": "1705.09436"}, "pdf": {"name": "1705.09436.pdf", "metadata": {"source": "CRF", "title": "Human Trajectory Prediction using Spatially aware Deep Attention Models", "authors": ["Daksh Varshneya", "G. Srinivasaraghavan"], "emails": ["daksh.varshneya@iiitb.org", "gsr@iiitb.ac.in"], "sections": [{"heading": "1 Introduction", "text": "Learning and inference from visual data have gained tremendous prominence in Artificial Intelligence research in recent times. Much of this has been due to the breakthrough advances in AI and Deep Learning that have enabled vision and image processing systems to achieve near human precision on many complex visual recognition tasks. In this paper we present a method to learn and predict the dynamic spatio-temporal behaviour of people moving using multiple navigational modes in crowded scenes. As humans we possess the ability to effortlessly navigate ourselves in crowded areas while walking or driving a vehicle. Here we propose an end-to-end Deep Learning system that can learn such constrained navigational behaviour by considering multiple influencing factors such as the neighbouring dynamic subjects and also the spatial context in which the subject is. We also show how our architecture can be naturally extended to handle multiple modes (say pedestrians, skaters, bikers and buses) simultaneously."}, {"heading": "2 Problem Statement", "text": "We formulate the problem as a learning-cum-inferencing task. The question we seek to answer is \u201chaving observed the trajectories [x(1)i , . . . ,x (T ) i ], 1 \u2264 i \u2264 N of N moving subjects for T time units,\n\u2217International Institute of Information Technology, Bangalore, KA, 560100\nSubmitted to 31st Conference on Neural Information Processing Systems (NIPS 2017). Do not distribute.\nar X\niv :1\n70 5.\n09 43\n6v 1\n[ cs\n.L G\n] 2\n6 M\nay 2\nwhere are each of these subjects likely to be at times T1, . . . , Tn after the initial observation time T?\u201d where x(t)i denotes the 2-dimensional spatial co-ordinate vector of the i\nth target subject at time t. In other words, we need to predict values of [x(T1)i , . . . ,x (Tn) i ]. We assume we have annotations of the spatial tracks of each unique subject that is participating in the scene."}, {"heading": "3 Related Work", "text": ""}, {"heading": "3.1 RNNs for Sequence to Sequence modeling", "text": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16]. These architectures encode the incoming sequential data into a fixed size hidden representation using a Recurrent Neural Network(RNN) and then decode this hidden representation using another RNN to produce a sequentially temporal output.\nThese networks have also been modified to introduce an attention mechanism into them. These networks are inspired by the attention mechanism that humans possess visually. We as humans, adjust our focal point over time to focus more at a specific region of our sight to a higher resolution and the surrounding area to a lower resolution. Attention Networks have been used very successfully for automatic fine-grained image description/annotation [7]."}, {"heading": "3.2 Object-Object Interaction modeling", "text": "Helbing and Molnar\u2019s social force model[17] was the first to learn interaction patterns between different objects such as attractive and repulsive forces. Since then, several variants such as (i) agent based modeling[18] to use human attributes as model priors to learn behavioral patterns, and (ii) feature engineered approaches like that of Alahi et.al.[19] which extract social affinity features to learn such patterns, have been explored.\nOther approaches include finding the most common object behaviour by means of clustering[14, 15]. Giannotti et. al.[14] analyzed GPS traces of several fleets of buses and extracted patterns in trajectories as concise descriptions of frequent behaviour, both temporally and spatially.\nMost recently, Alahi et.al.[22] captured the interactions between pedestrians using multiple Long Short Term Memory Networks(LSTMs)[23] and a social pooling mechanism to capture humanhuman interactions. While they captured this dynamic interaction, their model failed to understand the static spatial semantics of the scene. Such spatial modeling exists in [20, 21] but these do not include the dynamic modeling of the crowd.\nNone of the above approaches naturally extend to multiple classes of moving subjects."}, {"heading": "3.3 Spatial Context modeling", "text": "Earlier works include matching-based approaches[8, 9] which rely on keypoint feature matching techniques. These are slow to compute since they need to match each test image with a database of images. Also, since it is a direct matching approach there is no semantic understanding of the scene. Most of the conventional approaches tend to be brittle since they rely heavily on hand-crafted features.\nA deep learning approach was also used for spatial context modeling in [10]. This work hypothesizes that dynamic objects and static objects can be matched semantically based on the interaction they have between each other. The authors assume that a random image patch of the scene contains enough evidence, based on which the discrimination between likely patches and unlikely patches can be made for a particular object. They did not explore the possibility of adding additional static context around the patches to augment the model. We build on this distinction later on in section 4.1."}, {"heading": "4 Proposed System", "text": "This section describes our solution in detail and is organized as follows. Firstly, we describe our proposed Spatially Static Context Network(SSCN) to model the static spatial context around the subject of interest. Next, we define the Pooling Mechanism which captures the influence, the\nneighboring subjects and the nearby static artefacts have on a target subject. Next, we describe our complete model which uses an attention mechanism with LSTMs to learn patterns from spatial co-ordinates of subjects while preserving the spatial and dynamic context around the subjects of interest. Though LSTMs have traditionally been used to model (typically short-term) temporal dependencies in sequential data, their use along with an appropriate attention mechanism enables us to use them for tasks like trajectory planning that requires long-term dependency modeling. We also show how this extends in principle to multiple classes of moving subjects."}, {"heading": "4.1 Spatial Context Matching", "text": "Modeling the spatial context in a given scene is a challenging task since it should be semantically representative and also highly discriminative. It is also very important for the model to generalize to a variety of complex scenes and enable inferences about human-space interactions in them.\nOur proposed architecture is composed of Convolutional Neural Networks(CNNs)[24] and is inspired by the Spatial Matching Network introduced in [10]. Although, our architecture is very similar to the one proposed in this work, we differ significantly from this approach in two ways. First, it is redundant to build an input branch which takes image patches of different objects because an object belonging to the same semantic class (car, pedestrian, bicyclist) should ideally have the same spatial matching score with any particular random patch of the image. For example, any car should have the same matching score with a patch of road and hence we do not need to differentiate between different cars.\nSecond, it might be difficult for a network to look at a small scene patch and infer the matching score from it. For instance, a trained CNN might have learned different textures of different static artefacts like that of a road or a pavement, but such textures could occur anywhere in the image. For example, the texture of the roof in a particular scene could match the texture of the pavement in a different scene image. So it is important for the network to have information about the larger context or the region surrounding the input patch. This will help the network to generalize better across different scenes and have a better semantic understanding of different complex scenes.\nWe incorporate our hypothesis in our proposed network shown in figure 1. We call this network as the Spatially Static Context Network(SSCN). The network has three input streams - Subject stream, Patch stream and Context stream.\nFor the subject input stream, we input a class label s \u2208 {0, 1, 2, . . .} to indicate whichever semantic class the dynamic subject belongs to. This input is passed through an embedding layer followed by a dense fully connected layer.\nFor the patch stream the input image is that of the grid cell of interest \u03c1. To incorporate a local context around that patch, we also take the part of the image surrounding that cell. Hence, we take the size of the grid cell of interest as g\u03c1 \u00d7 g\u03c1 and add an annulus of width g\u03c1 around it (we appropriately pad the original image in order to add such a context for the cells on the boundary of the image). Thus, the final size of input patch becomes 3g\u03c1 \u00d7 3g\u03c1. This is later re-sized to dP \u00d7 dP for a fixed dP . We pass this input patch through a stack of convolutional layers along with pooling and local response normalization after each layer followed by a dense fully connected layer as shown in figure 1.\nThe third stream, that captures the overall image context, is in line with our second hypothesis. This stream of the network is incorporated to introduce a global context around the image patch of interest. The input to this stream \u03b6 is the whole scene image re-sized to a fixed dimension size of dI \u00d7 dI . We again use another stack of convolutional layers along with pooling and local response normalization\nafter each layer followed by a dense fully connected layer to extract hierarchical features of the scene image.\nTo train such a network, we merge all the input streams by concatenating the outputs of their respective dense fully connected layers and add a stack of 2 fully connected layers followed by a fully-connected output layer. The output layer consists of just a single sigmoid neuron, giving the likelihood of a subject of type s stepping on the given patch \u03c1. The ground truth likelihood we use for training is the value associated with the grid cell of interest (not the complete input patch).\nWe train the model over all triplets (s, \u03c1, \u03b6) \u2208 D of subject type, patch and image, by minimizing the cross entropy between the actual likelihood \u00b5s\u03c1\u03b6 and the predicted likelihood \u00b5\u0302s\u03c1\u03b6 -\nHD = \u2212 1 |D| \u2211\n(s,\u03c1,\u03b6)\u2208D\n(\u00b5s\u03c1\u03b6 log(\u00b5\u0302s\u03c1\u03b6) + (1\u2212 \u00b5s\u03c1\u03b6)log(1\u2212 \u00b5\u0302s\u03c1\u03b6))\nThe ground truth likelihood value for each patch is computed by counting the frequency of unique subjects of type s occupying the patch at some time during the course of their individual trajectories and dividing it by the total number of unique subjects of the same type in the scene. Note that D is constructed from the annotated videos by tracking and counting the incidence of each subject in the video with every patch in a subsample of video frames."}, {"heading": "4.2 Pooling Mechanism", "text": "Our model was trained on a subsample (one in every ten frames) of frames from the videos that formed our training dataset. We describe how we pool the static and dynamic contexts for a given frame F from the sample along with the representation of the historical trajectory. We use the subscript F to indicate the fact that the pooling being done is specific to the frame.\nDynamic Context Pooling\nHumans moving in a crowded area adapt their motion based on the behaviour of the people around them. For example, pedestrians often completely alter their paths when they see someone else or a group of people approaching them. Such behaviour cannot be predicted by observing a pedestrian in isolation without considering the surrounding dynamic and static context. This behaviour motivated the pooling mechanism of the Social LSTM model[22]. We borrow the same pooling mechanism to capture such influences from neighbouring subjects for our model.\nWe use LSTMs to learn an efficient hidden representation of the temporal behaviour of subjects as part of the encoder. Since these hidden representations would capture each subject\u2019s behaviour until the observed time step, we can use these representations to capture the influence that the neighbouring subjects would have on a target subject.\nWe consider a spatial neighbourhood of size (ds \u00d7 ds) around each moving subject (again with appropriate padding for the boundary cells) which in turn is subdivided into a (gs\u00d7gs) grid with each grid cell of size ( ds gs \u00d7 dsgs ) . Let h(t)jsF denote the LSTM encoded hidden representation (a vector of dimension dH ) of jth subject of type s at time t (when the current frame is F). Also let C be the number of subject classes. We construct social tensors[22] S(t)iF each of size (gs \u00d7 gs \u00d7 dH \u00d7 C) that capture the social context in a structured way\nS(t)iF (r, c, :, :) =  Ns\u2211 j=1 I(t)ircF [s, j].h (t\u22121) jsF C s=1\nwhere Ns is the total number of subjects of type s and I(t)ircF [s, j] is an indicator function which denotes whether the jth subject of type s is in the (r, c)th grid cell of the spatial neighbourhood of the ith subject at time t.\nStatic Context Pooling\nAs described earlier, the SSCN model is designed to predict the likelihood of a subject like a pedestrian stepping on a specific input image patch, given the larger context around the patch and the scene\nitself. We use this to provide a surrounding context for each subject from its current position which in turn influences the next position of the subject.\nWe first build a spatial map for each subject class and location in a frame of the video, with the probabilities of a subject of that class ever visiting that location. This map is built offline using the pretrained SSCN network described in Section 4.1. Given a subsample of frames, for every frame F in the sample, patch \u03c1 and subject class s we build the mapMFs\u03c1 = SSCN(s, \u03c1,F). We extract a Reachability Tensor R(t)iF for the i\nth subject (say of class s) at time t from the static context map MFs\u03c1. Given its current position x (t) i , let \u03a6 (t) i be the collection of all patches of size (3gp \u00d7 3gp) that\nare centered at (gp \u00d7 gp) patches at most ( dR 2 ) away on each axis from x(t)i . We basically construct a reachability context tensor accounting for a patch of size (dR \u00d7 dR) with x(t)i at the center. The extracted reachability context tensor is therefore\nR (t) iF = [SSCN(s, \u03c1,F)]\u03c1\u2208\u03a6(t)i"}, {"heading": "4.3 Spatio - Temporal Attention Model", "text": "In our work, we take the encoder-decoder architecture as the base model and apply a soft attention mechanism on top of it. The motivation for applying an attention mechanism is straight-forward. Subjects often change their pre-panned trajectories suddenly when the \u2019context\u2019 changes. Imagine a pedestrian in an airport walking towards the security, suddenly realizing that he/she needs to pick up the baggage tag and as a result making a sharp course-correction to move towards the check-in counter. Since, the model proposed by Alahi et. al.[22] only takes the last time step hidden representation of the pedestrian of interest, the model will be responsive to immediate instincts like collision avoidance but not be very useful in long term path planning. Moreover, once the model starts its predictions, even a small error in the prediction could mean that the erroneous hidden representations are propagated to future time steps.\nThe full architecture for our spatio-temporal attention model is shown in figure 2.\nWe first embed the spatial coordinate input x(t)iF , the dynamic context pooled tensor S (t) iF and static reachability tensor R(t)iF to fixed dimensions using three separate sigmoid embedding layers.\nptiF = \u03c6(x (t) iF ,We), a t iF = \u03c6(S t iF ,Wa), c t iF = \u03c6(R t iF ,Wc)\nwhere We,Wa and Wc are the embedding matrices and \u03c6 is the sigmoid function. Next, the three embeddings ptiF , a t iF and c t iF are concatenated to form the input to the encoder. The encoder outputs a fixed size hidden state representation htiF at each time step t,\nhtiF = LSTM(h t\u22121 iF , p t iF , a t iF , c t iF ,Wenc)\nwhere ht\u22121iF is the hidden state representation output of the decoder at the last timestep (t\u2212 1) and LSTM is the encoding function for the encoder with weights Wenc.\nWe use a context vector C(t)iF = \u2211t j=t\u2212k+1 \u03b1 (j) i .h (j) iF that depends on the encoder hidden states occuring in a fixed size temporal attention window of size k \u2014 [h(t\u2212k+1)iF , . . . .,h (t\u22121) iF ,h (t) iF ]. This makes it possible for the network to dynamically generate different context vectors at different timesteps with the knowledge of k encoded states back in time. We follow the previously proposed attention mechanism like in Bahdanau et. al.[6] to compute the attention weights \u03b1(j)i . This context vector is generated by the attention mechanism which puts an emphasis over the encoder states and generates a \u2019behaviour context\u2019 for the subject of interest.\nThe context vector C(t)iF feeds into the decoder unit to generate the decoder states. The decoder in turn generates a 5-tuple as its output representing the parameters of a bivariate Gaussian distribution over the predicted position of the subject at the next time step. Denoting the decoder state at time t (for the frame F) as s(t)iF , the decoder state and the predicted bivariate Gaussian model \u03b8\n(t) iF \u2261 N (\u00b5 (t) i ,\u03c3 (t) i , \u03c1 (t) i ) for the position of the subject x (t) iF at time t are computed as\ns (t) iF = LSTM(s (t\u22121) iF ,x (t\u22121) iF , C (t) iF ,Wdec), \u03b8 (t) iF = ReLU(s (t) iF ,Wo)\nNote that x(t\u22121)iF , the position at time (t\u2212 1), is taken as the actual ground truth value at time (t\u2212 1) during training and the model predicted value during inference. The above model can be trained for multiple classes of subjects with a separate model for each individual class.\nOur model therefore accounts the current spatial coordinates, social context that incorporates all the moving subjects in the scene and the static context that accounts for the reachability of the subjects across the patches in the scene.\nDuring inference, the final predicted position x\u0302i(t) is sampled from the predicted distribution \u03b8 (t) i .\nCost Formulation\nWe train the network by maximizing the likelihood of the ground truth position being generated from the predicted distribution. Hence, we jointly learn all the parameters by minimizing the negative log-Likelihood loss Li = \u2212 \u2211Tn t=T1 log(P (x (t) i | \u00b5 (t) i ,\u03c3 (t) i , \u03c1 (t) i ) for the i\nth trajectory. An important aspect of the training phase is that, since the LSTM layers of the encoder and decoder units are shared between all the subjects of a particular type, all parameters of the models have to be learned jointly. Thus, we back-propagate the loss for each trajectory i of each subject type at every time step t."}, {"heading": "5 Experiments", "text": "Dataset\nWe use three large scale multi-object tracking datasets - ETH [11] , UCY [12] and the Stanford Drone Dataset [13]. The ETH and UCY datasets consist of 5 scenes with 1536 unique pedestrians entering and exiting the scenes. It includes challenging scenarios like groups of people walking together, 2 different groups of people crossing each other and also behaviour such as a pedestrian deviating completely from it\u2019s followed path almost instantaneously.\nOn the other hand, the Stanford Drone Dataset[13] consists of multiple aerial imagery comprising of 8 different locations around the Stanford campus and objects belonging to 6 different classes moving around. We use only the trajectories of pedestrians to train and test our models.\nSetup\nWe set the hyperparameters of our model using a cross validation strategy following a leave one out approach. For the SSCN model we take the size of the grid g\u03c1 in the patch stream as 60 and the re-sized input dimension as 227 x 227. The input dimension of the context stream is set to 512 x 512. The overall network is trained using a learning rate of 0.002, with Gradient Descent Optimizer and a batch size of 32.\nTo compare our results with that of previous state of the art model - S-LSTM[22], we limit the subject type to only pedestrians. We set the common hyperparameters of the Spatio-Temporal Attention\nModel to be the same as that of theirs. Trajectories are downsampled so as to retain one in every ten frames. We observe the trajectories for a period of 8 time steps (T ) with an attentional window length k of 5 time steps and predict for future 12 time steps (n). We set the reachability distance dR to 60. We also limit the number of pedestrians in each frame to 40. The model is trained using a learning rate of 0.003 with RMSProp as the optimizer.\nEvaluation Metrics\nWe use two evaluation metrics as proposed in Alahi et.al.[22] - (i) Average Displacement error - The euclidean distance between the predicted trajectory and the actual trajectory averaged over all time-steps for all pedestrians and (ii) Final Displacement error - The average euclidean distance between the predicted trajectory point and the actual trajectory point at the end of n time steps."}, {"heading": "5.1 Quantitative Results", "text": "We build two separate models for the complete problem statement - one with only dynamic context pooling coupled with the attention mechanism which we denote as - D-ATT Model and the second with static context pooling added to the D-ATT model which we denote as - SD-ATT. We cannot test the SD-ATT model on ETH and UCY datasets as the resolution of the videos in these datasets is too low which makes it impossible to use the SSCN model for static context pooling. Still, we consistently outperform the S-LSTM[22] and O-LSTM[22] models on both the evaluation metrics on all three datasets as shown in table 1. We also show that the results of the SD-ATT model on the Stanford Drone Dataset are much better than the Social LSTM[22] model."}, {"heading": "5.2 Qualitative Results", "text": "We demonstrate scenarios where our models perform better than the S-LSTM[22] model. Firstly, in Figure 3 we show the results for SSCN model. The second column of the figure shows an example of the constructed saliency map with the shade of blue denoting the likelihood value of the corresponding patch. We can see that the likelihood values are low near the roundabout, car and trees and high for areas such as road and pavement. In rest of the figures, each unique pedestrian is depicted by a unique colour. In Figure 4, the first column shows that the SD-ATT model has learned to predict non linear trajectories as well. The next two columns depict the collision avoidance property learned by the model. In both the examples, the model either decelerates one of the pedestrian or it diverts it to avoid collision.\nFigure 5(a) compares the predictions of SD-ATT model against the Social LSTM[22] model. Since, the S-LSTM model only considers the last time step\u2019s hidden representation, it thinks that the pedestrian wants to take a turn and hence follows the curved path. On the other hand, our SD-ATT model interprets this as a sharp turn since it has only seen this change in the behaviour over the last two time steps and hence takes a gradual turn. This demonstrates the advantage of using an attention mechanism. Figure 5(b) demonstrates the advantage of static spatial pooling. In both the examples shown, the pedestrian walks straight for T time steps because of which S-LSTM[22] and D-ATT\nmodels predict a straight path. On the other hand, SD-ATT model captures a static obstacle in front of the pedestrian and hence takes a diversion from the followed path."}, {"heading": "6 Conclusion", "text": "We propose a novel deep learning approach to the problem of human trajectory prediction. Our model successfully extracts motion patterns in an unsupervised manner. Compared to the previous state of the art works, our approach models both dynamic and spatial context around any type of subject of interest which results in better prediction of trajectories. Our proposed method outperforms previous state of the art method on three large scale datasets. In addition to this, we also propose a novel CNN based SSCN architecture which helps in better semantic understanding of the scene. Future work includes evaluating the proposed models for multiple classes of objects."}], "references": [{"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning recursive distributed representations for holistic computation", "author": ["Lonnie Chrisman"], "venue": "Connection Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1991}, {"title": "Recursive hetero-associative memories for translation", "author": ["Mikel L Forcada", "Ram \u0301on P Neco"], "venue": "In International Work-Conference on Artificial Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "author": ["K. Cho", "B.V. Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR. Retrieved May", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Sift flow: Dense correspondence across different scenes", "author": ["C. Liu", "J. Yuen", "A. Torralba", "J. Sivic", "W.T. Free-man"], "venue": "Computer Vision\u2013ECCV 2008, pp. 28\u201342, Springer, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "A data-driven approach for event prediction", "author": ["J. Yuen", "A. Torralba"], "venue": "Computer Vision\u2013ECCV 2010, pp. 707\u2013720, Springer, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep Learning Driven Visual Path Prediction From a Single Image", "author": ["S. Huang", "X. Li", "Z. Zhang", "Z. He", "F. Wu", "W. Liu", "Y. Zhuang"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "You\u2019ll never walk alone: Modeling social behavior for multi-target tracking", "author": ["S. Pellegrini", "A. Ess", "K. Schindler", "L. Van Gool"], "venue": "In Computer Vision,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Crowds by example", "author": ["A. Lerner", "Y. Chrysanthou", "D. Lischinski"], "venue": "In Computer Graphics Forum,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Trajectory pattern mining", "author": ["F. Giannotti", "M. Nanni", "F. Pinelli", "D. Pedreschi"], "venue": "in: ACM SIGKDD,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Learning trajectory patterns by clustering: Experi- mental studies and comparative evaluation", "author": ["B. Morris", "M.M. Trivedi"], "venue": "in: CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Attention-based encoder-decoder model for answer selection in question answering", "author": ["Y. Nie", "Y. Han", "J. Huang", "B. Jiao", "A. Li"], "venue": "Frontiers of Information Technology & Electronic Engineering,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "Social force model for pedestrian dynamics", "author": ["D. Helbing", "P. Molnar"], "venue": "Physical review E,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Agent-based modeling: Methods and tech- niques for simulating human systems", "author": ["E. Bonabeau"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Socially-aware large-scale crowd forecasting", "author": ["A. Alahi", "V. Ramanathan", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Robust object tracking by hierarchical association of detection responses", "author": ["C. Huang", "B. Wu", "R. Nevatia"], "venue": "In ECCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Multi-hypothesis motion planning for visual object tracking", "author": ["H. Gong", "J. Sim", "M. Likhachev", "J. Shi"], "venue": "In Proceedings of the 2011 International Conference on Computer Vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Social LSTM: Human Trajectory Prediction in Crowded Spaces", "author": ["A. Alahi", "K. Goel", "V. Ramanathan", "A. Robicquet", "L. Fei-Fei", "S. Savarese"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Long Short Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Gradient-based learning applied to document recognition.", "author": ["LeCun", "Yann"], "venue": "Proceedings of the IEEE", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Describing Videos by Exploiting Temporal Structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "IEEE International Conference on Computer Vision (ICCV)", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16].", "startOffset": 23, "endOffset": 35}, {"referenceID": 1, "context": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16].", "startOffset": 23, "endOffset": 35}, {"referenceID": 2, "context": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16].", "startOffset": 23, "endOffset": 35}, {"referenceID": 3, "context": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16].", "startOffset": 23, "endOffset": 35}, {"referenceID": 4, "context": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16].", "startOffset": 95, "endOffset": 98}, {"referenceID": 14, "context": "Encoder Decoder Models [1, 2, 3, 4] were initially introduced for machine translation tasks in [5] followed by automated question answering in [16].", "startOffset": 143, "endOffset": 147}, {"referenceID": 6, "context": "Attention Networks have been used very successfully for automatic fine-grained image description/annotation [7].", "startOffset": 108, "endOffset": 111}, {"referenceID": 15, "context": "Helbing and Molnar\u2019s social force model[17] was the first to learn interaction patterns between different objects such as attractive and repulsive forces.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "Since then, several variants such as (i) agent based modeling[18] to use human attributes as model priors to learn behavioral patterns, and (ii) feature engineered approaches like that of Alahi et.", "startOffset": 61, "endOffset": 65}, {"referenceID": 17, "context": "[19] which extract social affinity features to learn such patterns, have been explored.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Other approaches include finding the most common object behaviour by means of clustering[14, 15].", "startOffset": 88, "endOffset": 96}, {"referenceID": 13, "context": "Other approaches include finding the most common object behaviour by means of clustering[14, 15].", "startOffset": 88, "endOffset": 96}, {"referenceID": 12, "context": "[14] analyzed GPS traces of several fleets of buses and extracted patterns in trajectories as concise descriptions of frequent behaviour, both temporally and spatially.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] captured the interactions between pedestrians using multiple Long Short Term Memory Networks(LSTMs)[23] and a social pooling mechanism to capture humanhuman interactions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] captured the interactions between pedestrians using multiple Long Short Term Memory Networks(LSTMs)[23] and a social pooling mechanism to capture humanhuman interactions.", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "Such spatial modeling exists in [20, 21] but these do not include the dynamic modeling of the crowd.", "startOffset": 32, "endOffset": 40}, {"referenceID": 19, "context": "Such spatial modeling exists in [20, 21] but these do not include the dynamic modeling of the crowd.", "startOffset": 32, "endOffset": 40}, {"referenceID": 7, "context": "Earlier works include matching-based approaches[8, 9] which rely on keypoint feature matching techniques.", "startOffset": 47, "endOffset": 53}, {"referenceID": 8, "context": "Earlier works include matching-based approaches[8, 9] which rely on keypoint feature matching techniques.", "startOffset": 47, "endOffset": 53}, {"referenceID": 9, "context": "A deep learning approach was also used for spatial context modeling in [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "Our proposed architecture is composed of Convolutional Neural Networks(CNNs)[24] and is inspired by the Spatial Matching Network introduced in [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "Our proposed architecture is composed of Convolutional Neural Networks(CNNs)[24] and is inspired by the Spatial Matching Network introduced in [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "This behaviour motivated the pooling mechanism of the Social LSTM model[22].", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "We construct social tensors[22] S iF each of size (gs \u00d7 gs \u00d7 dH \u00d7 C) that capture the social context in a structured way", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "[22] only takes the last time step hidden representation of the pedestrian of interest, the model will be responsive to immediate instincts like collision avoidance but not be very useful in long term path planning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] to compute the attention weights \u03b1 i .", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "We use three large scale multi-object tracking datasets - ETH [11] , UCY [12] and the Stanford Drone Dataset [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "We use three large scale multi-object tracking datasets - ETH [11] , UCY [12] and the Stanford Drone Dataset [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "To compare our results with that of previous state of the art model - S-LSTM[22], we limit the subject type to only pedestrians.", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "[22] - (i) Average Displacement error The euclidean distance between the predicted trajectory and the actual trajectory averaged over all time-steps for all pedestrians and (ii) Final Displacement error - The average euclidean distance between the predicted trajectory point and the actual trajectory point at the end of n time steps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Table 1: Comparison across datasets Metric Dataset O-LSTM[22] S-LSTM[22] D-ATT SD-ATT Avg.", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "Table 1: Comparison across datasets Metric Dataset O-LSTM[22] S-LSTM[22] D-ATT SD-ATT Avg.", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "Error ETH[11] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "47 HOTEL[11] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "12 ZARA1[12] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "Error ETH[11] 1.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "85 HOTEL[11] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "19 ZARA1[12] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "Still, we consistently outperform the S-LSTM[22] and O-LSTM[22] models on both the evaluation metrics on all three datasets as shown in table 1.", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "Still, we consistently outperform the S-LSTM[22] and O-LSTM[22] models on both the evaluation metrics on all three datasets as shown in table 1.", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "We also show that the results of the SD-ATT model on the Stanford Drone Dataset are much better than the Social LSTM[22] model.", "startOffset": 116, "endOffset": 120}, {"referenceID": 20, "context": "We demonstrate scenarios where our models perform better than the S-LSTM[22] model.", "startOffset": 72, "endOffset": 76}, {"referenceID": 20, "context": "Figure 5(a) compares the predictions of SD-ATT model against the Social LSTM[22] model.", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "In both the examples shown, the pedestrian walks straight for T time steps because of which S-LSTM[22] and D-ATT", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "(a) Advantage of Attention Mechanism (b) Advantage of Static Context Pooling Figure 5: Comparison of our models against S-LSTM[22] model", "startOffset": 126, "endOffset": 130}], "year": 2017, "abstractText": "Trajectory Prediction of dynamic objects is a widely studied topic in the field of artificial intelligence. Thanks to a large number of applications like predicting abnormal events, navigation system for the blind, etc. there have been many approaches to attempt learning patterns of motion directly from data using a wide variety of techniques ranging from hand-crafted features to sophisticated deep learning models for unsupervised feature learning. All these approaches have been limited by problems like inefficient features in the case of hand crafted features, large error propagation across the predicted trajectory and no information of static artefacts around the dynamic moving objects. We propose an end to end deep learning model to learn the motion patterns of humans using different navigational modes directly from data using the much popular sequence to sequence model coupled with a soft attention mechanism. We also propose a novel approach to model the static artefacts in a scene and using these to predict the dynamic trajectories. The proposed method, tested on trajectories of pedestrians, consistently outperforms previously proposed state of the art approaches on a variety of large scale data sets. We also show how our architecture can be naturally extended to handle multiple modes of movement (say pedestrians, skaters, bikers and buses) simultaneously.", "creator": "LaTeX with hyperref package"}}}