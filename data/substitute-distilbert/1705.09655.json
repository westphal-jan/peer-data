{"id": "1705.09655", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2017", "title": "Style Transfer from Non-Parallel Text by Cross-Alignment", "abstract": "this paper focuses on style transfer on the basis of non - parallel text. this is an instance of a broader family of problems encompassing machine translation, decipherment, and sentiment alteration. the leading technical challenge is to separate the content from desired linguistic characteristics such as sentiment. we leverage refined alignment of latent forms across mono - lingual organizational corpora with desired characteristics. we deliberately modify encoded examples according to their characteristics, requiring the reproduced instances to match each examples with the altered characteristics as a population. we identified high effectiveness of this cross - functional method on engineering tasks : sentiment modification, decipherment of word substitution ciphers, and recovery of word structures.", "histories": [["v1", "Fri, 26 May 2017 17:40:12 GMT  (120kb,D)", "http://arxiv.org/abs/1705.09655v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["tianxiao shen", "tao lei", "regina barzilay", "tommi jaakkola"], "accepted": true, "id": "1705.09655"}, "pdf": {"name": "1705.09655.pdf", "metadata": {"source": "CRF", "title": "Style Transfer from Non-Parallel Text by Cross-Alignment", "authors": ["Tianxiao Shen", "Tao Lei"], "emails": ["tianxiao@mit.edu", "tao@asapp.com", "regina@csail.mit.edu", "tommi@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Using massive amounts of parallel data has been essential for recent advances in text generation tasks, such as machine translation and summarization. However, in many text generation problems, we can only assume access to non-parallel or mono-lingual data. Problems such as decipherment or style transfer are all instances of this family of tasks. In all of these problems, we must preserve the content of the source sentence but render the sentence consistent with desired presentation constraints (e.g., style, cipher text).\nThe key challenge here is to separate the content from presentation characteristics. This separation typically occurs in the latent vector representation. With access only to non-parallel data, we must ensure that the disentangled representation really captures independent degrees of freedom with consistent effects on rendered sentences. A recent approach to this problem builds on variational auto-encoders (VAEs), dividing the latent representation into two or more parts, and enforcing additionally that the latent characteristics used to generate text can be reliably inferred from the text alone (Hu et al., 2017). The decomposition of latent representation can be used to perform style transfer. However, in this way the quality of altered text is only partially controlled by the characteristics classifier.\nIn this paper, we follow a different strategy, utilizing refined alignment of latent representations across mono-lingual text corpora that have different characteristics. We dispense with VAEs that align posteriors to an already specified fixed distribution thus reaping little from additional cross-alignment. Instead, we retain the complexity of the latent representation as opposed to how it is interpreted by the decoder. It leaves the transfer function relatively simple, which is helpful for content preservation. Our theoretical results support this argument. In addition, we deliberately modify encoded examples (e.g., positive sentences, plain text) according to their characteristics (altering them to be negative or cipher\nar X\niv :1\n70 5.\n09 65\n5v 1\n[ cs\n.C L\n] 2\n6 M\ntext), and requiring that the resulting reproduced instances match, as a population, available examples with the analogous characteristics (negative, cipher text). We call this method cross-alignment.\nTo demonstrate the flexibility of the proposed model, we evaluate it on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word reodering. In all of these applications, the model was trained on non-parallel data. On the sentiment modification task, the model successfully rewrote in a different sentiment 78.4% of review sentences, outperforming by 30% the closest baseline. The model achieves similarly strong performance on the decipherment and word order recovery tasks. For instance, on the decipherment task, the model reaches Bleu score of 57.4, obtaining 50.2 gap than a comparable method without cross-alignment."}, {"heading": "2 Related work", "text": "Style transfer in vision Non-parallel style transfer has been extensively studied in computer vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017). Gatys et al. (2016) explicitly extract content and style features, and then synthesize a new image by combining \u201ccontent\u201d features of one image with \u201cstyle\u201d features from another. More recent approaches learn generative networks directly via generative adversarial training (Goodfellow et al., 2014) from two given data domains X 1 and X 2. The key computational challenge in this non-parallel setting is aligning the two domains. For example, CoupledGANs (Liu and Tuzel, 2016) employ weight-sharing between networks to learn cross-domain representation, whereas CycleGAN (Zhu et al., 2017) introduces cycle consistency which relies on transitivity to regularize the transfer functions. While our approach has a similar high-level architecture, the discreteness of natural language does not allow us to reuse these models and necessitates the development of new methods.\nNon-parallel transfer in natural language In natural language processing, most tasks that involve generation (e.g., translation and summarization) are trained using parallel sentences. Our work most closely relates to approaches that do not utilize parallel data, but instead guide sentence generation from an indirect training signal (Mueller et al., 2017; Hu et al., 2017). For instance, Mueller et al. (2017) manipulate the hidden representation to generate sentences that satisfy a desired property (e.g., sentiment) as measured by a corresponding classifier. However, their model does not necessarily enforce content preservation. More similar to our work, Hu et al. (2017) facilitate content preservation by explicitly modeling the style and content factorization. Using ideas from computer vision (Chen et al., 2016), this factorization is learned in an unsupervised manner. The factorization algorithm of Hu et al. (2017) is primarily driven by the predictions of the property classifier. In contrast, our method utilizes adversarial training (Goodfellow et al., 2014) to guarantee distributional alignment of the common latent space driven by content/style independence. By encouraging generated sentences to be close to the target domain, our model provides a richer guidance to overall sentence quality than a property classifier.\nAdversarial training over discrete samples Recently, a wide range of techniques addresses challenges associated with adversarial training over discrete samples generated by recurrent networks (Yu et al., 2016; Lamb et al., 2016; Che et al., 2017; Hjelm et al., 2017). In our work, we employ the Professor-Forcing algorithm (Lamb et al., 2016) which was originally proposed to close the gap between teacher-forcing during training and self-feeding during testing for recurrent networks. This design fits well with our scenario of style transfer that calls for cross-alignment, and the continuous training procedure can be effectively optimized through back-propagation."}, {"heading": "3 Formulation", "text": "In this section, we formalize the task of non-parallel style transfer and discuss the feasibility of the learning problem. We assume the data are generated by the following process:\n1. a latent style variable y is generated from some distribution p(y);\n2. a latent content variable z is generated from some distribution p(z);\n3. a datapoint x is generated from conditional distribution p(x|y,z).\nWe observe two datasets with the same content distribution but different styles y1 and y2, where y1 and y2 are unknown. Specifically, the two observed datasetsX 1 = {x(1)1 , \u00b7 \u00b7 \u00b7 ,x (n) 1 } andX 2 = {x(1)2 , \u00b7 \u00b7 \u00b7 ,x (m) 2 } consist of samples drawn from p(x1|y1) and p(x2|y2) respectively. We want to estimate the style transfer functions between them, namely p(x1|x2;y1, y2) and p(x2|x1;y1, y2). A question we must address is when this estimation problem is feasible. Essentially, we only observe the marginal distributions of x1 and x2, yet we are going to recover their conditional/joint distribution:\np(x1,x2) = \u222b z p(z)p(x1|y1, z)p(x2|y2, z)dz (1)\nAs we only observe p(x1|y1) and p(x2|y2), y1 and y2 are unknown to us. If two different y and y \u2032 lead to the same distribution p(x|y) = p(x|y \u2032), then given a datasetX sampled from it, its underlying style can be either y or y \u2032. Consider the following two cases: (1) both datasets X 1 and X 2 are sampled from the same style y; (2)X 1 andX 2 are sampled from style y and y \u2032 respectively. These two scenarios have different joint distributions, but the observed marginal distributions are the same. To prevent such confusion, we constrain the underlying distributions as stated in the following proposition:\nProposition 1. In the generative framework above, x1 and x2\u2019s joint distribution can be recovered from their marginals only if for any different y,y \u2032 \u2208 Y , distributions p(x|y) and p(x|y \u2032) are different.\nThis proposition basically says thatX generated from different styles should be \u201cdistinct\u201d enough, otherwise the transfer task between styles is not well defined. While this seems trivial, it may not hold even for simplified data distributions. The following examples illustrate how the transfer (and recovery) becomes feasible or infeasible under different model assumptions. As we shall see, for a certain family of styles Y , the more complex distribution for z , the more probable it is to recover the transfer function and the easier it is to search for the transfer."}, {"heading": "3.1 Example 1: Gaussian", "text": "Consider the common choice that z \u223c N (0, I ) has a centered isotropic Gaussian distribution. Suppose a style y = (A,b) is an affine transformation, i.e. x = Az+b+ , where is a noise variable. For b = 0 and any orthogonal matrixA,Az + b \u223c N(0, I ) and hence x has the same distribution for any such styles y = (A,0). In this case, the effect of rotation cannot be recovered.\nInterestingly, if z has a more complex distribution, such as a Gaussian mixture, then affine transformations can be uniquely determined.\nLemma 1. Let z be a mixture of Gaussians p(z) = \u2211K k=1 \u03c0kN (z ;\u00b5k,\u03a3k). Assume K \u2265 2, and there are two different \u03a3i 6= \u03a3j . Let Y = {(A,b)||A| 6= 0} be all invertible affine transformations, and p(x|y,z) = N (x;Az + b, 2I ), in which is a noise. Then for all y 6= y \u2032 \u2208 Y , p(x|y) and p(x|y \u2032) are different distributions. Theorem 1. If the distribution of z is a mixture of Gaussians which has more than two different components, and x1,x2 are two affine transformations of z , then the transfer between them can be recovered given their respective marginals."}, {"heading": "3.2 Example 2: Word substitution", "text": "Consider here another example when z is a bi-gram language model and a style y is a vocabulary in use that maps each \u201ccontent word\u201d onto its surface form (textual form). If we observe two realizations x1 and x2 of the same language z , the transfer and recovery problem becomes inferring a word alignment between x1 and x2.\nNote that this is a simplified version of language decipherment or translation. Nevertheless, the recovery problem is still sufficiently hard. To see this, letM 1,M 2 \u2208 Rn\u00d7n be the estimated bi-gram probability matrix of data X 1 and X 2 respectively. Seeking the word alignment is equivalent to finding a permutation matrixP such thatP>M 1P \u2248M 2, which can be expressed as an optimization problem,\n\u2016P>M 1P \u2212M 2\u20162 \u21d0\u21d2 \u2016M 1P \u2212PM 2\u20162\nThe same formulation applies to graph isomorphism (GI) problems givenM 1 andM 2 as the adjacency matrices of two graphs, suggesting that determining the existence and uniqueness of P is at least GI hard. Fortunately, ifM as a graph is complex enough, the search problem could be more tractable. For instance, if each vertex\u2019s weights of incident edges as a set is unique, then finding the isomorphism can be done by simply matching the sets of edges. This assumption largely applies to our scenario where z is a complex language model. We empirically demonstrate this in the results section.\nThe above examples suggest that z as the latent content variable should carry most complexity of data x, while y as the latent style variable should have relatively simple effects. We construct the model accordingly in the next section."}, {"heading": "4 Method", "text": "Learning the style transfer function under our generative assumption is essentially learning the conditional distribution p(x1|x2;y1, y2) and p(x2|x1;y1, y2). Unlike in vision where images are continuous and hence the transfer functions can be learned and optimized directly, the discreteness of language requires us to operate through the latent space. Since x1 and x2 are conditionally independent given the latent content variable z ,\np(x1|x2;y1, y2) = \u222b z p(x1, z |x2;y1, y2)dz\n= \u222b z p(z |x2, y2) \u00b7 p(x1|y1, z)dz = Ez\u223cp(z|x2,y2)[p(x1|y1, z)]\n(2)\nThis suggests us learning an auto-encoder model. Specifically, a style transfer from x2 to x1 involves two steps\u2014an encoding step that infers x2\u2019s content z \u223c p(z |x2, y2), and a decoding step which generates the transferred counterpart from p(x1|y1, z). In this work, we approximate and train p(z |x,y) and p(x|y,z) using neural networks (where y \u2208 {y1, y2}). Of course, an important question is what training criterion should be used to guide the model. One option is to apply a variational auto-encoder (Kingma and Welling, 2013) and maximize the variational lower bound of data likelihood. However, VAEs require an explicit assumption of the prior density p(z), such as z \u223c N (0, I ). As we have argued in the previous section, restricting z to a simple and even distribution may not be a good strategy for non-parallel style transfer.\nIn contrast, a standard auto-encoder simply minimizes the reconstruction error, encouraging z to carry as much information about the \u201ccontent\u201d of x as possible. On the other hand, it lowers the entropy in p(x|y,z), which helps to produce meaningful style transfer in practice as we flip between y1 and y2. Specifically, let E : X \u00d7Y \u2192 Z be a deterministic encoder that infers the mass of content variable z for a given sentence x and a style y , and let G : Y \u00d7 Z \u2192 X be a probabilistic generator that generates a sentence x from a given style y and content z . Let \u03b8 be the parameters to estimate, the reconstruction loss is,\nLrec(\u03b8E , \u03b8G) = Ex1\u223cX1 [\u2212 log pG(x1|y1, E(x1, y1))] + Ex2\u223cX2 [\u2212 log pG(x2|y2, E(x2, y2))]\n(3)\nAn issue with this criterion is that it makes no use of our generative assumption that both x1 and x2 are generated using the same latent distribution p(z). Consequently, the latent states z for two styles can be totally different thus prevent a meaningful transfer. Nevertheless, it is still possible to force such distributional properties without explicitly modeling the density. We introduce two constrained variants of auto-encoder to offset the aforementioned issue."}, {"heading": "4.1 Aligned auto-encoder", "text": "The first variant enforces both z1 = E(x1, y1) and z2 = E(x2, y2) to have the same distribution (where x1 \u223c X 1 and x2 \u223c X 2). To see this, consider the stochastic version of this encoding process in which x \u223c p(x|y) and z \u223c p(z |x,y). The distribution of z through this process is\u222b x p(x|y)p(z |x,y)dx = p(z |y). Note since y and z are independent, p(z |y1) = p(z |y2) = p(z).\nFollowing this motivation, we revise the reconstruction objective (3) as a constrained optimization problem:\n\u03b8\u2217 = arg min \u03b8 Lrec(\u03b8E , \u03b8G)\ns.t. E(x1, y1) d = E(x2, y2) x1 \u223cX 1,x2 \u223cX 2\n(4)\nIn practice, a Lagrangian relaxation of the primal problem is instead optimized. We introduce an adversarial discriminator D to align E(x1, y1) and E(x2, y2). D aims to distinguish between these two distributions:\nLadv(\u03b8D) = Ex1\u223cX1 [\u2212 logD(E(x1, y1))] + Ex2\u223cX2 [\u2212 log(1\u2212D(E(x2, y2)))] (5)\nThe overall training objective is a min-max game played among the encoder E, generator G and discriminator D:\nmin E,G max D Lrec \u2212 \u03bbLadv (6)\nComparing to variational auto-encoder, where the objective function is the reconstruction error plus KL divergence regularizer DKL(pE(z |x,y)\u2016p(z)), they align both posteriors pE(z |x1, y1) and pE(z |x2, y2) to the assumed prior p(z). Here we align pE(z |y1) and pE(z |y2) with each other and do not make any explicit assumptions about p(z).\nFollowing standard practice, we implement the encoder E : X \u00d7 Y \u2192 Z using an RNN that takes an input sentence x with initial hidden state y , and outputs the last hidden state z as its content representation. The generator G : Y \u00d7 Z \u2192 X is an RNN that specifies a sentence distribution conditioned on latent state (y,z). To align the distributions of z1 = E(x1, y1) and z2 = E(x2, y2), the discriminator D is a feed-forward network with a single hidden layer and a sigmoid output layer."}, {"heading": "4.2 Cross-aligned auto-encoder", "text": "The second variant, cross-aligned auto-encoder, directly aligns the transfered samples with the true samples. Under the generative assumption, p(x2|y2) = \u222b x1 p(x2|x1;y1, y2)p(x1|y1)dx1, thus x2 (sampled from the left-hand side) should exhibit the same distribution as transferred x1 (sampled from the right-hand side), and vice versa. Similar to our first model, the second model uses two discriminators D1 and D2 to align the populations. D1\u2019s job is to distinguish between real x1 and transferred x2, and D2\u2019s job is to distinguish between real x2 and transferred x1.\nAdversarial training over the discrete samples generated byG hinders gradients propagation. Although sampling-based gradient computation such as REINFORCE (Williams, 1992) can by adopted, training with these methods can be unstable due to the high variance of the sampled gradient. Instead, we employ two recent techniques to approximate the discrete training (Hu et al., 2017; Lamb et al., 2016). First, instead of feeding a single sampled word as the input to the generator RNN, we use the softmax distribution over words instead. Specifically, during the generating process of transferred x2 from G(y1, z2), suppose at time step t the output logit vector is vt. We feed its peaked distribution softmax(vt/\u03b3) as the next input, where \u03b3 \u2208 (0, 1) is a temperature parameter. Secondly, we use Professor-Forcing (Lamb et al., 2016) to match the sequence of hidden states instead of the output words, which contains the information about outputs and is smoothly distributed. That is, the input to the discriminator D1 is the sequence of hidden states of either (1) G(y1, z1) teacher-forced by a real example x1, or (2) G(y1, z2) self-fed by previous soft distributions.\nFigure 1 illustrates the running procedure of our cross-aligned auto-encoder. Note that cross-aligning strengthens the alignment of latent variable z over the recurrent network of generator G. By aligning the whole sequence of hidden states, it prevents z1 and z2\u2019s initial misalignment from propagating through the recurrent generating process, as a result of which the transferred sentence may end up somewhere far from the target domain.\nWe implement both D1 and D2 using convolutional neural networks. The training algorithm is presented in Algorithm 1.\nAlgorithm 1 Cross-aligned auto-encoder training. The hyper-parameters are set as \u03bb = 1, \u03b3 = 0.001 and learning rate is 0.0001 for all experiments in this paper. Input: Two corpora of different stylesX 1,X 2. Lagrange multiplier \u03bb, temperature \u03b3.\nInitialize \u03b8E , \u03b8G, \u03b8D1 , \u03b8D2 repeat\nfor p = 1, 2; q = 2, 1 do Sample a mini-batch of k examples {x(i)p }ki=1 fromXp Get the latent content representations z(i)p = E(x (i) p , yp)\nUnroll G from initial state (yp, z (i) p ) by feeding x (i) p , and get the hidden states sequence h (i) p Unroll G from initial state (yq, z (i) p ) by feeding previous soft output distribution with temperature \u03b3, and get the transferred hidden states sequence h\u0303 (i)\np end for Compute the reconstruction Lrec by Eq. (3) Compute D1\u2019s loss Ladv1 = \u2212 1k \u2211k i=1 logD1(h (i) 1 )\u2212 1k \u2211k i=1 log(1\u2212D1(h\u0303 (i) 2 ))\nCompute D2\u2019s loss Ladv2 = \u2212 1k \u2211k i=1 logD2(h (i) 2 )\u2212 1k \u2211k i=1 log(1\u2212D2(h\u0303 (i)\n1 )) Update {\u03b8E , \u03b8G} by gradient descent on loss Lrec \u2212 \u03bb(Ladv1 + Ladv2) Update \u03b8D1 and \u03b8D2 by gradient descent on loss Ladv1 and Ladv2 respectively\nuntil convergence"}, {"heading": "5 Experimental Setup", "text": "Sentiment modification Our first experiment focuses on text rewriting with the goal of changing the underlying sentiment of input sentences. We run experiments on Yelp restaurant reviews, utilizing readily available user ratings associated with each review. Following standard practice, reviews with rating above three are considered positive, and those below three are considered negative. While our model operates at the sentence level, the sentiment annotations in our dataset are provided at the document level. We assume that all the sentences in a document have the same sentiment. This is clearly an oversimplification, since some sentences (e.g., background) are sentiment neutral. Given that such sentences are more common in long reviews, we filter out reviews that exceed 10 sentences. We further filter the remaining sentences by eliminating those that exceed 15 words. The resulting dataset has 250K negative sentences, and 350K positive ones.\nTo quantitatively evaluate the transfered sentences, we adopt a model-based evaluation metric similar to the one used for image transfer (Isola et al., 2016). Specifically, we measure how often a transferred\nsentence has the correct sentiment according to a pre-trained sentiment classifier. For this purpose, we use a pre-trained classifier with a prediction accuracy of 85.4%.\nIn these experiments, we compare the two variants of our model with variational autoencoder.\nWord substitution decipherment Our second set of experiments involves decoding of word substitution ciphers, which has been previously explored in NLP literature (Dou and Knight, 2012; Nuhn and Ney, 2013). These ciphers replace every word in plaintext (natural language) with a cipher token according to a 1-to-1 substitution key. The decipherment task is to recover the plaintext from ciphertext. It is trivial if we have access to parallel data. However we are interested to consider a non-parallel decipherment scenario. For training, we select 200K sentences as X 1, and apply a substitution cipher f on a different set of 200K sentences to getX 2. While these sentences are nonparallel, they are drawn from the same distribution from the review dataset. The development and test sets have 100K parallel sentences D1 = {x(1), \u00b7 \u00b7 \u00b7 ,x(n)} and D2 = {f(x(1)), \u00b7 \u00b7 \u00b7 , f(x(n))}. We can quantitatively compare betweenD1 and transferred (deciphered)D2 using Bleu score (Papineni et al., 2002).\nClearly, the difficulty of this decipherment task depends on the number of substituted words. Therefore, we report model performance with respect to the percentage of the substituted vocabulary. Note that the transfer models do not know that f is a word substitution function, they learn it entirely from the data distribution.\nIn addition to having different transfer models, we introduce a simple decipherment baseline based on word frequency. Specifically, we assume that words shared betweenX 1 andX 2 do not require translation. The rest of the words are mapped based on their frequency. The ties between words are broken randomly. Finally, to assess the difficulty of the task, we report the accuracy of machine translation system trained on a parallel corpus (Klein et al., 2017).\nWord order recovery Our final experiments focus on the word re-ordering task. The model is provided with a random permutation of a sentence, and it has to retrieve its grammatical ordering. As in the case of word substitution ciphers, the training consists of non-parallel sentences. The overall process for constructing the training and testing data follows the process described above."}, {"heading": "6 Results", "text": "Sentiment modification Table 1 shows the performance of various models on this task as measured by a supervised classifier. The cross-aligned auto-encoder achieves 78% accuracy, outperforming other models by a substantial margin. The low performance of variational auto-encoder (23%) clearly demonstrates that it is not suitable for this transfer task.\nTable 2 shows a few samples of sentiment transfer generated by our cross-aligned auto-encoder. Along with showing high-quality rewritings, the table reveals several examples of erroneous transformations. Our manual analysis of the data reveals that such mistakes commonly fall into two classes: content modification and grammaticality. The fourth pair in Table 2 is an instance of the first class \u2013 while it successfully makes the sentence positive, it changes the topic from Mexican to Italian food. The second class is exemplified by the very last pair in Table 2, where the rewritten sentence contains the grammatically incorrect phrase \"rushed with a couple of work\".\nWord substitution decipherment Table 3 summarizes the performance of our model and the baselines on the decipherment task, at various levels of word substitution. Consistent with our intuition, the last row in this table shows that the task is trivial when the parallel data is provided.\nIn non-parallel case, the difficulty of the task is driven by the substitution rate. However, across all the testing conditions, our cross-aligned model consistently outperforms its counterparts. The difference becomes more pronounced as the task becomes harder. When the substitution rate is 20%, all methods do a reasonably good job in recovering substitutions. However, when 100% of the words are substituted (as expected in real language decipherment), the poor performance of variational autoencoder and aligned auto-encoder rules out their application for this task.\nWord order recovery The last column in Table 3 demonstrates the performance on the word order recovery task. Order recovery is much harder\u2014even when trained with parallel data, the machine translation model achieves only 64.6 Bleu score. Note that some generated reorderings may be completely valid (e.g., reordering conjunctions), but the models will be penalized for producing them.\nIn this task, only the cross-aligned auto-encoder achieves grammatical reorder to a certain extent, demonstrated by its Bleu score 26.1. Other models fail this task, doing no better than no transfer. Table 4 shows examples of the reorderings generated by the system."}, {"heading": "7 Conclusion", "text": "Transferring languages from one style to another has been previously trained using parallel data. In this work, we formulate the task as a decipherment problem with only the access to un-paired data. The two data collections are assumed to be generated by a latent variable generative model. By leveraging this view, our method optimizes neural networks by forcing distributional alignment\n(invariance) over the latent space or sentence populations. We demonstrate the effectiveness of our method on tasks that permit quantitative evaluation, such as sentiment transfer, word substitution decipherment and word reorder. The decipherment view also provides an interesting open question\u2014 when can the joint distribution p(x1,x2) be recovered given only marginal distributions? We believe addressing this general question would improve the style transfer research in both vision and NLP."}, {"heading": "A Proof of lemma 1", "text": "Lemma 1. Let z be a mixture of Gaussians p(z) = \u2211K k=1 \u03c0kN (z ;\u00b5k,\u03a3k). Assume K \u2265 2, and there are two different \u03a3i 6= \u03a3j . Let Y = {(A,b)||A| 6= 0} be all invertible affine transformations, and p(x|y,z) = N (x;Az + b, 2I ), in which is a noise. Then for all y 6= y \u2032 \u2208 Y , p(x|y) and p(x|y \u2032) are different distributions.\nProof.\np(x|y = (A,b)) = K\u2211 k=1 \u03c0kN (x;A\u00b5k + b,A\u03a3kA> + 2I )\nFor different y = (A,b) and y \u2032 = (A\u2032, b\u2032), p(x|y) = p(x|y \u2032) entails that for k = 1, \u00b7 \u00b7 \u00b7 ,K,{ A\u00b5k + b = A \u2032\u00b5k + b \u2032\nA\u03a3kA > = A\u2032\u03a3kA \u2032>\nSince all Y are invertible, (A\u22121A\u2032)\u03a3k(A \u22121A\u2032)> = \u03a3k\nSuppose \u03a3k = QkDkQ>k is \u03a3k\u2019s orthogonal diagonalization. If k = 1, all solutions forA \u22121A\u2032 have the form: { QD1/2UD\u22121/2Q> \u2223\u2223\u2223U is orthogonal} However, when K \u2265 2 and there are two different \u03a3i 6= \u03a3j , the only solution is A\u22121A\u2032 = I , i.e. A = A\u2032, and thus b = b\u2032.\nTherefore, for all y 6= y \u2032, p(x|y) 6= p(x|y \u2032)."}], "references": [{"title": "Maximum-likelihood augmented discrete generative adversarial networks", "author": ["Tong Che", "Yanran Li", "Ruixiang Zhang", "R Devon Hjelm", "Wenjie Li", "Yangqiu Song", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1702.07983,", "citeRegEx": "Che et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Che et al\\.", "year": 2017}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Large scale decipherment for out-of-domain machine translation", "author": ["Qing Dou", "Kevin Knight"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Dou and Knight.,? \\Q2012\\E", "shortCiteRegEx": "Dou and Knight.", "year": 2012}, {"title": "Image style transfer using convolutional neural networks", "author": ["Leon A Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Gatys et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Boundaryseeking generative adversarial networks", "author": ["R Devon Hjelm", "Athul Paul Jacob", "Tong Che", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1702.08431,", "citeRegEx": "Hjelm et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hjelm et al\\.", "year": 2017}, {"title": "Controllable text generation", "author": ["Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P Xing"], "venue": "arXiv preprint arXiv:1703.00955,", "citeRegEx": "Hu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2017}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei A Efros"], "venue": "arXiv preprint arXiv:1611.07004,", "citeRegEx": "Isola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2016}, {"title": "Learning to discover cross-domain relations with generative adversarial networks", "author": ["Taeksoo Kim", "Moonsu Cha", "Hyunsoo Kim", "Jungkwon Lee", "Jiwon Kim"], "venue": "arXiv preprint arXiv:1703.05192,", "citeRegEx": "Kim et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Opennmt: Open-source toolkit for neural machine translation", "author": ["Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1701.02810,", "citeRegEx": "Klein et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2017}, {"title": "Professor forcing: A new algorithm for training recurrent networks", "author": ["Alex M Lamb", "Anirudh Goyal ALIAS PARTH GOYAL", "Ying Zhang", "Saizheng Zhang", "Aaron C Courville", "Yoshua Bengio"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Lamb et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lamb et al\\.", "year": 2016}, {"title": "Coupled generative adversarial networks", "author": ["Ming-Yu Liu", "Oncel Tuzel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Liu and Tuzel.,? \\Q2016\\E", "shortCiteRegEx": "Liu and Tuzel.", "year": 2016}, {"title": "Unsupervised image-to-image translation networks", "author": ["Ming-Yu Liu", "Thomas Breuel", "Jan Kautz"], "venue": "arXiv preprint arXiv:1703.00848,", "citeRegEx": "Liu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2017}, {"title": "Sequence to better sequence: continuous revision of combinatorial structures", "author": ["Jonas Mueller", "Tommi Jaakkola", "David Gifford"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Mueller et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mueller et al\\.", "year": 2017}, {"title": "Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Unsupervised cross-domain image generation", "author": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "venue": "arXiv preprint arXiv:1611.02200,", "citeRegEx": "Taigman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Dualgan: Unsupervised dual learning for image-to-image translation", "author": ["Zili Yi", "Hao Zhang", "Ping Tan Gong"], "venue": "arXiv preprint arXiv:1704.02510,", "citeRegEx": "Yi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2017}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu"], "venue": "arXiv preprint arXiv:1609.05473,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Unpaired image-to-image translation using cycle-consistent adversarial networks", "author": ["Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A Efros"], "venue": "arXiv preprint arXiv:1703.10593,", "citeRegEx": "Zhu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 6, "context": "A recent approach to this problem builds on variational auto-encoders (VAEs), dividing the latent representation into two or more parts, and enforcing additionally that the latent characteristics used to generate text can be reliably inferred from the text alone (Hu et al., 2017).", "startOffset": 263, "endOffset": 280}, {"referenceID": 3, "context": "Style transfer in vision Non-parallel style transfer has been extensively studied in computer vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017).", "startOffset": 101, "endOffset": 235}, {"referenceID": 20, "context": "Style transfer in vision Non-parallel style transfer has been extensively studied in computer vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017).", "startOffset": 101, "endOffset": 235}, {"referenceID": 12, "context": "Style transfer in vision Non-parallel style transfer has been extensively studied in computer vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017).", "startOffset": 101, "endOffset": 235}, {"referenceID": 13, "context": "Style transfer in vision Non-parallel style transfer has been extensively studied in computer vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017).", "startOffset": 101, "endOffset": 235}, {"referenceID": 16, "context": "Style transfer in vision Non-parallel style transfer has been extensively studied in computer vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017).", "startOffset": 101, "endOffset": 235}, {"referenceID": 8, "context": "Style transfer in vision Non-parallel style transfer has been extensively studied in computer vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017).", "startOffset": 101, "endOffset": 235}, {"referenceID": 18, "context": "Style transfer in vision Non-parallel style transfer has been extensively studied in computer vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017).", "startOffset": 101, "endOffset": 235}, {"referenceID": 4, "context": "More recent approaches learn generative networks directly via generative adversarial training (Goodfellow et al., 2014) from two given data domains X 1 and X 2.", "startOffset": 94, "endOffset": 119}, {"referenceID": 12, "context": "For example, CoupledGANs (Liu and Tuzel, 2016) employ weight-sharing between networks to learn cross-domain representation, whereas CycleGAN (Zhu et al.", "startOffset": 25, "endOffset": 46}, {"referenceID": 20, "context": "For example, CoupledGANs (Liu and Tuzel, 2016) employ weight-sharing between networks to learn cross-domain representation, whereas CycleGAN (Zhu et al., 2017) introduces cycle consistency which relies on transitivity to regularize the transfer functions.", "startOffset": 141, "endOffset": 159}, {"referenceID": 3, "context": "Style transfer in vision Non-parallel style transfer has been extensively studied in computer vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017). Gatys et al. (2016) explicitly extract content and style features, and then synthesize a new image by combining \u201ccontent\u201d features of one image with \u201cstyle\u201d features from another.", "startOffset": 102, "endOffset": 257}, {"referenceID": 14, "context": "Our work most closely relates to approaches that do not utilize parallel data, but instead guide sentence generation from an indirect training signal (Mueller et al., 2017; Hu et al., 2017).", "startOffset": 150, "endOffset": 189}, {"referenceID": 6, "context": "Our work most closely relates to approaches that do not utilize parallel data, but instead guide sentence generation from an indirect training signal (Mueller et al., 2017; Hu et al., 2017).", "startOffset": 150, "endOffset": 189}, {"referenceID": 1, "context": "Using ideas from computer vision (Chen et al., 2016), this factorization is learned in an unsupervised manner.", "startOffset": 33, "endOffset": 52}, {"referenceID": 4, "context": "In contrast, our method utilizes adversarial training (Goodfellow et al., 2014) to guarantee distributional alignment of the common latent space driven by content/style independence.", "startOffset": 54, "endOffset": 79}, {"referenceID": 4, "context": ", 2017; Hu et al., 2017). For instance, Mueller et al. (2017) manipulate the hidden representation to generate sentences that satisfy a desired property (e.", "startOffset": 8, "endOffset": 62}, {"referenceID": 4, "context": ", 2017; Hu et al., 2017). For instance, Mueller et al. (2017) manipulate the hidden representation to generate sentences that satisfy a desired property (e.g., sentiment) as measured by a corresponding classifier. However, their model does not necessarily enforce content preservation. More similar to our work, Hu et al. (2017) facilitate content preservation by explicitly modeling the style and content factorization.", "startOffset": 8, "endOffset": 329}, {"referenceID": 1, "context": "Using ideas from computer vision (Chen et al., 2016), this factorization is learned in an unsupervised manner. The factorization algorithm of Hu et al. (2017) is primarily driven by the predictions of the property classifier.", "startOffset": 34, "endOffset": 159}, {"referenceID": 19, "context": "Adversarial training over discrete samples Recently, a wide range of techniques addresses challenges associated with adversarial training over discrete samples generated by recurrent networks (Yu et al., 2016; Lamb et al., 2016; Che et al., 2017; Hjelm et al., 2017).", "startOffset": 192, "endOffset": 266}, {"referenceID": 11, "context": "Adversarial training over discrete samples Recently, a wide range of techniques addresses challenges associated with adversarial training over discrete samples generated by recurrent networks (Yu et al., 2016; Lamb et al., 2016; Che et al., 2017; Hjelm et al., 2017).", "startOffset": 192, "endOffset": 266}, {"referenceID": 0, "context": "Adversarial training over discrete samples Recently, a wide range of techniques addresses challenges associated with adversarial training over discrete samples generated by recurrent networks (Yu et al., 2016; Lamb et al., 2016; Che et al., 2017; Hjelm et al., 2017).", "startOffset": 192, "endOffset": 266}, {"referenceID": 5, "context": "Adversarial training over discrete samples Recently, a wide range of techniques addresses challenges associated with adversarial training over discrete samples generated by recurrent networks (Yu et al., 2016; Lamb et al., 2016; Che et al., 2017; Hjelm et al., 2017).", "startOffset": 192, "endOffset": 266}, {"referenceID": 11, "context": "In our work, we employ the Professor-Forcing algorithm (Lamb et al., 2016) which was originally proposed to close the gap between teacher-forcing during training and self-feeding during testing for recurrent networks.", "startOffset": 55, "endOffset": 74}, {"referenceID": 9, "context": "One option is to apply a variational auto-encoder (Kingma and Welling, 2013) and maximize the variational lower bound of data likelihood.", "startOffset": 50, "endOffset": 76}, {"referenceID": 17, "context": "Although sampling-based gradient computation such as REINFORCE (Williams, 1992) can by adopted, training with these methods can be unstable due to the high variance of the sampled gradient.", "startOffset": 63, "endOffset": 79}, {"referenceID": 6, "context": "Instead, we employ two recent techniques to approximate the discrete training (Hu et al., 2017; Lamb et al., 2016).", "startOffset": 78, "endOffset": 114}, {"referenceID": 11, "context": "Instead, we employ two recent techniques to approximate the discrete training (Hu et al., 2017; Lamb et al., 2016).", "startOffset": 78, "endOffset": 114}, {"referenceID": 11, "context": "Secondly, we use Professor-Forcing (Lamb et al., 2016) to match the sequence of hidden states instead of the output words, which contains the information about outputs and is smoothly distributed.", "startOffset": 35, "endOffset": 54}, {"referenceID": 7, "context": "To quantitatively evaluate the transfered sentences, we adopt a model-based evaluation metric similar to the one used for image transfer (Isola et al., 2016).", "startOffset": 137, "endOffset": 157}, {"referenceID": 2, "context": "Word substitution decipherment Our second set of experiments involves decoding of word substitution ciphers, which has been previously explored in NLP literature (Dou and Knight, 2012; Nuhn and Ney, 2013).", "startOffset": 162, "endOffset": 204}, {"referenceID": 15, "context": "We can quantitatively compare betweenD1 and transferred (deciphered)D2 using Bleu score (Papineni et al., 2002).", "startOffset": 88, "endOffset": 111}, {"referenceID": 10, "context": "Finally, to assess the difficulty of the task, we report the accuracy of machine translation system trained on a parallel corpus (Klein et al., 2017).", "startOffset": 129, "endOffset": 149}], "year": 2017, "abstractText": "This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broader family of problems including machine translation, decipherment, and sentiment modification. The key technical challenge is to separate the content from desired text characteristics such as sentiment. We leverage refined alignment of latent representations across mono-lingual text corpora with different characteristics. We deliberately modify encoded examples according to their characteristics, requiring the reproduced instances to match available examples with the altered characteristics as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.", "creator": "LaTeX with hyperref package"}}}