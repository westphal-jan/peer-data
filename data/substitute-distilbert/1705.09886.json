{"id": "1705.09886", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2017", "title": "Convergence Analysis of Two-layer Neural Networks with ReLU Activation", "abstract": "in recent researchers, adaptive gradient descent ( sgd ) based techniques has become the standard tools for training neural networks. however, formal theoretical understanding of why sgd can train neural programming in practice is now missing.", "histories": [["v1", "Sun, 28 May 2017 02:11:10 GMT  (1221kb,D)", "http://arxiv.org/abs/1705.09886v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuanzhi li", "yang yuan"], "accepted": true, "id": "1705.09886"}, "pdf": {"name": "1705.09886.pdf", "metadata": {"source": "CRF", "title": "Convergence Analysis of Two-layer Neural Networks with ReLU Activation", "authors": ["Yuanzhi Li", "Yang Yuan"], "emails": ["yuanzhil@cs.princeton.edu", "yangyuan@cs.cornell.edu"], "sections": [{"heading": null, "text": "In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called \u201cidentity mapping\u201d. We prove that, if input follows from Gaussian distribution, with standard O(1/ \u221a d) initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the \u201cidentity mapping\u201d makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks.\nOur convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in \u201ctwo phases\u201d: In phase I, the gradient points to the wrong direction, however, a potential function g gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims.\n1 Introduction Deep learning is the mainstream technique for many machine learning tasks, including image recognition, machine translation, speech recognition, etc. [14]. Despite its success, the theoretical understanding on how it works remains poor. It is well known that neural networks have great expressive power [19, 6, 3, 7, 27]. That is, for every function there exists a set of weights on the neural network such that it approximates the function everywhere. However, it is unclear how to obtain the desired weights. In practice, the most commonly used method is stochastic gradient descent based methods (e.g., SGD, Momentum [36], Adagrad [9], Adam [22]), but to the best of our knowledge, there were no theoretical guarantees that such methods will find good weights.\nIn this paper, we give the first convergence analysis of SGD for two-layer feedforward network with ReLU activations. For this basic network, it is known that even in the simplified setting where the weights are initialized symmetrically and the ground truth forms orthonormal basis, gradient descent might get stuck at saddle points [37]. Moreover, the origin is a singular point with noncontinuous gradient, which makes the analysis more difficult.\nInspired by the structure of residual network (ResNet) [18], we add an extra identity mapping for the hidden layer (see Figure 1). Surprisingly, we show that simply by adding this mapping, with the standard initialization scheme and small step size, SGD always converges to the ground truth. In other words, the optimization becomes significantly easier, after adding the identity mapping. See Figure 2, based on our analysis, the region near the identity matrix I contains only one global minimum without any saddle points or local minima, thus is easy for SGD to optimize. The role of the identity mapping here, is to move the initial point to this easier region.\nar X\niv :1\n70 5.\n09 88\n6v 1\n[ cs\n.L G\n] 2\n8 M\nOther than being feedforward and shallow, our network is different from ResNet in the sense that our identity mapping skips one layer instead of two. However, as we will show in Section 5.1, the skip-one-layer identity mapping already brings significant improvement to vanilla networks. Therefore, we believe our model captures one of the most important features of ResNet, and partially explains why ResNets are easier to train in practice.\nFormally, we consider the following function.\nf(x,W) = \u2016ReLU((I + W)>x)\u20161 (1)\nwhere ReLU(v) = max(v, 0) is the ReLU activation function. x \u2208 Rd is the input vector sampled from a Gaussian distribution, and W \u2208 Rd\u00d7d is the weight matrix, where d is the number of input units. Notice that I adds ei to column i of W, which makes f asymmetric in the sense that by switching any two columns in W, we get different functions. Fixing the second layer to be all one is without loss of generality, because ReLU is non-negative homogeneous, i.e., one can always multiply the input of ReLU and divide the output of ReLU by the same factor and get the same function.\nFollowing the standard setting [30, 37], we assume that there exists a two-layer teacher network with weight W\u2217. We train the student network using `2 loss:\nL(W) = Ex[(f(x,W)\u2212 f(x,W\u2217))2] (2)\nWe will define a potential function g, and show that if g is small, the gradient points to partially correct direction and we get closer to W\u2217 after every SGD step. However, g could be large and thus gradient might point to the reverse direction. Fortunately, we also show that if g is large, by doing SGD, it will keep decreasing until it is small enough while maintaining the weight W in a nice region. We call the process of decreasing g as Phase I, and the process of approaching W\u2217 as Phase II. See Figure 3 and simulations in Section 5.3.\nOur two phases framework is fundamentally different from any type of local convergence, as in Phase I, the gradient is pointing to the wrong direction to W\u2217, so the path from W to W\u2217 is non-convex, and SGD takes a long detour to arrive W\u2217. This framework could be potentially useful for analyzing other non-convex problems.\nTo support our theory, we have done a few other experiments and got interesting observations. For example, as predicted by our theorem, we found that for multilayer feedforward network with identity mappings, zero initialization performs as good as random initialization. At the first glance, it contradicts the common belief \u201crandom initialization is necessary to break symmetry\u201d, but actually the identity mapping itself serves as the asymmetric component. See Section 5.4.\nAnother common belief is that neural network has lots of local minima and saddle points [8], so even if there exists a global minimum, we may not be able to arrive there. As a result, even when the teacher network is shallow, the student network usually needs to be deeper, otherwise it will underfit. However, both our theorem and our experiment show that if the shallow teacher network is in a pretty large region near identity (Figure 2), SGD always converges to the global minimum by initializing the weights I + W in this region, with equally shallow student network. By contrast, wrong initialization gets stuck at local minimum and underfit. See Section 5.2.\nRelated Work Expressivity. Even two-layer network has great expressive power. For example, two-layer network with sigmoid activations could approximate any continuous function [19, 6, 3]. ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].\nLearning. Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2]. [31] proved that with high probability, there exists a continuous decreasing path from random initial point to the global minimum, but SGD may not follow this path.\nLinear network and independent activation. Some previous works simplified the model by ignoring the activation functions and considering deep linear networks [32, 21] or deep linear residual networks [16], which can only learn linear functions. Some previous results are based on independent activation assumption that the activations of ReLU and the input are independent, which is unrealistic [4, 21].\nSaddle points. It is observed that saddle point is not a big problem for neural networks [8, 15]. In general, if the objective is strict-saddle [10], SGD could escape all saddle points.\n2 Preliminaries Denote x as the input vector in Rd. For now, we first consider x sampled from normal distribution N (0, I). Denote W\u2217 = (w\u22171 , \u00b7 \u00b7 \u00b7 , w\u2217n) \u2208 Rd\u00d7d as the weights for the teacher network, W = (w1, \u00b7 \u00b7 \u00b7 , wn) \u2208 Rd\u00d7d as the weights for the student network, where w\u2217i , wi \u2208 Rd are column vectors. f(x,W\u2217), f(x,W) are defined in (1), representing the teacher and student network.\nWe want to know whether a randomly initialized W will converge to W\u2217, if we run SGD with l2 loss defined in (2). Alternatively, we can write the loss L(W) as\nEx[(\u03a3iReLU(\u3008ei + wi, x\u3009)\u2212 \u03a3iReLU(\u3008ei + w\u2217i , x\u3009))2]\nTaking derivative with respect to wj , we get\n\u2207L(W)j = 2Ex [(\u2211\ni\nReLU(\u3008ei + wi, x\u3009)\u2212 \u2211\ni\nReLU(\u3008ei + w\u2217i , x\u3009) ) x1\u3008ej+wj ,x\u3009\u22650 ]\nwhere 1e is the indicator function that equals 1 if the event e is true, and 0 otherwise. Here \u2207L(W) \u2208 Rd\u00d7d, and\u2207L(W)j is its j-th column.\nDenote \u03b8i,j as the angle between ei +wi and ej +wj , \u03b8i\u2217,j as the angle between ei +w\u2217i and ej +wj . Denote v\u0304 = v\u2016v\u20162 . Denote I + W\n\u2217 and I + W\u2217 as the column-normalized version of I + W\u2217 and I + W such that every column has unit norm. Since the input is from a normal distribution, one can compute the expectation inside the gradient as follows.\nLemma 2.1 (Eqn (13) from [37]). If x \u223c N (0, I), then \u2212\u2207L(W)j = \u2211d i=1 ( \u03c0 2 (w \u2217 i \u2212 wi) + ( \u03c0 2 \u2212 \u03b8i\u2217,j ) (ei + w\u2217i )\u2212 ( \u03c0 2 \u2212 \u03b8i,j ) (ei + wi) + ( \u2016ei + w\u2217i \u20162 sin \u03b8i\u2217,j \u2212 \u2016ei + wi\u20162 sin \u03b8i,j ) ej + wj )\nDenote u \u2208 Rd as the all one vector. Denote Diag(W) as the diagonal matrix of matrix W, Diag(v) as a diagonal matrix whose main diagonal equals to the vector v. Denote Off-Diag(W) , W \u2212 Diag(W). Denote [d] as the set {1, \u00b7 \u00b7 \u00b7 , d}. Throughout the paper, we abuse the notation of inner product between matrices W,W\u2217,\u2207L(W), such that \u3008\u2207L(W),W\u3009 means the summation of the entrywise products. \u2016W\u20162 is the spectral norm of W, and \u2016W\u2016F is the Frobenius norm of W. We define the potential function g and variables gj ,Aj ,A below, which will be useful in the proof. Definition 2.2. We define the potential function g , \u2211d i=1(\u2016ei + w\u2217i \u20162 \u2212 \u2016ei + wi\u20162), and variable gj ,\u2211\ni6=j(\u2016ei + w\u2217i \u20162 \u2212 \u2016ei + wi\u20162).\nDefinition 2.3. Denote Aj , \u2211 i 6=j((ei+w \u2217 i )ei + w \u2217 i >\u2212(ei+wi)ei + wi>),A , \u2211d i=1((ei+w \u2217 i )ei + w \u2217 i >\u2212 (ei + wi)ei + wi > ) = (I + W\u2217)I + W\u2217 > \u2212 (I + W)I + W>.\nIn this paper, we consider the standard SGD with mini batch method for training the neural network. Assume W0 is the initial point, and in step t > 0, we have the following updating rule:\nWt+1 = Wt \u2212 \u03b7tGt\nwhere the stochastic gradient Gt = \u2207L(Wt) + Et with E[Et] = 0 and \u2016Et\u2016F \u2264 \u03b5. Let G2 , 6d\u03b3 + \u03b5,GF , 6d1.5\u03b3 + \u03b5. As we will see in Lemma B.2, they are the upper bound of \u2016Gt\u20162 and \u2016Gt\u2016F respectively.\nIt\u2019s clear that L is not convex, In order to get convergence guarantees, we need a weaker condition called one point convexity.\nDefinition 2.4 (One point strongly convexity). A function f(x) is called \u03b4-one point strongly convex in domain D with respect to point x\u2217, if \u2200x \u2208 D, \u3008\u2212\u2207f(x), x\u2217 \u2212 x\u3009 > \u03b4\u2016x\u2217 \u2212 x\u201622.\nBy definition, if a function f is strongly convex, it is also one point strongly convex in the entire space with respect to the global minimum. However, the reverse is not necessarily true, e.g., see Figure 4. If a function is one point strongly convex, then in every step a positive fraction of the negative gradient is pointing to the optimal point. As long as the step size is small enough, we will finally arrive the optimal point, possibly by a winding path. See Figure 3 for illustration, where starting from W6 (Phase II), we get closer to W\u2217 in every step. Formally, we have the following lemma.\nLemma 2.5. For function f(W), consider the SGD update Wt+1 = Wt \u2212 \u03b7Gt, where E[Gt] = \u2207f(Wt), E[\u2016Gt\u20162F ] \u2264 G2. Suppose for all t, Wt is always inside the \u03b4-one point strongly convex region with diameter D, i.e., \u2016Wt \u2212W\u2217\u2016F \u2264 D. Then for any \u03b1 > 0 and any T such that T\u03b1 log T \u2265 D 2\u03b42 (1+\u03b1)G2 , if \u03b7 = (1+\u03b1) log T \u03b4T , we have \u2016WT \u2212W\u2217\u20162F \u2264 (1+\u03b1) log TG 2 \u03b42T .\nThe proof can be found in Appendix I. Lemma 2.5 uses constant step size, so it easily fits the standard practical scheme that shrinks \u03b7 by a factor of 10 after every a few epochs. For example, we may apply Lemma 2.5 every time \u03b7 gets changed.\n3 Main Theorem Theorem 3.1 (Main Theorem). There exists constants \u03b3 > \u03b30 > 0 such that If x \u223c N (0, I), \u2016W0\u20162, \u2016W\u2217\u20162 \u2264 \u03b30, d \u2265 100, \u03b5 \u2264 \u03b32, then SGD for L(W) will find the ground truth W\u2217 by two phases. In Phase I, by setting\n\u03b7 \u2264 \u03b32 G22 , the potential function will keep decreasing until it is smaller than 197\u03b32, which takes at most 116\u03b7 steps. In Phase II, for any \u03b1 > 0 and any T such that T\u03b1 log T \u2265 36d 1004(1+\u03b1)G2F , if we set \u03b7 = (1+\u03b1) log T\u03b4T , we have \u2016WT \u2212W\u2217\u20162F \u2264 1002(1+\u03b1) log TG2F 9T .\nRemarks. Randomly initializing the weights with O(1/ \u221a d) is standard in deep learning, see [23, 11, 17]. It\nis also well known that if the entries are initialized with O(1/ \u221a d), the spectral norm of the random matrix is O(1) [29]. So our result matches with the common practice. Moreover, as we will show in Section 5.5, networks with small average spectral norm already have good performance. Thus, our assumption \u2016W\u2217\u20162 = O(1) is reasonable. Notice that here we assume the spectral norm of W\u2217 to be constant, which means the Frobenius norm \u2016W\u2217\u2016F could be as big as O( \u221a d).\nThe assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]). We could easily generalize the analysis to rotation invariant distributions, and potentially more general distributions (see Section 6). Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent. By contrast, in our model the ReLU activations are highly correlated2 as \u2016W\u20162, \u2016W\u2217\u20162 = \u2126(1). As pointed out by [5], eliminating the unrealistic assumptions on activation independence is the central problem of analyzing the loss surface of neural network, which was not fully addressed by the previous analyses.\nTo prove the main theorem, we split the process and present the following two theorems, which will be proved in Appendix B and C.\nTheorem 3.2 (Phase I). There exists a constant \u03b3 > \u03b30 > 0 such that If \u2016W0\u20162, \u2016W\u2217\u20162 \u2264 \u03b30, d \u2265 100, \u03b7 \u2264 \u03b3 2\nG22 ,\n\u03b5 \u2264 \u03b32, then gt will keep decreasing by a factor of 1\u2212 0.5\u03b7d for every step, until gt1 \u2264 197\u03b32 for step t1 \u2264 116\u03b7 . After that, Phase II starts. That is, for every T > t1, we have \u2016WT \u20162 \u2264 1100 and gT \u2264 0.1.\nTheorem 3.3 (Phase II). There exists a constant \u03b3 such that if \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3, and g \u2264 0.1, then \u3008\u2212\u2207L(W),W\u2217 \u2212W\u3009 = \u2211dj=1\u3008\u2212\u2207L(W)j , w\u2217j \u2212 wj\u3009 > 0.03\u2016W\u2217 \u2212W\u20162F .\nWith these two theorems, we get the main theorem immediately.\nProof for Theorem 3.1. By Theorem 3.2, we know the statement for Phase I is true, and we will enter phase II in 116\u03b7 steps. After entering Phase II, based on Theorem 3.3, we simply use Lemma 2.5 by setting \u03b4 = 0.03, D = \u221a d\n50 , G = GF to get the convergence guarantee.\n4 Overview of the Proofs General Picture. In many convergence analyses for non-convex functions, one would like to show that L is one point strongly convex, and directly apply Lemma 2.5 to get the convergence result. However, this is not true for 2-layer neural network, as the gradient may point to the wrong direction, see Section 5.3.\nSo when is our L one point convex? Consider the following thought experiment: First, suppose \u2016W\u20162, \u2016W\u2217\u20162 \u2192 0, we know \u2016wi\u20162, \u2016w\u2217i \u20162 also go to 0. Thus, ei + wi and ei + w\u2217i are close to ei. As a result, \u03b8i,j , \u03b8i\u2217,j \u2248 \u03c02 , and \u03b8i\u2217,i \u2248 0. Based on Lemma 2.1, this gives us a na\u00efve approximation of the negative gradient, i.e., \u2212\u2207L(W)j \u2248 \u03c02 (w\u2217j \u2212 wj) + \u03c02 \u2211d i=1(w \u2217 i \u2212 wi) + ej + wj \u2211 i 6=j(\u2016ei + w\u2217i \u20162 \u2212 \u2016ei + wi\u20162) .\nWhile the first two terms \u03c02 (w \u2217 j \u2212 wj) and \u03c02 \u2211d i=1(w \u2217 i \u2212 wi) have positive inner product with W\u2217 \u2212W,\nthe last term gj = ej + wj \u2211 i 6=j(\u2016ei + w\u2217i \u20162 \u2212 \u2016ei + wi\u20162) can point to arbitrary direction. If the last term is small, it can be covered by the first two terms, and L becomes one point strongly convex. So we define a potential function closely related to the last term: g = \u2211d i=1(\u2016ei +w\u2217i \u20162\u2212\u2016ei +wi\u20162). We show that if g is small enough, L is also one point strongly convex (Theorem 3.3).\n1They assume input is Gaussian and the W\u2217 is orthonormal, which means the activations are independent in teacher network. 2 Let \u03c3i be the output of i-th ReLU unit, then in our setting, \u2211 i,j Cov[\u03c3i, \u03c3j ] can be as large as \u2126(d), which is far from being independent.\nHowever, from random initialization, g can be as large as of \u2126( \u221a d), which is too big to be covered. Fortunately, we show that if g is big, it will gradually decrease simply by doing SGD on L. More specifically, we introduce a two phases convergence analysis framework:\n1. In Phase I, the potential function g is decreasing to a small value. 2. In Phase II, g remains small, so L is one point convex and thus W starts to converge to W\u2217. We believe that this framework could be helpful for other non-convex problems. Technical difficulty: Phase I. Our key technical challenge is to show that in Phase I, the potential function actually decreases to O(1) after polynomial number of iterations. However, we cannot show this by merely looking at g itself. Instead, we introduce an auxiliary variable s = (W\u2217 \u2212W)u. By doing a careful calculation, we get their joint update rules (Lemma B.3 and Lemma B.4):\n{ st+1 \u2248 st \u2212 \u03c0\u03b7d2 st + \u03b7O( \u221a dgt + \u221a d\u03b3)\ngt+1 \u2248 gt \u2212 \u03b7dgt + \u03b7O(\u03b3 \u221a d\u2016st\u20162 + d\u03b32)\nSolving this dynamics, we can show that gt will approach to (and stay around) O(\u03b3), thus we enter Phase II. Technical difficulty: Phase II. Although the overall approximation in the thought experiment looks simple, the argument is based on an over simplified assumption that \u03b8i\u2217,j , \u03b8i,j \u2248 \u03c02 for i 6= j. However, when W\u2217 has constant spectral norm, even when W is very close to W\u2217, \u03b8i,j\u2217 could be constantly far away from \u03c02 , which prevents us from applying this approximation directly. To get a formal proof, we use the standard Taylor expansion and control the higher order terms. Specifically, we write \u03b8i\u2217,j as \u03b8i\u2217,j = arccos\u3008ei + w\u2217i , ej + wj\u3009 and expand arccos at point 0, thus,\n\u03b8i\u2217,j = \u03c0\n2 \u2212 \u3008ei + w\u2217i , ej + wj\u3009+O(\u3008ei + w\u2217i , ej + wj\u30093)\nHowever, even when W \u2248W\u2217, the higher order term O(\u3008ei + w\u2217i , ej + wj\u30093) still can be as large as a constant, which is too big for us. Our trick here is to consider the \u201cjoint Taylor expansion\u201d:\n\u03b8i\u2217,j \u2212 \u03b8i,j = \u3008ei + wi, ej + wj\u3009 \u2212 \u3008ei + w\u2217i , ej + wj\u3009+O(|\u3008ei + w\u2217i , ej + wj\u30093 \u2212 \u3008ei + wi, ej + wj\u30093|)\nAs W approaches W\u2217, |\u3008ei + w\u2217i , ej + wj\u30093 \u2212 \u3008ei + wi, ej + wj\u30093| also tends to zero, therefore our approximation has bounded error.\nIn the thought experiment, we already know that the constant part in the Taylor expansion of \u2207L(W) is \u03c0 2 \u2212O(g)-one point convex. We show that after taking inner product with W\u2217\u2212W, the first order terms are lower bounded by (roughly) \u22121.3\u2016W\u2217\u2212W\u20162F and the higher order terms are lower bounded by \u22120.085\u2016W\u2217\u2212W\u20162F . Adding them together, we can see that L(W) is one point convex as long as g is small. See Figure 5.\nGeometric Lemma. In order to get through the whole analysis, we need tight bounds on a few common terms that appear everywhere. Instead of using na\u00efve algebraic techniques, we come up with a nice geometric proof to get nearly optimal bounds. See Section D.\nFlowchart of the proofs. Although the proofs of our theorems are intricate, many lemmas have clear intuition behind the statement. Therefore, we add \u201c*\u201d to these lemmas, so that time constrained readers could feel confident to skip the proofs. We also plot a flowchart of the proofs in Figure 6 to help the readers spend time wisely.\nSince the proofs are long and complicated, we choose to present them in a top-down way. That is, we present the main theorems (Theorem 3.1, Theorem 3.2, and Theorem 3.3) in the main paper, and then present the necessary lemmas in order to prove those main theorems in Section A, Section B and Section C. Finally, we present the proofs for those lemma in Section F, Section G and Section H, respectively.\n5 Experiments In this section, we present several simulation results to support our theory. Our code can be found in Github3 and the supplementary materials.\n5.1 Importance of identity mapping In this experiment, we compare the standard ResNet [18] and single skip model where identity mapping skips only one layer. We used the open source code from Facebook4, and made minor modifications to get the single skip model. See Figure 7. We also ran the vanilla network, where the identity mappings are completely removed.\nIn this experiment, we choose Cifar-10 as the dataset, and all the networks have 56-layers. Other than the identity mappings, all other settings are identical and default, including network width, initialization, optimization methods, etc. We run the experiments for 5 times for both single skip model and vanilla network, and report the average test error. See Table 1 for the results.\nAs we can see, compared with vanilla network, by simply using a single skip identity mapping, one can already improve the test error by 3.03%, and is 2.04% close to the ResNet. So we claim that a single skip identity mapping already brings significant improvement on test accuracy.\n5.2 Global minimum convergence In this experiment, we verify our main theorem that for two-layer teacher network and student network with identity mappings, as long as \u2016W0\u20162, \u2016W\u2217\u20162 is small, SGD always converges to the global minimum W\u2217, thus gives almost 0 training error and test error. We consider three student networks. The first one (ResLink) is defined using (2), the second one (Vanilla) is the same model without the identity mapping. The last one (3-Block) is a three block network with each block containing a linear layer (500 hidden nodes), a batch normalization and a ReLU layer. The teacher network always shares the same structure as the student network.\n3https://github.com/anonymous 4https://github.com/facebook/fb.resnet.torch.git\nThe input dimension is 100. We generated a fixed W\u2217 for all the trials with \u2016W\u2217\u20162 \u2248 0.6, \u2016W\u2217\u2016F \u2248 5.7. We generated a training set of size 100, 000, and test set of size 10, 000, sampled from a Gaussian distribution. We use batch size 200, step size 0.001. We run ResLink for 5 times with random initialization (\u2016W\u20162 \u2248 0.6 and \u2016W\u2016F \u2248 5), and plot the curves by taking the average.\nFigure 8(a) shows test error and training error of the three networks. Comparing Vanilla with 3-Block, we find that 3-Block is more expressive, so its training error is smaller; but it suffers from overfitting and has bigger test error. This is the standard overfitting vs underfitting tradeoff. Surprisingly, with only one hidden layer, ResLink has both zero test error and training error. If we look at Figure 8(b), we know the distance between W and W\u2217 converges to 0, meaning ResLink indeed finds the global optimal in all 5 trials. By contrast, Vanilla, which is essentially the same network with different initialization, could not converge to the global optimal5. This is exactly what our theory predicted.\n5.3 Verify the dynamics In this experiment, we verify our claims on the dynamics. Based on the analysis, we construct a 1500\u00d7 1500 matrix W s.t. \u2016W\u20162 \u2248 0.15, \u2016W\u2016F \u2248 5 , and set W\u2217 = 0. By plugging them into (2), one can see that even in this simple case that W\u2217 = 0, initially the gradient is pointing to the wrong direction, i.e., not one point convex. We then run SGD on W by using samples x from Gaussian distribution, with batch size 300, step size 0.0001.\nFigure 9(a) shows the first 100 iterations. We can see that initially the inner product defined in Definition 2.4 is negative, then after about 15 iterations, it turns positive, which means W is in the one point strongly convex region. At the same time, the potential g keeps decreasing to a small value, while the distance to optimal (which also equals to \u2016W\u2016F in this experiment) is not affected. They precisely match with our description of Phase I in Theorem 3.2.\n5To make comparison meaningful, we set W \u2212 I to be the actual weight for Vanilla as its identity mapping is missing, which is why it has a much bigger initial norm.\nAfter that, we enter Phase II and slowly approach to W\u2217, see Figure 9(b). Notice that the potential g is always very small, the inner product is always positive, and the distance to optimal is slowly decreasing. Again, they precisely match with our Theorem 3.3.\n5.4 Zero initialization works In this experiment, we used a simple 5-block neural network on MNIST, where every block contains a 784\u2217784 feedforward layer, an identity mapping, and a ReLU layer. Cross entropy criterion is used. We compare zero initialization with standard O(1/ \u221a d) random initialization. We found that for zero initialization, we can get 1.28% test error, while for random initialization, we can get 1.27% test error. Both results were obtained by taking average among 5 runs and use step size 0.1, batch size 256. If the identity mapping is removed, zero initialization no longer works.\n5.5 Spectral norm of W\u2217\nWe have run a few experiments to find feedforward networks with identity mapping, small \u2016W\u20162 and good expressivity. For 15-block network, where each block contains a 784\u00d7784 feedforward layer, an identity mapping and a ReLU layer, we found that, in order to achieve 2.14% test error for MNIST, it suffices to have average \u2016W\u20162 \u2248 0.34 among all 15 feedforward layers. Actually, when the network becomes deeper, the average norm of the weight matrices will further decrease, otherwise the output of the network will explode. See for example [16] for justification.\nWe also applied the exact model f defined in (1) to distinguish two classes in MNIST. For any input image x, We say it\u2019s in class A if f(x,W) < TA,B , and in class B otherwise. Here TA,B is the optimal threshold for the function f(x,0) to distinguish A and B. If W = 0, we get 7% training error for distinguish class 0 and class 1. However, it can be improved to 1% with \u2016W\u20162 = 0.6. We tried this experiment for all possible 45 pairs of classes in MNIST, and improve the average training error from 34% (using W = 0) to 14% (using \u2016W\u20162 = 0.6). Therefore our model with \u2016W\u20162 = \u2126(1) has reasonable expressive power, and is substantially different from just using the identity mapping alone.\n6 Discussions The assumption that the input is Gaussian can be relaxed in several ways. For example, when the distribution is N (0,\u03a3) where \u2016\u03a3\u2212 I\u20162 is bounded by a small constant, the same result holds with slightly worse constants. Moreover, since the analysis relies Lemma 2.1, which is proved by converting the original input space into polar space, it is easy to generalize the calculation to rotation invariant distributions. Finally, for more general distributions, as long as we could explicitly compute the expectation, which is in the form of O(W\u2217 \u2212W) plus certain potential function, our analysis framework may also be applied.\nThere are many exciting open problems. For example, Our paper is the first one that gives solid SGD analysis for neural network with nonlinear activations, without unrealistic assumptions like independent activation assumption. It would be great if one could further extend it to multiple layers, which would be a major breakthrough of understanding optimization for deep learning. Moreover, our two phase framework could be applied to other non-convex problems as well.\nReferences [1] Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with neural\nnetworks. In ICML, pages 1908\u20131916, 2014.\n[2] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 584\u2013592, 2014.\n[3] Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Trans. Information Theory, 39(3):930\u2013945, 1993.\n[4] Anna Choromanska, Mikael Henaff, Micha\u00ebl Mathieu, G\u00e9rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In AISTATS, 2015.\n[5] Anna Choromanska, Yann LeCun, and G\u00e9rard Ben Arous. Open problem: The landscape of the loss surfaces of multilayer networks. In Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, pages 1756\u20131760, 2015.\n[6] George Cybenko. Approximation by superpositions of a sigmoidal function. MCSS, 5(4):455, 1992.\n[7] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In NIPS, pages 2253\u20132261, 2016.\n[8] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS 2014, pages 2933\u20132941, 2014.\n[9] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2011.\n[10] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic gradient for tensor decomposition. In COLT 2015, volume 40, pages 797\u2013842, 2015.\n[11] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, pages 249\u2013256, 2010.\n[12] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In AISTATS, pages 315\u2013323, 2011.\n[13] Surbhi Goel, Varun Kanade, Adam R. Klivans, and Justin Thaler. Reliably learning the relu in polynomial time. CoRR, abs/1611.10258, 2016.\n[14] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www. deeplearningbook.org.\n[15] Ian J. Goodfellow and Oriol Vinyals. Qualitatively characterizing neural network optimization problems. CoRR, abs/1412.6544, 2014.\n[16] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. CoRR, abs/1611.04231, 2016.\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, pages 1026\u20131034, 2015.\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.\n[19] Kurt Hornik, Maxwell B. Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359\u2013366, 1989.\n[20] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.\n[21] Kenji Kawaguchi. Deep learning without poor local minima. In NIPS, pages 586\u2013594, 2016.\n[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.\n[23] Yann LeCun, Leon Bottou, Genevieve B. Orr, and Klaus Robert M\u00fcller. Efficient BackProp, pages 9\u201350. Springer Berlin Heidelberg, Berlin, Heidelberg, 1998.\n[24] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In NIPS, pages 855\u2013863, 2014.\n[25] Guido F. Mont\u00fafar, Razvan Pascanu, KyungHyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In NIPS, pages 2924\u20132932, 2014.\n[26] Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, pages 807\u2013814, 2010.\n[27] Xingyuan Pan and Vivek Srikumar. Expressiveness of rectifier networks. In ICML, pages 2427\u20132435, 2016.\n[28] Razvan Pascanu, Guido Mont\u00fafar, and Yoshua Bengio. On the number of inference regions of deep feed forward networks with piece-wise linear activations. CoRR, abs/1312.6098, 2013.\n[29] M. Rudelson and R. Vershynin. Non-asymptotic theory of random matrices: extreme singular values. ArXiv e-prints, 2010.\n[30] David Saad and Sara A. Solla. Dynamics of on-line gradient descent learning for multilayer neural networks. Advances in Neural Information Processing Systems, 8:302\u2013308, 1996.\n[31] Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In ICML, pages 774\u2013782, 2016.\n[32] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. CoRR, abs/1312.6120, 2013.\n[33] Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with sparse connectivity. ICLR, 2015.\n[34] Ohad Shamir. Distribution-specific hardness of learning neural networks. CoRR, abs/1609.01037, 2016.\n[35] Jir\u00ed S\u00edma. Training a single sigmoidal neuron is hard. Neural Computation, 14(11):2709\u20132728, 2002.\n[36] Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum in deep learning. In ICML, pages 1139\u20131147, 2013.\n[37] Yuandong Tian. Symmetry-breaking convergence analysis of certain two-layered neural networks with relu nonlinearity. In Submitted to ICLR 2017, 2016.\n[38] Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks. In AISTATS, 2017.\n[39] Yuchen Zhang, Jason D. Lee, Martin J. Wainwright, and Michael I. Jordan. Learning halfspaces and neural networks with random initialization. CoRR, abs/1511.07948, 2015.\nA Compute Approximation Matrix The exact form of \u2212\u2207L(W)j in Lemma 2.1 contains variables like \u03b8i\u2217,j , \u03b8i,j , sin \u03b8i\u2217,j , sin \u03b8i,j , which are hard to deal with. In this section, we compute the approximation of these terms using Taylor series, and show that the approximation loss is minor. While the proofs are technically involved, the claims themselves are not surprising. Hence, we encourage the readers to skip the proofs (Appendix F) for the first reading.\nDefine the j-th column of the approximation matrix P as follows. See Definition 2.2 and Definition 2.3 for gj ,Aj .\nPj , P1,j + P2,j + P3,j , where\nP1,j , d\u2211\ni=1\n\u03c0 2 (w\u2217i \u2212 wi),\nP2,j , gjej + wj + (\nI\u2212 1 2 ej + wj \u00b7 ej + wj>\n) Ajej + wj ,\nP3,j , (\u03c0\n2 \u2212 \u03b8j\u2217,j\n) (ej + w \u2217 j )\u2212 \u03c0\n2 (ej + wj) + \u2016ej + w\u2217j \u2016 sin \u03b8j\u2217,jej + wj .\nTreat P1,j ,P2,j ,P3,j as j-th column of matrix P1,P2,P3 respectively, we have P = P1 + P2 + P3. Although P depends on W, we abuse the notation and simply write P.\nClaim A.1. Pj approximates\u2212\u2207L(W)j by setting (\u03c02\u2212\u03b8i,j) \u2248 \u3008ei + wi, ej + wj\u3009, (\u03c02\u2212\u03b8i\u2217,j) \u2248 \u3008ei + w\u2217i , ej + wj\u3009, sin \u03b8i,j \u2248 1\u2212 12 \u3008ei + wi, ej + wj\u30092 and sin \u03b8i\u2217,j \u2248 1\u2212 12 \u3008ei + w\u2217i , ej + wj\u30092.\nBelow we show that the approximation loss is negligible in terms of one point convexity and spectral norm.\nLemma* A.2. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , |\u3008P +\u2207L(W),W\u2217 \u2212W\u3009| < 0.085\u2016W\u2217 \u2212W\u20162F .\nLemma* A.3. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , \u2016P +\u2207L(W)\u20162 \u2264 3.5\u03b32.\nB Phase I: The Decreasing Potential Function As we saw in Theorem 3.3, if \u2016W\u20162, \u2016W\u2217\u20162 is bounded by a constant \u03b3 = 1100 , and the potential function g \u2264 0.1, L(W) is 0.03-one point convex, which will give us convergence guarantee according to Lemma 2.5. However, g could be larger than 0.1 initially, and as we run SGD, \u2016W\u20162 might be larger than 1100 as well.\nIn this section, we address both problems by analyzing the dynamics of SGD, thus prove Theorem 3.2. The proofs can be found in Appendix G. Before proceeding to the interesting stuff, we need a simpler form of\u2207L(W) to work with, see below.\nLemma B.1. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , the negative gradient of L(W) is approximately\nQ(W) , \u03c0 2\n(W\u2217 \u2212W) ( I + uu> ) + (W\u2217 \u2212W)> \u2212 2Diag(W\u2217 \u2212W) + gI + W\nwhere u is the all 1 vector. The approximation error is \u2016Q(W)\u2212 [\u2212\u2207L(W)]\u20162 \u2264 61\u03b32.\nWe immediately get the bound of the gradient norm.\nLemma* B.2. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , \u2016\u2207L(W)\u20162 \u2264 6d\u03b3.\nNow we are ready to analyze the dynamics. We use subscript t under each variable to denote its value at the step t. For simplicity, let Qt , Q(Wt). Define st , (W\u2217 \u2212Wt)u. We first compute the updating rule for gt.\nLemma B.3. If \u2016Wt\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , d \u2265 100, \u03b7 \u2264 \u03b32 G22 , then |gt+1| \u2264 (1 \u2212 0.95\u03b7d)|gt| + 86\u03b7d\u03b32 +\n1.03\u03b7 \u221a d\u03b5+ 4.8\u03b7\u2016st\u20162\u03b3 \u221a d.\nThe bound contains \u2016st\u20162 which could be large, so we also need to compute its updating rule:\nLemma B.4. If \u2016Wt\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , then \u2016st+1\u20162 \u2264 ( 1\u2212 \u03b7 (d+1)\u03c02 ) \u2016st\u20162+\u03b7(6.61\u03b3+1.03|gt|+\u03b5) \u221a d.\nCombining the two lemmas, we are ready to show that gt will shrink, conditioned on that \u2016Wt\u20162 is bounded by \u03b3.\nLemma B.5. If for every step t > 0, \u2016Wt\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 ,d \u2265 100, \u03b7 \u2264 \u03b32 G22 , \u03b5 \u2264 \u03b32, then |gt| will keep decreasing by a factor of 1\u2212 0.5\u03b7d for every step, until |gt1 | \u2264 197\u03b32 for t1 \u2264 116\u03b7 .\nFortunately, we also know that \u2016Wt\u20162 is always bounded by \u03b3 during the process described in Lemma B.5.\nLemma B.6. There exists a constant \u03b3 > \u03b30 > 0 such that if \u2016W0\u20162, \u2016W\u2217\u20162 \u2264 \u03b30, d \u2265 100, \u03b7 \u2264 \u03b3 2 G22 , \u03b5 \u2264 \u03b32, then in the process of Phase I (Lemma B.5), we always have \u2016WT \u20162 \u2264 \u03b3 \u2264 1100 for any T > 0.\nNow, we are at the state where |gt| is small, and \u2016WT \u20162 \u2264 \u03b3, which means we are in Phase II. The next lemma ensures that we will stay in Phase II forever.\nLemma B.7. There exists a constant \u03b30 > \u03b3 > 0 such that if \u2016W0\u20162, \u2016W\u2217\u20162 \u2264 \u03b30, d \u2265 100, \u03b7 \u2264 \u03b3 2 G22 , \u03b5 \u2264 \u03b32, then after |gt1 | \u2264 197\u03b32, Phase I ends and Phase II starts. That is, for every T > t1, \u2016WT \u20162 \u2264 \u03b3 and |gT | \u2264 0.1.\nProof for Theorem 3.2. We immediately get Theorem 3.2 by combining the above three lemmas. They show that gt will decrease to a small value in Phase I (Lemma B.5), \u2016Wt\u20162 will keep small during this process (Lemma B.6), and they all keep small afterwards (Lemma B.7).\nC Phase II: One Point Convexity In this section, we prove Theorem 3.3. See detailed proofs in Appendix H. Using Lemma A.2, it suffices to bound\n\u3008P,W\u2217 \u2212W\u3009 = d\u2211\nj=1\n\u3008P1,j + P2,j + P3,j , w\u2217j \u2212 wj\u3009\nHere the first term is easy to calculate.\nd\u2211\nj=1\n\u3008P1,j , w\u2217j \u2212 wj\u3009 = \u03c0\n2 \u2225\u2225\u2225\u2225\u2225 d\u2211\ni=1\n(w\u2217i \u2212 wi) \u2225\u2225\u2225\u2225\u2225 2\n2\n\u2265 0 (3)\nFor notational simplicity, denote\nxj , ( ej + wj \u00b7 ej + wj> ) (w\u2217j \u2212 wj),\nX , (x1, \u00b7 \u00b7 \u00b7 , xd) (4) zj , (\nI\u2212 1 2 ej + wj \u00b7 ej + wj>\n) (w\u2217j \u2212 wj) (5)\nBy Definition of P2,j and (5), we have\nd\u2211\nj=1\n\u3008P2,j , w\u2217j \u2212 wj\u3009 = d\u2211\nj=1\n\u2329 gjej + wj , w \u2217 j \u2212 wj \u232a + d\u2211\nj=1\nz>j Ajej + wj (6)\nWe bound the above two terms separately below.\nO A B\nC D\nE\nFGH\nLemma C.3. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , \u2211d j=1\u3008P3,j , w\u2217j \u2212 wj\u3009 \u2265 ( \u03c0 2 \u2212 0.021 ) \u2016W\u2217 \u2212W\u20162F .\nProof of Theorem 3.3. By (3), (6), Lemma C.1, Lemma C.2 and Lemma C.3, we know\n\u3008P,W\u2217 \u2212W\u3009 \u2265 ( \u03c0\n2 \u2212 1.321\u2212 8\u03b3 \u2212 (1 + \u03b3)g 2(1\u2212 2\u03b3)\n) \u2016W\u2217 \u2212W\u20162F > ( 0.169\u2212 (1 + \u03b3)g\n2(1\u2212 2\u03b3)\n) \u2016W\u2217 \u2212W\u20162F\nUsing Lemma A.2, we get\n\u3008\u2212\u2207L(W),W\u2217 \u2212W\u3009 > (\n0.084\u2212 (1 + \u03b3)g 2(1\u2212 2\u03b3)\n) \u2016W\u2217 \u2212W\u20162F > 0.03\u2016W\u2217 \u2212W\u20162F\nThe last inequality holds when g \u2264 0.1.\nD A Geometric Lemma In our proof, we need very tight bounds for a few terms. In order to get such bounds, we present a nice and intuitive geometric lemma as follows.\nLemma D.1. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3, then \u2200i \u2208 [d],\n1. \u2016ei + w\u2217i \u2212 ei + wi\u20162 \u2264 \u2016(I\u2212ei+wi\u00b7ei+wi>)(w\u2217i\u2212wi)\u20162\u221a 1\u22122\u03b3 \u2264 \u2016w\u2217i\u2212wi\u20162\u221a 1\u22122\u03b3\n2. \u2212\u2016w \u2217 i\u2212wi\u201622\n2(1\u22122\u03b3) \u2264 \u3008ei + w\u2217i \u2212 ei + wi, ei + wi\u3009 \u2264 0\n3. if \u03b3 \u2264 1100 ,0 \u2264 \u03b8i,i\u2217 \u2264 1.001\u2016w\u2217i \u2212 wi\u20162.\nProof. See Figure 10. Denote ei+w\u2217i as \u2212\u2212\u2192 OC, ei+wi as \u2212\u2212\u2192 OD, ei + w\u2217i as \u2212\u2192 OA, ei + wi as \u2212\u2212\u2192 OB. Thus, \u2016w\u2217i\u2212wi\u20162 = \u2016\u2212\u2212\u2192DC\u20162. 1. Since \u2212\u2212\u2192 OD\u22a5\u2212\u2212\u2192CF , we know \u2016\u2212\u2212\u2192CD\u20162 \u2265 \u2016 \u2212\u2212\u2192 CF\u20162. Since4CFO \u223c 4AEO, we know\n\u2016\u2212\u2212\u2192CD\u20162 \u2016\u2212\u2192AE\u20162 \u2265 \u2016 \u2212\u2212\u2192 CF\u20162 \u2016\u2212\u2192AE\u20162 = \u2016\u2212\u2212\u2192OC\u20162 \u2016\u2212\u2192OA\u20162 = \u2016ei + w\u2217i \u20162 \u2265 1\u2212 \u03b3 (7)\nThe last inequality holds as \u2016W\u2217\u20162 \u2264 \u03b3. Notice that \u2016\u2212\u2192OA\u20162 = \u2016 \u2212\u2212\u2192 OB\u20162 = 1, we know4ABO is a isosceles triangle. Thus, \u2016 \u2212\u2192 AG\u20162 = \u2016 \u2212\u2212\u2192 GB\u20162. Notice that4ABE \u223c 4BGO, we have\n\u2016\u2212\u2192AE\u20162 \u2016\u2212\u2212\u2192AB\u20162 = \u2016\u2212\u2212\u2192OG\u20162 \u2016\u2212\u2212\u2192OB\u20162 =\n\u221a 1\u2212 \u2016\u2212\u2212\u2192GB\u201622\n1 (8)\nWLOG, assume \u2016\u2212\u2212\u2192OC\u20162 \u2265 \u2016 \u2212\u2212\u2192 OD\u20162, as shown in the figure. We draw \u2212\u2212\u2192 HB \u2016 \u2212\u2212\u2192CD, and we know \u2016\u2212\u2212\u2192OH\u20162 \u2265\n\u2016\u2212\u2212\u2192OB\u20162 = \u2016 \u2212\u2192 OA\u20162. Since4CDO \u223c 4HBO, we have\n\u2016\u2212\u2212\u2192CD\u20162 \u2016\u2212\u2212\u2192HB\u20162 = \u2016\u2212\u2212\u2192OD\u20162 \u2016\u2212\u2212\u2192OB\u20162 = \u2016\u2212\u2212\u2192OD\u20162 \u2265 1\u2212 \u03b3\nSo \u2016\u2212\u2212\u2192CD\u20162 \u2265 (1 \u2212 \u03b3)\u2016 \u2212\u2212\u2192 HB\u20162. On the other hand, \u2220BAO < \u03c02 , and A is between H and O, so \u2220BAH > \u03c02 , which means \u2016\u2212\u2212\u2192HB\u20162 \u2265 \u2016 \u2212\u2212\u2192 AB\u20162 = 2\u2016 \u2212\u2212\u2192 GB\u20162. Thus, \u2016 \u2212\u2212\u2192 GB\u20162 \u2264 \u2016 \u2212\u2212\u2192 HB\u20162 2 \u2264 \u2016\u2212\u2212\u2192CD\u20162 2(1\u2212\u03b3) .\nSubstitute it into (8), we get\n\u2016\u2212\u2192AE\u20162 \u2016\u2212\u2212\u2192AB\u20162 \u2265\n\u221a\n1\u2212 \u2016 \u2212\u2212\u2192 CD\u201622 4(1\u2212 \u03b3)2 \u2265 \u221a 1\u2212 ( \u03b3 1\u2212 \u03b3 )2\nThe last inequality holds since \u2016\u2212\u2212\u2192CD\u20162 = \u2016w\u2217i \u2212 wi\u20162 \u2264 2\u03b3. Substitute this inequality into (7), we get\n\u2016ei + w\u2217i \u2212 ei + wi\u20162 = \u2016 \u2212\u2212\u2192 AB\u20162\n\u2264 \u2016 \u2212\u2192 AE\u20162\u221a\n1\u2212 (\n\u03b3 1\u2212\u03b3\n)2 \u2264 \u2016\u2212\u2212\u2192CF\u20162\n(1\u2212 \u03b3) \u221a 1\u2212 (\n\u03b3 1\u2212\u03b3\n)2 (9)\n\u2264 \u2016 \u2212\u2212\u2192 CD\u20162\n(1\u2212 \u03b3) \u221a 1\u2212 (\n\u03b3 1\u2212\u03b3\n)2 = \u2016w\u2217i \u2212 wi\u20162\u221a 1\u2212 2\u03b3 (10)\nNotice that ei + wi > (w\u2217i \u2212 wi) = \u2212\u2016 \u2212\u2212\u2192 DF\u20162, so ei + wi \u00b7 ei + wi>(w\u2217i \u2212 wi) = \u2212\u2212\u2192 DF . That means,\n\u2016(I\u2212 ei + wi \u00b7 ei + wi>)(w\u2217i \u2212 wi)\u20162 = \u2016 \u2212\u2212\u2192 DC \u2212\u2212\u2212\u2192DF\u20162 = \u2016 \u2212\u2212\u2192 CF\u20162\nThe lemma follows by (9) and (10). 2. By Figure 10, we know |\u3008ei + w\u2217i \u2212 ei + wi, ei + wi\u3009| = \u2016 \u2212\u2212\u2192 BE\u20162. Since4ABE \u223c 4GBO, we have\n\u2016\u2212\u2212\u2192BE\u20162 \u2016\u2212\u2212\u2192AB\u20162 = \u2016\u2212\u2212\u2192GB\u20162 \u2016\u2212\u2212\u2192BO\u20162 = \u2016\u2212\u2212\u2192AB\u20162 2\nTherefore, using (10) we get\n|\u3008ei + w\u2217i \u2212 ei + wi, ei + wi\u3009| = \u2016\u2212\u2212\u2192AB\u201622 2 \u2264 \u2016w \u2217 i \u2212 wi\u201622 2(1\u2212 2\u03b3)\nMoreover, \u3008ei + w\u2217i \u2212 ei + wi, ei + wi\u3009 =\u3008ei + w\u2217i , ei + wi\u3009 \u2212 1 \u2264 0. 3. We know that\n\u03b8i,i\u2217 = 2 arcsin \u2016 \u2212\u2192 AG\u20162 = 2 arcsin \u2016ei + w\u2217i \u2212 ei + wi\u20162 2\n\u2264 \u2016ei + w\u2217i \u2212 ei + wi\u20162 + \u2016ei + w\u2217i \u2212 ei + wi\u201632\n8\nThe last inequality holds by Taylor\u2019s Series for arcsin, and the fact \u2016ei + w\u2217i \u2212 ei + wi\u20162 = \u2016 \u2212\u2212\u2192 AB\u20162 \u2264 \u2016w\u2217i \u2212 wi\u20162 \u2264 2\u03b3 \u2264 150 . Thus, we have \u03b8i,i\u2217 \u2264 1.001\u2016w\u2217i \u2212 wi\u20162.\nE More Handy Lemmas Lemma* E.1. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3, then\n\u2022 (1\u2212\u03b3) 2 (1+\u03b3)2 I I + W > I + W (1+\u03b3) 2 (1\u2212\u03b3)2 I, (1\u2212\u03b3)2 (1+\u03b3)2 I I + W\u2217 > I + W\u2217 (1+\u03b3) 2 (1\u2212\u03b3)2 I,\n\u2022 (1\u2212 \u03b3)2I (I + W)>(I + W) (1 + \u03b3)2I, (1\u2212 \u03b3)2I (I + W\u2217)>(I + W\u2217) (1 + \u03b3)2I.\nTherefore, the singular value of I + W is at most 1+\u03b31\u2212\u03b3 and at least 1\u2212\u03b3 1+\u03b3 . The singular value of I + W is at most 1 + \u03b3 and at least 1\u2212 \u03b3. The same claims hold for I + W\u2217, I + W\u2217 respectively.\nProof. Since \u2016W\u20162 \u2264 \u03b3, we have 1 \u2212 \u03b3 \u2264 \u2016I + W\u20162 \u2264 1 + \u03b3, and 1 \u2212 \u03b3 \u2264 \u2016ei + wi\u20162 \u2264 1 + \u03b3. Therefore, I + W = \u03a3(I + W) where \u03a3 is a diagonal matrix whose entries are within [ 11+\u03b3 , 1 1\u2212\u03b3 ]. Putting into I + W > I + W, we have\nI + W > I + W = (I + W)>\u03a32(I + W) 1 (1\u2212 \u03b3)2 (I + W) >(I + W) (1 + \u03b3) 2 (1\u2212 \u03b3)2 I\nSimilarly we can show I + W > I + W (1\u2212\u03b3) 2\n(1+\u03b3)2 I. Thus we know the singular value of I + W is at most 1+\u03b3 1\u2212\u03b3 and at least 1\u2212\u03b3 1+\u03b3 . The same proof works for I + W, I + W \u2217 and I + W\u2217.\nLemma* E.2. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , we have\n|\u3008ei + w\u2217i , ej + wj\u3009| \u2264 2.1\u03b3, |\u3008ei + wi, ej + wj\u3009| \u2264 2.1\u03b3\nProof. We know\n|\u3008ei + w\u2217i , ej + wj\u3009| = |\u3008ei + w\u2217i , ej + wj\u3009| \u2016ei + w\u2217i \u20162\u2016ej + wj\u20162 \u2264 |\u3008ei + w \u2217 i , ej + wj\u3009| (1\u2212 \u03b3)2 = |w\u2217i,j |+ |wi,j |+ |\u3008wi, wj\u3009| (1\u2212 \u03b3)2 \u2264 (2 + \u03b3)\u03b3 (1\u2212 \u03b3)2 \u2264 2.1\u03b3\nwhere the last inequality holds since \u03b3 \u2264 1100 . The same analysis works for \u3008ei + wi, ej + wj\u3009.\nLemma* E.3 (Triangle inequality between ei+wi, ei+w\u2217i , w\u2217i \u2212wi). |\u2016ei+wi\u20162\u2212\u2016ei+w\u2217i \u20162| \u2264 \u2016w\u2217i \u2212wi\u20162.\nLemma* E.4. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3, |g| \u2264 2d\u03b3. Proof. By definition and Lemma E.3, we know |g| = \u2211di=1(\u2016ei + w\u2217i \u20162 \u2212 \u2016ei + wi\u20162) \u2264 \u2211d i=1 \u2016w\u2217i \u2212 wi\u20162 \u2264 2d\u03b3.\nLemma* E.5. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3, |\u3008ei + w\u2217i \u2212 ei + wi, ej + wj\u3009| \u2264 \u2016w\u2217i\u2212wi\u20162\u221a 1\u22122\u03b3 .\nProof. By Cauchy Schwartz and Lemma D.1 term 1.\nLemma* E.6. |xk \u2212 yk| \u2264 k2 |x\u2212 y|(|x|k\u22121 + |y|k\u22121). Proof. |xk \u2212 yk| = \u2223\u2223\u2223(x\u2212 y) \u2211k\u22121 t=1 xtyk\u2212t\u22121+ytxk\u2212t\u22121 2\n\u2223\u2223\u2223 \u2264 k2 |x\u2212 y|(|x|k\u22121 + |y|k\u22121), where the last inequality holds since |xtyk\u2212t\u22121 + ytxk\u2212t\u22121| \u2264 |x|t|y|k\u2212t\u22121 + |y|t|x|k\u2212t\u22121 \u2264 |x|k\u22121 + |y|k\u22121, by rearrangement inequality.\nLemma* E.7. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , for k \u2265 3, we have\n\u2016\u3008ei + w\u2217i , ej + wj\u3009k(ei + w\u2217i )\u2212 \u3008ei + wi, ej + wj\u3009k(ei + wi)\u20162 \u22646(2.2\u03b3)k\u22123 ( \u3008ei + w\u2217i , ej + wj\u30092 + \u3008ei + wi, ej + wj\u30092 ) \u2016w\u2217i \u2212 wi\u20162\nProof.\n\u2016\u3008ei + w\u2217i , ej + wj\u3009k(ei + w\u2217i )\u2212 \u3008ei + wi, ej + wj\u3009k(ei + wi)\u20162 \u2264\u2016w\u2217i \u2212 wi\u20162|\u3008ei + w\u2217i , ej + wj\u3009k|+ \u2016(\u3008ei + w\u2217i , ej + wj\u3009k \u2212 \u3008ei + wi, ej + wj\u3009k)(ei + wi)\u20162 \u2264\u2016w\u2217i \u2212 wi\u20162|\u3008ei + w\u2217i , ej + wj\u3009k|+ (1 + \u03b3)|\u3008ei + w\u2217i , ej + wj\u3009k \u2212 \u3008ei + wi, ej + wj\u3009k| \u00ac \u2264\u2016w\u2217i \u2212 wi\u20162(2.1\u03b3)k\u22122\u3008ei + w\u2217i , ej + wj\u30092\n+ (1 + \u03b3)k 2 |\u3008ei + w\u2217i \u2212 ei + wi, ej + wj\u3009|(|\u3008ei + w\u2217i , ej + wj\u3009|k\u22121 + |\u3008ei + wi, ej + wj\u3009|k\u22121)\n\u2264\u3008ei + w\u2217i , ej + wj\u30092 ( \u2016w\u2217i \u2212 wi\u20162(2.1\u03b3)k\u22122 + (1 + \u03b3)k(2.1\u03b3)k\u22123\n2 |\u3008ei + w\u2217i \u2212 ei + wi, ej + wj\u3009|\n)\n+ \u3008ei + wi, ej + wj\u30092 ( (1 + \u03b3)k(2.1\u03b3)k\u22123\n2 |\u3008ei + w\u2217i \u2212 ei + wi, ej + wj\u3009|\n)\n \u2264\u2016w\u2217i \u2212 wi\u20162 [( (2.1\u03b3)k\u22122 + 0.52k(2.1\u03b3)k\u22123 ) \u3008ei + w\u2217i , ej + wj\u30092 + 0.52k(2.1\u03b3)k\u22123\u3008ei + wi, ej + wj\u30092 ]\n\u00ae \u2264\u2016w\u2217i \u2212 wi\u20162 [ 0.55k(2.1\u03b3)k\u22123\u3008ei + w\u2217i , ej + wj\u30092 + 0.52k(2.1\u03b3)k\u22123\u3008ei + wi, ej + wj\u30092 ]\n\u00af \u22646(2.2\u03b3)k\u22123 ( \u3008ei + w\u2217i , ej + wj\u30092 + \u3008ei + wi, ej + wj\u30092 ) \u2016w\u2217i \u2212 wi\u20162\nwhere \u00ac uses Lemma E.2 and Lemma E.6,  uses Lemma E.5, \u00ae holds as \u03b3 \u2264 1100 , and \u00af holds since 0.55k(2.1)k\u22123 \u2264 6(2.2)k\u22123 for k \u2265 3.\nLemma* E.8. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , for k \u2265 2, \u2223\u2223\u2016ei + wi\u20162\u3008ei + wi, ej + wj\u30092k \u2212 \u2016ei + w\u2217i \u20162\u3008ei + w\u2217i , ej + wj\u30092k\n\u2223\u2223 \u22648(2.2\u03b3)2k\u22123 ( \u3008ei + wi, ej + wj\u30092 + \u3008ei + w\u2217i , ej + wj\u30092 ) \u2016w\u2217i \u2212 wi\u20162\nProof. \u2223\u2223\u2016ei + wi\u20162\u3008ei + wi, ej + wj\u30092k \u2212 \u2016ei + w\u2217i \u20162\u3008ei + w\u2217i , ej + wj\u30092k\n\u2223\u2223 \u2264\u2016ei + wi\u20162 \u2223\u2223\u3008ei + wi, ej + wj\u30092k \u2212 \u3008ei + w\u2217i , ej + wj\u30092k \u2223\u2223+ |\u2016ei + wi\u20162 \u2212 \u2016ei + w\u2217i \u20162| \u3008ei + w\u2217i , ej + wj\u30092k\n\u00ac \u2264\u2016ei + wi\u20162 \u2223\u2223\u3008ei + wi, ej + wj\u30092k \u2212 \u3008ei + w\u2217i , ej + wj\u30092k \u2223\u2223+ \u2016w\u2217i \u2212 wi\u20162(2.1\u03b3)2k\u22122\u3008ei + w\u2217i , ej + wj\u30092\n \u2264(1 + \u03b3)k|\u3008ei + wi \u2212 ei + w\u2217i , ej + wj\u3009| ( |\u3008ei + wi, ej + wj\u3009|2k\u22121 + |\u3008ei + w\u2217i , ej + wj\u3009|2k\u22121 )\n+ \u2016w\u2217i \u2212 wi\u20162(2.1\u03b3)2k\u22122\u3008ei + w\u2217i , ej + wj\u30092 \u00ae \u2264 [\n(1 + \u03b3)k(2.1\u03b3)2k\u22123\u221a 1\u2212 2\u03b3 \u3008ei + wi, ej + wj\u3009 2 +\n( (1 + \u03b3)k(2.1\u03b3)2k\u22123\u221a\n1\u2212 2\u03b3 + (2.1\u03b3) 2k\u22122\n) \u3008ei + w\u2217i , ej + wj\u30092 ] \u2016w\u2217i \u2212 wi\u20162\n\u00af \u22641.05k(2.1\u03b3)2k\u22123 ( \u3008ei + wi, ej + wj\u30092 + \u3008ei + w\u2217i , ej + wj\u30092 ) \u2016w\u2217i \u2212 wi\u20162\n\u00b0 \u22648(2.2\u03b3)2k\u22123 ( \u3008ei + wi, ej + wj\u30092 + \u3008ei + w\u2217i , ej + wj\u30092 ) \u2016w\u2217i \u2212 wi\u20162\nwhere \u00ac uses Lemma E.2 and Lemma E.3,  uses Lemma E.6, \u00ae uses Lemma E.5, \u00af holds as \u03b3 \u2264 1100 , and \u00b1 holds as 1.05k(2.1)2k\u22123 \u2264 8(2.2)2k\u22123 for k \u2265 2.\nLemma* E.9. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3, for fixed j \u2208 [d], \u2211\ni 6=j \u3008ei + wi, ej + wj\u30092 \u2264\n4\u03b3 (1\u2212 \u03b3)2 , \u2211\ni 6=j \u3008ei + w\u2217i , ej + wj\u30092 \u2264\n4\u03b3(1 + \u03b3)\n1\u2212 2\u03b3 .\nSimilarly, for fixed i \u2208 [d], \u2211\nj 6=i \u3008ei + wi, ej + wj\u30092 \u2264\n4\u03b3 (1\u2212 \u03b3)2 , \u2211\nj 6=i \u3008ei + w\u2217i , ej + wj\u30092 \u2264\n4\u03b3(1 + \u03b3)\n1\u2212 2\u03b3 .\nProof. By matrix multiplication,\nd\u2211\ni=1\n\u3008ei + w\u2217i , ej + wj\u30092 = d\u2211\ni=1\nej + wj > ei + w\u2217i \u00b7 ei + w\u2217i > ej + wj = ej + wj > I + W\u2217 \u00b7 I + W\u2217>ej + wj\nBy Lemma E.1, we know I + W\u2217 \u00b7 I + W\u2217> (1+\u03b3) 2 (1\u2212\u03b3)2 I. That means, \u2211d i=1\u3008ei + w\u2217i , ej + wj\u30092 \u2264 (1+\u03b3)2\n(1\u2212\u03b3)2 . On the other hand, by Lemma D.1 term 2, \u3008ej + w\u2217j , ej + wj\u30092 = (1 \u2212 \u3008ej + w\u2217j \u2212 ej + wj , ej + wj\u3009)2 \u2265 1\u2212 \u2016w\n\u2217 i\u2212wi\u201622 1\u22122\u03b3 .\nTherefore, we know\n\u2211 i6=j \u3008ei + w\u2217i , ej + wj\u30092 \u2264 (1 + \u03b3)2 (1\u2212 \u03b3)2 \u2212 1 + \u2016w\u2217i \u2212 wi\u201622 1\u2212 2\u03b3 = 4\u03b3 (1\u2212 \u03b3)2 + \u2016w\u2217i \u2212 wi\u201622 1\u2212 2\u03b3 \u2264 4\u03b3(1 + \u03b3) 1\u2212 2\u03b3\nUsing the same analysis, we get \u2211 i 6=j\u3008ei + wi, ej + wj\u30092 \u2264 (1+\u03b3)2 (1\u2212\u03b3)2 \u2212 1 = 4\u03b3\n(1\u2212\u03b3)2 . The analysis for fixed i is similar.\nLemma* E.10. For any matrix A, we have \u2016Diag(A)\u20162 \u2264 \u2016A\u20162 and \u2016Off-Diag(A)\u20162 \u2264 2\u2016A\u20162.\nProof. By definition, we know \u2016Diag(A)\u20162 = maxi\u2208[d] e>i Aei \u2264 maxv\u2208Rd v>Av = \u2016A\u20162, and \u2016Off-Diag(A)\u20162 \u2264 \u2016A\u20162 + \u2016Diag(A)\u20162 \u2264 2\u2016A\u20162.\nLemma* E.11. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3, \u2016A\u20162 \u2264 2\u03b3(\u03b3 2+3)\n1\u2212\u03b32 .\nProof. By Lemma E.1, we have\n\u2016A\u20162 = \u2016(I + W\u2217)I + W\u2217 > \u2212 (I + W)I + W>\u20162 \u2264\n(1 + \u03b3)2 1\u2212 \u03b3 \u2212 (1\u2212 \u03b3)2 1 + \u03b3 = 2\u03b3(\u03b32 + 3) 1\u2212 \u03b32 .\nLemma* E.12. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , |ej + wj > Aej + wj \u2212 e>j Aej | \u2264 5\u03b32.\nProof.\n|ej + wj>Aej + wj \u2212 e>j Aej | \u2264 |ej + wj > A(ej + wj \u2212 ej)|+ |(ej + wj \u2212 ej)>Aej | \u00ac \u2264 4\u03b3\n2(\u03b32 + 3) 1\u2212 \u03b32  < 5\u03b32\nwhere \u00ac uses Cauchy Schwartz, Lemma E.11 and \u2016ej + wj \u2212 ej\u20162 \u2264 \u03b3, and  holds as \u03b3 \u2264 1100 .\nLemma* E.13. For any i \u2208 [n], |\u2016[ei + w\u2217i \u20162 \u2212 \u2016ei + wi\u20162]\u2212 [w\u2217i,i \u2212 wi,i]| \u2264 6.07\u03b32.\nProof.\n\u2016ei + wi\u20162 \u2212 \u2016ei + w\u2217i \u20162 = \u3008ei + wi, ei + wi\u3009 \u2212 \u3008ei + w\u2217i , ei + w\u2217i \u3009 = \u3008ei + wi, ei + wi \u2212 ei + w\u2217i \u3009+ \u3008wi \u2212 w\u2217i , ei + w\u2217i \u3009 = \u3008wi \u2212 w\u2217i , ei\u3009+ \u3008ei + wi, ei + wi \u2212 ei + w\u2217i \u3009+ \u3008wi \u2212 w\u2217i , ei + w\u2217i \u2212 ei\u3009 = wi,i \u2212 w\u2217i,i + \u3008ei + wi, ei + wi \u2212 ei + w\u2217i \u3009+ \u3008wi \u2212 w\u2217i , ei + w\u2217i \u2212 ei\u3009\nAs a result,\n|[\u2016ei + wi\u20162 \u2212 \u2016ei + w\u2217i \u20162]\u2212 [wi,i \u2212 w\u2217i,i]| \u2264 ||\u3008ei + wi, ei + wi \u2212 ei + w\u2217i \u3009|+ |\u3008wi \u2212 w\u2217i , ei + w\u2217i \u2212 ei\u3009| \u00ac \u2264 (1 + \u03b3)2\u03b3 2\n1\u2212 2\u03b3 + 4\u03b3 2 \u2264 6.07\u03b32\nwhere \u00ac uses Lemma D.1 term 2 and \u2016ei + w\u2217i \u2212 ei\u20162 \u2264 2\u03b3, and Cauchy Schwartz. So the claim follows.\nCorollary E.14. |g \u2212 Tr(W\u2217 \u2212W)| \u2264 6.07d\u03b32.\nLemma* E.15. I + W is close to I on its diagonals, and close to W on its off-diagonals. More specifically, if \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 ,\n\u2016Diag(I + W)\u2212 I\u20162 \u2264 \u03b32\n2(1\u2212 \u03b3)2 , \u2016Diag(I + W \u2217)\u2212 I\u20162 \u2264\n\u03b32\n2(1\u2212 \u03b3)2\n\u2016Off-Diag(I + W \u2212W)\u20162 \u2264 4\u03b32\n1\u2212 \u03b3 , \u2016Off-Diag(I + W \u2217 \u2212W\u2217)\u20162 \u2264\n4\u03b32\n1\u2212 \u03b3 \u2016I + W \u2212 I\u20162 \u2264 2.05\u03b3, \u2016I + W\u2217 \u2212 I\u20162 \u2264 2.05\u03b3\nProof. For the diagonal terms,\n\u2016Diag(I + W)\u2212 I\u20162 = max j |I + Wj,j \u2212 1| = max j \u2223\u2223\u2223\u2223 1 + wj,j \u2212 \u2016ej + wj\u20162\n\u2016ej + wj\u20162\n\u2223\u2223\u2223\u2223\n\u2264max j \u2223\u2223\u2223\u2223 (1 + wj,j) 2 \u2212 \u2016ej + wj\u201622 \u2016ej + wj\u20162 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\n1\n1 + wj,j + \u2016ej + wj\u20162\n\u2223\u2223\u2223\u2223 \u2264 maxj \u2211 i 6=j w 2 j,i 2(1\u2212 \u03b3)2 \u2264 \u03b32 2(1\u2212 \u03b3)2\nFor the off-diagonal terms, we know I + W = (I + W)\u03a3 for some diagonal matrix \u03a3, so\n\u2016Off-Diag(I + W \u2212W)\u20162 = \u2016Off-Diag((I + W)\u03a3\u2212W)\u20162 = \u2016Off-Diag((\u03a3\u2212 I)W)\u20162 \u00ac \u2264 2\u2016(\u03a3\u2212 I)W\u20162 \u2264\n4\u03b32\n1\u2212 \u03b3\nwhere \u00ac uses Lemma E.10. For the difference between I + W and I, we split I + W into diagonal and offdiagonal parts:\n\u2016I + W \u2212 I\u20162 = \u2016Diag(I + W) + Off-Diag(I + W)\u2212 I\u20162\n=\u2016Off-Diag(W)\u20162 + \u03b32 2(1\u2212 \u03b3)2 + 4\u03b32 1\u2212 \u03b3 \u00ac \u2264 2\u2016W\u20162 + \u03b32(9\u2212 8\u03b3) 2(1\u2212 \u03b3)2 \u2264 2.05\u03b3\nwhere \u00ac uses Lemma E.10.\nLemma* E.16. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 ,\n\u2016A\u2212 [W\u2217 \u2212W + (W\u2217 \u2212W)> \u2212Diag(W\u2217 \u2212W)]\u20162 \u2264 9.2\u03b32\nProof. By definition, \u2225\u2225\u2225 [ (I + W\u2217)I + W\u2217 > \u2212 (I + W)I + W> ] \u2212 [ (W\u2217 \u2212W) + (I + W\u2217> \u2212 I + W>)\n]\u2225\u2225\u2225 2\n=\u2016W\u2217(I + W\u2217> \u2212 I)\u2212W(I + W> \u2212 I)\u20162 \u2264 \u2016W\u2217(I + W\u2217 > \u2212 I)\u20162 + \u2016W(I + W)> \u2212 I)\u20162\n\u22642.05\u03b32 + 2.05\u03b32 = 4.1\u03b32\nwhere the last inequality uses Lemma E.15. Below we further approximate I + W\u2217 > \u2212 I + W>.\n\u2225\u2225\u2225 [ I + W\u2217 > \u2212 I + W> ] \u2212 [ (W\u2217 \u2212W)> \u2212Diag(W\u2217 \u2212W) ]\u2225\u2225\u2225 2\n= \u2225\u2225\u2225Diag(I + W\u2217> \u2212 I + W>) + Off-Diag(I + W\u2217> \u2212 I + W>)\u2212 [ (W\u2217 \u2212W)> \u2212Diag(W\u2217 \u2212W) ]\u2225\u2225\u2225 2\n\u00ac \u2264\u2016Off-Diag(I + W\u2217> \u2212 I + W>)\u2212Off-Diag(W\u2217 \u2212W)>\u20162 +\n\u03b32\n(1\u2212 \u03b3)2  \u2264 4\u03b3 2\n1\u2212 \u03b3 + \u03b32 (1\u2212 \u03b3)2 \u2264 5.1\u03b3 2\nwhere \u00ac uses Lemma E.15,  uses Lemma E.15 Combining everything,\n\u2016A\u2212 [W\u2217 \u2212W + (W\u2217 \u2212W)> \u2212Diag(W\u2217 \u2212W)]\u20162 \u2264 9.2\u03b32\nUsing Lemma E.10, we immediately have the following corollary.\nCorollary E.17. \u2016Diag(A)\u2212Diag(W\u2217 \u2212W)\u20162 \u2264 9.2\u03b32.\nLemma* E.18. For \u03b7 \u2264 1\u03c0d , \u2225\u2225\u2225I\u2212 \u03b7\n(\u03c0 2 uu> + (\u03c0 2 + 1 ) I )\u2225\u2225\u2225 2 \u2264 ( 1\u2212 \u03b7 (\u03c0 2 + 1 ))\nProof. Consider another basis (e\u20321, \u00b7 \u00b7 \u00b7 , e\u2032d) where e\u20321 = u\u2016u\u20162 . For every unit vector v = (v1, \u00b7 \u00b7 \u00b7 , vd) in this new space, we know\nvT ( I\u2212 \u03b7 (\u03c0 2 uu> + (\u03c0 2 + 1 ) I )) v = \u2016v\u201622 \u2212 \u03b7 (\u03c0 2 + 1 ) \u2016v\u201622 \u2212 \u03c0\u03b7d 2 v21\nHence we get 0 \u2264 vT ( I\u2212 \u03b7 (\u03c0 2 uu> + (\u03c0 2 + 1 ) I )) v \u2264 ( 1\u2212 \u03b7 (\u03c0 2 + 1 )) \u2016v\u201622\nBy definition of matrix norm, the lemma follows.\nF Proofs for Section A\nF.1 Proof for Claim A.1 Comparing with Lemma 2.1, we know that for fixed j, P1,j is already contained in \u2212\u2207L(W)j as the first term, while P3,j is simply the summand when i = j, ignoring the first term. Below we show how to obtain P2,j from i 6= j cases. We will bound the approximation error in Lemma A.2 and Lemma A.3. \u2211\ni 6=j\n((\u03c0 2 \u2212 \u03b8i\u2217,j ) (ei + w \u2217 i )\u2212 (\u03c0 2 \u2212 \u03b8i,j ) (ei + wi) + (\u2016ei + w\u2217i \u2016 sin \u03b8i\u2217,j \u2212 \u2016ei + wi\u2016 sin \u03b8i,j) ej + wj )\n\u2248 \u2211\ni 6=j\n( \u3008ei + w\u2217i , ej + wj\u3009(ei + w\u2217i )\u2212 \u3008ei + wi, ej + wj\u3009(ei + wi) )\n+ \u2211\ni 6=j\n( \u2016ei + w\u2217i \u2016 ( 1\u2212 1\n2 \u3008ei + w\u2217i , ej + wj\u30092\n) \u2212 \u2016ei + wi\u2016 ( 1\u2212 1\n2 \u3008ei + wi, ej + wj\u30092\n)) ej + wj\n= \u2211\ni 6=j ((ei + w\n\u2217 i )ei + w \u2217 i > \u2212 (ei + wi)ei + wi>)ej + wj\n+ \u2211\ni 6=j\n( \u2016ei + w\u2217i \u2016 \u2212 \u2016ei + wi\u2016 \u2212 1\n2 ej + wj\n> ei + w\u2217i \u2016ei + w\u2217i \u2016ei + w\u2217i > ej + wj\n+ 1\n2 ej + wj\n> ei + wi\u2016ei + wi\u2016ei + wi>ej + wj ) ej + wj\n=Ajej + wj +\n \u2211\ni 6=j (\u2016ei + w\u2217i \u2016 \u2212 \u2016ei + wi\u2016)\u2212\n\u2211\ni 6=j\n1 2 ej + wj > (ei + w \u2217 i )ei + w \u2217 i > ej + wj\n+ \u2211\ni 6=j\n1 2 ej + wj > (ei + wi)ei + wi > ej + wj\n  ej + wj\n=Ajej + wj + ( gj \u2212 1\n2 ej + wj\n> Ajej + wj ) ej + wj = P2,j .\nF.2 Proof for Lemma A.2 In order to prove this lemma, we bound the approximation loss of \u03b8i,j , \u03b8i\u2217,j in Lemma F.1, and the approximation loss of sin \u03b8i,j , sin \u03b8i\u2217,j in Lemma F.2.\nLemma* F.1 (Approximation loss related to \u03b8i,j , \u03b8i\u2217,j). If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 ,\nd\u2211\nj=1\n\u2211\ni6=j\n\u2223\u2223\u2223 \u2329 ( \u03c0\n2 \u2212 \u03b8i\u2217,j \u2212 \u3008ei + w\u2217i , ej + wj\u3009)(ei + w\u2217i )\u2212 (\n\u03c0 2 \u2212 \u03b8i,j \u2212 \u3008ei + wi, ej + wj\u3009)(ei + wi), w\u2217j \u2212 wj\n\u232a\u2223\u2223\u2223\n\u22640.083\u2016W\u2217 \u2212W\u20162F\nProof. By definition, \u03c02 \u2212 \u03b8i\u2217,j = arcsin\u3008ei + w\u2217i , ej + wj\u3009, and \u03c02 \u2212 \u03b8i,j = arcsin\u3008ei + wi, ej + wj\u3009. The Taylor series of arcsinx at x = 0 is \u2211\u221e k=0 (2k)! 4k(k!)2(2k+1) x2k+1, where for k \u2265 1,\n(2k)! 4k(k!)2(2k + 1) \u2264 1 6 (11)\nThus,\nd\u2211\nj=1\n\u2211\ni6=j\n\u2223\u2223\u2223 \u2329 ( \u03c0\n2 \u2212 \u03b8i\u2217,j \u2212 \u3008ei + w\u2217i , ej + wj\u3009)(ei + w\u2217i )\u2212 (\n\u03c0 2 \u2212 \u03b8i,j \u2212 \u3008ei + wi, ej + wj\u3009)(ei + wi), w\u2217j \u2212 wj\n\u232a\u2223\u2223\u2223\n\u00ac \u2264\nd\u2211\nj=1\n\u2211\ni6=j\n\u221e\u2211\nk=1\n1\n6\n\u2223\u2223\u2329\u3008ei + w\u2217i , ej + wj\u30092k+1(ei + w\u2217i )\u2212 \u3008ei + wi, ej + wj\u30092k+1(ei + wi), w\u2217j \u2212 wj \u232a\u2223\u2223\n \u2264\nd\u2211\nj=1\n\u2211\ni6=j\n\u221e\u2211\nk=1\n1\n6\n\u2225\u2225\u3008ei + w\u2217i , ej + wj\u30092k+1(ei + w\u2217i )\u2212 \u3008ei + wi, ej + wj\u30092k+1(ei + wi) \u2225\u2225 2 \u2016w\u2217j \u2212 wj\u20162\n\u00ae \u2264\nd\u2211\nj=1\n\u2211\ni6=j\n\u221e\u2211\nk=1\n(2.2\u03b3)2k\u22122 ( \u3008ei + w\u2217i , ej + wj\u30092 + \u3008ei + wi, ej + wj\u30092 ) \u2016w\u2217i \u2212 wi\u20162\u2016w\u2217j \u2212 wj\u20162\n\u00af \u2264\nd\u2211\nj=1\n\u2211 i6=j 1.01 ( \u3008ei + w\u2217i , ej + wj\u30092 + \u3008ei + wi, ej + wj\u30092 ) \u2016w\u2217i \u2212 wi\u20162\u2016w\u2217j \u2212 wj\u20162\n\u00b0 \u22641.01   d\u2211\nj=1\n\u2211\ni 6=j\n( \u3008ei + w\u2217i , ej + wj\u30092 + \u3008ei + wi, ej + wj\u30092 ) \u2016w\u2217i \u2212 wi\u201622\n  1 2\n  d\u2211\nj=1\n\u2211\ni 6=j\n( \u3008ei + w\u2217i , ej + wj\u30092 + \u3008ei + wi, ej + wj\u30092 ) \u2016w\u2217j \u2212 wj\u201622\n  1 2\n\u22641.01\n  d\u2211\ni=1\n\u2016w\u2217i \u2212 wi\u201622\n \u2211\ni 6=j\n( \u3008ei + w\u2217i , ej + wj\u30092 + \u3008ei + wi, ej + wj\u30092\n)     1 2\n  d\u2211\nj=1\n\u2016w\u2217j \u2212 wj\u201622\n \u2211\ni 6=j\n( \u3008ei + w\u2217i , ej + wj\u30092 + \u3008ei + wi, ej + wj\u30092\n)     1 2\n\u00b1 \u22641.01\n( 4\u03b3\n(1\u2212 \u03b3)2 + 4\u03b3(1 + \u03b3) 1\u2212 2\u03b3\n) \u2016W\u2217 \u2212W\u20162F \u00b2 \u2264 0.083\u2016W\u2217 \u2212W\u20162F\nwhere \u00ac is by Taylor series,  uses Cauchy Schwartz, \u00ae uses Lemma E.7, \u00af holds as \u03b3 \u2264 1100 , \u00b0 uses Cauchy Schwartz, \u00b1 uses Lemma E.9, \u00b2 holds as \u03b3 \u2264 1100 .\nLemma* F.2 (Approximation loss related to sin \u03b8i,j , sin \u03b8i\u2217,j). If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 ,\nd\u2211\nj=1\n\u2211\ni 6=j\n\u2223\u2223\u2223\u2223 ( \u2016ei + w\u2217i \u20162 ( sin \u03b8i\u2217,j \u2212 1 + 1\n2 \u3008ei + w\u2217i , ej + wj\u30092\n) \u2212\n\u2016ei + wi\u20162 ( sin \u03b8i,j \u2212 1 + 1\n2 \u3008ei + wi, ej + wj\u30092\n)) \u3008ej + wj , w\u2217j \u2212 wj\u3009 \u2223\u2223\u2223\u2223 \u2264 0.002\u2016W\u2217 \u2212W\u20162F\nProof. By definition, we know \u03b8i\u2217,j = arccos\u3008ei + w\u2217i , ej + wj\u3009, and \u03b8i,j = arccos\u3008ei + wi, ej + wj\u3009. The Taylor series of sin(arccosx) at x = 0 is 1\u2212 x22 \u2212 x 4 8 \u2212 x 6 16 \u2212 5x 8 128 \u2212 \u00b7 \u00b7 \u00b7 = \u2211\u221e k=0 ckx 2k, where ck \u2264 18 for k \u2265 2.\nThus,\nd\u2211\nj=1\n\u2211\ni 6=j\n\u2223\u2223\u2223\u2223 ( \u2016ei + w\u2217i \u20162 ( sin \u03b8i\u2217,j \u2212 1 + 1\n2 \u3008ei + w\u2217i , ej + wj\u30092\n) \u2212\n\u2016ei + wi\u20162 ( sin \u03b8i,j \u2212 1 + 1\n2 \u3008ei + wi, ej + wj\u30092\n)) \u3008ej + wj , w\u2217j \u2212 wj\u3009 \u2223\u2223\u2223\u2223\n\u00ac \u2264\nd\u2211\nj=1\n\u2211\ni 6=j\n\u2223\u2223\u2223\u2223\u2223 \u221e\u2211\nk=2\n1\n8\n( \u2016ei + wi\u20162\u3008ei + wi, ej + wj\u30092k \u2212 \u2016ei + w\u2217i \u20162\u3008ei + w\u2217i , ej + wj\u30092k ) \u2223\u2223\u2223\u2223\u2223 \u2016w \u2217 j \u2212 wj\u20162\n \u2264\nd\u2211\nj=1\n\u2211\ni 6=j\n\u221e\u2211\nk=2\n(2.2\u03b3)2k\u22123 ( \u3008ei + wi, ej + wj\u30092 + \u3008ei + w\u2217i , ej + wj\u30092 ) \u2016w\u2217i \u2212 wi\u20162\u2016w\u2217j \u2212 wj\u20162\n\u00ae \u22642.3\u03b3   d\u2211\nj=1\n\u2211\ni 6=j\n( \u3008ei + wi, ej + wj\u30092 + \u3008ei + w\u2217i , ej + wj\u30092 ) \u2016w\u2217i \u2212 wi\u201622\n  1 2\n  d\u2211\nj=1\n\u2211\ni 6=j\n( \u3008ei + wi, ej + wj\u30092 + \u3008ei + w\u2217i , ej + wj\u30092 ) \u2016w\u2217j \u2212 wj\u201622\n  1 2\n\u22642.3\u03b3\n  d\u2211\ni=1\n\u2016w\u2217i \u2212 wi\u201622\n \u2211\nj 6=i\n( \u3008ei + wi, ej + wj\u30092 + \u3008ei + w\u2217i , ej + wj\u30092\n)     1 2\n  d\u2211\nj=1\n\u2016w\u2217j \u2212 wj\u201622\n \u2211\ni 6=j\n( \u3008ei + wi, ej + wj\u30092 + \u3008ei + w\u2217i , ej + wj\u30092\n)     1 2\n\u00af \u22642.3\u03b3\n( 4\u03b3\n(1\u2212 \u03b3)2 + 4\u03b3(1 + \u03b3) 1\u2212 2\u03b3\n) \u2016W\u2217 \u2212W\u20162F \u00b0 < 0.002\u2016W\u2217 \u2212W\u20162F\nwhere \u00ac is by Taylor series,  uses Lemma E.8 and Cauchy Schwartz, \u00ae uses Cauchy Schwartz and \u03b3 \u2264 1100 , \u00af uses Lemma E.9, and \u00b0 holds as \u03b3 \u2264 1100 .\nProof for Lemma A.2. Combining the results from Lemma F.1 and Lemma F.2, the lemma follows.\nF.3 Proof for Lemma A.3 Denote \u2206 , P + \u2207L(W). This lemma is harder to prove than the previous one since we need to bound the spectral norm of a matrix \u2206. First of all, we need to represent \u2206. Again, the difference has two parts: approximation for \u03b8i,j , \u03b8i\u2217,j , and sin \u03b8i,j , sin \u03b8i\u2217,j . Denote the two parts as \u22061,\u22062, where \u2206 = \u22061 + \u22062. From the proof of Lemma F.1, we know the j-th column of the first part is\n\u22061,j , \u2211\ni 6=j\n\u221e\u2211\nk=1\n(2k)!\n4k(k!)2(2k + 1)\n( \u3008ei + w\u2217i , ej + wj\u30092k+1(ei + w\u2217i )\u2212 \u3008ei + wi, ej + wj\u30092k+1(ei + wi) )\nAnd the j-th column of the second part is\n\u22062,j , \u2211\ni6=j\n\u221e\u2211\nk=2\nck ( \u2016ei + wi\u20162\u3008ei + wi, ej + wj\u30092k \u2212 \u2016ei + w\u2217i \u20162\u3008ei + w\u2217i , ej + wj\u30092k ) ej + wj\nBelow we bound \u2016\u22061\u20162 in Lemma F.3, and bounds \u2016\u22062\u20162 in Lemma F.4. Lemma* F.3. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , \u2016\u22061\u20162 \u2264 3.4\u03b32.\nProof. Define U,V such that for i = j,Ui,j = Vi,j = 0, and for i 6= j,\nUi,j =\n\u221e\u2211\nk=1\n(2k)!\n4k(k!)2(2k + 1) \u3008ei + w\u2217i , ej + wj\u30092k+1,Vi,j =\n\u221e\u2211\nk=1\n(2k)!\n4k(k!)2(2k + 1) \u3008ei + wi, ej + wj\u30092k+1\nBy matrix multiplication,\n\u22061 =\nd\u2211\ni=1\n[(I + W\u2217)\u2217,iUi,\u2217 \u2212 (I + W)\u2217,iVi,\u2217] = (I + W\u2217)U\u2212 (I + W)V (12)\nSo it suffices to bound \u2016U\u20162, \u2016V\u20162. For i 6= j,\n|Ui,j | = \u2223\u2223\u2223\u2223\u2223 \u221e\u2211\nk=1\n(2k)!\n4k(k!)2(2k + 1) \u3008ei + w\u2217i , ej + wj\u30092k+1 \u2223\u2223\u2223\u2223\u2223 \u00ac \u2264 \u221e\u2211\nk=1\n(2.1\u03b3)2k\u22121\n6 \u3008ei + w\u2217i , ej + wj\u30092 \u2264 0.4\u03b3\u3008ei + w\u2217i , ej + wj\u30092\nwhere \u00ac uses Lemma E.2 and (11). Now, we know\n\u2016U\u20161 \u00ac= max j\nd\u2211\ni=1\n|Ui,j | \u2264 max j\n\u2211 i 6=j 0.4\u03b3\u3008ei + w\u2217i , ej + wj\u30092  \u2264 1.6(1 + \u03b3)\u03b3 2 1\u2212 2\u03b3 \u2264 1.65\u03b3 2\nwhere \u00ac is by definition,  uses Lemma E.9. Similarly,\n\u2016U\u2016\u221e = max i\nd\u2211\nj=1\n|Ui,j | \u2264 max i\n\u2211 j 6=i 0.4\u03b3\u3008ei + w\u2217i , ej + wj\u30092 \u2264 1.65\u03b32\nBy H\u00f6lder\u2019s inequality, we have \u2016U\u20162 \u2264 \u221a \u2016U\u20161\u2016U\u2016\u221e \u2264 1.65\u03b32\nNow we do the same analysis for V.\n|Vi,j | = \u2223\u2223\u2223\u2223\u2223 \u221e\u2211\nk=1\n(2k)!\n4k(k!)2(2k + 1) \u3008ei + wi, ej + wj\u30092k+1 \u2223\u2223\u2223\u2223\u2223\n\u2264 \u221e\u2211\nk=1\n(2.1\u03b3)2k\u22121\n6 \u3008ei + wi, ej + wj\u30092 \u2264 0.4\u03b3\u3008ei + wi, ej + wj\u30092\nHence, \u2016V\u20161 = maxj \u2211d i=1 |Vi,j | \u2264 maxj \u2211 i 6=j 0.4\u03b3\u3008ei + wi, ej + wj\u30092 \u2264 1.65\u03b32. Similarly, \u2016V\u2016\u221e \u2264\n1.65\u03b32, and by H\u00f6lder\u2019s inequality, \u2016V\u20162 \u2264 \u221a \u2016V\u20161\u2016V\u2016\u221e \u2264 1.65\u03b32. Using (12), we get\n\u2016\u22061\u20162 \u2264 \u2016I + W\u2217\u20162\u2016U\u20162 + \u2016I + W\u20162\u2016V\u20162 \u2264 2(1 + \u03b3)1.65\u03b32 < 3.4\u03b32\nLemma* F.4. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , \u2016\u22062\u20162 \u2264 6\u03b33.\nProof. By definition, we can write\n\u22062 = I + WDiag    \u2211\ni 6=j\n\u221e\u2211\nk=2\nck ( \u2016ei + wi\u20162\u3008ei + wi, ej + wj\u30092k \u2212 \u2016ei + w\u2217i \u20162\u3008ei + w\u2217i , ej + wj\u30092k )    d\nj=1\nSo it suffices to bound the norm of the diagonal matrix, which is the maximum of the diagonal entries. For any j \u2208 [d], we have\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2211\ni 6=j\n\u221e\u2211\nk=2\nck ( \u2016ei + wi\u20162\u3008ei + wi, ej + wj\u30092k \u2212 \u2016ei + w\u2217i \u20162\u3008ei + w\u2217i , ej + wj\u30092k ) \u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 \u2211\ni 6=j\n\u221e\u2211\nk=2\n1\n8\n( \u2016ei + wi\u20162\u3008ei + wi, ej + wj\u30092k|+ |\u2016ei + w\u2217i \u20162\u3008ei + w\u2217i , ej + wj\u30092k )\n\u00ac \u2264 \u2211\ni 6=j\n\u221e\u2211\nk=2\n1 4 (1 + \u03b3)(2.1\u03b3)2k\u22122\n( \u3008ei + wi, ej + wj\u30092 + \u3008ei + w\u2217i , ej + wj\u30092 )\n \u22640.6\u03b32\n\u2211\ni 6=j\n( \u3008ei + wi, ej + wj\u30092 + \u3008ei + w\u2217i , ej + wj\u30092 )\n\u00ae \u22640.6\u03b32\n( 4\u03b3\n(1\u2212 \u03b3)2 + 4\u03b3(1 + \u03b3) 1\u2212 2\u03b3\n) < 5\u03b33\nwhere \u00ac uses Lemma E.2,  uses \u03b3 \u2264 1100 , \u00ae uses Lemma E.9. So we get \u2016\u22062\u20162 \u2264 1+\u03b3 1\u2212\u03b3 5\u03b3 3 \u2264 6\u03b33.\nProof for Lemma A.3. Combining the results from Lemma F.3 and Lemma F.4, the lemma follows.\nG Proofs for Section B\nG.1 Proof for Lemma B.1 In Lemma A.3, we use P(W) to approximate \u2212\u2207L(W) in terms of spectral norm, with approximation loss 3.5\u03b32. Below we will get Q(W) from P(W) by removing a few more lower order terms. By definition 2.3, we have P2,j =gej + wj \u2212 (\u2016ej + w\u2217j \u20162 \u2212 \u2016ej + wj\u20162)ej + wj + (\nI\u2212 1 2 ej + wj \u00b7 ej + wj>\n) Aej + wj\n+ ( I\u2212 1\n2 ej + wj \u00b7 ej + wj>\n) (ej + wj)\u2212 ( I\u2212 1\n2 ej + wj \u00b7 ej + wj>\n) (ej + w \u2217 j )ej + w \u2217 j > ej + wj\n=gej + wj \u2212 (\u2016ej + w\u2217j \u20162 \u2212 \u2016ej + wj\u20162)ej + wj + (\nI\u2212 1 2 ej + wj \u00b7 ej + wj>\n) Aej + wj\n+ 1 2 (ej + wj)\u2212 (ej + w\u2217j )ej + w\u2217j > ej + wj + 1 2 ej + wj\u2016ej + w\u2217j \u20162(ej + w\u2217j > ej + wj) 2\n=gej + wj +\n( I\u2212 1\n2 ej + wj \u00b7 ej + wj>\n) Aej + wj + 3\n2 (ej + wj)\u2212 ej + w\u2217j\n> ej + wj(ej + w \u2217 j )\n+\n( 1\n2 \u2016ej + w\u2217j \u20162(ej + w\u2217j > ej + wj)\n2 \u2212 \u2016ej + w\u2217j \u20162 ) ej + wj\n=gej + wj +\n( I\u2212 1\n2 ej + wj \u00b7 ej + wj>\n) Aej + wj \u2212 w\u2217j + wj + (1\u2212 ej + w\u2217j > ej + wj)(ej + w \u2217 j )\n+\n( 1\n2 \u2016ej + wj\u20162 +\n1 2 \u2016ej + w\u2217j \u20162(ej + w\u2217j > ej + wj)\n2 \u2212 \u2016ej + w\u2217j \u20162 ) ej + wj\nCombining every column together, we get\nP2 = gI + W+AI + W\u2212 1 2 I + WDiag({ej + wj>Aej + wj}dj=1)\u2212(W\u2217\u2212W)+I + W\u2217\u03a31+I + W\u03a32\nwhere\n\u03a31 = Diag({(\u2016ej + w\u2217j \u20162 \u2212 \u2016ej + w\u2217j \u20162ej + w\u2217j > ej + wj)}dj=1)\n\u03a32 = Diag({ 1\n2 \u2016ej + wj\u20162 +\n1 2 \u2016ej + w\u2217j \u20162(ej + w\u2217j > ej + wj) 2 \u2212 \u2016ej + w\u2217j \u20162}dj=1)\nUsing Lemma E.12, we replace ej + wj > Aej + wj with e>j Aej . By Lemma E.1, \u2225\u2225\u2225\u2225P2 \u2212 [ gI + W + AI + W \u2212 1\n2 I + WDiag(A)\u2212 (W\u2217 \u2212W) + I + W\u2217\u03a31 + I + W\u03a32 ]\u2225\u2225\u2225\u2225 2 \u2264 5(1 + \u03b3) 2(1\u2212 \u03b3) < 2.6\u03b3 2\nWe then focus on the middle two summands in the sum.\nAI + W \u2212 1 2 I + WDiag(A) = (A\u2212 1 2 Diag(A)) + A(I + W \u2212 I)\u2212 1 2 (I + W \u2212 I)Diag(A)\nBy Lemma E.10, \u2016Diag(A)\u20162 \u2264 \u2016A\u20162, so \u2225\u2225\u2225\u2225 [ AI + W \u2212 1\n2 I + WDiag(A)\n] \u2212 [ A\u2212 1\n2 Diag(A) ]\u2225\u2225\u2225\u2225 2 = \u2225\u2225\u2225\u2225A(I + W \u2212 I)\u2212 1 2 (I + W \u2212 I)Diag(A) \u2225\u2225\u2225\u2225 2\n\u2264\u2016A\u20162\u2016I + W \u2212 I\u20162 + 1\n2 \u2016I + W \u2212 I\u20162\u2016Diag(A)\u20162\n\u00ac \u2264 3\u03b3(\u03b3\n2 + 3)\n1\u2212 \u03b32 2.05\u03b3 < 18.5\u03b3 2\nwhere \u00ac uses Lemma E.11 and Lemma E.15. Moreover, by Lemma D.1 term 2, we know \u2016\u03a31\u20162 \u2264 maxi\u2208[d](1 + \u03b3)\u2016w \u2217 i\u2212wi\u201622\n2(1\u22122\u03b3) \u2264 2.07\u03b32, and in \u03a32, \u2223\u2223\u2223\u2223 1\n2 \u2016ej + w\u2217j \u20162(ej + w\u2217j > ej + wj) 2 \u2212 1 2 \u2016ej + w\u2217j \u20162 \u2223\u2223\u2223\u2223 \u2264 1 2 (1 + \u03b3) \u2223\u2223\u2223ej + w\u2217j > ej + wj \u2212 1 \u2223\u2223\u2223 \u2223\u2223\u2223ej + w\u2217j > ej + wj + 1 \u2223\u2223\u2223 \u2264 2.07\u03b32\nso the following terms approximates P2 with approximation loss (2.6 + 18.5 + 2.07 + 2.07)\u03b32 < 25.3\u03b32.\nI + W(gI\u2212\u03a33) + A\u2212 1 2 Diag(A)\u2212 (W\u2217 \u2212W)\nwhere \u03a33 = Diag({ 12\u2016ej + w\u2217j \u20162 \u2212 12\u2016ej + wj\u20162}dj=1). By Lemma E.16 and Corollary E.17, we know \u2016A\u2212 [W\u2217\u2212W+(W\u2217\u2212W)>\u2212Diag(W\u2217\u2212W)]\u20162 \u2264 9.2\u03b32 and \u2016Diag(A)\u2212Diag(W\u2217 \u2212W)\u20162 \u2264 9.2\u03b32. Therefore, with approximation loss of 18.4\u03b32, we get \u2225\u2225\u2225\u2225 [ A\u2212 1\n2 Diag(A)\n] \u2212 [ W\u2217 \u2212W + (W\u2217 \u2212W)> \u2212 3\n2 Diag(W\u2217 \u2212W) ]\u2225\u2225\u2225\u2225 2 \u2264 18.4\u03b32\nWe then approximate \u03a33:\n\u2016(I + W)\u03a33 \u2212 (I + W) 1 2 Diag(W\u2217 \u2212W)\u20162 \u2264 1 + \u03b3 1\u2212 \u03b3\n( 1\n2 max j |\u2016ej + w\u2217j \u20162 \u2212 \u2016ej + wj\u20162 \u2212 w\u2217j,j + wj,j |\n) < 3.1\u03b32\nwhere the last inequality is by Lemma E.13. Moreover,\n\u2016I + W ( 1\n2 Diag(W\u2217 \u2212W)\n) \u2212 1\n2 Diag(W\u2217 \u2212W)\u20162\n\u2264\u2016I + W \u2212 I\u20162 \u2225\u2225\u2225\u2225 1\n2 Diag(W\u2217 \u2212W) \u2225\u2225\u2225\u2225 2 < 2.05\u03b3 ( 1 2 max i |w\u2217i,i \u2212 wi,i| ) < 2.05\u03b32\nPutting everything together, with approximation loss of (25.3 + 18.4 + 3.1 + 2.05)\u03b32 = 49\u03b32 to P2, we get\n(W\u2217 \u2212W)> \u2212 2Diag(W\u2217 \u2212W) + gI + W\nFor P3, using the same idea in the proof of Lemma C.3, we have\nP3 = \u03c0 2 (W\u2217 \u2212W) +\n( I + W \u2212 I + W\u2217 ) \u03a34 + I + W\u03a35\nwhere \u03a34 = Diag({\u03b8j,j\u2217\u2016ej + w\u2217j \u20162}dj=1),\u03a35 = Diag({\u2016ej + w\u2217j \u20162 sin \u03b8j,j\u2217 \u2212 \u03b8j,j\u2217\u2016ej + w\u2217j \u20162}dj=1). By Taylor\u2019s Theorem, we know \u2016\u03a35\u20162 \u2264 \u2016Diag({\u2016ej + w\u2217j \u20162\u03b83j,j\u2217/3}dj=1)\u20162.\nNotice that \u03b8j,j\u2217 \u2264 2.002\u03b3 by Lemma D.1 term 3, and \u2016I + W \u2212 I + W\u2217\u20162 \u2264 1+\u03b31\u2212\u03b3 \u2212 1\u2212\u03b3 1+\u03b3 \u2264 4.001\u03b3. Consequently, \u2225\u2225\u2225P3 \u2212 \u03c0\n2 (W\u2217 \u2212W) \u2225\u2225\u2225 2 \u2264 \u2016 ( I + W \u2212 I + W\u2217 ) \u03a34\u20162 + \u2016I + W\u03a35\u20162\n<4.001 \u2217 2.002(1 + \u03b3)\u03b32 + (1 + \u03b3) 2\n3(1\u2212 \u03b3) (2.002\u03b3) 3 < 8.1\u03b32 + 2.8\u03b33 < 8.2\u03b32\nwe only need to keep the term \u03c02 (W \u2217 \u2212W) with approximation loss 8.2\u03b32 to P3.\nNow, combining the approximations to P2 and P3, and Lemma A.3, we have the following matrix with (49 + 8.2 + 3.5)\u03b32 < 61\u03b32 approximation loss to \u2212\u2207L(W):\n\u03c0 2 (W\u2217 \u2212W)\n( I + uu> ) + (W\u2217 \u2212W)> \u2212 2Diag(W\u2217 \u2212W) + gI + W\nwhere u is the all 1 vector.\nG.2 Proof for Lemma B.2 By Lemma E.4, we know |g| \u2264 2d\u03b3. Using Lemma B.1,\n\u2016\u2207L(W)\u20162 \u2264 61\u03b32 + \u2225\u2225\u2225\u03c0\n2 (W\u2217 \u2212W)\n( I + uu> ) + (W\u2217 \u2212W)> \u2212 2Diag(W\u2217 \u2212W) + gI + W \u2225\u2225\u2225 2\n\u226461\u03b32 + (d+ 1)\u03c0\u03b3 + 2\u03b3 + 4\u03b3 + |g|1 + \u03b3 1\u2212 \u03b3 < 61\u03b3 2 + (d+ 3)\u03c0\u03b3 + 2.05d\u03b3 < 6d\u03b3.\nG.3 Proof for Lemma B.3 In this proof, we use wj to represent the j-th column of Wt, and denote4wj as the j-th column of Gt. By definition we know \u2016Gt\u20162 = \u2016\u2207L(Wt) + Et\u20162 \u00ac \u2264 \u2016\u2207L(Wt)\u20162 + \u2016Et\u20162  \u2264 6d\u03b3 + \u03b5 = G2, where \u00ac uses triangle inequality,  uses Lemma B.2. We have\n\u03b7\u20164wj\u20162 \u2264 \u03b7\u2016Gt\u20162 \u2264 \u03b32 G2 \u2264 \u03b3 6d , \u03b72\u20164wj\u20162 \u2264 \u03b7\u2016Gt\u201622 \u2264 \u03b32 (13)\nBy Definition 2.2, we know\n4gt , gt+1 \u2212 gt = d\u2211\nj=1\n( \u3008ej + wj , ej + wj\u3009 \u2016ej + wj\u20162 \u2212 \u3008ej + wj \u2212 \u03b74wj , ej + wj \u2212 \u03b74wj\u3009\u2016ej + wj \u2212 \u03b74wj\u20162 )\n= d\u2211\nj=1\n( \u3008ej + wj , ej + wj\u3009\u2016ej + wj \u2212 \u03b74wj\u20162 \u2212 \u3008ej + wj \u2212 \u03b74wj , ej + wj \u2212 \u03b74wj\u3009\u2016ej + wj\u20162 \u2016ej + wj\u20162\u2016ej + wj \u2212 \u03b74wj\u20162 )\n= d\u2211\nj=1\n(\u2016ej + wj\u20162(\u2016ej + wj \u2212 \u03b74wj\u20162 \u2212 \u2016ej + wj\u20162) + 2\u03b7\u30084wj , ej + wj\u3009 \u2212 \u03b72\u20164wj\u201622 \u2016ej + wj \u2212 \u03b74wj\u20162 )\nIf we project \u03b74wj onto the ej + wj direction, we get\n\u2016ej + wj \u2212 \u03b74wj\u20162 = \u221a (\u2016ej + wj\u20162 \u2212 \u3008ej + wj , \u03b74wj\u3009)2 + (\u2016\u03b74j\u201622 \u2212 \u3008ej + wj , \u03b74wj\u30092)2\n\u2264 \u221a (\u2016ej + wj\u20162 \u2212 \u3008ej + wj , \u03b74wj\u3009)2 + \u2016\u03b74wj\u201622 \u00ac \u2264 \u2016ej + wj\u20162 \u2212 \u3008ej + wj , \u03b74wj\u3009+ \u2016\u03b74wj\u201622\nUsing (13), we have \u2016ej + wj\u20162 \u2212 \u3008ej + wj , \u03b74wj\u3009 \u2265 12 . By taking square on both sides, we know \u00ac holds. It is trivial to show that \u2016ej + wj \u2212 \u03b74wj\u20162 \u2265 \u2016ej + wj\u20162 \u2212 \u3008ej + wj , \u03b74wj\u3009, so we know\n\u2212\u3008ej + wj , \u03b74wj\u3009 \u2264 \u2016ej + wj \u2212 \u03b74wj\u20162 \u2212 \u2016ej + wj\u20162 \u2264 \u2212\u3008ej + wj , \u03b74wj\u3009+ \u2016\u03b74wj\u201622 (14)\nThus, with approximation loss \u2211d j=1 \u2016ej+wj\u20162\u2016\u03b74wj\u201622 \u2016ej+wj\u2212\u03b74wj\u20162 , we have :\n4gt \u2248 d\u2211\nj=1\n(\u2212\u2016ej + wj\u20162\u3008ej + wj , \u03b74wj\u3009+ 2\u03b7\u30084wj , ej + wj\u3009 \u2212 \u03b72\u20164wj\u201622 \u2016ej + wj \u2212 \u03b74wj\u20162 )\n= d\u2211\nj=1\n\u03b7\u30084wj , ej + wj\u3009 \u2212 \u03b72\u20164wj\u201622 \u2016ej + wj \u2212 \u03b74wj\u20162\n=\nd\u2211\nj=1\n\u2212\u03b72\u20164wj\u201622 \u2016ej + wj \u2212 \u03b74wj\u20162 +\nd\u2211\nj=1\n(\u2016ej + wj\u20162 \u2212 \u2016ej + wj \u2212 \u03b74wj\u20162)\u03b7\u30084wj , ej + wj\u3009 \u2016ej + wj \u2212 \u03b74wj\u20162 + \u03b7\u3008Gt, I + Wt\u3009\nThus we get the following approximation for4gt.\n|4gt \u2212 \u03b7\u3008Gt, I + Wt\u3009|\n\u2264 d\u2211\nj=1\n[ \u2212\u03b72\u20164wj\u201622 \u2016ej + wj \u2212 \u03b74wj\u20162 + (\u2016ej + wj\u20162 \u2212 \u2016ej + wj \u2212 \u03b74wj\u20162)\u03b7\u30084wj , ej + wj\u3009 \u2016ej + wj \u2212 \u03b74wj\u20162 + \u2016ej + wj\u20162\u2016\u03b74wj\u201622 \u2016ej + wj \u2212 \u03b74wj\u20162 ]\n\u00ac \u2264\nd\u2211\nj=1\n[ \u03b7\u30084wj , ej + wj\u3009(\u03b7\u30084wj , ej + wj\u3009+ \u2016\u03b74wj\u201622)\n\u2016ej + wj \u2212 \u03b74wj\u20162 + 0.02\u03b72\u20164wj\u201622\n]\n \u2264\nd\u2211\nj=1\n[ \u03b72\u20164wj\u201622 + \u03b73\u20164wj\u201632 \u2016ej + wj \u2212 \u03b74wj\u20162 + 0.02\u03b7\u03b32 ] \u00ae \u2264 1.04\u03b7d\u03b32\nwhere \u00ac uses (14) again, and  \u00ae uses (13), \u03b3 \u2264 1100 and \u2016ej + wj \u2212 \u03b74wj\u20162 \u2264 0.98. Thus |4gt \u2212 \u03b7\u3008\u2207L(Wt), I + Wt\u3009| \u2264 1.04\u03b7d\u03b32 + |\u03b7\u3008Et, I + Wt\u3009| < 1.04\u03b7d\u03b32 + 1.03\u03b7 \u221a d\u03b5\nWe want to approximate I + Wt with I. Below is the error bound.\n|\u3008\u2207L(Wt), I + Wt \u2212 I\u3009| = |\u3008\u2207L(Wt) + Qt \u2212Qt, I + Wt \u2212 I\u3009|\n\u00ac =d \u00b7 61\u03b32 \u00b7 2.05\u03b3 +\nd\u2211\ni=1\n2.05\u03b3 \u2225\u2225\u2225(Qt \u2212 \u03c0\n2 (W\u2217 \u2212Wt)uu>)i \u2225\u2225\u2225 2 + \u2329\u03c0 2 (W\u2217 \u2212Wt)uu>, I + Wt \u2212 I \u232a\n \u22641.251d\u03b32 + 2.05d\u03b3 ( \u03c0\u03b3 + 2\u03b3 + 4\u03b3 + 1 + \u03b3 1\u2212 \u03b3 |gt| ) + Tr ([\u03c0 2 (W\u2217 \u2212Wt)u ] [ u>I + Wt \u2212 I ]>)\n\u00ae \u226420d\u03b32 + 2.1d\u03b3|gt|+ \u2225\u2225\u2225\u03c0 2 (W\u2217 \u2212Wt)u \u2225\u2225\u2225 2 \u2225\u2225(I + Wt \u2212 I)u \u2225\u2225 2 \u00af \u2264 20d\u03b32 + 2.1d\u03b3|gt|+ 2.05\u03c0 2 \u2016s\u20162\u03b3 \u221a d\nwhere \u00ac uses Cauchy Schwartz and Lemma E.15,  uses the definition of Q and Lemma E.1, \u00ae holds as for any vector u, v, Tr(uv>) \u2264 \u2016u\u20162\u2016v\u20162, \u00af uses Lemma E.15.\nHence,\n|4gt \u2212 \u03b7\u3008\u2207L(Wt), I\u3009| \u22641.04\u03b7d\u03b32 + 1.03\u03b7 \u221a d\u03b5+ |\u03b7\u3008\u2207L(Wt), I + Wt \u2212 I\u3009| <1.04\u03b7d\u03b32 + 1.03\u03b7 \u221a d\u03b5+ 20\u03b7d\u03b32 + 2.1\u03b7d\u03b3|gt|+ 2.05\u03c0\n2 \u03b7\u2016s\u20162\u03b3\n\u221a d\n<21.1\u03b7d\u03b32 + 1.03\u03b7 \u221a d\u03b5+ 2.1\u03b7d\u03b3|gt|+ 2.05\u03c0\n2 \u03b7\u2016s\u20162\u03b3\n\u221a d\nSo with approximation loss of 21.1\u03b7d\u03b32 + 1.03\u03b7 \u221a d\u03b5+ 2.1\u03b7d\u03b3|gt|+ 2.05\u03c02 \u03b7\u2016s\u20162\u03b3 \u221a d, it suffices to consider \u03b7Tr(\u2207L(Wt)). On the other hand, according to Lemma B.1, with approximation loss of 61\u03b32, we can use \u2212Qt to approximate\u2207L(Wt).\nTr(Qt) = \u03c0 2 Tr ( (W\u2217 \u2212Wt) ( I + uu> )) + Tr(W\u2217 \u2212Wt)> \u2212 2Tr(Diag(W\u2217 \u2212Wt)) + gTr(I + Wt)\n= (\u03c0 2 \u2212 1 ) Tr(W\u2217 \u2212Wt) + \u03c0 2 Tr ( (W\u2217 \u2212Wt) ( uu> )) + gTr(I + Wt) = (\u03c0\n2 \u2212 1 ) (Tr(W\u2217 \u2212Wt)\u2212 gt) + (\u03c0 2 \u2212 1 ) gt + \u03c0 2 Tr ( (W\u2217 \u2212Wt) ( uu> )) + gtTr(I + Wt)\nTherefore, \u2223\u2223\u2223Tr(Qt)\u2212 gtTr(I)\u2212\n(\u03c0 2 \u2212 1 ) gt \u2223\u2223\u2223 = \u2223\u2223\u2223Tr(Qt)\u2212 ( d+ \u03c0 2 \u2212 1 ) gt \u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223 (\u03c0 2 \u2212 1 ) (Tr(W\u2217 \u2212Wt)\u2212 gt) + \u03c0 2 Tr ( (W\u2217 \u2212Wt) ( uu> )) + gt(Tr(I + Wt \u2212 I)) \u2223\u2223\u2223 \u00ac \u22646.07\n(\u03c0 2 \u2212 1 ) d\u03b32 + \u03c0 2 \u2016st\u20162 \u221a d+ 2.05|gt|d\u03b3\nwhere \u00ac uses Lemma E.14 and Lemma E.15. Thus, \u2223\u2223\u22234gt \u2212 [ \u2212\u03b7 ( d+ \u03c0\n2 \u2212 1 ) gt ]\u2223\u2223\u2223\n\u2264\u03b7 [ 21.1d\u03b32 + 1.03 \u221a d\u03b5+ 2.1d\u03b3|gt|+ 2.05\u03c0\n2 \u2016s\u20162\u03b3\n\u221a d+ 61d\u03b32 + 2.05|gt|d\u03b3 + 6.07 (\u03c0 2 \u2212 1 ) d\u03b32 + \u03c0 2 \u2016st\u20162 \u221a d\n]\n\u2264\u03b7 [ 86d\u03b32 + 1.03 \u221a d\u03b5+ 4.15d\u03b3|gt|+ 4.8\u2016st\u20162\u03b3 \u221a d ]\nNow we have\n|gt+1| = |gt \u22124gt| \u2264 ( 1\u2212 \u03b7 ( d+ \u03c0\n2 \u2212 1\u2212 4.15d\u03b3\n)) |gt|+ 86\u03b7d\u03b32 + 1.03\u03b7 \u221a d\u03b5+ 4.8\u03b7\u2016st\u20162\u03b3 \u221a d\n\u2264(1\u2212 0.95\u03b7d)|gt|+ 86\u03b7d\u03b32 + 1.03\u03b7 \u221a d\u03b5+ 4.8\u03b7\u2016st\u20162\u03b3 \u221a d\nG.4 Proof for Lemma B.4 By definition of st,\n4st , st+1 \u2212 st = (Wt \u2212Wt+1)u = \u03b7(\u2207L(Wt) + Et)u = \u2212\u03b7Qtu+ \u03b7(Qt +\u2207L(Wt) + Et)u\nBy definition of Qt,\nQtu = (\u03c0\n2 (W\u2217 \u2212Wt)\n( I + uu> ) + (W\u2217 \u2212Wt)> \u2212 2Diag(W\u2217 \u2212Wt) + gtI + Wt ) u\n= (d+ 1)\u03c0\n2 st +\n( (W\u2217 \u2212Wt)> \u2212 2Diag(W\u2217 \u2212Wt) + gtI + Wt ) u\nThus, we know \u2225\u2225\u2225\u2225Qtu\u2212 (d+ 1)\u03c0\n2 st \u2225\u2225\u2225\u2225 2 = \u2225\u2225((W\u2217 \u2212Wt)> \u2212 2Diag(W\u2217 \u2212Wt) + gtI + Wt ) u \u2225\u2225 2\n\u2264 \u221a d ( \u2016(W\u2217 \u2212Wt)>\u20162 + 2\u2016Diag(W\u2217 \u2212Wt)\u20162 + \u2016gtI + Wt\u20162 ) \u00ac \u2264 \u221a d ( 2\u03b3 + 4\u03b3 + |gt| 1 + \u03b3\n1\u2212 \u03b3\n) < (6\u03b3 + 1.03|gt|) \u221a d\nwhere \u00ac uses Lemma E.1 and Lemma E.10. By Lemma B.1, \u20164st\u2212 [\u2212\u03b7 (d+1)\u03c02 st]\u20162 < \u03b7(6\u03b3+ 1.03|gt|) \u221a d+\u03b7\u2016(Qt+\u2207L(Wt) +Et)u\u20162 \u2264 \u03b7(6.61\u03b3+\n1.03|gt|+ \u03b5) \u221a d.\nG.5 Proof for Lemma B.5 Combining Lemma B.3 and Lemma B.4, we get\n|gt+1|+ \u2016st+1\u20162 \u2264(1\u2212 0.95\u03b7d)(|gt|+ \u2016st\u20162) + \u03b7(6.6\u03b3 + 1.03|gt|+ \u03b5) \u221a d+ 86\u03b7d\u03b32 + 1.03\u03b7 \u221a d\u03b5+ (4.8\u03b7\u03b3 \u221a d\u2212 0.62\u03b7d)\u2016st\u20162\n\u00ac \u2264(1\u2212 0.95\u03b7d)(|gt|+ \u2016st\u20162) + 6.6\u03b7\u03b3 \u221a d+ 86\u03b7d\u03b32 + \u03b71.03|gt| \u221a d+ 2.03\u03b7 \u221a d\u03b5\n \u2264(1\u2212 0.84\u03b7d)(|gt|+ \u2016st\u20162) + 6.6\u03b7\u03b3 \u221a d+ 87\u03b7d\u03b32\nwhere \u00ac uses \u03b3 \u2264 1100 , d \u2265 100,  uses \u03b5 \u2264 \u03b32 and d \u2265 100. So if the following inequality holds, |gt|+ \u2016st\u20162 will always decrease by factor at least 1\u2212 0.5\u03b7d.\n0.34\u03b7d(|gt|+ \u2016st\u20162) \u2265 6.6\u03b7\u03b3 \u221a d+ 87\u03b7d\u03b32\nWhich gives\n|gt|+ \u2016st\u20162 \u2265 6.6\u03b7\u03b3\n\u221a d+ 87\u03b7d\u03b32\n0.34\u03b7d =\n6.6\u03b3\n0.34 \u221a d\n+ 87\u03b32\n0.34\nwhere the last expression is smaller than 4.5\u03b3. Hence, |gt|+ \u2016st\u20162 will keep decreasing by 1\u2212 0.5\u03b7d as long as it is larger than 4.5\u03b3. So we have \u2016st\u20162 \u2264 4.5\u03b3. Now plug it back to the updating rule of |gt|:\n|gt+1| \u2264(1\u2212 0.95\u03b7d)|gt|+ 86\u03b7d\u03b32 + 1.03\u03b7 \u221a d\u03b5+ 4.8\u03b7\u2016st\u20162\u03b3 \u221a d\n\u2264(1\u2212 0.95\u03b7d)|gt|+ 86\u03b7d\u03b32 + 1.03\u03b7 \u221a d\u03b5+ 21.6\u03b7\u03b32 \u221a d\nIn order to get factor 1\u2212 0.5\u03b7d, we have 0.45\u03b7d|gt| \u2265 86\u03b7d\u03b32 + 1.03\u03b7 \u221a d\u03b5+ 21.6\u03b7\u03b32 \u221a d\nSolve this inequality, we get\n86\u03b7d\u03b32 + 1.03\u03b7 \u221a d\u03b5+ 21.6\u03b7\u03b32 \u221a d\n0.45\u03b7d =\n86\u03b32 0.45 + 1.03\u03b5+ 21.6\u03b32 0.45 \u221a d \u2264 197\u03b32\nThe last inequality uses d \u2265 100, \u03b5 \u2264 \u03b32. So even after |gt|+ \u2016st\u20162 is below 4.5\u03b3, |gt| will keep decreasing by factor 1\u2212 0.5\u03b7d until it is smaller than 197\u03b32.\nFinally we bound the number of steps to arrive 197\u03b32. Let \u03b3 = 1400 , \u03b30 = 1 8000 . Again, the constants here are pretty loose. Since |gt| \u2264 (1 \u2212 0.5\u03b7d)t|g0| \u2264 (1 \u2212 0.5\u03b7d)t2d\u03b30, in order to let gt \u2264 197\u03b32, it suffices to have t \u2265 log 197\u03b32 2d\u03b30\nlog(1\u2212 \u03b7d2 ) . Since \u03b7d is small, by Taylor expansion we know log(1\u2212 \u03b7d2 ) \u2248 \u2212 \u03b7d 2 . Thus, it suffices to let\nt \u2265 2 log(0.203d)\u03b7d . Notice that log(0.203d) d is decreasing for d \u2265 100, we know it suffices to let t \u2265 116\u03b7 .\nG.6 Proof for Lemma B.6 Let H = W \u2212W\u2217, by the updating rule of Wt and the definition of Qt, we know\nHt+1 = Ht \u2212 \u03b7Ht (\u03c0\n2 uu> +\n\u03c0\n2\n) \u2212 \u03b7H>t + 2\u03b7Diag(Ht) + \u03b7gtI + W \u2212 \u03b7(Gt + Qt)\nThat gives,\n\u2016Ht+1 + H>t+1\u20162 \u2264 \u2225\u2225\u2225(Ht + H>t ) ( I\u2212 \u03b7 (\u03c0 2 uu> + \u03c0 2 + 1 ))\u2225\u2225\u2225 2 + 2\u03b7 \u2225\u2225Diag(Ht + H>t ) \u2225\u2225 2\n+ 2\u03b7|gt|\u2016I + W\u20162 + 2\u03b7 \u2016Et +\u2207L(Wt) + Qt\u20162 \u00ac \u2264 ( I\u2212 \u03b7\n(\u03c0 2 + 1 )) \u2016Ht + H>t \u20162 + 2\u03b7\u2016Ht + H>t \u20162 + 2(1 + \u03b3)\u03b7|gt| 1\u2212 \u03b3 + 2\u03b7\u03b5+ 122\u03b7\u03b3 2\n \u2264 ( I\u2212 \u03b7 (\u03c0 2 \u2212 1 )) \u2016Ht + H>t \u20162 + 2.05\u03b7|gt|+ 124\u03b7\u03b32 (15)\nwhere \u00ac uses Lemma E.18, Lemma E.10, \u2016Et\u20162 \u2264 \u03b5 and Lemma B.1.  uses \u03b5 \u2264 \u03b32 and \u03b3 \u2264 1100 . Similarly, we get\n\u2016Ht+1 \u2212H>t+1\u20162 \u00ac \u2264 \u2225\u2225\u2225(Ht \u2212H>t ) ( I\u2212 \u03b7 (\u03c0 2 uu> + \u03c0 2 \u2212 1 ))\u2225\u2225\u2225 2 + \u03b7|gt|\u2016I + W \u2212 I + I\u2212 I + W\n>\u20162 + 2\u03b7 \u2016Et +\u2207L(Wt) + Qt\u20162  \u2264 ( I\u2212 \u03b7\n(\u03c0 2 \u2212 1 )) \u2016Ht \u2212H>t \u20162 + 4.10\u03b7\u03b3|gt|+ 124\u03b7\u03b32 (16)\nwhere \u00ac holds as the diagonal terms cancel out,  uses Lemma E.18, Lemma E.15. Adding (15) and (16), we get\n\u2016Ht+1 + H>t+1\u20162 + \u2016Ht+1 \u2212H>t+1\u20162 \u2264 ( I\u2212 \u03b7\n(\u03c0 2 \u2212 1 )) ( \u2016Ht + H>t \u20162 + \u2016Ht \u2212H>t \u20162 ) + 2.1\u03b7|gt|+ 248\u03b7\u03b32 (17)\nFor any T > 0, by applying (17) recursively, we have\n\u2016HT + H>T \u20162 + \u2016HT \u2212H>T \u20162 \u2264 \u2016H0 + H>0 \u20162 + \u2016H0 \u2212H>0 \u20162 + 2.1\u03b7 T\u22121\u2211\nt=0\n|gt|+ 248\u03b7T\u03b32\nBy Lemma E.4 we know |g0| \u2264 2d\u03b30, so 2.1\u03b7 \u2211T\u22121 t=0 |gt| \u2264 2.1\u03b7|g0|(1\u2212(1\u22120.5\u03b7d)T ) (0.5\u03b7d) \u2264 4.2|g0| d \u2264 8.4\u03b30. By the proof of Lemma B.5, we know T \u2264 116\u03b7 , so 248\u03b7T\u03b32 \u2264 15.5\u03b32. By triangle inequality, we know \u2016H0\u20162 \u2264 \u2016W0\u20162 + \u2016W\u2217\u20162 \u2264 2\u03b30, so \u2016H0 + H>0 \u20162 + \u2016H0 \u2212H>0 \u20162 \u2264\n4\u2016H0\u20162 \u2264 8\u03b30. By triangle inequality again we get\n\u2016HT \u20162 \u2264 \u2016HT + H>T \u20162 + \u2016HT \u2212H>T \u20162 \u2264 \u2016H0 + H>0 \u20162 + \u2016H0\u2212H>0 \u20162 + 19\u03b32 + 8.4\u03b30 \u2264 16.4\u03b30 + 15.5\u03b32\nRecall we set \u03b3 = 1400 , \u03b30 = 1 8000 in the proof of Lemma B.5, we know \u2016WT \u20162 \u2264 \u2016W\u2217\u20162 + \u2016HT \u20162 \u2264 17.4\u03b30 + 15.5\u03b3 2 \u2264 1440 \u2264 \u03b3.\nG.7 Proof for Lemma B.7 First, by the proof of Lemma B.5, we know |gt| will keep small if \u2016Wt\u20162 \u2264 \u03b3 \u2264 1100 .\nAdding (15) and (16), we get\n\u2016Ht+1 + H>t+1\u20162 + \u2016Ht+1 \u2212H>t+1\u20162 \u2264 ( I\u2212 \u03b7\n(\u03c0 2 \u2212 1 )) ( \u2016Ht+1 + H>t+1\u20162 + \u2016Ht+1 \u2212H>t+1\u20162 ) + 2.1\u03b7|gt|+ 248\u03b7\u03b32\n\u00ac \u2264 ( I\u2212 \u03b7 (\u03c0 2 \u2212 1 )) ( \u2016Ht+1 + H>t+1\u20162 + \u2016Ht+1 \u2212H>t+1\u20162 ) + 661\u03b7\u03b32 (18)\nwhere \u00ac holds as |gt| \u2264 197\u03b32. So either \u2016Ht+1 + H>t+1\u20162 + \u2016Ht+1\u2212H>t+1\u20162 keeps decreasing, or it increases, i.e.,\n\u03b7 (\u03c0 2 \u2212 1 ) ( \u2016Ht+1 + H>t+1\u20162 + \u2016Ht+1 \u2212H>t+1\u20162 ) \u2264 197\u03b7\u03b32\nThat gives,\n\u2016Ht+1 + H>t+1\u20162 + \u2016Ht+1 \u2212H>t+1\u20162 \u2264 197\u03b32\n\u03c0 2 \u2212 1\n\u2264 346\u03b32\nTherefore, combined with the proof of Lemma B.6, we know \u2016Ht+1 + H>t+1\u20162 + \u2016Ht+1 \u2212H>t+1\u20162 will keep decreasing until it is at most 346\u03b32. Now,\n\u2016Wt\u20162 \u2264 \u2016Ht\u20162 + \u2016W\u2217\u20162 \u2264 \u2016Ht+1 + H>t+1\u20162 + \u2016Ht+1 \u2212H>t+1\u20162 + \u03b30 \u00ac \u2264 (346 + 20)\u03b32 \u2264 \u03b3\nwhere \u00ac holds as \u03b30 = 18000 . So \u2016Wt\u20162 is always bounded by \u03b3.\nH Proofs for Section C For notational simplicity, denote\nxj , ( ej + wj \u00b7 ej + wj> ) (w\u2217j \u2212 wj),\nX , (x1, \u00b7 \u00b7 \u00b7 , xd) (19) yj , ( I\u2212 ej + wj \u00b7 ej + wj> ) (w\u2217j \u2212 wj),\nY , (y1, \u00b7 \u00b7 \u00b7 , yd) (20) zj , (\nI\u2212 1 2 ej + wj \u00b7 ej + wj>\n) (w\u2217j \u2212 wj),\nZ , (z1, \u00b7 \u00b7 \u00b7 , zd)\nWe have the following relationship between xj , yj , zj .\nLemma H.1.\n\u2016zj\u201622 = 1 4 \u2016xj\u201622 + \u2016yj\u201622, \u2016xj\u201622 + \u2016yj\u201622 = \u2016w\u2217j \u2212 wj\u201622 (21)\nProof for Lemma H.1. By definition,\n\u2016zj\u201622 =\u2016w\u2217j \u2212 wj\u201622 (\nI\u2212 1 2 ej + wj \u00b7 ej + wj>\n)>( I\u2212 1\n2 ej + wj \u00b7 ej + wj>\n)\n=\u2016w\u2217j \u2212 wj\u201622 ( I\u2212 ej + wj \u00b7 ej + wj> + 1\n4 ej + wj \u00b7 ej + wj>ej + wj \u00b7 ej + wj>\n)\n=\u2016w\u2217j \u2212 wj\u201622 (\nI\u2212 3 4 ej + wj \u00b7 ej + wj>\n) ,\nand similarly\n\u2016yj\u201622 =\u2016w\u2217j \u2212 wj\u201622 ( I\u2212 ej + wj \u00b7 ej + wj> )> ( I\u2212 ej + wj \u00b7 ej + wj> ) = \u2016w\u2217j \u2212 wj\u201622 ( I\u2212 ej + wj \u00b7 ej + wj> ) , \u2016xj\u201622 =\u2016w\u2217j \u2212 wj\u201622 ( ej + wj \u00b7 ej + wj> )> ( ej + wj \u00b7 ej + wj> ) = \u2016w\u2217j \u2212 wj\u201622 ( ej + wj \u00b7 ej + wj> )\nThe lemma follows.\nH.1 Proof for Lemma C.1 In this proof, we heavily use the following trick between the summation of four vector products, and the trace of four matrix products. We give one example below, and other cases are similar. Lemma H.2. \u2211 i,j z > j (ei+w \u2217 i )(ei + w \u2217 i\u2212ei + wi)>ej + wj = Tr ([ Z>(I + W\u2217) ] [ (I + W\u2217 \u2212 I + W)>I + W ]) . Proof. By definition, Tr(AB) = \u2211d j=1(AB)j,j = \u2211 i,j Aj,iBi,j . Thus,\nTr ([ Z>(I + W\u2217) ] [ (I + W\u2217 \u2212 I + W)>I + W ]) = \u2211\ni,j\n[ Z>(I + W\u2217) ] j,i [ (I + W\u2217 \u2212 I + W)>I + W ] i,j\nBy definition, [ Z>(I + W\u2217) ] j,i = z>j (ei+w \u2217 i ), and [ (I + W\u2217 \u2212 I + W)>I + W ] i,j\n= (ei + w\u2217i\u2212ei + wi)>ej + wj , so the lemma follows.\nNow we proceed to prove Lemma C.1. We first bound \u2211d j=1 z > j Ajej + wj below by splitting Aj into three\nparts, and then improve the lower bound in Lemma H.4.\nLemma H.3. If \u2016W\u20162, \u2016W\u2217\u20162 \u2264 \u03b3 \u2264 1100 , we have\nd\u2211\nj=1\nz>j Ajej + wj \u2265 \u22128\u03b3\u2016W\u2217 \u2212W\u20162F \u2212 \u221a \u2016W\u2217 \u2212W\u20162f \u2212 3\n4 \u2016X\u20162F\n\u221a \u2016W\u2217 \u2212W\u20162F \u2212 \u2016X\u20162F\n.\nProof. We rewrite Aj as\nAj = Bj + 1\n2 Cj + Dj (22)\nwhere\nBj = \u2211\ni 6=j (ei+w\n\u2217 i )(ei + w \u2217 i\u2212ei + wi)>, Cj =\n\u2211 i6=j \u3008w\u2217i\u2212wi, ei + wi\u3009ei + wi\u00b7ei + wi > , Dj =\n \u2211\ni 6=j ziei + wi\n>  \nFor notational simplicity, we also write B,C,D as the corresponding terms with sum \u2211d i=1 instead of \u2211 i 6=j , so they do not depend on index j. We estimate B,C,D first, then estimate Bj ,Cj ,Dj respectively by taking the differences.\n1. From B to Bj :\nd\u2211\nj=1\nz>j Bej + wj = \u2211\ni,j\nz>j (ei + w \u2217 i )(ei + w \u2217 i \u2212 ei + wi)>ej + wj\n\u00ac =Tr ([ Z>(I + W) ] [ (I + W\u2217 \u2212 I + W)>I + W ])  \u2265 \u2212 \u2225\u2225(I + W)>Z \u2225\u2225 F \u2225\u2225\u2225I + W>(I + W\u2217 \u2212 I + W) \u2225\u2225\u2225 F\n\u00ae \u2265\u2212 \u2016I + W\u20162\u2016I + W\u20162 \u2016Z\u2016F \u2225\u2225I + W\u2217 \u2212 I + W \u2225\u2225 F \u00af \u2265 \u2212 (1 + \u03b3) 2 1\u2212 \u03b3 \u2016Z\u2016F \u2225\u2225I + W\u2217 \u2212 I + W \u2225\u2225 F\n(23)\nwhere \u00ac uses Lemma H.2,  uses Tr(AB) \u2265 \u2212\u2016A\u2016F \u2016B\u2016F , \u00ae uses \u2016AB\u2016F \u2264 \u2016A\u20162\u2016B\u2016F , and \u00af uses Lemma E.1. By Lemma D.1 term 1, we have\n\u2225\u2225I + W\u2217 \u2212 I + W \u2225\u2225 F \u2264 \u221a\u2211d i=1 \u2016yi\u201622 1\u2212 2\u03b3 = \u2016Y\u2016F\u221a 1\u2212 2\u03b3 (24)\nOn the other hand,\nd\u2211\nj=1\nz>j (Bj \u2212B)ej + wj = d\u2211\nj=1\nz>j (ej + w \u2217 j )(ej + w \u2217 j \u2212 ej + wj)>ej + wj\n= d\u2211\nj=1\n(w\u2217j \u2212 wj)>(I\u2212 1 2 ej + wj \u00b7 ej + wj>)(ej + w\u2217j )(ej + w\u2217j \u2212 ej + wj)>ej + wj\nFor any vector x, ej + wj \u00b7 ej + wj>x is the projection of x onto the direction ej + wj , so 12 \u2264 \u2016I\u2212 12ej + wj \u00b7 ej + wj >\u20162 \u2264 1, and\n|(w\u2217j \u2212 wj)>(ej + w\u2217j )(ej + w\u2217j \u2212 ej + wj)>ej + wj | \u00ac \u2264 |(w\u2217j \u2212 wj)>(ej + w\u2217j )| \u2016w\u2217j \u2212 wj\u201622 2(1\u2212 2\u03b3)\n \u2264 \u2016w\u2217j \u2212 wj\u201632(1 + \u03b3) 2(1\u2212 2\u03b3) \u2264 \u2016w\u2217j \u2212 wj\u201622(1 + \u03b3)\u03b3 1\u2212 2\u03b3 (25)\nwhere \u00ac uses Lemma D.1 term 2, and  uses Cauchy-Schwartz. Combining (23),(24),(25), we get\nd\u2211\nj=1\nz>j Bjej + wj \u2265 \u2212 (1 + \u03b3)2 (1\u2212 \u03b3)\u221a1\u2212 2\u03b3 \u2016Z\u2016F \u2016Y\u2016F \u2212 (1 + \u03b3)\u03b3 1\u2212 2\u03b3 \u2016W \u2217 \u2212W\u20162F\n2. From C to Cj :\nd\u2211\nj=1\nz>j Cej + wj = \u2211\ni,j\nz>j \u3008w\u2217i \u2212 wi, ei + wi\u3009ei + wi \u00b7 ei + wi > ej + wj\n\u00ac =Tr( [ Z>X ] [ I + W > I + W ] ) = Tr(Z>X) + Tr(Z>X(I + W > I + W \u2212 I))\n \u2265Tr(Z>X)\u2212 \u2016Z\u2016F \u2016X\u2016F \u2016I + W > I + W \u2212 I\u20162 \u00ae \u2265 Tr(Z>X)\u2212 4\u03b3\n(1\u2212 \u03b3)2 \u2016Z\u2016F \u2016X\u2016F\nwhere \u00ac uses Lemma H.2 and xj = \u3008w\u2217j \u2212 wj , ej + wj\u3009ej + wj ,  uses Tr(AB) \u2265 \u2212\u2016A\u2016F \u2016B\u2016F , and \u2016AB\u2016F \u2264 \u2016A\u20162\u2016B\u2016F , and \u00ae uses Lemma E.1. On the other hand,\nd\u2211\nj=1\nz>j (C\u2212Cj)ej + wj = d\u2211\nj=1\nz>j \u3008w\u2217j \u2212 wj , ej + wj\u3009ej + wj \u00b7 ej + wj > ej + wj\n=\nd\u2211\nj=1\nz>j \u3008w\u2217j \u2212 wj , ej + wj\u3009ej + wj = Tr(Z>X)\nThat implies, 12 \u2211d j=1 z > j Cjej + wj \u2265 \u2212 2\u03b3(1\u2212\u03b3)2 \u2016Z\u2016F \u2016X\u2016F . 3. From D to Dj :\nd\u2211\nj=1\nz>j Dej + wj = \u2211\ni,j\nz>j ziei + wi > ej + wj = Tr ([ Z>Z ] [ I + W > I + W ]) \u2265 (1\u2212 \u03b3) 2\n(1 + \u03b3)2 \u2016Z\u20162F\nwhere the last inequality holds by Lemma E.1. On the other hand,\nz>j (D\u2212Dj)ej + wj = \u2016zj\u201622 That gives, \u2211\nj\nz>j Djej + wj \u2265 \u2212 4\u03b3\n(1 + \u03b3)2 \u2016Z\u20162F\nNow, combining Bj ,Cj ,Dj together, using (22), we have\nd\u2211\nj=1\nz>j Ajej + wj \u2265\u2212 (1 + \u03b3)2 (1\u2212 \u03b3)\u221a1\u2212 2\u03b3 \u2016Z\u2016F \u2016Y\u2016F \u2212 (1 + \u03b3)\u03b3 1\u2212 2\u03b3 \u2016W \u2217 \u2212W\u20162F\n\u2212 2\u03b3 (1\u2212 \u03b3)2 \u2016Z\u2016F \u2016X\u2016F \u2212\n4\u03b3\n(1 + \u03b3)2 \u2016Z\u20162F\nBy definition, we know \u2016X\u2016F \u2264 \u2016W\u2217 \u2212W\u2016F , \u2016Y\u2016F \u2264 \u2016W\u2217 \u2212W\u2016F , \u2016Z\u2016F \u2264 \u2016W\u2217 \u2212W\u2016F , and \u03b3 \u2264 1100 , so\n\u2212 (1 + \u03b3)\u03b3 1\u2212 2\u03b3 \u2016W \u2217 \u2212W\u20162F \u2212 2\u03b3 (1\u2212 \u03b3)2 \u2016Z\u2016F \u2016X\u2016F \u2212 4\u03b3 (1 + \u03b3)2 \u2016Z\u20162F \u2265 \u22127\u03b3\u2016W\u2217 \u2212W\u20162F (26)\nMoreover,\n\u2212 (\n(1 + \u03b3)2 (1\u2212 \u03b3)\u221a1\u2212 2\u03b3 \u2212 1 ) \u2016Z\u2016F \u2016Y\u2016F \u2265 \u22120.05\u03b3\u2016W\u2217 \u2212W\u20162F (27)\nThus, those are small order terms. The only term left is \u2016Z\u2016F \u2016Y\u2016F . By (21), we know\n\u2016Z\u2016F \u2016Y\u2016F \u2264 \u221a \u2016W\u2217 \u2212W\u20162F \u2212 3\n4 \u2016X\u20162F\n\u221a \u2016W\u2217 \u2212W\u20162F \u2212 \u2016X\u20162F (28)\nCombining (26), (27), (28), we get:\nd\u2211\nj=1\nz>j Ajej + wj \u2265 \u22128\u03b3\u2016W\u2217 \u2212W\u20162F \u2212 \u221a \u2016W\u2217 \u2212W\u20162f \u2212 3\n4 \u2016X\u20162F\n\u221a \u2016W\u2217 \u2212W\u20162F \u2212 \u2016X\u20162F\nNow it remains to bound \u221a \u2016W\u2217 \u2212W\u20162f \u2212 34\u2016X\u20162F \u221a \u2016W\u2217 \u2212W\u20162F \u2212 \u2016X\u20162F .\nLemma H.4.\n\u2212 \u221a \u2016W\u2217 \u2212W\u20162F \u2212 3\n4 \u2016X\u20162F\n\u221a \u2016W\u2217 \u2212W\u20162F \u2212 \u2016X\u20162F \u2265 \u22121.3\u2016W\u2217 \u2212W\u20162F + \u2016W\u2217 \u2212W\u2016F \u2016X\u2016F\nProof. Consider the function f(x) = \u221a y2 \u2212 34x2 \u221a y2 \u2212 x2 + xy, where x \u2208 [0, y]. It suffices to show that f(x) \u2264 1.3y2. Indeed, we know\nf \u2032(x) = x(6x2 \u2212 7y2)\n2 \u221a 4y2 \u2212 3x2 \u221a y2 \u2212 x2 + y\nWhen x = 0, f \u2032(x) = y > 0, and when x\u2192 y, f \u2032(x) < 0. We want to find the place where f \u2032(x) = 0, which gives the maximum value. Assume x = \u03bby, this is equivalent to solve\n\u03bby(6(\u03bby)2 \u2212 7y2) = \u22122y \u221a 4y2 \u2212 3(\u03bby)2 \u221a y2 \u2212 (\u03bby)2\nCancel all y, and we get the solution x \u2248 0.566y, where f(x) \u2248 1.2845y2 < 1.3y2.\nProof of Lemma C.1. Combining Lemma H.3 and Lemma H.4, we have proved Lemma C.1.\nH.2 Proof for Lemma C.2 Again, we first consider the full sum, g = \u2211d i=1(\u2016ei + w\u2217i \u20162 \u2212 \u2016ei + wi\u20162).\nBy Lemma E.3, we have\n|g \u2212 gj | = |\u2016ej + w\u2217j \u20162 \u2212 \u2016ej + wj\u20162| \u2264 \u2016w\u2217j \u2212 wj\u20162 Thus by Cauchy Schwartz,\n|(g \u2212 gj)\u3008w\u2217j \u2212 wj , ej + wj\u3009| \u2264 \u2016w\u2217j \u2212 wj\u20162\u2016xj\u20162 Summing over j, we get\nd\u2211\nj=1\n|(g \u2212 gj)\u3008w\u2217j \u2212 wj , ej + wj\u3009| \u2264 d\u2211\nj=1\n\u2016w\u2217j \u2212 wj\u20162\u2016xj\u20162 \u2264 \u2016W\u2217 \u2212W\u2016F \u2016X\u2016F (29)\nwhere the last inequality is by Cauchy Schwartz. Now\ng\nd\u2211\nj=1\n\u3008w\u2217j \u2212 wj , ej + wj\u3009 = g d\u2211\nj=1\n\u3008ej + w\u2217j \u2212 ej + wj , ej + wj\u3009\n=g d\u2211\nj=1\n(\u2016ej + w\u2217j \u20162 \u2212 \u2016ej + wj\u20162 + \u3008ej + w\u2217j , ej + wj \u2212 ej + w\u2217j \u3009) = g2 + gb \u2265 gb (30)\nwhere b is defined to be \u2211d j=1\u3008ej + w\u2217j , ej + wj \u2212 ej + w\u2217j \u3009. By Lemma D.1 term 2 we know\n\u2212 (1 + \u03b3)\u2016W \u2217 \u2212W\u20162F\n2(1\u2212 2\u03b3) \u2264 b \u2264 0\nCombining (29), (30), the lemma follows.\nd\u2211\nj=1\n\u3008gjej + wj , w\u2217j \u2212 wj\u3009 = d\u2211\nj=1\n\u3008(gj \u2212 g)ej + wj , w\u2217j \u2212 wj\u3009+ d\u2211\nj=1\n\u3008gej + wj , w\u2217j \u2212 wj\u3009\n\u2265 \u2212 \u2016W\u2217 \u2212W\u2016F \u2016X\u2016F + g2 + gb \u2265 \u2212\u2016W\u2217 \u2212W\u2016F \u2016X\u2016F \u2212 (1 + \u03b3)g\u2016W\u2217 \u2212W\u20162F\n2(1\u2212 2\u03b3)\nH.3 Proof for Lemma C.3\nd\u2211\nj=1\n\u3008P3,j , w\u2217j \u2212 wj\u3009 = d\u2211\nj=1\n\u3008\u03c0 2 (w\u2217j \u2212 wj)\u2212 \u03b8j\u2217,j(ej + w\u2217j ) + \u2016ej + w\u2217j \u2016 sin \u03b8j\u2217,jej + wj , w\u2217j \u2212 wj\u3009\n\u00ac =\nd\u2211\nj=1\n\u3008\u03c0 2\n(w\u2217j \u2212 wj)\u2212 \u03b8j\u2217,j\u2016ej + w\u2217j \u20162(ej + w\u2217j \u2212 ej + wj) + \u03b1j\u2217,j |\u03b8j\u2217,j |3\u2016ej + w\u2217j \u2016ej + wj\n3 , w\u2217j \u2212 wj\u3009\n \u2265\u03c0 2 \u2016W\u2217 \u2212W\u20162F \u2212\nd\u2211\nj=1\n1.001(1 + \u03b3)\u2016w\u2217j \u2212 wj\u201622\u2016ej + w\u2217j \u2212 ej + wj\u20162 \u2212 d\u2211\nj=1\n0.335(1 + \u03b3)\u2016w\u2217j \u2212 wj\u201642\n\u00ae \u2265\u03c0 2 \u2016W\u2217 \u2212W\u20162F \u2212\nd\u2211\nj=1\n1.001(1 + \u03b3)\u221a 1\u2212 2\u03b3 \u2016w \u2217 j \u2212 wj\u201632 \u2212\nd\u2211\nj=1\n0.335(1 + \u03b3)\u2016w\u2217j \u2212 wj\u201642\n\u00af \u2265 (\u03c0\n2 \u2212 0.021\n) \u2016W\u2217 \u2212W\u20162F\nwhere \u00ac uses Taylor\u2019s Theorem for sin \u03b8j\u2217,j , so we know |\u03b1j\u2217,j | \u2264 1.  uses Lemma D.1 term 3 and Cauchy Schwartz, \u00ae uses Lemma D.1 term 1, \u00af holds since \u03b3 \u2264 1100 , and the two small order terms can be bounded by 0.021\u2016W\u2217 \u2212W\u20162F .\nI Proofs for Section 2\nI.1 Proof for Lemma 2.5 By the updating rule, we have\n\u2016Wt+1 \u2212W\u2217\u20162F = \u2016Wt \u2212W\u2217 \u2212 \u03b7Gt\u20162F = \u2016Wt \u2212W\u2217\u20162F \u2212 2\u3008Wt \u2212W\u2217, \u03b7\u2207f(W)\u3009+ \u03b72\u2016Gt\u20162F \u2264\u2016Wt \u2212W\u2217\u20162F \u2212 2\u3008Wt \u2212W\u2217, \u03b7\u2207f(W)\u3009+ \u03b72tG2 \u2264 (1\u2212 2\u03b7\u03b4)\u2016Wt \u2212W\u2217\u20162F + \u03b72G2\nNow if \u03b7\u03b4\u2016Wt\u2212W\u2217\u20162F \u2265 \u03b72G2, we know the \u2016Wt\u2212W\u2217\u20162F will decrease by a factor of (1\u2212 \u03b7\u03b4) for every step. Otherwise, although it could increase, we know\n\u2016Wt \u2212W\u2217\u20162F \u2264 \u03b7G2\n\u03b4\nBy setting \u03b7 = (1+\u03b1) log T\u03b4T , we know after T steps, either \u2016WT \u2212W\u2217\u20162F is already smaller than \u03b7G2\n\u03b4 = (1+\u03b1) log TG2\n\u03b42T , or it is decreasing by factor of (1\u2212 \u03b7\u03b4) for every step, which means\n\u2016WT \u2212W\u2217\u20162F \u2264 \u2016W0 \u2212W\u2217\u20162F (1\u2212 \u03b7\u03b4)T \u2264 D2e\u2212\u03b7\u03b4T = D2e\u2212(1+\u03b1) log T = D2T\u2212\u03b1\nT \u2264 (1 + \u03b1) log TG\n2\n\u03b42T .\nThe last inequality holds since\nT\u03b1 log T \u2265 D 2\u03b42\n(1 + \u03b1)G2\nThus, \u2016WT \u2212W\u2217\u20162F will be smaller than (1+\u03b1) log TG 2 \u03b42T ."}], "references": [{"title": "Learning polynomials with neural networks", "author": ["Alexandr Andoni", "Rina Panigrahy", "Gregory Valiant", "Li Zhang"], "venue": "In ICML, pages 1908\u20131916,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Provable bounds for learning some deep representations", "author": ["Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["Andrew R. Barron"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1993}, {"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Micha\u00ebl Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "In AISTATS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Open problem: The landscape of the loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Yann LeCun", "G\u00e9rard Ben Arous"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko"], "venue": "MCSS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "author": ["Amit Daniely", "Roy Frostig", "Yoram Singer"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "NIPS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John C. Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Escaping from saddle points - online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "In COLT 2015,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Reliably learning the relu in polynomial time", "author": ["Surbhi Goel", "Varun Kanade", "Adam R. Klivans", "Justin Thaler"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["Ian J. Goodfellow", "Oriol Vinyals"], "venue": "CoRR, abs/1412.6544,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In ICCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell B. Stinchcombe", "Halbert White"], "venue": "Neural Networks,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1989}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": "arXiv preprint arXiv:1506.08473,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Deep learning without poor local minima", "author": ["Kenji Kawaguchi"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Efficient BackProp, pages 9\u201350", "author": ["Yann LeCun", "Leon Bottou", "Genevieve B. Orr", "Klaus Robert M\u00fcller"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "On the computational efficiency of training neural networks", "author": ["Roi Livni", "Shai Shalev-Shwartz", "Ohad Shamir"], "venue": "In NIPS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F. Mont\u00fafar", "Razvan Pascanu", "KyungHyun Cho", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Expressiveness of rectifier networks", "author": ["Xingyuan Pan", "Vivek Srikumar"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Mont\u00fafar", "Yoshua Bengio"], "venue": "CoRR, abs/1312.6098,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Non-asymptotic theory of random matrices: extreme singular values", "author": ["M. Rudelson", "R. Vershynin"], "venue": "ArXiv e-prints,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Dynamics of on-line gradient descent learning for multilayer neural networks", "author": ["David Saad", "Sara A. Solla"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "On the quality of the initial basin in overspecified neural networks", "author": ["Itay Safran", "Ohad Shamir"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M. Saxe", "James L. McClelland", "Surya Ganguli"], "venue": "CoRR, abs/1312.6120,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Provable methods for training neural networks with sparse connectivity", "author": ["Hanie Sedghi", "Anima Anandkumar"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Distribution-specific hardness of learning", "author": ["Ohad Shamir"], "venue": "neural networks. CoRR,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Training a single sigmoidal neuron is hard", "author": ["Jir\u00ed S\u00edma"], "venue": "Neural Computation,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2002}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E. Dahl", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Symmetry-breaking convergence analysis of certain two-layered neural networks with relu nonlinearity", "author": ["Yuandong Tian"], "venue": "In Submitted to ICLR", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Diversity leads to generalization in neural networks", "author": ["Bo Xie", "Yingyu Liang", "Le Song"], "venue": "In AISTATS,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2017}], "referenceMentions": [{"referenceID": 16, "context": "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].", "startOffset": 66, "endOffset": 83}, {"referenceID": 5, "context": "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].", "startOffset": 66, "endOffset": 83}, {"referenceID": 2, "context": "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].", "startOffset": 66, "endOffset": 83}, {"referenceID": 6, "context": "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].", "startOffset": 66, "endOffset": 83}, {"referenceID": 24, "context": "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].", "startOffset": 66, "endOffset": 83}, {"referenceID": 33, "context": ", SGD, Momentum [36], Adagrad [9], Adam [22]), but to the best of our knowledge, there were no theoretical guarantees that such methods will find good weights.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": ", SGD, Momentum [36], Adagrad [9], Adam [22]), but to the best of our knowledge, there were no theoretical guarantees that such methods will find good weights.", "startOffset": 30, "endOffset": 33}, {"referenceID": 19, "context": ", SGD, Momentum [36], Adagrad [9], Adam [22]), but to the best of our knowledge, there were no theoretical guarantees that such methods will find good weights.", "startOffset": 40, "endOffset": 44}, {"referenceID": 34, "context": "For this basic network, it is known that even in the simplified setting where the weights are initialized symmetrically and the ground truth forms orthonormal basis, gradient descent might get stuck at saddle points [37].", "startOffset": 216, "endOffset": 220}, {"referenceID": 15, "context": "Inspired by the structure of residual network (ResNet) [18], we add an extra identity mapping for the hidden layer (see Figure 1).", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "Following the standard setting [30, 37], we assume that there exists a two-layer teacher network with weight W\u2217.", "startOffset": 31, "endOffset": 39}, {"referenceID": 34, "context": "Following the standard setting [30, 37], we assume that there exists a two-layer teacher network with weight W\u2217.", "startOffset": 31, "endOffset": 39}, {"referenceID": 7, "context": "Another common belief is that neural network has lots of local minima and saddle points [8], so even if there exists a global minimum, we may not be able to arrive there.", "startOffset": 88, "endOffset": 91}, {"referenceID": 16, "context": "For example, two-layer network with sigmoid activations could approximate any continuous function [19, 6, 3].", "startOffset": 98, "endOffset": 108}, {"referenceID": 5, "context": "For example, two-layer network with sigmoid activations could approximate any continuous function [19, 6, 3].", "startOffset": 98, "endOffset": 108}, {"referenceID": 2, "context": "For example, two-layer network with sigmoid activations could approximate any continuous function [19, 6, 3].", "startOffset": 98, "endOffset": 108}, {"referenceID": 23, "context": "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].", "startOffset": 88, "endOffset": 96}, {"referenceID": 11, "context": "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].", "startOffset": 88, "endOffset": 96}, {"referenceID": 22, "context": "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].", "startOffset": 182, "endOffset": 194}, {"referenceID": 25, "context": "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].", "startOffset": 182, "endOffset": 194}, {"referenceID": 24, "context": "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].", "startOffset": 182, "endOffset": 194}, {"referenceID": 32, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 62, "endOffset": 74}, {"referenceID": 21, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 62, "endOffset": 74}, {"referenceID": 31, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 62, "endOffset": 74}, {"referenceID": 17, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 131, "endOffset": 147}, {"referenceID": 30, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 131, "endOffset": 147}, {"referenceID": 12, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 131, "endOffset": 147}, {"referenceID": 0, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 189, "endOffset": 195}, {"referenceID": 1, "context": "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].", "startOffset": 189, "endOffset": 195}, {"referenceID": 28, "context": "[31] proved that with high probability, there exists a continuous decreasing path from random initial point to the global minimum, but SGD may not follow this path.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Some previous works simplified the model by ignoring the activation functions and considering deep linear networks [32, 21] or deep linear residual networks [16], which can only learn linear functions.", "startOffset": 115, "endOffset": 123}, {"referenceID": 18, "context": "Some previous works simplified the model by ignoring the activation functions and considering deep linear networks [32, 21] or deep linear residual networks [16], which can only learn linear functions.", "startOffset": 115, "endOffset": 123}, {"referenceID": 3, "context": "Some previous results are based on independent activation assumption that the activations of ReLU and the input are independent, which is unrealistic [4, 21].", "startOffset": 150, "endOffset": 157}, {"referenceID": 18, "context": "Some previous results are based on independent activation assumption that the activations of ReLU and the input are independent, which is unrealistic [4, 21].", "startOffset": 150, "endOffset": 157}, {"referenceID": 7, "context": "It is observed that saddle point is not a big problem for neural networks [8, 15].", "startOffset": 74, "endOffset": 81}, {"referenceID": 13, "context": "It is observed that saddle point is not a big problem for neural networks [8, 15].", "startOffset": 74, "endOffset": 81}, {"referenceID": 9, "context": "In general, if the objective is strict-saddle [10], SGD could escape all saddle points.", "startOffset": 46, "endOffset": 50}, {"referenceID": 34, "context": "1 (Eqn (13) from [37]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "Randomly initializing the weights with O(1/ \u221a d) is standard in deep learning, see [23, 11, 17].", "startOffset": 83, "endOffset": 95}, {"referenceID": 10, "context": "Randomly initializing the weights with O(1/ \u221a d) is standard in deep learning, see [23, 11, 17].", "startOffset": 83, "endOffset": 95}, {"referenceID": 14, "context": "Randomly initializing the weights with O(1/ \u221a d) is standard in deep learning, see [23, 11, 17].", "startOffset": 83, "endOffset": 95}, {"referenceID": 26, "context": "It is also well known that if the entries are initialized with O(1/ \u221a d), the spectral norm of the random matrix is O(1) [29].", "startOffset": 121, "endOffset": 125}, {"referenceID": 3, "context": "The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]).", "startOffset": 168, "endOffset": 179}, {"referenceID": 34, "context": "The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]).", "startOffset": 168, "endOffset": 179}, {"referenceID": 35, "context": "The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]).", "startOffset": 168, "endOffset": 179}, {"referenceID": 4, "context": "The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]).", "startOffset": 214, "endOffset": 217}, {"referenceID": 29, "context": "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.", "startOffset": 99, "endOffset": 111}, {"referenceID": 18, "context": "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.", "startOffset": 99, "endOffset": 111}, {"referenceID": 3, "context": "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.", "startOffset": 125, "endOffset": 132}, {"referenceID": 18, "context": "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.", "startOffset": 125, "endOffset": 132}, {"referenceID": 34, "context": "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.", "startOffset": 147, "endOffset": 151}, {"referenceID": 4, "context": "As pointed out by [5], eliminating the unrealistic assumptions on activation independence is the central problem of analyzing the loss surface of neural network, which was not fully addressed by the previous analyses.", "startOffset": 18, "endOffset": 21}, {"referenceID": 15, "context": "1 Importance of identity mapping In this experiment, we compare the standard ResNet [18] and single skip model where identity mapping skips only one layer.", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "References [1] Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Andrew R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Anna Choromanska, Mikael Henaff, Micha\u00ebl Mathieu, G\u00e9rard Ben Arous, and Yann LeCun.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Anna Choromanska, Yann LeCun, and G\u00e9rard Ben Arous.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] George Cybenko.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Amit Daniely, Roy Frostig, and Yoram Singer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] John C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Xavier Glorot and Yoshua Bengio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Xavier Glorot, Antoine Bordes, and Yoshua Bengio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Surbhi Goel, Varun Kanade, Adam R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Ian J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] Kurt Hornik, Maxwell B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] Kenji Kawaguchi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] Diederik P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] Yann LeCun, Leon Bottou, Genevieve B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] Guido F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] Vinod Nair and Geoffrey E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] Xingyuan Pan and Vivek Srikumar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[28] Razvan Pascanu, Guido Mont\u00fafar, and Yoshua Bengio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[30] David Saad and Sara A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] Itay Safran and Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[32] Andrew M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[33] Hanie Sedghi and Anima Anandkumar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[34] Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[35] Jir\u00ed S\u00edma.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[36] Ilya Sutskever, James Martens, George E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[37] Yuandong Tian.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[38] Bo Xie, Yingyu Liang, and Le Song.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing. In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called \u201cidentity mapping\u201d. We prove that, if input follows from Gaussian distribution, with standard O(1/ \u221a d) initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the \u201cidentity mapping\u201d makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks. Our convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in \u201ctwo phases\u201d: In phase I, the gradient points to the wrong direction, however, a potential function g gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims.", "creator": "LaTeX with hyperref package"}}}