{"id": "1705.10723", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Fast Regression with an $\\ell_\\infty$ Guarantee", "abstract": "sketching has benefited as a newer technique for speeding up problems in experimental linear algebra, such as regression. as randomly overconstrained regression problem, one is performing an $ n \\ times d $ matrix $ a $, with $ n \\ gg d $, as well as an $ n \\ times 1 $ \u03b3 $ b $, and one wants to find residual vector $ \\ hat { x } $ so as to minimize positive residual error $ \\ | true - algebra \\ | _ 2 $. using the sketch and solve paradigm, rm first computes $ s \\ cdot a $ and $ s \\ cdot b $ for another randomly chosen matrix $ s $, then outputs $ x'= ( sa ) ^ { \\ theta } e $ so as to minimize $ \\ | sax'- gamma \\ | _ _ $.", "histories": [["v1", "Tue, 30 May 2017 16:20:34 GMT  (129kb,D)", "http://arxiv.org/abs/1705.10723v1", "ICALP 2017"]], "COMMENTS": "ICALP 2017", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["eric price", "zhao song", "david p woodruff"], "accepted": false, "id": "1705.10723"}, "pdf": {"name": "1705.10723.pdf", "metadata": {"source": "CRF", "title": "Fast Regression with an `\u221e Guarantee\u2217", "authors": ["Eric Price", "Zhao Song", "David P. Woodruff"], "emails": ["zhaos@utexas.edu", "zhaos@utexas.edu", "dpwoodru@us.ibm.com"], "sections": [{"heading": null, "text": "The sketch-and-solve paradigm gives a bound on \u2016x\u2032 \u2212 x\u2217\u20162 when A is well-conditioned. Our main result is that, when S is the subsampled randomized Fourier/Hadamard transform, the error x\u2032 \u2212 x\u2217 behaves as if it lies in a \u201crandom\u201d direction within this bound: for any fixed direction a \u2208 Rd, we have with 1\u2212 d\u2212c probability that\n\u3008a, x\u2032 \u2212 x\u2217\u3009 . \u2016a\u20162\u2016x \u2032 \u2212 x\u2217\u20162\nd 1 2\u2212\u03b3\n, (1)\nwhere c, \u03b3 > 0 are arbitrary constants. This implies \u2016x\u2032 \u2212 x\u2217\u2016\u221e is a factor d 1 2\u2212\u03b3 smaller than \u2016x\u2032 \u2212 x\u2217\u20162. It also gives a better bound on the generalization of x\u2032 to new examples: if rows of A correspond to examples and columns to features, then our result gives a better bound for the error introduced by sketch-and-solve when classifying fresh examples. We show that not all oblivious subspace embeddings S satisfy these properties. In particular, we give counterexamples showing that matrices based on Count-Sketch or leverage score sampling do not satisfy these properties.\nWe also provide lower bounds, both on how small \u2016x\u2032 \u2212 x\u2217\u20162 can be, and for our new guarantee (1), showing that the subsampled randomized Fourier/Hadamard transform is nearly optimal. Our lower bound on \u2016x\u2032 \u2212 x\u2217\u20162 shows that there is an O(1/\u03b5) separation in the dimension of the optimal oblivious subspace embedding required for outputting an x\u2032 for which \u2016x\u2032\u2212 x\u2217\u20162 \u2264 \u2016Ax\u2217\u2212 b\u20162 \u00b7 \u2016A\u2020\u20162, compared to the dimension of the optimal oblivious subspace embedding required for outputting an x\u2032 for which \u2016Ax\u2032 \u2212 b\u20162 \u2264 (1 + )\u2016Ax\u2217 \u2212 b\u20162, that is, the former problem requires dimension \u2126(d/ 2) while the latter problem can be solved with dimension O(d/ ). This explains the reason known upper bounds on the dimensions of these two variants of regression have differed in prior work.\n\u2217A preliminary version of this paper appears in Proceedings of the 44th International Colloquium on Automata, Languages, and Programming (ICALP 2017).\nar X\niv :1\n70 5.\n10 72\n3v 1\n[ cs\n.D S]\n3 0\nM ay"}, {"heading": "1 Introduction", "text": "Oblivious subspace embeddings (OSEs) were introduced by Sarlos [Sar06] to solve linear algebra problems more quickly than traditional methods. An OSE is a distribution of matrices S \u2208 Rm\u00d7n with m n such that, for any d-dimensional subspace U \u2282 Rn, with \u201chigh\u201d probability S preserves the norm of every vector in the subspace. OSEs are a generalization of the classic JohnsonLindenstrauss lemma from vectors to subspaces. Formally, we require that with probability 1\u2212 \u03b4,\n\u2016Sx\u20162 = (1\u00b1 \u03b5)\u2016x\u20162\nsimultaneously for all x \u2208 U , that is, (1\u2212 \u03b5)\u2016x\u20162 \u2264 \u2016Sx\u20162 \u2264 (1 + \u03b5)\u2016x\u20162. A major application of OSEs is to regression. The regression problem is, given b \u2208 Rn and A \u2208 Rn\u00d7d for n \u2265 d, to solve for\nx\u2217 = arg min x\u2208Rd \u2016Ax\u2212 b\u20162 (2)\nBecause A is a \u201ctall\u201d matrix with more rows than columns, the system is overdetermined and there is likely no solution to Ax = b, but regression will find the closest point to b in the space spanned by A. The classic answer to regression is to use the Moore-Penrose pseudoinverse: x\u2217 = A\u2020b where\nA\u2020 = (A>A)\u22121A>\nis the \u201cpseudoinverse\u201d of A (assuming A has full column rank, which we will typically do for simplicity). This classic solution takes O(nd\u03c9\u22121 + d\u03c9) time, where \u03c9 < 2.373 is the matrix multiplication constant [CW90, Wil12, Gal14]: nd\u03c9\u22121 time to compute A>A and d\u03c9 time to compute the inverse.\nOSEs speed up the process by replacing (2) with\nx\u2032 = arg min x\u2208Rd \u2016SAx\u2212 Sb\u20162\nfor an OSE S on d + 1-dimensional spaces. This replaces the n \u00d7 d regression problem with an m \u00d7 d problem, which can be solved more quickly since m n. Because Ax \u2212 b lies in the d + 1- dimensional space spanned by b and the columns of A, with high probability S preserves the norm of SAx\u2212 Sb to 1\u00b1 \u03b5 for all x. Thus,\n\u2016Ax\u2032 \u2212 b\u20162 \u2264 1 + \u03b5 1\u2212 \u03b5\u2016Ax \u2217 \u2212 b\u20162.\nThat is, S produces a solution x\u2032 which preserves the cost of the regression problem. The running time for this method depends on (1) the reduced dimension m and (2) the time it takes to multiply S by A. We can compute these for \u201cstandard\u201d OSE types:\n\u2022 If S has i.i.d. Gaussian entries, then m = O(d/\u03b52) is sufficient (and in fact, m \u2265 d/ 2 is required [NN14]). However, computing SA takes O(mnd) = O(nd2/\u03b52) time, which is worse than solving the original regression problem (one can speed this up using fast matrix multiplication, though it is still worse than solving the original problem).\n\u2022 If S is a subsampled randomized Hadamard transform (SRHT) matrix with random sign flips (see Theorem 2.4 in [Woo14] for a survey, and also see [CNW16] which gives a recent improvement) then m increases to O\u0303(d/\u03b52 \u00b7 log n), where O\u0303(f) = fpoly(log(f)). But now, we can compute SA using the fast Hadamard transform in O(nd log n) time. This makes the overall regression problem take O(nd log n+ d\u03c9/\u03b52) time.\n\u2022 If S is a random sparse matrix with random signs (the \u201cCount-Sketch\u201d matrix), then m = d1+\u03b3/\u03b52 suffices for \u03b3 > 0 a decreasing function of the sparsity [CW13, MM13, NN13, BDN15, Coh16]. (The definition of a Count-Sketch matrix is, for any s \u2265 1, Si,j \u2208 {0,\u22121/ \u221a s, 1/ \u221a s},\n\u2200i \u2208 [m], j \u2208 [n] and the column sparsity of matrix S is s. Independently in each column s positions are chosen uniformly at random without replacement, and each chosen position is set to \u22121/\u221as with probability 1/2, and +1/\u221as with probability 1/2.) Sparse OSEs can benefit from the sparsity of A, allowing for a running time of O\u0303(nnz(A)) + O\u0303(d\u03c9/\u03b52), where nnz(A) denotes the number of non-zeros in A.\nWhen n is large, the latter two algorithms are substantially faster than the na\u00efve nd\u03c9\u22121 method."}, {"heading": "1.1 Our Contributions", "text": "Despite the success of using subspace embeddings to speed up regression, often what practitioners are interested is not in preserving the cost of the regression problem, but rather in the generalization or prediction error provided by the vector x\u2032. Ideally, we would like for any future (unseen) example a \u2208 Rd, that \u3008a, x\u2032\u3009 \u2248 \u3008a, x\u2217\u3009 with high probability.\nUltimately one may want to use x\u2032 to do classification, such as regularized least squares classification (RLSC) [RYP03], which has been found in cases to do as well as support vector machines but is much simpler [ZP04]. In this application, given a training set of examples with multiple (non-binary) labels identified with the rows of an n \u00d7 d matrix A, one creates an n \u00d7 r matrix B, each column indicating the presence or absence of one of the r possible labels in each example. One then solves the multiple response regression problem minX \u2016AX \u2212B\u2016F , and uses X to classify future examples. A commonly used method is for a future example a, to compute \u3008a, x1\u3009, . . . , \u3008a, xr\u3009, where x1, . . . , xr are the columns of X. One then chooses the label i for which \u3008a, xi\u3009 is maximum.\nFor this to work, we would like the inner products \u3008a, x\u20321\u3009, . . . , \u3008a, x\u2032r\u3009 to be close to \u3008a, x\u22171\u3009, . . ., \u3008a, x\u2217r\u3009, where X \u2032 is the solution to minX \u2016SAX \u2212 SB\u2016F and X\u2217 is the solution to minX \u2016AX \u2212 B\u2016F . For any O(1)-accurate OSE on d+ r dimensional spaces [Sar06], which also satisfies so-called approximate matrix multiplication with error \u03b5\u2032 = \u03b5/ \u221a (d+ r), we get that\n\u2016x\u2032 \u2212 x\u2217\u20162 \u2264 O(\u03b5) \u00b7 \u2016Ax\u2217 \u2212 b\u20162 \u00b7 \u2016A\u2020\u20162 (3)\nwhere \u2016A\u2020\u2016 is the spectral norm of A\u2020, which equals the reciprocal of the smallest singular value of A. To obtain a generalization error bound for an unseen example a, one has\n|\u3008a, x\u2217\u3009 \u2212 \u3008a, x\u2032\u3009| = |\u3008a, x\u2217 \u2212 x\u2032\u3009| \u2264 \u2016x\u2217 \u2212 x\u2032\u20162\u2016a\u20162 = O(\u03b5)\u2016a\u20162\u2016Ax\u2217 \u2212 b\u20162\u2016A\u2020\u20162, (4)\nwhich could be tight if given only the guarantee in (3). However, if the difference vector x\u2032 \u2212 x\u2217 were distributed in a uniformly random direction subject to (3), then one would expect an O\u0303( \u221a d) factor improvement in the bound. This is what our main theorem shows:\nTheorem 1 (Main Theorem, informal). Suppose n \u2264 poly(d) and matrix A \u2208 Rn\u00d7d and vector b \u2208 Rn are given. Let S \u2208 Rm\u00d7n be a subsampled randomized Hadamard transform matrix with m = d1+\u03b3/\u03b52 rows for an arbitrarily small constant \u03b3 > 0. For x\u2032 = arg minx\u2208Rd \u2016SAx\u2212 Sb\u20162 and x\u2217 = arg minx\u2208Rd \u2016Ax\u2212 b\u20162, and any fixed a \u2208 Rd,\n|\u3008a, x\u2217\u3009 \u2212 \u3008a, x\u2032\u3009| \u2264 \u03b5\u221a d \u2016a\u20162\u2016Ax\u2217 \u2212 b\u20162\u2016A\u2020\u20162. (5)\nwith probability 1\u2212 1/dC for an arbitrarily large constant C > 0. This implies that\n\u2016x\u2217 \u2212 x\u2032\u2016\u221e \u2264 \u03b5\u221a d \u2016Ax\u2217 \u2212 b\u20162\u2016A\u2020\u20162. (6)\nwith 1\u2212 1/dC\u22121 probability. If n > poly(d), then by first composing S with a Count-Sketch OSE with poly(d) rows, one can achieve the same guarantee.\n(Here \u03b3 is a constant going to zero as n increases; see Theorem 10 for a formal statement of Theorem 1.)\nNotice that Theorem 1 is considerably stronger than that of (4) provided by existing guarantees. Indeed, in order to achieve the guarantee (6) in Theorem 1, one would need to set \u03b5\u2032 = \u03b5/ \u221a d in existing OSEs, resulting in \u2126(d2/ 2) rows. In contrast, we achieve only d1+\u03b3/ 2 rows. We can improve the bound in Theorem 1 to m = d/\u03b52 if S is a matrix of i.i.d. Gaussians; however, as noted, computing S \u00b7A is slower in this case.\nNote that Theorem 1 also makes no distributional assumptions on the data, and thus the data could be heavy-tailed or even adversarially corrupted. This implies that our bound is still useful when the rows of A are not sampled independently from a distribution with bounded variance.\nThe `\u221e bound (6) of Theorem 1 is achieved by applying (5) to the standard basis vectors a = ei for each i \u2208 [d] and applying a union bound. This `\u221e guarantee often has a more natural interpretation than the `2 guarantee\u2014if we think of the regression as attributing the observable as a sum of various factors, (6) says that the contribution of each factor is estimated well. One may also see our contribution as giving a way for estimating the pseudoinverse A\u2020 entrywise. Namely,\nwe get that (SA)\u2020S \u2248 A\u2020 in the sense that each entry is within additive O(\u03b5 \u221a\nlog d d \u2016A\u2020\u20162). There is\na lot of work on computing entries of inverses of a matrix, see, e.g., [ADL+12, LAKD08]. Another benefit of the `\u221e guarantee is when the regression vector x\u2217 is expected to be ksparse (e.g. [Lee12]). In such cases, thresholding to the top k entries will yield an `2 guarantee a\nfactor \u221a\nk d better than (3).\nOne could ask if Theorem 1 also holds for sparse OSEs, such as the Count-Sketch. Surprisingly, we show that one cannot achieve the generalization error guarantee in Theorem 1 with high probability, say, 1\u2212 1/d, using such embeddings, despite the fact that such embeddings do approximate the cost of the regression problem up to a 1 + factor with high probability. This shows that the generalization error guarantee is achieved by some subspace embeddings but not all.\nTheorem 2 (Not all subspace embeddings give the `\u221e guarantee; informal version of Theorem 20). The Count-Sketch matrix with d1.5 rows and sparsity d.25\u2014which is an OSE with exponentially small failure probability\u2014with constant probability will have a result x\u2032 that does not satisfy the `\u221e guarantee (6).\nWe can show that Theorem 1 holds for S based on the Count-Sketch OSE T with dO(C)/ 2 rows with 1 \u2212 1/dC probability. We can thus compose the Count-Sketch OSE with the SRHT matrix and obtain an O(nnz(A)) + poly(d/ ) time algorithm to compute S \u00b7TA achieving (6). We can also compute R \u00b7 S \u00b7 T \u00b7A, where R is a matrix of Gaussians, which is more efficient now that STA only has d1+\u03b3/ 2 rows; this will reduce the number of rows to d/ 2.\nAnother common method of dimensionality reduction for linear regression is leverage score sampling [DMIMW12, LMP13, PKB14, CMM15], which subsamples the rows of A by choosing each row with probability proportional to its \u201cleverage scores\u201d. With O(d log(d/\u03b4)/\u03b52) rows taken, the result x\u2032 will satisfy the `2 bound (3) with probability 1\u2212 \u03b4. However, it does not give a good `\u221e bound:\nTheorem 3 (Leverage score sampling does not give the `\u221e guarantee; informal version of Theorem 23). Leverage score sampling with d1.5 rows\u2014which satisfies the `2 bound with exponentially\nsmall failure probability\u2014with constant probability will have a result x\u2032 that does not satisfy the `\u221e guarantee (6).\nFinally, we show that the d1+\u03b3/\u03b52 rows that SRHT matrices use is roughly optimal:\nTheorem 4 (Lower bounds for `2 and `\u221e guarantees; informal versions of of Theorem 14 and Corollary 18). Any sketching matrix distribution over m \u00d7 n matrices that satisfies either the `2 guarantee (3) or the `\u221e guarantee (6) must have m & min(n, d/\u03b52).\nNotice that our result shows the necessity of the 1/\u03b5 separation between the results originally defined in Equation (3) and (4) of Theorem 12 of [Sar06]. If we want to output some vector x\u2032 such that \u2016Ax\u2032 \u2212 b\u20162 \u2264 (1 + \u03b5)\u2016Ax\u2217 \u2212 b\u20162, then it is known that m = \u0398(d/\u03b5) is necessary and sufficient. However, if we want to output a vector x\u2032 such that \u2016x\u2032\u2212x\u2217\u20162 \u2264 \u03b5\u2016Ax\u2217\u2212 b\u20162 \u00b7 \u2016A\u2020\u20162, then we show that m = \u0398(d/\u03b52) is necessary and sufficient."}, {"heading": "1.1.1 Comparison to Gradient Descent", "text": "While this work is primarily about sketching methods, one could instead apply iterative methods such as gradient descent, after appropriately preconditioning the matrix, see, e.g., [AMT10, ZF13, CW13]. That is, one can use an OSE with constant \u03b5 to construct a preconditioner for A and then run conjugate gradient using the preconditioner. This gives an overall dependence of log(1/ ).\nThe main drawback of this approach is that one loses the ability to save on storage space or number of passes when A appears in a stream, or to save on communication or rounds when A is distributed. Given increasingly large data sets, such scenarios are now quite common, see, e.g., [CW09] for regression algorithms in the data stream model. In situations where the entries of A appear sequentially, for example, a row at a time, one does not need to store the full n\u00d7 d matrix A but only the m\u00d7 d matrix SA.\nAlso, iterative methods can be less efficient when solving multiple response regression, where one wants to minimize \u2016AX \u2212 B\u2016 for a d \u00d7 t matrix X and an n \u00d7 t matrix B. This is the case when \u03b5 is constant and t is large, which can occur in some applications (though there are also other applications for which \u03b5 is very small). For example, conjugate gradient with a preconditioner will take O\u0303(ndt) time while using an OSE directly will take only O\u0303(nd+ d2t) time (since one effectively replaces n with O (d) after computing S \u00b7 A), separating t from d. Multiple response regression, arises, for example, in the RLSC application above."}, {"heading": "1.1.2 Proof Techniques", "text": "Theorem 1. As noted in Theorem 2, there are some OSEs for which our generalization error bound does not hold. This hints that our analysis is non-standard and cannot use generic properties of OSEs as a black box. Indeed, in our analysis, we have to consider matrix products of the form S>S(UU>S>S)k for our random sketching matrix S and a fixed matrix U , where k is a positive integer. We stress that it is the same matrix S appearing multiple times in this expression, which considerably complicates the analysis, and does not allow us to appeal to standard results on approximate matrix product (see, e.g., [Woo14] for a survey). The key idea is to recursively reduce S>S(UU>S>S)k using a property of S. We use properties that only hold for specifics OSEs S: first, that each column of S is unit vector; and second, that for all pairs (i, j) and i 6= j, the inner product between Si and Sj is at most \u221a logn\u221a m with probability 1\u2212 1/poly(n).\nTheorems 20 and 23. To show that Count-Sketch does not give the `\u221e guarantee, we construct a matrix A and vector b as in Figure 1, which has optimal solution x\u2217 with all coordinates\n1/ \u221a d. We then show, for our setting of parameters, that there likely exists an index j \u2208 [d] satisfying the following property: the jth column of S has disjoint support from the kth column of S for all k \u2208 [d+ \u03b1] \\ {j} except for a single k > d, for which Sj and Sk share exactly one common entry in their support. In such cases we can compute x\u2032j explicitly, getting |x\u2032j \u2212 x\u2217j | = 1s\u221a\u03b1 . By choosing suitable parameters in our construction, this gives that \u2016x\u2032 \u2212 x\u2217\u2016\u221e 1\u221ad . The lower bound for leverage score sampling follows a similar construction.\nTheorem 14 and Corollary 18. The lower bound proof for the `2 guarantee uses Yao\u2019s minimax principle. We are allowed to fix an m \u00d7 n sketching matrix S and design a distribution over [A b]. We first write the sketching matrix S = U\u03a3V > in its singular value decomposition (SVD). We choose the d+ 1 columns of the adjoined matrix [A, b] to be random orthonormal vectors. Consider an n \u00d7 n orthonormal matrix R which contains the columns of V as its first m columns, and is completed on its remaining n\u2212m columns to an arbitrary orthonormal basis. Then S \u00b7 [A, b] = V >RR> \u00b7 [A, b] = [U\u03a3Im, 0] \u00b7 [R>A,R>b]. Notice that [R>A,R>b] is equal in distribution to [A, b], since R is fixed and [A, b] is a random matrix with d+ 1 orthonormal columns. Therefore, S \u00b7 [A, b] is equal in distribution to [U\u03a3G,U\u03a3h] where [G, h] corresponds to the first m rows of an n\u00d7 (d+ 1) uniformly random matrix with orthonormal columns.\nA key idea is that if n = \u2126(max(m, d)2), then by a result of Jiang [J+06], any m \u00d7 (d + 1) submatrix of a random n\u00d7n orthonormal matrix has o(1) total variation distance to a d\u00d7d matrix of i.i.d. N(0, 1/n) random variables, and so any events that would have occurred had G and h been independent i.i.d. Gaussians, occur with the same probability for our distribution up to an 1\u2212 o(1) factor, so we can assume G and h are independent i.i.d. Gaussians in the analysis.\nThe optimal solution x\u2032 in the sketch space equals (SA)\u2020Sb, and by using that SA has the form U\u03a3G, one can manipulate \u2016(SA)\u2020Sb\u2016 to be of the form \u2016\u03a3\u0303\u2020(\u03a3R)\u2020\u03a3h\u20162, where the SVD of G is R\u03a3\u0303T . We can upper bound \u2016\u03a3\u0303\u20162 by \u221a r/n, since it is just the maximum singular value of a Gaussian ma-\ntrix, where r is the rank of S, which allows us to lower bound \u2016\u03a3\u0303\u2020(\u03a3R)\u2020\u03a3h\u20162 by \u221a n/r\u2016(\u03a3R)\u2020\u03a3h\u20162. Then, since h is i.i.d. Gaussian, this quantity concentrates to 1\u221a r \u2016(\u03a3R)\u2020\u03a3h\u2016, since \u2016Ch\u20162 \u2248 \u2016C\u20162F /n for a vector h of i.i.d. N(0, 1/n) random variables. Finally, we can lower bound \u2016(\u03a3R)\u2020\u03a3\u20162F by \u2016(\u03a3R)\u2020\u03a3RR>\u20162F by the Pythagorean theorem, and now we have that (\u03a3R)\u2020\u03a3R is the identity, and so this expression is just equal to the rank of \u03a3R, which we prove is at least d. Noting that x\u2217 = 0 for our instance, putting these bounds together gives \u2016x\u2032 \u2212 x\u2217\u2016 \u2265 \u221a d/r. The last ingredient is a way to ensure that the rank of S is at least d. Here we choose another distribution on inputs A and b for which it is trivial to show the rank of S is at least d with large probability. We require S be good on the mixture. Since S is fixed and good on the mixture, it is good for both distributions individually, which implies we can assume S has rank d in our analysis of the first distribution above."}, {"heading": "1.2 Notation", "text": "For a positive integer, let [n] = {1, 2, . . . , n}. For a vector x \u2208 Rn, define \u2016x\u20162 = ( \u2211n i=1 x 2 i ) 1 2 and \u2016x\u2016\u221e = maxi\u2208[n] |xi|. For a matrix A \u2208 Rm\u00d7n, define \u2016A\u20162 = supx \u2016Ax\u20162/\u2016x\u20162 to be the spectral norm of A and \u2016A\u2016F = ( \u2211 i,j A 2 i,j)\n1/2 to be the Frobenius norm of A. We use A\u2020 to denote the Moore-Penrose pseudoinverse of m\u00d7n matrix A, which if A = U\u03a3V > is its SVD (where U \u2208 Rm\u00d7n, \u03a3 \u2208 Rn\u00d7n and V \u2208 Rn\u00d7 for m \u2265 n), is given by A\u2020 = V \u03a3\u22121U>.\nIn addition to O(\u00b7) notation, for two functions f, g, we use the shorthand f . g (resp. &) to indicate that f \u2264 Cg (resp. \u2265) for an absolute constant C. We use f h g to mean cf \u2264 g \u2264 Cf for constants c, C.\nDefinition 5 (Subspace Embedding). A (1\u00b1 ) `2-subspace embedding for the column space of an n\u00d7 d matrix A is a matrix S for which for all x \u2208 Rd, \u2016SAx\u201622 = (1\u00b1 )\u2016Ax\u201622. Definition 6 (Approximate Matrix Product). Let 0 < < 1 be a given approximation parameter. Given matrices A and B, where A and B each have n rows, the goal is to output a matrix C so that \u2016A>B \u2212 C\u2016F \u2264 \u2016A\u2016F \u2016B\u2016F . Typically C has the form A>S>SB, for a random matrix S with a small number of rows. In particular, this guarantee holds for the subsampled randomized Hadamard transform S with O( \u22122) rows [DMMS11]."}, {"heading": "2 Warmup: Gaussians OSEs", "text": "We first show that if S is a Gaussian random matrix, then it satisfies the generalization guarantee. This follows from the rotational invariance of the Gaussian distribution.\nTheorem 7. Suppose A \u2208 Rn\u00d7d has full column rank. If the entries of S \u2208 Rm\u00d7n are i.i.d. N(0, 1/m), m = O(d/\u03b52), then for any vectors a, b and x\u2217 = A\u2020b, we have, with probability 1 \u2212 1/poly(d),\n|a>(SA)\u2020Sb\u2212 a>x\u2217| . \u03b5 \u221a\nlog d\u221a d \u2016a\u20162\u2016b\u2212Ax\u2217\u20162\u2016A\u2020\u20162.\nBecause SA has full column rank with probability 1, (SA)\u2020SA = I. Therefore\n|a>(SA)\u2020Sb\u2212 a>x\u2217| = |a>(SA)\u2020S(b\u2212Ax\u2217)| = |a>(SA)\u2020S(b\u2212AA\u2020b)|.\nThus it suffices to only consider vectors b where A\u2020b = 0, or equivalently U>b = 0. In such cases, SU will be independent of Sb, which will give the result. The proof is in Appendix A."}, {"heading": "3 SRHT Matrices", "text": "We first provide the definition of the subsampled randomized Hadamard transform(SRHT): let S = 1\u221a\nrn PHnD. Here, D is an n\u00d7n diagonal matrix with i.i.d. diagonal entries Di,i, for which Di,i\nin uniform on {\u22121,+1}. The matrix Hn is the Hadamard matrix of size n\u00d7 n, and we assume n is a power of 2. Here, Hn = [Hn/2, Hn/2;Hn/2, \u2212Hn/2] and H1 = [1]. The r\u00d7 n matrix P samples r coordinates of an n dimensional vector uniformly at random.\nFor other subspace embeddings, we no longer have that SU and Sb are independent. To analyze them, we start with a claim that allows us to relate the inverse of a matrix to a power series.\nClaim 8. Let S \u2208 Rm\u00d7n, A \u2208 Rn\u00d7d have SVD A = U\u03a3V >, and define T \u2208 Rd\u00d7d by\nT = Id \u2212 U>S>SU.\nSuppose SA has linearly independent columns and \u2016T\u20162 \u2264 1/2. Then\n(SA)\u2020S = V \u03a3\u22121 ( \u221e\u2211 k=0 T k ) U>S>S. (7)\nProof.\n(SA)\u2020S = (A>S>SA)\u22121A>S>S\n= (V \u03a3U>S>SU\u03a3V >)\u22121V \u03a3U>S>S\n= V \u03a3\u22121(U>S>SU)\u22121U>S>S\n= V \u03a3\u22121(Id \u2212 T )\u22121U>S>S\n= V \u03a3\u22121 ( \u221e\u2211 k=0 T k ) U>S>S,\nwhere in the last equality, since \u2016T\u20162 < 1, the von Neumann series \u2211\u221e k=0 T k converges to\n(Id \u2212 T )\u22121.\nWe then bound the kth term of this sum:\nLemma 9. Let S \u2208 Rr\u00d7n be the subsampled randomized Hadamard transform, and let a be a unit vector. Then with probability 1\u2212 1/poly(n), we have\n|a>S>S(UU>S>S)kb| =O(logk n) \u00b7 (O(d(log n)/r) + 1) k\u221212 \u00b7 ( \u221a d\u2016b\u20162(log n)/r + \u2016b\u20162(log 1 2 n)/r 1 2 )\nHence, for r at least d log2k+2 n log2(n/\u03b5)/\u03b52, this is at most O(\u2016b\u20162\u03b5/ \u221a d) with probability at least 1\u2212 1/poly(n).\nWe defer the proof of this lemma to the next section, and now show how the lemma lets us prove that SRHT matrices satisfy the generalization bound with high probability:\nTheorem 10. Suppose A \u2208 Rn\u00d7d has full column rank with log n = do(1). Let S \u2208 Rm\u00d7n be a subsampled randomized Hadamard transform with m = O(d1+\u03b1/\u03b52) for \u03b1 = \u0398( \u221a log logn\nlog d ). For any\nvectors a, b and x\u2217 = A\u2020b, we have\n|a>(SA)\u2020Sb\u2212 a>x\u2217| . \u03b5\u221a d \u2016a\u20162\u2016b\u2212Ax\u2217\u20162\u2016\u03a3\u22121\u20162\nwith probability 1\u2212 1/poly(d). Proof. Define \u2206 = \u0398 (\n1\u221a m\n) (logc d)\u2016a\u20162\u2016b\u2212Ax\u2217\u20162\u2016\u03a3\u22121\u20162. For a constant c > 0, we have that S is\na (1\u00b1 \u03b3)-subspace embedding (Definition 5) for \u03b3 = \u221a\nd logc n m with probability 1\u2212 1/poly(d) (see,\ne.g., Theorem 2.4 of [Woo14] and references therein), so \u2016SUx\u20162 = (1 \u00b1 \u03b3)\u2016Ux\u20162 for all x, which we condition on. Hence for T = Id \u2212 U>S>SU , we have \u2016T\u20162 \u2264 (1 + \u03b3)2 \u2212 1 . \u03b3. In particular, \u2016T\u20162 < 1/2 and we can apply Claim 8.\nAs in Section 2, SA has full column rank if S is a subspace embedding, so (SA)\u2020SA = I and we may assume x\u2217 = 0 without loss of generality.\nBy the approximate matrix product (Definition 6), we have for some c that\n|a>V \u03a3\u22121U>S>Sb| \u2264 log c d\u221a m \u2016a\u20162\u2016b\u20162\u2016\u03a3\u22121\u20162 \u2264 \u2206 (8)\nwith 1\u2212 1/poly(d) probability. Suppose this event occurs, bounding the k = 0 term of (7). Hence it suffices to show that the k \u2265 1 terms of (7) are bounded by \u2206.\nBy approximate matrix product (Definition 6), we also have with 1\u2212 1/d2 probability that\n\u2016U>S>Sb\u2016F \u2264 logc d\u221a m \u2016U>\u2016F \u2016b\u20162 \u2264\nlogc d \u221a d\u221a\nm \u2016b\u20162.\nCombining with \u2016T\u20162 . \u03b3 we have for any k that\n|a>V \u03a3\u22121T kU>S>Sb| . \u03b3k(logc d) \u221a d\u221a m \u2016a\u20162\u2016\u03a3\u22121\u20162\u2016b\u20162.\nSince this decays exponentially in k at a rate of \u03b3 < 1/2, the sum of all terms greater than k is bounded by the kth term. As long as\nm & 1\n\u03b52 d1+\n1 k logc n, (9)\nwe have \u03b3 = \u221a\nd logc n m < \u03b5d \u22121/(2k)/ logc n, so that\u2211 k\u2032\u2265k |a>V \u03a3\u22121T k\u2032U>S>Sb| . \u03b5\u221a d \u2016a\u20162\u2016\u03a3\u22121\u20162\u2016b\u20162.\nOn the other hand, by Lemma 9, increasing m by a Ck factor, we have for all k that\n|a>V >\u03a3\u22121U>S>S(UU>S>S)kb| . 1 2k \u03b5\u221a d \u2016a\u20162\u2016b\u20162\u2016\u03a3\u22121\u20162\nwith probability at least 1\u2212 1/poly(d), as long as m & d log2k+2 n log2(d/\u03b5)/\u03b52. Since the T k term can be expanded as a sum of 2k terms of this form, we get that\nk\u2211 k\u2032=1 |a>V \u03a3\u22121T kU>S>Sb| . \u03b5\u221a d \u2016a\u20162\u2016b\u20162\u2016\u03a3\u22121\u20162\nwith probability at least 1\u2212 1/poly(d), as long as m & d(C log n)2k+2 log2(d/\u03b5)/\u03b52 for a sufficiently large constant C. Combining with (9), the result holds as long as\nm & d logc n\n\u03b52 max((C log n)2k+2, d\n1 k )\nfor any k. Setting k = \u0398( \u221a\nlog d log logn) gives the result.\nCombining Different Matrices. In some cases it can make sense to combine different matrices that satisfy the generalization bound.\nTheorem 11. Let A \u2208 Rn\u00d7d, and let R \u2208 Rm\u00d7r and S \u2208 Rr\u00d7n be drawn from distributions of matrices that are \u03b5-approximate OSEs and satisfy the generalization bound (6). Then RS satisfies the generalization bound with a constant factor loss in failure probability and approximation factor.\nWe defer the details to Appendix B."}, {"heading": "4 Proof of Lemma 9", "text": "Proof. Each column Si of the subsampled randomized Hadamard transform has the same distribu-\ntion as \u03c3iSi, where \u03c3i is a random sign. It also has \u3008Si, Si\u3009 = 1 for all i and | \u3008Si, Sj\u3009 | . \u221a log(1/\u03b4)\u221a r with probability 1\u2212 \u03b4, for any \u03b4 and i 6= j. See, e.g., [LDFU13]. By expanding the following product into a sum, and rearranging terms, we obtain\na>S>S(UU>S>S)kb = \u2211\ni0,j0,i1,j1,\u00b7\u00b7\u00b7 ,ik,jk\nai0bjk\u03c3i0\u03c3i1 \u00b7 \u00b7 \u00b7\u03c3ik\u03c3j0\u03c3j1 \u00b7 \u00b7 \u00b7\u03c3jk\n\u00b7\u3008Si0 , Sj0\u3009(UU>)j0,i1\u3008Si1 , Sj1\u3009 \u00b7 \u00b7 \u00b7 (UU>)jk\u22121,ik\u3008Sik , Sjk\u3009 = \u2211 i0,jk ai0bjk\u03c3i0\u03c3jk \u2211 j0,i1,j1,\u00b7\u00b7\u00b7 ,ik \u03c3i1 \u00b7 \u00b7 \u00b7\u03c3ik\u03c3j0\u03c3j1 \u00b7 \u00b7 \u00b7\u03c3jk\u22121\n\u00b7 \u3008Si0 , Sj0\u3009(UU>)j0,i1\u3008Si1 , Sj1\u3009 \u00b7 \u00b7 \u00b7 (UU>)jk\u22121,ik\u3008Sik , Sjk\u3009 = \u2211 i0,jk \u03c3i0\u03c3jkZi0,jk\nwhere Zi0,jk is defined to be\nZi0,jk = ai0bjk \u2211 i1,\u00b7\u00b7\u00b7ik j0,\u00b7\u00b7\u00b7jk\u22121 k\u220f c=1 \u03c3ic k\u22121\u220f c=0 \u03c3jc \u00b7 k\u220f c=0 \u3008Sic , Sjc\u3009 k\u220f c=1 (UU>)ic\u22121,jc\nNote that Zi0,jk is independent of \u03c3i0 and \u03c3jk . We observe that in the above expression if i0 = j0, i1 = j1, \u00b7 \u00b7 \u00b7 , ik = jk, then the sum over these indices equals a>(UU>) \u00b7 \u00b7 \u00b7 (UU>)b = 0, since\n\u3008Sic , Sjc\u3009 = 1 in this case for all c. Moreover, the sum over all indices conditioned on ik = jk is equal to 0. Indeed, in this case, the expression can be factored into the form \u03b6 \u00b7 U>b, for some random variable \u03b6, but U>b = 0.\nLet W be a matrix with Wi,j = \u03c3i\u03c3jZi,j . We need Khintchine\u2019s inequality:\nFact 12 (Khintchine\u2019s Inequality). Let \u03c31, . . . , \u03c3n be i.i.d. sign random variables, and let z1, . . . , zn be real numbers. Then there are constants C,C \u2032 > 0 so that\nPr [\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 zi\u03c3i \u2223\u2223\u2223\u2223\u2223 \u2265 Ct\u2016z\u20162 ] \u2264 e\u2212C\u2032t2 .\nWe note that Khintchine\u2019s inequality sometimes refers to bounds on the moment of |\u2211i zi\u03c3i|, though the above inequality follows readily by applying a Markov bound to the high moments.\nWe apply Fact 12 to each column ofW , so that ifWi is the i-th column, we have by a union bound that with probability 1\u2212 1/poly(n), \u2016Wi\u20162 = O(\u2016Zi\u20162 \u221a log n) simultaneously for all columns i. It\nfollows that with the same probability, \u2016W\u20162F = O(\u2016Z\u20162F log n), that is, \u2016W\u2016F = O(\u2016Z\u2016F \u221a\nlog n). We condition on this event in the remainder.\nThus, it remains to bound \u2016Z\u2016F . By squaring Zi0,j0 and using that E[\u03c3i\u03c3j ] = 1 if i = j and 0 otherwise, we have,\nE \u03c3\n[Z2i0,jk ] = a 2 i0b 2 jk \u2211 i1,\u00b7\u00b7\u00b7ik j0,\u00b7\u00b7\u00b7jk\u22121 k\u220f c=0 \u3008Sic , Sjc\u30092 k\u220f c=1 (UU>)2ic\u22121,jc (10)\nWe defer to Appendix E the proof that\nE S\n[\u2016Z\u20162F ] \u2264 (O(d(log n)/r) + 1)k\u22121 \u00b7 (d\u2016b\u201622(log2 n)/r2 + \u2016b\u201622(log n)/r)\nNote that we also have the bound:\n(O(d(log n)/r) + 1)k\u22121 \u2264 (eO(d(logn)/r))k\u22121 \u2264 eO(kd(logn)/r) \u2264 O(1)\nfor any r = \u2126(kd log n). Having computed the expectation of \u2016Z\u20162F , we now would like to show concentration. Consider\na specific\nZi0,jk = ai0bjk \u2211 ik \u03c3ik\u3008Sik , Sjk\u3009 \u00b7 \u00b7 \u00b7 \u2211 j1 \u03c3j1(UU >)j1,i2 \u2211 i1 \u03c3i1\u3008Si1 , Sj1\u3009 \u2211 j0 \u03c3j0\u3008Si0 , Sj0\u3009(UU>)j0,i1 .\nBy Fact 12, for each fixing of i1, with probability 1\u2212 1/poly(n), we have\n\u2211 j0 \u03c3j0\u3008Si0 , Sj0\u3009(UU>)j0,i1 = O( \u221a log n) \u2211 j0 \u3008Si0 , Sj0\u30092(UU>)2j0,i1  12 . (11) Now, we can apply Khintchine\u2019s inequality for each fixing of j1, and combine this with (11). With\nprobability 1\u2212 1/poly(n), again we have\u2211 i1 \u03c3i1\u3008Si1 , Sj1\u3009 \u2211 j0 \u03c3j0\u3008Si0 , Sj0\u3009(UU>)j0,i1\n= \u2211 i1 \u03c3i1\u3008Si1 , Sj1\u3009O( \u221a log n) \u2211 j0 \u3008Si0 , Sj0\u30092(UU>)2j0,i1  12\n= O(log n) \u2211 i1 \u3008Si1 , Sj1\u30092 \u2211 j0 \u3008Si0 , Sj0\u30092(UU>)2j0,i1  12\nThus, we can apply Khintchine\u2019s inequality recursively over all the 2k indexes j0, i1, j1, \u00b7 \u00b7 \u00b7 , jk\u22121, ik, from which it follows that with probability 1 \u2212 1/poly(n), for each such i0, jk, we have Z2i0,jk = O(logk n)E\nS [Z2i0,jk ], using (17). We thus have with this probability, that \u2016Z\u20162F = O(log k n)E S [\u2016Z\u20162F ], completing the proof."}, {"heading": "5 Lower bound for `2 and `\u221e guarantee", "text": "We prove a lower bound for the `2 guarantee, which immediately implies a lower bound for the `\u221e guarantee.\nDefinition 13. Given a matrix A \u2208 Rn\u00d7d, vector b \u2208 Rn and matrix S \u2208 Rr\u00d7n, denote x\u2217 = A\u2020b. We say that an algorithm A(A, b, S) that outputs a vector x\u2032 = (SA)\u2020Sb \u201csucceeds\u201d if the following property holds: \u2016x\u2032 \u2212 x\u2217\u20162 . \u03b5\u2016b\u20162 \u00b7 \u2016A\u2020\u20162 \u00b7 \u2016Ax\u2217 \u2212 b\u20162.\nTheorem 14. Suppose \u03a0 is a distribution over Rm\u00d7n with the property that for any A \u2208 Rn\u00d7d and b \u2208 Rn, Pr\nS\u223c\u03a0 [A(A, b, S) succeeds ] \u2265 19/20. Then m & min(n, d/\u03b52).\nProof. The proof uses Yao\u2019s minimax principle. Let D be an arbitrary distribution over Rn\u00d7(d+1), then E\n(A,b)\u223cD E S\u223c\u03a0 [A(A, b, S) succeeds ] \u2265 1 \u2212 \u03b4. Switching the order of probabilistic quantifiers, an\naveraging argument implies the existence of a fixed matrix S0 \u2208 Rm\u00d7n such that\nE (A,b)\u223cD [A(A, b, S0) succeeds ] \u2265 1\u2212 \u03b4.\nThus, we must construct a distribution Dhard such that\nE (A,b)\u223cDhard [A(A, b, S0) succeeds ] \u2265 1\u2212 \u03b4,\ncannot hold for any \u03a00 \u2208 Rm\u00d7n which does not satisfy m = \u2126(d/\u03b52). The proof can be split into three parts. First, we prove a useful property. Second, we prove a lower bound for the case rank(S) \u2265 d. Third, we show why rank(S) \u2265 d is necessary.\n(I) We show that [SA, Sb] are independent Gaussian, if both [A, b] and S are orthonormal matrices. We can rewrite SA in the following sense,\nS\ufe38\ufe37\ufe37\ufe38 m\u00d7n \u00b7 A\ufe38\ufe37\ufe37\ufe38 n\u00d7d = S\ufe38\ufe37\ufe37\ufe38 m\u00d7n R\ufe38\ufe37\ufe37\ufe38 n\u00d7n R>\ufe38\ufe37\ufe37\ufe38 n\u00d7n A\ufe38\ufe37\ufe37\ufe38 n\u00d7d\n= S [ S> S > ] [S S ] A = [ Im 0 ] [S S ] A = [ Im 0 ] A\u0303\ufe38\ufe37\ufe37\ufe38 n\u00d7d = A\u0303m\ufe38\ufe37\ufe37\ufe38 m\u00d7d\nwhere S is the complement of the orthonormal basis S, Im is a m\u00d7m identity matrix, and A\u0303m is the left m \u00d7 d submatrix of A\u0303. Thus, using [J+06] as long as m = o(\u221an) (because of n = \u2126(d3)) the total variation distance between [SA, Sb] and a random Gaussian matrix is small, i.e.,\nDTV ([SA, Sb], H) \u2264 0.01 (12)\nwhere each entry of H is i.i.d. Gaussian N (0, 1/n). (II) Here we prove the theorem in the case when S has rank r \u2265 d (we will prove this is necessary in part III. Writing S = U\u03a3V > in its SVD, we have\nS\ufe38\ufe37\ufe37\ufe38 m\u00d7n A = U\ufe38\ufe37\ufe37\ufe38 m\u00d7r \u03a3\ufe38\ufe37\ufe37\ufe38 r\u00d7r V >\ufe38\ufe37\ufe37\ufe38 r\u00d7n RR>A = U\u03a3G (13)\nwhere R = [ V V ] . By a similar argument in Equation (12), as long as r = o( \u221a n) we have that G also can be approximated by a Gaussian matrix, where each entry is sampled from i.i.d. N (0, 1/n). Similarly, Sb = U\u03a3h, where h also can be approximated by a Gaussian matrix, where each entry is sampled from i.i.d. N (0, 1/n).\nSince U has linearly independent columns, (U\u03a3G)\u2020U\u03a3h = (\u03a3G)\u2020U>U\u03a3h = (\u03a3G)\u2020\u03a3h. The r \u00d7 d matrix G has SVD G = R\ufe38\ufe37\ufe37\ufe38\nr\u00d7d \u03a3\u0303\ufe38\ufe37\ufe37\ufe38 d\u00d7d T\ufe38\ufe37\ufe37\ufe38 d\u00d7d , and applying the pseudo-inverse property\nagain, we have\n\u2016(SA)\u2020Sb\u20162 = \u2016(\u03a3G)\u2020\u03a3h\u20162 = \u2016(\u03a3R\u03a3\u0303T )\u2020\u03a3h\u20162 = \u2016T \u2020(\u03a3R\u03a3\u0303)\u2020\u03a3h\u20162 = \u2016(\u03a3R\u03a3\u0303)\u2020\u03a3h\u20162 = \u2016\u03a3\u0303\u2020(\u03a3R)\u2020\u03a3h\u20162,\nwhere the the first equality follows by Equation (13), the second equality follows by the SVD of G, the third and fifth equality follow by properties of the pseudo-inverse1 when T has orthonormal rows and \u03a3\u0303 is a diagonal matrix, and the fourth equality follows since \u2016T \u2020\u20162 = 1 and T is an orthonormal basis.\nBecause each entry of G = R\u03a3\u0303T \u2208 Rr\u00d7d is sampled from an i.i.d. Gaussian N (0, 1), using the result of [Ver10] we can give an upper bound for the maximum singular value of G: \u2016\u03a3\u0303\u2016 . \u221a r n with probability at least .99. Thus,\n\u2016\u03a3\u0303\u2020(\u03a3R)\u2020\u03a3h\u20162 \u2265 \u03c3min(\u03a3\u0303\u2020) \u00b7 \u2016(\u03a3R)\u2020\u03a3h\u20162 = 1 \u03c3max(\u03a3\u0303) \u2016(\u03a3R)\u2020\u03a3h\u20162 &\n\u221a n/r\u2016(\u03a3R)\u2020\u03a3h\u20162.\nBecause h is a random Gaussian vector which is independent of (\u03a3R)\u2020\u03a3, by Claim 15, Eh[\u2016(\u03a3R)\u2020\u03a3h\u201622] = 1 n \u00b7 \u2016(\u03a3R)\u2020\u03a3\u20162F , where each entry of h is sampled from i.i.d. Gaussian N (0, 1/n). Then, using the Pythagorean Theorem,\n\u2016(\u03a3R)\u2020\u03a3\u20162F = \u2016(\u03a3R)\u2020\u03a3RR>\u20162F + \u2016(\u03a3R)\u2020\u03a3(I \u2212RR>)\u20162F \u2265 \u2016(\u03a3R)\u2020\u03a3RR>\u20162F = \u2016(\u03a3R)\u2020\u03a3R\u20162F = rank(\u03a3R)\n= rank(SA)\n= d.\n1https://en.wikipedia.org/wiki/Moore-Penrose_pseudoinverse\nThus, \u2016x\u2032 \u2212 x\u2217\u20162 & \u221a d/r \u2265 \u221a d/m = \u03b5.\n(III) Now we show that we can assume that rank(S) \u2265 d. We sample A, b based on the following distribution Dhard: with probability 1/2, A, b are sampled from D1; with probability 1/2, A, b are sampled from D2. In distribution D1, A is a random orthonormal basis and d is always orthogonal to A. In distribution D2, A is a d\u00d7 d identity matrix in the top-d rows and 0s elsewhere, while b is a random unit vector. Then, for any (A, b) sampled from D1, S needs to work with probability at least 9/10. Also for any (A, b) sampled from D2, S needs to work with probability at least 9/10. The latter two statements follow since overall S succeeds on Dhard with probability at least 19/20.\nConsider the case where A, b are sampled from distribution D2. Then x\u2217 = b and OPT = 0. Then consider x\u2032 which is the optimal solution to minx \u2016SAx\u2212 Sb\u201622, so x\u2032 = (SA)\u2020Sb = (SL)\u2020SLb, where S can be decomposed into two matrices SL \u2208 Rr\u00d7d and SR \u2208 Rr\u00d7(n\u2212d), S = [ SL SR ] . Plugging x\u2032 into the original regression problem, \u2016Ax\u2032 \u2212 b\u201622 = \u2016A(SL)\u2020SLb\u2212 b\u201622, which is at most (1 + \u03b5) OPT = 0. Thus rank(SL) is d. Since SL is a submatrix of S, the rank of S is also d.\nIt remains to define several tools which are used in the main proof of the lower bound.\nClaim 15. For any matrix A \u2208 Rn\u00d7d, if each entry of a vector g \u2208 Rd is chosen from an i.i.d Gaussian N (0, \u03c32), then E\ng [\u2016Ag\u201622] = \u03c32\u2016A\u20162F .\nProof.\nE g [\u2016Ag\u201622] = Eg  n\u2211 i=1 ( d\u2211 j=1 Aijgj) 2  = E\ng  n\u2211 i=1 ( d\u2211 j=1 A2ijg 2 j + \u2211 j 6=j\u2032 AijAij\u2032gjgj\u2032)  =\nn\u2211 i=1 d\u2211 j=1 A2ij\u03c3 2\n= \u03c32\u2016A\u20162F .\nLet g1, g2, \u00b7 \u00b7 \u00b7 , gt be i.i.d. N (0, 1) random variables. The random variables \u2211t i=1 g 2 i are X 2 with\nt degree of freedom. Furthermore, the following tail bounds are known.\nFact 16 (Lemma 1 of [LM00]). Let g1, g2, \u00b7 \u00b7 \u00b7 , gt be i.i.d. N (0, 1) random variables. Then for any x \u2265 0,\nPr [ t\u2211 i=1 g2i \u2265 t+ 2 \u221a tx+ 2x ] \u2264 exp(\u2212x),\nand\nPr [ t\u2211 i=1 g2i \u2264 t\u2212 2 \u221a tx ] \u2264 exp(\u2212x).\nDefinition 17. Given a matrix A \u2208 Rn\u00d7d, vector b \u2208 Rn and matrix S \u2208 Rr\u00d7n, denote x\u2217 = A\u2020b. We say that an algorithm B(A, b, S) that outputs a vector x\u2032 = (SA)\u2020Sb \u201csucceeds\u201d if the following property holds:\n\u2016x\u2032 \u2212 x\u2217\u2016\u221e . \u03b5\u221a d \u2016b\u20162 \u00b7 \u2016A\u2020\u20162 \u00b7 \u2016Ax\u2217 \u2212 b\u20162.\nApplying \u2016x\u2032\u2212x\u2016\u221e \u2265 1\u221ad\u2016x \u2032\u2212x\u20162 to Theorem 14 ,we obtain the `\u221e lower bound as a corollary,\nCorollary 18. Suppose \u03a0 is a distribution over Rm\u00d7n with the property that for any A \u2208 Rn\u00d7d and b \u2208 Rn,\nPr S\u223c\u03a0\n[B(A, b, S) succeeds ] \u2265 9/10.\nThen m & min(n, d/\u03b52)."}, {"heading": "A Proof for Gaussian case", "text": "Lemma 19. If the entries of S \u2208 Rm\u00d7n are i.i.d. N(0, 1/m), m = O(d/\u03b52), and U>b = 0, then\n|a>(SA)\u2020Sb| . \u03b5 \u221a\nlog d\u221a d \u2016a\u20162\u2016b\u20162\u2016\u03a3\u22121\u20162\nfor any vectors a, b with probability 1\u2212 1/poly(d).\nProof. With probability 1, the matrix SA has linearly independent columns, and so (SA)\u2020 is\n= (A>S>SA)\u22121A>S>\n= (V \u03a3U>S>SU\u03a3V >)\u22121V \u03a3U>S>\n= V \u03a3\u22121(U>S>SU)\u22121\u03a3\u22121V >V \u03a3U>S>\n= V \u03a3\u22121(U>S>SU)\u22121U>S>.\nHence, we would like to bound\nX = a>V \u03a3\u22121(U>S>SU)\u22121U>S>Sb.\nIt is well-known (stated, for example, explicitly in Theorem 2.3 of [Woo14]) that with probability 1\u2212 exp(\u2212d), the singular values of SU are (1\u00b1 \u03b5) for m = O(d/\u03b52). We condition on this event. It follows that\n\u2016V \u03a3\u22121(U>S>SU)\u22121U>S\u20162 = \u2016\u03a3\u22121(U>S>SU)\u22121U>S\u20162 \u2264 \u2016\u03a3\u22121\u20162\u2016(U>S>SU)\u22121\u20162\u2016U>S\u20162 \u2264 \u2016\u03a3\u22121\u20162 \u00b7 1\n1\u2212 \u03b5 \u00b7 (1 + \u03b5)\n= O(\u2016\u03a3\u22121\u20162),\nwhere the first equality uses that V is a rotation, the first inequality follows by sub-multiplicativity, and the second inequality uses that the singular values of SU are in the range [1\u2212 \u03b5, 1 + \u03b5]. Hence, with probability 1\u2212 exp(\u2212d),\n\u2016a>V \u03a3\u22121(U>S>SU)\u22121U>S>\u20162 = O(\u2016\u03a3\u22121\u20162\u2016a\u20162). (14)\nThe main observation is that since U>b = 0, SU is statistically independent from Sb. Hence, Sb is distributed as N(0, \u2016b\u201622Im), conditioned on the vector a>V \u03a3\u22121(U>S>SU)\u22121U>S>. It follows that conditioned on the value of a>V \u03a3\u22121(U>S>SU)\u22121U>S>, X is distributed as\nN(0, \u2016b\u201622\u2016a>V \u03a3\u22121(U>S>SU)\u22121U>S>\u201622/m),\nand so using (14) , with probability 1\u2212 1/poly(d), we have |X| = O(\u03b5\u221alog d\u2016a\u20162\u2016b\u20162\u2016\u03a3\u22121\u20162/ \u221a d)."}, {"heading": "B Combining Different Matrices", "text": "In some cases it can make sense to combine different matrices that satisfy the generalization bound.\nTheorem 11. Let A \u2208 Rn\u00d7d, and let R \u2208 Rm\u00d7r and S \u2208 Rr\u00d7n be drawn from distributions of matrices that are \u03b5-approximate OSEs and satisfy the generalization bound (6). Then RS satisfies the generalization bound with a constant factor loss in failure probability and approximation factor.\nProof. For any vectors a, b, and x\u2217 = A\u2020b we want to show\n|a>(RSA)\u2020RSb\u2212 a>x\u2217| . \u03b5\u221a d \u2016a\u20162\u2016b\u2212Ax\u2217\u20162\u2016A\u2020\u20162\nAs before, it suffices to consider the x\u2217 = 0 case. We have with probability 1\u2212 \u03b4 that\n|a>(SA)\u2020Sb| . \u03b5\u221a d \u2016a\u20162\u2016b\u20162\u2016A\u2020\u20162;\nsuppose this happens. We also have by the properties of R, applied to SA and Sb, that\n|a>(RSA)\u2020RSb\u2212 a>(SA)\u2020Sb| . \u03b5\u221a d \u2016a\u20162\u2016Sb\u20162\u2016(SA)\u2020\u20162.\nBecause S is an OSE, we have \u2016Sb\u20162 \u2264 (1 + \u03b5) and \u2016(SA)\u2020\u20162 & (1\u2212 \u03b5)\u2016A\u2020\u20162. Therefore\n|a>(RSA)\u2020RSb| . \u03b5\u221a d \u2016a\u20162\u2016b\u20162\u2016A\u2020\u20162\nWe describe a few of the applications of combining sketches.\nB.1 Removing dependence on n via Count-Sketch\nOne of the limitations of the previous section is that the choice of k depends on n. To prove that theorem, we have to assume that log d > log log n. Here, we show an approach to remove that assumption.\nThe main idea is instead of applying matrix S \u2208 Rm\u00d7n to matrix A \u2208 Rn\u00d7d directly, we pick two matrices S \u2208 Rm\u00d7poly(d) and C \u2208 Rpoly(d)\u00d7n, e.g. S is FastJL matrix and C is Count-Sketch matrix with s = 1. We first compute C \u00b7 A, then compute S \u00b7 (CA). The benefit of these operations is S only needs to multiply with a matrix (CA) that has poly(d) rows, thus the assumption we need is log d > log log(poly(d)) which is always true. The reason for choosing C as a Count-Sketch matrix with s = 1 is: (1) nnz(CA) \u2264 nnz(A) (2) The running time is O(poly(d) \u00b7 d+ nnz(A)).\nB.2 Combining Gaussians and SRHT\nBy combining Gaussians with SRHT matrices, we can embed into the optimal dimension O(d/\u03b52) with fast O\u0303(nd log n+ d\u03c9/\u03b54) embedding time.\nB.3 Combining all three\nBy taking Gaussians times SRHT times Count-Sketch, we can embed into the optimal dimension O(d/\u03b52) with fast O(nnz(A) + d4poly(1\u03b5 , log d)) embedding time."}, {"heading": "C Count-Sketch does not obey the `\u221e guarantee", "text": "Here we demonstrate an A and a b such that Count-Sketch will not satisfy the `\u221e guarantee with constant probability, so such matrices cannot satisfy the generalization guarantee (6) with high probability.\nTheorem 20. Let S \u2208 Rm\u00d7n be drawn as a Count-Sketch matrix with s nonzeros per column. There exists a matrix A \u2208 Rn\u00d7d and b \u2208 Rn such that, if s2d . m . \u221a d3s, then the \u201ctrue\u201d solution x\u2217 = A\u2020b and the approximation x\u2032 = (SA)\u2020Sb have large `\u221e distance with constant probability:\n\u2016x\u2032 \u2212 x\u2217\u2016\u221e & \u221a d\nms \u2016b\u20162.\nPlugging in m = d1.5 and s = d0.25 we find that\n\u2016x\u2032 \u2212 x\u2217\u2016\u221e & 1/d3/8\u2016b\u20162 1/ \u221a d\u2016b\u20162,\neven though such a matrix is an OSE with probability exponential in s. Therefore there exists a constant c for which this matrix does not satisfy the generalization guarantee (6) with 1 \u2212 cd probability.\nProof. We choose the matrix A to be the identity on its top d rows: A = [ Id 0 ] . Choose some \u03b1 \u2265 1, set the value of the first d coordinates of vector b to be 1\u221a d and set the value to be 1/ \u221a \u03b1 for the next\n\u03b1 coordinates, with the remaining entries all zero. Note that \u2016b\u20162 = \u221a 2, x\u2217 = (1/ \u221a d, . . . , 1/ \u221a d), and \u2016Ax\u2217 \u2212 b\u20162 = 1. Let Sk denote the kth column vector of matrix S \u2208 Rm\u00d7n. We define two events, Event I, \u2200k\u2032 \u2208 [d] and k\u2032 6= k, we have supp(Sk\u2032)\u2229 supp(Sk) = \u2205; Event II, \u2203 a unique k\u2032 \u2208 {d+ 1, d+ 2, \u00b7 \u00b7 \u00b7 , d+\u03b1} such that | supp(Sk\u2032)\u2229 supp(Sk)| = 1, and all other k\u2032 have supp(Sk\u2032)\u2229 supp(Sk) = \u2205. Using Claim 21, with probability at least .99 there exists a k for which both events hold.\nGiven the constructions of A and b described early, it is obvious that\nAx\u2212 b = [ x1 \u2212 1\u221ad , \u00b7 \u00b7 \u00b7 , xd \u2212 1\u221a d ,\u2212 1\u221a \u03b1 , \u00b7 \u00b7 \u00b7 ,\u2212 1\u221a \u03b1 , 0, \u00b7 \u00b7 \u00b7 , 0 ]> .\nConditioned on event I and II are holding, then denote supp(Sj) = {i1, i2, \u00b7 \u00b7 \u00b7 , is}. Consider the terms involving xj in the quadratic form\nmin x \u2016SAx\u2212 Sb\u201622.\nit can be written as (s \u2212 1)(xj \u2212 1/ \u221a d)2 + (xj \u2212 1/ \u221a d \u00b1 1/\u221a\u03b1)2. Hence the optimal x\u2032 will have x\u2032j = 1\u221a d \u00b1 1 s \u221a \u03b1 , which is different from the desired 1/ \u221a d by 1 s \u221a \u03b1 . Plugging in our requirement of \u03b1 h m2/(s3d2), we have\n\u2016x\u2032 \u2212 x\u2217\u2016\u221e \u2265 1\ns \u221a \u03b1 & c\n\u221a sd2\nm2 & 1\u221a d\nwhere the last inequality follows by m . \u221a sd3. Thus, we get the result.\nClaim 21. If m = \u2126(s2d), m = o(d2), \u03b1 < d, and \u03b1 = O( m 2\ns3d2 ), with probability at least .99 there\nexists a k \u2208 [d] for which both event I and II hold.\nProof. If m = \u2126(s2d), then for any i in {1, 2, ..., d}, let Xi be an indicator that the entries of column i are disjoint from all i\u2032 in [d]\\{i}. Then E[Xi] \u2265 .9999, so by Markov\u2019s inequality, with probability .99, we have .99d columns having this property (indeed, the expected value of d \u2212 X is at most .0001d, so Pr[d \u2212 X \u2265 .01d] \u2264 E[d\u2212X].01d \u2264 .0001d.01d = .01). Define Event E to be that .99d columns of first d columns have the property that the entries of that column are disjoint from all the other d\u2212 1 columns. Let S be the set of these .99d columns. Let N be the union of supports of columns in S.\nEach column i in {d+ 1, ..., d+ \u03b1} chooses s non-zero entries. Define event F ( which is similar as event E) to be that .99\u03b1 columns of the next \u03b1 columns have the property that the entries of that column are disjoint from all the other \u03b1 \u2212 1 columns. By the same argument, since \u03b1 < d, with probability .99, we have .99\u03b1 columns in {d + 1, ..., d + \u03b1} being disjoint from other columns in {d+ 1, ..., d+ \u03b1}. Condition on event F holding. Let L be the multiset union of supports of all columns in {d+ 1, ..., d+ \u03b1}. Then L has size \u03b1 \u00b7 s. Let M be the union of supports of all columns in {d+ 1, ..., d+ \u03b1}, that is, the set union rather than the multiset union. Note that |M | \u2265 .99\u03b1 \u00b7 s because of .99\u03b1 columns are disjoint from each other.\nThe intersection size x of N and M is hyper-geometrically distributed with expectation\nE[x] = s|S| \u00b7 |M |\nm .\nBy a lower tail bound for the hypergeometric distribution 2 ,\nPr[x \u2264 (p\u2212 t)n] \u2264 exp(\u22122t2n),\nwhere p = s \u00b7 |S|/m and n = |M |, so\nPr[x \u2264 E[x]\u2212 t \u00b7 |M |] \u2264 exp(\u22122t2 \u00b7 |M |) \u2264 0.01,\nwhere the last inequality follows by setting t = \u0398(1/ \u221a |M |). Thus, we get with probability .99, the\nintersection size is at least s|S|\u00b7|M |m \u2212\u0398( \u221a |M |) .\nNow let W be the distinct elements in L\\M , so necessarily |W | \u2264 .01\u03b1 \u00b7 s. By an upper tail bound for the hypergeometric distribution, the intersection size y of W and N satisfies\nPr[y \u2265 (p+ t)n] \u2264 exp(\u22122t2n),\nwhere p = s \u00b7 |S|/m and n = |W |, we again get\nPr[y \u2265 E[y] + t \u00b7 |W |] \u2264 exp(\u22122t2 \u00b7 |W |).\nIf |W | = 0, then y = 0. Otherwise, we can set t = \u0398(1/ \u221a |W |) so that this probability is less than\n.01, and we get with probability .99, the intersection size y is at most s \u00b7 |S| \u00b7 |W |/m + \u0398( \u221a |W |).\nNote that we have that \u0398( \u221a |M |) and \u0398( \u221a |W |) are bounded by \u0398(\u221as \u00b7 \u03b1). Setting \u03b1 = O( m2\ns3d2 )\nsuffices to ensure y is at most (1.01)s \u00b7 |S| \u00b7 |W |/m, and earlier that x is at least .99 \u00b7 s \u00b7 |S| \u00b7 |M |/m. The probability one of the |S| blocks in N has two or more intersections with M is less than(\nx 2\n) times the probability two random distinct items in the intersection land in the block. This\nprobability is ( x 2 ) \u00b7 ( s 2 )( s\u00b7|S|\n2 ) = \u0398(x2/|S|2) = \u0398(x2/d2) = \u0398(m2/(d4s2)). So the expected number of such blocks is \u0398(m2sd/(d4s2)) = \u0398(m2/(d3s)) which is less than (.99 \u00b7 s \u00b7 |S| \u00b7 |M |)/(2m) \u2264 X/2 if m = o(d2), which we have. So, there are at least x/2 blocks which have intersection size exactly 1 with N . Note that the number of intersections of the |S| blocks with W is at most y, which is at most (1.01)s \u00b7 |S| \u00b7 |W |/m \u2264 (1.01)s \u00b7 |S| \u00b7 199 \u00b7 |M |/m < x/2, and therefore there exists a block, that is, a column among the first d columns, which intersects M in exactly one position and does not intersect W . This is our desired column. Thus, we complete the proof."}, {"heading": "D Leverage score sampling does not obey the `\u221e guarantee", "text": "Not only does Count-Sketch fail, but so does leverage score sampling, which is a technique that takes a subsample of rows of A with rescaling. In this section we show an A and a b such that leverage score sampling will not satisfy the `\u221e guarantee. We start with a formal definition of leverage scores.\nDefinition 22 (Leverage Scores). Given an arbitrary n\u00d7 d matrix A, with n > d, let U denote the n\u00d7 d matrix consisting of the d left singular vectors of A, let U(i) denote the i\u2212th row of the matrix U , so U(i) is a row vector. Then the leverage scores of the rows of A are given by li = \u2016U(i)\u201622, for i \u2208 [n].\n2https://en.wikipedia.org/wiki/Hypergeometric_distribution\nThe leverage score sampling matrix can be thought of as a square diagonal matrix D \u2208 Rn\u00d7n with diagonal entries chosen from some distribution. If Dii = 0, it means we do not choose the i-th row of matrix A., If Dii > 0, it means we choose that row of the matrix A and also rescale that row. We show that the leverage score sampling matrix cannot achieve `\u221e guarantee, nor can it achieve our notion of generalization error.\nTheorem 23. Let D \u2208 Rn\u00d7n be a leverage score sampling matrix with m nonzeros on the diagonal. There exists a matrix A \u2208 Rn\u00d7d and a vector b \u2208 Rn such that, if m . d \u221a d, then the \u201ctrue\u201d solution x\u2217 = A\u2020b and the approximation x\u2032 = (DA)\u2020Db have large `\u221e distance with constant probability:\n\u2016x\u2032 \u2212 x\u2217\u2016\u221e & 1\u221a d \u2016b\u20162.\nTherefore there exists a constant c for which this matrix does not satisfy the generalization guarantee (6) with 1\u2212 cd probability.\nProof. We choose the matrix A to be the identity on its top d rows, and L scaled identity matrices 1\u221a \u03b1d Id for the next dL rows, where L satisfies 1d + 1 \u03b1dL = 1 (to normalize each column of A), which implies L = \u03b1(d\u2212 1). Choose some \u03b2 \u2208 [1, d). Set the value of the first d coordinates of vector b to be 1\u221a\nd and set the value to be 1\u221a \u03b2 for the next \u03b2 coordinates, with the remaining entries all zero.\nNote that \u2016b\u20162 = \u221a\n2. First, we compute \u2016Ax\u2212 b\u201622. Because \u03b2 is less than d, there are two kinds of xj : one involves\nthe following term,\n( 1\u221a d xj \u2212 1\u221a d )2 + (L\u2212 1)( 1\u221a \u03b1d xj) 2, (15)\nwhere the optimal xj should be set to 1/d. The other involves the term:\n( 1\u221a d xj \u2212 1\u221a d )2 + ( 1\u221a \u03b1d xj \u2212 1\u221a \u03b2 )2 + (L\u2212 1)( 1\u221a \u03b1d xj) 2, (16)\nwhere the optimal xj should be set to 1/d+ 1/ \u221a \u03b1\u03b2d. Because we are able to choose \u03b1, \u03b2 such that \u03b1\u03b2 & d, then xj = 1/d+ 1/ \u221a \u03b1\u03b2d . 1/d.\nSecond, we compute \u2016DAx\u2212Db\u201622. With high probability, there exists a j satisfying Equation (16), but after applying leverage score sampling, the middle term of Equation (16) is removed. Let p1 = 1 d denote the leverage score of each of the top d rows of A, and let p2 = 1 \u03b1d denote the leverage score of each of the next Ld rows of A. We need to discuss the cases m > d and m \u2264 d separately. If m > d, then the following term involves xj ,\n( 1\u221a p1 1\u221a d xj \u2212 1\u221a p1 1\u221a d )2 + m\u2212 d d \u00b7 ( 1\u221a p2 1\u221a \u03b1d xj) 2\n= 1\np1 ( 1\u221a d xj \u2212 1\u221a d )2 + m\u2212 d d \u00b7 1 p2 ( 1\u221a \u03b1d xj) 2\n= d ( (\n1\u221a d xj \u2212 1\u221a d )2 + m\u2212 d d \u03b1( 1\u221a \u03b1d xj) 2\n) .\nwhere the optimal xj should be set to\nxj = 1/d\n1/d+ (m\u2212 d)\u03b1/(\u03b1d2) = 1\n1 + (m\u2212 d)/d & 1\n(m\u2212 d)/d 1\u221a\nd . by m d\n\u221a d\nIf m \u2264 d, then the term involving xj is ( 1\u221ap1 1\u221a d xj \u2212 1\u221ap1 1\u221a d )2 where the optimal xj should be set\nto be 1 1/ \u221a d.\nThird, we need to compute \u2016Ax\u2217 \u2212 b\u201622 and \u03c3min(A). It is easy to see that \u03c3min(A) because A is an orthonormal matrix. The upper bound for \u2016Ax\u2217 \u2212 b\u201622 = 2, and the lower bound is also a constant, which can be proved in the following way:\n\u2016Ax\u2217 \u2212 b\u201622 = \u03b2\u2211 j=1 (15) + d\u2211 j=\u03b2+1 (16) \u2265 d( 1\u221a d 1 d \u2212 1\u221a d )2 & d \u00b7 1 d = 1."}, {"heading": "E Bounding E[\u2016Z\u20162F ]", "text": "Before getting into the proof details, we define the key property of S being used in the rest of the proofs.\nDefinition 24 (All Inner Product Small(AIPS) Property). For any matrix S \u2208 Rr\u00d7n, if for all i, j \u2208 [n] with i 6= j we have\n|\u3008Si, Sj\u3009| = O( \u221a log n/ \u221a r),\nwe say that S satisfies the \u201cAIPS\u201d property.\nClaim 25. If S \u2208 Rr\u00d7n is a subsampled Hadamard transform matrix, then the AIPS property holds with probability at least 1\u2212 1/poly(n). Proof. From the structure of S, for any i 6= j, we have with probability 1 \u2212 1/poly(n) such that |\u3008Si, Sj\u3009| = O( \u221a log n/ \u221a r). Applying a union bound over O(n2) pairs, we obtain that\nPr[ AIPS holds ] \u2265 1\u2212 1/poly(n).\nThe main idea for bounding E[\u2016Z\u20162F ] is to rewrite it as E[\u2016Z\u20162F ] = E[ \u2016Z\u20162F | AIPS holds ] + E[ \u2016Z\u20162F | AIPS does not hold ]. Because Pr[ AIPS does not hold] is at most 1/poly(n), the first term dominates the second term, which means we only need to pay attention to the first term. We repeatedly apply this idea until all the S are removed.\nWe start by boundinbg E[\u2016Z\u20162F ] by squaring Zi0,j0 and using that E[\u03c3i\u03c3j ] = 1 if i = j and 0 otherwise. Then, we obtain,\nE \u03c3\n[Z2i0,jk ] = a 2 i0b 2 jk \u2211 i1,\u00b7\u00b7\u00b7ik,j0,\u00b7\u00b7\u00b7jk\u22121 k\u220f c=0 \u3008Sic , Sjc\u30092 k\u220f c=1 (UU>)2ic\u22121,jc . (17)\nWe thus have,\u2211 i0,jk,jk 6=ik a2i0\u3008Si0 , Sj0\u30092b2jk\u3008Sik , Sjk\u3009 2 = a2j0\u2016b\u201622O((log n)/r) + \u2016a\u201622\u2016b\u201622O((log2 n)/r2) def = Cj0 ,\nwhere the first equality is from our conditioning, and the second equality is the definition of Cj0 . Hence, E\nS [\u2016Z\u20162F ] is\n= \u2211\ni1,\u00b7\u00b7\u00b7ik,j0,\u00b7\u00b7\u00b7jk\u22121\nk\u22121\u220f c=1 \u3008Sic , Sjc\u30092 k\u220f c=1 (UU>)2jc\u22121,ic \u00b7 \u2211\ni0,jk,jk 6=ik\na2i0\u3008Si0 , Sj0\u30092b2jk\u3008Sik , Sjk\u3009 2\n= \u2211\ni1,\u00b7\u00b7\u00b7ik,j0,\u00b7\u00b7\u00b7jk\u22121\nk\u22121\u220f c=1 \u3008Sic , Sjc\u30092 k\u220f c=1 (UU>)2jc\u22121,icCj0\n= \u2211\ni1,\u00b7\u00b7\u00b7ik,j0,\u00b7\u00b7\u00b7jk\u22121\n\u3008Sik\u22121 , Sjk\u22121\u30092(UU>)2jk\u22121,ik \u00b7 k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,icCj0 ,\nwhere the first equality follows from (17), the second equality by definition of Cj0 , and the final equality by factoring out c = k \u2212 1 from one product and c = k \u2212 2 from the other product.\nThe way to bound the term \u3008Sik\u22121 , Sjk\u22121\u3009 is by separating the diagonal term where ik\u22121 = jk\u22121 and the non-diagonal term where ik\u22121 6= jk\u22121. We now use the aforementioned property of S, namely, that \u3008Sik\u22121 , Sjk\u22121\u3009 = 1, if ik\u22121 = jk\u22121, while for ik\u22121 6= jk\u22121, we have with probability 1\u2212 1/poly(n) that |\u3008Sik\u22121 , Sjk\u22121\u3009| = O( \u221a log n/ \u221a r) conditioned on AIPS holding.\nConditioned on AIPS holding, we can recursively reduce the number of terms in the product:\n\u2016Z\u20162F\n= \u2211\ni1,\u00b7\u00b7\u00b7ik,j0,\u00b7\u00b7\u00b7jk\u22121,ik\u22121 6=jk\u22121\nO((log n)/r) \u00b7 (UU>)2jk\u22121,ik \u00b7 k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,icCj0\n+ \u2211\ni1,\u00b7\u00b7\u00b7ik,j0,\u00b7\u00b7\u00b7jk\u22121,ik\u22121=jk\u22121\n1 \u00b7 (UU>)2jk\u22121,ik \u00b7 k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,icCj0\n\u2264 \u2211\ni1,\u00b7\u00b7\u00b7ik,j0,\u00b7\u00b7\u00b7jk\u22121\nO((log n)/r) \u00b7 (UU>)2jk\u22121,ik \u00b7 k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,icCj0\n+ \u2211\ni1,\u00b7\u00b7\u00b7ik,j0,\u00b7\u00b7\u00b7jk\u22121,ik\u22121=jk\u22121\n1 \u00b7 (UU>)2jk\u22121,ik \u00b7 k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,icCj0 ,\nwhere the first equality follows from the property just mentioned, and the inequality follows by including back the tuples of indices for which ik\u22121 = jk\u22121, using that each summand is non-negative.\nOur next step will be to bound the term (UU>)2jk\u22121,ik . We have, \u2016Z\u20162F is\n\u2264 \u2211\nik,jk\u22121\n(UU>)2ik,jk\u22121 \u2211 i1,\u00b7\u00b7\u00b7 ,ik\u22121 j0,\u00b7\u00b7\u00b7 ,jk\u22122 O((log n)/r)\n\u00b7 k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,icCj0\n+ \u2211\ni1,\u00b7\u00b7\u00b7 ,ik j0,\u00b7\u00b7\u00b7 ,jk\u22121 ik\u22121=jk\u22121\n1 \u00b7 (UU>)2jk\u22121,ik k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,icCj0\n= O(d(log n)/r) \u2211\ni1,\u00b7\u00b7\u00b7 ,ik\u22121 j0,\u00b7\u00b7\u00b7 ,jk\u22122\nk\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,icCj0\n\ufe38 \ufe37\ufe37 \ufe38 A\n+ \u2211\ni1,\u00b7\u00b7\u00b7 ,ik j0,\u00b7\u00b7\u00b7 ,jk\u22121 ik\u22121=jk\u22121\n1 \u00b7 (UU>)2jk\u22121,ik k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,icCj0\n\ufe38 \ufe37\ufe37 \ufe38 B\n,\nwhere the equality uses that \u2211\nik,jk\u22121 (UU>)2ik,jk\u22121 = \u2016UU>\u20162F = d. We first upper bound term B:\n= \u2211\ni1,\u00b7\u00b7\u00b7 ,ik j0,\u00b7\u00b7\u00b7 ,jk\u22121 ik\u22121=jk\u22121\n1 \u00b7 (UU>)2jk\u22121,ik k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,icCj0\n= \u2211\ni1,\u00b7\u00b7\u00b7 ,ik\u22121 j0,j1,\u00b7\u00b7\u00b7 ,jk\u22121 ik\u22121=jk\u22121\nCj0 k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,ic \u2211 ik (UU>)2jk\u22121,ik\n= \u2211\ni1,\u00b7\u00b7\u00b7 ,ik\u22121 j0,j1,\u00b7\u00b7\u00b7 ,jk\u22121 ik\u22121=jk\u22121\nCj0 k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,ic |ejk\u22121UU>|2\n\u2264 \u2211\ni1,\u00b7\u00b7\u00b7 ,ik\u22121 j0,j1,\u00b7\u00b7\u00b7 ,jk\u22121 ik\u22121=jk\u22121\nCj0 k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,ic1\n= \u2211\ni1,\u00b7\u00b7\u00b7 ,ik\u22121 j0,j1,\u00b7\u00b7\u00b7 ,jk\u22122\nCj0 k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,ic ,\nwhere the first equality is the definition of B, the second equality follows by separating out the\nindex ik, the third equality uses that \u2211\nik (UU>)2jk\u22121,ik = \u2016ejk\u22121UU>\u201622, that is, the squared norm\nof the jk\u22121-th row of UU>, the inequality follows since all rows of a projection matrix UU> have norm at most 1, and the final equality uses that jk\u22121 no longer appears in the expression.\nWe now merge our bounds for the terms A and B in the following way:\n\u2016Z\u20162F \u2264 A+B\n\u2264 O(d(log n)/r) \u2211\ni1,\u00b7\u00b7\u00b7 ,ik\u22121 j0,\u00b7\u00b7\u00b7 ,jk\u22122\nk\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,icCj0\n+ \u2211\ni1,\u00b7\u00b7\u00b7 ,ik\u22121 j0,j1,\u00b7\u00b7\u00b7 ,jk\u22122\nCj0 k\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,ic\n= (O(d(log n)/r) + 1) \u2211\ni1,\u00b7\u00b7\u00b7 ,ik\u22121 j0,\u00b7\u00b7\u00b7 ,jk\u22122\nk\u22122\u220f c=1 \u3008Sic , Sjc\u30092 k\u22121\u220f c=1 (UU>)2jc\u22121,icCj0\n\u2264 \u00b7 \u00b7 \u00b7\n\u2264 (O(d(log n)/r) + 1)2 \u2211\ni1,\u00b7\u00b7\u00b7 ,ik\u22122 j0,\u00b7\u00b7\u00b7 ,jk\u22123\nk\u22123\u220f c=1 \u3008Sic , Sjc\u30092 k\u22122\u220f c=1 (UU>)2jc\u22121,icCj0\n\u2264 \u00b7 \u00b7 \u00b7\n\u2264 (O(d(log n)/r) + 1)k\u22121 \u2211 i1,j0 1\u220f c=1 (UU>)2jc\u22121,icCj0 \u2264 (O(d(log n)/r) + 1)k\u22121 (d\u2016b\u201622(log2 n)/r2 + \u2016b\u201622(log n)/r),\nwhere the first two inequalities and first equality are by definition of A and B above. The first inequality follows by induction, since at this point we have replaced k with k \u2212 1, and can repeat the argument, incurring another multiplicative factor of O(d(log n)/r)+1. Repeating the induction in this way we arrive at the last inequality. Finally, the last inequality follows by plugging in the definition of Cj0 , using that \u2211 i1,j0\n(UU>)2j0,i1 = d, and\u2211 j0,i1 (UU>)2j0,i1a 2 j0 = \u2211 j0 a2j0 \u2211 i1 (UU>)2j0,i1 = \u2211 j0 a2j0\u2016ej0UU>\u201622 \u2264 1,\nwhere the inequality follows since each row of UU> has norm at most 1, and a is a unit vector. The final result is that\n\u2016Z\u20162F \u2264 (O(d(log n)/r) + 1)k\u22121 (d\u2016b\u201622(log2 n)/r2 + \u2016b\u201622(log n)/r)."}], "references": [{"title": "On computing inverse entries of a sparse matrix in an out-ofcore environment", "author": ["Patrick Amestoy", "Iain S. Duff", "Jean-Yves L\u2019Excellent", "Yves Robert", "Fran\u00e7ois-Henry Rouet", "Bora U\u00e7ar"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "Amestoy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Amestoy et al\\.", "year": 2012}, {"title": "Blendenpik: Supercharging lapack\u2019s least-squares solver", "author": ["Haim Avron", "Petar Maymounkov", "Sivan Toledo"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "Avron et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2010}, {"title": "Toward a unified theory of sparse dimensionality reduction in euclidean space", "author": ["Jean Bourgain", "Sjoerd Dirksen", "Jelani Nelson"], "venue": "In STOC,", "citeRegEx": "Bourgain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bourgain et al\\.", "year": 2015}, {"title": "Ridge leverage scores for low-rank approximation", "author": ["Michael B Cohen", "Cameron Musco", "Christopher Musco"], "venue": "arXiv preprint arXiv:1511.07263,", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "Optimal approximate matrix product in terms of stable rank", "author": ["Michael B Cohen", "Jelani Nelson", "David P. Woodruff"], "venue": "In Proceedings of the 43rd International Colloquium on Automata, Languages and Programming (ICALP 2016),", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Nearly tight oblivious subspace embeddings by trace inequalities", "author": ["Michael B. Cohen"], "venue": "In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Cohen.,? \\Q2016\\E", "shortCiteRegEx": "Cohen.", "year": 2016}, {"title": "Matrix multiplication via arithmetic progressions", "author": ["Don Coppersmith", "Shmuel Winograd"], "venue": "J. Symb. Comput.,", "citeRegEx": "Coppersmith and Winograd.,? \\Q1990\\E", "shortCiteRegEx": "Coppersmith and Winograd.", "year": 1990}, {"title": "Numerical linear algebra in the streaming model", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2009\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2009}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L Clarkson", "David P. Woodruff"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["Petros Drineas", "Malik Magdon-Ismail", "Michael W. Mahoney", "David P. Woodruff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2012}, {"title": "Faster least squares approximation", "author": ["Petros Drineas", "Michael W Mahoney", "S Muthukrishnan", "Tam\u00e1s Sarl\u00f3s"], "venue": "Numerische Mathematik,", "citeRegEx": "Drineas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2011}, {"title": "Powers of tensors and fast matrix multiplication", "author": ["Fran\u00e7ois Le Gall"], "venue": "In International Symposium on Symbolic and Algebraic Computation, ISSAC \u201914,", "citeRegEx": "Gall.,? \\Q2014\\E", "shortCiteRegEx": "Gall.", "year": 2014}, {"title": "How many entries of a typical orthogonal matrix can be approximated by independent normals", "author": ["Tiefeng Jiang"], "venue": "The Annals of Probability,", "citeRegEx": "Jiang,? \\Q2006\\E", "shortCiteRegEx": "Jiang", "year": 2006}, {"title": "Computing entries of the inverse of a sparse matrix using the FIND algorithm", "author": ["Song Li", "Shaikh S. Ahmed", "Gerhard Klimeck", "Eric Darve"], "venue": "J. Comput. Physics,", "citeRegEx": "Li et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "Faster ridge regression via the subsampled randomized hadamard transform", "author": ["Yichao Lu", "Paramveer Dhillon", "Dean Foster", "Lyle Ungar"], "venue": "In Proceedings of the Neural Information Processing Systems (NIPS) Conference,", "citeRegEx": "Lu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2013}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["Beatrice Laurent", "Pascal Massart"], "venue": "Annals of Statistics,", "citeRegEx": "Laurent and Massart.,? \\Q2000\\E", "shortCiteRegEx": "Laurent and Massart.", "year": 2000}, {"title": "Iterative row sampling", "author": ["Mu Li", "Gary L Miller", "Richard Peng"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression", "author": ["Xiangrui Meng", "Michael W Mahoney"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Meng and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Meng and Mahoney.", "year": 2013}, {"title": "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["Jelani Nelson", "Huy L Nguy\u00ean"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2013\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2013}, {"title": "Lower bounds for oblivious subspace embeddings. In Automata, Languages, and Programming ", "author": ["Jelani Nelson", "Huy L. Nguy\u00ean"], "venue": "41st International Colloquium,", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2014\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2014}, {"title": "Provable deterministic leverage score sampling", "author": ["Dimitris Papailiopoulos", "Anastasios Kyrillidis", "Christos Boutsidis"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Papailiopoulos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Papailiopoulos et al\\.", "year": 2014}, {"title": "Regularized least-squares classification", "author": ["Ryan Rifkin", "Gene Yeo", "Tomaso Poggio"], "venue": "Nato Science Series Sub Series III Computer and Systems Sciences,", "citeRegEx": "Rifkin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rifkin et al\\.", "year": 2003}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["Tam\u00e1s Sarl\u00f3s"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Sarl\u00f3s.,? \\Q2006\\E", "shortCiteRegEx": "Sarl\u00f3s.", "year": 2006}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin.", "year": 2010}, {"title": "Multiplying matrices faster than coppersmithwinograd", "author": ["Virginia Vassilevska Williams"], "venue": "In Proceedings of the 44th Symposium on Theory of Computing Conference,", "citeRegEx": "Williams.,? \\Q2012\\E", "shortCiteRegEx": "Williams.", "year": 2012}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["David P. Woodruff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Woodruff.,? \\Q2014\\E", "shortCiteRegEx": "Woodruff.", "year": 2014}, {"title": "Randomized extended kaczmarz for solving least squares", "author": ["Anastasios Zouzias", "Nikolaos M. Freris"], "venue": "SIAM J. Matrix Analysis Applications,", "citeRegEx": "Zouzias and Freris.,? \\Q2013\\E", "shortCiteRegEx": "Zouzias and Freris.", "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "Sketching has emerged as a powerful technique for speeding up problems in numerical linear algebra, such as regression. In the overconstrained regression problem, one is given an n \u00d7 d matrix A, with n d, as well as an n \u00d7 1 vector b, and one wants to find a vector x\u0302 so as to minimize the residual error \u2016Ax\u2212 b\u20162. Using the sketch and solve paradigm, one first computes S \u00b7 A and S \u00b7 b for a randomly chosen matrix S, then outputs x\u2032 = (SA)\u2020Sb so as to minimize \u2016SAx\u2032 \u2212 Sb\u20162. The sketch-and-solve paradigm gives a bound on \u2016x\u2032 \u2212 x\u2217\u20162 when A is well-conditioned. Our main result is that, when S is the subsampled randomized Fourier/Hadamard transform, the error x\u2032 \u2212 x\u2217 behaves as if it lies in a \u201crandom\u201d direction within this bound: for any fixed direction a \u2208 R, we have with 1\u2212 d\u2212c probability that \u3008a, x\u2032 \u2212 x\u2217\u3009 . \u2016a\u20162\u2016x \u2032 \u2212 x\u2217\u20162 d 1 2\u2212\u03b3 , (1) where c, \u03b3 > 0 are arbitrary constants. This implies \u2016x\u2032 \u2212 x\u2217\u2016\u221e is a factor d 1 2\u2212\u03b3 smaller than \u2016x\u2032 \u2212 x\u2217\u20162. It also gives a better bound on the generalization of x\u2032 to new examples: if rows of A correspond to examples and columns to features, then our result gives a better bound for the error introduced by sketch-and-solve when classifying fresh examples. We show that not all oblivious subspace embeddings S satisfy these properties. In particular, we give counterexamples showing that matrices based on Count-Sketch or leverage score sampling do not satisfy these properties. We also provide lower bounds, both on how small \u2016x\u2032 \u2212 x\u2217\u20162 can be, and for our new guarantee (1), showing that the subsampled randomized Fourier/Hadamard transform is nearly optimal. Our lower bound on \u2016x\u2032 \u2212 x\u2217\u20162 shows that there is an O(1/\u03b5) separation in the dimension of the optimal oblivious subspace embedding required for outputting an x\u2032 for which \u2016x\u2032\u2212 x\u2217\u20162 \u2264 \u2016Ax\u2217\u2212 b\u20162 \u00b7 \u2016A\u2020\u20162, compared to the dimension of the optimal oblivious subspace embedding required for outputting an x\u2032 for which \u2016Ax\u2032 \u2212 b\u20162 \u2264 (1 + )\u2016Ax\u2217 \u2212 b\u20162, that is, the former problem requires dimension \u03a9(d/ ) while the latter problem can be solved with dimension O(d/ ). This explains the reason known upper bounds on the dimensions of these two variants of regression have differed in prior work. \u2217A preliminary version of this paper appears in Proceedings of the 44th International Colloquium on Automata, Languages, and Programming (ICALP 2017). ar X iv :1 70 5. 10 72 3v 1 [ cs .D S] 3 0 M ay 2 01 7", "creator": "LaTeX with hyperref package"}}}