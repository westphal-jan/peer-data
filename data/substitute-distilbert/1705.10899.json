{"id": "1705.10899", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Propositional Knowledge Representation in Restricted Boltzmann Machines", "abstract": "representing symbolic knowledge protecting a speech network is not key element for the integration of scalable learning and sound reasoning. developments of the previous studies focus on discriminative neural networks which unnecessarily require a separation of input / output variables. recent development of generative neural networks such as restricted boltzmann machines ( rbms ) has shown a capability of learning semantic abstractions directly from data, posing a promise for general verbal learning and reasoning. previous work on penalty logic show weak link between reverse logic against symmetric connectionist networks, however it were not common to rbms. this paper proposes a novel method to represent propositional formulas into rbms / stack of rbms where gibbs analytics can be accomplished aboard maxsat. it also shows a special use of rbms methodology learn symbolic knowledge surrounding maximum likelihood estimation.", "histories": [["v1", "Wed, 31 May 2017 00:24:16 GMT  (52kb,D)", "http://arxiv.org/abs/1705.10899v1", null], ["v2", "Thu, 1 Jun 2017 00:19:24 GMT  (52kb,D)", "http://arxiv.org/abs/1705.10899v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["son n tran"], "accepted": false, "id": "1705.10899"}, "pdf": {"name": "1705.10899.pdf", "metadata": {"source": "CRF", "title": "Propositional Knowledge Representation in Restricted Boltzmann Machines", "authors": ["Son N. Tran"], "emails": ["son.tran@csiro.au"], "sections": [{"heading": "1 Introduction", "text": "In AI research, there have been much debate over symbolism and connectionism as they are two key opposed paradigms for information processing [Smolensky, 1987; Minsky, 1991]. The former has been known as the foundation language of AI which captures higher level of intelligence with explainable and reasoning capability. The latter is getting more attention due to its indisputable advantages in scalable learning and dealing with noisy data. Despite their difference, there is a strong argument that combination of the two should offer joint benefits [Smolensky, 1995; Valiant, 2006; Garcez et al., 2008]. The last two decades have witnessed consistent efforts in developing neural-symbolic integration systems [Towell and Shavlik, 1994; Avila Garcez and Zaverucha, 1999; Penning et al., 2011; Franc\u0327a et al., 2014; Tran and Garcez, 2016]. Such systems are well known not only for better reasoning but also for more efficient learning. The key success here is lying on a mechanism to represent symbolic knowledge in a connectionist network. This is also useful for knowledge extraction [Towell and Shavlik, 1993; d\u2019Avila Garcez et al., 2001], i.e. to seek for symbolic representation of the networks.\nCentred in the earliest neural-symbolic systems is the artificial neural networks which can be seen as a black box of input-output mapping function. However, such systems are limited in knowledge representation and reasoning due to their discriminative architecture. Different from discriminative neural networks which separate the variables in a domain into input and target variables, the generative counterparts treat all variables in a domain equally, hence are more useful for symbolic reasoning. Also, recent emergence of representation learning shows that unsupervised connectionist networks with latent variables such as restricted Boltzmann machines (RBMs) can learn semantic patterns from large amount of data efficiently [Smolensky, 1986; Hinton, 2002]. Notably, by stacking those generative networks one on top of the others, i.e. to construct a deep networks, we can not only extract different level of abstractions from domain\u2019s data but also achieve better performance [Hinton et al., 2006; Lee et al., 2009]. This poses a desire for a study of symbolic knowledge representation in RBMs.\nThe most related literature to this work is Penalty logic [Pinkas, 1995]. Penalty logic is an extension of propositional logic which is equivalent to symmetric connectionist networks (SCNs), including Hoffield networks and Boltzmann machines. However, we claim that it is difficult to apply Penalty logic to restricted Boltzmann machines, a simplified version of Boltzmann machines where there is no connections between units in the same visible/hidden layers. Indeed, in order to convert the energy function from high-order to quadratic form more hidden variables may need to be created, this results in adding more connections within the hidden layer. Moreover, the equivalence between a Penalty logic and SCNs is asymmetric, i.e the formulas represent a SCN are in different form from the formulas represented by it. Also, an unanswered question is how Penalty logic can benefit from stochastic learning in SCNs. Several attempts have been made recently to integrate symbolic representation and RBMs [Penning et al., 2011; Tran and Garcez, 2016]. Despite achieving good results they are still heuristic and lack a supporting theory.\nIn this paper we answer two questions: \u201cHow to represent propositional knowledge in RBMs?\u201d; and \u201cIs it possible to learn propositional knowledge from RBMs?\u201d. First, we show that any propositional formula can be represented in an RBM where symbolic reasoning is equivalent to minimising\nar X\niv :1\n70 5.\n10 89\n9v 1\n[ cs\n.A I]\n3 1\nM ay\n2 01\n7\nthe energy function. The idea is to convert a formula into disjunctive normal form (DNF) with only one conjunctive clause holds given a preferred model. We then extends this to show how to represent a propositional knowledge program (a set of weighted propositional formulas) in an RBM and a stack of RBMs, as known as deep belief networks (DBNs). Here, inference with Gibbs sampling can be seen as a MaxSAT solver. Finally, we show that it is possible to learn an RBM to approximate an unknown formula by applying maximum likelihood estimation over a set of preferred models (training samples). Although this is not always guaranteed it reveals a promising use of RBMs to approximate a symbolic program from training data.\nThe most exciting perspective of this work may be that by creating a bridge between the grand old propositional logic and the emerging representation learning networks, RBMs, it offers a vital basis for further exploration on unsupervised neural-symbolic integration and reasoning, and perhaps also on explanation of the effectiveness of deep architectures."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Restricted Boltzmann Machines", "text": "Connectionist systems normally refer to a set of models made by interconnected networks of computational neurons (also called \u2019units\u2019) [Smolensky, 1987; Hinton, 1989]. An symmetric connectionist network is a neural network with bidirectional connections which is characterised by a quadratic function called energy. The weights of the connections are stored in a symmetric matrix. A SCN behaves as a memory where information are stored in lower energy states. Inferences in such system may be viewed as searching for the minimum energy, which is either deterministic as in Hoffield networks (HN) [Hopfield, 1982], or stochastic as in Boltzmann machines (BM) [Hinton, 1989].\nA simplified version of stochastic SCN, i.e. Boltzmann machines, is called restricted Boltzmann machine (RBM) where units in the same layers are disconnected [Smolensky, 1986; Hinton, 2002]. For the convenience of presentation the symmetric weight matrix is reduced to the weights between visible and hidden units, denoted as W . The energy function of RBMs is :\nERBM (x,h) = \u2212 \u2211 i,j wijxihj \u2212 \u2211 i aixi \u2212 \u2211 j bjhj (1)\nCompare to BMs, learning in RBMs is easier due to the efficient inference mechanism. More importantly, it has shown that by stacking several RBMs, one on top of another we can not only extract different level of abstractions from domain\u2019s data but also achieve better performance [Hinton et al., 2006; Lee et al., 2009; Mohamed et al., 2012]. Recently, several attempts have been made to extract and encode symbolic knowledge into RBMs [Penning et al., 2011; Tran and Garcez, 2016]. However, it is not theoretically clear how such knowledge, as known as Confidence rules, represent the RBMs formally."}, {"heading": "2.2 Penalty Logic", "text": "Penalty logic is among the earliest works to study symbolic representation of neural networks [Pinkas, 1991a; Pinkas,\n1991b; Pinkas, 1995]. Different from the others which focus on feedforward neural networks, Penalty logic explains the relation between propositional formulas and SCNs.\nThe Penalty logic formulas, as known as penalty logic well-formed formulas (PLOFF), is defined as a finite set of pairs (\u03c1, \u03d5), in which each propositional well-formed formula (WFF) \u03d5 is associated with a real value \u03c1 called penalty. PLOFF define a violation rank function Vrank which is the sum the penalties of violated formulas. The preferred model is a truth assignment x that has minimum total penalty. Applied to classification, for example, to decide the truth-value of a target proposition y given an assignment x of the other propositions, one will choose the value of y that minimises Vrank(x, y)\nReasoning with Penalty logic is shown to be equivalent to minimising energy function in SCNs. This is the key foundation to form the link between propositional knowledge and connectionist networks. The equivalence is defined as:\nVrank(x) = Erank(x) + constant (2)\nwhere Erank(x) = minhE (x,h) is the energy function minimised over all hidden variables.\nLet us use BMs as an example. In [Pinkas, 1995] Gadi demonstrates the capability of Penalty logic to represent propositional knowledge in a BMs. An energy function is constructed by converting every WFF into a conjunction of triple form. More hidden variables may be added to convert the energy function to a quadratic form and also to guarantee the equivalence. Knowledge extraction, i.e representing a BM in PLOFF, can be done by eliminating all hidden variables to turn the energy function back to high-order form and translating every product term into a conjunction. For learning propositional formulas, a BM is constructed incrementally from truth assignments which is equivalent to a k-CNF formula.\nHowever, applying Penalty logic to RBMs is difficult even though, as a restricted version of BMs, RBMs is also SCNs. First, in order to represent propositional formulas to RBMs extra hidden variables are needed. This removes the connections within visible layer but, as a price, would create connections within hidden layer. Second, knowledge extraction with Penalty Logic is computationally expensive. Even eliminating one single hidden unit in an energy function is already exponential over the number of visible units connecting to it. This will make the extraction of knowledge from a complex domains intractable. Third, because there does not always exist an RBM to represent a propositional knowledge the learning method in [Pinkas, 1995] does not always hold."}, {"heading": "3 Propositional Calculus and RBMs", "text": "Since symmetric connectionist networks is the generalisation of RBMs we can apply Penalty logic to represent propositional knowledge in that restricted variant. However, it is unnecessarily complicated where, according to the proposed algorithm in [Pinkas, 1995], it needs to construct a higherorder BM and then transform its energy function to quadratic form by adding more hidden variables. The latter step must be repeated until an energy term has at most one visible variable. This paper introduces a much simpler method to\nrepresent a well form formula in an RBM. We show that this could be accomplished by converting WFFs into disjunctive normal form, as detailed below, instead of conjunctions of sub-formulas as in Penalty Logic [Pinkas, 1991b; Pinkas, 1995].\nIn propositional logic, any WFF \u03d5 can be represented in disjunctive normal form (DNF) [Russell and Norvig, 2003]:\n\u03d5 = \u2228 j ( \u2227\nt\u2208Tj\nxt \u2227 \u2227\nk\u2208Kj\n\u00acxk) where each ( \u2227 t\u2208Tj xt \u2227 \u2227 k\u2208Kj \u00acxk) is called a \u201cconjunctive clause\u201d. Here we denote the literals as xt, xk and Tj and Kj are the set of positive literals and the set of negative literals respectively. Definition 1. \u2022 A \u201cstrict DNF\u201d (SDNF) is a DNF where at most one\nsingle conjunctive clause is true. \u2022 A \u201cfull DNF\u201d is a DNF where each variable must ap-\npear at least once in every conjunctive clause. Any propositional well-formed formula can be presented in a SDNF. Indeed, suppose that \u03d5 is a WFF in disjunctive normal form. If \u03d5 is not SDNF then there exist some groups of conjunctive clauses which are true given a preferred assignment. We can always convert this group of conjunctive clauses to a full DNF which is also a SDNF. Definition 2. A WFF \u03d5 is said to equivalent to a SCN if and only if for any model x, s\u03d5(x) = \u2212AErank(x) + B, where s\u03d5(x) is the truth value of \u03d5 given x; A > 0 and B are fixed real numbers; Erank(x) = minhE (x,h) is the energy ranking function of N minimised over all hidden units.\nThis definition of equivalence is similar to (2) in Penalty logic. Here the equivalence guarantees that all preferred models of a WFF would also minimise the energy of the network. In addition, by construction, non-preferred models of the formula to false would result in maximum energy of the network. Lemma 1. Any SDNF \u03d5 can be mapped onto SCN with energy function E = \u2212 \u2211 j \u220f t\u2208Tj xt \u220f k\u2208Kj (1\u2212xk) where Tj , Kj are respectively the sets of positive and negative propositions of each conjunctive clause j in the SDNF.\nProof. By definition, \u03d5 = \u2228 j( \u2227 t\u2208Tj xt\u2227 \u2227 k\u2208Kj \u00acxk). Each\nconjunctive clause \u2227 t\u2208Tj xt \u2227 \u2227 k\u2208Kj \u00acxk corresponds to\u220f\nt\u2208Tj xt \u220f k\u2208Kj (1 \u2212 xk) which maps to 1 if and only if xt = 1 (xt = true) and xk = 0 (xk = false) for all t \u2208 Tj and k \u2208 Kj . Since \u03d5 is a SDNF, that is true if and only if one conjunctive clause is true, then the sum\u2211 j \u220f t\u2208Tj xt \u220f k\u2208Kj (1 \u2212 xk) = 1 if and only if the assignment of truth-values for xt , xk is a preferred model of \u03d5. Hence, there exists a SCN with energy function E = \u2212 \u2211 j \u220f t\u2208Tj xt \u220f k\u2208Kj (1 \u2212 xk) such that s\u03d5(x) = \u2212Erank(x).\nExample 1. The XOR formula (x\u2295y)\u2194 z can be converted into a SDNF as:\n\u03d5 = (\u00acx \u2227 \u00acy \u2227 \u00acz) \u2228 (\u00acx \u2227 y \u2227 z) \u2228 (x \u2227 \u00acy \u2227 z) \u2228 (x \u2227 y \u2227 \u00acz)\nFor each conjunctive clause, for example x \u2227 y \u2227 \u00acz we create a term xy(1 \u2212 z) and add it to the energy function. After all terms are added, we have the energy function for N :\nE(x, y, z) = \u2212(1\u2212x)(1\u2212 y)(1\u2212 z)\u2212xy(1\u2212 z)\u2212x(1\u2212 y)z\u2212 (1\u2212x)yz\nThe correspondence between \u03d5 and N exists because s\u03d5(x, y, z) = \u2212EN (x, y, z). If we expand the energy function above we can see it is equivalent to the energy function used by Penalty Logic minus one.\nTheorem 1. Any SDNF \u03d5 can be mapped onto an equivalent RBM with energy function E = \u2212 \u2211 j hj( \u2211 t xt \u2212 \u2211 k xk \u2212 |Tj | + ) where Tj , Kj are respectively the sets of positive and negative propositions of each conjunctive clause j in the SDNF.\nProof. We have seen in Lemma 1 that any SDNF \u03d5 can be mapped onto energy function E = \u2212 \u2211 j \u220f t\u2208Tj xt \u220f k\u2208Kj (1 \u2212 xk). Let us denote |Tj | as the number of positive propositions in a conjunctive clause j. For each term e\u0303j(x) = \u2212 \u220f t\u2208Tj xt \u220f k\u2208Kj (1 \u2212 xk) we can construct an energy term with an hidden variable hj as: ej(x, hj) = hj(|Tj | \u2212 \u2211 t\u2208Tj xt + \u2211 k\u2208Kj xk \u2212 ) with 0 < < 1 such that e\u0303j(x) = ej rank(x)\n. This equation holds because |Tj | \u2212 \u2211 t xt + \u2211 k xk \u2212 = \u2212 if and only if xt = 1 and xk = 0 for all t \u2208 Tj and k \u2208 Kj , which makes minhjej(x, hj) = \u2212 with hj = 1. Otherwise |Tj |\u2212 \u2211 t xt+ \u2211 k xk\u2212 > 0 and then minhjej(x, hj) = 0 with hj = 0. By repeating the process on every term e\u0303(x) we can conclude that any SDNF \u03d5 is equivalent with an RBM with the energy function:\nE = \u2212 \u2211 j hj( \u2211 t xt \u2212 \u2211 k xk \u2212 |Tj |+ ) (3)\nwhere: s\u03d5(x) = \u2212 1 Erank(x)\nIn what follows we show how to construct an RBM from a formula. Construction 1. An RBM can be constructed from a WFF by: \u2022 Convert a WFF into SDNF. \u2022 For all conjunctive clause j: \u2227 t\u2208Tj xt \u2227 \u2227 k\u2208Kj \u00acxk.\n\u2013 Create a hidden unit hj . \u2013 Create a connection between all visible unit i (t \u2208 Tj)\nand the hidden unit j with a weight wij = 1. \u2013 Create a connection between all visible unit k (k \u2208 Kj)\nand the hidden unit j with a weight wkj = \u22121. \u2013 Set the bias bj = \u2212|Tj | + with 0 < < 1 for the\nhidden unit j.\nExample 2. Applying Theorem 1 we can construct an RBM for a XOR function in Example 1 as in Figure 1a. In this example we choose = 0.5. The energy function of this RBM is:\nE = xh1 + yh1 + zh1 \u2212 0.5h1 \u2212 xh2 \u2212 yh2 + zh2 + 1.5h2 \u2212 xh3 + yh3 \u2212 zh3 + 1.5h3 + xh4 \u2212 yh4 \u2212 zh4 + 1.5h4\nFor comparison, we also construct an RBM for XOR using Penalty Logic. In this case, this is possible because we can represent the formula into a conjunction of triple form\nwithout adding more hidden variables. First, we compute a higher-oder energy function:\nE p = 4xyz \u2212 2xy \u2212 2xz \u2212 2yz + x+ y + z\nthen we transform it to a quadratic form by adding a hidden variable:\nE p = 2xy \u2212 2xz \u2212 2yz \u2212 8xh1 \u2212 8yh1 + 8zh1 + x+ y + z + 12h1\nThis is still not an energy function of an RBM, so we keep adding hidden variables until the energy function becomes:\nE p\n=\u2212 8xh1 \u2212 8yh1 + 8zh1 + 12h1 \u2212 4xh2 + 4yh2 + 2h2 \u2212 4yh3 \u2212 4zh3 + 6h3 \u2212 4xh4 \u2212 4zh4 + 6h4 + 3x+ y + z\nwhich is the RBMs in Figure 1b."}, {"heading": "4 Representing Propositional Knowledge in RBMs", "text": "The previous section explains how to map a propositional formula onto an unsupervised form of statistical calculus, the RBMs. We now generalise the symbolic presentation from conjunctive clauses to confidence rules [Penning et al., 2011; Son Tran and Garcez, 2012; Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016] to show the equivalence between RBMs and a propositional knowledge, i.e. a set of weighted propositional formulas. We then show that by stacking the rules in layer-wise fashion we can representing propositional knowledge in DBNs."}, {"heading": "4.1 Confidence Rules", "text": "We can present a set of formulas \u03a6 = {\u03b31, ...\u03b3N} in RBMs by applying Theorem 1 to the formula \u03d5 = \u03b31 \u2227 ...\u2227 \u03b3N . For a set of weighted formulas \u03a6 = {\u03b11 : \u03b31, ..., \u03b1N : \u03b3N} that cannot be straightforward. Now, let us extend a conjunctive clause to a Confidence rule, as defined in [Tran and Garcez, 2016]. Confidence rule is a if-and-only-if clause associated with a positive real values called \u201cconfidence value\u201das:\ncj : hj \u2194 \u2227\nt\u2208Tj\nxt \u2227 \u2227\nk\u2208Kj\n\u00acxk\nConfidence rules have been used as a intermediate language for symbolic knowledge extraction and encoding knowledge bases into RBMs/DBNs [Penning et al., 2011; Tran and Garcez, 2016]. However, there is no formal study to show the relations between Confidence rules and RBMs as the empirical results in previous work suggest. Based on the Theorem 1 we now can claim that one can represent a weighted knowledge base onto an RBM by converting it into a set of Confidence rules. Proposition 1. A set of Confidence rules can be represented in an RBM.\nProof. Similar to the proof of Theorem 1 we can show that a rule hj \u2194\n\u2227\nt\u2208Tj xt \u2227\n\u2227 k\u2208Kj \u00acxk is equivalent to the energy\nfunction ej(x, hj) = hj(|Tj | \u2212 \u2211 t\u2208Tj xt + \u2211 k\u2208Kj xk \u2212 ).\nThe confidence rule cj : hj \u2194 \u2227 t\u2208Tj xt \u2227 \u2227 k\u2208Kj \u00acxk therefore is equivalent to the energy cjej(x, hj). As the result, with each confidence rule we can add a hidden unit to an RBM and assign the weights wtj = cj and wkj = \u2212cj with all t \u2208 Tj and k \u2208 Kj . The bias for this hidden unit is cj(|Tj | \u2212 ). The set of Confidence rules and the RBM is equivalent such that s\u03a6 = \u2212 1 Erank 1.\nProposition 2. For every weighted knowledge base \u03a6 = {\u03b11 : \u03b31, ..., \u03b1N : \u03b3N} there exists an equivalent RBM such that s\u03a6 = \u2212 1 Erank.\nProof. Every weighted formula \u03b1n : \u03b3n can be converted into a set of of Confidence rules and a disjunctive clause of hidden literals as:\n\u03b3n = ( \u2228 j hj) \u2227 j (hj \u2194 \u2227\nt\u2208Tj\nxt \u2227 \u2227\nk\u2208Kj\n\u00acxk) (4)\nWe combine the rules that have the same sets Tj and Kj to a single one and its confidence value is the sum of the rules\u2019 confidence values. The final set of Confidence rules are used to construct an RBM as shown in Proposition 1.\nExample 3. Nixon diamond. 1000 : n\u2192 r Nixon is a republican. 1000 : n\u2192 q Nixon is also a quaker. 10 : r\u2192 \u00acp republicans tend not to be pacifist. 10 : q\u2192 p quakers tend to be pacifist.\nThe energy function of the RBM is: E = 1000h1n+ 1000h1r \u2212 500h1 \u2212 1000h2r + 500h2 + 1000h3n +1000h3q \u2212 500h3 \u2212 1000h4 + 500h4 \u2212 10h5p+ 10h5n+ 5h5 +10h6p\u2212 5h6 + 10h7q + 10h7p\u2212 5h7 \u2212 10h8p+ 5h8\nWe now show a theoretical idea of using RBMs to support satisfiability inference. Proposition 3. Given an RBM which is constructed from a weighted knowledge base, inference with Gibbs sampling is equivalent to minimising the total weighted satisfiability.\nProof. The proof is straightforward, since the energy function of the RBM and the satisfiability of the knowledge base is inversely proportional then every step of Gibbs sampling to reduce the energy function will also increase the total satisfiability.\n1s\u03a6 in this case is the sum of confidence values from satisfied rules\nHere, we can take the advantage of RBMs in that inference is efficient. One can see this satisfiability solver as a reconstruction from impaired data, where instead of searching the hypothesis space which is NP-complete one can approximate the solutions with Gibbs sampling. This can be done by clamping the variables which have been assigned with truth values then iteratively inferring the hidden variables and the unassigned ones until the RBM reach stationary state."}, {"heading": "4.2 Stacking of Confidence rules", "text": "In many cases some conjunctive clauses of a WFF share common literals which can be grouped and replaced by a hidden literal. For example, let us consider the WFF \u03d5 = x1 \u2227 x2 \u2227 (x3 \u2295 x4) which can be converted to be:\n\u03d5 =(x1 \u2227 x2 \u2227 \u00acx3 \u2227 x4) \u2228 (x1 \u2227 x2 \u2227 x3 \u2227 \u00acx4) =((h1 \u2227 h2) \u2228 (h1 \u2227 h3)) \u2227 (h1 \u2194 (x1 \u2227 x2)) \u2227 (h2 \u2194 (\u00acx3 \u2227 x4)) \u2227 (h3 \u2194 (x3 \u2227 \u00acx4))\nAs being shown in Proposition 1, we can represent three Confidence rules (h1 \u2194 (x1\u2227x2)), \u2227(h2 \u2194 (\u00acx3\u2227x4)) and (h3 \u2194 (x3 \u2227 \u00acx4)) in an RBM. The DNF of hidden literals can be seen a higher level formula and can be represented in another RBM on top of the previous one. Alternatively, we can repeat the process on the DNF to construct a multiple layer of RBMs, one on top of another. Now we extend this to show that it is possible to represent a knowledge base in a stack of RBMs (also known as a DBN). Theorem 2. Any weighted knowledge base can be approximately represented in a stack of RBMs.\nProof. (Sketch) Convert all WFFs from the knowledge base into SDNFs. Define a set of sub-clauses of conjunctions with at most K literals such that every conjunctive clause in the SDNFs can be the conjunction of at least one sub-clause. Convert every SDNF into a set of Confidence rules and a higher level DNF of hidden literals as shown above. Assign each Confidence rules with small confidence values c+0 from which we create an RBM as shown in Proposition 1.\nRepeat this process on the DNFs until the next higher level DNFs of hidden literals is a disjunctive clause2. At this point, we can construct the top RBM as shown in Proposition 2.\nThe energy ranking function of the DBN will be a sum of the energy ranking functions from all RBMs. Since the lower RBMs have small weights (due to the small confidence values of the lower Confidence rules) and the top RBM is equivalent to the knowledge base then the total energy ranking function of the DBN is to the satisfiability of the knowledge base. In particular s\u03c6 = \u2212 1 Erank +Mc+0 \u2248 \u2212 1 Erank where M is the number of satisfied Confidence rules at lower levels.\nExample 4. Let us consider the following knowledge base: 5 : x1 \u2227 x2 \u2227 (x3 \u2295 x4),10 : x1 \u2227 x3 \u2227 x5 \u2227 ((\u00acx2 \u2228 x5)\u2192 x4)\nWe convert them into the SDNFs with hidden units: 5 : ((h1 \u2227 h2) \u2228 (h1 \u2227 h3)) \u2227 (h1 \u2194 x1 \u2227 x2) \u2227 (h2 \u2194 \u00acx3 \u2227 x4) \u2227 (h3 \u2194 x3 \u2227 \u00acx4)\n10 : ((h1 \u2227 h4) \u2228 (h3 \u2227 h5)) \u2227 (h1 \u2194 x1 \u2227 x2) \u2227 (h3 \u2194 x3 \u2227 \u00acx4) \u2227 (h4 \u2194 x3 \u2227 \u00acx5) \u2227 (h5 \u2194 x1 \u2227 \u00acx5)\n2We can always generate a disjunctive clause as a higher level DNF by defining each sub-clause is exactly a conjunctive clause in lower DNF\nfrom which first level of Confidence rules with small confidence values can be constructed as:\nc+0 : h (1) 1 \u2194 (x1 \u2227 x2) c+0 : h (1) 2 \u2194 (\u00acx3 \u2227 x4) c+0 : h (1) 3 \u2194 (x3 \u2227 \u00acx4)\nc+0 : h (1) 4 \u2194 x3 \u2227 \u00acx5 c+0 : h (1) 5 \u2194 x1 \u2227 \u00acx5\nand two DNFs 5 : ((h1 \u2227 h2) \u2228 (h1 \u2227 h3)), 10 : ((h1 \u2227 h4) \u2228 (h3 \u2227 h5)) which then are converted into second level of Confidence rules:\n5 : h (2) 1 \u2194 h (1) 1 \u2227 h (1) 2 \u2227 \u00ach (1) 3 5 : h (2) 2 \u2194 h (1) 1 \u2227 h (1) 2 \u2227 h (1) 3\n5 : h (2) 3 \u2194 h (1) 1 \u2227 \u00ach (1) 2 \u2227 h (1) 3 10 : h (2) 4 \u2194 h (1) 1 \u2227 h (1) 4 \u2227 h (1) 5\n10 : h (2) 5 \u2194 \u00ach (1) 1 \u2227 h (1) 3 \u2227 h (1) 5 10 : h (2) 6 \u2194 h (1) 1 \u2227 h (1) 4 \u2227 \u00ach (1) 5\n10 : h (2) 7 \u2194 h (1) 1 \u2227 h (1) 3 \u2227 \u00ach (1) 4 \u2227 h (1) 5\nThis hierarchical of rules can be represented in a DBNs by constructing RBMs for every level, one after another."}, {"heading": "5 Knowledge Approximation", "text": "We have shown how to represent propositional knowledge in RBMs/stack of RBMs where finding the maximum satisfiability is equivalent to minimising the energy functions. For completeness, we are now in a position to discuss the capability of using RBM to obtain knowledge from data. As being shown in [Pinkas, 1995], one can use a symbolic learning rules to create a BM from truth assignments incrementally. We can apply the similar idea to construct an RBMs from the truth assignments. However, more interest is on how to obtain symbolic knowledge using the learning capability of connectionist networks rather than using symbolic learning rules to construct it. Learning in a connectionist network is seen as an approximation of parameters over a set of preferred models D = {x(n)|n = 1, .., N} from an unknown formula \u03d5\u2217. To start, let us consider the case where the set is complete, i.e. it consists of all preferred models. We will show that learning an RBM to represent the SDNF of \u03d5\u2217 is possible in this case even though it is not guaranteed. Proposition 4. Learning an RBM to represent a formula from its complete set of preferred model is possible but not guaranteed.\nProof. The gradient of the negative log-likelihood of the RBM is: \u2202-`\n\u2202\u03b8 = E[\n\u2202E(x,h)\n\u2202\u03b8 ]h|x\u2208D \u2212 E[\n\u2202E(x,h)\n\u2202\u03b8 ]h,x (5)\nThis function is not convex, i.e. there does not exist an apparent global minimum, therefore we are not always guaranteed to learn an RBM to represent the formula if it exists. At a local minimum such gradient is close to 0 for all parameters, which means:\n\u2202-` \u2202wij = \u2212 1 N \u2211 x\u2208D xip(hj |x) + \u2211 \u2200x xip(hj |x))p(x) \u2248 0 (6)\nA solution for this is: p(hj |x)p(x) \u2248 { p(hj |x) N if x \u2208 D\n0 otherwise (7)\nThe solution 7 can be achieved by either having p(hj |x) \u2248 0 or p(x) \u2248 0 for all x /\u2208 D and p(x) \u2248 1N for x \u2208 D. Since p(x) = 1Z \u2211 h exp(\u2212E (x,h)) then for a training sample (preferred model) x we have:\u2211 \u2200x\u2032 \u2211 h exp(\u2212E(x\u2032,h)) \u2248 N \u2211 h exp(\u2212E(x,h)) (8)\nthis means the solution 7 can be achieved if\u2211 h exp(\u2212E (x,h)) is equally large for all x \u2208 D, and much smaller otherwise. We can further factorize this sum to be \u2211 h exp(\u2212E (x,h)) \u221d \u220f j(1 + exp( \u2211 i wijxi + bj)).\nNow, suppose we have an RBM (W \u2217,b\u2217) to represent the unknown formula. Here we consider the case that the RBM is equivalent to a set of Confidence rules with equally large confidence-values c+\u221e. This RBM would allow only one hidden unit to be activated for a preferred model, while deactivates all hidden units for a non-preferred model. Similar to Theorem 1 and Proposition 1, we can show that a hidden neuron is activated with an input message c+\u221e 0; and it is deactivated with an input message at most as c+\u221e( \u2212 1) 0. Therefore we can choose c+\u221e large enough to meet the solution 7 because:\u220f\nj (1 + exp( \u2211 i w \u2217 ijxi + b \u2217 j )) \u2248 { exp(c+\u221e ) if x \u2208 D 1 otherwise\n(9)\nIt should be noted that, this is one of many possible local minima so we are not guaranteed to learn the rules. Therefore, in order to achieve this we may need to add more constraints which favour our desired minimum.\nExample 5. Learning XOR: We train an RBM on XOR truth table. In order to increase a chance to converge the wanted local optimum we only use four hidden units, as the same number of conjunctive clauses from DNF of XOR. We also omit the visible biases as they are not needed. Interestingly, by using Contrastive Divergence [Hinton, 2002] we can learn RBMs that represent exactly our theory. One is shown below:\nW = \u22127.0283 5.6875 \u22126.7200 6.2166\u22126.8593 6.0078 6.2008 \u22126.7347 \u22126.9774 \u22126.5855 6.0395 6.3059  b = [5.7909,\u22126.3370,\u22126.3478,\u22126.2275]>\nThe weight matrix and biases above resemble four Confidence rules that represent XOR. It can be seen that in each column vector of the weight matrix the strength of the weights are similar and can be approximated as a Confidence value cj which is their average. If we run the training for long time, the Confidence values are increasing as the optimised function is approaching a local/global minimum, see Figure 2. This is what we expect when we mentioned about large c+\u221e.\nBy extending Proposition 4 we can also show that learning RBMs or DBNs can be seen as an approximation of a set of weighted formulas \u03a6\u2217. Intuitively, one can see the learning of RBMs as lowering the energy of the training sample while raising the energy of the others. Under a particular condition, this will converge to a set of Confidence rules that assign minimum energy to the preferred models (training samples) and much higher energy to the others. We exemplify this on a car evaluation task 3, after pruning the small weights we obtain a mixing of sound and unsound rules. However, interestingly we observe that the sound rules tend to have higher confidence values and some of them look similar as the results of Valliant\u2019s elimination [Valiant, 1984]. For examples: h1 \u2194 low safety \u2227 car is unacceptable, h2 \u2194 2 seats\u2227car is unacceptable are sound and only contain 2 variables from total 6 variables of the domain."}, {"heading": "6 Discussion", "text": "This work addresses two main problems in unifying symbolic logic and neural networks when restricted Boltzmann machines is employed. First, we show how to represent a propositional logic program in an unsupervised energy-based connectionist networks. Second, we show the possibility of using statistical learning in RBMs to approximate propositional knowledge. The results from this work can set up a theoretical foundation for further exploration of unsupervised neural-symbolic integration, reasoning and knowledge learning and extraction.\nA promising application of this work can be neuralsymbolic integration and reasoning where background knowledge is encoded into RBMs/DBNs for better learning and more effective reasoning. Here, we may take the advantage of RBMs where inference is efficient. One exciting future work we are interested to investigate is to integrate firstorder logic into RBMs/DBNs.\nIn practice, learning the exact rules would need to constrain a weight to be discrete, i.e wij \u2208 {\u2212cj , 0, cj} and a bias must be cj(\u2016Tj\u2016 \u2212 ). Even though learning discrete neural network can be done efficiently with Expectation Back-propagation [Soudry et al., 2014], the difficulty here is the stochastic sampling since the negative log-likelihood is generally intractable. This may be solved by carefully herding the learning process using deterministic method [Welling, 2009]. Alternative approach for knowledge approximation is to extract rules from trained RBMs. For examples, in [Penning et al., 2011], the extraction is done by deterministic sampling from each activated hidden unit. In [Tran and Garcez, 2016], the rules are extracted to minimise the L2-norm of their difference with the network\u2019s weights. These methods are heuristic, i.e. do not guarantee the minimum energies for the preferred models, although we can still extract meaningful but incomplete knowledge. We hope that, based on the result of this work further study can find better extraction algorithms.\n3https://archive.ics.uci.edu/ml/datasets/ Car+Evaluation"}], "references": [{"title": "The connectionist inductive learning and logic programming system", "author": ["Avila Garcez", "Zaverucha", "1999] Artur S. Avila Garcez", "Gerson Zaverucha"], "venue": "Applied Intelligence,", "citeRegEx": "Garcez et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Garcez et al\\.", "year": 1999}, {"title": "Symbolic knowledge extraction from trained neural networks: A sound approach", "author": ["d\u2019Avila Garcez et al", "2001] A.S. d\u2019Avila Garcez", "K. Broda", "D.M. Gabbay"], "venue": "Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q2001\\E", "shortCiteRegEx": "al. et al\\.", "year": 2001}, {"title": "d\u2019AvilaGarcez. Fast relational learning using bottom clause propositionalization with artificial neural networks", "author": ["Fran\u00e7a et al", "2014] Manoel V.M. Fran\u00e7a", "Gerson Zaverucha", "Artur S"], "venue": "Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Neural-Symbolic Cognitive Reasoning", "author": ["Garcez et al", "2008] Artur S. d\u2019Avila Garcez", "Lus C. Lamb", "Dov M. Gabbay"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton et al", "2006] Geoffrey E. Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural Comput.,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["Lee et al", "2009] Honglak Lee", "Peter T. Pham", "Yan Largman", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Acoustic modeling using deep belief networks", "author": ["Mohamed et al", "2012] Abdel-rahman Mohamed", "George Dahl", "Geoffrey Hinton"], "venue": "IEEE Transactions on Audio, Speech & Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "A neuralsymbolic cognitive agent for online learning and reasoning", "author": ["Penning et al", "2011] Leo de Penning", "Artur S. d\u2019Avila Garcez", "Lus C. Lamb", "John-Jules Ch Meyer"], "venue": "In IJCAI,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Propositional non-monotonic reasoning and inconsistency in symmetric neural networks", "author": ["Pinkas", "1991a] Gadi Pinkas"], "venue": "In Proceedings of the 12th International Joint Conference on Artificial Intelligence - Volume 1,", "citeRegEx": "Pinkas and Pinkas.,? \\Q1991\\E", "shortCiteRegEx": "Pinkas and Pinkas.", "year": 1991}, {"title": "Symmetric neural networks and propositional logic satisfiability", "author": ["Pinkas", "1991b] Gadi Pinkas"], "venue": "Neural Comput.,", "citeRegEx": "Pinkas and Pinkas.,? \\Q1991\\E", "shortCiteRegEx": "Pinkas and Pinkas.", "year": 1991}, {"title": "Knowledge, reasoning, and planning", "author": ["Russell", "Norvig", "2003] Stuart Russell", "Peter Norvig"], "venue": "In Artificial Intelligent: A Modern Approach. Pearson Education,", "citeRegEx": "Russell et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2003}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["Paul Smolensky"], "venue": "[Smolensky,", "citeRegEx": "Smolensky.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky.", "year": 1986}, {"title": "Connectionist ai, symbolic ai, and the brain", "author": ["P. Smolensky"], "venue": "[Smolensky,", "citeRegEx": "Smolensky.,? \\Q1987\\E", "shortCiteRegEx": "Smolensky.", "year": 1987}, {"title": "Constituent structure and explanation in an integrated connectionist/symbolic cognitive architecture", "author": ["Paul Smolensky"], "venue": "[Smolensky,", "citeRegEx": "Smolensky.,? \\Q1995\\E", "shortCiteRegEx": "Smolensky.", "year": 1995}, {"title": "ICML logic extraction from deep belief networks", "author": ["Son Tran", "Garcez", "2012] Son Tran", "Artur Garcez"], "venue": "In ICML 2012 Representation Learning Workshop,", "citeRegEx": "Tran et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2012}, {"title": "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["Soudry et al", "2014] Daniel Soudry", "Itay Hubara", "Ron Meir"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "The extraction of refined rules from knowledgebased neural networks", "author": ["Towell", "Shavlik", "1993] Geoffrey G. Towell", "Jude W. Shavlik"], "venue": "In Machine Learning,", "citeRegEx": "Towell et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Towell et al\\.", "year": 1993}, {"title": "Knowledge-based artificial neural networks", "author": ["Towell", "Shavlik", "1994] Geoffrey G. Towell", "Jude W. Shavlik"], "venue": "Artificial Intelligence,", "citeRegEx": "Towell et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Towell et al\\.", "year": 1994}, {"title": "Knowledge extraction from deep belief networks for images", "author": ["Tran", "d\u2019Avila Garcez", "2013] Son N. Tran", "Artur d\u2019Avila Garcez"], "venue": "In IJCAI-2013 Workshop on NeuralSymbolic Learning and Reasoning,", "citeRegEx": "Tran et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2013}, {"title": "Deep logic networks: Inserting and extracting knowledge from deep belief networks", "author": ["Tran", "Garcez", "2016] Son Tran", "Artur Garcez"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Tran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Knowledge infusion", "author": ["G. Valiant"], "venue": "[Valiant,", "citeRegEx": "Valiant.,? \\Q2006\\E", "shortCiteRegEx": "Valiant.", "year": 2006}], "referenceMentions": [{"referenceID": 12, "context": "In AI research, there have been much debate over symbolism and connectionism as they are two key opposed paradigms for information processing [Smolensky, 1987; Minsky, 1991].", "startOffset": 142, "endOffset": 173}, {"referenceID": 13, "context": "Despite their difference, there is a strong argument that combination of the two should offer joint benefits [Smolensky, 1995; Valiant, 2006; Garcez et al., 2008].", "startOffset": 109, "endOffset": 162}, {"referenceID": 20, "context": "Despite their difference, there is a strong argument that combination of the two should offer joint benefits [Smolensky, 1995; Valiant, 2006; Garcez et al., 2008].", "startOffset": 109, "endOffset": 162}, {"referenceID": 11, "context": "Also, recent emergence of representation learning shows that unsupervised connectionist networks with latent variables such as restricted Boltzmann machines (RBMs) can learn semantic patterns from large amount of data efficiently [Smolensky, 1986; Hinton, 2002].", "startOffset": 230, "endOffset": 261}, {"referenceID": 12, "context": "Connectionist systems normally refer to a set of models made by interconnected networks of computational neurons (also called \u2019units\u2019) [Smolensky, 1987; Hinton, 1989].", "startOffset": 135, "endOffset": 166}, {"referenceID": 11, "context": "Boltzmann machines, is called restricted Boltzmann machine (RBM) where units in the same layers are disconnected [Smolensky, 1986; Hinton, 2002].", "startOffset": 113, "endOffset": 144}], "year": 2017, "abstractText": "Representing symbolic knowledge into a connectionist network is the key element for the integration of scalable learning and sound reasoning. Most of the previous studies focus on discriminative neural networks which unnecessarily require a separation of input/output variables. Recent development of generative neural networks such as restricted Boltzmann machines (RBMs) has shown a capability of learning semantic abstractions directly from data, posing a promise for general symbolic learning and reasoning. Previous work on Penalty logic show a link between propositional logic and symmetric connectionist networks, however it is not applicable to RBMs. This paper proposes a novel method to represent propositional formulas into RBMs/stack of RBMs where Gibbs sampling can be seen as MaxSAT. It also shows a promising use of RBMs to learn symbolic knowledge through maximum likelihood estimation.", "creator": "LaTeX with hyperref package"}}}