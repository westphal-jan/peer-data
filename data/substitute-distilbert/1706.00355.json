{"id": "1706.00355", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Grounding Symbols in Multi-Modal Instructions", "abstract": "as robots endeavor to cohabit with humans in semi - confined environments, the need arises to understand instructions involving rich variability - - - for instance, learning to ground symbols in the physical world. realistically, this task must cope with subtle challenges consisting of establishing particular users'contextual assignment of language to images. we present a method for processing a clear stream of cross - modal input - - - i. e., linguistic instructions, visual perception of a scene and a concurrent trace of 3d - tracking fixations - - - to produce the segmentation of objects with a correspondent association to cognition - level concepts. to test our framework we launch experiments in a table - top object theory scenario. our results show his model learns the user'historical notion of time and shape from a small number of physical demonstrations, generalising to identifying physical referents making novel combinations of the words.", "histories": [["v1", "Thu, 1 Jun 2017 15:42:50 GMT  (3040kb,D)", "http://arxiv.org/abs/1706.00355v1", "9 pages, 8 figures, To appear in the Proceedings of the ACL workshop Language Grounding for Robotics, Vancouver, Canada"]], "COMMENTS": "9 pages, 8 figures, To appear in the Proceedings of the ACL workshop Language Grounding for Robotics, Vancouver, Canada", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yordan hristov", "svetlin penkov", "alex lascarides", "subramanian ramamoorthy"], "accepted": false, "id": "1706.00355"}, "pdf": {"name": "1706.00355.pdf", "metadata": {"source": "CRF", "title": "Grounding Symbols in Multi-Modal Instructions", "authors": ["Yordan Hristov", "Svetlin Penkov", "Alex Lascarides"], "emails": ["{yordan.hristov@,", "sv.penkov@,", "alex@inf.,", "s.ramamoorthy@}ed.ac.uk"], "sections": [{"heading": null, "text": "As robots begin to cohabit with humans in semi-structured environments, the need arises to understand instructions involving rich variability\u2014for instance, learning to ground symbols in the physical world. Realistically, this task must cope with small datasets consisting of a particular users\u2019 contextual assignment of meaning to terms. We present a method for processing a raw stream of cross-modal input\u2014 i.e., linguistic instructions, visual perception of a scene and a concurrent trace of 3D eye tracking fixations\u2014to produce the segmentation of objects with a correspondent association to high-level concepts. To test our framework we present experiments in a table-top object manipulation scenario. Our results show our model learns the user\u2019s notion of colour and shape from a small number of physical demonstrations, generalising to identifying physical referents for novel combinations of the words."}, {"heading": "1 Introduction", "text": "Effective and efficient human-robot collaboration requires robots to interpret ambiguous instructions and concepts within a particular context, communicated to them in a manner that feels natural and unobtrusive to the human participant in the interaction. Specifically, the robot must be able to:\n\u2022 Understand natural language instructions, which might be ambiguous in form and meaning.\n\u2022 Ground symbols occurring in these instructions within the surrounding physical world.\n\u2022 Conceptually differentiate between instances of those symbolic terms, based on features pertaining to their grounded instantiation, e.g. shapes and colours of the objects.\nBeing able to relate abstract symbols to observations with physical properties in the real world is known as the physical symbol grounding problem (Vogt, 2002); which is recognised as being one of the main challenges for human-robot interaction and constitutes the focus of this paper.\nThere is increasing recognition that the meaning of natural language words derives from how they manifest themselves across multiple modalities. Researchers have actively studied this problem from a multitude of perspectives. This includes works that explore the ability of agents to interpret natural language instructions with respect\nar X\niv :1\n70 6.\n00 35\n5v 1\n[ cs\n.A I]\n1 J\nun 2\n01 7\nto a previously annotated semantic map (Matuszek et al., 2013) or fuse high-level natural language inputs with low-level sensory observations in order to produce a semantic map (Walter et al., 2014). Matuszek et al. (2014); Eldon et al. (2016) and Kollar et al. (2013) tackle learning symbol grounding in language commands combined with gesture input in a table-top scenario. However, all these approaches depend on having predefined specifications of different concepts in the environment: they either assume a pre-annotated semantic map with respect to which they ground the linguistic input or have an offline trained symbol classifier that decides whether a detected object can be labelled with a specific symbol; e.g. colour and shape in (Matuszek et al., 2014). Thus in order to deploy such a system, one should have access to an already trained classifier for every anticipated symbol, prior to any user interaction.\nMulti-modal learning algorithms based on deep neural networks are also popular for grounding natural language instructions to the shared physical environment (Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011). But the majority of these algorithms depend crucially on large and pre-labelled datasets, and the challenge is in collecting these large-scale labelled datasets so that they not only capture the variability in language but also manage to represent the nuances (especially across multiple high-bandwidth modalities, such as vision and eye-tracking) of inter-personal variability in assignment of meaning (e.g., what one person calls mustard another might call yellow), which we claim is a key attribute of freeform linguistic instructions in human-robot interaction applications. If a previously unseen instruction/visual observation is presented to these systems, they might fail to ground or recognize them in the way that the user might have intended in that specific setting. Tobin et al. (2017) potentially bypasses the need to collect a big dataset by demonstrating that a model trained in simulation can be successfully deployed on a robot in the real world. However, the problem is then shifted to generating task-specific training data in a simulator which approximates the real world well enough.\nA proposed alternative to this off-line learning approach is to interactively teach an embodied agent about its surrounding world, assuming limited prior knowledge. Al-Omari et al. (2016) demonstrates a model for incrementally learning\nthe visual representation of words, but relies on temporally aligned videos with corresponding annotated natural language inputs. Parde et al. (2015) and Thomason et al. (2016) represent the online concept learning problem as a variation of the interactive \u201cI Spy\u201d game. However, these approaches assume an initial learning/exploratory phase in the world and extracted features are used as training data for all concept models associated with an object.\nPenkov et al. (2017) introduce a method called GLIDE (see \u00a72.2 for details), which successfully teaches agents how to map abstract instructions, represented as a LISP-like program, into their physical equivalents in the world. Our work builds on this method: it uses it to achieve natural language symbol grounding, as a by-product of user interaction in a task-oriented scenario. Our approach achieves the following:\n\u2022 It maps natural language instructions to a planned behaviour, such as in a robotic manipulation domain; in so doing it supports a communication medium that human users find natural.\n\u2022 It learns symbol grounding by exploiting the concept of intersective modification (Morzycki, 2013) \u2014i.e., an object can be labelled with more than one symbol. The meaning of the symbols is learned with respect to the observed features of the instances of the object.\nIn our work the agent assumes some prior knowledge about the world in the form of lowlevel features that it can extract from objects in the visual input\u2014e.g. intensities in the primary colour channels and areas of pixel patches of any specific colour. On top of this, we learn classifiers for performing symbol grounding. Each symbol has a probabilistic model which is fit to a subset of the extracted (visual) features. When a new instruction is received, the classifier for each symbol makes a decision regarding the object in the world (and their respective features) to which the symbol may be grounded. Crucially, the data from which these classifiers are learned is collected from demonstrations at \u2018run time\u2019 and not prior to the specific human-robot interaction. Images of objects are extracted from the high-frequency eye tracking and video streams, while symbols that refer to these objects in the images are extracted from the parsed natural language instructions\u2014 see Figure 1. Through cross-modal instructions,\nthe human participant is simultaneously teaching the robot how to execute a task and what properties the surrounding objects must have for that execution to be successful. For instance, while observing how to make a fruit salad in a kitchen, apart from learning the sequence of steps, the system would also gain an initial approximation of the visual appearance of different pieces of fruit and their associated natural language symbols."}, {"heading": "2 Methods", "text": "Figure 2 depicts the architecture of the overall system. It consists of an end-to-end process, from raw linguistic and video inputs on the one hand to learned meanings of symbols that in turn are conceptually grouped: i.e., a symbol can correspond either to an object in the real world, or to a property of an object. The rest of the section is organized in the following fashion - each subsection corresponds to a numbered transition (1 to 4) indicated in Figure 2."}, {"heading": "2.1 Natural Language Semantic Parsing", "text": "The task of the semantic parser is to map natural language requests into instructions repre-\nsented in an abstract form. The abstract form we use is a list of tuples with the format (action target location) (Figure 2b), where action corresponds to an element from a predefined set A, target corresponds to a list of terms that describe an object in the world and location corresponds to a single symbol denoting a physical location in the environment.\nThe narration of the plan execution by the human comprises one sentence per abstract instruction. Therefore, given a plan description, our semantic parser finds a mapping from each sentence to a corresponding instruction as defined by our abstract plan language.\nElementary Dependency Structures (EDS) (Oepen et al., 2004), which are output by parsing the sentence with the wide-coverage English Resource Grammar (Flickinger et al., 2014), are used as an intermediate step in this mapping procedure. EDS are given as dependency graphs (Figure 3) and are a variable-free reduced form of the full Minimal Recursion Semantics (MRS) (Copestake et al., 2005) representation of the natural language input. Given EDS for a particular sentence, parsing proceeds in two steps:\n\u2022 The graph is extracted from nodes and their respective edges, cleaning up nodes and edges that do not contribute directly to the meaning of the instruction\u2014i.e., anything that is not a verb, adjective, preposition or a noun;\n\u2022 The processed graph is recursively traversed until all nodes have been visited at least once.\nKnowing the format of our abstract plan language, we know that action would always correspond to the verb in the input sentence, target would correspond to a noun phrase and location would correspond to a prepositional phrase (i.e., a combination of a preposition and noun phrase). For us, the noun phrases all consist of a noun, complemented by a possibly empty list of adjectives. Extracting the action is straightforward since the top node in the EDS always corresponds to the verb in the sentence; see Figure 3. The extracted action is then passed through a predefined rule-based filter which assigns it one of the values from A: e.g. pick, grab, take, get would all be interpreted as pick.\nThe target entry can be extracted by identifying noun node in the EDS that\u2019s connected to the verb node. Once such a noun is found, one can identify its connections to any adjective nodes\u2014 this gives a full list of symbols that define the object referenced by the target.\nThe location entry can be extracted by searching for preposition nodes in the EDS that are connected to the verb node. If there is no such node, then the location is constructed directly from the target by concatenating its labels - e.g. for a blue cube the location would be the symbol blue-cube-location. Extracting the location from a prepositional phrase is less constrained since different verbs can be related to spatial prepositions in varied ways\u2014 either the preposition node has an edge connect-\ning it to the verb node or vice versa. Once a prepositional node is visited, we proceed by recursively exploring any chains of interconnected nouns, prepositions, and adjectives. The recursion calls for backtracking whenever a node is reached with no unvisited incoming or outgoing edges: e.g., node cube on Figure 3 (bottom). For example, the symbol on-left-of-cube is produced for location for the bottom sentence in Figure 3.\nIn this way, the result of parsing is a sequence of abstract instructions\u2014i.e., an abstract plan\u2014 together with a symbol set S, containing all symbols which are part of any target entry. At this point, the symbols are still not actually grounded in the real world. Together with the raw video feed and the eye-tracking fixations, the abstract plan becomes an input to GLIDE (Penkov et al., 2017)."}, {"heading": "2.2 Grounding and Learning Instances through Demonstration and Eye tracking (GLIDE)", "text": "Penkov et al. (2017) introduce a framework for Grounding and Learning Instances through Demonstration and Eye tracking (GLIDE). In this framework, fixation programs are represented in terms of fixation traces obtained during task demonstrations combined with a highlevel plan. Through probabilistic inference over fixation programs, it becomes possible to infer latent properties of the environment and determine locations in the environment which correspond to each instruction in an input abstract plan that conforms to the format discussed above - (action target location)."}, {"heading": "2.2.1 3D Eye-Tracking", "text": "Mobile eye trackers provide fixation information in pixel coordinates corresponding to locations in the image of a first person view camera. In order to utilise information from multiple input sensors,\nan additional camera may be attached to augment an eye-tracker, by running mono camera SLAM algorithm in the background\u2014ORBSLAM (MurArtal et al., 2015). The SLAM algorithm provides 6D pose of the eye tracking glasses within the world frame; this allows for the fixation locations to be projected into the 3D world by ray casting and finding intersections with a 3D model of the environment. As a result, fixations can be represented as 3D locations, enabling the projection of fixations in the frame of any sensor in the environment."}, {"heading": "2.2.2 Model and Location Inference", "text": "In order to solve the problem of symbol grounding, inference is performed using a generative probabilistic model, which is shown in Figure 4. The sequence of fixations Y1 : YT depend both on the current environment state E and the action being executed A. Each action is part of the plan P which is determined by the task being demonstrated T . The sequence of fixations is observed and interpreted with respect to a task that is by this stage already known, while the state of the environment and the current action are unknown. The main inference task is to determine the structure of the model and assign each fixation to the action that is its cause. A crucial feature of this procedure is the fact, deriving from human sensorimotor behaviour, that the distribution of fixations is different when a person is attending to the execution of an action compared to periods of transitions between actions in the plan. By utilising this property and using samples from a Dirichlet distribution to describe these transition points, GLIDE is able to infer the correct partitioning of\nAlgorithm 1: Symbol Meaning Learning Input: \u03c3thresh Data: I , S, F Output: K = {(\u00b51,\u03a31), . . . , (\u00b5S ,\u03a3S)},\nC = {(F sinvar : s), . . . (FSinvar : S)} 1 Data\u2190 [s1 : {}, . . . , sS : {}]; 2 for image i in I do 3 symbolsi \u2190 GetSymbols(i); 4 featuresi \u2190 ExtractFeatures(i); 5 for symbol in symbolsi do 6 Append featuresi to Data[symbol]; 7 for s in S do 8 Ks \u2190 FitNormal(Data[s]); 9 Ks \u2190 CleanNoise(Data[s]);\n10 F sinvar \u2190 FindInvFeat(Ks, \u03c3thresh); 11 Ks \u2190 RefitNormal(Ks, F sinvar); 12 Append (F sinvar : Ks) to C;\nthe fixation sequence. This information allows us to localise each item of interest in the environment and extract labelled sensory signals from the environment. A complete description of this model and inference procedure can be found in (Penkov et al., 2017)."}, {"heading": "2.3 Feature Extraction", "text": "The parser produces a set of symbols S and GLIDE produces a set of image patches I , each of which is labelled with a subset of symbols from S. We proceed by extracting a number of features, drawn from a pre-existing set F . The features are derived from the objects in the image patches, after removing the background. This is achieved through a standard background subtraction method\u2014we know that the majority of the image will be occupied by a solid object with a uniform colour, anything else is a background. For instance, in the image patches in Figure 2 (c), the objects are the colourful blocks and the background is the black strips around them. Images containing only or predominantly background are considered noise in the dataset and are discarded. For each symbol s we group the extracted features from each image labelled with s resulting in S lists of Ms tuples with F entries in each tuple, where Ms is the number of images being labelled with s; see Figure 2 (d, left). The data for each feature is normalized to fall between 0 and 1."}, {"heading": "2.4 Symbol Meaning Learning", "text": "For each symbol s \u2208 S and each feature f \u2208 F we fit a 1-D Normal distribution resulting in a new list of tuples with size F - sj : [(\u00b5\nsj f1 , \u03c3 sj f1 ), . . . , (\u00b5 sj fF , \u03c3 sj fF )] for the jth symbol. Taking into account that the object location process in GLIDE could still produce noisy results\u2014 i.e., the label of an image can be associated with the wrong symbol\u2014we process our distributions to refit them to data that falls within two standard deviations from the means of the original distributions. We are then seeking observed features f that are invariant with respect to each token use of a specific symbol s within the user instructions so far\u2014i.e. their distributions are \u2018narrow\u2019 and with variance below a predefined threshold \u03c3thresh (see Figure 5). If we have a set of images that are of blue objects with different shapes and we extract a set of features from them, we would expect that features with lower variation (e.g. RGB channels as opposed to area) would explain the colour blue better than features with more variation (i.e. pixel area).\nIn the last step, we construct a set of the invariant features from the discovered narrow distributions for a given symbol l - (F sinvar) - and say that this set characterizes the symbol. The parameters for the symbol are the concatenation of the means\nof the features from (F linvar) into a mean vector and the concatenation of the variances into a diagonal covariance matrix. The resultant mean vector and covariance matrix are later used for inference when shown a new set of images."}, {"heading": "3 Experiments", "text": "We now present results from initial experiments based on the framework in Figure 2. We focus our explanation on steps 3 and 4 in that figure, as these are the pertinent and novel elements introduced here. The input data for Figure 2 (c) is derived from the process already well described in (Penkov et al., 2017)."}, {"heading": "3.1 Dataset", "text": "For our experiments we used a total of six symbols defining S: 3 for colour (red, blue and yellow); and 3 for shape (cell, block, cube). We used four extracted features for F : R, G, B values and pixel area. The objects used were construction blocks that can be stacked together and images of them were gathered in a tabletop robotic manipulation setup (see Figure 2 (a)). Based on the empirical statistics of the recognition process in (Penkov et al., 2017), our input dataset to the Symbol Meaning Learning algorithm consists of 75% correctly annotated and 25% mislabelled images. The total training dataset comprised of approximately 2000 labelled image patches, each of which is labelled with two symbols\u2014e.g. blue cell, red block, yellow cube, etc.\nThe additional test set was designed in two parts: one that would test colour recognition and one that would test shape recognition. Overall, 48 objects were presented to the algorithm where the features for each object would fall into one of the following categories:\n\u2022 Previously seen features (Figure 6 (left)) \u2022 Previously unseen features, close to the fea-\ntures of the training data (Figure 6 (middle))\n\u2022 Previously unseen features, not close to the features of the training data (Figure 6 (right))"}, {"heading": "3.2 Experimental Set up", "text": "Inference over new images is performed by thresholding the probability density function (PDF) values from the model parameters for each symbol. The idea is to test how well the algorithm can differentiate the learned concepts with slight variations from concepts it has not seen before: e.g.\ngiven that the algorithm was trained on 3 colours and 3 shapes, we would expect that it should recognize different hues of the 3 colours and objects with similar shapes to the original 3; however, it may not be able to recognize objects with completely different features. Moreover, we further group different symbols into concept groups. If any two symbols are described by the same features, it is safe to assume that those two symbols are mutually exclusive: that is, they can not both describe an object simultaneously. Thus we go over each concept group and if there are symbols yielding PDF values above a predefined threshold, we assign the new image the symbol from that group with the highest PDF."}, {"heading": "3.3 Results", "text": "The system successfully learns from the training dataset that the colour symbols are being characterized by the extracted RGB values, while (in contrast) the shape symbols from the pixel area of the image patch\u2014see Figure 7. Given a new test image with its extracted features, the algorithm recognises 93% of presented colours and 56% of presented shapes. Tables 1 and 2 report the confusion matrices for the testing set. This shows that the system is more robust when recognizing colours than when recognizing shapes. This can be attributed to the fact that while RGB values describe the concept of colour well enough, simply the pixel area is not enough to describe the concept of shape. Therefore the algorithm confuses the rubber duck with a cell, for example, and the arrow with a cube, see Figure 8, principally because they are of a similar size to each other! In future work, we would consider a wider range of features being extracted from the images, which in turn would support a finer-grained discrimination among objects."}, {"heading": "4 Discussion and Future Work", "text": "The experiments in this paper have demonstrated that it is possible to train classifiers for object appearance alongside symbols, which are analysed via a semantic parser, to achieve grounding of instructions that respect the specificity of the scenario within which that association derives its meaning. Although our framework supports an entire pipeline, from raw cross-modal input to an interpreted and grounded instruction, the presented scenarios are simple and the specific methods could be made (much) more sophisticated. Despite this, we claim that this provides stepping stones towards learning more complex language structures in the future: during the first few demonstrations a human could teach a robot fundamental concepts like colours, shapes, orientation, and then proceed to use this newly gained knowledge to ground, e.g., prepositional phrases (Forbes et al., 2015; Rosman and Ramamoorthy, 2011) or actions (Misra et al., 2016; Zampogiannis et al., 2015) in an online and context specific manner. Once the system knows what blue cubes look like, it would be easier to learn what it means for another cube to be on top of/around it.\nAnother fruitful line of exploration would be continuous learning of both known and unknown symbols, using the same conceptual groups the system has extracted from the training data. For instance, whenever the robot observes a new object it can either label it with a symbol or deem it as unknown for a particular concept group. Whenever a symbol is assigned, the feature model for that symbol is updated, taking into account the new data point. If on the other hand the symbol\nis unknown, the system can prompt the human for new linguistic input which together with its feature model is added to the knowledge base and allows for its future recognition. For example, if the robot observes a new hue of blue it would update its parameters for blue to account for that; whereas if it observes a new colour (e.g. green) it would ask the human for the unknown symbol and would record it for future reference.\nThe idea of teaching the system about compound nouns is also a relevant challenge and a possible extension of this work: our current setup relies on noun phrases consisting of predicative Adjs and a Noun (e.g. blue cube), and so we know that the associated image patch X satisfies both the adjective and the noun\u2014i.e., blue(X) and cube(X) are both true. However, this would not apply to a compound noun like steak knife: we know that the associated image patch X satisfies knife(X) but does not satisfy steak(X).\nRefinements to our model would be necessary in order to represent more complex symbol relations, e.g. in a hierarchical fashion (Sun et al., 2014)."}, {"heading": "5 Conclusion", "text": "We present a framework for using cross-modal input: a combination of natural language instructions, video and eye tracking streams, to simultaneously perform semantic parsing and grounding of symbols used in that process within the physical environment. This is achieved without reliance on pre-existing object models, which may not be particularly representative of the specifics of a particular user\u2019s contextual usage and assignment of meaning within that rich multi-modal stream. Instead, we present an online approach that exploits the pragmatics of human sensorimotor behaviour to derive cues that enable the grounding of symbols to objects in the stream. Our preliminary experiments demonstrate the usefulness of this framework, showing how a robot is not only able to learn a human\u2019s notion of colour and shape, but also that it is able to generalise to the recognition of these features in previously unseen objects from a small number of physical demonstrations."}, {"heading": "Acknowledgments", "text": "This work is partly supported by ERC Grant 269427 (STAC), a Xerox University Affairs Committee grant, and grants EP/F500385/1 and BB/F529254/1 for the DTC in Neuroinformatics and Computational Neuroscience from the UK EPSRC, BBSRC, and the MRC. We are very grateful for the feedback from anonymous reviewers."}], "references": [{"title": "Natural language acquisition and grounding for embodied robotic systems", "author": ["M Al-Omari", "P Duckworth", "DC Hogg", "AG Cohn."], "venue": "Proceedings of the 31st Association for the Advancement of Artificial Intelligence. AAAI Press.", "citeRegEx": "Al.Omari et al\\.,? 2016", "shortCiteRegEx": "Al.Omari et al\\.", "year": 2016}, {"title": "Minimal recursion semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Carl Pollard", "Ivan A Sag."], "venue": "Research on Language and Computation 3(2-3):281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "Interpreting multimodal referring expressions in real time", "author": ["Miles Eldon", "David Whitney", "Stefanie Tellex."], "venue": "International Conference on Robotics and Automation.", "citeRegEx": "Eldon et al\\.,? 2016", "shortCiteRegEx": "Eldon et al\\.", "year": 2016}, {"title": "ERG semantic documentation", "author": ["Dan Flickinger", "Emily M. Bender", "Stephan Oepen."], "venue": "Accessed on 2017-04-29. http://www.delph-in.net/esd.", "citeRegEx": "Flickinger et al\\.,? 2014", "shortCiteRegEx": "Flickinger et al\\.", "year": 2014}, {"title": "Robot programming by demonstration with situated spatial language understanding", "author": ["Maxwell Forbes", "Rajesh PN Rao", "Luke Zettlemoyer", "Maya Cakmak."], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE, pages", "citeRegEx": "Forbes et al\\.,? 2015", "shortCiteRegEx": "Forbes et al\\.", "year": 2015}, {"title": "Toward interactive grounded language acqusition", "author": ["Thomas Kollar", "Jayant Krishnamurthy", "Grant P Strimel."], "venue": "Robotics: Science and Systems.", "citeRegEx": "Kollar et al\\.,? 2013", "shortCiteRegEx": "Kollar et al\\.", "year": 2013}, {"title": "Learning from unscripted deictic gesture and language for human-robot interactions", "author": ["Cynthia Matuszek", "Liefeng Bo", "Luke Zettlemoyer", "Dieter Fox."], "venue": "AAAI. pages 2556\u20132563.", "citeRegEx": "Matuszek et al\\.,? 2014", "shortCiteRegEx": "Matuszek et al\\.", "year": 2014}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["Cynthia Matuszek", "Evan Herbst", "Luke Zettlemoyer", "Dieter Fox."], "venue": "Experimental Robotics. Springer, pages 403\u2013415.", "citeRegEx": "Matuszek et al\\.,? 2013", "shortCiteRegEx": "Matuszek et al\\.", "year": 2013}, {"title": "Tell me dave: Contextsensitive grounding of natural language to manipulation instructions", "author": ["Dipendra K Misra", "Jaeyong Sung", "Kevin Lee", "Ashutosh Saxena."], "venue": "The International Journal of Robotics Research 35(1-3):281\u2013300.", "citeRegEx": "Misra et al\\.,? 2016", "shortCiteRegEx": "Misra et al\\.", "year": 2016}, {"title": "Modification", "author": ["Marcin Morzycki."], "venue": "Book manuscript. In preparation for the Cambridge University Press series Key Topics in Semantics and Pragmatics. http://msu.edu/ morzycki/work/book.", "citeRegEx": "Morzycki.,? 2013", "shortCiteRegEx": "Morzycki.", "year": 2013}, {"title": "ORB-SLAM: a versatile and accurate monocular SLAM system", "author": ["Ra\u00fal Mur-Artal", "J.M.M. Montiel", "Juan D. Tard\u00f3s."], "venue": "IEEE Transactions on Robotics 31(5):1147\u20131163. https://doi.org/10.1109/TRO.2015.2463671.", "citeRegEx": "Mur.Artal et al\\.,? 2015", "shortCiteRegEx": "Mur.Artal et al\\.", "year": 2015}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng."], "venue": "Proceedings of the 28th international conference on machine learning (ICML11). pages 689\u2013696.", "citeRegEx": "Ngiam et al\\.,? 2011", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Grounding the meaning of words through vision and interactive gameplay", "author": ["Natalie Parde", "Adam Hair", "Michalis Papakostas", "Konstantinos Tsiakas", "Maria Dagioglou", "Vangelis Karkaletsis", "Rodney D Nielsen."], "venue": "IJCAI. pages 1895\u20131901.", "citeRegEx": "Parde et al\\.,? 2015", "shortCiteRegEx": "Parde et al\\.", "year": 2015}, {"title": "Physical symbol grounding and instance learning through demonstration and eye tracking", "author": ["Svetlin Penkov", "Alejandro Bordallo", "Subramanian Ramamoorthy"], "venue": null, "citeRegEx": "Penkov et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Penkov et al\\.", "year": 2017}, {"title": "Learning spatial relationships between objects", "author": ["Benjamin Rosman", "Subramanian Ramamoorthy."], "venue": "The International Journal of Robotics Research 30(11):1328\u20131342.", "citeRegEx": "Rosman and Ramamoorthy.,? 2011", "shortCiteRegEx": "Rosman and Ramamoorthy.", "year": 2011}, {"title": "Task and context determine where you look", "author": ["Constantin A Rothkopf", "Dana H Ballard", "Mary M Hayhoe."], "venue": "Journal of vision 7.", "citeRegEx": "Rothkopf et al\\.,? 2007", "shortCiteRegEx": "Rothkopf et al\\.", "year": 2007}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan R Salakhutdinov."], "venue": "Advances in neural information processing systems. pages 2222\u20132230.", "citeRegEx": "Srivastava and Salakhutdinov.,? 2012", "shortCiteRegEx": "Srivastava and Salakhutdinov.", "year": 2012}, {"title": "Learning to identify new objects", "author": ["Yuyin Sun", "Liefeng Bo", "Dieter Fox."], "venue": "Robotics and Automation (ICRA), 2014 IEEE International Conference on. IEEE, pages 3165\u20133172.", "citeRegEx": "Sun et al\\.,? 2014", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Learning multi-modal grounded linguistic semantics by playing i spy", "author": ["Jesse Thomason", "Jivko Sinapov", "Maxwell Svetlik", "Peter Stone", "Raymond J Mooney."], "venue": "Proceedings of the Twenty-Fifth international joint conference on Artificial Intelligence", "citeRegEx": "Thomason et al\\.,? 2016", "shortCiteRegEx": "Thomason et al\\.", "year": 2016}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author": ["Josh Tobin", "Rachel Fong", "Alex Ray", "Jonas Schneider", "Wojciech Zaremba", "Pieter Abbeel."], "venue": "arXiv preprint arXiv:1703.06907 .", "citeRegEx": "Tobin et al\\.,? 2017", "shortCiteRegEx": "Tobin et al\\.", "year": 2017}, {"title": "The physical symbol grounding problem", "author": ["Paul Vogt."], "venue": "Cognitive Systems Research 3(3):429\u2013457.", "citeRegEx": "Vogt.,? 2002", "shortCiteRegEx": "Vogt.", "year": 2002}, {"title": "A framework for learning semantic maps from grounded natural language descriptions", "author": ["Matthew R Walter", "Sachithra Hemachandra", "Bianca Homberg", "Stefanie Tellex", "Seth Teller."], "venue": "The International Journal of Robotics Research 33(9):1167\u2013", "citeRegEx": "Walter et al\\.,? 2014", "shortCiteRegEx": "Walter et al\\.", "year": 2014}, {"title": "Learning the spatial semantics of manipulation actions through preposition grounding", "author": ["Konstantinos Zampogiannis", "Yezhou Yang", "Cornelia Ferm\u00fcller", "Yiannis Aloimonos."], "venue": "Robotics and Automation (ICRA), 2015 IEEE International Confer-", "citeRegEx": "Zampogiannis et al\\.,? 2015", "shortCiteRegEx": "Zampogiannis et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Being able to relate abstract symbols to observations with physical properties in the real world is known as the physical symbol grounding problem (Vogt, 2002); which is recognised as being one of the main challenges for human-robot interaction and constitutes the focus of this paper.", "startOffset": 147, "endOffset": 159}, {"referenceID": 21, "context": ", 2013) or fuse high-level natural language inputs with low-level sensory observations in order to produce a semantic map (Walter et al., 2014).", "startOffset": 122, "endOffset": 143}, {"referenceID": 4, "context": "Matuszek et al. (2014); Eldon et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "(2014); Eldon et al. (2016) and Kollar et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 2, "context": "(2014); Eldon et al. (2016) and Kollar et al. (2013) tackle learning symbol grounding in language commands combined with gesture", "startOffset": 8, "endOffset": 53}, {"referenceID": 6, "context": "colour and shape in (Matuszek et al., 2014).", "startOffset": 20, "endOffset": 43}, {"referenceID": 16, "context": "neural networks are also popular for grounding natural language instructions to the shared physical environment (Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011).", "startOffset": 112, "endOffset": 168}, {"referenceID": 11, "context": "neural networks are also popular for grounding natural language instructions to the shared physical environment (Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011).", "startOffset": 112, "endOffset": 168}, {"referenceID": 19, "context": "Tobin et al. (2017) potentially bypasses the need to collect a big dataset by demonstrating that a model trained in simulation can be successfully deployed on a robot in the real world.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Al-Omari et al. (2016) demonstrates a model for incrementally learning the visual representation of words, but relies on temporally aligned videos with corresponding annotated natural language inputs.", "startOffset": 0, "endOffset": 23}, {"referenceID": 18, "context": "(2015) and Thomason et al. (2016) represent the online concept learning problem as a variation of the interactive \u201cI Spy\u201d game.", "startOffset": 11, "endOffset": 34}, {"referenceID": 13, "context": "Penkov et al. (2017) introduce a method called GLIDE (see \u00a72.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "concept of intersective modification (Morzycki, 2013) \u2014i.", "startOffset": 37, "endOffset": 53}, {"referenceID": 3, "context": ", 2004), which are output by parsing the sentence with the wide-coverage English Resource Grammar (Flickinger et al., 2014), are used as an intermediate step in this mapping", "startOffset": 98, "endOffset": 123}, {"referenceID": 1, "context": "EDS are given as dependency graphs (Figure 3) and are a variable-free reduced form of the full Minimal Recursion Semantics (MRS) (Copestake et al., 2005) representation of the natural language input.", "startOffset": 129, "endOffset": 153}, {"referenceID": 13, "context": "Together with the raw video feed and the eye-tracking fixations, the abstract plan becomes an input to GLIDE (Penkov et al., 2017).", "startOffset": 109, "endOffset": 130}, {"referenceID": 15, "context": "physical symbol grounding is based on the idea that \u201ctask and context determine where you look\u201d (Rothkopf et al., 2007).", "startOffset": 96, "endOffset": 119}, {"referenceID": 13, "context": "and inference procedure can be found in (Penkov et al., 2017).", "startOffset": 40, "endOffset": 61}, {"referenceID": 13, "context": "(Penkov et al., 2017).", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "cal statistics of the recognition process in (Penkov et al., 2017), our input dataset to the Symbol Meaning Learning algorithm consists of 75% correctly annotated and 25% mislabelled images.", "startOffset": 45, "endOffset": 66}, {"referenceID": 4, "context": ", prepositional phrases (Forbes et al., 2015; Rosman and Ramamoorthy, 2011) or actions (Misra et al.", "startOffset": 24, "endOffset": 75}, {"referenceID": 14, "context": ", prepositional phrases (Forbes et al., 2015; Rosman and Ramamoorthy, 2011) or actions (Misra et al.", "startOffset": 24, "endOffset": 75}, {"referenceID": 8, "context": ", 2015; Rosman and Ramamoorthy, 2011) or actions (Misra et al., 2016; Zampogiannis et al., 2015) in an online and context specific manner.", "startOffset": 49, "endOffset": 96}, {"referenceID": 22, "context": ", 2015; Rosman and Ramamoorthy, 2011) or actions (Misra et al., 2016; Zampogiannis et al., 2015) in an online and context specific manner.", "startOffset": 49, "endOffset": 96}, {"referenceID": 17, "context": "in a hierarchical fashion (Sun et al., 2014).", "startOffset": 26, "endOffset": 44}], "year": 2017, "abstractText": "As robots begin to cohabit with humans in semi-structured environments, the need arises to understand instructions involving rich variability\u2014for instance, learning to ground symbols in the physical world. Realistically, this task must cope with small datasets consisting of a particular users\u2019 contextual assignment of meaning to terms. We present a method for processing a raw stream of cross-modal input\u2014 i.e., linguistic instructions, visual perception of a scene and a concurrent trace of 3D eye tracking fixations\u2014to produce the segmentation of objects with a correspondent association to high-level concepts. To test our framework we present experiments in a table-top object manipulation scenario. Our results show our model learns the user\u2019s notion of colour and shape from a small number of physical demonstrations, generalising to identifying physical referents for novel combinations of the words.", "creator": "LaTeX with hyperref package"}}}