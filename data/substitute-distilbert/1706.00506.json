{"id": "1706.00506", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Morphological Embeddings for Named Entity Recognition in Morphologically Rich Languages", "abstract": "in this effort, we perform new state - of - the - art results of 93. 59, % and 79. 59, % for turkish tag czech named entity recognition based on the model of ( lample et al., 2016 ). we contribute first proposing several schemes for representing the morphological analysis given generic word in an context of named entity recognition. we show that a concatenation of this representation with the word and adjective embeddings improves the performance. the effect of these representation advantages on the tagging results is also investigated.", "histories": [["v1", "Thu, 1 Jun 2017 21:59:47 GMT  (58kb,D)", "http://arxiv.org/abs/1706.00506v1", "Working draft"]], "COMMENTS": "Working draft", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["onur gungor", "eray yildiz", "suzan uskudarli", "tunga gungor"], "accepted": false, "id": "1706.00506"}, "pdf": {"name": "1706.00506.pdf", "metadata": {"source": "CRF", "title": "Morphological Embeddings for Named Entity Recognition in Morphologically Rich Languages", "authors": ["Onur G\u00fcng\u00f6r", "Eray Y\u0131ld\u0131z"], "emails": ["onurgu@boun.edu.tr", "suzan.uskudarli@boun.edu.tr", "eray.yildiz@huawei.com", "gungort@boun.edu.tr"], "sections": [{"heading": "1 Introduction", "text": "Named Entity Recognition (NER) is an important task in Natural Language Processing (NLP) that aims to discover references to entities in some text. Identified entities are classified into predefined categories like person, location and organization. NER is mostly utilized prior to complex natural language understanding tasks such as relation extraction, knowledge base generation, and question answering (Liu and Ren, 2011; Lee et al., 2007). Additionally, NER systems are often part of search engines (Guo et al., 2009b) and machine translation systems (Babych and Hartley, 2003).\nEarly studies regarding NER propose using hand crafted rules and lists of names of people, places and organizations (Humphreys et al., 1998; Appelt et al., 1995). Traditional approaches typically use several hand-crafted features such as capitalization, word length, gazetteer related features, and syntactic features (part-ofspeech tags, chunk tags, etc.) (McCallum and Li, 2003; Finkel et al., 2005). A wide range of machine learning-based methods have been proposed to address named entity recognition. Some\nof the well known approaches are conditional random fields (CRF) (McCallum and Li, 2003; Finkel et al., 2005), maximum entropy (Borthwick, 1999), bootstrapping (Jiang and Zhai, 2007; Wu et al., 2009), latent semantic association (Guo et al., 2009a), and decision trees (Szarvas et al., 2006).\nRecently, deep learning models have been instrumental in deciding how the parts of the input should be composed to allow the most beneficial features to form leading to state-of-theart results (Collobert et al., 2011). Likewise, researchers have found that representing words with fixed length vectors in a dense space helps improving the overall performance of many tasks: sentiment analysis (Socher et al., 2013), syntactic parsing (Collobert and Weston, 2008), language modeling (Mikolov et al., 2010), part-of-speech tagging and NER (Collobert et al., 2011). These word representations or embeddings are automatically learned both during or before the training using various methods such as Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014).\nBuilding upon these findings, there are recent studies which treat the NER task as a sequence labeling problem which employ LSTM or GRU components (Lample et al., 2016; Huang et al., 2015; Ma and Hovy, 2016; Yang et al., 2016) to capture the syntactic and semantic relations between the units that make up a natural language sentence. However, these approaches are not well studied for morphologically rich languages. Unlike other languages, morphologically rich languages such as Turkish may retain important information in the morphology of the surface form of the word while the same information may be contained in the syntax of other languages. For example, the word \u201cI\u0307stanbul\u2019dayd\u0131\u201d means \u2018he/she was in Istanbul\u2019 in English. The morphological analysis of the word ar X iv :1\n70 6.\n00 50\n6v 1\n[ cs\n.C L\n] 1\nJ un\n2 01\n7\nis \u201cI\u0307stanbul+Noun+Prop+A3sg+Pnon+Loc\u02c6DB+ Verb+Zero+Past+A3sg\u201d where \u2018Prop\u2019 indicates a proper noun, \u2018A3sg\u2019 signifies the third singular person aggreement whereas \u2018Pnon\u2019 signifies no possesive agreement is active. \u2018DB\u2019 indicates a transition of Part-Of-Speech type usually induced by a derivative suffix (Oflazer, 1994). In this case, the derivation is triggered by the \u2018-di\u2019 suffix which was decoded as \u2018Past\u2019 tag which indicates past tense. As seen from the example, morphological tags may help in capturing syntactic and semantic information. In order to address this, character based embeddings in word representations (Lample et al., 2016) and entities tagged at character level (Kuru et al., 2016) were proposed for NER. Embedding based frameworks for representing morphology were also proposed in other contexts such as language modeling (Luong et al., 2013; dos Santos and Zadrozny, 2014; Xu and Liu, 2017; Bhatia et al., 2016; Lankinen et al., 2016) and morphological tagging and segmentation (Shen et al., 2016; Cotterell and Schu\u0308tze, 2017). However, even though morphological tags have been employed in the past (Tu\u0308r et al., 2003; Yeniterzi, 2011), our work is the first to propose an embedding based framework for representing the morphological analysis in the context of NER.\nWe build upon a state-of-the-art NER tagger (Lample et al., 2016) based on a sequential neural model with extensible word representations in Section 2. We show that augmenting the word representation with morphological embeddings based on Bi-LSTMs (Section 2.1) improves the performance of the base model, which uses only pretrained word embeddings. We contribute by investigating various configurations of the morphological analysis of the surface form of a word (Section 2.2). In Section 3.3, we compare the performance of several experiment setups which employ character and morphological embeddings in various combinations. We report F1-measures of 93.59% and 79.59% for Turkish and Czech respectively. These results are the state-of-the-art results compared to previous work (Demir and O\u0308zgu\u0308r, 2014; Seker and Eryig\u0306it, 2012) which rely on a regularized averaged perceptron and CRF respectively both with hand crafted features."}, {"heading": "2 Model", "text": "We formally define an input sentence as X = (x1, x2, . . . , xn) where each xi is a fixed length\nvector of size d, consisting of embeddings that represent the ith word (See Section 2.1). xi are then fed to a Bi-LSTM which is composed of two LSTMs (Hochreiter and Schmidhuber, 1997) treating the input forwards and backwards respectively. Thus we obtain these forward and backward components\u2019 cell matrices \u2212\u2192 H and \u2190\u2212 H which are both of size n \u00d7 p, where p is the number of dimensions of one component of the Bi-LSTM. Thus, \u2212\u2192 H i,j is the value of jth dimension of ith output vector of the right component which corresponds to the ith word in the sentence. We then feed the concatenation of these matricesH = [ \u2212\u2192 H, \u2190\u2212 H ] to a fully connected hidden layer ofK output neurons. To model the dependencies between the corresponding labels of consecutive input units, we follow a conditional random field (CRF) (Lafferty et al., 2001) based approach. This dependency is clearly indicated by labels in IOB tagging scheme, i.e. B-PERSON, I-PERSON, etc.\nTo do this, we obtain a score vector at each position i and aim to minimize the following objective function for a single sample sentence X:\ns(X, y) = \u2211 i Ayi,yi+1 + \u2211 i \u03bei,yi\nwhereAi,j represents the score of a transition from tag i to j and \u03bei are the tag scores at position i output by the uppermost fully connected layer. Using this model, we decode the most probable tagging sequence y\u2217 as argmaxy\u0303 s(X, y\u0303)."}, {"heading": "2.1 Embeddings", "text": "It has been shown that modeling units of information in a natural language input as fixed length vectors is more effective at encoding semantic properties of the words compared to deciding on the features apriori (Collobert et al., 2011; Turian et al., 2010). Therefore we represent the input words, xi, as a combination of three embeddings: word, character, and morphological. Thus the size d of xi is dw + 2dm + 2dc. We describe these embeddings below and illustrate in Figure 1.\nWord embeddings. A vector of size dw which is learned by the global objective function. However, we never learn this component from scratch, instead we load pretrained vectors.\nCharacter embeddings. We learn another fixed length vector of size 2dc for each word. However, in contrast with a word embedding, we\nwant to capture the covert relationships in the sequence of characters of the word. To achieve this, we have a separate Bi-LSTM component for this embedding type with a cell dimension of dc. We feed it with the characters of the surface form of xi and concatenate the cell output of the forward and backward LSTMs to obtain the character embedding of the word.\nMorphological embeddings. These are constructed similar to character embeddings. In this case, the individual tags of the morphological analysis are treated as a sequence and fed into the separate Bi-LSTM component for morphological embeddings to obtain a vector of length 2dm. We devised several different combinations of morphological tags which is explained in Section 2.2. To illustrate, we use the word \u2018evlerinde\u2019 which can both mean \u2018in their house\u2019 or \u2018in their houses\u2019 or \u2018in his/her houses\u2019 in Turkish. In Figure 1, we assume that the correct morphological analysis is \u2018ev+Noun+A3pl+P3sg+Loc\u2019, where \u2018A3pl\u2019 indicates 3rd person plural, \u2018P3sg\u2019 is the possessive marker for 3rd person singular, and \u2018Loc\u2019 is the locative case marker, thus can be translated as \u2018in his/her houses\u2019."}, {"heading": "2.2 Embedding configurations", "text": "We experimented with different combinations of morphological tags to discover an effective configuration for extracting the syntactic and semantic information in the morphological analysis of a token.\nA simple embedding configuration is to use all morphological tags in the analysis along with or without the root (we call this WR and WOR re-\nspectively). Secondly we tried to remove the tags between the root and the derivation boundary (DB) based on the information they carry may not be relevant in some aspects because of the transformation into a new word with partly different lexical and syntactic properties. We call this version WR ADB, i.e. \u201cI\u0307stanbul+\u02c6DB+Verb+ Zero+Past+A3sg\u201d. Lastly, we devised a scheme in which we treat the string of morphological analysis as a surface form and process each character of this surface form as we did for character embeddings and call this scheme as CHAR."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Training", "text": "The parameters to be learned by the training algorithm is the parameters of the Bi-LSTM in Section 2, the parameters of the Bi-LSTMs for the character and morphological embeddings and the word embeddings for each unique word. We experimented with several different choices for the number of dimensions for these parameters and observed that a choice of 100 for word embeddings, 200 for character embeddings and 200 for morphological embeddings. We trained the models by calculating the gradients using backpropagation algorithm and updating with stochastic gradient descent algorithm with a learning rate of 0.01. We also employed gradient clipping to handle gradients diverging from zero. We additionally used dropout on the inputs with probability 0.5."}, {"heading": "3.2 Dataset", "text": "We train and evaluate our model with a corpus which is widely used in previous work on Turkish NER (Tu\u0308r et al., 2003). In addition to the entity tags and tokens, the corpus also contains the disambiguated morphological tags of input words. We observed many morphological analysis errors and incorrect entity taggings in the corpus (Tu\u0308r et al., 2003), probably as a result of automated analysis and labeling.\nWe obtained word embeddings1 of Turkish words as vectors of length 100 using the skipgram algorithm (Mikolov et al., 2013) on a corpus of 951M words (Yildiz et al., 2016), 2,045,040 of which are unique. This corpus consists of Turkish text extracted from several national newspapers, news sites, and book transcripts.\n1We will make these word embeddings available at (we refrain from sharing the url during the review process)."}, {"heading": "3.3 Results", "text": "We observed that using pretrained word embeddings gave the best results compared to learning word embeddings while training the model. Therefore we only include the results of experiment setups with pretrained word embeddings in Table 1.\nWe start with comparing Setup 1 and 4 (and 5) and suggest that the morphological analysis does indeed contribute to higher performance with CHAR and WOR. However, Setup 2 and 3 did not reach the performance level of Setup 1. We suspect that the reason is the relatively high number of parameters when we include the 20030 roots into the model. This effect is also seen in Setup 6 and 7 which have a lower performance compared to Setups 8 and 9. However, we have to note that using character embeddings alone also improved the performance in Setup 10. Nevertheless, we see that the best performance is achieved in Setup 9 when both of them are employed. However, when we performed the McNemar\u2019s test (Dietterich, 1998), we observed that the difference between them is not significant at 95% confidence level. We explain the difference in performance between Setup 4 and 5 with the errors in the morphological analysis which are mostly due to unknown or mispelled words. In those cases, the analysis become usually the same nominal case with 3rd person singular. We suspect that the fact that ME(CHAR) can process the root even if it is faulty allows it to capture useful information into the embedding.\nDespite CE caused a large improvement gener-\nally, it provides a relatively small increase in Setup 8 compared to Setup 4. We believe that the reason behind this is that CE and ME(CHAR) competes with each other in representing the morphological information of the word. The reason that Setup 9 achieved higher performance compared to Setup 8 is probably because the missing roots in Setup 9 can be covered by CE combined with relatively lower complexity of ME(WOR).\nWe have also evaluated our model on text in Czech which is another morphologically rich language. To be able to compare our results, we used the CNEC 2.0 corpus in CoNLL format as other studies did (Konkol and Konop\u0131\u0301k, 2013). We chose to include only the ME(CHAR) setup for Czech because it gave good results both with and without character embeddings.\nLastly, we compare our best results with previous state-of-the-art in Table 2. The performance of (Seker and Eryig\u0306it, 2012) without gazetteers is 89.55%, (Kuru et al., 2016) does not employ any external data and (Demir and O\u0308zgu\u0308r, 2014) still relies on hand-crafted features despite exploiting word embeddings trained externally."}, {"heading": "4 Conclusions", "text": "In this work, we demonstrated a new state-of-theart system for Turkish and Czech named entity recognition using the model of (Lample et al., 2016). We introduced embedding configurations to understand the affect of different combinations of the morphological tags. Using these configurations, we showed that augmenting word representations with morphological embeddings improves the performance. However, the contribution of morphological embeddings seems to be subsumed by character embeddings in some of these configurations. Thus a thorough examination and comparison of character and morphological embeddings learned in this sense is required for further discussion."}], "references": [{"title": "SRI International FASTUS system: MUC-6 test results and analysis", "author": ["Douglas E Appelt", "Jerry R Hobbs", "John Bear", "David Israel", "Megumi Kameyama", "David Martin", "Karen Myers", "Mabry Tyson."], "venue": "Proceedings of the 6th Conference on Message Un-", "citeRegEx": "Appelt et al\\.,? 1995", "shortCiteRegEx": "Appelt et al\\.", "year": 1995}, {"title": "Improving machine translation quality with automatic named entity recognition", "author": ["Bogdan Babych", "Anthony Hartley."], "venue": "Proceedings of the 7th International EAMT Workshop on MT and Other Language Technology Tools, Improving MT through", "citeRegEx": "Babych and Hartley.,? 2003", "shortCiteRegEx": "Babych and Hartley.", "year": 2003}, {"title": "Morphological priors for probabilistic neural word embeddings", "author": ["Parminder Bhatia", "Robert Guthrie", "Jacob Eisenstein."], "venue": "EMNLP.", "citeRegEx": "Bhatia et al\\.,? 2016", "shortCiteRegEx": "Bhatia et al\\.", "year": 2016}, {"title": "A Maximum Entropy Approach to Named Entity Recognition", "author": ["Andrew Eliot Borthwick."], "venue": "Ph.D. thesis, New York University, New York, NY, USA.", "citeRegEx": "Borthwick.,? 1999", "shortCiteRegEx": "Borthwick.", "year": 1999}, {"title": "A unified architecture for natural language processing", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th International Conference on Machine Learning - ICML 08. ACM. https://doi.org/10.1145/1390156.1390177.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Joint semantic synthesis and morphological analysis of the derived word", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "CoRR abs/1701.00946.", "citeRegEx": "Cotterell and Sch\u00fctze.,? 2017", "shortCiteRegEx": "Cotterell and Sch\u00fctze.", "year": 2017}, {"title": "Improving named entity recognition for morphologically rich languages using word embeddings", "author": ["Hakan Demir", "Arzucan \u00d6zg\u00fcr."], "venue": "Machine Learning and Applications (ICMLA), 2014 13th International Conference on. IEEE, pages 117\u2013122.", "citeRegEx": "Demir and \u00d6zg\u00fcr.,? 2014", "shortCiteRegEx": "Demir and \u00d6zg\u00fcr.", "year": 2014}, {"title": "Approximate statistical tests for comparing supervised classification learning algorithms", "author": ["Thomas G Dietterich."], "venue": "Neural Computation 10(7):1895\u2013 1923.", "citeRegEx": "Dietterich.,? 1998", "shortCiteRegEx": "Dietterich.", "year": 1998}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["C\u0131\u0301cero Nogueira dos Santos", "Bianca Zadrozny"], "venue": null, "citeRegEx": "Santos and Zadrozny.,? \\Q2014\\E", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning."], "venue": "Proceedings of the 43rd Annual Meeting of ACL. ACL, pages 363\u2013370.", "citeRegEx": "Finkel et al\\.,? 2005", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Domain adaptation with latent semantic association for named entity recognition", "author": ["Honglei Guo", "Huijia Zhu", "Zhili Guo", "Xiaoxun Zhang", "Xian Wu", "Zhong Su."], "venue": "Proceedings of Human Language", "citeRegEx": "Guo et al\\.,? 2009a", "shortCiteRegEx": "Guo et al\\.", "year": 2009}, {"title": "Named entity recognition in query", "author": ["Jiafeng Guo", "Gu Xu", "Xueqi Cheng", "Hang Li."], "venue": "Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, pages 267\u2013274.", "citeRegEx": "Guo et al\\.,? 2009b", "shortCiteRegEx": "Guo et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8).", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Bidirectional LSTM-CRF models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "arXiv preprint arXiv:1508.01991 .", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "University of Sheffield: Description of the LaSIE-II system as used for MUC-7", "author": ["Kevin Humphreys", "Robert Gaizauskas", "Saliha Azzam", "Chris Huyck", "Brian Mitchell", "Hamish Cunningham", "Yorick Wilks."], "venue": "Proceedings of the Seventh Message Un-", "citeRegEx": "Humphreys et al\\.,? 1998", "shortCiteRegEx": "Humphreys et al\\.", "year": 1998}, {"title": "Instance weighting for domain adaptation in NLP", "author": ["Jing Jiang", "Cheng Xiang Zhai."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics. ACL, Prague, volume 7, pages 264\u2013271.", "citeRegEx": "Jiang and Zhai.,? 2007", "shortCiteRegEx": "Jiang and Zhai.", "year": 2007}, {"title": "Crfbased czech named entity recognizer and consolidation of czech ner research", "author": ["Michal Konkol", "Miloslav Konop\u0131\u0301k"], "venue": "In Ivan Habernal and Va\u0301clav Matous\u030cek, editors, Text, Speech and Dialogue,", "citeRegEx": "Konkol and Konop\u0131\u0301k.,? \\Q2013\\E", "shortCiteRegEx": "Konkol and Konop\u0131\u0301k.", "year": 2013}, {"title": "Charner: Character-level named entity recognition", "author": ["Onur Kuru", "Ozan Arkan Can", "Deniz Yuret."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Orga-", "citeRegEx": "Kuru et al\\.,? 2016", "shortCiteRegEx": "Kuru et al\\.", "year": 2016}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."], "venue": "Proceedings of the eighteenth international conference on machine learning, ICML.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "Proceedings of NAACL-HLT 2016. pages 260\u2013 270.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "A character-word compositional neural language model for finnish", "author": ["Matti Lankinen", "Hannes Heikinheimo", "Pyry Takala", "Tapani Raiko", "Juha Karhunen."], "venue": "CoRR abs/1612.03266.", "citeRegEx": "Lankinen et al\\.,? 2016", "shortCiteRegEx": "Lankinen et al\\.", "year": 2016}, {"title": "Fine-grained named entity recognition and relation extraction for question answering", "author": ["Changki Lee", "Yi-Gyu Hwang", "Myung-Gil Jang."], "venue": "Proceedings of the 30th Annual International", "citeRegEx": "Lee et al\\.,? 2007", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Japanese named entity recognition for question answering system", "author": ["Ye Liu", "Fuji Ren."], "venue": "2011 IEEE International Conference on Cloud Computing and Intelligence Systems. IEEE. https://doi.org/10.1109/ccis.2011.6045098.", "citeRegEx": "Liu and Ren.,? 2011", "shortCiteRegEx": "Liu and Ren.", "year": 2011}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL. pages 104\u2013113.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF", "author": ["Xuezhe Ma", "Eduard Hovy."], "venue": "arXiv preprint arXiv:1603.01354 .", "citeRegEx": "Ma and Hovy.,? 2016", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons", "author": ["Andrew McCallum", "Wei Li."], "venue": "Proceedings of the 7th Conference on Natural Language Learning at HLT-NAACL. Associa-", "citeRegEx": "McCallum and Li.,? 2003", "shortCiteRegEx": "McCallum and Li.", "year": 2003}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Interspeech. volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Two-level description of Turkish morphology", "author": ["Kemal Oflazer."], "venue": "Literary and Linguistic Computing 9(2):137\u2013148.", "citeRegEx": "Oflazer.,? 1994", "shortCiteRegEx": "Oflazer.", "year": 1994}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Initial explorations on using CRFs for Turkish named entity recognition", "author": ["G\u00f6khan Ak\u0131n Seker", "G\u00fcl\u015fen Eryi\u011fit."], "venue": "Proceedings of COLING 2012. The COLING 2012 Organizing Committee, Mumbai, India, pages 2459\u20132474.", "citeRegEx": "Seker and Eryi\u011fit.,? 2012", "shortCiteRegEx": "Seker and Eryi\u011fit.", "year": 2012}, {"title": "The role of context in neural morphological disambiguation", "author": ["Qinlan Shen", "Daniel Clothiaux", "Emily Tagtow", "Patrick Littell", "Chris Dyer."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguis-", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Conference on", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A multilingual named entity recognition system using boosting and C4.5 decision tree learning algorithms", "author": ["Gy\u00f6rgy Szarvas", "Rich\u00e1rd Farkas", "Andr\u00e1s Kocsor"], "venue": "In International Conference on Discovery", "citeRegEx": "Szarvas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Szarvas et al\\.", "year": 2006}, {"title": "A statistical information extraction system for turkish", "author": ["G\u00f6khan T\u00fcr", "Dilek Hakkani-T\u00fcr", "Kemal Oflazer."], "venue": "Natural Language Engineering 9(2):181\u2013 210.", "citeRegEx": "T\u00fcr et al\\.,? 2003", "shortCiteRegEx": "T\u00fcr et al\\.", "year": 2003}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Domain adaptive bootstrapping for named entity recognition", "author": ["Dan Wu", "Wee Sun Lee", "Nan Ye", "Hai Leong Chieu."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL, volume 3, pages 1523\u2013", "citeRegEx": "Wu et al\\.,? 2009", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "Implicitly incorporating morphological information into word embedding", "author": ["Yang Xu", "Jiawei Liu."], "venue": "CoRR abs/1701.02481.", "citeRegEx": "Xu and Liu.,? 2017", "shortCiteRegEx": "Xu and Liu.", "year": 2017}, {"title": "Multi-task cross-lingual sequence tagging from scratch", "author": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen."], "venue": "CoRR abs/1603.06270.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Exploiting morphology in turkish named entity recognition system", "author": ["Reyyan Yeniterzi."], "venue": "Proceedings of the ACL 2011 Student Session. Association for Computational Linguistics, Stroudsburg, PA, USA, HLT-SS \u201911, pages 105\u2013110.", "citeRegEx": "Yeniterzi.,? 2011", "shortCiteRegEx": "Yeniterzi.", "year": 2011}, {"title": "A morphology-aware network for morphological disambiguation", "author": ["Eray Yildiz", "Caglar Tirkaz", "H Bahadir Sahin", "Mustafa Tolga Eren", "Omer Ozan Sonmez."], "venue": "30th AAAI Conference on Artificial Intelligence.", "citeRegEx": "Yildiz et al\\.,? 2016", "shortCiteRegEx": "Yildiz et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Additionally, NER systems are often part of search engines (Guo et al., 2009b) and machine translation systems (Babych and Hartley, 2003).", "startOffset": 59, "endOffset": 78}, {"referenceID": 1, "context": ", 2009b) and machine translation systems (Babych and Hartley, 2003).", "startOffset": 41, "endOffset": 67}, {"referenceID": 15, "context": "Early studies regarding NER propose using hand crafted rules and lists of names of people, places and organizations (Humphreys et al., 1998; Appelt et al., 1995).", "startOffset": 116, "endOffset": 161}, {"referenceID": 0, "context": "Early studies regarding NER propose using hand crafted rules and lists of names of people, places and organizations (Humphreys et al., 1998; Appelt et al., 1995).", "startOffset": 116, "endOffset": 161}, {"referenceID": 26, "context": ") (McCallum and Li, 2003; Finkel et al., 2005).", "startOffset": 2, "endOffset": 46}, {"referenceID": 10, "context": ") (McCallum and Li, 2003; Finkel et al., 2005).", "startOffset": 2, "endOffset": 46}, {"referenceID": 26, "context": "Some of the well known approaches are conditional random fields (CRF) (McCallum and Li, 2003; Finkel et al., 2005), maximum entropy (Borthwick, 1999), bootstrapping (Jiang and Zhai, 2007; Wu et al.", "startOffset": 70, "endOffset": 114}, {"referenceID": 10, "context": "Some of the well known approaches are conditional random fields (CRF) (McCallum and Li, 2003; Finkel et al., 2005), maximum entropy (Borthwick, 1999), bootstrapping (Jiang and Zhai, 2007; Wu et al.", "startOffset": 70, "endOffset": 114}, {"referenceID": 3, "context": ", 2005), maximum entropy (Borthwick, 1999), bootstrapping (Jiang and Zhai, 2007; Wu et al.", "startOffset": 25, "endOffset": 42}, {"referenceID": 16, "context": ", 2005), maximum entropy (Borthwick, 1999), bootstrapping (Jiang and Zhai, 2007; Wu et al., 2009), latent semantic association (Guo", "startOffset": 58, "endOffset": 97}, {"referenceID": 37, "context": ", 2005), maximum entropy (Borthwick, 1999), bootstrapping (Jiang and Zhai, 2007; Wu et al., 2009), latent semantic association (Guo", "startOffset": 58, "endOffset": 97}, {"referenceID": 34, "context": ", 2009a), and decision trees (Szarvas et al., 2006).", "startOffset": 29, "endOffset": 51}, {"referenceID": 5, "context": "Recently, deep learning models have been instrumental in deciding how the parts of the input should be composed to allow the most beneficial features to form leading to state-of-theart results (Collobert et al., 2011).", "startOffset": 193, "endOffset": 217}, {"referenceID": 33, "context": "searchers have found that representing words with fixed length vectors in a dense space helps improving the overall performance of many tasks: sentiment analysis (Socher et al., 2013), syntactic parsing (Collobert and Weston, 2008), language", "startOffset": 162, "endOffset": 183}, {"referenceID": 4, "context": ", 2013), syntactic parsing (Collobert and Weston, 2008), language", "startOffset": 27, "endOffset": 55}, {"referenceID": 27, "context": "modeling (Mikolov et al., 2010), part-of-speech tagging and NER (Collobert et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 5, "context": ", 2010), part-of-speech tagging and NER (Collobert et al., 2011).", "startOffset": 40, "endOffset": 64}, {"referenceID": 30, "context": ", 2013) and GloVe (Pennington et al., 2014).", "startOffset": 18, "endOffset": 43}, {"referenceID": 29, "context": "\u2018DB\u2019 indicates a transition of Part-Of-Speech type usually induced by a derivative suffix (Oflazer, 1994).", "startOffset": 90, "endOffset": 105}, {"referenceID": 20, "context": "In order to address this, character based embeddings in word representations (Lample et al., 2016) and entities tagged at character level (Kuru et al.", "startOffset": 77, "endOffset": 98}, {"referenceID": 18, "context": ", 2016) and entities tagged at character level (Kuru et al., 2016) were proposed for NER.", "startOffset": 47, "endOffset": 66}, {"referenceID": 24, "context": "texts such as language modeling (Luong et al., 2013; dos Santos and Zadrozny, 2014; Xu and Liu, 2017; Bhatia et al., 2016; Lankinen et al., 2016) and morphological tagging and segmentation (Shen et al.", "startOffset": 32, "endOffset": 145}, {"referenceID": 38, "context": "texts such as language modeling (Luong et al., 2013; dos Santos and Zadrozny, 2014; Xu and Liu, 2017; Bhatia et al., 2016; Lankinen et al., 2016) and morphological tagging and segmentation (Shen et al.", "startOffset": 32, "endOffset": 145}, {"referenceID": 2, "context": "texts such as language modeling (Luong et al., 2013; dos Santos and Zadrozny, 2014; Xu and Liu, 2017; Bhatia et al., 2016; Lankinen et al., 2016) and morphological tagging and segmentation (Shen et al.", "startOffset": 32, "endOffset": 145}, {"referenceID": 21, "context": "texts such as language modeling (Luong et al., 2013; dos Santos and Zadrozny, 2014; Xu and Liu, 2017; Bhatia et al., 2016; Lankinen et al., 2016) and morphological tagging and segmentation (Shen et al.", "startOffset": 32, "endOffset": 145}, {"referenceID": 35, "context": "However, even though morphological tags have been employed in the past (T\u00fcr et al., 2003; Yeniterzi, 2011), our work is the first to propose an embedding based framework for representing the morphological analysis in the context of NER.", "startOffset": 71, "endOffset": 106}, {"referenceID": 40, "context": "However, even though morphological tags have been employed in the past (T\u00fcr et al., 2003; Yeniterzi, 2011), our work is the first to propose an embedding based framework for representing the morphological analysis in the context of NER.", "startOffset": 71, "endOffset": 106}, {"referenceID": 20, "context": "We build upon a state-of-the-art NER tagger (Lample et al., 2016) based on a sequential neural model with extensible word representations in Section 2.", "startOffset": 44, "endOffset": 65}, {"referenceID": 7, "context": "These results are the state-of-the-art results compared to previous work (Demir and \u00d6zg\u00fcr, 2014; Seker and Eryi\u011fit, 2012) which rely on a regularized averaged perceptron and CRF respectively both with hand crafted features.", "startOffset": 73, "endOffset": 121}, {"referenceID": 31, "context": "These results are the state-of-the-art results compared to previous work (Demir and \u00d6zg\u00fcr, 2014; Seker and Eryi\u011fit, 2012) which rely on a regularized averaged perceptron and CRF respectively both with hand crafted features.", "startOffset": 73, "endOffset": 121}, {"referenceID": 13, "context": "xi are then fed to a Bi-LSTM which is composed of two LSTMs (Hochreiter and Schmidhuber, 1997) treating the input forwards and backwards respectively.", "startOffset": 60, "endOffset": 94}, {"referenceID": 19, "context": "To model the dependencies between the corresponding labels of consecutive input units, we follow a conditional random field (CRF) (Lafferty et al., 2001) based approach.", "startOffset": 130, "endOffset": 153}, {"referenceID": 5, "context": "It has been shown that modeling units of information in a natural language input as fixed length vectors is more effective at encoding semantic properties of the words compared to deciding on the features apriori (Collobert et al., 2011; Turian et al., 2010).", "startOffset": 213, "endOffset": 258}, {"referenceID": 36, "context": "It has been shown that modeling units of information in a natural language input as fixed length vectors is more effective at encoding semantic properties of the words compared to deciding on the features apriori (Collobert et al., 2011; Turian et al., 2010).", "startOffset": 213, "endOffset": 258}, {"referenceID": 35, "context": "We train and evaluate our model with a corpus which is widely used in previous work on Turkish NER (T\u00fcr et al., 2003).", "startOffset": 99, "endOffset": 117}, {"referenceID": 35, "context": "We observed many morphological analysis errors and incorrect entity taggings in the corpus (T\u00fcr et al., 2003), probably as a result of automated analysis and labeling.", "startOffset": 91, "endOffset": 109}, {"referenceID": 28, "context": "We obtained word embeddings1 of Turkish words as vectors of length 100 using the skipgram algorithm (Mikolov et al., 2013) on a corpus of 951M words (Yildiz et al.", "startOffset": 100, "endOffset": 122}, {"referenceID": 41, "context": ", 2013) on a corpus of 951M words (Yildiz et al., 2016), 2,045,040 of which are unique.", "startOffset": 34, "endOffset": 55}, {"referenceID": 8, "context": "However, when we performed the McNemar\u2019s test (Dietterich, 1998), we observed that the difference between them is not significant at 95% confidence level.", "startOffset": 46, "endOffset": 64}, {"referenceID": 17, "context": "0 corpus in CoNLL format as other studies did (Konkol and Konop\u0131\u0301k, 2013).", "startOffset": 46, "endOffset": 73}, {"referenceID": 31, "context": "The performance of (Seker and Eryi\u011fit, 2012) without gazetteers is 89.", "startOffset": 19, "endOffset": 44}, {"referenceID": 18, "context": "55%, (Kuru et al., 2016) does not employ any external data and (Demir and \u00d6zg\u00fcr, 2014) still relies on hand-crafted features despite exploiting", "startOffset": 5, "endOffset": 24}, {"referenceID": 7, "context": ", 2016) does not employ any external data and (Demir and \u00d6zg\u00fcr, 2014) still relies on hand-crafted features despite exploiting", "startOffset": 46, "endOffset": 69}, {"referenceID": 18, "context": "(Kuru et al., 2016) 91.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "(Demir and \u00d6zg\u00fcr, 2014) 91.", "startOffset": 0, "endOffset": 23}, {"referenceID": 31, "context": "(Seker and Eryi\u011fit, 2012) 91.", "startOffset": 0, "endOffset": 25}, {"referenceID": 20, "context": "In this work, we demonstrated a new state-of-theart system for Turkish and Czech named entity recognition using the model of (Lample et al., 2016).", "startOffset": 125, "endOffset": 146}], "year": 2017, "abstractText": "In this work, we present new state-ofthe-art results of 93.59% and 79.59% for Turkish and Czech named entity recognition based on the model of (Lample et al., 2016). We contribute by proposing several schemes for representing the morphological analysis of a word in the context of named entity recognition. We show that a concatenation of this representation with the word and character embeddings improves the performance. The effect of these representation schemes on the tagging performance is also investigated.", "creator": "LaTeX with hyperref package"}}}