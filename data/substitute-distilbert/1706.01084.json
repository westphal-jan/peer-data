{"id": "1706.01084", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2017", "title": "Joint Text Embedding for Personalized Content-based Recommendation", "abstract": "learning a good representation of text is commonplace to many recommendation applications. examples include oracle recommendation where texts to be recommended are constantly published everyday. however, most existing recommendation techniques, such as matrix, based simulations, mainly rely on interaction histories to learn accounts of items. providing latent factors of items can be learned effectively from user interaction data, of many cases, such data is not available, especially since newly emerged items.", "histories": [["v1", "Sun, 4 Jun 2017 14:48:28 GMT  (1494kb,D)", "https://arxiv.org/abs/1706.01084v1", null], ["v2", "Fri, 23 Jun 2017 21:55:56 GMT  (1494kb,D)", "http://arxiv.org/abs/1706.01084v2", "typo fixes"]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.LG", "authors": ["ting chen", "liangjie hong", "yue shi", "yizhou sun"], "accepted": false, "id": "1706.01084"}, "pdf": {"name": "1706.01084.pdf", "metadata": {"source": "META", "title": "Joint Text Embedding for Personalized Content-based Recommendation", "authors": ["Ting Chen", "Liangjie Hong", "Yue Shi", "Yizhou Sun"], "emails": ["tingchen@cs.ucla.edu", "lhong@etsy.com", "yueshi@acm.org", "yzsun@cs.ucla.edu"], "sections": [{"heading": null, "text": "In this work, we aim to address the problem of personalized recommendation for completely new items with text information available. We cast the problem as a personalized text ranking problem and propose a general framework that combines text embedding with personalized recommendation. Users and textual content are embedded into latent feature space. e text embedding function can be learned end-to-end by predicting user interactions with items. To alleviate sparsity in interaction data, and leverage large amount of text data with li le or no user interactions, we further propose a joint text embedding model that incorporates unsupervised text embedding with a combination module. Experimental results show that our model can signi cantly improve the e ectiveness of recommendation systems on real-world datasets."}, {"heading": "1 INTRODUCTION", "text": "Personalized recommendation has gained a lot of a ention during the past few years [15, 25, 30]. Many models and algorithms have been proposed for personalized recommendation, among which, collaborative ltering techniques such as matrix factorization [14, 24] are shown to be most e ective. For these approaches, historical behavior data is critical for learning latent factors of both users and items. However, in many scenarios, behavior data is not available or very sparse, which motivates us to incorporate content/text information for recommendation. In this work, we study the problem of content-based recommendation for completely new items/texts, where historical user behavior data is not available for new items at the time of recommendation. However, when it comes to text \u2217Work done while the rst author was an intern at Yahoo! Research. \u2020Now at Facebook.\nConference\u201917, Washington, DC, USA 2016. 978-x-xxxx-xxxx-x/YY/MM. . .$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn\narticle recommendations, it is not straightforward to incorporate text content into existing collaborative ltering models.\nIn order to understand content of new items/texts for be er recommendation, a good representation based on textual information is essential. is issue is challenging and has not been satisfyingly solved yet. On one hand, traditional content-based [21] recommendation methods are usually based on simple text processing methods such as cosine similarity or logistic regression where both text and users are represented as bag-of-words. e limitations of such representation include the inability to encode similarity between words, as well as losing word order information [10, 19]. On the other hand, for collaborative ltering methods, although some of which has been extended to incorporate auxiliary information, text feature extraction functions are usually simple, and cannot leverage recent proposed representation learning techniques for text [5, 22, 27].\nWe address these issues with an approach that marries text embedding to personalized recommendation. In our proposed model, users and texts are simultaneously embedded into latent space where preferences can be depicted by simple dot product. While each user is directly associated with an embedding vector, text embedding requires an embedding function that maps a text sequence into a vector. Both user embedding and text embedding function can be trained end-to-end based on user-item interactions directly. With sophisticated neural networks (e.g., Convolutional Neural Networks) as text embedding function, high-level textual features can be be er captured.\nWhile end-to-end training of the embedding function delivers focused supervision for learning the task related representations. Interaction data is usually sparse, and there are still large amount of unlabeled data/corpora. Hence, we further propose a joint text embedding model to leverage unsupervised text embeddings that are pre-trained on large-scale unlabeled copora. To e ectively fuse both types of information, a novel combination module is constructed and incorporated into the uni ed framework. Experimental results on two real-world data sets demonstrate the e ectiveness of the proposed joint text embedding framework.\nar X\niv :1\n70 6.\n01 08\n4v 2\n[ cs\n.I R\n] 2\n3 Ju\nn 20"}, {"heading": "2 PRELIMINARIES AND RELATEDWORK", "text": ""}, {"heading": "2.1 Problem De nition", "text": "We use X = (x1, \u00b7 \u00b7 \u00b7 ,xN ) to denote the set of texts, the i-th text is represented by a sequence of words, i.e. xi = (w1, \u00b7 \u00b7 \u00b7 ,wt ). A matrixC is used to denote the historical interactions between users and texts, where Ci j indicates interaction between a user i and a text article j, such as click-or-not, like-or-not. 1\nGiven text information X and historical interaction data C , our goal is to learn a model which can rank completely new texts for an existing user i based on this user\u2019s interests and the text content."}, {"heading": "2.2 Personalized Recommendation", "text": "Existing methods of personalized recommendation algorithms can be roughly categorized into there categories: (1) collaborative ltering methods, (2) content-based methods, and (3) hybrid methods.\nMatrix factorization (MF) techniques [14, 24] is one of the most e ective collaborative ltering (CF) methods. In MF, each user or item is associated with latent factor vectors u or v and the score between a pair of user and item is computed by their dot product, i.e. si j = uTi vj . Since each item j is associated with latent factors vj , an new item cannot be handled properly as the training of vj depends on its interaction with users.\nContent based methods [21] usually build model for user and content based on term weighting schemes like TF-IDF. And cosine similarity or logistic regression can be used to match between a pair of user and item. It is di cult to work with such representations to encode similarities between words, as well as word orders.\nHybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31]. However, most of these methods cannot deal with completely new items.\nere are some work aiming at leveraging neural networks for be er text recommendations, such as Collaborative Deep Learning [31], and others [1]. Compared to their work, 1) we treat the problem as a ranking problem instead of a rating prediction problem, thus pairwise loss functions are adopted; 2) our model provide a more general framework, enabling various text embedding functions, thus subsumes [1] as a special case; 3) our model incorporates unsupervised text embedding from large-scale unlabeled corpora."}, {"heading": "2.3 Text Embedding", "text": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19]. Text embedding techniques aim at mapping text into vector representation that can be utilized for future predictive tasks. Such models have been proposed for addressing text classi cation/categorization problem [10, 12, 16]. Our task resembles a personalized text classi cation/ranking problem, in the sense that we try to classify/rank an article according to its interestingness w.r.t. a given user. Also, we utilize user behavior instead of labels of text as a supervised signal.\n1We consider C as implicit feedback in this work, which means only positive interactions are provided, and non-interactions are treated as negative feedback implicitly."}, {"heading": "3 THE PROPOSED MODEL", "text": "In this section, we rst introduce the supervised text embedding framework, which is trained in an end-to-end fashion for predicting user-item interactions. en we propose a joint text embedding model by incorporating unsupervised text embedding with a combination function."}, {"heading": "3.1 Supervised Text Embedding Framework", "text": "To simultaneously capture interests of users and semantics of texts, we embed both user and text into a common latent feature space, where dot product can be used to quantify their proximity.\nEach user i is directly associated with an embedding vector ui , which represents user\u2019s interests. For a text sequence x j for the j-th item, it is mapped into a xed-sized vector by an embedding function f (x j ) \u2208 Rk . e proximity score si j between the user and item pair (i, j) is computed by the dot product between their embeddings, as follows:\nsi j = u T i f (x j )\nText embedding function f (x). In our framework, the text embedding function is very exible. It can be speci ed by any di erentiable function that maps a text sequence into a x-sized embedding vector. Many neural network structures can be applied, such as Convolutional Neural Networks, Recurrent Neural Networks, and etc. Here we introduce two such functions, MoV and CNN, while other extensions are straightforward.\nMean of Vectors (MoV). To represent a text sequence x of length T , we rst embed each word in the text with an embedding vector w [18, 19], and then use the average of word embeddings to form the text/article embedding as follows:\nh = 1 t T\u2211 i=1 wxi\nTo be er extract non-linear interactions among words, a denselyconnected layer with non-linear activation can be applied. A single layer of such transformation is given by:\nh\u2032 = Relu(Wh + b)\nwhere Relu(\u00b7) = max(0, \u00b7) is Recti ed Linear Unit. Convolutional Neural Networks (CNN). Although MoW model is simple and relatively e cient, since text sequence is treated as a bag of words, orderings among words are ignored. As demonstrated in [10], ordering information of words can be helpful. To\nScore\n( : > ) ( : > ) \u2026...\naddress the issue, Convolutional Neural Networks is adopted for text embedding.\nIn CNN, instead of averaging over all word embeddings, it maintains several lters of given size(s). Each lter will slide over the whole text sequence. Additionally, at each position, an activation is computed by a dot product between the lter and local embedding vectors. To be more speci c, we use D = wx1 \u2295wx2 \u2295 \u00b7 \u00b7 \u00b7 \u2295wxT \u2208 RT\u00d7k to denote the concatenation of word vectors for the text. To apply convolution on text sequence D, we compute the j-th entry from applying i-th lter according to:\ncij = Relu(W j \u00b7 Di :i+s\u22121 + b j ).\nHereW j \u2208 Rs\u00d7k is the j-th lter of size s , and b j is the bias term. e output c of convolution layer can be downsized by pooling operator, such as taking max over all temporal dimensions of c, so a xed sized vector h can be produced. Due to the page limit, we refer the reader to [12] for more clear detailed descriptions.\nObjective Function and Training. To learn the user embedding and text embedding function, the output scores for each pair of user and item are used to predict their interactions. For a given user, we want to rank his/her interested articles higher than those he/she is not. So for each user i , a pair of a positive item p and a negative item n are both sampled, and similar to [23], the score di erence between positive and negative items is maximized, leading to a pairwise ranking loss function as follows:\nL = E(i,p,n)\u223cD [ \u2212 log\u03c3 (sip \u2212 sin ) ] (1)\nwhere p is a positive item for user i , and n is a negative item for user i . Each triplet (i,p,n) is drawn from some prede ned data distribution D. And \u03c3 is sigmoid function. e objective can be optimized by Stochastic Gradient Descent (SGD). In order to get triplets {(i,p,n)} for training, positive interactions {(i,p)} are rst sampled, and then negative items are sampled according to some prede ned distribution (e.g. item frequency).\ne framework is demonstrated in Figure 1. We name the above proposed framework TER, short for Text Embedding for contentbased Rcommendation."}, {"heading": "3.2 Incorporating Unsupervised Text Embedding", "text": "ere are two challenges faced by the supervised text embedding framework proposed above: 1) user-item interaction data may be sparse, and 2) there are many texts with li le to none user interactions. ese issues can lead to over- ing. To alleviate sparsity in interaction data and leverage a large amount of text data with li le to none user interactions, we propose to incorporate unsupervised text embedding with a new combination function. e overall framework with joint text embedding is summarized in Figure 2a.\nDi erent from the supervised model, a pre-trained text embedding module is added, so each text is rst mapped into two embedding vectors: h1 from text embedding function f (x) and h2 from pre-trained embedding matrix M . en to generate a cohesive text embedding vector for the item, we propose a combination function \u0434(h1,h2) to explicitly combine h1 and h2. Below we introduce these two additional components in detail.\nUnsupervised Text EmbeddingMatrixM . Unlike supervised text embedding, which requires user interactions for training mapping function f (x). e unsupervised text embedding can be pretrained with only text articles themselves, requiring no additional labels. To leverage a large-scale text corpus, we adopt Paragraph Vector [16] in our framework.\nGiven a set of text articles, Paragraph Vector associates each word i with a word embedding vector wi and each document d with a document embedding vector vd . To learn both types of embedding vectors simultaneously, a prediction task is formed: for each word occurrence, we rstly hide the word, and then model is asked to predict the exact word given neighboring word embeddings and the document embedding. e probability of the word is given as follows:\nP(token = i) = exp(vTdwi ) exp(\u2211j vTdw j ) As introduced in [16], the model is trained by maximum likelihood with negative sampling.\nA er training Paragraph Vector on the whole corpus, which includes text articles that have no related user interaction associated. We obtain a pre-trained text embedding module with embedding\nmatrix M , where each row Mi is an unsupervised text embedding vector for the i-th text article.\nCombination Function\u0434(h1,h2). To combine both text embedding vectors, i.e. h1 from text embedding function f (x), andh2 from pre-trained embedding matrix M , we introduce a combination function \u0434(h1,h2) \u2208 Rk where k is the user-de ned output dimension. Since the relation between two text embedding vectors h1 and h2 can be complicated and non-linear, in order to combine them e ectively, we specify the combination function \u0434(\u00b7) with a small neural network: Firstly a concatenation of the two vectors are formed, i.e. h = [h1,h2], and then it is further transformed by a denselyconnected layer with non-linear activations, i.e. h\u2032 = Relu(Wh +b).\nAlthough unsupervised text embeddings can provide useful text features [16], they might not be directly relevant to the task. So to control the degree of trust for unsupervised text embeddings, we introduce dropout [28] into unsupervised text vectors, i.e. h2, which randomly select entries and set them to zero. On one hand, when se ing the dropout to zero, the whole embedding vector is utilized; on the other hand, when se ing the dropout to one, the whole text vector is set to zero, hence it is equivalent to use none of pre-trained embeddings. When the dropout rate is between zero and one, it can be seen as a trade-o for the unsupervised module. Figure 2b illustrates the combination module.\nTraining of the Joint Model. e training procedure is separated into two stages. At the rst stage, a unsupervised text embedding matrix M is trained using unlabeled texts. At the second stage, similar to the supervised framework, the training objective is also pairwise ranking objective in Eq. 1. e parameters in the second stage involve both user embeddings and parameters in f (x) and \u0434(h1,h2). Finally we name the extended model TER+."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we present our empirical studies on two real-world text recommendation data sets."}, {"heading": "4.1 Data Collections", "text": "Two real-world data sets are used. e rst data set CiteULike, containing user-bookmarking-article behavior data from CiteULike.org, was provided in [30]. It contains 5,551 users, 16,980 items, and 204,986 interactions. e second data set is Yahoo! News Feed2. We randomly sampled 10,000 users (with at least 10 click behaviors) and their clicked news to form the data set, which contains 58,579 items, and 515,503 interactions. Since CiteULike and News data sets have both title and abstract/summary, for each data set, we create following two data sets: one contains only title information (i.e. short text), and the other contains both title and summary/abstract (i.e. long text). e average lengths of short text in CiteULike and News are 9 and 11 respectively, and that of long text are 194 and 89 respectively.\nTo ensure items at the test time are completely new, we rst select a portion (20%) of items to form the pool of test items. All user interactions with those test items are held-out during training, only the remaining user-item interactions are used as training data. For unsupervised text embedding pre-training, we also include many\n2h ps://webscope.sandbox.yahoo.com/catalog.php?datatype=r&did=75\ntexts that have no user interaction data. More speci cally, for CiteULike data set, additional 339,150 papers from DBLP (a superset of CiteULike) are included; and for news data set, additional 3,935,228 news articles are also included.\ne detailed data set statistics are shown in Table 1 and 2."}, {"heading": "4.2 Comparing Methods and Settings", "text": "We compare following methods in experiments: \u2022 Cosine similarity matching [21], which is based on similarities\nof TF-IDFs between candidate and user\u2019s historical items. \u2022 Regularized multi-task logistic regression [7], which can be\nseen as one-layer linear text model. \u2022 CDL (Collaborative Deep Learning) [31], which simultane-\nously trains auto-encoder for encoding text content, and matrix factorization for encoding user behavior. \u2022 Content Pre-trained, which rst pre-trains text embeddings by Paragraph Vector, and then used as xed item features for matrix factorization. \u2022 TER. is is our proposed supervised framework. Note that two variants of text embedding function f (x) are compared: MoV and CNN. \u2022 TER+. is is the joint text embedding framework. Both text embedding functions, MoV and CNN, are compared.\nParameter Settings: For CDL, both TER and TER+, we set the dimensions of both user embedding and nal text embedding vector to 50 for fair comparisons. For CNN, we use 50 lters with lter size of 3. Regularization is added using both weight decay on user embedding and dropout on item embedding. We use Adam [13] with learning rate of 0.001. For both baselines and our model, we tune the parameters with grid search.\nEvaluation Metrics: We adopt MAP (Mean Average Precision) and average AUC for evaluation. First, for each interaction between a user and a test item in the test set, we sample 10 negative samples from a test item-pool to form the candidate set. en, AP and AUC are computed based on the rankings given by the model and the nal MAP and average AUC are averaged over all users."}, {"heading": "4.3 Performance Comparison", "text": "Table 3 shows MAP and AUC results of di erent methods on four data sets. As shown in the results, our methods (both TER and\nTER+) consistently beat other baselines and achieve state-of-theart performance. Other several important observations can also be made from the results: 1) representation learning or embeddings methods (our methods, pre-trained method and CDL) can achieve be er results compared to traditional TF-IDF based methods, 2) the joint supervised and unsupervised text embedding can achieve be er results compared to supervised or unsupervised text embedding alone, and 3) the advantage of our model on short texts is more signi cant compared to longer one. We also observe that Mov outperforms CNN in some cases (e.g. in CiteULike data sets), we conjecture this is due to that words in CiteUlike may be more indicative w.r.t. user interests so simpler embedding functions can already well capture the semantics.\nFigure 3 shows performances of di erent dropout rate for pretrained text embedding vector h2 in combination function \u0434(h1,h2). We observe that, as dropout rate increases, most of the curves go up and then go down. e peak occurs mostly around 0.2 to 0.4, both the 0 and 1 two extreme points have worse results. is further con rms the e ectiveness of incorporating unsupervised text embedding, and also show that certain level of noise injected into pre-trained text embedding can improve performance."}, {"heading": "4.4 Case Studies", "text": "To further understand the proposed model, we conduct several case studies looking into the layout or nearest neighbors of words and articles in the embedding space.\nTo visualize the text embedding learned from di erent models, we rstly choose top conferences in ve domains (ML, DM, CV, HCI, BIO), and then randomly select articles that are published\nin those conferences. We apply TSNE [17] to visualize 2d map for these articles, and color them according to their domains of publication. e results are shown in Figure 4 where we found that our combined model can best distinguish papers from di erent domains.\nTable 4 shows similar words for given queried words, i.e. \u201cneural\u201d and \u201clearning\u201d, in CiteULike data set. From the result we clearly see the distinction between meanings of word learned from both methods. For example, the nearest word \u201cneural\u201d learned in unsupervised text embedding (articles with and without user like behavior) is mostly related to arti cial neural networks, but in supervised text embedding, it is mostly related to neuroscience, which is more close to biology. is is because that in the CiteULike data set, there exist a lot of biologists, so the word embedding learned from supervised text embedding is likely to be dominated by the neuroscience perspective. However, by incorporating the unsupervised text embedding learned from a larger corpus, more meanings of the words can be recovered.\nTable 5 shows the similar articles given a randomly selected queried article. We nd that although unsupervised text embedding can provide some similar articles, the proposed framework (both TER and TER+) can be er capture the similarity of articles."}, {"heading": "5 RELATEDWORK", "text": "Our work is related to both personalized recommendation and text embedding and understanding.\nCollaborative ltering [15] has been one of the most e ective methods in recommender system. Methods like matrix factorization [14, 24] are wide adopted, and recently some methods based on neural networks are also explored [26, 31, 33]. Content based methods are proposed [2, 21], but has not been well developed to exploit deep semantics of content information. Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31]. In our case, although we have historical data about users\u2019 interactions with items, but at the time of recommendation we are considering the items that have never been seen before, which cannot be handle directly by most existing matrix factorization based methods. Our model is similar to CDL [31], but with following di erences: (1) we treat the problem as ranking instead of rating prediction problem, (2) we provide a general framework which allows exible choice\nof text embedding function f (x), and (3) our model can explicitly incorporate unsupervised text embedding.\nTo understand text data, both supervised and unsupervised methods are proposed. Supervised methods are usually guided by text labels, such as sentiment labels or category labels. Di erent from traditional text classi cation, which train SVM or logistic regression classi ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11]. ose methods cannot be directly applied for recommendation as only a global classi cation/ranking model is provided. Also, instead of using labels as in existing supervised text embedding methods,\nwe utilize user item interactions as supervision to learn the text embedding function. ere are also unsupervised text embedding techniques [16, 18, 19], which do not require labels but cannot adapt to the task of interest.\nWe further generalize the proposed model and develop e cient training techniques in [3]."}, {"heading": "6 CONCLUSIONS", "text": "In this work, we tackle the problem of content-based recommendation for completely new texts. An novel joint text embedding based framework is proposed, in which user embedding and text embedding function are learned end-to-end based on interactions\nbetween users and items. e text embedding function is exible, and can be speci ed by deep neural networks. Both supervised and unsupervised text embeddings are fused together by an combination module as part of a uni ed model. Empirical evaluations based on real-world data sets demonstrate that our model can achieve state-of-the-art results for recommending new texts. As for the future work, it is interesting to explore other ways of incorporating unsupervised text embeddings."}, {"heading": "ACKNOWLEDGEMENTS", "text": "e authors would like to thank Qian Zhao, Yue Ning, and Qingyun Wu for helpful discussions. Yizhou Sun is partially supported by NSF CAREER #1741634."}], "references": [{"title": "Ask the GRU: Multi-task Learning for Deep Text Recommendations", "author": ["Trapit Bansal", "David Belanger", "Andrew McCallum"], "venue": "In Proceedings of the 10th ACM Conference on Recommender Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identi\u0080cation", "author": ["Ting Chen", "Yizhou Sun"], "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "On Sampling Strategies for Neural Network-based Collaborative Filtering", "author": ["Ting Chen", "Yizhou Sun", "Yue Shi", "Liangjie Hong"], "venue": "In Proceedings of the 23th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Entity Embedding-based Anomaly Detection for Heterogeneous Categorical Events", "author": ["Ting Chen", "Lu-An Tang", "Yizhou Sun", "Zhengzhang Chen", "Kai Zhang"], "venue": "In Proceedings of the Twenty-Fi\u0087h International Joint Conference on Arti\u0080cial Intelligence (IJCAI\u201916). Miami", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "SVDFeature: a toolkit for feature-based collaborative \u0080ltering", "author": ["Tianqi Chen", "Weinan Zhang", "Qiuxia Lu", "Kailong Chen", "Zhao Zheng", "Yong Yu"], "venue": "Journal of Machine Learning Research 13,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bo\u008aou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research 12,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Regularized multi\u2013task learning", "author": ["\u008ceodoros Evgeniou", "Massimiliano Pontil"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Content-based recommendations with poisson factorization", "author": ["Prem K Gopalan", "Laurent Charlin", "David Blei"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["\u008corsten Joachims"], "venue": "In European conference on machine learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "E\u0082ective use of word order for text categorization with convolutional neural networks", "author": ["Rie Johnson", "Tong Zhang"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Bag of Tricks for E\u0081cient Text Classi\u0080cation", "author": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1607.01759", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Convolutional neural networks for sentence classi\u0080cation", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P Kingma", "Jimmy Lei Ba"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative \u0080ltering model", "author": ["Yehuda Koren"], "venue": "In Proceedings of the 14th ACMSIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Distributed Representations of Sentences and Documents", "author": ["\u008boc V Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geo\u0082rey Hinton"], "venue": "Journal of Machine Learning Research 9,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "E\u0081cient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Je\u0082rey Dean"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems", "author": ["T Mikolov", "J Dean"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "\u008cumbs up?: sentiment classi\u0080cation using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Content-based recommendation systems", "author": ["Michael J Pazzani", "Daniel Billsus"], "venue": "In \u008ae adaptive web", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Factorization machines", "author": ["Ste\u0082en Rendle"], "venue": "IEEE International Conference on Data Mining", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the twenty-\u0080\u0087h conference on uncertainty in arti\u0080cial intelligence", "author": ["Ste\u0082en Rendle", "Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt- \u008cieme"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Probabilistic matrix factorization", "author": ["Ruslan Salakhutdinov", "Andriy Mnih"], "venue": "In NIPS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Restricted Boltzmann machines for collaborative \u0080ltering", "author": ["Ruslan Salakhutdinov", "Andriy Mnih", "Geo\u0082rey Hinton"], "venue": "In Proceedings of the 24th international conference on Machine learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Autorec: Autoencoders meet collaborative \u0080ltering", "author": ["Suvash Sedhain", "Aditya Krishna Menon", "Sco\u008a Sanner", "Lexing Xie"], "venue": "In Proceedings of the 24th International Conference on World Wide Web", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Relational learning via collective matrix factorization", "author": ["Ajit P Singh", "Geo\u0082rey J Gordon"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Dropout: a simple way to prevent neural networks from over\u0080\u008aing", "author": ["Nitish Srivastava", "Geo\u0082rey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research 15,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Document modeling with gated recurrent neural network for sentiment classi\u0080cation", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Collaborative topic modeling for recommending scienti\u0080c articles", "author": ["Chong Wang", "David M Blei"], "venue": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Collaborative deep learning for recommender systems", "author": ["Hao Wang", "Naiyan Wang", "Dit-Yan Yeung"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Character-level convolutional networks for text classi\u0080cation", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "A Neural Autoregressive Approach to Collaborative Filtering", "author": ["Yin Zheng", "Bangsheng Tang", "Wenkui Ding", "Hanning Zhou"], "venue": "arXiv preprint arXiv:1605.09477", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}], "referenceMentions": [{"referenceID": 23, "context": "Personalized recommendation has gained a lot of a\u008aention during the past few years [15, 25, 30].", "startOffset": 83, "endOffset": 95}, {"referenceID": 28, "context": "Personalized recommendation has gained a lot of a\u008aention during the past few years [15, 25, 30].", "startOffset": 83, "endOffset": 95}, {"referenceID": 13, "context": "Many models and algorithms have been proposed for personalized recommendation, among which, collaborative \u0080ltering techniques such as matrix factorization [14, 24] are shown to be most e\u0082ective.", "startOffset": 155, "endOffset": 163}, {"referenceID": 22, "context": "Many models and algorithms have been proposed for personalized recommendation, among which, collaborative \u0080ltering techniques such as matrix factorization [14, 24] are shown to be most e\u0082ective.", "startOffset": 155, "endOffset": 163}, {"referenceID": 19, "context": "On one hand, traditional content-based [21] recommendation methods are usually based on simple text processing methods such as cosine similarity or logistic regression where both text and users are represented as bag-of-words.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "\u008ce limitations of such representation include the inability to encode similarity between words, as well as losing word order information [10, 19].", "startOffset": 137, "endOffset": 145}, {"referenceID": 17, "context": "\u008ce limitations of such representation include the inability to encode similarity between words, as well as losing word order information [10, 19].", "startOffset": 137, "endOffset": 145}, {"referenceID": 4, "context": "On the other hand, for collaborative \u0080ltering methods, although some of which has been extended to incorporate auxiliary information, text feature extraction functions are usually simple, and cannot leverage recent proposed representation learning techniques for text [5, 22, 27].", "startOffset": 268, "endOffset": 279}, {"referenceID": 20, "context": "On the other hand, for collaborative \u0080ltering methods, although some of which has been extended to incorporate auxiliary information, text feature extraction functions are usually simple, and cannot leverage recent proposed representation learning techniques for text [5, 22, 27].", "startOffset": 268, "endOffset": 279}, {"referenceID": 25, "context": "On the other hand, for collaborative \u0080ltering methods, although some of which has been extended to incorporate auxiliary information, text feature extraction functions are usually simple, and cannot leverage recent proposed representation learning techniques for text [5, 22, 27].", "startOffset": 268, "endOffset": 279}, {"referenceID": 13, "context": "Matrix factorization (MF) techniques [14, 24] is one of the most e\u0082ective collaborative \u0080ltering (CF) methods.", "startOffset": 37, "endOffset": 45}, {"referenceID": 22, "context": "Matrix factorization (MF) techniques [14, 24] is one of the most e\u0082ective collaborative \u0080ltering (CF) methods.", "startOffset": 37, "endOffset": 45}, {"referenceID": 19, "context": "Content based methods [21] usually build model for user and content based on term weighting schemes like TF-IDF.", "startOffset": 22, "endOffset": 26}, {"referenceID": 4, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 90, "endOffset": 101}, {"referenceID": 20, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 90, "endOffset": 101}, {"referenceID": 25, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 90, "endOffset": 101}, {"referenceID": 7, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 131, "endOffset": 142}, {"referenceID": 28, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 131, "endOffset": 142}, {"referenceID": 29, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 131, "endOffset": 142}, {"referenceID": 29, "context": "\u008cere are some work aiming at leveraging neural networks for be\u008aer text recommendations, such as Collaborative Deep Learning [31], and others [1].", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "\u008cere are some work aiming at leveraging neural networks for be\u008aer text recommendations, such as Collaborative Deep Learning [31], and others [1].", "startOffset": 141, "endOffset": 144}, {"referenceID": 0, "context": "Compared to their work, 1) we treat the problem as a ranking problem instead of a rating prediction problem, thus pairwise loss functions are adopted; 2) our model provide a more general framework, enabling various text embedding functions, thus subsumes [1] as a special case; 3) our model incorporates unsupervised text embedding from large-scale unlabeled corpora.", "startOffset": 255, "endOffset": 258}, {"referenceID": 1, "context": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19].", "startOffset": 132, "endOffset": 154}, {"referenceID": 3, "context": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19].", "startOffset": 132, "endOffset": 154}, {"referenceID": 11, "context": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19].", "startOffset": 132, "endOffset": 154}, {"referenceID": 14, "context": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19].", "startOffset": 132, "endOffset": 154}, {"referenceID": 16, "context": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19].", "startOffset": 132, "endOffset": 154}, {"referenceID": 17, "context": "Recent advances in deep learning have demonstrated the importance of learning good representations for text and other types of data [2, 4, 12, 16, 18, 19].", "startOffset": 132, "endOffset": 154}, {"referenceID": 9, "context": "Such models have been proposed for addressing text classi\u0080cation/categorization problem [10, 12, 16].", "startOffset": 88, "endOffset": 100}, {"referenceID": 11, "context": "Such models have been proposed for addressing text classi\u0080cation/categorization problem [10, 12, 16].", "startOffset": 88, "endOffset": 100}, {"referenceID": 14, "context": "Such models have been proposed for addressing text classi\u0080cation/categorization problem [10, 12, 16].", "startOffset": 88, "endOffset": 100}, {"referenceID": 16, "context": "To represent a text sequence x of length T , we \u0080rst embed each word in the text with an embedding vector w [18, 19], and then use the average of word embeddings to form the text/article embedding as follows:", "startOffset": 108, "endOffset": 116}, {"referenceID": 17, "context": "To represent a text sequence x of length T , we \u0080rst embed each word in the text with an embedding vector w [18, 19], and then use the average of word embeddings to form the text/article embedding as follows:", "startOffset": 108, "endOffset": 116}, {"referenceID": 9, "context": "As demonstrated in [10], ordering information of words can be helpful.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "Due to the page limit, we refer the reader to [12] for more clear detailed descriptions.", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "So for each user i , a pair of a positive item p and a negative item n are both sampled, and similar to [23], the score di\u0082erence between positive and negative items is maximized, leading to a pairwise ranking loss function as follows:", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "To leverage a large-scale text corpus, we adopt Paragraph Vector [16] in our framework.", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "As introduced in [16], the model is trained by maximum likelihood with negative sampling.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "Although unsupervised text embeddings can provide useful text features [16], they might not be directly relevant to the task.", "startOffset": 71, "endOffset": 75}, {"referenceID": 26, "context": "So to control the degree of trust for unsupervised text embeddings, we introduce dropout [28] into unsupervised text vectors, i.", "startOffset": 89, "endOffset": 93}, {"referenceID": 28, "context": "org, was provided in [30].", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "\u2022 Cosine similarity matching [21], which is based on similarities of TF-IDFs between candidate and user\u2019s historical items.", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "\u2022 Regularized multi-task logistic regression [7], which can be seen as one-layer linear text model.", "startOffset": 45, "endOffset": 48}, {"referenceID": 29, "context": "\u2022 CDL (Collaborative Deep Learning) [31], which simultaneously trains auto-encoder for encoding text content, and matrix factorization for encoding user behavior.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "We use Adam [13] with learning rate of 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "We apply TSNE [17] to visualize 2d map for these articles, and color them according to their domains of publication.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "Methods like matrix factorization [14, 24] are wide adopted, and recently some methods based on neural networks are also explored [26, 31, 33].", "startOffset": 34, "endOffset": 42}, {"referenceID": 22, "context": "Methods like matrix factorization [14, 24] are wide adopted, and recently some methods based on neural networks are also explored [26, 31, 33].", "startOffset": 34, "endOffset": 42}, {"referenceID": 24, "context": "Methods like matrix factorization [14, 24] are wide adopted, and recently some methods based on neural networks are also explored [26, 31, 33].", "startOffset": 130, "endOffset": 142}, {"referenceID": 29, "context": "Methods like matrix factorization [14, 24] are wide adopted, and recently some methods based on neural networks are also explored [26, 31, 33].", "startOffset": 130, "endOffset": 142}, {"referenceID": 31, "context": "Methods like matrix factorization [14, 24] are wide adopted, and recently some methods based on neural networks are also explored [26, 31, 33].", "startOffset": 130, "endOffset": 142}, {"referenceID": 1, "context": "Content based methods are proposed [2, 21], but has not been well developed to exploit deep semantics of content information.", "startOffset": 35, "endOffset": 42}, {"referenceID": 19, "context": "Content based methods are proposed [2, 21], but has not been well developed to exploit deep semantics of content information.", "startOffset": 35, "endOffset": 42}, {"referenceID": 4, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 90, "endOffset": 101}, {"referenceID": 20, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 90, "endOffset": 101}, {"referenceID": 25, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 90, "endOffset": 101}, {"referenceID": 7, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 131, "endOffset": 142}, {"referenceID": 28, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 131, "endOffset": 142}, {"referenceID": 29, "context": "Hybrid methods can improve so-called \u201ccold-start\u201d issue by incorporating side information [5, 22, 27], or item content information [8, 30, 31].", "startOffset": 131, "endOffset": 142}, {"referenceID": 29, "context": "Our model is similar to CDL [31], but with following di\u0082erences: (1) we treat the problem as ranking instead of rating prediction problem, (2) we provide a general framework which allows \u0083exible choice", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 121, "endOffset": 128}, {"referenceID": 18, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 121, "endOffset": 128}, {"referenceID": 5, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 235, "endOffset": 246}, {"referenceID": 11, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 235, "endOffset": 246}, {"referenceID": 30, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 235, "endOffset": 246}, {"referenceID": 27, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 252, "endOffset": 256}, {"referenceID": 10, "context": "Di\u0082erent from traditional text classi\u0080cation, which train SVM or logistic regression classi\u0080ers based on n-gram features [9, 20], recent work take advantage of distributed representation brought by embedding methods, which include CNN [6, 12, 32], RNN [29] and others [11].", "startOffset": 268, "endOffset": 272}, {"referenceID": 14, "context": "\u008cere are also unsupervised text embedding techniques [16, 18, 19], which do not require labels but cannot adapt to the task of interest.", "startOffset": 53, "endOffset": 65}, {"referenceID": 16, "context": "\u008cere are also unsupervised text embedding techniques [16, 18, 19], which do not require labels but cannot adapt to the task of interest.", "startOffset": 53, "endOffset": 65}, {"referenceID": 17, "context": "\u008cere are also unsupervised text embedding techniques [16, 18, 19], which do not require labels but cannot adapt to the task of interest.", "startOffset": 53, "endOffset": 65}, {"referenceID": 2, "context": "We further generalize the proposed model and develop e\u0081cient training techniques in [3].", "startOffset": 84, "endOffset": 87}], "year": 2017, "abstractText": "Learning a good representation of text is key to many recommendation applications. Examples include news recommendation where texts to be recommended are constantly published everyday. However, most existing recommendation techniques, such as matrix factorization based methods, mainly rely on interaction histories to learn representations of items. While latent factors of items can be learned e\u0082ectively from user interaction data, in many cases, such data is not available, especially for newly emerged items. In this work, we aim to address the problem of personalized recommendation for completely new items with text information available. We cast the problem as a personalized text ranking problem and propose a general framework that combines text embedding with personalized recommendation. Users and textual content are embedded into latent feature space. \u008ce text embedding function can be learned end-to-end by predicting user interactions with items. To alleviate sparsity in interaction data, and leverage large amount of text data with li\u008ale or no user interactions, we further propose a joint text embedding model that incorporates unsupervised text embedding with a combination module. Experimental results show that our model can signi\u0080cantly improve the e\u0082ectiveness of recommendation systems on real-world datasets.", "creator": "LaTeX with hyperref package"}}}