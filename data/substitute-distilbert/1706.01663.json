{"id": "1706.01663", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Learning Pairwise Disjoint Simple Languages from Positive Examples", "abstract": "a classical function in grammatical construction is to collect a deterministic finite automaton ( dfa ) from many set of positive and negative examples. in this paper, we address the first - yet seemingly novel - problem of identifying a genus of dfas plus examples that belong to different unknown simple regular languages. we propose two methods and on compression for clustering the observed positive examples. we apply our search to a set of print jobs submitted to large industrial printers.", "histories": [["v1", "Tue, 6 Jun 2017 09:03:17 GMT  (292kb,D)", "http://arxiv.org/abs/1706.01663v1", "This paper has been accepted at the Learning and Automata (LearnAut) Workshop, LICS 2017 (Reykjavik, Iceland)"]], "COMMENTS": "This paper has been accepted at the Learning and Automata (LearnAut) Workshop, LICS 2017 (Reykjavik, Iceland)", "reviews": [], "SUBJECTS": "cs.LG cs.FL", "authors": ["alexis linard", "rick smetsers", "frits vaandrager", "umar waqas", "joost van pinxten", "sicco verwer"], "accepted": false, "id": "1706.01663"}, "pdf": {"name": "1706.01663.pdf", "metadata": {"source": "CRF", "title": "Learning Pairwise Disjoint Simple Languages from Positive Examples", "authors": ["Alexis Linard", "Rick Smetsers", "Frits Vaandrager", "Umar Waqas", "Joost van Pinxten", "Sicco Verwer"], "emails": ["f.vaandrager}@cs.ru.nl", "j.h.h.v.pinxten}@tue.nl", "s.e.verwer@tudelft.nl"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nA classical problem in grammatical inference is to find, i.e. learn, a regular language from a set of examples of that language. When this set is divided into positive examples (belonging to the language) and negative examples (not belonging to the language), the problem is typically solved by searching for the smallest deterministic finite automaton (DFA) that accepts the positive examples, and rejects the negative ones. Provided with sufficiently many examples, there exist algorithms that will correctly learn the unknown language from these examples [1]. This is not necessarily the case, however, if only positive examples are available.\nWe consider a setting where one can observe positive examples from multiple different regular languages, but it is not clear to which of these languages the examples belong. Moreover, it is known that each example belongs to exactly one of these languages, i.e. that the languages are disjoint. As a result, the positive examples for one language are negative examples for the other languages. The problem at hand is to cluster the examples by assigning them to a language.\nIn this paper, we present two clustering approaches based on an essential property of regular languages described by the pumping lemma. The pumping lemma is often used to prove that a certain language is not regular. It states that any sufficiently long example from a regular language has a middle section that may be repeated (i.e. pumped) to produce a new example that is also in that language [2]. Efficient algorithms can be used for finding such possible decompositions [3]. We use this set of possible decompositions for clustering our examples. The intuition behind this approach is that examples from the same language are more likely to have observable similarities than examples from different languages.\nOur problem is motivated by a case study of print jobs submitted to large industrial printers. Strings of symbols represent these print jobs, where each symbol denotes a different media type, such as a book cover or a newspaper page. Together, this set of print jobs make for a fairly complicated \u2018language\u2019. Nevertheless, we observed that each print job can be classified as belonging to one of a fixed set of categories, such as \u2018book\u2019 or \u2018newspaper\u2019. Two print jobs that belong to the same category are typically very similar, to the extent that they only differ in the number of repetitions of a certain media type. Therefore, the languages for the different categories are fairly simple. Our goal is to uncover these simple languages."}, {"heading": "II. PRELIMINARIES", "text": "a) Strings: Let \u03a3 denote a finite alphabet of symbols. A string is a finite sequence of symbols. The empty string is denoted . We denote with \u03a3\u2217 the set of all strings over \u03a3, and with \u03a3+ all nonempty strings over \u03a3 (i.e. \u03a3\u2217 = \u03a3+ \u222a { }). Similarly, we denote with \u03a3i the set of all strings over \u03a3 of length i. Let x be a string over \u03a3, then |x| denotes the length of x. We denote with xi the string x repeated i times, e.g. x2 = xx. We use x\u2265i to denote the language {xj : j \u2265 i}. Moreover, we use x+ to denote the language {xi : i \u2208 N+}, and x\u2217 to denote the language x+\u222a{ }. A string u is a (proper) prefix of x if there exists a (nonempty) string v such that x = uv. Similarly, u is a (proper) suffix of x if there exists a (nonempty) string v such that x = vu. We say u is a (proper) infix of x if there exist (nonempty) strings v, w such that x = vuw. In the remainder of this paper, we use a, b and c to refer to symbols in \u03a3, and we use r . . . z to refer to strings over \u03a3.\nb) Languages: A language is a set of strings. A regular language can be described by a deterministic finite automaton (DFA), which is a tuple (\u03a3, Q, q0, \u03b4, F ), where Q is a set of states, q0 \u2208 Q is the start state, \u03b4 : Q\u00d7\u03a3\u2192 Q is a transition function from states and symbols to states, and F \u2286 Q is a set of final states. The set of tails described by a state q is the set {x \u2208 \u03a3\u2217 : \u03b4(q, x) \u2208 F}. Hence, the set of tails of the start state of a DFA describes its language. Let q be a state, a be a symbol and u be a string, then we extend \u03b4 to \u03b4\u2217 : Q \u00d7 \u03a3\u2217 \u2192 Q by \u03b4\u2217(q, ) = q and \u03b4\u2217(q, au) = \u03b4\u2217(\u03b4(q, a), u). We slightly abuse\nar X\niv :1\n70 6.\n01 66\n3v 1\n[ cs\n.L G\n] 6\nJ un\n2 01\nnotation and simply use \u03b4 to refer to \u03b4\u2217 in the remainder of this paper.\nc) The pumping lemma: The pumping lemma [2] states that all sufficiently long strings of a regular language contain an infix that may be pumped, i.e. repeated an arbitrary number of times to construct a new string that is in the language.\nLet L be a regular language, then there exists an integer p \u2265 1 depending only on L such that every string x \u2208 L of length at least p can be written as x = uvw such that |v| \u2265 1, |uv| \u2264 p and \u2200i \u2265 0, uviw \u2208 L. The intuition is that v will always lead back to the same state, i.e. \u03b4(q0, u) = \u03b4(\u03b4(q0, u), v) = q. Therefore, a string uviw is in the language iff w is in the set of tails of q.\nWe say that two strings x and y have the same pumping decomposition if x = uviw and y = uvjw for some u, v, w, i and j.\nd) Compression: Compression refers to the practice of encoding information using fewer units of information (e.g. bits) than the original representation. The function that provides such an encoding is called a compressor. We formally define a compressor as a so-called \u2018code word length function\u2019 C : \u03a3\u2217 \u2192 N that maps strings to the length of their compressed encoding. A normal compressor can be used to approximate the Kolmogorov complexity [4] of a string (i.e. the length of the shortest computer program that produces the string as output), as the Kolmogorov complexity itself is not computable. In [5, Definition 3.1 and Lemma 3.4], a normal compressor is defined as follows.\nDefinition 1. A compressor C is normal if it satisfies, up to an additive O(log n) term, with n the maximal length of an element involved in the (in)equality concerned, the following axioms:\n1) C(uu) = C(u) and C( ) = 0 (idempotency) 2) C(uv) = C(vu) (symmetry) 3) C(uv) \u2264 C(u) + C(v) (subadditivity)\nThe normalized compression distance (NCD) is a quasiuniversal measure for the similarity of two strings. It is an approximation of their (uncomputable) information distance, which measures the minimum information required to go from one string to the other or vice versa. The NCD for two strings x, y is defined in [5] as follows.\nNCD(x, y) = max{C(xy)\u2212 C(x), C(yx)\u2212 C(y)}\nmax{C(x), C(y)} (1)"}, {"heading": "III. CLUSTERING BY COMPRESSION", "text": "The NCD can be used to cluster strings based on their pumping decomposition.\na) Pumping lemma and clustering based on NCD: Let us consider a setting where L and L\u2032 are two disjoint regular languages, i.e. L \u2229 L\u2032 = \u2205. Let x, y \u2208 L two strings such that x = uviw and y = uvjw with i, j \u2265 0 and i 6= j. Let now z \u2208 L\u2032 be a string with z = rskt, k \u2265 0 and (r 6= u or s 6= v or t 6= w). We note here that L and L\u2032 are special cases of simple regular languages in which any string belonging to\nthe language can be fully described by the star-free regular expression of the language, together with the multiplicities of the repetitions.\nWe now consider a proper normal compressor C, which would reduce any string in L or L\u2032 into the respective regular expressions of L and L\u2032, up to an additive O(log n) term (the number of bits needed to encode the multiplicities).\nThus, we can say that, up to an additive O(log n) term, \u2200x, y \u2208 L C(x) = C(y) = C(regexp(L))\nWe also assume that the normal compressor we use satisfies, up to an additive O(log n) term, the equality C(xy) = C(x), since a proper normal compressor C would compress equally the concatenation of two similar strings, i.e. belonging to the same simple language, and the strings separately. Since we know that C(x) = C(y), and that C(xy) = C(yx) by symmetry, we can then reformulate the NCD between x and y as\nNCD(x, y) = C(xy)\u2212 C(x)\nC(x) = 0\nAs in our setting, L and L\u2032 are two disjoint regular languages not sharing the same regular expressions, we can thus assume that\nNCD(x, y) \u2264 NCD(x, z) and NCD(x, y) \u2264 NCD(y, z)\nBy considering pairwise disjoint regular languages, we can say that a clustering based on the NCD will regroup sufficiently long strings in the format defined above and belonging to the same language.\nIn practice, note that the additive O(log n) term has an impact on the purity of the clustering. We will therefore carry out different clusterings of strings, possibly corresponding to clear distinct languages, depending on the granularity of the clustering, and of the number of clusters to find.\nGiven a set of strings, the pairwise NCDs are used to compute a distance matrix. The next step is to establish clusters from the previously computed distance matrix. The output of hierarchical clustering is a dendrogram, a binary tree with one leaf for every string. A dendrogram is composed of internal nodes, linking leaves or subtrees. The level of the nodes is set up according to a pre-defined linkage matrix computed as a function of the distance matrix.\nTo build the dendrogram, hierarchical agglomerative clustering algorithms find and merge the pair of clusters with the lowest linkage, and create a parent node at the level of the linkage. We gather the dendrogram, and thus clusters are computed by varying the level of linkage as a threshold. We assume that the number of languages to get, thus the right threshold to set, is known beforehand.\nAs shown in the example in Figure 1, clusters are created and highlighted with different colors, assuming a threshold of 0.15 for the separation. In this case, we gather 4 clusters, from which it is possible to infer distinct languages. It is straightforward here to see that, for a given cluster Cx, we can infer a language Lx such that \u2200i \u2265 0, uviw \u2208 Lx. In order to do so, it is possible for any cluster Cx to infer\nthe corresponding language Lx (for example, by using state merging [6]), using all strings clustered in Cx as positive examples, and all strings in \u22c3 y Cy : y 6= x as negative examples. We then gather 4 languages L1...4 respectively standing for a+ (blue), a+b+ (red), ab+a (green) and (ab)+ (purple). b) Granularity of Clustering: As shown above, choosing the right threshold enables getting the desired number of clusters. As a consequence, we assume that the right number of clusters is provided, in order to enhance the identification of different languages. In case the wrong number of clusters would have been provided, or an inadequate threshold found, the granularity of the clustering, that is to say, how fine the strings are clustered together, would have changed. More precisely, a greater threshold would tend to generalize the optimal solution (a solution with a threshold of 0.2 would give 3 languages, namely a+, (ab)+ and a+b+a[0..1]) whereas a smaller threshold would provide finer languages (up to singleton sets)."}, {"heading": "IV. TANDEM REPEATS", "text": "Another approach is to first compute the pumping decompositions of the strings (and regroup the strings accordingly) by computing tandem repeats.\nDefinition 2. A string v is a tandem repeat in x if x = uviw for some i \u2265 2, u,w \u2208 \u03a3\u2217 and v 6= .\nThere exist algorithms that can detect tandem repeats in a string [3], [7], [8], [9]. We relate the pumping lemma for regular languages to the identification of tandem repeats, in a context of simple regular languages.\na) Pumping lemma and identification of languages: According to the pumping lemma, if L is a regular language, then there exists an integer p \u2265 1 depending only on L such that every string x \u2208 L of length at least p can be written as x = uvw such that |v| \u2265 1, |uv| \u2264 p and \u2200i \u2265 0, uviw \u2208 L. If, in addition L is a simple language, then there is only a finite number of such decompositions. Therefore, if we assume an upper bound on the number of states of the DFA for L, two sufficiently long strings belong to the same language if they contain a similar prefix, tandem repeat, and suffix.\nAlgorithms for finding tandem repeats in a string can thus be used to cluster strings. Let us consider the string x = uvvw. Tandem repeat finder algorithms will naturally find v as being a tandem repeat. As a consequence, we can use the tandem repeat found to describe the string as x = uv2w. We can then generalize this to infer the simple language {uviw : i \u2265 2}.\nNow, let us consider a string x = abcbcbcbcd. String x contains two distinct tandem repeats, namely bc and bcbc. By taking the longest tandem repeat that is not a tandem repeat itself, we can say x = a(bc)4d. By generalizing, we can infer the simple language {a(bc)id : i \u2265 2}.\nTandem repeats can also be computed recursively. Let us consider the string x = aaabcbcaaabcbcaaabcbc. We can first detect the tandem repeat aaabcbc, and state that x = (aaabcbc)3. If we again process the result, we can detect tandem repeats a and bc, and describe x as x = (a3(bc)2)4. By generalizing, we can infer the simple language {(ai(bc)j)k : i, j, k \u2265 2}.\nb) Ambiguities: Let x be the string abaabaab. Then, x contains several possible branching tandem repeats, namely t1 = baa, t2 = aab or t3 = aba. Thus, we can describe the language for x in different ways, namely L1 = a(baa)\u22652b, L2 = ab(aab)\u22652 or L3 = (aba)\u22652ab. Nevertheless, all these representations are equal, i.e. \u2200i \u2265 0, a(baa)ib = ab(aab)i = (aba)iab. Therefore, we claim that any of these representations is valid in order to infer the language. It does not matter which one choose, as long as we check for equivalence.\nc) Hierarchical clustering of languages: We have defined several steps to generalize pumping decompositions of strings based on tandem repeats. As shown above, we can describe strings as x = uviw for all i \u2265 2, and cluster them into uv\u22652w. We then propose to lower the multiplicity of v to \u2265 1, to get uv+w. The next step of generalization is to get uv\u2217w by lowering the multiplicity of v to \u2265 0. We can then imagine \u03a3\u2217 to be an ultimate generalization. Indeed, {uvvw} \u2282 uv\u22652w \u2282 uv+w \u2282 uv\u2217w \u2282 \u03a3\u2217. In this generalization process, some strings might share some generalizations from given steps. In Table I we propose such a hierarchical clustering of regular languages, from singleton sets to \u03a3\u2217.\nV. INDUSTRIAL APPLICATION\nOur case study has been inspired by an industrial problem related to the domain of large-scale printers. Recent work [10] focused on the impact of design parameters of an industrial printer on its productivity. It appeared in the aforementioned\nstudy that the productivity depends on the print-job being rendered. To that end, the prior identification of the different print-job patterns is crucial in enabling printer engineers to find out optimal parameters related to paper flow, according to each significant type of job being printed. It is also important to identify significant usages of the printers, in order to set up components as a function of these usages. Indeed, the productivity of large-scale printers is sensitive to changes of media types, since some component settings (such as printheads height or paper speed for instance) need to be readjusted each time a different media type is used. Thus, frequent changes of the type of sheets within a print job may lead to a decrease of productivity due to different component settings, if not correctly supported.\nWe possess a dataset containing more than 23.000 distinct strings, each representing a print job. Each print job is composed of several pages, that can be of different media types (papers of different thickness, sizes, etc.). Thus, each string is composed of letters standing for the media-type of the printed page. We could have for instance aaaaaabbbbbbbbbb that would represent a print job consisting of 6 pages of type a, and 10 pages of type b.\nA characteristic of our dataset, and of our strings to cluster, is that the occurrence of new symbols in the strings follows alphabetical order. For instance, if we consider \u03a3 = {a, b, c}, our strings will always start with an a, followed eventually by either an a or a b, etc. In Table II, we present a sample of the significant print job patterns identified using the method involving tandem repeats."}, {"heading": "VI. CONCLUSION AND FURTHER WORK", "text": "In this paper, we have described different heuristics for learning pairwise disjoint simple languages from positive\nexamples. The results obtained within the scope of our industrial case study indicate that these methods are applicable in practice. Nevertheless, we realize that the success of our methods is related to the simplicity of the languages we learn. Therefore, in future work we aim to formally define a class of languages for which our methods work."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Lou Somers and Patrick Vestjens1 for providing industrial datasets as well as required expertise related to the case study. This research is supported by the Dutch Technology Foundation (STW) under the Robust CPS program (project 12693) and by the Netherlands Organisation for Scientific Research (NWO) project on Learning Extended State Machine for Malware Analysis (LEMMA, project 628.001.009)."}], "references": [{"title": "Language identification in the limit", "author": ["E.M. Gold"], "venue": "Information and Control, vol. 10, no. 5, pp. 447 \u2013 474, 1967. [Online]. Available: http://dx.doi.org/10.1016/S0019-9958(67)91165-5", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1967}, {"title": "Finite automata and their decision problems", "author": ["M.O. Rabin", "D. Scott"], "venue": "IBM journal of research and development, vol. 3, no. 2, pp. 114\u2013125, 1959.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1959}, {"title": "Linear time algorithms for finding and representing all the tandem repeats in a string", "author": ["D. Gusfield", "J. Stoye"], "venue": "Journal of Computer and System Sciences, vol. 69, no. 4, pp. 525\u2013546, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Clustering by compression", "author": ["R. Cilibrasi", "P.M. Vit\u00e1nyi"], "venue": "IEEE Transactions on Information theory, vol. 51, no. 4, pp. 1523\u20131545, 2005.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Grammatical inference: learning automata and grammars", "author": ["C. de la Higuera"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "An optimal algorithm for computing the repetitions in a word", "author": ["M. Crochemore"], "venue": "Information Processing Letters, vol. 12, no. 5, pp. 244\u2013250, 1981.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1981}, {"title": "Optimal off-line detection of repetitions in a string", "author": ["A. Apostolico", "F.P. Preparata"], "venue": "Theoretical Computer Science, vol. 22, no. 3, pp. 297\u2013315, 1983.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1983}, {"title": "An o (n log n) algorithm for finding all repetitions in a string", "author": ["M.G. Main", "R.J. Lorentz"], "venue": "Journal of Algorithms, vol. 5, no. 3, pp. 422\u2013432, 1984.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1984}, {"title": "A fast estimator of performance with respect to the design parameters of self re-entrant flowshops", "author": ["U. Waqas", "M. Geilen", "S. Stuijk", "J. v. Pinxten", "T. Basten", "L. Somers", "H. Corporaal"], "venue": "Euromicro Conference on Digital System Design, Aug 2016, pp. 215\u2013221. 1{lou.somers,patrick.vestjens}@oce.com", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Provided with sufficiently many examples, there exist algorithms that will correctly learn the unknown language from these examples [1].", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "pumped) to produce a new example that is also in that language [2].", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "Efficient algorithms can be used for finding such possible decompositions [3].", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "c) The pumping lemma: The pumping lemma [2] states that all sufficiently long strings of a regular language contain an infix that may be pumped, i.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "The NCD for two strings x, y is defined in [5] as follows.", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "the corresponding language Lx (for example, by using state merging [6]), using all strings clustered in Cx as positive examples, and all strings in \u22c3 y Cy : y 6= x as negative examples.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "There exist algorithms that can detect tandem repeats in a string [3], [7], [8], [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "There exist algorithms that can detect tandem repeats in a string [3], [7], [8], [9].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "There exist algorithms that can detect tandem repeats in a string [3], [7], [8], [9].", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "There exist algorithms that can detect tandem repeats in a string [3], [7], [8], [9].", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": "Recent work [10] focused on the impact of design parameters of an industrial printer on its productivity.", "startOffset": 12, "endOffset": 16}], "year": 2017, "abstractText": "A classical problem in grammatical inference is to identify a deterministic finite automaton (DFA) from a set of positive and negative examples. In this paper, we address the related \u2013 yet seemingly novel \u2013 problem of identifying a set of DFAs from examples that belong to different unknown simple regular languages. We propose two methods based on compression for clustering the observed positive examples. We apply our methods to a set of print jobs submitted to large industrial printers.", "creator": "LaTeX with hyperref package"}}}