{"id": "1706.01967", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Synergistic Union of Word2Vec and Lexicon for Domain Specific Semantic Similarity", "abstract": "semantic similarity measures are an extensive part in natural language processing tasks. however semantic similarity measures defined for general use don't lie well within specific domains. today in this study we introduce a domain reference semantic similarity measure that was created by the synergistic union charity word2vec, a word testing method that is used for semantic similarity calculation and lexicon based ( lexical ) semantic similarity methods. we see that this proposed methodology out of word embedding methods trained on dynamic corpus and vocabulary trained on domain specific corpus but don't use lexical terms similarity methods may augment the comparison. further, we prove that text lemmatization can improve the performance of word embedding methods.", "histories": [["v1", "Tue, 6 Jun 2017 20:45:30 GMT  (1663kb,D)", "https://arxiv.org/abs/1706.01967v1", "6 Pages, 3 figures"], ["v2", "Fri, 9 Jun 2017 01:54:32 GMT  (1664kb,D)", "http://arxiv.org/abs/1706.01967v2", "6 Pages, 3 figures"]], "COMMENTS": "6 Pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["keet sugathadasa", "buddhi ayesha", "nisansa de silva", "amal shehan perera", "vindula jayawardana", "dimuthu lakmal", "madhavi perera"], "accepted": false, "id": "1706.01967"}, "pdf": {"name": "1706.01967.pdf", "metadata": {"source": "CRF", "title": "Synergistic Union of Word2Vec and Lexicon for Domain Specific Semantic Similarity", "authors": ["Keet Sugathadasa", "Buddhi Ayesha", "Nisansa de Silva", "Amal Shehan Perera", "Vindula Jayawardana", "Dimuthu Lakmal", "Madhavi Perera"], "emails": ["keetmalin.13@cse.mrt.ac.lk"], "sections": [{"heading": null, "text": "keywords: Word Embedding, Semantic Similarity, Neural Networks, Lexicon, word2vec\nI. INTRODUCTION\nSemantic Similarity measurements based on linguistic features are a fundamental component of almost all Natural Language Processing (NLP) tasks: Information Retrieval, Information Extraction, and Natural Language Understanding [1]. In the case of NLP based Information Retrieval (IR), it plays into the task of obtaining the items that are most relevant to the query whereas Information Extraction (IE), plays into the task of correctly recognizing the linguistic elements to be extracted be it the Part of Speech (PoS) tags or Named Entities in Named Entity Recognition (NER). In the case of Text Understanding which is also known as Natural Language Understanding (NLU), it helps in identifying semantic connections between elements on the document that is being analyzed.\nLaw and order could be rather regarded as the cloak of invisibility that operates and controls the human behavior to its possible extents in the name of justice. Thus in terms of maintaining social order, quiddity of law within the society is mandatory. John Stuart Mill articulated a principle in On Liberty, where he stated that The only purpose for which power can be rightfully exercised over any member of a civilized community, against his will, is to prevent harm to others [2]. Being such a vital field, acquisition of laws and legal documents through technological means is on the list of necessities on the rise. Also, ample justification for the need\nof semantic disambiguation in the legal domain can be found in seminal cases such as: Fagan v MPC and R v Woollin. Therefore we selected the law as the domain for this study.\nThe legal domain contains a considerable amount of domain specific jargon where the etymology lies with mainly Latin and English. To complicate this fact, in certain cases, the meaning of the words and context differs by the legal officers interpretations.\nAnother field which suffers similarly from this issue is the medical industry [3]. Non-systematic organization and complexity of the medical documents result in medical domain falling victim to loss of having proper representations for its terminology. PubMed [4] attempts to remedy this. Later studies such as [5] utilize these repositories. Therefore, it is possible to claim that the problem addressed in this study is one that is not just limited to the legal domain but is one that transcends into a numerous other domains as well.\nMethods that treat words as independent atomic units is not sufficient to capture the expressiveness of language [6]. A solution to this is word context learning methods [7]\u2013 [9]. Another solution is lexical semantic similarity based methods [10]. Both of these approaches try to capture semantic and syntactic information of a word. In this study we propose a methodology to have a synergistic union of both of these methods. First for word context learning, we used a Word Embedding [7] method, word2vec [6]. Then we used a number of lexical semantic similarity measures [11]\u2013[13] to augment and improve the result.\nThe hypothesis of this study has three main claims: (1) A word embedding model trained on a small domain specific corpus can outperform a word embedding model trained on a large but generic corpus, (2) Word lemmatization, which removes inflected forms of words, would improve the performance of a word embedding model, (3) Usage of lexical semantic similarity measures trained over a machine learning system can improve the overall system performance. Our results sufficiently prove all of these claims to be true.\nThe structure of the paper is as follows. Section II gives a brief overview on the current tools being used and domains that have tackled this problem of word representations. Section III gives a description on the methodology being used in\nar X\niv :1\n70 6.\n01 96\n7v 2\n[ cs\n.C L\n] 9\nJ un\n2 01\nthis research in order to obtain the results and conclusions as necessary. That is followed by Section IV that presents and analyses results. The paper ends with Section V which gives the conclusion and discusses future work."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "This section illustrates the background of the techniques used in this study and work carried out by others in various areas relevant to this research. The subsections given below are the important key areas in this study."}, {"heading": "A. Lexical Semantic Similarity Measures", "text": "Lexical Semantic similarity of two entities is a measure of the likeness of the semantic content of those entities, most commonly calculated with the help of topological similarity existing within an ontology such as WordNet [14]. Wu and Palmer proposed a method to give the similarity between two words in the 0 to 1 range [11]. In comparison, Jiang and Conrath proposed a method to measure the lexical semantic similarity between word pairs using corpus statistics and lexical taxonomy [12]. Hirst & St-Onge\u2019s system [13] quantifies the amount that the relevant synsets are connected by a path that is not too long and that does not change direction often. The strengths of each of these algorithms were evaluated in [10] by means of [15]."}, {"heading": "B. Word Vector Embedding", "text": "Traditionally, in Natural Language Processing systems, words are treated as atomic units ignoring the correlation between the words as they are represented just by indices in a vocabulary [6]. To solve the inadequacies of that approach, distributed representation of words and phrases through word embeddings was proposed [16]. The idea is to create vector representations for each of the words in a text document along with word meanings and relationships between the words all mapped to a common vector space.\nA number of Word Vector Embedding systems have been proposed such as: GloVe [17], Latent Dirichlet Allocation (LDA) [18], and word2vec1 [19]. GloVe uses a word to neighboring word mapping when learning dense embeddings, which uses a matrix factorization mechanism. LDA uses a similar approach via matrices, but the concept is based on mapping words with relevant sets of documents. word2vec uses a neural network based approach that uses word to neighboring word mapping. Due to the flexibility and features it provided in terms of parameter passing when training the model using a text corpus, we use word2vec in this study. word2vec supports two main training models: Skip-gram [20] and Continuous Bag Of Words (CBOW) [19]."}, {"heading": "C. Legal Information Systems", "text": "Schweighofer [21], claims that there is a huge vacuum that should be addressed in eradicating the information crisis that the applications in the field of law suffer from. This vacuum is evident by the fact that, despite being important, there is a\n1https://code.google.com/p/word2vec/\nscarcity of legal information systems. Even though the two main commercial systems; WestLaw2 and LexisNexis3 are widely used, they only provide query based searching, where legal officers need to remember keywords which are predefined when querying for relevant legal information, there is still a hassle in accessing this information.\nOne of the most popular legal information retrieval systems is KONTERM [21], which was developed to represent document structures and contents. However, it too suffered from scalability issues. The currently existing implementation that is closest to our proposed model is Gov2Vec [22], which is a system that creates vector representations of words in the legal domain, by creating a vocabulary from across all corpora on, supreme court opinions, presidential actions and official summaries of congressional bills. It uses a neural network [7] to predict the target word with the mean of its context words\u2019 vectors. However, the text copora used here, itself was not sufficient enough to represent the entire legal domain. In addition to that, the Gov2Vec trained model is not available to use used by legal professionals or to be tested against."}, {"heading": "III. METHODOLOGY", "text": "This section describes the research that was carried out. Each section below, addresses a component in the overall methodology. An overview of the methodology we propose is illustrated in Fig. 1.\nThe first phase of this study was to gather the necessary legal cases from on-line repositories. We obtained over 35000 legal case documents, pertaining to various areas of practices in law, from Findlaw [23] by web crawling. Due to this reason, the system tends to be generalized well over many aspects of law."}, {"heading": "A. Text Lemmatization", "text": "The linguistic process of mapping inflected forms of a word to the word\u2019s core lemma is called lemmatization [24]. The crawled natural language text would contain words of all inflected forms. However, the default word2vec model does not run a lemmatizer before the word vector embeddings are calculated; which results in each of the inflected forms of a single word ending up with a separate embedding vector. This in turn leads to many drawbacks and inefficiencies. Maintaining a separate vector for each inflected form of each word makes the model bloat up an consume memory unnecessarily. This especially leads to problems in building the model. Further, having separate vector for each inflected form weakens the model because the values for words originating from the same lemma will be distributed over those multiple vectors. For example, when we search for similar words to the noun input \u201djudge\u201d, we get the following words as similar words: Judge, judges, Judges. Similarly, for verb input \u201dtrain\u201d, the model would return the following set of words: training, trains, trained.\n2https://www.westlaw.com/ 3https://www.lexisnexis.com/\nAs shown in Fig. 1, we use the raw legal text to train the word2vecLR model. But for both word2vecLL model and word2vecLLS model, we first lemmatize the legal document corpus to map all inflected forms of the words to their respective lemmas. For this task we use the Stanford CoreNLP library [25]."}, {"heading": "B. Training word2vec models", "text": "In this stage we trained wod2vec models. One was on the raw legal text corpus and the other was on the lemmatized law text corpus. Each input legal text corpus included text from over 35000 legal case documents amounting to 20 billion words in all. The training took over 2 days. As mentioned in the Section III-A, the model trained by the raw legal text corpus is the word2vecLRmodel shown in Fig. 1. The model trained on lemmatized law text corpus is word2vecLL. Further, a clone of the trained word2vecLLis passed forward to III-C in order to build the word2vecLLSmodel. Following are the important parameters we specified in training these models.\n\u2022 size (dimensionality): 200 \u2022 context window size: 10 \u2022 learning model: CBOW \u2022 min-count: 5 \u2022 training algorithm: hierarchical softmax For this phase, we used a neural network with a single hidden layer as depicted in figure 2. As mentioned above, to train the system to output a user-specified dimensional vector space, we picked the Continuous Bag Of Words approach for learning weights. The rationale as to why CBOW learning model was picked over skip-gram is that, Skip-gram is said to be accurate for infrequent words whereas CBOW is faster by a factor of window size which is also more appropriate for larger text corpora.\nIn CBOW, the current word or the target word is being predicted based on the context words, within the specified context window size.\nIn addition to the above trained models, we obtained the word2vecGmodel which was trained by Google on the Google News dataset. As shown in Fig. 1, this is a generic text corpus that contain data pertaining to a large number of topics. It is not fine tuned to the legal domain as the three models (word2vecLR, word2vecLL, and word2vecLLS) that we train. However, Google\u2019s model was trained over on around 100 billion words that add up to 3,000,000 unique phrases. This model was built with layer size of 300. Due to the general nature and the massive case of word2vecGmodel, it is possible to use the comparison of results obtained by our models against the results obtained by this model to showcase the effectiveness of a model trained using a specific domain in the applications of that domain over a model trained using a generic domain in application on the same specific domain."}, {"heading": "C. Lexical Semantic Similarity Enhancements", "text": "At this step we used established lexical semantic similarity measures to enhance the output. As mentioned in\nSection III-B, we get a clone of the trained word2vecLLmodel to use in this process. Unlike in the cases of the previous models where the training system is internal to the word2vec model, here it was important that the model is trained explicitly using n-fold cross validation. As such, a training dataset where each entry is a key value pair is obtained. The key k is a lemma of a word in the legal domain and the value is an array of lemmas of words G that are most relevant in the legal domain to the word lemma used as the key. The said array is of length l. Following paragraphs explain how the training of the model and the testing happen for the first fold in n-fold cross validation. It is imperative to understand that the same process will be done for each of the folds subsequently.\n1) Mathematical model: The first step was to obtain an entry from the training set and to query the word2vecLLmodel with the key lemma. Let us define an integer n such that n = Cl where C > 1. When executing the query, word2vecLLmodel was instructed to return the first n elements that matches the query best along with the word2vec similarity values. Let us call this resultant Matrix R. R has n columns where wi is the word similar to k and di is the word2vec similarity between k and wi. Equation 1 shows R.\nR = [ w1 w2 w3 . . . wn d1 d2 d3 . . . dn ] (1)\nFrom R, we created the vectors W and D as shown in Equation 2 and Equation 3 respectively. Each wi and di has the same value as they had in R.\nW = {w1, w2, w3, ..., wn} (2)\nD = {d1, d2, d3, ..., dn} (3)\nWe defined the following functions for words wi and wj . The lexical semantic similarity calculated between wi and wj using the Wu & Palmer\u2019s model [11] was given by wup(wi, wj). The lexical semantic similarity calculated by Jiang & Conrath\u2019s model [12] was given by jcn(wi, wj). hso(wi, wj) was used to indicate the lexical semantic similarity calculated using the Hirst & St-Onge\u2019s system [13].\nNext we created the lexical semantic similarity matrix M . The matrix M is a 4\u00d7N matrix where N has the same value as in Equation 1. The first row element m1,i of M was calculated by taking the Wu & Palmer similarity between k and wi. Here wi element at index i of W . Thus, m1,i is equal to wup(k,wi). Similarly, the second row element m2,i of M was calculated by taking the Jiang & Conrath similarity between k and wi. The third row consists of Hirst & St-Onge similarities while the forth row contains a series of 1s for the sake of the bias. The matrix M is shown in Equation 4.\nM =  wup(k,w1) wup(k,w2) . . . wup(k,wn) jcn(k,w1) jcn(k,w2) . . . jcn(k,wn) hso(k,w1) hso(k,w2) . . . hso(k,wn)\n1 1 . . . 1\n (4)\nWe defined the normalizing function given in Equation 5 to return a value xnorm when given a value xraw that exists between the maximum value vmax and the minimum value vmin. The returned xnorm is a double value that has the range [0, 1].\nxnorm = normalize(xraw, vmin, vmax) (5)\nWe created the matrix SM from the matrix M by calculating each smi,j using the Equation 5 on each mi,j . For this we used the min and max values shown in table I.\nWe defined the value matrix V using the vector D and the Matrix SM as shown by Equation 6.\nV = [ D SM ]T (6)\nWe defined the vector E where each element ei is given by activating a neural network by values vi, where vi is the ith row of V . The training of the aforementioned neural network is explained in Section III-C2.\n2) Machine Learning for weight calculation: The motive behind using machine learning is to train the system to take into account how each similarity measure value can be used to derive a new, compound, and better representative value for similarity. As mentioned in Section III-C1, a neural network was chosen as the machine learning method. Initially the weight values were initialized to random values and E was calculated. Next the matrix MI was defined as shown in Equation 7.\nMI = [ W E ] (7)\nThe matrix Y was obtained by sorting the columns of matrix MI in the descending order of elements in the second row. We defined a seeking function given in Equation 8 to return the index of word w in the first row of matrix P . If the word w does not exist in the first row of matrix P , it returns the column count of the matrix P . Also, observe that the new matrix Y has the same form as the initial matrix R shown in Equation 1. This symmetrical representation is important because it gives us the opportunity to use the same accuracy measures on all the word2vec models in Section IV to achieve a fair comparison.\ns = seek(w,P ) (8)\nNext we defined the error err according to Equation 10 where the value of xi was derived from Equation 9 and is a small constant.\nxi = { 1, if seek(gi, Y ) < l n\u2212seek(gi,Y )\nn\u2212l , otherwise (9)\nerr = 1\u2212 +\nl\u2211 i=1 xi\n|G \u22c2 W |+\n(10)\nThis error err is used to adjust the neural network. This training cycle is continued until convergence. The completed model that is trained this way is named word2vecLLSmodel."}, {"heading": "D. Query processing", "text": "In order to use and test our models, we built a query processing system. A user can enter a query in the legal domain using the provided interface. The system then takes the query and applies natural language processing techniques such as PoS tagging until the query is through the NLP pipeline to be lemmatized. We used the same Stanford Core NLP pipeline that we used in Section III-A for this task. The reason for this is to bring all the models to the same level to be compared equally. We have shown this step in Fig. 1."}, {"heading": "E. Experiments", "text": "We got experts in the legal field to create a golden standard to test our models. The golden standard includes 100 concepts with each containing 5 words that are most related to the given concept in the legal domain picked from a pool of over 1500 words by the legal experts.\nThe accuracy levels of these experiments are measured in terms of precision and recall [1], which are common place measures in information retrieval. The logical functionality of these two are based on the comparison of an expected result and the effective result of the evaluated system.\nIf the Golden Standard Word Vector is G and the word vector returned by the model is W (Same naming conventions as Section III-C1), the recall in this study is calculated with equation 11. This measures the completeness of the model. Our recall calculation used the same function suggested in [1].\nrecall = |G \u22c2 W |\n|G| (11)\nThe precision calculation in this study is not as clear cut as it is described in [1]. This is because in those systems the precision is only a matter of set membership and thus would simply be ratio between the correctly found similar words over the total number of returned words. However, in the case of word2vec models, it is the user who input the number of matching words to retrieve. Thus, it is wrong to use the total number of returned words to calculate the precision in cases where there is prefect recall. In the cases of imperfect recall, the classical precision is adequate.\nWhile word2vec make precision calculation difficult as shown above, it also has a quality that makes finding the solution to that problem somewhat easy. That is the fact that the returning word list of similar words is sorted in the\ndescending order. This is the same property that we used in the above Section III-C2 to calculate the error. Thus it is logical to derive that the precision is given by equation 12 where err is the error calculated by equation 10.\nprecision = 1\u2212 err (12)"}, {"heading": "IV. RESULTS", "text": "This section includes results obtained from the four different models (word2vecG, word2vecLR, word2vecLL, and word2vecLLS) as introduced in Section III-B. Results shown in table II were obtained for different k values, where k is the number of words requested from each of the models. As expected, the F1 of each model increases with k, where the possibility of finding the correct similar words against the golden standard increases. Given that the task here is to return the expected set of words, the recall is more important than precision (i.e: False-Negatives are more harmful than False-Positives). In that light, it is obvious that the word2vecLLSperforms better than all other models because it consistently has the highest recall for all values of k. In addition to that, the word2vecLLSmodel also has the highest F1 for all values of k, which is sufficient proof that the small loss in precision does not adversely affect the overall result.\nA graphical comparison of the changes in the F1 measure is shown in Fig. 3. As shown, the domain specific models such as word2vecLR, word2vecLL, and word2vecLLS , show better results than the general word2vecGmodel. It should be noted that this performance enhancement has happened despite the fact that word2vecGwas trained on a text corpus 3 times bigger than the text corpora we have used in this study for the models word2vecLR, word2vecLL, and word2vecLLS .\nIn the comparison of domain specific models, we can see a clear distinction between the word2vecLR and word2vecLL models, where the word2vecLL generally performs better. Further, it can be observed that the word2vecLLS model outperforms both word2vecLR and word2vecLL models."}, {"heading": "V. CONCLUSION AND FUTURE WORKS", "text": "The hypothesis of this study had three main claims and each of these claims were justified by the results presented in Section IV. The first claim of the hypothesis is that a word embedding model trained on a small domain specific corpus can out perform a word embedding model trained on a large but generic corpus. The success of word2vecLRmodel over the word2vecGmodel justifies this claim. In Section III-A we proposed the second claim: word lemmatization, which removes inflected forms of words and improve the performance of a word embedding model. word2vecLLmodel obtaining better results than word2vecLRmodel proved this claim to be true. The third claim was made in Section III-C. There, we proposed that usage of lexical semantic similarity measures trained over a machine learning system can improve the overall system performance. The significant improvement that we show for the word2vecLLSmodel over the word2vecLLmodel verified this claim also to be accurate. Therefore we can conclude that the proposed methodology of word vector embedding augmented by lexical semantic similarity measures, gives a more accurate evaluation of the extent of which a given pair of words is semantically similar in the considered domain.\nSemantic similarity measures are important in many areas of applications. Out of those, for future work, we expect to extend the findings of this study to the document level. Word based semantic similarity is the building block for sentence similarity measures, which in turn aggregates to build document similarity measures. This is the direction in which we intend to move. We will be using this word semantic similarity measure to build up to a document similarity measure which can be used for more efficient domain based document retrieval systems."}], "references": [{"title": "Ontology-based information extraction: An introduction and a survey of current approaches", "author": ["D.C. Wimalasuriya", "D. Dou"], "venue": "Journal of Information Science, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Representation of change in controlled medical terminologies", "author": ["D.E. Oliver", "Y. Shahar", "E.H. Shortliffe", "M.A. Musen"], "venue": "Artificial intelligence in medicine, vol. 15, no. 1, pp. 53\u201376, 1999.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Discovering inconsistencies in pubmed abstracts through ontology-based information extraction", "author": ["N. de Silva", "D. Dou", "J. Huang"], "venue": "ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB), p. to appear, 2017.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of machine learning research, vol. 3, no. Feb, pp. 1137\u20131155, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Semap-mapping dependency relationships into semantic frame relationships", "author": ["N. de Silva", "C. Fernando", "M. Maldeniya", "D. Wijeratne", "A. Perera", "B. Goertzel"], "venue": "17th ERU Research Symposium, vol. 17. Faculty of Engineering, University of Moratuwa, Sri Lanka, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach", "author": ["H.T. Ng", "H.B. Lee"], "venue": "Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996, pp. 40\u201347.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Safs3 algorithm: Frequency statistic and semantic similarity based semantic classification use case", "author": ["N. de Silva"], "venue": "Advances in ICT for Emerging Regions (ICTer), 2015 Fifteenth International Conference on. IEEE, 2015, pp. 77\u201383.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Verbs semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": "Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics, ser. ACL \u201994. Stroudsburg, PA, USA: Association for Computational Linguistics, 1994, pp. 133\u2013138. [Online]. Available: http://dx.doi.org/10.3115/981732.981751", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1994}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["J.J. Jiang", "D.W. Conrath"], "venue": "Proc of 10th International Conference on Research in Computational Linguistics, ROCLING97, 1997.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Lexical chains as representations of context for the detection and correction of malapropisms", "author": ["G. Hirst", "D. St-Onge"], "venue": "WordNet: An electronic lexical database, vol. 305, pp. 305\u2013332, 1998.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Introduction to wordnet: An on-line lexical database", "author": ["G.A. Miller", "R. Beckwith", "C. Fellbaum", "D. Gross", "K.J. Miller"], "venue": "International journal of lexicography, vol. 3, no. 4, pp. 235\u2013244, 1990.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "2016) Wordnet similarity for java (ws4j)", "author": ["H. Shima"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 2013, pp. 3111\u20133119.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation.", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in EMNLP, vol", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Gaussian lda for topic models with word embeddings.", "author": ["R. Das", "M. Zaheer", "C. Dyer"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "word2vec explained: Deriving mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Y. Goldberg", "O. Levy"], "venue": "arXiv preprint arXiv:1402.3722, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A simple word embedding model for lexical substitution", "author": ["O. Melamud", "O. Levy", "I. Dagan", "I. Ramat-Gan"], "venue": "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, 2015, pp. 1\u20137.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Legal expert system kontermautomatic representation of document structure and contents", "author": ["E. Schweighofer", "W. Winiwarter"], "venue": "International Conference on Database and Expert Systems Applications. Springer, 1993, pp. 486\u2013497.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1993}, {"title": "Gov2vec: Learning distributed representations of institutions and their legal text", "author": ["J.J. Nay"], "venue": "arXiv preprint arXiv:1609.06616, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Rules for mediation in findlaw for legal professionals", "author": ["J. Hughes"], "venue": "1999.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Stemming and lemmatization in the clustering of finnish text documents", "author": ["T. Korenius", "J. Laurikkala", "K. J\u00e4rvelin", "M. Juhola"], "venue": "Proceedings of the thirteenth ACM international conference on Information and knowledge management. ACM, 2004, pp. 625\u2013633.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "The stanford corenlp natural language processing toolkit.", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J.R. Finkel", "S. Bethard", "D. McClosky"], "venue": "ACL (System Demonstrations),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Semantic Similarity measurements based on linguistic features are a fundamental component of almost all Natural Language Processing (NLP) tasks: Information Retrieval, Information Extraction, and Natural Language Understanding [1].", "startOffset": 227, "endOffset": 230}, {"referenceID": 1, "context": "Another field which suffers similarly from this issue is the medical industry [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "Later studies such as [5] utilize these repositories.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "Methods that treat words as independent atomic units is not sufficient to capture the expressiveness of language [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "A solution to this is word context learning methods [7]\u2013 [9].", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "A solution to this is word context learning methods [7]\u2013 [9].", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "Another solution is lexical semantic similarity based methods [10].", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "First for word context learning, we used a Word Embedding [7] method, word2vec [6].", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "First for word context learning, we used a Word Embedding [7] method, word2vec [6].", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "of lexical semantic similarity measures [11]\u2013[13] to augment and improve the result.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "of lexical semantic similarity measures [11]\u2013[13] to augment and improve the result.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "Lexical Semantic similarity of two entities is a measure of the likeness of the semantic content of those entities, most commonly calculated with the help of topological similarity existing within an ontology such as WordNet [14].", "startOffset": 225, "endOffset": 229}, {"referenceID": 8, "context": "Wu and Palmer proposed a method to give the similarity between two words in the 0 to 1 range [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "In comparison, Jiang and Conrath proposed a method to measure the lexical semantic similarity between word pairs using corpus statistics and lexical taxonomy [12].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "Hirst & St-Onge\u2019s system [13] quantifies the amount that the relevant synsets are connected by a path that is not too long and that does not change direction often.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "The strengths of each of these algorithms were evaluated in [10] by means of [15].", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "The strengths of each of these algorithms were evaluated in [10] by means of [15].", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "Traditionally, in Natural Language Processing systems, words are treated as atomic units ignoring the correlation between the words as they are represented just by indices in a vocabulary [6].", "startOffset": 188, "endOffset": 191}, {"referenceID": 13, "context": "To solve the inadequacies of that approach, distributed representation of words and phrases through word embeddings was proposed [16].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "A number of Word Vector Embedding systems have been proposed such as: GloVe [17], Latent Dirichlet Allocation (LDA) [18], and word2vec1 [19].", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "A number of Word Vector Embedding systems have been proposed such as: GloVe [17], Latent Dirichlet Allocation (LDA) [18], and word2vec1 [19].", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "A number of Word Vector Embedding systems have been proposed such as: GloVe [17], Latent Dirichlet Allocation (LDA) [18], and word2vec1 [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 17, "context": "word2vec supports two main training models: Skip-gram [20] and Continuous Bag Of Words (CBOW) [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "word2vec supports two main training models: Skip-gram [20] and Continuous Bag Of Words (CBOW) [19].", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "Schweighofer [21], claims that there is a huge vacuum that should be addressed in eradicating the information crisis that the applications in the field of law suffer from.", "startOffset": 13, "endOffset": 17}, {"referenceID": 18, "context": "One of the most popular legal information retrieval systems is KONTERM [21], which was developed to represent document structures and contents.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "The currently existing implementation that is closest to our proposed model is Gov2Vec [22], which is a system that creates vector representations of words in the legal domain, by creating a vocabulary from across all corpora on, supreme court opinions, presidential actions and official summaries of congressional bills.", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "It uses a neural network [7] to predict the target word with the mean of its context words\u2019 vectors.", "startOffset": 25, "endOffset": 28}, {"referenceID": 20, "context": "We obtained over 35000 legal case documents, pertaining to various areas of practices in law, from Findlaw [23] by web crawling.", "startOffset": 107, "endOffset": 111}, {"referenceID": 21, "context": "The linguistic process of mapping inflected forms of a word to the word\u2019s core lemma is called lemmatization [24].", "startOffset": 109, "endOffset": 113}, {"referenceID": 22, "context": "For this task we use the Stanford CoreNLP library [25].", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "The lexical semantic similarity calculated between wi and wj using the Wu & Palmer\u2019s model [11] was given by wup(wi, wj).", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "The lexical semantic similarity calculated by Jiang & Conrath\u2019s model [12] was given by jcn(wi, wj).", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "hso(wi, wj) was used to indicate the lexical semantic similarity calculated using the Hirst & St-Onge\u2019s system [13].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "The returned xnorm is a double value that has the range [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "The accuracy levels of these experiments are measured in terms of precision and recall [1], which are common place measures in information retrieval.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "Our recall calculation used the same function suggested in [1].", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "The precision calculation in this study is not as clear cut as it is described in [1].", "startOffset": 82, "endOffset": 85}], "year": 2017, "abstractText": "Semantic similarity measures are an important part in Natural Language Processing tasks. However Semantic similarity measures built for general use do not perform well within specific domains. Therefore in this study we introduce a domain specific semantic similarity measure that was created by the synergistic union of word2vec, a word embedding method that is used for semantic similarity calculation and lexicon based (lexical) semantic similarity methods. We prove that this proposed methodology out performs word embedding methods trained on generic corpus and methods trained on domain specific corpus but do not use lexical semantic similarity methods to augment the results. Further, we prove that text lemmatization can improve the performance of word embedding methods. keywords: Word Embedding, Semantic Similarity, Neural Networks, Lexicon, word2vec", "creator": "LaTeX with hyperref package"}}}