{"id": "1706.02737", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM", "abstract": "we present a state - of - the - art end - line - end automatic speech recognition ( asr ) model. we decide to listen and write characters with a joint connectionist temporal classification ( ctc ) and attention - leading encoder - decoder network. the encoder is a deep convolutional neural network ( cs ) based on the complement network. the ctc network sits on top of the encoder and is jointly anchored with appropriate cognition - based decoder. during the beam search process, modules incorporate the ctc predictions, the attention - based auditory predictions and a separately deployed lstm radar model. we achieve a 5 - 10 \\ % error reduction compared to prior systems on spontaneous japanese and belgian interfaces, and our low - to - end model beats out traditional hybrid asr systems.", "histories": [["v1", "Thu, 8 Jun 2017 19:30:02 GMT  (65kb,D)", "http://arxiv.org/abs/1706.02737v1", "Accepted for INTERSPEECH 2017"]], "COMMENTS": "Accepted for INTERSPEECH 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["takaaki hori", "shinji watanabe", "yu zhang", "william chan"], "accepted": false, "id": "1706.02737"}, "pdf": {"name": "1706.02737.pdf", "metadata": {"source": "CRF", "title": "Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM", "authors": ["Takaaki Hori", "Shinji Watanabe", "Yu Zhang", "William Chan"], "emails": ["thori@merl.com,", "watanabe@merl.com,", "yzhang87@mit.edu,", "williamchan@cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Automatic Speech Recognition (ASR) is currently a mature set of technologies that have been widely deployed, resulting in great success in interface applications such as voice search [1]. A typical ASR system is factorized into several modules including acoustic, lexicon, and language models based on a probabilistic noisy channel model [2]. Over the last decade, dramatic improvements in acoustic and language models have been driven by machine learning techniques known as deep learning [3].\nHowever, current systems lean heavily on the scaffolding of complicated legacy architectures that grew up around traditional techniques, including Hidden Markov Model (HMM), Gaussian Mixture Model (GMM), Deep Neural Networks (DNN), followed by sequence discriminative training [4]. We also need to build a pronunciation dictionary and a language model, which require linguistic knowledge, and text preprocessing such as tokenization for some languages without explicit word boundaries. Finally, these modules are integrated into a Weighted Finite-State Transducer (WFST) for efficient decoding. Consequently, it is quite difficult for non-experts to use/develop ASR systems for new applications, especially for new languages.\nEnd-to-end ASR has the goal of simplifying the above module-based architecture into a single-network architecture within a deep learning framework, in order to address the above issues. End-to-end ASR methods typically rely only on paired acoustic and language data without linguistic knowledge, and train the model with a single algorithm. Therefore, the approach potentially makes it possible to build ASR systems without expert knowledge.\nThere are two major types of end-to-end architectures for\nASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols [5, 6, 7, 8, 9], and Connectionist Temporal Classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming [10, 11, 12]. While CTC requires several conditional independence assumptions to obtain the label sequence probabilities, the attentionbased methods do not use those assumptions. This property is advantageous to sequence modeling, but the attention mechanism is too flexible in the sense that it allows extremely nonsequential alignments like the case of machine translation, although the alignments are usually monotonic in speech recognition.\nTo solve this problem, we have proposed joint CTCattention-based end-to-end ASR [13], which effectively utilizes a CTC objective during training of the attention model. Specifically, we attach the CTC objective to an attention-based encoder network as a regularization technique, which also encourages the alignments to be monotonic. In our previous work, we demonstrated the approach improves the recognition accuracy over the individual use of CTC or attention-based method [13].\nIn this paper, we extend our prior work by incorporating several novel extensions to the model, and investigate the performance compared to traditional hybrid systems. The extensions we introduced are as follows.\n1. Joint CTC-attention decoding: In our prior work, we used the CTC objective only for training. In this work, we use the CTC probabilities for decoding in combination with the attention-based probabilities. We propose two methods to combine their probabilities, one is a rescoring method and the other is a one-pass method.\n2. Deep Convolutional Neural Network (CNN) encoder: We incorporate a VGG network in the encoder network, which is a deep CNN including 4 convolution and 2 maxpooling layers [14].\n3. Recurrent Neural Network Language Model (RNNLM): We combine an RNN-LM network in parallel with the attention decoder, which can be trained separately or jointly, where the RNN-LM is trained with character sequences.\nAlthough the efficacy of a deep CNN encoder has already been demonstrated in end-to-end ASR [15, 16], the other two extensions have not been experimented with yet. We present experimental results showing efficacy of each technique, and finally we show that our joint CTC-attention end-to-end ASR achieves performance superior to several state-of-the-art hybrid ASR systems in Spontaneous Japanese and Mandarin Chinese tasks.\nar X\niv :1\n70 6.\n02 73\n7v 1\n[ cs\n.C L\n] 8\nJ un\n2 01\n7"}, {"heading": "2. Joint CTC-attention", "text": "In this section, we explain the joint CTC-attention framework, which utilizes both benefits of CTC and attention during training [13]."}, {"heading": "2.1. Connectionist Temporal Classification (CTC)", "text": "Connectionist Temporal Classification (CTC) [17] is a latent variable model that monotonically maps an input sequence to an output sequence of shorter length. We assume here that the model outputs L-length letter sequence C = {cl \u2208 U|l = 1, \u00b7 \u00b7 \u00b7 , L} with a set of distinct characters U . CTC introduces framewise letter sequence with an additional \u201dblank\u201d symbol Z = {zt \u2208 U \u222a blank|t = 1, \u00b7 \u00b7 \u00b7 , T}. By using conditional independence assumptions, the posterior distribution p(C|X) is factorized as follows:\np(C|X) \u2248 \u2211 Z \u220f t\np(zt|zt\u22121, C)p(zt|X)\ufe38 \ufe37\ufe37 \ufe38 ,pctc(C|X) p(C) (1)\nAs shown in Eq. (1), CTC has three distribution components by the Bayes theorem similar to the conventional hybrid ASR case, i.e., framewise posterior distribution p(zt|X), transition probability p(zt|zt\u22121, C), and letter-based language model p(C). We also define the CTC objective function pctc(C|X) used in the later formulation.\nThe framewise posterior distribution p(zt|X) is conditioned on all inputs X , and it is quite natural to be modeled by using bidirectional long short-term memory (BLSTM):\np(zt|X) = Softmax(Lin(ht)) (2) ht = BLSTM(X). (3)\nSoftmax(\u00b7) is a softmax activation function, and Lin(\u00b7) is a linear layer to convert hidden vector ht to a (|U|+1) dimensional vector (+1 means a blank symbol introduced in CTC).\nAlthough Eq. (1) has to deal with a summation over all possible Z, we can efficiently compute this marginalization by using dynamic programming thanks to the Markov property. In summary, although CTC and hybrid systems are similar to each other due to conditional independence assumptions, CTC does not require pronunciation dictionaries and omits an HMM/GMM construction step."}, {"heading": "2.2. Attention-based encoder-decoder", "text": "Compared with CTC approaches, the attention-based approach does not make any conditional independence assumptions, and directly estimates the posterior p(C|X) based on the chain rule:\np(C|X) = \u220f l\np(cl|c1, \u00b7 \u00b7 \u00b7 , cl\u22121, X)\ufe38 \ufe37\ufe37 \ufe38 ,patt(C|X) , (4)\nwhere patt(C|X) is an attention-based objective function. p(cl|c1, \u00b7 \u00b7 \u00b7 , cl\u22121, X) is obtained by\np(cl|c1, \u00b7 \u00b7 \u00b7 , cl\u22121, X) = Decoder(rl,ql\u22121, cl\u22121) (5) ht = Encoder(X) (6) alt = Attention({al\u22121}t,ql\u22121,ht) (7)\nrl = \u2211 t altht. (8)\nEq. (6) converts input feature vectors X into a framewise hidden vector ht in an encoder network based on BLSTM, i.e., Encoder(X) , BLSTM(X). Attention(\u00b7) in Eq. (7) is based on a content-based attention mechanism with convolutional features, as described in [18]. alt is an attention weight, and represents a soft alignment of hidden vector ht for each output cl based on the weighted summation of hidden vectors to form letter-wise hidden vector rl in Eq. (8). A decoder network is another recurrent network conditioned on previous output cl\u22121 and hidden vector ql\u22121, similar to RNNLM, in addition to letter-wise hidden vector rl. We use Decoder(\u00b7) , Softmax(Lin(LSTM(\u00b7))).\nAttention-based ASR does not explicitly separate each module, but it implicitly combines acoustic models, lexicon, and language models as encoder, attention, and decoder networks, which can be jointly trained as a single deep neural network. Compared with CTC, attention-based models make predictions conditioned on all the previous predictions, and thus can learn language. However, the cost of using an explicit alignment without monotonic constraints means the alignment can become impaired."}, {"heading": "2.3. Multi-task learning", "text": "In [13], we used the CTC objective function as an auxiliary task to train the attention model encoder within the multi-task learning (MTL) framework. This approach substantially reduced irregular alignments during training and inference, and provided improved performance in several end-to-end ASR tasks.\nThe joint CTC-attention shares the same BLSTM encoder with CTC and attention decoder networks. Unlike the sole attention model, the forward-backward algorithm of CTC can enforce monotonic alignment between speech and label sequences during training. That is, rather than solely depending on the data-driven attention mechanism to estimate the desired alignments in long sequences, the forward-backward algorithm in CTC helps to speed up the process of estimating the desired alignment. The objective to be maximized is a logarithmic linear combination of the CTC and attention objectives, i.e., pctc(C|X) in Eq. (1) and patt(C|X) in Eq. (4):\nLMTL = \u03bb log pctc(C|X) + (1\u2212 \u03bb) log patt(C|X), (9)\nwith a tunable parameter \u03bb : 0 \u2264 \u03bb \u2264 1."}, {"heading": "3. Extended joint CTC-attention", "text": "This section introduces three extensions to our joint CTCattention end-to-end ASR. Figure 1 shows the extended architecture, which includes joint decoding, a deep CNN encoder and an RNN-LM network."}, {"heading": "3.1. Joint decoding", "text": "It is already been shown that the CTC objective helps guide the attention model during training to be more robust and effective, and produce a better model for speech recognition [13]. In this section, we propose to use the CTC predictions also in the decoding process.\nThe inference step of attention-based speech recognition is performed by output-label synchronous decoding with a beam search. But, we take the CTC probabilities into account to find a better aligned hypothesis to the input speech, i.e. the decoder finds the most probable character sequence C\u0302 given speech in-\nput X , according to\nC\u0302 = arg max C\u2208U\u2217\n{\u03bb log pctc(C|X)\n+(1\u2212 \u03bb) log patt(C|X)} . (10)\nIn the beam search process, the decoder computes a score of each partial hypothesis. With the attention model, the score can be computed recursively as\n\u03b1att(gl) = \u03b1att(gl\u22121) + log p(c|gl\u22121, X), (11)\nwhere gl is a partial hypothesis with length l, and c is the last character of gl, which is appended to gl\u22121, i.e. gl = gl\u22121 \u00b7 c. The score for gl is obtained as the addition of the original score \u03b1(gl\u22121) and the conditional log probability given by the attention decoder in (5). During the beam search, the number of partial hypotheses for each length is limited to a predefined number, called a beam width, to exclude hypotheses with relatively low scores, which dramatically improves the search efficiency.\nHowever, it is non-trivial to combine CTC and attentionbased scores in the beam search, because the attention decoder performs it character-synchronously while CTC does it framesynchronously. To incorporate CTC probabilities in the score, we propose two methods. One is a rescoring method, in which the decoder first obtains a set of complete hypotheses using the beam search only with the attention model, and rescores each hypothesis using Eq. (10), where pctc(C|X) can be computed with the CTC forward algorithm. The other method is a onepass decoding, in which we compute the probability of each partial hypothesis using CTC and the attention model. Here, we utilize the CTC prefix probability [19] defined as the cumulative probability of all label sequences that have gl as their prefix:\np(gl, . . . |X) = \u2211\n\u03bd\u2208(U\u222a{<eos>})+ P (gl \u00b7 \u03bd|X), (12)\nand we obtain the CTC score as\n\u03b1ctc(gl) = log p(gl, . . . |X), (13)\nwhere \u03bd represents all possible label sequences except the empty string, and <eos> indicates the end of sentence. The CTC score can not be obtained recursively as in Eq. (11), but\nit can be computed efficiently by keeping the forward probabilities over input frames for each partial hypothesis. Then it is combined with \u03b1att(gl) using \u03bb."}, {"heading": "3.2. Encoder with Deep CNN", "text": "Our encoder network is boosted by using deep CNN, which is motivated by the prior studies [16, 15]. We use the initial layers of the VGG net architecture [14] followed by BLSTM layers in the encoder network. We used the following 6-layer CNN architecture:\nConvolution2D(# in = 3, # out = 64, filter = 3\u00d7 3) Convolution2D(# in = 64, # out = 64, filter = 3\u00d7 3) Maxpool2D(patch = 3\u00d7 3, stride = 2\u00d7 2) Convolution2D(# in = 64, # out = 128, filter = 3\u00d7 3) Convolution2D(# in = 128, # out = 128, filter = 3\u00d7 3) Maxpool2D(patch = 3\u00d7 3, stride = 2\u00d7 2)\nThe initial three input channels are composed of the spectral features, delta, and delta delta features. Input speech feature images are downsampled to (1/4 \u00d7 1/4) images along with the time-frequency axises through the two max-pooling (Maxpool2D) layers."}, {"heading": "3.3. Decoder with RNN-LM", "text": "We combine an RNN-LM network in parallel with the attention decoder, which can be trained separately or jointly, where the RNN-LM is trained with character sequences without wordlevel knowledge. Although the attention decoder implicitly includes a language model as in Eq. (5), we aim at introducing language model states purely dependent on the output label sequence in the decoder, which potentially brings a complementary effect.\nAs shown in Fig. 1, the RNN-LM probabilities are used to predict the output label jointly with the decoder network. The RNN-LM information is combined at the logits level or presoftmax. If we use a pre-trained RNN-LM without any joint training, we need a scaling factor. If we train the model jointly with the other networks, we may combine their pre-activations before the softmax without a scaling factor as this is learnt. In effect, the attention-based decoder learns to use the LM prior.\nAlthough it is possible to apply the RNN-LM as a rescoring step, we combine the RNN-LM network in the end-to-end model because we do not wish to have an additional rescoring step. Also, we can view this as a single large neural network model, even if parts of it are separately pretrained. Furthermore, the RNN-LM can be trained jointly with the encoder and decoder networks."}, {"heading": "4. Experiments", "text": "We used Japanese and Mandarin Chinese ASR benchmarks to show the effectiveness of the extended joint CTC-attention approaches.\nThe Japanese task is lecture speech recognition using the Corpus of Spontaneous Japanese (CSJ) [20]. CSJ is a standard Japanese ASR task based on a collection of monologue speech data including academic lectures and simulated presentations. It has a total of 581 hours of training data and three types of evaluation data, where each evaluation task consists of 10 lectures (totally 5 hours). The Chinese task is HKUST Mandarin Chinese conversational telephone speech recognition (MTS) [21].\nIt has 5 hours recording for evaluation, and we extracted 5 hours from training data as a development set, and used the rest (167 hours) as a training set.\nAs input features, we used 80 mel-scale filterbank coefficients with pitch features as suggested in [22, 23] for the BLSTM encoder, and adding their delta and delta delta features for the CNN BLSTM encoder [15]. The encoder was a 4-layer BLSTM with 320 cells in each layer and direction, and linear projection layer is followed by each BLSTM layer. The 2nd and 3rd bottom layers of the encoder read every second hidden state in the network below, reducing the utterance length by the factor of 4 (subsampling). When we used the VGG architecture, as described in Section 3.2 as the CNN BLSTM encoder, the following BLSTM layers did not subsample the input features. We used the location-based attention mechanism [18], where the 10 centered convolution filters of width 100 were used to extract the convolutional features. The decoder network was a 1-layer LSTM with 320 cells. We also built an RNN-LM as a 1-layer LSTM for each task, where the CSJ model had 1000 cells and the MTS model had 800 cells. Each RNN-LM was first trained separately using the transcription, combined with the decoder network, and optionally re-trained with the encoder, decoder and CTC networks jointly. Note that there is no extra text data been used here but we believe more untranscribed data definitely can further improve the results.\nThe AdaDelta algorithm [24] with gradient clipping [25] was used for the optimization. We used the \u03bb = 0.1 for CSJ and the \u03bb = 0.5 for MTS in training and decoding based on our preliminary investigation. The beam width was set to 20 in decoding under all conditions. The joint CTC-attention ASR was implemented by using the Chainer deep learning toolkit [26].\nTables 1 and 2 show character error rates (CERs) of evaluated methods in CSJ and MTS tasks, respectively. In both tasks, we can see the effectiveness of joint decoding over the baseline attention model and our prior work with multi-task learning (MTL), especially showing the significant improvement of the joint decoding with the one-pass method and RNN-LM integration. We performed retraining of the entire network including the RNN-LM only in MTS task, because of time limitation. The joint training further improved the performance, which reached 32.1% CER as shown in Table 2.\nWe also built a larger network (MTL-large) for CSJ, which had a 6-layer encoder network and an RNN-LM, to compare our method with the conventional state-of-the-art techniques obtained by using linguistic resources. The state-of-the-art CERs of DNN-sMBR hybrid systems are obtained from the Kaldi\nrecipe [27] and a system based on syllable-based CTC with MAP decoding [28]. The Kaldi recipe systems originally only use academic lectures (236h) for AM training, but we extended to use all training data (581h). The LMs were trained with all training-data transcriptions. Finally, our extended joint CTCattention end-to-end ASR achieved lower CERs than already reported CERs obtained by the hybrid approaches for CSJ.\nIn MTS task, we generated more training data by linearly scaling the audio lengths by factors of 0.9 and 1.1 (speed perturb.). The final model including the VGG net and RNNLM achieved 28.0% without using linguistic resources, which defeats state-of-the-art systems including recently-proposed lattice-free MMI methods. Although we could not apply jointly-trained RNN-LM when using speed perturbation because of time limitation, we hopefully obtain further improvement by joint training."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed a novel approach for joint CTCattention decoding and RNN-LM integraton for end-to-end ASR model. We also explored deep CNN encoder to further improve the extracted acoustic features. Together, we significantly improved current best end-to-end ASR system without any linguistic resources such as morphological analyzer and pronunciation dictionary, which are essential components of conventional Mandarin Chinese and Japanese ASR systems. Our endto-end joint CTC-attention model outperforms hybrid systems without the use of any explicit language model on our Japanese task. Moreover, our method achieves state-of-the-art performance when combined with a pretrained character level language model on both Chinese and Japanese, even when compared to conventional hybrid-HMM systems. We note that despite using a pretrained RNN-LM, the model can be seen as one big neural network with a seperately pretrained components. Finally, we emphasize the text data we used to train our RNN-LM is from the same text data in the labelled audio data, we did not use any extra text. We believe our model can be further improved using vast quantities of unlabelled data to pretrain a RNN-LM and subsequently jointly trained with our model."}, {"heading": "6. References", "text": "[1] T. N. Sainath, O. Vinyals, A. Senior, and H. Sak, \u201cConvolutional,\nLong Short-Term Memory, Fully Connected Deep Neural Networks,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing, 2015.\n[2] F. Jelinek, \u201cContinuous speech recognition by statistical methods,\u201d Proceedings of the IEEE, vol. 64, no. 4, pp. 532\u2013556, 1976.\n[3] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.\n[4] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely, \u201cThe kaldi speech recognition toolkit,\u201d in IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Dec. 2011.\n[5] J. Chorowski, D. Bahdanau, K. Cho, and Y. Bengio, \u201cEnd-toend continuous speech recognition using attention-based recurrent NN: First results,\u201d arXiv preprint arXiv:1412.1602, 2014.\n[6] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.\n[7] L. Lu, X. Zhang, and S. Renals, \u201cOn training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 5060\u20135064.\n[8] W. Chan and I. Lane, \u201cOn Online Attention-based Speech Recognition and Joint Mandarin Character-Pinyin Training,\u201d in INTERSPEECH, 2016.\n[9] W. Chan, Y. Zhang, Q. Le, and N. Jaitly, \u201cLatent sequence decompositions,\u201d in International Conference on Learning Representations, 2017.\n[10] A. Graves and N. Jaitly, \u201cTowards end-to-end speech recognition with recurrent neural networks,\u201d in International Conference on Machine Learning (ICML), 2014, pp. 1764\u20131772.\n[11] Y. Miao, M. Gowayyed, and F. Metze, \u201cEESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding,\u201d in IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 167\u2013174.\n[12] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos et al., \u201cDeep speech 2: End-to-end speech recognition in english and mandarin,\u201d arXiv preprint arXiv:1512.02595, 2015.\n[13] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to-end speech recognition using multi-task learning,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 4835\u20134839.\n[14] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d arXiv preprint arXiv:1409.1556, 2014.\n[15] Y. Zhang, W. Chan, and N. Jaitly, \u201cVery deep convolutional networks for end-to-end speech recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing, 2017.\n[16] Y. Zhang, M. Pezeshki, P. Brakel, S. Zhang, C. L. Y. Bengio, and A. Courville, \u201cTowards end-to-end speech recognition with deep convolutional neural networks,\u201d arXiv preprint arXiv:1701.02720, 2017.\n[17] A. Graves, S. Ferna\u0301ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in International Conference on Machine learning (ICML), 2006, pp. 369\u2013376.\n[18] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, \u201cAttention-based models for speech recognition,\u201d in Advances in Neural Information Processing Systems (NIPS), 2015, pp. 577\u2013585.\n[19] A. Graves, \u201cSupervised sequence labelling with recurrent neural networks,\u201d PhD thesis, Technische Universita\u0308t Mu\u0308nchen, 2008.\n[20] K. Maekawa, H. Koiso, S. Furui, and H. Isahara, \u201cSpontaneous speech corpus of japanese,\u201d in International Conference on Language Resources and Evaluation (LREC), vol. 2, 2000, pp. 947\u2013 952.\n[21] Y. Liu, P. Fung, Y. Yang, C. Cieri, S. Huang, and D. Graff, \u201cHKUST/MTS: A very large scale mandarin telephone speech corpus,\u201d in Chinese Spoken Language Processing. Springer, 2006, pp. 724\u2013735.\n[22] P. Ghahremani, B. BabaAli, D. Povey, K. Riedhammer, J. Trmal, and S. Khudanpur, \u201cA pitch extraction algorithm tuned for automatic speech recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 2494\u20132498.\n[23] Y. Miao, M. Gowayyed, X. Na, T. Ko, F. Metze, and A. Waibel, \u201cAn empirical exploration of ctc acoustic models,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 2623\u20132627.\n[24] M. D. Zeiler, \u201cAdadelta: an adaptive learning rate method,\u201d arXiv preprint arXiv:1212.5701, 2012.\n[25] R. Pascanu, T. Mikolov, and Y. Bengio, \u201cOn the difficulty of training recurrent neural networks,\u201d arXiv preprint arXiv:1211.5063, 2012.\n[26] S. Tokui, K. Oono, S. Hido, and J. Clayton, \u201cChainer: a nextgeneration open source framework for deep learning,\u201d in Proceedings of Workshop on Machine Learning Systems (LearningSys) in NIPS, 2015.\n[27] T. Moriya, T. Shinozaki, and S. Watanabe, \u201cKaldi recipe for Japanese spontaneous speech recognition and its evaluation,\u201d in Autumn Meeting of ASJ, no. 3-Q-7, 2015.\n[28] N. Kanda, X. Lu, and H. Kawai, \u201cMaximum a posteriori based decoding for CTC acoustic models,\u201d in Interspeech 2016, 2016, pp. 1868\u20131872.\n[29] D. Povey, V. Peddinti, D. Galvez, P. Ghahrmani, V. Manohar, X. Na, Y. Wang, and S. Khudanpur, \u201cPurely sequence-trained neural networks for asr based on lattice-free MMI,\u201d in Interspeech, 2016, pp. 2751\u20132755."}], "references": [{"title": "Convolutional, Long Short-Term Memory, Fully Connected Deep Neural Networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous speech recognition by statistical methods", "author": ["F. Jelinek"], "venue": "Proceedings of the IEEE, vol. 64, no. 4, pp. 532\u2013556, 1976.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1976}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Dec. 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "End-toend continuous speech recognition using attention-based recurrent NN: First results", "author": ["J. Chorowski", "D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.1602, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["W. Chan", "N. Jaitly", "Q.V. Le", "O. Vinyals"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition", "author": ["L. Lu", "X. Zhang", "S. Renals"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 5060\u20135064.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "On Online Attention-based Speech Recognition and Joint Mandarin Character-Pinyin Training", "author": ["W. Chan", "I. Lane"], "venue": "INTER- SPEECH, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Latent sequence decompositions", "author": ["W. Chan", "Y. Zhang", "Q. Le", "N. Jaitly"], "venue": "International Conference on Learning Representations, 2017.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "International Conference on Machine Learning (ICML), 2014, pp. 1764\u20131772.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 167\u2013174.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint CTC-attention based end-to-end speech recognition using multi-task learning", "author": ["S. Kim", "T. Hori", "S. Watanabe"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 4835\u20134839.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for end-to-end speech recognition", "author": ["Y. Zhang", "W. Chan", "N. Jaitly"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, 2017.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "Towards end-to-end speech recognition with deep convolutional neural networks", "author": ["Y. Zhang", "M. Pezeshki", "P. Brakel", "S. Zhang", "C.L.Y. Bengio", "A. Courville"], "venue": "arXiv preprint arXiv:1701.02720, 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "International Conference on Machine learning (ICML), 2006, pp. 369\u2013376.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2015, pp. 577\u2013585.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["A. Graves"], "venue": "PhD thesis, Technische Universit\u00e4t M\u00fcnchen, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Spontaneous speech corpus of japanese", "author": ["K. Maekawa", "H. Koiso", "S. Furui", "H. Isahara"], "venue": "International Conference on Language Resources and Evaluation (LREC), vol. 2, 2000, pp. 947\u2013 952.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "HKUST/MTS: A very large scale mandarin telephone speech corpus", "author": ["Y. Liu", "P. Fung", "Y. Yang", "C. Cieri", "S. Huang", "D. Graff"], "venue": "Chinese Spoken Language Processing. Springer, 2006, pp. 724\u2013735.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "A pitch extraction algorithm tuned for automatic speech recognition", "author": ["P. Ghahremani", "B. BabaAli", "D. Povey", "K. Riedhammer", "J. Trmal", "S. Khudanpur"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 2494\u20132498.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical exploration of ctc acoustic models", "author": ["Y. Miao", "M. Gowayyed", "X. Na", "T. Ko", "F. Metze", "A. Waibel"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 2623\u20132627.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5063, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Chainer: a nextgeneration open source framework for deep learning", "author": ["S. Tokui", "K. Oono", "S. Hido", "J. Clayton"], "venue": "Proceedings of Workshop on Machine Learning Systems (LearningSys) in NIPS, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Kaldi recipe for Japanese spontaneous speech recognition and its evaluation", "author": ["T. Moriya", "T. Shinozaki", "S. Watanabe"], "venue": "Autumn Meeting of ASJ, no. 3-Q-7, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Maximum a posteriori based decoding for CTC acoustic models", "author": ["N. Kanda", "X. Lu", "H. Kawai"], "venue": "Interspeech 2016, 2016, pp. 1868\u20131872.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Purely sequence-trained neural networks for asr based on lattice-free MMI", "author": ["D. Povey", "V. Peddinti", "D. Galvez", "P. Ghahrmani", "V. Manohar", "X. Na", "Y. Wang", "S. Khudanpur"], "venue": "Interspeech, 2016, pp. 2751\u20132755.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Automatic Speech Recognition (ASR) is currently a mature set of technologies that have been widely deployed, resulting in great success in interface applications such as voice search [1].", "startOffset": 183, "endOffset": 186}, {"referenceID": 1, "context": "A typical ASR system is factorized into several modules including acoustic, lexicon, and language models based on a probabilistic noisy channel model [2].", "startOffset": 150, "endOffset": 153}, {"referenceID": 2, "context": "Over the last decade, dramatic improvements in acoustic and language models have been driven by machine learning techniques known as deep learning [3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "However, current systems lean heavily on the scaffolding of complicated legacy architectures that grew up around traditional techniques, including Hidden Markov Model (HMM), Gaussian Mixture Model (GMM), Deep Neural Networks (DNN), followed by sequence discriminative training [4].", "startOffset": 277, "endOffset": 280}, {"referenceID": 4, "context": "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols [5, 6, 7, 8, 9], and Connectionist Temporal Classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming [10, 11, 12].", "startOffset": 182, "endOffset": 197}, {"referenceID": 5, "context": "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols [5, 6, 7, 8, 9], and Connectionist Temporal Classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming [10, 11, 12].", "startOffset": 182, "endOffset": 197}, {"referenceID": 6, "context": "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols [5, 6, 7, 8, 9], and Connectionist Temporal Classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming [10, 11, 12].", "startOffset": 182, "endOffset": 197}, {"referenceID": 7, "context": "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols [5, 6, 7, 8, 9], and Connectionist Temporal Classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming [10, 11, 12].", "startOffset": 182, "endOffset": 197}, {"referenceID": 8, "context": "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols [5, 6, 7, 8, 9], and Connectionist Temporal Classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming [10, 11, 12].", "startOffset": 182, "endOffset": 197}, {"referenceID": 9, "context": "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols [5, 6, 7, 8, 9], and Connectionist Temporal Classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming [10, 11, 12].", "startOffset": 336, "endOffset": 348}, {"referenceID": 10, "context": "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols [5, 6, 7, 8, 9], and Connectionist Temporal Classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming [10, 11, 12].", "startOffset": 336, "endOffset": 348}, {"referenceID": 11, "context": "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols [5, 6, 7, 8, 9], and Connectionist Temporal Classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming [10, 11, 12].", "startOffset": 336, "endOffset": 348}, {"referenceID": 12, "context": "To solve this problem, we have proposed joint CTCattention-based end-to-end ASR [13], which effectively utilizes a CTC objective during training of the attention model.", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "In our previous work, we demonstrated the approach improves the recognition accuracy over the individual use of CTC or attention-based method [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "Deep Convolutional Neural Network (CNN) encoder: We incorporate a VGG network in the encoder network, which is a deep CNN including 4 convolution and 2 maxpooling layers [14].", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "Although the efficacy of a deep CNN encoder has already been demonstrated in end-to-end ASR [15, 16], the other two extensions have not been experimented with yet.", "startOffset": 92, "endOffset": 100}, {"referenceID": 15, "context": "Although the efficacy of a deep CNN encoder has already been demonstrated in end-to-end ASR [15, 16], the other two extensions have not been experimented with yet.", "startOffset": 92, "endOffset": 100}, {"referenceID": 12, "context": "In this section, we explain the joint CTC-attention framework, which utilizes both benefits of CTC and attention during training [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "Connectionist Temporal Classification (CTC) [17] is a latent variable model that monotonically maps an input sequence to an output sequence of shorter length.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "(7) is based on a content-based attention mechanism with convolutional features, as described in [18].", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "In [13], we used the CTC objective function as an auxiliary task to train the attention model encoder within the multi-task learning (MTL) framework.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "It is already been shown that the CTC objective helps guide the attention model during training to be more robust and effective, and produce a better model for speech recognition [13].", "startOffset": 179, "endOffset": 183}, {"referenceID": 18, "context": "Here, we utilize the CTC prefix probability [19] defined as the cumulative probability of all label sequences that have gl as their prefix:", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "Our encoder network is boosted by using deep CNN, which is motivated by the prior studies [16, 15].", "startOffset": 90, "endOffset": 98}, {"referenceID": 14, "context": "Our encoder network is boosted by using deep CNN, which is motivated by the prior studies [16, 15].", "startOffset": 90, "endOffset": 98}, {"referenceID": 13, "context": "We use the initial layers of the VGG net architecture [14] followed by BLSTM layers in the encoder network.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "The Japanese task is lecture speech recognition using the Corpus of Spontaneous Japanese (CSJ) [20].", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "The Chinese task is HKUST Mandarin Chinese conversational telephone speech recognition (MTS) [21].", "startOffset": 93, "endOffset": 97}, {"referenceID": 26, "context": "7 DNN-hybrid [27]\u2217 9.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "1 CTC-syllable [28] 9.", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "As input features, we used 80 mel-scale filterbank coefficients with pitch features as suggested in [22, 23] for the BLSTM encoder, and adding their delta and delta delta features for the CNN BLSTM encoder [15].", "startOffset": 100, "endOffset": 108}, {"referenceID": 22, "context": "As input features, we used 80 mel-scale filterbank coefficients with pitch features as suggested in [22, 23] for the BLSTM encoder, and adding their delta and delta delta features for the CNN BLSTM encoder [15].", "startOffset": 100, "endOffset": 108}, {"referenceID": 14, "context": "As input features, we used 80 mel-scale filterbank coefficients with pitch features as suggested in [22, 23] for the BLSTM encoder, and adding their delta and delta delta features for the CNN BLSTM encoder [15].", "startOffset": 206, "endOffset": 210}, {"referenceID": 17, "context": "We used the location-based attention mechanism [18], where the 10 centered convolution filters of width 100 were used to extract the convolutional features.", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "The AdaDelta algorithm [24] with gradient clipping [25] was used for the optimization.", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "The AdaDelta algorithm [24] with gradient clipping [25] was used for the optimization.", "startOffset": 51, "endOffset": 55}, {"referenceID": 25, "context": "The joint CTC-attention ASR was implemented by using the Chainer deep learning toolkit [26].", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "5 CTC with language model [23] \u2013 34.", "startOffset": 26, "endOffset": 30}, {"referenceID": 28, "context": ") [29] \u2013 28.", "startOffset": 2, "endOffset": 6}, {"referenceID": 26, "context": "recipe [27] and a system based on syllable-based CTC with MAP decoding [28].", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "recipe [27] and a system based on syllable-based CTC with MAP decoding [28].", "startOffset": 71, "endOffset": 75}], "year": 2017, "abstractText": "We present a state-of-the-art end-to-end Automatic Speech Recognition (ASR) model. We learn to listen and write characters with a joint Connectionist Temporal Classification (CTC) and attention-based encoder-decoder network. The encoder is a deep Convolutional Neural Network (CNN) based on the VGG network. The CTC network sits on top of the encoder and is jointly trained with the attention-based decoder. During the beam search process, we combine the CTC predictions, the attention-based decoder predictions and a separately trained LSTM language model. We achieve a 5-10% error reduction compared to prior systems on spontaneous Japanese and Chinese speech, and our end-to-end model beats out traditional hybrid ASR systems.", "creator": "LaTeX with hyperref package"}}}