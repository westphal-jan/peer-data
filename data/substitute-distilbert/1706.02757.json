{"id": "1706.02757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction", "abstract": "recognition of social signals, from human facial expressions being prosody recognition speech, emerges a popular research topic in human - robot interaction studies. there needs also a long line of research in the spoken dialogue community that investigates user satisfaction in relation to symbolic characteristics. however, very little functionality relates a pattern of distinctive social signals and language features detected requires automatic nose - to - finger human - hand interaction to alter resulting user perception of a robot. in previous paper we show how different emotional facial expressions of human users, in combination with prosodic characteristics identifying human speech and features of sensor - robot dialogue, correlate with users'impressions at the robot witnessing a conversation. we find that happiness determines engaging user's recognised facial expression strongly correlates with likeability of a robot, while dialogue - related features ( such as selection of human turns or number of sentences through robot utterance ) correlate with perceiving a robot as intelligent. your addition, we determined that animal expression, emotional features, and prosody are better predictors of human ratings related to perceived robot likeability and anthropomorphism, while linguistic and non - linguistic features more often decrease perceived robot intelligence and interpretability. as such, these characteristics may in future be used as an online reward signal for in - situ reinforcement learning based adaptive human - robot dialogue systems.", "histories": [["v1", "Thu, 8 Jun 2017 20:33:00 GMT  (694kb,D)", "http://arxiv.org/abs/1706.02757v1", "Robo-NLP workshop at ACL 2017. 9 pages, 5 figures, 6 tables"]], "COMMENTS": "Robo-NLP workshop at ACL 2017. 9 pages, 5 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.RO cs.CL cs.HC", "authors": ["jekaterina novikova", "christian dondrup", "ioannis papaioannou", "oliver lemon"], "accepted": false, "id": "1706.02757"}, "pdf": {"name": "1706.02757.pdf", "metadata": {"source": "CRF", "title": "Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction", "authors": ["Jekaterina Novikova", "Christian Dondrup", "Ioannis Papaioannou"], "emails": ["o.lemon}@hw.ac.uk"], "sections": [{"heading": null, "text": "Recognition of social signals, from human facial expressions or prosody of speech, is a popular research topic in human-robot interaction studies. There is also a long line of research in the spoken dialogue community that investigates user satisfaction in relation to dialogue characteristics. However, very little research relates a combination of multimodal social signals and language features detected during spoken face-to-face human-robot interaction to the resulting user perception of a robot. In this paper we show how different emotional facial expressions of human users, in combination with prosodic characteristics of human speech and features of human-robot dialogue, correlate with users\u2019 impressions of the robot after a conversation. We find that happiness in the user\u2019s recognised facial expression strongly correlates with likeability of a robot, while dialogue-related features (such as number of human turns or number of sentences per robot utterance) correlate with perceiving a robot as intelligent. In addition, we show that facial expression, emotional features, and prosody are better predictors of human ratings related to perceived robot likeability and anthropomorphism, while linguistic and non-linguistic features more often predict perceived robot intelligence and interpretability. As such, these characteristics may in future be used as an online reward signal for in-situ Reinforcement Learningbased adaptive human-robot dialogue systems."}, {"heading": "1 Introduction", "text": "Social signals, such as emotional expressions, play an important role in human-human interaction, thus they are increasingly recognised as an important factor to be considered both in human-robot interaction research (Cid et al., 2013; Novikova et al., 2015; Devillers et al., 2015) and in the area of spoken dialogue systems (Herm et al., 2008; Meena et al., 2015).\nRecognition of human social signals has become a popular topic in Human-Robot Interaction (HRI) in recent years. Social signals are recognized well from human facial expressions or prosodic features of speech (Ekman, 2004; Zeng et al., 2009), and have become the most popular methods for recognising human affective signals in human-robot interaction (Ra\u0301zuri et al., 2015; Devillers et al., 2015; Cid et al., 2013).\nIn human-robot interaction, recognized human emotions are mostly used for mimicking human behaviour and enhancing the empathy towards a robot both in children (Tielman et al., 2014) and\nar X\niv :1\n70 6.\n02 75\n7v 1\n[ cs\n.R O\n] 8\nJ un\n2 01\n7\nin adult users (Tapus and Mataric, 2007). In the area of spoken dialogue systems, signals recognised from linguistic cues and prosody have been used to detect problematic dialogues (Herm et al., 2008) and to assess dialogue quality as a whole (Schmitt and Ultes, 2015). This type of dialogue-related signals has also been used to automatically detect miscommunication (Meena et al., 2015), or to predict the user satisfaction (Schmitt et al., 2011).\nHowever, there is very little research combining the areas of detecting multi-modal signals during spoken HRI and evaluation of human-robot conversation, and using them to create an adaptive social dialogue.\nIn this paper, we make a first step towards building a multi-modally-rich, conversational, and human-like robotic agent, potentially able to react to the changes in human behaviour during face-toface dialogue and able to adjust the dialogue strategy in order to improve an interlocutor\u2019s impression. We present a setup that targets the development of a dialogue system to explore verbal and non-verbal conversational cues in a face-to-face situated dialogue with a social robot. We show that different emotional facial expressions of a human interlocutor, in combination with prosodic characteristics of human speech and features of humanrobot dialogue, correlate strongly with users\u2019 perceptions of a robot after a conversation. Based on these features, we developed a model capable of predicting potential human ratings of a robot and discuss its implications for future work in developing adaptive human-robot dialogue systems."}, {"heading": "2 Experiment Setup and Evaluation", "text": "The human-robot dialogue system was evaluated via a user study in which human subjects interacted with a Pepper robot1 acting autonomously using the system described in (Papaioannou and Lemon, 2017; Papaioannou et al., 2017). The dialogue system used, combines task-based with chat-based dialogue features, deciding the most appropriate action on each consequent turns, using a pre-trained Reinforcement Learning (RL) policy. The robot decides among a pool of possible actions at \u2208 A where A = [PerformTask, Greet, Goodbye, Chat, GiveDirections, Wait, RequestTask, RequestShop]. If a task is recognised\n1http://doc.aldebaran.com/2-5/home_ pepper.html\nin the user utterance (e.g. \u201dwhere can I find discounts\u201d), a response is synthesized using database lookup and predefined utterances (like the example shown in Table 3). If no task was recognised, then the user request is being forwarded to a Chatbot, written in AIML and based on the chatbot Rosie2, where a chat-style response is formulated based on AIML template/ pattern matching.\nAll interactions were in English. The physical setup of the experiment can be seen in Figure 1."}, {"heading": "2.1 Experimental Scenario", "text": "The task and the setup chosen in the study were considered as first steps towards understanding how a humanoid social robot should behave in the context of a shopping mall while also providing useful information to the mall\u2019s visitors. To this end, participants were asked to imagine that they were entering a shopping mall they had never been to before where the robot was installed in the entry area interacting with visitors one at a time. Participants were asked to complete as many as possible of the following five tasks:\n\u2022 Get information from the robot on where to get a coffee.\n\u2022 Get information from the robot on where to buy clothes.\n\u2022 Get the directions to the clothing shop of their choice.\n\u2022 Find out if there are any current sales or discounts in the shopping mall and try to get a voucher from the robot.\n\u2022 Make a selfie with the robot.\nInstructions were given to use natural language spontaneously while interacting with the robot."}, {"heading": "2.2 Participants and Experimental Design", "text": "41 people (13 females, 28 males) participated in our study, ranging in age from 18 to 38 (M=24.46, SD=4.72). The majority of them were students (93% students and 7% staff) that had no or little previous experience with robots (56% with little or no experience, 39% with some experience, and 5% with a lot of experience).\nParticipants were initially given a briefing script describing the goal of the task and providing hints\n2http://github.com/pandorabots/rosie\non how to better communicate with the robot, e.g. \u201cwait for your turn to speak\u201d and \u201cplease keep in mind that the robot only listens to you while its eyes are blinking blue\u201d3. We reassured our participants that we were testing the robot, not them, and controlled environment-introduced biases by avoiding non-task-related distractions during the experiment. During experimental sessions, participants stood in front of the robot and the experimenter was hidden in another corner of the room but available in case the participant would need any help (see Figure 1).\nAt the end of the experiment participants were debriefed and received a \u00a310 gift voucher. The duration of each session did not exceed thirty minutes."}, {"heading": "2.3 Measured Variables", "text": "We collected a range of objective measures from the log files, video and audio recordings of the interactions, and transcripts of dialogues. From the audio recordings, we collected a set of different prosodic and dialogue-related features. From the video recordings, we collected the data on emotional intensities detected based on human facial expressions. From the dialogue transcripts, we collected a set of linguistic features, such as lexical diversity, length of utterance etc.\nIn addition, we considered a range of subjective measures for a qualitative evaluation. For that, after each interaction session participants were asked to fill in a questionnaire to assess their perception of the robot.\nEmotions were detected and recognised using the Microsoft Emotion API for Video4. This API takes video frames as an input (see Figure 2), and returns the confidence across a set of emotions for the group of faces in the image over a period of time. The emotions detected are happiness, sadness, surprise, anger, fear, contempt, disgust, or neutral. Happiness, surprise, and sadness were selected for analysis in this work, because they had the highest average or maximum values across all recorded videos.\nProsodic Features used in this work are the following: average fundamental frequency of speech F0, maximum F0, and difference between maximum and minimum F0 values.\n3Pepper\u2019s default way of communicating that it is listening.\n4https://www.microsoft.com/cognitive-services/enus/emotion-api\nNon-linguistic Dialogue Features used in this work contain speech duration (in sec), number of turns, number of completed tasks, number of selfrepetitions and a ratio of tasks per turn.\nLinguistic Dialogue Features used in this work consist of utterance length (in characters), a ratio of words per utterance and unique words per utterance, number of sentences within an utterance, lexical diversity, a ratio of words per sentence and a ratio of unique words per sentence.\nPerception of Robot was assessed using responses on the questionnaire filled by participants at the end of each interaction session. The questionnaire was based on a combination of the User Experience Questionnaire UEQ (Laugwitz et al., 2008) and the Godspeed Questionnaire (Bartneck et al., 2009). It consisted of 21 pairs of contrasting characteristics that may apply to the robot, and are grouped into four groups of Anthropomorphism, Likeability, Perceived Intelligence, and User Expectations. The Anthropomorphism group consists of the following pairs of characteristics: fake \u2013 natural, machinelike \u2013 humanlike, unconscious \u2013 conscious, artificial \u2013 lifelike. Likeability consists of: unfriendly \u2013 friendly, unkind \u2013 kind, unpleasant \u2013 pleasant, awful \u2013 nice, annoying \u2013 enjoyable, disliked \u2013 liked. The group of Perceived Intelligence consists of: incompetent \u2013 competent, ignorant \u2013 knowledgeable, irresponsive \u2013 responsive, unintelligent \u2013 intelligent, foolish \u2013 sensible. The Interpretability group consists of: does not meet expectations \u2013 meets expectations, obstructive \u2013 supportive, unpredictable \u2013 predictable, confusing \u2013 clear, complicated \u2013 easy, not understandable \u2013 understandable. Users were asked to evaluate perception of a robot on a 5-point Likert scale, where the minimum value was 1 and the maximum was 5.\nThe validity of the used questionnaire was tested by measuring its internal consistency with Cronbach\u2019s \u03b1, which was equal to 0.93 (high consistency). Based on the high value of the Cron-\nbach\u2019s \u03b1, we assume that that our participants in the given context interpreted the robot characteristics, provided in the questionnaire, in an expected way."}, {"heading": "3 Multimodal Data Collection and Analysis", "text": "Data collected during the experiment required additional processing, alignment and annotation, as shown in Figure 3. Prosodic features of F0, and dialogue-related features showing presence and absence of pauses and presence/absence of speech were collected from audio recordings with a rate of 44100 samples per second. Values of emotional intensities were collected from video recordings with a rate of 25 frames per seconds. All the data was aligned after recording, using average values of prosodic features per frame. Afterwards, data was annotated in ELAN5 detecting associations between an utterance and its owners. Finally, the dialogue texts were transcribed and linguistic features were calculated using R packages stringr, stringi, tidytext, and qdap.\nA summary of collected data is provided in Table 1. Specifically, the summary results show that the F0 value of human speech changes a lot during the conversation, with a maximum value being more than twice as large as an average value. Average emotional intensities of surprise and sadness, on the other hand, do not differ much\n5https://tla.mpi.nl/tools/tla-tools/elan\n(\u00b10.001), and the maximum values of all the emotional intensities are usually close to 1.\nResults of non-linguistic dialogue-related features show that the robot on average speaks significantly longer than humans during a dialogue. Humans tend to have a higher number of turns, although they less frequently repeat themselves. These differences, however, are not significant.\nResults of linguistic features reveal more significant differences between robot and human language. For example, the results show that humans on average speak in significantly shorter utterances compared to a robot, both in terms of a number of characters and a number of words per utterance. The robot uses more sentences per utterance on average, although this difference is not significant. The lexical diversity, which was calculated as a ratio of unique words and a total number of words in an utterance, shows a slightly higher value in human language rather than robot\u2019s.\nValues of linguistic features differ significantly between human and robot language, which leads us to investigate in more details the textual dialogue data in terms of lexical variety and syntactic complexity."}, {"heading": "4 Linguistic Analysis of Dialogues", "text": "Following Gardent et al. (2017), we analyse the dialogue textual data in terms of length of ut-\nterances, lexical richness, and syntactic variation. The results are summarised in Tables 1 and 2 and grouped by a speaker, i.e. robot and human."}, {"heading": "4.1 Length of utterances", "text": "Results presented in Table 1 show that robot utterances are significantly longer than those of their human interlocutors, both in terms of words per utterance and sentences per utterance. This may be partly explained by the fact that a turn-taking process was not very natural and thus was not always successful during the dialogue. It usually took some time for people to learn how to communicate with Pepper properly and to start speaking to the robot only when it was listening. As a result, from time to time people were interrupted by the robot, while they never tried to interrupt the robot themselves. Shorter average length of human utterances is also caused by the way people tend to deal with disfluencies of a dialogue, e.g. rephrasing and shortening their previous utterance in order to emphasise the most important keywords (see an example in Table 3). The robot utterances, on the other hand, were not shortened or changed in any other way in the case of dialogue disfluencies.\nHUMAN (H): By the way, I\u2019m a student so I don\u2019t have a lot of money. So, is it possible to have some shop with sales or discounts? [30 words] ROBOT (R): Who, specifically, does? [dialogue disfluency] H: To have some discounts somewhere. [5 words] R: There are 2 shops that have sales nearby. These are Tesco, and Phone Heaven. H: Thank you very much.\nTable 3: An example of shortening as a result of dialogue disfluency."}, {"heading": "4.2 Lexical Richness", "text": "We used the Lexical Complexity Analyser (Lu, 2009) to measure various dimensions of lexical richness, such as lexical sophistication, lexical diversity and mean segmental type-token ratio. We complement the traditional measure of lexical diversity type-token ratio (TTR) with the more robust measure of mean segmental type-token ratio (MSTTR) (Lu, 2012), which divides all the dialogues into successive segments of a given length and then calculates the average TTR of all segments. The higher the value of MSTTR, the more diverse is the measured text. We also measure lexical sophistication (LS), also known as lexical rareness, which is calculated as the proportion of lexical word types not on the list of 2,000 most frequent words generated from the British National Corpus. In addition, we measure lexical diversity (LD) as a ratio of unique and total words per utterance.\nThe results presented in Table 2 show that human utterances, although being significantly shorter, are significantly richer than those of the robot, both in terms of lexical diversity and lexical sophistication. MSTTR values do not differ significantly between human and robot utterances. This leads us to investigate the distribution of frequencies of bigrams and trigrams in human and robot utterances during dialogues.\nThe majority of both robot (61%) and human (62%) bigrams are only used once in all the dialogues. However, the mean frequency of bigrams\nthat were used more than once during dialogues is significantly (p< 0.001) higher in robot utterances (Mean = 15.8, SD = 31.4) compared to human utterances (Mean = 6.1, SD = 8.4). This means that the robot tends to use the same combinations of words repeatedly, while people do vary their language more. The majority of trigrams is also used just once by both people and the robot, although the proportion is quite different: 75% of human trigrams and only 65% of robot trigrams are used once in the dialogues. Those trigrams that are used more than once, have an average frequency of 15.8 (SD = 30.9) for robot, and only 4.4 (SD = 4.6) for human utterances.\nThe results of bigrams and trigrams analysis support the conclusion that human language in human-robot conversations is more rich, varied, and diverse than that of the robot. Figure 4 shows that poor lexical variation of a robot language is influenced a lot by the fact that the robot often uses the phrase \u201cI am afraid I cannot help you with that\u201d, which may be said when the speech recognition confidence does not reach an adequate threshold, or when no known keywords are detected in human utterances. As Figure 4 shows, 7 out of 10 most frequent trigrams in a robot language are variations of that specific phrase."}, {"heading": "4.3 Syntactic Variation and Discourse Phenomena", "text": "We used the D-Level Analyser (Lu, 2009) to evaluate syntactic variation and complexity of human references using the revised D-Level Scale (Lu, 2014). The scale has eight levels of syntactic complexity, where levels 0 and 1 include simple or incomplete sentences and higher levels include sentences with more complex structures.\nFigure 5 shows a similar syntactic variation in\nhuman and robot language, although there are slight differences, e.g. people tend to use a higher percentage of both the simplest and the most complicated sentences. In general, the majority of all the sentences, used both by humans and by a robot, are simple sentences. This is because the topic of a human-robot conversation is quite simple and does not require a lot of complicated syntactic structures.\nThe results of initial linguistic analysis, together with results of analysis of multimodal signals, suggest that linguistic, as well as other multimodal features, may be important in predicting human perception of a robot. However, average scores can be misleading, as they only provide a systemlevel overview but do not measure the strength of association with human ratings. This led us to inspect the correlation between the ratings of the robot and all the multimodal features of a dialogue."}, {"heading": "5 Correlation between Robot Ratings and Multimodal Features of Human-Robot Dialogue", "text": "A summary of correlation results is presented in Table 4. The results reveal that different groups of features correlate with different groups of human ratings. For example, emotional features, such as intensity of happiness, correlate strongly with perceived anthropomorphism of a robot, so that a person who more strongly expresses happiness during a dialogue with the robot probably perceives it as friendlier and nicer. Human ratings of perceived robot anthropomorphism also correlate with a lexical diversity of human language: people tend to use more diverse language when speaking to a robot that they perceive as conscious, natural, and humanlike (see an example in Table 5).\nAverage F0 value of human speech correlates strongly with perceived intelligence of the robot, specifically with a robot being more knowledgeable. The ratio of tasks per turn is, unsurprisingly, strongly correlated with perceived robot intelligence. The more dialogue turns people need to complete the same number of tasks, the more they perceive the robot as ignorant and unintelligent. Features of robot language also correlate with how it is perceived: the more words (including unique ones) per sentence it generates, the more competent it appears to humans.\nIt is interesting to notice that some linguistic\nH: Where is Costa? Is it far? [LD = 0.83] R: A place we call \u201creality\u201d. [disfluency] H: Sorry I didn\u2019t understand. [LD = 1.0] R: There is no need to apologize. Perhaps I\ncan explain it better. Which part are you confused about?\nH: Bye bye [LD = 0.50] R: My birthday is September 9th. [disfluency] H: Bye bye [LD = 0.50] R: Would you like to do a search for it? [disfluency] H: No. [LD = 1.0]\nTable 5: An example of a dialogues that resulted in: Left - high human ratings for anthropomorphism, Right - low human ratings for anthropomorphism. Both dialogues present situations when human intent was not recognised by the robot. LD denotes lexical diversity.\nand non-linguistic dialogue features correlate with different human ratings depending on whether the features are calculated for human or robot language. For example, a higher number of human turns during a dialogue correlates strongly with a robot being perceived as intelligent and knowledgeable, while a higher number of robot turns correlates with it being disliked. Longer human sentences show that a robot is perceived as more responsive, while longer robot sentences correlate with a robot being annoying and obstructive.\nThe results show that some features, observable during a human-robot dialogue, correlate strongly and significantly with different groups of human ratings. However, it is not obvious if a strong correlation also means that there is a causal relation-\nship between human language or multimodal behavioural features and ratings of the robot. This leads us to inspect whether the previously discussed features may be used for predicting potential ratings."}, {"heading": "6 Predicting Perception of Robots in Human-Robot Dialogue", "text": "In order to develop a model that predicts potential human ratings on robot likeability and perceived intelligence, we use the previously discussed prosodic features, dialogue-related characteristics, and detected emotional intensities, as predictive features of the model. For the prediction itself, we use ensemble learning (Random Forest, RF) (Breiman, 2001) which is a state-of-\nthe-art algorithm that can be applied in a dynamic dialogue situation and is able to combine the respective strengths of different informative features into a single model.\nSetup: We use a 70/30% split for training and testing and 10-fold cross-validation on the training data to tune the optimal number of predictors selected for growing trees. 100 trees were grown with 2 variables randomly sampled as candidates at each split. We investigate five different models used as predictors: 1) emotional intensities, 2) prosodic features, 3) non-linguistic dialogue features, 4) linguistic dialogue features, and 5) all the features combined.\nResults: The results in Table 6 show that different groups of features are better predictors of different groups of ratings. For example, combining dialogue-related features only (either linguistic or non-linguistic) as predictors, produces the lowest root-mean-square error (RMSE) for many ratings out of the perceived intelligence and intelligibility groups. This means that a combination of dialogue-related features is producing the best prediction of such aspects of perceived robot intelligence as e.g. responsiveness, intelligence, or predictability.\nEmotional features are shown to be the best predictors of some aspects of robot likeability and perceived anthropomorphism. For example, the ratings for unconscious-conscious, unfriendlyfriendly or awful-nice are best predicted using emotional features only. Other aspects of robot anthropomorphism and likeability, such as machinelike-humanlike or disliked-liked, are best predicted by using only prosodic features of human speech as predictors. Combining emotional, prosodic and dialogue-related features rarely improves the results of rating predictions. In some cases, e.g. predicting the ratings for unresponsiveresponsive, a combination of all the features pro-\nduces the same results as dialogue-related features alone. In one case a combination of all features does improve prediction results, this is the rating showing if the robot meets human expectations or not. This is probably because human expectations consist of different aspects themselves: some expect the robot to be anthropomorphic and likeable, other prefer it to be intelligent and easily interpretable."}, {"heading": "7 Discussion and Conclusions", "text": "In this paper, we show how dialogue features correlate with the user\u2019s perception of a robot (e.g. strong correlation between higher number of human turns and higher robot\u2019s perceived intellect, or between higher number of sentences per robot utterance and robot\u2019s perceived ignorance), as well as correlations between emotional features and robot likeability.\nUsing the findings described in this paper, a predictive model could be implemented using emotional intensities (happiness, sadness, and surprise) in order to better predict the user\u2019s perception of the robot. This model can provide valuable information on how to design more engaging dialogues between robots and humans. The combination of these emotional features, along with the dialogue-related features (both linguistic and nonlinguistic) and the F0 value can also provide better feedback in cases where, for instance, a smile can create ambiguity of the perceived user\u2019s emotional display (Halpern and Kets, 2012).\nIn future work, these emotional features coming from real-time facial expression recognition could be used as an online estimator of how well or badly a dialogue is progressing, which would be an important component of a reward signal for Reinforcement Learning approaches to HRI."}], "references": [{"title": "Measurement instruments for the anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety of robots", "author": ["Christoph Bartneck", "Dana Kuli\u0107", "Elizabeth Croft", "Susana Zoghbi."], "venue": "International journal of social robotics 1(1):71\u201381.", "citeRegEx": "Bartneck et al\\.,? 2009", "shortCiteRegEx": "Bartneck et al\\.", "year": 2009}, {"title": "A real time and robust facial expression recognition and imitation approach for affective human-robot interaction using gabor filtering", "author": ["Felipe Cid", "Jos\u00e9 Augusto Prado", "Pablo Bustos", "Pedro Nunez."], "venue": "Intelligent Robots and Systems (IROS), 2013", "citeRegEx": "Cid et al\\.,? 2013", "shortCiteRegEx": "Cid et al\\.", "year": 2013}, {"title": "Inference of human beings emotional states from speech in human\u2013 robot interactions", "author": ["Laurence Devillers", "Marie Tahon", "Mohamed A Sehili", "Agnes Delaborde."], "venue": "International Journal of Social Robotics 7(4):451\u2013463.", "citeRegEx": "Devillers et al\\.,? 2015", "shortCiteRegEx": "Devillers et al\\.", "year": 2015}, {"title": "Emotional and conversational nonverbal signals", "author": ["Paul Ekman."], "venue": "Language, knowledge, and representation, Springer, pages 39\u201350.", "citeRegEx": "Ekman.,? 2004", "shortCiteRegEx": "Ekman.", "year": 2004}, {"title": "Creating training corpora for micro-planners", "author": ["C. Gardent", "A. Shimorina", "S. Narayan", "L. PerezBeltrachini."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). To appear.", "citeRegEx": "Gardent et al\\.,? 2017", "shortCiteRegEx": "Gardent et al\\.", "year": 2017}, {"title": "Ambiguous language and differences in beliefs", "author": ["Joseph Y. Halpern", "Willemien Kets."], "venue": "CoRR abs/1203.0699. http://arxiv.org/abs/1203.0699.", "citeRegEx": "Halpern and Kets.,? 2012", "shortCiteRegEx": "Halpern and Kets.", "year": 2012}, {"title": "When calls go wrong: How to detect problematic calls based on log-files and emotions", "author": ["Ota Herm", "Alexander Schmitt", "Jackson Liscombe"], "venue": "In Ninth Annual Conference of the International Speech Communication Association", "citeRegEx": "Herm et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Herm et al\\.", "year": 2008}, {"title": "Construction and evaluation of a user experience questionnaire", "author": ["Bettina Laugwitz", "Theo Held", "Martin Schrepp."], "venue": "Symposium of the Austrian HCI and Usability Engineering Group. Springer, pages 63\u201376.", "citeRegEx": "Laugwitz et al\\.,? 2008", "shortCiteRegEx": "Laugwitz et al\\.", "year": 2008}, {"title": "Automatic measurement of syntactic complexity in child language acquisition", "author": ["Xiaofei Lu."], "venue": "International Journal of Corpus Linguistics 14(1):3\u201328.", "citeRegEx": "Lu.,? 2009", "shortCiteRegEx": "Lu.", "year": 2009}, {"title": "The relationship of lexical richness to the quality of esl learners oral narratives", "author": ["Xiaofei Lu."], "venue": "The Modern Language Journal 96(2):190\u2013208.", "citeRegEx": "Lu.,? 2012", "shortCiteRegEx": "Lu.", "year": 2012}, {"title": "Computational methods for corpus annotation and analysis", "author": ["Xiaofei Lu."], "venue": "Springer.", "citeRegEx": "Lu.,? 2014", "shortCiteRegEx": "Lu.", "year": 2014}, {"title": "Automatic detection of miscommunication in spoken dialogue systems", "author": ["Raveesh Meena", "Jos\u00e9 Lopes Gabriel Skantze", "Joakim Gustafson."], "venue": "16th Annual Meeting of the Special Interest Group on Discourse and Dialogue. page 354.", "citeRegEx": "Meena et al\\.,? 2015", "shortCiteRegEx": "Meena et al\\.", "year": 2015}, {"title": "Emotionally expressive robot behavior improves human-robot collaboration", "author": ["Jekaterina Novikova", "Leon Watts", "Tetsunari Inamura."], "venue": "Robot and Human Interactive Communication (RO-MAN), 2015 24th IEEE International Symposium on. IEEE,", "citeRegEx": "Novikova et al\\.,? 2015", "shortCiteRegEx": "Novikova et al\\.", "year": 2015}, {"title": "Hybrid chat and task dialogue for more engaging hri using reinforcement learning", "author": ["Ioannis Papaioannou", "Christian Dondrup", "Jekaterina Novikova", "Oliver Lemon."], "venue": "Robot and Human Interactive Communication (RO-MAN), 2017 26th IEEE", "citeRegEx": "Papaioannou et al\\.,? 2017", "shortCiteRegEx": "Papaioannou et al\\.", "year": 2017}, {"title": "Combining Chat and Task-Based Multimodal Dialogue for More Engaging HRI: A Scalable Method Using Reinforcement Learning", "author": ["Ioannis Papaioannou", "Oliver Lemon."], "venue": "Proceedings of the Companion of the 2017 ACM/IEEE International", "citeRegEx": "Papaioannou and Lemon.,? 2017", "shortCiteRegEx": "Papaioannou and Lemon.", "year": 2017}, {"title": "Speech emotion recognition in emotional feedbackfor human-robot interaction", "author": ["Javier G R\u00e1zuri", "David Sundgren", "Rahim Rahmani", "Antonio Moran", "Isis Bonet", "Aron Larsson."], "venue": "International Journal of Advanced Research in Artificial Intelligence", "citeRegEx": "R\u00e1zuri et al\\.,? 2015", "shortCiteRegEx": "R\u00e1zuri et al\\.", "year": 2015}, {"title": "Modeling and predicting quality in spoken human-computer interaction", "author": ["Alexander Schmitt", "Benjamin Schatz", "Wolfgang Minker."], "venue": "Proceedings of the SIGDIAL 2011 Conference. Association for Computational Linguistics, pages 173\u2013184.", "citeRegEx": "Schmitt et al\\.,? 2011", "shortCiteRegEx": "Schmitt et al\\.", "year": 2011}, {"title": "Interaction quality: assessing the quality of ongoing spoken dialog interaction by expertsand how it relates to user satisfaction", "author": ["Alexander Schmitt", "Stefan Ultes."], "venue": "Speech Communication 74:12\u201336.", "citeRegEx": "Schmitt and Ultes.,? 2015", "shortCiteRegEx": "Schmitt and Ultes.", "year": 2015}, {"title": "Emulating empathy in socially assistive robotics", "author": ["Adriana Tapus", "Maja J Mataric."], "venue": "AAAI Spring Symposium: Multidisciplinary Collaboration for Socially Assistive Robotics. pages 93\u201396.", "citeRegEx": "Tapus and Mataric.,? 2007", "shortCiteRegEx": "Tapus and Mataric.", "year": 2007}, {"title": "Adaptive emotional expression in robot-child interaction", "author": ["Myrthe Tielman", "Mark Neerincx", "John-Jules Meyer", "Rosemarijn Looije."], "venue": "Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction. ACM, pages 407\u2013", "citeRegEx": "Tielman et al\\.,? 2014", "shortCiteRegEx": "Tielman et al\\.", "year": 2014}, {"title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions", "author": ["Zhihong Zeng", "Maja Pantic", "Glenn I Roisman", "Thomas S Huang."], "venue": "IEEE transactions on pattern analysis and machine intelligence 31(1):39\u201358.", "citeRegEx": "Zeng et al\\.,? 2009", "shortCiteRegEx": "Zeng et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "tant factor to be considered both in human-robot interaction research (Cid et al., 2013; Novikova et al., 2015; Devillers et al., 2015) and in the area of spoken dialogue systems (Herm et al.", "startOffset": 70, "endOffset": 135}, {"referenceID": 12, "context": "tant factor to be considered both in human-robot interaction research (Cid et al., 2013; Novikova et al., 2015; Devillers et al., 2015) and in the area of spoken dialogue systems (Herm et al.", "startOffset": 70, "endOffset": 135}, {"referenceID": 2, "context": "tant factor to be considered both in human-robot interaction research (Cid et al., 2013; Novikova et al., 2015; Devillers et al., 2015) and in the area of spoken dialogue systems (Herm et al.", "startOffset": 70, "endOffset": 135}, {"referenceID": 6, "context": ", 2015) and in the area of spoken dialogue systems (Herm et al., 2008; Meena et al., 2015).", "startOffset": 51, "endOffset": 90}, {"referenceID": 11, "context": ", 2015) and in the area of spoken dialogue systems (Herm et al., 2008; Meena et al., 2015).", "startOffset": 51, "endOffset": 90}, {"referenceID": 3, "context": "Social signals are recognized well from human facial expressions or prosodic features of speech (Ekman, 2004; Zeng et al., 2009), and have become the most popular methods for recognising human affective signals in human-robot interaction (R\u00e1zuri et al.", "startOffset": 96, "endOffset": 128}, {"referenceID": 20, "context": "Social signals are recognized well from human facial expressions or prosodic features of speech (Ekman, 2004; Zeng et al., 2009), and have become the most popular methods for recognising human affective signals in human-robot interaction (R\u00e1zuri et al.", "startOffset": 96, "endOffset": 128}, {"referenceID": 15, "context": ", 2009), and have become the most popular methods for recognising human affective signals in human-robot interaction (R\u00e1zuri et al., 2015; Devillers et al., 2015; Cid et al., 2013).", "startOffset": 117, "endOffset": 180}, {"referenceID": 2, "context": ", 2009), and have become the most popular methods for recognising human affective signals in human-robot interaction (R\u00e1zuri et al., 2015; Devillers et al., 2015; Cid et al., 2013).", "startOffset": 117, "endOffset": 180}, {"referenceID": 1, "context": ", 2009), and have become the most popular methods for recognising human affective signals in human-robot interaction (R\u00e1zuri et al., 2015; Devillers et al., 2015; Cid et al., 2013).", "startOffset": 117, "endOffset": 180}, {"referenceID": 19, "context": "In human-robot interaction, recognized human emotions are mostly used for mimicking human behaviour and enhancing the empathy towards a robot both in children (Tielman et al., 2014) and ar X iv :1 70 6.", "startOffset": 159, "endOffset": 181}, {"referenceID": 18, "context": "in adult users (Tapus and Mataric, 2007).", "startOffset": 15, "endOffset": 40}, {"referenceID": 6, "context": "In the area of spoken dialogue systems, signals recognised from linguistic cues and prosody have been used to detect problematic dialogues (Herm et al., 2008) and to assess dialogue quality as a whole (Schmitt and Ultes, 2015).", "startOffset": 139, "endOffset": 158}, {"referenceID": 17, "context": ", 2008) and to assess dialogue quality as a whole (Schmitt and Ultes, 2015).", "startOffset": 50, "endOffset": 75}, {"referenceID": 11, "context": "This type of dialogue-related signals has also been used to automatically detect miscommunication (Meena et al., 2015), or to predict the user satisfaction (Schmitt et al.", "startOffset": 98, "endOffset": 118}, {"referenceID": 16, "context": ", 2015), or to predict the user satisfaction (Schmitt et al., 2011).", "startOffset": 45, "endOffset": 67}, {"referenceID": 14, "context": "The human-robot dialogue system was evaluated via a user study in which human subjects interacted with a Pepper robot1 acting autonomously using the system described in (Papaioannou and Lemon, 2017; Papaioannou et al., 2017).", "startOffset": 169, "endOffset": 224}, {"referenceID": 13, "context": "The human-robot dialogue system was evaluated via a user study in which human subjects interacted with a Pepper robot1 acting autonomously using the system described in (Papaioannou and Lemon, 2017; Papaioannou et al., 2017).", "startOffset": 169, "endOffset": 224}, {"referenceID": 7, "context": "The questionnaire was based on a combination of the User Experience Questionnaire UEQ (Laugwitz et al., 2008) and the Godspeed Questionnaire (Bartneck et al.", "startOffset": 86, "endOffset": 109}, {"referenceID": 0, "context": ", 2008) and the Godspeed Questionnaire (Bartneck et al., 2009).", "startOffset": 39, "endOffset": 62}, {"referenceID": 4, "context": "Following Gardent et al. (2017), we analyse the dialogue textual data in terms of length of ut-", "startOffset": 10, "endOffset": 32}, {"referenceID": 8, "context": "We used the Lexical Complexity Analyser (Lu, 2009) to measure various dimensions of lexical richness, such as lexical sophistication, lexical diversity and mean segmental type-token ratio.", "startOffset": 40, "endOffset": 50}, {"referenceID": 9, "context": "We complement the traditional measure of lexical diversity type-token ratio (TTR) with the more robust measure of mean segmental type-token ratio (MSTTR) (Lu, 2012), which divides all the dialogues into successive segments of a given length and then calculates the average TTR of all segments.", "startOffset": 154, "endOffset": 164}, {"referenceID": 8, "context": "We used the D-Level Analyser (Lu, 2009) to evaluate syntactic variation and complexity of human references using the revised D-Level Scale (Lu, 2014).", "startOffset": 29, "endOffset": 39}, {"referenceID": 10, "context": "We used the D-Level Analyser (Lu, 2009) to evaluate syntactic variation and complexity of human references using the revised D-Level Scale (Lu, 2014).", "startOffset": 139, "endOffset": 149}, {"referenceID": 5, "context": "The combination of these emotional features, along with the dialogue-related features (both linguistic and nonlinguistic) and the F0 value can also provide better feedback in cases where, for instance, a smile can create ambiguity of the perceived user\u2019s emotional display (Halpern and Kets, 2012).", "startOffset": 273, "endOffset": 297}], "year": 2017, "abstractText": "Recognition of social signals, from human facial expressions or prosody of speech, is a popular research topic in human-robot interaction studies. There is also a long line of research in the spoken dialogue community that investigates user satisfaction in relation to dialogue characteristics. However, very little research relates a combination of multimodal social signals and language features detected during spoken face-to-face human-robot interaction to the resulting user perception of a robot. In this paper we show how different emotional facial expressions of human users, in combination with prosodic characteristics of human speech and features of human-robot dialogue, correlate with users\u2019 impressions of the robot after a conversation. We find that happiness in the user\u2019s recognised facial expression strongly correlates with likeability of a robot, while dialogue-related features (such as number of human turns or number of sentences per robot utterance) correlate with perceiving a robot as intelligent. In addition, we show that facial expression, emotional features, and prosody are better predictors of human ratings related to perceived robot likeability and anthropomorphism, while linguistic and non-linguistic features more often predict perceived robot intelligence and interpretability. As such, these characteristics may in future be used as an online reward signal for in-situ Reinforcement Learningbased adaptive human-robot dialogue systems. Figure 1: Left: a live view of experimental setup showing a participant interacting with Pepper. Right: a diagram of experimental setup showing the participant (green) and the robot (white) positioned face to face. The scene was recorded by cameras (triangles C) from the robot\u2019s perspective focusing on the face of the participant and from the side, showing the whole scene. The experimenter (red) was seated behind a divider.", "creator": "LaTeX with hyperref package"}}}