{"id": "1706.02929", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Evidence Against Evidence Theory (?!)", "abstract": "discussion paper is concerned with the apparent greatest variance of the mathematical strength of evidence ( mte ) on shafer \\ cite { shafer : 76 }, which has been strongly criticized by wasserman \\ cite { wasserman : 92ijar } - the relationship to accuracy.", "histories": [["v1", "Thu, 8 Jun 2017 17:23:34 GMT  (24kb)", "http://arxiv.org/abs/1706.02929v1", "30 pages. arXiv admin note: substantial text overlap witharXiv:1704.04000"]], "COMMENTS": "30 pages. arXiv admin note: substantial text overlap witharXiv:1704.04000", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mieczys{\\l}aw a k{\\l}opotek", "andrzej matuszewski"], "accepted": false, "id": "1706.02929"}, "pdf": {"name": "1706.02929.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["klopotek@ipipan.waw.pl"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n02 92\n9v 1\nThis paper is concerned with the apparent greatest weakness of the Mathematical Theory of Evidence (MTE) of Shafer [27], which has been strongly criticized by Wasserman [37] - the relationship to frequencies. Weaknesses of various proposals of probabilistic interpretation of MTE belief functions are demonstrated. A new frequency-based interpretation is presented overcoming various drawbacks of earlier interpretations."}, {"heading": "1 Introduction", "text": "Wasserman in [37] raised serious concerns against the Mathematical Theory of Evidence (MTE) developed by Dempster and Shafer since 1967 - hence also called Dempster-ShaferTheory (DST) (see [30] for a thorough review of this theory, for major formal definitions\nsee Appendix A). One of arguments against MTE is related to Shafer\u2019s attitude towards frequencies. Shafer in [30] claims that probability theory developed over last years from the old-style frequencies towards modern subjective probability theory within the framework of Bayesian theory. By analogy he claims that the very attempt to consider relation between MTE and frequencies is old-fashioned and out of date and should be at least forbidden - for the sake of progress of humanity. Wasserman opposes this view ([37], p.371) reminding \u201dmajor success story in Bayesian theory\u201d, the exchangeability theory of de Finetti [4]. It treats frequencies as special case of Bayesian belief. \u201dThe Bayesian theory contains within it a definition of frequency probability and a description of the exact assumptions necessary to invoke that definition\u201d [37]. Wasserman dismisses Shafer\u2019s suggestion that probability relies on analogy of frequency. .\nShafer, on the other hand, lets frequencies live a separate life. MTE beliefs and frequencies are separated. But in this way we are left without a definition of frequentistic belief function [37].\nThis paper is devoted to discussion of drawbacks of various interpretations of MTE. A way out is proposed in a separate paper, the summary of our proposal is contained in Appendix B."}, {"heading": "2 Basic Problems with Frequencies in MTE", "text": "Shafer in [28], [29] gave the following formal probabilistic interpretation of belief function: Let Pr be a probabilistic measure over the infinite discrete sample space \u2126, let \u0393 be a function \u0393 : \u2126 \u2192 2\u039e. Then Bel over the space (frame of discernment) \u039e is given as:\nBel(A) = Pr({\u03c9 \u2208 \u2126|\u0393(\u03c9) \u2286 A})\nThen clearly\nm(A) = Pr({\u03c9 \u2208 \u2126|\u0393(\u03c9) = A})\nand\nP l(A) = Pr({\u03c9 \u2208 \u2126|\u0393(\u03c9) \u2229A 6= \u2205})\nLet us consider the database in Table 1.\nLet the measurable A take values a1, a2, a3, a4, and let the non-observable attribute D take values d1, d2, d3. Let us define the function \u0393 as capability to predict values of attribute D given A and let us calculate it based on training sample contained in Table 1. We see that if A takes value a1, then we know that D takes value d1 - hence \u0393(A = a1) = {d1}. Similarly values a3 and a4 of attribute A determine uniquely the value of attribute D. But in case of A = a2 we have an ambiguity: D is equal either d2 or d3. Hence \u0393(A = a1) = {d2, d3}. Now, assuming frequency probabilities from Table 1 we calculate easily from Shafer\u2019s formula:\nm({d1}) = 0.4 Bel({d1}) = 0.4 P l({d1}) = 0.4\nm({d2}) = 0 Bel({d2}) = 0 P l({d2}) = 0.4 m({d3}) = 0.2 Bel({d3}) = 0.2 P l({d3}) = 0.6\nm({d1, d2}) = 0 Bel({d1, d2}) = 0.4 P l({d1, d2}) = 0.8 m({d1, d3}) = 0 Bel({d1, d3}) = 0.6 P l({d1, d3}) = 1 m({d2, d3}) = 0.4 Bel({d2, d3}) = 0.6 P l({d2, d3}) = 0.6\nm({d1, d2, d3}) = 0 Bel({d1, d2, d3}) = 1 P l({d1, d2, d3}) = 1\nIn probability theory two variables are independent if Pr(A \u2229 B) = Pr(A) \u00b7 Pr(B).\nLet us consider two measurables A and B from Table 2.\nFunction \u0393 be, as previously, be prediction of value of variable D based on value of A,\nand \u0393\u2032 be prediction of variable D given value of B. Let us define\nBel(Z) = Pr({\u03c9 \u2208 \u2126|\u0393(\u03c9) \u2286 Z})\nBel\u2032(Z) = Pr({\u03c9 \u2208 \u2126|\u0393\u2032(\u03c9) \u2286 Z})\nLet us imagine that we want to combine information from attributes A and B to improve prediction of D by formulating a new function \u0393\u201d being the base for a new belief function:\nBel\u201d(Z) = Pr({\u03c9 \u2208 \u2126|\u0393\u201d(\u03c9) \u2286 Z})\nAs observations being basis of functions \u0393 and \u0393\u2032 are obviously independent, so one would expect that the belief function Bel\u201d is simply the combination OF INDEPENDENT EVIDENCE Bel and Bel\u2032 via Dempster rule. And this is in fact the case:\nBel\u201d = Bel \u2295 Bel\u2032\nBut there is one weak point in all of this: Bel\u2032 is (and will always be) a vacuous belief function, hence it does not contribute anything to our knowledge of the value of the attribute. Reverting this example we can say that whenever we combine two non-vacuous belief functions, then the measurements underlying their empirical calculation are for sure statistically dependent. So we claim that: Under Shafer\u2019s frequentist interpretation, if two belief functions are (statistically) independent then at least one of them is non-informative.\nAnother practical limitation of Shafer\u2019s probabilistic interpretation is consideration of\nconditional beliefs. Let as look at Table 3.\nIf we want to calculate conditional probability of A = a1 given observation that A takes only one of values a1 or a2, we select cases from the database fitting the condition\nA = a1 \u2228 A = a2, and thereafter within this subset we calculate frequency probabilities: Pr(A = a1|A = a1 \u2228 A = a2) = 1/3 = 0.33.Now, based on Table 4 let us run similar procedure for MTE beliefs.\nLet us assume that we want to find out our degree of belief in values of D given that only values d1 or d2 are allowed. For this purpose we restrict the set of cases to those cases \u2126\u2032 for which our function \u0393 has non-empty intersection with the set of values of interest. For this group of cases we define the function \u0393\u2032(\u03c9) = \u0393(\u03c9)\u2229{d1, d2}. Let :\nBel(Z) = Pr({\u03c9 \u2208 \u2126|\u0393(\u03c9) \u2286 Z})\nBel\u2032(Z) = Pr({\u03c9 \u2208 \u2126\u2032|\u0393\u2032(\u03c9) \u2286 Z})\nAdditionally let us define the simple support function Bel\u201d such thatm\u201d({d1, d2}) = 1.\nIt is easily seen that:\nBel\u2032 = Bel \u2295Bel\u201d\n(as expected because the expression Bel \u2295 Bel\u201dmeans shaferian conditioning on event {d1, d2}). And everything would be O.K. if it were not that the function \u0393 \u2032 has little to do with the non-observable attribute D - compare line no.3 of Table 4. Let us remind that function \u0393 represented by definition for a given observed value of variable A the set of potentially possible values of attribute D, deducible from the training sample. For every object \u03c9, if we know the true value a of A one of the values from the set \u0393(\u03c9) was the true value of D for this object \u03c9. But within \u0393\u2032(\u03c9) the true value of attribute D does not need to be contained - compare line no.3 of Table 4. But, let us remind, Shafer claimed [28, 29] that function \u0393 indicates that the variable takes for object \u03c9 one of the values \u0393(\u03c9). But we have just demonstrated that already after a single step of conditioning function \u0393\u2032 simply tells lies. Its meaning is not dependent solely on subpopulation \u2126\u2032, to which it refers, but also on the history, how this population was selected. But we had for probability distributions that after conditioning a variable for not rejected objects took always those values which were indicated by the result of conditioning.\nBoth above failures of Shafer\u2019s probabilistic interpretation of his own theory of evidence were driving forces behind the elaboration of a new probabilistic interpretation of MTE presented subsequently.\nWe shall summarize this section saying that: Shafer\u2019s probabilistic interpretation of Dempster\u2019s & Shafer\u2019s Mathematical Theory of Evidence is not compatible with this theory: It does not fit the Dempster\u2019s rule of combination of independent evidence. .\nAs statistical properties of Shafer\u2019s [27] notion of evidence are concerned, further\ncriticism has been expressed by Halpern and Fagin ([11] in sections 4-5). Essentially the criticism is pointed there at the fact that \u201dthe belief that represents the joint observation is equal to the combination is in general not equal to the combination of the belief functions representing the individual (independent) observations\u201d (p.297). The other point raised there that though it is possible to capture properly in belief functions evidence in terms of probability of observations update functions (section 4 of [11]), it is not possible to do the same if we would like to capture evidence in terms of beliefs of observations update functions (section 5 of [11])."}, {"heading": "3 Smets\u2019 Approach to Frequencies", "text": "Smets [34] has made some strong statements in defense of the Dempster-Shafer theory against sharp criticism of this theory by its opponents as well as unfortunate users of the MTE who wanted to attach it to the \u201ddirty reality\u201d (that is objectively given databases). He insisted on Bels not being connected to any empirical measure (frequency, probability etc.) considering the domain of MTE applications as the one where \u201dwe are ignorant of the existence of probabilities\u201d, and not one with \u201dpoorly known probabilities\u201d ([34], p.324). The basic property of probability, which should be dropped in the MTE axiomatization, should be the additivity of belief measures. Surely, it is easy to imagine situations where - in the real life - the additivity is not granted: Imagine we have had a cage with 3 pigs, we put into it 3 hungry lions two hours ago, how many animals are there now ? (3 + 3 < 6). Or ten years ago we left one young man and one young woman on an island in the middle of the atlantic ocean with food and weapons sufficing for 20 years. How many human beings are there now ? (1 + 1 > 2). The trouble is, however, that the objects stored in databases of a computer behave usually (under normal operation) in an additive manner. Hence the MTE is simply disqualified for any reasoning within human collected data on real world, if we accept the philosophy\nof Smets and Shafer.\nThe question may be raised at this point, what else practically useful can be obtained from a computer reasoning on the basis of such a MTE. If the MTE models, as Smets and Shafer claim, human behaviour during evidential reasoning, then it would have to be demonstrated that humans indeed reason as MTE. We take e.g. 1000 people who never heard of Dempster-Shafer theory, briefly explain the static component, provide them with two opinions of independent experts and expect of them to answers what are their final beliefs. Should their answers correspond to results of the MTE (at least converge toward them), then the computer, if fed with our knowledge, would be capable to predict our conclusions on a given subject. However, to our knowledge, no experiment like this has ever been carried out. Under these circumstances the computer reasoning with MTE would tell us what we have to think and not what we think. But we don\u2019t suspect that anybody would be happy about a computer like this.\nHence, from the point of view of computer implementation the philosophy of Smets\nand Shafer is not acceptable.Compare also Discussion in [11] on the subject.\nSmets felt a bit uneasy about a total loss of reference to any scientific experiment checking practical applicability of the MTE and suggested some probabilistic background for decision making (e.g. the pigeonistic probabilities of Smets), but we are afraid that by these interpretations he falls precisely into the same pitfalls he claimed to avoid by his highly abstract philosophy.\nAs Smets probabilistic interpretations are concerned, let us \u201dcontinue\u201d the killer example of [34] on pages 330-331. \u201dThere are three potential killers, A, B, C. Each can use a gun or a knife. We shall select one of them, but you will not know how we select the killer. The killer selects his weapon by a random process with p(gun)=0.2 and p(knife)=0.8. Each of A, B, C has his own personal random device, the random devices are unrelated. ...... Suppose you are a Bayesian and you must express your \u201dbelief\u201d that the killer will\nuse a gun. The BF (belief function) solution gives Bel(gun) = 0.2 \u00d7 0.2 \u00d7 0.2 = 0.008. ..... Would you defend 0.2 ? But this applies only if I select a killer with a random device ...... But we never said we would use a random device; we might be a very hostile player and cheat whenever we can. ... . So you could interpret Bel(x) as the probability that you are sure to win whatever Mother Nature (however hostile) will do.\u201d Yes, we will try to continue the hostile Mother Nature game here. For completeness we understand that Bel(knife) = 0.83 = 0.512 and Bel({gun, knife}) = 1. But suppose there is another I, the chief of gangster science fiction physicians, making decisions independly of the chief I of the killers. The chief I of physicians knows of the planned murder and has three physicians X,Y,Z. Each can either rescue a killed man or let him die. I shall select one of them, but you will not know how I select the physician. The physician, in case of killing with a gun, selects his attitude by a random process with p(rescue|gun) = 0.2 and p(let die|gun) = 0.8 and he lets the person die otherwise. Each of X, Y, Z has his own personal random device, the random devices are unrelated. ...... Suppose you are a Bayesian and you must express your \u201dbelief\u201d that the physician will rescue if the killer will use a gun. The BF (belief function) solution gives Bel1(rescue|gun) = 0.2 3 = 0.008. Bel1(let die|gun) = 0.8 3 = 0.512, Bel1({recue, let die}|gun) = 1. Also Bel2(let die|knife) = 1. As the scenarios for Bel1 and Bel2 are independent, let us combine them by the Dempster rule: Bel12 = Bel1\u2295Bel2. We make use of the Smets\u2019 claim that \u201dthe de re and de dicto interpretations lead to the same results\u201d ([34], p. 333), that is Bel(A|B) = Bel(\u00acB \u2228 A). Hence\nm12({(gun, let die), (knife, let die), (gun, rescue)}) = 0.480\nm12({(gun, rescue), (knife, let die)}) = 0.008\nm12({(knife, let die), (gun, let die)}) = 0.512\nNow let us combine Bel12 with the original Bel. We obtain:\nm\u2295m12((gun, let die) = 0.008 \u00b7 0.480 + 0.008 \u00b7 0.512 = 0.008 \u00b7 0.992\nBut these two unfriendly chiefs of gangster organizations can be extremely unfriendly and in fact your chance of winning a bet may be as bad as 0.008 \u00b7 0.512 for the event (gun, let die). Hence the \u201dmodel\u201d proposed by Smets for understanding beliefs functions as \u201dunfriendly Mother Nature\u201d is simply wrong. If the Reader finds the combination of Bel2 with the other Bels a little tricky, then for justification He should refer to the paper of Smets and have a closer look at all the other examples.\nNow returning to the philosophy of \u201dsubjectivity\u201d of Bel measures: Even if a human being may possess his private view on a subject, it is only after we formalize the feeling of subjectiveness and hence ground it in the data that we can rely on any computer\u2019s \u201dopinion\u201d. We hope we have found one such formalization in this paper. The notion of labeling developed here substitutes one aspect of subjective human behaviour - if one has found one plausible explanation, one is too lazy to look for another one. So the process of labeling may express our personal attitudes, prejudices, sympathies etc. The interpretation drops deliberately the strive for maximal objectiveness aimed at by traditional statistical analysis. Hence we think this may be a promising path for further research going beyond the DS-Theory formalism.\nSmets [34] views the probability theory as a formal mathematical apparatus and hence puts it on the same footing as his view of the MTE. However, in our opinion, he ignores totally one important thing: The abstract concept of probability has its real world counterpart of relative frequency which tends to behave approximately like the theoretical probability in sufficiently many experimental settings as to make the abstract concept of probability useful for practical life. And a man-in-the-street will expect of the MTE to possess also such a counterpart or otherwise the MTE will be considered as another\nversion of the theory of counting devils on a pin-head.\nIt is worth mentioning that Smets made recently an attempt to justify usage of belief functions instead of Bayesian probabilities, to identify situations in which usage of belief functions is more reasonable than usage of probabilities [36]. However, the data modifying impact of application of Dempster-rule is not explicitly recognized there and numerical examples presented there deliberately avoid situations where more than one belief function may have more than one focal point."}, {"heading": "4 MTE and Random Sets", "text": "The canonic random set interpretation [20] is one with a statistical process over set instantiations. The rule of combination assumes then that two such statistically independent processes are run and we are interested in their intersections. This approach is not sound as empty intersection is excluded and this will render any two processes statistically dependent. We overcome this difficulty assuming in a straightforward manner that we are \u201dwalking\u201d from population to population applying the Rule of Combination. Classical DS theory in fact assumes such a walk implicitly or it drops in fact the assumption that Bel() of the empty set is equal 0. In this sense the random set approaches may be considered as sound as ours.\nHowever, in many cases the applications of the model are insane. For example, to imitate the logical inference it is frequently assumed that we have a Bel-function describing the actual observed value of a predicate P(x), and a Bel-function describing the implication \u201dIf P(x) then Q(x)\u201d [17]. It is assumed further that the evidence on the validity of both Bel\u2019s has been collected independently and one applies the DS-rule of combination to calculate the Bel of the predicate Q(x). One has then to assume that there is a focal\nm of the following expression: m({(P (x), Q(x)), (\u00acP (x), Q(x)), (\u00acP (x),\u00acQ(x))}) which actually means that with non-zero probability at the same time P (x) and \u00acP (x) hold for the same object as we will see in the following example: Let Bel1 represent our belief in the implication, with focal points:\nm1(P (x) \u2192 Q(x)) = 0.5, m1(\u00ac(P (x) \u2192 Q(x))) = 0.5,\nLet further the independent opinion Bel2 on P(x) be available in the form of focal points:\nm2(P (x)) = 0.5, m2(\u00acP (x)) = 0.5\nLet Bel12 = Bel1 \u2295 Bel2 represent the combined opinions of both experts. The focal points of Bel12 are:\nm12({(P (x), Q(x))}) = 0.33, m12({(P (x),\u00acQ(x))}) = 0.33,\nm12({(\u00acP (x), Q(x)), (\u00acP (x),\u00acQ(x))}) = 0.33\nm12({(P (x), Q(x))}) = 0.33 makes us believe that there exist objects for which both P(x) and Q(x) holds. However, a sober (statistical) look at expert opinions suggests that all situations for which the implication P (x) \u2192 Q(x) holds, must result from falsity of P (x), hence whenever Q(x) holds then \u00acP (x) holds. These two facts combined mean that P(x) and its negation have to hold simultaneously. This is actually absurdity overseen deliberately. The source of this misunderstanding is obvious: the lack of proper definition of what is and what is not independent. Our interpretation allows for sanitation of this situation. We are not telling that the predicate and its negation hold simultaneously. Instead we say that for one object we modify the measurement procedure (set a label) in such a way that it, applied for calculation of P (x), yields true and at the same time for another object, with the same original properties we make another modification of measurement procedure (attach a label to it) so that measurement of \u00acP (x) yields also true, because possibly two different persons were enforcing their different beliefs onto different\nsubsets of data.\nOur approach is also superior to canonical random set approach in the following sense: The canonical approach requires knowledge of the complete random set realizations of two processes on an object to determine the combination of both processes. We, however, postpone the acquisition of knowledge of the precise instantiation of properties of the object by interleaving the concept of measurement and the concept of labeling process. This has a close resemblance to practical processing whenever diagnosis for a patient is made. If a physician finds a set of hypotheses explaining the symptoms of a patient, he will usually not try to carry out other testing procedures than those related to the plausible hypotheses. He runs clearly at risk that there exists a different set of hypotheses also explaining the patients\u2019s symptoms, and so a disease unit possibly present may not be detected on time, but usually the risk is sufficiently low to proceed in this way, and the cost savings may prove enormous."}, {"heading": "5 Upper and Lower Probabilities", "text": "Still another approach was to handle Bel and Pl as lower and upper probabilities [5]. This approach is of limited use as not every set of lower and upper probabilities leads to Bel/Pl functions [16], hence establishing a unidirectional relationship between probability theory and the DS-theory. Under our interpretation, the Bel/Pl function pair may be considered as a kind of interval approximations to some \u201dintrinsic\u201d probability distributions which, however, cannot be accessed by feasible measurements and are only of interest as a kind of qualitative explanation to the physical quantities really measured.\nTherefore another approach was to handle them as lower/upper envelops to some prob-\nability function realization [16], [9]. However, the DS rule of combination of independent evidence failed."}, {"heading": "6 Inner and Outer Measures", "text": "Still another approach was to handle Bels/Pl in probabilistic structures rather than in probabilistic spaces [8]. Here, DS-rule could be justified as one of the possible outcomes of independent combinations, but no stronger properties were available. This is due to the previously mentioned fact that exclusion of empty intersections renders actually most of conceivable processes dependent. Please notice that under our interpretation no such ambiguity occurs. This is because we not only drop empty intersecting objects but also relabel the remaining ones so that any probability calculated afterwards does not refer to the original population.\nSo it was tried to drop the DS-rule altogether in the probabilistic structures, but then it was not possible to find a meaningful rule for multistage reasoning [11]. This is a very important negative outcome. As the Dempster-Shafer-Theory is sound in this respect and possesses many useful properties (as mentioned in the Introduction), it should be sought for an interpretation meeting the axiomatic system of DS Theory rather then tried to violate its fundamentals. Hence we consider our interpretation as a promising one for which decomposition of the joint distribution paralleling the results for probability distributions may be found based on the data."}, {"heading": "7 Rough Set Approach", "text": "An interesting alternative interpretation of the Dempster-Shafer Theory was found within the framework of the rough set theory [32], [10]. Essentially the rough set theory searches for approximation of the value of a decision attribute by some other (explaining) attributes. It usually happens that those attributes are capable only of providing a lower and upper approximation to the value of the decision attribute (that is the set of vectors of explaining attributes supporting only this value of the decision variable, and the set of vectors of explaining attributes supporting also this value of the decision variable resp.for details see texts of Skowron [32] and Grzyma la-Busse [10]). The Dempster Rule of combination is interpreted by Skowron [33] as combination of opinions of independent experts, who possibly look at different sets of explanation attributes and hence may propose different explanations.\nThe difference between our approach and the one based on rough sets lies first of all in the \u201dideological\u201d background: We assume that the \u201ddecision attribute\u201d is set-valued whereas the rough-set approach assumes it to be single-valued. This could have been overcome by some tricks which will not be explained in detail here.But the combination step is here essential: If we assume that the data sets for forming knowledge of these two experts are exhaustive, then it can never occur that these opinions are contradictory. But the MTE rule of combination uses the normalization factor for dealing with cases like this. Also the opinions of experts may have only the form of a simple (that is deterministic) support function. Hence, rough-set interpretation implies axioms not actually present in the MTE. Hence rough set interpretation is on the one hand restrictive, and on the other hand not fully conforming to the general MTE. From our point of view the MTE would change the values of decision variables rather then recover them from expert opinions.\nHere, we come again at the problem of viewing the independence of experts. The MTE assumes some strange kind of independence within the data: the proportionality\nof the distribution of masses of sets of values among intersecting subsets weight by their masses in the other expert opinion. Particularly unfortune is the fact for the rough set theory, that given a value of the decision variable, the respective indicating vectors of explaining variables values must be proportionally distributed among the experts not only for this decision attribute value, but also for all the other decision attribute values that ever belong to the same focal point. Hence applicability of the rough set approach is hard to justify by a simple(, \u201dusual\u201d as Shafer wants) statistical test. On the other hand, statistical independence required for Dempster rule application within our approach can be easily checked.\nTo demonstrate the problem of rough set theory with combination of opinions of independent experts let us consider an examle of two experts having the combined explanatory attributes E1 (for expert 1) and E2 (for expert 2) both trying to guess the decision attribute D. Let us assume that D takes one of two values: d1, d2, E1 takes one of three values e11, e12, e13, E2 takes one of three values e21, e22, e23. Furthermore let us assume that the rough set analysis of an exhaustive set of possible cases shows that the value e11 of the attribute E1 indicates the value d1 of the decision attribute D, e12 indicates d2, e13 indicates the set {d1, d2}, Also let us assume that the rough set analysis of an exhaustive set of possible cases shows that the value e21 of the attribute E2 indicates the value d1 of the decision attribute D, e22 indicates d2, e32 indicates the set {d1, d2}, From the point of view of Bayesian analysis four cases of causal influence may be distinguished (arrows indicate the direction of dependence).\nE1 \u2192 D \u2192 E2\nE1 \u2190 D \u2190 E2\nE1 \u2190 D \u2192 E2\nE1 \u2192 D \u2190 E2\nFrom the point of view of Bayesian analysis, in the last case attributes E1 and E2 have to be unconditionally independent, in the remaining cases: E1 and E2 have to be independent conditioned on D. Let us consider first unconditional independence of E1 and E2. Then we have that (For meaning of Prob P (\u03c9)\n\u03c9 see Appendix B):\n( ProbP (\u03c9)\n\u03c9\nE1(\u03c9) = e11 \u2227 E2(\u03c9) = e22) =\n= ( ProbP (\u03c9)\n\u03c9\nE1(\u03c9) = e11) \u00b7 ( ProbP (\u03c9)\n\u03c9\nE2(\u03c9) = e22) > 0\nHowever, it is impossible that (Prob P (\u03c9)\n\u03c9 E1(\u03c9) = e11 \u2227 E2(\u03c9) = e22) > 0 because we have\nto do with experts who may provide us possibly with information not specific enough, but will never provide us with contradictory information. We conclude that unconditional independence of experts is impossible. Let us turn to independence of E1 and E2 if conditioned on D. We introduce the following denotation:\np1 = ProbP (\u03c9)\n\u03c9\nD(\u03c9) = d1\np2 = ProbP (\u03c9)\n\u03c9\nD(\u03c9) = d2\ne\u20321 = Prob(D(\u03c9)=d1)\u2227P (\u03c9)\n\u03c9\nE1(\u03c9) = e11\ne\u20323 = Prob(D(\u03c9)=d1)\u2227P (\u03c9)\n\u03c9\nE1(\u03c9) = e13\nf \u20321 = Prob(D(\u03c9)=d1)\u2227P (\u03c9)\n\u03c9\nE2(\u03c9) = e21\nf \u20323 = Prob(D(\u03c9)=d1)\u2227P (\u03c9)\n\u03c9\nE2(\u03c9) = e23\ne2\u201d = Prob(D(\u03c9)=d2)\u2227P (\u03c9)\n\u03c9\nE1(\u03c9) = e12\ne3\u201d = Prob(D(\u03c9)=d2)\u2227P (\u03c9)\n\u03c9\nE1(\u03c9) = e13\nf2\u201d = Prob(D(\u03c9)=d2)\u2227P (\u03c9)\n\u03c9\nE2(\u03c9) = e22\nf3\u201d = Prob(D(\u03c9)=d2)\u2227P (\u03c9)\n\u03c9\nE2(\u03c9) = e23\nLet Bel1 and m1 be the belief function and the mass function representing the knowledge of the first expert, let Bel2 and m2 be the belief function and the mass function representing the knowledge of the second expert. Let Bel12 and m12 be the belief function and the mass function representing the knowledge contained in the combined usage of attributes E1, E2 if used for prediction of D - on the grounds of the rough set theory. It can be easily checked that:\nm1({d1}) = e \u2032 1 \u00b7 p1, m1({d2}) = e2\u201d \u00b7 p2, m1({d1, d2}) = e \u2032 3 \u00b7 p1,+e3\u201d \u2032 \u00b7 p2\nm2({d1}) = f \u2032 1 \u00b7 p1, m2({d2}) = f2\u201d \u00b7 p2, m2({d1, d2}) = f \u2032 3 \u00b7 p1,+f3\u201d \u2032 \u00b7 p2\nand if we assume the conditional independence of E1 and E2 conditioned on D, then we obtain:\nm12({d1}) = e \u2032 1 \u00b7 f \u2032 1 \u00b7 p1 + e \u2032 1 \u00b7 f \u2032 3 \u00b7 p1 + e \u2032 3 \u00b7 f \u2032 1 \u00b7 p1\nm12({d2}) = e2\u201d \u00b7 f2\u201d \u00b7 p2 + e2\u201d \u00b7 f3\u201d \u00b7 p2 + e3\u201d \u00b7 f2\u201d \u00b7 p2\nm12({d1, d2}) = e \u2032 3 \u00b7 f \u2032 3 \u00b7 p1 + e3\u201d \u00b7 f3\u201d \u00b7 p2\nHowever, the Dempster rule of combination would result in (c - normalization constant):\nm1 \u2295m2({d1}) = c \u00b7 (e \u2032 1 \u00b7 f \u2032 1 \u00b7 p 2 1 + e \u2032 1 \u00b7 f \u2032 3 \u00b7 p 2 1 + e \u2032 1 \u00b7 f3\u201d \u00b7 p1 \u00b7 p2 + e \u2032 3 \u00b7 f \u2032 1 \u00b7 p 2 1 + e3\u201d \u00b7 f \u2032 1 \u00b7 p1 \u00b7 p2)\nm1\u2295m2({d2}) = c \u00b7 (e2\u201d \u00b7f2\u201d \u00b7p 2 2+e2\u201d \u00b7f \u2032 3 \u00b7p1 \u00b7p2+e2\u201d \u00b7f3\u201d \u00b7p 2 2+e \u2032 3 \u00b7f2\u201d \u00b7p1 \u00b7p2+e3\u201d \u00b7f2\u201d \u00b7p 2 2)\nm1 \u2295m2({d1, d2}) = c \u00b7 e \u2032 3 \u00b7 f \u2032 3 \u00b7 p 2 1 + e3\u201d \u00b7 f3\u201d \u00b7 p 2 2 + e \u2032 3 \u00b7 f3\u201d \u00b7 p1 \u00b7 p2 + e3\u201d \u00b7 f \u2032 3 \u00b7 p1 \u00b7 p2)\nObviously, Bel12 andBel1\u2295Bel2 are not identical in general. We conclude that conditional independence of experts is also impossible. Hence no usual staatistical indeperndence\nassumption is valid for the rough set interpretation of the MTE. This fact points at where the difference between rough set interpretation and our interpretation lies in: in our interpretation, traditional statistical independence is incorporated into the Dempster\u2019s scheme of combination (labelling process).\nBy the way, lack of correspondence between statistical independence and Dempster rule of combination is characteristic not only for the rough set interpretation, but also of most of the other ones, e.g. [13, 35]. The Reader should read carefully clumsy statements of Shafer about MTE and statistical independence in [29]."}, {"heading": "8 Probability of Provability", "text": "Now we draw attention to the way several authors are writing about \u201dindependence of experts\u2019 opinions\u201d. Smets [35] suggests that independence in MTE should be understood intuitively in terms of experts independence - only intuitively, as was shown in the previous section. Hummel and Landy [13] give still more curious account of expert independence: It is a (totally ignorant) \u201dcommittee\u201d that combines opinions of experts which, it supposes, are independent.\nWe think that, what is really meant behind those clumsy expert opinion independence notions, has been correctly and explicitly stated is the probability of provability approach [22, 23]. The probability of provability approach means the following: we have a deterministic state of affairs. We have experts that make deterministic (categoric) statements about the state of affairs, each statement stemming from a different expert. To derive any theorem about the world we use propositional calculus. However, we know that experts may make errors (independently of one another), that is that their statements may be actually wrong. So each proposition is assigned a probability of being correct. What MTE achieves under this interpretation (if we refrain from normalization) is calculation of probability that the proof of our final statement is correct. First notice, that we do\nnot calculate the probability that the statement itself is correct (this probability will be usually higher). Then imagine what space of events is considered for calculating or guessing the expert\u2019s reliability. We are not concerned with any actual set of observations, but rather with the set of observations together with sets of opinions expressend by each expert on each observation. And we are not building a world model but rather a model of the set of experts. And to reason about a new event we cannot rely on previous knowledge (as we do not derive it), but on experts\u2019 opinions (which we then valuate with experience about experts\u2019 reliability). A statistical evaluation will be rather hard under such circumstances. First of all how an error of an expert could be treated ? Should be a more specific error be considered the same way as an error in a more general statement or not ?. Last not least we lose any statistical information about the real world behaviour under such interpretation."}, {"heading": "9 General Remarks", "text": "The Dempster-Shafer Theory exists already over two decades. Though it was claimed to reflect various aspects of human reasoning, it has not been widely used in expert systems until recently due to the high computational complexity. Three years ago, however, an important paper of Shenoy and Shafer [31] has been published, along with papers of other authors similar in spirit, which meant a break-through for application of both Bayesian and Dempster-Shafer theories in reasoning systems, because it demonstrated that if joint (Bayesian or DS) belief distribution can be decomposed in form of a belief network than it can be both represented in a compact manner and marginalized efficiently by local computations.\nThis fact makes them suitable as alternative fundamentals for representation of (un-\ncertain) knowledge in expert system knowledge bases [12].\nReasoning in Bayesian belief networks has been subject of intense research work also earlier [26], [31], [19], [21]. There exist methods of imposing various logical constraints on the probability density function and of calculating marginals not only of single variables but of complicated logical expressions over elementary statements of the type X =x (x belonging to the domain of the variable X ) [21]. There exist also methods determining the decomposition of a joint probability distribution given by a sample into a Bayesian belief network [3], [24], [1].\nIt is also known that formally probability distributions can be treated as special cases\nof Dempster-Shafer belief distributions (with singleton focal points) [11].\nHowever, for application of DS Belief-Functions for representation of uncertainty in expert system knowledge bases there exist several severe obstacles. The main one is the missing frequentist interpretation of the DS-Belief function and hence neither a comparison of the deduction results with experimental data nor any quantitative nor even qualitative conclusions can be drawn from results of deduction in Dempster-Shafer-theory based expert systems [17].\nNumerous attempts to find a frequentist interpretation have been reported (e.g. [8], [9], [10], [11], [16], [29], [32]). But, as Smets [34] states, they failed either trying to incorporate Dempster rule or when explaining the nature of probability interval approximation. The Dempster-Shafer Theory experienced therefore sharp criticism from several authors in the past [21], [11]. It is suggested in those critical papers that the claim of MTE to represent uncertainty stemming from ignorance is not valid. Hence alternative rules of combination of evidence have been proposed. However, these rules fail to fulfill ShenoyShafer axioms of local computation [31] and hence are not tractable in practice. These failures of those authors meant to us that one shall nonetheless try to find a meaningful frequentist interpretation of MTE compatible with Dempster rule of combination.\nWe have carefully studied several of these approaches and are convinced that the\nkey for many of those failures (beside those mentioned by Halpern in [11]) was: (1) treating the Bel-Pl pair as an interval approximation and (2) viewing combination of evidence as a process of approaching a point estimation. In this paper we claim that the most reasonable treatment of Bel\u2019s Pl\u2019s is to consider them to be POINT ESTIMATES of probability distribution over set-valued attributes (rather then interval estimates of probability distribution over single valued attributes). Of course, we claim also that BelPl estimates by an interval some probability density function but in our interpretation that \u201dintrinsic\u201d probability density function is of little interest for the user. The combination of evidence represents in our interpretation manipulation of data by imposing on them our prejudices (rather then striving for extraction of true values).\nUnder these assumptions a frequentionistically meaningful interpretation of the Bel\u2019s can be constructed (Appendix B), which remains consistent under combination of joint distribution with \u201devidence\u201d, giving concrete quantitative meaning to results of expert system reasoning. Within this interpretation we were able to prove the correctness of Dempster-Shafer rule. This means that this frequentist interpretation is consistent with the DS-Theory to the largest extent ever achieved.\nFinally, we feel obliged to apologize and to say that all critical remarks towards interpretations of MTE elaborated by other authors result from deviations of those interpretations from the formalism of the MTE. We do not consider, however, a deviation from MTE as a crime, because modifications of MTE may and possibly have a greater practical importance than the original theory. The purpose of this paper was to shed a bit more light onto the intrinsic nature of pure MTE and not to call for orthodox attitudes towards MTE."}, {"heading": "Appendix A: Formal Definitions of MTE", "text": "Let \u039e be a finite set of elements called elementary events. Any subset of \u039e be a composite event. \u039e be called also the frame of discernment. A basic probability assignment function m:2\u039e \u2192 [0, 1] such that\n\u2211\nA\u22082\u039e\n|m(A)| = 1\nm(\u2205) = 0\n\u2200A\u22082\u039e 0 \u2264 \u2211\nA\u2286B\nm(B)\n(|.| - absolute value).\nA belief function be defined as Bel:2\u039e \u2192 [0, 1] so that Bel(A) = \u2211\nB\u2286Am(B) A plausibility\nfunction be Pl:2\u039e \u2192 [0, 1] with \u2200A\u22082\u039e Pl(A) = 1 \u2212 Bel(\u039e \u2212 A). A commonnality function be Q:2\u039e \u2192 [0, 1] with \u2200A\u22082\u039e Q(A) = \u2211 A\u2286B m(B)\nFurthermore, a Rule of Combination of two Independent Belief Functions Bel1, Bel2 Over\nthe Same Frame of Discernment (the so-called Dempster-Rule), denoted\nBelE1,E2 = BelE1 \u2295BelE2\nis defined as follows: :\nmE1,E2(A) = c \u00b7 \u2211\nB,C;A=B\u2229C\nmE1(B) \u00b7mE2(C)\n(c - constant normalizing the sum of |m| to 1)\nWhenever m(A) > 0, we say that A is the focal point of the Bel-function. If the only focal point of a belief function is \u039e (m(\u039e) = 1), then Bel is called vacuous belief function (it does not contain any information on whatever value is taken by the variable)."}, {"heading": "Appendix B: Review of the New Interpretation of MTE", "text": "(A summary of the paper [14]). Prob P (x)\nx \u03b1(x) - the probability of \u03b1(x)] being true within\nthe population P. The P (population) is a unary predicate with P(x)=TRUE indicating that the object x(\u2208 \u2126, that is element of a universe of objects) belongs to the population under considerations. If P and P\u2019 are populations such that \u2200xP \u2032(x) \u2192 P (x) (that is membership in P\u2019 implies membership in P, or in other words: P\u2019 is a subpopulation of P), then we distinguish two cases: case 1: (Prob P (x)\nx P \u2032(x)) = 0 (that is probability of membership in P\u2019 with respect to P is equal\n0) - then (according to [18] for any expression \u03b1(x) in free variable x the following holds for the population P\u2019: (Prob P \u2032(x)\nx \u03b1(x)) = 1\ncase 2: (Prob P (x)\nx P \u2032(x)) > 0 then (according to [18] for any expression \u03b1(x) in free variable x\nthe following holds for the population P\u2019:\n( ProbP \u2032(x)\nx \u03b1(x)) =\nProb P (x)\nx (\u03b1(x) \u2227 P \u2032(x))\nProb P (x)\nx P \u2032(x)\nDefinition 1 X be a set-valued attribute taking as its values non-empty subsets of a finite domain \u039e. By a measurement method of value of the attribute X we understand a function:\nM : \u2126\u00d7 2\u039e \u2192 {TRUE,FALSE}\nwhere \u2126 is the set of objects, (or population of objects) such that\n\u2022 \u2200\u03c9;\u03c9\u2208\u2126 M(\u03c9,\u039e) = TRUE (X takes at least one of values from \u039e) \u2022 \u2200\u03c9;\u03c9\u2208\u2126 M(\u03c9, \u2205) = FALSE \u2022 whenever M(\u03c9,A) = TRUE for \u03c9 \u2208 \u2126, A \u2286 \u039e then for any B such that A \u2282 B M(\u03c9,B) = TRUE holds, \u2022 whenever M(\u03c9,A) = TRUE for \u03c9 \u2208 \u2126, A \u2286 \u039e and if card(A) > 1 then there exists B, B \u2282 A such that M(\u03c9,B) = TRUE holds. \u2022 for every \u03c9 and every A either M(\u03c9,A) = TRUE or M(\u03c9,A) = FALSE (but never both).\nM(\u03c9,A) tells us whether or not any of the elements of the set A belong to the actual value of the attribute X for the object \u03c9.\nDefinition 2 A label L of an object \u03c9 \u2208 \u2126 is a subset of the domain \u039e of the attribute X. A labeling under the measurement method M is a function l : \u2126 \u2192 2\u039e such that for any object \u03c9 \u2208 \u2126 either l(\u03c9) = \u2205 or M(\u03c9, l(\u03c9)) = TRUE. Each labelled object (under the labeling l) consists of a pair (\u03c9j, Lj), \u03c9j - the j\nth object,"}, {"heading": "Lj = l(\u03c9j) - its label.", "text": "By a population under the labeling l we understand the predicate P : \u2126 \u2192 {TRUE,FALSE} of the form P (\u03c9) = TRUE iff l(\u03c9) 6= \u2205 (or alternatively, the set of objects for which this predicate is true) If for every object of the population the label is equal to \u039e then we talk of an unlabeled population (under the labeling l), otherwise of a pre-labelled one.\nDefinition 3 Let l be a labeling under the measurement method M . Let us consider the population under this labeling. The modified measurement method\nMl : \u2126\u00d7 2 \u039e \u2192 {TRUE,FALSE}\nwhere \u2126 is the set of objects, is is defined as\nMl(\u03c9,A) = M(\u03c9,A \u2229 l(\u03c9))\n(Notice that Ml(\u03c9,A) = FALSE whenever A \u2229 l(\u03c9) = \u2205.)\nDefinition 4\nBelMP (A) = ProbP (\u03c9)\n\u03c9\n(\u00acM(\u03c9,\u039e\u2212A))\nwhich is the probability that the test M, while being true for A, rejects every hypothesis of the form X=vi for every vi not in A for the population P. We shall call this function \u201dthe belief exactly in the result of measurement\u201d.\nDefinition 5\nPlMP (A) = ProbP (\u03c9)\n\u03c9\n(M(\u03c9,A))\nwhich is the probability of the test M holding for A for the population P. Let us refer to this function as the \u201dPlausibility of taking any value from the set A\u201d.\nDefinition 6\nmMP (A) = ProbP (\u03c9)\n\u03c9 (\n\u2227\nB;B={vi}\u2286A\nM(\u03c9,B) \u2227 \u2227\nB;B={vi}\u2286\u039e\u2212A\n\u00acM(\u03c9,B))\nwhich is the probability that all the tests for the singleton subsets of A are true and those outside of A are false for the population P.\nTHEOREM 1 mMP is the mass Function in the sense of DS-Theory. THEOREM 2 BelMP is a Belief Function in the sense of DS-Theory corresponding to the m M P .\nTHEOREM 3 PlMP is a Plausibility Function in the sense of DS-Theory and it is the Plausibility Function corresponding to the BelMP . Definition 7 Let P be a population and l its labeling. Then\nBel Ml P (A) =\nProbP (\u03c9)\n\u03c9 \u00acMl(\u03c9,\u039e\u2212A)\nPl Ml P (A) =\nProbP (\u03c9)\n\u03c9\nMl(\u03c9,A)\nm Ml P (A) =\nProbP (\u03c9)\n\u03c9 (\n\u2227\nB;B={vi}\u2286A\nMl(\u03c9,B) \u2227 \u2227\nB;B={vi}\u2286\u039e\u2212A\n\u00acMl(\u03c9,B))\nTHEOREM 4 m Ml P is the mass Function in the sense of DS-Theory. THEOREM 5 Bel Ml P is a Belief Function in the sense of DS-Theory corresponding to the m Ml P . THEOREM 6 Pl Ml P is a Plausibility Function in the sense of DS-Theory and it is the Plausibility Function corresponding to the BelMlP . Definition 8 Let M be a measurement method, l be a labeling under this measurement method, and P be a population under this labeling (Note that the population may also be unlabeled). The (simple) labelling process on the population P is defined as a functional LP : 2\u039e\u00d7\u0393 \u2192 \u0393, where \u0393 is the set of all possible labelings under M , such that for the given labeling l and a given nonempty set of attribute values L (L \u2286 \u039e), it delivers a new labeling l\u2032 (l\u2032 = LP(L, l)) such that for every object \u03c9 \u2208 \u2126:\n1. if Ml(\u03c9,L) = FALSE then l \u2032(\u03c9) = \u2205\n(that is l\u2019 discards a labeled object (\u03c9, l(\u03c9)) if Ml(\u03c9,L) = FALSE 2. otherwise l\u2032(\u03c9) = l(\u03c9) \u2229 L (that is l\u2019 labels the object with l(\u03c9) \u2229 L otherwise.\nDefinition 9 \u201dlabelling process function\u201d mLP;L : 2\u039e \u2192 [0, 1]: is defined as:\nmLP;L(L) = 1\n\u2200B;B\u22082\u039e,B 6=Lm LP;L(B) = 0\nTHEOREM 7 mLP;L is a Mass Function in sense of DS-Theory.\nTHEOREM 8 Let M be a measurement function, l a labeling, P a population under this labeling. Let L be a subset of \u039e. Let LP be a labeling process and let l\u2032 = LP(L, l). Let P\u2019 be a population under the labeling l\u2032. Then Bel M l\u2032\nP \u2032 is a combination via DS Combination rule of\nBelMl, and BelLP;L., that is:\nBel M l\u2032 P \u2032 = Bel Ml P \u2295Bel LP;L\nDefinition 10 Let M be a measurement method, l be a labeling under this measurement method, and P be a population under this labeling (Note that the population may also be unlabeled). Let us take a set of (not necessarily disjoint) nonempty sets of attribute values {L1, L2, ..., Lk} and let us define the probability of selection as a function mLP,L 1,L2,...,Lk : 2\u039e \u2192 [0, 1] such that\n\u2211\nA;A\u2286\u039e\nmLP,L 1,L2,...,Lk(A) = 1\n\u2200A;A\u2208{L1,L2,...,Lk}m LP,L1,L2,...,Lk(A) > 0 \u2200A;A 6\u2208{L1,L2,...,Lk}m LP,L1,L2,...,Lk(A) = 0\nThe (general) labelling process on the population P is defined as a (randomized) functional LP : 22 \u039e \u00d7 \u2206 \u00d7 \u0393 \u2192 \u0393, where \u0393 is the set of all possible labelings under M , and \u2206 is a set of all possible probability of selection functions, such that for the given labeling l and a given set of (not necessarily disjoint) nonempty sets of attribute values {L1, L2, ..., Lk} and a given probability of selection mLP,L 1,L2,...,Lk it delivers a new labeling l\u201d such that for every object \u03c9 \u2208 \u2126: 1. a label L, element of the set {L1, L2, ..., Lk} is sampled randomly according to the probability distribution mLP,L 1,L2,...,Lk; This sampling is done independently for each individual object,\n2. if Ml(\u03c9,L) = FALSE then l\u201d(\u03c9) = \u2205 (that is l\u201d discards an object (\u03c9, l(\u03c9)) if Ml(\u03c9,L) = FALSE\n3. otherwise l\u201d(\u03c9) = l(\u03c9) \u2229 L (that is l\u201d labels the object with l(\u03c9) \u2229 L otherwise.)\nTHEOREM 9 mLP,L 1,...,Lk is a Mass Function in sense of DS-Theory.\nTHEOREM 10 Let M be a measurement function, l a labeling, P a population under this labeling. Let LP be a generalized labeling process and let l\u201d be the result of application of the LP for the set of labels from the set {L1, L2, ..., Lk} sampled randomly according to the probability distribution mLP,L 1,L2,...,Lk;. Let P\u201d be a population under the labeling l\u201d. Then The expected value over the set of all possible resultant labelings l\u201d (and hence populations P\u201d) (or, more precisely, value vector) of BelMl\u201dP\u201d is a combination via DS Combination rule of Bel Ml P , and BelLP,L 1,...,Lk., that is:\nE(Bel M \u2032 l\nP\u201d ) = Bel Ml P \u2295Bel\nLP,L1,...,Lk"}], "references": [{"title": "Learning with CASTLE, Symbolic and Quantitative Approaches", "author": ["S. Acid", "L.M. deCampos", "A. Gonzales", "B. Molina", "N. Perez de la Blanca"], "venue": "MIECZYS", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1991}, {"title": "L.p., a logic for representing and reasoning with statistical knowledge", "author": ["F. Bacchus"], "venue": "Computer Intelligence", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C.K. Chow", "C.N. Liu"], "venue": "IEEE Transactions on Information Theory , Vol. IT-14,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1968}, {"title": "Foresight: its logical laws, its subjective sources, 1937; translated and reprinted in Studies in Subjective Probability (H.E.Kyburg and H.E.Smokler", "author": ["B. deFinetti"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1964}, {"title": "Upper and lower probabilities induced by a multi-valued mapping", "author": ["A.P. Dempster"], "venue": "Ann. Math. Stat", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1967}, {"title": "A generalization of Bayesian inference", "author": ["A.P. Dempster"], "venue": "J. R. Stat. Soc. Ser.B", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1968}, {"title": "Halpern: Uncertainty, belief, and probability", "author": ["J.Y.R. Fagin"], "venue": "Proc. Int. Joint Conf. AI, IJCAI89,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Halpern: Uncertainty, belief, and probability", "author": ["J.Y.R. Fagin"], "venue": "Comput. Intell", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Halpern: A new approach to updating beliefs", "author": ["J.Y.R. Fagin"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "Two views of belief: belief as generalized probability and belief as evidence,Artificial", "author": ["J.Y. Halpern", "R. Fagin"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "An introduction to algorithm for inference in belief nets, in: Henrion M., Shachter R.D.,Kanal L.N", "author": ["M. Henrion"], "venue": "Lemmer J.F.: Uncertainty in Artificial Intelligence", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1990}, {"title": "lopotek: Reasoning from Data in the Mathematical Theory of Evidence, - to appear in Proc", "author": ["K M.A"], "venue": "Eighth International Symposium On Methodologies For Intelligent Systems (IS- MIS\u201994),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1994}, {"title": "Bayesian and non-Bayesian evidential updating", "author": ["H.E. Kyburg Jr."], "venue": "Artificial Intelligence", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1987}, {"title": "Induction of uncertain rules and the sociopathicity property in Dempster-Shafer theory, in Symbolic and Quantitative Approaches", "author": ["Y. Ma", "D.C. Wilkins"], "venue": "Lecture Notes In Computer Science", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1991}, {"title": "On random sets and belief functions", "author": ["H.T. Nguyen"], "venue": "J. Math. Anal. Appl", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1978}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Influence", "author": ["J. Pearl"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1988}, {"title": "Pearl: The recovery of causal poly-trees from statistical data, in Uncertainty in Artificial Intelligence", "author": ["J.G. Rebane"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1989}, {"title": "The logical foundation of evidential reasoning, Tech", "author": ["E.H. Ruspini"], "venue": "Note 408,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1986}, {"title": "Evidence absorption and propagation through evidence reversals", "author": ["R.D. Shachter"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1990}, {"title": "A Mathematical Theory of Evidence", "author": ["G. Shafer"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1976}, {"title": "Belief functions: An introduction", "author": ["G. Shafer"], "venue": "J. Pearl eds: Readings in Uncertain Reasoning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1990}, {"title": "Axioms for probability and belief-function propagation", "author": ["P.P. Shenoy", "G. Shafer"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1990}, {"title": "Boolean reasoning for decision rules generation, in: J. Komorowski, Z.W.Ra\u015b (Eds): Methodologies for Intelligent Systems, Lecture", "author": ["A. Skowron"], "venue": "Notes in Artificial Intelligence", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1993}, {"title": "Boolean reasoning for decision rules generation, a talk at the Intelligent Information Systems Workshop, August\u00f3w", "author": ["A. Skowron"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1993}, {"title": "Resolving misunderstandings about belief functions, International Journal of Approximate Reasoning 1992:6:321-344", "author": ["Ph. Smets"], "venue": "MIECZYS  lAW A. K lOPOTEK AND ANDRZEJ MATUSZEWSKI", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1992}, {"title": "Kennes: The tranferable belief model", "author": ["R. Ph. Smets"], "venue": "Artificial Intelligence", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1994}], "referenceMentions": [{"referenceID": 19, "context": "This paper is concerned with the apparent greatest weakness of the Mathematical Theory of Evidence (MTE) of Shafer [27], which has been strongly criticized by Wasserman [37] - the relationship to frequencies.", "startOffset": 115, "endOffset": 119}, {"referenceID": 3, "context": "371) reminding \u201dmajor success story in Bayesian theory\u201d, the exchangeability theory of de Finetti [4].", "startOffset": 98, "endOffset": 101}, {"referenceID": 20, "context": "Shafer in [28], [29] gave the following formal probabilistic interpretation of belief function: Let Pr be a probabilistic measure over the infinite discrete sample space \u03a9, let \u0393 be a function \u0393 : \u03a9 \u2192 2.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "But, let us remind, Shafer claimed [28, 29] that function \u0393 indicates that the variable takes for object \u03c9 one of the values \u0393(\u03c9).", "startOffset": 35, "endOffset": 43}, {"referenceID": 19, "context": "As statistical properties of Shafer\u2019s [27] notion of evidence are concerned, further", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "criticism has been expressed by Halpern and Fagin ([11] in sections 4-5).", "startOffset": 51, "endOffset": 55}, {"referenceID": 9, "context": "The other point raised there that though it is possible to capture properly in belief functions evidence in terms of probability of observations update functions (section 4 of [11]), it is not possible to do the same if we would like to capture evidence in terms of beliefs of observations update functions (section 5 of [11]).", "startOffset": 176, "endOffset": 180}, {"referenceID": 9, "context": "The other point raised there that though it is possible to capture properly in belief functions evidence in terms of probability of observations update functions (section 4 of [11]), it is not possible to do the same if we would like to capture evidence in terms of beliefs of observations update functions (section 5 of [11]).", "startOffset": 321, "endOffset": 325}, {"referenceID": 24, "context": "Smets [34] has made some strong statements in defense of the Dempster-Shafer theory against sharp criticism of this theory by its opponents as well as unfortunate users of the MTE who wanted to attach it to the \u201ddirty reality\u201d (that is objectively given databases).", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": ") considering the domain of MTE applications as the one where \u201dwe are ignorant of the existence of probabilities\u201d, and not one with \u201dpoorly known probabilities\u201d ([34], p.", "startOffset": 162, "endOffset": 166}, {"referenceID": 9, "context": "Compare also Discussion in [11] on the subject.", "startOffset": 27, "endOffset": 31}, {"referenceID": 24, "context": "As Smets probabilistic interpretations are concerned, let us \u201dcontinue\u201d the killer example of [34] on pages 330-331.", "startOffset": 94, "endOffset": 98}, {"referenceID": 24, "context": "We make use of the Smets\u2019 claim that \u201dthe de re and de dicto interpretations lead to the same results\u201d ([34], p.", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "Smets [34] views the probability theory as a formal mathematical apparatus and hence puts it on the same footing as his view of the MTE.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "It is worth mentioning that Smets made recently an attempt to justify usage of belief functions instead of Bayesian probabilities, to identify situations in which usage of belief functions is more reasonable than usage of probabilities [36].", "startOffset": 236, "endOffset": 240}, {"referenceID": 14, "context": "The canonic random set interpretation [20] is one with a statistical process over set instantiations.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "For example, to imitate the logical inference it is frequently assumed that we have a Bel-function describing the actual observed value of a predicate P(x), and a Bel-function describing the implication \u201dIf P(x) then Q(x)\u201d [17].", "startOffset": 223, "endOffset": 227}, {"referenceID": 4, "context": "Still another approach was to handle Bel and Pl as lower and upper probabilities [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 12, "context": "This approach is of limited use as not every set of lower and upper probabilities leads to Bel/Pl functions [16], hence establishing a unidirectional relationship between probability theory and the DS-theory.", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "ability function realization [16], [9].", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "ability function realization [16], [9].", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "Still another approach was to handle Bels/Pl in probabilistic structures rather than in probabilistic spaces [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 9, "context": "So it was tried to drop the DS-rule altogether in the probabilistic structures, but then it was not possible to find a meaningful rule for multistage reasoning [11].", "startOffset": 160, "endOffset": 164}, {"referenceID": 22, "context": "An interesting alternative interpretation of the Dempster-Shafer Theory was found within the framework of the rough set theory [32], [10].", "startOffset": 127, "endOffset": 131}, {"referenceID": 22, "context": "for details see texts of Skowron [32] and Grzyma la-Busse [10]).", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "The Dempster Rule of combination is interpreted by Skowron [33] as combination of opinions of independent experts, who possibly look at different sets of explanation attributes and hence may propose different explanations.", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "Three years ago, however, an important paper of Shenoy and Shafer [31] has been published, along with papers of other authors similar in spirit, which meant a break-through for application of both Bayesian and Dempster-Shafer theories in reasoning systems, because it demonstrated that if joint (Bayesian or DS) belief distribution can be decomposed in form of a belief network than it can be both represented in a compact manner and marginalized efficiently by local computations.", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "This fact makes them suitable as alternative fundamentals for representation of (uncertain) knowledge in expert system knowledge bases [12].", "startOffset": 135, "endOffset": 139}, {"referenceID": 18, "context": "Reasoning in Bayesian belief networks has been subject of intense research work also earlier [26], [31], [19], [21].", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "Reasoning in Bayesian belief networks has been subject of intense research work also earlier [26], [31], [19], [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 15, "context": "Reasoning in Bayesian belief networks has been subject of intense research work also earlier [26], [31], [19], [21].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "There exist methods of imposing various logical constraints on the probability density function and of calculating marginals not only of single variables but of complicated logical expressions over elementary statements of the type X =x (x belonging to the domain of the variable X ) [21].", "startOffset": 284, "endOffset": 288}, {"referenceID": 2, "context": "There exist also methods determining the decomposition of a joint probability distribution given by a sample into a Bayesian belief network [3], [24], [1].", "startOffset": 140, "endOffset": 143}, {"referenceID": 16, "context": "There exist also methods determining the decomposition of a joint probability distribution given by a sample into a Bayesian belief network [3], [24], [1].", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "There exist also methods determining the decomposition of a joint probability distribution given by a sample into a Bayesian belief network [3], [24], [1].", "startOffset": 151, "endOffset": 154}, {"referenceID": 9, "context": "It is also known that formally probability distributions can be treated as special cases of Dempster-Shafer belief distributions (with singleton focal points) [11].", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "The main one is the missing frequentist interpretation of the DS-Belief function and hence neither a comparison of the deduction results with experimental data nor any quantitative nor even qualitative conclusions can be drawn from results of deduction in Dempster-Shafer-theory based expert systems [17].", "startOffset": 300, "endOffset": 304}, {"referenceID": 7, "context": "[8], [9], [10], [11], [16], [29], [32]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[8], [9], [10], [11], [16], [29], [32]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 9, "context": "[8], [9], [10], [11], [16], [29], [32]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "[8], [9], [10], [11], [16], [29], [32]).", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": "[8], [9], [10], [11], [16], [29], [32]).", "startOffset": 34, "endOffset": 38}, {"referenceID": 24, "context": "But, as Smets [34] states, they failed either trying to incorporate Dempster rule or when explaining the nature of probability interval approximation.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "The Dempster-Shafer Theory experienced therefore sharp criticism from several authors in the past [21], [11].", "startOffset": 98, "endOffset": 102}, {"referenceID": 9, "context": "The Dempster-Shafer Theory experienced therefore sharp criticism from several authors in the past [21], [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "However, these rules fail to fulfill ShenoyShafer axioms of local computation [31] and hence are not tractable in practice.", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "key for many of those failures (beside those mentioned by Halpern in [11]) was: (1) treating the Bel-Pl pair as an interval approximation and (2) viewing combination of evidence as a process of approaching a point estimation.", "startOffset": 69, "endOffset": 73}], "year": 2017, "abstractText": "STRESZCZENIE This paper is concerned with the apparent greatest weakness of the Mathematical Theory of Evidence (MTE) of Shafer [27], which has been strongly criticized by Wasserman [37] the relationship to frequencies. Weaknesses of various proposals of probabilistic interpretation of MTE belief functions are demonstrated. A new frequency-based interpretation is presented overcoming various drawbacks of earlier interpretations.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}