{"id": "1706.03235", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "ACCNet: Actor-Coordinator-Critic Net for \"Learning-to-Communicate\" with Deep Multi-agent Reinforcement Learning", "abstract": "concurrency is a critical factor for getting big multi - agent world to are organized and productive. typically, most previous multi - agent \" learning - to - communicate \" studies try to predefine the cognitive protocols or use limitations such as tabular reinforcement learning for evolutionary algorithm, which can not generalize to changing environment or large collection of agents.", "histories": [["v1", "Sat, 10 Jun 2017 13:50:23 GMT  (1157kb,D)", "https://arxiv.org/abs/1706.03235v1", "Actor-Critic Method for Multi-agent Learning-to-Communicate based on Deep Reinforcement Learning, It is suitable for both continuous and discrete action space environments"], ["v2", "Tue, 13 Jun 2017 02:00:14 GMT  (1158kb,D)", "http://arxiv.org/abs/1706.03235v2", "Version-2 of original submission. Actor-Critic Method for Multi-agent Learning-to-Communicate based on Deep Reinforcement Learning, It is suitable for both continuous and discrete action space environments"], ["v3", "Sun, 29 Oct 2017 05:09:39 GMT  (2089kb,D)", "http://arxiv.org/abs/1706.03235v3", "V3 of original submission. Actor-Critic Method for Multi-agent Learning-to-Communicate based on Deep Reinforcement Learning, It is suitable for both continuous and discrete action space environments"]], "COMMENTS": "Actor-Critic Method for Multi-agent Learning-to-Communicate based on Deep Reinforcement Learning, It is suitable for both continuous and discrete action space environments", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["hangyu mao", "zhibo gong", "yan ni", "xiangyu liu", "quanbin wang", "weichen ke", "chao ma", "yiping song", "zhen xiao"], "accepted": false, "id": "1706.03235"}, "pdf": {"name": "1706.03235.pdf", "metadata": {"source": "CRF", "title": "ACCNet: Actor-Coordinator-Critic Net for \u201cLearning-to-Communicate\u201d with Deep Multi-agent Reinforcement Learning", "authors": ["Hangyu Mao", "Zhibo Gong", "Yan Ni", "Zhen Xiao"], "emails": [], "sections": [{"heading": null, "text": "Communication is a critical factor for the big multi-agent world to stay organized and productive. Typically, most previous multi-agent \u201clearning-to-communicate\u201d studies try to predefine the communication protocols or use technologies such as tabular reinforcement learning and evolutionary algorithm, which cannot generalize to the changing environment or large collection of agents directly.\nIn this paper, we propose an Actor-Coordinator-Critic Net (ACCNet) framework for solving multi-agent \u201clearning-to-communicate\u201d problem. The ACCNet naturally combines the powerful actor-critic reinforcement learning technology with deep learning technology. It can learn the communication protocols even from scratch under partially observable environments. We demonstrate that the ACCNet can achieve better results than several baselines under both continuous and discrete action space environments. We also analyse the learned protocols and discuss some design considerations."}, {"heading": "Introduction", "text": "Communication is an important factor for the big multi-agent world to stay organized and productive. For applications where individual agent has limited capability, it is particularly critical for multiple agents to learn communication protocols to work in a collaborative way, for example: data routing [1], congestion detection [2] and air traffic management [3].\nHowever, most previous multi-agent \u201clearning-to-communicate\u201d studies try to predefine the communication protocols or use technologies such as tabular reinforcement\n\u2217These authors contribute equally to this study.\nar X\niv :1\n70 6.\n03 23\n5v 3\n[ cs\n.A I]\nlearning (RL) and evolutionary algorithm, which cannot generalize to the changing environment or large collection of agents directly. We argue that this field requires more in-depth studies with new technologies.\nRecently, we researchers have seen the success of Deep MARL, i.e., the combination of deep learning (DL) and multi-agent reinforcement learning (MARL), in many applications, such as self-play Go [5], two-player Pong [6] and multi-player StarCraft [7]. However, those work either assume full observability of the environment or lack communication among multiple agents.\nNaturally, in this paper, we ask and try to answer a question: can we learn multiagent communication protocols even from scratch under partially observable distributed environments with the help of Deep MARL?\nWe consider the setting where multiple distributed agents are fully cooperative with the same goal to maximize the shared discounted sum of rewards R in a partially observable environment. Full cooperation means that all agents receive the same R independent of their contributions. Partially observable environments mean that no agent can observe the underlying Markov states and they must learn effective communication protocols. In fact, the problem setting can be exactly modelled as Dec-POMDP-Com [8, 9], which is an extension of Dec-POMDP [10, 11] when considering communication. The novelty is that the communication bandwidth is limited.\nThe limited communication bandwidth is a common setting for recent \u201clearningto-communicate\u201d studies [20, 12, 23, 4]. Traditional cooperative agents can share sensations, learned policies or even training episodes [19, 20], which is not suitable for real-world applications because communication itself takes up much bandwidth. In our opinion, limited communication bandwidth has two meanings. On the one hand, the message at a specific timestep can be transported using a few packets so that it will not take too much bandwidth. On the other hand, only valuable message is necessary to further reduce the bandwidth requirement. That is to say, message only comes from time to time, and the intermittent time is task-specific. To achieve the former limited bandwidth, we suggest to use deep neural networks to compress the message so that both the message dimension and the packets needed for transporting the message can be controlled. And for the latter, we will introduce corresponding methods based on Gating mechanism and Token mechanism in another paper because of the space limitation.\nTo this end, we propose an Actor-Coordinator-Critic Net (ACCNet) framework, which combines the powerful actor-critic RL technology with DL technology. The ACCNet has two paradigms. The first one is AC-CNet, which learns the communication protocols among actors with the help of coordinator and keeps critics being independent. However, the actors of AC-CNet inevitably need communication even during execution, which is impractical under some special situations [12]. The second one is A-CCNet, which learns the communication protocols among critics with the help of coordinator and keeps actors being independent. As actors are independent, they can cooperate with each other even without communication after A-CCNet is trained well. Note that, actor and critic are not two different agents but two services in one agent.\nWe explore the proposed ACCNet under different partially observable environments. Experiments show that: (1) both AC-CNet and A-CCNet can achieve good results for simple multi-agent environments; (2) for complex environments, A-CCNet\nhas a better generalization ability and performs almost like the ideal fully observable models. To the best of our knowledge, this is the first work to investigate multi-agent \u201clearning-to-communicate\u201d problem based on deep actor-critic RL architecture under partially observable environment1.\nThe rest of this paper starts from a brief review of actor-critic RL algorithms and the releted work. We then present the ACCNet, followed by experiments and conclusion."}, {"heading": "Background", "text": "Reinforcement learning (RL) [13] is a machine learning approach to solve sequential decision making problem. At each timestep t, the agent observes a state st and takes an action at, and then receives a feedback reward rt from the environment and observes a new state st+1. The goal of RL is to learn a policy \u03c0(a|s), i.e., a mapping from state to action, which can maximize the expected discount cumulative future reward E[R] = E[ \u2211T t=0 \u03b3\ntrt]. Model-free RL algorithms can be divided into three groups [14, 15]. (1) Actor-only methods directly learn the parameterized policy \u03c0(a|s; \u03b8). They can generate continuous action but suffer from high variance in the estimation of policy gradient. (2) Criticonly methods use low variance temporal difference learning to estimate the Q-value Q(s, a;w) = E[R; s, a]. The policy can be derived using greedy action selection, i.e., \u03c0(a|s) = a\u2217 = argmaxaQ(s, a;w). They are usually used for discrete action as finding a\u2217 is computationally intensive in continuous action space. (3) Actor-critic methods jointly learn \u03c0(a|s; \u03b8) and Q(s, a;w). They preserve the advantages of both actor-only and critic-only methods.\nThe schematic structure of actor-critic methods is shown in Figure 1. Two functions reinforce each other: correct actor \u03c0(a|s; \u03b8) gives high rewarding trajectory (s, a, r, s\u2032), which updates critic V (s;w) or Q(s, a;w) towards the right direction; correct critic V (s;w) or Q(s, a;w) picks out the good action for actor \u03c0(a|s; \u03b8) to reinforce. This mutual reinforcement behavior helps actor-critic methods avoid bad local minima and\n1One similar work [33] from OpenAI is released at the same time. Another concurrent work [34] from Oxford also uses a similar idea. They do not explicitly address the \u201clearning-to-communicate\u201d problem, but we affirm each other\u2019s methods and results mutually. A comparison between ACCNet and all those related studies are shown in Table 1.\nconverge faster, in particular for on-policy methods that follow the very recent policy to sample trajectory during training [7]. Specifically, if actor uses stochastic policy for action selection, the actor and critic are updated based on the following TD-error and Stochastic Policy Gradient Theorem [13]:\n\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2)\nIf actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]:\n\u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4)\nAs ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].\nDeep RL (DRL) uses deep neural networks to approximate \u03c0(a|s; \u03b8), Q(s, a;w) and/or the environment."}, {"heading": "Related Work", "text": "How to learn communication protocols efficiently is critical to the success of multiagent systems. Most previous work predefine the communication protocols [19, 20] and some others use technologies such as tabular RL [21] or evolutionary algorithm [22], which cannot generalize to the changing environment and large collection of agents directly as [13, 4] point out.\nRecently, the end-to-end differentiable communication channel embedded in deep neural network has been proven useful for learning communication protocols. Generally, the protocols can be optimized simultaneously while the network is optimized. Our work is an instance of this method, and the most relevant studies include the CommNet [23], DIAL [4] and BiCNet [7].\nCommNet is a single network designed for all agents. The input is the concatenation of current states from all agents. The communication channels are embedded between network layers. Each agent sends its hidden state as communication message to the current layer channel. The averaged message from other agents then is sent to the next layer of a specific agent. However, single network with a communication channel at each layer is not easy to scale up.\nDIAL trains a single network for each individual agent. At each timestep, the agent outputs its message as the input of other agents for the next timestep. To learn the communication protocols, it also pushes gradients from one agent to another through the communication channels. However, the message is delayed for one timestep and the environment will be non-stationary in multi-agent situation.\nBoth CommNet and DIAL are based on DQN [24] for discrete action. BiCNet is based on actor-critic methods for continuous action. It uses bi-directional recurrent neural networks as the communication channels. This approach allows single agent to maintain its own internal state and share information with other collaborators at the\nsame time. However, it assumes that agents can know the global Markov states of the environment, which is no so realistic except for some game environments.\nOther relevant excellent studies include but not limited to [37, 38, 39]. Those researchers have verified the possibility of learning communication protocols among agents. Nevertheless, we aim at providing a general framework to ease the learning of communication protocols among agents."}, {"heading": "Actor-Coordinator-Critic Net Framework", "text": "In this section, we present two paradigms of ACCNet framework for learning communication protocols based on actor-critic models."}, {"heading": "AC-CNet", "text": "The most straightforward approach is to build a communication channel between actors and keep critics being independent. As shown in Figure 2(b), a coordinator communication channel is used for coordinating the actors to generate coordinated actions, so we call this paradigm AC-CNet. Specifically, each agent encodes its local state into a local message and sends it to the coordinator, which further generates the global communication signal for this agent considering messages from all other agents. As the global signal is an encoding of all local messages, we expect that it can catch the global information of the system. The integrated state is the concatenation of local state and global signal, which will be fed as input into the actor-critic model. Then the whole AC-CNet is trained as the original actor-critic model.\nHowever, the AC-CNet inevitably needs communication between actor and coordinator to get the global information even during execution, which is impractical under some special situations [12, 45]."}, {"heading": "A-CCNet", "text": "Can those agents generate actions as if they have shared the global knowledge even without communication during training? What about during execution? The answer may be NO at the first glance and we also think so. Fortunately, machine learning has a fascinating property that we can do prediction after one model is trained and the auxiliary data on which the model is trained need no longer to be kept. We ask ourselves that can we move the communication among actors into critics so that actors can independently take actions according to their specific states during execution and the auxiliary critics during training need no longer to be kept. In fact, it is possible for actor-critic methods. However, both actor-only and critic-only methods are unsuitable for this task because the training and execution mechanisms of these methods are exactly the same.\nAs shown in Figure 2(c), a coordinator communication channel is used for coordinating the critics to generate better estimated Q-values, so we call this paradigm A-CCNet. Specifically, the actor in A-CCNet is the same as the actor in the original actor-critic model shown in Figure 2(a), but the critics should communicate with each other through coordinator before they can generate the estimated Q-values. Compared to AC-CNet where communication occurs among actors and the communication signal can only encode local state, A-CCNet put communication among critics where both state and action can be encoded into the communication signal. So we expect that ACCNet can generate better policies, which has been confirmed by the experiments.\nBesides, there are two designs for the critic. Critic1 uses the global signal to generate Q-values directly, while critic2 combines global signal and local message to generate Q-values. For both of the two designs, actors can generate their actions independently without communication during execution."}, {"heading": "Formal Formulation of ACCNet", "text": "For AC-CNet, as critics are independent, we can update each agent based on Equation (1-4) just like updating single actor-critic agent. One key difference is that we need to push the gradients of actors into the coordinator communication channels so that the communication protocols can also be optimized simultaneously.\nFor A-CCNet, as critics communicate with each other, the critic network of the i-th agent is now V i(si, sg;wi) or Qi(si, ai, sg;wi), where sg=f(s1, ..., sN , a1, ..., aN ) is the global communication signal2. We can then extend Equation (1-4) into multi-agent\n2Generally speaking, f is a injective function. Besides, using V i(si, sg ;wi) and Qi(si, ai, sg ;wi) for discrete and continuous action separately is natural. However, for discrete action, sg is a function of only the states (s1, ..., sN ); without knowing the actions (a1, ..., aN ), Equation (11) can no longer be true.\nformulations:\n\u03b4it = rt + \u03b3V i(sit+1, s g t+1;w i)\u2212 V i(sit, s g t ;w i) (5) \u03b8it+1 = \u03b8 i t + \u03b1 \u2217 \u03b4it \u2217 5\u03b8i log\u03c0i(ait|sit; \u03b8i) (6)\nyit = rt + \u03b3Q i(sit+1, a i t+1, s g t+1;w i) (7) \u03b4it = y i t \u2212Qi(sit, ait, s g t ;w\ni) (8) 5\u03b8it = 5aiQ i(sit, a i t, s g t ;w i) \u2217 5\u03b8i\u03c0i(ait|sit; \u03b8i) (9) \u03b8it+1 = \u03b8 i t + \u03b1 \u2217 5\u03b8it (10)\nOur primary insight about ACCNet (especially A-CCNet) is that once each agent knows the states and actions from other agents, the environment could be treated stationary regardless of the changing policies. More formally, Equation (11) always keeps true for any agent indexed by i with any changing policies \u03c0i 6= \u03c0\u2032i [33]:\nP (sit+1|sit;Env) =P (sit+1|sit; s g t , \u03c0 1, ..., \u03c0N )\n=P (sit+1|sit; s g t )\n=P (sit+1|sit; s g t , \u03c0\n\u20321, ..., \u03c0 \u2032N )\n(11)"}, {"heading": "Some Comparisons", "text": "Before the comparison, we first introduce the two concurrent studies mentioned in Footnote 1, i.e., COMA [34] from Oxford and MADDPG [33] from OpenAI.\nCOMA, MADDPG and A-CCNet share a similar idea: accelerating training with the help of critics and executing in real environment only based on actors. But the research purposes are different. COMA aims at solving the credit assignment problem in multi-agent cooperative environments. MADDPG wants to investigate both cooperation and competition among agents. The proposed ACCNet tries to provide a general framework to ease the learning of communication protocols among agents even from scratch. Specifically, COMA is based on Stochastic Policy Gradient Theorem [13] and REINFORCE [35] algorithm. It uses a counterfactual baseline and a centralised critic to address multi-agent credit assignment problem. However, they only do experiments for discrete action space environments and assume that the critic can get the entire game screen. MADDPG extends DDPG [16, 18] into multi-agent environments. The authors verify that this method is suitable for both cooperative and competitive tasks. However, their experiments are limited to continuous action space environments.\nAs COMA and MADDPG do not address the \u201clearning-to-communicate\u201d problem explicitly, both of them use the states and actions of all other agents directly, without considering the communication cost. Nevertheless, we affirm each other\u2019s methods and results mutually.\nNow, we are ready to give a brief comparison as shown in Table 1. As we can see, ACCNet has a better adaptability for different situations."}, {"heading": "Experiments", "text": "In this section, we test the proposed ACCNet under both continuous and discrete action space environments. Those environments are partial observable with multiple distributed and fully cooperative agents."}, {"heading": "Continuous Action Space Environment", "text": "Problem Definition. For continuous action space environment, we focus on the Network Routing Domain problem modified from [25]. Currently, the Internet is made up of many ISP networks. In each ISP network, as shown in Figure 3, there are several edge routers. Two edge routers are combined as ingress-egress router pair (IE-pair). The i-th IE-pair has a input flow demand Fi and K available paths that can be used to deliver the flow from ingress-router to egress-router. Each path P ki is made up of several links and each link can belong to several paths. The l-th link Ll has a flow transmission capacity Cl and a link utilization ratio Ul. As we know, high link utilization ratio is bad for dealing with burst traffic, so we want to find a good traffic splitting policy jointly for all IE-pairs across their available paths to minimize the maximum link utilization ratio in the network.\nSetting. We design the following RL elements. State. Current traffic demand and static network topology information are available. We also encode the estimated link utilization ratio into the state. Specifically, the local state is s = [Fi, U li ,max(0, 1\u2212 U li ),max(0, U li \u2212 1)].\nAction. The ingress-router should generate a splitting ratio yki with a constraint\u2211 k y k i = 1 for current traffic demand Fi. So the softmax activation is chosen as the final layer of actor network. This design is natural for the continuous action with sumto-one constraint.\nReward. As we want to minimize the maximum link utilization ratio, we set the reward signal to r = 1\u2212max(Ul).\nBaselines. As with CommNet and BiCNet, we also use the following baselines. Independent controller (IND): each agent learns its own actor-critic network without any communication. Fully-connected controller (FC): all agents are controlled by a big fully-connected actor-critic network to learn the traffic splitting policy. The communication channel is embedded in the network without any bandwidth limitation.\nIND model is the worst situation and FC model can be seen as the ideal situation. Besides, as mentioned before, we design two kinds of critics for A-CCNet: all critics share the same Q(s,a), or each critic separately learns its own Q(s,a). So we have the following models: IND, FC-sep, FC-sha, AC-CNet, A-CCNet-sep and A-CCNet-sha.\nExperiment Results. In this environment, we care about convergence ratio (CR) of all independent experiments and maximum link utilization ratio (MLUi) of the i-th bottleneck link after convergence. All results are shown in Table 2 and 3. Due to space limitation, we put the results of ThreeIE in the supplementary material.\nAs we can see, all models have high CR and low MLUi for simple TwoIE topology. But A-CCNet has a better performance than AC-CNet and IND. It even has a similar performance with the ideal fully observable FC model. For complex FiveIE topology, the performances of AC-CNet and IND drop severely, while A-CCNet can still keep its ability of performing almost like the ideal FC model. The reason may be that A-CCNet has more global information than other models (except for FC model): A-CCNet put communication among critics where both local state and action can be encoded into the communication signal while the communication signal of AC-CNet can only encode local state and IND does not exchange information at all. In this case, more information means that the environment could be seen stationary as illustrated by Equation (11).\nCommunication Message Analysis. We show the state-message-action changing of one convergent experiment in Figure 5. As the value of state become large (for\nexample, more packets should be transmitted), agent1 will emit large message value while agent2 usually emits small message value. For action value, if agent1 splits more traffic to L1, agent2 will split more traffic to L2 because L2 is now underused. Besides, agent1 has a wider range of state value, so the message value and action value generated by agent1 are also wider than agent2. Those sophisticated and coordinated behaviors are critical for MARL systems to stay organized."}, {"heading": "Discrete Action Space Environment", "text": "Problem Definition. We consider the Traffic Junction problem modified from [28, 23]. As shown in Figure 6, four cars are deriving on the 4-way junction road. New car will be generated if one car reaches its destination at the edge of the grid. The simulation will be classified as a failure if location overlaps have occurred in 40 timesteps. Our target is to learn a car driving policy so that we can get low failure rate (FR).\nSetting. We use the same RL elements as in CommNet. State. All cars can only know its location and driving direction. They cannot see other cars. So we represent the local state as a one-hot vector set {location, direction}. Action. A car has two possible actions: gassing itself by one cell on its route or braking to stay at its current location. Reward. A collision incurs a reward rcoll=-10.0, and each car gets reward of r\u03c4time=0.01\u03c4 at each timestep to discourage a traffic jam, where \u03c4 is the total timesteps since the car arrived. So the total reward at time t is: r(t) = Ctrcoll+ \u2211Nt i=1 r \u03c4i time, where C t is the number of location overlaps at time t, and N t is the number of cars. This setting is the same as CommNet.\nExperiment Results. Table 4 shows the results of this task. After training the models 300 episodes as CommNet, the proposed A-CCNet can get lower FR than CommNet and other baselines. When the training episode increases to 600, A-CCNet can further get a lower FR and a higher CR, while other models cannot get the same results.\nCommunication Message Analysis. We find a special car driving policy where the left car0 and the right car2 always brake to make space for the above car1 and the below car3. We illustrate the emitted messages by different cars under this policy in Figure 7. As we can see, messages for braking and gassing are naturally separated. For the same type (no matter braking or gassing) of messages, they can also be separated by different cars so that the ACCNet can distinguish them. Besides, gassing message is more diverse than braking message. The reason may be that braking positions are a few (near the junction) while each position of the grid road needs a different gassing message."}, {"heading": "Design Discussion of ACCNet", "text": "As we know, some design choices are very important for the success of DRL in realworld applications. For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23]. In this section, we briefly present a few design choices used by ACCNet, hoping that other researchers can confirm their usefulness for new environments. Note that all those design choices need to be further studied.\n(1) The embedded communication channel. We suggest to use deep neural networks to encode the communication message so that the final message dimension is controlled to be independent of the dimension of the original information. And most importantly, as the communication channel is embedded in deep neural networks, the communication protocols can be learned even from scratch in an end-to-end differentiable way while the network is optimized.\n(2) The concurrent experience replay (CER). Experience replay is beneficial for single-agent RL. Except for making the collection process of training data more efficient, it can also break the correlation among sequential training data to accelerate the convergence of models. However, as [4] point out, it is necessary to disable experience replay for MARL due to the non-concurrent property of local experiences when sampled independently for each agent. We propose CER to address this problem. Generally speaking, CER samples the concurrent experiences collected at the same timestep for all agents as they are trained concurrently. In fact, [40] use the same replay method and name it CER. We refer the readers to this paper for further details.\n(3) The current episode experience replay (CEER). Traditional experience replay methods uniformly sample a batch of experiences from replay buffer as training examples for model updating. [29] introduce prioritized experience replay based on the magnitude of TD-error to accelerate learning. The proposed CEER can be seen as a time-prioritized replay method. CEER keeps all experiences of current episode in a temporary buffer and combines them with experiences from the main replay buffer as training examples at the end of each episode. Our preliminary experiments show the effectiveness of this method. Detailed analyses can be found in the supplementary material.\n(4) Disabled experience replay for discrete action space environments. We find that the training of discrete action space environments is non-stable, no matter which replay method is used. Footnote 2 and [4, 41] explain this phenomenon in some extent, but further research is needed.\n(5) Full-information activation function for sensitive continuous action. Ideally (for continuous action environments), the policy \u03c0(a|s) is a one-to-one function mapping between state s and optimal action a\u2217. If we use a neural network \u03c0(a|s; \u03b8) to approximate \u03c0(a|s) to meet the one-to-one mapping requirement, we should not throw away any (useful) information in state s at any layer of the network \u03c0(a|s; \u03b8). Otherwise, similar states may be encoded into identical hidden vector and further be mapped to the same optimal action a\u2217. So we suggest to use sigmoid, elu, etc. rather than relu as activation functions for sensitive continuous action space applications. Similarly, we suggest to use relu for discrete action space applications because action is finite and similar states often correspond to the same optimal action a\u2217 in those applications. Our preliminary experiments show that relu based models will generate an averagely optimal action but not the exactly optimal action. Further analyses can be found in the supplementary material.\n(6) Centralized coordinator. Is there any single point failure? As ACCNet is fully distributed, any agent or special designed agents can act as the coordinator. In addition, the A-CCNet does not need the coordinator during execution, and centralized training is a common setting for MARL systems [4, 34, 33]."}, {"heading": "Conclusion", "text": "The proposed ACCNet, born with the combined abilities of deep models and actorcritic reinforce models, is a general framework to learn communication protocols from scratch for fully cooperative, partially observable MARL problems, no matter the ac-\ntion space is continuous or discrete. Specially, the A-CCNet, one concrete implementation of ACCNet, can make the training of MARL systems more stationary than previous methods as supported by both mathematical Equation (11) and experimental results of various environments. Another attractive advantage of A-CCNet is that it does not need communication during execution while still keeps a good generalization ability.\nFor the future work, we will put our efforts on the following important and challenging problems. (1) How to make the training of discrete action space MARL systems more stationary. Special experience replay method may be a powerful tool for this problem. (2) How to make the communication signals more sparse. In this paper, we use deep neural networks to compress the original communication messages. This method can achieve \u201cspatial-sparsity\u201d, i.e., the dimension of communication signals is limited and most values are around zero. Another one is \u201ctime-sparsity\u201d, i.e., the communication signals only come intermittently. Although those concepts are borrowed from sparse autoencoder [42, 43, 44], they are very useful in the real-world distributed MARL systems. We find that gating mechanism and token mechanism are very useful for achieving \u201ctime-sparsity\u201d. We will introduce our methods more formally in the future."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Xiangyu Liu, Weichen Ke, Chao Ma, Quanbin Wang, Yiping Song and the anonymous reviewers for their insightful comments. This work was supported by the National Natural Science Foundation of China under Grant No.61572044. The contact author is Zhen Xiao."}], "references": [{"title": "TCP-like congestion control for layered multicast data transfer[C]//INFOCOM\u201998", "author": ["L Vicisano", "J Crowcroft", "L. Rizzo"], "venue": "Seventeenth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "CODA: congestion detection and avoidance in sensor networks[C]//Proceedings of the 1st international conference on Embedded networked sensor systems", "author": ["Y Wan C", "B Eisenman S", "T. Campbell A"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "A multiagent approach to managing air traffic flow[J", "author": ["K Agogino A", "K. Tumer"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Learning to communicate with deep multi-agent reinforcement learning[C]//Advances in Neural Information Processing Systems", "author": ["J Foerster", "M Assael Y", "N de Freitas"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree search[J", "author": ["D Silver", "A Huang", "J Maddison C"], "venue": "Nature,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Multiagent cooperation and competition with deep reinforcement learning[J", "author": ["A Tampuu", "T Matiisen", "D Kodelja"], "venue": "PloS one, 2017,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games[J", "author": ["P Peng", "Q Yuan", "Y Wen"], "venue": "arXiv preprint arXiv:1703.10069,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Optimizing information exchange in cooperative multi-agent systems[C]// 2003:137-144", "author": ["V Goldman C", "S. Zilberstein"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Decentralized control of cooperative systems: categorization and complexity analysis[M", "author": ["V Goldman C", "S. Zilberstein"], "venue": "AI Access Foundation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Decentralized POMDPs[M]// Reinforcement Learning", "author": ["A. Oliehoek F"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A concise introduction to decentralized POMDPs[M", "author": ["A Oliehoek F", "C. Amato"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Decentralized Non-communicating Multiagent Collision Avoidance with Deep Reinforcement Learning[J", "author": ["F Chen Y", "M Liu", "M Everett"], "venue": "arXiv preprint arXiv:1609.07845,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Introduction to reinforcement learning[M", "author": ["S Sutton R", "G. Barto A"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Onactor-critic algorithms[J", "author": ["R Konda V", "N. Tsitsiklis J"], "venue": "SIAM journal on Control and Optimization,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "A survey of actor-critic reinforcement learning: Standard and natural policy gradients[J", "author": ["I Grondman", "L Busoniu", "D Lopes G A"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Deterministic policy gradient algorithms[J", "author": ["G. Lever"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A survey of actor-critic reinforcement learning: Standard and natural policy gradients[J", "author": ["I Grondman", "L Busoniu", "D Lopes G A"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Continuous control with deep reinforcement learning[J", "author": ["P Lillicrap T", "J Hunt J", "A Pritzel"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents[C]//Proceedings of the tenth international conference on machine learning", "author": ["M. Tan"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "Learning communication for multi-agent systems[C]//Workshop on Radical Agent Concepts", "author": ["L Giles C", "C. Jim K"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Learning multiagent communication with backpropagation[C]//Advances in Neural Information", "author": ["S Sukhbaatar", "R. Fergus"], "venue": "Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Human-level control through deep reinforcement learning[J", "author": ["V Mnih", "K Kavukcuoglu", "D Silver"], "venue": "Nature,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "MATE: MPLS adaptive traffic engineering[C]//INFOCOM", "author": ["A Elwalid", "C Jin", "S Low"], "venue": "Twentieth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "Walking the tightrope: Responsive yet stable traffic engineering[C]//ACM SIGCOMM Computer Communication Review", "author": ["S Kandula", "D Katabi", "B Davie"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Mazebase: A sandbox for learning from games[J", "author": ["S Sukhbaatar", "A Szlam", "G Synnaeve"], "venue": "arXiv preprint arXiv:1511.07401,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Prioritized experience replay[J", "author": ["T Schaul", "J Quan", "I Antonoglou"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning[C]//International", "author": ["V Mnih", "P Badia A", "M Mirza"], "venue": "Conference on Machine Learning", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1937}, {"title": "Reinforcement learning with unsupervised auxiliary tasks[J", "author": ["M Jaderberg", "V Mnih", "M Czarnecki W"], "venue": "arXiv preprint arXiv:1611.05397,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Deep attention recurrent Q-network[J", "author": ["I Sorokin", "A Seleznev", "M Pavlov"], "venue": "arXiv preprint arXiv:1512.01693,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments", "author": ["Lowe", "Ryan"], "venue": "arXiv preprint arXiv:1706.02275", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}, {"title": "Counterfactual Multi-Agent Policy Gradients", "author": ["Foerster", "Jakob"], "venue": "arXiv preprint arXiv:1705.08926", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2017}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1992}, {"title": "Actor-critic algorithms. Advances in neural information processing systems", "author": ["Konda", "Vijay R", "John N. Tsitsiklis"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "Emergence of Grounded Compositional Language in Multi-Agent Populations", "author": ["Mordatch", "Igor", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1703.04908", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2017}, {"title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning", "author": ["Das", "Abhishek"], "venue": "arXiv preprint arXiv:1703.06585", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2017}, {"title": "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols", "author": ["Havrylov", "Serhii", "Ivan Titov"], "venue": "arXiv preprint arXiv:1705.11192", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2017}, {"title": "Deep decentralized multi-task multi-agent reinforcement learning under partial observability", "author": ["Omidshafiei", "Shayegan"], "venue": "International Conference on Machine Learning", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2017}, {"title": "Stabilising experience replay for deep multi-agent reinforcement learning", "author": ["Foerster", "Jakob"], "venue": "arXiv preprint arXiv:1702.08887", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2017}, {"title": "Sparse deep belief net model for visual area V2", "author": ["Lee", "Honglak", "Chaitanya Ekanadham", "Andrew Y. Ng"], "venue": "Advances in neural information processing systems", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2008}, {"title": "Winner-take-all autoencoders", "author": ["Makhzani", "Alireza", "Brendan J. Frey"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach[J", "author": ["R Dobbe", "D Fridovich-Keil", "C. Tomlin"], "venue": "arXiv preprint arXiv:1707.06334,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "For applications where individual agent has limited capability, it is particularly critical for multiple agents to learn communication protocols to work in a collaborative way, for example: data routing [1], congestion detection [2] and air traffic management [3].", "startOffset": 203, "endOffset": 206}, {"referenceID": 1, "context": "For applications where individual agent has limited capability, it is particularly critical for multiple agents to learn communication protocols to work in a collaborative way, for example: data routing [1], congestion detection [2] and air traffic management [3].", "startOffset": 229, "endOffset": 232}, {"referenceID": 2, "context": "For applications where individual agent has limited capability, it is particularly critical for multiple agents to learn communication protocols to work in a collaborative way, for example: data routing [1], congestion detection [2] and air traffic management [3].", "startOffset": 260, "endOffset": 263}, {"referenceID": 4, "context": ", the combination of deep learning (DL) and multi-agent reinforcement learning (MARL), in many applications, such as self-play Go [5], two-player Pong [6] and multi-player StarCraft [7].", "startOffset": 130, "endOffset": 133}, {"referenceID": 5, "context": ", the combination of deep learning (DL) and multi-agent reinforcement learning (MARL), in many applications, such as self-play Go [5], two-player Pong [6] and multi-player StarCraft [7].", "startOffset": 151, "endOffset": 154}, {"referenceID": 6, "context": ", the combination of deep learning (DL) and multi-agent reinforcement learning (MARL), in many applications, such as self-play Go [5], two-player Pong [6] and multi-player StarCraft [7].", "startOffset": 182, "endOffset": 185}, {"referenceID": 7, "context": "In fact, the problem setting can be exactly modelled as Dec-POMDP-Com [8, 9], which is an extension of Dec-POMDP [10, 11] when considering communication.", "startOffset": 70, "endOffset": 76}, {"referenceID": 8, "context": "In fact, the problem setting can be exactly modelled as Dec-POMDP-Com [8, 9], which is an extension of Dec-POMDP [10, 11] when considering communication.", "startOffset": 70, "endOffset": 76}, {"referenceID": 9, "context": "In fact, the problem setting can be exactly modelled as Dec-POMDP-Com [8, 9], which is an extension of Dec-POMDP [10, 11] when considering communication.", "startOffset": 113, "endOffset": 121}, {"referenceID": 10, "context": "In fact, the problem setting can be exactly modelled as Dec-POMDP-Com [8, 9], which is an extension of Dec-POMDP [10, 11] when considering communication.", "startOffset": 113, "endOffset": 121}, {"referenceID": 11, "context": "The limited communication bandwidth is a common setting for recent \u201clearningto-communicate\u201d studies [20, 12, 23, 4].", "startOffset": 100, "endOffset": 115}, {"referenceID": 20, "context": "The limited communication bandwidth is a common setting for recent \u201clearningto-communicate\u201d studies [20, 12, 23, 4].", "startOffset": 100, "endOffset": 115}, {"referenceID": 3, "context": "The limited communication bandwidth is a common setting for recent \u201clearningto-communicate\u201d studies [20, 12, 23, 4].", "startOffset": 100, "endOffset": 115}, {"referenceID": 18, "context": "Traditional cooperative agents can share sensations, learned policies or even training episodes [19, 20], which is not suitable for real-world applications because communication itself takes up much bandwidth.", "startOffset": 96, "endOffset": 104}, {"referenceID": 11, "context": "However, the actors of AC-CNet inevitably need communication even during execution, which is impractical under some special situations [12].", "startOffset": 135, "endOffset": 139}, {"referenceID": 12, "context": "Reinforcement learning (RL) [13] is a machine learning approach to solve sequential decision making problem.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "Model-free RL algorithms can be divided into three groups [14, 15].", "startOffset": 58, "endOffset": 66}, {"referenceID": 14, "context": "Model-free RL algorithms can be divided into three groups [14, 15].", "startOffset": 58, "endOffset": 66}, {"referenceID": 29, "context": "This mutual reinforcement behavior helps actor-critic methods avoid bad local minima and 1One similar work [33] from OpenAI is released at the same time.", "startOffset": 107, "endOffset": 111}, {"referenceID": 30, "context": "Another concurrent work [34] from Oxford also uses a similar idea.", "startOffset": 24, "endOffset": 28}, {"referenceID": 6, "context": "converge faster, in particular for on-policy methods that follow the very recent policy to sample trajectory during training [7].", "startOffset": 125, "endOffset": 128}, {"referenceID": 12, "context": "Specifically, if actor uses stochastic policy for action selection, the actor and critic are updated based on the following TD-error and Stochastic Policy Gradient Theorem [13]:", "startOffset": 172, "endOffset": 176}, {"referenceID": 15, "context": "\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2) If actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]: \u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4) As ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].", "startOffset": 226, "endOffset": 230}, {"referenceID": 31, "context": "\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2) If actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]: \u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4) As ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].", "startOffset": 426, "endOffset": 430}, {"referenceID": 12, "context": "\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2) If actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]: \u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4) As ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].", "startOffset": 432, "endOffset": 436}, {"referenceID": 32, "context": "\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2) If actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]: \u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4) As ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].", "startOffset": 438, "endOffset": 442}, {"referenceID": 15, "context": "\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2) If actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]: \u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4) As ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].", "startOffset": 444, "endOffset": 448}, {"referenceID": 17, "context": "\u03b4t = rt + \u03b3V (st+1;w)\u2212 V (st;w) (1) \u03b8t+1 = \u03b8t + \u03b1 \u2217 \u03b4t \u2217 5\u03b8log\u03c0(at|st; \u03b8) (2) If actor uses deterministic policy for action selection, they are updated based on the following TD-error and Deterministic Policy Gradient Theorem [16]: \u03b4t = rt + \u03b3Q(st+1, at+1;w)\u2212Q(st, at;w) (3) \u03b8t+1 = \u03b8t + \u03b1 \u2217 5aQ(st, at;w) \u2217 5\u03b8\u03c0(at|st; \u03b8) (4) As ACCNet is based on actor-critic methods, the following articles are strongly recommended to read: [35], [13], [36], [16] and [18].", "startOffset": 453, "endOffset": 457}, {"referenceID": 18, "context": "Most previous work predefine the communication protocols [19, 20] and some others use technologies such as tabular RL [21] or evolutionary algorithm [22], which cannot generalize to the changing environment and large collection of agents directly as [13, 4] point out.", "startOffset": 57, "endOffset": 65}, {"referenceID": 19, "context": "Most previous work predefine the communication protocols [19, 20] and some others use technologies such as tabular RL [21] or evolutionary algorithm [22], which cannot generalize to the changing environment and large collection of agents directly as [13, 4] point out.", "startOffset": 149, "endOffset": 153}, {"referenceID": 12, "context": "Most previous work predefine the communication protocols [19, 20] and some others use technologies such as tabular RL [21] or evolutionary algorithm [22], which cannot generalize to the changing environment and large collection of agents directly as [13, 4] point out.", "startOffset": 250, "endOffset": 257}, {"referenceID": 3, "context": "Most previous work predefine the communication protocols [19, 20] and some others use technologies such as tabular RL [21] or evolutionary algorithm [22], which cannot generalize to the changing environment and large collection of agents directly as [13, 4] point out.", "startOffset": 250, "endOffset": 257}, {"referenceID": 20, "context": "Our work is an instance of this method, and the most relevant studies include the CommNet [23], DIAL [4] and BiCNet [7].", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "Our work is an instance of this method, and the most relevant studies include the CommNet [23], DIAL [4] and BiCNet [7].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "Our work is an instance of this method, and the most relevant studies include the CommNet [23], DIAL [4] and BiCNet [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 21, "context": "Both CommNet and DIAL are based on DQN [24] for discrete action.", "startOffset": 39, "endOffset": 43}, {"referenceID": 33, "context": "Other relevant excellent studies include but not limited to [37, 38, 39].", "startOffset": 60, "endOffset": 72}, {"referenceID": 34, "context": "Other relevant excellent studies include but not limited to [37, 38, 39].", "startOffset": 60, "endOffset": 72}, {"referenceID": 35, "context": "Other relevant excellent studies include but not limited to [37, 38, 39].", "startOffset": 60, "endOffset": 72}, {"referenceID": 11, "context": "However, the AC-CNet inevitably needs communication between actor and coordinator to get the global information even during execution, which is impractical under some special situations [12, 45].", "startOffset": 186, "endOffset": 194}, {"referenceID": 40, "context": "However, the AC-CNet inevitably needs communication between actor and coordinator to get the global information even during execution, which is impractical under some special situations [12, 45].", "startOffset": 186, "endOffset": 194}, {"referenceID": 29, "context": "More formally, Equation (11) always keeps true for any agent indexed by i with any changing policies \u03c0 6= \u03c0i [33]:", "startOffset": 109, "endOffset": 113}, {"referenceID": 30, "context": ", COMA [34] from Oxford and MADDPG [33] from OpenAI.", "startOffset": 7, "endOffset": 11}, {"referenceID": 29, "context": ", COMA [34] from Oxford and MADDPG [33] from OpenAI.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "Specifically, COMA is based on Stochastic Policy Gradient Theorem [13] and REINFORCE [35] algorithm.", "startOffset": 66, "endOffset": 70}, {"referenceID": 31, "context": "Specifically, COMA is based on Stochastic Policy Gradient Theorem [13] and REINFORCE [35] algorithm.", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "MADDPG extends DDPG [16, 18] into multi-agent environments.", "startOffset": 20, "endOffset": 28}, {"referenceID": 17, "context": "MADDPG extends DDPG [16, 18] into multi-agent environments.", "startOffset": 20, "endOffset": 28}, {"referenceID": 22, "context": "Figure 3: TwoIE and ThreeIE topologies for network flow control studies [26, 27].", "startOffset": 72, "endOffset": 80}, {"referenceID": 23, "context": "Figure 3: TwoIE and ThreeIE topologies for network flow control studies [26, 27].", "startOffset": 72, "endOffset": 80}, {"referenceID": 24, "context": "We consider the Traffic Junction problem modified from [28, 23].", "startOffset": 55, "endOffset": 63}, {"referenceID": 20, "context": "We consider the Traffic Junction problem modified from [28, 23].", "startOffset": 55, "endOffset": 63}, {"referenceID": 20, "context": "The results of CommNet and Discrete-CN are directly cited from [23].", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 25, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 26, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 27, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 17, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 28, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 20, "context": "For example, experience replay, frame skipping, target network, reward clipping, asynchronous training, auxiliary task and even methods of DL such as batch normalization, attention mechanism and skip connection are widely adopted [24, 29, 30, 31, 18, 32, 23].", "startOffset": 230, "endOffset": 258}, {"referenceID": 3, "context": "However, as [4] point out, it is necessary to disable experience replay for MARL due to the non-concurrent property of local experiences when sampled independently for each agent.", "startOffset": 12, "endOffset": 15}, {"referenceID": 36, "context": "In fact, [40] use the same replay method and name it CER.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "[29] introduce prioritized experience replay based on the magnitude of TD-error to accelerate learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Footnote 2 and [4, 41] explain this phenomenon in some extent, but further research is needed.", "startOffset": 15, "endOffset": 22}, {"referenceID": 37, "context": "Footnote 2 and [4, 41] explain this phenomenon in some extent, but further research is needed.", "startOffset": 15, "endOffset": 22}, {"referenceID": 3, "context": "In addition, the A-CCNet does not need the coordinator during execution, and centralized training is a common setting for MARL systems [4, 34, 33].", "startOffset": 135, "endOffset": 146}, {"referenceID": 30, "context": "In addition, the A-CCNet does not need the coordinator during execution, and centralized training is a common setting for MARL systems [4, 34, 33].", "startOffset": 135, "endOffset": 146}, {"referenceID": 29, "context": "In addition, the A-CCNet does not need the coordinator during execution, and centralized training is a common setting for MARL systems [4, 34, 33].", "startOffset": 135, "endOffset": 146}, {"referenceID": 38, "context": "Although those concepts are borrowed from sparse autoencoder [42, 43, 44], they are very useful in the real-world distributed MARL systems.", "startOffset": 61, "endOffset": 73}, {"referenceID": 39, "context": "Although those concepts are borrowed from sparse autoencoder [42, 43, 44], they are very useful in the real-world distributed MARL systems.", "startOffset": 61, "endOffset": 73}], "year": 2017, "abstractText": "Communication is a critical factor for the big multi-agent world to stay organized and productive. Typically, most previous multi-agent \u201clearning-to-communicate\u201d studies try to predefine the communication protocols or use technologies such as tabular reinforcement learning and evolutionary algorithm, which cannot generalize to the changing environment or large collection of agents directly. In this paper, we propose an Actor-Coordinator-Critic Net (ACCNet) framework for solving multi-agent \u201clearning-to-communicate\u201d problem. The ACCNet naturally combines the powerful actor-critic reinforcement learning technology with deep learning technology. It can learn the communication protocols even from scratch under partially observable environments. We demonstrate that the ACCNet can achieve better results than several baselines under both continuous and discrete action space environments. We also analyse the learned protocols and discuss some design considerations.", "creator": "LaTeX with hyperref package"}}}