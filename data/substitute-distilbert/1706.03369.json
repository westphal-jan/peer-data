{"id": "1706.03369", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2017", "title": "On the Sampling Problem for Kernel Quadrature", "abstract": "other standard kernel quadrature method for numerical integration within random point sets ( also called hyper monte carlo ) is known to converge in root mean square error at a rate determined by the ratio $ s / n $, where $ qc $ db $ d $ encode 1 associated loss dimension of the integrand. however, an empirical investigation reveals that the rate constant $ c $ is highly limiting to the distribution of the random points. in contrast via standard monte carlo integration, for which optimal importance alone is well - understood, the systematic distribution that minimises $ c $ for kernel quadrature does not admit a closed form. this paper argues that improved practical choice of sampling distribution is an unexpected open procedure. one solution is considered ; a novel automatic approach based on rapid tempering and sequential monte carlo. empirical results demonstrate a dramatic reduction in integration error of up with 4 orders of magnitude can be achieved with the proposed method.", "histories": [["v1", "Sun, 11 Jun 2017 16:08:17 GMT  (4491kb,D)", "http://arxiv.org/abs/1706.03369v1", "To appear at Thirty-fourth International Conference on Machine Learning (ICML 2017)"]], "COMMENTS": "To appear at Thirty-fourth International Conference on Machine Learning (ICML 2017)", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.NA stat.CO", "authors": ["fran\u00e7ois-xavier briol", "chris j oates", "jon cockayne", "wilson ye chen", "mark a girolami"], "accepted": true, "id": "1706.03369"}, "pdf": {"name": "1706.03369.pdf", "metadata": {"source": "CRF", "title": "On the Sampling Problem for Kernel Quadrature", "authors": ["Fran\u00e7ois-Xavier Briol", "Chris J. Oates", "Jon Cockayne", "Wilson Ye Chen", "Mark Girolami"], "emails": ["<f-x.briol@warwick.ac.uk>."], "sections": [{"heading": "1. INTRODUCTION", "text": "Consider approximation of the Lebesgue integral\n\u03a0(f) = \u222b X fd\u03a0 (1)\nwhere \u03a0 is a Borel measure defined over X \u2286 Rd and f is Borel measurable. Define P(f) to be the set of Borel measures \u03a0\u2032 such that f \u2208 L2(\u03a0\u2032), meaning that \u2016f\u20162L2(\u03a0\u2032) = \u222b X f\n2d\u03a0\u2032 < \u221e, and assume \u03a0 \u2208 P(f). In situations where \u03a0(f) does not admit a closed-form, Monte\n1University of Warwick, Department of Statistics. 2Imperial College London, Department of Mathematics. 3Newcastle University, School of Mathematics and Statistics 4The Alan Turing Institute for Data Science 5University of Technology Sydney, School of Mathematical and Physical Sciences. Correspondence to: Franc\u0327ois-Xavier Briol <f-x.briol@warwick.ac.uk>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nCarlo (MC) methods can be used to estimate the numerical value of Eqn. 1. A classical research problem in computational statistics is to reduce the MC estimation error in this context, where the integral can, for example, represent an expectation or marginalisation over a random variable of interest.\nThe default MC estimator comprises of\n\u03a0\u0302MC(f) = 1\nn n\u2211 j=1 f(xj),\nwhere xj are sampled identically and independently (i.i.d.) from \u03a0. Then we have a root mean square error (RMSE) bound \u221a\nE[\u03a0\u0302MC(f)\u2212\u03a0(f)]2 \u2264 CMC(f ; \u03a0)\u221a\nn ,\nwhere CMC(f ; \u03a0) = Std(f ; \u03a0) and the expectation is with respect to the joint distribution of the {xj}nj=1. For settings where the Lebesgue density of \u03a0 is only known up to normalising constant, Markov chain Monte Carlo (MCMC) methods can be used; the rate-constant CMC(f ; \u03a0) is then related to the asymptotic variance of f under the Markov chain sample path.\nConsiderations of computational cost place emphasis on methods to reduce the rate constant CMC(f ; \u03a0). For the MC estimator, this rate constant can be made smaller via importance sampling (IS): f 7\u2192 f \u00b7 d\u03a0/d\u03a0\u2032 where an optimal choice \u03a0\u2032 \u2208 P(f \u00b7 d\u03a0/d\u03a0\u2032), that minimises Std(f \u00b7 d\u03a0/d\u03a0\u2032; \u03a0\u2032), is available in explicit closed-form (see Robert and Casella, 2013, Thm. 3.3.4). However, the RMSE remains asymptotically gated at O(n\u22121/2).\nThe default Kernel Quadrature (KQ) estimate comprises of\n\u03a0\u0302(f) = n\u2211 j=1 wjf(xj), (2)\nwhere the xj \u223c \u03a0\u2032 are independent (or arise from a Markov chain) and supp(\u03a0) \u2286 supp(\u03a0\u2032). In contrast to MC, the weights {wj}nj=1 in KQ are in general non-uniform, real-valued and depend on {xj}nj=1. The KQ nomenclature derives from the (symmetric, positivedefinite) kernel k : X \u00d7 X \u2192 R that is used to construct an interpolant f\u0302(x) = \u2211n j=1 \u03b2jk(x,xj) such that\nar X\niv :1\n70 6.\n03 36\n9v 1\n[ st\nat .M\nL ]\n1 1\nJu n\n20 17\nf\u0302(xj) = f(xj) for j = 1, . . . , n. The weights wj in Eqn. 2 are implicitly defined via the equation \u03a0\u0302(f) = \u222b X f\u0302d\u03a0. The KQ estimator is identical to the posterior mean in Bayesian Monte Carlo (O\u2019Hagan, 1991; Rasmussen and Ghahramani, 2002), and its relationship with classical numerical quadrature rules has been studied (Diaconis, 1988; Sa\u0308rkka\u0308 et al., 2015).\nUnder regularity conditions, Briol et al. (2015b) established the following RMSE bound for KQ:\u221a\nE[\u03a0\u0302(f)\u2212\u03a0(f)]2 \u2264 C(f ; \u03a0 \u2032)\nns/d\u2212 , (s > d/2)\nwhere both the integrand f and each argument of the kernel k admit continuous mixed weak derivatives of order s and > 0 can be arbitrarily small. An information-theoretic lower bound on the RMSE is O(n\u2212s/d\u22121/2) (Bakhvalov, 1959). The faster convergence of the RMSE, relative to MC, can lead to improved precision in applications. Akin to IS, the samples {xj}nj=1 need not be draws from \u03a0 in order for KQ to provide consistent estimation (since \u03a0 is encoded in the weights wj). Importantly, KQ can be viewed as post-processing of MC samples; the kernel k can be reverse-engineered (e.g. via cross-validation) and does not need to be specified up-front.\nOne notable disadvantage of KQ methods is that little is known about how the rate constant C(f ; \u03a0\u2032) depends on the choice of sampling distribution \u03a0\u2032. In contrast to IS, no general closed-form expression has been established for an optimal distribution \u03a0\u2032 for KQ (the technical meaning of \u2018optimal\u2019 is defined below). Moreover, limited practical guidance is available on the selection of the sampling distribution (an exception is Bach, 2015, as explained in Sec. 2.4) and in applications it is usual to take \u03a0\u2032 = \u03a0. This choice is convenient but leads to estimators that are not efficient, as we demonstrate in dramatic empirical examples in Sec. 2.3.\nThe main contributions of this paper are twofold. First, we formalise the problem of optimal sampling for KQ as an important and open challenge in computational statistics. To be precise, our target is an optimal sampling distribution for KQ, defined as\n\u03a0\u2217 \u2208 arg min \u03a0\u2032 sup f\u2208F\n\u221a E[\u03a0\u0302(f)\u2212\u03a0(f)]2. (3)\nfor some functional class F to be specified. In general a (possibly non-unique) optimal \u03a0\u2217 will depend on F and, unlike for IS, also on the kernel k and the number of samples n.\nSecond, we propose a novel and automatic method for selection of \u03a0\u2032 that is rooted in approximation of the unavailable \u03a0\u2217. In brief, our method considers candidate sampling\ndistributions of the form \u03a0\u2032 = \u03a01\u2212t0 \u03a0 t for t \u2208 [0, 1] and \u03a00 a reference distribution on X . The exponent t is chosen such that \u03a0\u2032 minimises an empirical upper bound on the RMSE. The overall approach is facilitated with an efficient sequential MC (SMC) sampler and called SMC-KQ. In particular, the approach (i) provides practical guidance for selection of \u03a0\u2032 for KQ, (ii) offers robustness to kernel misspecification, and (iii) extends recent work on computing posterior expectations with kernels obtained using Stein\u2019s method (Oates et al., 2017).\nThe paper proceeds as follows: Empirical results in Sec. 2 reveal that the RMSE for KQ is highly sensitive to the choice of \u03a0\u2032. The proposed approach to selection of \u03a0\u2032 is contained in Sec. 3. Numerical experiments, presented in Sec. 4, demonstrate that dramatic reductions in integration error (up to 4 orders of magnitude) can be achieved with SMC-KQ. Lastly, a discussion is provided in Sec. 5."}, {"heading": "2. BACKGROUND", "text": "This section presents an overview of KQ (Sec. 2.1 and 2.2), empirical (Secs. 2.3) and theoretical (Sec. 2.4) results on the choice of sampling distribution, and discusses kernel learning for KQ (Sec. 2.5)."}, {"heading": "2.1. Overview of Kernel Quadrature", "text": "We now proceed to describe KQ: Recall the approximation f\u0302 to f ; an explicit form for the coefficients \u03b2j is given as \u03b2 = K\u22121f , where Ki,j = k(xi,xj) and fj = f(xj). It is assumed that K\u22121 exists almost surely; for non-degenerate kernels, this corresponds to \u03a0 having no atoms. From the above definition of KQ,\n\u03a0\u0302(f) = n\u2211 j=1 \u03b2j \u222b X k(x,xj)\u03a0(dx).\nDefining zj = \u222b X k(\u00b7,xj)d\u03a0 leads to the estimate in Eqn. 2 with weights w = K\u22121z. Pairs (\u03a0, k) for which the zj have closed form are reported in Table 1 of Briol et al. (2015b). Computation of these weights incurs a computational cost of at most O(n3) and can be justified when either (i) evaluation of f forms the computational bottleneck, or (ii) the gain in estimator precision (as a function in n) dominates this cost (i.e. whenever s/d > 3 + 1/2).\nNotable contributions on KQ include Diaconis (1988); O\u2019Hagan (1991); Rasmussen and Ghahramani (2002) who introduced the method and Huszar and Duvenaud (2012); Osborne et al. (2012a;b); Gunter et al. (2014); Bach (2015); Briol et al. (2015a;b); Sa\u0308rkka\u0308 et al. (2015); Kanagawa et al. (2016); Liu and Lee (2017) who provided consequent methodological extensions. KQ has been applied to a wide range of problems including probabilistic ODE\nsolvers (Kersting and Hennig, 2016), reinforcement learning (Paul et al., 2016), filtering (Pru\u0308her and S\u030cimandl, 2015) and design of experiments (Ma et al., 2014).\nSeveral characterisations of the KQ estimator are known and detailed below. LetH denote the Hilbert space characterised by the reproducing kernel k, and denote its norm as \u2016 \u00b7 \u2016H (Berlinet and Thomas-Agnan, 2011). Then we have the following: (a) The function f\u0302 is the minimiser of \u2016g\u2016H over g \u2208 H subject to g(xj) = f(xj) for all j = 1, . . . , n. (b) The function f\u0302 is the posterior mean for f under the Gaussian process prior f \u223c GP(0, k) conditioned on data f and \u03a0\u0302(f) is the mean of the implied posterior marginal over \u03a0[f ]. (c) The weightsw are characterised as the minimiser over \u03b3 \u2208 Rn of\nen(\u03b3; {xj}nj=1) = sup \u2016f\u2016H=1 \u2223\u2223\u2223\u2223\u2223 n\u2211 j=1 \u03b3jf(xj)\u2212\u03a0(f) \u2223\u2223\u2223\u2223\u2223, the maximal error in the unit ball of H. These characterisations connect KQ to (a) non-parametric regression, (b) probabilistic integration and (c) quasi-Monte Carlo (QMC) methods (Dick and Pillichshammer, 2010). The scattered data approximation literature (Sommariva and Vianello, 2006) and the numerical analysis literature (where KQ is known as the \u2018empirical interpolation method\u2019; Eftang and Stamm, 2012; Kristoffersen, 2013) can also be connected to KQ. However, our search of all of these literatures did not yield guidance on the optimal selection of the sampling distribution \u03a0\u2032 (with the exception of Bach (2015) reported in Sec. 2.4)."}, {"heading": "2.2. Over-Reliance on the Kernel", "text": "In Osborne et al. (2012a); Huszar and Duvenaud (2012); Gunter et al. (2014); Briol et al. (2015a), the selection ofxn was approached as a greedy optimisation problem, wherein the maximal integration error en(w; {xj}nj=1) was minimised, given the location of the previous {xj}n\u22121j=1 . This approach has demonstrated considerable success in applications. However, the error criterion en is strongly dependant on the choice of kernel k and the sequential optimisation approach is vulnerable to kernel misspecification. In particular, if the intrinsic length scale of k is \u201ctoo small\u201d then the {xj}nj=1 all cluster around the mode of \u03a0, leading to poor integral estimation (see Fig. 5 in the Appendix). Related work on sub-sample selection, such as leverage scores (Bach, 2013), can also be non-robust to mis-specified kernels. The partial solution of online kernel learning requires a sufficient number n of data and is not always practicable in small-n regimes that motivate KQ.\nThis paper considers sampling methods as a robust alternative to optimisation methods. Although our method also makes use of k to select \u03a0\u2032, it reverts to \u03a0\u2032 = \u03a0 in the\nlimit as the length scale of k is made small. In this sense, sampling offers more robustness to kernel mis-specification than optimisation methods, at the expense of a possible (non-asymptotic) decrease in precision in the case of a well-specified kernel. This line of research is thus complementary to existing work. However, we emphasise that robustness is an important consideration for general applications of KQ in which kernel specification may be a nontrivial task."}, {"heading": "2.3. Sensitivity to the Sampling Distribution", "text": "To date, we are not aware of a clear demonstration of the acute dependence of the performance of the KQ estimator on the choice of distribution \u03a0\u2032. It is therefore important to illustrate this phenomenon in order to build intuition.\nConsider the toy problem with state space X = R, target distribution \u03a0 = N(0, 1), a single test function f(x) = 1 + sin(2\u03c0x) and kernel k(x, x\u2032) = exp(\u2212(x\u2212 x\u2032)2). For this problem, consider a range of sampling distributions of the form \u03a0\u2032 = N(0, \u03c32) for \u03c3 \u2208 (0,\u221e). Fig. 1 plots\nR\u0302n,\u03c3 = \u221a\u221a\u221a\u221a 1 M M\u2211 m=1 (\u03a0\u0302n,m,\u03c3(f)\u2212\u03a0(f))2,\nan empirical estimate for the RMSE where \u03a0\u0302n,m,\u03c3(f) is the mth of M independent KQ estimates for \u03a0(f) based on n samples drawn from the distribution \u03a0\u2032 with standard deviation \u03c3 (M = 1000). In this case \u03a0(f) = 1 is available in closed-form. It is seen that the \u2018obvious\u2019 choice of \u03c3 = 1, i.e. \u03a0\u2032 = \u03a0, is sub-optimal. The intuition here is that \u2018extreme\u2019 samples xi from the tails of \u03a0 are rather informative for building the interpolant f\u0302 underlying KQ; we should therefore over-sample these values via a heaviertailed \u03a0\u2032. The same intuition is used for column sampling and to construct leverage scores (Mahoney, 2011; Drineas et al., 2012)."}, {"heading": "2.4. Established Results", "text": "Here we recall the main convergence results to-date on KQ and discuss how these relate to choices of sampling distribution. To reduce the level of detail below, we make several assumptions at the outset:\nAssumption on the domain: The domain X will either be Rd itself or a compact subset of Rd that satisfies an \u2018interior cone condition\u2019, meaning that there exists an angle \u03b8 \u2208 (0, \u03c0/2) and a radius r > 0 such that for every x \u2208 X there exists \u2016\u03be\u20162 = 1 such that the cone {x + \u03bby : y \u2208 Rd, \u2016y\u20162 = 1, yT \u03be \u2265 cos \u03b8, \u03bb \u2208 [0, r]} is contained in X (see Wendland, 2004, for background).\nAssumption on the kernel: Consider the integral operator \u03a3 : L2(\u03a0) \u2192 L2(\u03a0), with (\u03a3f)(x) defined as the\nBochner integral \u222b X f(x\n\u2032)k(x,x\u2032)\u03a0(dx\u2032). Assume that\u222b X k(x,x)\u03a0(dx) < \u221e, so that \u03a3 is self-adjoint, positive semi-definite and trace-class (Simon, 1979). Then, from an extension of Mercer\u2019s theorem (Ko\u0308nig, 1986) we have a decomposition k(x,x\u2032) = \u2211\u221e m=1 \u00b5mem(x)em(x\n\u2032), where \u00b5m and em(x) are the eigenvalues and eigenfunctions of \u03a3. Further assume thatH is dense in L2(\u03a0).\nThe first result is adapted and extended from Thm. 1 in Oates et al. (2016).\nTheorem 1. Assume that \u03a0\u2032 admits a density \u03c0\u2032 defined on a compact domain X . Assume that \u03c0\u2032 > c for some c > 0. Let x1, . . . ,xm be fixed and define the Euclidean fill distance\nhm = sup x\u2208X min j=1,...,m\n\u2016x\u2212 xj\u20162.\nLet xm+1, . . . ,xn be independent draws from \u03a0\u2032. Assume k gives rise to a Sobolev space Hs(\u03a0). Then there exists h0 > 0 such that, for hm < h0,\u221a\nE[\u03a0\u0302(f)\u2212\u03a0(f)]2 \u2264 C(f)n\u2212s/d+\nfor all > 0. Here C(f) = ck,\u03a0\u2032, \u2016f\u2016H for some constant 0 < ck,\u03a0\u2032, <\u221e independent of n and f .\nAll proofs are reserved for the Appendix. The main contribution of Thm. 1 is to establish a convergence rate for KQ when using importance sampling distributions. A similar result appeared in Thm. 1 of Briol et al. (2015b) for samples from \u03a0 (see the Appendix) and was extended to MCMC samples in Oates et al. (2016). An extension to\nthe case of a mis-specified kernel was considered in Kanagawa et al. (2016). However a limitation of this direction of research is that it does not address the question of how to select \u03a0\u2032.\nThe second result that we present is a consequence of the recent work of Bach (2015), who considered a particular choice of \u03a0\u2032 = \u03a0B, depending on a fixed \u03bb > 0, via the density \u03c0B(x;\u03bb) \u221d \u2211\u221e m=1 \u00b5m \u00b5m+\u03bb\ne2m(x). The following is adapted from Prop. 1 in Bach (2015): Theorem 2. Let x1, . . . ,xn \u223c \u03a0B be independent and \u03bb > 0. For \u03b4 \u2208 (0, 1) and n \u2265 5d(\u03bb) log 16d(\u03bb)\u03b4 , d(\u03bb) = \u2211\u221e m=1 \u00b5m \u00b5m+\u03bb , we have that\n|\u03a0\u0302(f)\u2212\u03a0(f)| \u2264 2\u03bb1/2\u2016f\u2016H,\nwith probability greater than 1\u2212 \u03b4.\nSome remarks are in order: (i) Bach (2015, Prop. 3) showed that, for \u03a0B, integration error scales at an optimal rate in n up to logarithmic terms and, after n samples, is of size \u221a \u00b5n. (ii) The distribution \u03a0B is obtained from minimising an upper bound on the integration error, rather than the error itself. It is unclear to us how well \u03a0B approximates an optimal sampling distribution for KQ. (iii) In general \u03a0B is hard to compute. For the specific case X = [0, 1]d, H equal to Hs(\u03a0) and \u03a0 uniform, the distribution \u03a0B is also uniform (and hence independent of n; see Sec. 4.4 of Bach (2015)). However, even for the simple example of Sec. 2.3, \u03a0B does not appear to have a closed form (details in Appendix). An approximation scheme was proposed in Sec. 4.2 of Bach (2015) but the error of this scheme was not studied.\nOptimal sampling for approximation in \u2016 \u00b7 \u2016L2(\u03a0) with weighted least squares (not in the kernel setting) was\nconsidered in Hampton and Doostan (2015); Cohen and Migliorati (2016)."}, {"heading": "2.5. Goals", "text": "Our first goal was to formalise the sampling problem for KQ; this is now completed. Our second goal was to develop a novel automatic approach to selection of \u03a0\u2032, called SMC-KQ; full details are provided in Sec. 3.\nAlso, observe that the integrand f will in general belong to an infinitude of Hilbert spaces, while for KQ a single kernel k must be selected. This choice will affect the performance of the KQ estimator; for example, in Fig. 2, the problem of Sec. 2.3 was reconsidered based on a class of kernels k(x, x\u2032) = exp(\u2212(x \u2212 x\u2032)2/`2) parametrised by ` \u2208 (0,\u221e). Results showed that, for all choices of \u03c3 parameter, the RMSE of KQ is sensitive to choice of `. In particular, the default choice of ` = 1 is not optimal. For this reason, an extension that includes kernel learning, called SMC-KQ-KL, is proposed in Sec. 3."}, {"heading": "3. METHODS", "text": "In this section the SMC-KQ and SMC-KQ-KL methods are presented. Our aim is to explain in detail the main components (SMC, temp, crit) of Alg. 1. To this end, Secs. 3.1 and 3.2 set up our SMC sampler to target tempered distributions, while Sec. 3.3 presents a heuristic for the choice of temperature schedule. Sec. 3.4 extends the approach to kernel learning and Sec. 3.5 proposes a novel criterion to determine when a desired error tolerance is reached."}, {"heading": "3.1. Thermodynamic Ansatz", "text": "To begin, consider f , k and n as fixed. The following ansatz is central to our proposed SMC-KQ method: An optimal distribution \u03a0\u2217 (in the sense of Eqn. 3) can be wellapproximated by a distribution of the form\n\u03a0t = \u03a0 1\u2212t 0 \u03a0 t, t \u2208 [0, 1] (4)\nfor a specific (but unknown) \u2018inverse temperature\u2019 parameter t = t\u2217. Here \u03a00 is a reference distribution to be specified and which should be chosen to be un-informative in practice. It is assumed that all \u03a0t exist (i.e. can be normalised). The motivation for this ansatz stems from Sec. 2.3, where \u03a0 = N(0, 1) and \u03a0t = N(0, \u03c32) can be cast in this form with t = \u03c3\u22121 and \u03a00 an (improper) uniform distribution on R. In general, tempering generates a class of distributions which over-represent extreme events relative to \u03a0 (i.e. have heavier tails). This property has the potential to improve performance for KQ, as demonstrated in Sec. 2.3.\nThe ansatz of Eqn. 4 reduces the non-parametric sampling problem for KQ to the one-dimensional parametric prob-\nlem of selecting a suitable t \u2208 [0, 1]. The problem can be further simplified by focusing on a discrete temperature ladder {ti}Ti=0 such that t0 = 0, ti < ti+1 and tT = 1. Discussion of the choice of ladder is deferred to Sec. 3.3. This reduced problem, where we seek an optimal index i\u2217 \u2208 {0, . . . , T}, is still non-trivial as no closed-form expression is available for the RMSE at each candidate ti. To overcome this impasse a novel approach to estimate the RMSE is presented in Sec. 3.5.\n3.2. Convex Ansatz (SMC)\nThe proposed SMC-KQ algorithm requires a second ansatz, namely that the RMSE is convex in t and possesses a global minimum in the range t \u2208 (0, 1). This second ansatz (borne out in numerical results in Fig. 1) motivates an algorithm that begins at t0 = 0 and tracks the RMSE until an increase is detected, say at ti; at which point the index i\u2217 = i\u2212 1 is taken for KQ.\nTo realise such an algorithm, this paper exploited SMC methods (Chopin, 2002; Del Moral et al., 2006). Here, a particle approximation {(wj ,xj)}Nj=1 to \u03a0t0 is first obtained where xj are independent draws from \u03a00, wj = N\u22121 and N n. Then, at iteration i, the particle approximation to \u03a0ti\u22121 is re-weighted, re-sampled and subject to a Markov transition, to deliver a particle approximation {(w\u2032j ,x\u2032j)}Nj=1 to \u03a0ti . This \u2018re-sample-move\u2019 algorithm, denoted SMC, is standard but, for completeness, pseudo-code is provided as Alg. 2 in the Appendix.\nAt iteration i, a subset of size n is drawn from the unique1 elements in {x\u2032j}Nj=1, from the particle approximation to \u03a0ti , and proposed for use in KQ. A criterion crit, defined in Sec. 3.5, is used to determine whether the resultant KQ error has increased relative to \u03a0ti\u22121 . If this is the case, then the distribution \u03a0ti\u22121 from the previous iteration is taken for use in KQ. Otherwise the algorithm proceeds to ti+1 and the process repeats. In the degenerate case where the RMSE has a minimum at tT , the algorithm defaults to standard KQ with \u03a0\u2032 = \u03a0.\nBoth ansatz of the SMC-KQ algorithm are justified through the strong empirical results presented in Sec. 4.\n3.3. Choice of Temperature Schedule (temp)\nThe choice of temperature schedule {ti}Ti=0 influences several aspects of SMC-KQ: (i) The SMC approximation to \u03a0ti is governed by the \u201cdistance\u201d (in some appropriate metric) between \u03a0ti\u22121 and \u03a0ti . (ii) The speed at which the minimum t\u2217 can be reached is linear in the number of\n1This ensures that kernel matrices have full rank. It does not introduce bias into KQ, since in general \u03a0\u2032 need not equal \u03a0. However, to keep notation clear, we do not make this operation explicit.\ntemperatures between 0 and t\u2217. (iii) The precision of KQ depends on the approximation t\u2217 \u2248 ti\u2217 . Factors (i,iii) motivate the use of a fine schedule with T large, while (ii) motivates a coarse schedule with T small.\nFor this work, a temperature schedule was used that is well suited to both (i) and (ii), while a strict constraint ti \u2212 ti\u22121 \u2264 \u2206 was imposed on the grid spacing to acknowledge (iii). The specific schedule used in this work was determined based on the conditional effective sample size of the current particle population, as proposed in the recent work of Zhou et al. (2016). Full details are presented in Algs. 4 and 5 in the Appendix."}, {"heading": "3.4. Kernel Learning", "text": "In Sec. 2.5 we demonstrated the benefit of kernel learning for KQ. From the Gaussian process characterisation of KQ from Sec. 2.1, it follows that kernel parameters \u03b8 can be estimated, conditional on a vector of function evaluations f , via maximum marginal likelihood:\n\u03b8\u2032 \u2190 arg max \u03b8 p(f |\u03b8) = arg min \u03b8 f>K\u22121\u03b8 f + log |K\u03b8|.\nIn SMC-KQ-KL, the function evaluations f are obtained at the first2 n (of N ) states {xj}nj=1 and the parameters \u03b8 are updated in each iteration of the SMC. This demands repeated function evaluation; this burden can be reduced with less frequent parameter updates and caching of all previous function evaluations. The experiments in Sec. 4 assessed both SMC-KQ and SMC-KQ-KL in terms of precision per total number of function evaluations, so that the additional cost of kernel learning was taken into account.\n3.5. Termination Criterion (crit)\nThe SMC-KQ-KL algorithm is designed to track the RMSE as t is increased. However, the RMSE is not available in closed form. In this section we derive a tight upper bound on the RMSE that is used for the crit component in Alg. 1.\nFrom the worst-case characterisation of KQ presented in Sec. 2.1, we have an upper bound\n|\u03a0\u0302(f)\u2212\u03a0(f)| \u2264 en(w; {xj}nj=1)\u2016f\u2016H. (5)\nThe term en(w; {xj}nj=1), denoted henceforth as en({xj}nj=1) (since w depends on {xj}nj=1), can be computed in closed form (see the Appendix). This motivates\n2This is a notational convention and is without loss of generality. In this paper these states were a random sample (without replacement) of size n, though stratified sampling among the N states could be used. More sophisticated alternatives that also involve the kernel k, such as leverage scores, were not considered, since in general these (i) introduce a vulnerability to mis-specified kernels and (ii) require manipulation of a N \u00d7 N kernel matrix (Patel et al., 2015).\nAlgorithm 1 SMC Algorithm for KQ function SMC-KQ(f,\u03a0, k,\u03a00, \u03c1, n,N) input f (integrand) input \u03a0 (target disn.) input k (kernel) input \u03a00 (reference disn.) input \u03c1 (re-sample threshold) input n (num. func. evaluations) input N (num. particles) i\u2190 0; ti \u2190 0; Rmin \u2190\u221e x\u2032j \u223c \u03a00 (initialise states \u2200j \u2208 1 : N ) w\u2032j \u2190 N\u22121 (initialise weights \u2200j \u2208 1 : N ) R\u2190 crit(\u03a0, k, {x\u2032j}Nj=1) (est\u2019d error) while test(R < Rmin) and ti < 1 do i\u2190 i+ 1; Rmin \u2190 R {(wj ,xj)}Nj=1 \u2190 {(w\u2032j ,x\u2032j)}Nj=1 ti \u2190 temp({(wj ,xj)}Nj=1, ti\u22121, \u03c1) (next temp.) {(w\u2032j ,x\u2032j)}Nj=1 \u2190 SMC({(wj ,xj)}Nj=1, ti, ti\u22121, \u03c1) (next particle approx.) R\u2190 crit(\u03a0, k, {x\u2032j}Nj=1) (est\u2019d error)\nend while fj \u2190 f(xj) (function eval. \u2200j \u2208 1 : n) zj \u2190 \u222b X k(\u00b7,xj)d\u03a0 (kernel mean eval. \u2200j \u2208 1 : n) Kj,j\u2032 \u2190 k(xj ,xj\u2032) (kernel eval. \u2200j, j\u2032 \u2208 1 : n) \u03a0\u0302(f)\u2190 z>K\u22121f (eval. KQ estimator) return \u03a0\u0302(f)\nthe following upper bound on MSE:\nE[\u03a0\u0302(f)\u2212\u03a0(f)]2 \u2264 E[en({xj}nj=1)2]\ufe38 \ufe37\ufe37 \ufe38 (\u2217) \u2016f\u20162H\ufe38 \ufe37\ufe37 \ufe38 (\u2217\u2217)\n(6)\nThe term (\u2217) can be estimated with the bootstrap approximation\nE[en({xj}nj=1)2] = M\u2211 m=1 en({x\u0303m,j}nj=1)2 M =: R2\nwhere x\u0303m,j are independent draws from {xj}Nj=1. In SMC-KQ the term (\u2217\u2217) is an unknown constant and the statistic R, an empirical proxy for the RMSE, is monitored at each iteration. The algorithm terminates once an increase in this statistic occurs. For SMC-KQ-KL the term (\u2217\u2217) is non-constant as it depends on the kernel hyper-parameters; then (\u2217\u2217) can in addition be estimated as \u2016f\u0302\u20162H = w>K\u03b8w and we monitor the product of R and \u2016f\u0302\u2016H, with termination when an increase is observed (c.f. test, defined in the Appendix).\nFull pseudo-code for SMC-KQ is provided as Alg. 1, while SMC-KQ-KL is Alg. 9 in the Appendix. To summarise, we have developed a novel procedure, SMC-KQ (and an extension SMC-KQ-KL), designed to approximate the optimal\nKQ estimator based on the unavailable optimal distribution in Eqn. 3 where F is the unit ball of H. Earlier empirical results in Sec. 2.3 suggest that SMC-KQ has potential to provide a powerful and general algorithm for numerical integration. The additional computational cost of optimising the sampling distribution does however have to be counterbalanced with the potential gain in error, and so this method will mainly be of practical interest for problems with expensive integrands or complex target distributions. The following section reports experiments designed to test this claim."}, {"heading": "4. RESULTS", "text": "Here we compared SMC-KQ (and SMC-KQ-KL) against the corresponding default approaches KQ (and KQ-KL) that are based on \u03a0\u2032 = \u03a0. Sec. 4.1 below reports an assessment in which the true value of integrals is known by design, while in Sec. 4.2 the methods were deployed to solve a parameter estimation problem involving differential equations."}, {"heading": "4.1. Simulation Study", "text": "To continue our illustration from Sec. 2, we investigated the performance of SMC-KQ and SMC-KQ-KL for integration of f(x) = 1 + sin(2\u03c0x) against the distribution \u03a0 = N(0, 1). Here the reference distribution was taken to be \u03a00 = N(0, 82). All experiments employed SMC with N = 300 particles, random walk Metropolis transitions (Alg. 3), the re-sample threshold \u03c1 = 0.95 and a maximum grid size \u2206 = 0.1. Dependence of the subsequent results on the choice of \u03a00 was investigated in Fig. 10 in the Appendix.\nFig. 3 (top) reports results for SMC-KQ against KQ, for fixed length-scale ` = 1. Corresponding results for SMC-KQ-KL against KQ-KL are shown in the bottom plot. It was observed that SMC-KQ (resp. SMC-KQ-KL) outperformed KQ (resp. KQ-KL) in the sense that, on a perfunction-evaluation basis, the MSE achieved by the proposed method was lower than for the standard method. The largest reduction in MSE achieved was about 8 orders of magnitude (correspondingly 4 orders of magnitude in RMSE). A fair approximation to the \u03c3 = 2 method, which is approximately optimal for n = 75 (c.f. results in Fig. 1), was observed. The termination criterion in Sec. 3.5 was observed to be a good approximation to the optimal temperature t\u2217 (Fig. 9 in Appendix). As an aside, we note that the MSE was gated at 10\u221216 for all methods due to numerical condition of the kernel matrix K (a known feature of the Gaussian kernel used in this experiment).\nThe investigation was extended to larger dimensions (d = 3 and d = 10) and more complex integrands f in the Ap-\npendix. In all cases, considerable improvements were obtained using SMC-KQ over KQ."}, {"heading": "4.2. Inference for Differential Equations", "text": "Consider the model given by dx/dt = f(t|\u03b8) with solution x(t|\u03b8) depending on unknown parameters \u03b8. Suppose we can obtain observations through the following noise model (likelihood): y(ti) = x(ti|\u03b8) + ei at times 0 = t1 < . . . < tn where we assume ei \u223c N(0, \u03c32) for known \u03c3 > 0. Our goal is to estimate x(T |\u03b8) for a fixed (potentially large) T > 0. To do so, we will use a Bayesian approach and specify a prior p(\u03b8), then obtain samples from the posterior \u03c0(\u03b8) := p(\u03b8|y) using MCMC. The posterior predictive mean is then defined as: \u03a0 ( x(T |\u00b7) ) = \u222b x(T |\u03b8)\u03c0(\u03b8)d\u03b8, and this can be estimated using an empirical average from the posterior samples. This type of integration problem is particularly challenging as the integrand requires simulating from the differential equation at each iteration. Furthermore, the larger T or the smaller the grid, the longer the simulation will be and the higher the computational cost.\nFor a tractable test-bed, we considered Hooke\u2019s law, given\nby the following second order homogeneous ODE given by\n\u03b85 d2x\ndt2 + \u03b84\ndx dt + \u03b83x = 0,\nwith initial conditions x(0) = \u03b81 and x\u2032(0) = \u03b82. This equation represents the evolution of a mass on a spring with friction (Robinson, 2004, Chapter 13). More precisely, \u03b83 denotes the spring constant, \u03b84 the damping coefficient representing friction and \u03b85 the mass of the object. Since this differential equation is an overdetermined system we fixed \u03b85 = 1. In this case, if \u03b824 \u2264 4\u03b83, we get a damped oscillatory behaviour as presented in Fig. 4 (top). Data were generated with \u03c3 = 0.4, (\u03b81, \u03b82, \u03b83, \u03b84) = (1, 3.75, 2.5, 0.5). with log-normal priors with scale equal to 0.5 for all parameters.\nTo implement KQ under an unknown normalisation constant for \u03a0, we followed Oates et al. (2017) and made use of a Gaussian kernel that was adapted with Stein\u2019s method (see the Appendix for details). The reference distribution \u03a00 was an wide uniform prior on the hypercube [0, 10]4. Brute force computation was used to obtain a benchmark value for the integral. For the SMC algorithm, an independent lognormal transition kernel was used at each iteration with parameters automatically tuned to the current set of particles. Results in Fig. 4 demonstrate that SMC-KQ outperforms KQ for these integration problems. These results improve upon those reported in Oates et al. (2016) for a similar integration problem based on parameter estimation for differential equations."}, {"heading": "5. DISCUSSION", "text": "In this paper we formalised the optimal sampling problem for KQ. A general, practical solution was proposed, based on novel use of SMC methods. Initial empirical results demonstrate performance gains relative to standard approach of KQ with \u03a0\u2032 = \u03a0. A more challenging example based on parameter estimation for differential equations was used to illustrate the potential of SMC-KQ for Bayesian computation in combination with Stein\u2019s method.\nOur methods were general but required user-specified choice of an initial distribution \u03a00. For compact state spaces X we recommend taking \u03a00 to be uniform. For non-compact spaces, however, there is a degree of flexibility here and default solutions, such as wide Gaussian distributions, necessarily require user input. However, the choice of \u03a00 is easier than the choice of \u03a0\u2032 itself, since \u03a00 is not required to be optimal. In our examples, improved performance (relative to standard KQ) was observed for a range of reference distributions \u03a00.\nA main motivation for this research was to provide an alternative to optimisation-based KQ that alleviates strong dependence on the choice of kernel (Sec. 2.2). This paper provides essential groundwork toward that goal, in developing sampling-based methods for KQ in the case of complex and expensive integration problems. An empirical comparison of sampling-based and optimisation-based methods is reserved for future work.\nTwo extensions of this research are identified: First, the curse of dimension that is intrinsic to standard Sobolev spaces can be alleviated by demanding \u2018dominating mixed smoothness\u2019; our methods are compatible with these (essentially tensor product) kernels (Dick et al., 2013). Second, the use of sequential QMC (Gerber and Chopin, 2015) can be considered, motivated by further orders of magnitude reduction in numerical error observed for deterministic point sets (see Fig. 13 in the Appendix)."}, {"heading": "ACKNOWLEDGEMENTS", "text": "FXB was supported by the EPSRC grant [EP/L016710/1]. CJO & MG we supported by the Lloyds Register Foundation Programme on Data-Centric Engineering. WYC was supported by the ARC Centre of Excellence in Mathematical and Statistical Frontiers. MG was supported by the EPSRC grants [EP/J016934/3, EP/K034154/1, EP/P020720/1], an EPSRC Established Career Fellowship, the EU grant [EU/259348], a Royal Society Wolfson Research Merit Award. FXB, CJO, JC & MG were also supported by the SAMSI working group on Probabilistic Numerics."}, {"heading": "A. Appendix", "text": "This appendix complements the paper \u201cOn the sampling problem for kernel quadrature\u201d. Section A.1 discusses the potential lack of robustness of greedy optimization methods, which motivated the development of SMC-KQ. Sections A.2 and A.3 discuss some of the theoretical aspects of KQ, whilst Section A.4 and A.5 presents additional numerical experiments and details for implementation. Finally, Section A.6 provides detailed pseudo-code for all algorithms used in this paper."}, {"heading": "A.1. Lack of Robustness of Optimisation Methods", "text": "To demonstrate the non-robustness to mis-specified kernels, that is a feature of optimisation-based methods, we considered integration against \u03a0 = N(0, 1) for functions that can be approximated by the kernel k(x, x\u2032) = exp(\u2212(x \u2212 x\u2032)2/`2). An initial state x1 was fixed at the origin and then for n = 2, 3, . . . the state xn was chosen to minimise the error criterion en(w; {xj}nj=1) given the location of the {xj}nj=1. This is known as \u2018sequential Bayesian quadrature\u2019 (SBQ; Huszar and Duvenaud, 2012; Gunter et al., 2014; Briol et al., 2015a). The kernel length scale was fixed at ` = 0.01 and we consider (as a thought experiment, since it does not enter into our selection of points) a more regular integrand, such as that shown in Fig. 5 (top). The location of the states {xj}nj=1 obtained in this manner are shown in Fig. 5 (bottom). It is clear that SBQ is not an efficient use of computation for integration of the integrand against N(0, 1). Of course, a bad choice of kernel length scale parameter ` can in principle be alleviated by kernel learning, but this will not be robust the case where n is very small.\nThis example motivates sampling-based methods as an alternative to optimisation-based methods. Future work will be required to better understand when methods such as SBQ can be reliable in the presence of unknown kernel parameters, but this was beyond the scope of this work."}, {"heading": "A.2. Additional Definitions", "text": "The space L2(\u03a0) is defined to be the set of \u03a0-measurable functions f : X \u2192 R such that the Lebesgue integral\u222b\nX f2 d\u03a0\nexists and is finite.\nFor a multi-index \u03b1 = (\u03b11, . . . , \u03b1d) define |\u03b1| = \u03b11 + \u00b7 \u00b7 \u00b7 + \u03b1d. The (standard) Sobolev space of order s \u2208 N is denoted\nHs(\u03a0) = {f : X \u2192 R s.t. (\u2202x1) \u03b11 . . . (\u2202xd) \u03b1df \u2208 L2(\u03a0) \u2200 |\u03b1| \u2264 s}.\nThis space is equipped with norm\n\u2016f\u2016Hs(\u03a0) = ( \u2211 |\u03b1|\u2264s \u2016(\u2202x1)\u03b11 . . . (\u2202xd)\u03b1df\u20162L2(\u03a0) )1/2 .\nTwo normed spaces (F , \u2016 \u00b7 \u2016) and (F , \u2016 \u00b7 \u2016\u2032) are said to be \u2018norm equivalent\u2019 if there exists 0 < c <\u221e such that\nc\u22121\u2016f\u2016\u2032 \u2264 \u2016f\u2016 \u2264 c\u2016f\u2016\u2032\nfor all f \u2208 F ."}, {"heading": "A.3. Theoretical Results", "text": ""}, {"heading": "A.3.1. PROOF OF THEOREM 1", "text": "Proof. From Thm. 11.13 in Wendland (2004) we have that there exist constants 0 < ck <\u221e, h0 > 0 such that\n|f\u0302(x)\u2212 f(x)| \u2264 ckhsn\u2016f\u2016H (7)\nfor all x \u2208 X , provided hn < h0, where\nhn = sup x\u2208X min i=1,...,n\n\u2016x\u2212 xi\u20162.\nUnder the hypotheses, we can suppose that the deterministic states x1, . . . ,xm ensure hm < h0. Then Eqn. 7 holds\nfor all n > m, where the xm+1, . . . ,xn are independent draws from \u03a0\u2032. It follows that\n|\u03a0\u0302(f)\u2212\u03a0(f)| \u2264 sup x\u2208X |f\u0302(x)\u2212 f(x)|\n\u2264 ckhsn\u2016f\u2016H.\nNext, Lem. 1 in Oates et al. (2016) establishes that, under the present hypotheses on X and \u03a0\u2032, there exists 0 < c\u03a0\u2032, <\u221e such that\nE[h2sn ] \u2264 c\u03a0\u2032, m\u22122s/d+\nfor all > 0, where c\u03a0\u2032, is independent of n.\nCombining the above results produces\nE[\u03a0\u0302(f)\u2212\u03a0(f)]2 \u2264 c2kE[h2sn ]\u2016f\u20162H \u2264 c2kc\u03a0\u2032, m\u22122s/d+ \u2016f\u20162H\nas required, with ck,\u03a0\u2032, = ckc 1/2 \u03a0\u2032, ."}, {"heading": "A.3.2. PROOF OF THEOREM 2", "text": "Proof. The Cauchy-Schwarz result for kernel mean embeddings (Smola et al., 2007) gives\n|\u03a0\u0302(f)\u2212\u03a0(f)| (8)\n\u2264 \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 wik(\u00b7,xi)\u2212 \u222b X k(\u00b7,x)\u03a0(dx) \u2225\u2225\u2225\u2225\u2225 H \u2016f\u2016H.\nConsider the first term above. Since H is dense in L2(\u03a0), it follows that \u03a31/2 (the unique positive self-adjoint square root of \u03a3) is an isometry from L2(\u03a0) to H. Now, since k(\u00b7,x) \u2208 H, there exists a unique element \u03c8(\u00b7,x) \u2208 L2(\u03a0) such that \u03a31/2\u03c8(\u00b7,x) = k(\u00b7,x). Then we have that\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 wik(\u00b7,xi)\u2212 \u222b X k(\u00b7,x)\u03a0(dx) \u2225\u2225\u2225\u2225\u2225 H\n= \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 wi\u03a3 1/2\u03c8(\u00b7,xi)\u2212 \u222b X \u03a31/2\u03c8(\u00b7,x)\u03a0(dx) \u2225\u2225\u2225\u2225\u2225 H\n= \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 wi\u03c8(\u00b7,xi)\u2212 \u222b X \u03c8(\u00b7,x)\u03a0(dx) \u2225\u2225\u2225\u2225\u2225 L2(\u03a0) .\nFor f \u2208 L2(\u03a0), we have f \u2208 H if and only if\nf = \u222b X g(x)\u03c8(\u00b7,x)\u03a0(dx) (9)\nfor some g \u2208 L2(\u03a0), in which case \u2016f\u2016H is equal to the infimum of \u2016g\u2016L2(\u03a0) under all such representations g. In particular, it follows that \u2016f\u2016H = 1 for the particular choice with g(x) = 1 for all x \u2208 X .\nUnder the hypothesis on n, Prop. 1 of Bach (2015) established that when x1, . . . ,xn \u223c \u03a0B are independent, then\nsup \u2016f\u2016H\u22641 inf \u2016\u03b2\u201622\u2264 4 n \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 \u03b2i \u03c0B(xi)1/2 \u03c8(\u00b7,xi)\u2212 f \u2225\u2225\u2225\u2225\u2225 2\nL2(\u03a0)\n\u2264 4\u03bb\nwith probability at least 1\u2212\u03b4. Fixing the function f in Eqn. 9 leads to the statement that\ninf \u2016\u03b2\u201622\u2264 4 n \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 \u03b2i \u03c0B(xi)1/2 \u03c8(\u00b7,xi)\u2212 \u222b X \u03c8(\u00b7,x)\u03a0(dx) \u2225\u2225\u2225\u2225\u2225 2\nL2(\u03a0)\nis at most 4\u03bb with probability at least 1 \u2212 \u03b4. The infimum over \u2016\u03b2\u201622 \u2264 4/n can be replaced with an unconstrained infimum over Rn to obtain the weaker statement that\ninf \u03b2\u2208Rn \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 \u03b2i \u03c0B(xi)1/2 \u03c8(\u00b7,xi)\u2212 \u222b X \u03c8(\u00b7,x)\u03a0(dx) \u2225\u2225\u2225\u2225\u2225 2\nL2(\u03a0)\nis at most 4\u03bb with probability at least 1 \u2212 \u03b4. Now, recall from Sec. 2.1 that the KQ weights w are characterised through the solution \u03b2\u2217 to this optimisation problem as wi = \u03b2 \u2217 i \u03c0B(xi)\n\u22121/2. It follows that\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 wi\u03c8(\u00b7,xi)\u2212 \u222b X \u03c8(\u00b7,x)\u03a0(dx) \u2225\u2225\u2225\u2225\u2225 2\nL2(\u03a0)\n\u2264 4\u03bb\nwith probability at least 1 \u2212 \u03b4. Combining this fact with Eqn. 8 completes the proof."}, {"heading": "A.3.3. \u03a0B FOR THE EXAMPLE OF FIGURE 1", "text": "In this section we consider scope to derive \u03a0B in closedform for the example of Fig. 1. The following will be used:\nProposition 1 (Prop. 1 in Shi et al. (2009)). Let X = R, \u03a0 = N(\u00b5, \u03c32) and k(x, x\u2032) = exp(\u2212(x \u2212 x\u2032)2/`2). Define \u03b2 = 4\u03c32/`2 and denote the jth Hermite polynomial as Hj(x). Then the eigenvalues \u00b5j and corresponding eigenfunctions ej of the integral operator \u03a3 are\n\u00b5j =\n\u221a 2\n(1 + \u03b2 + \u221a 1 + 2\u03b2) \u00d7 ( \u03b2 1 + \u03b2 + \u221a 1 + 2\u03b2 )j and\nej(x) = (1 + 2\u03b2)1/8\u221a\n2jj! exp\n( \u2212 (x\u2212 \u00b5) 2\n2\u03c32\n\u221a 1 + 2\u03b2 \u2212 1\n2 ) \u00d7Hj ((1 4 + \u03b2 2 )1/4x\u2212 \u00b5 \u03c3\n) for j \u2208 {0, 1, 2, . . . }.\nProposition 2 (Ex. 6.8 in Temme (1996), p.167). The bilinear generating function for Hermite polynomials is\n\u221e\u2211 j=0 tj j! Hj(x)Hj(z)\n= 1\u221a\n1\u2212 4t2 exp\n( x2 \u2212 (x\u2212 2zt) 2\n1\u2212 4t2\n) .\nProposition 3. For the example in Fig. 1 we have\n\u03c0B(x;\u03bb) \u221d\nexp(\u2212x2) \u221e\u2211 j=0\n1 1 + \u03bb2j+1 1 2jj! H2j (\u221a3 2 x ) .\nProof. For the example of Fig. 1, in the notation of Prop. 1, we have \u00b5 = 0, \u03c3 = 1, ` = 1 and \u03b2 = 4. Thus\n\u00b5j = (1\n2 )j+1 ej(x) 2 = \u221a\n3 exp(\u2212x2) 1 2jj! H2j (\u221a3 2 x )\nand so \u03c0B(x;\u03bb) \u221d \u2211 j \u00b5j \u00b5j + \u03bb e2j (x)\n\u221d exp(\u2212x2) \u221e\u2211 j=0\n1 1 + \u03bb2j+1 1 2jj! H2j (\u221a3 2 x )\nas required.\nTo the best of our knowledge, the expression for \u03a0B in Prop. 3 does not admit a closed form. This poses a practical challenge. However, some limited insight is available through basic approximations:\n\u2022 For large values of \u03bb we have 1 + \u03bb2j+1 \u2248 \u03bb2j+1 for all j \u2208 {0, 1, 2, . . . }, from which we obtain\n\u03c0B(x;\u03bb) \u221d\u223c exp(\u2212x 2) \u221e\u2211 j=0 1 4jj! H2j (\u221a3 2 x )\n\u221d exp(\u2212x2) exp(x2) = 1,\nwhere the second step made use of Prop. 2. Thus when large integration errors are tolerated, \u03a0B requires that we take the states xi to be approximately uniform over X (of course, this limiting distribution is improper and serves only for illustration).\n\u2022 For small values of \u03bb, the series in Prop. 3 is dominated by the first m terms such that j < m if and only if \u03bb2j+1 < 1. Indeed, for j \u2264 m we have\n1 + \u03bb2j+1 \u2248 1. Thus we have a computable approximation\n\u03c0B(x;\u03bb) \u221d\u223c exp(\u2212x 2) m\u2211 j=0 1 2jj! H2j (\u221a3 2 x )\nwhere m = d\u2212 log2(\u03bb)e. Empirical results (not shown) indicate that this is not a useful approximation from a practical standpoint, since at finite m the tails of the approximation are explosive (due to the use of a polynomial basis).\nThe approximation method in Bach (2015) was also used to obtain the numerical approximation to \u03a0B shown in Fig. 6. This appears to support the intuition that it is beneficial to over-sample from the tails of \u03a0.\nTo finish, we remark that Prop. 3 implies that the integration error in this example scales as\n\u221a \u00b5n \u223c 2\u2212n/2\nas n \u2192 \u221e when samples are drawn from \u03a0B. This agrees with both intuition and empirical results that concern approximation with exponentiated quadratic kernels."}, {"heading": "A.3.4. ADDITIONAL THEORETICAL MATERIAL", "text": "As mentioned in the Main Text, the worst-case error en({xj}nj=1) can be computed in closed form:\nen({xj}nj=1)2 = \u03a0\u2297\u03a0(k)\u2212 2w>Kz +w>Kw\nHere we have defined \u03a0\u2297\u03a0(k) = \u222b\u222b X\u00d7X k(x,x\u2032) \u03a0\u2297\u03a0(dx\u00d7 dx\u2032)\nwhere \u03a0\u2297\u03a0 is the product measure of \u03a0 with itself.\nNext, we report a result which does not address KQ itself, but considers importance sampling methods for integration\nof functions in a Hilbert space. The following is due to Plaskota et al. (2009); Hinrichs (2010) and we provide an elementary proof of their result:\nTheorem 3. The assumptions of Sec. 2.4 are taken to hold. In addition, we assume that distributions \u03a0,\u03a0\u2032 admit densities \u03c0, \u03c0\u2032. Introduce importance sampling estimators of the form\n\u03a0\u0302IS(f) = 1\nn n\u2211 i=1 f(xi) \u03c0(xi) \u03c0\u2032(xi) ,\nwhere x1, . . . ,xn \u223c \u03a0\u2032 are independent, and consider the distribution \u03a0\u2032 that minimises\nsup f\u2208F\n\u221a E[\u03a0\u0302IS(f)\u2212\u03a0(f)]2.\nFor F = {f} we have that \u03a0\u2032 is \u03c0\u2032(x) \u221d |f(x)|\u03c0(x), while for F = {f \u2208 H : \u2016f\u2016H \u2264 1} we have that \u03a0\u2032 is \u03c0\u2032(x) \u221d \u221a k(x,x)\u03c0(x).\nProof. The first result, for F = {f} is well-known; e.g. Thm. 3.3.4 in Robert and Casella (2013).\nFor the second case, where F is the unit ball inH, we start by establishing a (tight) upper bound for the supremum of f2 over f \u2208 F :\n|f(x)| = \u2223\u2223\u3008f, k(\u00b7,x)\u3009H\u2223\u2223\n\u2264 \u2016f\u2016H\u2016k(\u00b7,x)\u2016H = \u2016f\u2016H \u221a \u3008k(\u00b7,x), k(\u00b7,x)\u3009H\n= \u2016f\u2016H \u221a k(x,x)\nwhere the inequality here is Cauchy-Schwarz. Squaring both sides and taking the supremum over f \u2208 F gives\nsup f\u2208F f(x)2 \u2264 sup f\u2208F \u2016f\u20162H k(x,x) = k(x,x). (10)\nThis is in fact an equality, since for given x \u2208 X we can take f(x\u2032) = k(x\u2032,x)/ \u221a k(x,x) which has \u2016f\u2016H = 1 and f(x)2 = k(x,x).\nOur objective is expressed as\nsup f\u2208F\n\u221a E[\u03a0\u0302IS(f)\u2212\u03a0(f)]2 = sup\nf\u2208F\n1\u221a n Std (f\u03c0 \u03c0\u2032 ; \u03a0\u2032 )\nand since Std (f\u03c0 \u03c0\u2032 ; \u03a0\u2032 )2 = \u03a0\u2032 ((f\u03c0 \u03c0\u2032 )2) \u2212\u03a0\u2032 (f\u03c0 \u03c0\u2032 )2 we thus aim to minimise\nsup f\u2208F\n\u03a0\u2032 ((f\u03c0\n\u03c0\u2032 )2) over \u03a0\u2032 \u2208 P(F \u00b7 d\u03a0/d\u03a0\u2032). (Here F \u00b7 d\u03a0/d\u03a0\u2032 denotes the set of functions of the form f \u00b7 d\u03a0/d\u03a0\u2032 such that f \u2208 F .)\nCombining Eqns. 10 and A.3.4, we have\nsup f\u2208F\n\u03a0\u2032 ((f\u03c0\n\u03c0\u2032\n)2) \u2264 \u03a0\u2032 ( sup f\u2208F (f\u03c0 \u03c0\u2032 )2) = \u03a0\u2032 ( k(\u00b7, \u00b7) ( \u03c0(\u00b7) \u03c0\u2032(\u00b7)\n)2) As before, this is in fact an equality, as can be seen from f(x) = \u221a k(x,x).\nFrom Jensen\u2019s inequality, \u03a0\u2032 ( k(\u00b7, \u00b7) ( \u03c0(\u00b7) \u03c0\u2032(\u00b7) )2) \u2265 ( \u03a0\u2032 (\u221a k(\u00b7, \u00b7) \u03c0(\u00b7) \u03c0\u2032(\u00b7) ))2 (11)\n= ( \u03a0 (\u221a k(\u00b7, \u00b7) ))2 .\nSince the right hand side is independent of \u03a0\u2032, a choice of \u03a0\u2032 for which Eqn. 11 is an equality must be a minimiser of Eqn. A.3.4. It remains just to verify this fact for \u03c0\u2032(x) =\u221a k(x,x)\u03c0(x)/C, where the normalising constant is C =\n\u03a0( \u221a k(\u00b7, \u00b7)). For this choice\n\u03a0\u2032 ( k(\u00b7, \u00b7) ( \u03c0(\u00b7) \u03c0\u2032(\u00b7) )2) = \u03a0\u2032(C2)\n= (\u03a0( \u221a k(\u00b7, \u00b7)))2\nas required.\nA.4. Implementation of test(R < Rmin)\nHere we provide details for how the criterion R < Rmin was tested. The problem with the naive approach of comparingR estimated at ti\u22121 directly withR estimated at ti is that Monte Carlo error can lead to an incorrect impression thatR is increasing, when it is in fact decreasing, and cause the algorithm to terminate when estimation is poor (see Fig. 7 and note the jaggedness of the estimated R curve as a function of inverse temperature t). Our solution was to apply a least-squares linear smoother to the estimates for R over 5 consecutive temperatures. This approach, denoted test, illustrated in Fig. 7, determines whether the gradient of the linear smoother is positive or negative, and in this way we are able to provide robustness to Monte Carlo error in the termination criterion. To be precise, the algorithm requires at least 5 temperature evaluations before termination is considered (Fig. 7; left) and terminates when the gradient of the linear smoother becomes positive for the first time (Fig. 7; right). The success of this strategy was established in Fig. 9 later in the Appendix."}, {"heading": "A.5. Experimental Results", "text": "A.5.1. IMPLEMENTATION OF SIMULATION STUDY\nDenote by N(x|\u00b5,\u03a3) the p.d.f. of the multivariate Gaussian distribution with mean \u00b5 and covariance \u03a3. Furthermore, we denote by \u03a3\u03c3 the diagonal covariance matrix\nwith diagonal element \u03c32. Then elementary manipulation of Gaussian densities produces:\nk(x,y) := exp ( \u2212 \u2211d j=1 ( xj \u2212 yj )2 l2 ) = ( \u221a \u03c0l)d\u03c6 ( x|y,\u03a3l/\u221a2\n) \u2207lk(x, y) := 2 \u2211d j=1(xj \u2212 yj)2\nl3 k(x,y)\n\u03a0[k(\u00b7,x)] := ( \u221a \u03c0l)dN ( x|0,\u03a3\u03c3 + \u03a3l/\u221a2 ) \u03a0\u2297\u03a0(k) := ( \u221a \u03c0l)dN ( 0|0,\u03a3\u221a2\u03c3 + \u03a3l/\u221a2\n)"}, {"heading": "A.5.2. DEPENDENCE ON PARAMETERS FOR THE SIMULATION STUDY", "text": "For the running illustration with f(x) = 1 + sin(x), \u03a0 = N(0, 1), \u03a0\u2032 = N(0, \u03c32) and k(x, x\u2032) = exp(\u2212(x \u2212 x\u2032)2/`2), we explored how the RMSE of KQ depends on the choice of both \u03c3 and `. Here we go beyond the results presented in Fig. 2, which considered fixed n, to now consider the simultaneous choice of both \u03c3, ` for varying n. Note that in these numerical experiments the kernel matrix inverse K\u22121 was replaced with the regularised inverse (K + \u03bbI)\u22121 that introduces a small \u2018nugget\u2019 term \u03bb > 0 for stabilisation. Results, shown in Fig. 8, demonstrate two principles that guided the methodological development in this paper:\n\u2022 Length scales ` that are \u2018too small\u2019 to learn from n samples do not permit good approximations f\u0302 and lead in practice to high RMSE. At the same time, if ` is taken to be \u2018too large\u2019 then efficient approximation at size n will also be sacrificed. This is of course well understood from a theoretical perspective and is borne out in our empirical results. These results motivated extension of SMC-KQ to SMC-KQ-KL.\n\u2022 In general the \u2018sweet spot\u2019, where \u03c3 and ` lead to minimal RMSE, is quite small. However, the problem of optimal choice for \u03c3 and ` does not seem to become\nmore or less difficult as n increases. This suggests that a method for selection of \u03c3 (and possibly also of `) ought to be effective regardless of the number n of states that will be used."}, {"heading": "A.5.3. ADDITIONAL RESULTS FOR THE SIMULATION STUDY", "text": "To understand whether the termination criterion of Sec. 3.5 was suitable (and, by extension, to examine the validity of the convexity ansatz in Sec. 3.2), in Fig. 9 we presented histograms for both estimated and actual optimal (inverse) temperature parameter t\u2217. Results supported the use of the criterion, in the form described above for test.\nIn Fig. 10 reports the dependence of performance on the choice of initial distribution \u03a00. There was relatively little influence on the RMSE obtained by the method for this wide range of initial distribution, which supports the purported robustness of the method.\nWe also test the method on more complex integrands in Fig. 11: f(x) = 1 + sin(4\u03c0x) and f(x) = 1 + sin(8\u03c0x). These are more challenging for KQ compared to the illustration in the Main Text, since they are more difficult to interpolate due to their higher periodicity. However, SMC-KQ still manages to adapt to the complexity of the integrand and performs as well as the best importance sampling distribution (\u03c3 = 2).\nAs an extension, we also study the robustness to the dimensionality to the problem. In problem, we consider the generalisation of our main test function to f : Rd \u2192 R given by f(x) = 1 + \u220fd j=1 sin(2\u03c0xj). Notice that the integral can still be computed analytically and equals 1. We present results for d = 2 and d = 3 in Fig. 12. These two cases are more challenging for both the KQ and SMC-KQ methods, since the higher dimension implies a slower convergence rate. Once again, we notice that SMC-KQ manages to adapt to the complexity of the problem at hand, and provides improved performance on simpler sampling distributions.\nFinally, we considered replacing the independent samples xj \u223c \u03a0 with samples drawn from a quasi-random point sequence. Fig. 13 reports results where draws from N(0, 1) were produced based on a Halton quasi-random number generator. In this case, the performance is improved by up to 10 orders of magnitude in MSE when the sampling is done with respect to a range of tempered sampling distribution (here N(0, 32)). This suggests that a SQMC approach (Gerber and Chopin, 2015) could provide further improvement and this suggested for future work.\nA.5.4. IMPLEMENTATION OF STEIN\u2019S METHOD\nFollowing Oates et al. (2017) we considered the Stein operator\nS[f ](\u03b8) := [\u2207\u03b8 +\u2207 log \u03c0(\u03b8)][f ](\u03b8)\nand denote the score function by uj(\u03b8) = \u2207\u03b8j log \u03c0(\u03b8). Here \u03c0 is the p.d.f. for \u03a0. Applying the Stein operator to each argument of a base kernel kb, and adding a constant, gives produces the new kernel:\nk(\u03b8,\u03c6) := 1 + d\u2211 j=1 [\u2207\u03b8j\u2207\u03c6jkb(\u03b8,\u03c6) +uj(\u03b8)\u2207\u03c6jkb(\u03b8,\u03c6) +uj(\u03c6)\u2207\u03b8jkb(\u03b8,\u03c6) +uj(\u03b8)uj(\u03c6)kb(\u03b8,\u03c6)]\nwhich we will use for our KQ estimator. Using integration by parts, we can easily check that \u03a0[k(\u00b7,\u03b8)] = 1 and \u03a0 \u2297 \u03a0(k) = 1. In this experiment, the base kernel was taken to be Gaussian: kb(\u03b8,\u03c6) = exp(\u2212 \u2211d j=1(\u03b8j \u2212 \u03c6j)2/`2j ). We\nobtained the derivatives:\ndk(\u03b8,\u03c6) d\u03b8j = \u2212 2 `2j (\u03b8j \u2212 \u03c6j)k(\u03b8,\u03c6)\ndk(\u03b8,\u03c6)\nd\u03c6j =\n2 `2j (\u03b8j \u2212 \u03c6j)k(\u03b8,\u03c6)\ndk(\u03b8,\u03c6)\nd\u03b8jd\u03c6j =\n( 2`2j \u2212 4(\u03b8j \u2212 \u03c6j)2 ) `4j k(\u03b8,\u03c6)\nFurthermore, we can obtain expressions for the score function for posterior densities as follows:\nuj(\u03b8) = d\nd\u03b8j log \u03c0(\u03b8) +\nd\nd\u03b8j log \u03c0(y|\u03b8)."}, {"heading": "A.6. Algorithms and Implementation", "text": ""}, {"heading": "A.6.1. SMC SAMPLER", "text": "In Alg. 2 the standard SMC scheme is presented. Resampling occurs when the effective sample size, \u2016w\u2016\u221222\ndrops below a fraction \u03c1 of the total number N of particles. In this work we took \u03c1 = 0.95 which is a common default.\nAlgorithm 2 Sequential Monte Carlo Iteration function SMC({(wj ,xj)}Nj=1, ti, ti\u22121, \u03c1) input {(wj ,xj)}Nj=1 (particle approx. to \u03a0i\u22121) input ti (next inverse-temperature) input ti\u22121 (previous inverse-temperature) input \u03c1 (re-sample threshold) w\u2032j \u2190 wj \u00d7 [\u03c0(xj)/\u03c00(xj)]ti\u2212ti\u22121 (\u2200j \u2208 1 : N ) w\u2032 \u2190 w\u2032/\u2016w\u2032\u20161 (normalise weights) if \u2016w\u2032\u2016\u221222 < N \u00b7 \u03c1 then a \u223c Multinom(w\u2032) x\u2032j \u2190 xa(j) (re-sample \u2200j \u2208 1 : N ) w\u2032j \u2190 N\u22121 (reset weights \u2200j \u2208 1 : N )\nend if x\u2032j \u223c Markov(x\u2032j ; \u03a0i, {(wj ,xj)}Nj=1) (Markov update \u2208 1 : N ) return {(w\u2032j ,x\u2032j)}Nj=1 (particle approx. to \u03a0i)\nDenote\nq(x, \u00b7; {(wj ,xj)}Nj=1) = N(\u00b7;\u00b5,\u03a3)\n\u00b5 = N\u2211 j=1 wjxj\n\u03a3 = N\u2211 j=1 wj(xj \u2212 \u00b5)(xj \u2212 \u00b5)>.\nThe above standard adaptive independence proposal was used within a Metropolis-Hastings Markov transition:\nAlgorithm 3 Markov Iteration function Markov(x, \u03c0, {(wj ,xj)}Nj=1) input x (current state) input \u03c0 (density of invar. dist.) x\u2217 \u223c q(x,x\u2217; {(wj ,xj)}Nj=1) (propose)\nr \u2190 \u03c0i(x \u2217)q(x\u2217,x; {(wj ,xj)}Nj=1) \u03c0i(x)q(x,x\u2217; {(wj ,xj)}Nj=1)\nu \u223c Unif(0, 1) if u < r then x\u2190 x\u2217 (accept) end ifreturn x (next state)"}, {"heading": "A.6.2. CHOICE OF TEMPERATURE SCHEDULE", "text": "Following Zhou et al. (2016) we employed an adaptive temperature schedule construction. This was based on the conditional effective sample size of the SMC particle set, estimated as follows:\nAlgorithm 4 Conditional Effective Sample Size function CESS({(wj ,xj)}Nj=1, t) input {(wj ,xj)}Nj=1 (particle approx. \u03a0i\u22121) input t (candidate next inverse-temperature) zj \u2190 [\u03c0(xj)/\u03c00(xj)]ti\u2212ti\u22121 (\u2200j \u2208 1 : N )\nE \u2190 N (\u2211N\nj=1 wjzj )2 /(\u2211N j=1 wjz 2 j ) return E (est\u2019d. cond. ESS)\nThe specific construction for the temperature schedule is detailed in Alg. 5 below and makes use of a Sequential Least Squares Programming algorithm:\nAlgorithm 5 Adaptive Temperature Iteration function temp({(wj ,xj)}Nj=1, ti\u22121, \u03c1,\u2206) input {(wj ,xj)}Nj=1 (particle approx. \u03a0i\u22121) input ti\u22121 (current inverse-temperature) input \u03c1 (re-sample threshold) input \u2206 (max. grid size, default \u2206 = 0.1) t\u2190 solve(CESS({(wj ,xj)}Nj=1, t) = N \u00b7 \u03c1) (binary search in [ti\u22121, 1]) ti \u2190 min{ti\u22121 + \u2206, t} return ti (next inversetemperature)"}, {"heading": "A.6.3. TERMINATION CRITERION", "text": "For SMC-KQ we estimated an upper bound on the worst case error in the unit ball of the Hilbert space H. This was computed as follows, using a bootstrap algorithm:\nAlgorithm 6 Termination Criterion function crit(\u03a0, k, {xj}Nj=1) input \u03a0 (target disn.) input k (kernel) input {xj}Nj=1 (collection of states) R2 \u2190 0 e0 \u2190 \u222b\u222b X\u00d7X k(x,x\n\u2032)\u03a0\u2297\u03a0(dx\u00d7 dx\u2032) (in\u2019l error) for m = 1,. . . ,M do x\u0303j \u223c Unif({xj}Nj=1) (\u2200j \u2208 1 : n) zj \u2190 \u222b X k(\u00b7, x\u0303j)d\u03a0 (k\u2019l mean eval. \u2200j \u2208 1 : n)\nKj,j\u2032 \u2190 k(x\u0303j , x\u0303j\u2032) (kernel eval. \u2200j, j\u2032 \u2208 1 : n) w \u2190 zTK\u22121 (KQ weights) e2n \u2190 w>Kw \u2212 2w>z + e20 R2 \u2190 R2 + e2nM\u22121\nend for return R (est\u2019d error)\nNote that this could be slightly improved using a weighted bootstrap approach.\nFor SMC-KQ-KL an empirical upper bound on integration error was estimated. This requires that the norm \u2016f\u2016H be estimated, which was achieved as follows:\nAlgorithm 7 Termination Crit. + Kernel Learning function crit-KL(f,\u03a0, k, {xj}Nj=1) input f (integrand) input \u03a0 (target disn.) input k (kernel) input {xj}Nj=1 (collection of states) R2 \u2190 0 e0 \u2190 \u222b\u222b X\u00d7X k(x,x\n\u2032)\u03a0\u2297\u03a0(dx\u00d7 dx\u2032) (in\u2019l error) for m = 1,. . . ,M do x\u0303j \u223c Unif({xj}Nj=1) (\u2200j \u2208 1 : n) fj \u2190 f(x\u0303j) (function eval. \u2200j \u2208 1 : n) zj \u2190 \u222b X k(\u00b7, x\u0303j)d\u03a0 (k\u2019l mean eval. \u2200j \u2208 1 : n)\nKj,j\u2032 \u2190 k(x\u0303j , x\u0303j\u2032) (kernel eval. \u2200j, j\u2032 \u2208 1 : n) w \u2190 zTK\u22121 (KQ weights) e2n \u2190 w>Kw \u2212 2w>z + e20 R2 \u2190 R2 + e2nM\u22121\nend for zj \u2190 \u222b X k(\u00b7,xj)d\u03a0 (kernel mean eval. \u2200j \u2208 1 : n) Kj,j\u2032 \u2190 k(xj ,xj\u2032) (kernel eval. \u2200j, j\u2032 \u2208 1 : n) w \u2190 zTK\u22121 (KQ weights) S2 \u2190 R2 \u00d7w>Kw return S (est\u2019d error bound)\nIn Alg. 7 the literal interpretation, that f is re-evaluated on values of xj which have been previously examined, is clearly inefficient. In practice such function evaluations were cached and then do not contribute further to the total number of function evaluations that are required in the algorithm."}, {"heading": "A.6.4. KERNEL LEARNING", "text": "A generic approach to select kernel parameters is the maximum marginal likelihood method:\nAlgorithm 8 Parameter Update function kern-param(f , {xj}nj=1, k\u03b8) input f (integrand evals.) input {xj}nj=1 (associated states) input k\u03b8 (parametric kernel) \u03b8\u2032 \u2190 arg min\u03b8 f\n>K\u22121\u03b8 f + log |K\u03b8| (numer. opt.) (s.t. K\u03b8,j,j\u2032 = k\u03b8(xj ,xj\u2032)) return \u03b8\u2032 (optimal params)\nA.6.5. IMPLEMENTATION OF SMC-KQ-KL\nOur final algorithm to present is the full implementation for SMC-KQ-KL:\nAlgorithm 9 SMC for KQ with Kernel Learning function SMC-KQ-KL(f,\u03a0, k\u03b8,\u03a00, \u03c1, n,N) input f (integrand) input \u03a0 (target disn.) input k\u03b8 (parametric kernel) input \u03a00 (reference disn.) input \u03c1 (re-sample threshold) input n (num. func. evaluations) input N (num. particles) i\u2190 0; ti \u2190 0; Rmin \u2190\u221e x\u2032j \u223c \u03a00 (initialise states \u2200j \u2208 1 : N ) w\u2032j \u2190 N\u22121 (initialise weights \u2200j \u2208 1 : N ) \u03b8\u2032 \u2190 kern-param(f, {x\u2032j}nj=1) (kernel params) R\u2190 crit-KL(f,\u03a0, k\u03b8\u2032 , {x\u2032j}Nj=1) (est\u2019d error) while test(R < Rmin) and ti < 1 do i\u2190 i+ 1; Rmin \u2190 R; \u03b8 \u2190 \u03b8\u2032 {(wj ,xj)}Nj=1 \u2190 {(w\u2032j ,x\u2032j)}Nj=1 ti \u2190 temp({(wj ,xj)}Nj=1, ti\u22121) (next temp.) {(w\u2032j ,x\u2032j)}Nj=1 \u2190 SMC({(wj ,xj)}Nj=1, ti, ti\u22121, \u03c1) (next particle approx.) \u03b8\u2032 \u2190 kern-param(f, {x\u2032j}nj=1) (kernel params) R\u2190 crit-KL(f,\u03a0, k\u03b8\u2032 , {x\u2032j}Nj=1) (est\u2019d error)\nend while fj \u2190 f(xj) (function eval. \u2200j \u2208 1 : n) zj \u2190 \u222b X k\u03b8(\u00b7,xj)d\u03a0 (kernel mean eval. \u2200j \u2208 1 : n) Kj,j\u2032 \u2190 k\u03b8(xj ,xj\u2032) (kernel eval. \u2200j, j\u2032 \u2208 1 : n) \u03a0\u0302(f) \u2190 z>K\u22121f (eval. KQ estimator) return \u03a0\u0302(f) (estimator)\nAs stated here, Alg. 9 is inefficient as function evaluations that are produced in the kern-param and crit-KL components are not included in the KQ estimator \u03a0\u0302(f). Thus a trivial modification is to store all function evaluations (fj ,xj) that are produced and to include all of these in the ultimate KQ estimator. This was the approach taken in our experiments that involved SMC-KQ-KL. However, since it is somewhat cumbersome to include in the pseudocode, we have not made this explicit in the notation. Our reported results are on a per-function-evaluation basis and so we do adjust for this detail in our reported comparisons."}], "references": [{"title": "Sharp analysis of low-rank kernel matrix approximations", "author": ["F. Bach"], "venue": "In Proc. I. Conf. Learn. Theory,", "citeRegEx": "Bach.,? \\Q2013\\E", "shortCiteRegEx": "Bach.", "year": 2013}, {"title": "On the equivalence between kernel quadrature rules and random features", "author": ["F. Bach"], "venue": null, "citeRegEx": "Bach.,? \\Q2015\\E", "shortCiteRegEx": "Bach.", "year": 2015}, {"title": "On approximate computation of integrals", "author": ["N.S. Bakhvalov"], "venue": "Vestnik MGU, Ser. Math. Mech. Astron. Phys. Chem.,", "citeRegEx": "Bakhvalov.,? \\Q1959\\E", "shortCiteRegEx": "Bakhvalov.", "year": 1959}, {"title": "Reproducing kernel Hilbert spaces in probability and statistics", "author": ["A. Berlinet", "C. Thomas-Agnan"], "venue": "Springer Science & Business Media,", "citeRegEx": "Berlinet and Thomas.Agnan.,? \\Q2011\\E", "shortCiteRegEx": "Berlinet and Thomas.Agnan.", "year": 2011}, {"title": "Frank-Wolfe Bayesian quadrature: Probabilistic integration with theoretical guarantees", "author": ["F-X. Briol", "C.J. Oates", "M. Girolami", "M.A. Osborne"], "venue": "In Adv. Neur. Inf. Proc. Sys.,", "citeRegEx": "Briol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Briol et al\\.", "year": 2015}, {"title": "Probabilistic integration: A role for statisticians in numerical analysis", "author": ["F-X. Briol", "C.J. Oates", "M. Girolami", "M.A. Osborne", "D. Sejdinovic"], "venue": null, "citeRegEx": "Briol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Briol et al\\.", "year": 2015}, {"title": "A sequential particle filter method for static models", "author": ["N. Chopin"], "venue": "Biometrika, 89(3):539\u2013552,", "citeRegEx": "Chopin.,? \\Q2002\\E", "shortCiteRegEx": "Chopin.", "year": 2002}, {"title": "Optimal weighted leastsquares methods", "author": ["A. Cohen", "G. Migliorati"], "venue": null, "citeRegEx": "Cohen and Migliorati.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Migliorati.", "year": 2016}, {"title": "Sequential monte carlo samplers", "author": ["P. Del Moral", "A. Doucet", "A. Jasra"], "venue": "J. R. Stat. Soc. Ser. B. Stat. Methodol.,", "citeRegEx": "Moral et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moral et al\\.", "year": 2006}, {"title": "Bayesian Numerical Analysis, volume IV of Statistical Decision Theory and Related Topics, pages 163\u2013175", "author": ["P. Diaconis"], "venue": null, "citeRegEx": "Diaconis.,? \\Q1988\\E", "shortCiteRegEx": "Diaconis.", "year": 1988}, {"title": "Digital nets and sequences: Discrepancy Theory and Quasi\u2013Monte Carlo Integration", "author": ["J. Dick", "F. Pillichshammer"], "venue": null, "citeRegEx": "Dick and Pillichshammer.,? \\Q2010\\E", "shortCiteRegEx": "Dick and Pillichshammer.", "year": 2010}, {"title": "High-dimensional integration: The quasi-Monte Carlo way", "author": ["J. Dick", "F.Y. Kuo", "I.H. Sloan"], "venue": "Acta Numerica,", "citeRegEx": "Dick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dick et al\\.", "year": 2013}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["P. Drineas", "M. Magdon-Ismail", "M.W. Mahoney", "D.P. Woodruff"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Drineas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2012}, {"title": "Parameter multi-domain \u2018hp", "author": ["J.L. Eftang", "B. Stamm"], "venue": "empirical interpolation. I. J. Numer. Methods in Eng.,", "citeRegEx": "Eftang and Stamm.,? \\Q2012\\E", "shortCiteRegEx": "Eftang and Stamm.", "year": 2012}, {"title": "Sequential quasi Monte Carlo", "author": ["M. Gerber", "N. Chopin"], "venue": "J. R. Statist. Soc. B,", "citeRegEx": "Gerber and Chopin.,? \\Q2015\\E", "shortCiteRegEx": "Gerber and Chopin.", "year": 2015}, {"title": "Sampling for inference in probabilistic models with fast Bayesian quadrature", "author": ["T. Gunter", "R. Garnett", "M. Osborne", "P. Hennig", "S. Roberts"], "venue": "In Adv. Neur. Inf. Proc. Sys.,", "citeRegEx": "Gunter et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gunter et al\\.", "year": 2014}, {"title": "Coherence motivated sampling and convergence analysis of least squares polynomial Chaos regression", "author": ["J. Hampton", "A. Doostan"], "venue": "Comput. Methods Appl. Mech. Engrg.,", "citeRegEx": "Hampton and Doostan.,? \\Q2015\\E", "shortCiteRegEx": "Hampton and Doostan.", "year": 2015}, {"title": "Optimal importance sampling for the approximation of integrals", "author": ["A. Hinrichs"], "venue": "J. Complexity,", "citeRegEx": "Hinrichs.,? \\Q2010\\E", "shortCiteRegEx": "Hinrichs.", "year": 2010}, {"title": "Optimally-weighted herding is Bayesian quadrature", "author": ["F. Huszar", "D. Duvenaud"], "venue": "In Uncert. Artif. Intell.,", "citeRegEx": "Huszar and Duvenaud.,? \\Q2012\\E", "shortCiteRegEx": "Huszar and Duvenaud.", "year": 2012}, {"title": "Convergence guarantees for kernel-based quadrature rules in misspecified settings", "author": ["M. Kanagawa", "B. Sriperumbudur", "K. Fukumizu"], "venue": "In Adv. Neur. Inf. Proc. Sys.,", "citeRegEx": "Kanagawa et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kanagawa et al\\.", "year": 2016}, {"title": "Active uncertainty calibration in bayesian ode solvers", "author": ["H. Kersting", "P. Hennig"], "venue": "In Proc. Conf. Uncert. Artif. Intell.,", "citeRegEx": "Kersting and Hennig.,? \\Q2016\\E", "shortCiteRegEx": "Kersting and Hennig.", "year": 2016}, {"title": "Eigenvalues of compact operators with applications to integral operators", "author": ["H. K\u00f6nig"], "venue": "Linear Algebra Appl.,", "citeRegEx": "K\u00f6nig.,? \\Q1986\\E", "shortCiteRegEx": "K\u00f6nig.", "year": 1986}, {"title": "The empirical interpolation method", "author": ["S. Kristoffersen"], "venue": "Master\u2019s thesis, Department of Mathematical Sciences, Norwegian University of Science and Technology,", "citeRegEx": "Kristoffersen.,? \\Q2013\\E", "shortCiteRegEx": "Kristoffersen.", "year": 2013}, {"title": "Black-Box Importance Sampling", "author": ["Q. Liu", "J.D. Lee"], "venue": "I. Conf. Artif. Intell. Stat.,", "citeRegEx": "Liu and Lee.,? \\Q2017\\E", "shortCiteRegEx": "Liu and Lee.", "year": 2017}, {"title": "Active Area Search via Bayesian Quadrature", "author": ["Y. Ma", "R. Garnett", "J. Schneider"], "venue": "I. Conf Artif. Intell. Stat.,", "citeRegEx": "Ma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2014}, {"title": "Randomized algorithms for matrices and data", "author": ["M.W. Mahoney"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "Mahoney.,? \\Q2011\\E", "shortCiteRegEx": "Mahoney.", "year": 2011}, {"title": "Convergence Rates for a Class of Estimators", "author": ["C.J. Oates", "J. Cockayne", "F-X. Briol", "M. Girolami"], "venue": "Based on Stein\u2019s Identity", "citeRegEx": "Oates et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oates et al\\.", "year": 2016}, {"title": "Control Functionals for Monte Carlo Integration", "author": ["C.J. Oates", "M. Girolami", "N. Chopin"], "venue": "J. R. Stat. Soc. Ser. B. Stat. Methodol.,", "citeRegEx": "Oates et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Oates et al\\.", "year": 2017}, {"title": "Bayes-Hermite quadrature", "author": ["A. O\u2019Hagan"], "venue": "J. Statist. Plann. Inference,", "citeRegEx": "O.Hagan.,? \\Q1991\\E", "shortCiteRegEx": "O.Hagan.", "year": 1991}, {"title": "Active learning of model evidence using Bayesian quadrature", "author": ["M.A. Osborne", "D. Duvenaud", "R. Garnett", "C.E. Rasmussen", "S. Roberts", "Z. Ghahramani"], "venue": "In Adv. Neur. Inf. Proc. Sys.,", "citeRegEx": "Osborne et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 2012}, {"title": "Bayesian quadrature for ratios", "author": ["M.A. Osborne", "R. Garnett", "S. Roberts", "C. Hart", "S. Aigrain", "N. Gibson"], "venue": "In Proc. I. Conf. Artif. Intell. Stat.,", "citeRegEx": "Osborne et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 2012}, {"title": "OASIS: Adaptive Column Sampling for Kernel Matrix Approximation", "author": ["R. Patel", "T.A. Goldstein", "E.L. Dyer", "A.Mirhoseini", "R.G. Baraniuk"], "venue": null, "citeRegEx": "Patel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Patel et al\\.", "year": 2015}, {"title": "Alternating Optimisation and Quadrature for Robust Reinforcement Learning", "author": ["S. Paul", "K. Ciosek", "M.A. Osborne", "S. Whiteson"], "venue": null, "citeRegEx": "Paul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2016}, {"title": "New averaging technique for approximating weighted integrals", "author": ["L. Plaskota", "G.W. Wasilkowski", "Y. Zhao"], "venue": "J. Complexity,", "citeRegEx": "Plaskota et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Plaskota et al\\.", "year": 2009}, {"title": "Bayesian Quadrature in Nonlinear Filtering", "author": ["J. Pr\u00fcher", "M. \u0160imandl"], "venue": "In 12th I. Conf. Inform. Control Autom. Robot.,", "citeRegEx": "Pr\u00fcher and \u0160imandl.,? \\Q2015\\E", "shortCiteRegEx": "Pr\u00fcher and \u0160imandl.", "year": 2015}, {"title": "Bayesian Monte Carlo", "author": ["C.E. Rasmussen", "Z. Ghahramani"], "venue": "In Adv. Neur. Inf. Proc. Sys.,", "citeRegEx": "Rasmussen and Ghahramani.,? \\Q2002\\E", "shortCiteRegEx": "Rasmussen and Ghahramani.", "year": 2002}, {"title": "Monte Carlo statistical methods", "author": ["C. Robert", "G. Casella"], "venue": "Springer Science & Business Media,", "citeRegEx": "Robert and Casella.,? \\Q2013\\E", "shortCiteRegEx": "Robert and Casella.", "year": 2013}, {"title": "An introduction to ordinary differential equations", "author": ["J.C. Robinson"], "venue": null, "citeRegEx": "Robinson.,? \\Q2004\\E", "shortCiteRegEx": "Robinson.", "year": 2004}, {"title": "On the relation between Gaussian process quadratures and sigma-point methods", "author": ["S. S\u00e4rkk\u00e4", "J. Hartikainen", "L. Svensson", "F. Sandblom"], "venue": null, "citeRegEx": "S\u00e4rkk\u00e4 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "S\u00e4rkk\u00e4 et al\\.", "year": 2015}, {"title": "Data spectroscopy: Eigenspaces of convolution operators and clustering", "author": ["T. Shi", "M. Belkin", "B. Yu"], "venue": "Ann. Statist.,", "citeRegEx": "Shi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2009}, {"title": "Trace Ideals and Their Applications", "author": ["B. Simon"], "venue": null, "citeRegEx": "Simon.,? \\Q1979\\E", "shortCiteRegEx": "Simon.", "year": 1979}, {"title": "A Hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Algorithmic Learn. Theor.,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Numerical cubature on scattered data by radial basis functions", "author": ["A. Sommariva", "M. Vianello"], "venue": null, "citeRegEx": "Sommariva and Vianello.,? \\Q2006\\E", "shortCiteRegEx": "Sommariva and Vianello.", "year": 2006}, {"title": "Special Functions: An Introduction to the Classical Functions of Mathematical Physics", "author": ["N.M. Temme"], "venue": null, "citeRegEx": "Temme.,? \\Q1996\\E", "shortCiteRegEx": "Temme.", "year": 1996}, {"title": "Scattered Data Approximation", "author": ["H. Wendland"], "venue": null, "citeRegEx": "Wendland.,? \\Q2004\\E", "shortCiteRegEx": "Wendland.", "year": 2004}, {"title": "Towards Automatic Model Comparison: An Adaptive Sequential Monte Carlo Approach", "author": ["Y. Zhou", "A.M. Johansen", "J.A.D. Aston"], "venue": "J. Comput. Graph. Statist.,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 28, "context": "The KQ estimator is identical to the posterior mean in Bayesian Monte Carlo (O\u2019Hagan, 1991; Rasmussen and Ghahramani, 2002), and its relationship with classical numerical quadrature rules has been studied (Diaconis, 1988; S\u00e4rkk\u00e4 et al.", "startOffset": 76, "endOffset": 123}, {"referenceID": 35, "context": "The KQ estimator is identical to the posterior mean in Bayesian Monte Carlo (O\u2019Hagan, 1991; Rasmussen and Ghahramani, 2002), and its relationship with classical numerical quadrature rules has been studied (Diaconis, 1988; S\u00e4rkk\u00e4 et al.", "startOffset": 76, "endOffset": 123}, {"referenceID": 9, "context": "The KQ estimator is identical to the posterior mean in Bayesian Monte Carlo (O\u2019Hagan, 1991; Rasmussen and Ghahramani, 2002), and its relationship with classical numerical quadrature rules has been studied (Diaconis, 1988; S\u00e4rkk\u00e4 et al., 2015).", "startOffset": 205, "endOffset": 242}, {"referenceID": 38, "context": "The KQ estimator is identical to the posterior mean in Bayesian Monte Carlo (O\u2019Hagan, 1991; Rasmussen and Ghahramani, 2002), and its relationship with classical numerical quadrature rules has been studied (Diaconis, 1988; S\u00e4rkk\u00e4 et al., 2015).", "startOffset": 205, "endOffset": 242}, {"referenceID": 4, "context": "Under regularity conditions, Briol et al. (2015b) established the following RMSE bound for KQ: \u221a E[\u03a0\u0302(f)\u2212\u03a0(f)]2 \u2264 C(f ; \u03a0 \u2032) ns/d\u2212 , (s > d/2)", "startOffset": 29, "endOffset": 50}, {"referenceID": 2, "context": "An information-theoretic lower bound on the RMSE is O(n\u2212s/d\u22121/2) (Bakhvalov, 1959).", "startOffset": 65, "endOffset": 82}, {"referenceID": 27, "context": "In particular, the approach (i) provides practical guidance for selection of \u03a0\u2032 for KQ, (ii) offers robustness to kernel misspecification, and (iii) extends recent work on computing posterior expectations with kernels obtained using Stein\u2019s method (Oates et al., 2017).", "startOffset": 248, "endOffset": 268}, {"referenceID": 4, "context": "Pairs (\u03a0, k) for which the zj have closed form are reported in Table 1 of Briol et al. (2015b). Computation of these weights incurs a computational cost of at most O(n) and can be justified when either (i) evaluation of f forms the computational bottleneck, or (ii) the gain in estimator precision (as a function in n) dominates this cost (i.", "startOffset": 74, "endOffset": 95}, {"referenceID": 5, "context": "Notable contributions on KQ include Diaconis (1988); O\u2019Hagan (1991); Rasmussen and Ghahramani (2002) who introduced the method and Huszar and Duvenaud (2012); Osborne et al.", "startOffset": 36, "endOffset": 52}, {"referenceID": 5, "context": "Notable contributions on KQ include Diaconis (1988); O\u2019Hagan (1991); Rasmussen and Ghahramani (2002) who introduced the method and Huszar and Duvenaud (2012); Osborne et al.", "startOffset": 36, "endOffset": 68}, {"referenceID": 5, "context": "Notable contributions on KQ include Diaconis (1988); O\u2019Hagan (1991); Rasmussen and Ghahramani (2002) who introduced the method and Huszar and Duvenaud (2012); Osborne et al.", "startOffset": 36, "endOffset": 101}, {"referenceID": 5, "context": "Notable contributions on KQ include Diaconis (1988); O\u2019Hagan (1991); Rasmussen and Ghahramani (2002) who introduced the method and Huszar and Duvenaud (2012); Osborne et al.", "startOffset": 36, "endOffset": 158}, {"referenceID": 5, "context": "Notable contributions on KQ include Diaconis (1988); O\u2019Hagan (1991); Rasmussen and Ghahramani (2002) who introduced the method and Huszar and Duvenaud (2012); Osborne et al. (2012a;b); Gunter et al. (2014); Bach (2015); Briol et al.", "startOffset": 36, "endOffset": 206}, {"referenceID": 0, "context": "(2014); Bach (2015); Briol et al.", "startOffset": 8, "endOffset": 20}, {"referenceID": 0, "context": "(2014); Bach (2015); Briol et al. (2015a;b); S\u00e4rkk\u00e4 et al. (2015); Kanagawa et al.", "startOffset": 8, "endOffset": 66}, {"referenceID": 0, "context": "(2014); Bach (2015); Briol et al. (2015a;b); S\u00e4rkk\u00e4 et al. (2015); Kanagawa et al. (2016); Liu and Lee (2017) who provided consequent methodological extensions.", "startOffset": 8, "endOffset": 90}, {"referenceID": 0, "context": "(2014); Bach (2015); Briol et al. (2015a;b); S\u00e4rkk\u00e4 et al. (2015); Kanagawa et al. (2016); Liu and Lee (2017) who provided consequent methodological extensions.", "startOffset": 8, "endOffset": 110}, {"referenceID": 20, "context": "solvers (Kersting and Hennig, 2016), reinforcement learning (Paul et al.", "startOffset": 8, "endOffset": 35}, {"referenceID": 32, "context": "solvers (Kersting and Hennig, 2016), reinforcement learning (Paul et al., 2016), filtering (Pr\u00fcher and \u0160imandl, 2015) and design of experiments (Ma et al.", "startOffset": 60, "endOffset": 79}, {"referenceID": 34, "context": ", 2016), filtering (Pr\u00fcher and \u0160imandl, 2015) and design of experiments (Ma et al.", "startOffset": 19, "endOffset": 45}, {"referenceID": 24, "context": ", 2016), filtering (Pr\u00fcher and \u0160imandl, 2015) and design of experiments (Ma et al., 2014).", "startOffset": 72, "endOffset": 89}, {"referenceID": 3, "context": "LetH denote the Hilbert space characterised by the reproducing kernel k, and denote its norm as \u2016 \u00b7 \u2016H (Berlinet and Thomas-Agnan, 2011).", "startOffset": 103, "endOffset": 136}, {"referenceID": 10, "context": "These characterisations connect KQ to (a) non-parametric regression, (b) probabilistic integration and (c) quasi-Monte Carlo (QMC) methods (Dick and Pillichshammer, 2010).", "startOffset": 139, "endOffset": 170}, {"referenceID": 42, "context": "The scattered data approximation literature (Sommariva and Vianello, 2006) and the numerical analysis literature (where KQ is known as the \u2018empirical interpolation method\u2019; Eftang and Stamm, 2012; Kristoffersen, 2013) can also be connected to KQ.", "startOffset": 44, "endOffset": 74}, {"referenceID": 13, "context": "The scattered data approximation literature (Sommariva and Vianello, 2006) and the numerical analysis literature (where KQ is known as the \u2018empirical interpolation method\u2019; Eftang and Stamm, 2012; Kristoffersen, 2013) can also be connected to KQ.", "startOffset": 113, "endOffset": 217}, {"referenceID": 22, "context": "The scattered data approximation literature (Sommariva and Vianello, 2006) and the numerical analysis literature (where KQ is known as the \u2018empirical interpolation method\u2019; Eftang and Stamm, 2012; Kristoffersen, 2013) can also be connected to KQ.", "startOffset": 113, "endOffset": 217}, {"referenceID": 0, "context": "However, our search of all of these literatures did not yield guidance on the optimal selection of the sampling distribution \u03a0\u2032 (with the exception of Bach (2015) reported in Sec.", "startOffset": 151, "endOffset": 163}, {"referenceID": 0, "context": "Related work on sub-sample selection, such as leverage scores (Bach, 2013), can also be non-robust to mis-specified kernels.", "startOffset": 62, "endOffset": 74}, {"referenceID": 23, "context": "In Osborne et al. (2012a); Huszar and Duvenaud (2012); Gunter et al.", "startOffset": 3, "endOffset": 26}, {"referenceID": 13, "context": "(2012a); Huszar and Duvenaud (2012); Gunter et al.", "startOffset": 9, "endOffset": 36}, {"referenceID": 11, "context": "(2012a); Huszar and Duvenaud (2012); Gunter et al. (2014); Briol et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 2, "context": "(2014); Briol et al. (2015a), the selection ofxn was approached as a greedy optimisation problem, wherein the maximal integration error en(w; {xj}j=1) was minimised, given the location of the previous {xj} j=1 .", "startOffset": 8, "endOffset": 29}, {"referenceID": 25, "context": "The same intuition is used for column sampling and to construct leverage scores (Mahoney, 2011; Drineas et al., 2012).", "startOffset": 80, "endOffset": 117}, {"referenceID": 12, "context": "The same intuition is used for column sampling and to construct leverage scores (Mahoney, 2011; Drineas et al., 2012).", "startOffset": 80, "endOffset": 117}, {"referenceID": 40, "context": "Assume that \u222b X k(x,x)\u03a0(dx) < \u221e, so that \u03a3 is self-adjoint, positive semi-definite and trace-class (Simon, 1979).", "startOffset": 99, "endOffset": 112}, {"referenceID": 21, "context": "Then, from an extension of Mercer\u2019s theorem (K\u00f6nig, 1986) we have a decomposition k(x,x\u2032) = \u2211\u221e m=1 \u03bcmem(x)em(x \u2032), where \u03bcm and em(x) are the eigenvalues and eigenfunctions of \u03a3.", "startOffset": 44, "endOffset": 57}, {"referenceID": 21, "context": "Then, from an extension of Mercer\u2019s theorem (K\u00f6nig, 1986) we have a decomposition k(x,x\u2032) = \u2211\u221e m=1 \u03bcmem(x)em(x \u2032), where \u03bcm and em(x) are the eigenvalues and eigenfunctions of \u03a3. Further assume thatH is dense in L2(\u03a0). The first result is adapted and extended from Thm. 1 in Oates et al. (2016). Theorem 1.", "startOffset": 45, "endOffset": 295}, {"referenceID": 4, "context": "1 of Briol et al. (2015b) for samples from \u03a0 (see the Appendix) and was extended to MCMC samples in Oates et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 4, "context": "1 of Briol et al. (2015b) for samples from \u03a0 (see the Appendix) and was extended to MCMC samples in Oates et al. (2016). An extension to Figure 2.", "startOffset": 5, "endOffset": 120}, {"referenceID": 17, "context": "the case of a mis-specified kernel was considered in Kanagawa et al. (2016). However a limitation of this direction of research is that it does not address the question of how to select \u03a0\u2032.", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": "The second result that we present is a consequence of the recent work of Bach (2015), who considered a particular choice of \u03a0\u2032 = \u03a0B, depending on a fixed \u03bb > 0, via the density \u03c0B(x;\u03bb) \u221d \u2211\u221e m=1 \u03bcm \u03bcm+\u03bb em(x).", "startOffset": 73, "endOffset": 85}, {"referenceID": 0, "context": "The second result that we present is a consequence of the recent work of Bach (2015), who considered a particular choice of \u03a0\u2032 = \u03a0B, depending on a fixed \u03bb > 0, via the density \u03c0B(x;\u03bb) \u221d \u2211\u221e m=1 \u03bcm \u03bcm+\u03bb em(x). The following is adapted from Prop. 1 in Bach (2015): Theorem 2.", "startOffset": 73, "endOffset": 262}, {"referenceID": 0, "context": "Some remarks are in order: (i) Bach (2015, Prop. 3) showed that, for \u03a0B, integration error scales at an optimal rate in n up to logarithmic terms and, after n samples, is of size \u221a \u03bcn. (ii) The distribution \u03a0B is obtained from minimising an upper bound on the integration error, rather than the error itself. It is unclear to us how well \u03a0B approximates an optimal sampling distribution for KQ. (iii) In general \u03a0B is hard to compute. For the specific case X = [0, 1], H equal to Hs(\u03a0) and \u03a0 uniform, the distribution \u03a0B is also uniform (and hence independent of n; see Sec. 4.4 of Bach (2015)).", "startOffset": 31, "endOffset": 594}, {"referenceID": 0, "context": "Some remarks are in order: (i) Bach (2015, Prop. 3) showed that, for \u03a0B, integration error scales at an optimal rate in n up to logarithmic terms and, after n samples, is of size \u221a \u03bcn. (ii) The distribution \u03a0B is obtained from minimising an upper bound on the integration error, rather than the error itself. It is unclear to us how well \u03a0B approximates an optimal sampling distribution for KQ. (iii) In general \u03a0B is hard to compute. For the specific case X = [0, 1], H equal to Hs(\u03a0) and \u03a0 uniform, the distribution \u03a0B is also uniform (and hence independent of n; see Sec. 4.4 of Bach (2015)). However, even for the simple example of Sec. 2.3, \u03a0B does not appear to have a closed form (details in Appendix). An approximation scheme was proposed in Sec. 4.2 of Bach (2015) but the error of this scheme was not studied.", "startOffset": 31, "endOffset": 774}, {"referenceID": 15, "context": "considered in Hampton and Doostan (2015); Cohen and Migliorati (2016).", "startOffset": 14, "endOffset": 41}, {"referenceID": 7, "context": "considered in Hampton and Doostan (2015); Cohen and Migliorati (2016).", "startOffset": 42, "endOffset": 70}, {"referenceID": 6, "context": "To realise such an algorithm, this paper exploited SMC methods (Chopin, 2002; Del Moral et al., 2006).", "startOffset": 63, "endOffset": 101}, {"referenceID": 45, "context": "The specific schedule used in this work was determined based on the conditional effective sample size of the current particle population, as proposed in the recent work of Zhou et al. (2016). Full details are presented in Algs.", "startOffset": 172, "endOffset": 191}, {"referenceID": 31, "context": "More sophisticated alternatives that also involve the kernel k, such as leverage scores, were not considered, since in general these (i) introduce a vulnerability to mis-specified kernels and (ii) require manipulation of a N \u00d7 N kernel matrix (Patel et al., 2015).", "startOffset": 243, "endOffset": 263}, {"referenceID": 26, "context": "To implement KQ under an unknown normalisation constant for \u03a0, we followed Oates et al. (2017) and made use of a Gaussian kernel that was adapted with Stein\u2019s method (see the Appendix for details).", "startOffset": 75, "endOffset": 95}, {"referenceID": 26, "context": "To implement KQ under an unknown normalisation constant for \u03a0, we followed Oates et al. (2017) and made use of a Gaussian kernel that was adapted with Stein\u2019s method (see the Appendix for details). The reference distribution \u03a00 was an wide uniform prior on the hypercube [0, 10]. Brute force computation was used to obtain a benchmark value for the integral. For the SMC algorithm, an independent lognormal transition kernel was used at each iteration with parameters automatically tuned to the current set of particles. Results in Fig. 4 demonstrate that SMC-KQ outperforms KQ for these integration problems. These results improve upon those reported in Oates et al. (2016) for a similar integration problem based on parameter estimation for differential equations.", "startOffset": 75, "endOffset": 675}, {"referenceID": 11, "context": "Two extensions of this research are identified: First, the curse of dimension that is intrinsic to standard Sobolev spaces can be alleviated by demanding \u2018dominating mixed smoothness\u2019; our methods are compatible with these (essentially tensor product) kernels (Dick et al., 2013).", "startOffset": 260, "endOffset": 279}, {"referenceID": 14, "context": "Second, the use of sequential QMC (Gerber and Chopin, 2015) can be considered, motivated by further orders of magnitude reduction in numerical error observed for deterministic point sets (see Fig.", "startOffset": 34, "endOffset": 59}, {"referenceID": 18, "context": "This is known as \u2018sequential Bayesian quadrature\u2019 (SBQ; Huszar and Duvenaud, 2012; Gunter et al., 2014; Briol et al., 2015a).", "startOffset": 50, "endOffset": 124}, {"referenceID": 15, "context": "This is known as \u2018sequential Bayesian quadrature\u2019 (SBQ; Huszar and Duvenaud, 2012; Gunter et al., 2014; Briol et al., 2015a).", "startOffset": 50, "endOffset": 124}, {"referenceID": 44, "context": "13 in Wendland (2004) we have that there exist constants 0 < ck <\u221e, h0 > 0 such that |f\u0302(x)\u2212 f(x)| \u2264 ckhn\u2016f\u2016H (7)", "startOffset": 6, "endOffset": 22}, {"referenceID": 26, "context": "1 in Oates et al. (2016) establishes that, under the present hypotheses on X and \u03a0\u2032, there exists 0 < c\u03a0\u2032, <\u221e such that E[h n ] \u2264 c\u03a0\u2032, m\u22122s/d+ for all > 0, where c\u03a0\u2032, is independent of n.", "startOffset": 5, "endOffset": 25}, {"referenceID": 41, "context": "The Cauchy-Schwarz result for kernel mean embeddings (Smola et al., 2007) gives", "startOffset": 53, "endOffset": 73}, {"referenceID": 0, "context": "1 of Bach (2015) established that when x1, .", "startOffset": 5, "endOffset": 17}, {"referenceID": 39, "context": "1 in Shi et al. (2009)).", "startOffset": 5, "endOffset": 23}, {"referenceID": 43, "context": "8 in Temme (1996), p.", "startOffset": 5, "endOffset": 18}, {"referenceID": 0, "context": "The approximation method in Bach (2015) was also used to obtain the numerical approximation to \u03a0B shown in Fig.", "startOffset": 28, "endOffset": 40}, {"referenceID": 32, "context": "The following is due to Plaskota et al. (2009); Hinrichs (2010) and we provide an elementary proof of their result: Theorem 3.", "startOffset": 24, "endOffset": 47}, {"referenceID": 17, "context": "(2009); Hinrichs (2010) and we provide an elementary proof of their result: Theorem 3.", "startOffset": 8, "endOffset": 24}, {"referenceID": 36, "context": "4 in Robert and Casella (2013). For the second case, where F is the unit ball inH, we start by establishing a (tight) upper bound for the supremum of f over f \u2208 F : |f(x)| = \u2223\u2223\u3008f, k(\u00b7,x)\u3009H\u2223\u2223 \u2264 \u2016f\u2016H\u2016k(\u00b7,x)\u2016H = \u2016f\u2016H \u221a \u3008k(\u00b7,x), k(\u00b7,x)\u3009H = \u2016f\u2016H \u221a k(x,x)", "startOffset": 5, "endOffset": 31}, {"referenceID": 14, "context": "This suggests that a SQMC approach (Gerber and Chopin, 2015) could provide further improvement and this suggested for future work.", "startOffset": 35, "endOffset": 60}, {"referenceID": 26, "context": "Following Oates et al. (2017) we considered the Stein operator", "startOffset": 10, "endOffset": 30}, {"referenceID": 45, "context": "Following Zhou et al. (2016) we employed an adaptive temperature schedule construction.", "startOffset": 10, "endOffset": 29}], "year": 2017, "abstractText": "The standard Kernel Quadrature method for numerical integration with random point sets (also called Bayesian Monte Carlo) is known to converge in root mean square error at a rate determined by the ratio s/d, where s and d encode the smoothness and dimension of the integrand. However, an empirical investigation reveals that the rate constant C is highly sensitive to the distribution of the random points. In contrast to standard Monte Carlo integration, for which optimal importance sampling is wellunderstood, the sampling distribution that minimises C for Kernel Quadrature does not admit a closed form. This paper argues that the practical choice of sampling distribution is an important open problem. One solution is considered; a novel automatic approach based on adaptive tempering and sequential Monte Carlo. Empirical results demonstrate a dramatic reduction in integration error of up to 4 orders of magnitude can be achieved with the proposed method.", "creator": "LaTeX with hyperref package"}}}