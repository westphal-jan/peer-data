{"id": "1706.03643", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Tackling Over-pruning in Variational Autoencoders", "abstract": "variational autoencoders ( vae ) are numerous generative models that combine factorial latent variables. as predicted by burda et al. ( 2015 ), these participants exhibit the problem of factor over - pruning where a significant number of reactive factors fail to learn anything into become inactive. this can limit their dynamic power and their ability to discover diverse and meaningful latent representations. in this paper, we evaluate several methods to address this problem and propose a more effective model - switching approach then the epitomic variational autoencoder ( evae ). proposed so - called epitomes of crisis model are groups of mutually exclusive latent factors that compete to find misleading data. this approach helps prevent ambiguity actions since each group is pressured to explain the data. we approach the approaches with qualitative and quantitative results on mnist and tfd datasets. our results show that evae makes improved use of model capacity and generalizes effectiveness of vae.", "histories": [["v1", "Fri, 9 Jun 2017 10:13:00 GMT  (2271kb,D)", "http://arxiv.org/abs/1706.03643v1", null], ["v2", "Mon, 7 Aug 2017 01:13:29 GMT  (2271kb,D)", "http://arxiv.org/abs/1706.03643v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["serena yeung", "anitha kannan", "yann dauphin", "li fei-fei"], "accepted": false, "id": "1706.03643"}, "pdf": {"name": "1706.03643.pdf", "metadata": {"source": "META", "title": "Tackling Over-pruning in Variational Autoencoders", "authors": ["Serena Yeung", "Anitha Kannan", "Yann Dauphin", "Li Fei-Fei"], "emails": ["ena@cs.stanford.edu>."], "sections": [{"heading": "1. Introduction", "text": "Unsupervised learning holds the promise of learning the inherent structure in data so as to enable many future tasks including generation, prediction and visualization. Generative modeling is an approach to unsupervised learning wherein an explicit stochastic generative model of data is defined; independent draws from this model are to produce samples from the underlying data distribution, while the learned latent structure is useful for prediction, classification and visualization tasks.\nVariational autoencoder (VAE) (Kingma & Welling, 2014) is an example of one such generative model. VAE pairs a top-down generative model with a bottom-up recognition network for amortized probabilistic inference, and jointly\n1Stanford University, Stanford, CA, USA. Work done during an internship at Facebook AI Research. 2Facebook AI Research, Menlo Park, CA, USA. Correspondence to: Serena Yeung <serena@cs.stanford.edu>.\ntrains them to maximize a variational lower bound on the data likelihood. A number of recent works use VAE as a modeling framework, including iterative conditional generation of images (Gregor et al., 2015) and conditional future frame prediction (Xue et al., 2016).\nThe generative model of VAE has a set of independent stochastic latent variables that govern data generation; these variables aim to capture various factors of variation. However, a number of studies (Bowman et al., 2015; Kaae Sonderby et al., 2016; Kingma et al., 2016) have noted that straightforward implementations that optimize the variational bound on the probability of observations converge to a solution in which only a small subset of the stochastic latent units are active. While it may seem advantageous that the model can automatically regularize itself, the optimization leads to learning a suboptimal generative model by limiting its capacity to use only a small number of stochastic units. We call this well-known issue with training VAE as \u2019over-pruning\u2019. Existing methods propose training schemes to tackle the over-pruning problem that arises due to pre-maturely deactivating units (Bowman et al., 2015; Kaae Sonderby et al., 2016; Kingma et al., 2016). For instance, (Kingma et al., 2016) enforces minimum KL contribution from subsets of latent units while (Bowman et al., 2015) use KL cost annealing. However, these schemes are hand-tuned and takes away the principled regularization scheme that is built into VAE.\nWe address the over-pruning problem using a model-based approach. We present an extension of VAE called epitomic variational autoencoder (Epitomic VAE, or eVAE, for short) that automatically learns to utilize its model capacity more effectively, leading to better generalization. The motivation for eVAE stems from the following observation: Consider the task of learning a D-dimensional representation for the examples in a given dataset. A single example in the dataset can be sufficiently embedded in a smaller K-dimensional (K D) subspace of D. However, different data points may need different subspaces, hence the need for D. Sparse coding methods also exploit a similar hypothesis. Epitomic VAE exploits sparsity using an additional categorical latent variable in the encoder-decoder architecture of the VAE. Each value of the variable activates only a contiguous subset of latent stochastic variables to generate an observation. This enables learning multiple ar X\niv :1\n70 6.\n03 64\n3v 1\n[ cs\n.L G\n] 9\nJ un\n2 01\n7\nshared subspaces such that each subspace specializes, and also increases the use of model capacity (Fig. 3), enabling better representation. The choice of the name Epitomic VAE comes from the fact that multiple miniature models with shared parameters are trained simultaneously.\nThe rest of the paper is organized as follows. We first describe variational autoencoders and mathematically show the model pruning effect in \u00a7 2 and \u00a7 3. We then present our epitomic VAE model in \u00a7 4 that overcomes these shortcomings. Experiments showing qualitative and quantitative results are presented in \u00a7 5. We discuss related work in \u00a7 6, and conclude in \u00a7 7."}, {"heading": "2. Variational Autoencoders", "text": "The generative model of a VAE consists of first generating a sample from a D-dimensional stochastic variable z that is distributed according to a standard Gaussian:\np(z) = D\u220f d=1 N (zd; 0, 1) (1)\nEach component zi captures some latent source of variability in the data. Given z, the N-dimensional observation x is generated from a parametric family of distributions such as a Gaussian:\np\u03b8(x|z) = N (x; f1(z), exp(f2(z))) (2)\nwhere f1 and f2 are non-linear deterministic functions of z modeled using neural networks, and \u03b8 denotes the parameters of the generative model.\nThe model is trained by optimizing the likelihood p(X|\u03b8) using a dataset X of T i.i.d. samples. Since p(z|x) is intractable, VAE approximates the exact posterior using a variational approximation that is amortized across the training set, using a neural network (recognition network) with parameters \u03c6. The resulting variational bound is\nlogp\u03b8(X) = T\u2211 t=1 log \u222b z p\u03b8(x (t), z)\n\u2265 T\u2211 t=1 Eq\u03c6(z|x(t)) log p(x (t)|z)\u2212KL [ q\u03c6(z|x(t)) \u2016 p(z) ] (3)\nThe model is trained using backpropagation to minimize:\nCvae = \u2212 T\u2211 t=1 Eq\u03c6(z|x(t)) log p(x (t)|z)\n+ T\u2211 t=1 D\u2211 d=1 KL ( q\u03c6(zd|x(t)) \u2016 p(zd) ) (4)\nCvae trade-offs between explaining the data (first term) and ensuring that the posterior distribution is close to the prior p(z) (second term)."}, {"heading": "3. Over-pruning", "text": "We can better understand over-pruning in the VAE by considering different ways to minimize the sum of two terms in Cvae. The first term encourages proper reconstruction while the second term captures the divergence between the posterior q(zd) and its Gaussian distributed prior p(zd), independently for each component. The easiest way to minimize the sum is to have a large number of components collapse to the prior p(zd) to compensate for a few highly non-Gaussian components that help reconstruction. This is achieved by turning off the corresponding component1. This behavior is noticeable in the early iterations of training when the model for log p(x|z) is quite impoverished, and improvement to the loss can be easily obtained by optimizing this KL term. However, once the units have become inactive, it is almost impossible to resurrect them.\nA quantity that is useful in understanding this effect is the activity level of a unit. Following (Burda et al., 2015), we define a unit to be used, or \u201cactive\u201d, if Au = Covx(Eu\u223cq(u|x)[u]) > 0.02. In Figure 1 we plot the activity level and KL-divergence in Cvae for each component of a 50-unit VAE trained on MNIST. This result illustrates that all inactive units have collapsed to the prior, whereas active units are relatively far from the prior.\nOne could argue that over-pruning is a feature since the\n1log variance is modeled using the neural network, so turning it off to 0 corresponds to a variance of 1.\nmodel seems to discard unnecessary capacity. However, we observe over-pruning even when the model underfits the data. Moreover, over-pruning creates a discrepancy between training and generation time since it allows some components q(zd) to be highly different from the prior. Instead, it is desirable for each individual component to be close to the prior, since generation occurs by sampling from the prior."}, {"heading": "3.1. Weighting the KL term", "text": "One approach to reducing over-pruning is to introduce a trade-off between the two terms using a parameter \u03bb:\n\u2212Eq\u03c6(z|x)[log p(x|z)] + \u03bb D\u2211 i=1 KL ( q\u03c6(zi|x) \u2016 p(zi) ) \u03bb controls the importance of keeping the information encoded in z close to the prior. \u03bb = 0 corresponds to a vanilla autoencoder, and \u03bb = 1 to the correct VAE objective. Fig. 2 shows the effect of \u03bb on unit activity and generation. While tuning down \u03bb increases the number of active units, samples generated from the model are still poor. This is be-\ncause at small values of \u03bb, the model becomes closer to a vanilla autoencoder and hence spends its capacity in ensuring that reconstruction of the training set is optimized (sharper reconstructions as a function of \u03bb are shown in Appendix \u00a7 9.1), at the cost of generation capability."}, {"heading": "3.2. Dropout VAE", "text": "Another approach is to add dropout to the latent variable z of the VAE (Dropout VAE). While this increases the number of active units (Fig. 3), it generalizes poorly as it uses the dropout layers to merely replicate representation. This results in blurriness in both generation and reconstruction, and illustrates that simply utilizing additional units is not sufficient for proper utilization of these units to model additional factors of variation, as seen in Fig. 4."}, {"heading": "4. eVAE: A model-based approach", "text": "We propose epitomic variational autoencoders (eVAE) to overcome the over-pruning problem of VAEs. We base this on the observation that while we may need a Ddimensional representation to accurately represent every example in a dataset, each individual example can be represented with a smaller K-dimensional subspace. As an example, consider MNIST with its variability in terms of digits, strokes and thickness of ink, to name a few. While the overall D is large, it is likely that only a few K dimensions ofD are needed to capture the variability in strokes of some digits (see Fig. 5). Epitomic VAE can be viewed as a variational autoencoder with latent stochastic dimension D that is composed of a number of smaller variational autoen-\ncoders called epitomes, such that each epitome partially shares its encoder-decoder architecture with other epitomes in the composition. In this paper, we assume simple structured sparsity for each epitome: in particular, only K contiguous dimensions of D are active2.\nThe generative process can be described as follows: A Ddimensional stochastic variable z is drawn from a standard multivariate Gaussian p(z) = N (z; 0, I). In tandem, an epitome is implicitly chosen through an epitome selector variable y, which has a uniform prior over possible epitomes. TheN -dimensional observation x is then drawn from a Gaussian distribution:\np\u03b8(x|y, z) = N (x; f1(my z), exp(f2(my z))) (5)\nmy enforces the epitome constraint: it is also a Ddimensional vector that is zero everywhere except K contiguous dimensions that correspond to the epitome dictated by y. is element-wise multiplication between the two operands. Thus, my masks the dimensions of z other than those dictated by the choice of y. Fig. 5 illustrates this for an 8-d z with epitome size K = 2, such that there are four possible epitomes (the model also allows for overlapping epitomes, but this is not shown for illustration purposes). Epitome structure is defined using sizeK and stride s, where s = 1 corresponds to full overlap in D dimensions3. Our model generalizes the VAE and collapses to a VAE when D = K = s.\n2The model also allows for incorporating other forms of structured sparsity.\n3The strided epitome structure allows for learning O(D) specialized subspaces, that when sampled during generation can each produce good samples. In contrast, if only a simple sparsity prior is introduced over arbitrary subsets (e.g. with Bernoulli latent units to specify if a unit is active for a particular example), it can lead to poor generation results, which we confirmed empirically but do not report. The reason for this is as follows: due to an exponential number of potential combinations of latent units, sampling\nf1( ) and f2( ) define non-linear deterministic transformations of modeled using neural networks. Note that the model does not snip off the K dimensions corresponding to an epitome, but instead ignores the D \u2212 K dimensions that are not part of the chosen epitome. While the same deterministic functions f1 and f2 are used for any choice of epitome, the functions can still specialize due to the sparsity of their inputs. Neighboring epitomes will have more overlap than non-overlapping ones, which manifests itself in the representation space; an intrinsic ordering in the variability is learned."}, {"heading": "4.1. Overcoming over-pruning", "text": "Following (Kingma & Welling, 2014), we use a recognition network q(z, y|x) for approximate posterior inference, with the functional form\nq(z, y|x) = q(y|x)q(z|y,x) = q(y|x)N (z;my \u00b5, exp (my \u03c6)) (6)\nwhere \u00b5 = h1(x) and \u03c6 = h2(x) are neural networks that map x to D-dimensional space. We use a similar masking operation as the generative model. Unlike the generative model (eq. 5), the masking operation defined by y operates directly on outputs of the recognition network that characterizes the parameters of q(z|y,x). Similar to VAE, the lower bound on the log probability of a dataset can be derived, leading to the cost function (negative bound):\nCevae = \u2212 T\u2211 t=1 Eq(z,y|x(t))[log p(x (t)|y, z)]\n+ T\u2211 t=1 KL [ q\u03c6(y|x(t)) \u2016 p\u03b8(y) ] +\nT\u2211 t=1 \u2211 y q\u03c6(y|x(t))KL [ q\u03c6(z|y,x(t)) \u2016 p\u03b8(z)\n] (7)\neVAE departs from VAE in how the contribution from the KL term is constrained. Consider the third term expanded:\nT\u2211 t=1 \u2211 y q\u03c6(y|x(t))KL [ q\u03c6(z|y,x(t)) \u2016 p\u03b8(z) ]\n= T\u2211 t=1 \u2211 y q\u03c6(y|x(t)) D\u2211 d=1 1[md,y = 1]KL [ q(zd|x(t)) \u2016 p(zd) ] ,\n(8)\nwhere 1[?] is an indicator variable that evaluates to 1 if only if its operand ? is true. Unlike in VAE where this KL\na subset from the prior during generation cannot be straightforwardly guaranteed to be a good configuration for a subconcept in the data, and often leads to uninterpretable samples.\nterm decomposes into independent KL for each zi, contiguous dimensions of z are constrained by the choice of y. In addition, the number of KL terms that will contribute to Cevae for an input x(t) is exactly K by model design, with the other D \u2212K dimensions set to provide contribution of zero. Thus, only a fraction of examples in the training set contributes a possible non-zero value to zd\u2019s KL term in Cevae. This gives eVAE the ability to use more total units without having to prematurely prune the model to optimize the bound. In contrast, for Cvae to have a small contribution from the KL term of a particular zd, it has to infer that unit to have zero mean and unit variance for many examples in the training set. In practice, this results in VAE completely inactivating units.\nFig. 3 compares the activity levels of eVAE with VAE and dropout VAE. Even though Dropout VAE has similar activity profile to eVAE, its generative model is impoverished as it focuses on replicating representation as opposed to modeling variability. This can be evidenced by comparing the generation results from the two models, in Fig. 4 and Fig. 6."}, {"heading": "4.2. Training", "text": "The generative model and the recognition network are trained simultaneously, by minimizing Cevae in Eq. 7.\nFor the stochastic continuous variable z, we use the reparameterization trick as in VAE, which reparameterizes the recognition distribution in terms of auxiliary variables with fixed distributions. This allows efficient sampling from the posterior distribution as it becomes a deterministic function\nof inputs and auxiliary variables.\nFor the discrete variable y, we cannot use the reparameterization trick. We therefore approximate q(y|x) by a point estimate y\u2217 so that q(y|x) = \u03b4(y \u2212 y\u2217), where \u03b4 evaluates to 1 only if y = y\u2217 and the best y\u2217 = argmin Cevae. We also explored modeling q(y|x) =Mult(h(x)) as a discrete distribution with h being a neural network. In this case, the backward pass requires either using REINFORCE or passing through gradients for the categorical sampler. In our experiments, we found that these approaches did not work well, especially when the number of possible values of y becomes large. We leave this as future work to explore.\nThe recognition network first computes \u00b5 and \u03c6. It is then combined with the optimal y\u2217 for each example, to arrive at the final posterior. The model is trained using a simple algorithm outlined in Alg. 1. Backpropagation with minibatch updates is used, with each minibatch constructed to be balanced with respect to epitome assignment."}, {"heading": "5. Experiments", "text": "We present experimental results on two datasets, MNIST (LeCun et al., 1998) and Toronto Faces Database (TFD) (Susskind et al., 2010). We use standard splits for both MNIST and TFD. In our experiments, the encoder and decoder are fully-connected networks, and we show results for different depths and number of units of per layer. ReLU nonlinearities are used, and models are trained using the Adam update rule (Kingma & Ba, 2014) for 200 epochs (MNIST) and 250 epochs (TFD), with base\nAlgorithm 1 Learning Epitomic VAE 1: \u03b8, \u03c6\u2190Initialize parameters 2: for until convergence of parameters (\u03b8, \u03c6) do 3: Assign each x to its best y\u2217 = argmin Cevae 4: Randomize and partition data into minibatches,\nwith each minibatch having proportionate number of examples \u2200 y 5: for k \u2208 numbatches do 6: Update model parameters using kth minibatch\nconsisting of x, y pairs 7: end for 8: end for\nlearning rate 0.001. We emphasize that in all experiments, we optimize the correct lower bound for the corresponding models."}, {"heading": "5.1. Qualitative results: Reconstruction vs. Generation", "text": "We first qualitatively illustrate the ability of eVAE to overcome over-pruning and utilize latent capacity to model greater variability in data. Fig. 6 compares generation results for VAE and eVAE for different dimensions D of latent variable z. With D = 2, VAE generates realistic digits but suffers from lack of diversity. When D is increased to 5, the generation exhibits some greater variability but also begins to degrade in quality. As D is further increased to 10 and 20, the degradation continues. Contrast this with eVAE performance on generation: as the dimension D of z is increased while maintaining epitomes of size K = 2, eVAE is able to model greater variability in the data. Highlighted digits in the 20-d eVAE show multiple styles such as crossed versus un-crossed 7, and pointed, round, thick, and thin 4s. For both models, reconstruction improves with increasing D as provided in Appendix Fig. 10."}, {"heading": "5.2. Choice of epitome size", "text": "We next investigate how the choice of epitome size, K, affects generation performance. We measure sample quality using the Parzen window estimator (Rifai et al., 2012). Fig. 7 shows the Parzen log-density for different choices of epitome size on MNIST, with encoder and decoder consisting of a single deterministic layer of 500 units. Epitomes are non-overlapping, and the results are grouped by total dimension D of the latent variable z. For comparison, we also show the log-density for VAE models with the same dimension D, and for mixture VAE (mVAE), an ablative version of eVAE where parameters are not shared. mVAE can also be seen as a mixture of independent VAEs trained in the same manner as eVAE. The number of deterministic units in each mVAE component is computed so that the total number of parameters is comparable to eVAE.\nAs we increase D, the performance of VAE drops significantly, due to over-pruning. In fact, the number of active units for VAE are 8, 22 and 24, corresponding to D values of 8, 24 and 48, respectively. In contrast, eVAE performance increases as we increase D, with an epitome size K that is significantly smaller than D. This confirms the advantage of using eVAE to ensure good generation performance. Table 1 provides more comparisons. eVAE also performs comparably or better than mVAE at all epitome sizes. An explanation is that due to the parameter sharing in eVAE, each epitome benefits from general features learned across the training set."}, {"heading": "5.3. Increasing complexity of encoder and decoder", "text": "Here we investigate the impact of encoder and decoder architectures with respect to over-pruning and generation performance. We vary model complexity through number of layers L of deterministic hidden units, and number of hidden units H in each deterministic layer. Table 1 shows the Parzen log-densities of VAE, mVAE and eVAE models trained on TFD with different latent dimension D (See Appendix \u00a79.3 for MNIST). All epitomes are non-overlapping and of size K = 5. We observe that for VAE, increasing the number of hidden units H (e.g. from 500 to 1000) for a fixed network depth L has a negligible effect on the number of active units and performance. On the other hand, as the depth of the encoder and decoder L is increased, the number of active units in VAE decreases though performance is still able to improve. This illustrates that increase in the complexity of the interactions through multiple layers counteract the perils of the over-pruning. However, this comes with the cost of substantial increase in the number of model parameters to be learned.\nIn contrast, for any given model configuration, eVAE is able to avoid the over-pruning effect in the number of active units and outperform VAE. Table 1 also shows results for mVAE, the ablative version of eVAE where parameters are not shared. The number of deterministic units per layer in each mVAE component is computed to have total number of parameters comparable to eVAE. These results are in line with the intuition that parameter sharing is helpful in more challenging settings when each epitome can also benefit from general features learned across the training set."}, {"heading": "5.4. Log-likelihood evaluation", "text": "Table 2 shows importance weighted estimates as the mean of L5000 for VAE and eVAE on MNIST, with different dimensions D of latent variable z. All models have 2 deterministic hidden layers of 200 units, and are trained in 8 stages with a learning rate of 0.001 \u00b7 10\u221217 for 3i epochs, for each stage i = 0...7, following Burda et al. (2015). The VAE model has 20 active units at all D, so is not able\nto improve performance with increasing D. On the other hand, eVAE is able to leverage the additional latent capacity to improve on the log-likelihood. Note that these results can be improved through the tighter lower bound of IWAE (Burda et al., 2015), but this is an orthogonal consideration since epitomic training can also improve IWAE."}, {"heading": "5.5. Sample-based evaluation", "text": "In Table 3 we compare the generative performance of eVAE with other models through their samples. Encoders and de-\ncoders have L = 2 layers of H = 1000 deterministic units. D = 8 for MNIST, and D = 15 for TFD. VAE, mVAE, and eVAE refer to the best performing models over all architectures from Table 1. For MNIST, the VAE model is (L,H,D) = (3, 500, 8), mVAE is (3, 1000, 24), and eVAE is (3, 500, 48). For TFD, the VAE model is (3, 500, 15), mVAE is (3, 1000, 50), and eVAE is (3, 500, 25). We observe that eVAE significantly improves over VAE and is competitive with several state-of-the-art models, notably Adversarial Autoencoders. Samples from eVAE on MNIST and TFD are shown in Fig. 8."}, {"heading": "6. Related Work", "text": "A number of applications use variational autoencoders as a building block. In (Gregor et al., 2015), a generative model for images is proposed in which the generator of the VAE is an attention-based recurrent model that is conditioned on the canvas drawn so far. (Eslami et al., 2016) proposes a VAE-based recurrent generative model that describes images as formed by sequentially choosing an object to draw and adding it to a canvas that is updated over time. In (Kulkarni et al., 2015), VAEs are used for rendering 3D objects. Conditional variants of VAE are also used for attribute specific image generation (Yan et al., 2015) and future frame synthesis (Xue et al., 2016). All these applications suffer from the problem of model overpruning and hence have adopted strategies that takes away the clean mathematical formulation of VAE. We have discussed these in \u00a7 3. A complementary approach to the problem of model pruning in VAE was proposed in (Burda et al., 2015); the idea is to improve the variational bound by using multiple weighted posterior samples. Epitomic VAE provides improved latent capacity even when only a single sample is drawn from the posterior.\nMethods to increase the flexibility of posterior inference are proposed in (Salimans et al., 2015; Rezende & Mohamed, 2016; Kingma et al., 2016). In (Rezende & Mohamed, 2016), posterior approximation is constructed by\ntransforming a simple initial density into a complex one with a sequence of invertible transformations. (Kingma et al., 2016) augments the flexibility of the posterior through autoregression over projections of stochastic latent variables. However, the problem of over-pruning still persists: for instance, (Kingma et al., 2016) enforces a minimum information constraint to ensure all units are used.\nRelated is research in unsupervised sparse overcomplete representations, especially with group sparsity constraints c.f. (Gregor et al., 2011; Jenatton et al., 2011). In the epitomic VAE, we have similar motivations that enable learning better generative models of data."}, {"heading": "7. Conclusion", "text": "This paper introduces Epitomic VAE, an extension of variational autoencoders, to address the problem of model overpruning, which has limited the generation capability of VAEs in high-dimensional spaces. Based on the intuition that subconcepts can be modeled with fewer dimensions than the full latent space, epitomic VAE models the latent space as multiple shared subspaces that have learned specializations. We show how this model addresses the model over-pruning problem in a principled manner, and present qualitative and quantitative analysis of how eVAE enables increased utilization of the model capacity to model greater data variability. We believe that modeling the latent space as multiple structured subspaces is a promising direction of work, and allows for increased effective capacity that has potential to be combined with methods for increasing the flexibility of posterior inference."}, {"heading": "8. Acknowledgments", "text": "We thank Marc\u2019Aurelio Ranzato, Joost van Amersfoort and Ross Girshick for helpful discussions. We also borrowed the term \u2018epitome\u2019 from an earlier work of (Jojic et al., 2003)."}, {"heading": "9. Appendix", "text": ""}, {"heading": "9.1. Effect of KL weight \u03bb on reconstruction", "text": "We visualize VAE reconstructions as the KL term weight \u03bb is tuned down to keep latent units active. The top half of each figure are the original digits, and the bottom half are the corresponding reconstructions. While reconstruction performance is good, generation is poor (Fig. 2). This illustrates that VAE learns to model well only regions of the posterior manifold near training samples, instead of generalizing to model well the full posterior manifold."}, {"heading": "9.2. Effect of increasing latent dimension on reconstruction", "text": "In \u00a7 5.1 in the main paper, Fig. 6 shows the effect of increasing latent dimension on generation for VAE and eVAE models. Here we show the effect of the same factor on reconstruction quality for the models (Fig. 10). The top half of each figure are the original digits, and the bottom half are the corresponding reconstructions. As the dimension of the latent variable z increases from 2-d to 20-d, VAE reconstruction becomes very sharp (the best model), but generation degrades (Fig. 6). On the other hand, eVAE is able to achieve both good reconstruction and generation."}, {"heading": "9.3. MNIST encoder and decoder complexity", "text": "Table 4 shows the effect of increasing complexity in the encoder and decoder complexity on MNIST. We vary model complexity through number of layers L of deterministic hidden units, and number of hidden units H in each deterministic layer. Parzen log-densities are provided for VAE, mVAE and eVAE models trained on MNIST with different latent dimension D. The effect of model complexity on active units and performance aligns with that for TFD in the main paper."}], "references": [{"title": "Generating sentences from a continuous space", "author": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R Jozefowicz", "Bengio"], "venue": "arXiv preprint arXiv:1511.06349,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger B", "Salakhutdinov", "Ruslan"], "venue": "ICLR,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Attend, infer, repeat: Fast scene understanding with generative models", "author": ["Eslami", "S.M. Ali", "Heess", "Nicolas", "Weber", "Theophane", "Tassa", "Yuval", "Kavukcuoglu", "Koray", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Eslami et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eslami et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Structured sparse coding via lateral inhibition", "author": ["Gregor", "Karol", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "In Proceedings of the 24th International Conference on Neural Information Processing Systems,", "citeRegEx": "Gregor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2011}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.046239,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee-Whye"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Proximal methods for hierarchical sparse coding", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jenatton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2011}, {"title": "Epitomic analysis of appearance and shape", "author": ["Jojic", "Nebojsa", "Frey", "Brendan J", "Kannan", "Anitha"], "venue": "In Proceedings of International Conference on Computer Vision,", "citeRegEx": "Jojic et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Jojic et al\\.", "year": 2003}, {"title": "How to train deep variational autoencoders and probabilistic ladder networks", "author": ["C. Kaae Sonderby", "T. Raiko", "L. Maale", "S. Kaae Snderby", "O. Winther"], "venue": "arXiv preprint arXiv:1602.02282,", "citeRegEx": "Sonderby et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sonderby et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "arXiv preprint arXiv:1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "ICLR,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Deep convolutional inverse graphics", "author": ["T.D. Kulkarni", "W. Whitney", "P. Kohli", "J.B. Tenenbaum"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Generative moment matching networks", "author": ["Li", "Yujia", "Swersky", "Kevin", "Zemel", "Rich"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2016}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["Rifai", "Salah", "Bengio", "Yoshua", "Dauphin", "Yann", "Vincent", "Pascal"], "venue": "arXiv preprint arXiv:1206.6434,", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["T. Salimans", "D.P. Kingma", "M. Welling"], "venue": null, "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "The toronto face database", "author": ["Susskind", "Josh M", "Anderson", "Adam K", "Hinton", "Geoffrey E"], "venue": "Department of Computer Science,", "citeRegEx": "Susskind et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Thibodeau-Laufer", "Eric", "Alain", "Guillaume", "Yosinski", "Jason"], "venue": null, "citeRegEx": "Thibodeau.Laufer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Thibodeau.Laufer et al\\.", "year": 2014}, {"title": "Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks", "author": ["Xue", "Tianfan", "Wu", "Jiajun", "Bouman", "Katherine L", "Freeman", "William T"], "venue": "arXiv preprint arXiv:1607.02586,", "citeRegEx": "Xue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2016}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Yan", "Xinchen", "Yang", "Jimei", "Sohn", "Kihyuk", "Lee", "Honglak"], "venue": "CoRR, abs/1512.00570,", "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "As noted by Burda et al. (2015), these models exhibit the problem of factor overpruning where a significant number of stochastic factors fail to learn anything and become inactive.", "startOffset": 12, "endOffset": 32}, {"referenceID": 5, "context": "A number of recent works use VAE as a modeling framework, including iterative conditional generation of images (Gregor et al., 2015) and conditional future frame prediction (Xue et al.", "startOffset": 111, "endOffset": 132}, {"referenceID": 21, "context": ", 2015) and conditional future frame prediction (Xue et al., 2016).", "startOffset": 48, "endOffset": 66}, {"referenceID": 0, "context": "However, a number of studies (Bowman et al., 2015; Kaae Sonderby et al., 2016; Kingma et al., 2016) have noted that straightforward implementations that optimize the variational bound on the probability of observations converge to a solution in which only a small subset of the stochastic latent units are active.", "startOffset": 29, "endOffset": 99}, {"referenceID": 11, "context": "However, a number of studies (Bowman et al., 2015; Kaae Sonderby et al., 2016; Kingma et al., 2016) have noted that straightforward implementations that optimize the variational bound on the probability of observations converge to a solution in which only a small subset of the stochastic latent units are active.", "startOffset": 29, "endOffset": 99}, {"referenceID": 0, "context": "Existing methods propose training schemes to tackle the over-pruning problem that arises due to pre-maturely deactivating units (Bowman et al., 2015; Kaae Sonderby et al., 2016; Kingma et al., 2016).", "startOffset": 128, "endOffset": 198}, {"referenceID": 11, "context": "Existing methods propose training schemes to tackle the over-pruning problem that arises due to pre-maturely deactivating units (Bowman et al., 2015; Kaae Sonderby et al., 2016; Kingma et al., 2016).", "startOffset": 128, "endOffset": 198}, {"referenceID": 11, "context": "For instance, (Kingma et al., 2016) enforces minimum KL contribution from subsets of latent units while (Bowman et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 0, "context": ", 2016) enforces minimum KL contribution from subsets of latent units while (Bowman et al., 2015) use KL cost annealing.", "startOffset": 76, "endOffset": 97}, {"referenceID": 1, "context": "Following (Burda et al., 2015), we define a unit to be used, or \u201cactive\u201d, if Au = Covx(Eu\u223cq(u|x)[u]) > 0.", "startOffset": 10, "endOffset": 30}, {"referenceID": 14, "context": "We present experimental results on two datasets, MNIST (LeCun et al., 1998) and Toronto Faces Database (TFD) (Susskind et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 19, "context": ", 1998) and Toronto Faces Database (TFD) (Susskind et al., 2010).", "startOffset": 41, "endOffset": 64}, {"referenceID": 17, "context": "We measure sample quality using the Parzen window estimator (Rifai et al., 2012).", "startOffset": 60, "endOffset": 80}, {"referenceID": 1, "context": "7, following Burda et al. (2015). The VAE model has 20 active units at all D, so is not able", "startOffset": 13, "endOffset": 33}, {"referenceID": 1, "context": "Note that these results can be improved through the tighter lower bound of IWAE (Burda et al., 2015), but this is an orthogonal consideration since epitomic training can also improve IWAE.", "startOffset": 80, "endOffset": 100}, {"referenceID": 5, "context": "In (Gregor et al., 2015), a generative model for images is proposed in which the generator of the VAE is an attention-based recurrent model that is conditioned on the canvas drawn so far.", "startOffset": 3, "endOffset": 24}, {"referenceID": 2, "context": "(Eslami et al., 2016) proposes a VAE-based recurrent generative model that describes images as formed by sequentially choosing an object to draw and adding it to a canvas that is updated over time.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "In (Kulkarni et al., 2015), VAEs are used for rendering 3D objects.", "startOffset": 3, "endOffset": 26}, {"referenceID": 22, "context": "Conditional variants of VAE are also used for attribute specific image generation (Yan et al., 2015) and future frame synthesis (Xue et al.", "startOffset": 82, "endOffset": 100}, {"referenceID": 21, "context": ", 2015) and future frame synthesis (Xue et al., 2016).", "startOffset": 35, "endOffset": 53}, {"referenceID": 1, "context": "A complementary approach to the problem of model pruning in VAE was proposed in (Burda et al., 2015); the idea is to improve the variational bound by using multiple weighted posterior samples.", "startOffset": 80, "endOffset": 100}, {"referenceID": 18, "context": "Methods to increase the flexibility of posterior inference are proposed in (Salimans et al., 2015; Rezende & Mohamed, 2016; Kingma et al., 2016).", "startOffset": 75, "endOffset": 144}, {"referenceID": 11, "context": "Methods to increase the flexibility of posterior inference are proposed in (Salimans et al., 2015; Rezende & Mohamed, 2016; Kingma et al., 2016).", "startOffset": 75, "endOffset": 144}, {"referenceID": 6, "context": "DBN (Hinton et al., 2006) 138\u00b1 2 1909\u00b1 66 Deep CAE (Bengio et al.", "startOffset": 4, "endOffset": 25}, {"referenceID": 20, "context": ", 2013) 121\u00b1 1 2110\u00b1 50 Deep GSN (Thibodeau-Laufer et al., 2014) 214\u00b1 1 1890\u00b1 29 GAN (Goodfellow et al.", "startOffset": 33, "endOffset": 64}, {"referenceID": 3, "context": ", 2014) 214\u00b1 1 1890\u00b1 29 GAN (Goodfellow et al., 2014) 225\u00b1 2 2057\u00b1 26 GMMN + AE (Li et al.", "startOffset": 28, "endOffset": 53}, {"referenceID": 15, "context": ", 2014) 225\u00b1 2 2057\u00b1 26 GMMN + AE (Li et al., 2015) 282\u00b1 2 2204\u00b1 20 Adversarial AE (Makhzani et al.", "startOffset": 34, "endOffset": 51}, {"referenceID": 11, "context": "(Kingma et al., 2016) augments the flexibility of the posterior through autoregression over projections of stochastic latent variables.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "However, the problem of over-pruning still persists: for instance, (Kingma et al., 2016) enforces a minimum information constraint to ensure all units are used.", "startOffset": 67, "endOffset": 88}, {"referenceID": 4, "context": "(Gregor et al., 2011; Jenatton et al., 2011).", "startOffset": 0, "endOffset": 44}, {"referenceID": 7, "context": "(Gregor et al., 2011; Jenatton et al., 2011).", "startOffset": 0, "endOffset": 44}, {"referenceID": 8, "context": "We also borrowed the term \u2018epitome\u2019 from an earlier work of (Jojic et al., 2003).", "startOffset": 60, "endOffset": 80}], "year": 2017, "abstractText": "Variational autoencoders (VAE) are directed generative models that learn factorial latent variables. As noted by Burda et al. (2015), these models exhibit the problem of factor overpruning where a significant number of stochastic factors fail to learn anything and become inactive. This can limit their modeling power and their ability to learn diverse and meaningful latent representations. In this paper, we evaluate several methods to address this problem and propose a more effective model-based approach called the epitomic variational autoencoder (eVAE). The so-called epitomes of this model are groups of mutually exclusive latent factors that compete to explain the data. This approach helps prevent inactive units since each group is pressured to explain the data. We compare the approaches with qualitative and quantitative results on MNIST and TFD datasets. Our results show that eVAE makes efficient use of model capacity and generalizes better than VAE.", "creator": "LaTeX with hyperref package"}}}