{"id": "1706.03747", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Acoustic data-driven lexicon learning based on a greedy pronunciation selection framework", "abstract": "speech recognition implementations for simple - spelled languages like english normally require hand - written pronunciations. in this paper, we describe a system for automatically comparing pronunciations of words for which pronunciations are not available, but for which better data exists. appropriate method integrates information from the letter sequence and from preliminary acoustic evidence. the novel branch of the problem that we address is the problem of how to prune entries from deliberately limited lexicon ( since, empirically, lexicons with too many entries don't tend to be able for asr performance ). researchers on various asr tasks proposes that, for the proposed framework, starting with an initial lexicon of several more words, we are able to learn a lexicon which performs close to a full expert lexicon in terms of wer performance based test data, meaning is better than lexicons built using g2p alone or with optimal pruning criterion based upon pronunciation probability.", "histories": [["v1", "Mon, 12 Jun 2017 17:35:41 GMT  (60kb,D)", "http://arxiv.org/abs/1706.03747v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xiaohui zhang", "vimal manohar", "daniel povey", "sanjeev khudanpur"], "accepted": false, "id": "1706.03747"}, "pdf": {"name": "1706.03747.pdf", "metadata": {"source": "CRF", "title": "Acoustic data-driven lexicon learning based on a greedy pronunciation selection framework", "authors": ["Xiaohui Zhang", "Vimal Manohar", "Daniel Povey", "Sanjeev Khudanpur"], "emails": ["xiaohui@jhu.edu,", "vimal.manohar91@gmail.com,", "danielpovey@gmail.com,", "khudanpur@jhu.edu"], "sections": [{"heading": "1. Introduction", "text": "In the past few years, there has been an growing interest in investigating acoustic data-driven lexicon learning for continuous speech recognition, i.e. automatically obtaining pronunciations of words for which pronunciations are not available , but for which transcribed acoustic data exists. In order to develop ASR systems under limited lexicon resources, one solution is to adopt a graphemic lexicon [1, 2] or acoustic unit discovery methods [3, 4], which totally eliminate the expert efforts for developing a phonetic pronunciation lexicon. In real applications, however, a more common scenario is that we already have a phonetic inventory, and a small expert lexicon for a specific language. Our work focuses on this case, i.e. given a small expert lexicon, we want to derive pronunciations for Outof-Vocabulary (OOV) words, for which we know the text form and have acoustic examples.\nGiven a small expert lexicon, the most straightforward way to generate pronunciation candidates for OOV words is to train a Grapheme-to-Phoneme (G2P) [5] model using the seed lexicon\nThis work was partially supported by DARPA LORELEI Grant No HR0011-15-2-0024, NSF Grant No CRI-1513128 and IARPA Contract No 2012-12050800010. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, IARPA, DoD/ARL or the U.S. Government. Thanks Dr. Jack Godfrey, Dr. Alan MaCree, Dr. Mengyang Gu, Chloe Haviland for useful discussions.\nand apply it to these OOV words [6, 7, 8]. But for languages like English, and for proper names and abbreviations, G2P does not always give high quality pronunciations. Pronunciations from phonetic decoding can help to fill this gap. Previous work has combined these with G2P-generated pronunciations [9, 10], or added into G2P training examples [7, 11, 12]. In the work we describe here, we use candidates from both G2P and phonetic decoding.\nThe aspect of the problem that we focus on is candidate pruning. That is, given a set of pronunciation candidates from G2P and phonetic decoding (and maybe some from a manually created lexicon), which subset should we keep? Keeping all the pronunciations is impractical because it would make decoding slow, and also because too many pronunciations tend to hurt ASR performance, even when pronunciation probabilities are used [13].\nPrevious work on candidate pruning has relied on estimated pronunciation probabilities to determine which candidates should be cut [11, 6, 8, 7, 12]. The main defect with this is that for words with multiple pronunciations, it tends to give us too many minor pronunciation variants (e.g. reflecting coarticulation effects), which is undesirable for ASR. If we rely on pronunciation probabilities alone it is hard to discard those types of variants while keeping variants that come from different meanings of the word.\nThe core idea of this paper is a likelihood-based criterion for pronunciation-candidate pruning that naturally keeps candidates that are \u201cfar apart\u201d.\nThis paper is organized as follows. We discuss how we generate pronunciation candidates in Section 2; we explain how we collect acoustic evidence from training data in Section 3. We explain our likelihood-based pronunciation selection strategy in Section 4. Experimental results on various ASR tasks are provided in Section 5, and we conclude in Section 6."}, {"heading": "2. Collecting pronunciation candidates from multiple sources", "text": "In our framework, like [10], we first extend the seed lexicon to include OOV words in the training data, using a G2P model trained on the seed lexicon, and then train an acoustic model (AM) using the G2P-extended lexicon. Then we generate alignments for all training data, based on which we then train a bigram phone language model (LM). Using this phone LM and the AM, we construct a phonetic decoder and use it to generate phonetic transcription of training data. For each individual word token in the transcript, we can align it with a phone sequence using timing information from the alignments and phonetic transcriptions. Then for each specific word w, we can compute the\nar X\niv :1\n70 6.\n03 74\n7v 1\n[ cs\n.C L\n] 1\n2 Ju\nn 20\n17\nrelative frequency of each phone sequence being aligned to it, by normalizing each phone sequence\u2019s count by the most frequent phone sequence\u2019s count. Then we filter out those phone sequences whose relative frequency is too low (e.g. smaller than 0.1) and keep the left ones as the alternative pronunciations generated from phonetic decoding. Then we combine these alternative pronunciation candidates with the G2P-extended lexicon into a large lexicon (called combined lexicon). For each word w from the combined lexicon, let B denote the set of pronunciation candidates collected from multiple sources, and b denote one pronunciation (baseform) candidate. The source of b (denoted as s(b)) could be one of the three: G2P/phonetic decoding. In the next section we will specify how we collect acoustic evidence for all pronunciation candidates in B ."}, {"heading": "3. Acoustic evidence collection", "text": "First we introduce some notations. Let O = {O1,O2, ...,OM} denote acoustic sequences; Mw denote the number of utterances in O which contain the word w 1; Then we further define \u03b8wb , p(w, b) as the pronunciation probability of a pronunciation b for a word w ( \u2211 b\u2208B \u03b8wb = 1), and \u03b8w , {\u03b8wb : b \u2208 B} as the pronunciation model for word w. We define \u03c4uwb , p(Ou|w, b) as the conditional data likelihood given the pronunciation ofw being b, which is determined by the acoustic model. This is the \u201dacoustic evidence\u201d we want to derive from lattice statistics, which is needed by our pronunciation selection algorithm.\nWith the combined lexicon and an existing AM (the one we used for phonetic decoding in the candidate collection phase), we generate lattices for each training utterance. This lattice generation treats distinct pronunciations of words as distinct symbols for the purposes of lattice determinization, unlike our standard procedure described in [14]. This is achieved by putting both phone symbols and word symbols as the input sequence on the FST prior to lattice determinization. From the lattices, we can obtain per-utterance lattice pronunciation-posterior statistics \u03b3uwb , p(w, b|Ou).\nWhen the lattices were generated, we assign uniform priors over all pronunciation candidates of each word in the combined lexicon. By Bayes\u2019 rule, we can directly use the posterior statistics \u03b3uwb as the likelihoods \u03c4uwb 2. Because lattices are pruned,\n1we assume that each word appears in each utterance\u2019s transcript at most once. In practice, if a word appears multiple times in an utterance, we divide the utterance into sub-utterances where each one only contains one token of the word.\n2Strictly speaking, Bayes\u2019 rule only gives us \u03c4uwb \u221d \u03b3uwb, i.e.\na posterior \u03b3uwb could be zero even if w actually appears in a utterance u. So we always floor \u03b3uwb to a small positive scalar \u03b4 (In practice it\u2019s set between 10\u22127 and 10\u22125), so that we have \u03c4uwb \u2265 \u03b4,\u2200u,w, b.\nBased on \u03b3uwb, we can obtain another useful statistic, the average pronunciation posterior \u03b3wb , 1Mw \u2211 u \u03b3uwb, where\nthe summation \u2211 u is only taken over those utterances where the word w actually appears. After the lattices were dumped, for each word, we prune away its pronunciations whose average posterior \u03b3wb is too low (e.g. only keeping the top 10), construct a new combined lexicon, and then re-generate the lattices and re-collect acoustic evidence in the same way. We found this pruning is always helpful as it improves the accuracy of the posteriors."}, {"heading": "4. Data-likelihood-reduction based greedy pronunciation selection", "text": "We formulate the pronunciation selection process as a greedy model selection procedure, with data-likelihood-reduction as the selection criterion. In this section, we\u2019ll first specify how to compute the optimal data likelihood given a set of pronunciation candidates using EM and propose a pronunciation selection criterion based on likelihood reduction, and then use an illustrative example to compare the proposed selection criterion against other criteria. At last we talk about some practical issues in our algorithm, and summarize the whole iterative framework of pronunciation selection."}, {"heading": "4.1. A pronunciation selection criterion based on perutterance likelihood reduction", "text": "Given a set of pronunciation candidates for a specific word w, and the conditional likelihood \u03c4uwb (acoustic evidence) for each utterance Ou, we want to maximize the total data likelihood over the pronunciation model \u03b8w 3:\nL(\u03b8w) = \u2211 u log (\u2211 b \u03c4uwb\u03b8wb ) (1)\nwhere the summation \u2211 u is only taken over utterances where the word w actually appears. Since maximizing this objective doesn\u2019t have a closed form solution, like [8], we use EM which maximizes the following auxiliary function instead (n stands for the iteration index, \u03bbnuwb , p(w, b|Ou,\u03b8nw) is the pronunciation posterior computed at the nth iteration)\nQ(\u03b8n+1w ,\u03b8 n w) = \u2211 u \u2211 b \u03bbnuwb log \u03b8 n+1 wb (2)\nMaximizing the above function with the constraint \u2211 b \u03b8 n+1 wb = 1 gives the M-step:\n\u03b8n+1wb \u2190 \u2211 u \u03bb n uwb\u2211\nu \u2211 b \u03bb n uwb\n(3)\nAccording to Bayes\u2019 rule, we compute the updated posteriors \u03bbn+1uwb as the following:\n\u03bbn+1uwb \u2190 \u03c4uwb\u03b8 n+1 wb\u2211\nb \u03c4uwb\u03b8 n+1 wb\n(4)\n\u03c4uwb can only be treated as \u03b3uwb up to a constant, but the constant doesn\u2019t affect the objective (1) we want to optimize.\n3When we optimize the pronunciation probabilities for a specific word, we consider the pronunciation probabilities for other words as fixed.\nwhich is the E-step. By running (3) and (4) iteratively until convergence, we can find an optimal pronunciation model \u03b8\u2217w, and evaluate the optimal log-likelihood (1) L(\u03b8\u2217w) (denoted as L\u2217 for simplicity). In order to evaluate the importance of a specific pronunciation, say, b, we remove b from the pronunciation candidate set B, re-initialize the pronunciation model \u03b8 \u2032 w on top of B\\b , and run EM to optimize (1) with the model \u03b8 \u2032 w. Writing the likelihood at convergence after removing b as L\u2217b , we can compute the per-utterance likelihood reduction associated with the pronunciation b as:\n\u2206Lb , \u2206Lb Mw = L\u2217 \u2212 L\u2217b Mw ,\nThis metric reflects the contribution of each pronunciation to the total data likelihood. With this metric, we can iteratively remove least important pronunciations in a greedy fashion, which is efficient. The complete iterative framework is given in Section 4.4."}, {"heading": "4.2. An illustrative example", "text": "Here we show an example to illustrate the advantage of pronunciation selection based on the per-utterance log likelihood reduction \u2206Lb over the learned pronunciation probabilities \u03b8\u2217w, in terms of dealing with confusability of pronunciation variants.\nIn Table 1, we listed the pronunciation candidates, average pronunciation posteriors, learned pronunciation probabilities, and the per-utterance log likelihood reduction of two English words \u2018machine\u2019 and \u2018us\u2019 taken from the TED-LIUM [15] training corpus. Note that the two pronunciations of \u2018machine\u2019 only differ in one vowel, while the two pronunciations of \u2018us\u2019 represent two distinct meanings.\nWe want a selection criterion under which it\u2019s possible to put a threshold to rule out the reduction \u2018M IH SH IY N\u2019 (generated from phonetic-decoding) in the \u2018machine\u2019 case, while keeping the acronym \u2018Y UW EH S\u2019 in the \u2018us\u2019 case. Looking at the learned pronunciation probabilities \u03b8\u2217w, it gives lower values for \u2018Y UW EH S\u2019 than \u2018M IH SH IY N\u2019, and thereby cannot serve as the criterion we need. However, the per-utterance log likelihood reduction \u2206Lb of \u2018AH S\u2019 is much larger than \u2018M IH SH IY N\u2019 (0.034 v.s. 0.004). Thus it\u2019s possible to set a proper threshold on \u2206Lb to keep \u2018AH S\u2019 and remove \u2018M IH SH IY N\u2019.\nThe underlying reason is that the confusability between pronunciations is reflected in the sharpness of the per-utterance pronunciation posteriors \u03b3uwb. In the \u2018us\u2019 case, the two pronunciation variants cannot easily model each other, and therefore the posteriors are very sharp for most examples. Thereby removing the minor pronunciation \u2018Y UW EH S\u2019 would result in a greater reduction in the data likelihood. Thus, beyond reflecting the relative frequency, the proposed criterion \u2206Lb is capable of modeling the confusability between pronunciation candidates, which is preferable from the Maximum Likelihood point of view and therefore could help us to select an informative set of pronunciations."}, {"heading": "4.3. Refining the pronunciation selection criterion \u2206Lb", "text": "One difficulty of directly using \u2206Lb in an iterative pronunciation selection framework is that, we need to develop an interpretable threshold T in order to decide when to stop removing pronunciations. However, we notice the upper bound of \u2206Lb can be achieved in an extreme case, where we remove an absolutely dominating pronunciation p (meaning: the observed conditional likelihoods satisfy: \u03c4uwp = 1, \u03c4uwb = \u03b4, \u2200b 6= p ). Before removing p, it\u2019s obvious from (1) that the maximum\nL(\u03b8\u2217w) = 0 can be reached with \u03b8\u2217w being a one-hot vector s.t. \u03b8wp = 1. After removing p, with the constraint \u2211 b\u2208B\\p \u03b8 \u2032 wb = 1, the log-likelihood is a constant: L(\u03b8 \u2032 w) \u2261 Mw log \u03b4. Then we have: \u2206Lp = (0 \u2212Mw log \u03b4)/Mw = \u2212 log \u03b4. According to this, we scale this upper bound by a scalar \u03b1 between [0, 1] to get an interpretable threshold: T = \u2212\u03b1 log \u03b4 , where \u03b1 = 1 corresponds to the above extreme case, which means, for a pronunciation to be not removed, it would have to be present with probability 1 in 100% instances of the word, and \u03b1 = 0 means we will never remove any pronunciation candidates. In practice, it\u2019s set between 0.005 and 0.2. We also make \u03b1 dependent on the source s(b) of the pronunciation, which enables us to use a more conservatively threshold for selecting pronunciations from a source where the candidates\u2019 quality is lower in general, like phonetic-decoding (pd), e.g. by setting \u03b1g2p = 0.02, \u03b1pd = 0.01. So, we define the \u201cscore\u201d of a pronunciation candidate as \u201chow far away\u201d its \u2206Lb is to the corresponding threshold, i.e.:\nqb , \u2206Lb \u2212 Ts(b) = \u2206Lb\nMw + \u03b2s(b) + \u03b1s(b) log \u03b4\nIn our framework we iteratively prune the pronunciation with the lowest score and terminate pruning when all pronunciation have positive scores. Note that the count Mw is smoothed with a source-dependent scalar \u03b2s(b) (5-15 in practice). The purpose is to keep the score from being to high when Mw is small, so that in general we select fewer pronunciations if we only have a few acoustic examples of a word."}, {"heading": "4.4. Summary: an iterative framework", "text": "The proposed pronunciation selection algorithm, which iteratively prunes pronunciations from the initial candidate set B, is summarized as Algorithm 1 (Bt stands for the selected subset of pronunciation candidates at iteration t)."}, {"heading": "5. Experiments", "text": "In order to evaluate the performance of the proposed lexicon learning framework, a small seed lexicon is built by randomly sampling a small portion (5%) of words from the vocabulary of the expert lexicon of each task. With the seed lexicon, we train a G2P model using Sequitur [5] and apply it to all OOV (w.r.t the seed lexicon) words in the vocabulary of the expert lexicon, to get the \u201dG2P-extended\u201d lexicons.4 A baseline system called G2P-ext is built using a G2P-extended lexicon with the optimal number of variants per-word tuned on dev data, and another baseline system called G2P-1best is built using a G2Pextended lexicon where we only take the top G2P pronunciation\n4In this paper we focus on lexicon learning for alphabetic languages. Thereby a G2P model trained with a small seed lexicon is able to generate pronunciations for most words in the expert lexicon.\nAlgorithm 1 Greedy pronunciation selection set t = 0, B0 = B. While true:\nInitialize \u03b8w uniformly on Bt. Run EM on Bt to get \u03b8\u2217w and the optimal log-likelihood\nL\u2217. For b in Bt:\nInitialize \u03b8 \u2032 w on Bt\\b and run EM to get the opti-\nmal log-likelihood L\u2217b . Compute \u2206Lb = L\u2217 \u2212 L\u2217b Compute qb = \u2206LbMw+\u03b2s(b) + \u03b1s(b) log \u03b4\nIf min b\u2208Bt qb \u2265 0: Output Bt as the optimal pronunciation subset. Break. Else: b\u0302 = arg minb\u2208Btqb: Bt+1 = Bt\\b\u0302 t = t+ 1\nfor each word. With this G2P model and acoustic training data for each task, we can build a learned lexicon using the proposed framework, and then train an ASR system called \u201cLex-learn\u201d. Besides, we have an ASR system trained using the full expert lexicon as the \u201cOracle\u201d system. Note that the training recipes of three ASR systems (G2P-ext, G2P-1best, Oracle, and Lexlearn) for each task only differ in the lexicons (with the same vocabulary). All experiments were done with Kaldi [16].\nWe conduct experiments on the Librispeech-460 task [17]. For each lexicon condition, we use the 460h training data subset to build speaker-adaptive trained GMM (SAT) models (the same AM training recipe as the \u201dSAT 460\u201d from [17]), on top of which we then train sub-sampled time-delay neural networks (TDNNs) [18] with the lattice-free MMI (LF-MMI) [19] criterion. The WERs are shown in Table 2. It can be seen that the learned lexicon performs better than G2P-extended lexicons, and is close to the oracle lexicon. And the LF-MMI systems are much more robust to the lexicon quality than SAT systems, i.e. the G2P-extended and learned lexicons perform closer to the expert lexicon. The learned lexicon closes 88% (SAT)/ 36%(LFMMI) of the WER gap between the G2P-ext system and the oracle system. Also, looking at the average number of pronunciations per word, the learned lexicon (1.42) is much more compact than the G2P-extended lexicon (5.05), and is very close to the G2P-1best lexicon (1), though it performs much better than the G2P-1best lexicon by a large gap: 20.9% (SAT) / 7.1%(LFMMI) relatively in WER.\nIn Table 3, we compare the proposed framework with more\nbaseline lexicon expansion approaches, on the Librispeech-460 task (WER of SAT systems), with a smaller seed lexicon containing only 1%(2K) randomly sampled words from the same expert lexicon, in order to make the performance gap between different systems more noticeable. \u201cG2P-ext\u201d, as described before, is a baseline built with a G2P-extended lexicon (with a tuned size). \u201cpp-based selection on G2P candidates\u201d means, we first align acoustic training data with a large G2P-extended lexicon containing all G2P generated candidates (up to 10 candidates per word), and then use max-normalized pronunciation probabilities [11] to prune those candidates for each OOV word, with a tuned threshold (0.4). The pronunciation candidate pool here is the same as the G2P-ext system (i.e. G2P candidates only). \u201cpp-based selection on G2P+PD candidates\u201d uses the same lexicon expansion approach as the former one but we also add candidates from phonetic decoding (PD) before selection. Therefore this baseline has the same candidate pool as the proposed framework. The last system \u201clikelihood-reduction-based selection on G2P+PD candidates\u201d is the proposed framework (i.e. the \u201cLex-learn\u201d systems listed before). For fair comparison, under different lexicon conditions, the acoustic models were re-trained on top of the same acoustic model (the one used in the shown G2P-ext system). It can be seen that adding PD candidates to the candidate pool is crucial to the lexicon quality (0.82% WER improvement), and the proposed pronunciation selection method solely brings 0.18% WER gain and lowers the number of pronunciations per word from 5.43 to 1.59."}, {"heading": "6. Conclusion and future work", "text": "In this paper, we propose an acoustic-data driven lexicon learning framework using a likelihood-reduction based criterion for selecting pronunciation candidates from multiple sources, i.e. G2P and phonetic decoding. With the proposed criterion, the pronunciation candidates are pruned iteratively in a greedy way, based on the acoustic data likelihood reduction caused by removing each candidate. This approach enables us to construct a compact yet informative lexicon. Experiments on different ASR tasks show that, with the proposed framework, starting with a small expert lexicon (containing 0.88K to10K words), we are able to learn a lexicon which performs closer to a full expert lexicon in terms of WER performance on test data, than lexicons built using G2P alone or with a pruning criterion based on pronunciation probabilities. As future work, we\u2019d like to investigate how the amount of training data affects the lexicon learning performance."}, {"heading": "7. References", "text": "[1] M. J. Gales, K. M. Knill, and A. Ragni, \u201cUnicode-based\ngraphemic systems for limited resource languages,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5186\u20135190.\n[2] D. F. Harwath and J. R. Glass, \u201cSpeech recognition without a lexicon-bridging the gap between graphemic and phonetic systems.\u201d in INTERSPEECH, 2014, pp. 2655\u20132659.\n[3] C.-y. Lee, T. J. O\u2019Donnell, and J. Glass, \u201cUnsupervised lexicon discovery from acoustic input,\u201d Transactions of the Association for Computational Linguistics, vol. 3, pp. 389\u2013403, 2015.\n[4] C.-y. Lee, Y. Zhang, and J. R. Glass, \u201cJoint learning of phonetic units and word pronunciations for asr.\u201d in EMNLp, 2013, pp. 182\u2013 192.\n[5] M. Bisani and H. Ney, \u201cJoint-sequence models for grapheme-tophoneme conversion,\u201d Speech communication, vol. 50, no. 5, pp. 434\u2013451, 2008.\n[6] L. Lu, A. Ghoshal, and S. Renals, \u201cAcoustic data-driven pronunciation lexicon for large vocabulary speech recognition,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 374\u2013379.\n[7] N. Goel, S. Thomas, M. Agarwal, P. Akyazi, L. Burget, K. Feng, A. Ghoshal, O. Glembek, M. Karafia\u0301t, D. Povey et al., \u201cApproaches to automatic lexicon learning with limited training examples,\u201d in Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 2010, pp. 5094\u2013 5097.\n[8] I. McGraw, I. Badr, and J. R. Glass, \u201cLearning lexicons from speech using a pronunciation mixture model,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 2, pp. 357\u2013366, 2013.\n[9] R. Rasipuram et al., \u201cCombining acoustic data driven g2p and letter-to-sound rules for under resource lexicon generation,\u201d in Proceedings of INTERSPEECH, no. EPFL-CONF-192596, 2012.\n[10] A. Laurent, S. Meignier, T. Merlin, P. Dele\u0301glise, and F. Spe\u0301cinovTre\u0301laze\u0301, \u201cAcoustics-based phonetic transcription method for proper nouns.\u201d in INTERSPEECH, 2010, pp. 2286\u20132289.\n[11] G. Chen, D. Povey, and S. Khudanpur, \u201cAcoustic data-driven pronunciation lexicon generation for logographic languages,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5350\u20135354.\n[12] S. Tsujioka, S. Sakti, K. Yoshino, G. Neubig, and S. Nakamura, \u201cUnsupervised joint estimation of grapheme-to-phoneme conversion systems and acoustic model adaptation for non-native speech recognition,\u201d Interspeech 2016, pp. 3091\u20133095, 2016.\n[13] T. Hain, \u201cImplicit pronunciation modelling in asr,\u201d in ISCA Tutorial and Research Workshop (ITRW) on Pronunciation Modeling and Lexicon Adaptation for Spoken Language Technology, 2002.\n[14] D. Povey, M. Hannemann, G. Boulianne, L. Burget, A. Ghoshal, M. Janda, M. Karafia\u0301t, S. Kombrink, P. Motl\u0131\u0301c\u030cek, Y. Qian et al., \u201cGenerating exact lattices in the wfst framework,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 4213\u20134216.\n[15] A. Rousseau, P. Dele\u0301glise, and Y. Este\u0300ve, \u201cTed-lium: an automatic speech recognition dedicated corpus.\u201d in LREC, 2012, pp. 125\u2013 129.\n[16] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motl\u0131\u0301c\u030cek, Y. Qian, P. Schwarz et al., \u201cThe kaldi speech recognition toolkit,\u201d Proc. ASRU, 2011.\n[17] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5206\u20135210.\n[18] V. Peddinti, D. Povey, and S. Khudanpur, \u201cA time delay neural network architecture for efficient modeling of long temporal contexts.\u201d in INTERSPEECH, 2015, pp. 3214\u20133218.\n[19] D. Povey, V. Peddinti, D. Galvez, P. Ghahrmani, V. Manohar, X. Na, Y. Wang, and S. Khudanpur, \u201cPurely sequence-trained neural networks for asr based on lattice-free mmi,\u201d INTERSPEECH, 2016."}], "references": [{"title": "Unicode-based graphemic systems for limited resource languages", "author": ["M.J. Gales", "K.M. Knill", "A. Ragni"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5186\u20135190.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech recognition without a lexicon-bridging the gap between graphemic and phonetic systems.", "author": ["D.F. Harwath", "J.R. Glass"], "venue": "INTERSPEECH,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["C.-y. Lee", "T.J. O\u2019Donnell", "J. Glass"], "venue": "Transactions of the Association for Computational Linguistics, vol. 3, pp. 389\u2013403, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint learning of phonetic units and word pronunciations for asr.", "author": ["C.-y. Lee", "Y. Zhang", "J.R. Glass"], "venue": "EMNLp,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Joint-sequence models for grapheme-tophoneme conversion", "author": ["M. Bisani", "H. Ney"], "venue": "Speech communication, vol. 50, no. 5, pp. 434\u2013451, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition", "author": ["L. Lu", "A. Ghoshal", "S. Renals"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 374\u2013379.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Approaches to automatic lexicon learning with limited training examples", "author": ["N. Goel", "S. Thomas", "M. Agarwal", "P. Akyazi", "L. Burget", "K. Feng", "A. Ghoshal", "O. Glembek", "M. Karafi\u00e1t", "D. Povey"], "venue": "Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 2010, pp. 5094\u2013 5097.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning lexicons from speech using a pronunciation mixture model", "author": ["I. McGraw", "I. Badr", "J.R. Glass"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 2, pp. 357\u2013366, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining acoustic data driven g2p and letter-to-sound rules for under resource lexicon generation", "author": ["R. Rasipuram"], "venue": "Proceedings of INTERSPEECH, no. EPFL-CONF-192596, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1925}, {"title": "Sp\u00e9cinov- Tr\u00e9laz\u00e9, \u201cAcoustics-based phonetic transcription method for proper nouns.", "author": ["A. Laurent", "S. Meignier", "T. Merlin", "P. Del\u00e9glise"], "venue": "INTERSPEECH,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Acoustic data-driven pronunciation lexicon generation for logographic languages", "author": ["G. Chen", "D. Povey", "S. Khudanpur"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5350\u20135354.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised joint estimation of grapheme-to-phoneme conversion systems and acoustic model adaptation for non-native speech recognition", "author": ["S. Tsujioka", "S. Sakti", "K. Yoshino", "G. Neubig", "S. Nakamura"], "venue": "Interspeech 2016, pp. 3091\u20133095, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Implicit pronunciation modelling in asr", "author": ["T. Hain"], "venue": "ISCA Tutorial and Research Workshop (ITRW) on Pronunciation Modeling and Lexicon Adaptation for Spoken Language Technology, 2002.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Generating exact lattices in the wfst framework", "author": ["D. Povey", "M. Hannemann", "G. Boulianne", "L. Burget", "A. Ghoshal", "M. Janda", "M. Karafi\u00e1t", "S. Kombrink", "P. Motl\u0131\u0301\u010dek", "Y. Qian"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 4213\u20134216.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motl\u0131\u0301\u010dek", "Y. Qian", "P. Schwarz"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Librispeech: an asr corpus based on public domain audio books", "author": ["V. Panayotov", "G. Chen", "D. Povey", "S. Khudanpur"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5206\u20135210.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "A time delay neural network architecture for efficient modeling of long temporal contexts.", "author": ["V. Peddinti", "D. Povey", "S. Khudanpur"], "venue": "INTERSPEECH,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Purely sequence-trained neural networks for asr based on lattice-free mmi", "author": ["D. Povey", "V. Peddinti", "D. Galvez", "P. Ghahrmani", "V. Manohar", "X. Na", "Y. Wang", "S. Khudanpur"], "venue": "INTERSPEECH, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In order to develop ASR systems under limited lexicon resources, one solution is to adopt a graphemic lexicon [1, 2] or acoustic unit discovery methods [3, 4], which totally eliminate the expert efforts for developing a phonetic pronunciation lexicon.", "startOffset": 110, "endOffset": 116}, {"referenceID": 1, "context": "In order to develop ASR systems under limited lexicon resources, one solution is to adopt a graphemic lexicon [1, 2] or acoustic unit discovery methods [3, 4], which totally eliminate the expert efforts for developing a phonetic pronunciation lexicon.", "startOffset": 110, "endOffset": 116}, {"referenceID": 2, "context": "In order to develop ASR systems under limited lexicon resources, one solution is to adopt a graphemic lexicon [1, 2] or acoustic unit discovery methods [3, 4], which totally eliminate the expert efforts for developing a phonetic pronunciation lexicon.", "startOffset": 152, "endOffset": 158}, {"referenceID": 3, "context": "In order to develop ASR systems under limited lexicon resources, one solution is to adopt a graphemic lexicon [1, 2] or acoustic unit discovery methods [3, 4], which totally eliminate the expert efforts for developing a phonetic pronunciation lexicon.", "startOffset": 152, "endOffset": 158}, {"referenceID": 4, "context": "Given a small expert lexicon, the most straightforward way to generate pronunciation candidates for OOV words is to train a Grapheme-to-Phoneme (G2P) [5] model using the seed lexicon", "startOffset": 150, "endOffset": 153}, {"referenceID": 5, "context": "and apply it to these OOV words [6, 7, 8].", "startOffset": 32, "endOffset": 41}, {"referenceID": 6, "context": "and apply it to these OOV words [6, 7, 8].", "startOffset": 32, "endOffset": 41}, {"referenceID": 7, "context": "and apply it to these OOV words [6, 7, 8].", "startOffset": 32, "endOffset": 41}, {"referenceID": 8, "context": "Previous work has combined these with G2P-generated pronunciations [9, 10], or added into G2P training examples [7, 11, 12].", "startOffset": 67, "endOffset": 74}, {"referenceID": 9, "context": "Previous work has combined these with G2P-generated pronunciations [9, 10], or added into G2P training examples [7, 11, 12].", "startOffset": 67, "endOffset": 74}, {"referenceID": 6, "context": "Previous work has combined these with G2P-generated pronunciations [9, 10], or added into G2P training examples [7, 11, 12].", "startOffset": 112, "endOffset": 123}, {"referenceID": 10, "context": "Previous work has combined these with G2P-generated pronunciations [9, 10], or added into G2P training examples [7, 11, 12].", "startOffset": 112, "endOffset": 123}, {"referenceID": 11, "context": "Previous work has combined these with G2P-generated pronunciations [9, 10], or added into G2P training examples [7, 11, 12].", "startOffset": 112, "endOffset": 123}, {"referenceID": 12, "context": "That is, given a set of pronunciation candidates from G2P and phonetic decoding (and maybe some from a manually created lexicon), which subset should we keep? Keeping all the pronunciations is impractical because it would make decoding slow, and also because too many pronunciations tend to hurt ASR performance, even when pronunciation probabilities are used [13].", "startOffset": 360, "endOffset": 364}, {"referenceID": 10, "context": "Previous work on candidate pruning has relied on estimated pronunciation probabilities to determine which candidates should be cut [11, 6, 8, 7, 12].", "startOffset": 131, "endOffset": 148}, {"referenceID": 5, "context": "Previous work on candidate pruning has relied on estimated pronunciation probabilities to determine which candidates should be cut [11, 6, 8, 7, 12].", "startOffset": 131, "endOffset": 148}, {"referenceID": 7, "context": "Previous work on candidate pruning has relied on estimated pronunciation probabilities to determine which candidates should be cut [11, 6, 8, 7, 12].", "startOffset": 131, "endOffset": 148}, {"referenceID": 6, "context": "Previous work on candidate pruning has relied on estimated pronunciation probabilities to determine which candidates should be cut [11, 6, 8, 7, 12].", "startOffset": 131, "endOffset": 148}, {"referenceID": 11, "context": "Previous work on candidate pruning has relied on estimated pronunciation probabilities to determine which candidates should be cut [11, 6, 8, 7, 12].", "startOffset": 131, "endOffset": 148}, {"referenceID": 9, "context": "In our framework, like [10], we first extend the seed lexicon to include OOV words in the training data, using a G2P model trained on the seed lexicon, and then train an acoustic model (AM) using the G2P-extended lexicon.", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "This lattice generation treats distinct pronunciations of words as distinct symbols for the purposes of lattice determinization, unlike our standard procedure described in [14].", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "doesn\u2019t have a closed form solution, like [8], we use EM which maximizes the following auxiliary function instead (n stands for the iteration index, \u03bbuwb , p(w, b|Ou,\u03b8 w) is the pronunciation posterior computed at the nth iteration)", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "According to this, we scale this upper bound by a scalar \u03b1 between [0, 1] to get an interpretable threshold: T = \u2212\u03b1 log \u03b4 , where \u03b1 = 1 corresponds to the above extreme case, which means, for a pronunciation to be not removed, it would have to be present with probability 1 in 100% instances of the word, and \u03b1 = 0 means we will never remove any pronunciation candidates.", "startOffset": 67, "endOffset": 73}, {"referenceID": 4, "context": "With the seed lexicon, we train a G2P model using Sequitur [5] and apply it to all OOV (w.", "startOffset": 59, "endOffset": 62}, {"referenceID": 14, "context": "All experiments were done with Kaldi [16].", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "We conduct experiments on the Librispeech-460 task [17].", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "For each lexicon condition, we use the 460h training data subset to build speaker-adaptive trained GMM (SAT) models (the same AM training recipe as the \u201dSAT 460\u201d from [17]), on top of which we then train sub-sampled time-delay neural networks (TDNNs) [18] with the lattice-free MMI (LF-MMI) [19] criterion.", "startOffset": 167, "endOffset": 171}, {"referenceID": 16, "context": "For each lexicon condition, we use the 460h training data subset to build speaker-adaptive trained GMM (SAT) models (the same AM training recipe as the \u201dSAT 460\u201d from [17]), on top of which we then train sub-sampled time-delay neural networks (TDNNs) [18] with the lattice-free MMI (LF-MMI) [19] criterion.", "startOffset": 251, "endOffset": 255}, {"referenceID": 17, "context": "For each lexicon condition, we use the 460h training data subset to build speaker-adaptive trained GMM (SAT) models (the same AM training recipe as the \u201dSAT 460\u201d from [17]), on top of which we then train sub-sampled time-delay neural networks (TDNNs) [18] with the lattice-free MMI (LF-MMI) [19] criterion.", "startOffset": 291, "endOffset": 295}, {"referenceID": 10, "context": "\u201cpp-based selection on G2P candidates\u201d means, we first align acoustic training data with a large G2P-extended lexicon containing all G2P generated candidates (up to 10 candidates per word), and then use max-normalized pronunciation probabilities [11] to prune those candidates for each OOV word, with a tuned threshold (0.", "startOffset": 246, "endOffset": 250}], "year": 2017, "abstractText": "Speech recognition systems for irregularly-spelled languages like English normally require hand-written pronunciations. In this paper, we describe a system for automatically obtaining pronunciations of words for which pronunciations are not available, but for which transcribed data exists. Our method integrates information from the letter sequence and from the acoustic evidence. The novel aspect of the problem that we address is the problem of how to prune entries from such a lexicon (since, empirically, lexicons with too many entries do not tend to be good for ASR performance). Experiments on various ASR tasks show that, with the proposed framework, starting with an initial lexicon of several thousand words, we are able to learn a lexicon which performs close to a full expert lexicon in terms of WER performance on test data, and is better than lexicons built using G2P alone or with a pruning criterion based on pronunciation probability.", "creator": "LaTeX with hyperref package"}}}