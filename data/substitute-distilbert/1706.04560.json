{"id": "1706.04560", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Neural Models for Key Phrase Detection and Question Generation", "abstract": "we propose several neural models arranged in a two - panel framework to tackle question generation from documents. first, we estimate the probability amplitude \" modifying \" answers in a document using a neural model trained on computer question - answering corpus. the predicted key phrases are then used as answers uniquely condition a node - through - sequence question generation model. empirically, our neural key item detection models significantly outperform an entity - tagging baseline system. we demonstrate that the question generator formulates good quality natural language questions from standard document vocabulary. the resulting questions and answers can be used to assess reading depth in educational settings.", "histories": [["v1", "Wed, 14 Jun 2017 16:06:18 GMT  (26kb)", "http://arxiv.org/abs/1706.04560v1", null], ["v2", "Thu, 14 Sep 2017 17:43:48 GMT  (371kb,D)", "http://arxiv.org/abs/1706.04560v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["sandeep subramanian", "tong wang", "xingdi yuan", "saizheng zhang", "adam trischler", "yoshua bengio"], "accepted": false, "id": "1706.04560"}, "pdf": {"name": "1706.04560.pdf", "metadata": {"source": "CRF", "title": "Neural Models for Key Phrase Detection and Question Generation", "authors": ["Sandeep Subramanian", "Tong Wang", "Xingdi Yuan", "Saizheng Zhang", "Adam Trischler"], "emails": ["sandeep.subramanian.1@umontreal.ca", "tong.wang@microsoft.com", "eric.yuan@microsoft.com", "saizheng.zhang@umontreal.ca", "adam.trischler@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n04 56\n0v 1\n[ cs\n.C L\n] 1\n4 Ju\nn 20\n17\nWe propose several neural models arranged in a two-stage framework to tackle question generation from documents. First, we estimate the probability of \u201cinteresting\u201d answers in a document using a neural model trained on a questionanswering corpus. The predicted key phrases are then used as answers to condition a sequence-to-sequence question generation model. Empirically, our neural key phrase detection models significantly outperform an entity-tagging baseline system. We demonstrate that the question generator formulates good quality natural language questions from extracted key phrases. The resulting questions and answers can be used to assess reading comprehension in educational settings."}, {"heading": "1 Introduction", "text": "Many educational applications can benefit from automatic question generation, including vocabulary assessment (Brown et al., 2005), writing support (Liu et al., 2012), and assessment of reading comprehension (Mitkov and Ha, 2003; Kunichika et al., 2004). Formulating questions that test for certain skills at certain levels requires significant human effort that is difficult to scale, e.g., to massive open online courses (MOOCs). Despite their applications, the majority of existing models for automatic question generation rely on rule-based methods that likewise do not scale well across different domains and/or writing styles. To address this limitation, we propose and compare several end-to-end-trainable neural models for automatic question generation.\n\u2217 Project done during internship at Microsoft Maluuba.\nWe focus specifically on the assessment of reading comprehension. In this domain, question generation typically involves two inter-related components: first, a system to identify interesting entities or events (key phrases) within a passage or document (Becker et al., 2012); second, a question generator that constructs questions in natural language that ask specifically about the given key phrases. Key phrases thus act as the \u201ccorrect\u201d answers for generated questions. This procedure ensures that we can assess student performance against ground-truth targets.\nWe formulate key phrase detection as modeling the probability of potential answers conditioned on a given document, i.e., P (a|d). Inspired by successful work in question answering, we propose a sequence-to-sequence model that generates a set of key-phrase boundaries. This model can flexibly select an arbitrary number of key phrases from a document. To teach it to assign high probability to interesting answers, we train it on human-selected answers from a largescale, crowd-sourced question-answering dataset (SQuAD). We thus take a purely data-driven approach to the concept of interestingness, working from the premise that crowdworkers tend to select entities or events that interest them when they formulate their own comprehension questions. If this premise is correct, then the growing collection of crowd-sourced question-answering datasets (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Lai et al., 2017) can be harnessed to learn models for key phrases of interest to human readers.\nGiven a set of extracted key phrases, we approach question generation by modeling the conditional probability of a question given a document-answer pair, i.e., P (q|a, d). For this we use a sequence-to-sequence model with attention (Bahdanau et al., 2014) and the pointer-softmax\nmechanism (Gulcehre et al., 2016). This component is also trained on SQuAD, by maximizing the likelihood of questions in the dataset.\nEmpirically, our proposed model for key phrase detection outperforms two baseline systems by a significant margin. We support these quantitative findings with qualitative examples of generated question-answer pairs given documents."}, {"heading": "2 Related Work", "text": "Automatic question generation systems are often used to alleviate (or even eliminate) the burden of human generation of questions to assess reading comprehension (Mitkov and Ha, 2003; Kunichika et al., 2004). Various NLP techniques have been adopted in these systems to improve generation quality, including parsing (Heilman and Smith, 2010a; Mitkov and Ha, 2003), semantic role labeling (Lindberg et al., 2013), and the use of lexicographic resources like WordNet (Miller, 1995; Mitkov and Ha, 2003). However, the majority of proposed methods resort to simple rule-based techniques such as slot-filling with templates (Lindberg et al., 2013; Chali and Golestanirad, 2016; Labutov et al., 2015) or syntactic transformation heuristics (Agarwal and Mannem, 2011; Ali et al., 2010) (e.g., subject-auxiliary inversion, Heilman and Smith 2010a). These techniques can be inadequate to capture the diversity and high variance of natural language questions.\nTo address this limitation, end-to-endtrainable neural models have recently been proposed for question generation in both vision (Mostafazadeh et al., 2016) and language. For the latter, Du et al. (2017) used a sequence-tosequence model with an attention mechanism derived from the encoder states. Yuan et al. (2017) proposed a similar architecture but in addition improved model performance through policy gradient techniques.\nMeanwhile, a highly relevant yet less explored aspect of question generation is to identify which parts of a given document are important or interesting for asking questions. Existing studies formulate the task as a ranking problem with the help of crowd-sourcing. Heilman and Smith (2010b) asked crowdworkers to rate the acceptability of computer-generated natural language questions as quiz questions, while Becker et al. (2012) solicited quality ratings of text chunks as potential gaps for\nCloze-style questions.\nThese studies are closely related to our proposed work by the common goal of modeling the distribution of key phrases given a document. The major difference is that previous studies begin with a prescribed list of candidates, which might significantly bias the distribution estimate. In contrast, we adopt a dataset that was originally designed for question answering, where crowdworkers presumably tend to pick entities or events that interest them most. We postulate that the resulting distribution, learned directly from data, is more likely to reflect the true importance and appropriateness of answers."}, {"heading": "3 Model Description", "text": ""}, {"heading": "3.1 Key Phrase Detection", "text": "In this section, we describe a simple baseline as well as the two proposed neural models for extracting key phrases (answers) from documents.\nEntity Tagging Baseline Our baseline model (ENT) predicts all entities tagged by spaCy1 as key phrases. This is motivated by the fact that over 50% of the answers in SQuAD are entities (Table 2 in Rajpurkar et al. 2016). These include dates (September 1967), numeric entities (3, five), people (William Smith), locations (the British Isles) and other entities (Buddhism).\nNeural Entity Selection The baseline model above na\u0131\u0308vely selects all entities as candidate answers. One pitfall is that it exhibits high recall at the expense of precision (Table 1). We first attempt to address this with a neural entity selection model (NES) that selects a subset of entities from a list of candidates. Our neural entity selection model takes a document (i.e., a sequence of words) D = (wd1 , . . . , w d nd ) and a list of ne entities as a sequence of (start, end) locations within the document, E = ((estart1 , e end 1 ), . . . , (e start ne\n, eendne )). The model is then trained on the binary classification task of predicting whether an entity overlaps with any of the gold answers.\nSpecifically, we maximize \u2211ne\ni log(P (ei|D)). We parameterize P (ei|D) using a neural model that first embeds each word wdi in the document as a distributed vector using an embedding lookup table. We then encode the document using a bidirectional Long Short-Term Memory (LSTM)\n1https://spacy.io/docs/usage/entity-recognition\nnetwork (Hochreiter and Schmidhuber, 1997) into annotation vectors (hd1, . . . , h d nd ). We compute P (ei|D) using a multilayer perceptron (MLP) that takes as input the concatenation of three vectors \u3008hdnd ;h d avg ;hei\u3009. Here, h d avg and h d nd are the average and the final state of the annotation vectors, respectively, and hei is the average of the annotation vectors corresponding to the i-th entity (i.e., hd estarti , . . . , hd eendi ).\nDuring inference, we select the top k entities with highest likelihood as given by our model. We use k = 6 in our experiments as determined by hyper-parameter search.\nPointer Networks While a significant fraction of answers in SQuAD are entities, extracting interesting aspects of a document requires looking beyond entities. Many documents of interest may lack entities, or sometimes an entity tagger may fail to recognize some important entities. To remedy this, we build a neural model that is trained from scratch to extract all the answer key phrases in a particular document. We parameterize this model as a pointer network (Vinyals et al., 2015) trained to point sequentially to the start and end locations of all key phrase answers. As in our entity selection model, we first encode the document into a sequence of annotation vectors (hd1, . . . , h d nd ). A decoder LSTM is then trained to point to all of the start and end locations of answers in the document from left to right, conditioned on the annotation vectors, via an attention mechanism. We add a special termination token to the document, which the decoder is trained to attend on when it has generated all key phrases. This provides the flexibility to learn the number of key phrases the model should extract from a particular document. The pointer network parameterization is described in Appendix A.\nDuring inference, we employ a greedy decoding strategy that greedily picks the best location from the softmax vector at every step and post process results to remove duplicate key phrases."}, {"heading": "3.2 Question Generation", "text": "Our question generation model adopts a sequenceto-sequence framework (Sutskever et al., 2014) with an attention mechanism (Bahdanau et al., 2014) and a pointer-softmax decoder (Gulcehre et al., 2016). It is a slightly modified version of the model described by Yuan et al. (2017). The model takes a\ndocument D = (wd1 , . . . , w d nd ) and an answer A = (wa1 , . . . , w a na ) as input, and outputs a question Q = (w\u0302q1, . . . , w\u0302 q nq ).\nAn input word w {d,a} i is first embedded by concatenating both word- and character-level embeddings ei = \u3008e w i ; e ch i \u3009. Character-level information echi is captured with the final states of a BiLSTM on the character sequences of wi. The concatenated embeddings are subsequently encoded with another BiLSTM into annotation vectors hdi .\nTo take better advantage of the extractive nature of answers in documents, we encode the answer by extracting the document encodings at the answer word positions. These extracted vectors are then fed into a condition aggregation BiLSTM to produce the extractive condition encoding hak and we take the final state ha as the answer encoding.\nThe RNN-based decoder employs the pointersoftmax mechanism (Gulcehre et al., 2016). At each generation step, the decoder decides adaptively whether (a) to generate from a decoder vocabulary or (b) to point to a word in the source sequence (and copy over). Details of the decoder architecture can be found in Appendix B."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset", "text": "We conduct our experiments on the SQuAD corpus (Rajpurkar et al., 2016), a machine comprehension dataset consisting of over 100k crowdsourced question-answer pairs on 536 Wikipedia articles. Simple preprocessing is performed, including lower-casing and word tokenization using NLTK (Bird, 2006). The test split of SQuAD is hidden from the public. We therefore take 5,158 question-answer pairs (self-contained in 23 Wikipedia articles) from the training set as validation set, and use the official development data to report test results. Implementation details are reported in Appendix C."}, {"heading": "4.2 Quantitative Evaluation", "text": "Since each key phrase is itself a multi-word unit, a na\u2019\u0301ive word-level F1 score is unsuitable for evaluation due to the variable lengths of the key phrases. We thus propose a new metric called hierarchical F1, which is invariant to target length, by incorporating a notion of alignment between the gold and predicted phrases.\nThe metric is calculated as follows. Given the prediction sequence e\u0302i and the gold label sequence ej , we first construct a pairwise, token-level F1 score fi,j matrix between the two phrases e\u0302i and ej . Max-pooling along the gold-label axis essentially assesses the precision of each prediction, with partial matches accounted for by the pairwise F1 (identical to evaluation of a single answer in SQuAD) in the cells: pi = maxj(fi,j). Analogously, recall for label ej can be defined by max-pooling along the prediction axis: rj = maxi(fi,j). The hierarchical F1 is defined by the mean precision p\u0304 = avg(pi) and recall r\u0304 = avg(rj):\nF1H = 2p\u0304 \u00b7 r\u0304\n(p\u0304+ r\u0304) .\nInvariance to sample length is achieved by having equal weight for the overlap between the aligned phrases regardless of their lengths."}, {"heading": "4.3 Results and Discussions", "text": "Evaluation results are listed in Table 1. As expected, the entity tagging baseline achieved the best recall, likely by over-generating candidate answers. The NES model, on the other hand, exhibits a much larger advantage in precision and\nconsequently outperforms the entity tagging baseline by notable margins in F1. This trend persists in the comparison between the NESmodel and the pointer-network model.\nQualitatively, we observe that the entity-based models have a strong bias toward numeric types, which often fail to capture interesting information in an article. In Table 2 for example, in the upper example, the entity baselines are only able to tag first and one, missing all of the key phrases that are successfully detected by the pointer model.\nIn addition, we also notice that the entity-based systems tend to select the central topical entity as the answer, which can contradict the distribution of interesting answers selected by humans. For example, given a Wikipedia article on Kenya and the fact agriculture is the second largest contributor to kenya \u2019s gross domestic product ( gdp ), the entity-based systems propose kenya as a key phrase and asked what country is nigeria \u2019s second largest contributor to ?2 Given the same information, the pointer model picked agriculture as the answer and asked what is the second largest contributor to kenya \u2019s gross domestic product ?"}, {"heading": "5 Conclusion", "text": "We proposed a two-stage framework to tackle the problem of question generation from documents. First, we use a question answering corpus to train a neural model to estimate the distribution of key phrases that are interesting to question-asking humans. In particular, we propose two neural models, one that ranks entities proposed by an entity\n2Since the answer word kenya cannot appear in the output, the decoder produced a similar word nigeria instead.\ntagger, and another that points to key-phrase start and end boundaries with a pointer network. When compared to an entity tagging baseline, the proposed models exhibit significantly better results.\nWe then adopt a sequence-to-sequence model to generate questions conditioned on the key phrases selected in the first stage. Our question generator is inspired by an attention-based translation model, and uses the pointer-softmax mechanism to dynamically switch between copying a word from the document and generating a word from a vocabulary. Qualitative examples show that the generated questions exhibit both syntactic fluency and semantic relevance to the conditioning documents and answers, and appear useful for assessing reading comprehension in educational settings.\nIn future work we will investigate fine-tuning the complete framework end to end. Another interesting direction is to explore abstractive key phrase detection."}, {"heading": "Acknowledgement", "text": "We would like to thank Professor Yoshua Bengio for the insightful discussions that eventually lead to several basic premises of this study."}, {"heading": "A Pointer Network", "text": "Pointer networks (Vinyals et al., 2015) are an extension of sequence-to-sequence models (Sutskever et al., 2014) in which the target sequence consists of positions in the source sequence. We encode the source sequence document into a sequence of annotation vectors hd = (hd1, . . . h d nd ) using an embedding lookup table followed by a bidirectional LSTM. The decoder also consists of an embedding lookup that is shared with the encoder followed by an unidirectional LSTM with an attention mechanism. We denote the decoder\u2019s annotation vectors as (hp1, h p 2, . . . , h p 2na\u22121 , h p 2na\n) where na is the number of answer key phrases, h\np 1 and h p 2\ncorrespond to the start and end annotation vectors for the first answer key phrase and so on. We parameterize P (wdi = start|h p 1 . . . h p j , h d) and P (wdi = end|h p 1 . . . h p j , h\nd) using the dot product attention mechanism (Luong et al., 2015) between the decoder and encoder annotation vectors.\nP (wdi |h p 1 . . . h p j , h d) = softmax(W1h p j \u00b7 h d)\nwhere W1 is an affine transformation matrix. The inputs at each step of the decoder are words from the document that correspond to the start and end locations pointed to by the decoder."}, {"heading": "B Decoder Details of the Question Generation Model", "text": "With the pointer-softmax mechanims, the decoder in the question generation model consists of a pointing decoder and a generative decoder.\nIn the pointing decoder, recurrence is implemented with two cascading LSTM cells c1 and c2:\ns (t) 1 = c1(y (t\u22121), s (t\u22121) 2 ) (1) s (t) 2 = c2(v (t), s (t) 1 ), (2)\nwhere s (t) 1 and s (t) 2 are the recurrent states, y (t\u22121) is the embedding of decoder output from the previous time step, and v(t) is the context vector (to be defined shortly in Equation (3)).\nAt each time step t, the pointing decoder computes a distribution \u03b1(t) over the document word positions (i.e., a document attention, Bahdanau et al. 2014). Each element is defined as:\n\u03b1 (t) i = f(h d i ,h a, s1 (t\u22121)),\nwhere f is a two-layer MLP with tanh and softmax activation, respectively. The context vector v(t) used in Equation (2) is the sum of the document encoding weighted by the document attention:\nv(t) = n\u2211\ni=1\n\u03b1 (t) i h d i . (3)\nThe generative decoder, on the other hand, defines a distribution over a prescribed decoder vocabulary with a two-layer MLP g:\no(t) = g(y(t\u22121), s (t) 2 ,v (t),ha).\nThe switch scalar s(t) at each time step is computed by a three-layer MLP h:\ns(t) = h(s (t) 2 ,v (t),\u03b1(t),o(t)),\nThe first two layers of h use tanh activation and the final layer uses sigmoid. Highway connections are present between the first and the second layer.3\nFinally, the resulting switch is used to interpolate the pointing and the generative probabilities for predicting the next word:\nP (w\u0302t) \u223c s (t) \u03b1 (t) + (1\u2212 s(t))o(t).\nC Implementation Details\nAll models were trained using stochastic gradient descent with a minibatch size of 32 using the ADAM optimization algorithm (Kingma and Ba, 2014).\nKey Phrase Detection In key phrase detection models we used pretrained word embeddings of 300 dimensions, generated using a word2vec extension (Ling et al., 2015) trained on the English Gigaword 5 corpus. We used bidirectional LSTMs of 256 dimensions (128 forward and backward) to encode the document and an LSTM of 256 dimensions as our decoder in the pointer network model.\n3We also attach the entropy of the softmax distributions to the input of the final layer, postulating that this guides the switching mechanism by indicating the confidence of pointing vs generating. The addition is empirically observed to improve model performance.\nA dropout of 0.5 was used at the outputs of every layer in the network. We also experimented with a beam search decode strategy with a beam size of 5 in our pointer network approach but didn\u2019t observe significant improvements over greedy decoding. We believe that this could be because of short sequence lengths in the pointer network decoder.\nQuestion Generation In question generation, the decoder vocabulary uses the top 2000 words sorted by their frequency in the gold questions in the training data. The word embedding matrix is initialized with the 300-dimensional GloVe vectors (Pennington et al., 2014). The dimensionality of the character representations is 32. The number of hidden units is 384 for both of the encoder/decoder RNN cells. Dropout is applied at a rate of 0.3 to all embedding layers as well as between the hidden states in the encoder/decoder RNNs across time steps."}], "references": [{"title": "Automatic gap-fill question generation from text books", "author": ["Manish Agarwal", "Prashanth Mannem."], "venue": "Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational Linguistics, pages", "citeRegEx": "Agarwal and Mannem.,? 2011", "shortCiteRegEx": "Agarwal and Mannem.", "year": 2011}, {"title": "Automation of question generation from sentences", "author": ["Husam Ali", "Yllias Chali", "Sadid A Hasan."], "venue": "Proceedings of QG2010: The Third Workshop on Question Generation. pages 58\u201367.", "citeRegEx": "Ali et al\\.,? 2010", "shortCiteRegEx": "Ali et al\\.", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR", "citeRegEx": "Cho and Bengio.,? 2014", "shortCiteRegEx": "Cho and Bengio.", "year": 2014}, {"title": "Mind the gap: learning to choose gaps for question generation", "author": ["Lee Becker", "Sumit Basu", "Lucy Vanderwende."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Becker et al\\.,? 2012", "shortCiteRegEx": "Becker et al\\.", "year": 2012}, {"title": "Nltk: the natural language toolkit", "author": ["Steven Bird."], "venue": "Proceedings of the COLING/ACL on Interactive presentation sessions. Association for Computational Linguistics, pages 69\u201372.", "citeRegEx": "Bird.,? 2006", "shortCiteRegEx": "Bird.", "year": 2006}, {"title": "Automatic question generation for vocabulary assessment", "author": ["Jonathan C Brown", "Gwen A Frishkoff", "Maxine Eskenazi."], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. As-", "citeRegEx": "Brown et al\\.,? 2005", "shortCiteRegEx": "Brown et al\\.", "year": 2005}, {"title": "Ranking automatically generated questions using common human queries", "author": ["Yllias Chali", "Sina Golestanirad."], "venue": "The 9th International Natural Language Generation conference. page 217.", "citeRegEx": "Chali and Golestanirad.,? 2016", "shortCiteRegEx": "Chali and Golestanirad.", "year": 2016}, {"title": "Learning to ask: Neural question generation for reading comprehension", "author": ["Xinya Du", "Junru Shao", "Claire Cardie."], "venue": "arXiv preprint arXiv:1705.00106 .", "citeRegEx": "Du et al\\.,? 2017", "shortCiteRegEx": "Du et al\\.", "year": 2017}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1603.08148 .", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Good question! statistical ranking for question generation", "author": ["Michael Heilman", "Noah A Smith."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. As-", "citeRegEx": "Heilman and Smith.,? 2010a", "shortCiteRegEx": "Heilman and Smith.", "year": 2010}, {"title": "Rating computer-generated questions with mechanical turk", "author": ["Michael Heilman", "Noah A Smith."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. Association for Computa-", "citeRegEx": "Heilman and Smith.,? 2010b", "shortCiteRegEx": "Heilman and Smith.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Automated question generation methods for intelligent english learning systems and its evaluation", "author": ["Hidenobu Kunichika", "Tomoki Katayama", "Tsukasa Hirashima", "Akira Takeuchi."], "venue": "Proc. of ICCE.", "citeRegEx": "Kunichika et al\\.,? 2004", "shortCiteRegEx": "Kunichika et al\\.", "year": 2004}, {"title": "Deep questions without deep understanding", "author": ["Igor Labutov", "Sumit Basu", "Lucy Vanderwende."], "venue": "ACL (1). pages 889\u2013898.", "citeRegEx": "Labutov et al\\.,? 2015", "shortCiteRegEx": "Labutov et al\\.", "year": 2015}, {"title": "Race: Large-scale reading comprehension dataset from examinations", "author": ["Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy."], "venue": "arXiv preprint arXiv:1704.04683 .", "citeRegEx": "Lai et al\\.,? 2017", "shortCiteRegEx": "Lai et al\\.", "year": 2017}, {"title": "Generating natural language questions to support learning on-line", "author": ["David Lindberg", "Fred Popowich", "John Nesbit", "Phil Winne."], "venue": "ENLG 2013 page 105.", "citeRegEx": "Lindberg et al\\.,? 2013", "shortCiteRegEx": "Lindberg et al\\.", "year": 2013}, {"title": "Newsqa: A machine compre", "author": ["heer Suleman"], "venue": null, "citeRegEx": "Suleman.,? \\Q2016\\E", "shortCiteRegEx": "Suleman.", "year": 2016}, {"title": "Machine comprehension by text-to-text neural question generation", "author": ["Xingdi Yuan", "Tong Wang", "Caglar Gulcehre", "Alessandro Sordoni", "Philip Bachman", "Sandeep Subramanian", "Saizheng Zhang", "Adam Trischler."], "venue": "2nd Workshop on Representation", "citeRegEx": "Yuan et al\\.,? 2017", "shortCiteRegEx": "Yuan et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 5, "context": "Many educational applications can benefit from automatic question generation, including vocabulary assessment (Brown et al., 2005), writing support (Liu et al.", "startOffset": 110, "endOffset": 130}, {"referenceID": 13, "context": ", 2012), and assessment of reading comprehension (Mitkov and Ha, 2003; Kunichika et al., 2004).", "startOffset": 49, "endOffset": 94}, {"referenceID": 3, "context": "In this domain, question generation typically involves two inter-related components: first, a system to identify interesting entities or events (key phrases) within a passage or document (Becker et al., 2012); second, a question generator that constructs questions in natural language that ask specifically about the given key phrases.", "startOffset": 187, "endOffset": 208}, {"referenceID": 15, "context": "If this premise is correct, then the growing collection of crowd-sourced question-answering datasets (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Lai et al., 2017) can be harnessed to learn models for key phrases of interest to human readers.", "startOffset": 101, "endOffset": 188}, {"referenceID": 8, "context": "mechanism (Gulcehre et al., 2016).", "startOffset": 10, "endOffset": 33}, {"referenceID": 13, "context": "Automatic question generation systems are often used to alleviate (or even eliminate) the burden of human generation of questions to assess reading comprehension (Mitkov and Ha, 2003; Kunichika et al., 2004).", "startOffset": 162, "endOffset": 207}, {"referenceID": 9, "context": "Various NLP techniques have been adopted in these systems to improve generation quality, including parsing (Heilman and Smith, 2010a; Mitkov and Ha, 2003), semantic role labeling (Lindberg et al.", "startOffset": 107, "endOffset": 154}, {"referenceID": 16, "context": "Various NLP techniques have been adopted in these systems to improve generation quality, including parsing (Heilman and Smith, 2010a; Mitkov and Ha, 2003), semantic role labeling (Lindberg et al., 2013), and the use of lexicographic resources like WordNet (Miller, 1995; Mitkov and Ha, 2003).", "startOffset": 179, "endOffset": 202}, {"referenceID": 16, "context": "However, the majority of proposed methods resort to simple rule-based techniques such as slot-filling with templates (Lindberg et al., 2013; Chali and Golestanirad, 2016; Labutov et al., 2015) or syntactic transformation heuristics (Agarwal and Mannem, 2011; Ali et al.", "startOffset": 117, "endOffset": 192}, {"referenceID": 6, "context": "However, the majority of proposed methods resort to simple rule-based techniques such as slot-filling with templates (Lindberg et al., 2013; Chali and Golestanirad, 2016; Labutov et al., 2015) or syntactic transformation heuristics (Agarwal and Mannem, 2011; Ali et al.", "startOffset": 117, "endOffset": 192}, {"referenceID": 14, "context": "However, the majority of proposed methods resort to simple rule-based techniques such as slot-filling with templates (Lindberg et al., 2013; Chali and Golestanirad, 2016; Labutov et al., 2015) or syntactic transformation heuristics (Agarwal and Mannem, 2011; Ali et al.", "startOffset": 117, "endOffset": 192}, {"referenceID": 0, "context": ", 2015) or syntactic transformation heuristics (Agarwal and Mannem, 2011; Ali et al., 2010) (e.", "startOffset": 47, "endOffset": 91}, {"referenceID": 1, "context": ", 2015) or syntactic transformation heuristics (Agarwal and Mannem, 2011; Ali et al., 2010) (e.", "startOffset": 47, "endOffset": 91}, {"referenceID": 7, "context": "For the latter, Du et al. (2017) used a sequence-to-", "startOffset": 16, "endOffset": 33}, {"referenceID": 18, "context": "Yuan et al. (2017) proposed a similar architecture but in addition improved model performance through policy gradient techniques.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "Heilman and Smith (2010b) asked crowdworkers to rate the acceptability of computer-generated natural language questions as quiz questions, while Becker et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 3, "context": "Heilman and Smith (2010b) asked crowdworkers to rate the acceptability of computer-generated natural language questions as quiz questions, while Becker et al. (2012) solicited quality ratings of text chunks as potential gaps for Cloze-style questions.", "startOffset": 145, "endOffset": 166}, {"referenceID": 11, "context": "network (Hochreiter and Schmidhuber, 1997) into annotation vectors (h1, .", "startOffset": 8, "endOffset": 42}, {"referenceID": 8, "context": ", 2014) and a pointer-softmax decoder (Gulcehre et al., 2016).", "startOffset": 38, "endOffset": 61}, {"referenceID": 8, "context": ", 2014) and a pointer-softmax decoder (Gulcehre et al., 2016). It is a slightly modified version of the model described by Yuan et al. (2017). The model takes a Table 1: Model evaluation on question- and answer-generation.", "startOffset": 39, "endOffset": 142}, {"referenceID": 8, "context": "The RNN-based decoder employs the pointersoftmax mechanism (Gulcehre et al., 2016).", "startOffset": 59, "endOffset": 82}, {"referenceID": 4, "context": "Simple preprocessing is performed, including lower-casing and word tokenization using NLTK (Bird, 2006).", "startOffset": 91, "endOffset": 103}], "year": 2017, "abstractText": "We propose several neural models arranged in a two-stage framework to tackle question generation from documents. First, we estimate the probability of \u201cinteresting\u201d answers in a document using a neural model trained on a questionanswering corpus. The predicted key phrases are then used as answers to condition a sequence-to-sequence question generation model. Empirically, our neural key phrase detection models significantly outperform an entity-tagging baseline system. We demonstrate that the question generator formulates good quality natural language questions from extracted key phrases. The resulting questions and answers can be used to assess reading comprehension in educational settings.", "creator": "LaTeX with hyperref package"}}}