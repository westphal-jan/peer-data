{"id": "1706.05064", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning", "abstract": "as a step towards securing zero - shot task generalization capabilities in dynamic learning ( rl ), we introduce simple new rl problem where the agent primarily learn to execute sequences of instructions after learning useful skills that solve subtasks. in maze problem, we consider two tasks of generalizations : to previously unseen instructions and to longer sequences of instructions. for generalization over unseen instructions, we propose a new objective which encourages learning correspondences between independent subtasks essentially making analogies. for generalization over sequential phrases, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. to function with delayed reward, we propose a new neural architecture telling the text controller that instructions when to update the subtask, which makes learning more efficiently. experimental results on your stochastic 3d domain show that the previous ideas are crucial for generalization to longer instructions as well after unseen plans.", "histories": [["v1", "Thu, 15 Jun 2017 20:04:35 GMT  (3869kb,D)", "http://arxiv.org/abs/1706.05064v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["junhyuk oh", "satinder p singh", "honglak lee", "pushmeet kohli"], "accepted": true, "id": "1706.05064"}, "pdf": {"name": "1706.05064.pdf", "metadata": {"source": "META", "title": "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning", "authors": ["Junhyuk Oh", "Satinder Singh", "Honglak Lee", "Pushmeet Kohli"], "emails": ["<junhyuk@umich.edu>."], "sections": [{"heading": "1. Introduction", "text": "The ability to understand and follow instructions allows us to perform a large number of new complex sequential tasks even without additional learning. For example, we can make a new dish following a recipe, and explore a new city following a guidebook. Developing the ability to execute instructions can potentially allow reinforcement learning (RL) agents to generalize quickly over tasks for which such instructions are available. For example, factory-trained household robots could execute novel tasks in a new house following a human user\u2019s instructions (e.g., tasks involving household chores, going to a new place, picking up/manipulating new objects, etc.). In addition to generalization over instructions, an intelligent agent should also be able to handle unexpected events (e.g., low bat-\n1University of Michigan 2Google Brain 3Microsoft Research. Correspondence to: Junhyuk Oh <junhyuk@umich.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nFigure 1: Example of 3D world and instructions. The agent is tasked to execute longer sequences of instructions in the correct order after training on short sequences of instructions; in addition, previously unseen instructions can be given during evaluation (blue text). Additional reward is available from randomly appearing boxes regardless of instructions (green circle).\ntery, arrivals of reward sources) while executing instructions. Thus, the agent should not blindly execute instructions sequentially but sometimes deviate from instructions depending on circumstances, which requires balancing between two different objectives.\nProblem. To develop such capabilities, this paper introduces the instruction execution problem where the agent\u2019s overall task is to execute a given list of instructions described by a simple form of natural language while dealing with unexpected events, as illustrated in Figure 1. More specifically, we assume that each instruction can be executed by performing one or more high-level subtasks in sequence. Even though the agent can pre-learn skills to perform such subtasks (e.g., [Pick up, Pig] in Figure 1), and the instructions can be easily translated to subtasks, our problem is difficult due to the following challenges.\n\u2022 Generalization: Pre-training of skills can only be done on a subset of subtasks, but the agent is required to perform previously unseen subtasks (e.g., going to a new place) in order to execute unseen instructions during testing. Thus, the agent should learn to generalize to new subtasks in the skill learning stage. Furthermore, the agent is required to execute previously unseen and longer sequences of instructions during evaluation.\n\u2022 Delayed reward: The agent is not told which instruction to execute at any point of time from the environment but just given the full list of instructions as input. In addition, the agent does not receive any signal on completing in-\nar X\niv :1\n70 6.\n05 06\n4v 1\n[ cs\n.A I]\n1 5\nJu n\n20 17\ndividual instructions from the environment, i.e., successreward is provided only when all instructions are executed correctly. Therefore, the agent should keep track of which instruction it is executing and decide when to move on to the next instruction.\n\u2022 Interruption: As described in Figure 1, there can be unexpected events in uncertain environments, such as opportunities to earn bonuses (e.g., windfalls), or emergencies (e.g., low battery). It can be better for the agent to interrupt the ongoing subtask before it is finished, perform a different subtask to deal with such events, and resume executing the interrupted subtask in the instructions after that. Thus, the agent should achieve a balance between executing instructions and dealing with such events.\n\u2022 Memory: There are loop instructions (e.g., \u201cPick up 3 pig\u201d) which require the agent to perform the same subtask ([Pick up, Pig]) multiple times and take into account the history of observations and subtasks in order to decide when to move on to the next instruction correctly.\nDue to these challenges, the agent should be able to execute a novel subtask, keep track of what it has done, monitor observations to interrupt ongoing subtasks depending on circumstances, and switch to the next instruction precisely when the current instruction is finished.\nOur Approach and Technical Contributions. To address the aforementioned challenges, we divide the learning problem into two stages: 1) learning skills to perform a set of subtasks and generalizing to unseen subtasks, and 2) learning to execute instructions using the learned skills. Specifically, we assume that subtasks are defined by several disentangled parameters. Thus, in the first stage our architecture learns a parameterized skill (da Silva et al., 2012) to perform different subtasks depending on input parameters. In order to generalize over unseen parameters, we propose a new objective function that encourages making analogies between similar subtasks so that the underlying manifold of the entire subtask space can be learned without experiencing all subtasks. In the second stage, our architecture learns a meta controller on top of the parameterized skill so that it can read instructions and decide which subtask to perform. The overall hierarchical RL architecture is shown in Figure 3. To deal with delayed reward as well as interruption, we propose a novel neural network (see Figure 4) that learns when to update the subtask in the meta controller. This not only allows learning to be more efficient under delayed reward by operating at a larger time-scale but also allows interruptions of ongoing subtasks when an unexpected event is observed.\nMain Results. We developed a 3D visual environment using Minecraft based on Oh et al. (2016) where the agent can interact with many objects. Our results on multiple sets of parameterized subtasks show that our pro-\nposed analogy-making objective can generalize successfully. Our results on multiple instruction execution problems show that our meta controller\u2019s ability to learn when to update the subtask plays a key role in solving the overall problem and outperforms several hierarchical baselines. The demo videos are available at the following website: https://sites.google.com/a/umich. edu/junhyuk-oh/task-generalization.\nThe rest of the sections are organized as follows. Section 2 presents related work. Section 3 presents our analogymaking objective for generalization to parameterized tasks and demonstrates its application to different generalization scenarios. Section 4 presents our hierarchical architecture for the instruction execution problem with our new neural network that learns to operate at a large time-scale. In addition, we demonstrate our agent\u2019s ability to generalize over sequences of instructions, as well as provide a comparison to several alternative approaches."}, {"heading": "2. Related Work", "text": "Hierarchical RL. A number of hierarchical RL approaches are designed to deal with sequential tasks. Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007). However, much of the previous work assumes that the overall task is fixed (e.g., Taxi domain (Dietterich, 2000)). In other words, the optimal sequence of subtasks is fixed during evaluation (e.g., picking up a passenger followed by navigating to a destination in the Taxi domain). This makes it hard to evaluate the agent\u2019s ability to compose pre-learned policies to solve previously unseen sequential tasks in a zero-shot fashion unless we re-train the agent on the new tasks in a transfer learning setting (Singh, 1991; 1992; McGovern and Barto, 2002). Our work is also closely related to Programmable HAMs (PHAMs) (Andre and Russell, 2000; 2002) in that a PHAM is designed to execute a given program. However, the program explicitly specifies the policy in PHAMs which effectively reduces the state-action search space. In contrast, instructions are a description of the task in our work, which means that the agent should learn to use the instructions to maximize its reward.\nHierarchical Deep RL. Hierarhical RL has been recently combined with deep learning. Kulkarni et al. (2016) proposed hierarchical Deep Q-Learning and demonstrated improved exploration in a challenging Atari game. Tessler et al. (2017) proposed a similar architecture, but the highlevel controller is allowed to choose primitive actions directly. Bacon et al. (2017) proposed the option-critic architecture, which learns options without pseudo reward and demonstrated that it can learn distinct options in Atari\ngames. Heess et al. (2016) formulated the actions of the meta controller as continuous variables that are used to modulate the behavior of the low-level controller. Florensa et al. (2017) trained a stochastic neural network with mutual information regularization to discover skills. Most of these approaches build an open-loop policy at the highlevel controller that waits until the previous subtask is finished once it is chosen. This approach is not able to interrupt ongoing subtasks in principle, while our architecture can switch its subtask at any time.\nZero-Shot Task Generalization. There have been a few papers on zero-shot generalization to new tasks. For example, da Silva et al. (2012) introduced parameterized skills that map sets of task descriptions to policies. Isele et al. (2016) achieved zero-shot task generalization through dictionary learning with sparsity constraints. Schaul et al. (2015) proposed universal value function approximators (UVFAs) that learn value functions for state and goal pairs. Devin et al. (2017) proposed composing sub-networks that are shared across tasks and robots in order to achieve generalization to unseen configurations of them. Unlike the above prior work, we propose a flexible metric learning method which can be applied to various generalization scenarios. Andreas et al. (2016) proposed a framework to learn the underlying subtasks from a policy sketch which specifies a sequence of subtasks, and the agent can generalize over new sequences of them in principle. In contrast, our work aims to generalize over unseen subtasks as well as unseen sequences of them. In addition, the agent should handle unexpected events in our problem that are not described by the instructions by interrupting subtasks appropriately.\nInstruction Execution. There has been a line of work for building agents that can execute natural language instructions: Tellex et al. (2011; 2014) for robotics and MacMahon et al. (2006); Chen and Mooney (2011); Mei et al. (2015) for a simulated environment. However, these approaches focus on natural language understanding to map instructions to actions or groundings in a supervised setting. In contrast, we focus on generalization to sequences of instructions without any supervision for language understanding or for actions. Although Branavan et al. (2009) also tackle a similar problem, the agent is given a single instruction at a time, while our agent needs to learn how to align instructions and state given a full list of instructions."}, {"heading": "3. Learning a Parameterized Skill", "text": "In this paper, a parameterized skill is a multi-task policy corresponding to multiple tasks defined by categorical input task parameters, e.g., [Pick up, X]. More formally, we define a parameterized skill as a mappingO\u00d7G \u2192 A\u00d7B, where O is a set of observations, G is a set of task parameters, A is a set of primitive actions, and B = {0, 1} indicates whether the task is finished or not. A space of tasks is defined using the Cartesian product of task parameters: G = G(1) \u00d7 ... \u00d7 G(n), where G(i) is a set of the i-th parameters (e.g., G = {Visit, Pick up}\u00d7{X, Y, Z}). Given an observation xt \u2208 O at time t and task parameters g = [ g(1), ..., g(n) ] \u2208 G, where g(i) is a one-hot vector, the parameterized skill is the following functions:\nPolicy: \u03c0\u03c6(at|xt, g) Termination: \u03b2\u03c6(bt|xt, g),\nwhere \u03c0\u03c6 is the policy optimized for the task g, and \u03b2\u03c6 is a termination function (Sutton et al., 1999) which is the probability that the state is terminal at time t for the given task g. The parameterized skill is represented by a nonlinear function approximator \u03c6(\u00b7), a neural network in this paper. The neural network architecture of our parameterized skill is illustrated in Figure 2. The network maps input task parameters into a task embedding space \u03d5(g), which is combined with the observation followed by the output layers. More details are described in the Appendix."}, {"heading": "3.1. Learning to Generalize by Analogy-Making", "text": "Only a subset of tasks (G\u2032 \u2282 G) are available during training, and so in order to generalize to unseen tasks during evaluation the network needs to learn knowledge about the relationship between different task parameters when learning the task embedding \u03d5(g).\nTo this end, we propose an analogy-making objective inspired by Reed et al. (2015). The main idea is to learn correspondences between tasks. For example, if target objects and \u2018Visit/Pick up\u2019 actions are independent (i.e., each action can be applied to any target object), we can enforce the analogy [Visit, X] : [Visit, Y] :: [Pick up, X] : [Pick up, Y] for any X and Y in the embedding space, which means that the difference between \u2018Visit\u2019 and \u2018Pick up\u2019 is consistent regardless of target objects and vice versa. This allows the agent to generalize to unseen combinations of actions and target objects, such as performing [Pick up, Y] after it has learned to perform [Pick up, X] and [Visit, Y].\nMore specifically, we define several constraints as follows:\n\u2016\u2206 (gA, gB)\u2212\u2206 (gC , gD)\u2016 \u2248 0 if gA : gB :: gC : gD \u2016\u2206 (gA, gB)\u2212\u2206 (gC , gD)\u2016 \u2265 \u03c4dis if gA : gB 6= gC : gD\n\u2016\u2206 (gA, gB)\u2016 \u2265 \u03c4diff if gA 6= gB , where gk = [ g (1) k , g (2) k , ..., g (n) k ] \u2208 G are task parameters,\n\u2206 (gA, gB) = \u03d5(gA) \u2212 \u03d5(gB) is the difference vector between two tasks in the embedding space, and \u03c4dis and \u03c4diff are constant threshold distances. Intuitively, the first constraint enforces the analogy (i.e., parallelogram structure in the embedding space; see Mikolov et al. (2013); Reed et al. (2015)), while the other constraints prevent trivial solutions. We incorporate these constraints into the following objectives based on contrastive loss (Hadsell et al., 2006):\nLsim = EgA...D\u223cGsim [ \u2016\u2206 (gA, gB)\u2212\u2206 (gC , gD) \u20162 ] Ldis = EgA...D\u223cGdis [ (\u03c4dis \u2212 \u2016\u2206 (gA, gB)\u2212\u2206 (gC , gD) \u2016) 2 +\n] Ldiff = EgA,B\u223cGdiff [ (\u03c4diff \u2212 \u2016\u2206 (gA, gB) \u2016) 2 + ] ,\nwhere (\u00b7)+ = max(0, \u00b7) and Gsim,Gdis,Gdiff are sets of task parameters that satisfy corresponding conditions in the above three constraints. The final analogy-making objective is the weighted sum of the above three objectives."}, {"heading": "3.2. Training", "text": "The parameterized skill is trained on a set of tasks (G\u2032 \u2282 G) through the actor-critic method with generalized advantage estimation (Schulman et al., 2016). We also found that pre-training through policy distillation (Rusu et al., 2016; Parisotto et al., 2016) gives slightly better results as discussed in Tessler et al. (2017). Throughout training, the parameterized skill is also made to predict whether the current state is terminal or not through a binary classification objective, and the analogy-making objective is applied to the task embedding separately. The full details of the learning objectives are described in the Appendix."}, {"heading": "3.3. Experiments", "text": "Environment. We developed a 3D visual environment using Minecraft based on Oh et al. (2016) as shown in Figure 1. An observation is represented as a 64 \u00d7 64 pixel RGB image. There are 7 different types of objects: Pig, Sheep, Greenbot, Horse, Cat, Box, and Ice. The topology of the world and the objects are randomly generated for every episode. The agent has 9 actions: Look (Left/Right/Up/Down), Move (Forward/Backward), Pick up, Transform, and No operation. Pick up removes the object in front of the agent, and Transform changes the object in front of the agent to ice (a special object).\nImplementation Details. The network architecture of the parameterized skill consists of 4 convolution layers and one LSTM (Hochreiter and Schmidhuber, 1997) layer. We conducted curriculum training by changing the size of the world, the density of object and walls according to the agent\u2019s success rate. We implemented actor-critic method with 16 CPU threads based on Sukhbaatar et al. (2015). The parameters are updated after 8 episodes for each thread. The details of architectures and hyperparameters are described in the Appendix.\nResults. To see how useful analogy-making is for generalization to unseen parameterized tasks, we trained and evaluated the parameterized skill on three different sets of parameterized tasks defined below1.\n\u2022 Independent: The task space is defined as G = T \u00d7 X , where T = {Visit,Pick up,Transform} and X is the set of object types. The agent should move on top of the target object given \u2018Visit\u2019 task and perform the corresponding actions in front of the target given \u2018Pick up\u2019 and \u2018Transform\u2019 tasks. Only a subset of tasks are encountered during training, so the agent should generalize over unseen configurations of task parameters.\n\u2022 Object-dependent: The task space is defined as G = T \u2032 \u00d7 X , where T \u2032 = T \u222a {Interact with}. We divided objects into two groups, each of which should be either picked up or transformed given \u2018Interact with\u2019 task. Only a subset of target object types are encountered during training, so there is no chance for the agent to generalize without knowledge of the group of each object. We applied analogy-making so that analogies can be made only within the same group. This allows the agent to perform object-dependent actions even for unseen objects.\n\u2022 Interpolation/Extrapolation: The task space is defined as G = T \u00d7 X \u00d7 C, where C = {1, 2, ..., 7}. The agent should perform a task for a given number of times (c \u2208 C). Only {1, 3, 5} \u2282 C is given during training, and the agent should generalize over unseen numbers {2, 4, 6, 7}. Note that the optimal policy for a task can be derived from T \u00d7X , but predicting termination requires generalization to unseen numbers. We applied analogymaking based on arithmetic (e.g., [Pick up, X, 2] : [Pick up, X, 5] :: [Transform, Y, 3] : [Transform, Y, 6]).\nAs summarized in Table 1, the parameterized skill with our analogy-making objective can successfully generalize to unseen tasks in all generalization scenarios. This suggests that when learning a representation of task parameters, it is possible to inject prior knowledge in the form of the analogy-making objective so that the agent can learn to\n1The sets of subtasks used for training and evaluation are described in the Appendix.\ngeneralize over unseen tasks in various ways depending on semantics or context without needing to experience them."}, {"heading": "4. Learning to Execute Instructions using Parameterized Skill", "text": "We now consider the instruction execution problem where the agent is given a sequence of simple natural language instructions, as illustrated in Figure 1. We assume an already trained parameterized skill, as described in Section 3. Thus, the main remaining problem is how to use the parameterized skill to execute instructions. Although the requirement that instructions be executed sequentially makes the problem easier (than, e.g., conditional-instructions), the agent still needs to make complex decisions because it should deviate from instructions to deal with unexpected events (e.g., low battery) and remember what it has done to deal with loop instructions, as discussed in Section 1.\nTo address the above challenges, our hierarchical RL architecture (see Figure 3) consists of two modules: meta controller and parameterized skill. Specifically, a meta controller reads the instructions and passes subtask parameters to a parameterized skill which executes the given subtask and provides its termination signal back to the meta controller. Section 4.1 describes the overall architecture of the meta controller for dealing with instructions. Section 4.2 describes a novel neural architecture that learns when to update the subtask in order to better deal with delayed reward signal as well as unexpected events."}, {"heading": "4.1. Meta Controller Architecture", "text": "As illustrated in Figure 4, the meta controller is a mapping O \u00d7M\u00d7 G \u00d7 B \u2192 G, whereM is a list of instructions. Intuitively, the meta controller decides subtask parameters gt \u2208 G conditioned on the observation xt \u2208 O, the list of instructionsM \u2208M, the previously selected subtask gt\u22121, and its termination signal (b \u223c \u03b2\u03c6).\nIn contrast to recent hierarchical deep RL approaches where the meta controller can update its subtask (or option) only when the previous one terminates or only after a fixed number of steps, our meta controller can update the subtask at any time and takes the termination signal as additional input. This gives more flexibility to the meta controller and\nenables interrupting ongoing tasks before termination.\nIn order to keep track of the agent\u2019s progress on instruction execution, the meta controller maintains its internal state by computing a context vector (Section 4.1.1) and determines which subtask to execute by focusing on one instruction at a time from the list of instructions (Section 4.1.2)."}, {"heading": "4.1.1. CONTEXT", "text": "Given the sentence embedding rt\u22121 retrieved at the previous time-step from the instructions (described in Section 4.1.2), the previously selected subtask gt\u22121, and the subtask termination bt \u223c \u03b2\u03c6 ( bt|st, gt\u22121 ) , the meta controller computes the context vector (ht) as follows:\nht = LSTM (st,ht\u22121) st = f ( xt, rt\u22121, gt\u22121, bt ) ,\nwhere f is a neural network. Intuitively, gt\u22121 and bt provide information about which subtask was being solved by the parameterized skill and whether it has finished or not. Thus, st is a summary of the current observation and the ongoing subtask. ht takes the history of st into account through the LSTM, which is used by the subtask updater."}, {"heading": "4.1.2. SUBTASK UPDATER", "text": "The subtask updater constructs a memory structure from the list of instructions, retrieves an instruction by maintaining a pointer into the memory, and computes the subtask parameters.\nInstruction Memory. Given instructions as a list of sentences M = (m1,m2, ...,mK), where each sentence consists of a list of words, mi = ( w1, ..., w|mi| ) , the subtask updater constructs memory blocks M \u2208 RE\u00d7K (i.e., each column is an E-dimensional embedding of a sentence). The subtask updater maintains an instruction pointer (pt \u2208 RK) which is non-negative and sums up to 1 indicating which instruction the meta controller is executing. Memory construction and retrieval can be written as:\nMemory: M = [\u03d5w (m1) , \u03d5w (m2) , ..., \u03d5w (mK)] (1) Retrieval: rt = Mpt, (2)\nwhere \u03d5w (mi) \u2208 RE is the embedding of the i-th sentence (e.g., Bag-of-words), and rt \u2208 RE is the retrieved sentence embedding which is used for computing the subtask parameters. Intuitively, if pt is a one-hot vector, rt indicates a single instruction from the whole list of instructions. The meta controller should learn to manage pt so that it can focus on the correct instruction at each time-step.\nSince instructions should be executed sequentially, we use a location-based memory addressing mechanism (Zaremba and Sutskever, 2015; Graves et al., 2014) to manage the instruction pointer. Specifically, the subtask updater shifts the instruction pointer by [\u22121, 1] as follows:\npt = lt \u2217 pt\u22121 where lt = Softmax ( \u03d5shift(ht) ) , (3)\nwhere \u2217 is a convolution operator, \u03d5shift is a neural network, and lt \u2208 R3 is a soft-attention vector over the three shift operations {\u22121, 0,+1}. The optimal policy should keep the instruction pointer unchanged while executing an instruction and increase the pointer by +1 precisely when the current instruction is finished.\nSubtask Parameters. The subtask updater takes the context (ht), updates the instruction pointer (pt), retrieves an instruction (rt), and computes subtask parameters as:\n\u03c0\u03b8 (gt|ht, rt) = \u220f i \u03c0\u03b8 ( g (i) t |ht, rt ) , (4)\nwhere \u03c0\u03b8 ( g (i) t |ht, rt ) \u221d exp ( \u03d5goali (ht, rt) ) , and \u03d5goali is a neural network for the i-th subtask parameter."}, {"heading": "4.2. Learning to Operate at a Large Time-Scale", "text": "Although the meta controller can learn an optimal policy by updating the subtask at each time-step in principle, making a decision at every time-step can be inefficient because subtasks do not change frequently. Instead, having temporallyextended actions can be useful for dealing with delayed reward by operating at a larger time-scale (Sutton et al., 1999). While it is reasonable to use the subtask termination signal to define the temporal scale of the meta controller as in many recent hierarchical deep RL approaches (see Section 2), this approach would result in a mostly open-loop\nmeta-controller policy that is not able to interrupt ongoing subtasks before termination, which is necessary to deal with unexpected events not specified in the instructions.\nTo address this dilemma, we propose to learn the time-scale of the meta controller by introducing an internal binary decision which indicates whether to invoke the subtask updater to update the subtask or not, as illustrated in Figure 5. This decision is defined as: ct \u223c \u03c3 ( \u03d5update (st,ht\u22121)\n) where \u03c3 is a sigmoid function. If ct = 0, the meta controller continues the current subtask without updating the subtask updater. Otherwise, if ct = 1, the subtask updater updates its internal states (e.g., instruction pointer) and the subtask parameters. This allows the subtask updater to operate at a large time-scale because one decision made by the subtask updater results in multiple actions depending on c values. The overall meta controller architecture with this update scheme is illustrated in Figure 4.\nSoft-Update. To ease optimization of the nondifferentiable variable (ct), we propose a soft-update rule by using ct = \u03c3 ( \u03d5update (st,ht\u22121) ) instead of sampling it. The key idea is to take the weighted sum of both \u2018update\u2019 and \u2018copy\u2019 scenarios using ct as the weight. This method is described in Algorithm 1. We found that training the meta controller using soft-update followed by fine-tuning by sampling ct is crucial for training the meta controller. Note that the soft-update rule reduces to the original formulation if we sample ct and lt from the Bernoulli and multinomial distributions, which justifies our initialization trick.\nAlgorithm 1 Subtask update (Soft) Input: st, ht\u22121, pt\u22121, rt\u22121, gt\u22121 Output: ht, pt, rt, gt ct \u2190 \u03c3 ( \u03d5update (st, ht\u22121) ) # Decide update weight\nh\u0303t \u2190 LSTM (st, ht\u22121) # Update the context lt \u2190 Softmax ( \u03d5shift ( h\u0303t ))\n# Decide shift operation p\u0303t \u2190 lt \u2217 pt\u22121 # Shift the instruction pointer r\u0303t \u2190 Mp\u0303t # Retrieve instruction # Merge two scenarios (update/copy) using ct as weight [pt, rt, ht]\u2190 ct[p\u0303t, r\u0303t, h\u0303t] + (1\u2212 ct) [pt\u22121, rt\u22121, ht\u22121] g (i) t \u223c ct\u03c0\u03b8 ( g (i) t |h\u0303t, r\u0303t ) + (1\u2212 ct) g(i)t\u22121\u2200i\nIntegrating with Hierarchical RNN. The idea of learning the time-scale of a recurrent neural network is closely related to hierarchical RNN approaches (Koutnik et al., 2014; Chung et al., 2017) where different groups of recurrent hidden units operate at different time-scales to capture both long-term and short-term temporal information. Our idea can be naturally integrated with hierarchical RNNs by applying the update decision (c value) only for a subset of recurrent units instead of all the units. Specifically, we divide the context vector into two groups: ht = [ h(l)t ,h (h) t ] .\nThe low-level units (h(l)t ) are updated at every time-step,\nwhile the high-level units (h(h)t ) are updated depending on the value of c. This simple modification leads to a form of hierarchical RNN where the low-level units focus on shortterm temporal information while the high-level units capture long-term dependencies."}, {"heading": "4.3. Training", "text": "The meta controller is trained on a training set of lists of instructions. Given a pre-trained and fixed parameterized skill, the actor-critic method is used to update the parameters of the meta controller. Since the meta controller also learns a subtask embedding \u03d5(gt\u22121) and has to deal with unseen subtasks during evaluation, analogy-making objective is also applied. The details of the objective function are provided in the Appendix."}, {"heading": "4.4. Experiments", "text": "The experiments are designed to explore the following questions: (1) Will the proposed hierarchical architecture outperform a non-hierarchical baseline? (2) How beneficial is the meta controller\u2019s ability to learn when to update the subtask? We are also interested in understanding the qualitative properties of our agent\u2019s behavior.2\nEnvironment. We used the same Minecraft domain used in Section 3.3. The agent receives a time penalty (\u22120.1) for each step and receives +1 reward when it finishes the entire list of instructions in the correct order. Throughout an episode, a box (including treasures) randomly appears with probability of 0.03 and transforming a box gives +0.9 reward.\nThe subtask space is defined as G = T \u00d7X , and the semantics of each subtask are the same as the \u2018Independent\u2019 case in Section 3.3. We used the best-performing parameterized skill throughout this experiment.\nThere are 7 types of instructions: {Visit X, Pick up X, Transform X, Pick up 2 X, Transform 2 X, Pick up 3 X, Transform 3 X} where \u2018X\u2019 is the target object type. Note that the parameterized skill used in this experiment was not trained on loop instructions (e.g., Pick up 3 X), so the last four instructions require the meta controller to learn to repeat the corresponding subtask for the given number of times. To see how the agent generalizes to previously unseen instructions, only a subset of instructions and subtasks was presented during training.\nImplementation Details. The meta controller consists of 3 convolution layers and one LSTM layer. We also conducted curriculum training by changing the size of the world, the density of object and walls, and the number of instructions according to the agent\u2019s success rate. We used\n2For further analysis, we also conducted comprehensive experiments on a 2D grid-world domain. However, due to space limits, those results are provided in the Appendix.\nthe actor-critic implementation described in Section 3.3.\nBaselines. To understand the advantage of using the hierarchical structure and the benefit of our meta controller\u2019s ability to learn when to update the subtask, we trained three baselines as follows. \u2022 Flat: identical to our meta controller except that it di-\nrectly chooses primitive actions without using the parameterized skill. It is also pre-trained on the training set of subtasks.\n\u2022 Hierarchical-Long: identical to our architecture except that the meta controller can update the subtask only when the current subtask is finished. This approach is similar to recent hierarchical deep RL methods (Kulkarni et al., 2016; Tessler et al., 2017).\n\u2022 Hierarchical-Short: identical to our architecture except that the meta controller updates the subtask at every time-step.\nOverall Performance. The results on the instruction execution are summarized in Table 7 and Figure 10. It shows that our architecture (\u2018Hierarchical-Dynamic\u2019) can handle a relatively long list of seen and unseen instructions of length 20 with reasonably high success rates, even though it is trained on short instructions of length 4. Although the performance degrades as the number of instructions increases, our architecture finishes 18 out of 20 seen instructions and 14 out of 20 unseen instructions on average. These results show that our agent is able to generalize to longer compositions of seen/unseen instructions by just learning to solve short sequences of a subset of instructions.\nFlat vs. Hierarchy. Table 7 shows that the flat baseline completely fails even on training instructions. The flat controller tends to struggle with loop instructions (e.g., Pick up 3 pig) so that it learned a sub-optimal policy which moves to the next instruction with a small probability at each step regardless of its progress. This implies that it is hard for the flat controller to detect precisely when a subtask is finished, whereas hierarchical architectures can easily detect when a subtask is done, because the parameterized skill provides a termination signal to the meta controller.\nEffect of Learned Time-Scale. As shown in Table 7 and Figure 10, \u2018Hierarchical-Long\u2019 baseline performs significantly worse than our architecture. We found that whenever\na subtask is finished, this baseline puts a high probability to switch to [Transform, Box] regardless of the existence of box because transforming a box gives a bonus reward if a box exists by chance. However, this leads to wasting too much time finding a box until it appears and results in a poor success rate due to the time limit. This result implies that an open-loop policy that has to wait until a subtask finishes can be confused by such an uncertain event because it cannot interrupt ongoing subtasks before termination.\nOn the other hand, we observed that \u2018Hierarchical-Short\u2019 often fails on loop instructions by moving on to the next instruction before it finishes such instructions. This baseline should repeat the same subtask while not changing the instruction pointer for a long time and the reward is even more delayed given loop instructions. In contrast, the subtask updater in our architecture makes fewer decisions by operating at a large time-scale so that it can get more direct feedback from the long-term future. We conjecture that this is why our architecture performs better than this baseline. This result shows that learning when to update the subtask using the neural network is beneficial for dealing with delayed reward without compromising the ability to interrupt.\nAnalysis of The Learned Policy. We visualized our agent\u2019s behavior given a long list of instructions in Figure 7. Interestingly, when the agent sees a box, the meta controller immediately changes its subtask to [Transform, Box] to get a positive reward even though its instruction\npointer is indicating \u2018Pick up 2 pig\u2019 and resumes executing the instruction after dealing with the box. Throughout this event and the loop instruction, the meta controller keeps the instruction pointer unchanged as illustrated in (B-C) in Figure 7. In addition, the agent learned to update the instruction pointer and the subtask almost only when it is needed, which provides the subtask updater with temporally-extended actions. This is not only computationally efficient but also useful for learning a better policy."}, {"heading": "5. Conclusion", "text": "In this paper, we explored a type of zero-shot task generalization in RL with a new problem where the agent is required to execute and generalize over sequences of instructions. We proposed an analogy-making objective which enables generalization over unseen parameterized tasks in various scenarios. We also proposed a novel way to learn the time-scale of the meta controller that proved to be more efficient and flexible than alternative approaches for interrupting subtasks and for dealing with delayed sequential decision problems. Our empirical results on a stochastic 3D domain showed that our architecture generalizes well to longer sequences of instructions as well as unseen instructions. Although our hierarchical RL architecture was demonstrated in the simple setting where the set of instructions should be executed sequentially, we believe that our key ideas are not limited to this setting but can be extended to richer forms of instructions."}, {"heading": "Acknowledgement", "text": "This work was supported by NSF grant IIS-1526059. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsor."}, {"heading": "A. Experimental Setting for Parameterized Tasks", "text": "The episode terminates after 50 steps for \u2018Independent\u2019 and \u2018Object-dependent\u2019 cases and 70 steps for \u2018Inter/Extrapolation\u2019 case. The agent receives a positive reward (+1) when it successfully finishes the given task and receives a time penalty of \u22120.1 for each step. The details of each generalization scenario are described below.\nIndependent. The semantics of the tasks are consistent across all types of target objects. The training set of tasks is shown in Table 3. Examples of analogies used in this experiment are:\n\u2022 [Visit, X] : [Visit, Y] :: [Transform, X] : [Transform, Y] \u2022 [Visit, X] : [Visit, Y] 6= [Transform, X] : [Pick up, Y] \u2022 [Visit, X] 6= [Visit, Y],\nwhere X and Y can be any object type (X 6= Y).\nObject-dependent. We divided objects into two groups: Group A and Group B. Given \u2018Interact with\u2019 action, Group A should be picked up, whereas Group B should be transformed by the agent. The training set of tasks with groups for each object is shown in Table 4. Examples of analogies used in this experiment are:\n\u2022 [Visit, Sheep] : [Visit, Cat] :: [Interact with, Sheep] : [Interact with, Cat] \u2022 [Visit, Sheep] : [Visit, Greenbot] 6= [Interact with, Sheep] : [Interact with, Greenbot] \u2022 [Visit, Horse] : [Visit, Greenbot] :: [Interact with, Horse] : [Interact with, Greenbot]\nNote that the second example implies that Sheep and Greenbot should be treated in a different way because they belong to different groups. Given such analogies, the agent can learn to interact with Cat as it interacts with Sheep and interact with Greenbot as it interacts with Horse.\nInter/Extrapolation. In this experiment, a task is defined by three parameters: action, object, and number. The agent should repeat the same subtask for a given number of times. The agent is trained on all configurations of actions and target objects. However, only a subset of numbers is used during training. In order to interpolate and extrapolate, we define analogies based on simple arithmetic such as:\n\u2022 [Pick up, X, 1] : [Pick up, X, 6] :: [Pick up, X, 2] : [Pick up, X, 7] \u2022 [Pick up, X, 3] : [Pick up, X, 6] :: [Transform, Y, 4] : [Transform, Y, 7] \u2022 [Pick up, X, 1] : [Pick up, X, 3] 6= [Transform, Y, 2] : [Transform, Y, 3]\nVisit Pick up Transform Sheep X X Horse X X\nPig X X Box X X Cat X X Greenbot X X\nTable 3: Training set of tasks for the \u2018Independent\u2019 case."}, {"heading": "B. Experimental Setting for Instruction Execution", "text": "The episode terminates after 80 steps during training of the meta controller. A box appears with probability of 0.03 and disappears after 20 steps. During evaluation on longer instructions, the episode terminates after 600 steps. We constructed a training set of instructions and an unseen set of instructions as described in Table 5. We generated different sequences of instructions for training and evaluation by sampling instructions from such sets of instructions.\nC. Experiment on 2D Grid-World\nEnvironment. To see how our approach can be generally applied to different domains, we developed a 2D grid-world based on MazeBase (Sukhbaatar et al., 2015) where the agent can interact with many objects, as illustrated in Figure 8. Unlike the original MazeBase, an observation is represented as a binary 3D tensor: xt \u2208 R18\u00d710\u00d710 where 18 is the number of object types and 10 \u00d7 10 is the size of the grid world. Each channel is a binary mask indicating the presence of each object type. There are agent, blocks, water, and 15 types of objects with which the agent can interact, and all of them are randomly placed for each episode.\nThe agent has 13 primitive actions: No-operation, Move (North/South/West/East, referred to as \u201cNSWE\u201d), Pick\nup (NSWE), and Transform (NSWE). Move actions move the agent by one cell in the specified direction. Pick up actions remove the adjacent object in the corresponding relative position, and Transform actions either remove it or transform it to another object depending on the object type, as shown in Figure 8.\nThe agent receives a time penalty (\u22120.1) for each time-step. Water cells act as obstacles which give \u22120.3 when the agent visits them. The agent receives +1 reward when it finishes all instructions in the correct order. Throughout the episode, an enemy randomly appears, moves, and disappears after 10 steps. Transforming an enemy gives +0.9 reward.\nEvaluation on Parameterized Tasks. As in the main experiment, we evaluated the parameterized skill on seen and unseen parameterized tasks separately using the same generalization scenarios. As shown in Table 6, the parameterized skill with analogy-making can successfully generalize to unseen tasks both in independent and object-dependent scenarios.\nWe visualized the value function learned by the critic network of the parameterized skill in Figure 9. As expected from its generalization performance, our parameterized skill trained with analogy-making objective learned high values around the target objects given unseen tasks.\nEvaluation on Instruction Execution. There are 5 types of instructions: Visit X, Pick up X, Transform X, Pick up all X, and Transform all X, where \u2018X\u2019 is the target object type. While the first three instructions require the agent to perform the corresponding subtask, the last two instructions require the agent to repeat the same subtask until the target objects completely disappear from the world.\nThe overall result is consistent with the result on 3D environment as shown in Figure 10 and Table 7. The flat baseline learned a sub-optimal policy that transforms or picks up all target objects in the world even if there is no \u2018all\u2019 adverb in the instructions. This sub-optimal policy unnecessarily removes objects that can be potentially target objects in the future instructions. This is why the performance of the flat baseline drastically decreases as the number of instructions increases in Figure 10. Our architecture with learned time-scale (\u2018Hierarchical-Dynamic\u2019) performs much better than \u2018HierarchicalShort\u2019 baseline which updates the subtask at every time-step. This also suggests that learning to operate in a large-time scale is crucial for dealing with delayed reward. We also observed that our agent learned to deal with enemies whenever they appear, and thus it outperforms the \u2018Shortest Path\u2019 method which is near-optimal in executing instructions while ignoring enemies."}, {"heading": "D. Details of Learning Objectives", "text": "D.1. Parameterized Skill\nThe parameterized skill is first trained through policy distillation (Rusu et al., 2016; Parisotto et al., 2016) and fine-tuned using actor-critic method (Konda and Tsitsiklis, 1999) with generalized advantage estimation (GAE) (Schulman et al., 2016). The parameterized skill is also trained to predict whether the current state is terminal or not through binary classification objective.\nThe idea of policy distillation is to first train separate teacher policies (\u03c0gT (a|s)) for each task (g) through reinforcement learning and train a multi-task policy (\u03c0g\u03c6(a|s)) to mimic teachers\u2019 behavior by minimizing KL divergence between them as follows:\n\u2207\u03c6LRL = Eg\u223cU [ Es\u223c\u03c0g\u03c6 [ \u2207\u03c6DKL ( \u03c0gT ||\u03c0 g \u03c6 ) + \u03b1\u2207\u03c6Lterm ]] , (5)\nwhere DKL ( \u03c0gT ||\u03c0 g \u03c6 ) = \u2211 a \u03c0 g T (a|s) log \u03c0gT (a|s) \u03c0g\u03c6(a|s)\nand U \u2282 G is the training set of tasks. Lterm = \u2212 log \u03b2\u03c6 (st, g) = \u2212 logP\u03c6 (st \u2208 Tg) is the cross-entropy loss for termination prediction. Intuitively, we sample a mini-batch of tasks (g), use the parameterized skill to generate episodes, and train it to predict teachers\u2019 actions. This method has been shown to be efficient for multi-task learning.\nAfter policy distillation, the parameterized skill is fine-tuned through actor-critic with generalized advantage estimation (GAE) (Schulman et al., 2016) as follows:\n\u2207\u03c6LRL = Eg\u223cU [ Es\u223c\u03c0g\u03c6 [ \u2212\u2207\u03c6 log \u03c0\u03c6 (at|st, g) A\u0302(\u03b3,\u03bb)t + \u03b1\u2207\u03c6Lterm ]] , (6)\nwhere A\u0302(\u03b3,\u03bb)t = \u2211\u221e l=0(\u03b3\u03bb) l\u03b4Vt+l and \u03b4 V t = rt + \u03b3V \u03c0(st+1;\u03c6 \u2032) \u2212 V \u03c0(st;\u03c6\u2032). \u03c6\u2032 is optimized to minimize\nE [ (Rt \u2212 V \u03c0(st;\u03c6\u2032))2 ] . \u03b3, \u03bb \u2208 [0, 1] are a discount factor and a weight for balancing between bias and variance of\nthe advantage estimation.\nThe final update rule for the parameterized skill is:\n\u2206\u03c6 \u221d \u2212 (\u2207\u03c6LRL + \u03be\u2207\u03c6LAM ) , (7)\nwhere LAM = Lsim+\u03c11Ldis+\u03c12Ldiff is the analogy-making regularizer defined as the weighted sum of three objectives described in the main text. \u03c11, \u03c12, \u03be are hyperparameters for each objective.\nD.2. Meta Controller\nActor-critic method with GAE is used to update the parameter of the meta controller as follows:\n\u2207\u03b8LRL = \u2212  E [ ct (\u2211 i\u2207\u03b8 log \u03c0\u03b8 ( g (i) t |ht, rt ) +\u2207\u03b8 logP (lt|ht) ) A\u0302 (\u03b3,\u03bb) t (Hard) +\u2207\u03b8 logP (ct|st,ht\u22121) A\u0302(\u03b3,\u03bb)t + \u03b7\u2207\u03b8 \u2225\u2225\u03c3 (\u03d5update (st,ht\u22121))\u2225\u22251] E [\u2211 i\u2207\u03b8 log [ ct\u03c0\u03b8 ( g (i) t |h\u0303t, r\u0303t ) + (1\u2212 ct)g(i)t\u22121 ] A\u0302 (\u03b3,\u03bb) t ] (Soft),\n(8)\nwhere ct \u223c P (ct|st,ht\u22121) \u221d \u03c3 ( \u03d5update (st,ht\u22121) ) , and P (lt|ht) \u221d Softmax ( \u03d5shift(ht) ) . We applied L1-penalty to the probability of update to penalize too frequent updates, and \u03b7 is a weight for the update penalty.\nThe final update rule for the meta controller is:\n\u2206\u03b8 \u221d \u2212 (\u2207\u03b8LRL + \u03be\u2207\u03b8LAM ) , (9)\nwhere LAM is the analogy-making objective."}, {"heading": "E. Architectures and Hyperparameters", "text": "Background: Multiplicative Interaction For combining condition variables into a neural network (e.g., combining task embedding into the convolutional network in the parameterized skill), we used a form of multiplicative interaction instead of concatenating such variables as suggested by (Memisevic and Hinton, 2010; Oh et al., 2015). This is also related to parameter prediction approaches where the parameters of the neural network is produced by condition variables (e.g., exempler, class embedding). This approach has been shown to be effective for achieving zero-shot and one-shot generalization in image classification problems (Lei Ba et al., 2015; Bertinetto et al., 2016). More formally, given an input (x), the output (y) of a convolution and a fully-connected layer with parameters predicted by a condition variable (g) can be written as:\nConvolution: y = \u03d5 (g) \u2217 x + b Fully-connected: y = W\u2032diag (\u03d5 (g)) Wx + b,\nwhere \u03d5 is the embedding of the condition variable learned by a multi-layer perceptron (MLP). Note that we use matrix factorization (similar to (Memisevic and Hinton, 2010)) to reduce the number of parameters for the fully-connected layer. Intuitively, the condition variable is converted to the weight of the convolution or fully-connected layer through multiplicative interactions. We used this approach both in the parameterized skill and the meta controller. The details are described below.\nParameterized skill. The teacher architecture used for policy distillation is Conv1(32x8x8-4)-Conv2(64x5x5-2)LSTM(64).3 The network has two fully-connected output layers for actions and value (baseline) respectively. The parameterized skill architecture consists of Conv1(16x8x8-4)-Conv2(32x1x1-1)-Conv3(32x1x1-1)-Conv4(32x5x5-2)-LSTM(64). The parameterized skill takes two task parameters (g = [ g(1), g(2) ] ) as additional input and computes \u03d5(g) = ReLU(W(1)g(1) W(2)g(2)) to compute the subtask embedding. It is further linearly transformed into the weights of Conv3 and the (factorized) weight of LSTM through multiplicative interaction as described above. Finally, the network has three fully-connected output layers for actions, termination probability, and baseline, respectively.\nWe used RMSProp optimizer with the smoothing parameter of 0.97 and epsilon of 1e\u22126. When training the teacher policy through actor-critic, we used a learning rate of 2.5e \u2212 4. For training the parameterized skill, we used a learning rate of 2.5e \u2212 4 and 1e \u2212 4 for policy distillation and actor-critic fine-tuning respectively. We used \u03c4dis = \u03c4diff = 3, \u03b1 = 0.1 for analogy-making regularization and the termination prediction objective respectively. \u03b3 = 0.99 and \u03bb = 0.96 are used as a discount factor and a balancing weight for GAE. 16 threads with batch size of 8 are used to run 16 \u00d7 8 episodes in parallel, and the parameter is updated after each run (1 iteration = 16 \u00d7 8 episodes). For better exploration, we applied entropy regularization with a weight of 0.1 and linearly decreased it to zero for the first 7500 iterations. The total number of iterations was 15,000 for both policy distillation and actor-critic fine-tuning.\nMeta Controller. The meta controller consists of Conv1(16x8x8-4)-Conv2(32x1x1-1)-Conv3(32x1x1-1)-Pool(5)LSTM(256). The embedding of previously selected subtask (\u03d5(gt\u22121)), the previously retrieved instruction (rt\u22121), and the subtask termination (bt) are concatenated and given as input for one-layer MLP to compute a 256-dimensional joint embedding. This is further linearly transformed into the weights of Conv3 and LSTM through multiplicative interaction. The output of FC1 is used as the context vector (ht). We used the bag-of-words (BoW) representation as a sentence embedding which computes the sum of all word embeddings in a sentence: \u03d5w (mi) = \u2211|mi| j=1 W\nmwj where Wm is the word embedding matrix, each of which is 256-dimensional. An MLP with one hidden layer with 256 units is for \u03d5shift, a fullyconnected layer is used for \u03d5update. \u03d5goal is an MLP with one hidden layer with 256 units that takes the concatenation of rt and ht as an input and computes the probabilities over subtask parameters as the outputs. The baseline network is a linear regression from the concatenation of the memory pointer pt, a binary mask indicating the presence of given instruction, and the final hidden layer (256 hidden units in \u03d5goal).\nWe used the same hyperparameters used in the parameterized skill except that the batch size was 32 (1 iteration = 16\u00d7 32 episodes). We trained the soft-architecture (with soft-update) with a learning rate of 2.5e \u2212 4 using curriculum learning for 15,000 iterations and a weight of 0.015 for entropy regularization, and fine-tuned it with a learning rate of 1e \u2212 4 without curriculum learning for 5,000 iterations. Finally, we initialized the hard-architecture (with hard-update) to the\n3For convolution layers, NxKxK-S represents N kernels with size of KxK and stride of S. The number in LSTM represents the number of hidden units.\nsoft-architecture and fine-tuned it using a learning rate of 1e \u2212 4 for 5,000 iterations. \u03b7 = 0.001 is used to penalize frequent update decision in Eq (8).\nFlat Controller. The flat controller architecture consists of the same layers used in the meta controller with the following differences. The previously retrieved instruction (rt\u22121) is transformed through an MLP with two hidden layers to compute the weight of Conv3 and LSTM. The output is probabilities over primitive actions.\nCurriculum Learning. For training all architectures, we randomly sampled the size of the world from {5, 6, 7, 8}, the density of walls are sampled from [0, 0.1], and the density of objects are sampled from [0.1, 0.8] during training of parameterized skill and [0, 0.15] during training of the meta controller. We sampled the number of instructions from {1, 2, 3, 4} for training the meta controller. The sampling range was determined based on the success rate of the agent."}], "references": [{"title": "Programmable reinforcement learning agents", "author": ["D. Andre", "S.J. Russell"], "venue": "In NIPS,", "citeRegEx": "Andre and Russell.,? \\Q2000\\E", "shortCiteRegEx": "Andre and Russell.", "year": 2000}, {"title": "State abstraction for programmable reinforcement learning agents", "author": ["D. Andre", "S.J. Russell"], "venue": "In AAAI/IAAI,", "citeRegEx": "Andre and Russell.,? \\Q2002\\E", "shortCiteRegEx": "Andre and Russell.", "year": 2002}, {"title": "Modular multitask reinforcement learning with policy", "author": ["J. Andreas", "D. Klein", "S. Levine"], "venue": "sketches. CoRR,", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "The option-critic architecture", "author": ["P.-L. Bacon", "J. Harb", "D. Precup"], "venue": "In AAAI,", "citeRegEx": "Bacon et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2017}, {"title": "Learning feed-forward one-shot learners", "author": ["L. Bertinetto", "J.F. Henriques", "J. Valmadre", "P.H. Torr", "A. Vedaldi"], "venue": "arXiv preprint arXiv:1606.05233,", "citeRegEx": "Bertinetto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bertinetto et al\\.", "year": 2016}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S.R.K. Branavan", "H. Chen", "L.S. Zettlemoyer", "R. Barzilay"], "venue": "ACL/IJCNLP,", "citeRegEx": "Branavan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2009}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["D.L. Chen", "R.J. Mooney"], "venue": "In AAAI,", "citeRegEx": "Chen and Mooney.,? \\Q2011\\E", "shortCiteRegEx": "Chen and Mooney.", "year": 2011}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["J. Chung", "S. Ahn", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Chung et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2017}, {"title": "Learning parameterized skills", "author": ["B.C. da Silva", "G. Konidaris", "A.G. Barto"], "venue": "In ICML,", "citeRegEx": "Silva et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2012}, {"title": "Learning modular neural network policies for multi-task and multi-robot transfer", "author": ["C. Devin", "A. Gupta", "T. Darrell", "P. Abbeel", "S. Levine"], "venue": null, "citeRegEx": "Devin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Devin et al\\.", "year": 2017}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["T.G. Dietterich"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Stochastic neural networks for hierarchical reinforcement learning", "author": ["C. Florensa", "Y. Duan", "P. Abbeel"], "venue": "In ICLR,", "citeRegEx": "Florensa et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Florensa et al\\.", "year": 2017}, {"title": "Hierarchical policy gradient algorithms", "author": ["M. Ghavamzadeh", "S. Mahadevan"], "venue": "In ICML,", "citeRegEx": "Ghavamzadeh and Mahadevan.,? \\Q2003\\E", "shortCiteRegEx": "Ghavamzadeh and Mahadevan.", "year": 2003}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "Hadsell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "Learning and transfer of modulated locomotor controllers", "author": ["N. Heess", "G. Wayne", "Y. Tassa", "T.P. Lillicrap", "M.A. Riedmiller", "D. Silver"], "venue": "arXiv preprint arXiv:1610.05182,", "citeRegEx": "Heess et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Using task features for zero-shot knowledge transfer in lifelong learning", "author": ["D. Isele", "M. Rostami", "E. Eaton"], "venue": "In IJCAI,", "citeRegEx": "Isele et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Isele et al\\.", "year": 2016}, {"title": "Actor-critic algorithms", "author": ["V.R. Konda", "J.N. Tsitsiklis"], "venue": "In NIPS,", "citeRegEx": "Konda and Tsitsiklis.,? \\Q1999\\E", "shortCiteRegEx": "Konda and Tsitsiklis.", "year": 1999}, {"title": "Building portable options: Skill transfer in reinforcement learning", "author": ["G. Konidaris", "A.G. Barto"], "venue": "In IJCAI,", "citeRegEx": "Konidaris and Barto.,? \\Q2007\\E", "shortCiteRegEx": "Konidaris and Barto.", "year": 2007}, {"title": "Transfer in reinforcement learning via shared features", "author": ["G. Konidaris", "I. Scheidwasser", "A.G. Barto"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Konidaris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2012}, {"title": "A clockwork rnn", "author": ["J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": "In ICML,", "citeRegEx": "Koutnik et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutnik et al\\.", "year": 2014}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["T.D. Kulkarni", "K.R. Narasimhan", "A. Saeedi", "J.B. Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Predicting deep zero-shot convolutional neural networks using textual descriptions", "author": ["J. Lei Ba", "K. Swersky", "S. Fidler"], "venue": "In ICCV,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Walk the talk: Connecting language, knowledge, and action in route instructions", "author": ["M. MacMahon", "B. Stankiewicz", "B. Kuipers"], "venue": "In AAAI,", "citeRegEx": "MacMahon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "MacMahon et al\\.", "year": 2006}, {"title": "Autonomous discovery of temporal abstractions from interaction with an environment", "author": ["A. McGovern", "A.G. Barto"], "venue": "PhD thesis, University of Massachusetts,", "citeRegEx": "McGovern and Barto.,? \\Q2002\\E", "shortCiteRegEx": "McGovern and Barto.", "year": 2002}, {"title": "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences", "author": ["H. Mei", "M. Bansal", "M.R. Walter"], "venue": "arXiv preprint arXiv:1506.04089,", "citeRegEx": "Mei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2015}, {"title": "Learning to represent spatial transformations with factored higher-order boltzmann machines", "author": ["R. Memisevic", "G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Memisevic and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Memisevic and Hinton.", "year": 2010}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": "arXiv preprint arXiv:1309.4168,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Actionconditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "In NIPS,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Memorybased control of active perception and action in minecraft", "author": ["J. Oh", "V. Chockalingam", "S. Singh", "H. Lee"], "venue": "In ICML,", "citeRegEx": "Oh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2016}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["E. Parisotto", "J.L. Ba", "R. Salakhutdinov"], "venue": "In ICLR,", "citeRegEx": "Parisotto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2016}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["R. Parr", "S.J. Russell"], "venue": "In NIPS,", "citeRegEx": "Parr and Russell.,? \\Q1997\\E", "shortCiteRegEx": "Parr and Russell.", "year": 1997}, {"title": "Deep visual analogy-making", "author": ["S.E. Reed", "Y. Zhang", "H. Lee"], "venue": "In NIPS,", "citeRegEx": "Reed et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2015}, {"title": "Policy distillation", "author": ["A.A. Rusu", "S.G. Colmenarejo", "C. Gulcehre", "G. Desjardins", "J. Kirkpatrick", "R. Pascanu", "V. Mnih", "K. Kavukcuoglu", "R. Hadsell"], "venue": "In ICLR,", "citeRegEx": "Rusu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Universal value function approximators", "author": ["T. Schaul", "D. Horgan", "K. Gregor", "D. Silver"], "venue": "In ICML,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "In ICLR,", "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "The efficient learning of multiple task sequences", "author": ["S.P. Singh"], "venue": "In NIPS,", "citeRegEx": "Singh.,? \\Q1991\\E", "shortCiteRegEx": "Singh.", "year": 1991}, {"title": "Transfer of learning by composing solutions of elemental sequential tasks", "author": ["S.P. Singh"], "venue": "Machine Learning,", "citeRegEx": "Singh.,? \\Q1992\\E", "shortCiteRegEx": "Singh.", "year": 1992}, {"title": "Mazebase: A sandbox for learning from games", "author": ["S. Sukhbaatar", "A. Szlam", "G. Synnaeve", "S. Chintala", "R. Fergus"], "venue": "arXiv preprint arXiv:1511.07401,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S. Tellex", "T. Kollar", "S. Dickerson", "M.R. Walter", "A.G. Banerjee", "S.J. Teller", "N. Roy"], "venue": "In AAAI,", "citeRegEx": "Tellex et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Asking for help using inverse semantics", "author": ["S. Tellex", "R.A. Knepper", "A. Li", "D. Rus", "N. Roy"], "venue": "In RSS,", "citeRegEx": "Tellex et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tellex et al\\.", "year": 2014}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["C. Tessler", "S. Givony", "T. Zahavy", "D.J. Mankowitz", "S. Mannor"], "venue": "In AAAI,", "citeRegEx": "Tessler et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tessler et al\\.", "year": 2017}, {"title": "Reinforcement learning neural turing machines", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}], "referenceMentions": [{"referenceID": 29, "context": "We developed a 3D visual environment using Minecraft based on Oh et al. (2016) where the agent can interact with many objects.", "startOffset": 62, "endOffset": 79}, {"referenceID": 40, "context": "Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007).", "startOffset": 101, "endOffset": 248}, {"referenceID": 10, "context": "Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007).", "startOffset": 101, "endOffset": 248}, {"referenceID": 32, "context": "Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007).", "startOffset": 101, "endOffset": 248}, {"referenceID": 12, "context": "Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007).", "startOffset": 101, "endOffset": 248}, {"referenceID": 20, "context": "Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007).", "startOffset": 101, "endOffset": 248}, {"referenceID": 19, "context": "Typically these have the form of a meta controller and a set of lower-level controllers for subtasks (Sutton et al., 1999; Dietterich, 2000; Parr and Russell, 1997; Ghavamzadeh and Mahadevan, 2003; Konidaris et al., 2012; Konidaris and Barto, 2007).", "startOffset": 101, "endOffset": 248}, {"referenceID": 10, "context": ", Taxi domain (Dietterich, 2000)).", "startOffset": 14, "endOffset": 32}, {"referenceID": 37, "context": "This makes it hard to evaluate the agent\u2019s ability to compose pre-learned policies to solve previously unseen sequential tasks in a zero-shot fashion unless we re-train the agent on the new tasks in a transfer learning setting (Singh, 1991; 1992; McGovern and Barto, 2002).", "startOffset": 227, "endOffset": 272}, {"referenceID": 25, "context": "This makes it hard to evaluate the agent\u2019s ability to compose pre-learned policies to solve previously unseen sequential tasks in a zero-shot fashion unless we re-train the agent on the new tasks in a transfer learning setting (Singh, 1991; 1992; McGovern and Barto, 2002).", "startOffset": 227, "endOffset": 272}, {"referenceID": 0, "context": "Our work is also closely related to Programmable HAMs (PHAMs) (Andre and Russell, 2000; 2002) in that a PHAM is designed to execute a given program.", "startOffset": 62, "endOffset": 93}, {"referenceID": 21, "context": "Kulkarni et al. (2016) proposed hierarchical Deep Q-Learning and demonstrated improved exploration in a challenging Atari game.", "startOffset": 0, "endOffset": 23}, {"referenceID": 21, "context": "Kulkarni et al. (2016) proposed hierarchical Deep Q-Learning and demonstrated improved exploration in a challenging Atari game. Tessler et al. (2017) proposed a similar architecture, but the highlevel controller is allowed to choose primitive actions directly.", "startOffset": 0, "endOffset": 150}, {"referenceID": 3, "context": "Bacon et al. (2017) proposed the option-critic architecture, which learns options without pseudo reward and demonstrated that it can learn distinct options in Atari", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "Heess et al. (2016) formulated the actions of the meta controller as continuous variables that are used to modulate the behavior of the low-level controller.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "Florensa et al. (2017) trained a stochastic neural network with mutual information regularization to discover skills.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "For example, da Silva et al. (2012) introduced parameterized skills that map sets of task descriptions to policies.", "startOffset": 16, "endOffset": 36}, {"referenceID": 7, "context": "For example, da Silva et al. (2012) introduced parameterized skills that map sets of task descriptions to policies. Isele et al. (2016) achieved zero-shot task generalization through dictionary learning with sparsity constraints.", "startOffset": 16, "endOffset": 136}, {"referenceID": 7, "context": "For example, da Silva et al. (2012) introduced parameterized skills that map sets of task descriptions to policies. Isele et al. (2016) achieved zero-shot task generalization through dictionary learning with sparsity constraints. Schaul et al. (2015) proposed universal value function approximators (UVFAs) that learn value functions for state and goal pairs.", "startOffset": 16, "endOffset": 251}, {"referenceID": 7, "context": "For example, da Silva et al. (2012) introduced parameterized skills that map sets of task descriptions to policies. Isele et al. (2016) achieved zero-shot task generalization through dictionary learning with sparsity constraints. Schaul et al. (2015) proposed universal value function approximators (UVFAs) that learn value functions for state and goal pairs. Devin et al. (2017) proposed composing sub-networks that are shared across tasks and robots in order to achieve generalization to unseen configurations of them.", "startOffset": 16, "endOffset": 380}, {"referenceID": 2, "context": "Andreas et al. (2016) proposed a framework to learn the underlying subtasks from a policy sketch which specifies a sequence of subtasks, and the agent can generalize over new sequences of them in principle.", "startOffset": 0, "endOffset": 22}, {"referenceID": 22, "context": "(2011; 2014) for robotics and MacMahon et al. (2006); Chen and Mooney (2011); Mei et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 5, "context": "(2006); Chen and Mooney (2011); Mei et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 5, "context": "(2006); Chen and Mooney (2011); Mei et al. (2015) for a simulated environment.", "startOffset": 8, "endOffset": 50}, {"referenceID": 5, "context": "Although Branavan et al. (2009) also tackle a similar problem, the agent is given a single instruction at a time, while our agent needs to learn how to align instructions and state given a full list of instructions.", "startOffset": 9, "endOffset": 32}, {"referenceID": 40, "context": "Policy: \u03c0\u03c6(at|xt, g) Termination: \u03b2\u03c6(bt|xt, g), where \u03c0\u03c6 is the policy optimized for the task g, and \u03b2\u03c6 is a termination function (Sutton et al., 1999) which is the probability that the state is terminal at time t for the given task g.", "startOffset": 130, "endOffset": 151}, {"referenceID": 33, "context": "To this end, we propose an analogy-making objective inspired by Reed et al. (2015). The main idea is to learn correspondences between tasks.", "startOffset": 64, "endOffset": 83}, {"referenceID": 14, "context": "We incorporate these constraints into the following objectives based on contrastive loss (Hadsell et al., 2006):", "startOffset": 89, "endOffset": 111}, {"referenceID": 27, "context": ", parallelogram structure in the embedding space; see Mikolov et al. (2013); Reed et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 27, "context": ", parallelogram structure in the embedding space; see Mikolov et al. (2013); Reed et al. (2015)), while the other constraints prevent trivial solutions.", "startOffset": 54, "endOffset": 96}, {"referenceID": 36, "context": "Training The parameterized skill is trained on a set of tasks (G\u2032 \u2282 G) through the actor-critic method with generalized advantage estimation (Schulman et al., 2016).", "startOffset": 141, "endOffset": 164}, {"referenceID": 34, "context": "We also found that pre-training through policy distillation (Rusu et al., 2016; Parisotto et al., 2016) gives slightly better results as discussed in Tessler et al.", "startOffset": 60, "endOffset": 103}, {"referenceID": 31, "context": "We also found that pre-training through policy distillation (Rusu et al., 2016; Parisotto et al., 2016) gives slightly better results as discussed in Tessler et al.", "startOffset": 60, "endOffset": 103}, {"referenceID": 31, "context": ", 2016; Parisotto et al., 2016) gives slightly better results as discussed in Tessler et al. (2017). Throughout training, the parameterized skill is also made to predict whether the current state is terminal or not through a binary classification objective, and the analogy-making objective is applied to the task embedding separately.", "startOffset": 8, "endOffset": 100}, {"referenceID": 29, "context": "We developed a 3D visual environment using Minecraft based on Oh et al. (2016) as shown in Figure 1.", "startOffset": 62, "endOffset": 79}, {"referenceID": 16, "context": "The network architecture of the parameterized skill consists of 4 convolution layers and one LSTM (Hochreiter and Schmidhuber, 1997) layer.", "startOffset": 98, "endOffset": 132}, {"referenceID": 16, "context": "The network architecture of the parameterized skill consists of 4 convolution layers and one LSTM (Hochreiter and Schmidhuber, 1997) layer. We conducted curriculum training by changing the size of the world, the density of object and walls according to the agent\u2019s success rate. We implemented actor-critic method with 16 CPU threads based on Sukhbaatar et al. (2015). The parameters are updated after 8 episodes for each thread.", "startOffset": 99, "endOffset": 368}, {"referenceID": 44, "context": "Since instructions should be executed sequentially, we use a location-based memory addressing mechanism (Zaremba and Sutskever, 2015; Graves et al., 2014) to manage the instruction pointer.", "startOffset": 104, "endOffset": 154}, {"referenceID": 13, "context": "Since instructions should be executed sequentially, we use a location-based memory addressing mechanism (Zaremba and Sutskever, 2015; Graves et al., 2014) to manage the instruction pointer.", "startOffset": 104, "endOffset": 154}, {"referenceID": 40, "context": "Instead, having temporallyextended actions can be useful for dealing with delayed reward by operating at a larger time-scale (Sutton et al., 1999).", "startOffset": 125, "endOffset": 146}, {"referenceID": 21, "context": "The idea of learning the time-scale of a recurrent neural network is closely related to hierarchical RNN approaches (Koutnik et al., 2014; Chung et al., 2017) where different groups of recurrent hidden units operate at different time-scales to capture both long-term and short-term temporal information.", "startOffset": 116, "endOffset": 158}, {"referenceID": 7, "context": "The idea of learning the time-scale of a recurrent neural network is closely related to hierarchical RNN approaches (Koutnik et al., 2014; Chung et al., 2017) where different groups of recurrent hidden units operate at different time-scales to capture both long-term and short-term temporal information.", "startOffset": 116, "endOffset": 158}, {"referenceID": 22, "context": "This approach is similar to recent hierarchical deep RL methods (Kulkarni et al., 2016; Tessler et al., 2017).", "startOffset": 64, "endOffset": 109}, {"referenceID": 43, "context": "This approach is similar to recent hierarchical deep RL methods (Kulkarni et al., 2016; Tessler et al., 2017).", "startOffset": 64, "endOffset": 109}], "year": 2017, "abstractText": "As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.", "creator": "LaTeX with hyperref package"}}}