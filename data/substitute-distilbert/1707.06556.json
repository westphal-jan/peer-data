{"id": "1707.06556", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jul-2017", "title": "High-risk learning: acquiring new word vectors from tiny data", "abstract": "distributional linear models are known to struggle with small data. it is generally accepted that in order to manage'a good vector'for a word, a model can have sufficient examples of its interactions. this contradicts the fact that managers can guess the meaning of a word from a few occurrences errors. analyzing this study, we show that a neural language model such as word2vec only necessitates minor corrections to its standard architecture before learn new terms from tiny computers, using background knowledge from a previously learnt semantic space. we test our model versus word definitions and on a nonce homo involving 2 - 6 sentences'worth of context, proving a large increase performance performance over state - of - the - art models on the definitional task.", "histories": [["v1", "Thu, 20 Jul 2017 15:02:14 GMT  (21kb,D)", "http://arxiv.org/abs/1707.06556v1", "Accepted as short paper at EMNLP 2017"]], "COMMENTS": "Accepted as short paper at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["aur\u00e9lie herbelot", "marco baroni"], "accepted": true, "id": "1707.06556"}, "pdf": {"name": "1707.06556.pdf", "metadata": {"source": "CRF", "title": "High-risk learning: acquiring new word vectors from tiny data", "authors": ["Aur\u00e9lie Herbelot", "Marco Baroni"], "emails": ["aurelie.herbelot@cantab.net", "marco.baroni@unitn.it"], "sections": [{"heading": "1 Introduction", "text": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013), do not fare well in the absence of large corpora. That is, for a DS model to learn a word vector, it must have seen that word a sufficient number of times. This is in sharp contrast with the human ability to perform fast mapping, i.e. the acquisition of a new concept from a single exposure to information (Lake et al., 2011; Trueswell et al., 2013; Lake et al., 2016).\nThere are at least two reasons for wanting to acquire vectors from very small data. First, some words are simply rare in corpora, but potentially crucial to some applications (consider, for instance, the processing of text containing technical\nterminology). Second, it seems that fast-mapping should be a prerequisite for any system pretending to cognitive plausibility: an intelligent agent with learning capabilities should be able to make educated guesses about new concepts it encounters.\nOne way to deal with data sparsity issues when learning word vectors is to use morphological structure as a way to overcome the lack of primary data (Lazaridou et al., 2013; Luong et al., 2013; Kisselew et al., 2015; Pado\u0301 et al., 2016). Whilst such work has shown promising result, it is only applicable when there is transparent morphology to fall back on. Another strand of research has been started by Lazaridou et al. (2017), who recently showed that by using simple summation over the (previously learnt) contexts of a nonce word, it is possible to obtain good correlation with human judgements in a similarity task. It is important to note that both these strategies assume that rare words are special cases of the distributional semantics apparatus, and thus require separate approaches to model them.\nHaving different algorithms for modelling the same phenomenon means however that we need some meta-theory to know when to apply one or the other: it is for instance unclear at which frequency a rare word is not rare anymore. Further, methods like summation are naturally selflimiting: they create frustratingly strong baselines but are too simplistic to be extended and improved in any meaningful way. In this paper, our underlying assumption is thus that it would be desirable to build a single, all-purpose architecture to learn word representations from any amount of data. The work we present views fast-mapping as a component of an incremental architecture: the rare word case is simply the first part of the concept learning process, regardless of how many times it will eventually be encountered.\nWith the aim of producing such an incremen-\nar X\niv :1\n70 7.\n06 55\n6v 1\n[ cs\n.C L\n] 2\n0 Ju\nl 2 01\n7\ntal system, we demonstrate that the general architecture of neural language models like Word2Vec (Mikolov et al., 2013) is actually suited to modelling words from a few occurrences only, providing minor adjustments are made to the model itself and its parameters. Our main conclusion is that the combination of a heightened learning rate and greedy processing results in very reasonable oneshot learning, but that some safeguards must be in place to mitigate the high risks associated with this strategy."}, {"heading": "2 Task description", "text": "We want to simulate the process by which a competent speaker encounters a new word in known contexts. That is, we assume an existing vocabulary (i.e. a previously trained semantic space) which can help the speaker \u2018guess\u2019 the meaning of the new word. To evaluate this process, we use two datasets, described below.\nThe definitional nonce dataset We build a novel dataset based on encyclopedic data, simulating the case where the context of the unknown word is supposedly maximally informative.1 We first record all Wikipedia titles containing one word only (e.g. Albedo, Insulin). We then extract the first sentence of the Wikipedia page corresponding to each target title (e.g. Insulin is a peptide hormone produced by beta cells in the pancreas.), and tokenise that sentence using the Spacy toolkit.2 Each occurrence of the target in the sentence is replaced with a slot ( ).\nFrom this original dataset, we only retain sentences with enough information (i.e. a length over 10 words), corresponding to targets which are frequent enough in the UkWaC corpus (Baroni et al. (2009), minimum frequency of 200). The frequency threshold allows us to make sure that we have a high-quality gold vector to compare our learnt representation to. We then randomly sample 1000 sentences, manually checking the data to remove instances that are, in fact, not definitional. We split the data into 700 training and 300 test instances.\nOn this dataset, we simulate first-time exposure to the nonce word by changing the label of the gold standard vector in the background semantic space, and producing a new, randomly initialised vector\n1Data available at http://aurelieherbelot. net/resources/.\n2https://spacy.io/\nfor the nonce. So for instance, insulin becomes insulin gold, and a new random embedding is added to the input matrix for insulin. This setup allows us to easily measure the similarity of the newly learnt vector, obtained from one definition, to the vector produced by exposure to the whole Wikipedia. To measure the relative performance of various setups, we calculate the Reciprocal Rank (RR) of the gold vector in the list of all nearest neighbours to the learnt representation. We average RRs over the number of instances in the dataset, thus obtaining a single MRR figure (Mean Reciprocal Rank).\nThe Chimera dataset Our second dataset is the \u2018Chimera\u2019 dataset of (Lazaridou et al., 2017).3 This dataset was specifically constructed to simulate a nonce situation where a speaker encounters a word for the first time in naturally-occurring (and not necessarily informative) sentences. Each instance in the data is a nonce, associated with 2-6 sentences showing the word in context. The novel concept is created as a \u2018chimera\u2019, i.e. a mixture of two existing and somewhat related concepts (e.g., a buffalo crossed with an elephant). The sentences associated with the nonce are utterances containing one of the components of the chimera, randomly extracted from a large corpus.\nThe dataset was annotated by humans in terms of the similarity of the nonce to other, randomly selected concepts. Fig. 1 gives an example of a data point with 2 sentences of context, with the nonce capitalised (VALTUOR, a combination of cucumber and celery). The sentences are followed by the \u2018probes\u2019 of the trial, i.e. the concepts that the nonce must be compared to. Finally, human similarity responses are given for each probe with respect to the nonce. Each chimera was rated by an average of 143 subjects. In our experiments, we simply replace all occurrences of the original nonce with a slot ( ) and learn a representation for that slot. For each setting (2, 4 and 6 sentences), we randomly split the 330 instances in the data into 220 for training and 110 for testing.\nFollowing the authors of the dataset, we evaluate by calculating the correlation between system and human judgements. For each trial, we calculate Spearman correlation (\u03c1) between the similarities given by the system to each nonce-probe pair, and the human responses. The overall result is the average Spearman across all trials.\n3Available at http://clic.cimec.unitn.it/ Files/PublicData/chimeras.zip."}, {"heading": "3 Baseline models", "text": "We test two state-of-the art systems: a) Word2Vec (W2V) in its Gensim4 implementation, allowing for update of a prior semantic space; b) the additive model of Lazaridou et al. (2017), using a background space from W2V.\nWe note that both models allow for some sort of incrementality. W2V processes input one context at a time (or several, if mini-batches are implemented), performing gradient descent after each new input. The network\u2019s weights in the input, which correspond to the created word vectors, can be inspected at any time.5 As for addition, it also affords the ability to stop and restart training at any time: a typical implementation of this behaviour can be found in distributional semantics models based on random indexing (see e.g. QasemiZadeh et al., 2017). This is in contrast with so-called \u2018count-based\u2019 models calculated by computing a frequency matrix over a fixed corpus, which is then globally modified through a transformation such as Pointwise Mutual Information.\nWord2Vec We consider W2V\u2019s \u2018skip-gram\u2019 model, which learns word vectors by predicting the context words of a particular target. The W2V architecture includes several important parameters, which we briefly describe below.\nIn W2V, predicting a word implies the ability to distinguish it from so-called negative samples, i.e. other words which are not the observed item. The number of negative samples to be considered can be tuned. What counts as a context for a particular target depends on the window size around that target. W2V features random resizing of the window, which has been shown to increase the model\u2019s performance. Further, each sentence passed to the model undergoes subsampling, a random process by which some words are dropped out of the input\n4Available at https://github.com/ RaRe-Technologies/gensim.\n5Technically speaking, standard W2V is not fully incremental, as it requires a first pass through the corpus to compute a vocabulary, with associated frequencies. As we show in \u00a75, it however allows for an incremental interpretation, given minor modifications.\nas a function of their overall frequency. Finally, the learning rate \u03b1 measures how quickly the system learns at each training iteration. Traditionally, \u03b1 is set low (0.025 for Gensim) in order not to overshoot the system\u2019s error minimum.\nGensim has an update function which allows us to save a W2V model and continue learning from new data: this lets us simulate prior acquisition of a background vocabulary and new learning from a nonce\u2019s context. As background vocabulary, we use a semantic space trained on a Wikipedia snapshot of 1.6B words with Gensim\u2019s standard parameters (initial learning rate of 0.025, 5 negative samples, a window of\u00b15 words, subsampling 1e\u22123, 5 epochs). We use the skip-gram model with a minimum word count of 50 and vector dimensionality 400. This results in a space with 259, 376 word vectors. We verify the quality of this space by calculating correlation with the similarity ratings in the MEN dataset (Bruni et al., 2014). We obtain \u03c1 = 0.75, indicating an excellent fit with human judgements.\nAdditive model Lazaridou et al. (2017) use a simple additive model, which sums the vectors of the context words of the nonce, taking as context the entire sentence where the target occurs. Their model operates on multimodal vectors, built over both text and images. In the present work, however, we use the semantic space described above, built on Wikipedia text only. We do not normalise vectors before summing, as we found that the system\u2019s performance was better than with normalisation. We also discard function words when summing, using a stopword list. We found that this step affects results very positively.\nThe results for our state-of-the-art models are shown in the top sections of Tables 1 and 2. W2V is run with the standard Gensim parameters, under the skip-gram model. It is clear from the results that W2V is unable to learn nonces from definitions (MRR = 0.00007). The additive model, on the other hand, performs well: an MRR of 0.03686 means that the median rank of the true vector is 861, out of a challenging\n259, 376 neighbours (the size of the vocabulary). On the Chimeras dataset, W2V still performs well under the sum model \u2013 although the difference is not as marked and possibly indicates that this dataset is more difficult (which we would expect, as the sentences are not as informative as in the encyclopedia case)."}, {"heading": "4 Nonce2Vec", "text": "Our system, Nonce2Vec (N2V),6 modifies W2V in the following ways.\nInitialisation: since addition gives a good approximation of the nonce word, we initialise our vectors to the sum of all known words in the context sentences (see \u00a73). Note that this is not strictly equivalent to the pure sum model, as subsampling takes care of frequent word deletion in this setup (as opposed to a stopword list). In practice, this means that the initialised vectors are of slightly lesser quality than the ones from the sum model.\nParameter choice: we experiment with higher learning rates coupled with larger window sizes. That is, the model should take the risk of a) overshooting a minimum error; b) greedily considering irrelevant contexts in order to increase its chance to learn anything. We mitigate these risks through selective training and appropriate parameter decay (see below).\nWindow resizing: we suppress the random window resizing step when learning the nonce. This is because we need as much data as possible and accordingly need a large window around the target. Resizing would make us run the risk of ending up with a small window of a few words only, which would be uninformative.\nSubsampling: With the goal of keeping most of our tiny data, we adopt a subsampling rate that only discards extremely frequent words.\nSelective training: we only train the nonce. That is, we only update the weights of the network for the target. This ensures that, despite the high selected learning rate, the previously learnt vectors, associated with the other words in the sentence, will not be radically shifted towards the meaning expressed in that particular sentence.\nWhilst the above modifications are appropriate to deal with the first mention of a word, we must ask in what measure they still are applicable when the term is encountered again (see \u00a71). With a\n6Code available at https://github.com/ minimalparts/nonce2vec.\nview to cater for incrementality, we introduce a notion of parameter decay in the system. We hypothesise that the initial high-risk strategy, combining high learning rate and greedy processing of the data, should only be used in the very first training steps. Indeed, this strategy drastically moves the initialised vector to what the system assumes is the right neighbourhood of the semantic space. Once this positioning has taken place, the system should refine its guess rather than wildly moving in the space. We thus suggest that the learning rate itself, but also the subsampling value and window size should be returned to more conventional standards as soon as it is desirable. To achieve this, we apply some exponential decay to the learning rate of the nonce, proportional to the number of times the term has been seen: every time t that we train a pair containing the target word, we set \u03b1 to \u03b10e\n\u2212\u03bbt, where \u03b10 is our initial learning rate. We also decrease the window size and increase subsampling rate on a per-sentence basis (see \u00a75)."}, {"heading": "5 Experiments", "text": "We first tune N2V\u2019s initial parameters on the training part of the definitional dataset. We experiment with a range of values for the learning rate ([0.5, 0.8, 1, 2, 5, 10, 20]), window size ([5, 10, 15, 20]), the number of negative samples ([3, 5, 10]), the number of epochs ([1, 5]) and the subsampling rate ([500, 1000, 10000]). Here, given the size of the data, the minimum frequency for a word to be considered is 1. The best performance is obtained for a window of 15 words, 3 negative samples, a learning rate of 1, a subsampling rate of 10000, an exponential decay where \u03bb = 170 , and one single epoch (that is, the system truly implements fast-mapping). When applied to\nthe test set, N2V shows a dramatic improvement in performance over the simple sum model, reaching MMR = 0.04907 (median rank 623).\nOn the training set of the Chimeras, we further tune the per-sentence decrease in window size and increase in subsampling. For the window size, we experiment with a reduction of [1...6] words on either side of the target, not going under a window of \u00b13 words. Further, we adjust each word\u2019s subsampling rate by a factor in the range [1.1, 1.2...1.9, 2.0]. Our results confirm that indeed, an appropriate change in those parameters is required: keeping them constant results in decreasing performance as more sentences are introduced. On the training set, we obtain our best performance (averaged over the 2-, 4- and 6- sentences datasets) for a per-sentence window size decrease of 5 words on either side of the target, and adjusting subsampling by a factor of 1.9. Table 2 shows results on the three corresponding test sets using those parameters. Unfortunately, on this dataset, N2V does not improve on addition.\nThe difference in performance between the definitional and the Chimeras datasets may be explained in two ways. First, the chimera sentences were randomly selected and thus, are not necessarily hugely informative about the nature of the nonce. Second, the most informative sentences are not necessarily at the beginning of the fragment, so the system heightens its learning rate on the wrong data: the risk does not pay off. This suggests that a truly intelligent system should adjust its parameters in a non-monotonic way, to take into account the quality of the information it is processing. This point seems to be an important general requirement for any architecture that claims incrementality: our results indicate very strongly that a notion of informativeness must play a role in the learning decisions of the system. This conclusion is in line with work in other domains, e.g. interactive word learning using dialogue, where performance is linked to the ability of the system to measure its own confidence in particular pieces of knowledge and ask questions with a high information gain (Yu et al., 2016). It also meets with general considerations on language acquisition, which accounts for the ability of young children to learn from limited \u2018primary linguistic data\u2019 by restricting explanatory models to those that provide such efficiency (Clark and Lappin, 2010)."}, {"heading": "6 Conclusion", "text": "We have proposed Nonce2Vec, a Word2Vecinspired architecture to learn new words from tiny data. It requires a high-risk strategy combining heightened learning rate and greedy processing of the context. The particularly good performance of the system on definitions makes us confident that it is possible to build a unique, unified algorithm for learning word meaning from any amount of data. However, the less impressive performance on naturally-occurring sentences indicates that an ideal system should modulate its learning as a function of the informativeness of a context sentence, that is, take risks \u2018at the right time\u2019.\nAs pointed out in the introduction, Nonce2Vec is designed with a view to be an essential component of an incremental concept learning architecture. In order to validate our system as a suitable, generic solution for word learning, we will have to test it on various data sizes, from the type of low- to middle-frequency terms found in e.g. the Rare Words dataset (Luong et al., 2013), to highly frequent words. We would like to systematically evaluate, in particular, how fast the system can gain an understanding of a concept which is fully equivalent to a vector built from big data. We believe that both quality and speed of learning will be strongly influenced by the ability of the algorithm to detect what we called informative sentences. Our future work will thus investigate how to capture and measure informativeness."}, {"heading": "Acknowledgments", "text": "We are grateful to Katrin Erk for inspiring conversations about tiny data and fast-mapping, and to Raffaella Bernardi and Sandro Pezzelle for comments on an early draft of this paper. We also thank the anonymous reviewers for their time and valuable comments. We acknowledge ERC 2011 Starting Independent Research Grant No 283554 (COMPOSES). This project has also received funding from the European Union\u2019s Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement No 751250."}], "references": [{"title": "The WaCky wide web: a collection of very large linguistically processed", "author": ["Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2009}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of machine learning research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam Khanh Tran", "Marco Baroni."], "venue": "Journal of Artificial Intelligence Research, 49(1):1\u201347.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Computational learning theory and language acquisition", "author": ["Alexander Clark", "Shalom Lappin."], "venue": "Ruth M Kempson, Tim Fernando, and Nicholas Asher, editors, Philosophy of linguistics, pages 445\u2013 475. Elsevier.", "citeRegEx": "Clark and Lappin.,? 2010", "shortCiteRegEx": "Clark and Lappin.", "year": 2010}, {"title": "Vector space models of lexical meaning", "author": ["Stephen Clark."], "venue": "Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics \u2013 second edition. Wiley-Blackwell.", "citeRegEx": "Clark.,? 2012", "shortCiteRegEx": "Clark.", "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Vector space models of word meaning and phrase meaning: a survey", "author": ["Katrin Erk."], "venue": "Language and Linguistics Compass, 6:635\u2013653.", "citeRegEx": "Erk.,? 2012", "shortCiteRegEx": "Erk.", "year": 2012}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Obtaining a Better Understanding of Distributional Models of German Derivational Morphology", "author": ["Max Kisselew", "Sebastian Pad\u00f3", "Alexis Palmer", "Jan \u0160najder."], "venue": "Proceedings of the 11th International Conference on Computational Semantics", "citeRegEx": "Kisselew et al\\.,? 2015", "shortCiteRegEx": "Kisselew et al\\.", "year": 2015}, {"title": "One-shot learning of simple visual concepts", "author": ["Brenden M Lake", "Ruslan Salakhutdinov", "Jason Gross", "Joshua B Tenenbaum."], "venue": "Proceedings of the 33rd Annual Meeting of the Cognitive Science Society (CogSci2012), Boston, MA.", "citeRegEx": "Lake et al\\.,? 2011", "shortCiteRegEx": "Lake et al\\.", "year": 2011}, {"title": "Building machines that learn and think like people", "author": ["Brenden M. Lake", "Tomer D. Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman."], "venue": "arxiv, abs/1604.00289.", "citeRegEx": "Lake et al\\.,? 2016", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "Multimodal word meaning induction from minimal exposure to natural text", "author": ["Angeliki Lazaridou", "Marco Marelli", "Marco Baroni."], "venue": "Cognitive Science, 41(S4):677\u2013705.", "citeRegEx": "Lazaridou et al\\.,? 2017", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2017}, {"title": "Compositional-ly Derived Representations of Morphologically Complex", "author": ["Angeliki Lazaridou", "Marco Marelli", "Roberto Zamparelli", "Marco Baroni"], "venue": null, "citeRegEx": "Lazaridou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 17th Conference on Computational Natural Language Learning (CoNLL2013),", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad-", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Predictability of distributional semantics in derivational word formation", "author": ["Sebastian Pad\u00f3", "Aur\u00e9lie Herbelot", "Max Kisselew", "Jan \u0160najder."], "venue": "Proceedings of the 26th International Conference on Computational Linguistics (COLING2016), Osaka,", "citeRegEx": "Pad\u00f3 et al\\.,? 2016", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2016}, {"title": "Non-Negative Randomized Word Embeddings", "author": ["Behrang QasemiZadeh", "Laura Kallmeyer", "Aur\u00e9lie Herbelot."], "venue": "Proceedings of Traitement automatique des langues naturelles (TALN2017), Orl\u00e9ans, France.", "citeRegEx": "QasemiZadeh et al\\.,? 2017", "shortCiteRegEx": "QasemiZadeh et al\\.", "year": 2017}, {"title": "Propose but verify: Fast mapping meets cross-situational word learning", "author": ["John C Trueswell", "Tamara Nicol Medina", "Alon Hafri", "Lila R Gleitman."], "venue": "Cognitive psychology, 66(1):126\u2013156.", "citeRegEx": "Trueswell et al\\.,? 2013", "shortCiteRegEx": "Trueswell et al\\.", "year": 2013}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel."], "venue": "Journal of artificial intelligence research, 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Training an adaptive dialogue policy for interactive learning of visually grounded word meanings", "author": ["Yanchao Yu", "Arash Eshghi", "Oliver Lemon."], "venue": "Proceedings of the 17th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL2016), pages", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013), do not fare well in the absence of large corpora.", "startOffset": 124, "endOffset": 211}, {"referenceID": 5, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013), do not fare well in the absence of large corpora.", "startOffset": 124, "endOffset": 211}, {"referenceID": 7, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013), do not fare well in the absence of large corpora.", "startOffset": 124, "endOffset": 211}, {"referenceID": 14, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013), do not fare well in the absence of large corpora.", "startOffset": 124, "endOffset": 211}, {"referenceID": 9, "context": "the acquisition of a new concept from a single exposure to information (Lake et al., 2011; Trueswell et al., 2013; Lake et al., 2016).", "startOffset": 71, "endOffset": 133}, {"referenceID": 17, "context": "the acquisition of a new concept from a single exposure to information (Lake et al., 2011; Trueswell et al., 2013; Lake et al., 2016).", "startOffset": 71, "endOffset": 133}, {"referenceID": 10, "context": "the acquisition of a new concept from a single exposure to information (Lake et al., 2011; Trueswell et al., 2013; Lake et al., 2016).", "startOffset": 71, "endOffset": 133}, {"referenceID": 9, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 3, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al.", "startOffset": 53, "endOffset": 66}, {"referenceID": 3, "context": "Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al.", "startOffset": 53, "endOffset": 78}, {"referenceID": 12, "context": "One way to deal with data sparsity issues when learning word vectors is to use morphological structure as a way to overcome the lack of primary data (Lazaridou et al., 2013; Luong et al., 2013; Kisselew et al., 2015; Pad\u00f3 et al., 2016).", "startOffset": 149, "endOffset": 235}, {"referenceID": 13, "context": "One way to deal with data sparsity issues when learning word vectors is to use morphological structure as a way to overcome the lack of primary data (Lazaridou et al., 2013; Luong et al., 2013; Kisselew et al., 2015; Pad\u00f3 et al., 2016).", "startOffset": 149, "endOffset": 235}, {"referenceID": 8, "context": "One way to deal with data sparsity issues when learning word vectors is to use morphological structure as a way to overcome the lack of primary data (Lazaridou et al., 2013; Luong et al., 2013; Kisselew et al., 2015; Pad\u00f3 et al., 2016).", "startOffset": 149, "endOffset": 235}, {"referenceID": 15, "context": "One way to deal with data sparsity issues when learning word vectors is to use morphological structure as a way to overcome the lack of primary data (Lazaridou et al., 2013; Luong et al., 2013; Kisselew et al., 2015; Pad\u00f3 et al., 2016).", "startOffset": 149, "endOffset": 235}, {"referenceID": 11, "context": "Another strand of research has been started by Lazaridou et al. (2017), who recently showed that by using simple sum-", "startOffset": 47, "endOffset": 71}, {"referenceID": 14, "context": "tecture of neural language models like Word2Vec (Mikolov et al., 2013) is actually suited to modelling words from a few occurrences only, providing minor adjustments are made to the model itself and its parameters.", "startOffset": 48, "endOffset": 70}, {"referenceID": 0, "context": "a length over 10 words), corresponding to targets which are frequent enough in the UkWaC corpus (Baroni et al. (2009), minimum frequency of 200).", "startOffset": 97, "endOffset": 118}, {"referenceID": 11, "context": "The Chimera dataset Our second dataset is the \u2018Chimera\u2019 dataset of (Lazaridou et al., 2017).", "startOffset": 67, "endOffset": 91}, {"referenceID": 11, "context": "We test two state-of-the art systems: a) Word2Vec (W2V) in its Gensim4 implementation, allowing for update of a prior semantic space; b) the additive model of Lazaridou et al. (2017), using a background space from W2V.", "startOffset": 159, "endOffset": 183}, {"referenceID": 2, "context": "We verify the quality of this space by calculating correlation with the similarity ratings in the MEN dataset (Bruni et al., 2014).", "startOffset": 110, "endOffset": 130}, {"referenceID": 11, "context": "Additive model Lazaridou et al. (2017) use a simple additive model, which sums the vectors of the context words of the nonce, taking as context the entire sentence where the target occurs.", "startOffset": 15, "endOffset": 39}, {"referenceID": 3, "context": "It also meets with general considerations on language acquisition, which accounts for the ability of young children to learn from limited \u2018primary linguistic data\u2019 by restricting explanatory models to those that provide such efficiency (Clark and Lappin, 2010).", "startOffset": 236, "endOffset": 260}, {"referenceID": 13, "context": "the Rare Words dataset (Luong et al., 2013), to", "startOffset": 23, "endOffset": 43}], "year": 2017, "abstractText": "Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn \u2018a good vector\u2019 for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences\u2019 worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.", "creator": "LaTeX with hyperref package"}}}