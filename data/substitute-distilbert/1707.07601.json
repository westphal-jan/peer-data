{"id": "1707.07601", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jul-2017", "title": "Image Pivoting for Learning Multilingual Multimodal Representations", "abstract": "in this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, intending the aim of advancing multilingual versions facilitating image search and image discovery. our brain learns a common representation for images and their descriptions in two different languages ( which need not be parallel ) to considering the image as constant pivot between interacting data. we introduce a complete pairwise ranking efficiency function which can handle computational symmetric and asymmetric similarity between the two modalities. we evaluate our models on image - description ranking for german and bosnian, and on semantic sequence similarity of image descriptions in english. in both cases we achieve state - of - of - art performance.", "histories": [["v1", "Mon, 24 Jul 2017 15:08:13 GMT  (385kb,D)", "http://arxiv.org/abs/1707.07601v1", "7 pages, EMNLP 2017"]], "COMMENTS": "7 pages, EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["spandana gella", "rico sennrich", "frank keller", "mirella lapata"], "accepted": true, "id": "1707.07601"}, "pdf": {"name": "1707.07601.pdf", "metadata": {"source": "CRF", "title": "Image Pivoting for Learning Multilingual Multimodal Representations", "authors": ["Spandana Gella", "Rico Sennrich", "Frank Keller", "Mirella Lapata"], "emails": ["rico.sennrich}@ed.ac.uk", "keller@inf.ed.ac.uk", "mlap@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015). There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy\net al., 2014; Kiros et al., 2015; Socher et al., 2014; Donahue et al., 2015).\nPrevious work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017).\nMost work on learning a joint space for images and their descriptions is based on Canonical Correlation Analysis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016). Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space.\nHowever, all of this work is targeted at monolingual descriptions, i.e., mapping images and descriptions in a single language onto a joint embedding space. The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation (Wu and Wang, 2007; Firat et al., 2016) and to learn multilingual multimodal representations (Rajendran et al., 2016; Calixto et al., 2017). Rajendran et al. (2016) propose a\nar X\niv :1\n70 7.\n07 60\n1v 1\n[ cs\n.C L\n] 2\n4 Ju\nl 2 01\n7\nmodel to learn common representations between M views and assume there is parallel data available between a pivot view and the remaining M\u22121 views. Their multimodal experiments are based on English as the pivot and use large parallel corpora available between languages to learn their representations.\nRelated to our work Calixto et al. (2017) proposed a model for creating multilingual multimodal embeddings. Our work is different from theirs in that we choose the image as the pivot and use a different similarity function. We also propose a single model for learning representations of images and multiple languages, whereas their model is language-specific.\nIn this paper, we learn multimodal representations in multiple languages, i.e., our model yields a joint space for images and text in multiple languages using the image as a pivot between languages. We propose a new objective function in a multitask learning setting and jointly optimize the mappings between images and text in two different languages."}, {"heading": "2 Dataset", "text": "We experiment with the Multi30k dataset, a multilingual extension of Flickr30k corpus (Young et al., 2014) consisting of English and German image descriptions (Elliott et al., 2016). The Multi30K dataset has 29k, 1k and 1k images in the train, validation and test splits respectively, and contains two types of multilingual annotations: (i) a corpus of one English description per image and its translation into German; and (ii) a corpus of five independently collected English and German descriptions per image. We use the independently collected English and German descriptions to train our models. Note that these descriptions are not\ntranslations of each other, i.e., they are not parallel, although they describe the same image."}, {"heading": "3 Problem Formulation", "text": "Given an image i and its descriptions c1 and c2 in two different languages our aim is to learn a model which maps i, c1 and c2 onto same common space RN (where N is the dimensionality of the embedding space) such that the image and its gold-standard descriptions in both languages are mapped close to each other (as shown in Figure 1). Our model consists of the embedding functions fi and fc to encode images and descriptions and a scoring function S to compute the similarity between a description\u2013image pair.\nIn the following we describe two models: (i) the PIVOT model that uses the image as pivot between the description in both the languages; (ii) the PARALLEL model that further forces the image descriptions in both languages to be closer to each other in the joint space. We build two variants of PIVOT and PARALLEL with different similarity functions S to learn the joint space."}, {"heading": "3.1 Multilingual Multimodal Representation Models", "text": "In both PIVOT and PARALLEL we use a deep convolutional neural network architecture (CNN) to represent the image i denoted by fi(i) = Wi \u00b7 CNN(i) where Wi is a learned weight matrix and CNN(i) is the image vector representation. For each language we define a recurrent neural network encoder fc(ck) = GRU(ck) with gated recurrent units (GRU) activations to encode the description ck.\nIn PIVOT, we use monolingual corpora from multiple languages of sentences aligned with images to learn the joint space. The intuition of this model is that an image is a universal representation across all languages, and if we constrain a sentence representation to be closer to image, sentences in different languages may also come closer. Accordingly we design a loss function as follows:\nlosspivot = \u2211 k [ \u2211 (ck ,i) ( \u2211 c\u2032k max{0,\u03b1\u2212S(ck, i)+S(c\u2032k, i)}\n+\u2211 i\u2032\nmax{0,\u03b1\u2212S(ck, i)+S(ck, i\u2032)} )]\n(1)\nwhere k stands for each language. This loss function encourages the similarity S(ck, i) between gold-standard description ck and image i to be greater than any other irrelevant description c\u2032k by a margin \u03b1. A similar loss function is useful for learning multimodal embeddings in a single language (Kiros et al., 2015). For each minibatch, we obtain invalid descriptions by selecting descriptions of other images except the current image of interest and vice-versa.\nIn PARALLEL, in addition to making an image similar to a description, we make multiple descriptions of the same image in different languages similar to each other, based on the assumption that these descriptions, although not parallel, share some commonalities. Accordingly we enhance the previous loss function with an additional term:\nlosspara = losspivot + \u2211 (c1,c2) ( \u2211 c\u20321 max{0,\u03b1\u2212S(c1,c2)\n+S(c\u20321,c2)}+\u2211 c\u20322\nmax{0,\u03b1\u2212S(c1,c2)+S(c1,c\u20322)} ) (2)\nNote that we are iterating over all pairs of descriptions (c1,c2), and maximizing the similarity between descriptions of the same image and at the same time minimizing the similarity between descriptions of different images.\nWe learn models using two similarity functions: symmetric and asymmetric. For the former we use cosine similarity and for the latter we use the metric of Vendrov et al. (2016) which is useful for learning embeddings that maintain an order, e.g., dog and cat are more closer to pet than animal while being distinct. Such ordering is shown to be useful in building effective multimodal space of images and texts. An analogy in our setting would be two descriptions of an image are closer to the image while at the same time preserving the identity of each (which is useful when sentences describe two different aspects of the image). The similarity metric is defined as:\nS(a,b) =\u2212||max(0,b\u2212a)||2 (3)\nwhere a and b are embeddings of image and description.\nWe call the symmetric similarity variants of our models as PIVOT-SYM and PARALLEL-SYM, and the asymmetric variants PIVOT-ASYM and PARALLEL-ASYM."}, {"heading": "4 Experiments and Results", "text": "We test our model on the tasks of imagedescription ranking and semantic textual similarity. We work with each language separately. Since we learn embeddings for images and languages in the same semantic space, our hope is that the training data for each modality or language acts complementary data for the another modality or language, and thus helps us learn better embeddings.\nExperiment Setup We sampled minibatches of size 64 images and their descriptions, and drew all negative samples from the minibatch. We trained using the Adam optimizer with learning rate 0.001, and early stopping on the validation set. Following Vendrov et al. (2016) we set the dimensionality of the embedding space and the GRU hidden layer N to 1024 for both English and German. We set the dimensionality of the learned word embeddings to 300 for both languages, and the margin \u03b1 to 0.05 and 0.2, respectively, to learn asymmetric and symmetric similarity-based embeddings.1 We keep all hyperparameters constant across all models. We used the L2 norm to mitigate over-fitting (Kiros et al., 2015). We tokenize and truecase both English and German descriptions using the Moses Decoder scripts.2\nTo extract image features, we used a convolutional neural network model trained on 1.2M images of 1000 class ILSVRC 2012 object classification dataset, a subset of ImageNet (Russakovsky et al., 2015). Specifically, we used VGG 19-layer CNN architecture and extracted the activations of the penultimate fully connected layer to obtain features for all images in the dataset (Simonyan and Zisserman, 2015). We use average features from 10 crops of the re-scaled images.3\nBaselines As baselines we use monolingual models, i.e., models trained on each language separately. Specifically, we use Visual Semantic Embeddings (VSE) of Kiros et al. (2015) and Order Embeddings (OE) of Vendrov et al. (2016). We\n1We constrain the embeddings of descriptions and images to have non-negative entries when using asymmetric similarity by taking their absolute value.\n2https://github.com/moses-smt/mosesdecoder/ tree/master/scripts\n3We rescale images so that the smallest side is 256 pixels wide, we take 224 \u00d7 224 crops from the corners, center, and their horizontal reflections to get 10 crops for the image.\nuse a publicly available implementation to train both VSE and OE.4"}, {"heading": "4.1 Image-Description Ranking Results", "text": "To evaluate the multimodal multilingual embeddings, we report results on an image-description ranking task. Given a query in the form of a description or an image, the task its to retrieve all images or descriptions sorted based on the relevance. We use the standard ranking evaluation metrics of recall at position k (R@K, where higher is better) and median rank (Mr, where lower is better) to evaluate our models. We report results for both English and German descriptions. Note that we have one single model for both languages.\nIn Tables 1 and 2 we present the ranking results of the baseline models of Kiros et al. (2015) and Vendrov et al. (2016) and our proposed PIVOT and PARALLEL models. We do not compare our image-description ranking results with Calixto et al. (2017) since they report results on half of validation set of Multi30k whereas our results are on the publicly available test set of Multi30k. For English, PIVOT with asymmetric similarity is either competitive or better than monolingual models\n4https://github.com/ivendrov/order-embedding\nand symmetric similarity, especially in the R@10 category it obtains state-of-the-art. For German, both PIVOT and PARALLEL with the asymmetric scoring function outperform monolingual models and symmetric similarity. We also observe that the German ranking experiments benefit the most from the multilingual signal. A reason for this could be that the German description corpus has many singleton words (more than 50% of the vocabulary) and English description mapping might have helped in learning better semantic embeddings. These results suggest that the multilingual signal could be used to learn better multimodal embeddings, irrespective of the language. Our results also show that the asymmetric scoring function can help learn better embeddings. In Table 3 we present a few examples where PIVOT-ASYM and PARALLEL-ASYM models performed better on both the languages compared to baseline order embedding model even using descriptions of very different lengths as queries."}, {"heading": "4.2 Semantic Textual Similarity Results", "text": "In the semantic textual similarity task (STS), we use the textual embeddings from our model to compute the similarity between a pair of sen-\ntences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010).\nIn Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual semantic embeddings (Kiros et al., 2015), and order embeddings (Vendrov et al., 2016). The shared task baseline is computed based on word overlap and is high for both the 2014 and the 2015 dataset, indicating that there is substantial lexical overlap between the STS image description datasets. Our models outperform both the baseline system and the best system submitted to the shared task. For the 2012 video paraphrase corpus, our multilingual methods performed better than the monolingual methods showing that similarity across paraphrases can be learned using multilingual signals. Similarly, Wieting et al. (2017) have reported to learn better paraphrastic sentence embeddings with multilingual signals. Overall, we observe that models learned using the asymmetric scoring function outperform the state-of-theart on these datasets, suggesting that multilingual\nsharing is beneficial. Although the task has nothing to do German, because our models can make use of datasets from different languages, we were able to train on significantly larger training dataset of approximately 145k descriptions. Calixto et al. (2017) also train on a larger dataset like ours, but could not exploit this to their advantage. In Table 5 we present the example sentences with the highest and lowest difference between gold-standard and predicted semantic textual similarity scores using our best performing PARALLEL-ASYM model."}, {"heading": "5 Conclusions", "text": "We proposed a new model that jointly learns multilingual multimodal representations using the image as a pivot between languages. We introduced new objective functions that can exploit similarities between images and descriptions across languages. We obtained state-of-the-art results on two tasks: image-description ranking and semantic textual similarity. Our results suggest that exploiting multilingual and multimodal resources can help in learning better semantic representations."}, {"heading": "Acknowledgments", "text": "This work greatly benefited from discussions with Siva Reddy and Desmond Elliot. The authors would like to thank the anonymous reviewers for their helpful comments. The authors gratefully acknowledge the support of the European Research Council (Lapata: award number 681760)."}], "references": [{"title": "Semeval-2014 task 10: Multilingual semantic textual similarity", "author": ["Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "Proceedings of the", "citeRegEx": "Agirre et al\\.,? 2014", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "Semeval-2015 task 2: Semantic textual similarity, english, spanish", "author": ["Eneko Agirre", "Carmen Baneab", "Claire Cardiec", "Daniel Cerd", "Mona Diabe", "Aitor Gonzalez-Agirrea", "Weiwei Guof", "Inigo Lopez-Gazpioa", "Montse Maritxalara", "Rada Mihalceab"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2015}, {"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["Eneko Agirre", "Mona Diab", "Daniel Cer", "Aitor Gonzalez-Agirre."], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main", "citeRegEx": "Agirre et al\\.,? 2012", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "Deep canonical correlation analysis", "author": ["Galen Andrew", "Raman Arora", "Jeff A. Bilmes", "Karen Livescu."], "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages 1247\u20131255.", "citeRegEx": "Andrew et al\\.,? 2013", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Automatic description generation from images: A survey", "author": ["Raffaella Bernardi", "Ruket Cakici", "Desmond Elliott", "Aykut Erdem", "Erkut Erdem", "Nazli Ikizler-Cinbis", "Frank Keller", "Adrian Muscat", "Barbara Plank"], "venue": null, "citeRegEx": "Bernardi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bernardi et al\\.", "year": 2016}, {"title": "Multilingual multi-modal embeddings for natural language processing", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell."], "venue": "arXiv preprint arXiv:1702.01101.", "citeRegEx": "Calixto et al\\.,? 2017", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "Correlational neural networks", "author": ["Sarath Chandar", "Mitesh M. Khapra", "Hugo Larochelle", "Balaraman Ravindran."], "venue": "Neural Computation, 28(2):257\u2013285.", "citeRegEx": "Chandar et al\\.,? 2016", "shortCiteRegEx": "Chandar et al\\.", "year": 2016}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L Chen", "William B Dolan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 190\u2013200.", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "Proceedings of the IEEE", "citeRegEx": "Donahue et al\\.,? 2015", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Multi30k: Multilingual englishgerman image descriptions", "author": ["Desmond Elliott", "Stella Frank", "Khalil Sima\u2019an", "Lucia Specia"], "venue": "In Proceedings of the 5th Workshop on Vision and Language,", "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh K Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C Platt"], "venue": "In Proceedings of the IEEE Conference on Computer", "citeRegEx": "Fang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman-Vural", "Kyunghyun Cho."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Imagemediated learning for zero-shot cross-lingual document retrieval", "author": ["Ruka Funaki", "Hideki Nakayama."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Lisbon, Portugal.", "citeRegEx": "Funaki and Nakayama.,? 2015", "shortCiteRegEx": "Funaki and Nakayama.", "year": 2015}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Yunchao Gong", "Liwei Wang", "Micah Hodosh", "Julia Hockenmaier", "Svetlana Lazebnik."], "venue": "European Conference on Computer Vision, pages 529\u2013545.", "citeRegEx": "Gong et al\\.,? 2014", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Multimodal pivots for image caption translation", "author": ["Julian Hitschler", "Shigehiko Schamoni", "Stefan Riezler."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-", "citeRegEx": "Hitschler et al\\.,? 2016", "shortCiteRegEx": "Hitschler et al\\.", "year": 2016}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "Journal of Artificial Intelligence Research, 47:853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Fei Fei F Li."], "venue": "Advances in neural information processing systems, pages 1889\u20131897.", "citeRegEx": "Karpathy et al\\.,? 2014", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel."], "venue": "Transactions of the Association for Computational Linguistics.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Adding chinese captions to images", "author": ["Xirong Li", "Weiyu Lan", "Jianfeng Dong", "Hailong Liu."], "venue": "Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval, pages 271\u2013275. ACM.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Microsoft COCO: common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick."], "venue": "Computer Vision ECCV 2014 - 13th European Conference, Zurich,", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Leveraging visual question answering for image-caption ranking", "author": ["Xiao Lin", "Devi Parikh."], "venue": "European Conference on Computer Vision, pages 261\u2013277. Springer.", "citeRegEx": "Lin and Parikh.,? 2016", "shortCiteRegEx": "Lin and Parikh.", "year": 2016}, {"title": "Multimodal convolutional neural networks for matching image and sentence", "author": ["Lin Ma", "Zhengdong Lu", "Lifeng Shang", "Hang Li."], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2623\u20132631.", "citeRegEx": "Ma et al\\.,? 2015", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Mao et al\\.,? 2015", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Cross-lingual image caption generation", "author": ["Takashi Miyazaki", "Nobuyuki Shimizu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1780\u20131790. Association for Compu-", "citeRegEx": "Miyazaki and Shimizu.,? 2016", "shortCiteRegEx": "Miyazaki and Shimizu.", "year": 2016}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models", "author": ["Bryan A Plummer", "Liwei Wang", "Chris M Cervantes", "Juan C Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik."], "venue": "Proceedings of the IEEE In-", "citeRegEx": "Plummer et al\\.,? 2015", "shortCiteRegEx": "Plummer et al\\.", "year": 2015}, {"title": "Bridge correlational neural networks for multilingual multimodal representation learning", "author": ["Janarthanan Rajendran", "Mitesh M. Khapra", "Sarath Chandar", "Balaraman Ravindran."], "venue": "NAACL HLT 2016, The 2016 Conference of the North American", "citeRegEx": "Rajendran et al\\.,? 2016", "shortCiteRegEx": "Rajendran et al\\.", "year": 2016}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechan-", "citeRegEx": "Rashtchian et al\\.,? 2010", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Alexander C. Berg", "Fei-Fei Li."], "venue": "International Journal of Computer Vision, 115(3):211\u2013252.", "citeRegEx": "Berg and Li.,? 2015", "shortCiteRegEx": "Berg and Li.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Simonyan and Zisserman.,? 2015", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Transactions of Association of Computational Linguistics, 2:207\u2013", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "A shared task on multimodal machine translation and crosslingual image description", "author": ["Lucia Specia", "Stella Frank", "Khalil Simaan", "Desmond Elliott."], "venue": "Proceedings of the First Conference on Machine Translation, Berlin, Germany. Associa-", "citeRegEx": "Specia et al\\.,? 2016", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Vendrov et al\\.,? 2016", "shortCiteRegEx": "Vendrov et al\\.", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 3156\u2013", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Learning paraphrastic sentence embeddings from back-translated bitext", "author": ["John Wieting", "Jonathan Mallinson", "Kevin Gimpel."], "venue": "arXiv preprint arXiv:1706.01847.", "citeRegEx": "Wieting et al\\.,? 2017", "shortCiteRegEx": "Wieting et al\\.", "year": 2017}, {"title": "Pivot language approach for phrase-based statistical machine translation", "author": ["Hua Wu", "Haifeng Wang."], "venue": "Machine Translation, 21(3):165\u2013181.", "citeRegEx": "Wu and Wang.,? 2007", "shortCiteRegEx": "Wu and Wang.", "year": 2007}, {"title": "Deep correlation for matching images and text", "author": ["Fei Yan", "Krystian Mikolajczyk."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3441\u20133450.", "citeRegEx": "Yan and Mikolajczyk.,? 2015", "shortCiteRegEx": "Yan and Mikolajczyk.", "year": 2015}, {"title": "STAIR captions: Constructing a large-scale japanese image caption dataset", "author": ["Yuya Yoshikawa", "Yutaro Shigeto", "Akikazu Takeuchi."], "venue": "CoRR, abs/1705.00823.", "citeRegEx": "Yoshikawa et al\\.,? 2017", "shortCiteRegEx": "Yoshikawa et al\\.", "year": 2017}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguis-", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem", "startOffset": 78, "endOffset": 137}, {"referenceID": 38, "context": "An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem", "startOffset": 78, "endOffset": 137}, {"referenceID": 20, "context": "An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem", "startOffset": 78, "endOffset": 137}, {"referenceID": 4, "context": "(Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015).", "startOffset": 0, "endOffset": 82}, {"referenceID": 23, "context": "(Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015).", "startOffset": 0, "endOffset": 82}, {"referenceID": 33, "context": "(Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015).", "startOffset": 0, "endOffset": 82}, {"referenceID": 10, "context": "(Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015).", "startOffset": 0, "endOffset": 82}, {"referenceID": 16, "context": "There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy et al., 2014; Kiros et al., 2015; Socher et al., 2014; Donahue et al., 2015).", "startOffset": 163, "endOffset": 290}, {"referenceID": 12, "context": "There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy et al., 2014; Kiros et al., 2015; Socher et al., 2014; Donahue et al., 2015).", "startOffset": 163, "endOffset": 290}, {"referenceID": 17, "context": "There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy et al., 2014; Kiros et al., 2015; Socher et al., 2014; Donahue et al., 2015).", "startOffset": 163, "endOffset": 290}, {"referenceID": 18, "context": "There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy et al., 2014; Kiros et al., 2015; Socher et al., 2014; Donahue et al., 2015).", "startOffset": 163, "endOffset": 290}, {"referenceID": 30, "context": "There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy et al., 2014; Kiros et al., 2015; Socher et al., 2014; Donahue et al., 2015).", "startOffset": 163, "endOffset": 290}, {"referenceID": 8, "context": "There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy et al., 2014; Kiros et al., 2015; Socher et al., 2014; Donahue et al., 2015).", "startOffset": 163, "endOffset": 290}, {"referenceID": 13, "context": "Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017).", "startOffset": 94, "endOffset": 281}, {"referenceID": 9, "context": "Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017).", "startOffset": 94, "endOffset": 281}, {"referenceID": 26, "context": "Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017).", "startOffset": 94, "endOffset": 281}, {"referenceID": 24, "context": "Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017).", "startOffset": 94, "endOffset": 281}, {"referenceID": 31, "context": "Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017).", "startOffset": 94, "endOffset": 281}, {"referenceID": 19, "context": "Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017).", "startOffset": 94, "endOffset": 281}, {"referenceID": 15, "context": "Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017).", "startOffset": 94, "endOffset": 281}, {"referenceID": 37, "context": "Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017).", "startOffset": 94, "endOffset": 281}, {"referenceID": 16, "context": "relation Analysis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016).", "startOffset": 101, "endOffset": 211}, {"referenceID": 3, "context": "relation Analysis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016).", "startOffset": 101, "endOffset": 211}, {"referenceID": 36, "context": "relation Analysis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016).", "startOffset": 101, "endOffset": 211}, {"referenceID": 14, "context": "relation Analysis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016).", "startOffset": 101, "endOffset": 211}, {"referenceID": 6, "context": "relation Analysis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016).", "startOffset": 101, "endOffset": 211}, {"referenceID": 18, "context": "Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al.", "startOffset": 160, "endOffset": 240}, {"referenceID": 30, "context": "Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al.", "startOffset": 160, "endOffset": 240}, {"referenceID": 22, "context": "Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al.", "startOffset": 160, "endOffset": 240}, {"referenceID": 32, "context": "Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al.", "startOffset": 160, "endOffset": 240}, {"referenceID": 17, "context": ", 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space.", "startOffset": 79, "endOffset": 124}, {"referenceID": 25, "context": ", 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space.", "startOffset": 79, "endOffset": 124}, {"referenceID": 3, "context": ", 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016). Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space.", "startOffset": 8, "endOffset": 506}, {"referenceID": 35, "context": "The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation (Wu and Wang, 2007; Firat et al., 2016) and to learn multilingual multimodal representations (Rajendran et al.", "startOffset": 107, "endOffset": 146}, {"referenceID": 11, "context": "The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation (Wu and Wang, 2007; Firat et al., 2016) and to learn multilingual multimodal representations (Rajendran et al.", "startOffset": 107, "endOffset": 146}, {"referenceID": 26, "context": ", 2016) and to learn multilingual multimodal representations (Rajendran et al., 2016; Calixto et al., 2017).", "startOffset": 61, "endOffset": 107}, {"referenceID": 5, "context": ", 2016) and to learn multilingual multimodal representations (Rajendran et al., 2016; Calixto et al., 2017).", "startOffset": 61, "endOffset": 107}, {"referenceID": 5, "context": ", 2016; Calixto et al., 2017). Rajendran et al. (2016) propose a ar X iv :1 70 7.", "startOffset": 8, "endOffset": 55}, {"referenceID": 5, "context": "Related to our work Calixto et al. (2017) pro-", "startOffset": 20, "endOffset": 42}, {"referenceID": 38, "context": "We experiment with the Multi30k dataset, a multilingual extension of Flickr30k corpus (Young et al., 2014) consisting of English and German image descriptions (Elliott et al.", "startOffset": 86, "endOffset": 106}, {"referenceID": 9, "context": ", 2014) consisting of English and German image descriptions (Elliott et al., 2016).", "startOffset": 60, "endOffset": 82}, {"referenceID": 18, "context": "A similar loss function is useful for learning multimodal embeddings in a single language (Kiros et al., 2015).", "startOffset": 90, "endOffset": 110}, {"referenceID": 32, "context": "For the former we use cosine similarity and for the latter we use the metric of Vendrov et al. (2016) which is useful for learning embeddings that maintain an order, e.", "startOffset": 80, "endOffset": 102}, {"referenceID": 32, "context": "Following Vendrov et al. (2016) we set the dimensionality of the embedding space and the GRU hidden layer N to 1024 for both English and German.", "startOffset": 10, "endOffset": 32}, {"referenceID": 18, "context": "We used the L2 norm to mitigate over-fitting (Kiros et al., 2015).", "startOffset": 45, "endOffset": 65}, {"referenceID": 18, "context": "Specifically, we use Visual Semantic Embeddings (VSE) of Kiros et al. (2015) and Order Embeddings (OE) of Vendrov et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 18, "context": "Specifically, we use Visual Semantic Embeddings (VSE) of Kiros et al. (2015) and Order Embeddings (OE) of Vendrov et al. (2016). We", "startOffset": 57, "endOffset": 128}, {"referenceID": 18, "context": "System Text to Image Image to Text R@1 R@5 R@10 Mr R@1 R@5 R@10 Mr VSE (Kiros et al., 2015) 23.", "startOffset": 71, "endOffset": 91}, {"referenceID": 32, "context": "7 3 OE (Vendrov et al., 2016) 25.", "startOffset": 7, "endOffset": 29}, {"referenceID": 18, "context": "R@1 R@5 R@10 Mr R@1 R@5 R@10 Mr VSE (Kiros et al., 2015) 20.", "startOffset": 36, "endOffset": 56}, {"referenceID": 32, "context": "8 4 OE (Vendrov et al., 2016) 21.", "startOffset": 7, "endOffset": 29}, {"referenceID": 17, "context": "In Tables 1 and 2 we present the ranking results of the baseline models of Kiros et al. (2015) and Vendrov et al.", "startOffset": 75, "endOffset": 95}, {"referenceID": 17, "context": "In Tables 1 and 2 we present the ranking results of the baseline models of Kiros et al. (2015) and Vendrov et al. (2016) and our proposed PIVOT and PARALLEL models.", "startOffset": 75, "endOffset": 121}, {"referenceID": 5, "context": "We do not compare our image-description ranking results with Calixto et al. (2017) since they report results on half of validation set of Multi30k whereas our results are on the publicly available test set of Multi30k.", "startOffset": 61, "endOffset": 83}, {"referenceID": 34, "context": "4 GRAN (Wieting et al., 2017) \u2212 83.", "startOffset": 7, "endOffset": 29}, {"referenceID": 5, "context": "0 MLMME (Calixto et al., 2017) VGG19 \u2212 72.", "startOffset": 8, "endOffset": 30}, {"referenceID": 18, "context": "7 VSE (Kiros et al., 2015) VGG19 80.", "startOffset": 6, "endOffset": 26}, {"referenceID": 32, "context": "OE (Vendrov et al., 2016) VGG19 82.", "startOffset": 3, "endOffset": 25}, {"referenceID": 7, "context": "The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al.", "startOffset": 86, "endOffset": 108}, {"referenceID": 27, "context": "The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010).", "startOffset": 185, "endOffset": 210}, {"referenceID": 5, "context": "We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al.", "startOffset": 85, "endOffset": 107}, {"referenceID": 34, "context": ", 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual semantic embeddings (Kiros et al.", "startOffset": 42, "endOffset": 64}, {"referenceID": 18, "context": ", 2017), visual semantic embeddings (Kiros et al., 2015), and order embeddings (Vendrov et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 32, "context": ", 2015), and order embeddings (Vendrov et al., 2016).", "startOffset": 30, "endOffset": 52}, {"referenceID": 34, "context": "Similarly, Wieting et al. (2017) have reported to learn better paraphrastic sentence embeddings with multilingual signals.", "startOffset": 11, "endOffset": 33}, {"referenceID": 5, "context": "Calixto et al. (2017) also train on a larger dataset like ours, but could not exploit this to their advantage.", "startOffset": 0, "endOffset": 22}], "year": 2017, "abstractText": "In this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, with the aim of advancing multilingual versions of image search and image understanding. Our model learns a common representation for images and their descriptions in two different languages (which need not be parallel) by considering the image as a pivot between two languages. We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance.", "creator": "LaTeX with hyperref package"}}}