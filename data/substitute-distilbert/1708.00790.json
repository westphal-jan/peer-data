{"id": "1708.00790", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Aug-2017", "title": "Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition", "abstract": "unsupervised dependency parsing aims to learn a dependency parser from learning sentences. existing work focuses on either learning generative models using the expectation - maximization algorithm and its variants, or learning discriminative models using the gnu clustering algorithm. in this paper, we propose a new learning strategy that learns a linear model and a discriminative model jointly based on the dual decomposition method. neither method is simple and general, helping effective statistics capture the advantages of both models and improve their learning results. we repeated our concept on the classical treebank and added a coat - of - the - art standardization on thirty methods.", "histories": [["v1", "Wed, 2 Aug 2017 15:10:28 GMT  (44kb,D)", "http://arxiv.org/abs/1708.00790v1", "In EMNLP 2017"], ["v2", "Sun, 24 Sep 2017 14:30:34 GMT  (44kb,D)", "http://arxiv.org/abs/1708.00790v2", "In EMNLP 2017. A typo fixed in Algo 2"]], "COMMENTS": "In EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yong jiang", "wenjuan han", "kewei tu"], "accepted": true, "id": "1708.00790"}, "pdf": {"name": "1708.00790.pdf", "metadata": {"source": "CRF", "title": "Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition\u2217", "authors": ["Yong Jiang", "Wenjuan Han", "Kewei Tu"], "emails": ["jiangyong@shanghaitech.edu.cn", "hanwj@shanghaitech.edu.cn", "tukw@shanghaitech.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Dependency parsing is an important task in natural language processing. It identifies dependencies between words in a sentence, which have been shown to benefit other tasks such as semantic role labeling (Lei et al., 2015) and sentence classification (Ma et al., 2015). Supervised learning of a dependency parser requires annotation of a training corpus by linguistic experts, which can be time and resource consuming. Unsupervised dependency parsing eliminates the need for dependency annotation by directly learning from unparsed text.\nPrevious work on unsupervised dependency parsing mainly focuses on learning generative models, such as the dependency model with valence (DMV) (Klein and Manning, 2004) and combinatory categorial grammars (CCG) (Bisk and Hockenmaier, 2012). Generative models have\n\u2217This work was supported by the National Natural Science Foundation of China (61503248).\nmany advantages. For example, the learning objective function can be defined as the marginal likelihood of the training data, which is typically easy to compute in a generative model. In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; BergKirkpatrick et al., 2010; Jiang et al., 2016), and limiting center embedding (Noji et al., 2016), can be incorporated into generative models to achieve better parsing accuracy. However, due to the strong independence assumption in most generative models, it is difficult for these models to utilize context information that has been shown to benefit supervised parsing.\nRecently, a feature-rich discriminative model for unsupervised parsing is proposed that captures the global context information of sentences (Grave and Elhadad, 2015). Inspired by discriminative clustering, learning of the model is formulated as convex optimization of both the model parameters and the parses of training sentences. By utilizing language-independent rules between pairs of POS tags to guide learning, the model achieves state-ofthe-art performance on the UD treebank dataset.\nIn this paper we propose to jointly train two state-of-the-art models of unsupervised dependency parsing: a generative model called LCDMV (Noji et al., 2016) and a discriminative model called Convex-MST (Grave and Elhadad, 2015). We employ a learning algorithm based on the dual decomposition (Dantzig and Wolfe, 1960) inference algorithm, which encourages the two models to influence each other during training.\nWe evaluated our method on thirty languages and found that the jointly trained models surpass their separately trained counterparts in parsing accuracy. Further analysis shows that the two models\nar X\niv :1\n70 8.\n00 79\n0v 1\n[ cs\n.C L\n] 2\nA ug\n2 01\n7\npositively influence each other during joint training by implicitly sharing the inductive bias."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 DMV", "text": "The dependency model with valence (DMV) (Klein and Manning, 2004) is the first generative model that outperforms the left-branching baseline in unsupervised dependency parsing. In DMV, a sentence is generated by recursively applying three types of grammar rules to construct a parse tree from the top down. The probability of the generated sentence and parse tree is the probability product of all the rules used in the generation process. To learn the parameters (rule probabilities) of DMV, the expectation maximization algorithm is often used. Noji et al. (2016) exploited two universal syntactic biases in learning DMV: restricting the center-embedding depth and encouraging short dependencies. They achieved a comparable performance with state-of-the-art approaches."}, {"heading": "2.2 Convex-MST", "text": "Convex-MST (Grave and Elhadad, 2015) is a discriminative model for unsupervised dependency parsing based on the first-order maximum spanning tree dependency parser (McDonald et al., 2005). Given a sentence, whether each possible dependency exists or not is predicted based on a set of handcrafted features and a valid parse tree closest to the prediction is identified by the minimum spanning tree algorithm.\nFor each sentence x, a first-order dependency graph is built over the words of the sentence. The weight of each edge is calculated by wT f(x, i, j), where w is the parameters and f(x, i, j) is the handcrafted feature vector of the dependency from the i-th word to the j-th word in sentence x. For sentence x of length n, we can represent it as matrix X where each raw is a feature vector. The parse tree y is a spanning tree of the graph and can be represented as a binary vector with length n\u00d7nwhere each element is 1 if the corresponding arc is in the tree and 0 otherwise.\nLearning is based on discriminative clustering with the following objective function:\n1\nN N\u2211 \u03b1=1 ( 1 2n\u03b1 ||y\u03b1 \u2212X\u03b1w||22 \u2212 \u00b5vTy\u03b1 ) + \u03bb 2 ||w||22\nwhere X\u03b1 is a matrix where each row is a feature representation f(x\u03b1, i, j) of an edge in the depen-\ndency graph of sentence x\u03b1, v represents whether each dependency arc in y\u03b1 satisfies a set of prespecified linguistic rules, and \u03bb and \u00b5 are hyperparameters. The Frank-Wolfe algorithm is employed to optimize the objective function."}, {"heading": "2.3 Dual Decomposition", "text": "Dual decomposition (Dantzig and Wolfe, 1960), a special case of Lagrangian relaxation, is an optimization method that decomposes a hard problem into several small sub-problems. It has been widely used in machine learning (Komodakis et al., 2007) and natural language processing (Koo et al., 2010; Rush and Collins, 2012).\nKomodakis et al. (2007) proposed using dual decomposition to do MAP inference for Markov random fields. Koo et al. (2010) proposed a new dependency parser based on dual decomposition by combining a graph based dependency model and a non-projective head automata. In the work of Rush et al. (2010), they showed that dual decomposition can effectively integrate two lexicalized parsing models or two correlated tasks."}, {"heading": "2.4 Agreement based Learning", "text": "Liang et al. (2008) proposed agreement based learning that trains several tractable generative models jointly and encourages them to agree on certain latent variables. To effectively train the system, a product EM algorithm was used. They showed that the joint model can perform better than each independent model on the accuracy or convergence speed. They also showed that the objective function of the work of Klein and Manning (2004) is a special case of the product EM algorithm for grammar induction. Our approach has a similar motivation to agreement based learning but has two important advantages. First, while their approach only combines generative models, our approach can make use of both generative and discriminative models. Second, while their approach requires the sub-models to share the same dynamic programming structure when performing decoding, our approach does not have such restriction."}, {"heading": "3 Joint Training", "text": "We minimize the following objective function that combines two different models of unsupervised\ndependency parsing:\nJ(MF,MG)\n= N\u2211 \u03b1=1 min y\u03b1\u2208Y\u03b1 (F (x\u03b1,y\u03b1;MF) +G(x\u03b1,y\u03b1;MG))\nwhere N is the size of training data, MF and MG are the parameters of the first and second model respectively, F and G are their respective learning objectives, and Y\u03b1 is the set of valid dependency parses of sentence x\u03b1. While in principle this objective can be used to combine many different types of models, here we consider two state-ofthe-art models of unsupervised dependency parsing, a generative model LC-DMV (Noji et al., 2016) and a discriminative model Convex-MST (Grave and Elhadad, 2015). We denote the parameters of LC-DMV by \u0398 and the parameters of Convex-MST by w. Their respective objective functions are,"}, {"heading": "F (x\u03b1,y\u03b1; \u0398) = \u2212 log (P\u0398(x\u03b1,y\u03b1)f(x\u03b1,y\u03b1))", "text": "G(x\u03b1,y\u03b1;w)\n= 1\n2n\u03b1 ||y\u03b1 \u2212X\u03b1w||22 +\n\u03bb\n2N ||w||22 \u2212 \u00b5vTy\nwhere P\u0398(x\u03b1,y\u03b1) is the joint probability of sentence x\u03b1 and parse y\u03b1, f is a constraint factor, and the notations in the second objective function are explained in section 2.2."}, {"heading": "3.1 Learning", "text": "We use coordinate descent to optimize the parameters of the two models. In each iteration, we first fix the parameters and find the best dependency parses of the training sentences (see section 3.2); we then fix the parses and optimize the parameters. The detailed algorithm is shown in Algorithm 1.\nPretraining of the two models is done by running their original learning algorithms separately. When the parses of the training sentences are fixed, it is easy to show that the parameters of the two models can be optimized separately. Updating the parameters \u0398 of LC-DMV can be done by simply counting the number of times each rule is used in the parse trees and then normalizing the counts to get the maximum-likelihood probabilities. The parameters w of Convex-MST can be updated by stochastic gradient descent. After updating \u0398 and w at each iteration, we additionally train each model separately for three iterations, which we find further improves learning.\nAlgorithm 1 Parameter Learning Input: Training sentence x1,x2, ...,xN Pre-train \u0398 and w repeat\nFix \u0398 and w and solve the decoding problem to get y\u03b1, \u03b1 = 1, 2, . . . , N\nFix the parses and update \u0398 and w until Convergence\nAlgorithm 2 Decoding via Dual Decomposition Input: Sentence x, fixed parameters w and \u0398 Initialize vector u of size n\u00d7 n to 0 repeat y\u0302 = arg miny\u2208Y F (x,y; \u0398) + u\nTy z\u0302 = arg minz\u2208Y G(x, z;w)\u2212 uT z if y\u0302 = z\u0302 then\nreturn y\u0302 else u = u\u2212 \u03c4 (y\u0302 \u2212 z\u0302)\nend if until Convergence"}, {"heading": "3.2 Joint Decoding", "text": "Given a training sample x and parameters w,\u0398, the goal of decoding is to find the best parse tree:\ny\u0302 = arg min y\u2208Y\n1\n2n ||y\u2212Xw||22\u2212\u00b5vTy\u2212logP\u0398(x,y)\nWe employ the dual decomposition algorithm to solve this problem (shown in Algorithm 2), where \u03c4 represents the step size.\nThe most important part of the algorithm is solving the two separate decoding problems:\ny\u0302 = arg min y\u2208Y \u2212 log(P\u0398(x,y)f(x,y)) + uTy\nz\u0302 = arg min z\u2208Y\n1\n2n ||z\u2212Xw||22 \u2212 \u00b5vT z\u2212 uT z\nThe first decoding problem can be solved by a modified CYK parsing algorithm that takes into account the information in vector u. The second decoding problem can be solved using the same algorithm of Grave and Elhadad (2015) (we use the projective version in our approach)."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Setup", "text": "We use UD Treebank 1.4 as our datasets. We sorted the datasets in the treebank by the number\nof training sentences of length \u2264 15 and selected the top thirty datasets, which is similar to the setup of Noji et al. (2016). For each dataset, we trained our method on the training data with length \u2264 15 and tested our method on the testing data with length \u2264 40. We tuned the hyper-parameters of our method on the dataset of the English language and reported the results on the thirty datasets without any further parameter tuning. We compared our method with four baselines. The first two baselines are Convex-MST and LC-DMV that are independently trained. To construct the third baseline, we used the independently trained ConvexMST baseline to parse all the training sentences and then used the parses to initialize the training of LC-DMV. This can be seen as a simple method to combine two different approaches. On the other hand, we did not use the LC-DMV baseline to initialize Convex-MST training because the objective function of Convex-MST is convex and therefore the initialization does not matter."}, {"heading": "4.2 Results", "text": "In Table 1, we compare our jointly trained models with the four baselines. We can see that with joint training and independent decoding, LC-DMV and Convex-MST can achieve superior overall performance than when they are separately trained with or without mutual initialization. Joint decoding with our jointly trained models performs worse than independent decoding. We made the same observation when applying joint decoding to the separately trained models (not shown in the table). We believe this is because unsupervised parsers have relatively low accuracy and forcing them to reconcile would not lead to better parses. On the other hand, joint decoding during training helps propagate useful inductive biases between models and thus leads to better trained models."}, {"heading": "4.3 Analysis of Parsing Results", "text": "We analyze the parsing results from the two models to see how they benefit each other with joint training. Note that LC-DMV limits the depth of center embedding and encourages shorter dependency length, while Convex-MST encourages dependencies satisfying pre-specified linguistic rules. Therefore, we would like to see whether the jointly-trained LC-DMV produces more dependencies satisfying the linguistic priors than its separately-trained counterpart, and whether the jointly-trained Convex-MST produces parse trees\nwith less center embedding and shorter dependencies than its separately-trained counterpart.\nFigure 1 shows the percentages of dependencies satisfying linguistic rules when using the separately and jointly trained LC-DMV to parse the test sentences in the English dataset. As we can see, with joint training, LC-DMV is indeed influenced by Convex-MST and produces more dependencies satisfying linguistic rules.\nTable 2 shows the average dependency length when using the separately and jointly trained Convex-MST to parse the English test dataset. The dependency length can be seen to decrease with joint training, showing the influence from LC-DMV. As to center embedding depth, we find that separately trained Convext-MST already produces very few center embeddings of depth 2 or\nmore, so the influence from the center embedding constraint of LC-DMV during joint training is not obvious. We note that the influence on Convex-MST from LC-DMV during joint training is relatively small, which may contribute to the much smaller accuracy improvement (1.5%) of Convex-MST with joint training in comparison with the 9.3% improvement of LC-DMV. We conducted an additional experiment that scaled down the Convex-MST objective in joint training in order to increase the influence of LC-DMV. The results show that LC-DMV indeed influences Convex-MST to a greater degree, but the parsing accuracies of the two models decrease."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a new learning strategy for unsupervised dependency parsing that learns a generative model and a discriminative model jointly based on dual decomposition. We show that with joint training, two state-of-the-art models can positively influence each other and achieve better performance than their separately trained counterparts."}], "references": [{"title": "Painless unsupervised learning with features", "author": ["Taylor Berg-Kirkpatrick", "Alexandre Bouchard-C\u00f4t\u00e9", "John DeNero", "Dan Klein."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2010", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Simple robust grammar induction with combinatory categorial grammars", "author": ["Yonatan Bisk", "Julia Hockenmaier"], "venue": null, "citeRegEx": "Bisk and Hockenmaier.,? \\Q2012\\E", "shortCiteRegEx": "Bisk and Hockenmaier.", "year": 2012}, {"title": "Logistic normal priors for unsupervised probabilistic grammar induction", "author": ["Shay B Cohen", "Kevin Gimpel", "Noah A Smith."], "venue": "Advances in Neural Information Processing Systems, pages 321\u2013328.", "citeRegEx": "Cohen et al\\.,? 2008", "shortCiteRegEx": "Cohen et al\\.", "year": 2008}, {"title": "Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction", "author": ["Shay B Cohen", "Noah A Smith."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chap-", "citeRegEx": "Cohen and Smith.,? 2009", "shortCiteRegEx": "Cohen and Smith.", "year": 2009}, {"title": "Decomposition principle for linear programs", "author": ["George B Dantzig", "Philip Wolfe."], "venue": "Operations research, 8(1):101\u2013111.", "citeRegEx": "Dantzig and Wolfe.,? 1960", "shortCiteRegEx": "Dantzig and Wolfe.", "year": 1960}, {"title": "A convex and feature-rich discriminative approach to dependency grammar induction", "author": ["Edouard Grave", "No\u00e9mie Elhadad."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Grave and Elhadad.,? 2015", "shortCiteRegEx": "Grave and Elhadad.", "year": 2015}, {"title": "Unsupervised neural dependency parsing", "author": ["Yong Jiang", "Wenjuan Han", "Kewei Tu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 763\u2013771, Austin, Texas. Association for Computational Lin-", "citeRegEx": "Jiang et al\\.,? 2016", "shortCiteRegEx": "Jiang et al\\.", "year": 2016}, {"title": "Corpusbased induction of syntactic structure: Models of dependency and constituency", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 478. Association for Com-", "citeRegEx": "Klein and Manning.,? 2004", "shortCiteRegEx": "Klein and Manning.", "year": 2004}, {"title": "Mrf optimization via dual decomposition: Message-passing revisited", "author": ["Nikos Komodakis", "Nikos Paragios", "Georgios Tziritas."], "venue": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1\u20138. IEEE.", "citeRegEx": "Komodakis et al\\.,? 2007", "shortCiteRegEx": "Komodakis et al\\.", "year": 2007}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["Terry Koo", "Alexander M Rush", "Michael Collins", "Tommi Jaakkola", "David Sontag."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Koo et al\\.,? 2010", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "High-order low-rank tensors for semantic role labeling", "author": ["Tao Lei", "Yuan Zhang", "Llu\u0131\u0301s M\u00e0rquez", "Alessandro Moschitti", "Regina Barzilay"], "venue": "In Proceedings of the 2015 Conference of the North", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Agreement-based learning", "author": ["Percy S Liang", "Dan Klein", "Michael I. Jordan."], "venue": "J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 913\u2013920. Curran Associates, Inc.", "citeRegEx": "Liang et al\\.,? 2008", "shortCiteRegEx": "Liang et al\\.", "year": 2008}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou."], "venue": "arXiv preprint arXiv:1507.01839.", "citeRegEx": "Ma et al\\.,? 2015", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics, pages 91\u201398. Association for Computa-", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Using left-corner parsing to encode universal structural constraints in grammar induction", "author": ["Hiroshi Noji", "Yusuke Miyao", "Mark Johnson."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 33\u201343,", "citeRegEx": "Noji et al\\.,? 2016", "shortCiteRegEx": "Noji et al\\.", "year": 2016}, {"title": "A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing", "author": ["Alexander M. Rush", "Michael Collins."], "venue": "J. Artif. Int. Res., 45(1):305\u2013362.", "citeRegEx": "Rush and Collins.,? 2012", "shortCiteRegEx": "Rush and Collins.", "year": 2012}, {"title": "On dual decomposition and linear programming relaxations for natural language processing", "author": ["Alexander M Rush", "David Sontag", "Michael Collins", "Tommi Jaakkola."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language", "citeRegEx": "Rush et al\\.,? 2010", "shortCiteRegEx": "Rush et al\\.", "year": 2010}, {"title": "Annealing structural bias in multilingual weighted grammar induction", "author": ["Noah A Smith", "Jason Eisner."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Compu-", "citeRegEx": "Smith and Eisner.,? 2006", "shortCiteRegEx": "Smith and Eisner.", "year": 2006}], "referenceMentions": [{"referenceID": 10, "context": "It identifies dependencies between words in a sentence, which have been shown to benefit other tasks such as semantic role labeling (Lei et al., 2015) and sentence classification (Ma et al.", "startOffset": 132, "endOffset": 150}, {"referenceID": 12, "context": ", 2015) and sentence classification (Ma et al., 2015).", "startOffset": 36, "endOffset": 53}, {"referenceID": 7, "context": "Previous work on unsupervised dependency parsing mainly focuses on learning generative models, such as the dependency model with valence (DMV) (Klein and Manning, 2004) and combinatory categorial grammars (CCG) (Bisk and Hockenmaier, 2012).", "startOffset": 143, "endOffset": 168}, {"referenceID": 1, "context": "Previous work on unsupervised dependency parsing mainly focuses on learning generative models, such as the dependency model with valence (DMV) (Klein and Manning, 2004) and combinatory categorial grammars (CCG) (Bisk and Hockenmaier, 2012).", "startOffset": 211, "endOffset": 239}, {"referenceID": 17, "context": "In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al.", "startOffset": 88, "endOffset": 112}, {"referenceID": 2, "context": "In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; BergKirkpatrick et al., 2010; Jiang et al., 2016), and limiting center embedding (Noji et al.", "startOffset": 156, "endOffset": 249}, {"referenceID": 3, "context": "In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; BergKirkpatrick et al., 2010; Jiang et al., 2016), and limiting center embedding (Noji et al.", "startOffset": 156, "endOffset": 249}, {"referenceID": 6, "context": "In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; BergKirkpatrick et al., 2010; Jiang et al., 2016), and limiting center embedding (Noji et al.", "startOffset": 156, "endOffset": 249}, {"referenceID": 14, "context": ", 2016), and limiting center embedding (Noji et al., 2016), can be incorporated into generative models to achieve better parsing accuracy.", "startOffset": 39, "endOffset": 58}, {"referenceID": 5, "context": "Recently, a feature-rich discriminative model for unsupervised parsing is proposed that captures the global context information of sentences (Grave and Elhadad, 2015).", "startOffset": 141, "endOffset": 166}, {"referenceID": 14, "context": "In this paper we propose to jointly train two state-of-the-art models of unsupervised dependency parsing: a generative model called LCDMV (Noji et al., 2016) and a discriminative model called Convex-MST (Grave and Elhadad, 2015).", "startOffset": 138, "endOffset": 157}, {"referenceID": 5, "context": ", 2016) and a discriminative model called Convex-MST (Grave and Elhadad, 2015).", "startOffset": 53, "endOffset": 78}, {"referenceID": 4, "context": "We employ a learning algorithm based on the dual decomposition (Dantzig and Wolfe, 1960) inference algorithm, which encourages the two models to influence each other during training.", "startOffset": 63, "endOffset": 88}, {"referenceID": 7, "context": "The dependency model with valence (DMV) (Klein and Manning, 2004) is the first generative model that outperforms the left-branching baseline in unsupervised dependency parsing.", "startOffset": 40, "endOffset": 65}, {"referenceID": 7, "context": "The dependency model with valence (DMV) (Klein and Manning, 2004) is the first generative model that outperforms the left-branching baseline in unsupervised dependency parsing. In DMV, a sentence is generated by recursively applying three types of grammar rules to construct a parse tree from the top down. The probability of the generated sentence and parse tree is the probability product of all the rules used in the generation process. To learn the parameters (rule probabilities) of DMV, the expectation maximization algorithm is often used. Noji et al. (2016) exploited two universal syntactic biases in learning DMV: restricting the center-embedding depth and encouraging short dependencies.", "startOffset": 41, "endOffset": 566}, {"referenceID": 5, "context": "Convex-MST (Grave and Elhadad, 2015) is a discriminative model for unsupervised dependency parsing based on the first-order maximum spanning tree dependency parser (McDonald et al.", "startOffset": 11, "endOffset": 36}, {"referenceID": 13, "context": "Convex-MST (Grave and Elhadad, 2015) is a discriminative model for unsupervised dependency parsing based on the first-order maximum spanning tree dependency parser (McDonald et al., 2005).", "startOffset": 164, "endOffset": 187}, {"referenceID": 4, "context": "Dual decomposition (Dantzig and Wolfe, 1960), a special case of Lagrangian relaxation, is an optimization method that decomposes a hard problem into several small sub-problems.", "startOffset": 19, "endOffset": 44}, {"referenceID": 8, "context": "It has been widely used in machine learning (Komodakis et al., 2007) and natural language processing (Koo et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 9, "context": ", 2007) and natural language processing (Koo et al., 2010; Rush and Collins, 2012).", "startOffset": 40, "endOffset": 82}, {"referenceID": 15, "context": ", 2007) and natural language processing (Koo et al., 2010; Rush and Collins, 2012).", "startOffset": 40, "endOffset": 82}, {"referenceID": 7, "context": "They also showed that the objective function of the work of Klein and Manning (2004) is a special case of the product EM algorithm for grammar induction.", "startOffset": 60, "endOffset": 85}, {"referenceID": 14, "context": "While in principle this objective can be used to combine many different types of models, here we consider two state-ofthe-art models of unsupervised dependency parsing, a generative model LC-DMV (Noji et al., 2016) and a discriminative model Convex-MST (Grave and Elhadad, 2015).", "startOffset": 195, "endOffset": 214}, {"referenceID": 5, "context": ", 2016) and a discriminative model Convex-MST (Grave and Elhadad, 2015).", "startOffset": 46, "endOffset": 71}, {"referenceID": 5, "context": "The second decoding problem can be solved using the same algorithm of Grave and Elhadad (2015) (we use the projective version in our approach).", "startOffset": 70, "endOffset": 95}, {"referenceID": 14, "context": "of training sentences of length \u2264 15 and selected the top thirty datasets, which is similar to the setup of Noji et al. (2016). For each dataset, we trained our method on the training data with length \u2264 15 and tested our method on the testing data with length \u2264 40.", "startOffset": 108, "endOffset": 127}], "year": 2017, "abstractText": "Unsupervised dependency parsing aims to learn a dependency parser from unannotated sentences. Existing work focuses on either learning generative models using the expectation-maximization algorithm and its variants, or learning discriminative models using the discriminative clustering algorithm. In this paper, we propose a new learning strategy that learns a generative model and a discriminative model jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both models and improve their learning results. We tested our method on the UD treebank and achieved a state-ofthe-art performance on thirty languages.", "creator": "LaTeX with hyperref package"}}}