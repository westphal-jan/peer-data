{"id": "1708.01771", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Aug-2017", "title": "Neural Machine Translation with Word Predictions", "abstract": "with the encoder - decoder criterion for processing machine translation ( nmt ), consistent hidden states complement the recurrent structures providing the encoder and decoder carry the crucial information about the sentence. these vectors are generated by parameters which are updated by back - propagation of translation errors through context. we argue that propagating errors through predictable end - to - starting recurrent structures are not for direct way of control the hidden vectors. in conceptual paper, we propose i use word predictions as a mechanism for direct supervision. thus specifically, arguments require these vectors to be able to predict matching vocabulary in target sentence. somewhat simple mechanism ensures better representations in the encoder and decoder without using any extra data or representation. it is also helpful in reducing the target side vocabulary and improving the decoding efficiency. experiments on chinese - english and german - english machine translation tasks show bleu frequency by 4. 53 with 1. 3, respectively", "histories": [["v1", "Sat, 5 Aug 2017 13:38:10 GMT  (1823kb,D)", "http://arxiv.org/abs/1708.01771v1", "Accepted at EMNLP2017"]], "COMMENTS": "Accepted at EMNLP2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rongxiang weng", "shujian huang", "zaixiang zheng", "xin-yu dai", "jiajun chen"], "accepted": true, "id": "1708.01771"}, "pdf": {"name": "1708.01771.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation with Word Predictions", "authors": ["Rongxiang Weng", "Shujian Huang", "Zaixiang Zheng", "Xinyu Dai", "Jiajun Chen"], "emails": ["chenjj}@nlp.nju.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "The encoder-decoder based neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014) have been developing rapidly. Sutskever et al. (2014) propose to encode the source sentence as a fixed-length vector representation, based on which the decoder generates the target sequence, where both the encoder and decoder are recurrent neural networks (RNN) (Sutskever et al., 2014) or their variants (Cho et al., 2014; Chung et al., 2014; Bahdanau et al., 2014). In this framework, the fixedlength vector plays the crucial role of transitioning\nthe information of the sentence from the source side to the target side.\nLater, attention mechanisms are proposed to enhance the source side representations (Bahdanau et al., 2014; Luong et al., 2015b). The source side context is computed at each time-step of decoding, based on the attention weights between the source side representations and the current hidden state of the decoder. However, the hidden states in the recurrent decoder still originate from the single fixed-length representation (Luong et al., 2015b), or the average of the bi-directional representations (Bahdanau et al., 2014). Here we refer to the representation as initial state.\nInterestingly, Britz et al. (2017) find that the value of initial state does not affect the translation performance, and prefer to set the initial state to be a zero vector. On the contrary, we argue that initial state still plays an important role of translation, which is currently neglected. We notice that beside the end-to-end error back propagation for the initial and transition parameters, there is no direct control of the initial state in the current NMT architectures. Due to the large number of parameters, it may be difficult for the NMT system to learn the proper sentence representation as the initial state. Thus, the model is very likely to get stuck in local minimums, making the translation process arbitrary and unstable.\nIn this paper, we propose to augment the current NMT architecture with a word prediction mechanism. More specifically, we require the initial state of the decoder to be able to predict all the words in the target sentence. In this way, there is a specific objective for learning the initial state. Thus the learnt source side representation will be better constrained. We further extend this idea by applying the word predictions mechanism to all the hidden states of the decoder. So the transition between different decoder states could be controlled\nar X\niv :1\n70 8.\n01 77\n1v 1\n[ cs\n.C L\n] 5\nA ug\n2 01\n7\nas well. Our mechanism is simple and requires no additional data or annotation. The proposed word predictions mechanism could be used as a training method and brings no extra computing cost during decoding.\nExperiments on the Chinese-English and German-English translation tasks show that both the constraining of the initial state and the decoder hidden states bring significant improvement over the baseline systems. Furthermore, using the word prediction mechanism on the initial state as a word predictor to reduce the target side vocabulary could greatly improve the decoding efficiency, without a significant loss on the translation quality."}, {"heading": "2 Related Work", "text": "Many previous works have noticed the problem of training an NMT system with lots of parameters. Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016). Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016). Both techniques could bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments.\nThe way we add the word prediction is similar to the research of multi-task learning. Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder. Zhang and Zong (2016) propose to use multitask learning for incorporating source side monolingual data. Different from these attempts, our method focuses solely on the current translation task, and does not require any extra data or annotation.\nIn the other sequence to sequence tasks, Suzuki and Nagata (2017) propose the idea for predicting words by using encoder information. However, the purpose and the way of our mechanism are different from them.\nThe word prediction technique has been applied in the research of both statistical machine transla-\ntion (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al., 2016; L\u2019Hostis et al., 2016). In these research, word prediction mechanisms are employed to decide the selection of words or constrain the target vocabulary, while in this paper, we use word prediction as a control mechanism for neural model training."}, {"heading": "3 Notations and Backgrounds", "text": "We present a popular NMT framework with the encoder-decoder architecture (Cho et al., 2014; Bahdanau et al., 2014) and the attention networks (Luong et al., 2015b), based on which we propose our word prediction mechanism.\nDenote a source-target sentence pair as {x, y} from the training set, where x is the source word sequence (x1, x2, \u00b7 \u00b7 \u00b7 , x|x|) and y is the target word sequence (y1, y2, \u00b7 \u00b7 \u00b7 , y|y|), |x| and |y| are the length of x and y, respectively.\nIn the encoding stage, a bi-directional recurrent neural network is used (Bahdanau et al., 2014) to encode x into a sequence of vectors (h1,h2, \u00b7 \u00b7 \u00b7 ,h|x|). For each xi, the representation hi is:\nhi = [ \u2212\u2192 hi ; \u2190\u2212 hi ] (1)\nwhere [\u00b7; \u00b7] denotes the concatenation of column vectors; \u2212\u2192 hi and \u2190\u2212 hi denote the hidden vectors for the word xi in the forward and backward RNNs, respectively.\nThe gated recurrent unit (GRU) is used as the recurrent unit in each RNN, which is shown to have promising results in speech recognition and machine translation (Cho et al., 2014). Formally, the hidden state hi at time step i of the forward RNN encoder is defined by the GRU function g\u2212\u2192e (\u00b7, \u00b7), as follows:\n\u2212\u2192 h i = g\u2212\u2192e ( \u2212\u2192 h i\u22121, embxi) (2)\n= (1\u2212\u2212\u2192z i) \u2212\u2192 h i\u22121 +\u2212\u2192z i \u2212\u2192 h\u2032 i\n\u2212\u2192z i = \u03c3( \u2212\u2192 Wz[embxi ; \u2212\u2192 h i\u22121]) (3) \u2212\u2192 h\u2032 i = tanh( \u2212\u2192 W[embxi ; ( \u2212\u2192r i \u2212\u2192 h i\u22121)]) (4) \u2212\u2192r i = \u03c3( \u2212\u2192 Wr[embxi ; \u2212\u2192 h i\u22121]) (5)\nwhere denotes element-wise product between vectors and embxi is the word embedding of the xi. tanh(\u00b7) and \u03c3(\u00b7) are the tanh and sigmoid transformation functions that can be applied elementwise on vectors, respectively. For simplicity, we\nomit the bias term in each network layer. The backward RNN encoder is defined likewise.\nIn the decoding stage, the decoder starts with the initial state s0, which is the average of source representations (Bahdanau et al., 2014).\ns0 = \u03c3(Ws 1\n|x| |x|\u2211 i=1 hi) (6)\nAt each time step j, the decoder maximizes the conditional probability of generating the jth target word, which is defined as:\nP (yj |y<j , x) = fd(td([embyj\u22121 ; sj ; cj ])) (7) fd(u) = softmax(Wfu) (8) td(v) = tanh(Wtv) (9)\nwhere sj is the decoder\u2019s hidden state, which is computed by another GRU (as in Equation 2):\nsj = gd(sj\u22121, [embyj\u22121 ; cj ]) (10)\nand the context vector cj is from the attention mechanism (Luong et al., 2015b):\ncj = |x|\u2211 i=1 ajihi (11)\naji = exp(eji)\u2211|x|\nk=1 exp(ejk) (12)\neji = tanh(Wattd [sj\u22121;hi]). (13)"}, {"heading": "4 NMT with Word Predictions", "text": ""}, {"heading": "4.1 Word Prediction for the Initial State", "text": "The decoder starts the generation of target sentence from the initial state s0 (Equation 6) generated by the encoder. Currently, the update for\nthe encoder only happens when a translation error occurs in the decoder. The error is propagated through multiple time steps in the recurrent structure until it reaches the encoder. As there are hundreds of millions of parameters in the NMT system, it is hard for the model to learn the exact representation of source sentences. As a result, the values of initial state may not be exact during the translation process, leading to poor translation performances.\nWe propose word prediction as a mechanism to control the values of initial state. The intuition is that since the initial state is responsible for the translation of whole target sentence, it should at least contain information of each word in the target sentence. Thus, we optimize the initial state by making prediction for all target words. For simplicity, we assume each target word is independent of each other.\nHere the word prediction mechanism is a simpler sub-task of translation, where the order of words is not considered. The prediction task could be trained jointly with the translation task in a multi-task learning way (Luong et al., 2015a; Dong et al., 2015; Zhang and Zong, 2016), where both tasks share the same encoder. In other words, word prediction for the initial state could be interpreted as an improvement for the encoder. We denote this mechanism as WPE .\nAs shown in Figure 1, a prediction network is added to the initial state. We define the conditional probability of WPE as follows:\nPWPE(y|x) = |y|\u220f j=1 PWPE(yj |x) (14)\nPWPE(yj |x) = fp(tp([s0; cp])) (15)\nwhere fp(\u00b7) and tp(\u00b7) are the softmax layer and non-linear layer as defined in Equation 8-9, with\ndifferent parameters; cp is defined similar as the attention network, so the source side information could be enhanced.\ncp = |x|\u2211 i=1 aihi (16) ai = exp(ei)\u2211|x|\nk=1 exp(ek) (17)\nei = tanh(Wattp [s0,hi]). (18)"}, {"heading": "4.2 Word Predictions for Decoder\u2019s Hidden States", "text": "Similar intuition is also applied for the decoder. Because the hidden states of the decoder are responsible for the translation of target words, they should be able to predict the target words as well. The only difference is that we remove the already generated words from the prediction task. So each hidden state in the decoder is required to predict the target words which remain untranslated.\nFor the first state s1 of the decoder, the prediction task is similar with the task for the initial state. Since then, the prediction is no longer a separate training task, but integrated into each time step of the training process. We denote this mechanism as WPD.\nAs shown in Figure 2, for each time step j in the decoder, the hidden state sj is used for the prediction of (yj , yj+1, \u00b7 \u00b7 \u00b7 , y|y|). The conditional probability of WPD is defined as:\nPWPD(yj , yj+1, \u00b7 \u00b7 \u00b7 , y|y||y<j , x) (19)\n= |y|\u220f k=j PWPD(yk|y<j , x)\nPWPD(yk|y<j , x) =fd(p(td([embyj\u22121 ; sj ; cj ]))) (20)\nwhere fd(\u00b7) and td(\u00b7) are the softmax layer and non-linear layer as defined in Equation 8-9; p(\u00b7) is another non-linear transformation layer, which prepares the current state for the prediction:\np(u) = tanh(Wpu). (21)"}, {"heading": "4.3 Training", "text": "NMT models optimize the networks by maximizing the likelihood of the target translation y given source sentence x, denoted by LT.\nLT = 1\n|y| |y|\u2211 j=1 logP (yj |y<j , x) (22)\nwhere P (yj |y<j , x) is defined in Equation 7. To optimize the word prediction mechanism, we propose to add extra likelihood functions LWPE and LWPD into the training procedure.\nFor the WPE, we directly optimize the likelihood of translation and word prediction:\nL1 = LT + LWPE (23) LWPE = logPWPE (24)\nwhere PWPE is defined in Equation 14. For the WPD, we optimize the likelihood as:\nL2 = LT + LWPD (25)\nLWPD = |y|\u2211 j=1\n1\n|y| \u2212 j + 1 logPWPD (26)\nwhere PWPD is defined in Equation 19; the coefficient of the logarithm is used to calculate the average probability of each prediction.\nThe two mechanisms could also work together, so that both the encoder and the decoder could be improved:\nL3 = LT + LWPE + LWPD . (27)"}, {"heading": "4.4 Making Use of the Word Predictor", "text": "The previously proposed word prediction mechanism could be used only as a extra training objective, which will not be computed during the translation. Thus the computational complexity of our models for translation stays exactly the same.\nOn the other hand, using a smaller and specific vocabulary for each sentence or batch will improve translation efficiency. If the vocabulary is accurate enough, there is also a chance to improve the translation quality (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016). Our word prediction mechanism WPE provides a natural solution for generating a possible set of target words at sentence level. The prediction could be made from the initial state s0, without using extra resources such as word dictionaries, extracted phrases or frequent word lists, as in Mi et al. (2016)."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Data", "text": "We perform experiments on the Chinese-English (CH-EN) and German-English (DE-EN) machine translation tasks. For the CH-EN, the training data consists of about 8 million sentence pairs 1. We use NIST MT02 as our validation set, and the NIST MT03, MT04 and MT05 as our test sets. These sets have 878, 919, 1597 and 1082 source sentences, respectively, with 4 references for each sentence. For the DE-EN, the experiments trained on the standard benchmark WMT14, and it has about 4.5 million sentence pairs. We use newstest 2013 (NST13) as validation set, and newstest 2014(NST14) as test set. These sets have 3000 and 2737 source sentences, respectively, with 1 reference for each sentence. Sentences were encoded using byte-pair encoding (BPE) (Britz et al., 2017)."}, {"heading": "5.2 Systems and Techniques", "text": "We implement a baseline system with the bidirectional encoder (Bahdanau et al., 2014) and the attention mechanism (Luong et al., 2015b) as described in Section 3, denoted as baseNMT. Then our proposed word prediction mechanism on initial state and hidden states of decoder are implemented on the baseNMT system, denoted as WPE and WPD, respectively. We denote the system use\n1includes LDC2002E18, LDC2003E07, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005T06, LDC2005T10, LDC2006E26 and LDC2007T09\nboth techniques as WPED. We implement systems with variable-sized vocabulary following (Mi et al., 2016). For comparison, we also implement systems with dropout (with dropout rate 0.5 on the output layer) and ensemble (ensemble of 4 systems at the output layer) techniques."}, {"heading": "5.3 Implementation Details", "text": "Both our CH-EN and DE-EN experiments are implemented on the open source toolkit dl4mt 2, with most default parameter settings kept the same. We train the NMT systems with the sentences of length up to 50 words. The source and target vocabularies are limited to the most frequent 30K words for both Chinese and English, respectively, with the out-of-vocabulary words mapped to a special token UNK.\nThe dimension of word embedding is set to 512 and the size of the hidden layer is 1024. The recurrent weight matrices are initialized as random orthogonal matrices, and all the bias vectors as zero. Other parameters are initialized by sampling from the Gaussian distribution N (0, 0.01).\nWe use the mini-batch stochastic gradient descent (SGD) approach to update the parameters, with a batch size of 32. The learning rate is controlled by AdaDelta (Zeiler, 2012).\nFor efficient training of our system, we adopt a simple pre-train strategy. Firstly, the baseNMT system is trained. The training results are used as the initial parameters for pre-training our proposed models with word predictions.\nFor decoding during test time, we simply decode until the end-of-sentence symbol eos occurs, using a beam search with a beam width of 5."}, {"heading": "5.4 Translation Experiments", "text": "To see the effect of word predictions in translation, we evaluate these systems in case-insensitive IBM-BLEU (Papineni et al., 2002) on both CHEN and DE-EN tasks.\nThe detailed results are show in the Table 1 and Table 2. Compared to the baseNMT system, all of our models achieve significant improvements. On the CH-EN experiments, simply adding word predictions to the initial state (WPE) already brings considerable improvements. The average improvement on test set is 2.53 BLEU, showing that constraining the initial state does lead to a higher translation quality. Adding word predic-\n2https://github.com/nyu-dl/dl4mt-tutorial\ntions to the hidden states in the decoder (WPD) leads to further improvements against baseNMT (4.15 BLEU), because WPD adds constraints to the state transitions through different time steps in the decoder. Using both techniques improves the baseline by 4.53 BLEU. On the DE-EN experiments, the improvement of WPE model is 0.41 BLEU and WPD model is 0.86 BLEU on test set. When use both techniques, the WPED improves on the test set is 1.3 BLEU.\nWe compare our models with systems using dropout and ensemble techniques. The results show in Table 3 and 4. On the CH-EN experiments, the dropout method successfully improves the baseNMT system by 2.06 BLEU. However, it does not work on our WPED system. The ensemble technique improves the baseNMT system by 2.75 BLEU. It still improves WPED by 1.26\nBLEU, but the improvement is smaller than on the baseNMT. On the DE-EN experiments, the phenomenon of experiments is similar to CH-EN experiments. The baseNMT system improves 0.94 through dropout method and 0.9 BLEU through ensemble method. The dropout technique also does not work on WPED and the ensemble technique improves 1.79 BLEU. These comparisons suggests that our system already learns better and stable values for the parameters, enjoying some of the benefits of general training techniques like dropout and ensemble. Compared to dropout and ensemble, our method WPED achieves the highest improvement against the baseline system on both CH-EN and DE-EN experiments. Along with ensemble method, the improvement could be up to 5.79 BLEU and 1.79 BLEU respectively."}, {"heading": "5.5 Word Prediction Experiments", "text": "Since we include an explicit word prediction mechanism during the training of NMT systems, we also evaluate the prediction performance on the CH-EN experiments to see how the training is improved.\nFor each sentence in the test set, we use the initial state of the given model to make prediction about the possible words. We denote the set of top n words as Tn, the set of words in all the refer-\nences as R. We define the precision, recall of the word prediction as follows:\nprecision = |Tn \u2229R| |Tn| \u2217 100% (28)\nrecall = |Tn \u2229R| |R| \u2217 100% (29)\nWe compare the prediction performance of baseNMT and WPE. WPED has similar prediction results with WPE, so we omit its results. As shown in Table 5, baseNMT system has a relatively lower prediction precision, for example, 45% in top 10 prediction. With an explicit training, the WPE could achieve a much higher precision in all conditions. Specifically, the precision reaches 73% in top 10. This indicates that the initial state in WPE contains more specific information about the prediction of the target words, which may be a step towards better semantic representation, and leads to better translation quality.\nBecause the total words in the references are limited (around 50), the precision goes down, as expected, when a larger prediction set is considered. On the other hand, the recall of WPE is also much higher than baseNMT. When given 1k predictions, WPE could successfully predict 89% of the words in the reference. The recall goes up to 95% with 5k predictions, which is only 1/6 of the current vocabulary.\nTo analyze the process of word prediction, we draw the attention heatmap (Equation 16) between the initial state s0 and the bi-directional representation of each source side word hi for an example sentence. As shown in Figure 3, both examples show that the initial states have a very strong attention with all the content words in the source sentence. The blank cells are mostly functions words\nor high frequent tokens such as \u201cde(\u2019s)\u201d, \u201cshi(is)\u201d, \u201cer(and)\u201d, \u201cta(it)\u201d, comma and period. This indicates that the initial state successfully encodes information about most of the content words in the source sentence, which contributes for a high prediction performance and leads to better translation."}, {"heading": "5.6 Improving Decoding Efficiency", "text": "To make use of the word prediction, we conduct experiments using the predicted vocabulary, with different vocabulary size (1k to 10k) on the CHEN experiments, denoted as WPE-V and WPED-V. The comparison is made in both translation quality and decoding time. As all our models with fixed vocabulary size have exactly the same number of parameters for decoding (extra mechanism is used only for training), we only plot the decoding time of the WPED for comparison. Figure 4 and 5 show the results.\nWhen we start the experiments with top 1k vocabulary (1/30 of the baseline settings), the translation quality of both WPE-V and WPED-V are already higher than the baseNMT; while their decoding time is less than 1/3 of an NMT system with 30k vocabulary. When the size of vocabulary increases, the translation quality improves as well. With a 6k predicted vocabulary (1/5 of the baseline settings), the decoding time is about 60% of a full-\nvocabulary system; the performances of both systems with variable size vocabulary are comparable their corresponding fixed-vocabulary systems, which is higher than the baseNMT by 2.53 and 4.53 BLEU, respectively.\nAlthough the comparison may not be fair enough due to the language pair and training conditions, the above relative improvements (e.g. WPED-V v.s. baseNMT) is much higher than previous research of manipulating the vocabularies (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016). This is because our mechanism is not only about reducing the vocabulary itself for each sentence or batch, it also brings improvement to the overall translation model. Please note that\nunlike these research, we keep the target vocabulary to be 30k in all our experiments, because we are not focusing on increasing the vocabulary size in this paper. It will be interesting to combine our mechanism with larger vocabulary to further enhance the translation performance. Again, our mechanism requires no extra annotation, dictionary, alignment or separate discriminative predictor, etc."}, {"heading": "5.7 Translation Analysis", "text": "We also analyze real-case translations to see the difference between different systems (Table 6).\nIt is easy to see that the baseNMT system misses the translations of several important words, such as \u201cadvertising\u201d, \u201c1.5\u201d, which are marked with underline in the reference. It also wrongly translates the company name \u201ctime warner inc.\u201d as the redundant information \u201cinternet company\u201d; \u201camerica online\u201d as \u201cus line\u201d.\nThe results of dropout or ensemble show improvement compared to the baseNMT. But they still make mistakes about the translation of \u201conline\u201d and the company name \u201ctime warner inc.\u201d.\nWith WPED, most of these errors no longer exist, because we force the encoder and decoder to carry the exact information during translation."}, {"heading": "6 Conclusions", "text": "The encoder-decoder architecture provides a general paradigm for learning machine translation from the source language to the target language. However, due to the large amount of parameters and relatively small training data set, the end-toend learning of an NMT model may not be able to learn the best solution. We argue that at least part of the problem is caused by the long error backpropagation pipeline of the recurrent structures in multiple time steps, which provides no direct control of the information carried by the hidden states in both the encoder and decoder.\nInstead of looking for other annotated data, we notice that the words in the target language sentence could be viewed as a natural annotation. We propose to use the word prediction mechanism to enhance the initial state generated by the encoder and extend the mechanism to control the hidden states of decoder as well. Experiments show promising results on the Chinese-English and German-English translation tasks. As a byproduct, the word predictor could be used to im-\nsources r \u65f6\u4ee3\u534e\u7eb3\u516c\u53f8\u7684\u2f79\u7edc\u516c\u53f8\u7f8e\u56fd\u7ebf\u4e0a\u8bf4,\u5b83\u9884\u671f\u2f06 \u25cb \u25cb\u2f06\u5e74\u7684\u2f34\u544a\u4e0e\u5546\u4e1a\u9500\u552e\u5c06\u7531 \u2f06 \u25cb \u25cb\u2f00\u5e74\u7684\u2f06\u2f17\u4e03\u4ebf\u7f8e\u5143\u51cf\u5c11\u5230\u2f17\u4e94\u4ebf\u7f8e\u5143\u3002 reference america online , the internet arm of time warner conglomerate , said it expects advertising and commerce revenue to decline from us $ 2.7 billion in 2001 to us $ 1.5 in 2002 . baseNMT in the us line , the internet company \u2019s internet company said on the internet that it expected that the business sales in 2002 would fall from $ UNK billion to $ UNK billion in 2001 .\nbaseNMT +dropout\non the united states line , UNK \u2019s internet company said on the internet that it expects to reduce the annual advertising and commercial sales from $ UNK billion in 2001 to $ 1.5 billion .\nbaseNMT +ensemble in the us line , the internet company \u2019s internet company said that it expected that the advertising and commercial sales volume for 2002 would be reduced from us $ UNK billion to us $ 1.5 billion in 2001 . WPED the internet company of time warner inc. , the us online , said that it expects that the advertising and commercial sales in 2002 will decrease from $ UNK billion in 2001 to us $ 1.5 billion .\nTable 6: Comparisons of different systems in translating the same example sentence, which from CHEN test sets. (\u201csource\u201d indicates the source sentence; \u201creference\u201d indicates the human translation; the translation results are indicated by their system names, including our best \u201cWPED\u201d systems. The underline words in the reference are missed in the baseNMT output; the bold font indicates improvements over the baseNMT system; and the italic font indicates remaining translation errors.)\ncrucial for large scale applications. Our attempts demonstrate that the learning of the large scale neural network systems is still not good enough. In the future, it might be helpful to analyze the benefits of jointly learning other related tasks together with machine translation, to provide further control of the learning process. It is interesting to demonstrate the effectiveness of the proposed mechanism on other sequence to sequence learning tasks as well.\nAcknowledgments\nWe would like to thank the anonymous reviewers for their insightful comments. Shujian Huang is the corresponding author. This work is supported by the National Science Foundation of China (No. 61672277, 61300158, 61472183).\nReferences Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.\nSrinivas Bangalore, Patrick Haffner, and Stephan Kanthak. 2007. Statistical machine translation through global lexical selection and sentence reconstruction. In Proceedings of the 45th Annual Meeting of the\nAssociation of Computational Linguistics. Association for Computational Linguistics, pages 152\u2013159. http://aclweb.org/anthology/P07-1020.\nDenny Britz, Anna Goldie, Minh-Thang Luong, and Quoc Le. 2017. Massive Exploration of Neural Machine Translation Architectures. ArXiv e-prints .\nKyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, pages 1724\u20131734. https://doi.org/10.3115/v1/D14-1179.\nJunyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR abs/1412.3555. http://arxiv.org/abs/1412.3555.\nDaxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. 2015. Multi-task learning for multiple language translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, pages 1723\u20131732. https://doi.org/10.3115/v1/P15-1166.\nreference a erica online , the internet ar of ti e arner conglo erate , said it expects advertising and co erce revenue to decline from us $ 2.7 billion in 2001 to us $ 1.5 in 2002 . baseN T in the us line , the internet company \u2019s internet company said on the internet that it expected that the business sales in 2002 would fall from $ UNK billion to $ UNK billion in 2001 .\nbaseNMT +dropout\non the united states line , UNK \u2019s internet company said on the internet that it expects to reduce the annual advertising and commercial sales from $ UNK billion in 2001 to $ 1.5 billion .\nbaseNMT +ensemble in the us line , the internet company \u2019s internet company said that it expected that the advertising and commercial sales volume for 2002 would be reduced from us $ UNK billion to us $ 1.5 billion in 2001 . WPED the internet company of time warner inc. , the us online , said that it expects that the advertising and commercial sales in 2002 will decrease from $ UNK billion in 2001 to us $ 1.5 billion .\nTable 6: Comparisons of different systems in translating the same example sentence, which from CH-EN test sets. (\u201csource\u201d indicates the source sentence; \u201creference\u201d indicates the human translation; the translation results are indicated by their system names, including our best \u201cWPED\u201d systems. The underline words in the reference are missed in the baseNMT output; the bold font indicates improvements over the baseNMT system; and the italic font indicates remaining translation errors.)\nprove the efficiency of decoding, which may be crucial for large scale applications.\nOur attempts demonstrate that the learning of the large scale neural network systems is still not good enough. In the future, it might be helpful to analyze the benefits of jointly learning other related tasks together with machine translation, to provide further control of the learning process. It is interesting to demonstrate the effectiveness of the proposed mechanism on other sequence to sequence learning tasks as well.\nAckn wledgments\nWe would like to thank the anonymous reviewers for their insightful comments. Shujian Huang is the corresponding author. This work is supported by the National Science Foundation of China (No. 61672277, 61300158, 61472183)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Statistical machine translation through", "author": ["Srinivas Bangalore", "Patrick Haffner", "Stephan Kanthak"], "venue": null, "citeRegEx": "Bangalore et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bangalore et al\\.", "year": 2007}, {"title": "Massive Exploration of Neural Machine Translation Architectures", "author": ["Denny Britz", "Anna Goldi", "Minh-Thang Luong", "Quoc Le."], "venue": "ArXiv e-prints .", "citeRegEx": "Britz et al\\.,? 2017", "shortCiteRegEx": "Britz et al\\.", "year": 2017}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1412.3555. http://arxiv.org/abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "A discriminative lexicon model for complex morphology", "author": ["Minwoo Jeong", "Kristina Toutanova", "Hisami Suzuki", "Chris Quirk."], "venue": "Proceedings of the Ninth Conference of the Association for Machine Translation in the Americas (AMTA 2010).", "citeRegEx": "Jeong et al\\.,? 2010", "shortCiteRegEx": "Jeong et al\\.", "year": 2010}, {"title": "Vocabulary selection strategies for neural machine translation", "author": ["Gurvan L\u2019Hostis", "David Grangier", "Michael Auli"], "venue": "CoRR abs/1610.00072", "citeRegEx": "L.Hostis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "L.Hostis et al\\.", "year": 2016}, {"title": "Multitask sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "CoRR abs/1511.06114. http://arxiv.org/abs/1511.06114.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid wordcharacter models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "CoRR abs/1604.00788. http://arxiv.org/abs/1604.00788.", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Extending statistical machine translation with discriminative and trigger-based lexicon models", "author": ["Arne Mauser", "Sa\u0161a Hasan", "Hermann Ney."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Associa-", "citeRegEx": "Mauser et al\\.,? 2009", "shortCiteRegEx": "Mauser et al\\.", "year": 2009}, {"title": "Interactive attention for neural machine translation", "author": ["Fandong Meng", "Zhengdong Lu", "Hang Li", "Qun Liu."], "venue": "CoRR abs/1610.05011. http://arxiv.org/abs/1610.05011.", "citeRegEx": "Meng et al\\.,? 2016", "shortCiteRegEx": "Meng et al\\.", "year": 2016}, {"title": "Vocabulary manipulation for neural machine translation", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "CoRR abs/1605.03209. http://arxiv.org/abs/1605.03209.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Asso-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "J. Mach. Learn. Res. 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Sys-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Rnnbased encoder-decoder approach with word frequency estimation", "author": ["Jun Suzuki", "Masaaki Nagata."], "venue": "CoRR abs/1701.00138. http://arxiv.org/abs/1701.00138.", "citeRegEx": "Suzuki and Nagata.,? 2017", "shortCiteRegEx": "Suzuki and Nagata.", "year": 2017}, {"title": "Word translation prediction for morphologically rich languages with bilingual neural networks", "author": ["Ke Tran", "Arianna Bisazza", "Christof Monz."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Tran et al\\.,? 2014", "shortCiteRegEx": "Tran et al\\.", "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["Jiajun Zhang", "Chengqing Zong."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for", "citeRegEx": "Zhang and Zong.,? 2016", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "The encoder-decoder based neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014) have been developing rapidly.", "startOffset": 66, "endOffset": 108}, {"referenceID": 3, "context": "The encoder-decoder based neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014) have been developing rapidly.", "startOffset": 66, "endOffset": 108}, {"referenceID": 17, "context": "(2014) propose to encode the source sentence as a fixed-length vector representation, based on which the decoder generates the target sequence, where both the encoder and decoder are recurrent neural networks (RNN) (Sutskever et al., 2014) or their variants (Cho et al.", "startOffset": 215, "endOffset": 239}, {"referenceID": 3, "context": ", 2014) or their variants (Cho et al., 2014; Chung et al., 2014; Bahdanau et al., 2014).", "startOffset": 26, "endOffset": 87}, {"referenceID": 4, "context": ", 2014) or their variants (Cho et al., 2014; Chung et al., 2014; Bahdanau et al., 2014).", "startOffset": 26, "endOffset": 87}, {"referenceID": 0, "context": ", 2014) or their variants (Cho et al., 2014; Chung et al., 2014; Bahdanau et al., 2014).", "startOffset": 26, "endOffset": 87}, {"referenceID": 0, "context": "Later, attention mechanisms are proposed to enhance the source side representations (Bahdanau et al., 2014; Luong et al., 2015b).", "startOffset": 84, "endOffset": 128}, {"referenceID": 11, "context": "Later, attention mechanisms are proposed to enhance the source side representations (Bahdanau et al., 2014; Luong et al., 2015b).", "startOffset": 84, "endOffset": 128}, {"referenceID": 11, "context": "However, the hidden states in the recurrent decoder still originate from the single fixed-length representation (Luong et al., 2015b), or the average of the bi-directional representations (Bahdanau et al.", "startOffset": 112, "endOffset": 133}, {"referenceID": 0, "context": ", 2015b), or the average of the bi-directional representations (Bahdanau et al., 2014).", "startOffset": 63, "endOffset": 86}, {"referenceID": 1, "context": ", 2014; Cho et al., 2014) have been developing rapidly. Sutskever et al. (2014) propose to encode the source sentence as a fixed-length vector representation, based on which the decoder generates the target sequence, where both the encoder and decoder are recurrent neural networks (RNN) (Sutskever et al.", "startOffset": 8, "endOffset": 80}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014). In this framework, the fixedlength vector plays the crucial role of transitioning the information of the sentence from the source side to the target side. Later, attention mechanisms are proposed to enhance the source side representations (Bahdanau et al., 2014; Luong et al., 2015b). The source side context is computed at each time-step of decoding, based on the attention weights between the source side representations and the current hidden state of the decoder. However, the hidden states in the recurrent decoder still originate from the single fixed-length representation (Luong et al., 2015b), or the average of the bi-directional representations (Bahdanau et al., 2014). Here we refer to the representation as initial state. Interestingly, Britz et al. (2017) find that the value of initial state does not affect the translation performance, and prefer to set the initial state to be a zero vector.", "startOffset": 8, "endOffset": 802}, {"referenceID": 16, "context": "Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016).", "startOffset": 49, "endOffset": 114}, {"referenceID": 11, "context": "Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016).", "startOffset": 49, "endOffset": 114}, {"referenceID": 13, "context": "Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016).", "startOffset": 49, "endOffset": 114}, {"referenceID": 17, "context": "Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016).", "startOffset": 82, "endOffset": 150}, {"referenceID": 6, "context": "Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016).", "startOffset": 82, "endOffset": 150}, {"referenceID": 10, "context": "Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016).", "startOffset": 82, "endOffset": 150}, {"referenceID": 5, "context": "Dong et al. (2015) propose to share an encoder between different translation tasks.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder.", "startOffset": 0, "endOffset": 105}, {"referenceID": 5, "context": "Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder. Zhang and Zong (2016) propose to use multitask learning for incorporating source side monolingual data.", "startOffset": 0, "endOffset": 280}, {"referenceID": 18, "context": "In the other sequence to sequence tasks, Suzuki and Nagata (2017) propose the idea for predicting words by using encoder information.", "startOffset": 41, "endOffset": 66}, {"referenceID": 1, "context": "The word prediction technique has been applied in the research of both statistical machine translation (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al.", "startOffset": 109, "endOffset": 193}, {"referenceID": 12, "context": "The word prediction technique has been applied in the research of both statistical machine translation (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al.", "startOffset": 109, "endOffset": 193}, {"referenceID": 7, "context": "The word prediction technique has been applied in the research of both statistical machine translation (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al.", "startOffset": 109, "endOffset": 193}, {"referenceID": 19, "context": "The word prediction technique has been applied in the research of both statistical machine translation (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al.", "startOffset": 109, "endOffset": 193}, {"referenceID": 14, "context": ", 2014) and NMT (Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 16, "endOffset": 56}, {"referenceID": 8, "context": ", 2014) and NMT (Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 16, "endOffset": 56}, {"referenceID": 3, "context": "We present a popular NMT framework with the encoder-decoder architecture (Cho et al., 2014; Bahdanau et al., 2014) and the attention networks (Luong et al.", "startOffset": 73, "endOffset": 114}, {"referenceID": 0, "context": "We present a popular NMT framework with the encoder-decoder architecture (Cho et al., 2014; Bahdanau et al., 2014) and the attention networks (Luong et al.", "startOffset": 73, "endOffset": 114}, {"referenceID": 11, "context": ", 2014) and the attention networks (Luong et al., 2015b), based on which we propose our word prediction mechanism.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": "In the encoding stage, a bi-directional recurrent neural network is used (Bahdanau et al., 2014) to encode x into a sequence of vectors (h1,h2, \u00b7 \u00b7 \u00b7 ,h|x|).", "startOffset": 73, "endOffset": 96}, {"referenceID": 3, "context": "The gated recurrent unit (GRU) is used as the recurrent unit in each RNN, which is shown to have promising results in speech recognition and machine translation (Cho et al., 2014).", "startOffset": 161, "endOffset": 179}, {"referenceID": 0, "context": "In the decoding stage, the decoder starts with the initial state s0, which is the average of source representations (Bahdanau et al., 2014).", "startOffset": 116, "endOffset": 139}, {"referenceID": 11, "context": "and the context vector cj is from the attention mechanism (Luong et al., 2015b):", "startOffset": 58, "endOffset": 79}, {"referenceID": 9, "context": "The prediction task could be trained jointly with the translation task in a multi-task learning way (Luong et al., 2015a; Dong et al., 2015; Zhang and Zong, 2016), where both tasks share the same encoder.", "startOffset": 100, "endOffset": 162}, {"referenceID": 5, "context": "The prediction task could be trained jointly with the translation task in a multi-task learning way (Luong et al., 2015a; Dong et al., 2015; Zhang and Zong, 2016), where both tasks share the same encoder.", "startOffset": 100, "endOffset": 162}, {"referenceID": 21, "context": "The prediction task could be trained jointly with the translation task in a multi-task learning way (Luong et al., 2015a; Dong et al., 2015; Zhang and Zong, 2016), where both tasks share the same encoder.", "startOffset": 100, "endOffset": 162}, {"referenceID": 6, "context": "If the vocabulary is accurate enough, there is also a chance to improve the translation quality (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 96, "endOffset": 155}, {"referenceID": 14, "context": "If the vocabulary is accurate enough, there is also a chance to improve the translation quality (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 96, "endOffset": 155}, {"referenceID": 8, "context": "If the vocabulary is accurate enough, there is also a chance to improve the translation quality (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 96, "endOffset": 155}, {"referenceID": 6, "context": "If the vocabulary is accurate enough, there is also a chance to improve the translation quality (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016). Our word prediction mechanism WPE provides a natural solution for generating a possible set of target words at sentence level. The prediction could be made from the initial state s0, without using extra resources such as word dictionaries, extracted phrases or frequent word lists, as in Mi et al. (2016).", "startOffset": 97, "endOffset": 462}, {"referenceID": 2, "context": "Sentences were encoded using byte-pair encoding (BPE) (Britz et al., 2017).", "startOffset": 54, "endOffset": 74}, {"referenceID": 0, "context": "We implement a baseline system with the bidirectional encoder (Bahdanau et al., 2014) and the attention mechanism (Luong et al.", "startOffset": 62, "endOffset": 85}, {"referenceID": 11, "context": ", 2014) and the attention mechanism (Luong et al., 2015b) as described in Section 3, denoted as baseNMT.", "startOffset": 36, "endOffset": 57}, {"referenceID": 14, "context": "We implement systems with variable-sized vocabulary following (Mi et al., 2016).", "startOffset": 62, "endOffset": 79}, {"referenceID": 20, "context": "The learning rate is controlled by AdaDelta (Zeiler, 2012).", "startOffset": 44, "endOffset": 58}, {"referenceID": 15, "context": "To see the effect of word predictions in translation, we evaluate these systems in case-insensitive IBM-BLEU (Papineni et al., 2002) on both CHEN and DE-EN tasks.", "startOffset": 109, "endOffset": 132}, {"referenceID": 6, "context": "baseNMT) is much higher than previous research of manipulating the vocabularies (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 80, "endOffset": 139}, {"referenceID": 14, "context": "baseNMT) is much higher than previous research of manipulating the vocabularies (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 80, "endOffset": 139}, {"referenceID": 8, "context": "baseNMT) is much higher than previous research of manipulating the vocabularies (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 80, "endOffset": 139}], "year": 2017, "abstractText": "In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the encoder and decoder carry the crucial information about the sentence.These vectors are generated by parameters which are updated by back-propagation of translation errors through time. We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors. In this paper, we propose to use word predictions as a mechanism for direct supervision. More specifically, we require these vectors to be able to predict the vocabulary in target sentence. Our simple mechanism ensures better representations in the encoder and decoder without using any extra data or annotation. It is also helpful in reducing the target side vocabulary and improving the decoding efficiency. Experiments on Chinese-English and German-English machine translation tasks show BLEU improvements by 4.53 and 1.3, respectively.", "creator": "LaTeX with hyperref package"}}}