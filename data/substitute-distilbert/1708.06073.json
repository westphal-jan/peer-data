{"id": "1708.06073", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2017", "title": "The Microsoft 2017 Conversational Speech Recognition System", "abstract": "we describe these preview version of microsoft's premier speech recognition kit, in which players update our 2016 system with recent developments in neural - network - based acoustic and language modeling to radically advance the state of the art on the entire speech recognition task. the system adds a cnn - blstm voting environment to the mix of model architectures we combined previously, and includes character - based and dialog culturally aware lstm language models following rescoring. for system combination we adopt a two - piece approach, whereby subsets of acoustic models are first combined at the line / frame level, followed by a word - level voting via confusion networks. valve also added a confusion network rescoring step after system combination. the resulting system yields a 5. 1 \\ % word error rate on the outgoing switchboard evaluation set.", "histories": [["v1", "Mon, 21 Aug 2017 03:17:23 GMT  (49kb,D)", "http://arxiv.org/abs/1708.06073v1", null], ["v2", "Thu, 24 Aug 2017 23:30:37 GMT  (50kb,D)", "http://arxiv.org/abs/1708.06073v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["w xiong", "l wu", "f alleva", "j droppo", "x huang", "a stolcke"], "accepted": false, "id": "1708.06073"}, "pdf": {"name": "1708.06073.pdf", "metadata": {"source": "CRF", "title": "THE MICROSOFT 2017 CONVERSATIONAL SPEECH RECOGNITION SYSTEM", "authors": ["W. Xiong", "L. Wu", "F. Alleva", "J. Droppo", "X. Huang", "A. Stolcke"], "emails": [], "sections": [{"heading": null, "text": "1. INTRODUCTION\nWe have witnessed steady progress in the improvement of automatic speech recognition (ASR) systems for conversational speech, a genre that was once considered among the hardest in the speech recognition community due to its unconstrained nature and intrinsic variability [1]. The combination of deep networks and efficient training methods with older neural modeling concepts [2, 3, 4, 5, 6, 7, 8] have produced steady advances in both acoustic modeling [9, 10, 11, 12, 13, 14, 15] and language modeling [16, 17, 18, 19]. These systems typically use deep convolutional neural network (CNN) architectures in acoustic modeling, and multi-layered recurrent networks with gated memory (long-short-term memory, LSTM [8]) models for both acoustic and language modeling, driving the word error rate on the benchmark Switchboard corpus [20] down from its mid-2000s plateau of around 15% to well below 10%. We can attribute this progress to the neural models\u2019 ability to learn regularities over a wide acoustic context in both time and frequency dimensions, and, in the case of language models, to condition on unlimited histories and learn representations of functional word similarity [21, 22]. Given these developments, we carried out an experiment last year, to measure the accuracy of a state-of-the-art\nconversational speech recognition system against that of professional transcribers. We were trying to answer the question whether machines had effectively caught up with humans in this, originally very challenging, speech recognition task. To measure human error on this task, we submitted the Switchboard evaluation data to our standard conversational speech transcription vendor pipeline (who was left blind to the experiment), postprocessed the output to remove text normalization discrepancies, and then applied the NIST scoring protocol. The resulting human word error was 5.9%, not statistically different from the 5.8% error rate achieved by our ASR system [23]. In a follow-up study [24], we found that qualitatively, too, the human and machine transcriptions were remarkably similar: the same short function words account for most of the errors, the same speakers tend to be easy or hard to transcribe, and it is difficult for human subjects to tell whether an errorful transcript was produced by a human or ASR. Meanwhile, another research group carried out their own measurement of human transcription error [25], while multiple groups reported further improvements in ASR performance [25, 26]. The IBM human transcription study employed a more involved transcription process with more listening passes, a pool of transcribers, and access to the conversational context of each utterance, yielding a human error rate of 5.1%. Together with a prior study by LDC [27], we can conclude that human performance, unsurprisingly, falls within a range depending on the level of effort expended. In this paper we describe a new iteration in the development of our system, pushing well past the 5.9% benchmark we measured previously. The overall gain comes from a combination of smaller improvements in all components of the recognition system. We added an additional acoustic model architecture, a CNN-BLSTM, to our system. Language modeling was improved with an additional utterance-level LSTM based on characters instead of words, as well as a dialog sessionbased LSTM that uses the entire preceding conversation as history. Our system combination approach was refined by combining predictions from multiple acoustic models at both the senone/frame and word levels. Finally, we added an LM rescoring step after confusion network creation, bringing us to an overall error rate of 5.1%, thus surpassing the human accuracy level we had measured previously. The remainder ar X iv :1 70 8. 06 07 3v 1 [ cs .C L ] 2 1 A ug 2\n01 7\nof the paper describes each of these enhancements in turn, followed by overall results.\n2. ACOUSTIC MODELS"}, {"heading": "2.1. Convolutional Neural Nets", "text": "We used two types of CNN model architectures: ResNet and LACE (VGG, a third architecture used in our previous system, was dropped). The residual-network (ResNet) architecture [28] is a standard CNN with added highway connections [29], i.e., a linear transform of each layer\u2019s input to the layer\u2019s output [29, 30]. We apply batch normalization [31] before computing rectified linear unit (ReLU) activations. The LACE (layer-wise context expansion with attention) model is a modified CNN architecture [32]. LACE, first proposed in [32] and depicted in Figure 1, is a variant of time-delay neural network (TDNN) [4] in which each higher layer is a weighted sum of nonlinear transformations of a window of lower layer frames. Lower layers focus on extracting simple local patterns while higher layers extract complex patterns that cover broader contexts. Since not all frames in a window carry the same importance, a learned attention mask is applied, shown as the \u201celement-wise matrix product\u201d in Figure 1. The LACE model thus differs from the earlier TDNN models [4, 33] in this attention masking, as well as the ResNet-like linear passthrough connections. As shown in the diagram, the model is composed of four blocks, each with the same architecture. Each block starts with a convolution layer with stride two, which sub-samples the input and increases the number\nof channels. This layer is followed by four ReLU convolution layers with jump-links similar to those used in ResNet. As for ResNet, batch normalization [31] is used between layers."}, {"heading": "2.2. Bidirectional LSTM", "text": "For our LSTM-based acoustic models we use a bidirectional architecture (BLSTM) [34] without frame-skipping [11]. The core model structure is the LSTM defined in [10]. We found that using networks with more than six layers did not improve the word error rate on the development set, and chose 512 hidden units, per direction, per layer; this gave a reasonable trade-off between training time and final model accuracy. BLSTM performance was significantly enhanced using a spatial smoothing technique, first described in [23]. Briefly, a two-dimensional topology is imposed on each layer, and activation patterns in which neighboring units are correlated are rewarded."}, {"heading": "2.3. CNN-BLSTM", "text": "A new addition to our system this year is a CNN-BLSTM model inspired by [35]. Unlike the original BLSTM model, we included the context of each time point as an input feature in the model. The context windows was [\u22123, 3], so the input feature has size 40x7xt, with zero-padding in the frequency dimension, but not in the time dimension. We first apply three convolutional layers on the features at time t, and then apply six BLSTM layers to the resulting time sequence, similar to structure of our pure BLSTM model. Table 1 compares the layer structure and parameters of the two pure CNN architectures, as well as the CNN-BLSTM."}, {"heading": "2.4. Senone Set Diversity", "text": "One standard element of state-of-the-art ASR systems is the combination of multiple acoustic models. Assuming these models are diverse, i.e., make errors that are not perfectly correlated, an averaging or voting combination of these models should reduce error. In the past we have relied mainly on different model architectures to produce diverse acoustic models. However, results in [23] for multiple BLSTM models showed that diversity can also be achieved using different sets of senones (clustered subphonetic units). Therefore, we have now adopted a variety of senone sets for all model architectures. Senone sets differ by clustering detail (9k versus 27k senones), as well as two slightly different phone sets and corresponding dictionaries. The standard version is based on the CMU dictionary and phone set (without stress, but including a schwa phone). An alternate dictionary adds specialized vowel and nasal phones used exclusively for filled pauses and backchannel words, inspired by [36]. Combined with set sizes, this gives us a total of four distinct senone sets."}, {"heading": "2.5. Speaker Adaptation", "text": "Speaker adaptive modeling in our system is based on conditioning the network on an i-vector [37] characterization of each speaker [38, 39]. A 100-dimensional i-vector is generated for each conversation side (channel A or B of the audio file, i.e., all the speech coming from the same speaker). For the BLSTM systems, the conversation-side i-vector vs is appended to each frame of input. For convolutional networks, this approach is inappropriate because we do not expect to see spatially contiguous patterns in the input. Instead, for the CNNs, we add a learnable weight matrix W l to each layer, and add W lvs to the activation of the layer before the nonlinearity. Thus, in the CNN, the i-vector essentially serves as an speaker-dependent bias to each layer. For results showing the effectiveness of i-vector adaptation on our models, see [40]."}, {"heading": "2.6. Sequence training", "text": "All our models are sequence-trained using maximum mutual information (MMI) as the discriminative objective function. Based on the approaches of [41] and [42], the denominator graph is a full trigram LM over phones and senones. The forward-backward computations are cast as matrix operations, and can therefore be carried out efficiently on GPUs without requiring a lattice approximation of the search space. For details of our implementation and empirical evaluation relative to cross-entropy trained models, see [40]."}, {"heading": "2.7. Frame-level model combination", "text": "In our new system we added frame-level combination of senone posteriors from multiple acoustic models. Such a combination of neural acoustic models is effectively just another, albeit more complex, neural model. Frame-level model combination is constrained by the fact that the underlying senone sets must be identical. Table 2 shows the error rates achieved by various senone set, model architectures, and frame-level combination of multiple architectures. The results are based on N-gram language models, and all combinations are equal-weighted.\n3. LANGUAGE MODELS"}, {"heading": "3.1. Vocabulary size", "text": "In the past we had used a relatively small vocabulary of 30,500 words drawn only from in-domain (Switchboard and Fisher corpus) training data. While this yields an out-ofvocabulary (OOV) rate well below 1%, our error rates have reached levels where even small absolute reductions in OOVs could potentially have a significant impact on overall accuracy. We supplemented the in-domain vocabulary with the most frequent words in the out-of-domain sources also used for language model training: the LDC Broadcast News corpus and the UW Conversational Web corpus. Boosting the vocabulary size to 165k reduced the OOV rate (excluding word fragments) on the eval2002 devset from 0.29% to 0.06%. Devset error rate (using the 9k-senones BLSTM+ResNet+LACE acoustic models, see Table 2) dropped from 9.90% to 9.78%."}, {"heading": "3.2. LSTM-LM rescoring", "text": "For each acoustic model our system decodes with a slightly pruned 4-gram LM and generates lattices. These are then rescored with the full 4-gram LM to generate 500-best lists. The N-best lists in turn are then rescored with LSTM-LMs. Following promising results by other researchers [43, 19], we had already adopted LSTM-LMs in our previous system, with a few enhancements [23]:\n\u2022 Interpolation of models based on one-hot word encodings (with embedding layer) and another model using letter-trigram word encoding (without extra embedding layer).\n\u2022 Log-linear combination of forward- and backwardrunning models.\n\u2022 Pretraining on the large out-of-domain UW Web corpus (without learning rate adjustment), followed by final training on in-domain data only, with learning rate adjustment schedule.\n\u2022 Improved convergence through a variation of selfstabilization [44], in which each output vector x of nonlinearities are scaled by 14 ln(1 + e\n4\u03b2), where a \u03b2 is a scalar that is learned for each output. This has a similar effect as the scale of the well-known batch normalization technique [31], but can be used in recurrent loops.\n\u2022 Data-driven learning of the penalty to assign to words that occur in the decoder LM but not in the LSTM-LM\nAlso, for word-encoded LSTM-LMs, we use the approach from [45] to tie the input embedding and output embedding together. In our updated system, we add the following additional utterance-scoped LSTM-LM variants:\n\u2022 A character-based LSTM-LM\n\u2022 A letter-trigram word-based LSTM-LM using a variant version of text normalization\n\u2022 A letter-trigram word-based LSTM-LM using a subset of the full in-domain training corpus (a result of holding out a portion of training data for perplexity tuning)\nAll LSTM-LMs with word-level input use three 1000- dimensional hidden layers. The word embedding layer for the word-based is also of size 1000, and the letter-trigram encoding has size 7190 (the number of unique trigrams). The character-level LSTM-LM uses two 1000-dimensional hidden layers, on top of a 300-dimensional embedding layer. As before, we build forward and backward running versions of these models, and combine them additively in the log-probability space, using equal weights. Unlike before, we combine the different LSTM-architectures via log-linear combination in the rescoring stage, rather than via linear interpolation at the word level. The new approach is more convenient when the relative weighting of a large number of models needs to be optimized, and the optimization happens jointly\nwith the other knowledge sources, such as the acoustic and pronunciation model scores. Table 3 shows perplexities of the various LSTM language models on dev and test sets. The forward and backward versions have very similar perplexities, justifying tying their weights in the eventual score weighting. There are differences between the various input encodings, but they are small, on the order of 2-4% relative."}, {"heading": "3.3. Dialog session-based modeling", "text": "The task at hand is not just to recognize isolated utterances but entire conversations. We are already exploiting global conversation-level consistency via speaker adaptation, by extracting i-vector from all the speech on one side of the conversation, as described earlier. It stands to reason that the language model could also benefit from information beyond the current utterance, in two ways: first conversations exhibit global coherence, especially in terms of conversation topic, and especially since Switchboard conversations are nominally on a pre-defined topic. There is a large body of work on adaptation of language models to topics and otherwise [46], and recurrent neural network model extensions have been proposed to condition on global context [17]. A second set of conversation-level phenomena operate beyond the utterance, but more locally. The first words in the current utterance could be better predicted knowing the immediately preceding words from the previous utterance, as well as information about whether a speaker change occurred [47]. We can also include in the history information on whether utterances overlap, since overlap is partially predicted by the words spoken [48]. This type of conditioning information could help model dialog behaviors such as floor grabbing, back-channeling, and collaborative completions. In order to capture both global and local context for modeling the current utterance, we train session-based LSTM-LMs. We serialize the utterances in a conversation based on their onset times (using the waveform cut points as approximate utterance onset and end times). We then string the words from both speakers together to predict the following word at each position, as depicted in Figure 2.\nFig. 2. Use of conversation-level context in session-based LM\nOptionally, extra bits in the input are used to encode whether a speaker change occurred, or whether the current utterance overlaps in time with the previous one. When evaluating the session-based LMs on speech test data, we use the 1-best hypotheses from the N-best generation step (which uses only an N-gram LM) as a stand-in for the conversation history. Table 4 shows the effect of session-level modeling and of these optional elements on the model perplexity. There is a large perplexity reduction of 21% by conditioning on the previous word context, with smaller incremental reductions from adding speaker change and overlap information. The table also compares the word error rate with the full session-based model to the baseline, within-utterance LSTM-LM. As shown in the last row of the table, some of the perplexity gain over the baseline is negated by the use of 1-best recognition output for the conversation history. However, the perplexity degrades by only 7-8% relative due to the noisy history. For inclusion in the overall system, we built letter-trigram and a word-based versions of the session-based LSTM (in both directions). Both session-based LM scores are added to the utterance-based LSTM-LMs described earlier for log-linear combination.\n4. DATA\nThe datasets used for system training are unchanged [23]; they consist of the public and shared data sets used in the DARPA research community. Acoustic training used the English CTS (Switchboard and Fisher) corpora. Language model training, in addition, used the English CallHome transcripts, the BBN Switchboard-2 transcripts, the LDC Hub4 (Broadcast News) corpus, and the UW conversational web\ncorpus [49]. Evaluation is carried out on the NIST 2000 CTS test set Switchboard portion. The Switchboard-1 and Switchboard-2 portions of the NIST 2002 CTS test set were used for tuning and development.\n5. SYSTEM COMBINATION AND RESULTS"}, {"heading": "5.1. Confusion network combination", "text": "After rescoring all system outputs with all language models, we combine all scores log-linearly and normalize to estimate utterance-level posterior probabilities. All N-best outputs for the same utterance are then concatenated and merged into a single word confusion network (CN), using the SRILM nbestrover tool [50, 36]."}, {"heading": "5.2. System Selection", "text": "Unlike in our previous system, we do not apply estimated, system-level weights to the posterior probabilities estimated from the N-best hypotheses. All systems have equal weight upon combination. This simplification allows us to perform a brute-force search over all possible subsets of systems, picking the ones that give the lowest word error on the devlopement set. We started with 9 of our best individual systems, and eliminated two, leaving a combination of 7 systems,"}, {"heading": "5.3. Confusion network rescoring and backchannel modeling", "text": "As a final processing step, we generate new N-best lists from the confusion networks resulting from system combination. These are once more rescored using the N-gram LM, a subset of the utterance-level LSTM-LMs, and one additional knowledge source. The word log posteriors from the confusion network take the place of the acoustic model scores in this final rescoring step. The additional knowledge source at this stage was motivated by our analysis of differences between machine versus human transcription errors [24]. We found that the major machine-specific error pattern is a misrecognition of filled pauses (\u2018uh\u2019, \u2018um\u2019) as backchannel acknowledgments (\u2019uh-huh\u2019, \u2018mhm\u2019). In order to have the system learn a correction for this problem, we provide the number of backchannel tokens in a hypotheses as a pseudo-score and allow the score weight optimization to find a penalty for it. (Indeed, a negative weight is learned for the backchannel count.) Table 5 compares the individual systems that were selected for combination, before and after rescoring with LSTM-LMs. and then shows the progression of results in the final processing stages, starting with the LM-rescored individual systems, the system combination, and the CN rescoring. The collection of LSTM-LMs (which includes the session-based LMs) give a very consistent 22 to 25% relative error reducation on individual systems, compared to the N-gram LM. The system\ncombination reduces error by 4% relative over the best individual systems, and the CN rescoring improves another 2-3% relative.\n6. CONCLUSIONS AND FUTURE WORK\nWe have described the latest iteration of our conversational speech recognition system. The acoustic model was enhanced by adding a CNN-BLSTM system, and the more systematic use of a variety of senone sets, to benefit later system combination. We also switched to combining different model architectures first at the senone/frame level, resulting in several acoustic combined systems that are then fed into the confusion-network-based combination at the word level. The language model was updated with larger vocabulary (lowering the OOV rate by about 0.2% absolute), additional LSTM-LM variants for rescoring, and most importantly, session-level LSTM-LM that can model global and local coherence between utterances, as well as dialog phenomena. The session-level model gives over 20% relative perplexity reduction. Finally, we introduce a confusion network rescoring step with special treatment for backchannels (based on a prior error analysis), that gives a small additional gain after systems are combined. Overall, we have reduced error rate for the Switchboard tasks by 12% relative, from 5.8% for the 2016 system, to now 5.1%. We note that this level of error is on par with the multi-transcriber error rate previously reported on the same task. Future work we plan from here includes a more thorough evaluation, including on the CallHome genre of speech. We also want to gain a better understanding of the linguistic phenomena captured by sessionlevel language model, and reexamine the differences between human transcriber and machine errors. Acknowledgments. We wish to thank our colleagues Hakan Erdogan, Jinyu Li, Frank Seide, Mike Seltzer, and Takuya Yoshioka for their valued input during system development.\n7. REFERENCES\n[1] S. Greenberg, J. Hollenback, and D. Ellis, \u201cInsights into spoken language gleaned from phonetic transcription of the Switchboard corpus\u201d, in Proc. ICSLP, 1996.\n[2] F. J. Pineda, \u201cGeneralization of back-propagation to recurrent neural networks\u201d, Physical Review Letters, vol. 59, pp. 2229, 1987.\n[3] R. J. Williams and D. Zipser, \u201cA learning algorithm for continually running fully recurrent neural networks\u201d, Neural Computation, vol. 1, pp. 270\u2013280, 1989.\n[4] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang, \u201cPhoneme recognition using time-delay neural networks\u201d, IEEE Trans. Acoustics, Speech, and Signal Processing, vol. 37, pp. 328\u2013339, 1989.\n[6] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, \u201cBackpropagation applied to handwritten zip code recognition\u201d, Neural computation, vol. 1, pp. 541\u2013551, 1989.\n[7] T. Robinson and F. Fallside, \u201cA recurrent error propagation network speech recognition system\u201d, Computer Speech & Language, vol. 5, pp. 259\u2013274, 1991.\n[8] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory\u201d, Neural Computation, vol. 9, pp. 1735\u20131780, 1997.\n[9] F. Seide, G. Li, and D. Yu, \u201cConversational speech transcription using context-dependent deep neural networks\u201d, in Proc. Interspeech, pp. 437\u2013440, 2011.\n[10] H. Sak, A. W. Senior, and F. Beaufays, \u201cLong shortterm memory recurrent neural network architectures for large scale acoustic modeling\u201d, in Proc. Interspeech, pp. 338\u2013342, 2014.\n[11] H. Sak, A. Senior, K. Rao, and F. Beaufays, \u201cFast and accurate recurrent neural network acoustic models for speech recognition\u201d, in Proc. Interspeech, pp. 1468\u2013 1472, 2015.\n[12] G. Saon, H.-K. J. Kuo, S. Rennie, and M. Picheny, \u201cThe IBM 2015 English conversational telephone speech recognition system\u201d, in Proc. Interspeech, pp. 3140\u2013 3144, 2015.\n[13] T. Sercu, C. Puhrsch, B. Kingsbury, and Y. LeCun, \u201cVery deep multilingual convolutional neural networks\n[14] M. Bi, Y. Qian, and K. Yu, \u201cVery deep convolutional neural networks for LVCSR\u201d, in Proc. Interspeech, pp. 3259\u20133263, 2015.\n[15] Y. Qian, M. Bi, T. Tan, and K. Yu, \u201cVery deep convolutional neural networks for noise robust speech recognition\u201d, IEEE/ACM Trans. Audio, Speech, and Language Processing, vol. 24, pp. 2263\u20132276, Aug. 2016.\n[16] T. Mikolov, M. Karafia\u0301t, L. Burget, J. Cernocky\u0300, and S. Khudanpur, \u201cRecurrent neural network based language model\u201d, in Proc. Interspeech, pp. 1045\u20131048, 2010.\n[17] T. Mikolov and G. Zweig, \u201cContext dependent recurrent neural network language model\u201d, in Proc. Interspeech, pp. 901\u2013904, 2012.\n[18] M. Sundermeyer, R. Schlu\u0308ter, and H. Ney, \u201cLSTM neural networks for language modeling\u201d, in Proc. Interspeech, pp. 194\u2013197, 2012.\n[19] I. Medennikov, A. Prudnikov, and A. Zatvornitskiy, \u201cImproving English conversational telephone speech recognition\u201d, in Proc. Interspeech, pp. 2\u20136, 2016.\n[20] J. J. Godfrey, E. C. Holliman, and J. McDaniel, \u201cSwitchboard: Telephone speech corpus for research and development\u201d, in Proc. IEEE ICASSP, vol. 1, pp. 517\u2013520. IEEE, 1992.\n[21] Y. Bengio, H. Schwenk, J.-S. Sene\u0301cal, F. Morin, and J.L. Gauvain, \u201cNeural probabilistic language models\u201d, in Studies in Fuzziness and Soft Computing, vol. 194, pp. 137\u2013186. 2006.\n[22] T. Mikolov, W.-t. Yih, and G. Zweig, \u201cLinguistic regularities in continuous space word representations\u201d, in HLT-NAACL, vol. 13, pp. 746\u2013751, 2013.\n[23] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig, \u201cAchieving human parity in conversational speech recognition\u201d, Technical Report MSR-TR-2016-71, Microsoft Research, Oct. 2016, https://arxiv.org/abs/1610.05256.\n[24] A. Stolcke and J. Droppo, \u201cComparing human and machine errors in conversational speech transcription\u201d, in Proc. Interspeech, pp. 137\u2013141, Stockholm, Aug. 2017.\n[25] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, X. Cui, B. Ramabhadran, M. Picheny, L.-L. Lim, B. Roomi, and P. Hall, \u201cEnglish conversational telephone speech recognition by humans and machines\u201d, in Proc. Interspeech, pp. 132\u2013136, Stockholm, Aug. 2017.\n[26] K. J. Han, S. Hahm, B.-H. Kim, J. Kim, and I. Lane, \u201cDeep learning-based telephony speech recognition in the wild\u201d, in Proc. Interspeech, pp. 1323\u20131327, Stockholm, Aug. 2017.\n[27] M. L. Glenn, S. Strassel, H. Lee, K. Maeda, R. Zakhary, and X. Li, \u201cTranscription methods for consistency, volume and efficiency\u201d, in Proc. 7th Intl. Conf. on Language Resources and Evaluation, pp. 2915\u20132920, Valleta, Malta, 2010.\n[28] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition\u201d, arXiv preprint arXiv:1512.03385, 2015.\n[29] R. K. Srivastava, K. Greff, and J. Schmidhuber, \u201cHighway networks\u201d, CoRR, vol. abs/1505.00387, 2015.\n[30] P. Ghahremani, J. Droppo, and M. L. Seltzer, \u201cLinearly augmented deep neural network\u201d, in Proc. IEEE ICASSP, pp. 5085\u20135089. IEEE, 2016.\n[31] S. Ioffe and C. Szegedy, \u201cBatch normalization: Accelerating deep network training by reducing internal covariate shift\u201d, Proceedings of Machine Learning Research, vol. 37, pp. 448\u2013456, 2015.\n[32] D. Yu, W. Xiong, J. Droppo, A. Stolcke, G. Ye, J. Li, and G. Zweig, \u201cDeep convolutional neural networks with layer-wise context expansion and attention\u201d, in Proc. Interspeech, pp. 17\u201321, 2016.\n[33] A. Waibel, H. Sawai, and K. Shikano, \u201cConsonant recognition by modular construction of large phonemic time-delay neural networks\u201d, in Proc. IEEE ICASSP, pp. 112\u2013115. IEEE, 1989.\n[34] A. Graves and J. Schmidhuber, \u201cFramewise phoneme classification with bidirectional LSTM and other neural network architectures\u201d, Neural Networks, vol. 18, pp. 602\u2013610, 2005.\n[35] T. N. Sainath, O. Vinyals, A. Senior, and H. Sak, \u201cConvolutional, long short-term memory, fully connected deep neural networks\u201d, in Proc. IEEE ICASSP, 2015.\n[36] A. Stolcke et al., \u201cThe SRI March 2000 Hub-5 conversational speech transcription system\u201d, in Proceedings NIST Speech Transcription Workshop, College Park, MD, May 2000.\n[37] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, \u201cFront-end factor analysis for speaker verification\u201d, IEEE Trans. Audio, Speech, and Language Processing, vol. 19, pp. 788\u2013798, 2011.\n[38] G. Saon, H. Soltau, D. Nahamoo, and M. Picheny, \u201cSpeaker adaptation of neural network acoustic models using i-vectors\u201d, in IEEE Speech Recognition and Understanding Workshop, pp. 55\u201359, 2013.\n[39] G. Saon, T. Sercu, S. J. Rennie, and H. J. Kuo, \u201cThe IBM 2016 English conversational telephone speech recognition system\u201d, in Proc. Interspeech, pp. 7\u201311, Sep. 2016.\n[40] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig, \u201cThe Microsoft 2016 conversational speech recognition system\u201d, in Proc. IEEE ICASSP, pp. 5255\u20135259, 2017.\n[41] S. F. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon, H. Soltau, and G. Zweig, \u201cAdvances in speech transcription at IBM under the DARPA EARS program\u201d, IEEE Trans. Audio, Speech, and Language Processing, vol. 14, pp. 1596\u20131608, 2006.\n[42] D. Povey, V. Peddinti, D. Galvez, P. Ghahrmani, V. Manohar, X. Na, Y. Wang, and S. Khudanpur, \u201cPurely sequence-trained neural networks for ASR based on lattice-free MMI\u201d, in Proc. Interspeech, pp. 2751\u20132755, 2016.\n[43] M. Sundermeyer, H. Ney, and R. Schlu\u0308ter, \u201cFrom feedforward to recurrent LSTM neural networks for language modeling\u201d, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, pp. 517\u2013 529, Mar. 2015.\n[44] P. Ghahremani and J. Droppo, \u201cSelf-stabilized deep neural network\u201d, in Proc. IEEE ICASSP, pp. 5450\u20135454. IEEE, 2016.\n[45] O. Press and L. Wolf, \u201cUsing the output embedding to improve language models\u201d, arXiv preprint arXiv:1608.05859, 2016.\n[46] J. R. Bellegarda, \u201cStatistical language model adaptation: review and perspectives\u201d, Speech Communication, vol. 42, pp. 93\u2013108, 2004.\n[47] G. Ji and J. Bilmes, \u201cMulti-speaker language modeling\u201d, in S. Dumais, D. Marcu, and S. Roukos, editors, Proceedings of HLT-NAACL 2004, Conference of the North American Chapter of the Association of Computational Linguistics, vol. Short Papers, pp. 133\u2013136, Boston, May 2004. Association for Computational Linguistics.\n[48] E. Shriberg, A. Stolcke, and D. Baron, \u201cObservations on overlap: Findings and implications for automatic processing of multi-party conversation\u201d, in P. Dalsgaard, B. Lindberg, H. Benner, and Z. Tan, editors, Proceedings of the 7th European Conference on Speech Communication and Technology, vol. 2, pp. 1359\u20131362, Aalborg, Denmark, Sep. 2001.\n[49] I. Bulyko, M. Ostendorf, and A. Stolcke, \u201cGetting more mileage from web text sources for conversational speech language modeling using class-dependent mixtures\u201d, in M. Hearst and M. Ostendorf, editors, Proceedings of HLT-NAACL 2003, Conference of the North American Chapter of the Association of Computational Linguistics, vol. 2, pp. 7\u20139, Edmonton, Alberta, Canada, Mar. 2003. Association for Computational Linguistics.\n[50] A. Stolcke, \u201cSRILM\u2014an extensible language modeling toolkit\u201d, in Proc. Interspeech, pp. 2002\u20132005, 2002."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We describe the 2017 version of Microsoft\u2019s conversational<lb>speech recognition system, in which we update our 2016<lb>system with recent developments in neural-network-based<lb>acoustic and language modeling to further advance the state<lb>of the art on the Switchboard speech recognition task. The<lb>system adds a CNN-BLSTM acoustic model to the set of<lb>model architectures we combined previously, and includes<lb>character-based and dialog session aware LSTM language<lb>models in rescoring. For system combination we adopt a two-<lb>stage approach, whereby subsets of acoustic models are first<lb>combined at the senone/frame level, followed by a word-level<lb>voting via confusion networks. We also added a confusion<lb>network rescoring step after system combination. The result-<lb>ing system yields a 5.1% word error rate on the 2000 Switch-<lb>board evaluation set.", "creator": "LaTeX with hyperref package"}}}