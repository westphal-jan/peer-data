{"id": "1709.00348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2017", "title": "Inferring Networked Device Categories from Low-Level Activity Indicators", "abstract": "we study the problem of inferring the type of a networked device in a commodity network by calculating low level traffic activity indicators seen at commodity home gateways. we analyze a dataset of detailed device network activity obtained from 240 subscriber homes of a large retailer isp and extract a number of traffic and spatial fingerprints for defined devices. we develop a two level metric to describe devices by which we map individual devices using a number of heuristics. we produce the heuristically allocated labels then train classifiers because distinguish service classes based mostly the traffic and spatial fingerprints of a device. our comparisons show an accuracy level up to 91 % for the thread level category and up to 84 % for the fine grained category. by incorporating information efficiently distributed sources ( e. g., mac network ), we are able to further improve accuracy score above 97 % and 92 %, though. finally, we also extract a set of simple and human - readable rules toward concisely capture all behaviour of these distinct device categories.", "histories": [["v1", "Fri, 1 Sep 2017 14:47:23 GMT  (383kb,D)", "http://arxiv.org/abs/1709.00348v1", "14 pages, 9 figures, 7 tables"]], "COMMENTS": "14 pages, 9 figures, 7 tables", "reviews": [], "SUBJECTS": "cs.NI cs.AI", "authors": ["kyumars sheykh esmaili", "jaideep chandrashekar", "pascal le guyadec"], "accepted": false, "id": "1709.00348"}, "pdf": {"name": "1709.00348.pdf", "metadata": {"source": "CRF", "title": "Inferring Networked Device Categories from Low-Level Activity Indicators", "authors": ["Kyumars Sheykh Esmaili", "Jaideep Chandrashekar", "Pascal Le Guyadec"], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "To a home Internet gateway (and the ISP operating the gateway), devices on a home network are represented by a MAC address (and perhaps a hostname). Associating a higher level semantic to a device (smartphone, game console, smart meter) to devices bene ts a wide range of scenarios. For example, this capability will allow a ISP help-desk operator to quickly identify the particular device (with MAC address) that a customer is complaining about; an ISP could better target particular broadband or other service o erings to subscribers by pro ling the devices used; an ISP subscriber can set-up tra c policies for their home network more expressively (prioritize tra c from game consoles), and so on. Such a capability is particularly relevant today given the proliferation of networked devices that are growing the Internet-of-Things (weighing scales, smart meters, thermostats, etc.). In this paper, we investigate device classi cation from the point of view of a home Internet gateway (and the ISP). To this end, we analyze a large dataset of device activity, extract a set of features that allow us to di erentiate between di erent types of\ndevices (and to di erent extents) and apply known classi cation techniques to map individual devices to device categories. While there are a number of di erent approaches to identifying device types, the vantage point (the home gateway) and the interested entity (the ISP) present a number of challenges and restrict the space of solutions. A method that relies on packet inspection could reliably uncover the device type (e.g., by examining HTTP U/A strings). However, deep packet inspection is not viable on most of today's resource constrained home gateways. While ISPs could easily implement such a function upstream, they are very reluctant to do so due to consumer privacy concerns [5]. Yet another method leverages the fact that user's often assign descriptive names to their devices (e.g., John-iPhone) which can be parsed to infer the device type. Unfortunately, relatively few devices in the home involve the con guration step (typically personal devices) and the majority have default strings which reveal little about the nature of the device (we present data to support this later in the paper). Finally, one may look to exploit information in the MAC OUI (which identi es the device manufacturer). However, the discriminative power of the MAC OUI depends greatly on the product range of a manufacturer and the incidence of a manufacturer in a particular dataset. We explore this issue in detail in a later section. In this paper, we develop a systematic methodology for accurately classifying devices based on their behavior while on the home network. Our methodology is based on measurements and metrics that are already available today in commodity gateways deployed by all ISPs. The underlying intuition is that the devices of a certain class say, tablets are likely to be used by their owners in similar ways and we expect their long term behavior to be more similar to each other than devices from a di erent category, for instance laptops. For example network extenders (which aggregate several devices behind them) are likely to have higher tra c volumes than mobile devices or peripheral devices (e.g., printers). Based on this intuition, we extract behavioral signatures that capture a device's behavior along several\nar X\niv :1\n70 9.\n00 34\n8v 1\n[ cs\n.N I]\n1 S\nep 2\n01 7\ndimensions: how much tra c it generates, how much it moves, what is its daily usage? The signatures that we construct for a device leverage common wisdom captured in previous work; we also develop a number of novel behavior traits that are shown to have good discriminative power. We develop a two level taxonomy of device categories a coarse grained device class and a ner grained device type based on an informal survey of home network users. We then cast the problem of identifying the device categor(ies) as a multi-class classi cation problem and present the results from applying two well known classi cation methods Decision Trees and Support Vector Machines. While our dataset contains both wired and wireless devices, the work is this paper mainly focuses on wireless devices which constitute the majority of the device population. However, our methodology is general and may be applied across the wider set of devices. The key contributions of this paper are summarized as follows:\n1. We explore in detail a large dataset of home network activity taken from over 240 subscribers of an ISP for a period of 1 year and which contains observations of over 5000 devices. We then identify and extract a number of features, a number of which are identi ed based on insights obtained from the data exploration, which capture tra c and spatial behavior of individual devices. 2. We develop a two level taxonomy of common home network devices that associates a device with a coarse grained device class and a ner grained device type. We then use a number of simple heuristics over the static descriptors for a device to map each device onto the two categories. We are able to obtain coarse grained labels for 62% of the devices and ne-grained labels for 36% of all devices in our dataset. 3. We study the performance of two well established classi cation techniques Decision trees and SVM to infer the labels for 1878 wirelessly connected devices. We nd that the SVM based classi er provides the best classi cation accuracy and is 90.47% accurate on coarse grained labels (83.11% for ne grained labels). Furthermore, we explore the impact of incorporating other information such as the MAC OUI and we show an additional improvement of roughly 8-10% for the ne grained labels, and 6% for the coarse grained labels. We further examine the Decision Tree generated to undersatnd the importance of particular features that inuence the type of a device.\nThe rest of this paper is structured as follows. In 2, we brie y describe related work di erentiate them from our contributions. In 3, we describe the device dataset used in this paper and in 4, we present our device taxonomy and describe some heuristics that we apply to extract labels for a subset of the devices. We discuss feature extraction in detail in 5 and explain the intuition behind each of features extracted from the raw\nmetrics. In 6, we introduce two classi cation methods and report on the results of applying these on our dataset(s). in 7, we conclude by discussing some open issues and extensions."}, {"heading": "2. RELATED WORK", "text": "In this paper, we study the problem of identifying the type or class of a networked device, and disambiguating between devices of di erent classes coexisting on the same home network. While we are not aware of any work in the literature that looks at this speci c problem, there has been considerable work in closely related areas Tra c & Application Inference, Device Disambiguation and Device Fingerprinting. We brie y discuss representative work in each. Tra c & Application Inference: A large body of work has addressed the problem of determining the underlying applications corresponding to tra c ows observed in the network. The high level intuition behind all of these is that the di erences in application classes is re ected in the composition of tra c and packets generated by these applications. For e.g., the graphical relationships between source(s) and destinations(s) are exploited to identify tra c classes, as is described [14]. In another approach, an initial inference is improved upon by incorporating more global information [12]. A comprehensive survey of the techniques in this area can be found in [20] and we highlight two other notable works. In contrast to all of these, our work focuses on identifying device classes. Device Disambiguation: A number of previous efforts have addressed the problem of distinguishing devices behind a rewall or NAT device. For example, Bellovin proposed a method to exploit IP header elds to identify distinct hosts sharing a single IP address [2]. In [18], the authors describe an approach relying on the IP TTL eld and HTTP user-agent strings to correctly count and distinguish devices behind a single DSL line. While these are applicable to a narrow part of our work (identifying devices behind a non-transparent network bridge, for instance), the data available to us is less semantically rich (aggregated byte counters, in our case) and thus these methods are not directly applicable. Device Fingerprinting: There are two main approaches to uniquely identifying devices (or software stacks, for that matter). In the active approach, a set of crafted packets is directed at a device/network interface and the responses are analyzed to identify particular customizations or properties of the underlying code base these constitute a signature of the entity being ngerprinted. This is the approach taken with some security scanners (e.g., Nmap [21]), browser ngerprinting tools [7], or wireless chipset [3]. In contrast, passive ngerprinting methods attempt to extract unique signatures from simply observing the device interacting with the network.\nThis approach has been applied very successfully in the past in ngerprinting wireless cards and chipsets [9,22], or even devices [16]. Our own work di ers from these in that rather than extract unique ngerprints of a particular device or chipset, we seek to extract (behavioral) signatures for a certain device type (which is greatly affected by when, where and how often a device is used). While some of the cited methods may be applicable in general, our speci c approach is dictated by the speci c nature of our dataset (which does not support any of the other methods)."}, {"heading": "3. DATASET", "text": "Our work in this paper uses a dataset of over 5789 devices collected across 240 distinct homes over a year (from 01/11/2013 to 01/10/2014). Here, we elaborate how this dataset was collected and describe the recorded metrics in detail. The 240 households captured in our dataset were subscribers of a (single) large European ISP during the period indicated and were recruited to take part in a trial deployment. Home gateways in these households were identical to all other subscribers except in one aspect: gateways in the deployment periodically queried a set of software counters and state variables on the gateway and reported these to a back-end server. The subscriber home gateways, spanning 4 distinct hardware models, were con gured and provisioned for a triple-play service (voice, broadband, IPTV) and came with 4 physical Ethernet ports and a 802.11b/g/n 2x2 WiFi access point. Importantly, the WiFi access point of every gateway in the trial was operational and in use for the entire period."}, {"heading": "3.1 Collected Raw Metrics", "text": "In the following, we enumerate and describe in detail the particular set of recorded parameters that are relevant to our work in this paper. We note that each gateway observes all of the devices that were on the home network at any point during the deployment and thus our dataset is a complete record of all the network activity across the 240 households during the period. If there are any mobile cellular devices that are not con gured to use the homeWiFi network (perhaps exclusively using the cellular network), there is no record of it in our dataset. Host Descriptors: These correspond to the static properties of individual devices that do not change over time and include the following:\n\u2022MAC Address: the 48 bit address of the network interface that is connected to the local area network. The rst three octets of the MAC address (the OUI) identies the vendor of the network interface. In most cases but not all this also corresponds to the device manufacturer.\n\u2022HostName: a descriptive string associated with a particular device, as recorded in the DHCP table of the home gateway. In the case of many personal devices computers, phones, tablets, etc., users often select descriptive, parse-able strings that can hint at the type of device (and ownership). However, in the more common case, either the device self-selects a hostname or else the gateway assigns a default hostname that incorporates the MAC address (to ensure it is unique).\n\u2022Physical Interface: describes the nature of the connectivity to the gateway and is either wired (via one of the 4 Ethernet ports on the gateway) or over WiFi.\nThe host descriptions do not change often and are reported infrequently (every hour or so). In contrast the dynamic variables and parameters that we discuss next can change very frequently and are reported more frequently by the gateway (roughly every minute). Tra c Volume Metrics: These report cumulative tra c volumes (in bytes) associated with particular (physical) gateway interfaces (for wired devices), or stations (for wireless devices). We can compute the tra c seen between samples by computing successive di erences. In the case of WiFi stations, tra c records are indexed by the MAC address of the station. We point out an important artifact that is observed in our dataset. The deployment covers 4 distinct tra c models, and in the case of all but one, the wired tra c counters use 16 bits (and wrap around even with light tra c). In these cases, we are unable to accurately extract the tra c volumes associated with the devices connected on those ports. While we do have some gateways that use 32 bit counters, they are few in number. Due to the paucity of tra c data on the wired ports, we remove wired devices from consideration and focus mainly on classifying wireless devices which are always associated with 32 bit counters. Wireless Metrics: For the wireless station that is associated with the gateway, two additional metrics are reported:\n\u2022Tra c Rates: these are reports of the actual throughput (sent and received bits per unit time) experienced by each station associated with the gateway AP. Importantly, the gateway driver represents these as integer values that are clipped to zero when the actual value is less than 1 kbps. Note that this rate accounts for low level management overhead on the wireless network (not accounted for in the tra c volume metric).\n\u2022RSSI: This captures the signal strength from the to the gateway to the station. High(er) values typically represent better coverage. Per station RSSI values are computed at the gateway based on received data frames. The actual value reported by the gateway is an average value over all received packets in the reporting interval."}, {"heading": "4. DEVICE TAXONOMY & LABELING", "text": "We polled 140 distinct subscriber households from our dataset and asked them to enumerate all the various networked devices in their home (along with the corresponding MAC address). We received results from 34 homes, which together named 137 devices. Table 2 presents a very small selection of these. The hostnames indicated in the rst column were obtained by checking the MAC addresses against the DHCP entries in the dataset. The last column re ects the user description of the device. Based on our intuition from analyzing this data, we constructed a two level taxonomy of device categories. The lower level the ne-grained class captures how most users' identify their devices. At a higher level, we also categorize the devices into coarse grained classes which model how users relate to their devices, how they use them, and how these devices behave on the network. We utilize the coarse grained labels for two reasons. First, we see a gradual blending of device functions as device form factors converge. For example, the so called \"phablet\" form factors blur the line between smartphones and tablets. Second, relying purely on the ne grained classi cation results in a very unbalanced data and poor results for under-represented devices (in our dataset). Using a two level taxonomy addresses both of these factors and this categorization is presented in Table 1. The rst row enumerates the coarse grained classes. Compute devices are essentially laptops or desktops, which are general purpose and used for a variety of tasks by users (and sometimes shared between users). Mobile Handheld devices are devices typically associated with an individual and often carried on their person (or carried around a home) and where the interaction periods are shorter (than compute devices). Network Equipment refers to devices that extend the range or functionality of a device, and typically aggregate the behavior of several other devices placed behind them. Finally, we have the Consumer Electronics (CE) category of more specialized devices, which are built or optimized for speci c tasks or to consume particular services. The second row enumerates all the ne-grained classes inside of each coarse-grained category For example smartphones, tablets and eBook Readers all come under under the high level categorization ofMobile Handheld devices. With respect to the Compute Device class, we do not di erentiate desktops and laptop computers. In general, we do not expect signi cant di erences in how people use these devices. While a laptop may be used in more than one location in a home (while a desktop generally stays in a single location); we expect they are both used the same way (as a general purpose compute device) to do di erent things (check email, browse the web, etc). Pragmatically, distinguishing these two is that extracting ground truth labels are di cult; we ex-\nplain this further in the next section. Finally, we have devices such as printers/scanners, ISP supplied set-topboxes (STB), over-the-top (OTT) devices, and media or game consoles, which are often designed to support one particular use case. We point out that this categorization is slightly arbitrary - several game consoles today support video streaming (as do OTT boxes). This does complicate the inference of the ne grained labels, the coarse grained label inference is less sensitive. We also note the ne grained categorization is not exhaustive we observe single instances of several device types in our dataset (a smartwatch, a wireless smart-meter, etc.). We ignore device types for which we observe less than 3 instances in our dataset. In the following, we describe a number of heuristics to infer the coarse and ne grained labels for a device (as identi ed by its MAC address); these heuristics are based on our intuition and a careful exploration of the dataset."}, {"heading": "4.1 Extracting \u201cGround Truth\u201d", "text": "We developed a set of heuristics against the Host Descriptors for all the devices in our dataset and this yielded a smaller set of devices for which we believe the inferred labels are accurate. We additionally veri ed the heuristics against the 137 devices for which user supplied labels and ensured that the heuristic inferred labels are consistent with that supplied by users. \u2022Name Based Heuristics: Hostnames often contain common descriptive strings, e.g., android, pc, iMac, etc., which can be leveraged to infer the device's category. Often, the descriptive strings are speci c enough to yield a ner grained categorization (e.g., john-iphone), in which case we have both a coarse label (mobile handheld) as well as a ne grained label (smartphone). Sometimes, hostnames re ect very speci c device models (e.g., wrt54g a home router manufactured by Linksys) or take on factory default names that are well known (e.g., the ISP supplied STB is named identically across all the homes it is deployed in). In such cases, we are able to infer the nature of the device.\n\u2022MAC Address Re nement: In some cases, the MAC address of a device can be helpful in its identi cation, particularly when the vendor (identi ed by the MAC OUI) manufactures has a very limited product range (that may live in a home). One example: the MAC OUI 00:00:48 is registered to Seiko Epson corporation; it is likely that devices with this pre x are printers or scanners. However, the OUI is far less useful in distinguishing devices in the case of vendors with diverse product portfolios. For instance, Apple Inc. manufactures devices that span all of the coarse grained categories in Table 1. Here, knowing that a device is manufactured by Apple does not allow us to identify the particular category. Yet another caveat in using the MAC address is that it may be incorrect (on rare\noccasion): we saw one corner case where a vendor incorrectly borrowed the mac OUI of a di erent manufacturer (this was also reported in [17]).\n\u2022BSSID based: Several printers and media bridge devices allow WiFi-direct connections for which the device temporarily advertises a network that uses the device's MAC address (or an adjacent address) as the BSSID. In some cases, the advertised SSIDs explicitly contain the device model we are able to leverage these. We are able to identify some wireless range extenders or network bridges using the same technique.\n\u2022Other heuristics: We identi ed several instances in the Host Descriptor data where a single MAC address is associated with a number of hostnames (which are unrelated), and this occurs when several (potentially!) devices are connected behind the network extender device. When this count is greater than two, we tentatively label the device as a network extender. Then, we carry out an additional check to verify if there are overlapping sessions (distinct MAC addresses associated with the same name sending/receiving tra c at the same time). If such behavior is found, we con rm that the device is a network extender. We distinguish between Powerline Ethernet connectors and WiFi Extenders by comparing the set of hostnames against an exhaustive set of models of each, and also leveraging the MAC OUI.\nFinally, we present a census of all the devices that are present in our dataset in Figure 1. We see host descriptors for 5789 distinct mac addresses. Of these, 4234 are associated with the wireless network, and the rest are wired devices. Of the 1555 wired devices, only 676 are observed on the gateways that use 32 bit tra c counters. Further, of the remaining, only 153 devices were found not to share the physical port with other devices (which is essential to correctly attributing tra c to the device). Given this very small population (relative to the whole), we omit wired devices from consideration in the rest of this paper. Moving now to the devices seen on the wireless network, we nd 1244 devices to be transient; that is, they are observed for very brief periods of time on the network- the duration being insu cient to contribute su cient metrics to aid in classi cation (this is further discussed in the data su ciency experiments in Section 6); we remove these from consideration. Finally, we are left with 2990 non-transient wireless devices for which su cient data is available. However, we\ncan establish the ground truth categorization only in 1878 instances for the coarse categories (denoted DC ), and in 1089 cases with a ne grained label (denoted DF ). Unless indicated otherwise, the results and analysis in the paper pertains to the sets DC and DF . It should be pointed out that our labeling is conservative. We also implemented a number of sanity checks, for e.g., a tablet or a smartphone cannot appear on a wired interface without an extender present, an android phone could not be manufactured by Apple, and so on). Any discordance between the inferred labels and the sanity check led to the label being rejected. By being conservative, we are unable to categorize a large number of devices in our dataset; however, we have a high degree of con dence in the extracted labels are accurate.\nIt should be clearly pointed out that the labeling that we described in this section is only to obtain training and testing data for the statistical classi cation methods (Section 6.1). It is unlikely that an ISP would use these heuristics by themselves and this for two reasons: (i) the heuristics described are unable to classify a large number of devices, and (ii) there are potential privacy concerns with an ISP being able to read hostname strings, which can sometimes contain names of people."}, {"heading": "5. EXTRACTING FEATURES", "text": "In this section, we analyze the time series' of traf-\nc and wireless metrics and subsequently construct a number of features that are suitable to input to a classi cation framework. We use the term feature class to describe a certain metric, possibly post processed, collected from the gateways and from which a distribution is constructed, and feature to refer to a speci c attribute extracted for that feature class. As an example, Daily Tra c Volume is a timeseries and a feature class, while the median daily tra c volume is a feature. In the following, we walk through each feature class and describe how they are processed. In selecting these features, we rely on a combination of common best practices and also intuition obtained from initial explorations of the dataset. The complete set of 92 features that are associated with each individual device is summarized in Table 3. Table Table 3(a) enumerates feature classes while 3(b)\nrepresents the features extracted from each class. Finally, Table 3(c) describes 4 additional features that are all extracted from the RSSI feature class."}, {"heading": "5.1 Feature Classes", "text": "We rst describe all the feature classes that are constructed from tra c (volume and rate) counters, and subsequently describe the feature classes corresponding to the RSSI data. Tra c-based Features\nDaily Tra c Volume: The amount of data (in bytes) that a device transmits (or receives) each day that it was active. Typically, we expect devices such as PCs, OTT video streamers, or network extenders (which aggregate tra c for others) to have higher values for this feature (in contrast to mobile hand-held devices). Importantly, we also use this feature to distinguish between transient and non-transient devices: a device is non-transient if it has non-zero tra c volume for at least 3 days (setting this threshold is discussed in the\nResults section) and transient otherwise1 . The rest of this paper only considers devices that are non-transient. Figure 2a depicts the median of medians of outgoing (from the gateway to the device) daily tra c for the devices within each group (the incoming tra c graphs exhibit similar patterns). It shows that PCs and Network Extenders consume the largest amount tra c per days.There is also a considerable di erence between Tablets and Smartphones.\nSession Length: The duration of time that a device was active, i.e., generating tra c, is computed as follows: sessions are initiated when the corresponding trafc counter is non-zero and terminated when there is no tra c activity for at least 15 minutes (this is the ARP timeout value). Thus, for each device, we obtain a list of sessions of the form (start time, end time). Intuitively, we expect small hand-held devices to have shorter sessions, but more of them, than xed devices such as computers and set top boxes. The result is a time series of session lengths for each device and from this, we construct a distribution. Figure 2b depicts the distribution of median session length across the various (coarse) device classes. Here, the distribution for mobile devices (smartphones, tablets) re ects shorter sessions than for devices like OTT boxes, Network Extenders, or even PCs (slightly).\nPer Session Tra c: is the cumulative tra c volume associated with each distinct session for a device. To allow for fair comparisons across di erent sessions, we normalize these volumes by the length of the session. We then construct a distribution for each device with these normalized values and extract all the previously listed statistics as individual features. Figure 2c shows the distribution of the median values for the normalized session rates.\nTra c Rate: We build a distribution over the tra c rate metrics obtained for wireless devices. Recall from Section 3.1 that this rate only accounts for user trafc (ignoring the e ect of wireless control tra c) and thus contains a large number of zeros. We nd that the median tra c rate across almost all the devices in our dataset is zero. In fact, we expect this particular 1These could, for example, be visiting guests' devices that have been connected to the home's AP.\nfact to help discriminate between device types we expect devices such as computers to have short(er) runs of zero values (generally when they're on, they are used) as compared to mobile devices, where the device can be on for long periods of time with only some minimal activities. Similarly, we expect fewer zeros associated with network extenders (which aggregate tra c from other devices).\nSpatial and Mobility Features\nThe RSSI reported for each device (as recorded by the gateway) loosely tracks the physical distance of the device from the gateway [13,15,19]. For a given network environment (and device), we expect that distances increase as the RSSI increases. Thus, as a device is used in di erent locations over time, the set of all RSSI measurements constitutes a spatial signature and the short term di erences in the RSSI captures the mobility pattern of the device inside the home. With this intuition, we extract a number of feature classes to capture this aspect of the device behavior.\nRaw RSSI: For each device, we construct a distribution of the reported values based on the timeseries of the raw RSSI values. From this, we extract all the typical distribution related metrics as we do for the tra c features (max, median, iqr, etc). Note that we include the minimum value of the distribution as a feature, since it is not uniformly zero (across devices), unlike in the tra c scenario.\nRSSI Diameter: This is computed as the di erence of the extreme values in the RSSI distribution, i.e., the max-min statistic. Note that this is not as useful for the tra c based features where the minimum values across devices are generally zero. Figure 3a shows the distribution of this feature across the coarse grained device classes. From the gure, the distribution for mobile hand held devices is shifted slightly higher than the other classes; this indicates that such devices have higher portability, i.e., can be used in a wider spatial range. RSSI M-D product: This feature is constructed\nas the product of the RSSI Diameter (previous) with the median value of the RSSI distribution. We include this feature to amplify the di erences (in the individual product terms) across the device classes.\nRSSI Allan Deviation: Speci c to the timeseries of RSSI values X = {x1, x2, . . . , xn}, this is de ned as\nAD(X) = \u221a\u221a\u221a\u221a 1 2(n\u2212 1) n\u22121\u2211\ni=1\n(xi+1 \u2212 xi)2\nAllan Deviation (or AD, for short) captures the instantaneous variation for time-indexed data and was originally designed to study frequency stability in clocked analog circuits [1]. Applied to the RSSI time series, the allan deviation captures the short time scale mobility of a device. We expect this to be high(er) for mobile hand held devices as compared to laptops or other more stationary devices. Fig. 4 shows the distribution of this feature over the di erent device classes. In Fig. 4a, we see that the distribution for mobile hand-helds is pushed upwards; this is expected, since we expect these devices to be carried by users as they move around and this is captured in short-term variations in the observed RSSI. In Fig. 4b, the distributions for smartphones and tablets are quite di erent, with the former tending to have higher AD values. Again, this is borne out by intuition; a smartphone might be in a user's pocket and connected to the network as they walk around the home.\nLocation Modes: This feature quanti es the number of modes in the rssi distribution. Intuitively, it captures the number of locations inside a home, where the device is used most. By exploring the data, we nd that mobile devices have wider distributions (re ecting usage at multiple locations and in-motion), while on the other hand static devices (desktops) have narrow RSSI distributions and single modes. To extract this feature, we developed a segmentation algorithm, inspired by the Median Shift algorithm [24] which infers the number of local maxima in a distribution, to identify the modes of the RSSI distribution. We omit a detailed description of the algorithm due to a lack of space. This metric is very intuitive and ought to clearly distinguish device types used in multiple places from those that are more xed. However, the number of locations reported is quite low across all devices (at most 3). Upon close examination, we nd a number of mobile phones to have single modes, but extremely wide distributions; we believe that natural variations and noise in\nthe RSSI result in washing out the peaks. To deal with this, we introduce the next feature.\nRSSI AuC: This feature is computed as the area under the curve of the RSSI distribution. The main intuition here is that this area captures the degree of in-motion usage. In order to allow for fair comparisons across different devices (with di erent scales), we normalize the AuC value by dividing with the absolute maximum frequency. One immediate conclusion is that the higher the AUC is the more in-motion usage the device has. Figure 5 shows the distribution of this metric across the di erent device classes. This feature complements the Allan Deviation, as it is more resilient to noise. Concretely, a few noisy measurements can have a large value on the Allan Deviation as computed, especially for devices with short RSSI time series, the impact is less pronounced on the computed AuC values."}, {"heading": "5.2 Feature Analysis", "text": "In this section we post-process the features extracted previously by applying some standard techniques. The goal here is to transform the features so as to improve their ability to discriminate between di erent classes, and thus improve the classi cation results.\n5.2.1 Feature Rescaling Many statistical models, including the classi cation\nmethods that we use in this paper, make implicit assumptions about the data being Gaussian; when this is not the case, classi cation performance may be improved by transforming them. We systematically explored the features to identify those with heavily-skewed distributions. We use the Extreme Value detector which proceeds by identifying the fraction of the population that lies outside a central range. Speci cally, it labels all the values x, such that Q1 \u2212 EV F \u2217 IQR > x or x > Q3 + EV F \u2217 IQR. Here Q1 (Q3) are the rst (third) quartiles, and IQR is the inter-quartile range, i.e. Q3\u2212Q1. If at least 1% of the values in a particular feature lies outside this interval, we consider the feature to be excessively skewed . Table 4 enumerates the 10 most skewed features extracted from our data for EVF=6 (the skewness test is fairly stable and we did not observe di erent outcomes with EVF in [3, 10]. It is worth noting that all of detected features are skewed only to the right side of the distributions. In short, the two feature families of our datasets tra c based, and rssi based behave di erently. Distributions constructed from RSSI based features have a Gaussian shape. On the other hand, distributions from tra c based features tend to have long tails. This is likely because tra c phenomenon are inherently bursty, and devices often have long inactivity periods during which no tra c is generated. Consequently, for each attribute identi ed as skewed, a logarithmic transformation is applied to rescale the data. We attach a su x of ln to the rescaled feature to di erentiate it from the original. For example, the rebalanced version of session_min is replaced with session_min_ln.\n5.2.2 Feature Reduction\nClassi cation performance is strongly related to the number of features employed. Looking over the set of features shown in Table 3, we may expect a certain level of correlation between some subsets. For example, it is very likely that the incoming tra c volume and outgoing tra c volume are likely to be highly correlated - since most client end-hosts ( rst) request content from remote servers which is subsequently delivered. To quantify the extent of redundancy in our dataset, we carry out a Principal Component Analysis (PCA) [6] over the feature space. PCA is a well known linear transformation onto a new orthogonal space which has the property of concentrating the variance inherent in the original data. In a PCA transformed space, the principal components are rank ordered by the fraction of variance (in the original dataset) accounted for. Thus, the rst principal component is associated with the largest amount of variance; each succeeding component has the highest remaining variance and under the constraint that it is orthogonal to all the previous components. Figure 6 plots the cumulative variance contribution of the component axes obtained from a PCA transform. From the gure we see that all of the variance in the feature space can be accounted for with roughly 30 components, and about 20 components can explain over 95% of the variance in the data. This indicates that roughly half of the original features in our dataset are redundant and can be removed without adversely a ecting classi er accuracy while at the same time speeding up the classi ers. However, one of the drawbacks of PCA is that the transformed space (where variance is concentrated along orthogonal axes) is not intuitive and does not inform us as to which features must be selected, and which can be dropped. What it does give us is a guideline for how many features should be retained.\nEven though the feature space that we are dealing with is relatively modest, it is still an interesting excerise to try to reduce the set of features it provides some insights into the most important features in the dataset. If this reduced set of features is small, it might even be feasible to perform the classi cation on the gateways themselves. In this paper, we examine the reduced feature set, and leave the viability of gatewaybased classi cation to future work.\nA number of techniques have been proposed in the literature towards feature sub-selection. We use a method known as Correlation-based Feature Selection (CFS) technique, that is known to produce feature subsets that generalize well to many di erent classi cation methods [11] and which produces a ranking over the features. CFS is based on the intuition that the ideal subset of features contain those features that are highly correlated with the labeled class, and at the same time poorly correlated with each other. Note that CFS is supervised in that it leverages the label to model feature dependence and thus, this part of our methodology is applied on the data (sub)sets DC and DF . Examining the list of features selected with CFS (Table 5), RSSIbased features particularly rssi_ad and rssi_auc) and tra c rates (both overall and per session) are rated very highly by CFS."}, {"heading": "6. RESULTS", "text": "In this section, we brie y describe two well known classi cation methods and subsequently present detailed results from applying these classi ers on the datasets DC and dataset DF , and for which we use the labels inferred in Section 4 as the ground truth."}, {"heading": "6.1 Classification Methods", "text": "In the course of working on this problem, we experimented with several di erent classi cation algorithms and techniques of varying complexity. In this paper, we describe the results from two particular methods Decision Trees and Support Vector Machines (SVM). We found the SVM based classi er to have the best over-\nall accuracy. However, the results it provides are hard to interpret in terms of the features are are fed into it. On the other hand the Decision Tree based classi er yielded slightly lower accuracy, but it provides a framework to reason about the relevance of various features in classifying a device. We rst report on the overall results, and later use the results of the Decision Tree to understand the importance of individual features. In the earlier stage of our work, we extensively experimented with di erent classi cation algorithms/techniques i.e. advanced methods (Neural Networks, Support Vector Machines, Logistic Regression) as well as more traditional/simpler methods (Decision Tree, Naive Bayes and kNN) and at the end chose the following two classi ers (one from each group) based on their accuracy, speed, stability, and readability of results.\n\u2022Decision Trees are widely used in multi class classi - cation applications. Construction starts with one node containing all the labeled instances and proceeds with a recursive splitting procedure. The splitting selects the most discriminative feature (using metrics such as Gini impurity, information gain, etc.) and partitions the parent node along the chosen feature. The recursion proceeds until no more partitioning is possible, and the tree is subsequently pruned (e.g. using minimal costcomplexity pruning). Weka implements a number of decision tree algorithms; we use SimpleCART [4] in our study. The de ning characteristic of this method is that features are ranked and considered sequentially. \u2022Support Vector Machines (SVM, for short) are a family of powerful binary (supervised) classi ers which operate by identifying the boundary or optimal separating hyperplane between instances in di erent classes. One of the salient features of SVMs is the support for non-linear classi cation by mapping input data into a higher dimensional space (i.e., the so called kerneltrick ). They computation in SVMs is to estimate the parameters of the hyperplane and a number of optimization methods have been proposed. In our work, we use a linear SVM with an implementation that uses the sequential minimal optimization (SMO) method [23]. One important advantage over Decision Trees is that SVMs can explicitly model dependencies between features.\nParameter Tuning. Both techniques described above involve a number of parameters (e.g., SVM penalty) and a key step in employing them e ectively is to identify good parameter values. We use the Weka Machine Learning framework [10] which includes implementations for both of these, and which supplies a set of default parameters that are generally considered good. In our work, we further use a grid search over the available parameter space for each classi er and identify the optimal values that maximize classi cation accuracy.\nWe compare each classi cation method against each\nother, and against a baseline value that is obtained by a very trivial classi er, ZeroR, which essentially maps every device instance to the most common class in the training set. The result of the ZeroR classi er serves as a lower bound the accuracy."}, {"heading": "6.2 Overall Results", "text": "For evaluation metric, we use the accuracy (de ned as ratio of correctly-classi ed instances in the test set). Moreover, the results are based on 10-fold cross-validation where the data is divided into 10 equally-sized subsets and the model is trained 10 times, each time using 9 subsets for training and the remaining subset for test.The nal accuracy is the average of these 10 runs. We also computed the F-measure metric for the classi ers and found the results to be consistent with accuracy (more precisely, in almost all cases F-measure is between 1 to 2 percent less than accuracy); we nd the latter more intuitive to understand and only report on it. Unless otherwise speci ed, the results in this section are shown as the improvement over the ZeroR classier. We report this, and not the absolute value, for the following reasons. When the dataset is severely unbalanced (e.g. if a single device class dominates), the trivial classi er can perform as well as the more complex methods. Reporting the improvement over the trivial classi er as we do points to how much better a more principled classi er can do.\n6.2.1 Accuracy In Figures 7a and 7b, we show computed accuracy\nfor the two classi ers and for three distinct feature sets. The baseline value, i.e., the starting value on the y-axis is the accuracy as reported by the trivial ZeroR classi er (recall that this corresponds to the population fraction of the most common device class). The feature subset all includes all of the features introduced in Table 3, while CFS:10 and CFS:20 correspond to the scenario where we only use the top 10 (or 20) features identi ed by the CFS algorithm. The accuracy reported corresponds to the optimal parameters (for each classi er) identi ed by the grid-search discussed previously. We point out that the improvement over using the default parameter settings in Weka are small (less than 2% at most). Looking at Figure 7a in detail, i.e., accuracy in predicting coarse grained classes, we nd that both classi ers perform quite well even as the SVM has slightly higher accuracy than the DecisionTree (90.47% as compared to 86.68%). Both classi ers do perform signi - cantly better than the baseline. We suspect this is due to the fact that two categories are predominant in the device population and these are also easy to tell apart with the RSSI based features. Note that reducing the number of features does not necessarily degrade clas-\nsi er accuracy con rming the existence of redundancy in the feature as well as the e ectiveness of the CFS method (DecisionTree's accuracy is generally less prone to feature reduction since the top-ranked features are less likely to be removed). However, we do point out that the number of features has a very large impact on the time taken to train the classi er and obtain a model. Turning now to Figure 7b which reports accuracy numbers when the classi ers are applied to the dataset DF , we nd that the accuracy drops by 6%-7%. As before the starting value on the y-axis is the accuracy reported by ZeroR. As expected this is lower than in the previous case there are more device classes and the device population is more fragmented. Overall, we see that the SVM based classi er again outperforms DecisionTree (83.11% vs. 77.60%). In comparing the performance across the three di erent feature sets shown, we nd mixed results across the classi ers. Overall, the reduction in performance due to using a smaller subset of features is not signi cant. The fact that the SVM based classi er consistently performs better than the Decision Tree (indeed, better than any other classi er that we experimented with) leads us to believe that there is some amount of coupling between features. Methods that examine features sequentially (e.g., Decision Trees) or those that assume independence (e.g., Naive Bayes) are unable to model this dependence and hence yield lower accuracies. However, the gains by using a more complicated method like an SVM are modest and this may be because some device classes are easier to identify than others. Understanding this better requires looking at how well each classi er does in identifying individual device classes. Table 6 presents the confusion matrix that corresponds to the SVM classi er we picked the best performing technique applied to ne grained data (dataset DF ). We omit a discussion of the same for coarse grained labels due to a lack of space. In Table 6, the columns are labeled by the letters a through g, which correspond to the classes indicated on the rows. We see two patterns emerge from the matrix. First, the classi er often labels smartphones as tablets,2 and vice\n2This confusion does not impact the accuracy for the coarse\nversa. Second, the classi er also has di culty clearly distinguishing PC devices (laptops and desktops) from network extenders. To rationalize the former, one possible reason is they really are used in similar ways (note that we do not have any information about the cellular activity of the smartphone). It is entirely possible that a particular individual would use his smartphone to watch lots of media, and this is what another individual uses his tablet device for. Looking at the second trend, i.e., the confusion between compute devices and network extenders, consider that a network extender does not really have a personality of its own, and simply aggregates many devices behind it. Thus, when the tra c features do match up between these classes (the distributions for these two entities do overlap as seen in Fig. 2), they would appear indistinguishable to the gateway. Furthermore, we manually inspected the network extenders that are incorrectly labeled as laptops/desktops and noticed that in most, there are a small number of hostnames associated with the MAC address (indicating few devices behind the extender).\n6.2.2 Data Sufficiency Most of our features are constructed starting from a\ntime series of a particular metric. It is then interesting to ask if there is a dependence between observation time, i.e., the period of time over which the time series is constructed, and the resulting accuracy in classifying the device. We reformulate this question to ask: what is the minimum time duration for a device to be observed before it is can be classifed to a degree of accuracy?\ngrained classes since both of these are grouped as Mobile Handheld devices\nTo consider an extreme example where we use a threshold of 1 day. This implies that a device that has been active (observed by the gateway) for at least 1 day is included into the training set. Thus, with a threshold of 1 day, the training set may include devices active for just a day or slightly longer. Obviously, the features extracted from these devices have a lot of variance and are likely to be misclassi ed. At the other extreme, if we were to use a high threshold (say 1 month), we would reject devices that were active for less than that period. This reduces the training set and rejects devices that may be used infrequently. To nd the right balance and identify the appropriate cut o duration, we ran a set of experiments where the threshold (number of days a device was active) was varied. The results for the SVM classi er are shown in Figure 7c. The x-axis indicates the threshold used (number of days), and the y-axis describes the relative improvement over the ZeroR classi er. For e.g., when the threshold is 1 day, the accuracy of the SVM classi er is 25% more than the ZeroR classi er. As seen in the gure, there is a initial improvement in the accuracy (around x=4) after which it begins to atten out. As x increases, two things happen: (i) more devices are rejected from the training set, and (ii) this also changes the device class distribution. The rejected devices are likely to have been misclassi ed (so removing them increases accuracy). Further, as the class distribution changes, the baseline accuracy also changes, which further ampli es the di erence between the ZeroR classi er and the SVM. The surprising result (for us) was that a device has to be active for only a few days for it to be classi ed accurately. Hence, as a tradeo between maximum accuracy and maximum dataset size, we have used the threshold value of 3 in this paper to de ne the transient versus non-transient devices (explained in Section 1)."}, {"heading": "6.3 Incorporating MAC Information", "text": "Recall from Section 3.1 that the MAC OUI identi es the manufacturer of the network interface. Manufacturers vary greatly in the number of types of devices that they manufacture. On one hand, Apple builds devices that cover all of the coarse grained device classes (and here the OUI is not likely to possess signi cant discriminatory power). On the other hand, Roku manufactures exactly one type of device an OTT box; here, the OUI completely determines the class of device. Most manufacturers fall somewhere inside this spectrum and this can be leveraged to improve the classi cation accuracy. It is very important to point out that the correlation between OUI to category correlation is not a very general one. Conceivably, a di erent dataset has contains predominantly Apple manufactured devicesis unlikely to bene t from the addition of the OUI as a feature.\nSince the learned classi er function(s) depend greatly on the distribution of device manufacturers across devices and this can vary a great deal based on country, user demographic, etc. we do not incorporate the OUI into our core set of features. That being said, examining its utility inside our speci c dataset is interesting. Figures 8 shows the improvement in classi cation accuracy for the datasets DC and DF by incorporating the OUI as categorical feature. Note that we used the entire set of features here (rather than the subsets extracted from CFS). The gure clearly shows that the classi er accuracy increases (by more than 10% in the best case). The improvement is greater for classi ers operating on the ne grained data. To point to an example: the accuracy of the SVM increases by about 7% when applied to ne grained dataset. However, the higher gain in accuracy corresponds to the DecisionTree classi er. Recall that this classi er recursively splits the data based on the feature that has the highest (remaining) discriminative power; the OUI is selected early because of the heterogeneity of OUI's in our dataset. This factor also cautions against generalizing these results too much; a di erent dataset with a di erent distribution of manufacturers may produce completely di erent results."}, {"heading": "6.4 Human-Readable Behavior Inference", "text": "While the SVM based classi er is the best performing, the results are hard to interpret and understand which particular features lead to certain devices being classi ed as they are, or their relative importance in deciding on the nal labeling. On the other hand, the DecisionTree proceeds by greedily partitioning the dataset by successively selecting discriminating features. Thus, the partitioning order directly indicates the relative importance of the associated feature. Note that the individuals labels generated by DecisionTree might be different from those assigned by SVM; however the performance of the DecisionTree is not very far from the SVM. Fig. 9 shows a tree generated using SimpleCart. Here, features (internal nodes) are ordered by the distance to the root of the tree. In this particular case, the most important feature is determined to be rssi_ad (this captures device mobility). Notice that the right side of the tree, where the feature value is greater than 2.04 only contains leaf nodes relating to Mobile hand held devices. Knowing that a device has high mobility is su cient to rule out devices like computers, game consoles, and OTT boxes, which are typically used in a few xed locations. The structure of the tree also provides a few hints that could explain the confusion between smartphones and tablets as well as PCs and Network Extenders (cf. Table 6); these appear as siblings in the upper levels of the tree (near the root). Going further, we seek to understand the essence of a particular device class and to be able to describe it\nsuccinctly with a set of concise rules. To state it another way, we wish to uncover what makes a smartphone a smartphone, and not a game console. There are two drawbacks in directly using the decision tree as constructed previously: (i) a certain class may appear in several leaf nodes throughout the tree, which confounds e orts to describe the classes purely in terms of a single path from the root of the tree, (ii) more importantly, minority classes are not captured in the tree (in our tree, for example Printer/Scanner), as the classi er can a ord to ignore them (labeling them incorrectly degrades accuracy very little). To address this, we employ the one-versus-all-the-rest approach for individual classes and use the ConjunctiveRule [25] classi er implemented in Weka; this classi er learns a simple set of conjunctions that can be used to predict nominal class labels. Before we apply it, we need to balance the device classes and to this end we use a technique called Synthetic Minority Oversampling TEchnique (SMOTE [8]). This generates new arti cial instances for the minority class to make the dataset more balanced. The result of the ConjunctiveRule learner is described in Table 7, which roughly correspond to concise behavior descriptors of the device. The table also indicates the accuracy that results from using only the induced rule to di erentiate devices of that class from the rest of the population. For some ne grained categories, single features are extremely discriminative; for e.g., rssi_ad\n(almost) completely describes smartphones, and thresholding tx_p25_ln completely captures printer/scanner devices. The latter is rule is quite intuitive since we expect these devices to be used very rarely, and thus, would generate low volumes of tra c on an average day. One salient property of these rules is that they can be used directly on the gateways to carry out a reasonably good classi cation of the devices in the home."}, {"heading": "7. CONCLUSIONS", "text": "In this paper, we described a methodology to identify the category of a networked device based on on low level indicators of network activity logged on home internet gateways. We analyzed in detail a dataset of 240 subscriber homes and extracted a number of features that succinctly capture the tra c and spatial behavior of devices in our dataset. We also de ned a two level taxonomy of device categories (coarse and ne) and used a number of heuristics on static device descriptors to associate devices with these labels (which were then extensively checked by manual inspection). We then experimented with a number of well known classication methods towards predicting the labels that we previously obtained. Overall, we nd that the coarse grained (higher level) label of a class can be inferred with very high accuracy (91% with the best case SVM classi er), with ne grained labels accurate up to 84%. We also examined the impact of incorporating mac address information into the classi cation and showed an accuracy improvement of about 6% (coarse grained) and 8% ( ne grained). While these improvements may not generalize outside of our speci c dataset, they provide useful insights. Moving past just classi er performance, we attempt to understand the relationship between feature types and individual device classes. To this end, we construct a set of simple, concise predicate conjunctions (over the features) that capture the latent character of each device type. One of the takeaways in our work is that the high accuracy is realized even with a relatively small feature set, and all of which focus purely on two aspects of device behavior how much tra c it exchanges on the network, and its positioning (relative to the gateway) over time. This was enabled by\na detailed dataset exploration and selection of features that combined intuition with existing best practices. To highlight one particular case of this: all of the spatial features rely on the same metric reported from the gateway; by extracting various behaviors from this single metric, we are able to successfully discriminate between di erent kinds of mobile devices. Our work is described in the context of an ISP (which owns and operates the home internet gateway) that wishes to classify devices in a customer's home. The model that we envision is that there is an initial ofine procedure (with a large training set) carried out in the ISP's premises to learn the classi cation models. Subsequently, gateways periodically report the feature summaries the ISP where they are run against the selected model(s) after which device labels are sent back to the gateway. In the future, we plan to investigate classi cation methods that can directly be run on the gateway itself."}, {"heading": "8. REFERENCES", "text": "[1] D. Allan. Time and frequency (time-domain)\ncharacterization, estimation, and prediction of precision clocks and oscillators. IEEE Trans. on Ultrasonics, Ferroelectrics, and Frequency\nControl, 34(6):647 654, Nov 1987. [2] S. M. Bellovin. A technique for counting natted\nhosts. In IMW, New York, NY, USA, 2002. ACM. [3] S. Bratus, C. Cornelius, D. Kotz, and D. Peebles.\nActive behavioral ngerprinting of wireless devices. In WiSec, NY, USA, 2008. ACM.\n[4] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classi cation and Regression Trees. Wadsworth International Group, 1984. [5] http://bit.ly/1fnTjOJ. BT and Phorm: how an online privacy scandal unfolded. [6] G. H. Dunteman. Principal components analysis. Number 69. Sage, 1989. [7] P. Eckersley. How unique is your web browser? In International Conference on Privacy Enhancing\nTechnologies, Berlin, Heidelberg, 2010. Springer-Verlag.\n[8] N. V. C. et. al. Synthetic minority over-sampling technique. Journal of Arti cial Intelligence Research, 16:321 357, 2002. [9] J. Franklin, D. McCoy, P. Tabriz, V. Neagoe, J. Van Randwyk, and D. Sicker. Passive data link layer 802.11 wireless device driver ngerprinting. In USENIX Security Symposium, Berkeley, CA, USA, 2006. USENIX Association.\n[10] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The weka data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10 18, 2009. [11] M. A. Hall. Correlation-based Feature Subset\nSelection for Machine Learning. PhD thesis, Univ. of Waikato, Hamilton, New Zealand, 1998. [12] Y. Jin, N. Du eld, P. Ha ner, S. Sen, and Z.-L. Zhang. Inferring applications at the network layer using collective tra c statistics. In SIGMETRICS, pages 351 352, 2010. [13] K. Kaemarungsi and P. Krishnamurthy. Analysis of wlan's received signal strength indication for indoor location ngerprinting. Pervasive Mob. Comput., 8(2), Apr. 2012. [14] T. Karagiannis, K. Papagiannaki, and M. Faloutsos. Blinc: Multilevel tra c classi cation in the dark. SIGCOMM CCR, 35(4), Aug. 2005. [15] K. Kleisouris, B. Firner, R. Howard, Y. Zhang, and R. P. Martin. Detecting intra-room mobility with signal strength descriptors. In MobiHoc, pages 71 80, 2010. [16] T. Kohno, A. Broido, and K. C. Cla y. Remote physical device ngerprinting. IEEE Trans. Dependable Secur. Comput., 2(2):93 108, Apr. 2005. [17] http://bit.ly/1Pnea52. MAC address and Manufacturer name change! [18] G. Maier, F. Schneider, and A. Feldmann. Nat usage in residential broadband networks. In PAM, Berlin, Heidelberg, 2011. Springer-Verlag. [19] K. Muthukrishnan, B. J. van der Zwaag, and P. Havinga. Inferring motion and location using wlan rssi. In MELT, pages 163 182, 2009. [20] T. T. Nguyen and G. Armitage. A survey of techniques for internet tra c classi cation using machine learning. Commun. Surveys Tuts., 10(4):56 76, Oct. 2008. [21] Nmap security scanner. http://www.nmap.org. [22] J. Pang, B. Greenstein, R. Gummadi, S. Seshan,\nand D. Wetherall. 802.11 user ngerprinting. In MobiCom, pages 99 110, New York, NY, USA, 2007. ACM.\n[23] J. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Schoelkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector\nLearning. MIT Press, 1998. [24] L. Shapira, S. Avidan, and A. Shamir.\nMode-detection via median-shift. In ICCV, pages 1909 1916, 2009.\n[25] I. H. Witten and E. Frank. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, 2005."}], "references": [{"title": "Time and frequency (time-domain) characterization, estimation, and prediction of precision clocks and oscillators", "author": ["D. Allan"], "venue": "IEEE Trans. on Ultrasonics, Ferroelectrics, and Frequency Control,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "A technique for counting natted hosts", "author": ["S.M. Bellovin"], "venue": "IMW, New York, NY, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Active behavioral ngerprinting of wireless devices", "author": ["S. Bratus", "C. Cornelius", "D. Kotz", "D. Peebles"], "venue": "WiSec, NY, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Classi cation and Regression Trees", "author": ["L. Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": "Wadsworth International Group", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1984}, {"title": "Principal components analysis", "author": ["G.H. Dunteman"], "venue": "Number 69. Sage", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "How unique is your web browser? In International Conference on Privacy Enhancing Technologies", "author": ["P. Eckersley"], "venue": "Berlin, Heidelberg", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Synthetic minority over-sampling technique", "author": ["N.V.C. et. al"], "venue": "Journal of Arti cial Intelligence Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "and D", "author": ["J. Franklin", "D. McCoy", "P. Tabriz", "V. Neagoe", "J. Van Randwyk"], "venue": "Sicker. Passive data link layer 802.11 wireless device driver ngerprinting. In USENIX Security Symposium, Berkeley, CA, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newsletter, 11(1):10 18", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Correlation-based Feature Subset  Selection for Machine Learning", "author": ["M.A. Hall"], "venue": "PhD thesis, Univ. of Waikato, Hamilton, New Zealand", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "N", "author": ["Y. Jin"], "venue": "Du eld, P. Ha ner, S. Sen, and Z.-L. Zhang. Inferring applications at the network layer using collective tra c statistics. In SIGMETRICS, pages 351 352", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Analysis of wlan's received signal strength indication for indoor location ngerprinting", "author": ["K. Kaemarungsi", "P. Krishnamurthy"], "venue": "Pervasive Mob. Comput.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Blinc: Multilevel tra c classi cation in the dark", "author": ["T. Karagiannis", "K. Papagiannaki", "M. Faloutsos"], "venue": "SIGCOMM CCR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Detecting intra-room mobility with signal strength descriptors", "author": ["K. Kleisouris", "B. Firner", "R. Howard", "Y. Zhang", "R.P. Martin"], "venue": "MobiHoc, pages 71 80", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Remote physical device ngerprinting", "author": ["T. Kohno", "A. Broido", "K.C. Cla y"], "venue": "IEEE Trans. Dependable Secur. Comput.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Nat usage in residential broadband networks", "author": ["G. Maier", "F. Schneider", "A. Feldmann"], "venue": "PAM, Berlin, Heidelberg", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "B", "author": ["K. Muthukrishnan"], "venue": "J. van der Zwaag, and P. Havinga. Inferring motion and location using wlan rssi. In MELT, pages 163 182", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey of techniques for internet tra c classi cation using machine learning", "author": ["T.T. Nguyen", "G. Armitage"], "venue": "Commun. Surveys Tuts.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "and D", "author": ["J. Pang", "B. Greenstein", "R. Gummadi", "S. Seshan"], "venue": "Wetherall. 802.11 user ngerprinting. In MobiCom, pages 99 110, New York, NY, USA", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J. Platt"], "venue": "B. Schoelkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Mode-detection via median-shift", "author": ["L. Shapira", "S. Avidan", "A. Shamir"], "venue": "ICCV, pages 1909 1916", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I.H. Witten", "E. Frank"], "venue": "Morgan Kaufmann", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 12, "context": ", the graphical relationships between source(s) and destinations(s) are exploited to identify tra c classes, as is described [14].", "startOffset": 125, "endOffset": 129}, {"referenceID": 10, "context": "In another approach, an initial inference is improved upon by incorporating more global information [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "A comprehensive survey of the techniques in this area can be found in [20] and we highlight two other notable works.", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "For example, Bellovin proposed a method to exploit IP header elds to identify distinct hosts sharing a single IP address [2].", "startOffset": 121, "endOffset": 124}, {"referenceID": 15, "context": "In [18], the authors describe an approach relying on the IP TTL eld and HTTP user-agent strings to correctly count and distinguish devices behind a single DSL line.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": ", Nmap [21]), browser ngerprinting tools [7], or wireless chipset [3].", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": ", Nmap [21]), browser ngerprinting tools [7], or wireless chipset [3].", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "This approach has been applied very successfully in the past in ngerprinting wireless cards and chipsets [9,22], or even devices [16].", "startOffset": 105, "endOffset": 111}, {"referenceID": 18, "context": "This approach has been applied very successfully in the past in ngerprinting wireless cards and chipsets [9,22], or even devices [16].", "startOffset": 105, "endOffset": 111}, {"referenceID": 14, "context": "This approach has been applied very successfully in the past in ngerprinting wireless cards and chipsets [9,22], or even devices [16].", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "the gateway) loosely tracks the physical distance of the device from the gateway [13,15,19].", "startOffset": 81, "endOffset": 91}, {"referenceID": 13, "context": "the gateway) loosely tracks the physical distance of the device from the gateway [13,15,19].", "startOffset": 81, "endOffset": 91}, {"referenceID": 16, "context": "the gateway) loosely tracks the physical distance of the device from the gateway [13,15,19].", "startOffset": 81, "endOffset": 91}, {"referenceID": 0, "context": "inally designed to study frequency stability in clocked analog circuits [1].", "startOffset": 72, "endOffset": 75}, {"referenceID": 20, "context": "To extract this feature, we developed a segmentation algorithm, inspired by the Median Shift algorithm [24] which infers the number of local maxima in a distribution, to identify the modes of the RSSI distribution.", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "Table 4 enumerates the 10 most skewed features extracted from our data for EVF=6 (the skewness test is fairly stable and we did not observe di erent outcomes with EVF in [3, 10].", "startOffset": 170, "endOffset": 177}, {"referenceID": 8, "context": "Table 4 enumerates the 10 most skewed features extracted from our data for EVF=6 (the skewness test is fairly stable and we did not observe di erent outcomes with EVF in [3, 10].", "startOffset": 170, "endOffset": 177}, {"referenceID": 4, "context": "To quantify the extent of redundancy in our dataset, we carry out a Principal Component Analysis (PCA) [6] over the feature space.", "startOffset": 103, "endOffset": 106}, {"referenceID": 9, "context": "t Coarse- and FineGrain Classes, Selected using CFS [11]", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "known as Correlation-based Feature Selection (CFS) technique, that is known to produce feature subsets that generalize well to many di erent classi cation methods [11] and which produces a ranking over the features.", "startOffset": 163, "endOffset": 167}, {"referenceID": 3, "context": "Weka implements a number of decision tree algorithms; we use SimpleCART [4] in our study.", "startOffset": 72, "endOffset": 75}, {"referenceID": 19, "context": "In our work, we use a linear SVM with an implementation that uses the sequential minimal optimization (SMO) method [23].", "startOffset": 115, "endOffset": 119}, {"referenceID": 8, "context": "We use the Weka Machine Learning framework [10] which includes implementations for both of these, and which supplies a set of default parameters that are generally considered good.", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "ploy the one-versus-all-the-rest approach for individual classes and use the ConjunctiveRule [25] classi er implemented in Weka; this classi er learns a simple set of conjunctions that can be used to predict nominal class labels.", "startOffset": 93, "endOffset": 97}, {"referenceID": 6, "context": "Before we apply it, we need to balance the device classes and to this end we use a technique called Synthetic Minority Oversampling TEchnique (SMOTE [8]).", "startOffset": 149, "endOffset": 152}], "year": 2017, "abstractText": "We study the problem of inferring the type of a networked device in a home network by leveraging low level traffic activity indicators seen at commodity home gateways. We analyze a dataset of detailed device network activity obtained from 240 subscriber homes of a large European ISP and extract a number of traffic and spatial fingerprints for individual devices. We develop a two level taxonomy to describe devices onto which we map individual devices using a number of heuristics. We leverage the heuristically derived labels to train classifiers that distinguish device classes based on the traffic and spatial fingerprints of a device. Our results show an accuracy level up to 91% for the coarse level category and up to 84% for the fine grained category. By incorporating information from other sources (e.g., MAC OUI), we are able to further improve accuracy to above 97% and 92%, respectively. Finally, we also extract a set of simple and human-readable rules that concisely capture the behavior of these distinct device categories.", "creator": "LaTeX with hyperref package"}}}