{"id": "1709.00609", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2017", "title": "Security Evaluation of Pattern Classifiers under Attack", "abstract": "pattern classification systems are commonly used in adversarial applications, like biometric authentication, remote intrusion detection, and spam filtering, in which data can be purposely manipulated by humans to undermine their operation. as this adversarial scenario is not taken into account by classical engineering methods, traditional classification systems may exhibit vulnerabilities, whose exploitation risks severely affect their performance, and consequently limit their practical impact. teaching pattern classification theory in design methods to adversarial settings is thus a novel and very relevant research direction, which serves not yet been pursued in a systematic way. in this paper, we address most of the main open scenarios : evaluating at design phase specific security of pattern offenders, namely, the performance degradation under potential that they may incur enhanced operation. we propose a framework for empirical evaluation of classifier security that formalizes and generalizes the main ideas proposed in the summary, and give examples of security use in microsoft real applications. reported results show that structured updates can provide a more complete correlation among the classifier's behavior in adversarial environments, and lead to better utility choices.", "histories": [["v1", "Sat, 2 Sep 2017 17:38:45 GMT  (1204kb,D)", "http://arxiv.org/abs/1709.00609v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["battista biggio", "giorgio fumera", "fabio roli"], "accepted": false, "id": "1709.00609"}, "pdf": {"name": "1709.00609.pdf", "metadata": {"source": "CRF", "title": "Security Evaluation of Pattern Classifiers under Attack", "authors": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "emails": ["battista.biggio@diee.unica.it,", "fumera@diee.unica.it,", "roli@diee.unica.it,"], "sections": [{"heading": null, "text": "Index Terms\u2014Pattern classification, adversarial classification, performance evaluation, security evaluation, robustness evaluation.\nF"}, {"heading": "1 INTRODUCTION", "text": "Pattern classification systems based on machine learning algorithms are commonly used in security-related applications like biometric authentication, network intrusion detection, and spam filtering, to discriminate between a \u201clegitimate\u201d and a \u201cmalicious\u201d pattern class (e.g., legitimate and spam emails). Contrary to traditional ones, these applications have an intrinsic adversarial nature since the input data can be purposely manipulated by an intelligent and adaptive adversary to undermine classifier operation. This often gives rise to an arms race between the adversary and the classifier designer. Well known examples of attacks against pattern classifiers are: submitting a fake biometric trait to a biometric authentication system (spoofing attack) [1], [2]; modifying network packets belonging to intrusive traffic to evade intrusion detection systems [3]; manipulating the content of spam emails to get them past spam filters (e.g., by misspelling common spam words to avoid their detection) [4]\u2013[6]. Adversarial scenarios can also occur in intelligent data analysis [7] and information retrieval [8]; e.g., a malicious webmaster may manipulate search engine rankings to artificially promote her1 web site.\nIt is now acknowledged that, since pattern classification systems based on classical theory and design\nPlease cite this work as: B. Biggio, G. Fumera, and F. Roli. Security evaluation of pattern classifiers under attack. IEEE Transactions on Knowledge and Data Engineering, 26(4):984-996, April 2014. The authors are with the Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d\u2019Armi, 09123 Cagliari, Italy Battista Biggio: e-mail battista.biggio@diee.unica.it, phone +39 070 675 5776 Giorgio Fumera: e-mail fumera@diee.unica.it, phone +39 070 675 5754 Fabio Roli (corresponding author): e-mail roli@diee.unica.it, phone +39 070 675 5779, fax (shared) +39 070 675 5782\n1. The adversary is hereafter referred to as feminine due to the popular interpretation as \u201cEve\u201d or \u201cCarol\u201d in cryptography and computer security.\nmethods [9] do not take into account adversarial settings, they exhibit vulnerabilities to several potential attacks, allowing adversaries to undermine their effectiveness [6], [10]\u2013[14]. A systematic and unified treatment of this issue is thus needed to allow the trusted adoption of pattern classifiers in adversarial environments, starting from the theoretical foundations up to novel design methods, extending the classical design cycle of [9]. In particular, three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing novel design methods to guarantee classifier security in adversarial environments [1], [6], [10].\nAlthough this emerging field is attracting growing interest [13], [18], [19], the above issues have only been sparsely addressed under different perspectives and to a limited extent. Most of the work has focused on application-specific issues related to spam filtering and network intrusion detection, e.g., [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet provide practical guidelines and tools for designers of pattern recognition systems.\nBesides introducing these issues to the pattern recognition research community, in this work we address issues (i) and (ii) above by developing a framework for the empirical evaluation of classifier security at design phase that extends the model selection and performance evaluation steps of the classical design cycle of [9].\nIn Sect. 2 we summarize previous work, and point out three main ideas that emerge from it. We then formalize\nar X\niv :1\n70 9.\n00 60\n9v 1\n[ cs\n.L G\n] 2\nS ep\n2 01\n7\n2 and generalize them in our framework (Sect. 3). First, to pursue security in the context of an arms race it is not sufficient to react to observed attacks, but it is also necessary to proactively anticipate the adversary by predicting the most relevant, potential attacks through a what-if analysis; this allows one to develop suitable countermeasures before the attack actually occurs, according to the principle of security by design. Second, to provide practical guidelines for simulating realistic attack scenarios, we define a general model of the adversary, in terms of her goal, knowledge, and capability, which encompasses and generalizes models proposed in previous work. Third, since the presence of carefully targeted attacks may affect the distribution of training and testing data separately, we propose a model of the data distribution that can formally characterize this behavior, and that allows us to take into account a large number of potential attacks; we also propose an algorithm for the generation of training and testing sets to be used for security evaluation, which can naturally accommodate application-specific and heuristic techniques for simulating attacks.\nIn Sect. 4 we give three concrete examples of applications of our framework in spam filtering, biometric authentication, and network intrusion detection. In Sect. 5, we discuss how the classical design cycle of pattern classifiers should be revised to take security into account. Finally, in Sect. 6, we summarize our contributions, the limitations of our framework, and some open issues."}, {"heading": "2 BACKGROUND AND PREVIOUS WORK", "text": "Here we review previous work, highlighting the concepts that will be exploited in our framework."}, {"heading": "2.1 A taxonomy of attacks against pattern classifiers", "text": "A taxonomy of potential attacks against pattern classifiers was proposed in [11], [15], and subsequently extended in [14]. We will exploit it in our framework, as part of the definition of attack scenarios. The taxonomy is based on two main features: the kind of influence of attacks on the classifier, and the kind of security violation they cause. The influence can be either causative, if it undermines the learning algorithm to cause subsequent misclassifications; or exploratory, if it exploits knowledge of the trained classifier to cause misclassifications, without affecting the learning algorithm. Thus, causative attacks may influence both training and testing data, or only training data, whereas exploratory attacks affect only testing data. The security violation can be an integrity violation, if it allows the adversary to access the service or resource protected by the classifier; an availability violation, if it denies legitimate users access to it; or a privacy violation, if it allows the adversary to obtain confidential information from the classifier. Integrity violations result in misclassifying malicious samples as legitimate, while availability violations can also cause legitimate samples to be misclassified as malicious. A third feature of the taxonomy is the specificity\nof an attack, that ranges from targeted to indiscriminate, depending on whether the attack focuses on a single or few specific samples (e.g., a specific spam email misclassified as legitimate), or on a wider set of samples."}, {"heading": "2.2 Limitations of classical performance evaluation methods in adversarial classification", "text": "Classical performance evaluation methods, like k-fold cross validation and bootstrapping, aim to estimate the performance that a classifier will exhibit during operation, by using data D collected during classifier design.2 These methods are based on the stationarity assumption that the data seen during operation follow the same distribution as D. Accordingly, they resample D to construct one or more pairs of training and testing sets that ideally follow the same distribution as D [9]. However, the presence of an intelligent and adaptive adversary makes the classification problem highly non-stationary, and makes it difficult to predict how many and which kinds of attacks a classifier will be subject to during operation, that is, how the data distribution will change. In particular, the testing data processed by the trained classifier can be affected by both exploratory and causative attacks, while the training data can only be affected by causative attacks, if the classifier is retrained online [11], [14], [15].3 In both cases, during operation, testing data may follow a different distribution than that of training data, when the classifier is under attack. Therefore, security evaluation can not be carried out according to the classical paradigm of performance evaluation.4"}, {"heading": "2.3 Arms race and security by design", "text": "Security problems often lead to a \u201creactive\u201d arms race between the adversary and the classifier designer. At each step, the adversary analyzes the classifier defenses, and develops an attack strategy to overcome them. The designer reacts by analyzing the novel attack samples, and, if required, updates the classifier; typically, by retraining it on the new collected samples, and/or adding features that can detect the novel attacks (see Fig. 1, left). Examples of this arms race can be observed in spam filtering and malware detection, where it has led to a\n2. By design we refer to the classical steps of [9], which include feature extraction, model selection, classifier training and performance evaluation (testing). Operation denotes instead the phase which starts when the deployed classifier is set to operate in a real environment.\n3. \u201cTraining\u201d data refers both to the data used by the learning algorithm during classifier design, coming from D, and to the data collected during operation to retrain the classifier through online learning algorithms. \u201cTesting\u201d data refers both to the data drawn from D to evaluate classifier performance during design, and to the data classified during operation. The meaning will be clear from the context.\n4. Classical performance evaluation is generally not suitable for non-stationary, time-varying environments, where changes in the data distribution can be hardly predicted; e.g., in the case of concept drift [20]. Further, the evaluation process has a different goal in this case, i.e., to assess whether the classifier can \u201crecover\u201d quickly after a change has occurred in the data distribution. Instead, security evaluation aims at identifying the most relevant attacks and threats that should be countered before deploying the classifier (see Sect. 2.3).\n3\nconsiderable increase in the variability and sophistication of attacks and countermeasures.\nTo secure a system, a common approach used in engineering and cryptography is security by obscurity, that relies on keeping secret some of the system details to the adversary. In contrast, the paradigm of security by design advocates that systems should be designed from the ground-up to be secure, without assuming that the adversary may ever find out some important system details. Accordingly, the system designer should anticipate the adversary by simulating a \u201cproactive\u201d arms race to (i) figure out the most relevant threats and attacks, and (ii) devise proper countermeasures, before deploying the classifier (see Fig. 1, right). This paradigm typically improves security by delaying each step of the \u201creactive\u201d arms race, as it requires the adversary to spend a greater effort (time, skills, and resources) to find and exploit vulnerabilities. System security should thus be guaranteed for a longer time, with less frequent supervision or human intervention.\nThe goal of security evaluation is to address issue (i) above, i.e., to simulate a number of realistic attack scenarios that may be incurred during operation, and to assess the impact of the corresponding attacks on the targeted classifier to highlight the most critical vulnerabilities. This amounts to performing a what-if analysis [21], which is a common practice in security. This approach has been implicitly followed in several previous works (see Sect. 2.4), but never formalized within a general framework for the empirical evaluation of classifier security. Although security evaluation may also suggest specific countermeasures, the design of secure classifiers, i.e., issue (ii) above, remains a distinct open problem."}, {"heading": "2.4 Previous work on security evaluation", "text": "Many authors implicitly performed security evaluation as a what-if analysis, based on empirical simulation methods; however, they mainly focused on a specific application, classifier and attack, and devised ad hoc security evaluation procedures based on the exploitation of problem knowledge and heuristic techniques [1]\u2013[6], [15], [22]\u2013[36]. Their goal was either to point out a previously unknown vulnerability, or to evaluate security against a known attack. In some cases, specific countermeasures were also proposed, according to\na proactive/security-by-design approach. Attacks were simulated by manipulating training and testing samples according to application-specific criteria only, without reference to more general guidelines; consequently, such techniques can not be directly exploited by a system designer in more general cases.\nA few works proposed analytical methods to evaluate the security of learning algorithms or of some classes of decision functions (e.g., linear ones), based on more general, application-independent criteria to model the adversary\u2019s behavior (including PAC learning and game theory) [10], [12], [14], [16], [17], [37]\u2013[40]. Some of these criteria will be exploited in our framework for empirical security evaluation; in particular, in the definition of the adversary model described in Sect. 3.1, as high-level guidelines for simulating attacks."}, {"heading": "2.5 Building on previous work", "text": "We summarize here the three main concepts more or less explicitly emerged from previous work that will be exploited in our framework for security evaluation.\n1) Arms race and security by design: since it is not possible to predict how many and which kinds of attacks a classifier will incur during operation, classifier security should be proactively evaluated using a what-if analysis, by simulating potential attack scenarios.\n2) Adversary modeling: effective simulation of attack scenarios requires a formal model of the adversary.\n3) Data distribution under attack: the distribution of testing data may differ from that of training data, when the classifier is under attack."}, {"heading": "3 A FRAMEWORK FOR EMPIRICAL EVALUATION OF CLASSIFIER SECURITY", "text": "We propose here a framework for the empirical evaluation of classifier security in adversarial environments, that unifies and builds on the three concepts highlighted in Sect. 2.5. Our main goal is to provide a quantitative and general-purpose basis for the application of the what-if analysis to classifier security evaluation, based on the definition of potential attack scenarios. To this end, we propose: (i) a model of the adversary, that allows us to define any attack scenario; (ii) a corresponding model of the data distribution; and (iii) a method for\n4 generating training and testing sets that are representative of the data distribution, and are used for empirical performance evaluation."}, {"heading": "3.1 Attack scenario and model of the adversary", "text": "Although the definition of attack scenarios is ultimately an application-specific issue, it is possible to give general guidelines that can help the designer of a pattern recognition system. Here we propose to specify the attack scenario in terms of a conceptual model of the adversary that encompasses, unifies, and extends different ideas from previous work. Our model is based on the assumption that the adversary acts rationally to attain a given goal, according to her knowledge of the classifier, and her capability of manipulating data. This allows one to derive the corresponding optimal attack strategy.\nAdversary\u2019s goal. As in [17], it is formulated as the optimization of an objective function. We propose to define this function based on the desired security violation (integrity, availability, or privacy), and on the attack specificity (from targeted to indiscriminate), according to the taxonomy in [11], [14] (see Sect. 2.1). For instance, the goal of an indiscriminate integrity violation may be to maximize the fraction of misclassified malicious samples [6], [10], [14]; the goal of a targeted privacy violation may be to obtain some specific, confidential information from the classifier (e.g., the biometric trait of a given user enrolled in a biometric system) by exploiting the class labels assigned to some \u201cquery\u201d samples, while minimizing the number of query samples that the adversary has to issue to violate privacy [14], [16], [41].\nAdversary\u2019s knowledge. Assumptions on the adversary\u2019s knowledge have only been qualitatively discussed in previous work, mainly depending on the application at hand. Here we propose a more systematic scheme for their definition, with respect to the knowledge of the single components of a pattern classifier: (k.i) the training data; (k.ii) the feature set; (k.iii) the learning algorithm and the kind of decision function (e.g., a linear SVM); (k.iv) the classifier\u2019s decision function and its parameters (e.g., the feature weights of a linear classifier); (k.v) the feedback available from the classifier, if any (e.g., the class labels assigned to some \u201cquery\u201d samples that the adversary issues to get feedback [14], [16], [41]). It is worth noting that realistic and minimal assumptions about what can be kept fully secret from the adversary should be done, as discussed in [14]. Examples of adversary\u2019s knowledge are given in Sect. 4.\nAdversary\u2019s capability. It refers to the control that the adversary has on training and testing data. We propose to define it in terms of: (c.i) the attack influence (either causative or exploratory), as defined in [11], [14]; (c.ii) whether and to what extent the attack affects the class priors; (c.iii) how many and which training and testing samples can be controlled by the adversary in each class; (c.iv) which features can be manipulated, and to what extent, taking into account application-specific\nconstraints (e.g., correlated features can not be modified independently, and the functionality of malicious samples can not be compromised [1], [3], [6], [10]).\nAttack strategy. One can finally define the optimal attack strategy, namely, how training and testing data should be quantitatively modified to optimize the objective function characterizing the adversary\u2019s goal. Such modifications are defined in terms of: (a.i) how the class priors are modified; (a.ii) what fraction of samples of each class is affected by the attack; and (a.iii) how features are manipulated by the attack. Detailed examples are given in Sect. 4.\nOnce the attack scenario is defined in terms of the adversary model and the resulting attack strategy, our framework proceeds with the definition of the corresponding data distribution, that is used to construct training and testing sets for security evaluation."}, {"heading": "3.2 A model of the data distribution", "text": "We consider the standard setting for classifier design in a problem which consists of discriminating between legitimate (L) and malicious (M) samples: a learning algorithm and a performance measure have been chosen, a set D of n labelled samples has been collected, and a set of d features have been extracted, so that D = {(xi, yi)}ni=1, where xi denotes a d-dimensional feature vector, and yi \u2208 {L,M} a class label. The pairs (xi, yi) are assumed to be i.i.d. samples of some unknown distribution pD(X, Y ). Since the adversary model in Sect. 3.1 requires us to specify how the attack affects the class priors and the features of each class, we consider the classical generative model pD(X, Y ) = pD(Y )pD(X|Y ).5 To account for the presence of attacks during operation, which may affect either the training or the testing data, or both, we denote the corresponding training and testing distributions as ptr and pts, respectively. We will just write p when we want to refer to either of them, or both, and the meaning is clear from the context.\nWhen a classifier is not under attack, according to the classical stationarity assumption we have ptr(Y ) = pts(Y ) = pD(Y ), and ptr(X|Y ) = pts(X|Y ) = pD(X|Y ). We extend this assumption to the components of ptr and pts that are not affected by the attack (if any), by assuming that they remain identical to the corresponding distribution pD (e.g., if the attack does not affect the class priors, the above equality also holds under attack).\nThe distributions p(Y ) and p(X|Y ) that are affected by the attack can be defined as follows, according to the definition of the attack strategy, (a.i-iii).\nClass priors. ptr(Y ) and pts(Y ) can be immediately defined based on assumption (a.i).\nClass-conditional distributions. ptr(X|Y ) and pts(X|Y ) can be defined based on assumptions (a.ii-iii). First, to account for the fact that the attack may not\n5. In this paper, for the sake of simplicity, we use the lowercase notation p(\u00b7) to denote any probability density function, with both continuous and discrete random variables.\n5\nmodify all training or testing samples, according to (a.ii), we model p(X|Y ) as a mixture controlled by a Boolean random variable A, which denotes whether a given sample has been manipulated (A = T) or not (A = F):\np(X|Y ) = p(X|Y,A = T)p(A = T|Y ) + p(X|Y,A = F)p(A = F|Y ). (1)\nWe name the samples of the component p(X|Y,A = T) attack samples to emphasize that their distribution is different from that of samples which have not been manipulated by the adversary, p(X|Y,A = F). Note that p(A = T|Y = y) is the probability that an attack sample belongs to class y, i.e., the percentage of samples of class y controlled by the adversary. It is thus defined by (a.ii).\nFor samples which are unaffected by the attack, the stationarity assumption holds:\np(X|Y,A = F) = pD(X|Y ). (2)\nThe distribution p(X|Y,A = T) depends on assumption (a.iii). Depending on the problem at hand, it may not be possible to analytically define it. Nevertheless, for the purpose of security evaluation, we will show in Sect. 3.3 that it can be defined as the empirical distribution of a set of fictitious attack samples.\nFinally, p(X|Y,A = F) can be defined similarly, if its analytical definition is not possible. In particular, according to Eq. (2), it can be defined as the empirical distribution of D.\nThe above generative model of the training and testing distributions ptr and pts is represented by the Bayesian network in Fig. 2, which corresponds to factorize p(X, Y, A) as follows:\np(X, Y, A) = p(Y )p(A|Y )p(X|Y,A). (3)\nOur model can be easily extended to take into account concurrent attacks involving m > 1 different kinds of sample manipulations; for example, to model attacks against different classifiers in multimodal biometric systems [1], [2]. To this end, one can define m different Boolean random variables A = (A1, . . . , Am), and the corresponding distributions. The extension of Eq. 3 is then straightforward (e.g., see [32]). The dependence between the different attacks, if any, can be modeled by the distribution p(A|Y ).\nIf one is also interested in evaluating classifier security subject to temporal, non-adversarial variations of\nthe data distribution (e.g., the drift of the content of legitimate emails over time), it can be assumed that the distribution p(X, Y, A = F) changes over time, according to some model p(X, Y, A = F, t) (see, e.g., [20]). Then, classifier security at time t can be evaluated by modeling the attack distribution over time p(X, Y, A = T, t) as a function of p(X, Y, A = F, t).\nThe model of the adversary in Sect. 3.1, and the above model of the data distribution provide a quantitative, well-grounded and general-purpose basis for the application of the what-if analysis to classifier security evaluation, which advances previous work."}, {"heading": "3.3 Training and testing set generation", "text": "Here we propose an algorithm to sample training (TR) and testing (TS) sets of any desired size from the distributions ptr(X, Y ) and pts(X, Y ).\nWe assume that k \u2265 1 different pairs of training and testing sets (DiTR,DiTS), i = 1, . . . , k, have been obtained from D using a classical resampling technique, like crossvalidation or bootstrapping. Accordingly, their samples follow the distribution pD(X, Y ). In the following, we describe how to modify each of the sets DiTR to construct a training set TRi that follows the distribution ptr(X, Y ). For the sake of simplicity, we will omit the superscript i. An identical procedure can be followed to construct a testing set TSi from each of the DiTS. Security evaluation is then carried out with the classical method, by averaging (if k > 1) the performance of the classifier trained on TRi and tested on TSi.\nIf the attack does not affect the training samples, i.e., ptr(X, Y ) = pD(X, Y ), TR is simply set equal to DTR. Otherwise, two alternatives are possible. (i) If ptr(X|Y,A) is analytically defined for each Y \u2208 {L,M} and A \u2208 {T,F}, then TR can be obtained by sampling the generative model of p(X, Y, A) of Eq. (3): first, a class label y is sampled from ptr(Y ), then, a value a from ptr(A|Y = y), and, finally, a feature vector x from ptr(X|Y = y,A = a). (ii) If ptr(X|Y = y,A = a) is not analytically defined for some y and a, but a set of its samples is available, denoted in the following as Dy,aTR, it can be approximated as the empirical distribution of Dy,aTR. Accordingly, we can sample with replacement from Dy,aTR [42]. An identical procedure can be used to construct the testing set TS. The procedure to obtain TR or TS is formally described as Algorithm 1.6\nLet us now discuss how to construct the sets Dy,aTR, when ptr(X|Y = y,A = a) is not analytically defined. The same discussion holds for the sets Dy,aTS . First, the two sets DL,FTR and D M,F TR can be respectively set equal\n6. Since the proposed algorithm is based on classical resampling techniques such as cross-validation and bootstrapping, it is reasonable to expect that the bias and the variance of the estimated classification error (or of any other performance measure) will enjoy similar statistical properties to those exhibited by classical performance evaluation methods based on the same techniques. In practice, these error components are typically negligible with respect to errors introduced by the use of limited training data, biased learning/classification algorithms, and noisy or corrupted data.\n6 Algorithm 1 Construction of TR or TS. Input: The number n of desired samples; the distributions p(Y ) and p(A|Y ); for each y \u2208 {L,M}, a \u2208 {T,F}, the distribution p(X|Y = y,A = a), if analytically defined, or the set of samples Dy,a, otherwise. Output: A data set S (either TR or TS) drawn from p(Y )p(A|Y )p(X|Y,A).\n1: S \u2190 \u2205 2: for i = 1, . . . , n do 3: sample y from p(Y ) 4: sample a from p(A|Y = y) 5: draw a sample x from p(X|Y = y,A = a), if analyt-\nically defined; otherwise, sample with replacement from Dy,a\n6: S \u2190 S \u22c3 {(x, y)} 7: end for 8: return S\nto the legitimate and malicious samples in DTR, since the distribution of such samples is assumed to be just ptr(X|Y = L, A = F) and ptr(X|Y = M, A = F) (see Eq. 2): DL,FTR = {(x, y) \u2208 DTR : y = L}, D M,F TR = {(x, y) \u2208 DTR : y = M}. The two sets of attack samples Dy,TTR , for y = L,M, must come instead from ptr(X|Y = y,A = T). They can thus be constructed according to the point (a.iii) of the attack strategy, using any technique for simulating attack samples. Therefore, all the ad hoc techniques used in previous work for constructing fictitious attack samples can be used as methods to define or empirically approximate the distribution p(X|Y = y,A = T), for y = L,M.\nTwo further considerations must be made on the simulated attack samples Dy,TTR . First, the number of distinct attack samples that can be obtained depends on the simulation technique, when it requires the use of the data in DTR; for instance, if it consists of modifying deterministically each malicious sample in DTR, no more than |{(x, y) \u2208 DTR : y = M}| distinct attack samples can be obtained. If any number of attack samples can be generated, instead (e.g., [5], [28], [34], [36]), then the sets Dy,TTR , for y = L,M, do not need to be constructed beforehand: a single attack sample can be generated online at step 5 of Algorithm 1, when a = T. Second, in some cases it may be necessary to construct the attack samples incrementally; for instance, in the causative attacks considered in [11], [30], [36], the attack samples are added to the training data one at a time, since each of them is a function of the current training set. This corresponds to a non-i.i.d. sampling of the attack distribution. In these cases, Algorithm 1 can be modified by first generating all pairs y, a, then, the feature vectors x corresponding to a = F, and, lastly, the attack samples corresponding to a = T, one at a time."}, {"heading": "3.4 How to use our framework", "text": "We summarize here the steps that the designer of a pattern classifier should take to evaluate its security using our framework, for each attack scenario of interest. They extend the performance evaluation step of the classical design cycle of [9], which is used as part of the model selection phase, and to evaluate the final classifier to be deployed.\n1) Attack scenario. The attack scenario should be defined at the conceptual level by making specific assumptions on the goal, knowledge (k.i-v), and capability of the adversary (c.i-iv), and defining the corresponding attack strategy (a.i-iii), according to the model of Sect. 3.1.\n2) Data model. According to the hypothesized attack scenario, the designer should define the distributions p(Y ), p(A|Y ), and p(X|Y,A), for Y \u2208 {L,M}, A \u2208 {F,T}, and for training and testing data. If p(X|Y,A) is not analytically defined for some Y = y and A = a, either for training or testing data, the corresponding set Dy,aTR or Dy,aTS must be constructed. The sets D y,F TR (D y,F TS ) are obtained from DTR (DTS). The sets Dy,TTR and D y,T TS can be generated, only if the attack involves sample manipulation, using an attack sample simulation technique according to the attack strategy (a.iii).\n3) Construction of TR and TS. Given k \u2265 1 pairs (DiTR,DiTS), i = 1, . . . , k, obtained from classical resampling techniques like cross-validation or bootstrapping, the size of TR and TS must be defined, and Algorithm 1 must be run with the corresponding inputs to obtain TRi and TSi. If the attack does not affect the training (testing) data, TRi (TSi) is set to DiTR (DiTS).\n4) Performance evaluation. The classifier performance under the simulated attack is evaluated using the constructed (TRi, TSi) pairs, as in classical techniques."}, {"heading": "4 APPLICATION EXAMPLES", "text": "While previous work focused on a single application, we consider here three different application examples of our framework in spam filtering, biometric authentication, and network intrusion detection. Our aim is to show how the designer of a pattern classifier can use our framework, and what kind of additional information he can obtain from security evaluation. We will show that a trade-off between classifier accuracy and security emerges sometimes, and that this information can be exploited for several purposes; e.g., to improve the model selection phase by considering both classification accuracy and security."}, {"heading": "4.1 Spam filtering", "text": "Assume that a classifier has to discriminate between legitimate and spam emails on the basis of their textual content, and that the bag-of-words feature representation has been chosen, with binary features denoting the occurrence of a given set of words. This kind of classifier\n7 Sect. 4.1 Sect. 4.2 Sect. 4.3 Attack scenario Indiscrim.\nExploratory Integrity Targeted Exploratory Integrity Indiscrim. Causative Integrity\nptr(Y ) pD(Y ) pD(Y ) ptr(M)=pmax ptr(A = T|Y = L) 0 0 0 ptr(A = T|Y = M) 0 0 1 ptr(X|Y = L, A = T) - - - ptr(X|Y = M, A = T) - - empirical ptr(X|Y = L, A = F) pD(X|L) pD(X|L) pD(X|L) ptr(X|Y = M, A = F) pD(X|M) pD(X|M) - pts(Y ) pD(Y ) pD(Y ) pD(Y ) pts(A = T|Y = L) 0 0 0 pts(A = T|Y = M) 1 1 0 pts(X|Y = L, A = T) - - - pts(X|Y = M, A = T) empirical empirical - pts(X|Y = L, A = F) pD(X|L) pD(X|L) pD(X|L) pts(X|Y = M, A = F) - - pD(X|M)\nTABLE 1 Parameters of the attack scenario and of the data model for each application example. \u2018Empirical\u2019 means that p(X|Y = y,A = a) was approximated as the empirical distribution of a set of samples Dy,a; and \u2018-\u2019 means that no samples from this distribution were needed to simulate the attack.\nhas been considered by several authors [6], [28], [43], and it is included in several real spam filters.7\nIn this example, we focus on model selection. We assume that the designer wants to choose between a support vector machine (SVM) with a linear kernel, and a logistic regression (LR) linear classifier. He also wants to choose a feature subset, among all the words occurring in training emails. A set D of legitimate and spam emails is available for this purpose. We assume that the designer wants to evaluate not only classifier accuracy in the absence of attacks, as in the classical design scenario, but also its security against the wellknown bad word obfuscation (BWO) and good word insertion (GWI) attacks. They consist of modifying spam emails by inserting \u201cgood words\u201d that are likely to appear in legitimate emails, and by obfuscating \u201cbad words\u201d that are typically present in spam [6]. The attack scenario can be modeled as follows.\n1) Attack scenario. Goal. The adversary aims at maximizing the percentage of spam emails misclassified as legitimate, which is an indiscriminate integrity violation.\nKnowledge. As in [6], [10], the adversary is assumed to have perfect knowledge of the classifier, i.e.: (k.ii) the feature set, (k.iii) the kind of decision function, and (k.iv) its parameters (the weight assigned to each feature, and the decision threshold). Assumptions on the knowledge of (k.i) the training data and (k.v) feedback from the classifier are not relevant in this case, as they do not provide any additional information.\nCapability. We assume that the adversary: (c.i) is only\n7. SpamAssassin, http://spamassassin.apache.org/; Bogofilter, http://bogofilter.sourceforge.net/; SpamBayes http://spambayes. sourceforge.net/\nable to influence testing data (exploratory attack); (c.ii) can not modify the class priors; (c.iii) can manipulate each malicious sample, but no legitimate ones; (c.iv) can manipulate any feature value (i.e., she can insert or obfuscate any word), but up to a maximum number nmax of features in each spam email [6], [10]. This allows us to evaluate how gracefully the classifier performance degrades as an increasing number of features is modified, by repeating the evaluation for increasing values of nmax.\nAttack strategy. Without loss of generality, let us further assume that x is classified as legitimate if g(x) =\u2211n\ni=1 wixi +w0 < 0, where g(\u00b7) is the discriminant function of the classifier, n is the feature set size, xi \u2208 {0, 1} are the feature values (1 and 0 denote respectively the presence and the absence of the corresponding term), wi are the feature weights, and w0 is the bias.\nUnder the above assumptions, the optimal attack strategy can be attained by: (a.i) leaving the class priors unchanged; (a.ii) manipulating all testing spam emails; and (a.iii) modifying up to nmax words in each spam to minimize the discriminant function of the classifier.\nEach attack sample (i.e., modified spam) can be thus obtained by solving a constrained optimization problem. As in [10], the generation of attack samples can be represented by a function A : X 7\u2192 X , and the number of modified words can be evaluated by the Hamming distance. Accordingly, for any given x \u2208 X , the optimal attack strategy amounts to finding the attack sample A(x) which minimizes g(A(x)), subject to the constraint that the Hamming distance between x and A(x) is no greater than nmax, that is:\nA(x) = argminx\u2032 n\u2211\ni=1\nwix \u2032 i\ns.t.\nn\u2211 i=1 |x\u2032i \u2212 xi| \u2264 nmax. (4)\nThe solution to problem (4) is straightforward. First, note that inserting (obfuscating) a word results in switching the corresponding feature value from 0 to 1 (1 to 0), and that the minimum of g(\u00b7) is attained when all features that have been assigned a negative (positive) weight are equal to 1 (0). Accordingly, for a given x, the largest decrease of g(A(x)) subject to the above constraint is obtained by analyzing all features for decreasing values of |wi|, and switching from 0 to 1 (from 1 to 0) the ones corresponding to wi < 0 (wi > 0), until nmax changes have been made, or all features have been analyzed. Note that this attack can be simulated by directly manipulating the feature vectors of spam emails instead of the emails themselves.\n2) Data model. Since the adversary can only manipulate testing data, we set ptr(Y ) = pD(Y ), and ptr(X|Y ) = pD(X|Y ). Assumptions (a.i-ii) are directly encoded as: (a.i) pts(Y ) = pD(Y ); and (a.ii) pts(A = T|Y = M) = 1, and pts(A = T|Y = L) = 0. The latter implies that pts(X|Y = L) = pD(X|Y = L). Assumption (a.iii) is encoded into the above function A(x), which empirically\n8 defines the distribution of the attack samples pts(X|Y = M, A = T). Similarly, p(X|Y = y,A = F) = pD(X|y), for y = L,M, will be approximated as the empirical distribution of the set Dy,FTS obtained from DTS, as described below; the same approximation will be made in all the considered application examples.\nThe definition of the attack scenario and the data model is summarized in Table 1 (first column).\n3) Construction of TR and TS. We use a publicly available email corpus, TREC 2007. It consists of 25,220 legitimate and 50,199 real spam emails.8 We select the first 20,000 emails in chronological order to create the data set D. We then split D into two subsets DTR and DTS, respectively made up of the first 10,000 and the next 10,000 emails, in chronological order. We use DTR to construct TR and DTS to construct TS, as in [5], [6]. Since the considered attack does not affect training samples, we set TR = DTR. Then, we define DL,FTS and D M,F TS respectively as the legitimate and malicious samples in DTS, and construct the set of attack samples DM,TTS by modifying all the samples in DM,FTS according to A(x), for any fixed nmax value. Since the attack does not affect the legitimate samples, we set DL,TTS = \u2205. Finally, we set the size of TS to 10,000, and generate TS by running Algorithm 1 on DL,FTS , D M,F TS and D M,T TS .\nThe features (words) are extracted from TR using the SpamAssassin tokenization method. Four feature subsets with size 1,000, 2,000, 10,000 and 20,000 have been selected using the information gain criterion [44].\n4) Performance evaluation. The performance measure we use is the area under the receiver operating characteristic curve (AUC) corresponding to false positive (FP) error rates in the range [0, 0.1] [6]: AUC10% =\u222b 0.1\n0 TP(FP)dFP \u2208 [0, 0.1], where TP is the true positive error rate. It is suited to classification tasks like spam filtering, where FP errors (i.e., legitimate emails misclassified as spam) are much more harmful than false negative (FN) ones.\nUnder the above model selection setting (two classifiers, and four feature subsets) eight different classifier models must be evaluated. Each model is trained on TR. SVMs are implemented with the LibSVM software [45]. The C parameter of their learning algorithm is chosen by maximizing the AUC10% through a 5-fold crossvalidation on TR. An online gradient descent algorithm is used for LR. After classifier training, the AUC10% value is assessed on TS, for different values of nmax. In this case, it is a monotonically decreasing function of nmax. The more graceful its decrease, the more robust the classifier is to the considered attack. Note that, for nmax = 0, no attack samples are included in the testing set: the corresponding AUC10% value equals that attained by classical performance evaluation methods.\nThe results are reported in Fig. 3. As expected, the AUC10% of each model decreases as nmax increases. It drops to zero for nmax values between 30 and 50\n8. http://plg.uwaterloo.ca/\u223cgvcormac/treccorpus07\nFig. 3. AUC10% attained on TS as a function of nmax, for the LR (top) and SVM (bottom) classifier, with 1,000 (1K), 2,000 (2K), 10,000 (10K) and 20,000 (20K) features. The AUC10% value for nmax = 0, corresponding to classical performance evaluation, is also reported in the legend between square brackets.\n(depending on the classifier): this means that all testing spam emails got misclassified as legitimate, after adding or obfuscating from 30 to 50 words.\nThe SVM and LR classifiers perform very similarly when they are not under attack (i.e., for nmax = 0), regardless of the feature set size; therefore, according to the viewpoint of classical performance evaluation, the designer could choose any of the eight models. However, security evaluation highlights that they exhibit a very different robustness to the considered attack, since their AUC10% value decreases at very different rates as nmax increases; in particular, the LR classifier with 20,000 features clearly outperforms all the other ones, for all nmax values. This result suggests the designer a very different choice than the one coming from classical performance evaluation: the LR classifier with 20,000 features should be selected, given that it exhibit the same accuracy as the other ones in the absence of attacks, and a higher security under the considered attack."}, {"heading": "4.2 Biometric authentication", "text": "Multimodal biometric systems for personal identity recognition have received great interest in the past few years. It has been shown that combining information coming from different biometric traits can overcome the limits and the weaknesses inherent in every individual biometric, resulting in a higher accuracy. Moreover, it is commonly believed that multimodal systems also improve security against spoofing attacks, which consist of claiming a false identity and submitting at least one\n9 fake biometric trait to the system (e.g., a \u201cgummy\u201d fingerprint or a photograph of a user\u2019s face). The reason is that, to evade a multimodal system, one expects that the adversary should spoof all the corresponding biometric traits. In this application example, we show how the designer of a multimodal system can verify if this hypothesis holds, before deploying the system, by simulating spoofing attacks against each of the matchers. To this end, we partially exploit the analysis in [1], [2].\nWe consider a typical multimodal system, made up of a fingerprint and a face matcher, which operates as follows. The design phase includes the enrollment of authorized users (clients): reference templates of their biometric traits are stored into a database, together with the corresponding identities. During operation, each user provides the requested biometric traits to the sensors, and claims the identity of a client. Then, each matcher compares the submitted trait with the template of the claimed identity, and provides a real-valued matching score: the higher the score, the higher the similarity. We denote the score of the fingerprint and the face matcher respectively as xfing and xface. Finally, the matching scores are combined through a proper fusion rule to decide whether the claimed identity is the user\u2019s identity (genuine user) or not (impostor).\nAs in [1], [2], we consider the widely used likelihood ratio (LLR) score fusion rule [46], and the matching scores as its features. Denoting as x the feature vector (xfing, xface), the LLR can be written as:\nLLR(x) = { L if p(x|Y = L)/p(x|Y = M) \u2265 t, M if p(x|Y = L)/p(x|Y = M) < t, (5)\nwhere L and M denote respectively the \u201cgenuine\u201d and \u201cimpostor\u201d class, and t is a decision threshold set according to application requirements. The distributions p(xfing, xface|Y ) are usually estimated from training data, while t is estimated from a validation set.\n1) Attack scenario. Goal. In this case, each malicious user (impostor) aims at being accepted as a legitimate (genuine) one. This corresponds to a targeted integrity violation, where the adversary\u2019s goal is to maximize the matching score.\nKnowledge. As in [1], [2], we assume that each impostor knows: (k.i) the identity of the targeted client; and (k.ii) the biometric traits used by the system. No knowledge of (k.iii) the decision function and (k.iv) its parameters is assumed, and (k.v) no feedback is available from the classifier.\nCapability. We assume that: (c.i) spoofing attacks affect only testing data (exploratory attack); (c.ii) they do not affect the class priors;9 and, according to (k.i), (c.iii) each adversary (impostor) controls her testing malicious samples. We limit the capability of each impostor by assuming that (c.iv) only one specific trait can be spoofed at a time (i.e., only one feature can be manipulated), and\n9. Further, since the LLR considers only the ratio between the classconditional distributions, changing the class priors is irrelevant.\nthat it is the same for all impostors. We will thus repeat the evaluation twice, considering fingerprint and face spoofing, separately. Defining how impostors can manipulate the features (in this case, the matching scores) in a spoofing attack is not straightforward. The standard way of evaluating the security of biometric systems against spoofing attacks consists of fabricating fake biometric traits and assessing performance when they are submitted to the system. However, this is a cumbersome task. Here we follow the approach of [1], [2], in which the impostor is assumed to fabricate \u201cperfect\u201d fake traits, i.e., fakes that produce the same matching score of the \u201clive\u201d trait of the targeted client. This allows one to use the available genuine scores to simulate the fake scores, avoiding the fabrication of fake traits. On the other hand, this assumption may be too pessimistic. This limitation could be overcome by developing more realistic models of the distribution of the fake traits; however, this is a challenging research issue on its own, that is out of the scope of this work, and part of the authors\u2019 ongoing work [47], [48].\nAttack strategy. The above attack strategy modifies only the testing data, and: (a.i) it does not modify the class priors; (a.ii) it does not affect the genuine class, but all impostors; and (a.iii) any impostor spoofs the considered biometric trait (either the face or fingerprint) of the known identity. According to the above assumption, as in [1], [2], any spoofing attack is simulated by replacing the corresponding impostor score (either the face or fingerprint) with the score of the targeted genuine user (chosen at random from the legitimate samples in the testing set). This can be represented with a function A(x), that, given an impostor x = (xfing, xface), returns either a vector (x\u2032fing, xface) (for fingerprint spoofing) or (xfing, x \u2032 face) (for face spoofing), where x \u2032 fing and x \u2032 face are the matching scores of the targeted genuine user. 2) Data model. Since training data is not affected, we set ptr(X, Y ) = pD(X, Y ). Then, we set (a.i) pts(Y ) = pD(Y ); and (a.ii) pts(A = T|Y = L) = 0 and pts(A = T|Y = M) = 1. Thus, pts(X|Y = L) = pD(X|Y = L), while all the malicious samples in TS have to be manipulated by A(x), according to (a.iii). This amounts to empirically defining pts(X|Y = M, A = T). As in the spam filtering case, pts(X|Y = y,A = F), for y = L,M, will be empirically approximated from the available data in D.\nThe definition of the above attack scenario and data model is summarized in Table 1 (second column).\n3) Construction of TR and TS. We use the NIST Biometric Score Set, Release 1.10 It contains raw similarity scores obtained on a set of 517 users from two different face matchers (named \u2018G\u2019 and \u2018C\u2019), and from one fingerprint matcher using the left and right index finger. For each user, one genuine score and 516 impostor scores are available for each matcher and each modality, for a total of 517 genuine and 266,772 impostor samples. We use the scores of the \u2018G\u2019 face matcher and the ones\n10. http://www.itl.nist.gov/iad/894.03/biometricscores/\n10\nof the fingerprint matcher for the left index finger, and normalize them in [0, 1] using the min-max technique. The data set D is made up of pairs of face and fingerprint scores, each belonging to the same user. We first randomly subdivide D into a disjoint training and testing set, DTR and DTS, containing respectively 80% and 20% of the samples. As the attack does not affect the training samples, we set TR= DTR. The sets DL,FTS and D M,F TS are constructed using DTS, while DL,FTS = \u2205. The set of attack samples DM,TTS is obtained by modifying each sample of DM,FTS with A(x). We finally set the size of TS as |DTS|, and run Algorithm 1 to obtain it.\n4) Performance evaluation. In biometric authentication tasks, the performance is usually measured in terms of genuine acceptance rate (GAR) and false acceptance rate (FAR), respectively the fraction of genuine and impostor attempts that are accepted as genuine by the system. We use here the complete ROC curve, which shows the GAR as a function of the FAR for all values of the decision threshold t (see Eq. 5).\nTo estimate p(X|Y = y), for y = L,M, in the LLR score fusion rule (Eq. 5), we assume that xfing and xface are conditionally independent given Y , as usually done in biometrics. We thus compute a maximum likelihood estimate of p(xfing, xface|Y ) from TR using a product of two Gamma distributions, as in [1].\nFig. 4 shows the ROC curve evaluated with the standard approach (without spoofing attacks), and the ones corresponding to the simulated spoofing attacks. The FAR axis is in logarithmic scale to focus on low FAR values, which are the most relevant ones in security applications. For any operational point on the ROC curve (namely, for any value of the decision threshold t), the effect of the considered spoofing attack is to increase the FAR, while the GAR does not change. This corresponds to a shift of the ROC curve to the right. Accordingly, the FAR under attack must be compared with the original one, for the same GAR.\nFig. 4 clearly shows that the FAR of the biometric system significantly increases under the considered attack scenario, especially for fingerprint spoofing. As far as this attack scenario is deemed to be realistic, the system\ndesigner should conclude that the considered system can be evaded by spoofing only one biometric trait, and thus does not exhibit a higher security than each of its individual classifiers. For instance, in applications requiring relatively high security, a reasonable choice may be to chose the operational point with FAR=10\u22123 and GAR=0.90, using classical performance evaluation, i.e., without taking into account spoofing attacks. This means that the probability that the deployed system wrongly accepts an impostor as a genuine user (the FAR) is expected to be 0.001. However, under face spoofing, the corresponding FAR increases to 0.10, while it jumps to about 0.70 under fingerprint spoofing. In other words, an impostor who submits a perfect replica of the face of a client has a probability of 10% of being accepted as genuine, while this probability is as high as 70%, if she is able to perfectly replicate the fingerprint. This is unacceptable for security applications, and provides further support to the conclusions of [1], [2] against the common belief that multimodal biometric systems are intrinsically more robust than unimodal systems."}, {"heading": "4.3 Network intrusion detection", "text": "Intrusion detection systems (IDSs) analyze network traffic to prevent and detect malicious activities like intrusion attempts, port scans, and denial-of-service attacks.11 When suspected malicious traffic is detected, an alarm is raised by the IDS and subsequently handled by the system administrator. Two main kinds of IDSs exist: misuse detectors and anomaly-based ones. Misuse detectors match the analyzed network traffic against a database of signatures of known malicious activities (e.g., Snort).12 The main drawback is that they are not able to detect never-before-seen malicious activities, or even variants of known ones. To overcome this issue, anomaly-based detectors have been proposed. They build a statistical model of the normal traffic using machine learning techniques, usually one-class classifiers (e.g., PAYL [49]), and raise an alarm when anomalous traffic is detected. Their training set is constructed, and periodically updated to follow the changes of normal traffic, by collecting unsupervised network traffic during operation, assuming that it is normal (it can be filtered by a misuse detector, and should be discarded if some system malfunctioning occurs during its collection). This kind of IDS is vulnerable to causative attacks, since an attacker may inject carefully designed malicious traffic during the collection of training samples to force the IDS to learn a wrong model of the normal traffic [11], [17], [29], [30], [36].\n11. The term \u201cattack\u201d is used in this field to denote a malicious activity, even when there is no deliberate attempt of misleading an IDS. In adversarial classification, as in this paper, this term is used to specifically denote the attempt of misleading a classifier, instead. To avoid any confusion, in the following we will refrain from using the term \u201cattack\u201d with the former meaning, using paraphrases, instead.\n12. http://www.snort.org/\n11\nHere we assume that an anomaly-based IDS is being designed, using a one-class \u03bd-SVM classifier with a radial basis function (RBF) kernel and the feature vector representation proposed in [49]. Each network packet is considered as an individual sample to be labeled as normal (legitimate) or anomalous (malicious), and is represented as a 256-dimensional feature vector, defined as the histogram of byte frequencies in its payload (this is known as \u201c1-gram\u201d representation in the IDS literature). We then focus on the model selection stage. In the above setting, it amounts to choosing the values of the \u03bd parameter of the learning algorithm (which is an upper bound on the false positive error rate on training data [50]), and the \u03b3 value of the RBF kernel. For the sake of simplicity, we assume that \u03bd is set to 0.01 as suggested in [24], so that only \u03b3 has to be chosen.\nWe show how the IDS designer can select a model (the value of \u03b3) based also on the evaluation of classifier security. We focus on a causative attack similar to the ones considered in [38], aimed at forcing the learned model of normal traffic to include samples of intrusions to be attempted during operation. To this end, the attack samples should be carefully designed such that they include some features of the desired intrusive traffic, but do not perform any real intrusion (otherwise, the collected traffic may be discarded, as explained above).\n1) Attack scenario. Goal. This attack aims to cause an indiscriminate integrity violation by maximizing the fraction of malicious testing samples misclassified as legitimate.\nKnowledge. The adversary is assumed to know: (k.ii) the feature set; and (k.iii) that a one-class classifier is used. No knowledge of (k.i) the training data and (k.iv) the classifiers\u2019 parameters is available to the adversary, as well as (k.v) any feedback from the classifier.\nCapability. The attack consists of injecting malicious samples into the training set. Accordingly, we assume that: (c.i) the adversary can inject malicious samples into the training data, without manipulating testing data (causative attack); (c.ii) she can modify the class priors by injecting a maximum fraction pmax of malicious samples into the training data; (c.iii) all the injected malicious samples can be manipulated; and (c.iv) the adversary is able to completely control the feature values of the malicious attack samples. As in [17], [28]\u2013[30], we repeat the security evaluation for pmax \u2208 [0, 0.5], since it is unrealistic that the adversary can control the majority of the training data.\nAttack strategy. The goal of maximizing the percentage of malicious testing samples misclassified as legitimate, for any pmax value, can be attained by manipulating the training data with the following attack strategy: (a.i) the maximum percentage of malicious samples pmax will be injected into the training data; (a.ii) all the injected malicious samples will be attack samples, while no legitimate samples will be affected; and (a.iii) the feature values of the attack samples will be equal to that of the malicious testing samples. To understand the latter assumption,\nnote that: first, the best case for the adversary is when the attack samples exhibit the same histogram of the payload\u2019s byte values as the malicious testing samples, as this intuitively forces the model of the normal traffic to be as similar as possible to the distribution of the testing malicious samples; and, second, this does not imply that the attack samples perform any intrusion, allowing them to avoid detection. In fact, one may construct an attack packet which exhibit the same feature values of an intrusive network packet (i.e., a malicious testing sample), but looses its intrusive functionality, by simply shuffling its payload bytes. Accordingly, for our purposes, we directly set the feature values of the attack samples equal to those of the malicious testing samples, without constructing real network packets.\n2) Data model. Since testing data is not affected by the considered causative attack, we set pts(X, Y ) = pD(X, Y ). We then encode the attack strategy as follows. According to (a.ii), ptr(A = T|Y = L) = 0 and ptr(A = T|Y = M) = 1. The former assumption implies that ptr(X|Y = L) = pD(X|Y = L). Since the training data of one-class classifiers does not contain any malicious samples (see explanation at the beginning of this section), i.e., ptr(A = F|Y = M) = 0, the latter assumption implies that the class prior ptr(Y = M) corresponds exactly to the fraction of attack samples in the training set. Therefore, assumption (a.i) amounts to setting ptr(Y = M) = pmax. Finally, according to (a.iii), the distribution of attack samples ptr(X|Y = M, A = T) will be defined as the empirical distribution of the malicious testing samples, namely, DM,TTR = D M,F TS . The distribution ptr(X|Y = L, A = F) = pD(X|L) will also be empirically defined as the set DL,FTR .\nThe definition of the attack scenario and data model is summarized in Table 1 (third column).\n3) Construction of TR and TS. We use the data set of [24]. It consists of 1,699,822 legitimate samples (network packets) collected by a web server during five days in 2006, and a publicly available set of 205 malicious samples coming from intrusions which exploit the HTTP protocol [51].13 To construct TR and TS, we take into account the chronological order of network packets as in Sect. 4.1, and the fact that in this application all the malicious samples in D must be inserted in the testing set [27], [51]. Accordingly, we set DTR as the first 20,000 legitimate packets of day one, and DTS as the first 20,000 legitimate samples of day two, plus all the malicious samples. Since this attack does not affect testing samples, we set TS = DTS. We then set DL,FTR = DTR, and DM,FTR = D L,T TR = \u2205. Since the feature vectors of attack samples are identical to those of the malicious samples, as above mentioned, we set DM,TTR = D M,F TS (where the latter set clearly includes the malicious samples in DTS). The size of TR is initially set to 20,000. We then consider different attack scenarios by increasing the number of attack samples in the training data, up to 40,000 samples\n13. http://www.i-pi.com/HTTP-attacks-JoCN-2006/\n12\nin total, that corresponds to pmax = 0.5. For each value of pmax, TR is obtained by running Algorithm 1 with the proper inputs.\n4) Performance evaluation. Classifier performance is assessed using the AUC10% measure, for the same reasons as in Sect. 4.1. The performance under attack is evaluated as a function of pmax, as in [17], [28], which reduces to the classical performance evaluation when pmax = 0. For the sake of simplicity, we consider only two values of the parameter \u03b3, which clearly point out how design choices based only on classical performance evaluation methods can be unsuitable for adversarial environments.\nThe results are reported in Fig. 5. In the absence of attacks (pmax = 0), the choice \u03b3 = 0.5 appears slightly better than \u03b3 = 0.01. Under attack, the performance for \u03b3 = 0.01 remains almost identical as the one without attack, and starts decreasing very slightly only when the percentage of attack samples in the training set exceeds 30%. On the contrary, for \u03b3 = 0.5 the performance suddenly drops as pmax increases, becoming lower than the one for \u03b3 = 0.01 when pmax is as small as about 1%. The reason for this behavior is the following. The attack samples can be considered outliers with respect to the legitimate training samples, and, for large \u03b3 values of the RBF kernel, the SVM discriminant function tends to overfit, forming a \u201cpeak\u201d around each individual training sample. Thus, it exhibits relatively high values also in the region of the feature space where the attack samples lie, and this allows many of the corresponding testing intrusions. Conversely, this is not true for lower \u03b3 values, where the higher spread of the RBF kernel leads to a smoother discriminant function, which exhibits much lower values for the attack samples.\nAccording to the above results, the choice of \u03b3 = 0.5, suggested by classical performance evaluation for pmax = 0 is clearly unsuitable from the viewpoint of classifier security under the considered attack, unless it is deemed unrealistic that an attacker can inject more than\n1% attack samples into the training set. To summarize, the designer should select \u03b3 = 0.01 and trade a small decrease of classification accuracy in the absence of attacks for a significant security improvement."}, {"heading": "5 SECURE DESIGN CYCLE: NEXT STEPS", "text": "The classical design cycle of a pattern classifier [9] consists of: data collection, data pre-processing, feature extraction and selection, model selection (including the choice of the learning and classification algorithms, and the tuning of their parameters), and performance evaluation. We pointed out that this design cycle disregards the threats that may arise in adversarial settings, and extended the performance evaluation step to such settings. Revising the remaining steps under a security viewpoint remains a very interesting issue for future work. Here we briefly outline how this open issue can be addressed.\nIf the adversary is assumed to have some control over the data collected for classifier training and parameter tuning, a filtering step to detect and remove attack samples should also be performed (see, e.g., the data sanitization method of [27]).\nFeature extraction algorithms should be designed to be robust to sample manipulation. Alternatively, features which are more difficult to be manipulated should be used. For instance, in [52] inexact string matching was proposed to counteract word obfuscation attacks in spam filtering. In biometric recognition, it is very common to use additional input features to detect the presence (\u201cliveness detection\u201d) of attack samples coming from spoofing attacks, i.e., fake biometric traits [53]. The adversary could also undermine feature selection, e.g., to force the choice of a set of features that are easier to manipulate, or that are not discriminant enough with respect to future attacks. Therefore, feature selection algorithms should be designed by taking into account not only the discriminant capability, but also the robustness of features to adversarial manipulation.\nModel selection is clearly the design step that is more subject to attacks. Selected algorithms should be robust to causative and exploratory attacks. In particular, robust learning algorithms should be adopted, if no data sanitization can be performed. The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].\nFinally, a secure system should also guarantee the privacy of its users, against attacks aimed at stealing confidential information [14]. For instance, privacy preserving methods have been proposed in biometric recognition systems to protect the users against the so-called hillclimbing attacks, whose goal is to get information about the users\u2019 biometric traits [41], [53]. Randomization of some classifiers parameters has been also proposed to preserve privacy in [14], [54].\n13"}, {"heading": "6 CONTRIBUTIONS, LIMITATIONS AND OPEN ISSUES", "text": "In this paper we focused on empirical security evaluation of pattern classifiers that have to be deployed in adversarial environments, and proposed how to revise the classical performance evaluation design step, which is not suitable for this purpose.\nOur main contribution is a framework for empirical security evaluation that formalizes and generalizes ideas from previous work, and can be applied to different classifiers, learning algorithms, and classification tasks. It is grounded on a formal model of the adversary, and on a model of data distribution that can represent all the attacks considered in previous work; provides a systematic method for the generation of training and testing sets that enables security evaluation; and can accommodate application-specific techniques for attack simulation. This is a clear advancement with respect to previous work, since without a general framework most of the proposed techniques (often tailored to a given classifier model, attack, and application) could not be directly applied to other problems.\nAn intrinsic limitation of our work is that security evaluation is carried out empirically, and it is thus datadependent; on the other hand, model-driven analyses [12], [17], [38] require a full analytical model of the problem and of the adversary\u2019s behavior, that may be very difficult to develop for real-world applications. Another intrinsic limitation is due to fact that our method is not application-specific, and, therefore, provides only highlevel guidelines for simulating attacks. Indeed, detailed guidelines require one to take into account applicationspecific constraints and adversary models. Our future work will be devoted to develop techniques for simulating attacks for different applications.\nAlthough the design of secure classifiers is a distinct problem than security evaluation, our framework could be also exploited to this end. For instance, simulated attack samples can be included into the training data to improve security of discriminative classifiers (e.g., SVMs), while the proposed data model can be exploited to design more secure generative classifiers. We obtained encouraging preliminary results on this topic [32]."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors are grateful to Davide Ariu, Gavin Brown, Pavel Laskov, and Blaine Nelson for discussions and comments on an earlier version of this paper. This work was partly supported by a grant awarded to Battista Biggio by Regione Autonoma della Sardegna, PO Sardegna FSE 2007-2013, L.R. 7/2007 \u201cPromotion of the scientific research and technological innovation in Sardinia\u201d, by the project CRP-18293 funded by Regione Autonoma della Sardegna, L.R. 7/2007, Bando 2009, and by the TABULA RASA project, funded within the 7th Framework Research Programme of the European Union."}], "references": [{"title": "Robustness of multimodal biometric fusion methods against spoof attacks", "author": ["R.N. Rodrigues", "L.L. Ling", "V. Govindaraju"], "venue": "J. Vis. Lang. Comput., vol. 20, no. 3, pp. 169\u2013179, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Multimodal fusion vulnerability to non-zero effort (spoof) imposters", "author": ["P. Johnson", "B. Tan", "S. Schuckers"], "venue": "IEEE Int\u2019l Workshop on Inf. Forensics and Security, 2010, pp. 1\u20135.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Polymorphic blending attacks", "author": ["P. Fogla", "M. Sharif", "R. Perdisci", "O. Kolesnikov", "W. Lee"], "venue": "Proc. 15th Conf. on USENIX Security Symp. CA, USA: USENIX Association, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "On attacking statistical spam filters", "author": ["G.L. Wittel", "S.F. Wu"], "venue": "1st Conf. on Email and Anti-Spam, CA, USA, 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Good word attacks on statistical spam filters", "author": ["D. Lowd", "C. Meek"], "venue": "2nd Conf. on Email and Anti-Spam, CA, USA, 2005.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Feature weighting for improved classifier robustness", "author": ["A. Kolcz", "C.H. Teo"], "venue": "6th Conf. on Email and Anti-Spam, CA, USA, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Adversarial knowledge discovery", "author": ["D.B. Skillicorn"], "venue": "IEEE Intell. Syst., vol. 24, pp. 54\u201361, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Adversarial information retrieval: The manipulation of web content", "author": ["D. Fetterly"], "venue": "ACM Computing Reviews, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "Mausam", "S. Sanghai", "D. Verma"], "venue": "10th ACM SIGKDD Int\u2019l Conf. on Knowl. Discovery and Data Mining, WA, USA, 2004, pp. 99\u2013108.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Can machine learning be secure?", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "in Proc. Symp. Inf., Computer and Commun. Sec. (ASIACCS). NY, USA: ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Evaluation of classifiers: Practical considerations for security applications", "author": ["A.A. C\u00e1rdenas", "J.S. Baras"], "venue": "AAAI Workshop on Evaluation Methods for Machine Learning, MA, USA, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Machine learning in adversarial environments", "author": ["P. Laskov", "R. Lippmann"], "venue": "Machine Learning, vol. 81, pp. 115\u2013119, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B. Rubinstein", "J.D. Tygar"], "venue": "4th ACM Workshop on Artificial Intelligence and Security, IL, USA, 2011, pp. 43\u201357.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A. Joseph", "J. Tygar"], "venue": "Machine Learning, vol. 81, pp. 121\u2013148, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "Proc. 11th ACM SIGKDD Int\u2019l Conf. on Knowl. Discovery and Data Mining, A. Press, Ed., IL, USA, 2005, pp. 641\u2013647.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "A framework for quantitative security analysis of machine learning", "author": ["P. Laskov", "M. Kloft"], "venue": "Proc. 2nd ACM Workshop on Security and Artificial Intelligence. NY, USA: ACM, 2009, pp. 1\u20134.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "A framework for generating data to simulate changing environments", "author": ["A.M. Narasimhamurthy", "L.I. Kuncheva"], "venue": "Artificial Intell. and Applications. IASTED/ACTA Press, 2007, pp. 415\u2013420.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "What-if analysis", "author": ["S. Rizzi"], "venue": "Enc. of Database Systems, pp. 3525\u2013 3529, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Paragraph: Thwarting signature learning by training maliciously", "author": ["J. Newsome", "B. Karp", "D. Song"], "venue": "Recent Advances in Intrusion Detection, ser. LNCS. Springer, 2006, pp. 81\u2013105.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["A. Globerson", "S.T. Roweis"], "venue": "Proc. 23rd Int\u2019l Conf. on Machine Learning. ACM, 2006, pp. 353\u2013360.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Using an ensemble of oneclass SVM classifiers to harden payload-based anomaly detection systems", "author": ["R. Perdisci", "G. Gu", "W. Lee"], "venue": "Int\u2019l Conf. Data Mining. IEEE CS, 2006, pp. 488\u2013498.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Advanced allergy attacks: does a corpus really help", "author": ["S.P. Chung", "A.K. Mok"], "venue": "Recent Advances in Intrusion Detection, ser. RAID \u201907. Berlin, Heidelberg: Springer-Verlag, 2007, pp. 236\u2013255.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "A multiple instance learning strategy for combating good word attacks on spam filters", "author": ["Z. Jorgensen", "Y. Zhou", "M. Inge"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 1115\u20131146, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Casting out demons: Sanitizing training data for anomaly sensors", "author": ["G.F. Cretu", "A. Stavrou", "M.E. Locasto", "S.J. Stolfo", "A.D. Keromytis"], "venue": "IEEE Symp. on Security and Privacy. CA, USA: IEEE CS, 2008, pp. 81\u201395.  14", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Exploiting machine learning to subvert your spam filter", "author": ["B. Nelson", "M. Barreno", "F.J. Chi", "A.D. Joseph", "B.I.P. Rubinstein", "U. Saini", "C. Sutton", "J.D. Tygar", "K. Xia"], "venue": "Proc. 1st Workshop on Large-Scale Exploits and Emergent Threats. CA, USA: USENIX Association, 2008, pp. 1\u20139.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Antidote: understanding and defending against poisoning of anomaly detectors", "author": ["B.I. Rubinstein", "B. Nelson", "L. Huang", "A.D. Joseph", "S.-h. Lau", "S. Rao", "N. Taft", "J.D. Tygar"], "venue": "Proc. 9th ACM SIGCOMM Internet Measurement Conf., ser. IMC \u201909. NY, USA: ACM, 2009, pp. 1\u201314.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Online anomaly detection under adversarial impact", "author": ["M. Kloft", "P. Laskov"], "venue": "Proc. 13th Int\u2019l Conf. on Artificial Intell. and Statistics, 2010, pp. 405\u2013412.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning to classify with missing and corrupted features", "author": ["O. Dekel", "O. Shamir", "L. Xiao"], "venue": "Machine Learning, vol. 81, pp. 149\u2013178, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Design of robust classifiers for adversarial environments", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Int\u2019l Conf. on Systems, Man, and Cybernetics, 2011, pp. 977\u2013982.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple classifier systems for robust classifier design in adversarial environments", "author": ["\u2014\u2014"], "venue": "Int\u2019l Journal of Machine Learning and Cybernetics, vol. 1, no. 1, pp. 27\u201341, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Bagging classifiers for fighting poisoning attacks in adversarial environments", "author": ["B. Biggio", "I. Corona", "G. Fumera", "G. Giacinto", "F. Roli"], "venue": "Proc. 10th Int\u2019l Workshop on Multiple Classifier Systems, ser. LNCS, vol. 6713. Springer-Verlag, 2011, pp. 350\u2013359.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Poisoning adaptive biometric systems", "author": ["B. Biggio", "G. Fumera", "F. Roli", "L. Didaci"], "venue": "Structural, Syntactic, and Statistical Pattern Recognition, ser. LNCS, vol. 7626. Springer, 2012, pp. 417\u2013425.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "Proc. 29th Int\u2019l Conf. on Machine Learning, 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning in the presence of malicious errors", "author": ["M. Kearns", "M. Li"], "venue": "SIAM J. Comput., vol. 22, no. 4, pp. 807\u2013837, 1993.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1993}, {"title": "A framework for the evaluation of intrusion detection systems", "author": ["A.A. C\u00e1rdenas", "J.S. Baras", "K. Seamon"], "venue": "Proc. IEEE Symp. on Security and Privacy. DC, USA: IEEE CS, 2006, pp. 63\u201377.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}, {"title": "Multiple classifier systems for adversarial classification tasks", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "Proc. 8th Int\u2019l Workshop on Multiple Classifier Systems, ser. LNCS, vol. 5519. Springer, 2009, pp. 132\u2013141.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Static prediction games for adversarial learning problems", "author": ["M. Br\u00fcckner", "C. Kanzow", "T. Scheffer"], "venue": "J. Mach. Learn. Res., vol. 13, pp. 2617\u20132654, 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Vulnerabilities in biometric encryption systems", "author": ["A. Adler"], "venue": "5th Int\u2019l Conf. on Audio- and Video-Based Biometric Person Authentication, ser. LNCS, vol. 3546. NY, USA: Springer, 2005, pp. 1100\u20131109.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2005}, {"title": "An Introduction to the Bootstrap", "author": ["B. Efron", "R.J. Tibshirani"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1993}, {"title": "Support vector machines for spam categorization", "author": ["H. Drucker", "D. Wu", "V.N. Vapnik"], "venue": "IEEE Trans. on Neural Networks, vol. 10, no. 5, pp. 1048\u20131054, 1999.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1999}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM Comput. Surv., vol. 34, pp. 1\u201347, 2002.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "LibSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "2001. [Online]. Available: http://www.csie.ntu.edu. tw/\u223ccjlin/libsvm/", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2001}, {"title": "Likelihood ratio-based biometric score fusion", "author": ["K. Nandakumar", "Y. Chen", "S.C. Dass", "A. Jain"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intell., vol. 30, pp. 342\u2013347, February 2008.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Robustness of multi-modal biometric verification systems under realistic spoofing attacks", "author": ["B. Biggio", "Z. Akhtar", "G. Fumera", "G. Marcialis", "F. Roli"], "venue": "Int\u2019l Joint Conf. on Biometrics, 2011, pp. 1\u20136.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Security evaluation of biometric authentication systems under real spoofing attacks", "author": ["B. Biggio", "Z. Akhtar", "G. Fumera", "G.L. Marcialis", "F. Roli"], "venue": "IET Biometrics, vol. 1, no. 1, pp. 11\u201324, 2012.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Anomalous payload-based network intrusion detection", "author": ["K. Wang", "S.J. Stolfo"], "venue": "RAID, ser. LNCS, vol. 3224. Springer, 2004, pp. 203\u2013222.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2004}, {"title": "New support vector algorithms", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "R.C. Williamson", "P.L. Bartlett"], "venue": "Neural Comput., vol. 12, no. 5, pp. 1207\u20131245, 2000.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2000}, {"title": "Comparing anomaly detection techniques for http", "author": ["K. Ingham", "H. Inoue"], "venue": "Recent Advances in Intrusion Detection, ser. LNCS. Springer, 2007, pp. 42\u201362.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}, {"title": "Spam filtering using inexact string matching in explicit feature space with on-line linear classifiers", "author": ["D. Sculley", "G. Wachman", "C.E. Brodley"], "venue": "15th Text Retrieval Conf. NIST, 2006.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Well known examples of attacks against pattern classifiers are: submitting a fake biometric trait to a biometric authentication system (spoofing attack) [1], [2]; modifying network packets belonging to intrusive traffic to evade intrusion detection systems [3]; manipulating the", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "Well known examples of attacks against pattern classifiers are: submitting a fake biometric trait to a biometric authentication system (spoofing attack) [1], [2]; modifying network packets belonging to intrusive traffic to evade intrusion detection systems [3]; manipulating the", "startOffset": 158, "endOffset": 161}, {"referenceID": 2, "context": "Well known examples of attacks against pattern classifiers are: submitting a fake biometric trait to a biometric authentication system (spoofing attack) [1], [2]; modifying network packets belonging to intrusive traffic to evade intrusion detection systems [3]; manipulating the", "startOffset": 257, "endOffset": 260}, {"referenceID": 3, "context": ", by misspelling common spam words to avoid their detection) [4]\u2013[6].", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": ", by misspelling common spam words to avoid their detection) [4]\u2013[6].", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "Adversarial scenarios can also occur in intelligent data analysis [7] and information retrieval [8]; e.", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "Adversarial scenarios can also occur in intelligent data analysis [7] and information retrieval [8]; e.", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "allowing adversaries to undermine their effectiveness [6], [10]\u2013[14].", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "allowing adversaries to undermine their effectiveness [6], [10]\u2013[14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "allowing adversaries to undermine their effectiveness [6], [10]\u2013[14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 148, "endOffset": 152}, {"referenceID": 5, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 306, "endOffset": 309}, {"referenceID": 10, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 311, "endOffset": 315}, {"referenceID": 14, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 317, "endOffset": 321}, {"referenceID": 15, "context": "three main open issues can be identified: (i) analyzing the vulnerabilities of classification algorithms, and the corresponding attacks [11], [14], [15]; (ii) developing novel methods to assess classifier security against these attacks, which is not possible using classical performance evaluation methods [6], [12], [16], [17]; (iii) developing", "startOffset": 323, "endOffset": 327}, {"referenceID": 0, "context": "novel design methods to guarantee classifier security in adversarial environments [1], [6], [10].", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "novel design methods to guarantee classifier security in adversarial environments [1], [6], [10].", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "novel design methods to guarantee classifier security in adversarial environments [1], [6], [10].", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "Although this emerging field is attracting growing interest [13], [18], [19], the above issues have only been sparsely addressed under different perspectives and to", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": ", [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": ", [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": ", [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": ", [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": ", [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": ", [3]\u2013[6], [10], while only a few theoretical models of adversarial classification problems have been proposed in the machine learning literature [10], [14], [15]; however, they do not yet", "startOffset": 158, "endOffset": 162}, {"referenceID": 9, "context": "sifiers was proposed in [11], [15], and subsequently extended in [14].", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "sifiers was proposed in [11], [15], and subsequently extended in [14].", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "sifiers was proposed in [11], [15], and subsequently extended in [14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "be affected by both exploratory and causative attacks, while the training data can only be affected by causative attacks, if the classifier is retrained online [11], [14], [15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 12, "context": "be affected by both exploratory and causative attacks, while the training data can only be affected by causative attacks, if the classifier is retrained online [11], [14], [15].", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "be affected by both exploratory and causative attacks, while the training data can only be affected by causative attacks, if the classifier is retrained online [11], [14], [15].", "startOffset": 172, "endOffset": 176}, {"referenceID": 16, "context": ", in the case of concept drift [20].", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "[21], which is a common practice in security.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "tion of problem knowledge and heuristic techniques [1]\u2013[6], [15], [22]\u2013[36].", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "tion of problem knowledge and heuristic techniques [1]\u2013[6], [15], [22]\u2013[36].", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "tion of problem knowledge and heuristic techniques [1]\u2013[6], [15], [22]\u2013[36].", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "tion of problem knowledge and heuristic techniques [1]\u2013[6], [15], [22]\u2013[36].", "startOffset": 66, "endOffset": 70}, {"referenceID": 32, "context": "tion of problem knowledge and heuristic techniques [1]\u2013[6], [15], [22]\u2013[36].", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 20, "endOffset": 24}, {"referenceID": 14, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 32, "endOffset": 36}, {"referenceID": 33, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 38, "endOffset": 42}, {"referenceID": 36, "context": "theory) [10], [12], [14], [16], [17], [37]\u2013[40].", "startOffset": 43, "endOffset": 47}, {"referenceID": 15, "context": "As in [17], it is formulated as the optimization of an objective function.", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "We propose to define this function based on the desired security violation (integrity, availability, or privacy), and on the attack specificity (from targeted to indiscriminate), according to the taxonomy in [11], [14] (see Sect.", "startOffset": 208, "endOffset": 212}, {"referenceID": 12, "context": "We propose to define this function based on the desired security violation (integrity, availability, or privacy), and on the attack specificity (from targeted to indiscriminate), according to the taxonomy in [11], [14] (see Sect.", "startOffset": 214, "endOffset": 218}, {"referenceID": 5, "context": "For instance, the goal of an indiscriminate integrity violation may be to maximize the fraction of misclassified malicious samples [6], [10], [14]; the goal of a targeted privacy violation may be to obtain some specific, confidential information", "startOffset": 131, "endOffset": 134}, {"referenceID": 8, "context": "For instance, the goal of an indiscriminate integrity violation may be to maximize the fraction of misclassified malicious samples [6], [10], [14]; the goal of a targeted privacy violation may be to obtain some specific, confidential information", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "For instance, the goal of an indiscriminate integrity violation may be to maximize the fraction of misclassified malicious samples [6], [10], [14]; the goal of a targeted privacy violation may be to obtain some specific, confidential information", "startOffset": 142, "endOffset": 146}, {"referenceID": 12, "context": ", the biometric trait of a given user enrolled in a biometric system) by exploiting the class labels assigned to some \u201cquery\u201d samples, while minimizing the number of query samples that the adversary has to issue to violate privacy [14], [16], [41].", "startOffset": 231, "endOffset": 235}, {"referenceID": 14, "context": ", the biometric trait of a given user enrolled in a biometric system) by exploiting the class labels assigned to some \u201cquery\u201d samples, while minimizing the number of query samples that the adversary has to issue to violate privacy [14], [16], [41].", "startOffset": 237, "endOffset": 241}, {"referenceID": 37, "context": ", the biometric trait of a given user enrolled in a biometric system) by exploiting the class labels assigned to some \u201cquery\u201d samples, while minimizing the number of query samples that the adversary has to issue to violate privacy [14], [16], [41].", "startOffset": 243, "endOffset": 247}, {"referenceID": 12, "context": ", the class labels assigned to some \u201cquery\u201d samples that the adversary issues to get feedback [14], [16], [41]).", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": ", the class labels assigned to some \u201cquery\u201d samples that the adversary issues to get feedback [14], [16], [41]).", "startOffset": 100, "endOffset": 104}, {"referenceID": 37, "context": ", the class labels assigned to some \u201cquery\u201d samples that the adversary issues to get feedback [14], [16], [41]).", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "assumptions about what can be kept fully secret from the adversary should be done, as discussed in [14].", "startOffset": 99, "endOffset": 103}, {"referenceID": 9, "context": "i) the attack influence (either causative or exploratory), as defined in [11], [14];", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "i) the attack influence (either causative or exploratory), as defined in [11], [14];", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": ", correlated features can not be modified independently, and the functionality of malicious samples can not be compromised [1], [3], [6], [10]).", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": ", correlated features can not be modified independently, and the functionality of malicious samples can not be compromised [1], [3], [6], [10]).", "startOffset": 128, "endOffset": 131}, {"referenceID": 5, "context": ", correlated features can not be modified independently, and the functionality of malicious samples can not be compromised [1], [3], [6], [10]).", "startOffset": 133, "endOffset": 136}, {"referenceID": 8, "context": ", correlated features can not be modified independently, and the functionality of malicious samples can not be compromised [1], [3], [6], [10]).", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "Our model can be easily extended to take into account concurrent attacks involving m > 1 different kinds of sample manipulations; for example, to model attacks against different classifiers in multimodal biometric systems [1], [2].", "startOffset": 222, "endOffset": 225}, {"referenceID": 1, "context": "Our model can be easily extended to take into account concurrent attacks involving m > 1 different kinds of sample manipulations; for example, to model attacks against different classifiers in multimodal biometric systems [1], [2].", "startOffset": 227, "endOffset": 230}, {"referenceID": 28, "context": ", see [32]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": ", [20]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 38, "context": "Accordingly, we can sample with replacement from D TR [42].", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": ", [5], [28], [34], [36]), then the sets D TR , for y = L,M, do not need to be constructed beforehand: a single attack sample can be generated online at step 5 of Algorithm 1, when a = T.", "startOffset": 2, "endOffset": 5}, {"referenceID": 24, "context": ", [5], [28], [34], [36]), then the sets D TR , for y = L,M, do not need to be constructed beforehand: a single attack sample can be generated online at step 5 of Algorithm 1, when a = T.", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": ", [5], [28], [34], [36]), then the sets D TR , for y = L,M, do not need to be constructed beforehand: a single attack sample can be generated online at step 5 of Algorithm 1, when a = T.", "startOffset": 13, "endOffset": 17}, {"referenceID": 32, "context": ", [5], [28], [34], [36]), then the sets D TR , for y = L,M, do not need to be constructed beforehand: a single attack sample can be generated online at step 5 of Algorithm 1, when a = T.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "samples incrementally; for instance, in the causative attacks considered in [11], [30], [36], the attack samples are added to the training data one at a time, since each of them is a function of the current training set.", "startOffset": 76, "endOffset": 80}, {"referenceID": 26, "context": "samples incrementally; for instance, in the causative attacks considered in [11], [30], [36], the attack samples are added to the training data one at a time, since each of them is a function of the current training set.", "startOffset": 82, "endOffset": 86}, {"referenceID": 32, "context": "samples incrementally; for instance, in the causative attacks considered in [11], [30], [36], the attack samples are added to the training data one at a time, since each of them is a function of the current training set.", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "has been considered by several authors [6], [28], [43], and it is included in several real spam filters.", "startOffset": 39, "endOffset": 42}, {"referenceID": 24, "context": "has been considered by several authors [6], [28], [43], and it is included in several real spam filters.", "startOffset": 44, "endOffset": 48}, {"referenceID": 39, "context": "has been considered by several authors [6], [28], [43], and it is included in several real spam filters.", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "appear in legitimate emails, and by obfuscating \u201cbad words\u201d that are typically present in spam [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "As in [6], [10], the adversary is assumed to have perfect knowledge of the classifier, i.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "As in [6], [10], the adversary is assumed to have perfect knowledge of the classifier, i.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": ", she can insert or obfuscate any word), but up to a maximum number nmax of features in each spam email [6], [10].", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": ", she can insert or obfuscate any word), but up to a maximum number nmax of features in each spam email [6], [10].", "startOffset": 109, "endOffset": 113}, {"referenceID": 8, "context": "As in [10], the generation of attack samples can be represented by a function A : X 7\u2192 X , and the number of modified words can be evaluated by the Hamming distance.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "We use DTR to construct TR and DTS to construct TS, as in [5], [6].", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "We use DTR to construct TR and DTS to construct TS, as in [5], [6].", "startOffset": 63, "endOffset": 66}, {"referenceID": 40, "context": "Four feature subsets with size 1,000, 2,000, 10,000 and 20,000 have been selected using the information gain criterion [44].", "startOffset": 119, "endOffset": 123}, {"referenceID": 5, "context": "1] [6]: AUC10% = \u222b 0.", "startOffset": 3, "endOffset": 6}, {"referenceID": 41, "context": "SVMs are implemented with the LibSVM software [45].", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "To this end, we partially exploit the analysis in [1], [2].", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "To this end, we partially exploit the analysis in [1], [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "As in [1], [2], we consider the widely used likelihood ratio (LLR) score fusion rule [46], and the matching scores as its features.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "As in [1], [2], we consider the widely used likelihood ratio (LLR) score fusion rule [46], and the matching scores as its features.", "startOffset": 11, "endOffset": 14}, {"referenceID": 42, "context": "As in [1], [2], we consider the widely used likelihood ratio (LLR) score fusion rule [46], and the matching scores as its features.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "As in [1], [2], we assume that each impostor", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "As in [1], [2], we assume that each impostor", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "Here we follow the approach of [1], [2], in which the impostor is assumed to fabricate \u201cperfect\u201d fake traits,", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "Here we follow the approach of [1], [2], in which the impostor is assumed to fabricate \u201cperfect\u201d fake traits,", "startOffset": 36, "endOffset": 39}, {"referenceID": 43, "context": "of the distribution of the fake traits; however, this is a challenging research issue on its own, that is out of the scope of this work, and part of the authors\u2019 ongoing work [47], [48].", "startOffset": 175, "endOffset": 179}, {"referenceID": 44, "context": "of the distribution of the fake traits; however, this is a challenging research issue on its own, that is out of the scope of this work, and part of the authors\u2019 ongoing work [47], [48].", "startOffset": 181, "endOffset": 185}, {"referenceID": 0, "context": "According to the above assumption, as in [1], [2], any spoofing attack is simulated by replacing", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "According to the above assumption, as in [1], [2], any spoofing attack is simulated by replacing", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "of the fingerprint matcher for the left index finger, and normalize them in [0, 1] using the min-max technique.", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": "We thus compute a maximum likelihood estimate of p(xfing, xface|Y ) from TR using a product of two Gamma distributions, as in [1].", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "is unacceptable for security applications, and provides further support to the conclusions of [1], [2] against the common belief that multimodal biometric systems are intrinsically more robust than unimodal systems.", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "is unacceptable for security applications, and provides further support to the conclusions of [1], [2] against the common belief that multimodal biometric systems are intrinsically more robust than unimodal systems.", "startOffset": 99, "endOffset": 102}, {"referenceID": 45, "context": ", PAYL [49]), and raise an alarm when anomalous traf-", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "kind of IDS is vulnerable to causative attacks, since an attacker may inject carefully designed malicious traffic during the collection of training samples to force the IDS to learn a wrong model of the normal traffic [11], [17], [29], [30], [36].", "startOffset": 218, "endOffset": 222}, {"referenceID": 15, "context": "kind of IDS is vulnerable to causative attacks, since an attacker may inject carefully designed malicious traffic during the collection of training samples to force the IDS to learn a wrong model of the normal traffic [11], [17], [29], [30], [36].", "startOffset": 224, "endOffset": 228}, {"referenceID": 25, "context": "kind of IDS is vulnerable to causative attacks, since an attacker may inject carefully designed malicious traffic during the collection of training samples to force the IDS to learn a wrong model of the normal traffic [11], [17], [29], [30], [36].", "startOffset": 230, "endOffset": 234}, {"referenceID": 26, "context": "kind of IDS is vulnerable to causative attacks, since an attacker may inject carefully designed malicious traffic during the collection of training samples to force the IDS to learn a wrong model of the normal traffic [11], [17], [29], [30], [36].", "startOffset": 236, "endOffset": 240}, {"referenceID": 32, "context": "kind of IDS is vulnerable to causative attacks, since an attacker may inject carefully designed malicious traffic during the collection of training samples to force the IDS to learn a wrong model of the normal traffic [11], [17], [29], [30], [36].", "startOffset": 242, "endOffset": 246}, {"referenceID": 45, "context": "designed, using a one-class \u03bd-SVM classifier with a radial basis function (RBF) kernel and the feature vector representation proposed in [49].", "startOffset": 137, "endOffset": 141}, {"referenceID": 46, "context": "[50]), and the \u03b3 value of the RBF kernel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "01 as suggested in [24], so that only \u03b3 has to be chosen.", "startOffset": 19, "endOffset": 23}, {"referenceID": 34, "context": "We focus on a causative attack similar to the ones considered in [38], aimed at forcing the learned model of normal traffic to include samples of intrusions to be attempted during operation.", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "As in [17], [28]\u2013[30], we repeat the security evaluation for pmax \u2208 [0, 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": "As in [17], [28]\u2013[30], we repeat the security evaluation for pmax \u2208 [0, 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "As in [17], [28]\u2013[30], we repeat the security evaluation for pmax \u2208 [0, 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "We use the data set of [24].", "startOffset": 23, "endOffset": 27}, {"referenceID": 47, "context": "packets) collected by a web server during five days in 2006, and a publicly available set of 205 malicious samples coming from intrusions which exploit the HTTP protocol [51].", "startOffset": 170, "endOffset": 174}, {"referenceID": 23, "context": "1, and the fact that in this application all the malicious samples in D must be inserted in the testing set [27], [51].", "startOffset": 108, "endOffset": 112}, {"referenceID": 47, "context": "1, and the fact that in this application all the malicious samples in D must be inserted in the testing set [27], [51].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "The performance under attack is evaluated as a function of pmax, as in [17], [28], which reduces to the classical performance evaluation when pmax = 0.", "startOffset": 71, "endOffset": 75}, {"referenceID": 24, "context": "The performance under attack is evaluated as a function of pmax, as in [17], [28], which reduces to the classical performance evaluation when pmax = 0.", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": ", the data sanitization method of [27]).", "startOffset": 34, "endOffset": 38}, {"referenceID": 48, "context": "For instance, in [52] inexact string matching was proposed to counteract word obfuscation attacks in spam filtering.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].", "startOffset": 162, "endOffset": 166}, {"referenceID": 25, "context": "The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].", "startOffset": 168, "endOffset": 172}, {"referenceID": 5, "context": "The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].", "startOffset": 243, "endOffset": 246}, {"referenceID": 19, "context": "The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].", "startOffset": 248, "endOffset": 252}, {"referenceID": 22, "context": "The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].", "startOffset": 254, "endOffset": 258}, {"referenceID": 28, "context": "The use of robust statistics has already been proposed to this aim; in particular, to devise learning algorithms robust to a limited amount of data contamination [14], [29], and classification algorithms robust to specific exploratory attacks [6], [23], [26], [32].", "startOffset": 260, "endOffset": 264}, {"referenceID": 12, "context": "Finally, a secure system should also guarantee the privacy of its users, against attacks aimed at stealing confidential information [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 37, "context": "methods have been proposed in biometric recognition systems to protect the users against the so-called hillclimbing attacks, whose goal is to get information about the users\u2019 biometric traits [41], [53].", "startOffset": 192, "endOffset": 196}, {"referenceID": 12, "context": "preserve privacy in [14], [54].", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "[12], [17], [38] require a full analytical model of the problem and of the adversary\u2019s behavior, that may be very difficult to develop for real-world applications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[12], [17], [38] require a full analytical model of the problem and of the adversary\u2019s behavior, that may be very difficult to develop for real-world applications.", "startOffset": 6, "endOffset": 10}, {"referenceID": 34, "context": "[12], [17], [38] require a full analytical model of the problem and of the adversary\u2019s behavior, that may be very difficult to develop for real-world applications.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "We obtained encouraging preliminary results on this topic [32].", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "Pattern classification systems are commonly used in adversarial applications, like biometric authentication, network intrusion detection, and spam filtering, in which data can be purposely manipulated by humans to undermine their operation. As this adversarial scenario is not taken into account by classical design methods, pattern classification systems may exhibit vulnerabilities, whose exploitation may severely affect their performance, and consequently limit their practical utility. Extending pattern classification theory and design methods to adversarial settings is thus a novel and very relevant research direction, which has not yet been pursued in a systematic way. In this paper, we address one of the main open issues: evaluating at design phase the security of pattern classifiers, namely, the performance degradation under potential attacks they may incur during operation. We propose a framework for empirical evaluation of classifier security that formalizes and generalizes the main ideas proposed in the literature, and give examples of its use in three real applications. Reported results show that security evaluation can provide a more complete understanding of the classifier\u2019s behavior in adversarial environments, and lead to better design choices.", "creator": "LaTeX with hyperref package"}}}