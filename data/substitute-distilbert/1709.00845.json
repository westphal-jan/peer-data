{"id": "1709.00845", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Semi-supervised Learning with Deep Generative Models for Asset Failure Prediction", "abstract": "this work requires a novel semi - supervised learning interface for data - driven modeling of asset failures whereas health status is only partially known in historical terminology. we combine a generative model maintained by deep neural networks with non - linear embedding technique. technology shows us to build prognostic models with the limited amount of health status information for the precise prediction assess prospective asset reliability. the proposed method is evaluated on a publicly available specification for remaining useful life ( rul ) estimation, which shows significant improvement even when a fraction of attribute data with known resource status is as sparse as 1 % of her total. our study propose that the non - linear embedding based on a deep generative model we efficiently regularize a complex model requiring deep architectures while achieving high prediction accuracy that is far less sensitive to the availability of health status information.", "histories": [["v1", "Mon, 4 Sep 2017 07:42:57 GMT  (3145kb,D)", "http://arxiv.org/abs/1709.00845v1", "9 pages, 6 figures, 1 table, KDD17 Workshop on Machine Learning for Prognostics and Health Management.August 13-17, 2017, Halifax, Nova Scotia - Canada"]], "COMMENTS": "9 pages, 6 figures, 1 table, KDD17 Workshop on Machine Learning for Prognostics and Health Management.August 13-17, 2017, Halifax, Nova Scotia - Canada", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andre s yoon", "taehoon lee", "yongsub lim", "deokwoo jung", "philgyun kang", "dongwon kim", "keuntae park", "yongjin choi"], "accepted": false, "id": "1709.00845"}, "pdf": {"name": "1709.00845.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Learning with Deep Generative Models for Asset Failure Prediction", "authors": ["Andre S. Yoon", "Taehoon Lee", "Yongsub Lim", "Deokwoo Jung", "Philgyun Kang", "Dongwon Kim", "Keuntae Park", "Yongjin Choi"], "emails": ["permissions@acm.org."], "sections": [{"heading": null, "text": "Keywords Data-driven prognostics; semi-supervised learning; deep learning; deep generative models; predictive maintenance"}, {"heading": "1. INTRODUCTION", "text": "With increasingly more sensor data available either transient in stream or permanent in database, numerous physical assets in different industrial sectors such as manufacturing, transportation, and energy are subjected to asset condition and health monitoring [17]. When sufficient information present in the data, referred to as condition monitoring (CM) data, the asset health status and its evolution can be modeled to predict a specific point in time after which an asset will no longer meet its normal operational requirements, i.e., asset failure [16]. The concept of RUL, also referred to as an estimated time to failure, is widely used in the future reliability prediction with an important industrial application known as predictive maintenance (PdM) [21]. The PdM attempts to reduce costly unscheduled maintenance or maintenance that is excessively scheduled.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD\u201917, Workshop on Machine Learning for Prognostics and Health Management. August 13-17, 2017, Halifax, Nova Scotia - Canada. Copyright 2017 ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nExisting approaches for predictive maintenance belong two categories: physics-driven (deduction from physical equations underlying mechanical operations) and data-driven (induction from measurements under actual operations). In this paper, we narrow down our scope to only focus on data-driven methods. Gaussian and Markov processes have produced promising results [16, 28, 29] in the field of predictive maintenance since 1990\u2019s, but they often suffer from practical limitations such as scalability (i.e., a basic form of Gaussian process requires O(n3)). On small datasets, they are very useful because strong priors can cover lack of data with reasonable computations. However, as data volumes are exploding, more complicated functions are required to model massive, high-dimensional operational measurements which are hard to be characterized by well-known priors and kernel functions. The advent of deep learning has enabled us to capture the complicated functions; for example, [31] exhibited performance improvements with deep learning-based prognostic models to predict the RUL of given assets.\nWhile many deep learning-based approaches rely on the abundance of historical data with the availability of corresponding health indicators either from the CM data itself or from extra measurements or modelings, obtaining exact health status of a given asset in reality is highly expensive or impossible in some cases. It is because labeling requires expertise or special devices to produce it, and generally highly time consuming. Above all, failures are rare and not easily replicated, especially for those that are mission-critical. This is also closely related to the problem of intermittent failures also known as No Fault Found (NFF) problem [16]. For example, in the case of mission-critical assets such as an aircraft engine or equipment in nuclear power plant, even a single failure is catastrophic and therefore, regular maintenance or replacement is carried out regardless of the true condition of an asset. Or in the case of fresh start of newly installed assets, one may have to wait long to collect any run-to-failure data to precisely model the degradation and consequently failure.\nIn the lack of sufficient health status information, which we call a label or a target variable, y, data-driven modeling of degradation becomes immensely difficult in many cases. It is often the case where there is abundant data with relatively few labels since unlabeled data is relatively cheap. We recognize while the complexity of various data-driven models increases, especially with the latest deep neural net architectures, there has been relatively little attention paid to the problem of insufficient labels in data-driven prognostics. In the field of machine learning, researchers have long\nar X\niv :1\n70 9.\n00 84\n5v 1\n[ cs\n.L G\n] 4\nS ep\n2 01\n7\nrecognized such difficulty and explored various techniques to make learning better generalized in the lack of sufficient labels [3].\nSemi-supervised learning (SSL) is a class of techniques that falls between unsupervised learning and supervised learning (SL) [3]. While supervised learning relies solely on labeled data, SSL makes use of unlabeled data as well to improve learning accuracy. Thus, SSL is resorted to deal with a situation where relatively few labeled data are available with large unlabeled data. There exist many different approaches in SSL, which include heuristic approaches such as self-learning and approaches based on generative models, and low-density separation [3]. Among different approaches, generative models parameterized by deep neural networks referred to as deep generative models (DGM) have recently achieved state-of-the-art performance in various unsupervised and semi-supervised learning tasks [14, 19, 25]. Two most popular approaches of DGM rely on variational inference [15] and generative adversarial network (GAN) [6]. The latter being probably the most popular in deep learning research nowadays is grounded in game theory.\nIn this paper, we propose a semi-supervised learning technique to improve data-driven asset failure prediction when labeled data is highly limited compared to the unlabeled. For example, we consider the case where historical CM data are abundant from a fleet of assets but it is only partially labeled with health status. In such a case, we observe the prediction accuracy becomes highly degraded even with sophisticated deep learning models based on the Recurrent Neural Networks (RNN) architectures such as Gated Recurrent Unit (GRU) or Long Short-Term Memory (LSTM). Our proposed method uses a non-linear embedding, which is obtained from one of the DGMs known as variational autoencoder (VAE). The variational autoencoder makes use of all the available data, both labeled and unlabeled. A part of the VAE is taken to form an embedding network, which is used to define a latent space where a model for reliability prediction is trained.\nIn the remainder of this paper, we first give the background of this work in Section 2 by discussing deep neural networks and a variational autoencoder as a choice of deep generative model. We describe our proposed methods in Section 3. In Section 4, we further the details of our methods with the experimental evaluations. Lastly, we discuss our findings in Section 5."}, {"heading": "2. BACKGROUND", "text": ""}, {"heading": "2.1 Problem Statement", "text": "In our problem, we have multivariate data for sensor measurement and their corresponding RUL. Let x(i,j) denote ith measurement value of sensor j. Furthermore, let us x (i) denote a vector of multivariate sensor measurement such that x(i) = [x(i,1), \u00b7 \u00b7 \u00b7 , x(i,m)] where m is the number of sensors. Formally, we describe a sensor measurement matrix by denoting x = {x(1), \u00b7 \u00b7 \u00b7 ,x(n)} where x(i) \u2208 Rm and n is the number of observed measurements. We also denote the corresponding set of RUL by y = {y(1), \u00b7 \u00b7 \u00b7 , y(n)} where y(i) \u2208 R+. More compactly, we define the data set D by D = {(x(i), y(i))}i=1,\u00b7\u00b7\u00b7 ,n.\nIn general, the data-driven prognostics approach is to learn the best predictor of RUL (or equivalent health indicator used to predict RUL) from the previously observed data\nset DT , i.e.training data set. The challenging problem is to find an appropriate transformation of the raw measurement X to efficiently learn a reliable RUL predictor. Such problem can be described by finding a mapping function F such that F : x 7\u2192 z where a latent variable z \u2208 Rk and m < k. Then the optimal predictor can be defined by a function of z with parameter \u03b3 denoted by f\u03b3(z) such that f\u03b3(z) = argmax\ny p(y | z, \u03b3).\nIn traditional machine learning approaches, it first finds appropriate latent variable z by well known linear transformation methods (e.g. Principal Component Analysis ) often with heuristics. Then it estimates the optimal parameter \u03b3 by training f\u03b3(z) with DT given a certain simplified assumption for the distribution p(y | z, \u03b3)(e.g. multivariate normal distribution). Such conventional approaches inherently has its own limitations in providing a general solution for RUL prediction. First, it is often that the mapping function F for latent variable z is non-linear and its specific form is largely unknown. Second, any strong assumption on distribution p(y | z, \u03b3) often poorly captures complex cross dependency over time and sensors. Meanwhile, deep neural network can provide a generic solution to jointly learn the non-linear mapping function F and the predictor f\u03b3 from training data set DT using directed graph network with various levels of abstraction. We drop the superscript T for the rest of the paper.\nIn our problem setting, we assume that only a small fraction of the samples has label information. Let DTL and DTU denote the part of the data set with labels and the part without the labels, respectively. The primary goal in this paper is to develop deep learning-based approach to predict RUL from DTL where |DTL | |DTU |, which is a class of semisupervised learning. In the semi-supervised learning setting, we assume that DTL comes from exactly the same distribution of DTU . Our semi-supervised approach fully exploits DTU to learn the non-linear mapping function f to get a robust feature representation of x for stable RUL predictor f\u03b3 given DTL ."}, {"heading": "2.2 Neural Network Overview", "text": "In this section we briefly discuss the definition and the architectures of basic deep neural network and its variants.\n2.2.1 feed-forward neural network We can formally describe a feed-forward neural network\nas follows. Let fl,i and gl,i denote a linear pre-activation and non-linear activation function of ith neuron at layer l, respectively where fl,i, gl,i \u2208 Rk for i = 1, \u00b7 \u00b7 \u00b7 , nl, nl is the number of neurons at layer l, and k is the dimension of a neuron\u2019s outputs. We can conveniently assume that the output dimension k is the dimension of a single sensor measurement. In our problem setting, k = 1 since each sensor measurement is a scalar value. Then layer l has two function vectors ; fl = [fl,1 \u00b7 \u00b7 \u00b7 fl,nl ] T and gl = [gl,1 \u00b7 \u00b7 \u00b7 gl,nl ] T .\nLet xl = [xl,i]i=1,\u00b7\u00b7\u00b7 ,nl denote activation output at layer l where xl,i = gl,i \u25e6 fl,i(xl\u22121). The pre-activation of layer l is computed by linear transformation\nfl(xl\u22121) = Wlxl\u22121 + bl (1)\nwhere Wl \u2208 Rnl\u00d7nl\u22121 and bl \u2208 Rnl\u22121 . For the non-linear activation, we use a popular rectified linear unit (ReLU) [22] where gi(fi) = max{0, fi} for fi \u2208 R. The layer l\u2019s parameter, \u03b8l = (Wl,bl) is estimated by\nthe backpropagation algorithm [18] given DT . Finally, feedforward neural network of L layers is defined as following :\nF (x; \u03b8) = fout \u25e6 hL(x) (2)\nwhere hl = (gl \u25e6 fl) \u25e6 hl\u22121 and \u03b8 = (\u03b80, \u00b7 \u00b7 \u00b7 , \u03b8L).\n2.2.2 Recurrent Neural Networks The feed-forward neural network we described in previ-\nous section does not allow temporal dependency of samples to be embedded in its model. In our problem, the current sample are mostly likely to be dependent on its previous samples. Hence, we need a more generalized model capable of elegantly incorporating temporal dependency in it. RNN is such kind of a model that can capture into model the influence of an arbitrary number of previous samples on the current [5]. The basic RNN structure consists of a cell with a cyclic loop whose internal state evolves over time by the current sample input and its previous state output. The output of RNN is simply a series of cell states. When we represents the cell state at each time as a neuron, its resultant structure is a non-cyclic horizontally connected neural network where the current neuron\u2019s output feeds into the next neuron\u2019s input.\nUsing the horizontal representation of cell states we can formally describe RNN with a simple modification of (1) and (2). Let us assume that we use the previous T samples for RNN input. Then the RNN input at time t is the array of multivariate sensor measurements from t to tT denoted by xtt\u2212T = [x\nt, \u00b7 \u00b7 \u00b7 ,xt\u2212T ]. Let st denote a cell state at time t such that st \u2208 Rk where k is a dimension of the state. Then pre-activation function of a neuron is defined by a linear mapping of the cell\u2019s previous state st\u22121 and the current input xt \u2208 Rm at time t as shown in (3).\nf(st\u22121,xt) = Ust\u22121 + Wxt + b (3)\nwhere b \u2208 Rm, U \u2208 Rk\u00d7k, and W \u2208 Rk\u00d7m. The cell state at time t is updated by a non-linear activation function g on the f output such that st = g \u25e6 f(st\u22121,xt) = h(st\u22121,xt). Finally, RNN for Tth order time dependency is compactly defined as following :\nF (xtt\u2212T ; \u03b8) = [h(st\u2032\u22121,xt\u2032)]t\u2032=t,\u00b7\u00b7\u00b7 ,t\u2212T (4)\nwhere h = (g \u25e6 f) and \u03b8 = (U,W,b). Note that all neural cells share parameter \u03b8 over time.\nThe RNN parameter \u03b8 can be trained using backpropagation through time (BPTT) algorithm [30]. However, RNNs with the BPTT suffer from the vanishing and exploding gradient problem [2] because a recurrent network grows as deep as sequence length. One of popular solutions is to exploit Long Short-Term Memory (LSTM). An LSTM [9] is a second-order recurrent network that has three additional gate layers called input, forget, and update gates. The gate parameters helps a recurrent network have good information flow over time by alleviating the vanishing gradient. Thus, a recurrent network with gates can work better than one without gates as more long-term dependencies exist.\nGRU [4] is a simpler variant of LSTM which combines forget input gate and read input gate into overwrite gate by setting an input gate to the inverse of forget gate, ,i.e it = 1 \u2212 ft. It is shown that the performance of GRU is on par with LSTM but more computationally efficient [10].\nIn our study, we find there is no meaningful difference in prediction accuracy between the two variants."}, {"heading": "2.3 Variational Autoencoder (VAE)", "text": "An autoencoder is a neural net with an encoder-decoder architecture that is trained to reconstruct its own input. Given an input x, the autoencoder, fenc(x), encodes and then the decoder, fdec(z), decodes the encoded input (a representation) back to reconstruct the input as closely as possible by minimizing a loss function of the form [5]:\nL(x,x\u2032) = ||x\u2212 fdec(fenc(x))|| (5)\nwhere fenc and fdec are symmetric in topology. The autoencoder is trained using backpropagation algorithm in the same manner as in the feed-forward neural network in Section 2.2.1. However, when the architecture becomes deep with many hidden layers, pretraining is necessary to set the initial weights of the autoencoder close to the final ones [8]. The VAE inherits autoencoder\u2019s encoder-decoder architecture, but it is different in that the latent representation z of given data x are replaced with stochastic variables. The encoder and the decoder of VAE are probabilistic and given by q\u03c6(z|x), an approximate posterior, and p\u03b8(x|z), likelihood of the data x given the latent variable z, respectively. The approximate posterior is parameterized by a neural network and the likelihood is given by a multivariate Gaussian whose probabilities are obtained from z.\nThe objective function of VAE is the variational lower bound to the marginal likelihood of the data, i.e., log p\u03b8(x) =\u2211n i=1 log p\u03b8(x (i)). It can be rewritten for individual data points, x(i) as [15]:\nlog p\u03b8(x (i)) = DKL(q\u03c6(z|x(i))||p\u03b8(z|x(i))) + L(\u03b8, \u03c6; x(i))\n(6) where DKL is Kullback-Leibeler (KL) divergence. Since DKL \u2265 0, (6) can be rewritten as:\nlog p\u03b8(x (i)) \u2265 L(\u03b8, \u03c6; x(i)) (7)\nThe left-hand side of (7), which is called variational lowerbound, can further be rewritten using Bayes\u2019 rule and the definition of KL divergence as [15]:\n\u2212DKL(q\u03c6(z|x(i))||p\u03b8(z)) + Eq\u03c6(z|x(i)) [ log p\u03b8(x (i)|z) ] (8)\nwhich can be viewed as the sum of reconstruction error plus a term that acts as a regularization.\nWhile obtaining the gradients of the encoder is relatively straightforward, that of the decoder is not. This is because, the reconstruction error term in Eq. 8 requires the Monte Carlo estimate of the expectation, which is not easily differentiable [15]. This problem is solved by introducing a reparameterization of z with a deterministic variable such that z = \u00b5+ \u03c3 , with \u223c N (0, 1), which is known as \u201creparameterization trick\u201d [15]. With this trick, a differentiable estimator of the variational lower bound can be obtained and the backpropagation algorithm can be applied to train VAE. The overall architecture of VAE is shown in Fig. 2."}, {"heading": "3. OUR APPROACH", "text": ""}, {"heading": "3.1 Overview", "text": "In this section, we describe our overall approach for semisupervised learning for RUL prediction using neural network framework and a deep generative model described in\nSection 2.2 and 2.3. We provide a description of the implemented architectures for RUL prediction and several important architectural decisions on design parameters in Section 4.\n3.1.1 Non-linear Embedding Embedding in semi-supervised learning is a procedure to\nmap x in the original space into a lower dimensional space, z, with the mapping obtained from unsupervised learning. The goal of embedding is to improve generalization of a supervised model. In our approach, we use the VAE to obtain the non-linear embedding. It is done by training the VAE as described in Section 2.3 using x from both labeled DL and unlabeled data DU to learn a mapping function fembed for latent variable z. The latent space is defined by the output layer chosen in the encoder of VAE as illustrated in Fig. 3(a). We choose kth layer of the encoder with k less than Lencoder (a total number of layers used in the encoder) to avoid using approximate samples from posterior distribution.\nWhile for the parameterization of the approximate posterior in VAE, feed-forward neural network is conventionally\nchosen as originally suggested in [15]. In our case, we choose the RNN-architecture for the parameterization to model the temporal dependency of data. Incorporating the RNN architecture into VAE to include the latent random variables into the hidden state of a RNN requires the estimation of timestep-wise variational lower bound, which is not trivial. We instead simply introduce a weight factor \u03b1 such that the two terms in Eq. 8 is differently weighted while modeling the temporal dependence in data with the RNN architecture.\n\u2212DKL(q\u03c6(z|x(i))||p\u03b8(z)) +\u03b1Eq\u03c6(z|x(i)) [ log p\u03b8(x (i)|z) ] (9)\nThe weight factor is tuned considering the overall reconstruction efficiency of VAE.\n3.1.2 Reliability Model After obtaining the non-linear embedding from the VAE,\nwe train a model (reliability model) with the RNN architecture in Eq. 4 using (z(i), yi) where z(i) = fembed(x i) and xi \u2208 DL. The resulting reliability model is used to predict RUL for a given set of historical CM data. It is noted the resulting model is similar to the latent feature discriminative\n0.0 0.2 0.4 0.6 0.8 1.0 0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0 (a)\n0.0 0.2 0.4 0.6 0.8 1.0 0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5 (b)\n0.0 0.2 0.4 0.6 0.8 1.0 0\n1\n2\n3\n4\n5\n6\n7\n8\n9 (c)\n0.0 0.2 0.4 0.6 0.8 1.0 0\n1\n2\n3\n4\n5\n6\n7 (d)Fr\neq ue\nnc y\nNormalized sensor values\nFigure 4: Normalized frequency of the values of selected sensors in Fig 1 (a)-(b) from all the engines in the dataset: original (blue) and reconstructed (red).\nmodel (M1), which shows the state-of-the-art performance in various classification tasks [14]. The biggest difference lies in the way the non-linear embedding is obtained. The M1 model uses approximate samples from the posterior distribution as an embedding. In our case we rather use the kth layer output as an embedding as we want to avoid using the approximate sampling in order to achieve high precision RUL prediction1. The corresponding procedure is illustrated in the architectures of the embedding network and the reliability model in Fig. 3(a) and 3(b).\nThe entire procedure of our approach from generative modeling to supervised learning of reliability model is given in Algorithm 1, where the mini-batch xM = {x(i)}Mi=1 is a randomly drawn sample of size M . The L indicates a corresponding loss function with L\u0303 implying that it is calculated for the data points in mini-batch as an approximation to the quantity for the entire data points. For the VAE, for example,\nL(\u03b8, \u03c6; x) ' L\u0303(\u03b8, \u03c6; xM ) = n M n\u2211 i=1 L(\u03b8, \u03c6; x(i)) (10)\n3.1.3 Self-learning Approach Self-learning also known as self-labeling is an iterative al-\ngorithm that uses supervised learning in repetition [3]. As the name suggest, the algorithm tries to\u201cbootstrap\u201dusing its own prediction. We use the simplest version of self-learning algorithm in order to see if it can help the generalization of supervised learning for the reliability prediction. It is done by first building a reliability model by training a RNN model using the labeled data, (x(i), yi) \u2208 DL. The resulting model, so called base learner, is then used to make RUL estimations\n1We tried with the approximate samples from the posterior distribution as features to train the reliability model and found the prediction precision is lowered because of the stochastic nature of sampling.\nfor the un-labeled data x(j) \u2208 DU . The estimated RUL, y\u0303(j), is then used in the next iteration of training such that the base learner is trained again with (x(i), yi) and (x(j), y\u0303(j)), treating y\u0303(j) as real labels. The iteration is repeated until a certain convergence condition is met. In our study, we use a single iteration since we do not impose any convergence condition for the algorithm.\nAlgorithm 1: Semi-supervised learning based on nonlinear embedding with VAE. We use settings M , N = 100 and k = 3. The first phase follows the minibatch version of the AEVB algorithm [15]\nPhase 1 Generative Modeling\nInput: xM = {x(i)}Mi=1 \u2208 D 1 \u03b8, \u03c6 \u2190 Initialize parameters 2 repeat 3 xM \u2190 Random mini-batch of size, M 4 zM \u223c q\u03c6(zM|xM) 5 g\u2190 \u2207\u03b8,\u03c6L\u0303MVAE 6 \u03b8, \u03c6 \u2190 Update parameters using gradient g\n(e.g., Stochastic Gradient Descent)\n7 until convergence of parameters (\u03b8,\u03c6) 8 return \u03b8, \u03c6"}, {"heading": "9 end", "text": "Phase 2 Supervised Learning with Embedding Input: (xN, yN) \u2208 DL and \u03b8\n10 \u03b3 \u2190 Initialize parameters 11 Embedding xN into the latent space defined by the\nkth output layer of the encoder: zNk \u2190 fembed(xN; \u03b8)\n12 repeat 13 zN \u2190 Random mini-batch of size, N 14 yN \u223c fRNN (xN; \u03b3) 15 g\u2190 \u2207\u03b3L\u0303NRNN 16 \u03b3 \u2190 Update parameters using gradient g (e.g., Stochastic Gradient Descent) 17 until convergence of parameters (\u03b3) 18 return \u03b3\n19 end"}, {"heading": "4. EXPERIMENTAL RESULTS", "text": "This section presents the evaluation of our approach on a publicly available dataset, Turbofan Engine Dataset, provided by Prognostics Center of Excellence in National Aeronautics and Space Administration (NASA) [26]. The dataset is composed of simulated multi-sensor readings from a fleet of aircraft engines of the same type, created with a realistic damage propagation modeling based on C-MAPSS [27]. The dataset is further divided into training and testing sets. We use the training data to build models for RUL estimation and use the testing data to assess the performance of the models with different metrics. For the evaluations of the proposed approach, we consider the case where the true historical RUL is partially known in the training set. It is done by randomly dropping RUL information for a fraction of the engines in the training samples. We vary the fraction in order to cover a multitude of conditions."}, {"heading": "4.1 C-MAPSS Turbofan Engine Dataset", "text": "The dataset, FD001, consists of multiple multivariate (24 sensors including 3 operational settings) time-series from a fleet of engines with different degree of initial wear and manufacturing variation that are unknown. The training and the testing sets contain 100 such engines for each. For the engines in the training set, run-to-failure time-series trajectories are provided. Whereas for the engines in the testing set, time-series trajectories are truncated prior to failures with true RUL values at the point of truncation given in a separate file for validation. A total number of cycles in the training set and in the testing set reach about 20K and 13K, respectively."}, {"heading": "4.2 Evaluations", "text": "4.2.1 Overview We directly model the engine failure by parameterizing the\nRNN architecture in Eq. 4 such that the input sensor readings are directly mapped to historical RUL. RUL is assigned to each sequence for a given engine by simply counting the number of cycles left before failure (by definition). In this way, we avoid to create any domain-specific assumptions for the modeling. We fist build a reliability model in a purely supervised setting, where all the training samples are used with their corresponding RULs. We then omit the RULs for a randomly chosen set of engines in the training set and repeat the procedure. The faction of engines, for which the RUL information is dropped, is varied from 100% down to 1% (f = 100, 80, 50, 30, 20, 10, 5, and 1%). To remove any selection bias, we repeat the random selection five times for each fraction. The performance measures are then averaged over the five different sets of samples except for the 100% labeled scenario, where no selection is involved. Also in or-\nder to take the model variability into account, we always use the homogeneous ensemble of the models obtained from five independent training, which in general produce a better result.\n4.2.2 Preprocessing Based on the 3 operational settings, we identify 14 differ-\nent operational modes that are uniquely defined. We normalize the 21 sensor readings (excluding the 3 operational settings) before feeding into our models to train. The normalization is done for each sensor based on min-max scaling and it is done operational-mode-wise such that each sensor is scaled to have a range between 0 and 1 in each operational mode. The sensors that are constant (i.e., \u03c3x = 0) after the normalization are dropped. After checking carefully with its impact on the prediction accuracy, we further drop discrete sensors with the number of unique values falling below 20. For the sake of simplicity, the discrete variables were excluded in the training of VAE. For the RUL prediction, we limit the value below a maximum RUL of 140 cycles as suggested in [20, 7]. Consequently, we limit the assigned RULs in the training set by setting those that exceed the maximum RUL to be the maximum.\n4.2.3 Metrics Different performance metrics are employed to assess the\nperformance of the models. This includes the mean squared error (MSE), mean absolute error (MAE), and the score metric proposed in [27]. The proposed score metric is defined such that late predictions are more heavily penalized than early prediction:\nS = n\u2211 i=1 (exp (\u03b1|\u2206i|)\u2212 1) (11)\nwhere \u2206i is the difference between the estimated and the true RUL values, i.e., estimated RUL - true RUL, for the ith engine in the testing set and \u03b1 is equal to 1/13 if \u2206i < 0 or 1/10 otherwise. We also use the coefficient of determination also known as R2 score as a different measure of regression quality2."}, {"heading": "4.3 Results", "text": ""}, {"heading": "4.3.1 Reconstruction with Variational Autoencoder", "text": "We use a three-layer architecture in the encoder (17-8-4)\nand in the decoder (4-8-17) with batch normalization applied to each layer except for the last layer. The architecture is chosen considering the reconstruction error and the performance of the non-linear embedding. The use of batch normalization is considered essential for training VAE with when the architecture becomes deep (i.e., L > 2) [11]. As mentioned earlier, we incorporate the RNN architecture into the VAE such that latent variables are modeled with the dynamic hidden state of a RNN architecture. This requires a modification in the objective function in Eq 8 so that sum of loss is averaged in temporal dimension. The model is trained end-to-end using the Adam optimizer [13] with a mini-batch size of 100.\nFig. 1 shows four sample sensor readings for several engines in sequence overlaid with the reconstructed sensor readings from VAE. In Fig. 4, we show the frequency of the values in histograms to see distributions. The results clearly demonstrate even with the one-fourth of the original input dimension in the bottleneck layer of VAE, the reconstruction is done reasonably well, capturing time dependent features of the signals. The sum of reconstruction loss and the KLdivergence stays below 0.3 when the trained VAE is used to reconstruct the validation sets3. This may support the manifold hypothesis that high dimensional data tend to lie in the vicinity of a low dimensional manifold [5].\n4.3.2 Baseline Results\n2R2 can yield negative values as models can be arbitrary worse. 3A fraction of the training data is kept from training and used for validation\nFig. 5 shows the baseline results of RNN-based models with a four-layer architecture4 trained in a purely supervised setting. It shows the true RUL in the x-axis and the estimated RUL in the y-axis in different labeled fraction scenarios. From the top left to the bottom right, the labeled fraction, f , is varied from 100% to 1%. When the full training set is used, the trained model shows good performance with the measured MSE, MAE, Score and R2 of 228, 11.3, 345, and 0.87, respectively. This result is comparable to that of other approaches [20, 1, 12, 23, 24].\nNow as the fraction changes, the prediction accuracy decreases. It can clearly be seen from Fig. 5 that the correlation between the actual and the estimated RULs get worse and worse with decreasing fraction, also indicated by the R2 score shown in the figure. The performance degradation is expected because the complexity of model requires a large amount of training data. In the current implementation of the RNN-based model, the number of tunable parameters reaches few hundreds. Nevertheless, the performance degradation is not prominent until the fraction is decreased down to 30%, indicating that only 30% of the training samples are already sufficient for the model to give a reasonably good result. As such, we rather focus in the region below 30%, where the performance degradation is significantly showing.\n4.3.3 Semi-supervised Approach As discussed in Section 3.1, we apply semi-supervised learn-\ning techniques to see if the performance can be improved when only a small fraction of the training data is labeled. We try two different semi-supervised learning techniques, i.e., self-learning and VAE-based non-linear embedding. Fig. 6 shows the results of the RUL estimation quantified by the three different metrics plotted as a function of f . From left to right, MAE, MSE, Score, and R2, are plotted with corresponding statistical uncertainties. The score is plotted in logarithmic scale. It quantitatively shows the performance\n4We adopt both GRU and LSTM architecture in the models. However, as we find no quantitative difference in performance, we choose the simpler architecture, namely, GRU for the final results.\ndegradation becomes increasingly larger as the fraction decreases. The result from the self-learning plotted in a triangular shape reveals that the performance is essentially the same within statistical uncertainty except for the score at f = 5%, where a distinctive improvement is shown.\nNow, the result from the non-linear embedding plotted in a circle shows improvements for f below 30. The level of improvement becomes increasingly larger reaching x1.7, x2.4, and x3.6 improvement, respectively for MAE, MSE, and Score. The R2 improvement is also notable. We note the prediction accuracy of the model is kept rather high even down to 5% fraction. The numerical compilations of the measured performance metrics are given in Table 1 for all three approaches: SL indicates supervised learning only. Self -SSL is the self-learning. V AE-SSL is the proposed method."}, {"heading": "5. DISCUSSION", "text": "This paper described a semi-supervised learning approach to predict asset failures when there exist relatively small label information available. Our approach uses the non-linear embedding based on a deep generative model. We used the variational autoencoder as a deep generative model choice because of its relatively easy trainability on top of its firm theoretical foundation. The VAE was trained unsupervised while utilizing all the available data whether it is labeled or unlabeled. With this approach, we pursued achieving good prediction accuracy even when the available label information is highly limited.\nThe approach was extensively evaluated on a publicly available Turbofan Engine dataset by varying the fraction of labels down to 1%. The evaluation shows significant improvements in prediction accuracy when compared to the supervised and a naive semi-supervised approach, i.e., selflearning at 1%, the improvement reaches up to x3.6 for the proposed score metric. While the 1% fraction could be considered rather extreme, it is not unusual in practice since the acquisition of exact health status information that can be related to failures is exceedingly difficult in many cases. In image recognition tasks, order of 0.1 to 1% labeled fraction is typically considered for similar studies.\nOverall, our study suggests the semi-supervised learning approach with the non-linear embedding based on deep generative model is effective when dealing with the problem\nof insufficient labels in future reliability prediction. Such problem is of tremendous practical interest in a wide range of data-driven PHM applications, especially when sophisticated deep learning architectures are used in modeling. In principle, there is little reason why our approach can not be extended to use other deep generative models for the embedding. In ensuing efforts, we are extending the approach to incorporate non-linear embedding using GAN."}, {"heading": "6. REFERENCES", "text": "[1] G. S. Babu, P. Zhao, and X.-L. Li. Deep convolutional\nneural network based regression approach for estimation of remaining useful life. In International Conference on Database Systems for Advanced Applications, pages 214\u2013228. Springer, 2016.\n[2] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. Transaction of Neural Network, 5(2):157\u2013166, Mar. 1994. [3] O. Chapelle, B. Schlkopf, and A. Zien. Semi-Supervised Learning. The MIT Press, 1st edition, 2010. [4] K. Cho, B. van Merrie\u0308nboer, C\u0327. Gu\u0308lc\u0327ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734. Association for Computational Linguistics, Oct. 2014. [5] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org. [6] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative Adversarial Networks. ArXiv e-prints, June 2014. [7] F. O. Heimes. Recurrent neural networks for remaining useful life estimation. In Prognostics and Health Management, 2008. PHM 2008. International Conference on, pages 1\u20136. IEEE, 2008. [8] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504\u2013507, 2006. [9] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735\u20131780, Nov. 1997.\n[10] R. Jozefowicz, W. Zaremba, and I. Sutskever. An empirical exploration of recurrent network architectures. Journal of Machine Learning Research, 2015. [11] C. Kaae S\u00f8nderby, T. Raiko, L. Maal\u00f8e, S. Kaae S\u00f8nderby, and O. Winther. Ladder Variational Autoencoders. ArXiv e-prints, Feb. 2016. [12] R. Khelif, S. Malinowski, B. Chebel-Morello, and N. Zerhouni. Rul prediction based on a new similarity-instance based approach. In Industrial Electronics\n(ISIE), 2014 IEEE 23rd International Symposium on, pages 2463\u20132468. IEEE, 2014.\n[13] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. ArXiv e-prints, Dec. 2014. [14] D. P. Kingma, D. J. Rezende, S. Mohamed, and M. Welling. Semi-supervised learning with deep generative models. CoRR, abs/1406.5298, 2014. [15] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. ArXiv e-prints, Dec. 2013. [16] Q. Z. Y. H. Kwok L. Tsui, Nan Chen and W. Wang. Prognostics and health management: A review on data driven approaches. Mathematical Problems in Engineering, 2015(793161):17, 2015. [17] D. Kwon, M. R. Hodkiewicz, J. Fan, T. Shibutani, and M. G. Pecht. Iot-based prognostics and systems health management for industrial applications. IEEE Access, 4:3659\u20133670, 2016. [18] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Mu\u0308ller. Effiicient backprop. In Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, London, UK, UK, 1998. Springer-Verlag. [19] L. Maal\u00f8e, C. Kaae S\u00f8nderby, S. Kaae S\u00f8nderby, and O. Winther. Auxiliary Deep Generative Models. ArXiv e-prints, Feb. 2016. [20] P. Malhotra, V. TV, A. Ramakrishnan, G. Anand, L. Vig, P. Agarwal, and G. Shroff. Multi-sensor prognostics using an unsupervised health index based on LSTM encoder-decoder. CoRR, abs/1608.06154, 2016. [21] R. K. Mobley. An introduction to predictive maintenance. Butterworth-Heinemann, 2002. [22] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010. [23] Y. Peng, H. Wang, J. Wang, D. Liu, and X. Peng. A modified echo state network based remaining useful life estimation approach. In Prognostics and Health Management (PHM), 2012 IEEE Conference on, pages 1\u20137. IEEE, 2012. [24] E. Ramasso. Investigating computational geometry for failure prognostics. International Journal of Prognostics and Health Management, 5(1):005, 2014. [25] A. Rasmus, H. Valpola, M. Honkala, M. Berglund, and T. Raiko. Semi-supervised learning with ladder network. CoRR, abs/1507.02672, 2015. [26] A. Saxena and K. Goebel. Turbofan engine degradation simulation data set. NASA Ames Prognostics Data Repository, 2008. [27] A. Saxena, K. Goebel, D. Simon, and N. Eklund. Damage propagation modeling for aircraft engine run-to-failure simulation. In Prognostics and Health Management, 2008. PHM 2008. International Conference on, pages 1\u20139. IEEE, 2008. [28] X.-S. Si, W. Wang, C.-H. Hu, and D.-H. Zhou. Remaining useful life estimation\u2013a review on the statistical data driven approaches. European journal of operational research, 213(1):1\u201314, 2011. [29] J. Sikorska, M. Hodkiewicz, and L. Ma. Prognostic modelling options for remaining useful life estimation by industry. Mechanical Systems and Signal Processing, 25(5):1803\u20131836, 2011. [30] P. Werbos. Backpropagation through time: what does it do and how to do it. In Proceedings of IEEE, volume 78, 1990. [31] R. Zhao, R. Yan, Z. Chen, K. Mao, P. Wang, and R. X. Gao. Deep learning and its applications to machine health monitoring: A survey. arXiv preprint arXiv:1612.07640, 2016."}], "references": [{"title": "Deep convolutional neural network based regression approach for estimation of remaining useful life", "author": ["G.S. Babu", "P. Zhao", "X.-L. Li"], "venue": "International Conference on Database Systems for Advanced Applications, pages 214\u2013228. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Transaction of Neural Network,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Schlkopf", "A. Zien"], "venue": "The MIT Press, 1st edition", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. van Merri\u00ebnboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "MIT Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative Adversarial Networks", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Recurrent neural networks for remaining useful life estimation", "author": ["F.O. Heimes"], "venue": "Prognostics and Health Management, 2008. PHM 2008. International Conference on, pages 1\u20136. IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "science, 313(5786):504\u2013507", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "Journal of Machine Learning Research", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Ladder Variational Autoencoders", "author": ["C. Kaae S\u00f8nderby", "T. Raiko", "L. Maal\u00f8e", "S. Kaae S\u00f8nderby", "O. Winther"], "venue": "ArXiv e-prints,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Rul prediction based on a new similarity-instance based approach", "author": ["R. Khelif", "S. Malinowski", "B. Chebel-Morello", "N. Zerhouni"], "venue": "Industrial Electronics  (ISIE), 2014 IEEE 23rd International Symposium on, pages 2463\u20132468. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "ArXiv e-prints,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "CoRR, abs/1406.5298", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "ArXiv e-prints,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Prognostics and health management: A review on data driven approaches", "author": ["Q.Z.Y.H. Kwok L. Tsui", "Nan Chen", "W. Wang"], "venue": "Mathematical Problems in Engineering,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Iot-based prognostics and systems health management for industrial applications", "author": ["D. Kwon", "M.R. Hodkiewicz", "J. Fan", "T. Shibutani", "M.G. Pecht"], "venue": "IEEE Access, 4:3659\u20133670", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Effiicient backprop", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, London, UK, UK", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Auxiliary Deep Generative Models", "author": ["L. Maal\u00f8e", "C. Kaae S\u00f8nderby", "S. Kaae S\u00f8nderby", "O. Winther"], "venue": "ArXiv e-prints,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Multi-sensor prognostics using an unsupervised health index based on LSTM encoder-decoder", "author": ["P. Malhotra", "V. TV", "A. Ramakrishnan", "G. Anand", "L. Vig", "P. Agarwal", "G. Shroff"], "venue": "CoRR, abs/1608.06154", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "An introduction to predictive maintenance", "author": ["R.K. Mobley"], "venue": "Butterworth-Heinemann", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "A modified echo state network based remaining useful life estimation approach", "author": ["Y. Peng", "H. Wang", "J. Wang", "D. Liu", "X. Peng"], "venue": "Prognostics and Health Management (PHM), 2012 IEEE Conference on, pages 1\u20137. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Investigating computational geometry for failure prognostics", "author": ["E. Ramasso"], "venue": "International Journal of Prognostics and Health Management, 5(1):005", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised learning with ladder network", "author": ["A. Rasmus", "H. Valpola", "M. Honkala", "M. Berglund", "T. Raiko"], "venue": "CoRR, abs/1507.02672", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Turbofan engine degradation simulation data set", "author": ["A. Saxena", "K. Goebel"], "venue": "NASA Ames Prognostics Data Repository", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Damage propagation modeling for aircraft engine run-to-failure simulation", "author": ["A. Saxena", "K. Goebel", "D. Simon", "N. Eklund"], "venue": "Prognostics and Health Management, 2008. PHM 2008. International Conference on, pages 1\u20139. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Remaining useful life estimation\u2013a review on the statistical data driven approaches", "author": ["X.-S. Si", "W. Wang", "C.-H. Hu", "D.-H. Zhou"], "venue": "European journal of operational research, 213(1):1\u201314", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Prognostic modelling options for remaining useful life estimation by industry", "author": ["J. Sikorska", "M. Hodkiewicz", "L. Ma"], "venue": "Mechanical Systems and Signal Processing, 25(5):1803\u20131836", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Backpropagation through time: what does it do and how to do it", "author": ["P. Werbos"], "venue": "Proceedings of IEEE, volume 78", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1990}, {"title": "Deep learning and its applications to machine health monitoring: A survey", "author": ["R. Zhao", "R. Yan", "Z. Chen", "K. Mao", "P. Wang", "R.X. Gao"], "venue": "arXiv preprint arXiv:1612.07640", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "With increasingly more sensor data available either transient in stream or permanent in database, numerous physical assets in different industrial sectors such as manufacturing, transportation, and energy are subjected to asset condition and health monitoring [17].", "startOffset": 260, "endOffset": 264}, {"referenceID": 15, "context": ", asset failure [16].", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "The concept of RUL, also referred to as an estimated time to failure, is widely used in the future reliability prediction with an important industrial application known as predictive maintenance (PdM) [21].", "startOffset": 201, "endOffset": 205}, {"referenceID": 15, "context": "Gaussian and Markov processes have produced promising results [16, 28, 29] in the field of predictive maintenance since 1990\u2019s, but they often suffer from practical limitations such as scalability (i.", "startOffset": 62, "endOffset": 74}, {"referenceID": 27, "context": "Gaussian and Markov processes have produced promising results [16, 28, 29] in the field of predictive maintenance since 1990\u2019s, but they often suffer from practical limitations such as scalability (i.", "startOffset": 62, "endOffset": 74}, {"referenceID": 28, "context": "Gaussian and Markov processes have produced promising results [16, 28, 29] in the field of predictive maintenance since 1990\u2019s, but they often suffer from practical limitations such as scalability (i.", "startOffset": 62, "endOffset": 74}, {"referenceID": 30, "context": "The advent of deep learning has enabled us to capture the complicated functions; for example, [31] exhibited performance improvements with deep learning-based prognostic models to predict the RUL of given assets.", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "This is also closely related to the problem of intermittent failures also known as No Fault Found (NFF) problem [16].", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "recognized such difficulty and explored various techniques to make learning better generalized in the lack of sufficient labels [3].", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "Semi-supervised learning (SSL) is a class of techniques that falls between unsupervised learning and supervised learning (SL) [3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "There exist many different approaches in SSL, which include heuristic approaches such as self-learning and approaches based on generative models, and low-density separation [3].", "startOffset": 173, "endOffset": 176}, {"referenceID": 13, "context": "Among different approaches, generative models parameterized by deep neural networks referred to as deep generative models (DGM) have recently achieved state-of-the-art performance in various unsupervised and semi-supervised learning tasks [14, 19, 25].", "startOffset": 239, "endOffset": 251}, {"referenceID": 18, "context": "Among different approaches, generative models parameterized by deep neural networks referred to as deep generative models (DGM) have recently achieved state-of-the-art performance in various unsupervised and semi-supervised learning tasks [14, 19, 25].", "startOffset": 239, "endOffset": 251}, {"referenceID": 24, "context": "Among different approaches, generative models parameterized by deep neural networks referred to as deep generative models (DGM) have recently achieved state-of-the-art performance in various unsupervised and semi-supervised learning tasks [14, 19, 25].", "startOffset": 239, "endOffset": 251}, {"referenceID": 14, "context": "Two most popular approaches of DGM rely on variational inference [15] and generative adversarial network (GAN) [6].", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "Two most popular approaches of DGM rely on variational inference [15] and generative adversarial network (GAN) [6].", "startOffset": 111, "endOffset": 114}, {"referenceID": 21, "context": "For the non-linear activation, we use a popular rectified linear unit (ReLU) [22] where gi(fi) = max{0, fi} for fi \u2208 R.", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "the backpropagation algorithm [18] given D .", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "RNN is such kind of a model that can capture into model the influence of an arbitrary number of previous samples on the current [5].", "startOffset": 128, "endOffset": 131}, {"referenceID": 29, "context": "The RNN parameter \u03b8 can be trained using backpropagation through time (BPTT) algorithm [30].", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "However, RNNs with the BPTT suffer from the vanishing and exploding gradient problem [2] because a recurrent network grows as deep as sequence length.", "startOffset": 85, "endOffset": 88}, {"referenceID": 8, "context": "An LSTM [9] is a second-order recurrent network that has three additional gate layers called input, forget, and update gates.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "GRU [4] is a simpler variant of LSTM which combines forget input gate and read input gate into overwrite gate by setting an input gate to the inverse of forget gate, ,i.", "startOffset": 4, "endOffset": 7}, {"referenceID": 9, "context": "It is shown that the performance of GRU is on par with LSTM but more computationally efficient [10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 4, "context": "Given an input x, the autoencoder, fenc(x), encodes and then the decoder, fdec(z), decodes the encoded input (a representation) back to reconstruct the input as closely as possible by minimizing a loss function of the form [5]:", "startOffset": 223, "endOffset": 226}, {"referenceID": 7, "context": "However, when the architecture becomes deep with many hidden layers, pretraining is necessary to set the initial weights of the autoencoder close to the final ones [8].", "startOffset": 164, "endOffset": 167}, {"referenceID": 14, "context": "It can be rewritten for individual data points, x as [15]: log p\u03b8(x ) = DKL(q\u03c6(z|x)||p\u03b8(z|x)) + L(\u03b8, \u03c6; x) (6) where DKL is Kullback-Leibeler (KL) divergence.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "The left-hand side of (7), which is called variational lowerbound, can further be rewritten using Bayes\u2019 rule and the definition of KL divergence as [15]:", "startOffset": 149, "endOffset": 153}, {"referenceID": 14, "context": "8 requires the Monte Carlo estimate of the expectation, which is not easily differentiable [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "This problem is solved by introducing a reparameterization of z with a deterministic variable such that z = \u03bc+ \u03c3 , with \u223c N (0, 1), which is known as \u201creparameterization trick\u201d [15].", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "0 100 200 300 400 500 600 Cycle (d) S en so r v al ue s in ra ng e [0 ,1 ]", "startOffset": 67, "endOffset": 74}, {"referenceID": 14, "context": "chosen as originally suggested in [15].", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "model (M1), which shows the state-of-the-art performance in various classification tasks [14].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "Self-learning also known as self-labeling is an iterative algorithm that uses supervised learning in repetition [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 14, "context": "The first phase follows the minibatch version of the AEVB algorithm [15]", "startOffset": 68, "endOffset": 72}, {"referenceID": 25, "context": "This section presents the evaluation of our approach on a publicly available dataset, Turbofan Engine Dataset, provided by Prognostics Center of Excellence in National Aeronautics and Space Administration (NASA) [26].", "startOffset": 212, "endOffset": 216}, {"referenceID": 26, "context": "The dataset is composed of simulated multi-sensor readings from a fleet of aircraft engines of the same type, created with a realistic damage propagation modeling based on C-MAPSS [27].", "startOffset": 180, "endOffset": 184}, {"referenceID": 19, "context": "For the RUL prediction, we limit the value below a maximum RUL of 140 cycles as suggested in [20, 7].", "startOffset": 93, "endOffset": 100}, {"referenceID": 6, "context": "For the RUL prediction, we limit the value below a maximum RUL of 140 cycles as suggested in [20, 7].", "startOffset": 93, "endOffset": 100}, {"referenceID": 26, "context": "This includes the mean squared error (MSE), mean absolute error (MAE), and the score metric proposed in [27].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": ", L > 2) [11].", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "The model is trained end-to-end using the Adam optimizer [13] with a mini-batch size of 100.", "startOffset": 57, "endOffset": 61}, {"referenceID": 4, "context": "This may support the manifold hypothesis that high dimensional data tend to lie in the vicinity of a low dimensional manifold [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 19, "context": "This result is comparable to that of other approaches [20, 1, 12, 23, 24].", "startOffset": 54, "endOffset": 73}, {"referenceID": 0, "context": "This result is comparable to that of other approaches [20, 1, 12, 23, 24].", "startOffset": 54, "endOffset": 73}, {"referenceID": 11, "context": "This result is comparable to that of other approaches [20, 1, 12, 23, 24].", "startOffset": 54, "endOffset": 73}, {"referenceID": 22, "context": "This result is comparable to that of other approaches [20, 1, 12, 23, 24].", "startOffset": 54, "endOffset": 73}, {"referenceID": 23, "context": "This result is comparable to that of other approaches [20, 1, 12, 23, 24].", "startOffset": 54, "endOffset": 73}], "year": 2017, "abstractText": "This work presents a novel semi-supervised learning approach for data-driven modeling of asset failures when health status is only partially known in historical data. We combine a generative model parameterized by deep neural networks with non-linear embedding technique. It allows us to build prognostic models with the limited amount of health status information for the precise prediction of future asset reliability. The proposed method is evaluated on a publicly available dataset for remaining useful life (RUL) estimation, which shows significant improvement even when a fraction of the data with known health status is as sparse as 1% of the total. Our study suggests that the non-linear embedding based on a deep generative model can efficiently regularize a complex model with deep architectures while achieving high prediction accuracy that is far less sensitive to the availability of health status information.", "creator": "LaTeX with hyperref package"}}}