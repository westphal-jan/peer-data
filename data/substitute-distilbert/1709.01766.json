{"id": "1709.01766", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "Information-Propogation-Enhanced Neural Machine Translation by Relation Model", "abstract": "even though sequence - to - sequence neural machine translation ( nmt ) model have achieved state - of - mind performance in the theoretically fewer systems, but it is widely concerned that the recurrent neural network ( gps ) nodes are reasonably slow to capture comparing long - distance state information, which means rnn can hardly find the feature increases long term dependency as the sequence becomes longer. similarly, convolutional neural network ( cnn ) technology introduced into nmt for speeding connectivity, however, cnn focus on capturing the local pixels beside the sequence ; to relieve this issue, we incorporate a relation sentence into one standard capture - decoder framework dramatically enhance information - propogation in neural network, ensuring that the information of the source sentence can flow into the decoder adequately. experiments show that proposed framework outperforms the statistical mt model and the state - of - art nmt model significantly on merging data sets with different scales.", "histories": [["v1", "Wed, 6 Sep 2017 11:13:50 GMT  (2308kb,D)", "http://arxiv.org/abs/1709.01766v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wen zhang", "jiawei hu", "yang feng", "qun liu"], "accepted": false, "id": "1709.01766"}, "pdf": {"name": "1709.01766.pdf", "metadata": {"source": "CRF", "title": "Information-Propogation-Enhanced Neural Machine Translation by Relation Model", "authors": ["Wen Zhang", "Jiawei Hu", "Yang Feng", "Qun Liu"], "emails": ["zhangwen@ict.ac.cn", "hujiawei@ict.ac.cn", "fengyang@ict.ac.cn", "qliu@computing.dcu.ie"], "sections": [{"heading": "Introduction", "text": "In recent years, neural machine transaiton (NMT) (Cho et al. 2014b; Sutskever, Vinyals, and Le 2014; Cho et al. 2014a) has achived great success in some language pairs, over the state-of-the-art statistical machine translation (SMT) (Koehn, Hoang, and Birch 2007). The encoder-decoder architecture is widely used for NMT, the principle behind which is that: encoding the meaning of the input into a concept space and performing translation based on this encoding. This \u2018meaning encoding\u2019 principle leads to a deeper understanding and learning of the translation rules, and hence a better translation than conventional statistic machine translation (SMT) that considers only surface forms, i.e., words and phrases.\nHowever, even attention-based NMT uses bi-directional RNN to encode the source sentence in two directions, the representation contains only the sequential relationship among the words in the source sentence, and, as the length of the source sentence increases, the encoded vector may forget the information in the words fay away from it, bidirectional encoder may solve this problem in some degree; However, sentences in natural language are regarded as relational, which means it may have grammatical relation (such\nCopyright \u00a9 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nas dependency relationship) between some word and another in one sentence, recurrent neural network models like GRU or LSTM are very difficult to capture these kinds of relations between word-pairs in one sentence, thus, when making dynamic alignment, the attention model is also hard to recognize this relation; To address this problem, (Bastings et al. 2017) introduces graph-convolutinal networks (GCNs) into the encoder of attention-based NMT, which simultaneously take the representation and the syntactic dependency tree of the source sentence as input to produce another representation for each word, beneficially, these representations may be sensitive to their syntactic neighborhoods.\nIn this paper, we propose to introduce another simple neural network called Relation Networks (RNs) (Santoro et al. 2017) into the attention module of attention-based NMT compatibly, to let the NMT model capture the relations between each word-pair in the source sentence during the process of dynamic alignment. We validate our proposed model on NIST Chinese-English translation task. Experiment results show that, after incorporated with RN , the attentionbased NMT model can generate a entangled sequential representation of source sentence by recognizing relations between words, then build higher quality dynamic alignment according to these relations, finally, produce more accurate performance for machine translation further."}, {"heading": "Background", "text": "Based on RNNSearch, we build our machine translation system; RNNSearch improved the attention mechanism of attention-based NMT (Bahdanau, Cho, and Bengio 2014), which first represents the source sentence into a sequence of vectors by an bi-directional RNN encoder, then another RNN decoder learns to align and generates target translation word by word, in the process of which, a feed-forward neural network is applied to produce dynamic alignment according to the representation of the source sentence, previous target word and the previous hidden state of the decoder. We start this section by describing the attention-based NMT model."}, {"heading": "Attention-based Neural Machine Translation", "text": "Figure 1 shows the framework of attention-based NMT, which is composed of three components: Encoder, Attention layer and Decoder.\nar X\niv :1\n70 9.\n01 76\n6v 1\n[ cs\n.C L\n] 6\nS ep\n2 01\n7\nGenerally, given a source sequence x = {x1, \u00b7 \u00b7 \u00b7 , xls} and the incomplete target sequence y<j = {y1, \u00b7 \u00b7 \u00b7 , yj} which have been generated. The model predict next word according following target-word probability distribution:\np(yj |x,y<j) \u221d exp(f(yj\u22121, sj , aj)\u00d7Wv) (1) where function f is a recurrent neural network, Wv is a mapping weighted matrix to map the output of f into the dimension of target vocabulary size, which is limited to the most frequent 15K words (Cho et al. 2014b). Then, we apply softmax function to the final product to get the probability distribution over the whole target vocabulary for each target word. aj can be treated as the context, which is actually the weighted sum of each annotation hi in the source sentence, computed by the attention model:\naj =\nls\u2211\ni=1\n\u03b1ijhi (2)\nBi-directional RNN encoder embodies two RNNs (here we use GRUs), which respectively encode x into a sequence of fixed-length vectors \u2212\u2192 h = {\u2212\u2192h 1, \u00b7 \u00b7 \u00b7 , \u2212\u2192 h ls} from left to right and \u2190\u2212 h = {\u2190\u2212h 1, \u00b7 \u00b7 \u00b7 , \u2190\u2212 h ls} from right to left, each hi is the concatenation of \u2212\u2192 hi and \u2190\u2212 hi:\nhi = [\u2212\u2192 h i\u2190\u2212 h i ] = [\u2212\u2212\u2212\u2192 GRU(xi, \u2212\u2192 h i\u22121)\u2190\u2212\u2212\u2212 GRU(xi, \u2190\u2212 h i+1) ] (3)\nhi contains the information about xi and all other words in the source sentence with a strong focus on the parts surrounding xi. \u03b1ij is the weight of hi, and can be considered as probability that the (j\u22121)th word in the target sentence is aligned to the ith word in the source sentence, it is computed by\n\u03b1ij = exp[g(sj\u22121,hi)]\u2211ls i=1 exp[g(sj\u22121,hi)]\n(4)\nwhere we follow the definition of function g in (Bahdanau, Cho, and Bengio 2014)\ng(sj\u22121,hi) = v T a tanh(Wasj\u22121 +Uahi) (5)\ng is actually a feed-forward network, which can measure how well previous hidden state sj\u22121 and hi match with each other, hidden state sj is calculated by\nsj = GRU(yj\u22121, sj\u22121, aj) (6)"}, {"heading": "Improved Decoder", "text": "The improved decoder model is shown in Figure 2, we only replace the decoder part in Figure 1 with the improved counterpart, leaving other modules unaltered. In the new decoder, a new intermediate hidden state s\u2032j is introduced, and the new final hidden state sj is computed based on s\u2032j ; So sj would not be calculated by yj\u22121, sj\u22121 and aj directly, Formula ( 6) is decomposed into two others:\nOne GRU computes the intermediate hidden state s\u2032j according to yj\u22121 and sj\u22121\ns\u2032j = GRU \u2032(yj\u22121, sj\u22121) (7)\nThen, the other one is used to calculate next final hidden state based on the intermediate state and attention\nsj = GRU(s \u2032 j , aj) (8)\nDifferently, Formula ( 4) and ( 5) are also updated, instead of sj\u22121, s\u2032j is used to generate \u03b1ij together with hi, as shown by Attention Layer in Figure 2"}, {"heading": "Relational Attention Model", "text": "Based on improved attention model in Figure 2, we incorporate another relation layer between the attention layer and bi-directional encoder of NMT; As shown in Figure 3, the relation layer is composed of three components:\n\u2022 Convolutional Encoder (CENC)\n\u2022 Graph Propogation Layer (GPL)\n\u2022 Multi-Layer Perceptron Decoder (MLPDEC)\nCENC: the initial representation of the source sequence generated by NMT encoder are convolved through two layers 1-dimensional convolutional neural network (CNN) on the dimension of sentence length, with the length unchanged, so, after that, the encoder outputs d k-dimensional feature maps, which is a new convolutional representation for each source word.\nGPL: The graph propogation layer concatenates the convolutional representations of each source word-pair, it is this layer that try to recognize the relation between each pair of\nsource words by concatenating their representation vectors. By concatenating the convolutional representation of some word w with those of all other ones, the word w is related to a set of vectors; Then, through another Multi-Layer Perceptron (MLP) layer, the word w will be represented by another set of vectors, which might contains the information of relations with all other words in the sentence except this source word; At last, we sum these vectors together to generate the output for word w (see Figure 3)\nMLPDEC: The last module of the relation layer called MLP decoder changes the representation dimension for each source word and get the final representation of each source word which contains the relations information from all other words, we call it as entangled sequential representation. Accordingly, for convenient distinction, we name the initial representation generated by NMT encoder as recurrent sequential representation.\nAs shown in Figure 3, the entangled sequential representations together with recurrent sequential representations of all source words are feed into the attention layer of NMT decoder, we update the Formula ( 5) here\ng(sj\u22121,hi) = v T a tanh(Wast\u22121 +Uahi + ri) (9)\nwhere ri is the entangled sequential representation of each source word xi created by relation layer. ri is calculated by taking the i-th vector of matrix r\nr = MLPDEC(GPL(CENC(h))) (10)\nAs noted in Formula ( 10), the recurrent sequential representations are encoded by the convolutional layer to be fixed-dimension vectors, then GPL is adopted to collect information propogated between each vectors pair and feed new vectors hi to the MLP decoder layer to recover vector\ndimension back. hi is the sum of the i-th column vectors produced by GPL layer as shown in Formula ( 11), MLPDEC is actually a multi-layer fully connected network.\nhi = \u2211\nj\nGPL(hi,hj) (11)\nResidual connections is adopted from recurrent sequential representation to entangled sequential representation by addition, multiplication or concatenation operation to generate final output."}, {"heading": "Convolution Gated Recurrent Unit", "text": "Here, we introduce a Convolution Gated Recurrent Unit (CGRU) (Kaiser and Sutskever 2015) to replace the 1- dimension convolutional encoder layer in Figure 3, CGRU is a unit which is similar with the standard GRU but combines the convolution operation into GRU (Cho et al. 2014b), with some following changes, the state st\u22121 is updated into st layer by layer:\nst = ut \u25e6 st\u22121 + (1\u2212 ut) \u25e6 ct ct = tanh(Wc \u2217 (rt \u25e6 st\u22121) + bc) ut = \u03c3(Wu \u2217 st\u22121 + bu) rt = \u03c3(Wr \u2217 st\u22121 + br)\nwhere W and b can be treated as the weights and bias of convolutional operation, they are learnable parameters. Such as the operation Wu \u2217 st\u22121 represents a convolution operation on the state st\u22121, \u25e6 denotes the element-wise multiplication, and \u03c3 is the sigmoid activation function.\nWe treat the recurrent operation among layers, without recurrent unit inside one layer, recurrent sequential representation is feed into one CGRU layer to produce the next state, which is actually the input of next CGRU layer; we apply the CGRU to the recurrent sequential representation for several\ntimes, and the final output is used as the representation of the source sentence, feed into the graph propogation layer."}, {"heading": "Experiments", "text": ""}, {"heading": "Data Preparation", "text": "We conduct experiments on Chinese-English translation task by using two different scale data sets, one of which is a smaller data set, and the other is larger one.\nIWSLT Data The training data set of the IWSLT corpus consists of 44K sentences from the tourism and travel domain. The development data set was composed of the ASR devset 1 and devset 2 from IWSLT 2005, and the test data set is the IWSLT 2005 test set.\nNIST Data The training data is consisted of 1M ChineseEnglish parallel sentence pairs with 19M source tokens and 24M target tokens from the LDC corpora of LDC2002E18, LDC2003E07, LDC2003E14, and Hansard\u2019s portion of LDC2004T07, LDC2004T08 and LDC2005T06. and we use NIST 2002 test data set as validation data set, and other four NIST test data sets are selected as the test sets: NIST 2003 (919 sentences), NIST 2004 (1788 sentences), NIST 2005 (1082 sentences) and NIST 2006 (1664 sentences)."}, {"heading": "Systems", "text": "We compare the translation performance of our system with four other baseline translation systems:\n\u2022 Traditional statistic machine translation (SMT) system (MOSES)\n\u2022 The RNNsearch-based translation system (named GROUNDHOG) we reimplement based on (Bahdanau, Cho, and Bengio 2014), this model is conducted as our weaker baseline model.\n\u2022 The improved RNNsearch-based translation system (denoted by RNNSEARCH?) described above, which we use as a stronger baseline model.\n\u2022 Google\u2019s new neural translation system (indicated by TRANSFOMER) without RNN units (Vaswani et al. 2017)."}, {"heading": "Evaluation Mitrcs", "text": "Without UNK replacement and de-tokenization, the translation is evaluated by using case-insensitive 4-gram BLEU score (Papineni et al. 2002) with test of statistical significance (Collins, Koehn, and Kuc\u030cerov\u00e1 2005) between the proposed model and baseline models."}, {"heading": "Improvement to Translation Performance", "text": "To validate the stability and effectiveness of proposed model, we compare it with others on two different scale data sets according to the BLEU score. Firstly, two NMT baseline models do not have advantages comparing with MOSES on small training data, achieving similar BLEU score with MOSES on the test data, which we could get from Tabel 1; On the toy data set, we stably improve BLEU score by using kinds of different relation network configurations, our model with best setting significantly outperforms MOSES by 4.05 points and RNNSEARCH? by 4.02 points. Models with other\nsettings all produce more than 1 points than three baseline models.\nOn the 1M training data set, RNNSEARCH? produces 3.47+ and 4.12+ BLEU score averagely comaparing with MOSES and GROUNDHOG which shows that RNNSEARCH? is a strong baseline model; Besides, it is consistent with Google\u2019s paper that TRANSFORMER produces similar results with RNNSEARCH? on our training data set. Table 2 shows the final results of the systems, our model with best configurations outperforms the strong NMT baseline model RNNSEARCH? by 1.13 points on average, achieving statistically significant improvements on three testing data sets; Moreover, all other configurations in the Table 2 enable our model to improve translation performance in contrast to the strong baseline model."}, {"heading": "Linguistic Analysis", "text": "To explore the reason why translation performance becomes better, we analyse from three following aspects:\nTranslation results Table 3 shows several translation samples produced by the NMT strong baseline model and the proposed model. By comparison, from the boldfaced section of our translation result, we could conclude that the NMT baseline model often miss some information of the source sentence (we call under-translation), such as the baseline translation of the first example do not produce the information about when i arrive at the intersection, RNNSEARCH? model loses the information about haircut when generating the target text for the second sample; It similarly happens that, for the sixth one, baseline model fails to capture the latter clause with adversative relation and so on; Besides, another phenomenon we can get is that the longer the source sentence is, it is easier to ignore important imformations for the baseline model; While our model could capture all the information source sentence contains successfully.\nSpecifically, Whether RNN in the encoder and decoder of baseline model or the CNN are both weak to capture the long dependency information, RNNs are skilled in modeling the order imformation of the sequence, while CNNs mainly focus on local features around some specific word. However, facts prove that proposed relation layer integrates several CNN layers into Bidirectional RNN or based on Bi-RNN, which alleviates the both disadvantages effectively.\nWord Alignment Along with the translation results, we also produce the word alignment matrix based on each target word\u2019s attention probability distribution over the whole source sentence for each decoding step. We randomly sample two source sentences from the testing data set, and output both of their alignment matrics on the baseline model RNNSEARCH? and the proposed model RNNSEARCH?+RN.\nAs shown in Figure 5, for the first example, from the view of source side, the source chinese word yi is contributed to generate three english words the, is and for, which obviously do not make sense, knowledges in the grammar level show that the word yi is only aligned to the english word for, just like the result of our model; Beside, on the target\u2019s ground,\none single word in the english phrase should be aligned to specific source word together with other words in the phrase, such as english new and is; For the second sentence, unlike the baseline model, our model produce the correct translation jazz music for jueshi yinyue and the right alignment, the together with origin is aligned to the source word, while baseline model mistakenly aligns the to two source words amost equably.\nThe baseline RNNSEARCH? model only use RNN to model the representation of the source sentence and the generation of the target words. Thus, it may ignore the long-distance information; More importantly, because bidirectional RNN models the source sentence sequentially, so, each encoded source word contains the information of all words before it and after it, the generation of one specific\ntarget word may be misled by some irrelative or translated source words and target words (Tu et al. ). While our relation network make more explicit guidance for the generation of each decoding step, thereby producing more unambiguous word alignment.\nRelationship of Translation with Source Length the performance of the conventional RNN Encoder\u2013Decoder (Cho et al. 2014b) dramatically drops as the length of the sentences increases, and the RNNsearch model are already more robust to the length of the sentences; However, our proposed model behaves stronger for the long source sentences than RNNsearch model (shown in Figure 4).\nIn the figure, we compare our model with RNNSEARCH? and TRANSFORMER to observe which one have an advan-\ntage over translating long source sentences; For short sentence, three models all get close BLEU, our model only performs slightly better than two others; the performance of three models drops together when the length of the source sentences beyonds 50, textscTransformer model has striking decline, RNNSEARCH? generates better results comparing to Google\u2019s model, our model outperforms two others and gets best translations on the long sentences including more than 50 words, which proves that proposed model acts more robust capability to translate long sentence."}, {"heading": "Related Works", "text": "(Meng et al. 2016) introduces a new attention mechanism to model the interaction between the decoder and the representation of the source sentence during translation by reading memory and writing memory, for original Groundhog or RNNSearch model, the representation of the source sentence keep unchanged during the whole decoding process (Bahdanau, Cho, and Bengio 2014) , interactive attention mechanism write a new source representation into memory for each step in decoding and could keep track of the interaction history between the decoder and the representation of source sentence during translation. (Tu et al. ) try to employ a coverage model to enhance the standard attention mechanism, enforcing decoder to tend to attend those source words untranslated and ignore those translated ones; (Bastings et al. 2017) adopt the dependency tree of the source sentence to restraint the decoder to focus on those words having dependency relation when generating each target words.\nOur proposed model add an extra relation network layer on top of the traditional bi-directional RNN encoder, with no need for importing complicated syntactic knowledge to constrain decoding, the model can learn to concentrate on those source words uninvolved and forget those translated one more precisely, simultaneously, without losing the important source information for the long source sentence."}, {"heading": "Conclusion", "text": "We introduce a relation network layer on the top of the standard bi-directional RNN encoder in the NMT model; Experiments show that our model stably improves the translation performance on both toy and large-scale data sets; Analysis indicates that relation network layer makes the attention mechanism more specific, meanwhile without losing the universality of modeling the order of sequence, which means our model can learn which section should be attended in the process of generating each target word more precisely, simultaneously, capable of keeping the important source information for the long source sentence."}, {"heading": "Hyper Parameters and Training Details", "text": "\u2022 MOSES: We use the open-source translation system\nMoses1 (Koehn, Hoang, and Birch 2007) as our SMT baseline; Following detailed configurations are applied to train the SMT model: we train the 4-gram language model on the target side of Large NIST training data by using KenLM open-source toolkit2 (Heafield et al. 2013); GIZA++3 (Och 2003) is used to produce the word alignment on the given parallel training data; Default maximal phrase length 7 is employed when we extract the phrase pairs based on word alignment results; Relative translation frequencies and lexical translation probabilities are calculated on both directions, meanwhile, distortion distance and word penalty are also applied; Besides, we set the maximum iterations to be 20 when tuning the parameters, without filtering the phrase table.\n\u2022 GROUNDHOG: We reimplement the attention-based RNNsearch model of (Bahdanau, Cho, and Bengio 2014) by PyTorch deep learning framework 4. Concretely, for 1http://www.statmt.org/moses/ 2https://github.com/kpu/kenlm 3https://github.com/moses-smt/giza-pp 4http://pytorch.org\nall NMT baseline systems, we employ a little bit different settings with (Bahdanau, Cho, and Bengio 2014): we filter the training sentence-pairs whose source sentence or target sentence contains more than 50 tokens, the dimension of source and target word embedding is 512, dimensions of all hidden units in both encoder and decoder RNN are all 512, the vocabulary sizes of the source and target side are both 30K. We initialize all weight matrices or vectors by using uniform distribution, all parameters are updated once for each mini-batch by Stochastic Gradient Descent (SGD), the batch size option is 80, in addition, the learning rate is adjusted by optimizer AdaDelta (Zeiler 2012) with the coefficient \u03c1 = 0.9 used for computing a running average of squared gradients, and the term = 10e-06 added to the denominator to improve numerical stability. Dropout rate 0.5 is applied in the training process, without dropout and with beam size 10 in decoding. At last, best model on validation set is used to generate translations of testing data.\n\u2022 RNNSEARCH\u2217: The decoder of the standard RNNsearch model is changed into the improved one, with all other configurations and training details the same with the GROUNDHOG system above. For leaky ReLU activation function, we use the coefficient 0.1 to control the angle of the negative slope.\n\u2022 TRANSFORMER: We evaluate this model on a open source platform sockeye5 developed by amazon which is a sequence-to-sequence framework with a focus on NMT based on Apache MXNet, configuration is set by default with both the source and target vocabulary sizes the same as those of above NMT systems. We set layers number to be 4 for both encoder and decoder with no dropout, model hidden size and attention head number are 512 and 8 respectively. During training, smoothed cross entropy loss is adopted and weight tying is used to regularize weight parameters in classification layer.\n5https://github.com/awslabs/sockeye"}], "references": [{"title": "Neural Machine Translation By Jointly Learning To Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "Iclr 2015 1\u201315.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Graph Convolutional Encoders for Syntaxaware Neural Machine Translation. Emnlp", "author": ["J. Bastings", "I. Titov", "W. Aziz", "D. Marcheggiani", "K. Sima \u2019an"], "venue": null, "citeRegEx": "Bastings et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bastings et al\\.", "year": 2017}, {"title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1"], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics - ACL \u201905 (June):531\u2013540.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Scalable Modified Kneser-Ney Language Model Estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Compu- tational Linguistics (Volume 2: Short Papers) 690\u2013696.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Densely Connected Convolutional Networks", "author": ["G. Huang", "Z. Liu", "K.Q. Weinberger", "L. van der Maaten"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Neural gpus learn algorithms", "author": ["\u0141. Kaiser", "I. Sutskever"], "venue": "arXiv preprint arXiv:1511.08228.", "citeRegEx": "Kaiser and Sutskever,? 2015", "shortCiteRegEx": "Kaiser and Sutskever", "year": 2015}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch"], "venue": "Proceedings of the 45th . . . (June):177\u2013180.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Interactive Attention for Neural Machine Translation", "author": ["F. Meng", "Z. Lu", "H. Li", "Q. Liu"], "venue": "Coling-2016 2174\u2013 2185.", "citeRegEx": "Meng et al\\.,? 2016", "shortCiteRegEx": "Meng et al\\.", "year": 2016}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F.J. Och"], "venue": "Annual Meeting of the ACL 1001(1):160.", "citeRegEx": "Och,? 2003", "shortCiteRegEx": "Och", "year": 2003}, {"title": "BLEU: a method for automatic evaluation of machine", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A simple neural network module for relational reasoning", "author": ["A. Santoro", "D. Raposo", "D.G. Barrett", "M. Malinowski", "R. Pascanu", "P. Battaglia", "T. Lillicrap"], "venue": "Arxiv 1\u201316. Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence to sequence learning with neural networks. Advances in Neural", "citeRegEx": "Santoro et al\\.,? 2017", "shortCiteRegEx": "Santoro et al\\.", "year": 2017}, {"title": "Modeling Coverage for Neural Machine Translation", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li"], "venue": "Information Processing Systems", "citeRegEx": "Tu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 1, "context": "as dependency relationship) between some word and another in one sentence, recurrent neural network models like GRU or LSTM are very difficult to capture these kinds of relations between word-pairs in one sentence, thus, when making dynamic alignment, the attention model is also hard to recognize this relation; To address this problem, (Bastings et al. 2017) introduces graph-convolutinal networks (GCNs) into the encoder of attention-based NMT, which simultaneously take the representation and the syntactic dependency tree of the source sentence as input to produce another representation for each word, beneficially, these representations may be sensitive to their syntactic neighborhoods.", "startOffset": 338, "endOffset": 360}, {"referenceID": 12, "context": "In this paper, we propose to introduce another simple neural network called Relation Networks (RNs) (Santoro et al. 2017) into the attention module of attention-based NMT compatibly, to let the NMT model capture the relations between each word-pair in the source sentence during the process of dynamic alignment.", "startOffset": 100, "endOffset": 121}, {"referenceID": 7, "context": "Here, we introduce a Convolution Gated Recurrent Unit (CGRU) (Kaiser and Sutskever 2015) to replace the 1dimension convolutional encoder layer in Figure 3, CGRU is a unit which is similar with the standard GRU but combines the convolution operation into GRU (Cho et al.", "startOffset": 61, "endOffset": 88}, {"referenceID": 11, "context": "Without UNK replacement and de-tokenization, the translation is evaluated by using case-insensitive 4-gram BLEU score (Papineni et al. 2002) with test of statistical significance (Collins, Koehn, and Ku\u010derov\u00e1 2005) between the proposed model and baseline models.", "startOffset": 118, "endOffset": 140}, {"referenceID": 6, "context": "Table 1: Comparison among Systems on IWSLT data set, \u201cReLU\u201d, \u201cSigmoid\u201d and \u201cLR\u201d stands for the rectified linear, sigmoid and the leaky rectified linear activation function; \u201cRes\u201d represents the residual connection whose operations we used include addition (\u201cAdd\u201d) and multiplication (\u201cMul\u201d); while \u201cDC\u201d is the abbreviation of dense concatenation (Huang et al. 2016).", "startOffset": 346, "endOffset": 365}, {"referenceID": 9, "context": "(Meng et al. 2016) introduces a new attention mechanism to model the interaction between the decoder and the representation of the source sentence during translation by reading memory and writing memory, for original Groundhog or RNNSearch model, the representation of the source sentence keep unchanged during the whole decoding process (Bahdanau, Cho, and Bengio 2014) , interactive attention mechanism write a new source representation into memory for each step in decoding and could keep track of the interaction history between the decoder and the representation of source sentence during translation.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": ") try to employ a coverage model to enhance the standard attention mechanism, enforcing decoder to tend to attend those source words untranslated and ignore those translated ones; (Bastings et al. 2017) adopt the dependency tree of the source sentence to restraint the decoder to focus on those words having dependency relation when generating each target words.", "startOffset": 180, "endOffset": 202}], "year": 2017, "abstractText": "Even though sequence-to-sequence neural machine translation (NMT) model have achieved state-of-art performance in the recent fewer years, but it is widely concerned that the recurrent neural network (RNN) units are very hard to capture the long-distance state information, which means RNN can hardly find the feature with long term dependency as the sequence becomes longer. Similarly, convolutional neural network (CNN) is introduced into NMT for speeding recently, however, CNN focus on capturing the local feature of the sequence; To relieve this issue, we incorporate a relation network into the standard encoder-decoder framework to enhance information-propogation in neural network, ensuring that the information of the source sentence can flow into the decoder adequately. Experiments show that proposed framework outperforms the statistical MT model and the state-ofart NMT model significantly on two data sets with different scales.", "creator": "TeX"}}}