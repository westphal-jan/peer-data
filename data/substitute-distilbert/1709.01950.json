{"id": "1709.01950", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "\"Having 2 hours to write a paper is fun!\": Detecting Sarcasm in Numerical Portions of Text", "abstract": "sarcasm occurring due to the presence of numerical portions involving text has been quoted as effective approximation made by automatic sarcasm detection approaches in centuries past. we present a first study in detecting sarcasm in numbers, as in the chorus of the sentence'love waking up approximately 4 am '. we analyze the challenges today pursuing subsequent, and present rule - based, machine learning and deep learning approaches to detect words in numerical portions of poetry. our deep learning approach outperforms four past works for sarcasm detection and algorithm - based and machine learning approaches on a dataset of tweets, creating an f1 - score of 0. 01. this shows that special attention to text containing numbers may be useful to improve state - of - the - art in sarcasm detection.", "histories": [["v1", "Wed, 6 Sep 2017 18:09:15 GMT  (266kb,D)", "http://arxiv.org/abs/1709.01950v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lakshya kumar", "arpan somani", "pushpak bhattacharyya"], "accepted": false, "id": "1709.01950"}, "pdf": {"name": "1709.01950.pdf", "metadata": {"source": "CRF", "title": "\u201cHaving 2 hours to write a paper is fun!\u201d: Detecting Sarcasm in Numerical Portions of Text", "authors": ["Lakshya Kumar", "Arpan Somani", "Pushpak Bhattacharyya"], "emails": ["lakshya@cse.iitb.ac.in", "somani@cse.iitb.ac.in", "pb@cse.iitb.ac.in"], "sections": [{"heading": "1 Introduction", "text": "Computational detection of sarcasm has seen attention from the sentiment analysis community in the past few years (Joshi et al., 2016a). Sarcasm is an interesting problem for sentiment analysis because surface sentiment of words in a sarcastic text may be different from the implied sentiment. For example, \u2018Being stranded in traffic is the best way to start a week\u2019 is a sarcastic sentence because the surface sentiment of the word \u2018best\u2019 (positive) is different from the implied sentiment of the sentence (negative), considering remaining portions of the text.\nPast approaches for sarcasm detection report features related to sentiment (Gonza\u0301lez-Iba\u0301nez et al., 2011), author\u2019s historical context (Ra-\njadesingan et al., 2015), and conversational context (Joshi et al., 2016b). Error analysis presented in many of these works has served as a motivation for future work. Our paper is based on an error observed by Joshi et al. (2015): \u2018Incongruity in numbers, resulting in sarcasm\u2019. Consider the sentence in the title of this submission: \u2018Having 2 hours to write a paper is fun1\u2019. The number \u20182 hours\u2019 is a crucial indicator of the sarcasm in this sentence. The sarcasm is based on the understanding that two hours may not be sufficient to write a paper. If the number of hours is changed to a higher value, the sarcasm in the sentence may not hold, given that the value is a sufficient duration to write a paper. This paper deals with detecting sarcasm in numerical portions of text, as in this example.\nWe first present a rule-based approach to detect sarcasm expressed due to numbers. Our approach compares numerical magnitudes with those seen in similar contexts in a training dataset. Since \u2018similar context\u2019 is key here, we consider two variants of our approach in order to match the context. Then we present Machine learning based approach and its variant that take different features as input for learning. Further we propose deep learning based approaches to numerical sarcasm detection on social media that does not require extensive manual feature engineering. Instead, we develop Convolution Neural Network (CNN) to capture local correlations of spatial or temporal structure. We also develop Long-short Term Memory (LSTM) network which is able to handle sequences of any length and capture longterm dependencies. We compare our approaches with four past works, and show an improvement.\n1This sentence is only an example. This paper was not written in 2 hours.\nar X\niv :1\n70 9.\n01 95\n0v 1\n[ cs\n.C L\n] 6\nS ep\n2 01\n7\nTo the best of our knowledge, this is the first reported work that deals with sarcasm in numerical portions of text. We present our evaluation on a dataset of tweets.\nThe rest of the paper is organized as follows. We describe the related work in Section 2. Then, we motivate this work in Section 3, and describe various approaches to detect sarcasm in numerical portions of text in Section 4. The experiment setup is outlined in Section 5. Results of our experiments are given in Section 6, while Section 7 discusses the errors. Finally, the conclusion and future work is described in Section 8."}, {"heading": "2 Related Work", "text": "Sarcasm and irony detection has been extensively studied in linguistic, psychology and cognitive science (Gibbs, 1986; Utsumi, 2000). Computational detection of sarcasm has become a popular area of natural language processing research in recent years Joshi et al. (2016a). Tepperman et al. (2006) present sarcasm recognition in speech using spectral (average pitch, pitch slope, etc.), prosodic and contextual cues. Carvalho et al. (2009) use simple linguistic features like interjection, changed names, etc. for irony detection. Davidov et al. (2010) train a sarcasm classifier with syntactic and pattern-based features. Gonza\u0301lez-Iba\u0301nez et al. (2011) states that sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite. Liebrecht et al. (2013) showed that sarcasm is often signaled by hyperbole, using intensifiers and exclamations; in contrast, nonhyperbolic sarcastic messages often receive an explicit marker.\nRiloff et al. (2013) states that sarcasm is a contrast between a positive sentiment word and a negative situation. Buschmeier et al. (2014) provided the baseline for classification of ironic or sarcastic reviews. They analyzed the impact of different features for the classification task. The work by Joshi et al. (2015) shows how sarcasm arises because of implicit or explicit incongruity in the sentence. Bouazizi and Ohtsuki (2016) proposed a pattern-based approach to detect sarcasm on Twitter. They proposed four sets of features that cover the different types of sarcasm.\nAs deep learning techniques gain popularity, few deep learning based architectures for sarcasm detection have also appeared in literature. Ghosh and Veale (2016) provides a neural net-\nwork semantic model for sarcasm detection. Their model composed of Convolution Neural Network (CNN) followed by a Long Short Term Memory (LSTM) network and finally a Deep Neural Network(DNN). Poria et al. (2016) proposed a novel method to detect sarcasm using Convolution Neural Networks. They have developed models based on a pre-trained convolutional neural network for extracting sentiment, emotion and personality features for sarcasm detection. Amir et al. (2016) proposed a deep-learning based architecture to automatically learn user embeddings. In their proposed approach they have used this user embeddings to provide contextual features, going beyond the lexical and syntactic cues for sarcasm. Zhang et al. (2016) used a bi-directional gated recurrent neural network followed by a pooling neural network to detect sarcasm.\nAll these past works deals with the detection of sarcasm that occurs in text but they did not talk about incongruity due to numbers. Our work is the first in this area that tackles the task to identify the presence of numerical sarcasm in tweets."}, {"heading": "3 Motivation", "text": "Consider the following sentences:\n1. This phone has an awesome battery back-up of 38 hours. (Non-sarcastic).\n2. This phone has an awesome battery back-up of 2 hours. (Sarcastic).\n3. This phone has a terrible battery back-up of 2 hours (Non-sarcastic).\nSentences 1 and 3 are non-sarcastic while Sentence 2 is sarcastic. Consider Sentences 1 and 2. The two sentences differ only in the numerical value (\u201838\u2019 versus \u20182\u2019). The sarcasm in Sentence 2 can be understood in terms of the incongruity2 between the word \u2018awesome\u2019 and \u20182 hours\u2019 in case of the battery life. On the contrary, Sentences 2 and 3 differ in one word with varying surface sentiment (\u2018awesome\u2019 versus \u2018terrible\u2019). Detecting sarcasm in sentences like Sentence 2 using information from Sentences 1 and 3 in a dataset is the key idea of our rule-based approach to detect sarcasm in numerical portions of text.\nSince sarcasm is an infrequent phenomenon and we deal with a specific form of sarcasm (namely\n2Ivanko and Pexman (2003) describe the relationship between incongruity and sarcasm.\nsarcasm in numerical text), it is worthwhile to estimate how many sarcastic sentences contain numbers. A set of approximately 100,000 sarcastic tweets contained 11,488 tweets with numbers in them, amounting to 11.48%."}, {"heading": "4 Approaches", "text": "In order to detect numerical sarcasm, we implement two rule-based approaches as described in the forthcoming subsections."}, {"heading": "4.1 Approach-1: Noun phrase exact matching", "text": "In this approach, we first create two repositories, i.e., sarcastic and non-sarcastic using a training dataset. Each entry in the repository is of the format: (Tweet Index No., Noun Phrase list, Mean of Number unit3, Std Dev of Number unit4, Number Unit5). The repositories are created as follows. For each sarcasm-labeled tweet in the training dataset, we perform the following steps:\n\u2022 Step-1: Extract noun phrases in the tweet, using a parser.\n\u2022 Step-2: Select Number unit as the word following the word POS tagged as \u2019CD\u2019. Examples of number units are minutes, hour, days, years etc.\n\u2022 Step-3: We add an entry to the corresponding repository according to the label of the tweet. This entry is of the format as specified above.\nFor example, if there is a sarcastic tweet- \u201cThis phone has an awesome battery back-up of 2 hours\u201d, constituency parse tree of this tweet obtained from nltk parser is shown in Figure 1 :\nWe extract the Noun phrases from this constituency parse tree and create a noun-phrase list of this sarcastic tweet as follows: [\u2018phone\u2019, \u2018awesome\u2019, \u2018battery\u2019, \u2018backup\u2019, \u2018hours\u2019] After obtaining the noun phrase list, tweet is stored in the sarcastic repository as: (Tweet Index No., [\u2018phone\u2019, \u2018awesome\u2019, \u2018battery\u2019, \u2018backup\u2019, \u2018hours\u2019], mean of numbers\n3The average value of all the numbers corresponding to the Number unit\n4Standard Deviation of all the numbers corresponding to the Number unit\n5We calculate the Mean and SD for all possible units present in the dataset\nhaving unit as \u2018hours\u2019, Std dev of numbers having unit as \u2018hours\u2019 ,\u2018hours\u2019 ) Similarly, for all the other tweets, i.e, numericalsarcastic/ non-sarcastic tweets, same approach is used to store them in their respective repositories.\nNumerical Sarcasm in a test tweet is predicted as follows. We extract noun phrases, number and number unit from the test tweet. Then, following rules are applied:\n\u2022 We first consult the sarcastic tweet repository. On matching words in the noun phrase list between the test tweet and entries in the repository, the most similar entry is selected from the sarcastic repository. We then match the number unit of the entry with that in the test tweet.\n\u2022 If the number unit matches, we check whether the number present in the test tweet lies within \u00b12.586 Std dev of the mean value for that number unit present in the matched sarcastic entry. For example: Test Tweet: \u2018I love writing this paper at 11 am\u2019. Matched Sarcastic Tweet: \u2018I love writing this paper daily at 3.5 am\u2019. So, the number 11 is not within desired confidence interval and the test tweet is non-sarcastic.\n\u2022 If the number unit does not match, we consult the non-sarcastic tweet repository and find the most similar entry based on exact matching of the noun phrase list of test tweet and entry in the repository. If the number unit matches, we check whether the number present in the test tweet lies within\u00b12.58 Std dev of the mean value for that number unit present in the matched non-sarcastic entry. If it lies then predict the tweet as non-sarcastic\n6z value of 2.58 indicates the 99% Confidence Interval\nand if it does not then predict sarcastic. For example: Test Tweet: \u2018I am so productive when my room is 81 degrees\u2019. Matched Non-sarcastic Tweet: \u2018I am very much productive in my room as it has 21.27 degrees\u2019. So, the number 81 is not within desired confidence interval and the tweet becomes sarcastic.\n\u2022 Finally, if no match is found, then the tweet is predicted as non-sarcastic."}, {"heading": "4.2 Approach-2: Noun phrase cosine similarity matching", "text": "Approach-1 is restrictive in terms of the phrase matching. Therefore, Approach-2 relaxes the constraint by using cosine similarity to match the phrases in the test tweet and the repository entry. This approach also creates two repositories, i.e., sarcastic and non-sarcastic as follows:\n\u2022 Step-1: We first convert the noun phrase list into its vector representation.\n\u2022 Step-2: This vector representation is created by summation of the 200-dimension word embeddings7 of the words present in the noun phrase list and then dividing it by their count.\n\u2022 Step-3: Now the repositories are of the form: (Tweet Index No., Vector representation of Noun phrase list, Mean of Number unit, Std Dev of Number unit, Number Unit).\nFor example, if there is a numerical sarcastic tweet- \u201c8:30 am meetings are the best way to start birthday weekend\u201d. The Noun phrase list of this tweet is : [\u2018meetings\u2019, \u2018way\u2019, \u2018birthday\u2019, \u2018weekend\u2019] In order to create the vector representation of this tweet, 200-dimension vector embedding of each of the word in the noun-phrase list is added and then each component of the resulting embedding is divided by 4 as their are four words in the nounphrase list. The intuition behind using the vector representation created from the noun-phrase list is that it helps to easily capture the tweets that are similar to the new tweet. The rules that are used in this approach are also same as used in Approach-1 but now to match the test tweet and tweets from repository, cosine similarity is used. In this case, the entry with the maximum cosine similarity is\n7Learned from a tweet corpus containing 6 million tweet words.\nselected (as against the entry with exact match in Approach-1)."}, {"heading": "4.3 Machine Learning based approach", "text": "In order to create machine learning based approach for detecting numerical sarcasm we train SVM, KNN and Random Forest classifiers using different types of features as described below :\n\u2022 Sentiment-based features : These features include number of positive words, number of negative words, number of highly emotional positive words, number of highly emotional negative words. Positive/Negative word is said to be highly emotional if its part-ofspeech tag is one among the following : \u2018JJ\u2019, \u2018JJR\u2019, \u2018JJS\u2019, \u2018RB\u2019, \u2018RBR\u2019, \u2018RBS\u2019, \u2018VB\u2019, \u2018VBD\u2019, \u2018VBG\u2019, \u2018VBN\u2019, \u2018VBP\u2019, \u2018VBZ\u2019.\n\u2022 Emoticon-based features : These features includes positive emoticon, negative emoticon, contrast between word, i.e, a boolean feature that will be one if both positive and negative words are present in the tweet, contrast between emoji, i.e, it will take the value as one when either positive word and negative emoji is present or negative word and positive emoji is present in the tweet.\n\u2022 Punctuation-based features : These features include number of exclamation marks, number of dots, number of question mark, number of capital letter words, number of single quotations.\n\u2022 Number in the tweet : This feature is simply the number present in the tweet.\n\u2022 Number unit in the tweet : This feature is a one hot representation of the type of unit present in the tweet. Example of number unit can be hour, minute, etc. So, based on the unit present in the tweet that position in the one hot vector takes the value as one and the rest takes the value zero.\n\u2022 Tweet Embeddings: We learn word embeddings of different dimensions, i.e., 25-D, 50- D, 100-D, 150-D, 200-D, 250-D, 300-D of tweet words using word2vec (Mikolov et al., 2013) tool on a large corpora of 6 million tweets. These word embeddings are used to create the tweet embedding by summing up the word vectors of all the words present in\nthe tweet and finally dividing each component of the resulting vector by the count of total number of words present in the tweet. Finally, we obtain the tweet embeddings of different dimension that we use to train the classifiers."}, {"heading": "4.4 Deep Learning based approach", "text": ""}, {"heading": "4.4.1 CNN-FF Model", "text": "The architecture of the CNN-FF model is shown in Figure 2. There is an embedding matrix E \u2208 IR|V |\u00d7d where |V | is the vocabulary size and d is the tweet word embedding dimension. For the input tweet we obtain an input matrix I \u2208 IR|S|\u00d7d where |S| is the length of the tweet including padding, where Ii be the d-dimension vector for i-th word in the tweet in the input matrix. Let k be the length of the filter, and the vector f \u2208 IR|k|\u00d7d is a filter for the convolution operation. For each position p in the input matrix I, there is a window wp of k consecutive words, denoted as:\nwp = [Ip, Ip+1, ..., Ip+k\u22121] (1)\nA filter f convolves with the window vectors (kgrams) at each position in a valid way to generate a feature map c \u2208 IR|S|\u2212k+1 each element cp of the feature map for window vector wp is produced as follows:\ncp = func(wp \u25e6 f + b) (2)\nwhere \u25e6 is element-wise multiplication, b \u2208 IR is a bias term and func is a nonlinear transformation function that can be sigmoid, hyperbolic tangent, etc. Max-over-time pooling is then applied over the obtained feature map. We use multiple filters of different sizes and output from each filter is concatenated to get the final overall feature vector. This feature vector act as input for the fully-connected layer. We train the entire model by minimizing the binary cross-entropy loss over a mini-batch of training examples of size e.\nE(y, y\u0302) = e\u2211\ni=1\nyi log(y\u0302i) (3)"}, {"heading": "4.4.2 LSTM-FF Model", "text": "RNN have demonstrated the power to capture sequential information in a chain-like neural network. Standard RNNs becomes unable to learn long-term dependencies as the gap between two\ntime steps becomes large. We adopted the standard architecture of LSTM proposed by (Hochreiter and Schmidhuber, 1997).\nThe LSTM architecture has a range of repeated modules for each time step as in a standard RNN. At each time step, the output of the module is controlled by a set of gates in IRd as a function of the old hidden state ht\u22121 and the input at the current time step xt: the forget gate ft, the input gate it, and the output gate ot. These gates collectively decide how to update the current memory cell ct and the current hidden state ht . We use d to denote the memory dimension in the LSTM and all vectors in this architecture share the same dimension. The LSTM transition functions are defined as follows:\nit = \u03c3(Wi \u00b7 [ht\u22121, xt] + bi) (4)\nft = \u03c3(Wf \u00b7 [ht\u22121, xt] + bf ) (5)\nC\u0303t = tanh(WC \u00b7 [ht\u22121, xt] + bC) (6)\nCt = ft Ct\u22121 + it C\u0303t (7)\not = \u03c3(Wo \u00b7 [ht\u22121, xt] + bo) (8)\nht = ot tanh(Ct) (9)\nThis architecture is shown in Figure 3. In order to convert the input tweet T into its matrix representation I , embedding matrix E is used as explained in 4.4.1. This input matrix is given as input to LSTM cell one word at a time. The output from each time step is stored, on which mean-pooling operation is performed to get the final feature vector of the tweet. This feature vector is passed to the fully connected layer and model is trained by minimizing binary cross-entropy error."}, {"heading": "4.4.3 CNN-LSTM-FF Model", "text": "This architecture is shown in Figure 4. The input matrix representation for the Tweet T is obtained from the Embedding matrix E as described in 4.4.1. Filters of size 5Xd, where d is the tweet word embedding dimension, slides over the input matrix I of the tweet in order to extract the features. We passed the output of the convolutional network through a pooling layer and max-pooling is used with size 4. All the filters are of same dimension and after performing pooling operation over their outputs, we obtained a concatenated feature matrix denoted as:\nC = [c1; c2; .......cn]\nwhere n in cn denotes the total number of filters used in the architecture. Feature matrix C \u2208 IRl\u00d7n, each ci \u2208 IRl, where l is the dimension obtained after pooling operation. Let xj \u2208 IR1\u00d7n is vector obtained from matrix C. Vector xj is the input for the LSTM cell at jth timestep and the LSTM cell runs for l timesteps taking different input obtained from matrix C, at each timestep. At the end of lth timestep, output from the LSTM cell act as input for the fully connected layer and training is performed similar to other architectures, i.e., by minimizing binary cross-entropy loss."}, {"heading": "5 Experiment Setup", "text": "We use three datasets for performing experiments which are described in Table 1 and their creation details are described below: (A) Dataset-1: To create this dataset, we extract tweets from Twitter-API (https://dev. twitter.com). The tweets containing hashtags #sarcasm, #sarcastic, #BeingSarcastic are labeled\nas sarcastic, while those with #nonsarcasm, #notsarcastic are labeled as the non-sarcastic. We remove URLs, duplicate tweets, retweets, Username and other Non-ASCII characters in these tweets. (B) Dataset-2: From Dataset-1, we retain only the tweets that contain numerical characters to create Dataset-2. Additional processing is performed to remove irrelevant tweets, like the ones which contains alphabet or special character adjacent to a number like Model34d, 4s, <3 (heart smiley) etc. We divide this dataset into Training set containing 8681 Non-Sarcastic tweets and 8681 Numerical Sarcastic tweets. (C) Dataset-3: For Deep learning experiments, we need a bigger dataset, so we created Dataset3 from dataset-1, whose details are given in Table 1. Since there were no more Numeric-Sarcastic tweets we added more Non-Sarcastic tweets to increase dataset size. (D) Test Data: Test set details are given in Table 1. This dataset is used to evaluate previous approaches as well as all the approaches mentioned in this paper.\nWe re-implement work reported by Buschmeier et al. (2014), Gonza\u0301lez-Iba\u0301nez et al. (2011), Liebrecht et al. (2013) and Joshi et al. (2015) for Sarcasm detection, to show the degradation in performance for Numeric-Sarcastic Dataset. We train classifiers for the features introduced by these approaches, using SVMperf by Joachims (2006) with RBF kernel. We compare their performance\nagainst our approaches, and report the average 5- fold cross-validation values in the next section.\nWe train the SVM with RBF kernel and c = 1.0 using grid-search, Random-forest with number of estimators = 10 and KNN with neighbors K = 3 using scikit8 using features as described in section 4.3 and using the same dataset, i.e., Dataset-2 which is also used in Rule-based approach. The test data that we use is the same as already described in Table 1.\nFrom Dataset-3, we calculated the max length of the tweet as 36 words so we padded all the shorter tweets by special PAD character. In all deep learning experiments, we initialized embedding matrix E first by random values and then by pre-trained word embeddings and tried to investigate results with different size word embeddings.\nFor CNN-FF Model, we use 128 number of filters each of size 3, 4 and 5, i.e., total 128 \u00d7 3 filters. Drop-out probability for this implementation is 0.5 and training is performed by applying mini-batch gradient descent. In the pooling layer, we perform max-over time pooling over the output from each filter.\nFor LSTM-FF model, training is performed by applying batch-gradient descent with Adagrad optimizer having learning rate as 0.3. We also investigated with different hidden unit dimension as 20, 40 and 128 and drop-out probability of 0.25.\nFor CNN-LSTM-FF Model, number of filters are 64, each with the same dimension, i.e., 5 \u00d7 Embeddingsize. We investigated the results with different size embeddings and drop-out probability is 0.25 for this architecture. All the deep learning experiments were implemented using tensorflow (2016)."}, {"heading": "6 Results", "text": "In this section, we evaluate our approaches to detect sarcasm in numerical portions of text.\nTable 2 evaluates the performance of four previous approaches on Dataset-1 and on Dataset-2. We see that three of the past four works give an\n8http://scikit-learn.org/stable/modules\nF1-score that is close to each other on Dataset-2. On Dataset-1 and Dataset-2, the best F1-score of 0.72 and 0.25 respectively are obtained by using features from Joshi et al. (2015), there is a degradation of 47% on Dataset-2 which contains only numerical tweets. This degradation clearly shows that these past approaches are not able to capture the sarcasm that arise due to numbers in the text. All the past 4 approaches are build to detect the normal sarcasm in which the incongruity arises due to text. When the incongruity arises due to numbers these approaches gets degraded in their performance as shown in Table 2. This clearly shows that there is a need to develop a system that is able to capture numerical sarcasm.\nThe results of Rule-based approaches are shown Table 6, along with results of other approaches. We see that among the 2 rule based approach, Approach-1 performs better with an F1-score of 0.82.\nThe evaluation of machine learning experiments is done using different combination of features, i.e., Sentiment (S), Punctuation (P), Emoticon based (E), Number value and Number Unit. We have also investigated with different dimensional (25-D, 50-D...300-D) tweet embeddings as features. We train SVM classifier and reported the results in Table 3. Results show that SVM give best F1-score of 0.83 with 300-D Tweet word embedding . We train KNN classifier and results are reported in 4. This classifiers also give best result with tweet-embedding as feature but of different dimension, i.e., 50-D. Table 5 shows the result of Random Forest classifier. We observe from the table that as we append more features like number\nvale and number unit to S, P and E the F1-score becomes 0.75. Finally, when we give 100-D tweet embedding as input, the best F1-score is 0.82.\nAs mentioned in section 5, we performed deep learning experiments by initializing embedding matrix in two settings, the best result for all the deep-learning experiments are obtained by initializing with the pre-trained tweet word embeddings. Refer Table 6 for deep learning experiments results. For CNN-FF model, the F1-score of 0.93 is obtained with embedding size of 250-D. For CNNLSTM-FF model, the F1-score of 0.91 with 200- D embedding size and for LSTM-FF model, we obtain the F1-score of 0.90 with 25-D embedding size.\nTable 6 compares the results for all the implemented approaches and also the past approaches. The best overall F1-score of 0.93 is obtained by CNN-FF Model. We see an improvement of 68% in F1-score against the best performing past approach of Joshi et al. (2015)."}, {"heading": "7 Error Analysis", "text": "We classify the errors by our approaches into the following categories:\n\u2022 Unit Mismatch/Unit Missing: There are some tweets in which the number unit is present in very informal way like min for minutes and hr for hours. So these types of unit created problem while performing the unit matching test. In some tweets the number unit is missing for example- \u2018i love waking up at 545\u2019. Unit is not present as well as\nthe time is present in wrong format.\n\u2022 Presence of multiple numbers: Some tweets contain multiple numbers and this makes it more challenging to identify sarcasm due to numbers. For example- \u2018 $34.04 for a 10 mile trip that takes 19 minutes? that makes sense\u2019.\n\u2022 Sarcasm due to text but not number: Some tweets contain number but they are sarcastic not because of the presence of number. For example- \u2018First asthma attack in 6 years. forgot how much fun they are\u2019, in this tweet the sarcasm is arising because of the word \u201cfun\u201d present at the end of the tweet."}, {"heading": "8 Conclusion & Future Work", "text": "Our paper presents the novel idea for identifying sarcasm that arises due to the presence of numerical portions in the tweets. Numerical sarcasm is a special case of sarcasm where incongruity arises between textual and numerical content. It shows the degradation in performance of the four past approaches over the dataset containing numerical sarcastic tweets. We further present Rule based, Machine learning and Deep learning approaches for numerical sarcasm detection and obtains best overall F1-score of 0.93 from CNN-FF model. In this work we try to build a system to detect numerical sarcasm in the tweets. Our work opens a new avenue in sarcasm detection as previous approaches are unable to capture numerical sarcasm because of their ability to capture the cues for normal sarcasm. In future, all the past approaches for\nsarcasm detection can benefit with out work with by separating the normal sarcasm from numerical sarcasm and improve performance."}], "references": [{"title": "Tensorflow: A system for large-scale machine learning", "author": ["Paul Tucker", "Vijay Vasudevan", "Pete Warden", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng."], "venue": "12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). pages 265\u2013", "citeRegEx": "Tucker et al\\.,? 2016", "shortCiteRegEx": "Tucker et al\\.", "year": 2016}, {"title": "Modelling context with user embeddings for sarcasm detection in social media", "author": ["Silvio Amir", "Byron C Wallace", "Hao Lyu", "Paula Carvalho M\u00e1rio J Silva."], "venue": "arXiv preprint arXiv:1607.00976 .", "citeRegEx": "Amir et al\\.,? 2016", "shortCiteRegEx": "Amir et al\\.", "year": 2016}, {"title": "A pattern-based approach for sarcasm detection on twitter", "author": ["Mondher Bouazizi", "Tomoaki Otsuki Ohtsuki."], "venue": "IEEE Access 4:5477\u20135488.", "citeRegEx": "Bouazizi and Ohtsuki.,? 2016", "shortCiteRegEx": "Bouazizi and Ohtsuki.", "year": 2016}, {"title": "An impact analysis of features in a classification approach to irony detection in product reviews", "author": ["Konstantin Buschmeier", "Philipp Cimiano", "Roman Klinger."], "venue": "Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sen-", "citeRegEx": "Buschmeier et al\\.,? 2014", "shortCiteRegEx": "Buschmeier et al\\.", "year": 2014}, {"title": "Clues for detecting irony in user-generated contents: oh...!! it\u2019s so easy;", "author": ["Paula Carvalho", "Lu\u0131\u0301s Sarmento", "M\u00e1rio J Silva", "Eug\u00e9nio De Oliveira"], "venue": "In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for", "citeRegEx": "Carvalho et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Carvalho et al\\.", "year": 2009}, {"title": "Semi-supervised recognition of sarcastic sentences in twitter and amazon", "author": ["Dmitry Davidov", "Oren Tsur", "Ari Rappoport."], "venue": "Proceedings of the fourteenth conference on computational natural language learning. Association for Computational Lin-", "citeRegEx": "Davidov et al\\.,? 2010", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Fracking sarcasm using neural network", "author": ["Aniruddha Ghosh", "Tony Veale."], "venue": "Proceedings of NAACL-HLT . pages 161\u2013169.", "citeRegEx": "Ghosh and Veale.,? 2016", "shortCiteRegEx": "Ghosh and Veale.", "year": 2016}, {"title": "On the psycholinguistics of sarcasm", "author": ["Raymond W Gibbs."], "venue": "Journal of Experimental Psychology: General 115(1):3.", "citeRegEx": "Gibbs.,? 1986", "shortCiteRegEx": "Gibbs.", "year": 1986}, {"title": "Identifying sarcasm in twitter: a closer look", "author": ["Roberto Gonz\u00e1lez-Ib\u00e1nez", "Smaranda Muresan", "Nina Wacholder."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies:", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.,? 2011", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.", "year": 2011}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Context incongruity and irony processing", "author": ["Stacey L Ivanko", "Penny M Pexman."], "venue": "Discourse Processes 35(3):241\u2013279.", "citeRegEx": "Ivanko and Pexman.,? 2003", "shortCiteRegEx": "Ivanko and Pexman.", "year": 2003}, {"title": "Training linear svms in linear time", "author": ["Thorsten Joachims."], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, pages 217\u2013226.", "citeRegEx": "Joachims.,? 2006", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Automatic sarcasm detection: A survey", "author": ["Aditya Joshi", "Pushpak Bhattacharyya", "Mark James Carman."], "venue": "arXiv preprint arXiv:1602.03426 .", "citeRegEx": "Joshi et al\\.,? 2016a", "shortCiteRegEx": "Joshi et al\\.", "year": 2016}, {"title": "Harnessing context incongruity for sarcasm detection", "author": ["Aditya Joshi", "Vinita Sharma", "Pushpak Bhattacharyya."], "venue": "ACL (2). pages 757\u2013762.", "citeRegEx": "Joshi et al\\.,? 2015", "shortCiteRegEx": "Joshi et al\\.", "year": 2015}, {"title": "Harnessing sequence labeling for sarcasm detection in dialogue from tv series friends", "author": ["Aditya Joshi", "Vaibhav Tripathi", "Pushpak Bhattacharyya", "Mark Carman."], "venue": "CoNLL 2016 page 146.", "citeRegEx": "Joshi et al\\.,? 2016b", "shortCiteRegEx": "Joshi et al\\.", "year": 2016}, {"title": "The perfect solution for detecting sarcasm in tweets", "author": ["CC Liebrecht", "FA Kunneman", "APJ van den Bosch"], "venue": null, "citeRegEx": "Liebrecht et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liebrecht et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A deeper look into sarcastic tweets using deep convolutional neural networks", "author": ["Soujanya Poria", "Erik Cambria", "Devamanyu Hazarika", "Prateek Vij."], "venue": "arXiv preprint arXiv:1610.08815 .", "citeRegEx": "Poria et al\\.,? 2016", "shortCiteRegEx": "Poria et al\\.", "year": 2016}, {"title": "Sarcasm detection on twitter: A behavioral modeling approach", "author": ["Ashwin Rajadesingan", "Reza Zafarani", "Huan Liu."], "venue": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining. ACM, pages 97\u2013106.", "citeRegEx": "Rajadesingan et al\\.,? 2015", "shortCiteRegEx": "Rajadesingan et al\\.", "year": 2015}, {"title": "Sarcasm as contrast between a positive sentiment and negative situation", "author": ["Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang."], "venue": "EMNLP. volume 13, pages 704\u2013714.", "citeRegEx": "Riloff et al\\.,? 2013", "shortCiteRegEx": "Riloff et al\\.", "year": 2013}, {"title": " yeah right\u201d: sarcasm recognition for spoken dialogue systems", "author": ["Joseph Tepperman", "David R Traum", "Shrikanth Narayanan."], "venue": "INTERSPEECH.", "citeRegEx": "Tepperman et al\\.,? 2006", "shortCiteRegEx": "Tepperman et al\\.", "year": 2006}, {"title": "Verbal irony as implicit display of ironic environment: Distinguishing ironic utterances from nonirony", "author": ["Akira Utsumi."], "venue": "Journal of Pragmatics 32(12):1777\u2013 1806.", "citeRegEx": "Utsumi.,? 2000", "shortCiteRegEx": "Utsumi.", "year": 2000}, {"title": "Tweet sarcasm detection using deep neural network", "author": ["Meishan Zhang", "Yue Zhang", "Guohong Fu."], "venue": "Proceedings of the 26th International Conference on Computational Linguistics. pages 2449\u20132460.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Computational detection of sarcasm has seen attention from the sentiment analysis community in the past few years (Joshi et al., 2016a).", "startOffset": 114, "endOffset": 135}, {"referenceID": 8, "context": "Past approaches for sarcasm detection report features related to sentiment (Gonz\u00e1lez-Ib\u00e1nez et al., 2011), author\u2019s historical context (Rajadesingan et al.", "startOffset": 75, "endOffset": 105}, {"referenceID": 18, "context": ", 2011), author\u2019s historical context (Rajadesingan et al., 2015), and conversational context (Joshi et al.", "startOffset": 37, "endOffset": 64}, {"referenceID": 14, "context": ", 2015), and conversational context (Joshi et al., 2016b).", "startOffset": 36, "endOffset": 57}, {"referenceID": 8, "context": "Past approaches for sarcasm detection report features related to sentiment (Gonz\u00e1lez-Ib\u00e1nez et al., 2011), author\u2019s historical context (Rajadesingan et al., 2015), and conversational context (Joshi et al., 2016b). Error analysis presented in many of these works has served as a motivation for future work. Our paper is based on an error observed by Joshi et al. (2015): \u2018Incongruity in numbers, resulting in sarcasm\u2019.", "startOffset": 76, "endOffset": 369}, {"referenceID": 7, "context": "Sarcasm and irony detection has been extensively studied in linguistic, psychology and cognitive science (Gibbs, 1986; Utsumi, 2000).", "startOffset": 105, "endOffset": 132}, {"referenceID": 21, "context": "Sarcasm and irony detection has been extensively studied in linguistic, psychology and cognitive science (Gibbs, 1986; Utsumi, 2000).", "startOffset": 105, "endOffset": 132}, {"referenceID": 5, "context": "Sarcasm and irony detection has been extensively studied in linguistic, psychology and cognitive science (Gibbs, 1986; Utsumi, 2000). Computational detection of sarcasm has become a popular area of natural language processing research in recent years Joshi et al. (2016a). Tepperman et al.", "startOffset": 106, "endOffset": 272}, {"referenceID": 5, "context": "Sarcasm and irony detection has been extensively studied in linguistic, psychology and cognitive science (Gibbs, 1986; Utsumi, 2000). Computational detection of sarcasm has become a popular area of natural language processing research in recent years Joshi et al. (2016a). Tepperman et al. (2006) present sarcasm recognition in speech using spectral (average pitch, pitch slope, etc.", "startOffset": 106, "endOffset": 297}, {"referenceID": 4, "context": "Carvalho et al. (2009) use simple linguistic features like interjection, changed names, etc.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Carvalho et al. (2009) use simple linguistic features like interjection, changed names, etc. for irony detection. Davidov et al. (2010) train a sarcasm classifier with syntactic and pattern-based features.", "startOffset": 0, "endOffset": 136}, {"referenceID": 4, "context": "Carvalho et al. (2009) use simple linguistic features like interjection, changed names, etc. for irony detection. Davidov et al. (2010) train a sarcasm classifier with syntactic and pattern-based features. Gonz\u00e1lez-Ib\u00e1nez et al. (2011) states that sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite.", "startOffset": 0, "endOffset": 236}, {"referenceID": 4, "context": "Carvalho et al. (2009) use simple linguistic features like interjection, changed names, etc. for irony detection. Davidov et al. (2010) train a sarcasm classifier with syntactic and pattern-based features. Gonz\u00e1lez-Ib\u00e1nez et al. (2011) states that sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite. Liebrecht et al. (2013) showed that sarcasm is often signaled by hyperbole, using intensifiers and exclamations; in contrast, nonhyperbolic sarcastic messages often receive an explicit marker.", "startOffset": 0, "endOffset": 371}, {"referenceID": 2, "context": "Buschmeier et al. (2014) provided the baseline for classification of ironic or sarcastic reviews.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "Buschmeier et al. (2014) provided the baseline for classification of ironic or sarcastic reviews. They analyzed the impact of different features for the classification task. The work by Joshi et al. (2015) shows how sarcasm arises because of implicit or explicit incongruity in the sentence.", "startOffset": 0, "endOffset": 206}, {"referenceID": 2, "context": "Bouazizi and Ohtsuki (2016) proposed a pattern-based approach to detect sarcasm on Twitter.", "startOffset": 0, "endOffset": 28}, {"referenceID": 5, "context": "Ghosh and Veale (2016) provides a neural network semantic model for sarcasm detection.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "Ghosh and Veale (2016) provides a neural network semantic model for sarcasm detection. Their model composed of Convolution Neural Network (CNN) followed by a Long Short Term Memory (LSTM) network and finally a Deep Neural Network(DNN). Poria et al. (2016) proposed a novel method to detect sarcasm using Convolution Neural Networks.", "startOffset": 0, "endOffset": 256}, {"referenceID": 1, "context": "Amir et al. (2016) proposed a deep-learning based architecture to automatically learn user embeddings.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Amir et al. (2016) proposed a deep-learning based architecture to automatically learn user embeddings. In their proposed approach they have used this user embeddings to provide contextual features, going beyond the lexical and syntactic cues for sarcasm. Zhang et al. (2016) used a bi-directional gated recurrent neural network followed by a pooling neural network to detect sarcasm.", "startOffset": 0, "endOffset": 275}, {"referenceID": 16, "context": ", 25-D, 50D, 100-D, 150-D, 200-D, 250-D, 300-D of tweet words using word2vec (Mikolov et al., 2013) tool on a large corpora of 6 million tweets.", "startOffset": 77, "endOffset": 99}, {"referenceID": 9, "context": "We adopted the standard architecture of LSTM proposed by (Hochreiter and Schmidhuber, 1997).", "startOffset": 57, "endOffset": 91}, {"referenceID": 3, "context": "We re-implement work reported by Buschmeier et al. (2014), Gonz\u00e1lez-Ib\u00e1nez et al.", "startOffset": 33, "endOffset": 58}, {"referenceID": 3, "context": "We re-implement work reported by Buschmeier et al. (2014), Gonz\u00e1lez-Ib\u00e1nez et al. (2011), Liebrecht et al.", "startOffset": 33, "endOffset": 89}, {"referenceID": 3, "context": "We re-implement work reported by Buschmeier et al. (2014), Gonz\u00e1lez-Ib\u00e1nez et al. (2011), Liebrecht et al. (2013) and Joshi et al.", "startOffset": 33, "endOffset": 114}, {"referenceID": 3, "context": "We re-implement work reported by Buschmeier et al. (2014), Gonz\u00e1lez-Ib\u00e1nez et al. (2011), Liebrecht et al. (2013) and Joshi et al. (2015) for Sarcasm detection, to show the degradation in performance for Numeric-Sarcastic Dataset.", "startOffset": 33, "endOffset": 138}, {"referenceID": 3, "context": "We re-implement work reported by Buschmeier et al. (2014), Gonz\u00e1lez-Ib\u00e1nez et al. (2011), Liebrecht et al. (2013) and Joshi et al. (2015) for Sarcasm detection, to show the degradation in performance for Numeric-Sarcastic Dataset. We train classifiers for the features introduced by these approaches, using SVMperf by Joachims (2006) with RBF kernel.", "startOffset": 33, "endOffset": 334}, {"referenceID": 3, "context": "Approach Dataset-1 Dataset-2 Buschmeier et al. (2014) 0.", "startOffset": 29, "endOffset": 54}, {"referenceID": 3, "context": "Approach Dataset-1 Dataset-2 Buschmeier et al. (2014) 0.69 0.16 Gonz\u00e1lez-Ib\u00e1nez et al. (2011) 0.", "startOffset": 29, "endOffset": 94}, {"referenceID": 3, "context": "Approach Dataset-1 Dataset-2 Buschmeier et al. (2014) 0.69 0.16 Gonz\u00e1lez-Ib\u00e1nez et al. (2011) 0.68 0.15 Liebrecht et al. (2013) 0.", "startOffset": 29, "endOffset": 128}, {"referenceID": 3, "context": "Approach Dataset-1 Dataset-2 Buschmeier et al. (2014) 0.69 0.16 Gonz\u00e1lez-Ib\u00e1nez et al. (2011) 0.68 0.15 Liebrecht et al. (2013) 0.67 0.17 Joshi et al. (2015) 0.", "startOffset": 29, "endOffset": 158}, {"referenceID": 12, "context": "25 respectively are obtained by using features from Joshi et al. (2015), there is a degradation of 47% on Dataset-2 which contains only numerical tweets.", "startOffset": 52, "endOffset": 72}, {"referenceID": 12, "context": "We see an improvement of 68% in F1-score against the best performing past approach of Joshi et al. (2015).", "startOffset": 86, "endOffset": 106}], "year": 2017, "abstractText": "Sarcasm occurring due to the presence of numerical portions in text has been quoted as an error made by automatic sarcasm detection approaches in the past. We present a first study in detecting sarcasm in numbers, as in the case of the sentence \u2018Love waking up at 4 am\u2019. We analyze the challenges of the problem, and present Rulebased, Machine Learning and Deep Learning approaches to detect sarcasm in numerical portions of text. Our Deep Learning approach outperforms four past works for sarcasm detection and Rule-based and Machine learning approaches on a dataset of tweets, obtaining an F1-score of 0.93. This shows that special attention to text containing numbers may be useful to improve state-of-the-art in sarcasm detection.", "creator": "LaTeX with hyperref package"}}}