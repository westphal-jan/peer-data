{"id": "1709.02448", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Network Vector: Distributed Representations of Networks with Global Context", "abstract": "we propose a neural embedding algorithm called network vector, which learns distributed representations - nodes and the neighbor networks simultaneously. by embedding networks in a low - dimensional space, thy algorithm allows us regularly compare networks in domains of structural similarity and to solve outstanding predictive problems. unlike alternative approaches that focus on computing level features, we learn algorithm continuous global vector that captures each representation's preferred context by maximizing the predictive likelihood of random walk paths in the network. neither algorithm is scalable to real world graphs with many nodes. we evaluate our algorithm on datasets from temporal domains, and compare it with state - of - the - art techniques in node classification, role discovery and tree analogy tasks. the empirical results predict the effectiveness and the efficiency drive our approach.", "histories": [["v1", "Thu, 7 Sep 2017 20:51:27 GMT  (760kb,D)", "http://arxiv.org/abs/1709.02448v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.LG", "authors": ["hao wu", "kristina lerman"], "accepted": false, "id": "1709.02448"}, "pdf": {"name": "1709.02448.pdf", "metadata": {"source": "CRF", "title": "Network Vector: Distributed Representations of Networks with Global Context", "authors": ["Hao Wu", "Kristina Lerman"], "emails": ["hwu732@usc.edu", "lerman@isi.edu"], "sections": [{"heading": "1 Introduction", "text": "Applications in network analysis, including network pattern recognition, classification, role discovery, and anomaly detection, among others, critically depend on the ability to measure similarity between networks or between individual nodes within networks. For example, given an email communications network within an enterprise, one may want to classify individuals according to their functional roles, or given one individual, find another one playing a similar role.\nComputing network similarity requires going beyond comparing networks at a node level to measuring their structural similarity. To characterize network structure, traditional approaches extract features such as node degrees, clustering coefficients, eigenvalues, the lengths of shortest paths and so on [Berlingerio et al., 2012; Henderson et al., 2012; Gilpin et al., 2013]. However, these hand-crafted features are usually heterogeneous and it is often not clear how to integrate them within a learning framework. In addition, some graph features, such as eigenvalues, are computationally expensive and do not scale well in tasks involving large networks. Recent advances in distributed representation of nodes [Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016] in networks created an alternate\nframework for unsupervised feature learning of nodes in networks. These methods are based on the idea of preserving local neighborhoods of nodes with neural network embeddings. An objective is defined on the proximity between nodes in exploring the network neighborhood with various strategies, mainly Depth-First Search (DFS) and Breadth-First Search (BFS). The objective is optimized using single layer neural network for efficient training. However, the embeddings used for feature representation limit scope to the local context of nodes, without directly exploiting the global context of the network. To represent the whole network, these approaches require us to integrate the representations of all nodes, for example, by averaging their representations. However, not all nodes contribute equally to the global representation of the network, and in order to account for their varying importance, aggregation schemes need to weigh nodes, which adds an extra layer of complexity to the learning task.\nTo address above-mentioned challenge we describe a neural network algorithm called Network Vector, which learns distributed representations of networks that account for their global context. The algorithm is scalable to real world networks with large numbers of nodes and can be applied to generic networks such as social networks, knowledge graphs, and citation networks. Networks are compressed into realvalued vectors that preserve the network structure, so that the learned distributed representations can be used to effectively measure network similarity. Specifically, given two networks, even those with different size and topology, the distance between the learned vector representations can be used to measure their structural similarity. In addition, this approach allows us to compare individual nodes by looking at the similarity of their ego-networks, i.e., networks that contain the focal node and all their neighbors and connections between them.\nOur approach is inspired by Paragraph Vector [Le and Mikolov, 2014] that learns distributed representations of texts of variable length such as sentences and documents [Le and Mikolov, 2014]. By exchanging the notions of ordered \u201cword\u201d sequences in sentences and \u201cnodes\u201d in paths along edges on networks. We learn network representations in a similar way of learning representations of sentences and documents. Specifically, we sample sequences of nodes from a network using random walks, same as in [Perozzi et al., 2014; Grover and Leskovec, 2016]. In contrast to existing ap-\nar X\niv :1\n70 9.\n02 44\n8v 1\n[ cs\n.S I]\n7 S\nep 2\n01 7\nproaches, the likelihood of next node in a random walk sequence predicted by our algorithm depends not only on the previous nodes, but also on the global context of the network. The global context vector representation of the network is learned to maximize the average predicted likelihood of nodes in random walk sequences sampled from the network. The learned representations can be used as the signatures of the networks for comparison, or as features for classification and other predictive tasks.\nWe evaluate the algorithm on several real world datasets from a diversity of domains, including citation network of knowledge concepts in Wikipedia, email interaction network, legal citations network, social network of bloggers, proteinprotein interaction network and language network. We focus on predictive tasks including role discovery in networks that aims to identify individual nodes serving similar roles, inference of analogous relations between concept pairs in Wikipedia and multi-label node classification. We compare Network Vector with state-of-the-art feature learning algorithm node2vec [Grover and Leskovec, 2016], LINE [Tang et al., 2015], DeepWalk [Perozzi et al., 2014] and featurebased baselines such as node degrees, clustering coefficients and eigenvalues. Experiments demonstrate the superior performance of Network Vector, due to its capacity of learning the global context of the network.\nIn summary, our contributions are summarized as follows:\n1. We propose Network Vector algorithm, a distributed feature learning algorithm for representing an entire network and its nodes simultaneously. We define an objective function that preserves the local neighborhood of nodes and the global context of the entire network.\n2. We evaluate Network Vector on role discovery, concept analogy and node multi-label classification tasks. Experiments on several benchmark datasets from diverse domains show its effectiveness and efficiency."}, {"heading": "2 Related Work", "text": "Our algorithm builds its foundation on learning distributed representations of concepts [Hinton, 1986] . Distributed representations encode structural relationships between concepts and are typically learned using back-propagation through neural networks. Recent advances in natural language processing have successfully adopted distributed representation learning and introduced a family of neural language models [Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b] to model word sequences in sentences and documents. These approaches embed words such that words in similar contexts tend to have similar representations in latent space.\nBy exchanging the notions of nodes in a network and words in a document, recent research [Perozzi et al., 2014; Tang et al., 2015; Cao et al., 2015; Grover and Leskovec, 2016] attempt to learn node representations in a network in a similar way of learning word embeddings in neural language models. Our work follows this line of approaches in which nodes in a neighborhood will have similar embeddings in vector space. Different node sampling strategies are explored\nfor characterizing the neighborhood structure. For example, DeepWalk [Perozzi et al., 2014] samples node sequences from a network using a stream of short first-order random walks, and model them just like word sequences in documents using neural embeddings. LINE [Tang et al., 2015] samples nodes in pairwise manner and model the first-order and second-order proximity between them. GrapRep [Cao et al., 2015] extends LINE to exploit structural information beyond second-order proximity. To offer a flexible node sampling scheme, node2vec [Grover and Leskovec, 2016] utilizes second-order random walks, and combines Depth-First Search (DFS) and Breadth-First Search (BFS) strategies to explore the local neighborhood structure.\nHowever, existing approaches only consider the local network structures (i.e., the neighborhoods of nodes) in learning node embeddings, but exploit little information of the global structure of the network. Although recent approach GrapRep [Cao et al., 2015] attempts to capture long distance relationship between two different nodes, it limits scope to a fixed number of hops. More importantly, existing approaches focus on node representations, and it requires additional effort to compute the representation of the entire network. The simple scheme of averaging the representations of all nodes to represent the network is by no means a good choice as it ignores the statistics of node frequency and their roles in the network. In contrast, we introduce a notion called the global network vector, which aims to represent the structural properties of an entire network. The global vector representation of the network acts as a memory which is asked to contribute to the prediction of a node accompanying with the node\u2019s neighbors, and updated to maximize the predictive likelihood. As a result, our algorithm can simultaneously learn the global representation of a network and the representations of nodes in the network. This is inspired by Paragraph Vector [Le and Mikolov, 2014], which learns a continuous vector to represent a piece of text with variable-length, such as sentences, paragraphs and documents."}, {"heading": "3 Network Vector", "text": "We consider the problem of embedding nodes of a network and the entire network into a low-dimensional vector space. Let G = {V,E} denote a graph, and V is the set of vertices and E = V \u00d7 V is the set of edges with weights W . The goal of our approach is to map the entire graph to a low-dimensional vector, represented by vG \u2208 Rd, and map each node i to a unique vector vi \u2208 Rd in the same vector space. Although the dimensionality of network representation vG can be different from that of node representations vi in theory, we adopt the same dimensionality d for the ease of computation in real world applications. Suppose that there are M graphs given (e.g., ego-networks of M persons of interest in a social network) and N distinct nodes in the corpus, then there are (M +N)\u00d7 d parameters to be learned."}, {"heading": "3.1 A Neural Architecture", "text": "Our approach of modeling networks is motivated by learning distributed representations of variable-length texts, e.g., sentences and documents [Le and Mikolov, 2014]. The concept of \u201cwords\u201d in a document [Le and Mikolov, 2014] is\nreplaced by \u201cnodes\u201d in a network in our modeling. The goal is to predict a node given other nodes in its local context as well as the global context of the network. Text has a linear property that the local context of a word can be naturally defined by surrounding words in ordered sequences. However, networks are not linear. In order to characterize the local context of a node, without loss of generality, we sample node sequences from the given network with second-order random walks in [Grover and Leskovec, 2016], which offer a flexible notion of a node\u2019s local neighborhood by combining Depth-First Search (DFS) and Breadth-First Search (BFS) strategies. Our learning framework can easily adopt higherorder random walks, but with higher computation cost. Each random walk starts from an arbitrary root node and generates an ordered sequence of nodes with second-order Markov chains. Specifically, consider node va that has been visited in the previous step, and the random walk currently reaches node vb. Consecutively, the next node vc will be sampled in random walks, with probability:\nP (vc|va, vb) = 1\nZ MabcWbc (1)\nwhere Mabc is the unweighted transition probability of moving from node vb to vc given va, Wbc is the weight of edge (vb, vc), and Z is the normalization term. We define Mabc as:\nMabc =  1 p if d(va, vc) = 0 1 if d(va, vc) = 1 1 q if d(va, vc) = 2\n(2)\nwhere d(va, vc) is the shortest path distance between va and vc. The parameters p and q control how the random walk biases toward visited nodes in previous step and nodes that are further away. The random walk terminates when l vertices are sampled, and the procedure repeats r times for each root node.\nFigure 1 illustrates a neural network architecture for learning the global network vector and node vectors simultaneously. A sliding window (v1, \u00b7 \u00b7 \u00b7 , vn) with fixed-length n\nis repeatedly sampled over node sequences. The algorithm predicts the target node vn given preceding nodes v1:n\u22121 as local context and the entire network G as the global context, with probability P (vn|v1:n\u22121, G). Formally, the probability distribution of a target node is defined as:\nP (vn|v1:n\u22121;G) = 1\nZc exp[\u2212E(G, v1:n\u22121; vn)] (3) where Zc = \u2211\nvm\u2208V exp[\u2212E(G, v1:n\u22121; vm)] is the normalization term. We extend the scalable version of Log-Bilinear model [Mnih and Hinton, 2007], called vector Log-Bilinear model (vLBL) [Mnih and Kavukcuoglu, 2013]. In our model, the energy function E(G, v1:n\u22121; vn) is specified as:\nE(G, v1:n\u22121; vn) = \u2212v\u0302>vn (4) where v\u0302 is the predicted representation of the target node:\nv\u0302 = vG + ( n\u22121\u2211 i=1 ci vi ) (5)\nHere denotes the Hadamard (element-wise) product, and ci is the weight vector for the context node in position i. ci parameterizes the context nodes at different hops away from the target node in random walks. The global network vector vG is shared across all sliding windows of node sequences. After being trained, the global network vector preserves the structural information of the network, and can be used as feature input for the network. In our model, in order to impose symmetry in feature space of nodes, and activate more interactions between the feature vector vG and the node vectors, we use the same set of feature vectors for both the target nodes and the context nodes. This is different from [Mnih and Kavukcuoglu, 2013], where two separated sets of representations are used for the target node and the context nodes respectively. In practice, we find our approach improves the performance of Network Vector."}, {"heading": "3.2 Learning with Negative Sampling", "text": "The global network vector vG, the node vectors vi and the position-dependent context parameters ci are initialized with random values, and optimized by maximizing the objective in Eq. (3). Stochastic gradient ascent is performed to update the set of parameters \u03b8 = {vG,vi, ci}:\n\u2206\u03b8 = \u2207\u03b8 logP (vn|v1:n\u22121, G)\n= \u2202\n\u2202\u03b8\n[ exp(v\u0302>vn)\u2211N\nvm=1 exp(v\u0302>vm)\n] (6)\nwhere is the learning rate. The computation involves the normalization term and is proportional to the number of distinct nodes N . The complexity of computation is expensive and impractical in real applications. In our approach, we adopt negative sampling [Mikolov et al., 2013b] for optimization. Negative sampling represents a simplified version of noise contrastive estimation [Mnih and Teh, 2012], and trains a logistic regression to distinguish between data samples of vn from \u201cnoise\u201d distribution. Our objective is to maximize\nlog \u03c3(v\u0302>vn) + k\u2211 m=1 Evm \u223c Pn(v) [ log \u03c3(\u2212v\u0302>vm) ] (7)\nwhere \u03c3(x) = 1/(1 + exp(\u2212x)) is the sigmoid function. Pn(v) is the global unigram distribution of the training data acting as the noise distribution where we draw k negative samples of nodes. Negative sampling allows us to train our model efficiently that no longer requires explicitly normalized in Eq. (6), and hence are more scalable."}, {"heading": "3.3 An Inverse Architecture", "text": "The architecture in Figure 1 utilizes the linear combination of the global network vector and the context node vectors to predict the target node in a sliding window. Another way of training the global network vector is to model the likelihood of observing a sampled node vt from the sliding window conditioned on the feature vector vG, given by\nP (vt|G) = 1\nZG exp[\u2212E(G; vt)] (8)\nwhere ZG is the normalization term specific to the feature representation of G. The energy function E(G; vt) is:\nE(G; vt) = \u2212v>Gvt (9)\nThis architecture is a counterpart of the Distributed Bag-ofWords version of Paragraph Vector [Le and Mikolov, 2014]. However, this architecture ignores the order of the nodes in the sliding window and perform poorly in practice when it is used alone. We extend the framework by simultaneously training network and node vectors using a Skipgram [Mikolov et al., 2013a; Mikolov et al., 2013b] like model. The model additionally maximizes the likelihood of observing the local context vt\u2212n:t+n (excluding vt) for the target node vt, conditioned on the feature representation of vt. Unfortunately, modeling the joint distribution of a set of context nodes is not tractable. This problem can be relaxed by assuming the node in different context positions are conditionally independent given the target word:\nP (vt\u2212n:t+n|vt) = t+n\u220f\ni=t\u2212n P (vi|vt) (10)\nwhere P (vi|vt) = 1Zt exp[\u2212E(vt; vi)]. The energy function is:\nE(vt; vi) = v > t (ci vi) (11)\nThe objective is to maximize the log-likelihood of the product of the probabilities, P (vt|G) and P (vt\u2212n:t+n|vt)"}, {"heading": "3.4 Complexity Analysis", "text": "The computation of Network Vector consists of two key parts: sampling of node sequences with random walks and optimization of vectors. For each node sequence of fixed length l, we start from a randomly chosen root node. At each step, the walk visits a new node based on the transition probabilities P (vc|va, vb) in Eq. (1). The transition probabilities P (vc|va, vb) can be precomputed and stored in memory usingO(|E|2/|V |) space. Sampling a new node in the walk can be efficiently done inO(1) time using alias sampling [Walker, 1977]. The overall time complexity is O(r|V |l) for repeating r times of random walks of fixed length l by taking each node as root.\nThe time complexity of optimization with negative sampling in Eq. (7) is proportional to the dimensionality of vectors d, the length of context window n and the number of negative samples k. It takes O(dnk) time for nodes within the sliding window (v1, \u00b7 \u00b7 \u00b7 , vn). The introduced global vector vG requires O(dk) time to optimize, same as any other node vectors in the sliding window. Given r random walks of fixed length l starting from every node, the overall time complexity is O(dnkr|V |l). To store the node vectors and the global network vector, it requires O(d|V |+ d) space."}, {"heading": "3.5 The Property of Network Vector", "text": "The property of the global network vector vG in the architecture (as shown in Figure 1) can be explained by looking at the objective in Eq. (3). vG is part of the input to the neural network, and can be viewed as a term that helps to represent the distribution of the target node vn. The relevant part v>Gvn is related logarithmically to the probability P (vn|v1:n\u22121;G). Therefore, the more frequently a particular vn is observed in the data, the larger the value v>Gvn will have, and hence vG will be closer to vn in vector space. The training objective is to maximize the logarithm of the product of all probabilities P (vn|v1:n\u22121;G), and the value is related to v>Gv\u0304n, where v\u0304n is the expected vector that can be obtained by averaging all observed vn in the data. It is also true for Eq. (8) in the inverse architecture where the global network vector vG is the only input to the neural network, in order to predict every node vt."}, {"heading": "Karate Network", "text": "As an illustrative example, we apply Network Vector to the classic Karate network [Zachary, 1977]. The nodes in the network represent members in a karate club, and the edges are social links between the members outside the club. There are 34 nodes and 78 undirected edges in total. We use the inverse architecture to train the vectors. Figure 2 shows the output of\nour method in two dimensional space. We use green circles to denote nodes and orange circle to denote the entire graph. The size of a node is proportional to its degree in the graph. We can see that the learned global network vector is close to these high-degree nodes, such as node 1 and 34, which serve as the hubs of two splits of the club. The resulting global vector mostly represent the backbone nodes (e.g., hubs) in the network and compensates the lack of global information in local neighborhoods."}, {"heading": "Legal Citation Networks", "text": "Given a citation network of documents, for example, scientific papers or legal opinions, we want to identify similar documents. These could be groundbreaking works that serve to open new fields of discourse in science and law, or foundational works that span disciplines but have less of an impact on discourse, such as \u201cmethods\u201d papers in science.\nFor the purpose of case study, we collected a large digitized record of federal court opinions from the CourtListener project1. The most cited legal decisions from the United States Supreme Court are selected and ego-networks of citations are constructed for these legal cases. Two distinct graph patterns are observed. One is \u201cCitations have a few giant hubs\u201d and \u201cCitations are well connected\u201d. We list a few examples in Table 1, where the titles of the cases with different citation patterns are colored as red and blue, respectively. The ego-networks of the first six cases listed in Table 1 have just a few giant hubs which are linked by many other cases. For example, the case \u201cBuckley v. Valeo, 424 U.S. 1 (1976)\u201d is a landmark decision in American campaign finance law. The case \u201cColeman v. Miller, 307 U.S. 433 (1939)\u201d is a landmark decision centered on the Child Labor Amendment, which was proposed for ratification by Congress in 1924. These cases are generally centered on a specific topic, and their citations may have a narrowed topic. There are only a few hubs cited frequently by others and the citations generally do not cite each other. On the other side, the ego-networks of the last six cases listed in Table 1 have citations that are well connected. For example, \u201cYick Wo v. Hopkins, 118 U.S. 356 (1886) \u201d was the first case where the United States Supreme Court ruled that a law that is race-neutral on its face, but is administered in a prejudicial manner; The case \u201cStromberg v. California, 283 U.S. 359 (1931)\u201d is a landmark in the history of First Amendment constitutional law to include a protection of the substance of the First Amendment. These cases are influential in the history and cited by many diverse subsequent legal decisions, which usually cite each other.\nOur Network Vector algorithm is used to learn twodimensional embeddings from the ego-networks of the legal cases, and their projections are shown as open dots in the right figure of Table 1. The structures of the ego-networks for four sampled Supreme Court legal cases (Case IDs: 110578, 93272,101957,105547) are also illustrated in the figure. Note that the ego network includes the case itself (does not show in the figure), and all the cases it cites (smaller circle on the right) as well as all the cases that cite it (larger circle on the left). Lines represent citations among these cases. The two groups of ego-networks contrast each other. Compared to the\n1https://www.courtlistener.com/\nego-networks in red boxes, which is cited by unrelated legal cases, there is clearly more coherence in discourse related to the cases in blue boxes, as indicated by citations among other Supreme Court cases that cite this one. Although the differences between these two ego-networks could be captured in a standard way, by features related to the degree distribution of the ego-networks, or their clustering coefficients, the distinctions between other ego-networks may be more subtle necessitating a new approach for evaluating their similarity. In this representation, the position of the case in the learned space captures the similarity of the structure of their ego-networks. Cases that are more similar to the ego-networks in red boxes fall in the top half of the 2-D plane (red open dots); while cases similar to those in blue boxes fall in the bottom half (blue open dots). Thus, distances between the learned representations of the ego-networks of legal cases can be used to quantitatively capture their similarity."}, {"heading": "4 Experiments", "text": "Network Vector learns representations of network nodes and the entire network simultaneously. We evaluate both representations on predictive tasks. First, we apply Network Vector to a setting where only local information about nodes, such as their immediate neighbors, is available. We learn representations for ego-networks of a few nodes using Network Vector and evaluate on role discovery in social networks and concept analogy in encyclopedia. Second, when the information of node connectivities in the entire network is available, we may learn node representations using Network Vector, where the additional global vector for the network is used to help in learning high-quality node representations. The resulting node representations are evaluated on multi-label classification."}, {"heading": "4.1 Role Discovery", "text": "Roles reflect individuals\u2019 functions within social networks. For example, email communication network within an enterprise reflects employees\u2019 responsibilities and organizational hierarchies. An engineer\u2019s interactions with her team are different from those of a senior manager\u2019s. In the Wikipedia network, each article cites other concepts that explain the meaning of the article\u2019s concept. Some concepts may \u201cbridge\u201d the network by connecting different concept categories. For example, the concept Bat belongs to the category Mammals, however since a bat resembles a bird, it refers to many similar articles about the category Birds."}, {"heading": "Datasets", "text": "We use the following datasets in the evaluation:\n\u2022 Enron Email Network: It contains email interaction data from about 150 users, mostly senior management of Enron. There are about half million emails communicated by 85,601 distinct email addresses2. We have 362,467 links left after removing duplicates and self links. Each of the email addresses belonging to Enron employees has one of 9 different positions: CEO, President, Vice\n2http://www.cs.cmu.edu/\u02dcenron/\nPresident, Director, Managing Director, Manager, Employee, In House Lawyer and Trader. We use the positions as roles. This categorization is fine-grained. In order to understand how the feature representations can reflect the properties of different stratum in the corporation, we also use coarse-grained labels Leader (aggregates CEO, President, Vice President), Manager (aggregates Director, Managing Director, Manager) and Employee (includes Employee, In House Lawyer and Trader) to divide the users into 3 roles.\n\u2022 Wikipedia for Schools Network: We use a subset of articles available at Wikipedia for Schools3. This datasetcontains 4,604 articles and 119,882 links between them. The articles are categorized by subjects. For example, the article about Cat is categorized as subject.Science.Biology.Mammals. We use one of 15 second-level category names (e.g., Science in the case of Cat) as the role label."}, {"heading": "Methods for Comparison", "text": "For real-world networks, such as email, information about all connectivities of nodes may not be fully available, e.g., for privacy reasons. For this reason, we explore prediction task with local information (i.e., immediate neighbors). For each node, we first generate its ego-network, which represents the induced subgraph of its immediate neighbors, and learn global vector representations for the set of ego-networks through Network Vector. We use the architecture as in Eq. (3). In our experiments, we repeat \u03b3 = 10 times for root node initialization in random walks and the length of each random walks is fixed as l = 80. For comparison, we evalu-\n3http://schools-wikipedia.org/\nate the performance of Network Vector against the following network feature-based algorithms [Berlingerio et al., 2012]:\n\u2022 Degrees: number of nodes and edges, average node degree, maximum \u201cin\u201d and \u201cout\u201d node degrees. The degree features are aggregated to form the representations of the ego-networks.\n\u2022 Clustering Coefficients: measure the degree to which nodes tend to cluster. We compute global clustering coefficient and average clustering coefficient of nodes for representing each ego-network.\n\u2022 Eigens: For each ego-network, we compute 10 largest eigenvalues of its adjacency matrix.\n\u2022 node2vec [Grover and Leskovec, 2016]: This approach learns low-dimensional feature representations of nodes in a network by interpolating between BFS and DFS for sampling node sequences. A parameter p and q is introduced to control the likelihood of revisiting a node in walks, and to dis/encourage outward exploration, resulting in BFS/DFS like sampling strategy. It\u2019s interesting to note when p = 1 and q = 1, node2vec boils down to DeepWalk [Perozzi et al., 2014], which utilizes uniform random walks. We adapt node2vec, and use the mean of learned node vectors to represent each ego-network."}, {"heading": "Results", "text": "Given a node\u2019s ego-network, we rank other nodes\u2019 egonetworks by their distance to it in vector space of feature representations. Table 2 shows the average precision of retrieved nodes with the same roles (class labels) at cut-off k = 1, 5, 10. For simplicity, Cosine similarity is used to compute the distance between two nodes. From the result, we can see how the global context allows Network Vector outperform"}, {"heading": "Method Hit@1 Hit@5 Hit@10", "text": "node2vec in role discovery. However, the performance gain is dependent on different datasets. We observe Network Vector performs slightly better than node2vec on Enron email interaction network, while the improvement of performance is over 150% on Wikipedia network. Compared to the combination of Degrees, Clustering Coefficients and Eigenvalues, the improvement of the two learning algorithms Network Vector and node2vec are outstanding, with over 100% performance gain in all cases."}, {"heading": "4.2 Concept Analogy", "text": "We also evaluate the feature representations of ego-networks on the analogy task. For Wikipedia network, we follow the word analogy task defined in [Mikolov et al., 2013a]. Given a pair of Wikipedia articles describing two concepts (a, b), and an article describing another concept c. The task aims to find a concept d such that a is to b as c is to d. For example, Europe is to euro as USA is to dollar. This analogy task can be solved by finding the concept that is closest to vb\u2212va+vc in vector space, where the distance is computed using Cosine similarity.\nThere are 1,632 semantic tuples in Wikipedia network matched for the semantic pairs in [Mikolov et al., 2013a]. We use them as evaluation benchmark. Table 3 shows the accuracy of hitting the answer d within cut-off k = 1, 5, 10 positions in the ranking list. From the results, we can see Network Vector performs much better than the baseline, which use degree, clustering coefficients and eigenvalues of the adjacency matrix. The combination of heterogeneous features (degrees, clustering coefficients and eigenvalues) in different scale causes the difficulty to utilize an efficient distance metric. However, Network Vector does not suffer from this problem by automating the feature learning using an objective function. In this task, we empirically fix the dimensionality of vectors as 100 and context window as 10."}, {"heading": "4.3 Multi-label Classification", "text": "Multi-label classification is a challenge task, where each node may have one or multiple labels. A classifier is trained to predict multiple possible labels for each test node. In our Network Vector algorithm, the global representation of entire network serves as additional context along with local neighborhood in learning node representations."}, {"heading": "Datasets", "text": "To understand whether the global representation helps learning better node representation, we perform multi-label classification with the same benchmarks and experimental procedure as [Grover and Leskovec, 2016] using the same datasets:\n\u2022 BlogCatalog [Zafarani and Liu, 2009; Tang and Liu, 2009]: This is a network of social relationships provided by bloggers on the BlogCatalog website. The labels represent the interests of bloggers on a list of topic categories. There are 10,312 nodes, 333,983 edges in the network and 39 distinct labels for nodes. \u2022 Protein-Protein Interactions (PPI) [Breitkreutz et al.,\n2008; Grover and Leskovec, 2016]: This is a subgraph of the entire PPI network for Homo Sapiens. The node labels are obtained from hallmark gene sets [Liberzon et al., 2011] and represent biological states. There are 3,890 nodes, 76,584 edges in the network and 50 distinct labels for nodes.\n\u2022 Wikipedia Cooccurrences [Mahoney, 2009; Grover and Leskovec, 2016]: This is a network of words appearing in the first million bytes of the Wikipedia dump. The edge weight is defined by the cooccurrence of two words within a 2-length slide window. The Part-of-Speech (POS) tags [Marcus et al., 1993] inferred using the Stanford POS-Tagger [Toutanova et al., 2003] are used as labels. There are 4,777 nodes, 184,812 edges in the network and 40 distinct labels for nodes."}, {"heading": "Methods for Comparison", "text": "We compare the node representations learned by Network Vector against the following feature learning methods for node representations: \u2022 Spectral clustering [Tang and Liu, 2011]: This method\nlearns the d-smallest eigenvectors of the normalized graph Laplacian matrix, and utilize them as the ddimensional feature representations for nodes.\n\u2022 DeepWalk [Perozzi et al., 2014]: This method learns d-dimensional feature representations using Skipgram [Mikolov et al., 2013a; Mikolov et al., 2013b] from node sequences, that are generated by uniform random walks from the source nodes on graph.\n\u2022 LINE [Tang et al., 2015]: This method learns ddimensional feature representations by sampling nodes at 1-hop and 2-hop distance from the source nodes in BFS-like manner.\n\u2022 node2vec [Grover and Leskovec, 2016]: We use the original node2vec algorithm with optimal parameter settings of (p, q) reported in [Grover and Leskovec, 2016].\nNetwork Vector utilizes only first-order or second-order proximity between nodes in two-layer neural embedding framework. The first layer computes the context feature vector, and the second layer computes the probability distribution of target nodes. It is similar to other neural embedding based feature learning methods DeepWalk, LINE and node2vec. For fair comparison, we exclude recent approaches GraRep [Cao et al., 2015], HNE [Chang et al., 2015] and SDNE [Wang et al., 2016]. It is because GraRep utilizes information from network neighborhoods beyond second-order proximity, and both HNE and SDNE employ deep neural networks that have multiple layers (more than two). GraRep, HNE and SDNE are less computational efficient and cannot scale well, as compared to DeepWalk, LINE, node2vec\nand our algorithm Network Vector. For fair comparison, we use the inverse architecture of Network Vector, which is Skip-gram [Mikolov et al., 2013a] like and similar to that of node2vec. The parameter settings for Network Vector are in favor of node2vec, and exactly the same as in [Grover and Leskovec, 2016]. Specifically, we set d = 128, r = 10, l = 80, and a context size n = 10, and are aligned with typical values used for DeepWalk and LINE. A single pass of the data (one epoch) is used for optimization. In order to perform multi-label classification, the learned node representations from each approach are used as feature input to a one-vs-rest logistic regression with L2 regularization. Our experiments are repeated for 10 random equal splits of train and test data, and average results are reported."}, {"heading": "Results", "text": "Macro-F1 scores are used as evaluation metrics, and Table 4 shows the results. We run Network Vector with node sequences generated by biased random walks from node2vec. The default parameter setting (p = 1, q = 1) used in DeepWalk and the optimal parameter setting of node2vec reported in [Grover and Leskovec, 2016] are used.\nFrom the results, we can see Network Vector outperforms node2vec using the same biased random walks, and DeepWalk using the same uniform random walks. It is evident that the global representation of the entire network allows Network Vector to exploit the global structure of the networks to learn better node representations. Network Vector achieves a slight performance gain, 1.0% over node2vec, and a significant 12.4% gain over DeepWalk on BlogCatalog. As we can see on PPI, The gain of Network Vector over node2vec and DeepWalk are significant and similar, 9.6% and 9.7% respectively. In the case of Wikipedia word cooccurrence network, Network Vector outperforms node2vec with a decent margin, achieving 13.7% performance gain, while with a less gain, 8.9% over DeepWalk. Overall, sampling strategies even with optimal parameter settings (p, q) in node2vec are limited in exploration of local neighborhood of the source nodes, but cannot exploit the global network structure well. Network Vector overcomes the limitation of locality. By utilizing an additional global vector to memorize the collective information from all the local neighborhoods of nodes even within 2- hops, Network Vector learns improved node representations."}, {"heading": "Parameter Sensitivity", "text": "In order to understand how Network Vector improves in learning node representations with biased random walks in fine-grained settings, we evaluate performance while varying the parameter settings of (p, q). We fix p = \u221e to discourage revisiting sampled nodes at the previous step in random walks, and varying the value q in the range from 2\u22124 to 24 to perform DFS-like sampling in various degrees.\nFigure 3 shows the comparison results for Network Vector and ndoe2vec in both Macro-F1 and Micro-F1 scores. As we can see, Network Vector consistently outperforms node2vec in different parameter settings of q in all the three datasets. However, we observe on BlogCatalog, Network Vector achieves relatively larger gains over node2vec when q is large that the random walks is biased towards BFS-like sampling, as compared to that when q is small that the sampling is more DFS-like. It is mainly because when the random walks is biased towards nodes close to the source nodes, the global information of network structure that are exploited by Network Vector can compensate more for locality information using BFS-like sampling. However, when q is small, the random walks is biased towards sampling nodes far away from the source nodes, and explore information close to the global network structure. Hence, Network Vector is not quite\nhelpful in this case. We can see similar patterns of performance margin between Network Vector and node2vec when q tends to be large in word cooccurrence network of Wikipedia. However, in the case of PPI, the performance gains achieved by Network Vector over node2vec are stable even various values of q are used. The reason is probably because the biological states of proteins in a protein-protein interaction network exhibit a high degree of homophily, since proteins in local neighborhood usually organize together to perform similar functions. Hence, the global network structure is not quite informative to predict the biological states of proteins as we set a large value of q."}, {"heading": "Effect of Training Data", "text": "To see the effect of training data, we compare performance while varying the fraction of labeled data from 10% to 90%. Figure 4 shows the results on PPI. The parameters (p, q) is fixed using optimal values (4, 1). As we can see, when using more labeled data, the performance of node2vec and Network Vector generally increases. Network Vector achieves the largest gain over node2vec of 9.0% in Macro-F1 score and 10.3% at 40% labeled data. When only 10% labeled data is used, Network Vector only yields 1.5% gain in Macro-F1 score, and 7.1% in Micro-F1 score. We have similar observations on BlogCatalog and Wikipedia datasets, and the results are not shown."}, {"heading": "5 Conclusion", "text": "We have presented Network Vector, an algorithm for learning distributed representations of nodes and networks simultaneously. By embedding the network in a lower-dimensional vector space, our algorithm allows for quantitative comparison of networks. It also allows for the comparison of individual network nodes, since each node can be represented by its ego-network\u2014a network containing the node itself, its network neighbors, and all connections between them.\nIn contrast to existing network embedding methods, which only learn representations of component nodes, Network Vector directly learns the representation of an entire network. Learning a representation of a network allows us to evaluate the similarity between two networks or two individual nodes, which enables us to answer questions that were difficult to address with existing methods. For instance, given a node in a network, for example, a manager within an organization, we can identify other people serving a similar role within that organization. Also, given a connection, denoting some relation-\nship between two people within a social network, we could find another pair in an analogous relationship. Beyond social networks, we can also answer new questions about knowledge networks that connect concepts or documents to each others, for example, Wikipedia and citations networks. This can be useful especially in cases where the contents of documents is not available for privacy or other reasons, but the network of interactions exists.\nFor the networks in which content is available for the nodes, the learning method could be extended to account for it. For example, for knowledge networks, the approach could be combined with text to learn representations of networks that will give a more fine-grained view of their similarity. Additionally, other non-textual attributes could also be included in the learning algorithm. The flexibility of such learning algorithms make them ideal candidates for applications requiring similarity comparison of different types of objects."}], "references": [{"title": "Journal of Machine Learning Research", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin. A neural probabilistic language model"], "venue": "3:1137\u20131155,", "citeRegEx": "Bengio et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Netsimile: a scalable approach to size-independent network similarity", "author": ["Michele Berlingerio", "Danai Koutra", "Tina Eliassi-Rad", "Christos Faloutsos"], "venue": "arXiv preprint arXiv:1209.2684,", "citeRegEx": "Berlingerio et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "The biogrid interaction database: 2008 update", "author": ["Breitkreutz et al", "2008] Bobby-Joe Breitkreutz", "Chris Stark", "Teresa Reguly", "Lorrie Boucher", "Ashton Breitkreutz", "Michael Livstone", "Rose Oughtred", "Daniel H Lackner", "J\u00fcrg B\u00e4hler", "Valerie Wood"], "venue": "Nucleic acids research,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Grarep: Learning graph representations with global structural information", "author": ["Shaosheng Cao", "Wei Lu", "Qiongkai Xu"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 891\u2013900. ACM,", "citeRegEx": "Cao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Heterogeneous network embedding via deep architectures", "author": ["Chang et al", "2015] Shiyu Chang", "Wei Han", "Jiliang Tang", "Guo-Jun Qi", "Charu C Aggarwal", "Thomas S Huang"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Guided learning for role discovery (glrd): framework, algorithms, and applications", "author": ["Gilpin et al", "2013] Sean Gilpin", "Tina Eliassi-Rad", "Ian Davidson"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "node2vec: Scalable feature learning for networks", "author": ["Aditya Grover", "Jure Leskovec"], "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 855\u2013864,", "citeRegEx": "Grover and Leskovec. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Rolx: structural role extraction & mining in large graphs", "author": ["Henderson et al", "2012] Keith Henderson", "Brian Gallagher", "Tina Eliassi-Rad", "Hanghang Tong", "Sugato Basu", "Leman Akoglu", "Danai Koutra", "Christos Faloutsos", "Lei Li"], "venue": "In Proceedings of the 18th ACM SIGKDD international", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "volume 1", "author": ["Geoffrey E Hinton. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society"], "venue": "page 12. Amherst, MA,", "citeRegEx": "Hinton. 1986", "shortCiteRegEx": null, "year": 1986}, {"title": "In Proceedings of the 31st International Conference on Machine Learning (ICML-14)", "author": ["Quoc Le", "Tomas Mikolov. Distributed representations of sentences", "documents"], "venue": "pages 1188\u20131196,", "citeRegEx": "Le and Mikolov. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Jill P Mesirov", "author": ["Arthur Liberzon", "Aravind Subramanian", "Reid Pinchback", "Helga Thorvaldsd\u00f3ttir", "Pablo Tamayo"], "venue": "Molecular signatures database (msigdb) 3.0. Bioinformatics, 27(12):1739\u20131740,", "citeRegEx": "Liberzon et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "URL: http://www", "author": ["Matt Mahoney. Large text compression benchmark"], "venue": "mattmahoney. net/text/text. html,", "citeRegEx": "Mahoney. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics, 19(2):313\u2013330,", "citeRegEx": "Marcus et al.. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Mikolov et al", "2010] Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al.. 2013a", "shortCiteRegEx": null, "year": 2013}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111\u20133119,", "citeRegEx": "Mikolov et al.. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 24th international conference on Machine learning", "author": ["Andriy Mnih", "Geoffrey Hinton. Three new graphical models for statistical language modelling"], "venue": "pages 641\u2013648. ACM,", "citeRegEx": "Mnih and Hinton. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning word embeddings efficiently with", "author": ["Mnih", "Kavukcuoglu", "2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh"], "venue": "arXiv preprint arXiv:1206.6426,", "citeRegEx": "Mnih and Teh. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Deepwalk: Online learning of social representations", "author": ["Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701\u2013710. ACM,", "citeRegEx": "Perozzi et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining", "author": ["Lei Tang", "Huan Liu. Relational learning via latent social dimensions"], "venue": "pages 817\u2013826. ACM,", "citeRegEx": "Tang and Liu. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Data Mining and Knowledge Discovery", "author": ["Lei Tang", "Huan Liu. Leveraging social media networks for classification"], "venue": "23(3):447\u2013478,", "citeRegEx": "Tang and Liu. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Line: Largescale information network embedding", "author": ["Tang et al", "2015] Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei"], "venue": "In Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Featurerich part-of-speech tagging with a cyclic dependency network", "author": ["Toutanova et al", "2003] Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "ACM Transactions on Mathematical Software (TOMS)", "author": ["Alastair J Walker. An efficient method for generating discrete random variables with general distributions"], "venue": "3(3):253\u2013256,", "citeRegEx": "Walker. 1977", "shortCiteRegEx": null, "year": 1977}, {"title": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "author": ["Daixin Wang", "Peng Cui", "Wenwu Zhu. Structural deep network embedding"], "venue": "pages 1225\u2013 1234. ACM,", "citeRegEx": "Wang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Journal of anthropological research", "author": ["Wayne W Zachary. An information flow model for conflict", "fission in small groups"], "venue": "33(4):452\u2013473,", "citeRegEx": "Zachary. 1977", "shortCiteRegEx": null, "year": 1977}, {"title": "Social computing data repository at asu", "author": ["Zafarani", "Liu", "2009] Reza Zafarani", "Huan Liu"], "venue": null, "citeRegEx": "Zafarani et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zafarani et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "To characterize network structure, traditional approaches extract features such as node degrees, clustering coefficients, eigenvalues, the lengths of shortest paths and so on [Berlingerio et al., 2012; Henderson et al., 2012; Gilpin et al., 2013].", "startOffset": 175, "endOffset": 246}, {"referenceID": 19, "context": "Recent advances in distributed representation of nodes [Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016] in networks created an alternate framework for unsupervised feature learning of nodes in networks.", "startOffset": 55, "endOffset": 123}, {"referenceID": 6, "context": "Recent advances in distributed representation of nodes [Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016] in networks created an alternate framework for unsupervised feature learning of nodes in networks.", "startOffset": 55, "endOffset": 123}, {"referenceID": 9, "context": "Our approach is inspired by Paragraph Vector [Le and Mikolov, 2014] that learns distributed representations of texts of variable length such as sentences and documents [Le and Mikolov, 2014].", "startOffset": 45, "endOffset": 67}, {"referenceID": 9, "context": "Our approach is inspired by Paragraph Vector [Le and Mikolov, 2014] that learns distributed representations of texts of variable length such as sentences and documents [Le and Mikolov, 2014].", "startOffset": 168, "endOffset": 190}, {"referenceID": 19, "context": "Specifically, we sample sequences of nodes from a network using random walks, same as in [Perozzi et al., 2014; Grover and Leskovec, 2016].", "startOffset": 89, "endOffset": 138}, {"referenceID": 6, "context": "Specifically, we sample sequences of nodes from a network using random walks, same as in [Perozzi et al., 2014; Grover and Leskovec, 2016].", "startOffset": 89, "endOffset": 138}, {"referenceID": 6, "context": "We compare Network Vector with state-of-the-art feature learning algorithm node2vec [Grover and Leskovec, 2016], LINE [Tang et al.", "startOffset": 84, "endOffset": 111}, {"referenceID": 19, "context": ", 2015], DeepWalk [Perozzi et al., 2014] and featurebased baselines such as node degrees, clustering coefficients and eigenvalues.", "startOffset": 18, "endOffset": 40}, {"referenceID": 8, "context": "Our algorithm builds its foundation on learning distributed representations of concepts [Hinton, 1986] .", "startOffset": 88, "endOffset": 102}, {"referenceID": 0, "context": "Recent advances in natural language processing have successfully adopted distributed representation learning and introduced a family of neural language models [Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b] to model word sequences in sentences and documents.", "startOffset": 159, "endOffset": 271}, {"referenceID": 16, "context": "Recent advances in natural language processing have successfully adopted distributed representation learning and introduced a family of neural language models [Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b] to model word sequences in sentences and documents.", "startOffset": 159, "endOffset": 271}, {"referenceID": 14, "context": "Recent advances in natural language processing have successfully adopted distributed representation learning and introduced a family of neural language models [Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b] to model word sequences in sentences and documents.", "startOffset": 159, "endOffset": 271}, {"referenceID": 15, "context": "Recent advances in natural language processing have successfully adopted distributed representation learning and introduced a family of neural language models [Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b] to model word sequences in sentences and documents.", "startOffset": 159, "endOffset": 271}, {"referenceID": 19, "context": "By exchanging the notions of nodes in a network and words in a document, recent research [Perozzi et al., 2014; Tang et al., 2015; Cao et al., 2015; Grover and Leskovec, 2016] attempt to learn node representations in a network in a similar way of learning word embeddings in neural language models.", "startOffset": 89, "endOffset": 175}, {"referenceID": 3, "context": "By exchanging the notions of nodes in a network and words in a document, recent research [Perozzi et al., 2014; Tang et al., 2015; Cao et al., 2015; Grover and Leskovec, 2016] attempt to learn node representations in a network in a similar way of learning word embeddings in neural language models.", "startOffset": 89, "endOffset": 175}, {"referenceID": 6, "context": "By exchanging the notions of nodes in a network and words in a document, recent research [Perozzi et al., 2014; Tang et al., 2015; Cao et al., 2015; Grover and Leskovec, 2016] attempt to learn node representations in a network in a similar way of learning word embeddings in neural language models.", "startOffset": 89, "endOffset": 175}, {"referenceID": 19, "context": "For example, DeepWalk [Perozzi et al., 2014] samples node sequences from a network using a stream of short first-order random walks, and model them just like word sequences in documents using neural embeddings.", "startOffset": 22, "endOffset": 44}, {"referenceID": 3, "context": "GrapRep [Cao et al., 2015] extends LINE to exploit structural information beyond second-order proximity.", "startOffset": 8, "endOffset": 26}, {"referenceID": 6, "context": "To offer a flexible node sampling scheme, node2vec [Grover and Leskovec, 2016] utilizes second-order random walks, and combines Depth-First Search (DFS) and Breadth-First Search (BFS) strategies to explore the local neighborhood structure.", "startOffset": 51, "endOffset": 78}, {"referenceID": 3, "context": "Although recent approach GrapRep [Cao et al., 2015] attempts to capture long distance relationship between two different nodes, it limits scope to a fixed number of hops.", "startOffset": 33, "endOffset": 51}, {"referenceID": 9, "context": "This is inspired by Paragraph Vector [Le and Mikolov, 2014], which learns a continuous vector to represent a piece of text with variable-length, such as sentences, paragraphs and documents.", "startOffset": 37, "endOffset": 59}, {"referenceID": 9, "context": ", sentences and documents [Le and Mikolov, 2014].", "startOffset": 26, "endOffset": 48}, {"referenceID": 9, "context": "The concept of \u201cwords\u201d in a document [Le and Mikolov, 2014] is", "startOffset": 37, "endOffset": 59}, {"referenceID": 6, "context": "In order to characterize the local context of a node, without loss of generality, we sample node sequences from the given network with second-order random walks in [Grover and Leskovec, 2016], which offer a flexible notion of a node\u2019s local neighborhood by combining Depth-First Search (DFS) and Breadth-First Search (BFS) strategies.", "startOffset": 164, "endOffset": 191}, {"referenceID": 16, "context": "We extend the scalable version of Log-Bilinear model [Mnih and Hinton, 2007], called vector Log-Bilinear model (vLBL) [Mnih and Kavukcuoglu, 2013].", "startOffset": 53, "endOffset": 76}, {"referenceID": 15, "context": "In our approach, we adopt negative sampling [Mikolov et al., 2013b] for optimization.", "startOffset": 44, "endOffset": 67}, {"referenceID": 18, "context": "Negative sampling represents a simplified version of noise contrastive estimation [Mnih and Teh, 2012], and trains a logistic regression to distinguish between data samples of vn from \u201cnoise\u201d distribution.", "startOffset": 82, "endOffset": 102}, {"referenceID": 9, "context": "This architecture is a counterpart of the Distributed Bag-ofWords version of Paragraph Vector [Le and Mikolov, 2014].", "startOffset": 94, "endOffset": 116}, {"referenceID": 14, "context": "We extend the framework by simultaneously training network and node vectors using a Skipgram [Mikolov et al., 2013a; Mikolov et al., 2013b] like model.", "startOffset": 93, "endOffset": 139}, {"referenceID": 15, "context": "We extend the framework by simultaneously training network and node vectors using a Skipgram [Mikolov et al., 2013a; Mikolov et al., 2013b] like model.", "startOffset": 93, "endOffset": 139}, {"referenceID": 24, "context": "Sampling a new node in the walk can be efficiently done inO(1) time using alias sampling [Walker, 1977].", "startOffset": 89, "endOffset": 103}, {"referenceID": 26, "context": "As an illustrative example, we apply Network Vector to the classic Karate network [Zachary, 1977].", "startOffset": 82, "endOffset": 97}, {"referenceID": 1, "context": "org/ ate the performance of Network Vector against the following network feature-based algorithms [Berlingerio et al., 2012]:", "startOffset": 98, "endOffset": 124}, {"referenceID": 6, "context": "\u2022 node2vec [Grover and Leskovec, 2016]: This approach learns low-dimensional feature representations of nodes in a network by interpolating between BFS and DFS for sampling node sequences.", "startOffset": 11, "endOffset": 38}, {"referenceID": 19, "context": "It\u2019s interesting to note when p = 1 and q = 1, node2vec boils down to DeepWalk [Perozzi et al., 2014], which utilizes uniform random walks.", "startOffset": 79, "endOffset": 101}, {"referenceID": 14, "context": "For Wikipedia network, we follow the word analogy task defined in [Mikolov et al., 2013a].", "startOffset": 66, "endOffset": 89}, {"referenceID": 14, "context": "There are 1,632 semantic tuples in Wikipedia network matched for the semantic pairs in [Mikolov et al., 2013a].", "startOffset": 87, "endOffset": 110}, {"referenceID": 6, "context": "To understand whether the global representation helps learning better node representation, we perform multi-label classification with the same benchmarks and experimental procedure as [Grover and Leskovec, 2016] using the same datasets: \u2022 BlogCatalog [Zafarani and Liu, 2009; Tang and Liu, 2009]: This is a network of social relationships provided by bloggers on the BlogCatalog website.", "startOffset": 184, "endOffset": 211}, {"referenceID": 20, "context": "To understand whether the global representation helps learning better node representation, we perform multi-label classification with the same benchmarks and experimental procedure as [Grover and Leskovec, 2016] using the same datasets: \u2022 BlogCatalog [Zafarani and Liu, 2009; Tang and Liu, 2009]: This is a network of social relationships provided by bloggers on the BlogCatalog website.", "startOffset": 251, "endOffset": 295}, {"referenceID": 6, "context": "\u2022 Protein-Protein Interactions (PPI) [Breitkreutz et al., 2008; Grover and Leskovec, 2016]: This is a subgraph of the entire PPI network for Homo Sapiens.", "startOffset": 37, "endOffset": 90}, {"referenceID": 10, "context": "The node labels are obtained from hallmark gene sets [Liberzon et al., 2011] and represent biological states.", "startOffset": 53, "endOffset": 76}, {"referenceID": 11, "context": "\u2022 Wikipedia Cooccurrences [Mahoney, 2009; Grover and Leskovec, 2016]: This is a network of words appearing in the first million bytes of the Wikipedia dump.", "startOffset": 26, "endOffset": 68}, {"referenceID": 6, "context": "\u2022 Wikipedia Cooccurrences [Mahoney, 2009; Grover and Leskovec, 2016]: This is a network of words appearing in the first million bytes of the Wikipedia dump.", "startOffset": 26, "endOffset": 68}, {"referenceID": 12, "context": "The Part-of-Speech (POS) tags [Marcus et al., 1993] inferred using the Stanford POS-Tagger [Toutanova et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 21, "context": "\u2022 Spectral clustering [Tang and Liu, 2011]: This method learns the d-smallest eigenvectors of the normalized graph Laplacian matrix, and utilize them as the ddimensional feature representations for nodes.", "startOffset": 22, "endOffset": 42}, {"referenceID": 19, "context": "\u2022 DeepWalk [Perozzi et al., 2014]: This method learns d-dimensional feature representations using Skipgram [Mikolov et al.", "startOffset": 11, "endOffset": 33}, {"referenceID": 14, "context": ", 2014]: This method learns d-dimensional feature representations using Skipgram [Mikolov et al., 2013a; Mikolov et al., 2013b] from node sequences, that are generated by uniform random walks from the source nodes on graph.", "startOffset": 81, "endOffset": 127}, {"referenceID": 15, "context": ", 2014]: This method learns d-dimensional feature representations using Skipgram [Mikolov et al., 2013a; Mikolov et al., 2013b] from node sequences, that are generated by uniform random walks from the source nodes on graph.", "startOffset": 81, "endOffset": 127}, {"referenceID": 6, "context": "\u2022 node2vec [Grover and Leskovec, 2016]: We use the original node2vec algorithm with optimal parameter settings of (p, q) reported in [Grover and Leskovec, 2016].", "startOffset": 11, "endOffset": 38}, {"referenceID": 6, "context": "\u2022 node2vec [Grover and Leskovec, 2016]: We use the original node2vec algorithm with optimal parameter settings of (p, q) reported in [Grover and Leskovec, 2016].", "startOffset": 133, "endOffset": 160}, {"referenceID": 3, "context": "For fair comparison, we exclude recent approaches GraRep [Cao et al., 2015], HNE [Chang et al.", "startOffset": 57, "endOffset": 75}, {"referenceID": 25, "context": ", 2015] and SDNE [Wang et al., 2016].", "startOffset": 17, "endOffset": 36}, {"referenceID": 14, "context": "For fair comparison, we use the inverse architecture of Network Vector, which is Skip-gram [Mikolov et al., 2013a] like and similar to that of node2vec.", "startOffset": 91, "endOffset": 114}, {"referenceID": 6, "context": "The parameter settings for Network Vector are in favor of node2vec, and exactly the same as in [Grover and Leskovec, 2016].", "startOffset": 95, "endOffset": 122}, {"referenceID": 6, "context": "The default parameter setting (p = 1, q = 1) used in DeepWalk and the optimal parameter setting of node2vec reported in [Grover and Leskovec, 2016] are used.", "startOffset": 120, "endOffset": 147}], "year": 2017, "abstractText": "We propose a neural embedding algorithm called Network Vector, which learns distributed representations of nodes and the entire networks simultaneously. By embedding networks in a lowdimensional space, the algorithm allows us to compare networks in terms of structural similarity and to solve outstanding predictive problems. Unlike alternative approaches that focus on node level features, we learn a continuous global vector that captures each node\u2019s global context by maximizing the predictive likelihood of random walk paths in the network. Our algorithm is scalable to real world graphs with many nodes. We evaluate our algorithm on datasets from diverse domains, and compare it with state-of-the-art techniques in node classification, role discovery and concept analogy tasks. The empirical results show the effectiveness and the efficiency of our algorithm.", "creator": "LaTeX with hyperref package"}}}