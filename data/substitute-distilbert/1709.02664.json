{"id": "1709.02664", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Multi-level Feedback Web Links Selection Problem: Learning and Optimization", "abstract": "selecting the entire wireless links for a website is important because appropriate links not only can provide high turnover but additionally also increase the website's revenue. in this work, we therefore show that smart links yield an intrinsic \\ factor { multi - level feedback structure }. for example, consider a $ 2 $ - level feedback web link : the $ 1 $ st level feedback provides the click - through rate ( ctr ) and the $ 2 $ nd level feedback provides the potential revenue, which collectively produce the compound $ 250 $ - 275 revenue. we consider the context - free links selection mechanisms of selecting links for customer homepage so as to create the total compound $ 2 $ - level revenue while keeping the total $ b $ st level profit above a preset cost. we simply generalize the problem to coincide with $ n ~ ( n \\ ge2 ) $ - 135 feedback bandwidth. the central challenge is when the links'value - level feedback structures are unobservable unless the links are imposed on the homepage. to our best knowledge, we are the first to model the links selection problem as a constrained multi - armed bandit problem and design an effective links labeling algorithm by learning the links'multi - input structure with estimated \\ link { sub - linear } regret and violation bounds. we uncover the multi - level feedback structures of web links in two wider - world narratives. we also conduct extensive experiments on the datasets to compare our proposed \\ textbf { lexp } algorithm with two terms - of - cal - art context - protected bandit algorithms and show that \\ textbf { qc } algorithm is the most effective in links selection while satisfying the constraint.", "histories": [["v1", "Fri, 8 Sep 2017 11:55:00 GMT  (1180kb,D)", "http://arxiv.org/abs/1709.02664v1", "8 pages (with full proof), 4 figures, ICDM 2017 technical report"]], "COMMENTS": "8 pages (with full proof), 4 figures, ICDM 2017 technical report", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kechao cai", "kun chen", "longbo huang", "john c s lui"], "accepted": false, "id": "1709.02664"}, "pdf": {"name": "1709.02664.pdf", "metadata": {"source": "CRF", "title": "Multi-level Feedback Web Links Selection Problem: Learning and Optimization", "authors": ["Kechao Cai", "Kun Chen", "Longbo Huang", "John C.S. Lui"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION Websites nowadays are offering many web links on their homepages to attract users. For example, news websites such as Flipboard, CNN, and BBC constantly update links to the news shown on their homepages to attract news readers. Online shopping websites such as Amazon and Taobao frequently refresh various items on their homepages to attract customers for more purchase.\nEach link shown on a homepage is intrinsically associated with a \u201cmulti-level feedback structure\u201d which provides valuable information on users\u2019 behaviors. Specifically, based on the user-click information, the website can estimate the probability (or Click-Through Rate (CTR)) that a user clicks that link, and we refer to this as the 1st level feedback. Moreover, by tracking the behaviors of users after clicking the link (e.g., whether users will purchase products associated with that link), the website can determine the revenue it can collect on that web page, we refer to this as the 2nd level feedback. The compound 2-level feedback is a function of the 1st level feedback and the 2nd level feedback. Naturally, the 1st level feedback measures the attractiveness of the link, while the 2nd level feedback measures the potential revenue of the link given that the link is clicked, and the compound 2-level feedback measures the compound revenue of the link. In summary, for a given homepage, its total attractiveness is the sum of CTRs of all\nlinks on that homepage, and its total compound revenue is the sum of the compound revenue of all links on that homepage. Both the total attractiveness and the total compound revenue of a homepage are important measures for investors to assess the value of a website [1].\nDue to the limited screen size of mobile devices or the limited size of an eye-catching area on a web page, homepages usually can only contain a finite number of web links (e.g., Flipboard only shows 6 to 8 links for its users on its homepage frame without sliding the frame.). Moreover, contextual information (e.g., the users\u2019 preferences) is not always available due to visits from casual users, cold start [2] or cookie blocking [3]. Furthermore, a website with an unattractive homepage would be difficult to attract investments. In this case, it is important for website operators to consider the context-free web links selection problem: how to select a finite number of links from a large pool of web links to show on its homepage so to maximize the total compound revenue, while keeping the total attractiveness of the homepage above a preset threshold?\nThe threshold constraint on the attractiveness of the homepage makes the above links selection problem challenging. On the one hand, selecting those links with the highest CTRs ensures that the attractiveness of the homepage is above the threshold, but it does not necessarily guarantee the homepage will have high total compound revenue. On the other hand, selecting links with the highest compound revenue cannot guarantee that the total attractiveness of the homepage satisfies the threshold constraint. Further complicating the links selection problem is the multi-level feedback structures of web links, i.e., the CTRs (1st level feedback) and the potential revenues (2nd level feedback), are unobservable if the links are not selected into the homepage.\nTo tackle this challenging links selection problem, we formulate a stochastic constrained multi-armed bandit with multi-level rewards. Specifically, arms correspond to links in the pool, and each arm is associated with a 1st level reward corresponding to the 1st level feedback (the CTR) of a link, a 2nd level reward corresponding to the 2nd level feedback (the potential revenue) of the link, and a compound 2-level reward corresponding to the compound 2-level feedback of the same link. Our objective is to select a finite number of links on the homepage so as to maximize the cumulative compound 2- level rewards (or minimizing the regret) subject to a threshold constraint while learning/mining of links\u2019 multi-level feedback structures. To achieve this objective, we design a constrained\nar X\niv :1\n70 9.\n02 66\n4v 1\n[ cs\n.L G\n] 8\nS ep\n2 01\n7\nbandit algorithm LExp, which is not only effective in links selection, but also achieves provable sub-linear regret and violation bounds. Contributions: (i) We show that a web link is intrinsically associated with a multi-level feedback structure. (ii) To our best knowledge, we are the first to model the links selection problem with multi-level feedback structures as a stochastic constrained bandit problem (Sec. II). (iii) We design an bandit algorithm LExp that selects L arms from K arms (L \u2264 K) with provable sub-linear regret and violation bounds (Sec. III). (iv) We show that LExp is more effective in links selection than two state-of-the-art context-free bandit algorithms, CUCB [4] and EXP3.M [5], via extensive experiments on two real-world datasets (Sec. IV)."}, {"heading": "II. MODEL", "text": "In this section, we first introduce the context-free web links selection problem with a 2-level feedback structure. Then we show how to formulate it as a stochastic constrained bandit problem, and illustrate how it can model the links selection problem. Finally, we generalize the links selection problem to links selection problems with n-level feedback with n \u2265 2.\nA. Bandit Formulation (Constrained 2-level Feedback)\nConsider a website structure with a homepage frame and a pool of K web pages, W = {w1, . . . , wK}. Each web page wi \u2208 W is addressed by a URL link and so we have K links in total. The homepage frame can only accommodate up to L \u2264 K links. When we select the link associated with web page wi, 1 \u2264 i \u2264 K, and put it into the homepage frame, we can observe the following information when users browse the homepage:\n1) Ai \u2265 0, the probability that a user clicks the link to wi, which is also referred to as the click-through rate (CTR); 2) Bi \u2265 0, the potential revenue received from the user who clicks the link and then purchases products (or browses ads) on the web page wi.\nTherefore, for the link associated with web page wi, the compound revenue is AiBi, 1 \u2264 i \u2264 K. Our task is to select L links from the pool of K links for the homepage frame. The objective is to maximize the total compound revenue of the selected L links, subject to the constraint that the total CTR of these selected L links is greater than or equal to a preset threshold h > 0. Let I = {i|wi \u2208 W} and |I| = L be the set of indices of any L links. Denote the feasible set of the above links selection problem as S, which contains all possible subsets of indices of any L links such that satisfy the total CTR requirement h. Specifically, the optimal set of the L links for the described links selection problem is the solution to the following constrained knapsack problem,\narg max I\u2208S \u2211 i\u2208I AiBi,\nS = { I = {i|wi \u2208 W} \u2223\u2223|I| = L,\u2211 i\u2208I Ai \u2265 h } . (1)\nProblem (1) is known to be NP-hard [6]. To tackle this problem, we relax (1) to a probabilistic linear programming problem (2) as follows,\narg max x\u2208S\u2032 \u2211K i=1 xiAiBi,\nS \u2032 = { x \u2208 [0, 1]K \u2223\u2223\u2211K i=1 xiAi \u2265 h, \u2211K i=1 xi = L } , (2)\nwhere x = (x1, . . . , xi, . . . , xK) and xi represents the probability of selecting the web page wi, 1 \u2264 i \u2264 K. Note that problem (2) is still non-trivial to solve because Ai and Bi are only observable if the web page wi is selected to the homepage frame. If wi is not selected, one cannot observe Ai or Bi.\nTo answer problem (2), we formulate the links selection problem as a stochastic constrained multi-armed bandit problem with 2-level rewards and design a constrained bandit algorithm. Formally, let K = {1, . . . ,K} denote the set of arms, where each arm corresponds to a specific link to a web page in W . Each arm i \u2208 K is associated with two unknown random processes, Ai(t) and Bi(t), t = 1, . . . , T . Specifically, Ai(t) characterizes the arm i\u2019s 1st level reward which corresponds to link i\u2019s 1st level feedback (CTR), and Bi(t) characterizes arm i\u2019s 2nd level reward which corresponds to link i\u2019s 2nd level feedback (potential revenue) that can be collected from wi. We assume that Ai(t) are stationary and independent across i, and the probability distribution of Ai(t) has a finite support. As for Bi(t), they are not necessarily stationary due to the heterogeneity of users but are bounded across i. Without loss of generality, we normalize Ai(t) \u2208 [0, 1] and Bi(t) \u2208 [0, 1]. We also assume that Ai(t) is independent of Bi(t) for i \u2208 K, t \u2265 1. Note that this assumption is reasonable as we have observed and validated that different level feedbacks of links in the multi-level feedback structure do not have strong correlations in the real-world datasets (please refer to Sec. IV).\nThe stationary random process Ai(t), is assumed to have unknown mean ai = E[Ai(t)] for 1 \u2264 i \u2264 K. Let a = (a1, . . . , aK).1 Let at = (at1, . . . , a t K) and bt = (bt1, . . . , b t K) denote the realization vectors for the random processes Ai(t) and Bi(t), respectively for 1 \u2264 i \u2264 K. Let xt = (x t 1, . . . , x t i, . . . , x t K) be the probabilistic selection vector of the K arms at time t, where xti \u2208 [0, 1] is the probability of selecting of the arm i at time t. The number of selected arms is L at each time t, i.e., 1\u1d40xt = L, where 1 = (1, . . . , 1) is the one vector. At time t, a set of L \u2264 K arms It \u2208 K is selected via a dependent rounding procedure [7], which guarantees the probability that i \u2208 It is xti at time t (see Sec. III). For each arm i \u2208 It, the algorithm observes a 1st level reward ati generated by Ai(t) as well as a 2nd level reward b t i generated by Bi(t), and receives a compound 2-level reward. Specifically, the compound 2-level reward, gti , of an arm i at time t is generated by the random process Gi(t) = Ai(t)Bi(t). Let gti = a t ib t i, 1 \u2264 i \u2264 K and gt = (gt1, . . . , gti , . . . , gtK). In addition, there is a preset threshold h > 0 such that the average of the sum of the 1st level rewards needs to be above\n1All vectors defined in this paper are column vectors.\nthis threshold, i.e., a\u1d40 E[xt] \u2265 h.2 At time t, the expected total compound 2-level reward is E[ \u2211 t g \u1d40 t xt] with the probabilistic selection vector xt, t = 1, . . . , T . Our objective is to design an algorithm to choose the selection vectors xt for t = 1, . . . , T such that the regret, which is also referred to as loss compared with the oracle maxa\u1d40x\u2265h \u2211T t=1 g \u1d40 t x, is minimized. Specifically, the regret for an algorithm \u03c0 is,\nReg\u03c0(T ) = max a\u1d40x\u2265h \u2211T t=1 g\u1d40t x\u2212 E [\u2211T t=1 g\u1d40t x \u03c0 t ] , (3)\nwhere x\u03c0t is the probabilistic selection vector calculated by the algorithm \u03c0 at time t. Note that x\u03c0t may violate the constraint initially especially when we have little information about the arms. To measure the overall violations of the constraint at time T , the violation of the algorithm \u03c0 is defined as,\nVio\u03c0(T ) = E [\u2211T\nt=1 (h\u2212 a\u1d40x\u03c0t ) ] + , (4)\nwhere [x]+ = max(x, 0). Note that if the regret and violation of an algorithm are linear, the algorithm is not learning. A simple example of such algorithms is the uniform arm selection algorithm where any L arms are selected with equal probability. Such a random policy would result in both linear regret and linear violation as there is a constant loss compared to the optimal policy at each time t.\nB. Generalization to n-level Feedback, where n \u2265 2 We can further extend the constrained multi-armed bandit model with 2-level reward to the constrained multi-armed bandit model with n-level (n \u2265 2) reward, and this allows us to model links selection problem with n-level feedback structure. Specifically, we can take each web page wi \u2208 W as a pseudo homepage frame. For the pseudo homepage frame, there is a pool of web pagesW \u2032, |W| = K \u2032. Then we consider selecting a subset of L\u2032 links I \u2032, |I \u2032| = L\u2032 (that each links to a web page in W \u2032) for the pseudo home page frame, with the constraint that the total CTR on the pseudo homepage frame is above the threshold h\u2032. Formally, for each web page wi \u2208 W , we consider the potential revenue of Bi in a much more precise way, i.e., Bi = \u2211 j\u2208I\u2032 A \u2032 jB \u2032 j , where A \u2032 j is the CTR of the link associated with the web page w\u2032j \u2208 I \u2032 and B\u2032j is the potential revenue collected from the web page w\u2032j \u2208 I \u2032. As such, we extend the links selection problem with 2-level feedback (Ai and Bi where i \u2208 I) to a problem with 3-level feedback And similarly, we can further extend the problem to the problems with n-level feedback structure where n \u2265 2."}, {"heading": "III. ALGORITHM & ANALYSIS", "text": "In this section, we first elaborate the design of our constrained bandit algorithm LExp (which stands for \u201c(KL)Lagrangian Exponential weights\u201d) and present the algorithmic details. Then we provide both regret and violation analysis and show that our algorithm has the attractive property of being sub-linear in both regret and violation.\n2If h = 0, the problem is equivalent to the classic unconstrained multiple play multi-armed bandit problem (MP-MAB) [8].\nAlgorithm 1 LExp (\u03b3, \u03b4) Init: \u03b71 = 1, \u03bb1 = 0, h > 0, \u03b2 = (1/L\u2212 \u03b3/K)/(1\u2212 \u03b3)\n1: for t = 1, . . . , T do 2: At = \u2205, It = \u2205, \u03b1t = 0. 3: if maxi\u2208K \u03b7ti \u2265 \u03b2 \u2211K i=1 \u03b7 t i then 4: Find \u03b1t such that\n\u03b1t/ (\u2211K\ni=1,\u03b7ti\u2265\u03b1t \u03b1t + \u2211K i=1,\u03b7ti<\u03b1t \u03b7ti ) = \u03b2\n5: At = {i : \u03b7ti \u2265 \u03b1t} 6: for i = 1, . . . ,K do\n\u03b7\u0303ti = \u03b1t if i \u2208 At; otherwise, \u03b7\u0303ti = \u03b7ti 7: for i = 1, . . . ,K do\nx\u0303ti = L[(1\u2212 \u03b3)\u03b7\u0303ti/ \u2211K i=1 \u03b7\u0303 t i + \u03b3/K]\n8: It = DependentRounding(L, x\u0303t) 9: for i \u2208 It do receive ati, and bti\n10: for i = 1, . . . ,K do a\u0302ti = a t i/x\u0303 t i1(i \u2208 It), g\u0302ti = atibti/x\u0303ti1(i \u2208 It)\n11: for i = 1, . . . ,K do\n\u03b7t+1i = { \u03b7ti if i \u2208 At; \u03b7ti exp[\u03b6(g\u0302 t i + \u03bbta\u0302 t i)] if i /\u2208 At\n12: \u03bbt+1 = [(1\u2212 \u03b4\u03b6)\u03bbt \u2212 \u03b6( a\u0302 \u1d40 t x\u0303t 1\u2212\u03b3 \u2212 h)]+\n13: function DependentRounding(L,x) 14: while exist xi \u2208 (0, 1) do 15: Find i, j, i 6= j, such that xi,j \u2208 (0, 1) 16: p = min{1\u2212 xi, xj}, q = min{xi, 1\u2212 xj}\n17: (xi, xj) = { (xi + p, xj \u2212 p) with prob. qp+q ; (xi \u2212 q, xj + q) with prob. pp+q .\nreturn I = {i |xi = 1, 1 \u2264 i \u2264 K}"}, {"heading": "A. Constrained Bandit Algorithm", "text": "The unique challenge for our algorithmic design is to balance between maximizing the compound multi-level rewards (or minimizing the regret) and at the same time, satisfying the threshold constraint. To address this challenge, we incorporate the theory of Lagrange method in constrained optimization into the design of LExp. We consider minimizing a modified regret function that includes the violation with an adjustable penalty coefficient that increases the regret when there is any non-zero violation. Specifically, LExp introduces a sub-linear bound for the Lagrange function of Reg\u03c0(T ) and Vio\u03c0(T ) in the following structure,\nReg\u03c0(T ) + \u03c1(T )Vio 2 \u03c0(T ) \u2264 T 1\u2212\u03b8, 0 < \u03b8 \u2264 1, (5)\nwhere \u03c1(T ) plays the role of a Lagrange multiplier. From (5), we can derive a bound for Reg\u03c0(T ) and a bound for Vio\u03c0(T ) as follows:\nReg\u03c0(T )\u2264O(T 1\u2212\u03b8),Vio\u03c0(T )\u2264 \u221a O(T 1\u2212\u03b8+LT )/\u03c1(T ), (6)\nwhere the bound for Vio\u03c0(T ) in (6) is for the fact that \u2212Reg\u03c0(T ) \u2264 O(LT ) for any algorithm \u03c0. Thus, with properly\nchosen algorithm parameter \u03c1(T ), both the regret and violation can be bounded by sub-linear functions of T .\nThe details of LExp are shown in Algorithm 1. In particular, LExp maintains a weight vector \u03b7t at time t, which is used to calculate the probabilistic selection vector x\u0303t (line 3 to line 7). Specially, line 3 to line 6 ensure that the probabilities in x\u0303t are less than or equal to 1. At line 8, we deploy the dependent rounding function (line 13 to line 17) to select L arms using the calculated x\u0303t. At line 9, the algorithm obtains the rewards ati and b t i, and then gives unbiased estimates of a\u0302ti and g\u0302 t i at line 10. Specifically, the 1st level reward a\u0302 t i, and the compound 2-level reward g\u0302ti are estimated by a t i/x\u0303 t i, and atib t i/x\u0303 t i, respectively, such that E[a\u0302ti] = ati, and E[g\u0302ti ] = atibti. Finally, the weight vector \u03b7t and the Lagrange multiplier \u03bbt are updated (line 11 and line 12) using previous estimations."}, {"heading": "B. Regret and Violation Analysis", "text": "Theorem 1. Let \u03b6 = \u03b3\u03b4L(\u03b4+L)K , \u03b3 = \u0398(T \u2212 13 ) and \u03b4 = \u0398(T\u2212 1 3 ) that satisfy \u03b4 \u2265 4(e\u22122)\u03b3L1\u2212\u03b3 \u2212L. By running the LExp algorithm \u03c0\u0303, we achieve sub-linear bounds for both the regret in (3) and violation in (4) as follows:\nReg\u03c0\u0303(T )\u2264O(LK ln(K)T 2 3 ) and Vio\u03c0\u0303(T )\u2264O(L 1 2K 1 2T 5 6 ).\nProof. From line 12 of the algorithm, we have: \u03bbt+1 = [ (1\u2212 \u03b4\u03b6)\u03bbt\u2212 \u03b6( a\u0302 \u1d40 t x\u0303t 1\u2212\u03b3 \u2212 h) ] + \u2264 [ (1\u2212 \u03b4\u03b6)\u03bbt + \u03b6h) ] +\n. By induction on \u03bbt, we can obtain \u03bbt \u2264 h\u03b4 . Let \u03a6t = \u2211K i=1 \u03b7\nt i and \u03a6\u0303t =\u2211K\ni=1 \u03b7\u0303 t i . Define rt = gt + \u03bbtat and r\u0302t = g\u0302t + \u03bbta\u0302t. Let x be an arbitrary probabilistic selection vector which satisfies xi \u2208 [0, 1], 1\u1d40xt = L and a\u1d40x \u2265 h. We know that\nT\u2211 t=1 ln \u03a6t+1 \u03a6t = ln \u03a6T+1 \u03a61 = ln( K\u2211 i=1 \u03b7T+1i )\u2212 lnK\n\u2265 ln( K\u2211 i=1 xi\u03b7 T+1 i )\u2212 lnK \u2265 K\u2211 i=1 xi L \u2211 t:i/\u2208At \u03b6r\u0302ti \u2212 ln K L\n= \u03b6\nL K\u2211 i=1 xi \u2211 t:i/\u2208At r\u0302ti \u2212 ln K L . (7)\nAs \u03b6 = \u03b3\u03b4L(\u03b4+L)K and \u03bbt \u2264 h\u03b4 , we have \u03b6r\u0302ti \u2264 1. Therefore,\n\u03a6t+1 \u03a6t\n= \u2211\ni\u2208K/At\n\u03b7t+1i \u03a6t + \u2211 i\u2208At \u03b7t+1i \u03a6t = \u2211 i\u2208K/At \u03b7ti \u03a6t exp(\u03b6r\u0302ti)+ \u2211 i\u2208At \u03b7ti \u03a6t\n\u2264 \u2211\ni\u2208K/At\n\u03b7ti \u03a6t [1 + \u03b6r\u0302ti + (e\u2212 2)\u03b62(r\u0302ti)2] + \u2211 i\u2208At \u03b7ti \u03a6t\n(8)\n= 1 + \u03a6\u0303t \u03a6t \u2211 i\u2208K/At \u03b7ti \u03a6\u0303t [ \u03b6r\u0302ti + (e\u2212 2)\u03b62(r\u0302ti)2 ] \u2264 1 +\n\u2211 i\u2208K/At x\u0303ti/L\u2212 \u03b3/K 1\u2212 \u03b3 [ \u03b6r\u0302ti + (e\u2212 2)\u03b62(r\u0302ti)2 ] \u2264 1+ \u03b6 L(1\u2212 \u03b3) \u2211\ni\u2208K/At\nx\u0303ti r\u0302 t i + (e\u2212 2)\u03b62 L(1\u2212 \u03b3) \u2211 i\u2208K/At x\u0303ti(r\u0302 t i) 2\n\u2264 1+ \u03b6 L(1\u2212 \u03b3) \u2211 i\u2208K/At x\u0303ti r\u0302 t i + (e\u2212 2)\u03b62 L(1\u2212 \u03b3) K\u2211 i=1 (1 + \u03bbt)r\u0302 t i . (9)\nInequality (8) holds because ey \u2264 1+y+(e\u22122)y2 for y \u2264 1, and inequality (9) uses the fact that x\u0303ti r\u0302 t i = r t i \u2264 1 + \u03bbt for i \u2208 It and x\u0303ti r\u0302ti = 0 for i /\u2208 It. Since ln(1 +y) \u2264 y for y \u2265 0, we can get\nln \u03a6t+1 \u03a6t \u2264 \u03b6 L(1\u2212 \u03b3) \u2211 i\u2208K/At x\u0303ti r\u0302 t i + (e\u2212 2)\u03b62 L(1\u2212 \u03b3) K\u2211 i=1 (1 + \u03bbt)r\u0302 t i .\nThen using (7), it follows that\n\u03b6\nL K\u2211 i=1 xi \u2211 t:i/\u2208At r\u0302ti \u2212 ln K L \u2264 \u03b6 L(1\u2212 \u03b3) T\u2211 t=1 \u2211 i\u2208K/At x\u0303ti r\u0302 t i\n+ (e\u2212 2)\u03b62 L(1\u2212 \u03b3) T\u2211 t=1 K\u2211 i=1 (1 + \u03bbt)r\u0302 t i .\nAs x\u0303ti = 1 for i \u2208 At, and \u2211K i=1 xi \u2211 t:i\u2208At r\u0302 t i \u2264\n1 1\u2212\u03b3 \u2211T t=1 \u2211 i\u2208At r\u0302 t i trivially holds, we have T\u2211 t=1 r\u0302\u1d40t x\u2212 L \u03b6 ln K L \u2264 \u2211T t=1 r\u0302 \u1d40 t x\u0303t 1\u2212 \u03b3 + (e\u2212 2)\u03b6 1\u2212 \u03b3 T\u2211 t=1 K\u2211 i=1 (1+\u03bbt)r\u0302 t i .\nTaking expectation on both sides, we have E [\u2211T\nt=1 r\u0302\u1d40t x\u2212\n1 1\u2212 \u03b3 \u2211T t=1 r\u0302\u1d40t x\u0303t ] \u2264 L \u03b6 ln K L + (e\u2212 2)\u03b6 1\u2212 \u03b3 \u2211T t=1 E [\u2211K i=1 (1 + \u03bbt)r\u0302 t i\n] \u2264 L \u03b6 ln K L + 2(e\u2212 2)\u03b6K 1\u2212 \u03b3 T + 2(e\u2212 2)\u03b6K 1\u2212 \u03b3 \u2211T t=1 \u03bb2t , (10)\nwhere (10) is from the inequality E[ \u2211K i=1(1 + \u03bbt)r\u0302\nt i ] =\u2211K\ni=1(1+\u03bbt)(g t i+\u03bbta t i) \u2264 2K+2K\u03bb2t . Next, we define a se-\nries of functions ft(\u03bb) = \u03b42\u03bb 2+\u03bb( 11\u2212\u03b3 a\u0302 \u1d40 t x\u0303t\u2212h), t = 1, . . . , T , and we have \u03bbt+1 = [\u03bbt \u2212 \u03b6\u2207ft(\u03bbt)]+. It is clear that ft(\u00b7) is a convex function for all t. Thus, for an arbitrary \u03bb, we have\n(\u03bbt+1 \u2212 \u03bb)2 = ([\u03bbt \u2212 \u03b6\u2207ft(\u03bbt)]+ \u2212 \u03bb)2\n\u2264 (\u03bbt \u2212 \u03bb)2+\u03b62(\u03b4\u03bbt \u2212 h+ a\u0302\u1d40t x\u0303t 1\u2212 \u03b3 ) 2\u22122\u03b6(\u03bbt \u2212 \u03bb)\u2207ft(\u03bbt) \u2264 (\u03bbt \u2212 \u03bb)2 + 2\u03b62h2 + 2\u03b62 (a\u0302\u1d40t x\u0303t) 2\n(1\u2212 \u03b3)2 + 2\u03b6[ft(\u03bb)\u2212 ft(\u03bbt)].\nLet \u2206 = [(\u03bbt \u2212 \u03bb)2 \u2212 (\u03bbt+1 \u2212 \u03bb)2]/(2\u03b6) + \u03b6L2. We have,\nft(\u03bbt)\u2212 ft(\u03bb) \u2264 \u2206 + \u03b6(a\u0302\u1d40t x\u0303t) 2 (1\u2212 \u03b3)2 =\u2206 + \u03b6L2( 1L a\u0302 \u1d40 t x\u0303t) 2 (1\u2212 \u03b3)2\n\u2264 \u2206 + \u03b6L 2 (1\u2212 \u03b3)2 1 L K\u2211 i=1 (x\u0303tia\u0302 t i) 2 \u2264 \u2206 + \u03b6L (1\u2212 \u03b3)2 K\u2211 i=1 ati.\nTaking expectation over \u2211T t=1[ft(\u03bbt)\u2212 ft(\u03bb)], we have\nE [\u03b4 2 \u2211T t=1 \u03bb2t \u2212 \u03b4 2 \u03bb2T + \u2211T t=1 \u03bbt( a\u0302\u1d40t x\u0303t 1\u2212 \u03b3 \u2212 h)\n\u2212 \u03bb \u2211T\nt=1 ( a\u0302\u1d40t x\u0303t 1\u2212 \u03b3 \u2212 h)\n] \u2264 \u03bb 2\n2\u03b6 + \u03b6L2T +\n\u03b6LK\n(1\u2212 \u03b3)2T. (11)\nCombining (10) and (11), we have,\u2211T t=1 g\u1d40t x\u2212 E[ \u2211T t=1 g \u1d40 t x\u0303t] 1\u2212 \u03b3 + E [ \u2212 (\u03b4T 2 + 1 2\u03b6 )\u03bb2\n+ \u03bb \u2211T\nt=1 (h\u2212 a \u1d40x\u0303t 1\u2212 \u03b3 ) ] \u2264 L \u03b6 ln K L + 2(e\u2212 2)\u03b6KT 1\u2212 \u03b3\n+ \u03b6L2T + \u03b6LKT (1\u2212 \u03b3)2 + ( 2(e\u2212 2)\u03b6K 1\u2212 \u03b3 \u2212 \u03b4 2 ) \u2211T t=1 \u03bb2t\n+ E[ \u2211T\nt=1 \u03bbt(h\u2212 a\u1d40x)].\nSince \u03b6 = \u03b3\u03b4L(\u03b4+L)K and \u03b4 \u2265 4(e\u22122)\u03b3L 1\u2212\u03b3 \u2212L, we have 2(e\u22122)\u03b6K 1\u2212\u03b3 \u2264 \u03b4 2 . As a \u1d40x \u2265 h, we have\n(1\u2212 \u03b3) \u2211T\nt=1 g\u1d40t x\u2212 E [\u2211T t=1 g\u1d40t x\u0303t ]\n+ E [ \u03bb \u2211T\nt=1 ((1\u2212 \u03b3)h\u2212 a\u1d40x\u0303t)\u2212 (\n\u03b4T\n2 +\n1 2\u03b6 )\u03bb2 ]\n\u2264 L \u03b6 ln K L + 2(e\u2212 2)\u03b6KT + \u03b6L2T + \u03b6LKT 1\u2212 \u03b3 .\nLet \u03bb = \u2211T t=1((1\u2212\u03b3)h\u2212a \u1d40x\u0303t)\n\u03b4T+1/\u03b6 . Maximize over x and we have,\nmax a\u1d40x\u2265h \u2211T t=1 g\u1d40t x\u2212 E [\u2211T t=1 g\u1d40t x\u0303t ] + E {[\u2211T t=1((1\u2212 \u03b3)h\u2212 a\u1d40x\u0303t) ]2 +\n2(\u03b4T + 1/\u03b6) } \u2264 L \u03b6 ln K L + 2(e\u2212 2)\u03b6KT + \u03b6L2T + \u03b6LKT 1\u2212 \u03b3 + \u03b3LT.\nLet F (T ) = L\u03b6 ln K L + 2(e\u2212 2)\u03b6KT + \u03b6L2T + \u03b6LKT 1\u2212\u03b3 + \u03b3LT . Then we have results in the form of (6): Reg\u03c0\u0303(T ) \u2264 F (T ), and Vio\u03c0\u0303(T ) \u2264 \u221a 2 (F (T ) + LT ) (\u03b4T + 1/\u03b6) +\u03b3LT. Let \u03b3 = \u0398(T\u2212 1 3 ) and \u03b4 = \u0398(T\u2212 1 3 ). Thus, we have \u03b6 = \u0398( 1KT \u2212 23 ). Finally, we have Reg\u03c0\u0303(T ) \u2264 O(LK ln(K)T 2 3 ) and Vio\u03c0\u0303(T ) \u2264 O(L 1 2K 1 2T 5 6 )."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we first examine the web links\u2019 multilevel feedback structures in two real-world datasets from the Kaggle Competitions, Avito Context Ad Clicks [9] and Coupon Purchase Prediction [10], referred to as \u201cAd-Clicks\u201d and \u201cCoupon-Purchase\u201d in this paper. Then we conduct a comparative study by applying LExp and two state-of-the-art context-free bandit algorithms, CUCB [4] and EXP3.M [5], to show the effectiveness of LExp in links selection."}, {"heading": "A. Multi-level Feedback Structure Discovery", "text": "The Ad-Clicks data is collected from users of the website Avito.ru where a user who is interested in an ad has to first click to view the ad before making a phone request for further inquiries. The data involves the logs of the visit stream and the phone request stream of 71, 677, 831 ads. We first perform some data cleaning. For each ad in Ad-Clicks, we count the number of views of the ad and thereafter, count the number of\nphone requests that ad received. In particular, we filter out the ads that have an abnormally large number of views (greater than 2000 ), and the ads that receive few numbers of phone requests (smaller than 100 ). Finally, we obtain 225 ads from Ad-Clicks. For each of these 225 ads, we divide the number of phone requests by the number of views to get the Phone Request Rate. We normalize the numbers of views of each ad to the interval [0, 1] using min-max scaling. The normalized number of views can be taken as the CTRs of the ads. As such, we find the multi-level feedback structure for each ad in the Ad-Clicks data: the CTR corresponds to the ad\u2019s 1st level feedback, the Phone Request Rate corresponds to the ad\u2019s 2nd level feedback, and the product of CTR and the Phone Request Rate corresponds to the compound 2-level feedback.\nThe Coupon-Purchase data is extracted from transaction logs of 32, 628 coupons on the site ponpare.jp, where users first browse a coupon and then decide whether to purchase the coupon or not. We extract 271 coupons from CouponPurchase. For each coupon, we divide its number of purchase by the number it was browsed, and take the ratio as the Coupon Purchase Rate. We then normalize all the browsed times to [0, 1] using min-max scaling and refer to the normalized browsed times as to the CTRs of the coupons. Thus, for each coupon in Coupon-Purchase, the CTR corresponds to the coupon\u2019s 1st level feedback, the Coupon Purchase Rate corresponds to the coupon\u2019s 2nd level feedback, and the product of the CTR and the Coupon Purchase Rate is the compound 2-level feedback.\nNext, we validate our previous claim in Sec. II that the 1st level feedback and the 2nd level feedback in the multilevel feedback structure do not have a strong correlation. In Ad-Clicks, we find that the CTRs and the Phone Request Rates are not strongly correlated with a correlation coefficient \u22120.47. Similarly, low correlation can also be found in CouponPurchase where the correlation coefficient is \u22120.27 only."}, {"heading": "B. Comparative Study on Links Selection", "text": "We simulate the multi-level feedback structures in the real-world datasets and model the probabilistic ads/coupons selection process with time-variant rewards. In particular, for each of the 225 ads in Ad-Clicks, we treat its CTR/1st level feedback as a Bernoulli random variable with the mean CTR taking from Ad-Clicks, and we vary the Phone Request Rate/2nd level reward over time in a similar fashion as a sinusoidal wave (similar to [11]): the 2nd level reward starts from a random value drawn uniformly from 0 to the mean Phone Request Rate taking from Ad-Clicks; then in each time slot, it increases or decreases at rate 10/T until reaching the mean or 0. This time-variant rewards can model the seasonal fluctuations of the potential revenue of the ads. For each of the 271 coupons in Coupon-Purchase, we simulate its 2-level rewards in the same way.\nIn our performance comparison, the CUCB algorithm always selects the top-L arms with the highest UCB (upper confidence bound) indices without considering any constraint. For the CUCB algorithm, on the one hand, if we only want to\nmaximize the total 1st level rewards, the L arms of the highest UCB indices a\u0302ti+ \u221a 3 ln t/(2Ni(t)) are selected at each time t (CUCB-1), where Ni(t) is the number of times that the arm i has been selected by time t. On the other hand, if we only want to maximize the total compound 2-level rewards, the L arms of the highest UCB indices g\u0302ti+ \u221a 3 ln t/(2Ni(t)) will be selected (CUCB-2). For EXP3.M, only the 1st level reward estimation a\u0302t is considered when we only maximize the total 1st level rewards (EXP3.M-1), and only the compound 2-level reward estimation g\u0302t is considered when we only maximize the total compound 2-level rewards (EXP3.M-2). In our experiments, the cumulative 1st level reward and the cumulative compound 2-level reward at time t are calculated using \u2211t t\u2032=1 \u2211 i\u2208It\u2032 at \u2032 i\nand \u2211t t\u2032=1 \u2211 i\u2208It\u2032 gt \u2032\ni , respectively. The regret at time t is calculated using \u2211t t\u2032=1 g \u1d40 t\u2032x \u2217 \u2212 \u2211tt\u2032=1\u2211i\u2208It\u2032 gt\u2032i where x\u2217 is the optimal probabilistic selection vector by solving maxa\u1d40x\u2265h \u2211t t\u2032=1 g \u1d40 t\u2032x with a taking from the datasets. The\nviolation at time t is calculated using \u2211t t\u2032=1(h\u2212 \u2211 i\u2208It\u2032 at \u2032\ni )+. For Ad-Clicks, we run LExp, the EXP3.M variants: EXP3.M-1 and EXP3.M-2, and the CUCB variants: CUCB1 and CUCB-2. The parameter settings are shown in Fig. 1. \u2022 Experiment 1 (considering total 1st level rewards only): In Fig. 1(a), we compare the cumulative rewards of LExp, EXP3.M-1 and CUCB-1. Specifically, the cumulative threshold constraint is a linear function of t, i.e., h \u00b7 t, shown by the black-dash line. One can observe that LExp satisfies the threshold constraint because its slope is equal to h after time t = 841, meaning that there is no violation for LExp after t = 841. On the contrary, CUCB-1 and EXP3.M-1 initially do not satisfy the threshold constraint and both exceed the threshold constraint. Furthermore, the cumulative compound 2-level rewards of CUCB-1 and EXP3.M-1 are both below that of LExp as the ads selected by CUCB-1 and EXP3.M1 with high 1st level rewards do not necessarily result in a high 2nd level rewards. Thus, by selecting ads that satisfy the threshold constraint, even when the cumulative 1st level rewards is restricted by the threshold, LExp gives the highest total compound 2-level rewards with the compound 2-level rewards gain shown in the blue shaded area. \u2022 Experiment 2 (Regrets & Violations on Experiment 1): Fig. 1(b) shows the regrets and violations of CUCB-1, EXP3.M-1 and LExp. First, we draw the linear regret line which shows the largest regret for the ads selection problem where no ads are selected at each time, i.e., all the rewards are lost. The regret of CUCB-1 and the regret of EXP3.M-1 are very close to the linear regret line. They are also greater than the regret of LExp, which has a sub-linear property, and this further confirms the fact that cumulative compound 2-level rewards of CUCB-1 and EXP3.M-1 are both below that of LExp in Fig. 1(a). For the violations, all the three algorithms, LExp, EXP3.M-1 and CUCB-1 first show some increases and then remain constant. This is because they all are in the exploration phase: they first select ads with random 1st level rewards when the 1st level rewards are still unknown, and later select ads with high 1st level rewards that satisfy or exceed the threshold constraint. LExp is less aggressive than EXP3.M-1 and CUCB-1 which both select the ads with total 1st level rewards that far exceed the threshold. But as we can observe in Fig. 1(a), LExp performs much better than these algorithms. In summary, LExp takes longer to explore the optimal ads but after some trials, the violation at each time diminishes to zero. This can be observed from Fig. 1(b) since the violations remains unchanged after about 850 time slots. \u2022 Experiment 3 (considering the compound 2-level rewards only): Fig. 1(c) shows the cumulative rewards of LExp, EXP3.M-2 and CUCB-2. Specially, the cumulative compound 2-level rewards of CUCB-2 and EXP3.M-2 are both larger than that of LExp as they consider maximizing the total compound 2-level rewards only. However, the cumulative 1st level rewards of both EXP3.M-2 and CUCB-2 increase slower than the threshold constraint h \u00b7 t as their slopes are less than h. This means that they violate the constraint and the gap between their cumulative 1st level rewards and the threshold constraint continues to grow as time goes by. For the cumulative 1st level rewards of LExp, the slope increases up to h and maintains at h and therefore the gap becomes a constant. In summary, LExp ensures that the threshold constraint is satisfied and produces the additional 1st level reward gain (blue-shaded area) compared with EXP3.M-2 and CUCB-2. \u2022 Experiment 4 (Regrets & Violations on Experiment 3): Fig. 1(d) shows the regrets and violations of LExp, EXP3.M2 and CUCB-2. For the regrets, the regrets of EXP3.M-2, CUCB-2 and LExp are all sub-linear and below the linear regret line, as these three algorithms all aimed at minimizing the regret. Among them, LExp has a comparable regret but it also has an addition property, which is to satisfy the threshold constraint. As for the violation, the violations of both CUCB-2 and EXP3.M-2 end up linear as their cumulative 1st level rewards increase slower than the cumulative threshold\nconstraint as shown in Fig. 1(c). In contrast, the violation of LExp increases and then eventually stays constant. This confirms that LExp aims to satisfy the constraint and it will not make mistake after some rounds of learning. This confirms that LExp aims to satisfy the constraint and it will not make mistake after some rounds of learning.\nFor Coupon-Purchase, we obtain similar experimental results and we can draw similar conclusions. Therefore, we omit the detailed descriptions for conciseness. In summary, our experimental results show that LExp is the only algorithm which balances the regret and violation in the ads/coupons selection problem."}, {"heading": "V. RELATED WORK", "text": "One common approach to the links selection problem is to perform A/B testing [12], which splits the traffic for different sets of links on two different web pages, and then evaluate their rewards. However, A/B testing does not have any loss/regret guarantee as it splits equal amounts of traffic to the links regardless of the links\u2019 rewards. That said, A/B testing is still widely used in commercial web systems. Our algorithm can be viewed as a complementary approach to A/B testing, e.g., our algorithm can select the set of links with the 1st level reward above a given threshold and facilitate a more efficient A/B testing for the links selection problem.\nAnother approach is to model the links selection problem as contextual bandit problems. [13] first formulated a contextual bandit problem aiming at selecting articles/links that maximize the total number of clicks based on the user-click feedback. Recently, [14] and [15] incorporated the collaborative filtering method into contextual bandit algorithms using users\u2019 profiles to recommend web links. However, contextual information is not always available due to cold start [2] or blocking of cookie tracking [3]. Moreover, contextual bandit problem formulations neglect the multi-level feedback structures of the links and do not consider any constraint.\nOur bandit formulation is related to the bandit models with multiple plays, where multiple arms are selected in each round. [5] presented the EXP3.M bandit algorithm that extends the single-played EXP3 algorithm [16] to multiple-played cases using exponential weights. [4] proposed an algorithm that selects multiple arms with the highest upper confidence bound (UCB) indices. [17] presented the multiple-play Thompson Sampling algorithm (MP-TS) for arms with binary rewards. [18] proposed a bandit-based ranking algorithm for ranking search queries. Our bandit model differs from these bandit models as we further consider the constraint on the total 1st level rewards in selecting the multiple arms.\nNote that the constraint in our constrained bandit model is very different from that in bandit with budgets [19], [20] and bandit with knapsacks [21]. For these works, the optimal stopping time is considered since no arms can be selected/played if the budget/knapsacks constraints are violated. However, the constraint in our model does not pose such restrictions and the arm selection procedure can continue without stopping. Finally, our constrained bandit problem is related but different\nfrom the bandit model considered in [22] which tries to balance regret and violation. They only considered selecting a single arm without any multi-level rewards. While in our work, we consider how to select multiple arms and each arm is associated with multi-level rewards, making our model more challenging and applicable to the web links selection problem."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we reveal the intrinsic multi-level feedback structures of web links and formulate the web links selection problem. To our best knowledge, we are the first to model the links selection problem with multi-level feedback structures as a stochastic constrained bandit problem. We propose and design an effective links selection algorithm LExp with provable sub-linear regret and violation bounds. Furthermore, we demonstrate how to learn/mine the multi-level reward structures of web links in two real-world datasets. We carry out extensive experiments to compare LExp with the stateof-the-art context-free bandit algorithms and show that LExp is superior in selecting web links with constrained multi-level feedback by balancing both regret and violation."}], "references": [{"title": "Seven rules of thumb for web site experimenters", "author": ["R. Kohavi", "A. Deng", "R. Longbotham", "Y. Xu"], "venue": "Proceedings of SIGKDD, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey of active learning in collaborative filtering recommender systems", "author": ["M. Elahi", "F. Ricci", "N. Rubens"], "venue": "Computer Science Review, vol. 20, pp. 29\u201350, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Trackmeornot: Enabling flexible control on web tracking", "author": ["W. Meng", "B. Lee", "X. Xing", "W. Lee"], "venue": "Proceedings of WWW, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Combinatorial multi-armed bandit: General framework, results and applications", "author": ["W. Chen", "Y. Wang", "Y. Yuan"], "venue": "Proceedings of ICML, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Algorithms for adversarial bandit problems with multiple plays", "author": ["T. Uchiya", "A. Nakamura", "M. Kudo"], "venue": "Proceedings of ACL\u201910, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "On the existence of fast approximation schemes, ser", "author": ["B. Korte", "R. Schrader"], "venue": "Reprint series. Inst. fu\u0308r O\u0308konometrie u. Operations- Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Dependent rounding and its applications to approximation algorithms", "author": ["R. Gandhi", "S. Khuller", "S. Parthasarathy", "A. Srinivasan"], "venue": "Journal of the ACM (JACM), vol. 53, no. 3, pp. 324\u2013360, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Asymptotically efficient allocation rules for the multiarmed bandit problem with multiple playspart i: Iid rewards", "author": ["V. Anantharam", "P. Varaiya", "J. Walrand"], "venue": "IEEE Transactions on Automatic Control, vol. 32, no. 11, pp. 968\u2013976, 1987.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1987}, {"title": "Avito context ad clicks", "author": ["Kaggle"], "venue": "2015, https://www.kaggle.com/c/ avito-context-ad-clicks.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Coupon purchase prediction", "author": ["\u2014\u2014"], "venue": "2016, https://www.kaggle.com/c/ coupon-purchase-prediction.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Stochastic multi-armed-bandit problem with non-stationary rewards", "author": ["O. Besbes", "Y. Gur", "A. Zeevi"], "venue": "Proceedings of NIPS, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Trustworthy analysis of online a/b tests: Pitfalls, challenges and solutions", "author": ["A. Deng", "J. Lu", "J. Litz"], "venue": "Proceedings of WSDM, 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R. Schapire"], "venue": "Proceedings of WWW, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Collaborative filtering with low regret", "author": ["G. Bresler", "D. Shah", "L.F. Voloch"], "venue": "Proceedings of ACM SIGMETRICS, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Collaborative filtering bandits", "author": ["S. Li", "A. Karatzoglou", "C. Gentile"], "venue": "Proceedings of SIGIR, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Comput., vol. 32, no. 1, Jan. 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Optimal regret analysis of thompson sampling in stochastic multi-armed bandit problem with multiple plays", "author": ["J. Komiyama", "J. Hondaand", "H. Nakagawa"], "venue": "ICML, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Gathering additional feedback on search results by multi-armed bandits with respect to production ranking", "author": ["A. Vorobev", "D. Lefortier", "G. Gusev", "P. Serdyukov"], "venue": "Proceedings of WWW, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Banditbased algorithms for budgeted learning", "author": ["K. Deng", "C. Bourke", "S. Scott", "J. Sunderman", "Y. Zheng"], "venue": "Proceedings of ICDM, 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Budgeted multi-armed bandits with multiple plays", "author": ["Y. Xia", "T. Qin", "W. Ma", "N. Yu", "T.-Y. Liu"], "venue": "Proceedings of IJCAI, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives", "author": ["S. Agrawal", "N.R. Devanur", "L. Li", "N. Rangarajan"], "venue": "Proceedings of COLT, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Online decision making under stochastic constraints", "author": ["M. Mahdavi", "T. Yang", "R. Jin"], "venue": "NIPS workshop on Discrete Optimization in Machine Learning, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Both the total attractiveness and the total compound revenue of a homepage are important measures for investors to assess the value of a website [1].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": ", the users\u2019 preferences) is not always available due to visits from casual users, cold start [2] or cookie blocking [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": ", the users\u2019 preferences) is not always available due to visits from casual users, cold start [2] or cookie blocking [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "(iv) We show that LExp is more effective in links selection than two state-of-the-art context-free bandit algorithms, CUCB [4] and EXP3.", "startOffset": 123, "endOffset": 126}, {"referenceID": 4, "context": "M [5], via extensive experiments on two real-world datasets (Sec.", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "(1) Problem (1) is known to be NP-hard [6].", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "S \u2032 = { x \u2208 [0, 1] \u2223\u2223\u2211K i=1 xiAi \u2265 h, \u2211K", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "Without loss of generality, we normalize Ai(t) \u2208 [0, 1] and Bi(t) \u2208 [0, 1].", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "Without loss of generality, we normalize Ai(t) \u2208 [0, 1] and Bi(t) \u2208 [0, 1].", "startOffset": 68, "endOffset": 74}, {"referenceID": 0, "context": ", x t K) be the probabilistic selection vector of the K arms at time t, where xi \u2208 [0, 1] is the probability of selecting of the arm i at time t.", "startOffset": 83, "endOffset": 89}, {"referenceID": 6, "context": "At time t, a set of L \u2264 K arms It \u2208 K is selected via a dependent rounding procedure [7], which guarantees the probability that i \u2208 It is xi at time t (see Sec.", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "2If h = 0, the problem is equivalent to the classic unconstrained multiple play multi-armed bandit problem (MP-MAB) [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "Let x be an arbitrary probabilistic selection vector which satisfies xi \u2208 [0, 1], 1xt = L and aTx \u2265 h.", "startOffset": 74, "endOffset": 80}, {"referenceID": 8, "context": "In this section, we first examine the web links\u2019 multilevel feedback structures in two real-world datasets from the Kaggle Competitions, Avito Context Ad Clicks [9] and Coupon Purchase Prediction [10], referred to as \u201cAd-Clicks\u201d and \u201cCoupon-Purchase\u201d in this paper.", "startOffset": 161, "endOffset": 164}, {"referenceID": 9, "context": "In this section, we first examine the web links\u2019 multilevel feedback structures in two real-world datasets from the Kaggle Competitions, Avito Context Ad Clicks [9] and Coupon Purchase Prediction [10], referred to as \u201cAd-Clicks\u201d and \u201cCoupon-Purchase\u201d in this paper.", "startOffset": 196, "endOffset": 200}, {"referenceID": 3, "context": "Then we conduct a comparative study by applying LExp and two state-of-the-art context-free bandit algorithms, CUCB [4] and EXP3.", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "M [5], to show the effectiveness of LExp in links selection.", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "We normalize the numbers of views of each ad to the interval [0, 1] using min-max scaling.", "startOffset": 61, "endOffset": 67}, {"referenceID": 0, "context": "We then normalize all the browsed times to [0, 1] using min-max scaling and refer to the normalized browsed times as to the CTRs of the coupons.", "startOffset": 43, "endOffset": 49}, {"referenceID": 10, "context": "In particular, for each of the 225 ads in Ad-Clicks, we treat its CTR/1st level feedback as a Bernoulli random variable with the mean CTR taking from Ad-Clicks, and we vary the Phone Request Rate/2nd level reward over time in a similar fashion as a sinusoidal wave (similar to [11]): the 2nd level reward starts from a random value drawn uniformly from 0 to the mean Phone Request Rate taking from Ad-Clicks; then in each time slot, it increases or decreases at rate 10/T until reaching the mean or 0.", "startOffset": 277, "endOffset": 281}, {"referenceID": 11, "context": "One common approach to the links selection problem is to perform A/B testing [12], which splits the traffic for different sets of links on two different web pages, and then evaluate their rewards.", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "[13] first formulated a contextual bandit problem aiming at selecting articles/links that maximize the total number of clicks based on the user-click feedback.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Recently, [14] and [15] incorporated the collaborative filtering method into contextual bandit algorithms using users\u2019 profiles to recommend web links.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "Recently, [14] and [15] incorporated the collaborative filtering method into contextual bandit algorithms using users\u2019 profiles to recommend web links.", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "However, contextual information is not always available due to cold start [2] or blocking of cookie tracking [3].", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "However, contextual information is not always available due to cold start [2] or blocking of cookie tracking [3].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "[5] presented the EXP3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "M bandit algorithm that extends the single-played EXP3 algorithm [16] to multiple-played cases using exponential weights.", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "[4] proposed an algorithm that selects multiple arms with the highest upper confidence bound (UCB) indices.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] presented the multiple-play Thompson Sampling algorithm (MP-TS) for arms with binary rewards.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] proposed a bandit-based ranking algorithm for ranking search queries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Note that the constraint in our constrained bandit model is very different from that in bandit with budgets [19], [20] and bandit with knapsacks [21].", "startOffset": 108, "endOffset": 112}, {"referenceID": 19, "context": "Note that the constraint in our constrained bandit model is very different from that in bandit with budgets [19], [20] and bandit with knapsacks [21].", "startOffset": 114, "endOffset": 118}, {"referenceID": 20, "context": "Note that the constraint in our constrained bandit model is very different from that in bandit with budgets [19], [20] and bandit with knapsacks [21].", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "Finally, our constrained bandit problem is related but different from the bandit model considered in [22] which tries to balance regret and violation.", "startOffset": 101, "endOffset": 105}], "year": 2017, "abstractText": "Selecting the right web links for a website is important because appropriate links not only can provide high attractiveness but can also increase the website\u2019s revenue. In this work, we first show that web links have an intrinsic multilevel feedback structure. For example, consider a 2-level feedback web link: the 1st level feedback provides the Click-Through Rate (CTR) and the 2nd level feedback provides the potential revenue, which collectively produce the compound 2-level revenue. We consider the context-free links selection problem of selecting links for a homepage so as to maximize the total compound 2-level revenue while keeping the total 1st level feedback above a preset threshold. We further generalize the problem to links with n (n\u22652)-level feedback structure. To our best knowledge, we are the first to model the links selection problem as a constrained multi-armed bandit problem and design an effective links selection algorithm by learning the links\u2019 multi-level structure with provable sub-linear regret and violation bounds. We uncover the multi-level feedback structures of web links in two real-world datasets. We also conduct extensive experiments on the datasets to compare our proposed LExp algorithm with two state-of-the-art context-free bandit algorithms and show that LExp algorithm is the most effective in links selection while satisfying the constraint.", "creator": "LaTeX with hyperref package"}}}