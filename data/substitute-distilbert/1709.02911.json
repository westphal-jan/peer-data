{"id": "1709.02911", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2017", "title": "Semi-Supervised Instance Population of an Ontology using Word Vector Embeddings", "abstract": "in many modern day systems such as information extraction and knowledge management agents, ontologies remain a vital role in maintaining the concept hierarchies of multiple selected domain. however, ontology population has become a problematic process due to its nature of quantitative coupling at manual human intervention. with the use of word embeddings in the field of natural language processing, it became a big topic owing largely its ability to bottom up with constraint sensitivity. hence, in this study, we propose a novel way supporting semi - supervised ontology population through numerical validation as the basis. we built several models including traditional benchmark models and new types of models which are based on word embeddings. finally, we ensemble them together to come up with consensus synergistic model with better accuracy. we hoped that our ensemble model still outperform traditional conceptual models.", "histories": [["v1", "Sat, 9 Sep 2017 05:04:19 GMT  (3105kb,D)", "http://arxiv.org/abs/1709.02911v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vindula jayawardana", "dimuthu lakmal", "nisansa de silva", "amal shehan perera", "keet sugathadasa", "buddhi ayesha", "madhavi perera"], "accepted": false, "id": "1709.02911"}, "pdf": {"name": "1709.02911.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Instance Population of an Ontology using Word Vector Embeddings", "authors": ["Vindula Jayawardana", "Dimuthu Lakmal", "Nisansa de Silva", "Amal Shehan Perera", "Keet Sugathadasa", "Buddhi Ayesha", "Madhavi Perera"], "emails": ["vindula.13@cse.mrt.ac.lk"], "sections": [{"heading": null, "text": "keywords: Ontology, Ontology Population, Word Embeddings, word2vec\nI. INTRODUCTION\nIn various computational tasks in many different fields, the use of ontologies is becoming increasingly involved. Many of the research areas such as knowledge engineering and representation, information retrieval and extraction, and knowledge management and agent systems [1] have incorporated the use of ontologies to a greater extent. As defined by Thomas R. Gruber [2], an ontology is a \u201cformal and explicit specification of a shared conceptualization\u201d. Due to the evolving ability of ontologies to overcome limitations in traditional natural language processing methods, the popularity of using ontologies in modern computation tasks are getting increased day by day. For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.\nAccording to Carla Faria et al. [14], ontology population looks for instantiating the constituent elements of an ontology, like properties and non-taxonomic relationships. However, most of the time, ontology populations are done by domain experts and knowledge engineers as a manual process, which is both time consuming and expensive. As majority of the world\u2019s knowledge is encoded in natural language text, automating\nthe population of these ontologies using results obtained from Natural Language Processing (NLP) based analysis of documents, has recently become a major challenge for NLP applications [15].\nIn this study, we propose a novel way for semi-supervised instance population of an ontology using word vector embeddings. Word Embeddings could be identified as a collective name for a set of language modeling and feature learning techniques in natural language processing. The basic idea behind word embedding is based on the concept where words or phrases from the vocabulary are mapped to vectors of real numbers. We use these vectors as a method of arriving at instance population in an ontology. For this purpose, we built an iterative model based on the class representative vector for ontology classes [16]. In our implementation, we built multiple models based on different methodologies. In one model we assigned membership to natural language tokens by distance to the representative vectors. In another, we used word2Vec\u2019s internal dissimilar exclusion method to identify the membership. In another model, we used set expansion as described by [5], for the purpose of ontology population. As each model outputs a set of candidate words for a given class, we then collaborate with domain experts and knowledge engineers to identify the performance of each model.\nSemi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). It has been observed that many machine leaning approaches elucidate considerable improvement in learning accuracy, when unlabeled data is used in conjunction with a small amount of labeled data.\nThe legal context contains jargon which is complex and most of the time impossible to have stored in mind; whether it be an average person or a paralegal, given that it consists terminology derived from ancient Latin terms, as well as various distinctive terminology depending on the category of laws and the geographical settings of practice. Therefore, knowing them manually is rather an impossible task which drove us to select the legal domain for this study of semisupervised ontology population.\nThe rest of this paper is organized as follows: In Section II we review previous studies related to this work. The details\nar X\niv :1\n70 9.\n02 91\n1v 1\n[ cs\n.C L\n] 9\nS ep\n2 01\nof our methodology for semi-supervised instance population of an ontology using word vector embeddings is introduced in Section III. In Section IV, we demonstrate that our proposed methodology produces superior results outperforming traditional approaches. Finally, we conclude and discuss some future works in Section V."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "The following sections depict the background of this study and other related studies."}, {"heading": "A. Ontologies", "text": "Ontologies are mainly used to organize information as a form of knowledge representation in many areas. As defined by Thomas R. Gruber [2], \u2019ontologies are an explicit and formal specifications of the terms in the domain and the relations among them\u2019. Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13]. Ontologies have become common on the semantic iteration of the World-Wide Web. An ontology may model either the world or a part of it as seen by the said area\u2019s viewpoint [5].\nThe basic ground units of an ontology are the Individuals (instances). By grouping these Individuals which can either be concrete objects or abstract objects, the structures called classes are built. A class in an ontology is a representation of a concept, type, category, or a kind. However, these definitions may be altered depending on the domain of the ontology. Often these classes form taxonomic hierarchies among them by subsuming, or being subsumed by, another class."}, {"heading": "B. Word Vector Embeddings", "text": "As first proposed by Tomas Mikolov et al. [19], word embedding systems, are a set of natural language modeling and feature learning techniques, where words from a domain are mapped to vectors to create a model that has a distributed representation of words. Word2vec1[20], GloVe [21], and Latent Dirichlet Allocation (LDA) [22] are the leading Word Vector Embedding systems. However, due to the flexibility and ease of customization, we picked word2vec as the word embedding method for this study.\nWord2vec has been used in many areas due to its capability in coping up with the challenge of preserving the semantic sensitivity of a given context. It has been used in sentiment analysis [23\u201326] and text classification [27]. Gerhard Wohlgenannt et al. [28]\u2019s approach to emulate a simple ontology using word2vec and Harmen Prins [29]\u2019s usage of word2vec extension: node2vec [30], to overcome the problems in vectorization of an ontology, are two major works that have been carried out in relation to ontologies with the use of word2vec. More recently there have been successful studies on using word2vec on the legal domain [16, 31].\n1https://code.google.com/p/word2vec/"}, {"heading": "C. Word Set Expansion", "text": "Word lists that contain closely related sets of words is a critical requirement in machine understanding and processing of natural languages. Creating and maintaining such closely related word lists is a complex process that requires human input and is carried out manually in the absence of tools [5]. The said word-lists usually contain words that are deemed to be homogeneous in the level of abstraction involved in the application. Thus, two words W1 and W2 might belong to a single word-list in one application, but belong to different word-lists in another application. This fuzzy definition and usage is what makes creation and maintenance of these wordlists a complex task.\nDe Silva et al. [5] describe a supervised learning mechanism which employs a word ontology to expand word lists containing closely related sets of words. This study has been an extension of their previous work [17], which was done to enhance the refactoring process of the RelEx2Frame component of OpenCog AGI Framework, by expanding concept variables used in RelEx."}, {"heading": "D. Ontology Population", "text": "Being a knowledge acquisition task, ontology population is inherently a complex activity. Ontology population has been approached by using techniques such as rule based and machine learning. SPRAT [32] combines aspects from traditional named entity recognition, ontology-based information extraction, and relation extraction, in order to identify patterns for the extraction of a variety of entity types and relations between them, and to re-engineer them into concepts and instances in an ontology. Rene Witte et al. [15] has developed a GATE resource called the OwlExporter, that allows to easily map existing NLP analysis pipelines to OWL ontologies, thereby allowing language engineers to create ontology population systems without requiring extensive knowledge of ontology APIs.\nHowever modern day recherches are more focused on semi supervised ontology population due to the nature of less manual intervention."}, {"heading": "E. Semi Supervised Ontology Population", "text": "Although supervised machine learning methodologies have showed promising results when it comes to information extraction, they accumulate more cost for training since they require vast number of labeled training data. As a solution, semi-supervised machine learning methodologies have been introduced, requiring considerably less amount of labeled training data.\nCarlson [33] proposed a semi-supervised learning model to populate instances of a set of target categories and relations of an ontology by providing seed labeled data and a set of constraints which couples classes and relationships of an ontology. Semi-supervised algorithms tend to show unacceptable results due to \u2018semantic drift\u2019 and constraints have been introduced to overcome the issue. Carlson has used \u2018Bootstrapping\u2019 method for semi-supervised learning which starts with a small number\nof labeled data and grows labeled data iteratively, which are chosen from a set of candidates, which is classified using the current semi-supervised model. Three types of constraints have been introduced by Carlson to conform mutual exclusion, type checking, and text features.\nCarlson [34] has expanded coupled semi-supervised learning [33] to never-ending language learning (NELL); an agent that runs forever to extract information from the web and populate them continuously into a knowledge base. A prototype of the system that they have implemented is able to extract noun phrases related to various semantic categories, and semantic relations between categories. Its information extracting ability increases day by day which is evidenced by the ability to extract more information from previous day\u2019s text sources more accurately. Input ontology in the system was included with seed instances for each ontology class and then sub systems which consist of previously described coupled semi supervised methodologies extract candidate instances and relationships from the text corpus. Knowledge Integrator of the system choose strongly supported sets of instances and relations from the candidate set, as new beliefs of the system.\nZhilin Yang [35] has presented a semi supervised learning methodology based on graph embeddings. The system consists of two main sections namely \u2018transductive\u2019 and \u2018inductive\u2019. The \u2018transductive\u2019 approach predicts instances which are already observed in the graph in the training period. In \u2018inductive\u2019 approach, predictions can be made on unobserved instances in the training period. A probabilistic model was developed to learn node embeddings to generate edges in a graph."}, {"heading": "III. METHODOLOGY", "text": "We discuss the methodology used in this research study in the upcoming sections. Each of the following subsections describe a step of our process. An overview of the methodology we propose is illustrated in Fig. 1."}, {"heading": "A. Ontology Creation", "text": "For the ontology creation, we focused on the consumer protection law domain and created a legal ontology, based on Findlaw [36] as the reference. However, for the sake of clarity of this paper, we extract a sub-ontology from it and use it to explain the methodology to make the process simple and intuitive to understand. In selecting a part of the ontology, we mainly focused on more sophisticated relationships and taxonomic presences. An overview of the selected part of ontology is illustrated in Fig. 2. After the creation of subontology, we manually populated the ontology with seed instances with collaboration of domain experts and knowledge engineers."}, {"heading": "B. Training word Embeddings", "text": "The word embeddings method used in this study was built using a word2vec model. We obtained a large legal text corpus from Findlaw [36] and built a word2vec model using the corpus. The reason for selecting word2vec word embedding\nfor this study is the success demonstrated by other studies such as [16] and [31] in the legal domain that uses word2vec as the word embedding method. The text corpus consisted of legal cases under 78 law categories. In creating the legal text corpus, we used the Stanford CoreNLP for preprocessing the text with tokenizing and sentence splitting. Following are the important parameters we specified in training the model. \u2022 size (dimensionality): 200 \u2022 context window size: 10 \u2022 learning model: CBOW \u2022 min-count: 5 \u2022 training algorithm: hierarchical softmax"}, {"heading": "C. Deriving Representative Class Vectors", "text": "Ontology classes are sets of homogeneous instance objects that can be converted to a vector space by word vector embeddings. A methodology to derive a representative vector for ontology classes, whose instances were mapped to a vector space is presented in [16]. We followed the same approach and started by deriving five candidate vectors which are then used to train a machine learning model that would calculate a representative vector for each of the classes in the selected sub-ontology shown in Fig. 2. In the following sections, we describe in-depth, the manner in how we used this derived class vectors in our proposed methodology.\nD. Instance Corpus for Ontology Population\nIn order to perform semi-supervised ontology population, we used legal cases from Findlaw [36] to create an instance corpus. We performed Stanford CoreNLP based preprocessing on the raw text with tokenizing and sentence splitting to generate the instance corpus. This legal corpus was used in the subsequent models for the purpose of ontology population."}, {"heading": "E. Candidate Model Building", "text": "Based on the aforementioned components, we built five candidate models for semi-supervised population of the ontology. The five models are illustrated below: \u2022 Membership by distance model (M1) \u2022 Membership by dissimilar exclusion model (M2) \u2022 Set expansion based model (M3) \u2022 Semi-supervised K-Means clustering based model (M4) \u2022 Semi-supervised hierarchical clustering based model\n(M5)\n1) Membership by Distance Model (M1): In this model, the candidate vectors for the ontology were generated from the instance corpus based on the minimum distance to the representative class vector derived in Section III-C. In taking the vector similarity, we used cosine similarity. Given an instance i which has the vector embedding Xi, Equation 1 describes which class the particular instance belongs to.\nCM1 =\n{ j \u2223\u2223\u2223\u2223\u2223 argmaxcj\u2208C { Xi.cj |Xi||cj | }} (1)\nHere, the set C denotes the set of representative class vectors. CM1 is the selected class index of the instance i out of class set C.\n2) Membership by dissimilar exclusion model (M2): In this model, we used the word2vec based dissimilar exclusion method in identifying the membership of a particular instance to a given class. This is a utilization of an internal method of word2vec where given a set of members, it would return the member that should be removed from the set in order to increase the set cohesion. For example, given the set of instances: breakfast, cereal, dinner and lunch, the word2vec dissimilar exclusion method would identify the instance cereal as the item that should be removed from the set to increase the set cohesion. We define this method as shown in Equation 2 where S is the set provided and e is the member selected to be excluded.\ne = Exclusion(S) (2)\nWe used the Equation 3 to decide whether the instance i should belong to class j. Here, Sj is the seed set of class j and Xi is the vector representation of the instance i. If the value Ei,j gets evaluated to TRUE we declare that instance i should belong to class j under model M2.\nEi,j = { e \u2208 Sj \u2223\u2223\u2223\u2223e = Exclusion(Sj \u2229Xi)} (3) When using the aforementioned method in identifying the membership of an instance, there is a possibility of getting more than one class for a given instance as a possible parent class. Hence;\nCM2 = { Ck \u2223\u2223\u2223 0 < k 6 N} (4) Here in Equation 4, CM2 is the set of classes for a given instance i. Ck denote the common representation of those set of classes and N denotes the total number of classes we have in the ontology.\n3) Set Expansion Based Model (M3): For the purpose of set expansion based model, we selected the algorithm presented in [5], which was built on the earlier algorithm described in [17]. The rationale behind this selection is the fact that as per [5], WordNet [6] based linguistic processes are reliable due to the fact that the WordNet lexicon was built on the knowledge of expert linguists.\nIn this model, the idea is to increase the ontology class instances based on a WordNet hierarchy based expansion. Simply put, it discovers the WordNet synsets pertaining to the seed words and proceeds up the hierarchy to find the minimum common ancestors for each of the senses of the words. Next, the most common word sense is selected by majority. The relevant rooted tree is extracted and the gazetteer list of that rooted synset tree is created. The gazetteer list is subjected to a set subtraction of the original seed set. The set intersection of the remaining set with the candidate word set is declared to be the word set assigned to the given class. However, it should be noted that as we showed in model M2, after running the set expansion algorithm, one candidate instance may be tentatively assigned to more than one class.\n4) Semi-Supervised K-Means Clustering Based Model (M4): Out of the models proposed in this study so far, this model is the first semi-supervised model. First, the seed instances are put together with the unlabeled data from instance corpus. Let Nlabeled be the number of labeled (seed) instances\nand Nunlabeled be the total number of unlabeled instances. Thus, by mixing up the labeled and unlabeled data, we get a total of Nlabeled + Nunlabeled number of instances. Next all the instances are subjected to the k-means algorithm where k is selected to the the same, as the number of classes in the ontology.\nOnce the k-means clustering is finished, primary class cluster assignment for cluster L is done by voting of seed instances according to Equation 5, where C is the set of ontology classes, cj is the jth class from C, yi is the ith instance from L, and di is defined according to Equation 6.\nCl =\n{ j \u2223\u2223\u2223\u2223\u2223 argmaxcj\u2208C { \u2211 di yi\u2208L }} (5)\ndi = { 1 if yi \u2208 cj 0 otherwise\n(6)\nAt this point, it should be noted that there can be three situations where it is possible to not get a cl value assigned to some class L by Equation 5 without ambiguity: (1) L not having any seed instances to vote. (2) L has multiple seed instances but the majority voting ended in a tie. (3) Two (or more) clusters, claim the same class. To solve these problems we defined Equation 7, which selected the unassigned class that is closest to an unassigned cluster. Here, an unassigned cluster L\u2032 is considered. C \u2032 is the set of representative class vectors of unassigned classes. Cl\u2032 is the selected class index of the cluster L\u2032.\nCl\u2032 =\n{ j \u2223\u2223\u2223\u2223\u2223 argmaxcj\u2208C\u2032 { \u2211\nxi\u2208L\u2032\n{ Xi.cj |Xi||cj | }}} (7)\nThe first problem to be solved is the problem of L having multiple seed instances but the majority voting ending in a tie. In this case the C \u2032 of Equation 7 is limited to the set intersection of tied classes and unassigned classes. Next, the problem of Two (or more) clusters, claiming the same class is solved. In this case C \u2032 of Equation 7 is limited to the contested class. These steps are repeated until there is an iteration where there are no new assignments. Finally, all the remaining unassigned classes are put in c\u2032 and Equation 7 is executed repetitively with tie breaking, done with precedence until all the clusters are uniquely assigned to some class.\n5) Semi-Supervised Hierarchical Clustering Based Model (M5): The next model being used is a semi-supervised method based on hierarchical clustering. Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. We built a model which creates such hierarchy of clusters using the word embeddings taken from the word2vec model,of the entire corpus similar to the process in Section III-E4. In this model, we extracted the slice of hierarchical clusters such that the number of clusters in the slice is equal to the number of classes in the sub-ontology. Next, the clusterclass assignment was done similar to the process in III-E4."}, {"heading": "F. Model Accuracy Measure", "text": "After building the aforementioned models, we evaluated the accuracy of each model. As each model outputs an unordered set of suggested words, we sorted them using the Neural Network, trained according to the methodology proposed in [31]. Upon completing the sorting, we applied a threshold to select the best candidates. Finally, we measured each model\u2019s accuracy as below. For this task, we involved with domain experts and knowledge engineers. For a given model Mi in the context of class j:\nPrecisionMi,j = WMi,j \u2229Wj,g\nWMi,j (8)\nRecallMi,j = WMi,j \u2229Wj,g\nWj,g (9)\nHere, WMi denotes words by the model Mi and Wj,g denotes the set of the words proposed by domain experts that needs to be the golden standard for class j. The model precision and model recall of Mi was calculated by averaging the class values for precision and recall of those models.\nF1Mi = 2. P recisionMi .RecallMi PrecisionMi +RecallMi\n(10)"}, {"heading": "G. Ensemble Model", "text": "Next, we came up with an ensemble model based on the models identified earlier. In the task of creating the ensemble model, we allocated a candidate weight for each model based on each model\u2019s F1 measure as calculated in the previous step.\nLet Mi be a model, out of the obtained models, and let F1Mi be the F1 measure of model Mi. Hence with the models in consideration, weight of the model Wi is calculated as shown in equation 11, where p is the total number of models.\nWi = F1i p\u2211\ni=1\nF1i\n(11)\nAs identified above, upon calculating the weight of each model, we created the ensemble model as shown in equation 12. Given an unlabeled instance Y , let Mensemble be a p \u00d7 n matrix where n denotes the number of classes in the ontology and p denotes the number of basic models. Each\ncolumn of the matrix corresponds to a class in the ontology and each row corresponds to a model, while each mi,j is derived from Equation 13.\nMensemble =  m1,1 m1,2 . . . m1,n m2,1 m2,2 . . . m2,n m3,1 m3,2 . . . m3,n . . . . . . . . . . . . mp,1 mp,2 . . . mp,n  (12)\nmi,j = { 1 if Y \u2208Mi 0 otherwise\n(13)\nLetMweights be the p length vector which defines the weights of each model calculated by equation 11.\nMweights = [ w1 w2 w3 . . . wp ] (14)\nThen we calculate the total score vector for the instance Y by,\nS = Mweights.Mensemble (15)\nHere, S is the score vector of size n, where element i in the vector denotes the total score for instance Y for the membership in Class Ci. Next, we selected the class with the highest membership score as the parent class of instance Y . It is illustrated in Equation 16.\nCMensemble =\n{ i \u2223\u2223\u2223\u2223 argmax SCi\u2208S { SCi }} (16)\nWith that, we got the final class of the instance Y . Hence, we populate that selected class with the instance Y ."}, {"heading": "IV. RESULTS", "text": "In testing our ensemble model, we used another instance corpus. In this corpus, we subdivided in the order of 70%, 20%, and 10% as the training set, the validation set, and the test set respectively. Training set was used in training the models individually. Validation set was used to fine tune the models. Finally, the test set was used in verifying the accuracy of the models. We report our findings below in the table I, where we compare the individual models: membership by distance model(M1), membership by dissimilar exclusion model(M2), set expansion based model(M3), kmeans clustering based model(M4), hierarchical clustering based model(M5) and the ensemble model as a whole. In Fig. 4, we compare the precision, recall and F1 of each of the candidate models along with the ensemble model.\nIn defining the ensemble model, Equation 17 defines the calculated weights of each model in the order of models M1 to M5. These calculations are based on the above mentioned training set.\nMweights = [ 0.15 0.27 0.33 0.13 0.12 ] (17)\nAs can be seen from Table I, our ensemble model\u2019s F1 has been improved by 0.30, compared to the best of the\ncandidate models. Hence, from the results obtained, as a proof of concept, we can demonstrate that word embeddings can be used effectively in semi-supervised ontology population."}, {"heading": "V. CONCLUSION AND FUTURE WORKS", "text": "Through this work we demonstrated the use of word embeddings on semi-supervised ontology population. We mainly focused on semi-supervised population which basically falls between the supervised population and unsupervised population. The main motive behind making the process semisupervised is to reduce the level of manual interventions in ontology populations while maintaining a considerable amount of accuracy. As shown in the results, our ensemble model outperforms the five individual models in populating the selected legal ontology. The findings in this study is mainly important in two ways as mentioned below.\nFirstly, an important part of the ontology engineering cycle is the ability to keep a handcrafted ontology up to date. Through the semi-supervised ontology population we can reduce the hassle involved in manual intervention to keep the ontology updated.\nSecondly, there is novelty in the methodology proposed in our study. We proved that, since word embeddings map words or phrases from the vocabulary to vectors of real numbers based on the semantic context, a methodology based upon it can yield more sophisticated results when it comes to context sensitive tasks like ontology population. This indeed is a step up from the traditional information extraction based ontology population and maintenance processes, towards new horizons.\nWe can improve the methodology proposed, to yield better accuracy performances. For an example, we only considered\nthe single word instances in populating the ontology using the defined models. However, in some of the scenarios, phrases also could be instances of ontology classes. Hence, it is important to convert phases to vectors and use them in the methodology as well. Also, as illustrated with models M4 and M5, we can perform more sophisticated semi-supervised ontology populations based on the concept of this study with more improvements. We keep them to be the future works of this study."}], "references": [{"title": "Formal ontology and information systems,", "author": ["N. Guarino"], "venue": "Proceedings of FOIS\u201998, Trento, Italy,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "A translation approach to portable ontology specifications,", "author": ["T.R. Gruber"], "venue": "Knowledge Acquisition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "N", "author": ["X.-Q. Yang"], "venue": "Sun, T.-L. Sun et al., \u201cThe application of latent semantic indexing and ontology in text classification,\u201d International Journal of Innovative Computing, Information and Control, vol. 5, no. 12, pp. 4491\u20134499", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Safs3 algorithm: Frequency statistic and semantic similarity based semantic classification use case,", "author": ["N. de Silva"], "venue": "Advances in ICT for Emerging Regions (ICTer),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "and M", "author": ["N. De Silva", "A. Perera"], "venue": "Maldeniya, \u201cSemisupervised algorithm for concept ontology based word set expansion,\u201d Advances in ICT for Emerging Regions (ICTer), 2013 International Conference on, pp. 125\u2013131", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "and K", "author": ["G.A. Miller", "R. Beckwith", "C. Fellbaum", "D. Gross"], "venue": "J. Miller, \u201cIntroduction to wordnet: An on-line lexical database,\u201d International journal of lexicography, vol. 3, no. 4, pp. 235\u2013244", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Nouns in wordnet: a lexical inheritance system,", "author": ["G.A. Miller"], "venue": "International journal of Lexicography,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1990}, {"title": "WordNet", "author": ["C. Fellbaum"], "venue": "Wiley Online Library", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "and N", "author": ["I. Wijesiri", "M. Gallage", "B. Gunathilaka", "M. Lakjeewa", "D.C. Wimalasuriya", "G. Dias", "R. Paranavithana"], "venue": "De Silva, \u201cBuilding a wordnet for sinhala,\u201d in 7th Global Wordnet Conference", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Y", "author": ["J. Huang", "F. Gutierrez", "H.J. Strachan", "D. Dou", "W. Huang", "B. Smith", "J.A. Blake", "K. Eilbeck", "D.A. Natale"], "venue": "Lin et al., \u201cOmnisearch: a semantic search system based on the ontology for microrna target (omit) for micrornatarget gene interaction data,\u201d Journal of biomedical semantics, vol. 7, no. 1, p. 1", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["J. Huang", "K. Eilbeck", "B. Smith", "J.A. Blake", "D. Dou", "W. Huang", "D.A. Natale", "A. Ruttenberg", "J. Huan"], "venue": "T. Zimmermann et al., \u201cThe development of non-coding rna ontology,\u201d International journal of data mining and bioinformatics, vol. 15, no. 3, pp. 214\u2013232", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Ontology-based information extraction: An introduction and a survey of  current approaches,", "author": ["D.C. Wimalasuriya", "D. Dou"], "venue": "Journal of Information Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Discovering inconsistencies in pubmed abstracts through ontology-based information extraction,", "author": ["N. de Silva", "D. Dou", "J. Huang"], "venue": "Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics. ACM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Ivo Serrab", "author": ["R.G. Carla Fariaa"], "venue": "\u201cA domain-independent process for automatic ontology population from text,\u201d Science of Computer Programming", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "N", "author": ["V. Jayawardana", "D. Lakmal"], "venue": "de Silva, A. S. Perera, K. Sugathadasa, and B. Ayesha, \u201cDeriving a representative vector for ontology classes with instance word vector embeddings,\u201d arXiv preprint arXiv:1706.02909", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Semap-mapping dependency relationships into semantic frame relationships,", "author": ["N. de Silva", "C. Fernando", "M. Maldeniya", "D. Wijeratne", "A. Perera", "B. Goertzel"], "venue": "ERU Research Symposium,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Subject specific stream classification preprocessing algorithm for twitter data stream,", "author": ["N. de Silva", "D. Maldeniya", "C. Wijeratne"], "venue": "arXiv preprint arXiv:1705.09995,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "and J", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado"], "venue": "Dean, \u201cDistributed representations of words and phrases and their compositionality,\u201d Advances in neural information processing systems, pp. 3111\u20133119", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "and J", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado"], "venue": "Dean, \u201cEfficient estimation of word representations in vector space,\u201d arXiv preprint arXiv:1301.3781", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "and C", "author": ["J. Pennington", "R. Socher"], "venue": "D. Manning, \u201cGlove: Global vectors for word representation.\u201d in EMNLP, vol. 14", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "and C", "author": ["R. Das", "M. Zaheer"], "venue": "Dyer, \u201cGaussian lda for topic models with word embeddings.\u201d in ACL (1)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "and B", "author": ["D. Tang", "F. Wei", "N. Yang", "M. Zhou", "T. Liu"], "venue": "Qin, \u201cLearning sentiment-specific word embedding for twitter sentiment classification,\u201d Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, vol. 1, pp. 1555\u20131565", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "and Z", "author": ["B. Xue", "C. Fu"], "venue": "Shaobin, \u201cStudy on sentiment computing and classification of sina weibo with word2vec,\u201d Big Data (BigData Congress), 2014 IEEE International Congress on. IEEE, pp. 358\u2013363", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "and Y", "author": ["D. Zhang", "H. Xu", "Z. Su"], "venue": "Xu, \u201cChinese comments sentiment classification based on word2vec and svm perf,\u201d Expert Systems with Applications, vol. 42, no. 4, pp. 1857\u20131863", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Sentiment analysis of citations using word2vec,", "author": ["H. Liu"], "venue": "arXiv preprint arXiv:1704.00177,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "and Y", "author": ["J. Lilleberg", "Y. Zhu"], "venue": "Zhang, \u201cSupport vector machines and word2vec for text classification with se-  mantic features,\u201d in Cognitive Informatics & Cognitive Computing (ICCI* CC), 2015 IEEE 14th International Conference on. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Using word2vec to build a simple ontology learning system", "author": ["G. Wohlgenannt", "F. Minic"], "venue": "Available at: http: //ceur-ws.org/Vol-1690/paper37.pdf. Accessed:", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2017}, {"title": "node2vec: Scalable feature learning for networks,", "author": ["A. Grover", "J. Leskovec"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "N", "author": ["K. Sugathadasa", "B. Ayesha"], "venue": "de Silva, A. S. Perera, V. Jayawardana, D. Lakmal, and M. Perera, \u201cSynergistic union of word2vec and lexicon for domain specific semantic similarity,\u201d arXiv preprint arXiv:1706.01967", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Sprat: a tool for automatic semantic pattern-based ontology population,", "author": ["A.F. Diana Maynard", "W. Peters"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "E", "author": ["A. Carlson", "J. Betteridge", "R.C. Wang"], "venue": "R. Hruschka, Jr., and T. M. Mitchell, \u201cCoupled semi-supervised learning for information extraction,\u201d WSDM \u201910 Proceedings of the third ACM international conference on Web search and data mining, pp. 101\u2013110", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "E", "author": ["A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles"], "venue": "R. Hruschka, Jr., and T. M. Mitchell, \u201cToward an architecture for never-ending language learning,\u201d Proceeding AAAI\u201910 Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, pp. 1306\u20131313", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "William W", "author": ["R.S. Zhilin Yang"], "venue": "Cohen, \u201cRevisiting semisupervised learning with graph embeddings,\u201d Proceedings of International Conference on Machine Learning", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Many of the research areas such as knowledge engineering and representation, information retrieval and extraction, and knowledge management and agent systems [1] have incorporated the use of ontologies to a greater extent.", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "Gruber [2], an ontology is a \u201cformal and explicit specification of a shared conceptualization\u201d.", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 36, "endOffset": 42}, {"referenceID": 3, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 36, "endOffset": 42}, {"referenceID": 4, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 103, "endOffset": 108}, {"referenceID": 6, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 103, "endOffset": 108}, {"referenceID": 7, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 103, "endOffset": 108}, {"referenceID": 8, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 103, "endOffset": 108}, {"referenceID": 9, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 141, "endOffset": 149}, {"referenceID": 10, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 141, "endOffset": 149}, {"referenceID": 11, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 178, "endOffset": 186}, {"referenceID": 12, "context": "For an example, text classification [3, 4], word set expansions [5], linguistic information management [6\u20139], medical information management [10, 11], and information extraction [12, 13] emphasize the growing popularity of the ontology based computations and processing.", "startOffset": 178, "endOffset": 186}, {"referenceID": 13, "context": "[14], ontology population looks for instantiating the constituent elements of an ontology, like properties and non-taxonomic relationships.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "For this purpose, we built an iterative model based on the class representative vector for ontology classes [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "In another model, we used set expansion as described by [5], for the purpose of ontology population.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Gruber [2], \u2019ontologies are an explicit and formal specifications of the terms in the domain and the relations among them\u2019.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 123, "endOffset": 140}, {"referenceID": 4, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 123, "endOffset": 140}, {"referenceID": 8, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 123, "endOffset": 140}, {"referenceID": 15, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 123, "endOffset": 140}, {"referenceID": 16, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 123, "endOffset": 140}, {"referenceID": 14, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 9, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 161, "endOffset": 173}, {"referenceID": 10, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 161, "endOffset": 173}, {"referenceID": 12, "context": "Ontologies have been expanding out from the realm of Artificial-Intelligence to domain specific tasks such as: Linguistics [4, 5, 9, 17, 18], Law [16], Medicine [10, 11, 13].", "startOffset": 161, "endOffset": 173}, {"referenceID": 4, "context": "An ontology may model either the world or a part of it as seen by the said area\u2019s viewpoint [5].", "startOffset": 92, "endOffset": 95}, {"referenceID": 17, "context": "[19], word embedding systems, are a set of natural language modeling and feature learning techniques, where words from a domain are mapped to vectors to create a model that has a distributed representation of words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Word2vec1[20], GloVe [21], and Latent Dirichlet Allocation (LDA) [22] are the leading Word Vector Embedding systems.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "Word2vec1[20], GloVe [21], and Latent Dirichlet Allocation (LDA) [22] are the leading Word Vector Embedding systems.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "Word2vec1[20], GloVe [21], and Latent Dirichlet Allocation (LDA) [22] are the leading Word Vector Embedding systems.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "It has been used in sentiment analysis [23\u201326] and text classification [27].", "startOffset": 39, "endOffset": 46}, {"referenceID": 22, "context": "It has been used in sentiment analysis [23\u201326] and text classification [27].", "startOffset": 39, "endOffset": 46}, {"referenceID": 23, "context": "It has been used in sentiment analysis [23\u201326] and text classification [27].", "startOffset": 39, "endOffset": 46}, {"referenceID": 24, "context": "It has been used in sentiment analysis [23\u201326] and text classification [27].", "startOffset": 39, "endOffset": 46}, {"referenceID": 25, "context": "It has been used in sentiment analysis [23\u201326] and text classification [27].", "startOffset": 71, "endOffset": 75}, {"referenceID": 26, "context": "[28]\u2019s approach to emulate a simple ontology using word2vec and Harmen Prins [29]\u2019s usage of word2vec extension: node2vec [30], to overcome the problems in vectorization of an ontology, are two major works that have been carried out in relation to ontologies with the use of word2vec.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28]\u2019s approach to emulate a simple ontology using word2vec and Harmen Prins [29]\u2019s usage of word2vec extension: node2vec [30], to overcome the problems in vectorization of an ontology, are two major works that have been carried out in relation to ontologies with the use of word2vec.", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "More recently there have been successful studies on using word2vec on the legal domain [16, 31].", "startOffset": 87, "endOffset": 95}, {"referenceID": 28, "context": "More recently there have been successful studies on using word2vec on the legal domain [16, 31].", "startOffset": 87, "endOffset": 95}, {"referenceID": 4, "context": "Creating and maintaining such closely related word lists is a complex process that requires human input and is carried out manually in the absence of tools [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 4, "context": "[5] describe a supervised learning mechanism which employs a word ontology to expand word lists containing closely related sets of words.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "This study has been an extension of their previous work [17], which was done to enhance the refactoring process of the RelEx2Frame component of OpenCog AGI Framework, by expanding concept variables used in RelEx.", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "SPRAT [32] combines aspects from traditional named entity recognition, ontology-based information extraction, and relation extraction, in order to identify patterns for the extraction of a variety of entity types and relations between them, and to re-engineer them into concepts and instances in an ontology.", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "Carlson [33] proposed a semi-supervised learning model to populate instances of a set of target categories and relations of an ontology by providing seed labeled data and a set of constraints which couples classes and relationships of an ontology.", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "Carlson [34] has expanded coupled semi-supervised learning [33] to never-ending language learning (NELL); an agent that runs forever to extract information from the web and populate them continuously into a knowledge base.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "Carlson [34] has expanded coupled semi-supervised learning [33] to never-ending language learning (NELL); an agent that runs forever to extract information from the web and populate them continuously into a knowledge base.", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "Zhilin Yang [35] has presented a semi supervised learning methodology based on graph embeddings.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "The reason for selecting word2vec word embedding for this study is the success demonstrated by other studies such as [16] and [31] in the legal domain that uses word2vec as the word embedding method.", "startOffset": 117, "endOffset": 121}, {"referenceID": 28, "context": "The reason for selecting word2vec word embedding for this study is the success demonstrated by other studies such as [16] and [31] in the legal domain that uses word2vec as the word embedding method.", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "A methodology to derive a representative vector for ontology classes, whose instances were mapped to a vector space is presented in [16].", "startOffset": 132, "endOffset": 136}, {"referenceID": 4, "context": "3) Set Expansion Based Model (M3): For the purpose of set expansion based model, we selected the algorithm presented in [5], which was built on the earlier algorithm described in [17].", "startOffset": 120, "endOffset": 123}, {"referenceID": 15, "context": "3) Set Expansion Based Model (M3): For the purpose of set expansion based model, we selected the algorithm presented in [5], which was built on the earlier algorithm described in [17].", "startOffset": 179, "endOffset": 183}, {"referenceID": 4, "context": "The rationale behind this selection is the fact that as per [5], WordNet [6] based linguistic processes are reliable due to the fact that the WordNet lexicon was built on the knowledge of expert linguists.", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "The rationale behind this selection is the fact that as per [5], WordNet [6] based linguistic processes are reliable due to the fact that the WordNet lexicon was built on the knowledge of expert linguists.", "startOffset": 73, "endOffset": 76}, {"referenceID": 28, "context": "As each model outputs an unordered set of suggested words, we sorted them using the Neural Network, trained according to the methodology proposed in [31].", "startOffset": 149, "endOffset": 153}], "year": 2017, "abstractText": "In many modern day systems such as information extraction and knowledge management agents, ontologies play a vital role in maintaining the concept hierarchies of the selected domain. However, ontology population has become a problematic process due to its nature of heavy coupling with manual human intervention. With the use of word embeddings in the filed of natural language processing, it became a popular topic due to its ability to cope up with semantic sensitivity. Hence, in this study we propose a novel way of semi-supervised ontology population through word embeddings as the basis. We built several models including traditional benchmark models and new types of models which are based on word embeddings. Finally, we ensemble them together to come up with a synergistic model with better accuracy. We demonstrate that our ensemble model can outperform the individual models. keywords: Ontology, Ontology Population, Word Embeddings, word2vec", "creator": "LaTeX with hyperref package"}}}