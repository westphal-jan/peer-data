{"id": "1709.06009", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents", "abstract": "the arcade learning environment ( ale ) is an evaluation platform that poses the challenge of building ai agents or general competency across dozens simultaneous dos 2600 games. it supports a variety of software problem settings and it has been receiving increasing attention from the scientific community, leading to some high - profile success stories such as the much developed deep q - networks ( dqn ). in this article we take a big picture look at case new ale is being used by the research community. we show how challenging the evaluation methodologies in the ale have become with time, and highlight some key concerns when focusing agents in exploring ale. we use this discussion how find clear methodological best practices and assign new baseline results using these best practices. to further the progress underlying her field, we provide a new version of the ale that supports multiple game modes and provides a form of stochasticity vs call sticky actions. we conclude this big picture look by revisiting challenges posed when the ale was introduced, summarizing the state - of - our - art in various problems and highlighting others that remain open.", "histories": [["v1", "Mon, 18 Sep 2017 15:32:57 GMT  (1143kb,D)", "http://arxiv.org/abs/1709.06009v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marlos c machado", "marc g bellemare", "erik talvitie", "joel veness", "matthew hausknecht", "michael bowling"], "accepted": false, "id": "1709.06009"}, "pdf": {"name": "1709.06009.pdf", "metadata": {"source": "CRF", "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents", "authors": ["Marlos C. Machado", "Marc G. Bellemare", "Erik Talvitie", "Joel Veness", "Matthew Hausknecht", "Michael Bowling"], "emails": ["machado@ualberta.ca", "bellemare@google.com", "erik.talvitie@fandm.edu", "aixi@google.com", "matthew.hausknecht@microsoft.com", "mbowling@ualberta.ca"], "sections": [{"heading": "1. Introduction", "text": "The Arcade Learning Environment (ALE) is both a challenge problem and a platform for evaluating general competency in artificial intelligence (AI). Originally proposed by Bellemare, Naddaf, Veness, and Bowling (2013), the ALE makes available dozens of Atari 2600 games for agent evaluation. The agent is expected to do well in as many games as possible without game-specific information, generally perceiving the world through a video stream. Atari 2600 games are excellent environments for evaluating AI agents for three main reasons: 1) they are varied enough to provide multiple different tasks, requiring\n\u2020 Work performed at DeepMind.\nar X\niv :1\n70 9.\n06 00\n9v 1\n[ cs\ngeneral competence, 2) they are interesting and challenging for humans, and 3) they are free of experimenter\u2019s bias, having been developed by an independent party.\nThe usefulness of the ALE is reflected in the amount of attention it has received from the scientific community. The number of papers using the ALE as a testbed has exploded in recent years. This has resulted in some high-profile success stories, such as the much publicized Deep Q-Networks (DQN), the first algorithm to achieve human-level control in a large fraction of Atari 2600 games (Mnih et al., 2015). This interest has also led to the first dedicated workshop on the topic, the AAAI Workshop on Learning for General Competency in Video Games (Albrecht et al., 2015). Several of the ideas presented in this article were first discussed at this workshop, such as the need for standardizing evaluation and for distinguishing open-loop behaviours from closed-loop ones.\nGiven the ALE\u2019s increasing importance in the AI literature, this article aims to be a \u201ccheck-up\u201d for the Arcade Learning Environment, taking a big picture look at how the ALE is being used by researchers. The primary goal is to highlight some subtle issues that are often overlooked and propose some small course corrections to maximize the scientific value of future research based on this testbed. The ALE has incentivized the AI community to build more generally competent agents. The lessons learned from that experience may help further that progress and may inform best practices as other testbeds for general competency are developed (e.g., Levine et al., 2013; Beattie et al., 2016; Brockman et al., 2016; Johnson et al., 2016).\nThe main contributions of this article are: 1) To discuss the different evaluation methods present in the literature and to identify, for the typical reinforcement learning setting, some methodological best practices gleaned from experience with the ALE (Sections 3 and 4). 2) To address concerns regarding the deterministic dynamics of previous versions of the platform, by introducing a new version of the ALE that supports a form of stochasticity we call sticky actions (Section 5). 3) To provide new benchmark results in the reinforcement learning setting that ease comparison and reproducibility of experiments in the ALE. These benchmark results also encourage the development of sample efficient algorithms (Section 6). 4) To revisit challenges posed when the ALE was introduced, summarizing the state-of-theart in various problems and highlighting problems that are currently open (Section 7). 5) To introduce a new feature to the platform that allows existent environments to be instantiated in multiple difficult levels and game modes (Section 7.4.1)"}, {"heading": "2. Background", "text": "In this section we introduce the formalism behind reinforcement learning (Sutton & Barto, 1998), as well as how it is instantiated in the Arcade Learning Environment. We also present the two most common value function representations used in reinforcement learning for Atari 2600 games: linear approximation and neural networks. As a convention, we indicate scalar-valued random variables by capital letters (e.g., St, Rt), vectors by bold lowercase letters (e.g., \u03b8,\u03c6), functions by non-bold lowercase letters (e.g., v, q), and sets with a calligraphic font (e.g., S,A)."}, {"heading": "2.1 Setting", "text": "We consider an agent interacting with its environment in a sequential manner, aiming to maximize cumulative reward. It is often assumed that the environment satisfies the Markov property and is modeled as a Markov decision process (MDP). An MDP is formally defined as a 4-tuple \u3008S,A, p, r\u3009. Starting from state S0 \u2208 S, at each step the agent takes an action At \u2208 A, to which the environment responds with a state St \u2208 S, according to a transition probability kernel p(s\u2032 | s, a) .= Pr(St+1 = s\u2032 |St = s,At = a), and a reward Rt+1, which is generated by the function r(s, a, s\u2032) . = r(St = s,At = a, St+1 = s\n\u2032) \u2208 R. In the context of the ALE, an action is the composition of a joystick direction and an optional button press. The agent observes a reward signal, which is typically the change in the player\u2019s score (the difference in score between the previous time step and the current time step), and an observation Ot \u2208 O of the environment. This observation can be a single 210 \u00d7 160 image and/or the current 1024-bit RAM state. Because a single image typically does not satisfy the Markov property, we distinguish between observations and the environment state, with the RAM data being the real state of the emulator.1 A frame (as a unit of time) corresponds to 1/60th of a second, the time interval between two consecutive images rendered to the television screen. The ALE is deterministic: given a particular emulator state s and a joystick input a there is a unique resulting next state s\u2032, that is, p(s\u2032 | s, a) = 1. We will return to this important characteristic in Section 5.\nAgents interact with the ALE in an episodic fashion. An episode begins by resetting the ALE to its initial configuration, and ends at a natural endpoint of a game\u2019s playthrough (this often corresponds to the player losing their last life). The primary measure of an agent\u2019s performance is the score achieved during an episode, namely the undiscounted sum of rewards for that episode. While this performance measure is quite natural, it is important to realize that score, in and of itself, is not necessarily an indicator of AI progress. In some games, agents can maximize their score by \u201cgetting stuck\u201d in a loop of \u201csmall\u201d rewards, ignoring what human players would consider to be the game\u2019s main goal. Nevertheless, score is currently the most common measure of agent performance so we focus on it here.\nBeyond the minimal interface described above, almost all agents designed for the ALE implement some form of reward normalization. The magnitude of rewards can vary wildly across games; transforming the reward to fit into a roughly uniform scale makes it more feasible to find game-independent meta-parameter settings. For instance, some agents divide every reward by the magnitude of the first non-zero reward value encountered, implicitly assuming that the first non-zero reward is \u201ctypical\u201d (Bellemare et al., 2013). Others account only for the sign of the reward, replacing each reward value with -1, 0, or 1, accordingly (Mnih et al., 2015). Most agents also employ some form of hard-coded preprocessing to simplify the learning and acting process. We briefly review the three most common preprocessing steps as they will play a role in the subsequent discussion. 1) Frame skipping (Naddaf, 2010) restricts the agent\u2019s decision points by repeating a selected action for k consecutive frames. Frame skipping results in a simpler reinforcement learning problem and speeds up execution; values of k = 4 and k = 5 have been commonly used in the literature. 2) Color averaging (Bellemare et al., 2013) and frame pooling (Mnih et al., 2015) are\n1The internal emulator state also includes registers and timers, but the RAM information and joystick inputs are sufficient to infer the next emulator state.\ntwo image-based mechanisms to flatten two successive frames into a single one in order to reduce visual artifacts resulting from limitations of the Atari 2600 hardware \u2013 by leveraging the slow decay property of phosphors on 1970s televisions, objects on the screen could be displayed every other frame without compromising the game\u2019s visual aspect (Montfort & Bogost, 2009). Effectively, color averaging and frame pooling remove the most benign form of partial observability in the ALE. Finally, 3) frame stacking (Mnih et al., 2015) concatenates previous frames with the most recent in order to construct a richer observation space for the agent. Frame stacking also reduces the degree of partial observability in the ALE, making it possible for the agent to detect the direction of motion in objects."}, {"heading": "2.2 Control in the Arcade Learning Environment", "text": "The typical goal of reinforcement learning (RL) algorithms is to learn a policy \u03c0 : S\u00d7A\u2192 [0, 1] that maps each state to a probability distribution over actions. Ideally, following the learned policy will maximize the discounted cumulative sum of rewards.2 Many RL algorithms accomplish this by learning an action-value function q\u03c0 : S \u00d7 A \u2192 R, which encodes the long-range value of taking action a in state s and then following policy \u03c0 thereafter. More specifically, q\u03c0(s, a) . = E [\u2211\u221e i=1 \u03b3 i\u22121Rt+i | St = s,At = a ] , the expected discounted sum of rewards for some discount factor \u03b3 \u2208 [0, 1], where the expectation is over both the policy \u03c0 and the probability kernel p. However, in the ALE it is not feasible to learn an individual value for each state-action pair due to the large number of possible states. A common way to address this issue is to approximate the action-value function by parameterizing it with a set of weights \u03b8 \u2208 Rn such that q\u03c0(s, a) \u2248 q\u03c0(s, a,\u03b8). We discuss below two approaches to value function approximation that have been successfully applied to the games available in the ALE. We focus on these particular methods because they are by now well-established, well-understood, achieve a reasonable level of performance, and reflect the issues we study here.\nThe first approach is to design a function that, given an observation, outputs a vector \u03c6(s, a) denoting a feature representation of the state s when taking action a. With this approach, we estimate q\u03c0 through a linear function approximator q\u03c0(s, a,\u03b8) = \u03b8\n>\u03c6(s, a). Sarsa(\u03bb) (Rummery & Niranjan, 1994) is a control algorithm that learns an approximate action-value function of a continually improving policy \u03c0. As states are visited, and rewards are observed, q\u03c0 is updated and \u03c0 is consequently improved. The update equations are:\n\u03b4t = Rt+1 + \u03b3\u03b8 > t \u03c6(st+1, at+1)\u2212 \u03b8>t \u03c6(st, at) et = \u03b3\u03bbet\u22121 + \u03c6(st, at)\n\u03b8t+1 = \u03b8t + \u03b1\u03b4tet\nwhere \u03b1 denotes the step-size, et the eligibility trace vector (e\u22121 . = 0), \u03b4t the temporal difference error, and \u03b3 the discount factor. The first benchmarks in the ALE applied this approach with a variety of simple feature representations (Naddaf, 2010; Bellemare, Veness, & Bowling, 2012b; Bellemare et al., 2013). Recently, Liang, Machado, Talvitie, and\n2We use the discounted sum of rewards in our formalism because this is commonly employed by agents in the ALE. Empirical evidence has shown that agents generally perform better when maximizing the discounted cumulative sum of rewards, even though they are actually evaluated in the undiscounted case. This formulation disincentivizes agents to postpone scoring.\nBowling (2016) introduced a feature representation (Blob-PROST) that allows Sarsa(\u03bb) to achieve comparable performance to DQN (described below) in several Atari 2600 games. We refer to such an approach as Sarsa(\u03bb) + Blob-PROST. Recently, Martin et al. (2017) combined Sarsa(\u03bb) and the Blob-PROST features with a method for incentivizing exploration in hard games.\nA recent trend in reinforcement learning is to use neural networks to estimate q\u03c0(s, a,\u03b8), substituting the requirement of a good handcrafted feature representation with the requirement of an effective network architecture and algorithm. Mnih et al. (2015) introduced Deep Q-Networks (DQN), an algorithm that learns representations in a neural network composed of three hidden convolutional layers followed by a fully-connected hidden layer. The network weights are updated through backpropagation with the following update rule:\n\u03b8t+1 = \u03b8t + \u03b1 [ Rt+1 + \u03b3max\na\u2208A q\u0303(St+1, a,\u03b8t)\u2212 q(St, At,\u03b8t)\n] \u2207\u03b8tq(St, At,\u03b8t)\nwhere q\u0303 denotes the action-values estimated by a second network. This second network is updated less frequently for stability purposes. Additional components of the algorithm include clipping the rewards (as described above) and the use of experience replay (Lin, 1993) to decorrelate observations. DQN has inspired much follow-up work combining reinforcement learning and deep neural networks (e.g., Jaderberg et al., 2017; Mnih et al., 2016; Schaul et al., 2016; van Hasselt et al., 2016)."}, {"heading": "3. Divergent Evaluation Methodologies in the ALE", "text": "The ALE has received significant attention since it was introduced as a platform to evaluate general competency in AI. Hundreds of papers have used the ALE as a testbed, employing many distinct experimental protocols for evaluating agents. Unfortunately, these different evaluation protocols are often not carefully distinguished, making direct comparisons difficult or misleading. In this section we discuss a number of methodological differences that have emerged in the literature. In subsequent sections we give special focus to two particularly important methodological issues: 1) different metrics for summarizing agent performance, and 2) different mechanisms for injecting stochasticity in the environment.\nThe discussion about the divergence of evaluation protocols and the need for standardizing them first took place at the AAAI Workshop on Learning for General Competency in Video Games. One of the reasons that authors compare results generated with differing experimental protocols is the high computational cost of evaluating algorithms in the ALE \u2013 it is difficult to re-evaluate existing approaches to ensure matching methodologies. For that reason it is perhaps especially important to establish a standard methodology for the ALE in order to reduce the cost of principled comparison and analysis. One of the main goals of this article is to propose such a standard, and to introduce benchmark results obtained under it for straightforward comparison to future work."}, {"heading": "3.1 Methodological Differences", "text": "To illustrate the diversity in evaluation protocols, we discuss some methodological differences found in the literature. While these differences may be individually benign, they\nare frequently ignored when comparing results, which undermines the validity of direct comparisons.\nEpisode termination. In the initial ALE benchmark results (Bellemare et al., 2013), episodes terminate when the game is over. However, in some games the player has a number of \u201clives\u201d which are lost one at a time. Terminating only when the game is over often makes it difficult for agents to learn the significance of losing a life. Mnih et al. (2015) terminated training episodes when the agent lost a life, rather than when the game is over (evaluation episodes still lasted for the entire game). While this approach has the potential to teach an agent to avoid \u201cdeath,\u201d Bellemare et al. (2016b) noted that it can in fact be detrimental to an agent\u2019s performance. Currently, both approaches are still common in the literature. We often see episodes terminating when the game is over (e.g., Hausknecht et al., 2014; Liang et al., 2016; Lipovetzky et al., 2015; Martin et al., 2017), as well as when the agent loses a life (e.g., Nair et al., 2015; Schaul et al. 2016; van Hasselt et al., 2016). Considering the ideal of minimizing the use of game-specific information and the questionable utility of termination using the \u201clives\u201d signal, we recommend that only the game over signal be used for termination.\nSetting of hyperparameters. One of the primary goals of the ALE is to enable the evaluation of agents\u2019 general ability to learn in complex, high-dimensional decisionmaking problems. Ideally agents would be evaluated in entirely novel problems to test their generality, but this is of course impractical. With only 60 available games in the standard suite there is a risk that methods could \u201coverfit\u201d to the finite set of problems. In analogy to typical methodology in supervised learning, Bellemare et al. (2013) split games into \u201ctraining\u201d and \u201ctest\u201d sets, only using results from training games for the purpose of selecting hyperparameters, then fully evaluating the agent in the test games only once hyperparameters have been selected. This methodology has been inconsistently applied in subsequent work \u2013 for example, hyperparameters are sometimes selected using the entire suite of games, and in some cases hyperparameters are optimized on a per-game basis (e.g., Jaderberg et al., 2017). For the sake of evaluating generality, we advocate for a train/test game split as a way to evaluate agents in problems they were not specifically tuned for.\nMeasuring training data. The first benchmarks in the ALE (Bellemare et al., 2013) trained agents for a fixed number of episodes before evaluating them. This can be misleading since episode lengths differ from game to game. Worse yet, in many games the better an agent performs the longer episodes last. Thus, under this methodology, agents that learn a good policy early receive more training data overall than those that learn more slowly, potentially magnifying their differences. Recently it has become more common to measure the amount of training data in terms of the total number of frames experienced by the agent (Mnih et al., 2015), which aids reproducibility, inter-game analysis, and fair comparisons. That said, since performance is measured on a per-episode basis, it may not be advisable to end training in the middle of an episode. For example, Mnih et al. (2015) interrupt the training as soon as the maximum number of frames is reached, while Liang et al. (2016) pick a total number of training frames, and then train each agent until the end of the episode in which the total is exceeded. This typically results in a negligible number of extra frames of experience beyond the limit. Another important aspect to be taken into consideration is frame skipping, which is a common practice in the ALE but is not reported consistently in the literature. We advocate evaluating from full training episodes\nfrom a fixed number of frames, as was done by Liang et al. (2016), and we advocate taking the number of skipped frames into consideration when measuring training data, as the time scale in which the agent operates is also an algorithmic choice.\nSummarizing learning performance. When evaluating an agent in 60 games, it becomes necessary to compactly summarize the agent\u2019s performance in each game in order to make the results accessible and to facilitate comparisons. Authors have employed various statistics for summarizing agent performance and this diversity makes it difficult to directly compare reported results. We recommend reporting training performance at different intervals during learning. We discuss this issue in more detail in Section 4.\nInjecting stochasticity. The original Atari 2600 console had no source of entropy for generating pseudo-random numbers. The Arcade Learning Environment is also fully deterministic \u2013 each game starts in the same state and outcomes are fully determined by the state and the action. As such, it is possible to achieve high scores by learning an open-loop policy, i.e., by simply memorizing a good action sequence, rather than learning to make good decisions in a variety of game scenarios (Bellemare, Naddaf, Veness, & Bowling, 2015). Various approaches have been developed to add forms of stochasticity to the ALE dynamics in order to encourage and evaluate robustness in agents (e.g., Brockman et al., 2016; Hausknecht and Stone, 2015; Mnih et al., 2015; Nair et al., 2015). Our recommendation is to use sticky actions, implemented in the latest version of the ALE. We discuss this issue in more detail in Section 5."}, {"heading": "4. Summarizing Learning Performance", "text": "One traditional goal in reinforcement learning is for agents to continually improve their performance as they obtain more data (Wilson, 1985; Thrun & Mitchell, 1993; Ring, 1997; Singh et al., 2004; Hutter, 2005; Sutton et al., 2011). Measuring the extent to which this is the case for a given agent can be a challenge, and this challenge is exacerbated in the Arcade Learning Environment, where the agent is evaluated across 60 games. When evaluating an agent in only a few problems, it is common practice to plot learning curves, which provide a rich description of the agent\u2019s performance: how quickly it learns, the highest performance it attains, the stability of its solutions, whether it is likely to continue to improve with more data, etc.\nWhile some have reported results in the ALE using learning curves (e.g., Mnih et al. 2016; Ostrovski et al. 2017; Schaul et al. 2016), it is difficult to even effectively display, let alone comprehend and compare, 60 learning curves. For the sake of comparison and compact reporting, most researchers have applied various approaches to numerically summarize an agent\u2019s performance in each game (e.g., Bellemare et al., 2013; Hausknecht et al., 2014; Munos et al., 2016; Nair et al., 2015). Unfortunately, the variety of different summary statistics in results tables makes direct comparison difficult. In this section we consider some common performance measures seen in the literature and ultimately identify one as being particularly in line with the continual learning goal and advocate for it as the standard for reporting learning results in the ALE."}, {"heading": "4.1 Common Performance Measures", "text": "Here we discuss some common summary statistics of learning performance that have been employed in the Arcade Learning Environment in the past.\nEvaluation after learning. In the first ALE benchmark results, Bellemare et al. (2013) trained agents for a fixed training period, then evaluated the learned policy using the average score in a number of evaluation episodes with no learning. Naturally, a number of subsequent studies used this evaluation protocol (e.g., Defazio and Graepel, 2013; Liang et al., 2016; Martin et al., 2017). One downside to this approach is that it hides issues of sample efficiency, since agents are not evaluated during the entire training period. Furthermore, an agent can receive a high score using this metric without continually improving its performance. For instance, an agent could spend its training period in a purely exploratory mode, gathering information but performing poorly, and then at evaluation time switch to an exploitative mode. While the problem of developing a good policy during an unevaluated training period is an interesting one, in reinforcement learning the agent is typically expected to continually improve with experience. Importantly, -greedy policies tend to perform better than greedy policies in the ALE (Bellemare et al., 2013; Mnih et al., 2015). Therefore, this protocol does not necessarily benefit from turning off exploration during evaluation. In fact, often the reported results under this protocol do use -greedy policies during evaluation.\nEvaluation of the best policy. When evaluating Deep Q-Networks, Mnih et al. (2015) also trained agents for a fixed training period. Along the way, they regularly evaluated the performance of the learned policy. At the end of the training period they evaluated the best policy in a number of evaluation episodes with no learning. A great deal of followup work has replicated this methodology (e.g., Schaul et al., 2016; van Hasselt et al., 2016). This protocol retains the downsides of evaluation after learning, and adds an additional one: it does not evaluate the stability of the agent\u2019s learning progress. Figure 1 illustrates the importance of this issue by showing different learning curves in the game Centipede. On one hand, Sarsa(\u03bb) + Blob-PROST achieves a high score early on but then becomes unstable and fails to retain this successful policy. DQN\u2019s best score is much lower but it is also more stable (though not perfectly so). Reporting the performance of the best policy fails to recognize the plummeting behavior of both algorithms and DQN\u2019s more stable performance. Note also that the best score achieved across training is a statistically biased estimate of an agent\u2019s best performance: to avoid this bias, one should perform a second, independent evaluation of the agent at that particular point in time, as reported by Wang et al. (2016).\nArea under the learning curve. Recently, eschewing an explicit evaluation phase, Stadie, Levine, and Abbeel (2015) proposed the area under the learning curve as an evaluation metric. Intuitively, the area under the learning curve is generally proportional to how long a method achieves \u201cgood\u201d performance, i.e., the average performance during training. Methods that only have performance spikes and methods that are unstable generally perform poorly under such metric. However, area under the learning curve does not capture the \u201cplummeting\u201d behavior illustrated in Figure 1. For example, in this case, Sarsa(\u03bb) + Blob-PROST looks much better than DQN using this metric. Furthermore, area under the\ncurve cannot distinguish a high-variance, unstable learning process from steady progress towards a good policy, even though we typically prefer the latter."}, {"heading": "4.2 Proposal: Performance During Training", "text": "The performance metric we propose as a standard is simple and has been adopted before (e.g. Bellemare et al. 2012). At the end of training (and ideally at other points as well) report the average performance of the last k episodes. This protocol does not use the explicit evaluation phase, thus requiring an agent to perform well while it is learning. This better aligns the performance metric with the goal of continual learning while also simplifying experimental methodology. Unstable methods that exhibit spiking and/or plummeting learning curves will score poorly compared to those that stably and continually improve, even if they perform well during most of training.\nAnother advantage is that this metric is well-suited for analysis of an algorithm\u2019s sample efficiency. While the agent\u2019s performance near the end of training is typically of most interest, it is also straightforward to report the same statistic at various points during training, effectively summarizing the learning curve with a few selected points along the curve. Furthermore, if researchers make their full learning curve data publicly available, others can easily perform post-hoc analysis for the sake of comparison for any amount of training without having to fully re-evaluate existing methods. Currently, it is fairly standard to train agents for 200 million frames, in order to facilitate comparison with the DQN results reported by Mnih et al. (2015). This is equivalent to approximately 38 days of real-time gameplay and even at fast frame rates represents a significant computational expense. By reporting performance at multiple points during training, researchers can easily draw comparisons earlier in the learning process, reducing the computational burden of evaluating agents.\nIn accordance with this proposal, the benchmark results we present in Section 6 report the agent\u2019s average score of the last 100 episodes before the agent reaches 10, 50, 100, and\n200 million frames and our full learning curve data is publicly available3. This allows us to derive insight regarding the learning rate and stability of the algorithms and will offer flexibility to researchers wishing to compare to these benchmarks in the future."}, {"heading": "5. Determinism and Stochasticity in the Arcade Learning Environment", "text": "In almost all games, the dynamics within Stella itself (the Atari 2600 VCS emulator embedded within the ALE) are deterministic given the agent\u2019s actions. The agent always starts at the same initial state, and a given sequence of actions always leads to the same outcome. Bellemare et al. (2015) and Braylan et al. (2015) showed that this determinism can be exploited by agents that simply memorize an effective sequence of actions, attaining state-of-the-art scores while ignoring the agent\u2019s perceived state altogether. Such an approach is not likely to be successful beyond the ALE \u2013 in most problems of interest it is difficult, if not impossible, to exactly reproduce a specific state-action sequence, and closed-loop decision-making is required. An agent that relies upon the determinism of the ALE may achieve high scores, but may also be highly sensitive to small perturbations. For example, Hausknecht and Stone (2015) analyzed the role of determinism in the success of HyperNEAT-GGP (Hausknecht et al., 2014). Figure 2 shows that memorizing-NEAT (solid boxes) performs significantly worse under multiple forms of mild stochasticity, whereas randomized-NEAT (hollow, pinched boxes), which is trained with some stochastic perturbations, performs worse in the deterministic setting, but is more robust to various forms of stochasticity. As an evaluation platform, the deterministic ALE does not effectively distinguish between agents that learn robust, closed-loop policies from brittle memorization-based agents.\nRecognizing this limitation in earlier versions of the ALE, many researchers have augmented the standard behavior of the ALE to evaluate the robustness of their agents and to discourage memorization (e.g., injecting stochasticity, Hausknecht and Stone, 2015; no-ops, Mnih et al., 2015; human starts, Nair et al., 2015; random frame skips, Brockman et al., 2016). Again, this wide range of experimental protocols makes direct comparison of results difficult. We believe the research community would benefit from a single standard protocol that empirically distinguishes between brittle, open-loop solutions and robust, closed-loop solutions.\nIn this section we discuss the Brute (first briefly introduced in Bellemare et al., 2015) as an example of an algorithm that explicitly and effectively exploits the environment\u2019s determinism. We present results in five Atari 2600 games comparing the Brute\u2019s performance with traditionally successful reinforcement learning methods. We then introduce the sticky actions method for injecting stochasticity into the ALE and show that it effectively distinguishes the Brute from methods that learn more robust policies. We also discuss pros and cons of several alternative experimental protocols aimed at discouraging open-loop policies, ultimately proposing sticky actions as a standard training and evaluation protocol, which will be incorporated in a new version of the Arcade Learning Environment.\n3http://www.marcgbellemare.info/results_ale.tgz\nRevisiting the ALE: Evaluation Protocols and Open Problems\nEpsilon-Greedy Action Selection \u270f-greedy action selection chooses a random legal action at each frame with probability \u270f. (Bellemare et al. 2013; Mnih et al. 2013) used \u270f-greedy action selection with \u270f = .05.\nEnforcing an \u270f-greedy action selection step in ALE would be difficult to implement in an algorithm-friendly manner. Two main factors to consider: 1) should ALE overwrite a requested action with a random one or simply insert a random action after a requested action? and 2) Should ALE report back to the algorithm that it overwrote/inserted a random action or should it silently take the random action and report the resulting reward and next state as if nothing special happened? The former would require a more complex agent/ALE interface, while the latter would hide potentially important information from the agent. Given the dissatisfying qualities of both options, perhaps the least of all evils is to encourage some standard value of \u270f and rely on practitioners to implement and self-report.\nFigure 2: \u270f-Greedy Action Selection: Even small values of \u270f drastically reduce memorizing-NEAT\u2019s performance. \u270f = 0 corresponds to the entirely deterministic agent while \u270f = 1 is a completely random agent.\nFigure 2 indicates that \u270f-greedy action selection is effective at derailing memorizing-NEAT even at small values of \u270f such as 0.005. Perhaps the prior practice of using \u270f = .05 could be relaxed, leading to increased agent performance.\nFigure 3: \u270f-Repeat Action Selection has the most detrimental effects towards memorizing-NEAT and the least effect on randomized-NEAT.\nEpsilon-Repeat Action Selection Rather than choosing an entirely random action with probability \u270f, ALE could instead repeat the last requested action for an extra frame. This would have a randomizing effect for all but the most degenerate of policies.1 Additionally, as Figure 3 shows, repeating a selected action is less detrimental than selecting an action entirely at random. Implementationwise, enforcing randomized action repeats in ALE would have the same complications as enforcing \u270f-greedy action selection. Figure 3 confirms that \u270f-repeat action selection is just as effective as \u270f-greedy action selection at degrading memorizing-NEAT\u2019s performance but has very little effect on randomized-NEAT.\nDiscussion Any form of forced randomness that does not come from the environment will necessarily degrade the performance of a learning agent. Of the different methods for adding stochasticity to Atari 2600 games, \u270f-repeat action selection best fits the desired criteria: it has the most detrimental effects towards memorizing agents and is the least detrimental to already randomized agents.\nIn the future, perhaps the best way to overcome the Atari 2600\u2019s determinism is through two-player games (or competitions) in which randomness stems from the other player.\nReferences Bellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M. 2013. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res. (JAIR) 47:253\u2013 279. Hausknecht, M.; Lehman, J.; Miikkulainen, R.; and Stone, P. 2013. A neuroevolution approach to general atari game playing. In IEEE Transactions on Computational Intelligence and AI in Games. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Playing atari with deep reinforcement learning. CoRR abs/1312.5602.\n1A policy that only selects a single action would be unaffected.\n20\nFigure 1: Effects of random initialization on memorizingNEAT (solid rectangular boxplots) and randomized-NEAT (pinched hollow boxplots). Reference scores of each agent are provided in a fully deterministic environment and a fully random environment (enforced \u270f = 1 greedy action selection). Higher aggregat Z-Scores are better.\nEpsilon-Greedy Action Selection \u270f-greedy action selection chooses a random legal action at each frame with probability \u270f. (Bellemare et al. 2013; Mnih et al. 2013) used \u270f-greedy action selection with \u270f = .05.\nEnforcing an \u270f-greedy action s lection step in ALE would be difficult to implement in an algorithm-friendly manner. Two main factors to consider: 1) should ALE overwrite a requested action with a random one or simply insert a random action after a requested action? and 2) Should ALE report back to the algorithm that it overwrote/inserted a random action or should it silently take the ran om action an report the res lting reward and next state as if nothing special happened? The former would require a more complex agent/ALE interface, while the latter would hide potentially important information from the agent. Given the dissatisfying qualities of both options, perhaps the least of all evils is to encourage some standard value of \u270f and rely on practitioners to implement a d self-report.\nFigure 2: \u270f-Greedy Action Selection: Even small values of \u270f drastically reduce memorizing-NEAT\u2019s performance. \u270f = 0 corresponds to the entirely deterministic agent while \u270f = 1 is a completely random agent.\nFigure 2 indicates that \u270f-greedy action selection is effective at derailing memorizing-NEAT even at small values of \u270f such as 0.005. Perhaps the prior practice of using \u270f = .05 could be relaxed, leading to increased agent performance.\nFigure 3: \u270f-Repeat Action Selection has the most detrimental effects towards memorizing-NEAT and the least effect on randomized-NEAT.\nEpsilon-Repeat Action Selection\nRather than choosing an entirely random action with probability \u270f, ALE could instead repeat the last requested action for an extra frame. This would have a randomizing effect for all but the most degenerate of policies.1 Additionally, as Figure 3 shows, repeating a selected action is less detrimental than selecting an action entirely at random. Implementationwise, enforcing randomized action repeats in ALE would have the same complications as enforcing \u270f-greedy action selection. Figure 3 confirms that \u270f-repeat action selection is just as effective as \u270f-greedy action selection at degrading memorizing-NEAT\u2019s performance but has very little effect on randomized-NEAT.\nDiscussion Any form of forced rando ness that does not come fro the environment will necessarily degrade the performance of a learning agent. Of the different methods for adding stochasticity to Atari 2600 games, \u270f-repeat action selection best fits the desired criteria: it has the most detrimental effects toward memorizing agents and is the least detrimental to already randomized agents.\nIn the future, perhaps the best way to overcome the Atari 2600\u2019s determinism is through two-player games (or competitions) in which randomness stems from the other player.\nReferences Bellemare, M. G.; Naddaf, Y.; Veness, J.; nd Bowling, M. 2013. The arcade learning environment: An evaluation pla - form for general agents. J. Artif. Intell. Res. (JAIR) 47:253\u2013 279. Hausknecht, M.; Lehman, J.; Miikkulainen, R.; and Stone, P. 2013. A neuroevolution approach to general atari game playing. In IEEE Transactions on Computational Intelligence and AI in Games. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Playing atari with deep reinforcement learning. CoRR abs/1312.5602.\n1A policy that only selects a single action would be unaffected.\n20\nFigure 1: Effects of random initialization on memorizingNEAT (solid rectangular boxplots) and randomized-NEAT (pinched hollow boxplots). Reference scores of each agent are provided in a fully deterministic environment and a fully random environment (enforced \u270f = 1 greedy action selection). Higher aggregate Z-Scores are better. Epsilon-Greedy Action Selection \u270f-greedy action selection chooses a random legal action at each frame with probability \u270f. (Bellemare et al. 2013; Mnih et al. 2013) used \u270f-greedy action selection with \u270f = .05.\nEnforcing an \u270f-greedy action selection step in ALE would be difficult to implement in an algorithm-friendly manner. Two main factors to consider: 1) should ALE overwrite a requested action with a random one or simply insert a random action after a requested action? and 2) Should ALE report back to the algorithm that it overwrote/inserted a random action or should it silently take the random action and report the resulting reward and next state as if nothing special happened? The former would require a more complex agent/ALE interface, while the latter would hide potentially important information from the agent. Given the dissatisfying qualities of both options, perhaps the least of all evils is to encourage some standard value of \u270f and rely on practitioners to implement and self-report.\nFigure 2: \u270f-Greedy Action Selection: Even small values of \u270f drastically reduce memorizing-NEAT\u2019s performance. \u270f = 0 corresponds to the entirely deterministic agent while \u270f = 1 is a completely random agent.\nFigure 2 indicates that \u270f-greedy action selection is effective at derailing memorizing-NEAT even at small values of \u270f such as 0.005. Perhaps the prior practice of using \u270f = .05 could be relaxed, leading to increased agent performance.\nFigure 3: \u270f-Repeat Action Selection has the most detrimental effects towards memorizing-NEAT and the least effect on randomized-NEAT.\nEpsilon-Repeat Action Selection Rather than choosing an entirely random action with probability \u270f, ALE could instead repeat the last requested action for an extra frame. This would have a randomizing effect for all but the most degenerate of policies.1 Additionally, as Figure 3 shows, repeating a selected action is less detrimental than selecting an action entirely at random. Implementationwise, enforcing randomized action repeats in ALE would have the same complications as enforcing \u270f-greedy action selection. Figure 3 confirms that \u270f-repeat action selection is just as effective as \u270f-greedy action selection at degrading memorizing-NEAT\u2019s performance but has very little effect on randomized-NEAT.\nDiscussion Any form of forced randomness that does not come from the environment will necessarily degrade the performance of a learning agent. Of the different methods for adding stochasticity to Atari 2600 games, \u270f-repeat action selection best fits the desired criteria: it has the most detrimental effects towards memorizing agents and is the least detrimental to already randomized agents.\nIn the future, perhaps the best way to overcome the Atari 2600\u2019s determinism is through two-player games (or competitions) in which randomness stems from the other player.\nReferences Bellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M. 2013. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res. (JAIR) 47:253\u2013 279. Hausknecht, M.; Lehman, J.; Miikkulainen, R.; and Stone, P. 2013. A neuroevolution approach to general atari game playing. In IEEE Transactions on Computational Intelligence and AI in Games. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Playing atari with deep reinforcement learning. CoRR abs/1312.5602.\n1A policy that only selects a single action would be unaffected.\n20\nFigure 2: Final performance of HyperNEAT agent under vario s odels of stochasti -\nity in ALE. Each plot corresponds to a diff r nt stochastic ty model. R tangular boxplots\ncor espond to memorizing-NEAT while hollow, pinched b xplots correspond to randomized-\nNEAT (Hausknecht et al., 2014). Each boxplot represents a single evaluation of 61 At ri 2600 games. Z-Score normalization is applied to normalize the per-game scores. The agent\u2019s over ll perf rma ce is epicted in the y-ax s while the amount of stochasticity in the enviro ment increases along the x-axis. The first figu e depicts the impact of random no-ops at the beginning of the game. Reference scores for a fully deterministic and fully random environments are provided. The second graph depicts the performance of both algorithms whe forced t sel ct actio s -greedily for different values of . The third raph depicts the performance of both algorithms when forced to repeat the previous action with probability (equivalen to sticky actions). Reproduced f om Hauskn cht and Stone (2015)."}, {"heading": "5.1 The Brute", "text": "The Brute is an algorithm designed to exploit features of the riginal Arcade Learning Environment. Although developed independently by some of t is article\u2019s author , it shares many similarities with the trajectory tree method of Kearns et al. (1999). The Brute uses the agent\u2019s trajectory ht = a , o1, 2, o2, . . . , ot as stat representa ion, assigning individual values to each state. Because of the ALE\u2019s determinism, a single sample from each state-action pair is sufficient for a perfect estimate of the agent\u2019s return up to that point. The Brute maintai s a partial history tree that contains ll visited histories. Each node, associated with a history, maintains an action-conditional transition function and a reward function. The Brute estimates the valu for any history-action pair using bottom-up dynamic programming. The agent follows the best trajectory found so far, with infrequent random actions used to search for better trajectories.\nIn order to be able to apply the Brute to stochastic environments, our implementation aintai s the maximum likelihood estimate for both transition and reward functions. We\nprovide a full description of the Brute in ppendix A."}, {"heading": "5.1.1 Empirical Evaluation", "text": "We evaluated the performance of the Brute on the five training games proposed by Bellemare et al. (2013). The average score obtained by the Brute, as well as of DQN and Sarsa(\u03bb) + Blob-PROST, are presented in Table 1. Agents interacted with the environment for 50 million frames and the numbers reported are the average scores agents obtained in the last 100 episodes played while learning. We discuss our experimental setup in Appendix B.\nThe Brute is crude but we see that it leads to competitive performance in a number of games. In fact, Bellemare et al. (2015), using a different evaluation protocol, reports that the Brute outperformed the best learning method at the time on 45 out of 55 Atari 2600 games. However, as we will see, this performance critically depends on the environment\u2019s determinism. In the next section we discuss how we modified the ALE to introduce a form of stochasticity we call sticky actions; and we show that the Brute fails when small random perturbations are introduced."}, {"heading": "5.2 Sticky Actions", "text": "This section introduces sticky actions, our approach to injecting stochasticity into the ALE. This approach also evaluates the robustness of learned policies. Its design is based on the following desiderata:\n\u2022 the stochasticity should be minimally non-Markovian with respect to the environment, i.e., the action to be executed by the emulator should be conditioned only on the action chosen by the agent and on the previous action executed by the emulator,\n\u2022 the difficulty of existing tasks should not be changed, i.e., algorithms that do not rely on the environment\u2019s determinism should not have their performance hindered by the introduction of stochasticity, and\n\u2022 it should be easy to implement in the ALE, not requiring changes inside the Stella emulator, but only on the framework itself.\nIn sticky actions there is a stickiness parameter \u03c2, the probability at every time step that the environment will execute the agent\u2019s previous action again, instead of the agent\u2019s new action. More specifically, at time step t the agent decides to execute action a; however, the action At that the environment in fact executes is:\nAt =\n{ a, with prob. 1\u2212 \u03c2,\nat\u22121, with prob. \u03c2.\nIn other words, if \u03c2 = 0.25, there is 25% chance the environment will not execute the desired action right away. Figure 3 (left) illustrates this process.\nNotice that if an agent decides to select the same action for several time steps, the time it will take to have this action executed in the environment follows a geometric distribution. The probability the previous action is executed k times before the new action is executed is (1\u2212 \u03c2)k\u03c2.\nSticky actions are different from random delays because, in the former, the agent can change its mind at any time by sending a new action to the emulator. To see why this matters, consider the game Q*bert, where a single wrong action may cause the agent to jump off the pyramid and lose a life (Figure 3, right). Under sticky actions, the agent can switch to a no-op before landing on the edge, knowing that with high probability the action will not be continued up to the point it pushes the agent off the pyramid. With random delays, the previous action will be executed until the delay is passed, even if the agent switched to a no-op before landing on the edge. This increases the likelihood the agent will be forced to continue moving once it lands on the edge, making it more likely to fall off the pyramid.\nSticky actions also interplay well with other aspects of the Arcade Learning Environment. Most Atari 2600 games are deterministic and it would be very hard to change their dynamics. Our approach only impacts which actions are sent to be executed. Sticky actions also interacts well with frame skipping (c.f. Section 2). With sticky actions, at each intermediate time step between the skipped frames there is a probability \u03c2 of executing the previous action. Obviously, this applies until the current action is executed, when the previous action taken and the current action become the same. Figure 3 depicts the process for a frame skip of 4."}, {"heading": "5.2.1 Evaluating the Impact of Sticky Actions", "text": "We now re-evaluate the performance of the Brute, DQN and Sarsa(\u03bb) + Blob-PROST under the sticky actions protocol. The intuition is that the Brute, which exploits the assumption that the environment is deterministic, should perform worse when stochasticity is introduced. We repeated the experiments from Section 5.1.1, but with \u03c2 = 0.25. Table 2 depicts the algorithms\u2019 performance in both the stochastic environment and in the deterministic environment.\nWe can see that the Brute is the only algorithm substantially impacted by the sticky actions. These results suggest that sticky actions enable us to empirically evaluate an agent\u2019s robustness to perturbation."}, {"heading": "5.3 Alternative Forms of Stochasticity", "text": "To conclude this section, we briefly discuss some alternatives to sticky actions, listing their pros (+) and cons (\u2212). These alternatives fall in two broad categories: start-state methods and stochastic methods. In start-state methods, the first state of an episode is chosen randomly, but the deterministic dynamics remain unchanged. These approaches are less intrusive as the agent retains full control over its actions, but do not preclude exploiting the environment\u2019s determinism. This may be undesirable in games where the agent can exploit game bugs by executing a perfectly timed sequence of actions, as in, for example, the game Q*bert.4 On the other hand, stochastic methods impact the agent\u2019s ability to control the environment uniformly throughout the episode, and thus its performance. We believe our proposed method minimizes this impact.\nInitial no-ops. When evaluating the agent, begin the episode by taking from 0 to k no-op actions, selected uniformly at random (Mnih et al., 2015). By affecting the initial emulator state, this prevents the simplest form of open-loop control.\n+ No interference with agent action selection.\n\u2212 Impact varies across games. For example, initial no-ops have no effect in Freeway.\n\u2212 The environment remains deterministic beyond the choice of starting state.\n\u2212 Brute-like methods still perform well.\nRandom human starts. When evaluating the agent, randomly pick one of k predetermined starting states. Nair et al. (2015), for example, sampled starting states at random from a human\u2019s gameplay.\n+ Allows evaluating the agent in very different situations.\n\u2212 The environment remains deterministic beyond the choice of starting state.\n\u2212 Brute-like methods still perform well. 4Personal communication from Ziyu Wang.\n\u2212 It may be difficult to provide starting states that are both meaningful and free of researcher bias. For example, scores as reported by Nair et al. (2015) are not comparable across starting states: although in a full game of Pong an agent can score 21 points, from a much later starting state this score is unachievable.\nUniformly random action noise. With a small probability \u03c2, the agent\u2019s selected action is replaced with another action drawn uniformly from the set of legal actions.\n+ Matches the most commonly used form of exploration, -greedy.\n\u2212 May significantly interfere with agent\u2019s policy, e.g., when navigating a narrow cliff such as in the game Q*bert.\nRandom frame skips. This approach, implemented in OpenAI\u2019s Gym (Brockman et al., 2016), is closest to our method. Each action randomly lasts between k1 and k2 frames.\n+ Does not interfere with action selection, only the timing of action execution.\n\u2212 This restricts agents to using frame skip. In particular, the agent cannot react to events occurring during an action\u2019s period.\n\u2212 Discounting must also be treated more carefully, as this makes the effective discount factor random.\n\u2212 The agent has perfect reaction time since its actions always have an immediate effect.\nAsynchronous environment. More complex environments might involve unpredictable communication delays between the agent and the environment. This is the case in Minecraft (Project Malmo; Johnson et al., 2016), Starcraft (Ontanon et al., 2013), and robotic RL platforms (Sutton et al., 2011).\n+ This setting naturally discourages agents relying on determinism.\n\u2212 Lacks reproducibility across platforms and hardware.\n\u2212 With sufficiently fast communications, reverts to a deterministic environment.\nOverall comparison. Our proposed solution, sticky actions, leverages some of the main benefits of other approaches without most of their drawbacks. It is free from researcher bias, it does not interfere with agent action selection, and it discourages agents from relying on memorization. The new environment is stochastic for the whole episode, generated results are reproducible, and our approach interacts naturally with frame skipping and discounting."}, {"heading": "6. Benchmark Results in the Arcade Learning Environment", "text": "In this section we introduce new benchmark results for DQN and Sarsa(\u03bb) + Blob-PROST in 60 different Atari 2600 games using sticky actions. It is our hope that future work will adopt the experimental methodology described in this paper, and thus be able to directly compare results with this benchmark."}, {"heading": "6.1 Experimental Method", "text": "We evaluated DQN and Sarsa(\u03bb) + Blob-PROST in 60 different Atari 2600 games. We report results using the sticky actions option in the new version of the ALE (\u03c2 = 0.25), evaluating the final performance while learning, at 10, 50, 100 and 200 million frames. We computed score averages of each trial using the 100 final episodes until the specified threshold, including the episode in which the total is exceeded. We report the average over 5 trials for DQN and the average over 24 trials for Sarsa(\u03bb) + Blob-PROST. To ease reproducibility, we listed all the relevant parameters used by Sarsa(\u03bb) + Blob-PROST and DQN in Appendix B. We encourage researchers to present their results on the ALE in the same reproducible fashion."}, {"heading": "6.2 Benchmark Results", "text": "We present excerpts of the obtained results for Sarsa(\u03bb) + Blob-PROST and DQN in Tables 3 and 4. These tables report the obtained scores in the games we used for training. These games were originally proposed by Bellemare et al. (2013). The complete results are available in Appendix C.\nBecause we report the algorithms\u2019 performance at different points in time, these results give us insights about learning progress made by each algorithm. Such analysis allows us to verify, across 60 games, how often an agent\u2019s performance plummets; as well as how often agents reach their best performance before 200 million frames.\nIn most games, Sarsa(\u03bb) + Blob-PROST\u2019s performance steadily increases for the whole learning period. In only 10% of the games the scores obtained with 200 million frames are lower than the scores obtained with 100 million frames. This difference is statistically significant in only 3 games:5 Carnival, Centipede, and Wizard of Wor. However, in most games we observe diminishing improvements in an agent\u2019s performance. In only 22 out of 60 games we observe statistically significant improvements from 100 million frames to 200 million frames.5 In several games such as Montezuma\u2019s Revenge this stagnation is due to exploration issues; the agent is not capable of finding additional rewards in the environment.\nDQN has much higher variability in the learning process and it does not seem to benefit much from additional data. DQN obtained its highest scores using 200 million frames in only 35 out of 60 games. Agents\u2019 performance at 200 million frames was statistically better\n5Welch\u2019s t-test (p < 0.05; n = 24).\nthan agents\u2019 performance at 100 million frames in only 18 out of 60 games.6 In contrast, Sarsa(\u03bb) + Blob-PROST achieves its highest scores with 200 million samples in 50 out of 60 games. We did not observe statistically significant performance decreases for DQN when comparing agents\u2019 performance at 100 and 200 million samples.6 It is important to add a caveat that the lack of statistically significant results may be due to our sample size (n = 5). The t-test\u2019s power may still be too low to detect significant differences in DQN\u2019s performance. It is worth pointing out that when DQN was originally introduced, its results consisted of only one independent trial. Despite its high computational cost we evaluated it on 5 trials in an attempt to evaluate such an important algorithm more thoroughly, addressing the methodological concerns we discussed above and offering a more reproducible and statistically comparable DQN benchmark.\nWe also compared the performance of both algorithms in each game to understand specific trends such as performance plumetting and absence of learning. Performance drops seem to be algorithm dependent, not game dependent. Centipede is the only game in which plummeting performance was observed for both both DQN and Sarsa(\u03bb) + Blob-PROST. The decrease in performance we observe in other games occurs only for one algorithm. On the other hand, we were able to identify some games that seem to be harder than others for both algorithms. Both algorithms fail to make much progress on games such as Asteroids, Pitfall, and Tennis. These games generally pose hard exploration tasks to the agent; or have complex dynamics, demanding better representations capable of accurately encoding value function approximations.\nWe can also compare our results to previously published results to verify the impact our proposed evaluation protocol has in agents\u2019 performance. This new setting does not seem to benefit a specific algorithm. Sarsa(\u03bb) + Blob-PROST and DQN still present comparable performance, with each algorithm being better in an equal number of games, as suggested by Liang et al. (2016). As we already discussed in Section 5, using sticky actions seems to only substantially hinder the performance of the Brute agent, not having much impact in the performance of DQN and Sarsa(\u03bb) + Blob-PROST. We observed decreased performance for DQN and Sarsa(\u03bb) + Blob-PROST only in three games: Breakout, Gopher, and Pong."}, {"heading": "7. Open Problems and the Current State-of-the-Art in the ALE", "text": "To provide a complete big picture of how the ALE is being used by the research community. It is also important to discuss the variety of research problems for which the community has\n6Welch\u2019s t-test (p < 0.05; n = 5).\nused the ALE as a testbed. In the past few years we have seen several successes showcased in the ALE, with new results introduced at a rapid pace.\nWe list five important research directions the community has worked on using the ALE, and we use current results in the literature to argue that while there has been substantial progress these problems still remain open. These research directions are:\n\u2022 representation learning,\n\u2022 exploration,\n\u2022 transfer learning,\n\u2022 model learning, and\n\u2022 off-policy learning."}, {"heading": "7.1 Representation Learning", "text": "The ALE was originally introduced to pose the problem of general competency: expecting a single algorithm to be capable of playing dozens of Atari 2600 games. Therefore, agents must either use generic encodings capable of representing all games (e.g., Liang et al., 2016), or be able to automatically learn representations. The latter is obviously more desirable for the potential of discovering better representations while alleviating the burden of having handcrafted features.\nDeep Q-Networks (DQN) of Mnih et al. (2015) demonstrate it is possible to learn representations jointly with control policies. However, reinforcement learning methods based on neural networks still have a high sample complexity, requiring at least dozens of millions of samples before achieving good performance, in part due to the need for learning this representation. In the results we report, DQN\u2019s performance (Table 9) is better than Sarsa(\u03bb) + Blob-PROST\u2019s (Table 8) in less than 20% of the games when evaluated at 10 million frames, and achieves comparable performance at 100 million frames. The high sample complexity also seems to hinder the agents\u2019 performance in specific environments, such as when non-zero rewards are very sparse. Figure 4 illustrates this point by showing how DQN sees non-zero rewards occasionally while playing Montezuma\u2019s Revenge (Figure 6a), but it does not learn to obtain non-zero rewards consistently. Recently, researchers have tried to address this issue by weighting samples differently, prioritizing those that seem to provide more information to the agent (Schaul et al., 2016). Another approach is to use auxiliary tasks that allow agents to start learning a representation before the first extrinsic reward is observed (Jaderberg et al., 2017); the distributions output by the C51 algorithm of Bellemare et al. (2017) may be viewed as a particularly meaningful set of auxiliary tasks. Finally, intrinsically generated rewards (Bellemare et al., 2016b) may also provide a useful learning signal which the agent can use to build a representation.\nDespite this high sample complexity, DQN and DQN-like approaches remain the best performing methods overall when compared to simple, hand-coded representations (Liang et al., 2016). However, these improvements are not as dramatic as they are in other applications (e.g., computer vision; Krizhevsky et al., 2012). Furthermore, this superior performance often comes at the cost of additional tuning, as recently reported by Islam et al.\n(2017) in the context of continuous control. This suggests that there is still room for significant progress on effectively learning good representations in the ALE.\nDifferent approaches that learn an internal representation in a sample efficient way have also been proposed (Veness et al., 2015), although they have not yet been fully explored in this setting. Other directions the research community has been looking at are the development of better visualization methods (Zahavy, Ben-Zrihem, & Mannor, 2016), the proposal of algorithms that alleviate the need for specialized hardware (Mnih et al., 2016), and genetic algorithms (Kelly & Heywood, 2017)."}, {"heading": "7.2 Planning and Model-Learning", "text": "Despite multiple successes of search algorithms in artificial intelligence (e.g., Campbell et al., 2002; Schaeffer et al., 2007; Silver et al., 2016), planning in the Arcade Learning Environment remains rare compared to methods that learn policies or value functions (but see Bellemare et al., 2013b; Guo et al., 2014; Lipovetzky et al., 2015; Shleyfman et al., 2016; Jinnai and Fukunaga, 2017, for published planning results in the ALE). Developing heuristics that are general enough to be successfully applied to dozens of different games is a challenging problem. The problem\u2019s branching factor and the fact that goals are sometimes thousands of steps ahead of the agent\u2019s initial state are also major difficulties.\nAlmost all successes of planning in the ALE use the generative model provided by the Stella emulator, and so have an exact model of the environment. Learning generative models is a very challenging task (Bellemare et al., 2013, 2014; Oh et al., 2015; Chiappa et al., 2017) and so far, there has been no clear demonstration of successful planning with a learned model in the ALE. Learned models tend to be accurate for a small number of time steps until errors start to compound (Talvitie, 2014). As an example, Figure 5 depicts rollouts obtained with one of the first generative models trained on the ALE (Bellemare et al., 2013). In this figure we can see how the accuracy of rollouts start to drop after a few dozen time steps. Probably the most successful example of model learning in the ALE is\ndue to Oh et al. (2015) who learned multistep models that, up to one hundred time steps, appear accurate. These models are able to assist with exploration, an indication of the models\u2019 accuracy. However, because of compounding errors, the algorithm still needs to frequently restore its model to the real state of the game. More recently, Chiappa et al. (2017) showed significant improvements over this original model, including the ability to plan with the internal state. In both cases, however, the models are much slower than the emulator itself; designing a fast, accurate model remains an open problem.\nA related open problem is how to plan with an imperfect model. Although an error-free model might be unattainable, there is plenty of evidence that even coarse value functions are sufficient for the model-free case (Veness et al., 2015), raising the question of how to compensate for a model\u2019s flaws. Training set augmentation (Talvitie, 2014, 2017; Venkatraman, Hebert, & Bagnell, 2015) has shown that it is possible to improve an otherwise limited model. Similarly, Farahmand et al. (2017) showed that better planning performance could be obtained by using a value-aware loss function when training the model. We believe this to be a rich research direction."}, {"heading": "7.3 Exploration", "text": "Most approaches for exploration focus on the tabular case and generally learn models of the environment (e.g., Kearns and Singh, 2002; Brafman and Tennenholtz, 2002; Strehl and\nLittman, 2008). The community is just beginning to investigate exploration strategies in model-free settings when function approximation is required (e.g., Bellemare et al., 2016b; Osband et al., 2016; Ostrovski et al., 2017; Machado et al., 2017; Martin et al., 2017; Vezhnevets et al., 2017). This is the setting in which the ALE lies. Visiting every state does not seem to be a feasible strategy given the large number of possible states in a game (potentially 21024 different states since the Atari 2600 has 1024 bits of RAM memory). In several games such as Montezuma\u2019s Revenge and Private Eye (see Figure 6) even obtaining any feedback is difficult because thousands of actions may be required before a first positive reward is seen. Given the usual sample constraints (200 million frames), random exploration is highly unlikely to guide the agent towards positive rewards. In fact, some games such as Pitfall! and Tennis (see Figure 6) pose an even harder challenge: random exploration is more likely to yield negative rewards than positive ones. In consequence, many simpler agents learn that staying put is the myopically best policy, although recent state-of-the-art agents (e.g., Bellemare et al., 2017; Jaderberg et al., 2017) can sometimes overcome this negative reward gradient.\nSome researchers recently started trying to address the exploration problem in the ALE. Machado et al. (2015) extended optimistic initialization to function approximation. Oh et al. (2015) and Stadie et al. (2015) learned models to predict which actions lead the agent to frames observed least often, or with more uncertainty. Bellemare et al. (2016b), Ostrovski et al. (2017) and Martin et al. (2017) extended state visitation counters to the case of function approximation. Osband et al. (2016) uses randomized value functions to better explore the environment. Machado et al. (2017) and Vezhnevets et al. (2017) proposed the use of options to generate decisive agents, avoiding the dithering commonly observed in random walks. However, despite successes in individual games, such as Bellemare et al.\u2019s success in Montezuma\u2019s Revenge, none of these approaches has been able to improve, in a meaningful way, agents\u2019 performance in games such as Pitfall!, where the only successes to date involve some form of apprenticeship (e.g., Hester et al., 2017).\nThere is still much to be done to narrow the gap between solutions applicable to the tabular case and solutions applicable to the ALE. An aspect that still seems to be missing are agents capable of committing to a decision for extended periods of time, exploring in a different level of abstraction, something that humans frequently do. Maybe agents should not be exploring in terms of joystick movements, but in terms of object configurations and game levels. Finally, for intrinsically difficult games, agents may need some form of intrinsic motivation (Oudeyer, Kaplan, & Hafner, 2007; Barto, 2013) to keep playing despite the apparent impossibility of scoring in the game."}, {"heading": "7.4 Transfer Learning", "text": "Most work in the ALE involves training agents separately in each game, but many Atari 2600 games have similar dynamics. We can expect knowledge transfer to reduce the required number of samples needed to learn to play similar games. As an example, Space Invaders and Demon Attack (Figure 7) are two similar games in which the agent is represented by a spaceship at the bottom of the screen and it is expected to shoot incoming enemies. A more ambitious research question is how to leverage general video game experience, sharing knowledge across games that are not directly analogous. In this case, more abstract concepts could be learned, such as \u201csometimes new screens are seen when the avatar goes to the edge of the current screen\u201d.\nThere are attempts to apply transfer learning in the ALE (Rusu et al., 2016; Parisotto et al., 2016). Such attempts are restricted to a dozen games that tend to be similar and generally require an \u201cexpert\u201d network first, instead of learning how to play all games concurrently. Taylor and Stone (2009) have shown one can face negative transfer depending on the similarity between the tasks being used. It is not clear how this should be addressed in the ALE. Ideally one would like to have an algorithm automatically deciding which games are helpful and which ones are not. Finally, current approaches are only based on the use of neural networks to perform transfer, conflating representation and policy transfer. It may be interesting to investigate how to transfer each one of these entities independently. To help explore these issues, the most recent version of the ALE supports game modes and difficulty settings."}, {"heading": "7.4.1 Modes and Difficulties in the Arcade Learning Environment", "text": "Originally, many Atari 2600 games had a default game mode and difficulty level that could be changed by select switches on the console. These mode/difficulty switches had different consequences such as changing the game dynamics or introducing new actions (see Figure 8). Until recently, the ALE allowed agents to play games only in their default mode and difficulty. The newest version of the ALE allows one to select among all different game modes and difficulties that are single player games. We call each mode-difficulty pair a flavor.\nThis new feature opens up research avenues by introducing dozens of new environments that are very similar. Because the underlying state representations across different flavors are probably highly related, we believe negative transfer is less likely, giving an easier setup\nfor transfer. The list of such games the ALE will initially support, and their number of flavors, is available in Appendix D."}, {"heading": "7.5 Off-Policy Learning", "text": "Off-policy learning algorithms seem to be brittle when applied to the ALE. Defazio and Graepel (2014) have reported divergence when using algorithms such as GQ(\u03bb) and Qlearning, for instance.\nBesides the proposal of new algorithms that are theoretically better behaved (e.g., Maei and Sutton, 2010), attempts to reduce divergence in off-policy learning currently consist of heuristics that try to decorrelate observations, such as the use of an experience replay buffer and the use of a target network in DQN (Mnih et al., 2015). Recent papers introduce changes in the update rules of Q-Learning to reduce overestimation of value functions (van Hasselt et al., 2016), new operators that increase the action-gap of value function estimates (Bellemare et al., 2016a), and more robust off-policy multi-step algorithms (Harutyunyan et al., 2016; Munos et al., 2016). However, besides a better theoretical understanding about convergence, stable (and practical) off-policy learning algorithms with function approximation are still an incomplete piece in the literature. Such improvements may free us from the increased complexity of using experience replay and/or target networks. Also, they would allow us to better reuse samples from policies that are very different from the one being learned."}, {"heading": "8. Conclusion", "text": "In this article we took a big picture look at how the Arcade Learning Environment is being used by the research community. We discussed the different evaluation methodologies that have been employed and how they have been frequently conflated in the literature. To further the progress in the field, we presented some methodological best practices and a new version of the Arcade Learning Environment that supports stochasticity and multiple game modes. We hope such methodological practices, with the new ALE, allow one to clearly distinguish between the different evaluation protocols. Also, we provide benchmark results following these methodological best practices that may serve as a point of comparison for future work in the ALE. We evaluated reinforcement learning algorithms that use linear and non-linear function approximation, and we hope to have promoted the discussion about\nsample efficiency by reporting algorithms\u2019 performance at different moments of the learning period. In the final part of this paper we concluded the big picture look we took by revisiting the challenges posed in the ALE\u2019s original article. We summarized the current state-of-theart and we highlighted five problems we consider to remain open: representation learning, planning and model-learning, exploration, transfer learning, and off-policy learning."}, {"heading": "Acknowledgements", "text": "The authors would like to thank David Silver and Tom Schaul for their thorough feedback on an earlier draft, and Re\u0301mi Munos, Will Dabney, Mohammad Azar, Hector Geffner, Jean Harb, and Pierre-Luc Bacon for useful discussions. We would also like to thank the several contributors to the Arcade Learning Environment GitHub repository, specially Nicolas Carion for implementing the mode and difficult selection and Ben Goodrich for providing a Python interface to the ALE. Yitao Liang implemented, with Marlos C. Machado, the Blob-PROST features. This work was supported by grants from Alberta Innovates \u2013 Technology Futures (AITF), through the Alberta Machine Intelligence Institute (Amii), and by the NSF grant IIS-1552533. Computing resources were provided by Compute Canada through CalculQue\u0301bec."}, {"heading": "Appendix A. The Brute", "text": "The Brute is an algorithm designed to exploit features of the original Arcade Learning Environment. Although developed independently by some of the authors, it shares many similarities with the trajectory tree method of Kearns et al. (1999). The Brute relies on the following observations:\n\u2022 The ALE is deterministic, episodic, and guarantees a unique starting state, and\n\u2022 in most Atari 2600 games, purpose matters more than individual actions, i.e., most Atari 2600 games have important high-level goals, but individual actions have little impact.\nThis algorithm is crude but leads to competitive performance in a number of games.\nA.1 Determinism and starting configurations\nA history is a sequence of actions and observations ht = a1, o1, a2, o2, . . . , ot, with the reward rt included in the observation ot.\n7 Histories describe sequential interactions between an agent and its environment. Although most of reinforcement learning focuses on a Markov state, a sufficient statistic of the history, we may also reason directly about this history. This approach is particularly convenient when the environment is partially observable (Kearns et al., 1999; Even-Dar, Kakade, & Mansour, 2005) or non-Markov (Hutter, 2005). Given a history ht, the transition function for an action a and subsequent observation o is\nPr(Ht+1 = ht, a, o |Ht = ht, At = a) = Pr(Ot+1 = o |Ht = ht, At = a).\nThis transition function induces a Markov decision process over histories. This MDP is an infinite history tree (Figure 9) whose states correspond to distinct histories.\nAn environment is deterministic if taking action a from history h always produces the same observation. It is episodic when we have zero-valued, absorbing states called terminal states. In the episodic setting learning proceeds by means of resets to one or many start states. Since the agent is informed of this reset, we equate it with the empty history (Figure 10). The Stella emulator is deterministic and, by the nature of Atari 2600 games, defines an episodic problem.\nDepending on the game, both software and hardware resets of the emulator may leave the system in a number of initial configurations. These different configurations arise from\n7In the interest of legibility and symmetry, we follow here the convention of beginning histories with an action. Note, however, that the ALE provides the agent with an initial frame o0.\nchanging timer values, registers, and memory contents at reset. However, these effects are game-dependent and difficult to control. In fact, the ALE contains code to avoid these effects and guarantee a unique starting configuration. We will use the term reproducible to describe an environment like the ALE that is deterministic, episodic, and has a unique starting configuration.\nDeterminism simplifies the learning of an environment\u2019s transition model: a single sample from each state-action pair is sufficient. Reproducibility allows us to effectively perform experiments on the history tree, answering questions of the form \u201cwhat would happen if I performed this exact sequence of actions?\u201d Not unlike Monte-Carlo tree search in a deterministic domain, each experiment begins at the root of the history tree and selects actions until a terminal state is reached, observing rewards along the way. Although it is possible to do the same in any episodic environment, learning stochastic transitions and reward functions is harder, not only because they require more samples but also because the probability of reaching a particular state (i.e., a history) is exponentially small in its length.\nA.2 Value estimation in a history tree\nAccording to Bellman\u2019s optimality equation (Bellman, 1957), the optimal value of executing action a in state s is\nq\u2217(s, a) = \u2211 s\u2032 p(s\u2032 | s, a) ( r(s, a, s\u2032) + \u03b3max b\u2208A q\u2217(s \u2032, b) ) .\nGiven a full history tree of finite depth, estimating the value for any history-action pair is simply a matter of bottom-up dynamic programming, since all states (i.e., histories) are transient. We can in fact leverage an important property of history trees: Consider a partially known history tree for a deterministic environment and define q\u0302(h, a) = \u2212\u221e for any unknown history-action pair. Then the equation\nq\u0302(h, a) = \u2211 h\u2032 p(h\u2032 |h, a) ( r(h, a, h\u2032) + \u03b3max b\u2208A q\u0302(h\u2032, b) ) (1)\ndefines a lower bound on q\u2217(h, a). When learning proceeds in episodes, we can update the lower bound q\u0302(h, a) iteratively. We begin at the terminal node hT corresponding to the episode just played. We then follow the episode steps aT\u22121, hT\u22121, aT\u22122, hT\u22122, . . . in reverse, updating q\u0302(ht, at) along this path,\nup to and including the starting history-action pair ( , a1). Since no information has been gathered outside of this path, all other action-values must remain unchanged, and this procedure is correct. If \u03c0(h) . = arg maxa\u2208A q\u0302(h, a) is stored at each node, then updating one episode requires time O(T ). Figure 11 illustrates the inclusion of a new episode into a partial history tree.\nThe Brute maintains a partial history tree that contains all visited histories. Each node, associated with a history, maintains an action-conditional transition function and reward function. Our implementation maintains the maximum likelihood estimate for both functions. This allows us to apply the Brute to stochastic environments, although q\u0302(h, a) is only guaranteed to be a proper lower bound if the subtree rooted at h is fully deterministic. This allowed us to apply the exact same algorithm in the context of sticky actions (Section 5). The value q\u0302(h, a) is maintained at each node and updated from the maximum likelihood estimates at the end of each episode, as described above.\nA.3 Narrow exploration\nIn Atari 2600 games, most actions have little individual effect. An agent can thus be more efficient if it focuses on a few narrow, promising trajectories rather than explore every detail of its environment. We may think of this focus as emphasizing purpose, i.e., achieving specific goals. The sequence of actions which maximizes the lower bound q\u0302(h, a) at each node is one such purposeful path. Since exploration is less relevant at nodes which have been visited often, we also progressively reduce the rate of exploration in the upper parts of the history tree.\nTo encourage the exploration of the most promising trajectory, the Brute\u2019s policy is an -greedy policy over q\u0302(h, a): with probability 1\u2212 , we choose one of the maximum-valued actions (breaking ties uniformly at random), and with probability we select an action uniformly at random. To encourage the exploration of narrow paths, is decreased with\nthe number of visits n(h) to a particular node in the history tree. Specifically,\n(h) = min\n{ 0.05\nlog(n(h) + 1) , 1.0\n} ."}, {"heading": "Appendix B. Experimental Setup", "text": "We used the same evaluation protocol, and parameters, in all experiments discussed in this article. In the next section we list the parameters used when defining the task in the Arcade Learning Environment. Later we discuss the parameters used by the Brute, Sarsa(\u03bb) + Blob-PROST, and DQN.\nB.1 Evaluation Protocol and Arcade Learning Environment Parameters\nWe report our results aiming at evaluating the robustness of the learned policy and of the learning algorithm. All results we report for the Brute and for Sarsa(\u03bb) + Blob-PROST are averaged over 24 trials, and all results we report for DQN are averaged over 5 trials. We evaluated DQN fewer times because its empirical validation is more expensive due to its requirement for specialized hardware (i.e., GPUs). We obtained the result of each trial by averaging over the last 100 episodes that led the agent to observe a total of k frames. Along this article we reported results for k equals to 10, 50, 100, and 200 million frames.\nThe unique parameter in the Arcade Learning Environment that is not fixed across all sections in this article is \u03c2, i.e., the amount of stochasticity present in the environment. We set \u03c2 to 0.0 in Section 5.1.1 while we set \u03c2 to 0.25 in the rest of the article. We do not use game-specific information. Episodes terminate after 5 minutes of gameplay or when the agent has lost all of its lives. Agents have access to all 18 primitive actions available in the ALE, not knowing if specific actions have any effect in the environment. Finally, all algorithms used a frame skip equals to 5 when playing the games. We summarize all parameters that are shared across all methods in Table 5.\nB.2 Parameters used by the Brute\nThe Brute has only two parameters to be set: \u03b3 and . We defined \u03b3 = 1.0 and = 0.005/ log(ni + 2), where ni denotes the number of times we have seen the history hi (see Appendix A for details). An important implementation detail is that we used Spooky Hash8 as our hashing function. We do not average current and previous ALE screens as other methods do.\nB.3 Parameters used by DQN\nDQN was ran using the same parameters used in its original paper (Mnih et al., 2015), with the exception of the frame skip, which we set to 5 after preliminary experiments, and , which we set to 0.01 due to the absence of an evaluation phase. Also, we did not use gamespecific information and we evaluated DQN in the continual learning setting, as discussed in Section B.1. Table 6 lists the values of all DQN parameters used throughout this article.\nB.4 Parameters used by Sarsa(\u03bb) + Blob-PROST\nWe evaluated Sarsa(\u03bb) + Blob-PROST using \u03b1 = 0.5, \u03bb = 0.9, and \u03b3 = 0.99. Agents followed an -greedy policy ( = 0.01). We did not sweep most of the parameters, using\n8http://burtleburtle.net/bob/hash/spooky.html\nthe parameters reported by Liang et al. (2016). However, we did verify, in preliminary experiments, the impact different values of frame skip have in this algorithm. We also verified whether color averaging impacts agents\u2019 performance. We decide to use a frame skip of 5 and to average colors. For most games, averaging screen colors significantly improves the results, while the impact of different number of frames to skip varies across games. Table 7 summarizes, for Sarsa(\u03bb) + Blob-PROST, all the parameters we use throughout this article."}, {"heading": "Appendix C. Complete Benchmark Results", "text": "We extend the results presented in Section 6 (Tables 3 and 4) by reporting algorithms\u2019 performance in 60 games supported by the ALE. We used the evaluation protocol described in Appendix B when generating the results below. Table 8 summarizes the performance of Sarsa(\u03bb) + Blob-PROST and Table 9 summarizes DQN\u2019s performance. The games originally used as training games by each method are highlighted with the \u2020 symbol. In Table 8, the list of games we used for training Sarsa(\u03bb) + Blob-PROST is longer than the one in Table 9 because we are reporting the training games used by Liang et al. (2016), which was the setting we initially replicated. The columns with an asterisk will be filled in later."}, {"heading": "Appendix D. Number of Game Modes and Difficulties in the Games", "text": "Supported by the Arcade Learning Environment"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "The Arcade Learning Environment (ALE) is an evaluation platform that poses the<lb>challenge of building AI agents with general competency across dozens of Atari 2600 games.<lb>It supports a variety of different problem settings and it has been receiving increasing<lb>attention from the scientific community, leading to some high-profile success stories such as<lb>the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at<lb>how the ALE is being used by the research community. We show how diverse the evaluation<lb>methodologies in the ALE have become with time, and highlight some key concerns when<lb>evaluating agents in the ALE. We use this discussion to present some methodological best<lb>practices and provide new benchmark results using these best practices. To further the<lb>progress in the field, we introduce a new version of the ALE that supports multiple game<lb>modes and provides a form of stochasticity we call sticky actions. We conclude this big<lb>picture look by revisiting challenges posed when the ALE was introduced, summarizing the<lb>state-of-the-art in various problems and highlighting problems that remain open.", "creator": "TeX"}}}