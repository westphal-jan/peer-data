{"id": "0908.0570", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Aug-2009", "title": "The Infinite Hierarchical Factor Regression Model", "abstract": "We proposals close timekeeping Bayesian increases fractal vehicle neither bulk any uncertainty in taken listed in factors, and three marriage divided factors. To accomplish see, we propose when sparse simplified among itself Indian Buffet Process more get now. followed client-server model over factors, international opened Kingman ' s side-on. We prohibit far model but 17 problems (levels analysis also reduce regression) in gene - interaction data psychological.", "histories": [["v1", "Wed, 5 Aug 2009 01:10:09 GMT  (78kb)", "http://arxiv.org/abs/0908.0570v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["piyush rai", "hal daum\u00e9 iii"], "accepted": true, "id": "0908.0570"}, "pdf": {"name": "0908.0570.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["piyush@cs.utah.edu", "hal@cs.utah.edu"], "sections": [{"heading": null, "text": "ar X\niv :0\n90 8.\n05 70\nv1 [\ncs .L\nG ]\n5 A\nug 2\nWe propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman\u2019s coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis."}, {"heading": "1 Introduction", "text": "Factor analysis is the task of explaining data by means of a set of latent factors. Factor regression couples this analysis with a prediction task, where the predictions are made solely on the basis of the factor representation. The latent factor representation achieves two-fold benefits: (1) discovering the latent process underlying the data; (2) simpler predictive modeling through a compact data representation. In particular, (2) is motivated by the problem of prediction in the \u201clarge P small N\u201d paradigm [1], where the number of featuresP greatly exceeds the number of examplesN , potentially resulting in overfitting.\nWe address three fundamental shortcomings of standard factor analysis approaches [2, 3, 4, 1]: (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis. Our motivation for this work stems from the task of reconstructing regulatory structure from gene-expression data. In this context, factors correspond to regulatory pathways. Our contributions thus parallel the needs of gene pathway modeling. In addition, we couple predictive modeling (for factor regression) within the factor analysis framework itself, instead of having to model it separately.\nOur factor regression model is fundamentally nonparametric. In particular, we treat the gene-tofactor relationship nonparametrically by proposing a sparse variant of the Indian Buffet Process (IBP) [5], designed to account for the sparsity of relevant genes (features). We couple this IBP with a hierarchical prior over the factors. This prior explains the fact that pathways are fundamentally related: some are involved in transcription, some in signaling, some in synthesis. The nonparametric nature of our sparse IBP requires that the hierarchical prior also be nonparametric. A natural choice is Kingman\u2019s coalescent [6], a popular distribution over infinite binary trees.\nSince our motivation is an application in bioinformatics, our notation and terminology will be drawn from that area. In particular, genes are features, samples are examples, and pathways are factors. However, our model is more general. An alternative application might be to a collaborative filtering problem, in which case our genes might correspond to movies, our samples might correspond to users and our pathways might correspond to genres. In this context, all three contributions of our model still make sense: we do not know how many movie genres there are; some genres are closely related (romance to comedy versus to action); many movies may be spurious."}, {"heading": "2 Background", "text": "Our model uses a variant of the Indian Buffet Process to model the feature-factor (i.e., gene-pathway) relationships. We further use Kingman\u2019s coalescent to model latent pathway hierarchies."}, {"heading": "2.1 Indian Buffet Process", "text": "The Indian Buffet Process [7] defines a distribution over infinite binary matrices, originally motivated by the need to model the latent factor structure of a given set of observations. In the standard form it is parameterized by a scale value, \u03b1. The distribution can be explained by means of a simple culinary analogy. Customers (in our context, genes) enter an Indian restaurant and select dishes (in our context, pathways) from an infinite array of dishes. The first customer selects Poisson(\u03b1) dishes. Thereafter, each incoming customer i selects a previously-selected dish k with a probability mk/(i\u2212 1), where mk is the number of previous customers who have selected dish k. Customer i then selects an additional Poisson(\u03b1/i) new dishes. We can easily define a binary matrix Z with value Zik = 1 precisely when customer i selects dish k. This stochastic process thus defines a distribution over infinite binary matrices.\nIt turn out [7] that the stochastic process defined above corresponds to an infinite limit of an exchangeable process over finite matrices with K columns. This distribution takes the form p(Z | \u03b1) = \u220fK\nk=1\n\u03b1 K \u0393(mk+ \u03b1 K )\u0393(P\u2212mk\u22121)\n\u0393(P+1+ \u03b1 K\n) , where mk = \u2211\ni Zik and P is the total number of customers. Taking K \u2192 \u221e yields the IBP. The IBP has several nice properties, the most important of which is exchangeablility. It is the exchangeablility (over samples) that makes efficient sampling algorithms possible. There also exists a two-parameter generalization to IBP where the second parameter \u03b2 controls the sharability of dishes."}, {"heading": "2.2 Kingman\u2019s Coalescent", "text": "Our model makes use of a latent hierarchical structure over factors; we use Kingman\u2019s coalescent [6] as a convenient prior distribution over hierarchies. Kingman\u2019s coalescent originated in the study of population genetics for a set of single-parent organisms. The coalescent is a nonparametric model over a countable set of organisms. It is most easily understood in terms of its finite dimensional marginal distributions over n individuals, in which case it is called an n-coalescent. We then take the limit n \u2192 \u221e. In our case, the individuals are factors.\nThe n-coalescent considers a population of n organisms at time t = 0. We follow the ancestry of these individuals backward in time, where each organism has exactly one parent at time t < 0. The n-coalescent is a continuous-time, partition-valued Markov process which starts with n singleton clusters at time t = 0 and evolves backward, coalescing lineages until there is only one left. We denote by ti the time at which the ith coalescent event occurs (note ti \u2264 0), and \u03b4i = ti\u22121 \u2212 ti the time between events (note \u03b4i > 0). Under the n-coalescent, each pair of lineages merges indepentently with exponential rate 1; so \u03b4i \u223c Exp (( n\u2212i+1 2 ))\n. With probability one, a random draw from the n-coalescent is a binary tree with a single root at t = \u2212\u221e and n individuals at time t = 0. We denote the tree structure by \u03c0. The marginal distribution over tree topologies is uniform and independent of coalescent times; and the model is infinitely exchangeable. We therefore consider the limit as n \u2192 \u221e, called the coalescent.\nOnce the tree structure is obtained, one can define an additional Markov process to evolve over the tree. One common choice is a Brownian diffusion process. In Brownian diffusion in D dimensions, we assume an underlying diffusion covariance of \u039b \u2208 RD\u00d7D p.s.d. The root is a D-dimensional vector drawn z. Each non-root node in the tree is drawn Gaussian with mean equal to the value of the parent, and variance \u03b4i\u039b, where \u03b4i is the time that has passed.\nRecently, Teh et al. [8] proposed efficient bottom-up agglomerative inference algorithms for the coalescent. These (approximately) maximize the probability of \u03c0 and \u03b4s, marginalizing out internal nodes by Belief Propagation. If we associate with each node in the tree a mean y and variance v message, we update messages as Eq (1), where i is the current node and li and ri are its children.\nvi = [ (vli + (tli \u2212 ti)\u039b) \u22121 + (vri + (tri \u2212 ti)\u039b)\n\u22121 ]\u22121\n(1)\nyi = [ yli(vli + (tli \u2212 ti)\u039b) \u22121 + yri(vri + (tri \u2212 ti)\u039b)\n\u22121 ]\u22121\nvi"}, {"heading": "3 Nonparametric Bayesian Factor Regression", "text": "Recall the standard factor analysis problem: X = AF + E, for standardized data X. X is a P \u00d7 N matrix consisting of N samples [x1, ...,xN ] of P features each. A is the factor loading matrix of size P \u00d7K and F = [f1, ...,fN ] is the factor matrix of size K \u00d7N . E = [e1, ..., eN ] is the matrix of idiosyncratic variations. K , the number of factors, is known.\nRecall that our goal is to treat the factor analysis problem nonparametrically, to model feature relevance, and to model hierarchical factors. For expository purposes, it is simplest to deal with each of these issues in turn. In our context, we begin by modeling the gene-factor relationship nonparametrically (using the IBP). Next, we propose a variant of IBP to model gene relevance. We then present the hierarchical model for inferring factor hierarchies. We conclude with a presentation of the full model and our mechanism for modifying the factor analysis problem to factor regression."}, {"heading": "3.1 Nonparametric Gene-Factor Model", "text": "We begin by directly using the IBP to infer the number of factors. Although IBP has been applied to nonparametric factor analysis in the past [5], the standard IBP formulation places IBP prior on the factor matrix (F) associating samples (i.e. a set of features) with factors. Such a model assumes that the sample-fctor relationship is sparse. However, this assumption is inappropriate in the geneexpression context where it is not the factors themselves but the associations among genes and factors (i.e., the factor loading matrix A) that are sparse. In such a context, each sample depends on all the factors but each gene within a sample usually depends only on a small number of factors.\nThus, it is more appropriate to model the factor loading matrix (A) with the IBP prior. Note that since A and F are related with each other via the number of factors K, modeling A nonparametrically allows our model to also have an unbounded number of factors.\nFor most gene-expression problems [1], a binary factor loadings matrix (A) is inappropriate. Therefore, we instead use the Hadamard (element-wise) product of a binary matrix Z and a matrix V of reals. Z and V are of the same size as A. The factor analysis model, for each sample i, thus becomes: xi = (Z \u2299 V )f i + ei. We have Z \u223c IBP(\u03b1, \u03b2). \u03b1 and \u03b2 are IBP hyperparameters and have vague gamma priors on them. Our initial model assumes no factor hierarchies and hence the prior over V would simply be a Gaussian: V \u223c Nor(0, \u03c32vI) with an inverse-gamma prior on \u03c3v . F has a zero mean, unit variance Gaussian prior, as used in standard factor analysis. Finally, ei = Nor(0,\u03a8) models the idiosyncratic variations of genes where \u03a8 is a P \u00d7 P diagonal matrix (diag(\u03a81, ...,\u03a8P )). Each entry \u03a8P has an inverse-gamma prior on it."}, {"heading": "3.2 Feature Selection Prior", "text": "Typical gene-expression datasets are of the order of several thousands of genes, most of which are not associated with any pathway (factor). In the above, these are accounted for only by the idiosyncratic noise term. A more realistic model is that certain genes simply do not participate in the factor analysis: for a culinary analogy, the genes enter the restaurant and leave before selecting any dishes. Those genes that \u201cleave\u201d, we term \u201cspurious.\u201d We add an additional prior term to account for such spurious genes; effectively leading to a sparse solution (over the rows of the IBP matrix). It is important to note that this notion of sparsity is fundamentally different from the conventional notion of sparsity in the IBP. The sparsity in IBP is over columns, not rows. To see the difference, recall that the IBP contains a \u201crich get richer\u201d phenomenon: frequently selected factors are more likely to get reselected. Consider a truly spurious gene and ask whether it is likely to select any factors. If some factor k is already frequently used, then a priori this gene is more likely to select it. The only downside to selecting it is the data likelihood. By setting the corresponding value in V to zero, there is no penalty.\nOur sparse-IBP prior is identical to the standard IBP prior with one exception. Each customer (gene) p is associated with Bernoulli random variable Tp that indicates whether it samples any dishes. The T vector is given a parameter \u03c1, which, in turn, is given a Beta prior with parameters a, b."}, {"heading": "3.3 Hierarchical Factor Model", "text": "In our basic model, each column of the matrix Z (and the corresponding column in V ) is associated with a factor. These factors are considered unrelated. To model the fact that factors are, in fact, re-\nlated, we introduce a factor hierarchy. Kingman\u2019s coalescent [6] is an attractive prior for integration with IBP for several reasons. It is nonparametric and describes exchangeable distributions. This means that it can model a varying number of factors. Moreover, efficient inference algorithms exist [8]."}, {"heading": "3.4 Full Model and Extension to Factor Regression", "text": "Our proposed graphical model is depicted in Figure 1. The key aspects of this model are: the IBP prior over Z, the sparse binary vector T, and the Coalescent prior over V.\nIn standard Bayesian factor regression [1], factor analysis is followed by the regression task. The regression is performed only on the basis of F, rather than the full data X. For example, a simple linear regression problem would involve estimating a K-dimensional parameter vector \u03b8 with regression value \u03b8\u22a4F. Our model, on the other hand, integrates factor regression component in the nonparametric factor analysis framework itself. We do so by prepending the responses yi to the expression vector xi and joining the training and test data (see figure 2). The unknown responses in the test data are treated as missing variables to be iteratively imputed in our MCMC inference procedure. It is straightforward to see that it is equivalent to fitting another sparse model relating factors to responses. Our model thus allows the factor analysis to take into account the regression task as well. In case of binary responses, we add an extra probit regression step to predict binary outcomes from real-valued responses."}, {"heading": "4 Inference", "text": "We use Gibbs sampling with a few M-H steps. The Gibbs distributions are summarized here.\nSampling the IBP matrix Z: Sampling Z consists of sampling existing dishes, proposing new dishes and accepting or rejecting them based on the acceptance ratio in the associated M-H step. For sampling existing dishes, an entry in Z is set as 1 according to p(Zik = 1|X, Z\u2212ik,V,F,\u03a8) \u221d\nm\u2212i,k (P+\u03b2\u22121)p(X|Z,V,F,\u03a8) whereas it is set as 0 according to p(Zik = 0|X, Z\u2212ik,V,F,\u03a8) \u221d P+\u03b2\u22121\u2212m\u2212i,k\n(P+\u03b2\u22121) p(X|Z,V,F,\u03a8). m\u2212i,k = \u2211 j 6=i Zjk is how many other customers chose dish k.\nFor sampling new dishes, we use an M-H step where we simultaneously propose \u03b7 = (Knew, V new, Fnew) where Knew \u223c Poisson(\u03b1\u03b2/(\u03b2 + P \u2212 1)). We accept the proposal with an acceptance probability (following [9]) given by a = min{1, p(rest|\u03b7\n\u2217) p(rest|\u03b7) }. Here, p(rest|\u03b7) is the\nlikelihood of the data given parameters \u03b7. We propose V new from its prior (either Gaussian or Coalescent) but, for faster mixing, we propose Fnew from its posterior.\nSampling V new from the coalescent is slightly involved. As shown pictorially in figure 3, proposing a new column of V corresponds to adding a new leaf node to the existing coalescent tree. In particular, we need to find a sibling (s) to the new node y\u2032 and need to find an insertion point on the branch joining the sibling s to its parent p (the grandparent of y\u2032). Since the marginal distribution over trees under the coalescent is uniform, the sibling s is chosen uniformly over nodes in the tree. We then use importance sampling to select an insertion time for the new node y\u2032 between ts and tp, according to the exponential distribution given by the coalescent prior (our proposal distribution is uniform). This gives an insertion point in the tree, which corresponds to the new parent of y\u2032.\nWe denote this new parent by p\u2032 and the time of insertion as t. The predictive density of the newly inserted node y\u2032 can be obtained by marginalizing the parent p\u2032. This yields Nor(y0,v0), given by:\nv0 = [(vs + (ts \u2212 t)\u039b) \u22121 + (vp + (t\u2212 tp)\u039b) \u22121]\u22121\ny0 = [ys/(vs + (ts \u2212 t)\u039b) + yp/(vp + (tp \u2212 t)\u039b)]v0\nHere, ys and vs are the messages passed up through the tree, while yp and vp are the messages passed down through the tree (compare to Eq (1)).\nSampling the sparse IBP vector T: In the sparse IBP prior, recall that we have an additional P -many variables Tp, indicating whether gene p \u201ceats\u201d any dishes. Tp is drawn from Bernoulli with parameter \u03c1, which, in turn, is given a Bet(a, b) prior. For inference, we collapse \u03c1 and \u03a8 and get Gibbs posterior over Tp of the form p(Tp = 1|.) \u221d (a + \u2211\nq 6=p Tp)Stu(xp|(Zp \u2299 Vp)F , g/h, g)) and p(Tp = 0|.) \u221d (b + P \u2212 \u2211\nq 6=p Tq)Stu(xp|0, g/h, g), where Stu is the non-standard Student\u2019s t-distribution. g, h are hyperparameters of the inverse-gamma prior on the entries of \u03a8.\nSampling the real valued matrix V: For the case when V has a Gaussian prior on it, we sample V from its posterior p(Vg,j |X,Z,F,\u03a8) \u221d\nNor(Vg,j |\u00b5g,j ,\u03a3g,j), where \u03a3g,j = ( \u2211N\ni=1 F 2j,i \u03a8g + 1 \u03c32v )\u22121 and\n\u00b5g,j = \u03a3g,j( \u2211N i=1 Fj,iX \u2217 g,j)\u03a8 \u22121 g . We define X \u2217 g,j = Xg,i \u2212 \u2211K\nl=1,l 6=j(Ag,lVg,l)Fl,i, and A = Z \u2299 V. The hyperparameter \u03c3v on V has an inverse-gamma prior and posterior also has the same form. For the case with coalescent prior on V, we have\n\u03a3g,j = ( \u2211N\ni=1 F 2j,i \u03a8g + 1 v0j )\u22121 and \u00b5g,j = \u03a3g,j( \u2211N i=1 Fj,iX \u2217 g,j)(\u03a8g + y0g,j v0j\n)\u22121, where y0 and v0 are the Gaussian posteriors of the leaf node added in the coalescent tree (see Eq (1)), which corresponds to the column of V being sampled.\nSampling the factor matrix F: We sample for F from its posterior p(F|X,Z,V,\u03a8) \u221d Nor(F|\u00b5,\u03a3) where \u00b5 = AT(AAT +\u03a8)\u22121X and \u03a3 = I \u2212AT(AAT +\u03a8)\u22121A, where A = Z \u2299 V\nSampling the idiosyncratic noise term: We place an inverse-gamma prior on the diagonal entries of \u03a8 and the posterior too is inverse-gamma: p(\u03a8p|.) \u221d IG(g + N2 , h\n1+h 2 tr(ET E)\n), where E =\nX \u2212 (Z\u2299V)F.\nSampling IBP parameters: We sample the IBP parameter \u03b1 from its posterior: p(\u03b1|.) \u223c Gam(K+ + a, b1+bHP (\u03b2) ), where K+ is the number of active features at any moment and HP (\u03b2) = \u2211P\ni=1 1/(\u03b2 + i\u2212 1). \u03b2 is sampled from a prior proposal using an M-H step.\nSampling the Factor Tree: Use the Greedy-Rate1 algorithm [8]."}, {"heading": "5 Related Work", "text": "A number of probabilistic approaches have been proposed in the past for the problem of generegulatory network reconstruction [2, 3, 4, 1]. Some take into account the information on the prior network topology [2], which is not always available. Most assume the number of factors is known. To get around this, one can perform model selection via Reversible Jump MCMC [10] or evolutionary stochastic model search [11]. Unfortunately, these methods are often difficult to design and may take quite long to converge. Moreover, they are difficult to integrate with other forms of prior knowledge (eg., factor hierarchies). A somewhat similar approach to ours is the infinite independent component analysis (iICA) model of [12] which treats factor analysis as a special case of ICA. However, their model is limited to factor analysis and does not take into account feature selection, factor hierarchy and factor regression. As a generalization to the standard ICA model, [13] proposed a model in which the components can be related via a tree-structured graphical model. It, however, assumes a fixed number of components.\nStructurally, our model with Gaussian-V (i.e. no hierarchy over factors) is most similar to the Bayesian Factor Regression Model (BFRM) of [1]. BFRM assumes a sparsity inducing mixture prior on the factor loading matrix A. Specifically, Apk \u223c (1 \u2212 \u03c0pk)\u03b40(Apk) + \u03c0pkNor(Apk|0, \u03c4k)\nwhere \u03b40() is a point mass centered at zero. To complete the model specification, they define \u03c0pk \u223c (1\u2212\u03c1k)\u03b40(\u03c0pk)+\u03c1kBet(\u03c0pk|sr, s(1\u2212 r)) and \u03c1k \u223c Bet(\u03c1k|av, a(1\u2212v)). Now, integrating out \u03c0pk gives: Apk \u223c (1\u2212v\u03c1k)\u03b40(Apk)+v\u03c1kNor(Apk|0, \u03c4k). It is interesting to note that the nonparametric prior of our model (factor loading matrix defined as A = Z\u2299V) is actually equivalent to the (parametric) sparse mixture prior of the BFRM as K \u2192\u221e. To see this, note that our prior on the factor loading matrix A (composed of Z having an IBP prior, and V having a Gaussian prior), can be written as Apk \u223c (1\u2212\u03c1k)\u03b40(Apk)+\u03c1kNor(Apk|0, \u03c32v), if we define \u03c1k \u223c Bet(1, \u03b1\u03b2/K). It is easy to see that, for BFRM where \u03c1k \u223c Bet(av, a(1\u2212 v)), setting a = 1+\u03b1\u03b2/K and v = 1\u2212\u03b1\u03b2/(aK) recovers our model in the limiting case when K\u2192\u221e."}, {"heading": "6 Experiments", "text": "In this section, we report our results on synthetic and real datasets. We compare our nonparametric approach with the evolutionary search based approach proposed in [11], which is the nonparametric extension to BFRM.\nWe used the gene-factor connectivity matrix of E-coli network (described in [14]) to generate a synthetic dataset having 100 samples of 50 genes and 8 underlying factors. Since we knew the ground truth for factor loadings in this case, this dataset was ideal to test for efficacy in recovering the factor loadings (binding sites and number of factors). We also experimented with a real geneexpression data which is a breast cancer dataset having 251 samples of 226 genes and 5 prominent underlying factors (we know this from domain knowledge)."}, {"heading": "6.1 Nonparametric Gene-Factor Modeling and Variable Selection", "text": "For the synthetic dataset generated by the E-coli network, the results are shown in figure 4 comparing the actual network used to generate the data and the inferred factor loading matrix. As shown in figure 4, we recovered exactly the same number (8) of factors, and almost exactly the same factor loadings (binding sites and number of factors) as the ground truth. In comparison, the evolutionary search based approach overestimated the number of factors and the inferred loadings clearly seem to be off from the actual loadings (even modulo column permutations).\nOur results on real data are shown in figure 5. To see the effect of variable selection for this data, we also introduced spurious genes by adding 50 random features in each sample. We observe the following: (1) Without variable selection being on, spurious genes result in an overestimated number of factors and falsely discovered factor loadings for spurious genes (see figure 5(a)), (2) Variable selection, when on, effectively filters out spurious genes, without overestimating the number of factors (see figure 5(b)). We also investigated the effect of noise on the evolutionary search based approach and it resulted in an overestimated number of factor, plus false discovered factor loadings for spurious genes (see figure 5(c)). To conserve space, we do not show here the cases when there are no spurious genes in the data but it turns out that variable selection does not filter out any of 226 relevant genes in such a case."}, {"heading": "6.2 Hierarchical Factor Modeling", "text": "Our results with hierarchical factor modeling are shown in figure 6 for synthetic and real data. As shown, the model correctly infers the gene-factor associations, the number of factors, and the factor\nhierarchy. There are several ways to interpret the hierarchy. From the factor hierarchy for E-coli data (figure 6), we see that column-2 (corresponding to factor-2) of the V matrix is the most prominent one (it regulates the highest number of genes), and is closest to the tree-root, followed by column2, which it looks most similar to. Columns corresponding to lesser prominent factors are located further down in the hierarchy (with appropriate relatedness). Figure 6 (d) can be interpreted in a similar manner for breast-cancer data. The hierarchy can be used to find factors in order of their prominence. The higher we chop off the tree along the hierarchy, the more prominent the factors, we discover, are. For instance, if we are only interested in top 2 factors in E-coli data, we can chop off the tree above the sixth coalescent point. This is akin to the agglomerative clustering sense which is usually done post-hoc. In contrast, our model discovers the factor hierarchies as part of the inference procedure itself. At the same time, there is no degradation of data reconstruction (in mean squared error sense) and the log-likelihood, when compared to the case with Gaussian prior on V (see figure 7 - they actually improve). We also show in section 6.3 that hierarchical modeling results in better predictive performance for the factor regression task. Empirical evidences also suggest that the factor hierarchy leads to faster convergence since most of the unlikely configurations will never be visited as they are constrained by the hierarchy."}, {"heading": "6.3 Factor Regression", "text": "We report factor regression results for binary and real-valued responses and compare both variants of our model (Gaussian V and Coalescent V) against 3 different approaches: logistic regression, BFRM, and fitting a separate predictive model on the discovered factors (see figure 7 (c)). The breast-cancer dataset had two binary response variables (phenotypes) associated with each sample. For this binary prediction task, we split the data into training-set of 151 samples and test-set of 100 samples. This is essentially a transduction setting as described in section 3.4 and shown in figure 2. For real-valued prediction task, we treated a 30x20 block of the data matrix as our held-out data and predicted it based on the rest of the entries in the matrix. This method of evaluation is akin to the task of image reconstruction [15]. The results are averaged over 20 random initializations and the low error variances suggest that our method is fairly robust w.r.t. initializations.\n4"}, {"heading": "7 Conclusions and Discussion", "text": "We have presented a fully nonparametric Bayesian approach to sparse factor regression, modeling the gene-factor relationship using a sparse variant of the IBP. However, the true power of nonparametric priors is evidenced by the ease of integration of task-specific models into the framework. Both gene selection and hierarchical factor modeling are straightforward extensions in our model that do not significantly complicate the inference procedure, but lead to improved model performance and more understandable outputs. We applied Kingman\u2019s coalescent as a hierarhical model on V, the matrix modulating the expression levels of genes in factors. An interesting open question is whether the IBP can, itself, be modeled hierarchically."}], "references": [{"title": "Bayesian Factor Regression Models in the \u201cLarge p, Small n", "author": ["M. West"], "venue": "Paradigm. In Bayesian Statistics", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Bayesian Sparse Hidden Components Analysis for Transcription Regulation Networks", "author": ["C. Sabatti", "G. James"], "venue": "Bioinformatics 22,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Probabilistic Inference of Transcription Factor Concentrations and Gene-specific", "author": ["G. Sanguinetti", "N.D. Lawrence", "M. Rattray"], "venue": "Regulatory Activities. Bioinformatics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "A Bayesian Approach to Reconstructing Genetic Regulatory Networks with Hidden Factors", "author": ["M.J. Beal", "F. Falciani", "Z. Ghahramani", "C. Rangel", "D.L. Wild"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Bayesian Nonparametric Latent Feature Models", "author": ["Z. Ghahramani", "T.L. Griffiths", "P. Sollich"], "venue": "In Bayesian Statistics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "The coalescent", "author": ["J.F.C. Kingman"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Infinite Latent Feature Models and the Indian Buffet Process", "author": ["T. Griffiths", "Z. Ghahramani"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Bayesian Agglomerative Clustering with Coalescents", "author": ["Y.W. Teh", "H. Daum\u00e9 III", "D.M. Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Modeling Dyadic Data with Binary Latent Factors", "author": ["E. Meeds", "Z. Ghahramani", "R.M. Neal", "S.T. Roweis"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Reversible jump markov chain monte carlo computation and bayesian model determination", "author": ["P. Green"], "venue": "Biometrica 82,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "High-Dimensional Sparse Factor Modelling - Applications in Gene Expression Genomics", "author": ["C. Carvalho", "J. Lucas", "Q. Wang", "J. Chang", "J. Nevins", "M. West"], "venue": "JASA,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Infinite Sparse Factor Analysis and Infinite Independent Components Analysis", "author": ["D. Knowles", "Z. Ghahramani"], "venue": "ICA", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Beyond independent components: trees and clusters", "author": ["Francis R. Bach", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Factor Analysis for Gene Regulatory Networks and Transcription Factor Activity Profiles", "author": ["I. Pournara", "L. Wernisch"], "venue": "BMC Bioinformatics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Non-linear CCA and PCA by Alignment of Local Models", "author": ["J.J. Verbeek", "S.T. Roweis", "N. Vlassis"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "In particular, (2) is motivated by the problem of prediction in the \u201clarge P small N\u201d paradigm [1], where the number of featuresP greatly exceeds the number of examplesN , potentially resulting in overfitting.", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "We address three fundamental shortcomings of standard factor analysis approaches [2, 3, 4, 1]: (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis.", "startOffset": 81, "endOffset": 93}, {"referenceID": 2, "context": "We address three fundamental shortcomings of standard factor analysis approaches [2, 3, 4, 1]: (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis.", "startOffset": 81, "endOffset": 93}, {"referenceID": 3, "context": "We address three fundamental shortcomings of standard factor analysis approaches [2, 3, 4, 1]: (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis.", "startOffset": 81, "endOffset": 93}, {"referenceID": 0, "context": "We address three fundamental shortcomings of standard factor analysis approaches [2, 3, 4, 1]: (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis.", "startOffset": 81, "endOffset": 93}, {"referenceID": 4, "context": "In particular, we treat the gene-tofactor relationship nonparametrically by proposing a sparse variant of the Indian Buffet Process (IBP) [5], designed to account for the sparsity of relevant genes (features).", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "A natural choice is Kingman\u2019s coalescent [6], a popular distribution over infinite binary trees.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "1 Indian Buffet Process The Indian Buffet Process [7] defines a distribution over infinite binary matrices, originally motivated by the need to model the latent factor structure of a given set of observations.", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "It turn out [7] that the stochastic process defined above corresponds to an infinite limit of an exchangeable process over finite matrices with K columns.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "2 Kingman\u2019s Coalescent Our model makes use of a latent hierarchical structure over factors; we use Kingman\u2019s coalescent [6] as a convenient prior distribution over hierarchies.", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "[8] proposed efficient bottom-up agglomerative inference algorithms for the coalescent.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Although IBP has been applied to nonparametric factor analysis in the past [5], the standard IBP formulation places IBP prior on the factor matrix (F) associating samples (i.", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": "For most gene-expression problems [1], a binary factor loadings matrix (A) is inappropriate.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "Kingman\u2019s coalescent [6] is an attractive prior for integration with IBP for several reasons.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "Moreover, efficient inference algorithms exist [8].", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "In standard Bayesian factor regression [1], factor analysis is followed by the regression task.", "startOffset": 39, "endOffset": 42}, {"referenceID": 8, "context": "We accept the proposal with an acceptance probability (following [9]) given by a = min{1, p(rest|\u03b7 \u2217) p(rest|\u03b7) }.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "Sampling the Factor Tree: Use the Greedy-Rate1 algorithm [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "5 Related Work A number of probabilistic approaches have been proposed in the past for the problem of generegulatory network reconstruction [2, 3, 4, 1].", "startOffset": 140, "endOffset": 152}, {"referenceID": 2, "context": "5 Related Work A number of probabilistic approaches have been proposed in the past for the problem of generegulatory network reconstruction [2, 3, 4, 1].", "startOffset": 140, "endOffset": 152}, {"referenceID": 3, "context": "5 Related Work A number of probabilistic approaches have been proposed in the past for the problem of generegulatory network reconstruction [2, 3, 4, 1].", "startOffset": 140, "endOffset": 152}, {"referenceID": 0, "context": "5 Related Work A number of probabilistic approaches have been proposed in the past for the problem of generegulatory network reconstruction [2, 3, 4, 1].", "startOffset": 140, "endOffset": 152}, {"referenceID": 1, "context": "Some take into account the information on the prior network topology [2], which is not always available.", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "To get around this, one can perform model selection via Reversible Jump MCMC [10] or evolutionary stochastic model search [11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "To get around this, one can perform model selection via Reversible Jump MCMC [10] or evolutionary stochastic model search [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 11, "context": "A somewhat similar approach to ours is the infinite independent component analysis (iICA) model of [12] which treats factor analysis as a special case of ICA.", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "As a generalization to the standard ICA model, [13] proposed a model in which the components can be related via a tree-structured graphical model.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "no hierarchy over factors) is most similar to the Bayesian Factor Regression Model (BFRM) of [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 10, "context": "We compare our nonparametric approach with the evolutionary search based approach proposed in [11], which is the nonparametric extension to BFRM.", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "We used the gene-factor connectivity matrix of E-coli network (described in [14]) to generate a synthetic dataset having 100 samples of 50 genes and 8 underlying factors.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "This method of evaluation is akin to the task of image reconstruction [15].", "startOffset": 70, "endOffset": 74}], "year": 2009, "abstractText": "We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman\u2019s coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}