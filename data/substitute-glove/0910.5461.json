{"id": "0910.5461", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2009", "title": "Anomaly Detection with Score functions based on Nearest Neighbor Graphs", "abstract": "We proposals after films non - parametric predictive aberration detection algorithm besides to geometrical maps the through score functionality distinguishes from nearest comes subset coming $ d $ - to nominal data. Anomalies smaller declared whenever for score one a second sample rise below $ \\ gamma $, by instance supposed failed be the desired false flare keeping. The sustained anomaly detector, because coming those quadratically calculation in thought time much uniformly similar turned its the specified false uproar despite, $ \\ alpha $, should the possibility after rest anomaly density is it crust within the token both a a density. Our algorithm way error-prone oriented, initially linear ago dimension and equation for users size. It does not giving choosing involve inputs constants cannot exists approximation enrolled and it want can to local larger such turned many way days dimensionality. We demonstrate, algorithm on both artificial and looking data breaking second than analogous feature enclosed.", "histories": [["v1", "Wed, 28 Oct 2009 18:46:41 GMT  (73kb)", "http://arxiv.org/abs/0910.5461v1", "10 pages, 10 figures, accepted by NIPS 2009"]], "COMMENTS": "10 pages, 10 figures, accepted by NIPS 2009", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["manqi zhao", "venkatesh saligrama"], "accepted": true, "id": "0910.5461"}, "pdf": {"name": "0910.5461.pdf", "metadata": {"source": "CRF", "title": "Anomaly Detection with Score functions based on Nearest Neighbor Graphs", "authors": ["Manqi Zhao"], "emails": ["mqzhao@bu.edu", "srv@bu.edu"], "sections": [{"heading": null, "text": "ar X\niv :0\n91 0.\n54 61\nv1 [\ncs .L\nG ]\n2 8"}, {"heading": "1 Introduction", "text": "Anomaly detection involves detecting statistically significant deviations of test data from nominal distribution. In typical applications the nominal distribution is unknown and generally cannot be reliably estimated from nominal training data due to a combination of factors such as limited data size and high dimensionality.\nWe propose an adaptive non-parametric method for anomaly detection based on score functions that maps data samples to the interval [0, 1]. Our score function is derived from a K-nearest neighbor graph (K-NNG) on n-point nominal data. Anomaly is declared whenever the score of a test sample falls below \u03b1 (the desired false alarm error). The efficacy of our method rests upon its close connection to multivariate p-values. In statistical hypothesis testing, p-value is any transformation of the feature space to the interval [0, 1] that induces a uniform distribution on the nominal data. When test samples with p-values smaller than \u03b1 are declared as anomalies, false alarm error is less than \u03b1.\nWe develop a novel notion of p-values based on measures of level sets of likelihood ratio functions. Our notion provides a characterization of the optimal anomaly detector, in that, it is uniformly most powerful for a specified false alarm level for the case when the anomaly density is a mixture of the nominal and a known density. We show that our score function is asymptotically consistent, namely, it converges to our multivariate p-value as data length approaches infinity.\nAnomaly detection has been extensively studied. It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature. Approaches to anomaly detection can be grouped into several categories. In parametric approaches [7] the nominal densities are assumed to come from a parameterized family and generalized likelihood ratio tests are used for detecting deviations from nominal. It is difficult to use parametric approaches when the distribution is unknown and data is limited. A K-nearest neighbor\n(K-NN) anomaly detection approach is presented in [3, 8]. There an anomaly is declared whenever the distance to the K-th nearest neighbor of the test sample falls outside a threshold. In comparison our anomaly detector utilizes the global information available from the entire K-NN graph to detect deviations from the nominal. In addition it has provable optimality properties. Learning theoretic approaches attempt to find decision regions, based on nominal data, that separate nominal instances from their outliers. These include one-class SVM of Scho\u0308lkopf et. al. [9] where the basic idea is to map the training data into the kernel space and to separate them from the origin with maximum margin. Other algorithms along this line of research include support vector data description [10], linear programming approach [1], and single class minimax probability machine [11]. While these approaches provide impressive computationally efficient solutions on real data, it is generally difficult to precisely relate tuning parameter choices to desired false alarm probability.\nScott and Nowak [12] derive decision regions based on minimum volume (MV) sets, which does provide Type I and Type II error control. They approximate (in appropriate function classes) level sets of the unknown nominal multivariate density from training samples. Related work by Hero [13] based on geometric entropic minimization (GEM) detects outliers by comparing test samples to the most concentrated subset of points in the training sample. This most concentrated set is the K-point minimum spanning tree(MST) for n-point nominal data and converges asymptotically to the minimum entropy set (which is also the MV set). Nevertheless, computing K-MST for n-point data is generally intractable. To overcome these computational limitations [13] proposes heuristic greedy algorithms based on leave-one out K-NN graph, which while inspired by K-MST algorithm is no longer provably optimal. Our approach is related to these latter techniques, namely, MV sets of [12] and GEM approach of [13]. We develop score functions on K-NNG which turn out to be the empirical estimates of the volume of the MV sets containing the test point. The volume, which is a real number, is a sufficient statistic for ensuring optimal guarantees. In this way we avoid explicit high-dimensional level set computation. Yet our algorithms lead to statistically optimal solutions with the ability to control false alarm and miss error probabilities.\nThe main features of our anomaly detector are summarized. (1) Like [13] our algorithm scales linearly with dimension and quadratic with data size and can be applied to high dimensional feature spaces. (2) Like [12] our algorithm is provably optimal in that it is uniformly most powerful for the specified false alarm level, \u03b1, for the case that the anomaly density is a mixture of the nominal and any other density (not necessarily uniform). (3) We do not require assumptions of linearity, smoothness, continuity of the densities or the convexity of the level sets. Furthermore, our algorithm adapts to the inherent manifold structure or local dimensionality of the nominal density. (4) Like [13] and unlike other learning theoretic approaches such as [9, 12] we do not require choosing complex tuning parameters or function approximation classes."}, {"heading": "2 Anomaly Detection Algorithm: Score functions based on K-NNG", "text": "In this section we present our basic algorithm devoid of any statistical context. Statistical analysis appears in Section 3. Let S = {x1, x2, \u00b7 \u00b7 \u00b7 , xn} be the nominal training set of size n belonging to the unit cube [0, 1]d. For notational convenience we use \u03b7 and xn+1 interchangeably to denote a test point. Our task is to declare whether the test point is consistent with nominal data or deviates from the nominal data. If the test point is an anomaly it is assumed to come from a mixture of nominal distribution underlying the training data and another known density (see Section 3).\nLet d(x, y) be a distance function denoting the distance between any two points x, y \u2208 [0, 1]d. For simplicity we denote the distances by dij = d(xi, xj). In the simplest case we assume the distance function to be Euclidean. However, we also consider geodesic distances to exploit the underlying manifold structure. The geodesic distance is defined as the shortest distance on the manifold. The Geodesic Learning algorithm, a subroutine in Isomap [14, 15] can be used to efficiently and consistently estimate the geodesic distances. In addition by means of selective weighting of different coordinates note that the distance function could also account for pronounced changes in local dimensionality. This can be accomplished for instance through Mahalanobis distances or as a by product of local linear embedding [16]. However, we skip these details here and assume that a suitable distance metric is chosen.\nOnce a distance function is defined our next step is to form a K nearest neighbor graph (K-NNG) or alternatively an \u01eb neighbor graph (\u01eb-NG). K-NNG is formed by connecting each xi to the K closest\npoints {xi1 , \u00b7 \u00b7 \u00b7 , xiK } in S \u2212 {xi}. We then sort the K nearest distances for each xi in increasing order di,i1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 di,iK and denote RS(xi) = di,iK , that is, the distance from xi to its K-th nearest neighbor. We construct \u01eb-NG where xi and xj are connected if and only if dij \u2264 \u01eb. In this case we define NS(xi) as the degree of point xi in the \u01eb-NG.\nFor the simple case when the anomalous density is an arbitrary mixture of nominal and uniform density1 we consider the following two score functions associated with the two graphs K-NNG and \u01eb-NNG respectively. The score functions map the test data \u03b7 to the interval [0, 1].\nK-LPE: p\u0302K(\u03b7) = 1\nn\nn \u2211\ni=1\nI{RS(\u03b7)\u2264RS(xi)} (1)\n\u01eb-LPE: p\u0302\u01eb(\u03b7) = 1\nn\nn \u2211\ni=1\nI{NS(\u03b7)\u2265NS(xi)} (2)\nwhere I{\u00b7} is the indicator function.\nFinally, given a pre-defined significance level \u03b1 (e.g., 0.05), we declare \u03b7 to be anomalous if p\u0302K(\u03b7), p\u0302\u01eb(\u03b7) \u2264 \u03b1. We call this algorithm Localized p-value Estimation (LPE) algorithm. This choice is motivated by its close connection to multivariate p-values(see Section 3).\nThe score function K-LPE (or \u01eb-LPE) measures the relative concentration of point \u03b7 compared to the training set. Section 3 establishes that the scores for nominally generated data is asymptotically uniformly distributed in [0, 1]. Scores for anomalous data are clustered around 0. Hence when scores below level \u03b1 are declared as anomalous the false alarm error is smaller than \u03b1 asymptotically (since the integral of a uniform distribution from 0 to \u03b1 is \u03b1).\nFigure 1 illustrates the use of K-LPE algorithm for anomaly detection when the nominal data is a 2D Gaussian mixture. The middle panel of figure 1 shows the detection results based on K-LPE are consistent with the theoretical contour for significance level \u03b1 = 0.05. The right panel of figure 1 shows the empirical distribution (derived from the kernel density estimation) of the score function K-LPE for the nominal (solid blue) and the anomaly (dashed red) data. We can see that the curve for the nominal data is approximately uniform in the interval [0, 1] and the curve for the anomaly data has a peak at 0. Therefore choosing the threshold \u03b1 = 0.05 will approximately control the Type I error within 0.05 and minimize the Type II error. We also take note of the inherent robustness of our algorithm. As seen from the figure (right) small changes in \u03b1 lead to small changes in actual false alarm and miss levels.\n1 When the mixing density is not uniform but, say f1, the score functions must be modified to p\u0302K(\u03b7) = 1 n Pn i=1 I\n\n1 RS(\u03b7)f1(\u03b7) \u2264 1 RS(xi)f1(xi)\nff\nand p\u0302\u01eb(\u03b7) = 1n Pn i=1 I\n\nNS(\u03b7) f1(\u03b7) \u2265 NS(xi) f1(xi)\nff\nfor the two graphs K-NNG and \u01eb-NNG respectively.\nTo summarize the above discussion, our LPE algorithm has three steps:\n(1) Inputs: Significance level \u03b1, distance metric (Euclidean, geodesic, weighted etc.). (2) Score computation: Construct K-NNG (or \u01eb-NG) based on dij and compute the score function K-LPE from Equation 1 (or \u01eb-LPE from Equation 2). (3) Make Decision: Declare \u03b7 to be anomalous if and only if p\u0302K(\u03b7) \u2264 \u03b1 (or p\u0302\u01eb(\u03b7) \u2264 \u03b1).\nComputational Complexity: To compute each pairwise distance requires O(d) operations; and O(n2d) operations for all the nodes in the training set. In the worst-case computing the K-NN graph (for small K) and the functions RS(\u00b7), NS(\u00b7) requires O(n2) operations over all the nodes in the training data. Finally, computing the score for each test data requires O(nd+n) operations(given that RS(\u00b7), NS(\u00b7) have already been computed).\nRemark: LPE is fundamentally different from non-parametric density estimation or level set estimation schemes (e.g., MV-set). These approaches involve explicit estimation of high dimensional quantities and thus hard to apply in high dimensional problems. By computing scores for each test sample we avoid high-dimensional computation. Furthermore, as we will see in the following section the scores are estimates of multivariate p-values. These turn out to be sufficient statistics for optimal anomaly detection."}, {"heading": "3 Theory: Consistency of LPE", "text": "A statistical framework for the anomaly detection problem is presented in this section. We establish that anomaly detection is equivalent to thresholding p-values for multivariate data. We will then show that the score functions developed in the previous section is an asymptotically consistent estimator of the p-values. Consequently, it will follow that the strategy of declaring an anomaly when a test sample has a low score is asymptotically optimal.\nAssume that the data belongs to the d-dimensional unit cube [0, 1]d and the nominal data is sampled from a multivariate density f0(x) supported on the d-dimensional unit cube [0, 1]d. Anomaly detection can be formulated as a composite hypothesis testing problem. Suppose test data, \u03b7 comes from a mixture distribution, namely, f(\u03b7) = (1\u2212\u03c0)f0(\u03b7)+\u03c0f1(\u03b7) where f1(\u03b7) is a mixing density supported on [0, 1]d. Anomaly detection involves testing the nominal hypotheses H0 : \u03c0 = 0 versus the alternative (anomaly) H1 : \u03c0 > 0. The goal is to maximize the detection power subject to false alarm level \u03b1, namely, P(declare H1 | H0) \u2264 \u03b1.\nDefinition 1. Let P0 be the nominal probability measure and f1(\u00b7) be P0 measurable. Suppose the likelihood ratio f1(x)/f0(x) does not have non-zero flat spots on any open ball in [0, 1]d. Define the p-value of a data point \u03b7 as\np(\u03b7) = P0\n(\nx : f1(x)\nf0(x) \u2265\nf1(\u03b7)\nf0(\u03b7)\n)\nNote that the definition naturally accounts for singularities which may arise if the support of f0(\u00b7) is a lower dimensional manifold. In this case we encounter f1(\u03b7) > 0, f0(\u03b7) = 0 and the p-value p(\u03b7) = 0. Here anomaly is always declared(low score).\nThe above formula can be thought of as a mapping of \u03b7 \u2192 [0, 1]. Furthermore, the distribution of p(\u03b7) under H0 is uniform on [0, 1]. However, as noted in the introduction there are other such transformations. To build intuition about the above transformation and its utility consider the following example. When the mixing density is uniform, namely, f1(\u03b7) = U(\u03b7) where U(\u03b7) is uniform over [0, 1]d, note that \u2126\u03b1 = {\u03b7 | p(\u03b7) \u2265 \u03b1} is a density level set at level \u03b1. It is well known (see [12]) that such a density level set is equivalent to a minimum volume set of level \u03b1. The minimum volume set at level \u03b1 is known to be the uniformly most powerful decision region for testing H0 : \u03c0 = 0 versus the alternative H1 : \u03c0 > 0 (see [13, 12]). The generalization to arbitrary f1 is described next.\nTheorem 1. The uniformly most powerful test for testing H0 : \u03c0 = 0 versus the alternative (anomaly) H1 : \u03c0 > 0 at a prescribed level \u03b1 of significance P(declare H1 | H0) \u2264 \u03b1 is:\n\u03c6(\u03b7) =\n{\nH1, p(\u03b7) \u2264 \u03b1 H0, otherwise\nProof. We provide the main idea for the proof. First, measure theoretic arguments are used to establish p(X) as a random variable over [0, 1] under both nominal and anomalous distributions. Next whenX d \u223c f0, i.e., distributed with nominal density it follows that the random variable p(X) d \u223c U [0, 1]. When X d \u223c f = (1 \u2212 \u03c0)f0 + \u03c0f1 with \u03c0 > 0 the random variable, p(X) d \u223c g where g(\u00b7) is a monotonically decreasing PDF supported on [0, 1]. Consequently, the uniformly most powerful test for a significance level \u03b1 is to declare p-values smaller than \u03b1 as anomalies.\nNext we derive the relationship between the p-values and our score function. By definition, RS(\u03b7) and RS(xi) are correlated because the neighborhood of \u03b7 and xi might overlap. We modify our algorithm to simplify our analysis. We assume n is odd (say) and can be written as n = 2m + 1. We divide training set S into two parts:\nS = S1 \u2229 S2 = {x0, x1, \u00b7 \u00b7 \u00b7 , xm} \u2229 {xm+1, \u00b7 \u00b7 \u00b7 , x2m}\nWe modify \u01eb-LPE to p\u0302\u01eb(\u03b7) = 1m \u2211 xi\u2208S1 I{NS2(\u03b7)\u2265NS1(xi)} (or K-LPE to p\u0302K(\u03b7) = 1 m \u2211 xi\u2208S1 I{RS2 (\u03b7)\u2264RS1(xi)} ). Now RS2(\u03b7) and RS1(xi) are independent.\nFurthermore, we assume f0(\u00b7) satisfies the following two smoothness conditions:\n1. the Hessian matrix H(x) of f0(x) is always dominated by a matrix with largest eigenvalue \u03bbM , i.e., \u2203M s.t. H(x) M \u2200x and \u03bbmax(M) \u2264 \u03bbM\n2. In the support of f0(\u00b7), its value is always lower bounded by some \u03b2 > 0.\nWe have the following theorem.\nTheorem 2. Consider the setup above with the training data {xi}ni=1 generated i.i.d. from f0(x). Let \u03b7 \u2208 [0, 1]d be an arbitrary test sample. It follows that for a suitable choice K and under the above smoothness conditions,\n|p\u0302K(\u03b7)\u2212 p(\u03b7)| n\u2192\u221e \u2212\u2192 0 almost surely, \u2200\u03b7 \u2208 [0, 1]d\nFor simplicity, we limit ourselves to the case when f1 is uniform. The proof of Theorem 2 consists of two steps:\n\u2022 We show that the expectation ES1 [p\u0302\u01eb(\u03b7)] n\u2192\u221e \u2212\u2192 p(\u03b7) (Lemma 3). This result is then ex-\ntended to K-LPE (i.e. ES1 [p\u0302K(\u03b7)] n\u2192\u221e \u2212\u2192 p(\u03b7)) in Lemma 4.\n\u2022 Next we show that p\u0302K(\u03b7) n\u2192\u221e \u2212\u2192 ES1 [p\u0302K(\u03b7)] via concentration inequality (Lemma 5).\nLemma 3 (\u01eb-LPE). By picking \u01eb = m\u2212 3 5d\n\u221a\nd 2\u03c0e , with probability at least 1\u2212 e \u2212\u03b2m1/15/2,\nlm(\u03b7) \u2264 ES1 [p\u0302\u01eb(\u03b7)] \u2264 um(\u03b7) (3)\nwhere\nlm(\u03b7) = P0{x : (f0(\u03b7)\u2212\u22061) (1\u2212\u22062) \u2265 (f0(x) + \u22061) (1 + \u22062)} \u2212 e \u2212\u03b2m1/15/2\num(\u03b7) = P0{x : (f0(\u03b7) + \u22061) (1 + \u22062) \u2265 (f0(x)\u2212\u22061) (1\u2212\u22062)}+ e \u2212\u03b2m1/15/2\n\u22061 = \u03bbMm \u22126/5d/(2\u03c0e(d+ 2)) and \u22062 = 2m\u22121/6.\nProof. We only prove the lower bound since the upper bound follows along similar lines. By interchanging the expectation with the summation,\nES1 [p\u0302\u01eb(\u03b7)] = ES1\n[\n1\nm\n\u2211\nxi\u2208S1\nI{NS2(\u03b7)\u2265NS1(xi)}\n]\n= 1\nm\n\u2211\nxi\u2208S1\nExiES1\\xi\n[\nI{NS2(\u03b7)\u2265NS1(xi)}\n]\n= Ex1 [PS1\\x1(NS2(\u03b7) \u2265 NS1(x1))]\nwhere the last inequality follows from the symmetric structure of {x0, x1, \u00b7 \u00b7 \u00b7 , xm}.\nClearly the objective of the proof is to show PS1\\x1(NS2(\u03b7) \u2265 NS1(x1)) n\u2192\u221e \u2212\u2192 I{f0(\u03b7)\u2265f0(x1)}. Skipping technical details, this can be accomplished in two steps. (1) Note that NS(x1) is a binomial random variable with success probability q(x1) := \u222b\nB\u01eb f0(x1+ t)dt. This relates PS1\\x1(NS2(\u03b7) \u2265\nNS1(x1)) to I{q(\u03b7)\u2265q(x1)}. (2) We relate I{q(\u03b7)\u2265q(x1)} to I{f0(\u03b7)\u2265f0(x1)} based on the function smoothness condition. The details of these two steps are shown in the below.\nNote that NS1(x1) \u223c Binom(m, q(x1)). By Chernoff bound of binomial distribution, we have\nPS1\\x1(NS1(x1)\u2212mq(x1) \u2265 \u03b4) \u2264 e \u2212 \u03b4\n2\n2mq(x1)\nthat is, NS1(x1) is concentrated around mq(x1). This implies,\nPS1\\x1(NS2(\u03b7) \u2265 NS1(x1)) \u2265 I{NS2(\u03b7)\u2265mq(x1)+\u03b4x1} \u2212 e\n\u2212 \u03b42x1\n2mq(x1) (4)\nWe choose \u03b4x1 = q(x1)m \u03b3(\u03b3 will be specified later) and reformulate equation (4) as\nPS1\\x1(NS2(\u03b7) \u2265 NS1(x1)) \u2265 I \nNS2 (\u03b7) mVol(B\u01eb) \u2265 q(x1) Vol(B\u01eb)(1+ 2 m1\u2212\u03b3 ) ff \u2212 e\u2212\nq(x1)m 2\u03b3\u22121\n2 (5)\nNext, we relate q(x1)(or \u222b\nB\u01eb f0(x1+ t)dt) to f0(x1) via the Taylor\u2019s expansion and the smoothness\ncondition of f0, \u2223\n\u2223 \u2223 \u2223 \u2223\n\u222b\nB\u01eb f0(x1 + t)dt\nVol(B\u01eb) \u2212 f0(x1)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2264 \u03bbM 2 \u00b7 1\nVol(B\u01eb)\n\u222b\nB\u01eb\n\u2016t\u20162dt = \u03bbM \u01eb\n2\n2d(d+ 2) (6)\nand then equation (5) becomes\nPS1\\x1(NS2(\u03b7) \u2265 NS1(x1)) \u2265 I \nNS2 (\u03b7) mVol(B\u01eb) \u2265 \u201c f0(x1)+ \u03bbM\u01eb 2 2d(d+2) \u201d (1+ 2 m1\u2212\u03b3\n) ff \u2212 e\u2212\nq(x1)m 2\u03b1\u22121\n2\nBy applying the same steps to NS2(\u03b7) as equation 4 (Chernoff bound) and equation 6 (Taylor\u2019s explansion), we have with probability at least 1\u2212 e\u2212 q(\u03b7)m2\u03b1\u22121 2 ,\nEx1 [PS1\\x1(NS2(\u03b7) \u2265 NS1(x1))] \u2265 Px1 \u201e f0(\u03b7)\u2212 \u03bbM\u01eb 2 2d(d+2) \u00ab \u201c 1\u2212 2 m1\u2212\u03b3 \u201d \u2265 \u201e f0(x1)+ \u03bbM\u01eb 2 2d(d+2) \u00ab \u201c 1+ 2 m1\u2212\u03b3 \u201d ff \u2212e\u2212 q(x1)m\n2\u03b1\u22121\n2\nFinally, by choosing \u01eb2 = m\u2212 6 5d \u00b7 d2\u03c0e and \u03b3 = 5/6, we prove the lemma.\nLemma 4 (K-LPE). By picking K = ( 1\u2212 2m\u22121/6 ) m2/5 (f0(\u03b7)\u2212\u22061), with probability at least 1\u2212 e\u2212\u03b2m 1/15/2,\nlm(\u03b7) \u2264 ES1 [p\u0302K(\u03b7)] \u2264 um(\u03b7) (7)\nProof. The proof is very similar to the proof to Lemma 3 and we only give a brief outline here. Now the objective is to show PS1\\x1(RS2(\u03b7) \u2264 RS1(x1)) n\u2192\u221e \u2212\u2192 I{f0(\u03b7)\u2265f0(x1)}.The basic idea is to use the result of Lemma 3. To accomplish this, we note that {RS2(\u03b7) \u2264 RS1(x1)} contains the events {NS2(\u03b7) \u2265 K} \u2229 {NS1(x1) \u2264 K}, or equivalently\n{NS2(\u03b7) \u2212 q(\u03b7)m \u2265 K \u2212 q(\u03b7)m} \u2229 {NS1(x1)\u2212 q(x1)m \u2264 K \u2212 q(x1)m} (8)\nBy the tail probability of Binomial distribution, the probability of the above two events converges to 1 exponentially fast if K\u2212 q(\u03b7)m < 0 and K\u2212 q(x1)m > 0. By using the same two-step bounding techniques developed in the proof to Lemma 3, these two inequalities are implied by\nK \u2212m2/5 (f0(\u03b7)\u2212\u22061) < 0 and K \u2212m2/5 (f0(x1) + \u22061) > 0\nTherefore if we choose K = ( 1\u2212 2m\u22121/6 ) m2/5 (f0(\u03b7) \u2212\u22061), we have with probability at least 1\u2212 e\u2212\u03b2m \u22121/15/2,\nPS1\\x1(RS2(\u03b7) \u2264 RS1(x1)) \u2265 I{(f0(\u03b7)\u2212\u22061)(1\u2212\u22062)\u2265(f0(x1)+\u22061)(1+\u22062)} \u2212 e \u2212\u03b2m\u22121/15/2\nRemark: Lemma 3 and Lemma 4 were proved with specific choices for \u01eb and K . However, \u01eb and K can be chosen in a range of values, but will lead to different lower and upper bounds. We will show in Section 4 through simulations that our LPE algorithm is generally robust to choice of parameter K .\nLemma 5. Suppose K = cm2/5 and denote p\u0302K(\u03b7) = 1m \u2211 xi\u2208S1 I{RS2(\u03b7)\u2264RS1 (xi)} . We have\nP0 (|ES1 [p\u0302K(\u03b7)]\u2212 p\u0302K(\u03b7)| > \u03b4) \u2264 2e \u2212 2\u03b4\n2m1/5 c2\u03b32 d\nwhere \u03b3d is a constant and is defined as the minimal number of cones centered at the origin of angle \u03c0/6 that cover Rd.\nProof. We can not apply Law of Large Number in this case because I{RS2(\u03b7)\u2264RS1(xi)} are correlated. Instead, we need to use the more generalized concentration-of-measure inequality such as MacDiarmid\u2019s inequality[17]. Denote F (x0, \u00b7 \u00b7 \u00b7 , xm) = 1m \u2211 xi\u2208S1 I{RS2 (\u03b7)\u2264RS1(xi)}\n. From Corollary 11.1 in [18],\nsup x0,\u00b7\u00b7\u00b7 ,xm,x\u2032i\n|F (x0, \u00b7 \u00b7 \u00b7 , xi, \u00b7 \u00b7 \u00b7 , xm)\u2212 F (x0, \u00b7 \u00b7 \u00b7 , x \u2032 i, \u00b7 \u00b7 \u00b7 , xn)| \u2264 K\u03b3d/m (9)\nThen the lemma directly follows from applying McDiarmid\u2019s inequality.\nTheorem 2 directly follows from the combination of Lemma 4 and Lemma 5 and a standard application of the first Borel-Cantelli lemma. We have used Euclidean distance in Theorem 2. When the support of f0 lies on a lower dimensional manifold (say d\u2032 < d) adopting the geodesic metric leads to faster convergence. It turns out that d\u2032 replaces d in the expression for \u22061 in Lemma 3."}, {"heading": "4 Experiments", "text": "We apply our method on both artificial and real-world data. Our method enables plotting the entire ROC curve by varying the thresholds on our scores.\nTo test the sensitivity of K-LPE to parameter changes, we first run K-LPE on the benchmark artificial data-set Banana [19] with K varying from 2 to 12. Banana dataset contains points with their labels(+1 or \u22121). We randomly pick 109 points with +1 label and regard them as the nominal training data. The test data comprises of 108 +1 data and 183 \u22121 data (ground truth) and the algorithm is supposed to predict +1 data as \u201cnominal\u201d and \u22121 data as \u201canomaly\u201d. See Figure 2(a) for the configuration of the training points and test points. Scores computed for test set using Equation 1 is oblivious to true f1 density (\u22121 labels). Euclidean distance metric is adopted for this example.\nFalse alarm (also called false positive) is defined as the percentage of nominal points that are predicted as anomaly by the algorithm. To control false alarm at level \u03b1, point with score < \u03b1 is predicted as anomaly. Empirical false alarm and true positives (percentage of anomalies declared as anomaly) can be computed from ground truth. We vary \u03b1 to obtain the empirical ROC curve. We follow this procedure for all the other experiments in this section. We are relatively insensitive to K as shown in Figure 2(b).\nFor comparison we plot the empirical ROC curve of the one-class SVM of [9]. There are two tuning parameters in OC-SVM \u2014 bandwidth c (we use RBF kernel) and \u03bd \u2208 (0, 1) (which is supposed to control FA). Note that training data does not contain \u22121 labels and this implies we can never make use of \u22121 labels to cross-validate, or, to optimize over the choice of pair (c, \u03bd). In our OCSVM implementation, by following the same procedure, we can obtain the empirical ROC curve by varying \u03bd but fixing a certain bandwidth c. Finally we iterated over different c to obtain the best (in terms of AUC) ROC curve and it turns out to be c = 1.5. Fixing c for entire ROC is equivalent to fixing K in our score function. Note that in real practice what can be done is even worse than this implementation because there is also no natural way to optimize over c without being revealed the \u22121 labels.\nIn Figure 2(b), we can see that our algorithm is consistently better than one-class SVM on the Banana dataset. Furthermore, we found that choosing suitable tuning parameters to control false alarms is generally difficult in the one-class SVM approach. In our approach if we set \u03b1 = 0.05 we\nget empirical FA = 0.06 and for \u03b1 = 0.08, empirical FA = 0.09. For OC-SVM we can not see any natural way of picking c and \u03bd to control FA rate based only on training data.\nIn Figure 3, we apply our K-LPE to another 2D artificial example where the nominal distribution f0 is a mixture Gaussian and the anomalous distribution is very close to uniform (see Figure 3(a) for their configuration):\nf0 \u223c 1\n2 N\n([\n8 0\n]\n,\n[\n1 0 0 9\n])\n+ 1\n2 N\n([\n\u22128 0\n]\n,\n[\n1 0 0 9\n])\n, f1 \u223c N\n(\n0,\n[\n49 0 0 49\n])\n(10)\nIn this example, we can exactly compute the optimal ROC curve. We call this curve the Clairvoyant ROC (the red dashed curve in Figure 3(b)). The other two curves are averaged (over 15 trials) empirical ROC curves with respect to different sizes of training sample (n = 40, 160) for K = 6. Larger n results in better ROC curve. We see that for a relatively small training set of size 160 the average empirical ROC curve is very close to the clairvoyant ROC curve.\nNext, we ran LPE on three real-world datasets: Wine, Ionosphere[20] and MNIST US Postal Service (USPS) database of handwritten digits. The procedure and setup of the experiments is almost the same as the that of the Banana data set. However, there are two differences. (1) If the number of different labels is greater than two, we always treat points with one particular label as\nnominal(+1) and regard the points with other labels as anomalous(\u22121). For example, for the USPS dataset, we regard instances of digit 0 as nominal training samples and instances of digits 1, \u00b7 \u00b7 \u00b7 , 9 as anomaly. (2) For high dimensional data set, the data points are normalized to be within [0, 1]d and we use geodesic distance [14](instead of Euclidean distance) as the input to LPE.\nThe ROC curves of these three datasets are shown in Figure 4. In Wine dataset, the dimension of the feature space is 13. The training set is composed of 39 data points and we apply the \u01eb-LPE algorithm with \u01eb = 0.9. The test set is a mixture of 20 nominal points and 158 anomaly points (ground truth). In Ionosphere dataset, the dimension of the feature space is 34. The training set is composed of 175 data points and we apply the K-LPE algorithm with K = 9. The test set is a mixture of 50 nominal points and 126 anomaly points (ground truth). In USPS dataset, the dimension of the feature space is 16 \u00d7 16 = 256. The training set is composed of 400 data points and we apply the K-LPE algorithm with K = 9. The test set is a mixture of 367 nominal points and 33 anomaly points (ground truth).\nFor comparison purposes we note that for the USPS data set by setting \u03b1 = 0.5 we get empirical false-positive 6.1% and empirical false alarm rate 5.7% (In contrast FP = 7% and FA = 9% with \u03bd = 5% for OC-SVM as reported in [9]). Practically we find that K-LPE is more preferable to \u01eb-LPE due to easiness of choosing the parameter K . We find that the value of K is relatively independent of dimension d. As a rule of thumb we found that setting K around n2/5 was generally effective."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a novel non-parametric adaptive anomaly detection algorithm which leads to a computationally efficient solution with provable optimality guarantees. Our algorithm takes a K-nearest neighbor graph as an input and produces a score for each test point. Scores turn out to be empirical estimates of the volume of minimum volume level sets containing the test point. While minimum volume level sets provide an optimal characterization for anomaly detection, they are high dimensional quantities and generally difficult to reliably compute in high dimensional feature spaces. Nevertheless, a sufficient statistic for optimal tradeoff between false alarms and misses is the volume of the MV set itself, which is a real number. By computing score functions we avoid computing high dimensional quantities and still ensure optimal control of false alarms and misses. The computational cost of our algorithm scales linearly in dimension and quadratically in data size."}], "references": [{"title": "A linear programming approach to novelty detection", "author": ["C. Campbell", "K.P. Bennett"], "venue": "Advances in Neural Information Processing Systems 13. MIT Press, 2001, pp. 395\u2013401.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Novelty detection: a review \u2013 part 1: statistical approaches", "author": ["M. Markou", "S. Singh"], "venue": "Signal Processing, vol. 83, pp. 2481\u20132497, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient algorithms for mining outliers from large data sets", "author": ["R. Ramaswamy", "R. Rastogi", "K. Shim"], "venue": "Proceedings of the ACM SIGMOD Conference, 2000. 9", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Consistency and convergence rates of one-class svms and related algorithms", "author": ["R. Vert", "J. Vert"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 817\u2013854, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Feature extraction for one-class classification", "author": ["D. Tax", "K.R. M\u00fcller"], "venue": "Artificial neural networks and neural information processing, Istanbul, TURQUIE, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Optimal singl-class classification strategies", "author": ["R. El-Yaniv", "M. Nisenson"], "venue": "Advances in Neural Information Processing Systems 19. MIT Press, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Detection of abrupt changes: theory and applications", "author": ["I.V. Nikiforov", "M. Basseville"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "A new local distance-based outlier detection approach for scattered real-world data", "author": ["K. Zhang", "M. Hutter", "H. Jin"], "venue": "March 2009, arXiv:0903.3257v1[cs.LG].", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "J. Shawe-Taylor", "A.J. Smola", "R. Williamson"], "venue": "Neural Computation, vol. 13, no. 7, pp. 1443\u20131471, 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "One-class classification: Concept-learning in the absence of counter-examples", "author": ["D. Tax"], "venue": "Ph.D. dissertation, Delft University of Technology, June 2001.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Robust novelty detection with single-class MPM", "author": ["G.R.G. Lanckriet", "L.E. Ghaoui", "M.I. Jordan"], "venue": "Neural Information Processing Systems Conference, vol. 18, 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning minimum volume sets", "author": ["C. Scott", "R.D. Nowak"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 665\u2013704, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Geometric entropy minimization(GEM) for anomaly detection and localization", "author": ["A.O. Hero"], "venue": "Neural Information Processing Systems Conference, vol. 19, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "A global geometric framework fo nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": "Science, vol. 290, pp. 2319\u20132323, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Graph approximations to geodesics on embedded manifolds", "author": ["M. Bernstein", "V.D. Silva", "J.C. Langford", "J.B. Tenenbaum"], "venue": "2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Nonlinear dimensionality reduction by local linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, vol. 290, pp. 2323\u20132326, 2000.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "On the method of bounded differences", "author": ["C. McDiarmid"], "venue": "Surveys in Combinatorics. Cambridge University Press, 1989, pp. 148\u2013188.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1989}, {"title": "A Probabilistic Theory of Pattern Recognition", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1996}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D.J. Newman"], "venue": "2007. [Online]. Available: http://www.ics.uci.edu/$\\sim$mlearn/{MLR}epository.html 10", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "We propose an adaptive non-parametric method for anomaly detection based on score functions that maps data samples to the interval [0, 1].", "startOffset": 131, "endOffset": 137}, {"referenceID": 0, "context": "In statistical hypothesis testing, p-value is any transformation of the feature space to the interval [0, 1] that induces a uniform distribution on the nominal data.", "startOffset": 102, "endOffset": 108}, {"referenceID": 0, "context": "It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature.", "startOffset": 44, "endOffset": 50}, {"referenceID": 1, "context": "It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature.", "startOffset": 44, "endOffset": 50}, {"referenceID": 2, "context": "It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature.", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature.", "startOffset": 100, "endOffset": 106}, {"referenceID": 4, "context": "It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature.", "startOffset": 100, "endOffset": 106}, {"referenceID": 5, "context": "It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature.", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "In parametric approaches [7] the nominal densities are assumed to come from a parameterized family and generalized likelihood ratio tests are used for detecting deviations from nominal.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "(K-NN) anomaly detection approach is presented in [3, 8].", "startOffset": 50, "endOffset": 56}, {"referenceID": 7, "context": "(K-NN) anomaly detection approach is presented in [3, 8].", "startOffset": 50, "endOffset": 56}, {"referenceID": 8, "context": "[9] where the basic idea is to map the training data into the kernel space and to separate them from the origin with maximum margin.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Other algorithms along this line of research include support vector data description [10], linear programming approach [1], and single class minimax probability machine [11].", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "Other algorithms along this line of research include support vector data description [10], linear programming approach [1], and single class minimax probability machine [11].", "startOffset": 119, "endOffset": 122}, {"referenceID": 10, "context": "Other algorithms along this line of research include support vector data description [10], linear programming approach [1], and single class minimax probability machine [11].", "startOffset": 169, "endOffset": 173}, {"referenceID": 11, "context": "Scott and Nowak [12] derive decision regions based on minimum volume (MV) sets, which does provide Type I and Type II error control.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "Related work by Hero [13] based on geometric entropic minimization (GEM) detects outliers by comparing test samples to the most concentrated subset of points in the training sample.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "To overcome these computational limitations [13] proposes heuristic greedy algorithms based on leave-one out K-NN graph, which while inspired by K-MST algorithm is no longer provably optimal.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Our approach is related to these latter techniques, namely, MV sets of [12] and GEM approach of [13].", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "Our approach is related to these latter techniques, namely, MV sets of [12] and GEM approach of [13].", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "(1) Like [13] our algorithm scales linearly with dimension and quadratic with data size and can be applied to high dimensional feature spaces.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "(2) Like [12] our algorithm is provably optimal in that it is uniformly most powerful for the specified false alarm level, \u03b1, for the case that the anomaly density is a mixture of the nominal and any other density (not necessarily uniform).", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "(4) Like [13] and unlike other learning theoretic approaches such as [9, 12] we do not require choosing complex tuning parameters or function approximation classes.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "(4) Like [13] and unlike other learning theoretic approaches such as [9, 12] we do not require choosing complex tuning parameters or function approximation classes.", "startOffset": 69, "endOffset": 76}, {"referenceID": 11, "context": "(4) Like [13] and unlike other learning theoretic approaches such as [9, 12] we do not require choosing complex tuning parameters or function approximation classes.", "startOffset": 69, "endOffset": 76}, {"referenceID": 0, "context": "Let S = {x1, x2, \u00b7 \u00b7 \u00b7 , xn} be the nominal training set of size n belonging to the unit cube [0, 1].", "startOffset": 94, "endOffset": 100}, {"referenceID": 0, "context": "Let d(x, y) be a distance function denoting the distance between any two points x, y \u2208 [0, 1].", "startOffset": 87, "endOffset": 93}, {"referenceID": 13, "context": "The Geodesic Learning algorithm, a subroutine in Isomap [14, 15] can be used to efficiently and consistently estimate the geodesic distances.", "startOffset": 56, "endOffset": 64}, {"referenceID": 14, "context": "The Geodesic Learning algorithm, a subroutine in Isomap [14, 15] can be used to efficiently and consistently estimate the geodesic distances.", "startOffset": 56, "endOffset": 64}, {"referenceID": 15, "context": "This can be accomplished for instance through Mahalanobis distances or as a by product of local linear embedding [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "The score functions map the test data \u03b7 to the interval [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "Section 3 establishes that the scores for nominally generated data is asymptotically uniformly distributed in [0, 1].", "startOffset": 110, "endOffset": 116}, {"referenceID": 0, "context": "We can see that the curve for the nominal data is approximately uniform in the interval [0, 1] and the curve for the anomaly data has a peak at 0.", "startOffset": 88, "endOffset": 94}, {"referenceID": 0, "context": "Assume that the data belongs to the d-dimensional unit cube [0, 1] and the nominal data is sampled from a multivariate density f0(x) supported on the d-dimensional unit cube [0, 1].", "startOffset": 60, "endOffset": 66}, {"referenceID": 0, "context": "Assume that the data belongs to the d-dimensional unit cube [0, 1] and the nominal data is sampled from a multivariate density f0(x) supported on the d-dimensional unit cube [0, 1].", "startOffset": 174, "endOffset": 180}, {"referenceID": 0, "context": "Suppose test data, \u03b7 comes from a mixture distribution, namely, f(\u03b7) = (1\u2212\u03c0)f0(\u03b7)+\u03c0f1(\u03b7) where f1(\u03b7) is a mixing density supported on [0, 1].", "startOffset": 134, "endOffset": 140}, {"referenceID": 0, "context": "Suppose the likelihood ratio f1(x)/f0(x) does not have non-zero flat spots on any open ball in [0, 1].", "startOffset": 95, "endOffset": 101}, {"referenceID": 0, "context": "The above formula can be thought of as a mapping of \u03b7 \u2192 [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "Furthermore, the distribution of p(\u03b7) under H0 is uniform on [0, 1].", "startOffset": 61, "endOffset": 67}, {"referenceID": 0, "context": "When the mixing density is uniform, namely, f1(\u03b7) = U(\u03b7) where U(\u03b7) is uniform over [0, 1], note that \u03a9\u03b1 = {\u03b7 | p(\u03b7) \u2265 \u03b1} is a density level set at level \u03b1.", "startOffset": 84, "endOffset": 90}, {"referenceID": 11, "context": "It is well known (see [12]) that such a density level set is equivalent to a minimum volume set of level \u03b1.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "The minimum volume set at level \u03b1 is known to be the uniformly most powerful decision region for testing H0 : \u03c0 = 0 versus the alternative H1 : \u03c0 > 0 (see [13, 12]).", "startOffset": 155, "endOffset": 163}, {"referenceID": 11, "context": "The minimum volume set at level \u03b1 is known to be the uniformly most powerful decision region for testing H0 : \u03c0 = 0 versus the alternative H1 : \u03c0 > 0 (see [13, 12]).", "startOffset": 155, "endOffset": 163}, {"referenceID": 0, "context": "First, measure theoretic arguments are used to establish p(X) as a random variable over [0, 1] under both nominal and anomalous distributions.", "startOffset": 88, "endOffset": 94}, {"referenceID": 0, "context": ", distributed with nominal density it follows that the random variable p(X) d \u223c U [0, 1].", "startOffset": 82, "endOffset": 88}, {"referenceID": 0, "context": "When X d \u223c f = (1 \u2212 \u03c0)f0 + \u03c0f1 with \u03c0 > 0 the random variable, p(X) d \u223c g where g(\u00b7) is a monotonically decreasing PDF supported on [0, 1].", "startOffset": 132, "endOffset": 138}, {"referenceID": 0, "context": "Let \u03b7 \u2208 [0, 1] be an arbitrary test sample.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "It follows that for a suitable choice K and under the above smoothness conditions, |p\u0302K(\u03b7)\u2212 p(\u03b7)| n\u2192\u221e \u2212\u2192 0 almost surely, \u2200\u03b7 \u2208 [0, 1] For simplicity, we limit ourselves to the case when f1 is uniform.", "startOffset": 127, "endOffset": 133}, {"referenceID": 16, "context": "Instead, we need to use the more generalized concentration-of-measure inequality such as MacDiarmid\u2019s inequality[17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 17, "context": "1 in [18], sup x0,\u00b7\u00b7\u00b7 ,xm,x\u2032i |F (x0, \u00b7 \u00b7 \u00b7 , xi, \u00b7 \u00b7 \u00b7 , xm)\u2212 F (x0, \u00b7 \u00b7 \u00b7 , x \u2032 i, \u00b7 \u00b7 \u00b7 , xn)| \u2264 K\u03b3d/m (9)", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "For comparison we plot the empirical ROC curve of the one-class SVM of [9].", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "K-LPE for Banana Data Figure 2: Performance Robustness of LPE;(a) The configuration of the nominal training points (red \u2018+\u2019) and unlabeled test points (black \u2018 \u2022\u2019) for the banana dataset [19]; (b) Empirical ROC curve of K-LPE on the banana dataset with K = 2, 4, 6, 8, 10, 12 (with n = 400) vs the empirical ROC curve of one class SVM developed in [9].", "startOffset": 348, "endOffset": 351}, {"referenceID": 18, "context": "Next, we ran LPE on three real-world datasets: Wine, Ionosphere[20] and MNIST US Postal Service (USPS) database of handwritten digits.", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "(2) For high dimensional data set, the data points are normalized to be within [0, 1] and we use geodesic distance [14](instead of Euclidean distance) as the input to LPE.", "startOffset": 79, "endOffset": 85}, {"referenceID": 13, "context": "(2) For high dimensional data set, the data points are normalized to be within [0, 1] and we use geodesic distance [14](instead of Euclidean distance) as the input to LPE.", "startOffset": 115, "endOffset": 119}, {"referenceID": 8, "context": "7% (In contrast FP = 7% and FA = 9% with \u03bd = 5% for OC-SVM as reported in [9]).", "startOffset": 74, "endOffset": 77}], "year": 2009, "abstractText": "We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on n-point nominal data. Anomalies are declared whenever the score of a test sample falls below \u03b1, which is supposed to be the desired false alarm level. The resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the specified false alarm level, \u03b1, for the case when the anomaly density is a mixture of the nominal and a known density. Our algorithm is computationally efficient, being linear in dimension and quadratic in data size. It does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality. We demonstrate the algorithm on both artificial and real data sets in high dimensional feature spaces.", "creator": "LaTeX with hyperref package"}}}