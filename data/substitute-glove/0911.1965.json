{"id": "0911.1965", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2009", "title": "Active Learning for Mention Detection: A Comparison of Sentence Selection Strategies", "abstract": "We recommend and n\u2019t specific sentence selection capabilities some newly learning year two aim some detecting mentions of entities. The best strategy 400 when formula_1 in infima with each comparative classifiers trained on different fact. after account. Our experimental surprise even more, 1.4 to the random selection objectives, ever strategy reduces first exposure of standards labeled skills provided by over 11% take attainment the this performance. The effect is even be continuing his been john explaining are subject: the can astonishing long fact outstanding by using all 42% related with instruction data required by the descriptions selection strategy.", "histories": [["v1", "Tue, 10 Nov 2009 19:52:01 GMT  (40kb,D)", "http://arxiv.org/abs/0911.1965v1", "12 pages, 9 figures"]], "COMMENTS": "12 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["nitin madnani", "hongyan jing", "nanda kambhatla", "salim roukos"], "accepted": false, "id": "0911.1965"}, "pdf": {"name": "0911.1965.pdf", "metadata": {"source": "CRF", "title": "Active Learning for Mention Detection: A Comparison of Sentence Selection Strategies", "authors": ["Nitin Madnani", "Hongyan Jing", "Nanda Kambhatla"], "emails": ["nmadnani@umiacs.umd.edu", "hjing@us.ibm.com", "nanda@us.ibm.com", "roukos@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Human annotation is expensive, yet it is needed in many tasks in order to create training data. Given the high cost, it is critical to improve the e ciency of such annotation. Active learning involves selecting samples intelligently rather than randomly, for human annotation. By doing so, it is possible for systems to attain better performance with the same amount of annotation or achieve the same level of performance with a lot less annotated data.\nIn this paper, we present several new active learning strategies for the task of Mention Detection (MD). Here, we employ the terminology used in the Automatic Content Extraction Conferences [1]. Mentions are references to real-world entities that can be named (e.g. John ), nominal (e.g. survivor ) or pronominal (e.g. he ).\nWe propose and investigate a variety of sentence selection criteria for active learning, including various sentence scoring metrics that combine uncertaintybased and query-by-committee like measurements. Experimental results show that these sentence selection strategies are quite e ective for mention detection: compared to the random selection strategy, the best strategy reduces the amount of required annotated training data by over 50% while achieving the same performance. The e ect is even more signi cant when only named mentions are considered: the system achieves the same performance by using only 42% of the training data required by the random strategy.\nIn the next section, we discuss related work on active learning. Section 3 describes our framework in detail and presents our experimental setup. Section 4\nar X\niv :0\n91 1.\n19 65\nv1 [\ncs .C\nL ]\n1 0\nN ov\npresents the results of the di erent experiments. Finally, we discuss observations from our experiments and present some ideas for future work."}, {"heading": "2 Related Work", "text": "Active learning has been utilized for many NLP applications, most noticeably for text classi cation [2 5]. It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].\nThere are few reported instances of applying active learning techniques to mention detection. [14] investigated several document, rather than sentence, selection strategies for Information Extraction. They observed that some strategies lead to improvement in recall while others improve precision, but it is di cult to get signi cant improvement in both recall and precision for an active learner to perform better than random selection.\n[15] proposed a multi-criteria-based active learning approach for named entity recognition. The multiple criteria include informativeness, representativeness, and diversity. They proposed measures to quantify these properties and investigated di erent selection strategies. They showed that the labeling cost can be reduced by at least 80% without degrading the performance for their data sets.\n[16] investigated a query-by-committee-based active learner for information extraction in the astronomy domain. It studied the e ects of selective sampling on human annotators. Although active learning improved annotation e ciency overall, they observed lower inter-annotator agreement and higher per-token annotation times for the data selected by active learning."}, {"heading": "3 Active Learning", "text": "Active learning techniques are usually divided into two types: uncertainty sampling for a single learner [17], or disagreement measurement between a committee of learners [18]. In each case, seed data needs to be provided to build an initial model or models. In the uncertainly-based approach, a single learner labels unlabeled examples and provides a con dence score for each predicted label. Samples that have the lowest con dence scores are chosen for manual labeling. In the query-by-committee approach, a committee of learners is built and each learner labels the unlabeled samples. Samples that have the highest disagreement among committee members are chosen for manual labeling.\nIn our work, we experiment with query-by-committee-based approaches as well as hybrid approaches in which we employ a weighted combination of multiple learners. We also explore the e ect of using di erent sets of committee learners. We carry out a number of experiments to compare the selection strategies that we propose. In this section, we rst describe the corpus used in the experiments and then present the statistical classi ers used in the committees, along with the scoring metrics that we use."}, {"heading": "3.1 The MALACH Information Extraction Corpus", "text": "The MALACH collection [19, 20] contains 116,000 hours of digitized interviews and testimonies in 32 languages from 52,000 survivors, liberators, rescuers and witnesses of the Holocaust. We are interested in automatic information extraction from this corpus of spontaneous conversational speech. For a small number of English testimonies, we manually transcribed them and manually annotated the transcripts with three types of information:\nMentions. Named, nominal and pronominal mentions of 20 categories of entities. Co-reference. Sets of mentions that refer to the same real-world entity. Relations. Sets of relationships between pairs of mentions. For instance, given the sentence I was in Auschwitz for a year , the following relation exists: LocatedAt(I, Auschwitz).\nFor the experiments described in this paper, we chose to focus on mention annotation only. We plan to explore co-reference and relation annotation in the future. For mention annotation, we excluded pronouns; their inclusion leads to in ated numbers since detection of pronominal mentions is easy and they occur with high frequency.\nWe split the annotated data into two parts: the pool from which the learners select the next batch of data for annotation, and the development set. The rst pool consists of 99 documents, including 4772 sentences and 198K words. A total of 43K mentions have been annotated for the documents in the pool (16K mentions if pronouns are excluded). The development evaluation set consists of 5 complete testimonies, which cover over 10 hours of speech. It consists of 1700 sentences, 73K words and 16K total mentions (6K mentions if pronouns are excluded)."}, {"heading": "3.2 Granularity of Selection", "text": "When using active learning techniques to select samples for human annotation, we need to rst decide on the granularity of sample selection. The samples can be documents, sentences, or tokens. A possible problem with selecting samples at the document level [14] is that a document may only be partially useful from a learning point of view and and it is impossible to add only the interesting examples so as to maximize the e ect of active learning. This problem is particularly acute in our corpus since the documents, which are testimonies of survivors, are very long and contain a lot of redundant information. Selecting tokens as samples, as in [15], has the advantage that all samples are of equal length but su ers from the problem that a human annotator has to annotate mentions by looking at only the tokens. The selected tokens may also contain partial mentions when mention boundaries are incorrectly identi ed. Sentences, on the other hand, contain enough non-redundant contextual information for e ective annotation. Therefore, we use sentence-level blocks for active learning, as in [16]."}, {"heading": "3.3 Framework Description", "text": "We propose a new active learning based framework for sample selection which provides considerable savings in the annotation task and can provide better performance for the same amount of annotation. Figure 1 shows the architecture of the framework. As mentioned before, the central idea is to use an ensemble of classi ers -each one di ering from the others in some respect. Once trained, the classi ers can then be used to detect mentions in the unlabeled sentences. These detected mentions can then be compared, for each sentence, and a score assigned to this sentence indicating the agreement, or lack thereof, among the ensemble classi ers. The sentences on which the classi ers disagree the most are, intuitively, the ones that can contribute the most to the learning. The sentences are then ranked according to this metric and the required number sampled from this ranked list.\nIn the case of true active learning, these samples would be provided to the annotator to label and then added to the training data for the main classi er used for the actual task of mention detection. However, for this paper, we consider controlled experiments where the entire dataset has been annotated in advance. Therefore, the path between the selection of the samples to the annotator has e ectively been short-circuited and the samples are added directly to the training data for the main classi er. We structured our setup in such a way since we wanted to experiment with adding successively increasing amounts of training data without incurring the overhead of annotation at each step. The same exact advantages and savings will apply to a real setting where the selected samples need to be annotated at each step of learning. At each step, the set of sampled sentences is split into equal parts and added to the seed data for the ensemble classi ers as well.\nOur approach di ers from previous approaches to the same problem in two signi cant ways. First, we measure the disagreement at the granularity of sentences rather than documents, which allows us to be much more discerning when selecting samples. The second point is that our framework allows targeting the sample selection towards speci c types of mention that can be speci ed by the user. This is important because for some tasks, like ACE, some types of mentions carry more weight than the others."}, {"heading": "3.4 Statistical classi ers", "text": "At each step in the active learning process, we build two maximum-entropy based statistical classi ers [21] using the labeled data available at that step. There are two dimensions of classi er training - the feature set of the classier and the data used for training. For our experiments, we use two di erent combinations of these dimensions:\nEach classi er is trained with the same features but on a di erent half of the available labeled data. We refer to this as the data-di erent (DD) setting.\nEach classi er is trained on the entire available labeled data but using a di erent feature set : the inside classi er uses lexical features derived solely from the current token, and the outside classi er uses features involving surrounding tokens only. We refer to this experimental setting as featuredi erent (FD)."}, {"heading": "3.5 Sentence scoring metrics", "text": "We employ a variety of metrics for measuring the degree of disagreement between two classi ers over the output labels for a given sentence. The simplest metrics look at only the output labels predicted by the classi ers. However, we propose another set of metrics that utilizes the con dence values associated with those labels as well.\nF-measure As the harmonic mean of precision and recall, the F-measure is often used to assess the agreement between two classi ers for the named entity recognition task. The value of F-measure is between 0 and 1, with higher values indicating greater agreement. During sentence selection, we compute the F-measure of the two classi ers on each unannotated sentence, and select sentences with the lowest F-measure values for annotation.\nMacro-averaged F-measure Instead of computing the F-measure over the entire set of mentions, as in the previous metric, another option is to compute the F-measure for each mention category and then take the average over all categories. This metric allows categories with a small number of mentions to be weighted equally.\nCon dence Sum. Our statistical classi ers can provide a probability value for each output label, indicating its con dence in choosing the label. To leverage this information, we use the normalized sum of the con dence values of the two classi ers as another metric. The higher this value, the more con dent both classi ers are about a particular sentence. Therefore, that sentence will provide little or no information if added to the training set and should be ranked lower for selection. Con dence Di erence. We also use the absolute di erence of the two condence values. A sentence with a higher con dence di erence value indicates an explicit disagreement between the two classi ers when detecting mentions and, therefore, would prove to be more useful for annotation.\nIn addition to the above selection metrics, there are three additional parameters that were used in our experiments:\nMinimum number of mentions per sentence. This parameter is used to deal with the sparse mention problem in sentence selection. For instance, for a given sentence S1, if the rst classi er nds only one mention in it, and the second classi er nds zero mentions, then the F-measure for the sentence is 0, which puts the given sentence at the top of the selection list. In contrast, for another sentence S2, if the rst classi er nds 5 mentions, and the second classi er nds 3 mentions, two of which overlap with the output by the rst classi er, then the F-measure is 50%. Therefore, S2 is erroneously ranked lower than S1 on the selection list. To eliminate sentences with very few mentions, a user can set the minimal number of mentions per sentence parameter to N , where N is a non-negative number. Any sentence with less than N mentions is not considered as a candidate for active learning. Mention category weights. Each mention category is associated with a weight in the above metrics. This gives us the exibility to focus on certain categories during active learning. For instance, if a system performs weakly in the ORGANIZATION category, we may want to customize the scoring metric so that the active learner can pick up samples that are particularly useful in improving the performance in this category. The categories with higher weights are considered more important; the categories with zero weights are not considered for active learning. Mention level weights. The mention level indicates whether a mention is named, nominal, or pronominal. We can customize our learner to focus on a particular mention level by adjusting the weight associated with it."}, {"heading": "4 Results", "text": "We carried out a number of experiments to evaluate the di erent sentence selection strategies presented in the previous section. All the strategies perform better than random selection, but the con dence sum metric with the feature-di erent classi er training setting gives the greatest improvement.\nIdeally, the active learners should be retrained each time a new sample has been selected by active learning and annotated by a human. In reality, this is rarely done due to time and computational cost. Instead, active learners usually operate in batch mode a set of samples, instead of a single sample, is selected at each step and the active learners are retrained by adding all the samples in the set. Given our granularity of selection, one way to de ne the size of this batch can be in term of number of sentences to be added at each step. However, this approach does not provide a way to di erentiate between a sentence that is 100 words long and another that is only 10 words long; both are equally important. Therefore, we de ne the batch in terms of the number of words contained in the sentences. At each step, we add just enough complete sentences that, among themselves, contain the given number of words or as close to that number as possible. In the experiments described in this section, this size is 20000 words. First, a set of 20000 words is randomly selected to build seed models. Then at each step of active learning, an additional 20000 words are selected. We also describe the e ect of this step size at the end of this section. All the results presented in this section are on our development evaluation set after each step of active learning, we train a model using all the data that have been selected as of that step (including the seed data), and the model's performance on the development set is reported."}, {"heading": "4.1 E ect of di erent scoring metrics", "text": "In the experiments described here, we consider only named and nominal mentions. The weights for all mention categories are set to 1.\nRandom Selection. We established a baseline performance by selecting sentences at random from the unlabeled data pool. We performed 5 runs and averaged the numbers from each run. The performance for the random selection strategy is shown in Figure 2, including each run and their average. The average performance is used as the baseline and shown in later graphs. The X-axis represents the data size in number of words, and the Y-axis presents the F-measure on the development set.\nF-measure and Macro-Averaged F-measure. Figure 3 shows the curves for the F-measure and Macro-Averaged F-measure metrics compared to the baseline. As we can see, the F-measure metric works very well. Even at 40K words (after only one step of active learning), we observe an increase of about 2 absolute F-measure points with the same amount of labeled data, compared to random sentence selection. To attain the same performance, the random strategy needs 1.5 times as much labeled data. The selection strategy based on macro-averaged F-measure also performs better than random selection, but the improvement is smaller than that for the F-measure metric. As indicated, these metrics were computed only for the data-di erent classi er setting.\nCon dence Sum. Figure 4 shows the curves for the con dence sum metric. This metric is computed for both the data-di erent and the feature-di erent setting. As shown in the gure, the classi ers trained on di erent features (FD) lead to slightly higher improvement in the rst two steps than the classi ers with the same feature set (DD). Overall, the con dence sum performs much better than random selection: at 60K words (after only two steps of active learning), the con dence sum strategy attains better performance than the baseline does with twice this amount of labeled data.\n0 20 40 60 80 100 120 Data Size (x 1000 words)\n68\n70\n72\n74\n76\nFm\nea su re Random DD F-meas. FD Conf. Sum FD Conf. Diff.\nFig. 6. Comparing the 3 best selection strategies.\nCon dence Di erence. Figure 5 shows the results for the con dence difference metric. Similar to the con dence sum metric, using the classi ers trained on di erent features (FD) leads to better improvements in the rst two steps. At 60K words, the system achieves the same performance as the random strategy at 100K words.\nThe Best 3 Strategies. Figure 6 compares the curves for the 3 best strategies - FD con dence sum , FD con dence di erence, and DD F-measure. The FD con dence sum strategy outperforms the other two strategies in the rst three active learning steps. At the fourth step, the FD con dence di erence strategy catches up. At 60K words, the FD con dence sum strategy achieves the performance of the random strategy at about 130K words. This indicates a reduction of over 50% in the amount of annotated data and demonstrates the e ectiveness of active learning for making annotation more e cient."}, {"heading": "4.2 Focusing on speci c types of mentions", "text": "As we have described in the previous section, our system allows us to weight the di erent types and levels of mentions di erently in order to focus our learning on speci c mention types and levels. We provide the results of two such experiments: one focusing on the PERSON category and one focusing on named mentions.\nFirst, we target the learning to mentions of type PERSON. To achieve this, we weight the PERSON category twice as much as any of the other mention categories. The results are shown in Figure 7. Note that the F-measure along the y-axis in the graph is the performance for the PERSON category.\nThe second experiment was for named mentions. For this experiment, we set the weights to all other levels to 0. The result is an active learner speci cally\n0 20 40 60 80 100 120 Data Size (x 1000 words)\n60\n65\n70\n75\n80\nFm\nea su\nre\n5K 10K 20K 40K Random\nFig. 9. E ect of step size on con dence sum metric.\ntuned to maximizing the performance of the system on named mentions. Figure 8 shows the results for this experiment. This plot clearly illustrates the advantages of active learning. We see a 5-point F-measure increase with only one step of learning, which could otherwise have only been achieved by annotating almost 2.5 times as much data."}, {"heading": "4.3 E ect of step size", "text": "The size of the step used in the learning needs to be optimized. On the one hand, getting higher performance gains from smaller amounts of labeled data to be added at each step represents a huge savings in the annotation task; on the other hand, retraining active learners at each step takes time and resources. We need to balance the cost of retraining active learners and the gains from smaller batches.\nWe experimented with di erent step sizes for the feature-di erent (FD) con dencesum metric. The results are shown in Figure 9. From that graph, it seems that a step size of 10000 words might have provided the best balance between annotation and performance. Step sizes higher than 20000 do not seem to have the right granularity for e ective learning."}, {"heading": "5 Conclusions", "text": "We conducted several active learning experiments for the task of detecting mentions of entities in human transcripts of spontaneous conversational speech. Speci cally, we proposed and compared a variety of sentence selection strategies for active learning. The best strategy uses the sum of the con dence values\nof a pair of classi ers trained with di erent feature sets. Compared to random sentence selection, this strategy required 50% of the data to achieve the same performance. For named mentions, the strategy required only 42% of the data needed by random selection to achieve the same performance.\nIn the future, we would like to test our active learning strategies on data from a di erent domain and data in languages other than English, such as the ACE mention annotation corpus. We would also like to explore active learning for co-reference and relation annotation."}, {"heading": "Acknowledgment", "text": "This project was part of a research e ort funded by NSF. Any opinions, ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect the views of the NSF."}], "references": [{"title": "Automatic Content Extraction", "author": ["ACE"], "venue": "http://www.nist.gov/speech/tests/ace/", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Heterogeneous uncertainty sampling for supervised learning", "author": ["D. Lewis", "J. Catlett"], "venue": "Proceedings of ICML 1994, 11th International Conference on Machine Learning, New Brunswick, NJ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Employing EM in pool-based active learning for text classi cation", "author": ["A.K. McCallum", "K. Nigam"], "venue": "Proceedings of ICML 1998, 15th International Conference on Machine Learning, Madison, US", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Less is more: Active learning with support vector machines", "author": ["G. Schohn", "D. Cohn"], "venue": "Proceedings of ICML 2000, 17th International Conf. on Machine Learning.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Support vector machine active learning with applications to text classi cation", "author": ["S. Tong", "D. Koller"], "venue": "Journal of Machine Learning Research 2", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Committee-based sample selection for probabilistic classi ers", "author": ["S. Argamon-Engelson", "I. Dagan"], "venue": "Journal of Arti cial Intelligence Research 11", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Active learning for natural language parsing and information extraction", "author": ["C.A. Thompson", "M.E. Cali", "R.J. Mooney"], "venue": "Proceedings of ICML 1999, 16th International Conference on Machine Learning.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Active learning for statistical natural language parsing", "author": ["M. Tang", "X. Luo", "S. Roukos"], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Lingusitics, Philadelphia, PA, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Example selection for bootstrapping statistical parsers", "author": ["M. Steedman", "R. Hwa", "S. Clark", "M. Osborne", "A. Sarkar", "J. Hockenmacier", "P. Ruhlen", "S. Baker", "J. Crim"], "venue": "Proceedings of HLT-NAACL 2003, Edmonton, Canada", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Ensemble-based active learning for parse selection", "author": ["M. Osborne", "J. Baldridge"], "venue": "Proceedings of HLT-NAACL 2004, Boston, Massachusetts, USA", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Rule writing or annotation: Cost-e cient resource usage for base noun phrase chunking", "author": ["G. Ngai", "D. Yarowsky"], "venue": "Proceedings of 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "An empirical study of active learning with support vector machines for japanese word segmentation", "author": ["M. Sassano"], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Scaling to very very large corpora for natural language disambiguation", "author": ["M. Banko", "E. Brill"], "venue": "Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, Toulouse, France", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Active learning selection strategies for information extraction", "author": ["A. Finn", "N. Kushmerick"], "venue": "ECML-03 Workshop on Adaptive Text Extraction and Mining, Croatia", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Multi-criteria-based active learning for named entity recognition", "author": ["D. Shen", "J. Zhang", "J. Su", "G. Zhou", "C.L. Tan"], "venue": "Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL'04), Main Volume, Barcelona, Spain", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Investigating the e ects of selective sampling on the annotation task", "author": ["B. Hachey", "B. Alex", "M. Becker"], "venue": "Proceedings of the 9th Conference on Compuational Natural Language Learning(CoNLL), Ann Arbor", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Active learning with statistical models", "author": ["D.A. Cohn", "Z. Ghahramani", "M.I. Jordan"], "venue": "Journal of Arti cial Intelligence Research 4", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "Query by committee", "author": ["H.S. Seung", "M. Opper", "H. Sompolinsky"], "venue": "Computational Learning Theory.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1992}, {"title": "Supporting access to large digital oral history archives", "author": ["S. Gustman", "D.S.D. Oard", "W. Byrne", "M. Picheny", "B. Ramabhadran", "D. Greenberg"], "venue": "Proceedings of the Joint Conference on Digital Libraries.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Building an information retrieval test collection for spontaneous conversational speech", "author": ["D. Oard", "D. Soergel", "D. Doermann", "X. Huang", "G. Murray", "J. Wang", "B. Ramabhadran", "M. Franz", "S. Gustman", "J. May eld", "L. Kharevych", "S. Strassel"], "venue": "Proceedings of SIGIR'04, She eld, U.K.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "A statistical model for multilingual entity detection and tracking", "author": ["R. Florian", "H. Hassan", "A. Ittycheriah", "H. Jing", "N. Kambhatla", "X. Luo", "N. Nicolov", "S. Roukos"], "venue": "Proceedings of HLT-NAACL 2004, Boston, Massachusetts, USA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Here, we employ the terminology used in the Automatic Content Extraction Conferences [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "Active learning has been utilized for many NLP applications, most noticeably for text classi cation [2 5].", "startOffset": 100, "endOffset": 105}, {"referenceID": 4, "context": "Active learning has been utilized for many NLP applications, most noticeably for text classi cation [2 5].", "startOffset": 100, "endOffset": 105}, {"referenceID": 5, "context": "It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].", "startOffset": 77, "endOffset": 83}, {"referenceID": 9, "context": "It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].", "startOffset": 77, "endOffset": 83}, {"referenceID": 10, "context": "It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].", "startOffset": 178, "endOffset": 182}, {"referenceID": 13, "context": "[14] investigated several document, rather than sentence, selection strategies for Information Extraction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] proposed a multi-criteria-based active learning approach for named entity recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] investigated a query-by-committee-based active learner for information extraction in the astronomy domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Active learning techniques are usually divided into two types: uncertainty sampling for a single learner [17], or disagreement measurement between a committee of learners [18].", "startOffset": 105, "endOffset": 109}, {"referenceID": 17, "context": "Active learning techniques are usually divided into two types: uncertainty sampling for a single learner [17], or disagreement measurement between a committee of learners [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 18, "context": "The MALACH collection [19, 20] contains 116,000 hours of digitized interviews and testimonies in 32 languages from 52,000 survivors, liberators, rescuers and witnesses of the Holocaust.", "startOffset": 22, "endOffset": 30}, {"referenceID": 19, "context": "The MALACH collection [19, 20] contains 116,000 hours of digitized interviews and testimonies in 32 languages from 52,000 survivors, liberators, rescuers and witnesses of the Holocaust.", "startOffset": 22, "endOffset": 30}, {"referenceID": 13, "context": "A possible problem with selecting samples at the document level [14] is that a document may only be partially useful from a learning point of view and and it is impossible to add only the interesting examples so as to maximize the e ect of active learning.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "Selecting tokens as samples, as in [15], has the advantage that all samples are of equal length but su ers from the problem that a human annotator has to annotate mentions by looking at only the tokens.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "Therefore, we use sentence-level blocks for active learning, as in [16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "At each step in the active learning process, we build two maximum-entropy based statistical classi ers [21] using the labeled data available at that step.", "startOffset": 103, "endOffset": 107}], "year": 2014, "abstractText": "We propose and compare various sentence selection strategies for active learning for the task of detecting mentions of entities. The best strategy employs the sum of con dences of two statistical classi ers trained on di erent views of the data. Our experimental results show that, compared to the random selection strategy, this strategy reduces the amount of required labeled training data by over 50% while achieving the same performance. The e ect is even more signi cant when only named mentions are considered: the system achieves the same performance by using only 42% of the training data required by the random selection strategy.", "creator": "TeX"}}}