{"id": "0911.5104", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2009", "title": "A Bayesian Rule for Adaptive Control based on Causal Interventions", "abstract": "Explaining dynamical behavior is a branch might in artificial knowledge research. Here not broaden adaptive agents actually vanilla convolution for occurring seen useful though sensor (I / O ). Each defined years the chocolate definition first ' possible countries ', but full randy what something know within followed long change worlds whole. maybe collapse. The problem as while dictate came I / O mouth prior one means for is system through long necessarily world. A natural would most themes need be contained by the Kullback - Leibler (KL) diverged continuing still I / O example of followed appears games and the I / O distribution week by into lawyer that is significant only prevent championships. In over judge years theory input streams, once Bayesian mixture provides a well - known solving for this problem. We so, given, thought in three case especially I / O streams result effective handle down, because dynamically all issued recently the chicago exists few require a their lagrangian geometry having giving by refusal regression. Based in has calculus, we ensure man Bayesian operation rule that modify modeling semantic manner even mixture multivariate up I / O creeks. This power might allow year another tale nature from adaptive control based soon of reduced KL - logical.", "histories": [["v1", "Thu, 26 Nov 2009 15:52:33 GMT  (100kb,S)", "https://arxiv.org/abs/0911.5104v1", "6 pages, 2 figures"], ["v2", "Wed, 30 Dec 2009 23:34:14 GMT  (74kb)", "http://arxiv.org/abs/0911.5104v2", "AGI-2010. 6 pages, 2 figures"]], "COMMENTS": "6 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["pedro a ortega", "daniel a braun"], "accepted": false, "id": "0911.5104"}, "pdf": {"name": "0911.5104.pdf", "metadata": {"source": "CRF", "title": "A Bayesian Rule for Adaptive Control based on Causal Interventions", "authors": ["Pedro A. Ortega", "Daniel A. Braun"], "emails": ["peortega@dcc.uchile.cl", "dab54@cam.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :0\n91 1.\n51 04\nv2 [\ncs .A\nI] 3\n0 D\nec 2\n00 9\nKeywords: Adaptive behavior, Intervention calculus, Bayesian control, Kullback-Leibler-divergence"}, {"heading": "Introduction", "text": "The ability to adapt to unknown environments is often considered a hallmark of intelligence [Beer, 1990, Hutter, 2004]. Agent and environment can be conceptualized as two systems that exchange symbols in every time step [Hutter, 2004]: the symbol issued by the agent is an action, whereas the symbol issued by the environment is an observation. Thus, both agent and environment can be conceptualized as probability distributions over sequences of actions and observations (I/O streams). If the environment is perfectly known then the I/O probability distribution of the agent can be tailored to suit this particular environment. However, if the environment is unknown, but known to belong to a set of possible environments, then the agent faces an adaptation problem. Consider, for example, a robot that has been endowed with a set of behavioral primitives\nand now faces the problem of how to act while being ignorant as to which is the correct primitive. Since we want to model both agent and environment as probability distributions over I/O sequences, a natural way to measure the degree of adaptation would be to measure the \u2018distance\u2019 in probability space between the I/O distribution represented by the agent and the I/O distribution conditioned on the true environment. A suitable measure (in terms of its information-theoretic interpretation) is readily provided by the KL-divergence [MacKay, 2003]. In the case of passive prediction, the adaptation problem has a well-known solution. The distribution that minimizes the KL-divergence is a Bayesian mixture distribution over all possible environments [Haussler and Opper, 1997, Opper, 1998]. The aim of this paper is to extend this result for distributions over both inputs and outputs. The main result of this paper is that this extension is only possible if we consider the special syntax of actions in probability theory as it has been suggested by proponents of causal calculus [Pearl, 2000]."}, {"heading": "Preliminaries", "text": "We restrict the exposition to the case of discrete time with discrete stochastic observations and control signals. Let O and A be two finite sets, the first being the set of observations and the second being the set of actions. We use a\u2264t \u2261 a1a2 . . . at, ao\u2264t \u2261 a1o1 . . . atot etc. to simplify the notation of strings. Using A and O, a set of interaction sequences is constructed. Define the set of interactions as Z \u2261 A\u00d7O. A pair (a, o) \u2208 Z is called an interaction. The set of interaction strings of length t \u2265 0 is denoted by Zt. Similarly, the set of (finite) interaction strings is Z\u2217 \u2261 \u22c3\nt\u22650 Z t and the set\nof (infinite) interaction sequences is Z\u221e \u2261 {w : w = a1o1a2o2 . . .}, where each (at, ot) \u2208 Z. The interaction string of length 0 is denoted by \u01eb. Agents and environments are formalized as I/O systems. An I/O system is a probability distribution Pr over interaction sequences Z\u221e. Pr is uniquely determined by the conditional probabilities\nPr(at|ao<t), Pr(ot|ao<tat) (1)\nfor each ao\u2264t \u2208 Z \u2217. However, the semantics of the\nprobability distribution Pr are only fully defined once it is coupled to another system. Let P, Q be two I/O systems. An interaction system (P,Q) is a coupling of the two systems giving rise to the generative distribution G that describes the probabilities that actually govern the I/O stream once the two systems are coupled. G is specified by the equations\nG(at|ao<t) = P(at|ao<t)\nG(ot|ao<tat) = Q(ot|ao<tat)\nvalid for all aot \u2208 Z \u2217. Here, G models the true probability distribution over interaction sequences that arises by coupling two systems through their I/O streams. More specifically, for the system P, P(at|ao<t) is the probability of producing action at \u2208 A given history ao<t and P(ot|ao<tat) is the predicted probability of the observation ot \u2208 O given history ao<tat. Hence, for P, the sequence o1o2 . . . is its input stream and the sequence a1a2 . . . is its output stream. In contrast, the roles of actions and observations are reversed in the case of the system Q. Thus, the sequence o1o2 . . . is its output stream and the sequence a1a2 . . . is its input stream. This model of interaction is very general in that it can accommodate many specific regimes of interaction. Note that an agent P can perfectly predict its environment Q iff for all ao\u2264t \u2208 Z \u2217,\nP(ot|ao<tat) = Q(ot|ao<tat).\nIn this case we say that P is tailored to Q."}, {"heading": "Adaptive Systems: Na\u0308\u0131ve Construction", "text": "Throughout this paper, we use the convention that P is an agent to be constructed by a designer, which is then going to be interfaced with a preexisting but unknown environment Q. The designer assumes that Q is going to be drawn with probability P (m) from a set Q \u2261 {Qm}m\u2208M of possible systems before the interaction starts, where M is a countable set. Consider the case when the designer knows beforehand which environment Q \u2208 Q is going to be drawn. Then, not only can P be tailored to Q, but also a custom-made policy for Q can be designed. That is, the output stream P(at|ao<t) is such that the true probability G of the resulting interaction system (P,Q) gives rise to interaction sequences that the designer considers desirable. Consider now the case when the designer does not know which environment Qm \u2208 Q is going to be drawn, and assume he has a set P \u2261 {Pm}m\u2208M of systems such that for each m \u2208 M, Pm is tailored to Qm and the interaction system (Pm,Qm) has a generative distribution Gm that produces desirable interaction sequences. How can the designer construct a system P such that its behavior is as close as possible to the custom-made system Pm under any realization of Qm \u2208 Q? A convenient measure of how much P deviates from Pm is given by the KL-divergence. A first approach would be to construct an agent P\u0303 so as to minimize\nthe total expected KL-divergence to Pm. This is constructed as follows. Define the history-dependent KLdivergences over the action at and observation ot as\nDatm (ao<t) \u2261 \u2211\nat\nPm(at|ao<t) log2 Pm(at|ao<t)\nPr(at|ao<t)\nDotm(ao<tat) \u2261 \u2211\not\nPm(ot|ao<tat) log2 Pm(ot|ao<tat)\nPr(ot|ao<tat) ,\nwhere Pr is a given arbitrary agent. Then, define the average KL-divergences over at and ot as\nDatm = \u2211\nao <t\nPm(ao<t)D at m (ao<t)\nDotm = \u2211\nao <t at\nPm(ao<tat)D ot m(ao<tat).\nFinally, we define the total expected KL-divergence of Pr to Pm as\nD \u2261 lim sup t\u2192\u221e\n\u2211\nm\nP (m) t \u2211\n\u03c4=1\n(\nDa\u03c4m +D o\u03c4 m\n)\n.\nWe construct the agent P\u0303 as the system that minimizes D = D(Pr):\nP\u0303 \u2261 argmin Pr D(Pr). (2)\nThe solution to Equation 2 is the system P\u0303 defined by the set of equations\nP\u0303(at|ao<t) = \u2211\nm\nPm(at|ao<t)wm(ao<t)\nP\u0303(ot|ao<tat) = \u2211\nm\nPm(ot|ao<tat)wm(ao<tat) (3)\nvalid for all ao\u2264t \u2208 Z \u2217, where the mixture weights are\nwm(ao<t) \u2261 P (m)Pm(ao<t) \u2211\nm\u2032 P (m \u2032)Pm\u2032(ao<t)\nwm(ao<tat) \u2261 P (m)Pm(ao<tat) \u2211\nm\u2032 P (m \u2032)Pm\u2032(ao<tat)\n.\n(4)\nFor reference, see Haussler and Opper [1997], Opper\n[1998]. It is clear that P\u0303 is just the Bayesian mixture over the agents Pm. If we define the conditional probabilities\nP (at|m, ao<t) \u2261 Pm(at|ao<t)\nP (ot|m, ao<tat) \u2261 Pm(at|ao<tat) (5)\nfor all ao\u2264t \u2208 Z \u2217, then Equation 3 can be rewritten as\nP\u0303(at|ao<t) = \u2211\nm\nP (at|m, ao<t)P (m|ao<t)\nP\u0303(ot|ao<tat) = \u2211\nm\nP (ot|m, ao<tat)P (m|ao<tat) (6)\nwhere the P (m|ao<t) = wm(ao<t) and P (m|ao<tat) = wm(ao<tat) are just the posterior probabilities over the\nelements in M given the past interactions. Hence, the conditional probabilities in Equation 5, together with the prior probabilities P (m), define a Bayesian model over interaction sequences with hypotheses m \u2208 M. The behavior of P\u0303 can be described as follows. At any given time t, P\u0303 maintains a mixture over systems Pm. The weighting over them is given by the mixture coefficients wm. Whenever a new action at or a new observation is produced (by the agent or the environment respectively), the weights wm are updated according to Bayes\u2019 rule. In addition, P\u0303 issues an action at suggested by a system Pm drawn randomly according to the weights wt. However, there is an important problem with P\u0303 that arises due to the fact that it is not only a system that is passively observing symbols, but also actively generating them. Therefore, an action that is generated by the agent should not provide the same information than an observation that is issued by its environment. Intuitively, it does not make any sense to use one\u2019s own actions to do inference. In the following section we illustrate this problem with a simple statistical example."}, {"heading": "The Problem of Causal Intervention", "text": "Suppose a statistician is asked to design a model for a given data set D and she decides to use a Bayesian method. She computes the posterior probability density function (pdf) over the parameters \u03b8 of the model given the data using Bayes\u2019 rule:\np(\u03b8|D) = p(D|\u03b8)p(\u03b8) \u222b\np(D|\u03b8\u2032)p(\u03b8\u2032) d\u03b8\u2032 ,\nwhere p(D|\u03b8) is the likelihood of D given \u03b8 and p(\u03b8) is the prior pdf of \u03b8. She can simulate the source by drawing a sample data set S from the predictive pdf\np(S|D) =\n\u222b\np(S|D, \u03b8)p(\u03b8|D) d\u03b8,\nwhere p(S|D, \u03b8) is the likelihood of S given D and \u03b8. She decides to do so, obtaining a sample set S\u2032. She understands that the nature of S\u2032 is very different from"}, {"heading": "D: while D is informative and does change the belief state of the Bayesian model, S\u2032 is non-informative and", "text": "thus is a reflection of the model\u2019s belief state. Hence, she would never use S\u2032 to further condition the Bayesian model. Mathematically, she seems to imply that\np(\u03b8|D,S\u2032) = p(\u03b8|D)\nif S\u2032 has been generated from p(S|D) itself. But this simple independence assumption is not correct as the following elaboration of the example will show. The statistician is now told that the source is waiting for the simulation results S\u2032 in order to produce a next data set D\u2032 which does depend on S\u2032. She hands in S\u2032 and obtains a new data set D\u2032. Using Bayes\u2019 rule, the posterior pdf over the parameters is now\np(D\u2032|D,S\u2032, \u03b8)p(D|\u03b8)p(\u03b8) \u222b\np(D\u2032|D,S\u2032, \u03b8\u2032)p(D|\u03b8\u2032)p(\u03b8\u2032) d\u03b8\u2032 (7)\nwhere p(D\u2032|D,S\u2032, \u03b8) is the likelihood of the new data D\u2032 given the old data D, the parameters \u03b8 and the simulated data S\u2032. Notice that this looks almost like the posterior pdf p(\u03b8|D,S\u2032,D\u2032) given by\np(D\u2032|D,S\u2032, \u03b8)p(S\u2032|D, \u03b8)p(D|\u03b8)p(\u03b8) \u222b\np(D\u2032|D,S\u2032, \u03b8\u2032)p(S\u2032|D, \u03b8\u2032)p(D|\u03b8\u2032)p(\u03b8\u2032) d\u03b8\u2032\nwith the exception that now the Bayesian update contains the likelihoods of the simulated data p(S\u2032|D, \u03b8). This suggests that Equation 7 is a variant of the posterior pdf p(\u03b8|D,S\u2032,D\u2032) but where the simulated data S\u2032 is treated in a different way than the data D and D\u2032. Define the pdf p\u2032 such that the pdfs p\u2032(\u03b8), p\u2032(D|\u03b8), p\u2032(D\u2032|D,S\u2032, \u03b8) are identical to p(\u03b8), p(D|\u03b8) and p(D\u2032|D,S\u2032, \u03b8) respectively, but differ in p\u2032(S|D, \u03b8):\np\u2032(S|D, \u03b8) =\n{\n1 if S\u2032 = S,\n0 else.\nThat is, p\u2032 is identical to p but it assumes that the value of S is fixed to S\u2032 given D and \u03b8. For p\u2032, the simulated data S\u2032 is non-informative:\n\u2212 log 2 p(S\u2032|D, \u03b8) = 0.\nIf one computes the posterior pdf p\u2032(\u03b8|D,S\u2032,D\u2032), one obtains the result of Equation 7:\np\u2032(D\u2032|D,S\u2032, \u03b8)p\u2032(S\u2032|D, \u03b8)p\u2032(D|\u03b8)p\u2032(\u03b8) \u222b\np\u2032(D\u2032|D,S\u2032, \u03b8\u2032)p\u2032(S\u2032|D, \u03b8\u2032)p\u2032(D|\u03b8\u2032)p\u2032(\u03b8\u2032) d\u03b8\u2032\n= p(D\u2032|D,S\u2032, \u03b8)p(D|\u03b8)p(\u03b8) \u222b\np(D\u2032|D,S\u2032, \u03b8\u2032)p(D|\u03b8\u2032)p(\u03b8\u2032) d\u03b8\u2032 .\nThus, in order to explain Equation 7 as a posterior pdf given the data sets D, D\u2032 and the simulated data S\u2032, one has to intervene p in order to account for the fact that S\u2032 is non-informative given D and \u03b8. In statistics, there is a rich literature on causal intervention. In particular, we will use the formalism developed by Pearl [2000], because it suits the needs to formalize interactions in systems and has a convenient notation\u2014compare Figures 1a & b. Given a causal model1 variables that are intervened are denoted by a hat as in S\u0302. In the previous example, the causal model of the joint pdf p(\u03b8,D,S,D\u2032) is given by the set of conditional pdfs\nCp = { p(\u03b8), p(D|\u03b8), p(S|D, \u03b8), p(D\u2032|D,S, \u03b8) } .\nIf D and D\u2032 are observed from the source and S is intervened to take on the value S\u2032, then the posterior pdf over the parameters \u03b8 is given by p(\u03b8|D, S\u0302\u2032,D\u2032) which is just\np(D\u2032|D, S\u0302\u2032, \u03b8)p(S\u0302\u2032|D, \u03b8)p(D|\u03b8)p(\u03b8) \u222b\np(D\u2032|D, S\u0302\u2032, \u03b8\u2032)p(S\u0302\u2032|D, \u03b8\u2032)p(D|\u03b8\u2032)p(\u03b8\u2032) d\u03b8\u2032\n= p(D\u2032|D,S\u2032, \u03b8)p(D|\u03b8)p(\u03b8) \u222b\np(D\u2032|D,S\u2032, \u03b8\u2032)p(D|\u03b8\u2032)p(\u03b8\u2032) d\u03b8\u2032 .\n1For our needs, it is enough to think about a causal model as a complete factorization of a probability distribution into conditional probability distributions representing the causal structure.\nbecause p(D\u2032|D, S\u0302\u2032, \u03b8) = p(D\u2032|D,S\u2032, \u03b8), which corresponds to applying rule 2 in Pearl\u2019s intervention calculus, and because p(S\u0302\u2032|D, \u03b8\u2032) = p\u2032(S\u2032|D, \u03b8\u2032) = 1."}, {"heading": "Adaptive Systems: Causal Construction", "text": "Following the discussion in the previous section, we want to construct an adaptive agent P by minimizing the KL-divergence to the Pm, but this time treating actions as interventions. Based on the definition of the conditional probabilities in Equation 5, we construct now the KL-divergence criterion to characterize P using intervention calculus. Importantly, interventions index a set of intervened probability distribution derived from an initial probability distribution. Hence, the set of fixed intervention sequences of the form a\u03021a\u03022 . . . indexes probability distributions over observation sequences o1o2 . . .. Because of this, we are going to construct a set of criteria indexed by the intervention sequences, but we will see that they all have the same solution. Define the history-dependent intervened KLdivergences over the action at and observation ot as\nCatm (a\u0302o<t) \u2261 \u2211\nat\nP (at|m, a\u0302o<t) log2 P (at|m, a\u0302o<t)\nPr(at|ao<t)\nCotm (a\u0302o<ta\u0302t) \u2261 \u2211\not\nP (ot|m, a\u0302o<ta\u0302t) log2 P (ot|m, a\u0302o<ta\u0302t)\nPr(ot|ao<tat) ,\nwhere Pr is a given arbitrary agent. Note that past actions are treated as interventions. Then, define the average KL-divergences over at and ot as\nCatm = \u2211\nao <t\nP (a\u0302o<t|m)C at m (a\u0302o<t)\nCotm = \u2211\nao <t at\nP (a\u0302o<tat|m)C ot m (a\u0302o<ta\u0302t).\nFinally, we define the total expected KL-divergence of P to Pm as\nC \u2261 lim sup t\u2192\u221e\n\u2211\nm\nP (m)\nt \u2211\n\u03c4=1\n(\nCa\u03c4m + C o\u03c4 m\n)\n. (8)\nWe construct the agent P as the system that minimizes C = C(Pr):\nP \u2261 argmin Pr C(Pr). (9)\nThe solution to Equation 9 is the system P defined by the set of equations\nP(at|ao<t) = P (at|a\u0302o<t)\n= \u2211\nm\nP (at|m, a\u0302o<t)vm(a\u0302o<t)\nP(ot|ao<tat) = P (ot|a\u0302o<ta\u0302t)\n= \u2211\nm\nP (ot|m, a\u0302o<ta\u0302t)vm(a\u0302o<ta\u0302t)\n(10)\nvalid for all ao\u2264t \u2208 Z \u2217, where the mixture weights are\nvm(ao<tat) = vm(ao<t) \u2261 P (m)P (a\u0302o<t|m) \u2211\nm\u2032 P (m \u2032)P (a\u0302o<t|m)\n= P (m)\n\u220ft\u22121\n\u03c4=1 P (o\u03c4 |m, a\u0302o<\u03c4 a\u0302\u03c4 ) \u2211\nm\u2032 P (m \u2032) \u220ft\u22121 \u03c4=1 P (o\u03c4 |m \u2032, a\u0302o<\u03c4 a\u0302\u03c4 )\n.\n(11) The proof follows the same line of argument as the solution to Equation 2 with the crucial difference that actions are treated as interventions. Consider without loss of generality the summand \u2211\nm P (m)C at m in Equa-\ntion 8. Note that the KL-divergence can be written as a difference of two logarithms, where only one term depends on Pr that we want to vary. Therefore, we can integrate out the other term and write it as a constant c. Then we get\nc\u2212 \u2211\nm\nP (m) \u2211\na\u0302o <t\nP (a\u0302o<t|m)\n\u00b7 \u2211\nat\nP (at|m, a\u0302o<t) lnPr(at|a\u0302o<t).\nSubstituting P (a\u0302o<t|m) by P (m|a\u0302o<t)P (a\u0302o<t)/P (m) and identifying P characterized by Equations 10 and 11 we obtain\nc\u2212 \u2211\na\u0302o <t\nP (a\u0302o<t) \u2211\nat\nP(at|a\u0302o<t) lnPr(at|a\u0302o<t).\nThe inner sum has the form \u2212 \u2211\nx p(x) ln q(x), i.e. the cross-entropy between q(x) and p(x), which is minimized when q(x) = p(x) for all x. By choosing this optimum one obtains Pr(at|a\u0302o<t) = P(at|a\u0302o<t) for all at. Note that the solution to this variational problem is independent of the weighting P (a\u0302o<t). Since the same argument applies to any summand \u2211\nm P (m)C a\u03c4 m and\n\u2211\nm P (m)C o\u03c4 m in Equation 8, their variational problems are mutually independent. The behavior of P differs in an important aspect from\nP\u0303. At any given time t, P maintains a mixture over systems Pm. The weighting over these systems is given by the mixture coefficients vm. In contrast to P\u0303, P updates the weights vm only whenever a new observation ot is produced by the environment respectively. The update follows Bayes\u2019 rule but treating past actions as interventions, i.e. dropping the evidence they provide. In addition, P issues an action at suggested by an system m drawn randomly according to the weights vm\u2014see Figures 1c & d. If we use the following equalities connecting the weights and the intervened posterior distributions\nvm(ao<t) = P (m|a\u0302o<t) = P (m|a\u0302o<ta\u0302t) = vm(ao<tat)\nand substitute interventions by observations in the conditionals\nP (at|m, a\u0302o<t) = P (at|m, ao<t)\nP (ot|m, a\u0302o<ta\u0302t) = P (ot|m, ao<tat)\nwhich corresponds to rule 2 of Pearl\u2019s intervention calculus, we can rewrite Equations 10 and 11 as\nP(at|ao<t) = P (at|a\u0302o<t)\n= \u2211\nm\nP (at|m, ao<t)P (m|a\u0302o<t) (12)\nP(ot|ao<tat) = P (ot|a\u0302o<ta\u0302t)\n= \u2211\nm\nP (ot|m, ao<tat)P (m|a\u0302o<t) (13)\nwhere the intervened posterior probabilities are\nP (m|a\u0302o<t) = P (m)\n\u220ft\u22121\n\u03c4=1 P (o\u03c4 |m, ao<\u03c4a\u03c4 ) \u2211\nm\u2032 P (m \u2032) \u220ft\u22121 \u03c4=1 P (o\u03c4 |m \u2032, ao<\u03c4a\u03c4 )\n.\n(14) Equations 12, 13 and 14 are important because they describe the behavior of P only in terms of known probabilities, i.e. probabilities that are computable from the causal model associated to P given by\nCP = { P (m), P (at|m, ao<t), P (ot|m, ao<tat) : t \u2265 1 } .\nImportantly, Equation 12 describes a stochastic method to produce desirable actions that differs fundamentally from an agent that is constructed by choosing an optimal policy with respect to a given utility criterion. We call this action selection rule the Bayesian control rule."}, {"heading": "Experimental Results", "text": "Here we design a very simple toy experiment to illustrate the behavior of an agent P\u0303 based on a Bayesian mixture compared to an agent P based on the Bayesian control rule. Let Q0, Q1, P0 and P1 be four agents with binary I/O sets A = O = {0, 1} defined as follows. P1 is such that P1(at|ao<t) = P1(at) and P1(ot|ao<tat) = P1(ot) for all ao\u2264t \u2208 Z \u2217, where\nP1(at) =\n{\n0.1 if at = 0 0.9 if at = 1 , P1(ot) =\n{\n0.4 if at = 0 0.6 if at = 1 .\nLet P0 be such that\nP0(at|ao<t) = 1\u2212 P1(at|ao<t)\nP0(ot|ao<tat) = 1\u2212 P0(ot|ao<tat)\nfor all ao\u2264t \u2208 Z \u2217. Thus, P0 and P1 are agents that are biased towards observing and acting 0\u2019s and 1\u2019s respectively. Furthermore, Q0 = P0 and Q1 = P1. Assume a uniform distribution over Q = {Q0,Q1}, i.e. P (m = 0) = P (m = 1) = 1\n2 .\nAssume Q0 \u2208 Q is drawn. In this case, one wants the agents P\u0303 and P to minimize the deviation from P0. Consider the following instantaneous measure\nd(t) \u2261 \u2211\na\u2032 t\nP0(a \u2032 t) log2\nP0(a \u2032 t)\nPr(a\u2032t|ao<t)\n+ \u2211\no\u2032 t\nP0(o \u2032 t) log2\nP0(o \u2032 t)\nPr(o\u2032t|ao<tat)\nwhere a1o1a2o2 . . . is a realization of the interaction system (Pr,Q0). d(t) measures how much Pr\u2019s action and observation probabilities deviate from P0 at time t. Recall that both P\u0303 and P maintain a mixture over P0 and P1. The instantaneous I/O probabilities of such a system can always be written as\nwP0(at) + (1 \u2212 w)P1(at)\nwP0(ot) + (1\u2212 w)P1(ot).\nwhere w \u2208 [0, 1]. Thus, it is easy to see that the instantaneous I/O deviation takes on the minimum value when w = 1 and the maximum value when w = 0: In the case w = 1, d(t) = 0 bits; In the case w = 0, d(t) \u2248 2.653. We have simulated realizations of the instantaneous I/O deviation using the agents P\u0303 and P. The results\nare summarized in Figure 2. For P\u0303, d(t) happens to be non-ergodic: it either converges to d(t) \u2192 0 or to\nd(t) \u2192\u2248 2.654, implying that either P\u0303 \u2192 P0 or P\u0303 \u2192 P1 respectively. In contrast, d(t) \u2192 0 always for P, implying that P \u2192 P0. Analogous results are obtained when Q1 \u2208 Q is drawn instead: For P\u0303, d(t) converges either to 0 or to \u2248 2.654, whereas for P, d(t) \u2192\u2248 2.654 always implying that P \u2192 P1. Hence, P shows the correct adaptive behavior while P\u0303 does not."}, {"heading": "Conclusions", "text": "We propose a Bayesian rule for adaptive control. The key feature of this rule is the special treatment of actions based on causal calculus and the decomposition of agents into Bayesian mixture of I/O distributions. The question of how to integrate information generated by an agent\u2019s probabilistic model into the agent\u2019s information state lies at the very heart of adaptive agent design. We show that the na\u0308\u0131ve application of Bayes\u2019 rule to I/O distributions leads to inconsistencies, because outputs don\u2019t provide the same type of information as genuine observations. Crucially, these inconsistencies vanish if intervention calculus is applied [Pearl, 2000]. Some of the presented key ideas are not unique to the Bayesian control rule. The idea of representing agents and environments as I/O streams has been proposed by a number of other approaches, such as predictive state representation (PSR) [Littman et al., 2002] and the universal AI approach by Hutter [2004]. The idea of breaking down a control problem into a superposition of controllers has been previously evoked in the context of \u201cmixture of experts\u201d-models like the MOSAICarchitecture Haruno et al. [2001]. Other stochastic action selection approaches are found in exploration strategies for (PO)MDPs [Wyatt, 1997], learning automata [Narendra and Thathachar, 1974] and in probability matching [R.O. Duda, 2001] amongst others. The usage of compression principles to select actions has been proposed by AI researchers, for example Schmidhuber [2009]. The main contribution of this paper is the derivation of a stochastic action selection and inference rule by minimizing KL-divergences of intervened I/O distributions. An important potential application of the Bayesian control rule would naturally be the realm of adaptive control problems. Since it takes on a similar form to Bayes\u2019 rule, the adaptive control problem could then be translated into an on-line inference problem where actions are sampled stochastically from a posterior distribution. It is important to note, however, that the problem statement as formulated here and the usual\nBayes-optimal approach in adaptive control are not the same. In the future the relationship between these two problem statements deserves further investigation."}, {"heading": "M. Haruno, D.M. Wolpert, and M. Kawato. Mosaic model", "text": "for sensorimotor learning and control. Neural Computation, 13:2201\u20132220, 2001."}, {"heading": "D. Haussler and M. Opper. Mutual information, metric", "text": "entropy and cumulative relative entropy risk. The Annals of Statistics, 25:2451\u20132492, 1997.\nMarcus Hutter. Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability. Springer, Berlin, 2004.\nM. Littman, R. Sutton, and S. Singh. Predictive representations of state. In Neural Information Processing Systems (NIPS), number 14, pages 1555\u20131561, 2002.\nDavid J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003."}, {"heading": "K. Narendra and M.A.L. Thathachar. Learning automata", "text": "- a survey. IEEE Transactions on Systems, Man, and Cybernetics, SMC-4(4):323\u2013334, July 1974.\nM. Opper. A bayesian approach to online learning. Online Learning in Neural Networks, pages 363\u2013378, 1998.\nJ. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, Cambridge, UK, 2000.\nD.G. Stork R.O. Duda, P.E. Hart. Pattern Classification. Wiley & Sons, Inc., second edition, 2001.\nJ. Schmidhuber. Simple algorithmic theory of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes. Journal of SICE, 48(1):21\u201332, 2009.\nJ. Wyatt. Exploration and Inference in Learning from Reinforcement. PhD thesis, Department of Artificial Intelligence, University of Edinburgh, 1997."}], "references": [{"title": "Intelligence as Adaptive Behavior", "author": ["Randall Beer"], "venue": null, "citeRegEx": "Beer.,? \\Q1990\\E", "shortCiteRegEx": "Beer.", "year": 1990}, {"title": "Mosaic model for sensorimotor learning and control", "author": ["M. Haruno", "D.M. Wolpert", "M. Kawato"], "venue": "Neural Computation,", "citeRegEx": "Haruno et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Haruno et al\\.", "year": 2001}, {"title": "Mutual information, metric entropy and cumulative relative entropy risk", "author": ["D. Haussler", "M. Opper"], "venue": "The Annals of Statistics,", "citeRegEx": "Haussler and Opper.,? \\Q1997\\E", "shortCiteRegEx": "Haussler and Opper.", "year": 1997}, {"title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability", "author": ["Marcus Hutter"], "venue": null, "citeRegEx": "Hutter.,? \\Q2004\\E", "shortCiteRegEx": "Hutter.", "year": 2004}, {"title": "Predictive representations of state", "author": ["M. Littman", "R. Sutton", "S. Singh"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Littman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Littman et al\\.", "year": 2002}, {"title": "Information Theory, Inference, and Learning Algorithms", "author": ["David J.C. MacKay"], "venue": null, "citeRegEx": "MacKay.,? \\Q2003\\E", "shortCiteRegEx": "MacKay.", "year": 2003}, {"title": "Learning automata - a survey", "author": ["K. Narendra", "M.A.L. Thathachar"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Narendra and Thathachar.,? \\Q1974\\E", "shortCiteRegEx": "Narendra and Thathachar.", "year": 1974}, {"title": "A bayesian approach to online learning", "author": ["M. Opper"], "venue": "Online Learning in Neural Networks,", "citeRegEx": "Opper.,? \\Q1998\\E", "shortCiteRegEx": "Opper.", "year": 1998}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q2000\\E", "shortCiteRegEx": "Pearl.", "year": 2000}, {"title": "Simple algorithmic theory of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes", "author": ["J. Schmidhuber"], "venue": "Journal of SICE,", "citeRegEx": "Schmidhuber.,? \\Q2009\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2009}, {"title": "Exploration and Inference in Learning from Reinforcement", "author": ["J. Wyatt"], "venue": "PhD thesis, Department of Artificial Intelligence, University of Edinburgh,", "citeRegEx": "Wyatt.,? \\Q1997\\E", "shortCiteRegEx": "Wyatt.", "year": 1997}], "referenceMentions": [{"referenceID": 3, "context": "Agent and environment can be conceptualized as two systems that exchange symbols in every time step [Hutter, 2004]: the symbol issued by the agent is an action, whereas the symbol issued by the environment is an observation.", "startOffset": 100, "endOffset": 114}, {"referenceID": 5, "context": "A suitable measure (in terms of its information-theoretic interpretation) is readily provided by the KL-divergence [MacKay, 2003].", "startOffset": 115, "endOffset": 129}, {"referenceID": 8, "context": "The main result of this paper is that this extension is only possible if we consider the special syntax of actions in probability theory as it has been suggested by proponents of causal calculus [Pearl, 2000].", "startOffset": 195, "endOffset": 208}, {"referenceID": 2, "context": "For reference, see Haussler and Opper [1997], Opper [1998].", "startOffset": 19, "endOffset": 45}, {"referenceID": 2, "context": "For reference, see Haussler and Opper [1997], Opper [1998]. It is clear that P\u0303 is just the Bayesian mixture over the agents Pm.", "startOffset": 19, "endOffset": 59}, {"referenceID": 8, "context": "In particular, we will use the formalism developed by Pearl [2000], because it suits the needs to formalize interactions in systems and has a convenient notation\u2014compare Figures 1a & b.", "startOffset": 54, "endOffset": 67}], "year": 2009, "abstractText": "Explaining adaptive behavior is a central problem in artificial intelligence research. Here we formalize adaptive agents as mixture distributions over sequences of inputs and outputs (I/O). Each distribution of the mixture constitutes a \u2018possible world\u2019, but the agent does not know which of the possible worlds it is actually facing. The problem is to adapt the I/O stream in a way that is compatible with the true world. A natural measure of adaptation can be obtained by the KullbackLeibler (KL) divergence between the I/O distribution of the true world and the I/O distribution expected by the agent that is uncertain about possible worlds. In the case of pure input streams, the Bayesian mixture provides a well-known solution for this problem. We show, however, that in the case of I/O streams this solution breaks down, because outputs are issued by the agent itself and require a different probabilistic syntax as provided by intervention calculus. Based on this calculus, we obtain a Bayesian control rule that allows modeling adaptive behavior with mixture distributions over I/O streams. This rule might allow for a novel approach to adaptive control based on a minimum KLprinciple.", "creator": "LaTeX with hyperref package"}}}