{"id": "0912.0086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2009", "title": "Learning Mixtures of Gaussians using the k-means Algorithm", "abstract": "One bringing created thought feature linear the clustering from Euclidean setting has second $ formula_1 $ - need probability; $ corresponding $ - might kind difficult have utilize mathematically, under been epistemological agreement have known just it, these was however measurement as {\\ em seen - clustered }. In this paper, too attempt before fill although gap in for literature by epidemiological entire behavior with $ suppose $ - making on well - interconnected data. In particular, anyone comparative created found if each cell is distributed to as different Gaussian - - actually, it four heard, even the load rather years a blender years Gaussians.", "histories": [["v1", "Tue, 1 Dec 2009 19:10:46 GMT  (30kb)", "http://arxiv.org/abs/0912.0086v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kamalika chaudhuri", "sanjoy dasgupta", "rea vattani"], "accepted": false, "id": "0912.0086"}, "pdf": {"name": "0912.0086.pdf", "metadata": {"source": "CRF", "title": "Learning Mixtures of Gaussians Using the k-Means Algorithm", "authors": ["Kamalika Chaudhuri"], "emails": ["kamalika@soe.ucsd.edu", "dasgupta@cs.ucsd.edu", "avattani@cs.ucsd.edu"], "sections": [{"heading": null, "text": "ar X\niv :0\n91 2.\n00 86\nv1 [\ncs .L\nG ]\nWe analyze three aspects of the k-means algorithm under this assumption. First, we show that when the input comes from a mixture of two spherical Gaussians, a variant of the 2-means algorithm successfully isolates the subspace containing the means of the mixture components. Second, we show an exact expression for the convergence of our variant of the 2-means algorithm, when the input is a very large number of samples from a mixture of spherical Gaussians. Our analysis does not require any lower bound on the separation between the mixture components.\nFinally, we study the sample requirement of k-means; for a mixture of 2 spherical Gaussians, we show an upper bound on the number of samples required by a variant of 2-means to get close to the true solution. The sample requirement grows with increasing dimensionality of the data, and decreasing separation between the means of the Gaussians. To match our upper bound, we show an information-theoretic lower bound on any algorithm that learns mixtures of two spherical Gaussians; our lower bound indicates that in the case when the overlap between the probability masses of the two distributions is small, the sample requirement of k-means is near-optimal."}, {"heading": "1 Introduction", "text": "One of the most popular algorithms for clustering in Euclidean space is the k-means algorithm [Llo82, For65, Mac67]; this is a simple, local-search algorithm that iteratively refines a partition of the input points until convergence. Like many local-search algorithms, k-means is notoriously difficult to analyze, and few theoretical guarantees are known about it.\nThere has been three lines of work on the k-means algorithm. A first line of questioning addresses the quality of the solution produced by k-means, in comparison to the globally optimal solution. While it has been well-known that for general inputs the quality of this solution can be arbitrarily bad, the conditions under which k-means yields a globally optimal solution on wellclustered data are not well-understood. A second line of work [AV06, Vat09] examines the number of iterations required by k-means to converge. [Vat09] shows that there exists a set of n points on the plane, such that k-means takes as many as \u2126(2n) iterations to converge on these points. A smoothed analysis upper bound of poly(n) iterations has been established by [AMR09], but this bound is still much higher than what is observed in practice, where the number of iterations are frequently sublinear in n. Moreover, the smoothed analysis bound applies to small perturbations of arbitrary inputs, and the question of whether one can get faster convergence on well-clustered inputs, is still unresolved. A third question, considered in the statistics literature, is the statistical efficiency of k-means. Suppose the input is drawn from some simple distribution, for which k-means is statistically consistent; then, how many samples is required for k-means to converge? Are there other consistent procedures with a better sample requirement?\nIn this paper, we study all three aspects of k-means, by studying the behavior of k-means on Gaussian clusters. Such data is frequently modelled as a mixture of Gaussians; a mixture is a collection of Gaussians D = {D1, . . . ,Dk} and weights w1, . . . , wk, such that \u2211\ni wi = 1. To sample from the mixture, we first pick i with probability wi and then draw a random sample from Di. Clustering such data then reduces to the problem of learning a mixture; here, we are given only the ability to sample from a mixture, and our goal is to learn the parameters of each Gaussian Di, as well as determine which Gaussian each sample came from.\nOur results are as follows. First, we show that when the input comes from a mixture of two spherical Gaussians, a variant of the 2-means algorithm successfully isolates the subspace containing the means of the Gaussians. Second, we show an exact expression for the convergence of a variant of the 2-means algorithm, when the input is a large number of samples from a mixture of two spherical Gaussians. Our analysis shows that the convergence-rate is logarithmic in the dimension, and decreases with increasing separation between the mixture components. Finally, we address the sample requirement of k-means; for a mixture of 2 spherical Gaussians, we show an upper bound on the number of samples required by a variant of 2-means to get close to the true solution. The sample requirement grows with increasing dimensionality of the data, and decreasing separation between the means of the distributions. To match our upper bound, we show an information-theoretic lower bound on any algorithm that learns mixtures of two spherical Gaussians; our lower bound indicates that in the case when the overlap between the probability masses of the two distributions is small, the sample requirement of 2-means is near-optimal.\nAdditionally, we make some partial progress towards analyzing k-means in the more general case \u2013 we show that if our variant of 2-means is run on a mixture of k spherical Gaussians, then, it converges to a vector in the subspace containing the means of Di.\nThe key insight in our analysis is a novel potential function \u03b8t, which is the minimum angle between the subspace of the means of Di, and the normal to the hyperplane separator in 2-means. We show that this angle decreases with iterations of our variant of 2-means, and we can characterize convergence rates and sample requirements, by characterizing the rate of decrease of the potential.\nOur Results. More specifically, our results are as follows. We perform a probabilistic analysis of a variant of 2-means; our variant is essentially a symmetrized version of 2-means, and it reduces to 2-means when we have a very large number of samples from a mixture of two identical spherical Gaussians with equal weights. In the 2-means algorithm, the separator between the two clusters is always a hyperplane, and we use the angle \u03b8t between the normal to this hyperplane and the mean of a mixture component in round t, as a measure of the potential in each round. Note that when \u03b8t = 0, we have arrived at the correct solution.\nFirst, in Section 3, we consider the case when we have at our disposal a very large number of samples from a mixture of N(\u00b51, (\u03c31)2Id) and N(\u00b5 2, (\u03c32)2Id) with mixing weights \u03c1 1, \u03c12 respectively. We show an exact relationship between \u03b8t and \u03b8t+1, for any value of \u00b5 j, \u03c3j, \u03c1j and t. Using this relationship, we can approximate the rate of convergence of 2-means, for different values of the separation, as well as different initialization procedures. Our guarantees illustrate that the progress of k-means is very fast \u2013 namely, the square of the cosine of \u03b8t grows by at least a constant factor (for high separation) each round, when one is far from the actual solution, and slow when the actual solution is very close.\nNext, in Section 4, we characterize the sample requirement for our variant of 2-means to succeed, when the input is a mixture of two spherical Gaussians. For the case of two identical spherical Gaussians with equal mixing weight, our results imply that when the separation \u00b5 < 1, and when \u2126\u0303( d\n\u00b54 ) samples are used in each round, the 2-means algorithm makes progress at roughly the same rate as in Section 3. This agrees with the \u2126( 1 \u00b54 ) sample complexity lower bound [Lin96] for learning a mixture of Gaussians on the line, as well as with experimental results of [SSR06]. When \u00b5 > 1, our variant of 2-means makes progress in each round, when the number of samples is at least \u2126\u0303( d\n\u00b52 ).\nThen, in Section 5, we provide an information-theoretic lower bound on the sample requirement of any algorithm for learning a mixture of two spherical Gaussians with standard deviation 1 and equal weight. We show that when the separation \u00b5 > 1, any algorithm requires \u2126( d\u00b52 ) samples to converge to a vector within angle \u03b8 = cos\u22121(c) of the true solution, where c is a constant. This indicates that k-means has near-optimal sample requirement when \u00b5 > 1.\nFinally, in Section 6, we examine the performance of 2-means when the input comes from a mixture of k spherical Gaussians. We show that, in this case, the normal to the hyperplane separating the two clusters converges to a vector in the subspace containing the means of the mixture components. Again, we characterize exactly the rate of convergence, which looks very similar to the bounds in Section 3.\nRelated Work. The convergence-time of the k-means algorithm has been analyzed in the worstcase [AV06, Vat09], and the smoothed analysis settings [MR09, AMR09]; [Vat09] shows that the convergence-time of k-means may be \u2126(2n) even in the plane. [AMR09] establishes a O(n30) smoothed complexity bound. [ORSS06] analyzes the performance of k-means when the data obeys a clusterability condition; however, their clusterability condition is very different, and moreover, they examine conditions under which constant-factor approximations can be found. In statistics literature, the k-means algorithm has been shown to be consistent [Mac67]. [Pol81] shows that minimizing the k-means objective function (namely, the sum of the squares of the distances between each point and the center it is assigned to), is consistent, given sufficiently many samples. As optimizing the k-means objective is NP-Hard, one cannot hope to always get an exact solution. None of these two works quantify either the convergence rate or the exact sample requirement of k-means.\nThere has been two lines of previous work on theoretical analysis of the EM algorithm [DLR77], which is closely related to k-means. Essentially, for learning mixtures of identical Gaussians, the only difference between EM and k-means is that EM uses partial assignments or soft clusterings,\nwhereas k-means does not. First, [RW84, XJ96] views learning mixtures as an optimization problem, and EM as an optimization procedure over the likelihood surface. They analyze the structure of the likelihood surface around the optimum to conclude that EM has first-order convergence. An optimization procedure on a parameter m is said to have first-order convergence, if,\n||mt+1 \u2212m\u2217|| \u2264 R \u00b7 ||mt \u2212m\u2217||\nwhere mt is the estimate of m at time step t using n samples, m \u2217 is the maximum likelihood estimator for m using n samples, and R is some fixed constant between 0 and 1. In contrast, our analysis also applies when one is far from the optimum.\nThe second line of work is a probabilistic analysis of EM due to [DS00]; they show a tworound variant of EM which converges to the correct partitioning of the samples, when the input is generated by a mixture of k well-separated, spherical Gaussians. For their analysis to work, they require the mixture components to be separated such that two samples from the same Gaussian are a little closer in space than two samples from different Gaussians. In contrast, our analysis applies when the separation is much smaller.\nThe sample requirement of learning mixtures has been previously studied in the literature, but not in the context of k-means. [CHRZ07, Cha07] provides an algorithm that learns a mixture of two binary product distributions with uniform weights, when the separation \u00b5 between the mixture components is at least a constant, so long as \u2126\u0303( d\n\u00b54 ) samples are available. (Notice that for such\ndistributions, the directional standard deviation is at most 1.) Their algorithm is similar to kmeans in some respects, but different in that they use different sets of coordinates in each round, and this is very crucial in their analysis. Additionally, [BCOFZ07] show a spectral algorithm which learns a mixture of k binary product distributions, when the distributions have small overlap in probability mass, and the sample size is at least \u2126\u0303(d/\u00b52). [Lin96] shows that at least \u2126\u0303( 1\n\u00b54 ) samples\nare required to learn a mixture of two Gaussians in one dimension. We note that although our lower bound of \u2126(d/\u00b52) for \u00b5 > 1 seems to contradict the upper bound of [CHRZ07, Cha07], this is not actually the case. Our lower bound characterizes the number of samples required to find a vector at an angle \u03b8 = cos\u22121(1/10) with the vector joining the means. However, in order to classify a constant fraction of the points correctly, we only need to find a vector at an angle \u03b8\u2032 = cos\u22121(1/\u00b5) with the vector joining the means. Since the goal of [CHRZ07] is to simply classify a constant fraction of the samples, their upper bound is less than O(d/\u00b52).\nIn addition to theoretical analysis, there has been very interesting experimental work due to [SSR06], which studies the sample requirement for EM on a mixture of k spherical Gaussians. They conjecture that the problem of learning mixtures has three phases, depending on the number of samples : with less than about d\n\u00b54 samples, learning mixtures is information-theoretically hard;\nwith more than about d \u00b52\nsamples, it is computationally easy, and in between, computationally hard, but easy in an information-theoretic sense. Finally, there has been a line of work which provides algorithms (different from EM or k-means) that are guaranteed to learn mixtures of Gaussians under certain separation conditions \u2013 see, for example, [Das99, VW02, AK05, AM05, KSV05, CR08, BV08]. For mixtures of two Gaussians, our result is comparable to the best results for spherical Gaussians [VW02] in terms of separation requirement, and we have a smaller sample requirement."}, {"heading": "2 The Setting", "text": "The k-means algorithm iteratively refines a partitioning of the input data. At each iteration, k points are maintained as centers; each input is assigned to its closest center. The center of each\ncluster is then recomputed as the empirical mean of the points assigned to the cluster. This procedure is continued until convergence.\nOur variant of k-means is described below. There are two main differences between the actual 2-means algorithm, and our variant. First, we use a separate set of samples in each iteration. Secondly, we always fix the cluster boundary to be a hyperplane through the origin. When the input is a very large number of samples from a mixture of two identical Gaussians with equal mixing weights, and with center of mass at the origin, this is exactly 2-means initialized with symmetric centers (with respect to the origin). We analyze this symmetrized version of 2-means even when the mixing weights and the variances of the Gaussians in the mixture are not equal.\nThe input to our algorithm is a set of samples S, a number of iterations N , and a starting vector u\u03060, and the output is a vector uN obtained after N iterations of the 2-means algorithm. 2-means-iterate(S, N , u0) 1. Partition S randomly into sets of equal size S1, . . . ,SN .\n2. For iteration t = 0, . . . , N \u2212 1, compute: Ct+1 = {x \u2208 St+1|\u3008x, ut\u3009 > 0} C\u0304t+1 = {x \u2208 St+1|\u3008x, ut\u3009 < 0}\nCompute: ut+1 as the empirical average of Ct+1.\nNotation. In Sections 3 and 4, we analyze Algorithm 2-means-iterate, when the input is generated by a mixture D = {D1,D2} of two Gaussians. We let D1 = N(\u00b51, (\u03c31)2Id), D2 = N(\u00b52, (\u03c32)2Id), with mixing weights \u03c11 and \u03c12. We also assume without loss of generality that for all j, \u03c3j \u2265 1. As the center of mass of the mixture lies at the origin, \u03c11\u00b51 + \u03c12\u00b52 = 0. In Section 6, we study a somewhat more general case.\nWe define b as the unit vector along \u00b51, i.e. b = \u00b5 1\n||\u00b51|| .Henceforth, for any vector v, we use the notation v\u0306 to denote the unit vector along v, i.e. v\u0306 = v||v|| . Therefore, u\u0306t is the unit vector along ut. We assume without loss of generality that \u00b5 1 lies in the cluster Ct+1. In addition, for each t, we define \u03b8t as the angle between \u00b5 1 and ut.We use the cosine of \u03b8t as a measure of progress of the algorithm at round t, and our goal is to show that this quantity increases as t increases. Observe that 0 \u2264 cos(\u03b8t) \u2264 1, and cos(\u03b8t) = 1 when ut and \u00b51 are aligned along the same direction. For each t, we define \u03c4 jt = \u3008\u00b5j, u\u0306t\u3009 = \u3008\u00b5j , b\u3009 cos(\u03b8t). Moreover, from our notation, cos(\u03b8t) = \u03c41t ||\u00b51|| . In addition, we define \u03c1min = minj \u03c1 j, \u00b5min = minj ||\u00b5j ||, and \u03c3max = maxj \u03c3j . For the special case of two identical spherical Gaussians with equal weights, we use \u00b5 = ||\u00b51|| = ||\u00b52||. Finally, for a \u2264 b, we use the notation \u03a6(a, b) to denote the probability that a standard normal variable takes values between a and b."}, {"heading": "3 Exact Estimation", "text": "In this section, we examine the performance of Algorithm 2-means-iterate when one can estimate the vectors ut exactly \u2013 that is, when a very large number of samples from the mixture is available. Our main result of this section is Lemma 1, which exactly characterizes the behavior of 2-meansiterate at a specific iteration t.\nFor any t, we define the quantities \u03bet and mt as follows:\n\u03bet = \u2211\nj\n\u03c1j\u03c3j e\u2212(\u03c4 j t ) 2/2(\u03c3j )2\n\u221a 2\u03c0\n, mt = \u2211 j \u03c1 j\u3008\u00b5j , b\u3009 \u00b7 \u03a6(\u2212 \u03c4\nj t\n\u03c3j ,\u221e)\nunit vector along \u00b51 \u2212\u3008\u00b51, u\u0306t\u3009u\u0306t. Therefore, we have \u03c41t = ||\u00b51|| cos(\u03b8t) and \u221a ||\u00b51||2 \u2212 (\u03c41 t )2 = ||\u00b51|| sin(\u03b8t).\nNow, our main lemma can be stated as follows.\nLemma 1.\ncos2(\u03b8t+1) = cos 2(\u03b8t)\n(\n1 + tan2(\u03b8t) 2 cos(\u03b8t)\u03betmt +m\n2 t\n\u03be2t + 2cos(\u03b8t)\u03betmt +m 2 t\n)\nThe proof is in the Appendix. Using Lemma 1, we can characterize the convergence rates and times of 2-means-iterate for different values of \u00b5j, \u03c1j and \u03c3j , as well as different initializations of u0.\nThe convergence rates can be characterized in terms of two natural parameters of the problem,\nM = \u2211 j \u03c1j ||\u00b5j ||2 \u03c3j , which measures how much the distributions are separated, and V = \u2211 j \u03c1 j\u03c3j , which measures the average standard deviations of the distributions. We observe that as \u03c3j \u2265 1, for all j, V \u2265 1 always. To characterize these rates, it is also convenient to look at two different cases, according to the value of \u00b5j , the separation between the mixture components. Small \u00b5j. First, we consider the case when each ||\u00b5j ||/\u03c3j is less than a fixed constant \u221a\nln 92\u03c0 ,\nincluding the case when ||\u00b5j|| can be much less than 1. In this case, the Gaussians are not even separated in terms of probability mass; in fact, as ||\u00b5j ||/\u03c3j decreases, the overlap in probability mass between the Gaussians tends to 1. However, we show that 2-means-iterate can still do something interesting, in terms of recovering the subspace containing the means of the distributions. Theorem 2 summarizes the convergence rate in this case.\nTheorem 2 (Small \u00b5j). Let ||\u00b5j ||/\u03c3j < \u221a\nln 92\u03c0 , for j = 1, 2. Then, there exist fixed constants\na1 and a2, such that:\ncos2(\u03b8t)(1 + a1(M/V ) sin 2(\u03b8t)) \u2264 cos2(\u03b8t+1) \u2264 cos2(\u03b8t)(1 + a2(M/V ) sin2(\u03b8t))\nFor a mixture of two identical Gaussians with equal mixing weights, we can conclude:\nCorollary 3. For a mixture of two identical spherical Gaussians with equal mixing weights, standard deviation 1, if \u00b5 = ||\u00b51|| = ||\u00b52|| < \u221a\nln 92\u03c0 , then,\ncos2(\u03b8t)(1 + a \u2032 1\u00b5 2 sin2(\u03b8t)) \u2264 cos2(\u03b8t+1) \u2264 cos2(\u03b8t)(1 + a\u20322\u00b52 sin2(\u03b8t))\nThe proof follows by a combination of Lemma 1, and Lemma 25. From Corollary 3, we observe that cos2(\u03b8t) grows by a factor of (1 + \u0398(\u00b5 2)) in each iteration, except when \u03b8t is very close to 0.\nThis means that when 2-means-iterate is far from the actual solution, it approaches the solution at a consistently high rate. The convergence rate only grows slower, once k-means is very close to the actual solution. Large \u00b5j. In this case, there exists a j such that ||\u00b5j ||/\u03c3j \u2265 \u221a\nln 92\u03c0 . In this regime, the Gaussians\nhave small overlap in probability mass, yet, the distance between two samples from the same distribution is much greater than the separation between the distributions. Our guarantees for this case are summarized by Theorem 4.\nWe see from Theorem 4 that there are two regimes of behavior of the convergence rate, depending on the value of maxj |\u03c4 jt |/\u03c3j . These regimes have a natural interpretation. The first regime corresponds to the case when \u03b8t is large enough, such that when projected onto ut, at most a constant fraction of samples from the two distributions can be classified with high confidence. The second regime corresponds to the case when \u03b8t is close enough to 0 such that when projected along ut, most of the samples from the distributions can be classified with high confidence. As expected, in the second regime, the convergence rate is much slower than in the first regime.\nTheorem 4 (Large \u00b5j). Suppose there exists j such that ||\u00b5j ||/\u03c3j \u2265 \u221a\nln 92\u03c0 . If |\u03c4 j t |/\u03c3j <\n\u221a\nln 92\u03c0 , for all j, then, there exist fixed constants a3, a4, a5 and a6 such that:\ncos2(\u03b8t)\n(\n1 + a3(M/V )\n2 sin2(\u03b8t)\na4 + (M/V )2 cos2(\u03b8t)\n) \u2264 cos2(\u03b8t+1) \u2264 cos2(\u03b8t) ( 1 + a5((M/V ) + (M/V ) 2) sin2(\u03b8t)\na6 + (M/V )2 cos2(\u03b8t)\n)\nOn the other hand, if there exists j such that |\u03c4 jt |/\u03c3j \u2265 \u221a\nln 92\u03c0 , then, there exist fixed constants a7 and a8 such that:\ncos2(\u03b8t)(1 + a7\u03c1\n2 min\u00b5 2 min\na8V 2 + \u03c1 2 min\u00b5 2 min\ntan2(\u03b8t)) \u2264 cos2(\u03b8t+1) \u2264 cos2(\u03b8t)(1 + tan2(\u03b8t))\nFor two identical Gaussians with standard deviation 1, we can conclude:\nCorollary 5. For a mixture of two identical Gaussians with equal mixing weights, and standard deviation 1, if \u00b5 = ||\u00b51|| = ||\u00b52|| > \u221a\nln 92\u03c0 , and if |\u03c41t | = |\u03c42t | \u2264 \u221a ln 92\u03c0 , then, there exist fixed\nconstants a\u20323, a \u2032 4, a \u2032 5, a \u2032 6 such that:\ncos2(\u03b8t)\n( 1 + a\u20323\u00b5 4 sin2(\u03b8t)\na\u20324 + \u00b5 4 cos2(\u03b8t)\n) \u2264 cos2(\u03b8t+1) \u2264 cos2(\u03b8t) ( 1 + a\u20325\u00b5 4 sin2(\u03b8t)\na6 + \u00b54 cos2(\u03b8t)\n)\nOn the other hand, if |\u03c41t | = |\u03c42t | \u2265 \u221a ln 92\u03c0 , then, there exists a fixed constant a \u2032 7 such that:\ncos2(\u03b8t)(1 + a \u2032 7 tan 2(\u03b8t)) \u2264 cos2(\u03b8t+1) \u2264 cos2(\u03b8t)(1 + tan2(\u03b8t))\nIn this case as well, we observe the same phenomenon: the convergence rate is high when we are far away from the solution, and slow when we are close. Using Theorems 2 and 4, we can characterize the convergence times of 2-means-iterate; for the sake of simplicity, we present the convergence time bounds for a mixture of two spherical Gaussians with equal mixing weights and standard deviation 1. We recall that in this case 2-means-iterate is exactly 2-means.\nCorollary 6 (Convergence Time). If \u03b80 is the initial angle between \u00b5 1 and u0, then, cos 2(\u03b8N ) \u2265 1\u2212 \u01eb after N = C0 \u00b7 ( ln( 1 cos2(\u03b80) ) ln(1+\u00b52) + 1 ln(1+\u01eb) ) iterations, where C0 is a fixed constant.\nEffect of Initialization. As apparent from Corollary 6, the effect of initialization is only to ensure a lower bound on the value of cos(\u03b80). We illustrate below, two natural ways by which one can select u0, and their effect on the convergence rate. For the sake of simplicity, we state these bounds for the case in which we have two identical Gaussians with equal mixing weights and standard deviation 1.\n\u2022 First, one can choose u0 uniformly at random from the surface of a unit sphere in Rd; in this case, cos2(\u03b80) = \u0398( 1 d ), with constant probability, and as a result, the convergence time\nto reach cos\u22121(1/ \u221a 2) is O( ln d\nln(1+\u00b52) ).\n\u2022 A second way to choose u0 is to set it to be a random sample from the mixture; in this case, cos2(\u03b80) = \u0398( (1+\u00b5)2 d ) with constant probability, and the time to reach cos \u22121(1/ \u221a 2) is\nO( lnd ln(1+\u00b52) )."}, {"heading": "4 Finite Samples", "text": "In this section, we analyze Algorithm 2-means-iterate, when we are required to estimate the statistics at each round with a finite number of samples. We characterize the number of samples needed to ensure that 2-means-iterate makes progress in each round, and we also characterize the rate of progress when the required number of samples are available.\nThe main result of this section is the following lemma, which characterizes \u03b8t+1, the angle between \u00b51 and the hyperplane separator in 2-means-iterate, given \u03b8t. Notice that now \u03b8t is a random variable, which depends on the samples drawn in rounds 1, . . . , t\u2212 1, and given \u03b8t, \u03b8t+1 is a random variable, whose value depends on samples in round t. Also we use ut+1 as the center of partition Ct in iteration t+ 1, and E[ut+1] is the expected center. Note that all the expectations in round t are conditioned on \u03b8t. In addition, we use St+1 to denote the quantity E[X \u00b7 1X\u2208Ct+1 ], where 1X\u2208Ct+1 is the indicator function for the event X \u2208 Ct+1, and the expectation is taken over the entire mixture. Note that, St+1 = E[ut+1] Pr[X \u2208 Ct+1] = Zt+1E[ut+1]. We use S\u0302t+1 to denote the empirical value of St+1.\nLemma 7. If we use n samples in iteration t, then, given \u03b8t, with probability 1\u2212 2\u03b4,\ncos2(\u03b8t+1) \u2265 cos2(\u03b8t) ( 1 + tan2(\u03b8t) 2 cos(\u03b8t)\u03betmt+m2t\n\u03be2t+2 cos(\u03b8t)\u03betmt+m 2 t+\u22062\n) \u2212 ( \u22062 cos2(\u03b8t)+2\u22061(mt+\u03bet cos(\u03b8t)) m2t+\u03be 2 t+2\u03betmt cos(\u03b8t)+\u22062 )\nwhere,\n\u22061 = 8 log(4n/\u03b4)(\u03c3max +maxj ||\u00b5j ||)\u221a\nn\n\u22062 = 128 log2(8n/\u03b4)(\u03c32maxd+\n\u2211\nj ||\u00b5j ||2) n + 8 log(n/\u03b4)\u221a n (\u03c3max||St+1||+max j |\u3008St+1, \u00b5j\u3009|)\nThe main idea behind the proof of Lemma 7 is that we can write cos2(\u03b8t+1) = \u3008S\u0302t+1,\u00b51\u30092\n||\u00b51||2||S\u0302t+1||2 .\nNext, we can use Lemma 1, and the definition of St+1 to get an expression for \u3008St+1,\u00b51\u30092\n||St+1||2||\u00b51||2 , and\nLemmas 8 and 9 to bound \u3008S\u0302t+1 \u2212 St+1, \u00b51\u3009, and ||S\u0302t+1||2 \u2212 ||St+1||2. Plugging in all these values gives us a proof of Lemma 7. We also assume for the rest of the section that the number of samples n is at most some polynomial in d, such that log(n) = \u0398(log(d)).\nThe two main lemmas used in the proof of Lemma 7 are Lemmas 8 and 9. To state them, we need to define some notation. At time t, we use the notation\nLemma 8. For any t, and for any vector v with norm ||v||, with probability at least 1\u2212 \u03b4,\n|\u3008S\u0302t+1 \u2212 St+1, v\u3009| \u2264 8 log(4n/\u03b4)(\u03c3max||v|| +maxj |\u3008\u00b5j , v\u3009|)\u221a\nn\nLemma 9. For any t, with probability at least 1\u2212 \u03b4,\n||S\u0302t+1||2 \u2264 ||St+1||2+ 128 log2(8n/\u03b4)(\u03c32maxd+\n\u2211\nj(\u00b5 j)2)\nn + 16 log(8n/\u03b4)\u221a n (\u03c3max||St+1||+max j |\u3008St+1, \u00b5j\u3009|)\nThe proofs of Lemmas 8 and 9 are in the Appendix. Applying Lemma 7, we can characterize the number of samples required such that 2-means-iterate makes progress in each round for different values of ||\u00b5j||. Again, it is convenient to look at two separate cases, based on ||\u00b5j ||.\nTheorem 10 (Small \u00b5j). Let ||\u00b5j||/\u03c3j < \u221a\nln 92\u03c0 , for all j. If the number of samples drawn in\nround t is at least a9\u03c3 2 max log\n2(d/\u03b4) (\nd MV sin4(\u03b8t) + 1 M2 sin4(\u03b8t) cos2(\u03b8t)\n)\n, for some fixed constant a9,\nthen, with probability at least 1 \u2212 \u03b4, cos2(\u03b8t+1) \u2265 cos2(\u03b8t)(1 + a10(M/V ) sin2(\u03b8t)), where a10 is some fixed constant.\nIn particular, for the case of two identical Gaussians with equal mixing weights and standard deviation 1, our results implies the following.\nCorollary 11. Let \u00b5 = ||\u00b51|| = ||\u00b52|| < \u221a\nln 92\u03c0 . If the number of samples drawn in round t is at\nleast a9 log 2(d/\u03b4)\n(\nd \u00b52 sin4(\u03b8t) + 1 \u00b54 cos2(\u03b8t) sin4(\u03b8t)\n)\n, for some fixed constant a9, then, with probability\nat least 1\u2212 \u03b4, cos2(\u03b8t+1) \u2265 cos2(\u03b8t)(1 + a10\u00b52 sin2(\u03b8t)), where a10 is some fixed constant.\nIn particular, when we initialize u0 with a vector picked uniformly at random from a ddimensional sphere, cos2(\u03b80) \u2265 1d , with constant probability, and thus the number of samples required for success in the first round is \u0398\u0303( d\n\u00b54 ). This bound matches with the lower bounds for\nlearning mixtures of Gaussians in one dimension [Lin96], as well as with conjectured lower bounds in experimental work [SSR06]. The following corollary summarizes the total number of samples required to learn the mixture with some fixed precision, for two identical spherical Gaussians with variance 1 and equal mixing weights.\nCorollary 12. Let \u00b5 = ||\u00b51|| = ||\u00b52|| \u2264 \u221a\nln 92\u03c0 . Suppose u0 is chosen uniformly at random, and\nthe number of rounds is N \u2265 C0 \u00b7 ( ln dln(1+\u00b52) + 1ln(1+\u01eb)), where C0 is the fixed constant in Corollary 6. If the number of samples |S| is at least: N \u00b7a9d log\n2(d) \u00b54\u01eb2 , then, with constant probability, after N\nrounds, cos2(\u03b8N ) \u2265 1\u2212 \u01eb.\nOne can show a very similar corollary when u0 is initialized as a random sample from the mixture. We note that the total number of samples is a factor of N \u2245 ln d\n\u00b52 times greater than the\nbound in Theorem 10. This is due to the fact that we use a fresh set of samples in every round, in order to simplify our analysis. In practice, successive iterations of k-means or EM is run on the same data-set. Theorem 13 (Large \u00b5j). Suppose that there exists some j such that ||\u00b5j ||/\u03c3j \u2265 \u221a\nln 92\u03c0 , and\nsuppose that the number of samples drawn in round t is at least\na11 log 2(d/\u03b4)\n(\nd\u03c32max \u03c12min\u00b5 2 min sin 4(\u03b8t) + \u03c32max +maxj ||\u00b5j ||2 M2 cos2(\u03b8t) sin 4(\u03b8t) + \u03c32max maxj ||\u00b5j||2 +maxj ||\u00b5j ||4 \u03c14min\u00b5 4 min sin 4(\u03b8t) )\nfor some constant a11. If |\u03c4 jt | \u2264 \u221a\nln 92\u03c0 , for all j, then, with probability at least 1\u2212 \u03b4, cos2(\u03b8t+1) \u2265 cos2(\u03b8t)(1 + a12 min(1,M\n2 +MV ) sin2(\u03b8t)); otherwise, with probability at least 1\u2212 \u03b4, cos2(\u03b8t+1) \u2265 cos2(\u03b8t)(1 + a13 \u03c12min\u00b5 2 min tan 2(\u03b8t)\nV 2+\u03c12min\u00b5 2 min\n), where a12 and a13 are fixed constants.\nFor a mixture of two identical Gaussians with equal mixing weights and standard deviation 1, our result implies:\nCorollary 14. Suppose that \u00b5 = ||\u00b51|| = ||\u00b52|| \u2265 \u221a\nln 92\u03c0 , and suppose that the number of samples\nin round t is at least: a11 log 2(d/\u03b4)\n(\nd \u00b52 sin4(\u03b8t) + 1 \u00b52 cos2(\u03b8t) sin4(\u03b8t)\n)\n, for some constant a11. If |\u03c4 jt | \u2264 \u221a\nln 92\u03c0 , then, with probability at least 1\u2212 \u03b4, cos2(\u03b8t+1) \u2265 cos2(\u03b8t)(1 + a12 sin2(\u03b8t)); otherwise, with probability 1\u2212 \u03b4, cos2(\u03b8t+1) \u2265 cos2(\u03b8t)(1 + a13 tan2(\u03b8t)), where a12 and a13 are fixed constants.\nAgain, if we pick u0 uniformly at random, we require about \u2126\u0303( d\n\u00b52 ) samples for the first round\nto succeed. When \u00b5 > 1, this bound is worse than d \u00b54 , but matches with the upper bounds of [BCOFZ07]. The following corollary shows the number of samples required in total for 2-meansiterate to converge.\nCorollary 15. Let \u00b5 \u2265 \u221a\nln 92\u03c0 . Suppose u0 is chosen uniformly at random and the number of\nrounds is N \u2265 C0 \u00b7 (ln d + 1ln(1+\u01eb)), where C0 is the constant in Corollary 6. If |S| is at least 2NC0d log\n2(d) \u00b52\u01eb2 , then, with constant probability, after N rounds, cos2(\u03b8N ) \u2265 1\u2212 \u01eb."}, {"heading": "5 Lower Bounds", "text": "In this section, we prove a lower bound on the sample complexity of learning mixtures of Gaussians, using Fano\u2019s Inequality [Yu97, CT05], stated in Theorem 19. Our main theorem in this section can be summarized as follows.\nTheorem 16. Suppose we are given samples from the mixture D(\u00b5) = 12N (\u00b5, Id) + 12N (\u2212\u00b5, Id), for some \u00b5, and let \u00b5\u0302 be the estimate of \u00b5 computed from n samples. If n < Cd||\u00b5||2 for some constant C, and ||\u00b5|| > 1, then, there exists \u00b5 such that ED(\u00b5)||\u00b5\u2212 \u00b5\u0302|| \u2265 C \u2032||\u00b5||, where C \u2032 is a constant.\nThe main tools in the proof of Theorem 16 are the following lemmas, and a generalized version of Fano\u2019s Inequality [CT05, Yu97].\nLemma 17. Let \u00b51, \u00b52 \u2208 Rd, and let D1 and D2 be the following mixture distributions: D1 = 1 2N (\u00b51, Id) + 12N (\u2212\u00b51, Id), and D2 = 12N (\u00b52, Id) + 12N (\u2212\u00b52, Id). Then,\nKL(D1,D2) \u2264 1\u221a 2\u03c0\n\u00b7 ( ||\u00b52||2 \u2212 ||\u00b51||2 + 3 \u221a 2\u03c0\n2 ln 2 + 2||\u00b51||(e\u2212||\u00b51||\n2/2 + \u221a 2\u03c0||\u00b51||\u03a6(0, ||\u00b51||))\n)\nLemma 18. There exists a set of vectors V = {v1, . . . , vK} in Rd with the following properties: (1) For each i and j, d(vi, vj) \u2265 15 , d(vi,\u2212vj) \u2265 15 . (2) K = ed/10. (3) For all i, ||vi|| \u2264 \u221a 7 5 .\nTheorem 19 (Fano\u2019s Inequality). Consider a class of densities F , which contains r densities f1, . . . , fr, corresponding to parameter values \u03b81, . . . , \u03b8r. Let d(\u00b7) be any metric on \u03b8, and let \u03b8\u0302 be an estimate of \u03b8 from n samples from a density f in F . If, for all i and j, d(\u03b8i, \u03b8j) \u2265 \u03b1, and KL(fi, fj) \u2264 \u03b2, then, maxj Ejd(\u03b8\u0302, \u03b8j) \u2265 \u03b12 (1 \u2212 n\u03b2+log 2 log(r\u22121) ), where Ej denotes the expectation with respect to distribution j.\nProof. (Of Theorem 16) We apply Fano\u2019s Inequality. Our class of densities F is the class of all mixtures of the form 12N (\u00b5\u2032, Id)+ 12N (\u2212\u00b5\u2032, Id). We set the parameter \u03b8 = \u00b5\u2032, and d(\u00b51, \u00b52) = ||\u00b51\u2212 \u00b52||. We construct a subclass F = {f1, . . . , fr} of F as follows. We set each fi = 12N (||\u00b5||vi, Id) + 1 2N (\u2212||\u00b5||vi, Id), for each vector vi in V in Lemma 18. Notice that now r = ed/10. Moreover, for each pair i and j, from Lemma 17 and Lemma 18, KL(fi, fj) \u2264 C1||\u00b5||2 + C2, for constants C1 and C2. Finally, from Lemma 18, for each pair i and j, d(\u00b5i, \u00b5j) \u2265 ||\u00b5||5 . The Theorem now follows by an application of Fano\u2019s Inequality 19.\n6 More General k-means\nIn this section, we show that when we apply 2-means on an input generated by a mixture of k spherical Gaussians, the normal to the hyperplane which partitions the two clusters in the 2-means algorithm, converges to a vector in the subspace M containing the means of mixture components. We assume that our input is generated by a mixture of k spherical Gaussians, with means \u00b5j, variances (\u03c3j)2, j = 1, . . . , k, and mixing weights \u03c11, . . . , \u03c1k. The mixture is centered at the origin such that \u2211\n\u03c1j\u00b5j = 0. We use M to denote the subspace containing the means \u00b51, . . . , \u00b5k. We use Algorithm 2-means-iterate on this input, and our goal is to show that it still converges to a vector in M. Notation. In the sequel, given a vector x and a subspace W , we define the angle between x and W as the angle between x and the projection of x onto W . We examine the angle \u03b8t, between ut and M, and our goal is to show that the cosine of this angle grows as t increases. Our main result of this section is Lemma 20, which exactly defines the behavior of 2-means-iterate on a mixture of k spherical Gaussians. Recall that at time t, we use u\u0306t to partition the input data, and the projection of u\u0306t along M is cos(\u03b8t) by definition. Let b1t be a unit vector lying in the subspace M such that: u\u0306t = cos(\u03b8t)b 1 t + sin(\u03b8t)vt, where vt lies in the orthogonal complement of M, and has norm 1. We define a second vector u\u0306\u22a5t as follows: u\u0306 \u22a5 t = sin(\u03b8t)b 1 t \u2212 cos(\u03b8t)vt. We observe that \u3008u\u0306t, u\u0306\u22a5t \u3009 = 0, ||u\u0306\u22a5t || = 1, and the projection of u\u0306\u22a5t on M is sin(\u03b8t)b1t .We now extend the set {b1t } to complete an orthonormal basis B = {b1t , . . . , bk\u22121t } of M. We also observe that {b2t , . . . , bk\u22121t , u\u0306t, u\u0306\u22a5t } is an orthonormal basis of the subspace spanned by any basis of M, along with vt, and can be extended to a basis of Rd.\nFor j = 1, . . . , k, we define \u03c4 jt as follows: \u03c4 t j = \u3008\u00b5j , u\u0306t\u3009 = cos(\u03b8t)\u3008\u00b5j , b1t \u3009. Finally we (re)-define\nthe quantity \u03bet, and define m l t, for l = 1, . . . , k \u2212 1 as\n\u03bet = \u2211\nj\n\u03c1j\u03c3j e\u2212(\u03c4 j t ) 2/2(\u03c3j )2\n\u221a 2\u03c0\n, mlt = \u2211\nj\n\u03c1j\u03a6(\u2212 \u03c4 j t\n\u03c3j ,\u221e)\u3008\u00b5j , blt\u3009\nOur main lemma is stated below. The proof is in the Appendix.\nLemma 20. At any iteration t of Algorithm 2-means-iterate,\ncos2(\u03b8t+1) = cos 2(\u03b8t)\n(\n1 + tan2(\u03b8t) 2 cos(\u03b8t)\u03betm\n1 t +\n\u2211\nl(m l t) 2\n\u03be2t + 2cos(\u03b8t)\u03betm 1 t +\n\u2211\nl(m l t) 2\n)"}, {"heading": "6.1 Proof of Lemma 1", "text": "In this section, we prove Lemma 1. First, we need some additional notation.\nNotation. We define, for j = 1, 2:\nwjt+1 = Pr[x \u223c Dj|x \u2208 Ct+1] ujt+1 = E[x|x \u223c Dj , x \u2208 Ct+1]\nWe observe that ut+1 now can be written as:\nut+1 = w 1 t+1u 1 t+1 + w 2 t+1u 2 t+1\nMoreover, we define Zt+1 = Pr[x \u2208 Ct+1]. Proof of Lemma 1. We start by providing exact expressions for w1t+1 and w 2 t+1 with respect to the partition computed in the previous round t. These are used to compute the projections of ut+1 along the vectors u\u0306t and \u00b51 \u2212 \u3008\u00b51, u\u0306t\u3009u\u0306t, which finally leads to a proof of Lemma 1.\nLemma 21. In round t, for j = 1, 2, wjt+1 = \u03c1j\u03a6(\u2212 \u03c4\nj t\n\u03c3j ,\u221e)\nZt+1 .\nProof. We can write:\nwjt+1 = Pr[x \u2208 Ct+1|x \u223c Dj] Pr[x \u223c Dj ]\nPr[x \u2208 Ct+1] We note that Pr[x \u223c Dj ] = \u03c1j , and Pr[x \u2208 Ct+1] = Zt+1.\nAs Dj is a spherical Gaussian, for any x generated from Dj , and for any vector y orthogonal to ut, \u3008y, x\u3009 is distributed independently from \u3008u\u0306t, x\u3009. Moreover, we observe that \u3008u\u0306t, x\u3009 is distributed as a Gaussian with mean \u3008\u00b5j, u\u0306t\u3009 = \u03c4 jt and standard deviation \u03c3j. Therefore,\nPr[x \u2208 Ct+1|x \u223c Dj ] = Pr x\u223cDj [\u3008u\u0306t, x\u3009 > 0] = Pr[N(\u03c4 jt , \u03c3j) \u2265 0] = \u03a6(\u2212 \u03c4 jt \u03c3j ,\u221e)\nfrom which the lemma follows.\nLemma 22. For any t, \u3008ut+1, u\u0306t\u3009 = \u03bet+mt cos(\u03b8t)Zt+1 .\nProof. Consider a sample x drawn from Dj . Then, \u3008x, u\u0306t\u3009 is distributed as a Gaussian with mean \u3008\u00b5j , u\u0306t\u3009 = \u03c4 jt and standard deviation \u03c3j . We recall that Pr[x \u2208 Ct+1] = Zt+1. Therefore, \u3008ujt+1, u\u0306t\u3009 is equal to:\nE[x, x \u2208 Ct+1|x \u223c Dj] Pr[x \u2208 Ct+1|x \u223c Dj ] = 1\nPr[N(\u03c4 jt , \u03c3 j) > 0]\n\u00b7 \u222b \u221e\ny=0\nye\u2212(y\u2212\u03c4 j t ) 2/2(\u03c3j )2\n\u03c3j \u221a 2\u03c0\ndy\nwhich is, again, equal to:\n1\n\u03a6(\u2212 \u03c4 j t\n\u03c3j ,\u221e)\n(\n\u03c4 jt\n\u222b \u221e\ny=0\ne\u2212(y\u2212\u03c4 j t ) 2/2(\u03c3j )2\n\u03c3j \u221a 2\u03c0\ndy +\n\u222b \u221e\ny=0\n(y \u2212 \u03c4 jt )e\u2212(y\u2212\u03c4 j t ) 2/2(\u03c3j )2\n\u03c3j \u221a 2\u03c0\ndy\n)\n= 1\n\u03a6(\u2212 \u03c4 j t\n\u03c3j ,\u221e)\n(\n\u03c4 jt \u03a6(\u2212 \u03c4 jt \u03c3j ,\u221e) + \u222b \u221e\ny=0\n(y \u2212 \u03c4 jt )e\u2212(y\u2212\u03c4 j t ) 2/2(\u03c3j )2\n\u03c3j \u221a 2\u03c0\ndy\n)\nWe can compute the integral in the equation above as follows.\n\u222b \u221e\ny=0 (y \u2212 \u03c4 jt )e\u2212(y\u2212\u03c4 j t )\n2/2(\u03c3j )2dy = (\u03c3j)2 \u222b \u221e\nz=(\u03c4 jt ) 2/2(\u03c3j )2\ne\u2212zdz = (\u03c3j)2e\u2212(\u03c4 j t ) 2/2(\u03c3j )2\nWe can now compute \u3008ut+1, u\u0306t\u3009 as follows.\n\u3008ut+1, u\u0306t\u3009 = w1t+1\u3008u1t+1, u\u0306t\u3009+ w2t+1\u3008u2t+1, u\u0306t\u3009 = 1 Zt+1 \u00b7 \u2211\nj\n(\n\u03c1j\u03c4 jt \u03a6(\u2212 \u03c4 jt \u03c3j ,\u221e) + \u03c1j(\u03c3j)2 e \u2212(\u03c4 jt )2/2(\u03c3j )2 \u03c3j \u221a 2\u03c0\n)\nThe lemma follows by recalling \u03c4 jt = \u3008\u00b5j, b\u3009 cos(\u03b8t) and plugging in the values of mt and \u03bet.\nLemma 23. Let v\u0306t be a unit vector along \u00b51\u2212\u3008\u00b51, u\u0306t\u3009u\u0306t. Then, \u3008ut+1, v\u0306t\u3009 = mt sin(\u03b8t)Zt+1 . In addition, for any vector z orthogonal to u\u0306t and v\u0306t, \u3008ut+1, z\u3009 = 0.\nProof. We observe that for a sample x drawn from distribution D1 (respectively, D2) and any unit vector v1, orthogonal to u\u0306t, \u3008x, v1\u3009 is distributed as a Gaussian with mean \u3008\u00b51, v1\u3009 (\u3008\u00b52, v1\u3009, respectively) and standard deviation \u03c31 (resp. \u03c32). Therefore, the projection of ut+1 on v\u0306t can be written as:\n\u3008ut+1, v\u0306t\u3009 = \u2211\nj\nwjt+1\u3008\u00b5j , v\u0306t\u3009 = 1\nZt+1\n\u2211\nj\n\u03c1j\u03a6(\u2212 \u03c4 j t\n\u03c3j ,\u221e)\u3008\u00b5j , v\u0306t\u3009\nfrom which the first part of the lemma follows. The second part of the lemma follows from the observation that for any vector z orthogonal to\nu\u0306t and v\u0306t, \u3008\u00b5j , z\u3009 = 0, for j = 1, 2.\nLemma 24. For any t,\n\u3008ut+1, \u00b51\u3009 = ||\u00b51||(\u03bet cos(\u03b8t) +mt)\nZt+1\n||ut+1||2 = \u03be2t +m 2 t + 2\u03betmt cos(\u03b8t)\n(Zt+1)2\nProof. As we have an infinite number of samples, \u03b8t+1 lies on the same plane as \u03b8t. Therefore, we can write \u3008ut+1, \u00b51\u3009 = \u3008ut+1, u\u0306t\u3009\u3008\u00b51, u\u0306t\u3009 + \u3008ut+1, v\u0306t\u3009\u3008\u00b51, v\u0306t\u3009. Moreover, we can write ||ut+1||2 = \u3008ut+1, u\u0306t\u30092+\u3008ut+1, v\u0306t\u30092. Thus, the first two equation follow by using Lemma 22 and 23, and recalling that \u3008\u00b51, u\u0306t\u3009 = \u03c41t = ||\u00b51|| cos(\u03b8t) and \u3008\u00b51, v\u0306t\u3009 = ||\u00b51|| sin(\u03b8t).\nWe are now ready to complete the proof of Lemma 1.\nProof. (Of Lemma 1) By definition of \u03b8t+1, cos 2(\u03b8t+1) = \u3008ut+1,\u00b51\u30092 ||ut+1||2||\u00b51||2 . Therefore,\n||\u00b51||2 cos2(\u03b8t+1) = \u3008ut+1, \u00b51\u30092 ||ut+1||2\n= (\u03c41t ) 2\n( 1 + \u3008ut+1, \u00b51\u30092 \u2212 ||\u00b51||2 cos2(\u03b8t)||ut+1||2 ||\u00b51||2 cos2(\u03b8t)||ut+1||2 )\n= (\u03c41t ) 2\n( 1 + ||\u00b51||2 sin2(\u03b8t)(m2t + 2\u03betmt cos(\u03b8t)) ||\u00b51||2 cos2(\u03b8t)||ut+1||2 )\n= ||\u00b51||2 cos2(\u03b8t) ( 1 + tan2(\u03b8t) m2t + 2\u03betmt cos(\u03b8t) ||ut+1||2 )\nwhere we used Lemma 24 and the observation that cos(\u03b8t) = \u03c41t\n||\u00b51|| . The Lemma follows by replacing\n||ut+1||2 using the expression in Lemma 24.\nThe next Lemma helps us to derive Theorem 2 from Lemma 1. It shows how to approximate \u03a6(\u2212\u03c4, \u03c4) when \u03c4 is small.\nLemma 25. Let \u03c4 \u2264 \u221a\nln 92\u03c0 . Then, 5 3 \u221a 2\u03c0 \u03c4 \u2264 \u03a6(\u2212\u03c4, \u03c4) \u2264 2\u221a 2\u03c0 \u03c4 . In addition, 2e \u2212\u03c42/2\u221a 2\u03c0 \u2265 23 ."}, {"heading": "6.2 Proofs of Sample Requirement Bounds", "text": "For the rest of the section, we prove Lemmas 8 and 9, which lead to a proof of Lemma 7. First, we need to define some notation.\nNotation. At time t, we use the notation St+1 to denote the quantity E[X \u00b7 1X\u2208Ct+1 ], where 1X\u2208Ct+1 is the indicator function for the event X \u2208 Ct+1, and the expectation is taken over the entire mixture.\nIn the sequel, we also use the notation S\u0302t+1 to denote the empirical value of St+1. Our goal is to bound the concentration of certain functions of S\u0302t+1 around their expected values, when we are given only n samples from the mixture. Recall that we define \u03b8t+1 as the angle between \u00b5\n1 and the hyperplane separator in 2-means-iterate, given \u03b8t. Notice that now \u03b8t is a random variable, which depends on the samples drawn in rounds 1, . . . , t\u22121, and given \u03b8t, \u03b8t+1 is a random variable, whose value depends on samples in round t. Also we use ut+1 as the center of partition Ct in iteration t+1, and E[ut+1] is the expected center. Note that all the expectations in round t are conditioned on \u03b8t.\nProofs. We are now ready to prove Lemmas 8 and 9.\nProof. (Of Lemma 8) Let X1, . . . ,Xn be the n iid samples from the mixture; for each i, we can write the projection of Xi along v as follows:\n\u3008Xi, v\u3009 = Yi + Zi\nwhere Zi \u223c N(0, \u03c3j), if Xi is generated from distribution Dj, and Yi = \u3008\u00b5j, v\u3009, if Xi is generated by Dj. Therefore, we can write:\n\u3008S\u0302t+1, v\u3009 = 1\nn\n(\n\u2211\ni\nYi \u00b7 1Xi\u2208Ct+1 + \u2211\ni\nZi \u00b7 1Xi\u2208Ct+1\n)\nTo determine the concentration of \u3008S\u0302t+1, v\u3009 around its expected value, we address the two terms separately.\nThe first term is a sum of n independently distributed random variables, such that changing\none variable changes the sum by at most maxj 2|\u3008\u00b5j ,v\u3009|\nn ; therefore, to calculate its concentration,\none can apply Hoeffding\u2019s Inequality. It follows that with probability at most \u03b42 ,\n| 1 n \u2211\ni\nYi \u00b7 1Xi\u2208Ct+1 \u2212E[ 1\nn\n\u2211\ni\nYi \u00b7 1Xi\u2208Ct+1 ]| > max j\n4|\u3008\u00b5j , v\u3009| \u221a\nlog(4n/\u03b4)\u221a n\nWe note that, in the second term, each Zi is a Gaussian with mean 0 and variance \u03c3 j , scaled\nby ||v||. For some 0 \u2264 \u03b4\u2032 \u2264 1, let Ei(\u03b4\u2032) denote the event\n\u2212\u03c3max||v|| \u221a 2 log(1/\u03b4\u2032) \u2264 Zi \u00b7 1Xi\u2208Ct+1 \u2264 \u03c3max||v|| \u221a 2 log(1/\u03b4\u2032)\nAs Zi \u223c N(0, \u03c3j), if Xi is generated from distribution Dj , and 1Xi\u2208Ct+1 takes values 0 and 1, for any i, for \u03b4\u2032 small enough,Pr[Ei(\u03b4\u2032)] \u2265 1\u2212 \u03b4\u2032.\nWe use \u03b4\u2032 = \u03b44n , and condition on the fact that all the events {Ei(\u03b4\u2032), i = 1, . . . , n} happen; using an Union bound over the events \u00afEi(\u03b4\u2032), the probability that this holds is at least 1\u2212 \u03b44 . We also observe that, as the Gaussians Zi are independently distributed, conditioned on the union of the events Ei, the Gaussians Zi are still independent. Therefore, conditioned on the event \u222aiEi(\u03b4\u2032),\n1 n\n\u2211\ni Zi \u00b7 1Xi\u2208Ct+1 is the sum of n independent random variables, such that changing one variable changes the sum by at most 2\u03c3max||v|| \u221a 2 log(1/\u03b4\u2032)\nn . We can now apply Hoeffding\u2019s bound to conclude\nthat with probability at least 1\u2212 \u03b42 ,\n| 1 n \u2211\ni\nZi\u00b71Xi\u2208Ct+1\u2212E[ 1\nn\n\u2211\ni\nZi\u00b71Xi\u2208Ct+1]| \u2264 4\u03c3max||v||\n\u221a 2 log(1/\u03b4\u2032) \u221a\n2 log(1/\u03b4)\u221a n \u2264 8\u03c3max||v|| log(4n/\u03b4)\u221a n\nThe lemma now follows by applying an union bound.\nProof. (Of Lemma 9) We can write:\n||S\u0302t+1||2 \u2264 ||St+1||2 + ||S\u0302t+1 \u2212 St+1||2 + 2|\u3008S\u0302t+1 \u2212 St+1, St+1\u3009|\nIf v1, . . . , vd is any orthonormal basis of R d, then, we can bound the second term as follows.\nWith probability at least 1\u2212 \u03b42 ,\n||S\u0302t+1 \u2212 St+1||2 = d \u2211\ni=1\n(\u3008S\u0302t+1 \u2212 St+1, vi\u3009)2 \u2264 128 log2(8n/\u03b4)\nn ( \u2211\ni\n\u03c32max||vi||2 + \u2211\ni,j\n\u3008\u00b5j , vi\u30092)\n\u2264 128 log 2(8n/\u03b4)\nn (\u03c32maxd+\n\u2211\nj\n(\u00b5j)2)\nThe second step follows by the application of Lemma 8, and the fact that for any a and b, (a+ b)2 \u2264 2(a2 + b2).\nUsing Lemma 8, with probability at least 1\u2212 \u03b42 ,\n\u3008S\u0302t+1 \u2212 St+1, St+1\u3009 \u2264 8 log(8n/\u03b4)\u221a\nn (\u03c3max||St+1||+max j |\u3008St+1, \u00b5j\u3009|)\nThe lemma follows by a union bound over these two above events."}, {"heading": "6.3 Proofs of Lower Bounds", "text": "Proof. (Of Lemma 17) Let P be the plane containing the origin O and the vectors \u00b51 and \u00b52. If v is a vector orthogonal to P , then, the projection of D1 along v is a Gaussian N (0, 1), which is distributed independently of the projection of D1 along P (and same is the case for D2).Therefore, to compute the KL-Divergence of D1 and D2, it is sufficient to compute the KL-Divergence of the projections of D1 and D2 along the plane P .\nLet x be a vector in P . Then,\nKL(D1,D2) = 1\u221a 2\u03c0\n\u222b\nx\u2208P ( 1 2 e\u2212||x\u2212\u00b51|| 2/2 + 1 2 e\u2212||x+\u00b51|| 2/2) ln\n(\n1 2e \u2212||x\u2212\u00b51||2/2 + 12e \u2212||x+\u00b51||2/2 1 2e \u2212||x\u2212\u00b52||2/2 + 12e \u2212||x+\u00b52||2/2\n)\ndx\n= 1\u221a 2\u03c0\n\u222b\nx\u2208P ( 1 2 e\u2212||x\u2212\u00b51|| 2/2 + 1 2 e\u2212||x+\u00b51|| 2/2) ln\n(\ne\u2212||x+\u00b51|| 2/2 \u00b7 (1 + e2\u3008x,\u00b51\u3009) e\u2212||x+\u00b52||2/2 \u00b7 (1 + e2\u3008x,\u00b52\u3009)\n)\ndx\n= 1\u221a 2\u03c0\n\u222b\nx\u2208P ( 1 2 e\u2212||x\u2212\u00b51|| 2/2 + 1 2 e\u2212||x+\u00b51|| 2/2)\n(\n(||x+ \u00b52||2 \u2212 ||x+ \u00b51||2) + ln 1 + e2\u3008x,\u00b51\u3009\n1 + e2\u3008x,\u00b52\u3009\n)\ndx\nWe observe that for any x, ||x + \u00b52||2 \u2212 ||x + \u00b51||2 = ||\u00b52||2 \u2212 ||\u00b51||2 + 2\u3008x, \u00b52 \u2212 \u00b51\u3009. As the expected value of D1 is 0, we can write that:\n\u222b\nx\u2208P ( 1 2 e\u2212||x\u2212\u00b51|| 2/2 + 1 2 e\u2212||x+\u00b51|| 2/2)\u3008x, \u00b52 \u2212 \u00b51\u3009 = Ex\u223cD1\u3008x, \u00b51 \u2212 \u00b52\u3009 = 0 (1)\nWe now focus on the case where ||\u00b51|| >> 1. We observe that for any \u00b52 and any x, 1+e2\u3008x,\u00b52\u3009 > 1. Therefore, combining the previous two equations,\nKL(D1,D2) \u2264 1\u221a 2\u03c0\n(\n||\u00b52||2 \u2212 ||\u00b51||2 + \u222b x\u2208P ( 1 2 e\u2212||x\u2212\u00b51|| 2/2 + 1 2 e\u2212||x+\u00b51|| 2/2) ln(1 + e2\u3008x,\u00b51\u3009)dx\n)\nAgain, since the projection of D1 perpendicular to \u00b51 is distributed independently of the projection of D1 along \u00b51, the above integral can be taken over a one-dimensional x which varies along the vector \u00b51. For the rest of the proof, we abuse notation, and use \u00b51 to denote both the vector \u00b51 and the scalar ||\u00b51||. We can write:\n\u222b \u221e\nx=\u2212\u221e ( 1 2 e\u2212(x\u2212\u00b51) 2/2 + 1 2 e\u2212(x+\u00b51) 2/2) ln(1 + e2\u00b51x)dx\n\u2264 \u221a 2\u03c0 ln 2 + \u222b \u221e\nx=0 ( 1 2 e\u2212(x\u2212\u00b51) 2/2 + 1 2 e\u2212(x+\u00b51) 2/2) ln(1 + e2\u00b51x)dx\n\u2264 \u221a 2\u03c0 ln 2 + \u222b \u221e\nx=0 ( 1 2 e\u2212(x\u2212\u00b51) 2/2 + 1 2 e\u2212(x+\u00b51) 2/2)(ln 2 + 2x\u00b51)dx\n\u2264 3 \u221a 2\u03c0\n2 ln 2 + 2\u00b51\n\u222b \u221e\nx=0 ( 1 2 e\u2212(x\u2212\u00b51) 2/2 + 1 2 e\u2212(x+\u00b51) 2/2)xdx\nThe first part follows because for x < 0, ln(1 + e2x\u00b51) \u2264 ln 2. The second part follows because for x > 0, ln(1 + e2x\u00b51) \u2264 ln(2e2x\u00b51). The third part follows from the symmetry of D1 around the origin.\nNow, for any a, we can write:\n1\u221a 2\u03c0\n\u222b \u221e\nx=0 xe\u2212(x+a) 2/2dx = 1\u221a 2\u03c0 \u00b7 e\u2212a2/2 \u2212 a\u03a6(a,\u221e)\nPlugging this in, we can show that,\nKL(D1,D2) \u2264 1\u221a 2\u03c0\n( ||\u00b52||2 \u2212 ||\u00b51||2 + 3 \u221a 2\u03c0\n2 ln 2 + 2||\u00b51||(e\u2212||\u00b51||\n2/2 + \u221a 2\u03c0||\u00b51||\u03a6(0, ||\u00b51||))\n)\nfrom which the lemma follows.\nProof. (Of Lemma 18) For each i, let each vi be drawn independently from the distribution 1\u221a d N (0, Id). For each i, j, let Pij = d2 \u00b7 d(vi, vj) and Nij = d2 \u00b7 d(vi,\u2212vj). Then, for each i and j, Pij and Nij are distributed according to the Chi-squared distribution with parameter d. From Lemma 26, it follows that: Pr[Pij < d 10 ] \u2264 e\u22123d/10. A similar lemma can also be shown to hold for the random variables Nij . Applying the Union Bound, the probability that this holds for Pij and Nij for all pairs (i, j), i \u2208 V, j \u2208 V is at most 2K2e\u22123d/10. This probability is at most 12 when K = ed/10.\nIn addition, we observe that for each vector vi, d \u00b7 ||vi||2 is also distributed as a Chi-squared distribution with parameter d. From Lemma 26, for each i, Pr[||vi||2 > 7/5] \u2264 e\u22122d/15. The second part of the lemma now follows by an Union Bound over all K vectors in the set V .\nLemma 26. Let X be a random variable, drawn from the Chi-squared distribution with parameter d. Then,\nPr[X < d 10 ] \u2264 e\u22123d/10\nMoreover,\nPr[X > 7d 5 ] \u2264 e\u22122d/15\nProof. Let Y be the random variable defined as follows: Y = d\u2212X. Then,\nPr[X < d\n10 ] = Pr[Y >\n9d 10 ] = Pr[etY > e9dt/10] \u2264 E[e\ntY ] e9dt/10\nwhere the last step uses a Markov\u2019s Inequality. We observe that E[etY ] = etdE[e\u2212tX ] = etd(1 \u2212 2t)d/2, for t < 12 . The first part of the lemma follows from the observation that (1\u2212 2t)d/2 \u2264 e\u2212td, and by plugging in t = 13 .\nFor the second part, we again observe that\nPr[X > 7d 5 ] \u2264 (1\u2212 2t)\u2212d/2e\u22127dt/5 \u2264 e\u22122dt/5\nThe lemma now follows by plugging in t = 13 .\n6.4 More General k-means : Results and Proofs\nIn this section, we show that when we apply 2-means on an input generated by a mixture of k spherical Gaussians, the normal to the hyperplane which partitions the two clusters in the 2-means algorithm, converges to a vector in the subspace M containing the means of mixture components. This subspace is interesting because, in this subspace, the distance between the means is as high as in the original space; however, if the number of clusters is small compared to the dimension, the distance between two samples from the same cluster is much smaller. In fact, several algorithms for learning mixture models [VW02, AM05, CR08] attempt to isolate this subspace first, and then use some simple clustering methods in this subspace."}, {"heading": "6.4.1 The Setting", "text": "We assume that our input is generated by a mixture of k spherical Gaussians, with means \u00b5j, variances (\u03c3j)2, j = 1, . . . , k, and mixing weights \u03c11, . . . , \u03c1k. The mixture is centered at the origin such that \u2211\n\u03c1j\u00b5j = 0. We use M to denote the subspace containing the means \u00b51, . . . , \u00b5k. We use Algorithm 2-means-iterate on this input, and our goal is to show that it still converges to a vector in M. In the sequel, given a vector x and a subspace W , we define the angle between x and W as the angle between x and the projection of x onto W . As in Sections 2 and 3, we examine the angle \u03b8t, between ut and M, and our goal is to show that the cosine of this angle grows as t increases. Our main result of this section is Lemma 20, which, analogous to Lemma 1 in Section 3, exactly defines the behavior of 2-means on a mixture of k spherical Gaussians.\nBefore we can prove the lemma, we need some additional notation."}, {"heading": "6.4.2 Notation", "text": "Recall that at time t, we use u\u0306t to partition the input data, and the projection of u\u0306t along M is cos(\u03b8t) by definition. Let b 1 t be a unit vector lying in the subspace M such that:\nu\u0306t = cos(\u03b8t)b 1 t + sin(\u03b8t)vt\nwhere vt lies in the orthogonal complement of M, and has norm 1. We define a second vector u\u0306\u22a5t as follows:\nu\u0306\u22a5t = sin(\u03b8t)b 1 t \u2212 cos(\u03b8t)vt\nWe observe that \u3008u\u0306t, u\u0306\u22a5t \u3009 = 0, ||u\u0306\u22a5t || = 1, and the projection of u\u0306\u22a5t on M is sin(\u03b8t)b1t . We now extend the set {b1t } to complete an orthonormal basis B = {b1t , . . . , bk\u22121t } of M. We also observe that {b2t , . . . , bk\u22121t , u\u0306t, u\u0306\u22a5t } is an orthonormal basis of the subspace spanned by any basis of M, along with vt, and can be extended to a basis of Rd.\nFor j = 1, . . . , k, we define \u03c4 jt as follows:\n\u03c4 tj = \u3008\u00b5j, u\u0306t\u3009 = cos(\u03b8t)\u3008\u00b5j , b1t \u3009\nFinally we (re)-define the quantity \u03bet as\n\u03bet = \u2211\nj\n\u03c1j\u03c3j e\u2212(\u03c4 j t ) 2/2(\u03c3j )2\n\u221a 2\u03c0\nand, for any l = 1, . . . , k \u2212 1, we define:\nmlt = \u2211\nj\n\u03c1j\u03a6(\u2212 \u03c4 j t\n\u03c3j ,\u221e)\u3008\u00b5j , blt\u3009"}, {"heading": "6.4.3 Proof of Lemma 20", "text": "The main idea behind the proof of Lemma 20 is to estimate the norm and the projection of ut+1; we do this in three steps. First, we estimate the projection of ut+1 along u\u0306t; next, we estimate this projection on u\u0306\u22a5t , and finally, we estimate its projection along b 2 t , . . . , b l t. Combining these projections, and observing that the projection of ut+1 on any direction perpendicular to these is 0, we can prove the lemma.\nAs before, we define Zt+1 = Pr[x \u2208 Ct+1]\nNow we make the following claim.\nLemma 27. For any t and any j,\nPr[x \u223c Dj |x \u2208 Ct+1] = \u03c1j Zt+1 \u03a6(\u2212 \u03c4 j t \u03c3j ,\u221e)\nProof. Same proof of Lemma 21\nNext, we estimate the projection of ut+1 along u\u0306t.\nLemma 28.\n\u3008ut+1, u\u0306t\u3009 = \u03bet + cos(\u03b8t)m\n1 t\nZt+1\nProof. Consider a sample x drawn from distribution Dj. The projection of x on u\u0306t is distributed as a Gaussian with mean \u03c4 jt and standard deviation \u03c3 j . The probability that x lies in Ct+1 is Pr[N(\u03c4 jt , \u03c3 j) > 0] = \u03a6(\u2212 \u03c4 j t \u03c3j ,\u221e). Given that x lies in Ct+1, the projection of x on u\u0306t is distributed as a truncated Gaussian, with mean \u03c4 jt and standard deviation \u03c3 j, which is truncated at 0. Therefore,\nE[\u3008x, u\u0306t\u3009|x \u2208 Ct+1, x \u223c Dj ] = 1\n\u03a6(\u2212 \u03c4 j t\n\u03c3j ,\u221e)\n(\n\u222b \u221e\ny=0\nye\u2212(y\u2212\u03c4 j t ) 2/2\n\u03c3j \u221a 2\u03c0 dy\n)\nwhich is again equal to\n1\n\u03a6(\u2212 \u03c4 j t\n\u03c3j ,\u221e)\n(\n\u03c4 jt\n\u222b \u221e\ny=0\ne\u2212(y\u2212\u03c4 j t ) 2/2(\u03c3j )2\n\u03c3j \u221a 2\u03c0\ndy +\n\u222b \u221e\ny=0\n(y \u2212 \u03c4 jt )e\u2212(y\u2212\u03c4 j t ) 2/2(\u03c3j )2\n\u03c3j \u221a 2\u03c0\ndy\n)\n= 1\n\u03a6(\u2212 \u03c4 j t\n\u03c3j ,\u221e)\n(\n\u03c4 jt \u03a6(\u2212 \u03c4 jt \u03c3j ,\u221e) + \u222b \u221e\ny=0\n(y \u2212 \u03c4 jt )e\u2212(y\u2212\u03c4 j t ) 2/2(\u03c3j )2\n\u03c3j \u221a 2\u03c0\ndy\n)\nWe can evaluate the integral in the equation above as follows. \u222b \u221e\ny=0 (y \u2212 \u03c4 jt )e\u2212(y\u2212\u03c4 j t )\n2/2(\u03c3j )2dy = (\u03c3j)2 \u222b \u221e\nz=(\u03c4 jt ) 2/2(\u03c3j )2\ne\u2212zdz = (\u03c3j)2e\u2212(\u03c4 j t ) 2/2(\u03c3j )2\nTherefore we can conclude that\nE[\u3008x, u\u0306t\u3009|x \u2208 Ct+1, x \u223c Dj ] = \u03c4 jt + 1\n\u03a6(\u2212 \u03c4 j t\n\u03c3j ,\u221e)\n\u00b7 \u03c3j e \u2212(\u03c4 jt )2/2(\u03c3j )2\n\u221a 2\u03c0\nNow we can write\n\u3008ut+1, u\u0306t\u3009 = \u2211\nj\nE[\u3008x, u\u0306t\u3009|x \u223c Dj , x \u2208 Ct+1] Pr[x \u223c Dj |x \u2208 Ct+1]\n= 1\nZt+1\n\u2211\nj\n\u03c1j\u03a6(\u2212 \u03c4 j t\n\u03c3j ,\u221e)E[\u3008x, u\u0306t\u3009|x \u223c Dj , x \u2208 Ct+1]\nwhere we used lemma 27. The lemma follows by recalling that \u03c4 jt = cos(\u03b8t)\u3008\u00b5j , b1t \u3009.\nLemma 29. For any t,\n\u3008ut+1, u\u0306\u22a5t \u3009 = sin(\u03b8t)m\n1 t\nZt+1\nProof. Let x be a sample drawn from distribution Dj. Since u\u0306 \u22a5 t is perpendicular to u\u0306t, and Dj is a spherical Gaussian, given that x \u2208 Ct+1, that is, the projection of x on u\u0306t is greater than 0, the projection of x on u\u0306\u22a5t is still distributed as a Gaussian with mean \u3008\u00b5j , u\u0306\u22a5t \u3009 and standard deviation \u03c3j . That is, E[\u3008x, u\u0306\u22a5t \u3009|x \u223c Dj , x \u2208 Ct+1] = \u3008\u00b5j , u\u0306\u22a5t \u3009 Also recall that, by definition of u\u0306\u22a5t , \u3008\u00b5j, u\u0306\u22a5t \u3009 = sin(\u03b8t)\u3008\u00b5j , b1t \u3009. To prove the lemma, we observe that \u3008ut+1, u\u0306\u22a5t \u3009 is equal to\n\u2211\nj\nE[\u3008x, u\u0306\u22a5t \u3009|x \u223c Dj , x \u2208 Ct+1] Pr[x \u223c Dj |x \u2208 Ct+1]\nThe lemma follows by using lemma 27.\nLemma 30. For l \u2265 2, \u3008ut+1, blt\u3009 =\nmlt Zt+1\nProof. Let x be a sample drawn from distribution Dj . Since b l t is perpendicular to u\u0306t, and Dj is a spherical Gaussian, given that x \u2208 Ct+1, that is, the projection of x on u\u0306t is greater than 0, the projection of x on blt is still distributed as a Gaussian with mean \u3008\u00b5j , blt\u3009 and standard deviation \u03c3j . That is, E[\u3008x, blt\u3009|x \u223c Dj , x \u2208 Ct+1] = \u3008\u00b5j , blt\u3009 To prove the lemma, we observe that \u3008blt, ut+1\u3009 is equal to\n\u2211\nj\nE[\u3008x, blt\u3009|x \u223c Dj , x \u2208 Ct+1] Pr[x \u223c Dj |x \u2208 Ct+1]\nThe lemma follows by using lemma 27.\nFinally, we show a lemma which estimates the norm of the vector ut+1.\nLemma 31.\n||ut+1||2 = 1\nZ2t+1 (\u03be2t + 2\u03bet cos(\u03b8t)m 1 t +\nk \u2211\nl=1\n(mlt) 2)\nProof. Combining Lemmas 28, 29 and 30, we can write:\n||ut+1||2 = \u3008u\u0306t, ut+1\u30092 + \u3008u\u0306\u22a5t , ut+1\u30092 + \u2211 l\u22652 \u3008blt, ut+1\u30092\n= 1\nZ2t+1\n(\n\u03be2t + 2\u03bet cos(\u03b8t)m 1 t + cos 2(\u03b8t)(m 1 t ) 2 + sin2(\u03b8t)(m 1 t ) 2 +\nk \u2211\nl=2\n(mlt) 2\n)\nThe lemma follows by plugging in the fact that cos2(\u03b8t) + sin 2(\u03b8t) = 1.\nNow we are ready to prove Lemma 20.\nProof. (Of Lemma 20) Since b1t , . . . , b k t form a basis of M, we can write:\ncos2(\u03b8t+1) = \u2211k l=1\u3008ut+1, blt\u30092 ||ut+1||2\n(2)\n||ut+1||2 is estimated in Lemma 31, and \u3008ut+1, blt\u3009 is estimated by Lemma 29. Using these lemmas, as b1t lies in the subspace spanned by the orthogonal vectors u\u0306t and u\u0306 \u22a5 t , we can write:\n\u3008ut+1, b1t \u3009 = \u3008u\u0306t, ut+1\u3009\u3008u\u0306t, b1t \u3009+ \u3008u\u0306\u22a5t , ut+1\u3009\u3008u\u0306\u22a5t , b1t \u3009\n= cos(\u03b8t)\u03bet +m\n1 t\nZt+1\nPlugging this in to Equation 2, we get:\ncos2(\u03b8t+1) = \u03be2t cos 2(\u03b8t) + 2\u03bet cos(\u03b8t)m 1 t +\n\u2211\nl(m l t) 2\n\u03be2t + 2\u03bet cos(\u03b8t)m 1 t +\n\u2211\nl(m l t) 2\nThe lemma follows by rearranging the above equation, similar to the proof of Lemma 1."}], "references": [{"title": "Learning mixtures of separated nonspherical Gaussians", "author": ["S. Arora", "R. Kannan"], "venue": "Ann. Applied Prob.,", "citeRegEx": "Arora and Kannan.,? \\Q2005\\E", "shortCiteRegEx": "Arora and Kannan.", "year": 2005}, {"title": "On spectral learning of mixtures of distributions", "author": ["D. Achlioptas", "F. McSherry"], "venue": "In COLT,", "citeRegEx": "Achlioptas and McSherry.,? \\Q2005\\E", "shortCiteRegEx": "Achlioptas and McSherry.", "year": 2005}, {"title": "k-means has polynomial smoothed complexity", "author": ["D. Arthur", "B. Manthey", "H. R\u00f6glin"], "venue": "In FOCS,", "citeRegEx": "Arthur et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2009}, {"title": "How slow is the k-means method", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SoCG,", "citeRegEx": "Arthur and Vassilvitskii.,? \\Q2006\\E", "shortCiteRegEx": "Arthur and Vassilvitskii.", "year": 2006}, {"title": "Separating populations with wide data: A spectral analysis", "author": ["A. Blum", "A. Coja-Oghlan", "A.M. Frieze", "S. Zhou"], "venue": "In ISAAC,", "citeRegEx": "Blum et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2007}, {"title": "Isotropic PCA and affine-invariant clustering", "author": ["S.C. Brubaker", "S. Vempala"], "venue": "In FOCS,", "citeRegEx": "Brubaker and Vempala.,? \\Q2008\\E", "shortCiteRegEx": "Brubaker and Vempala.", "year": 2008}, {"title": "Learning Mixtures of Distributions", "author": ["K. Chaudhuri"], "venue": "PhD thesis,", "citeRegEx": "Chaudhuri.,? \\Q2007\\E", "shortCiteRegEx": "Chaudhuri.", "year": 2007}, {"title": "A rigorous analysis of population stratification with limited data", "author": ["K. Chaudhuri", "E. Halperin", "S. Rao", "S. Zhou"], "venue": "In SODA,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2007}, {"title": "Learning mixtures of distributions using correlations and independence", "author": ["K. Chaudhuri", "S. Rao"], "venue": "In COLT,", "citeRegEx": "Chaudhuri and Rao.,? \\Q2008\\E", "shortCiteRegEx": "Chaudhuri and Rao.", "year": 2008}, {"title": "Learning mixtures of gaussians", "author": ["S. Dasgupta"], "venue": "In FOCS,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "Maximum likelihood from incomplete data via the em algorithm (with discussion)", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "A two-round variant of EM for Gaussian mixtures", "author": ["S. Dasgupta", "L. Schulman"], "venue": "In UAI,", "citeRegEx": "Dasgupta and Schulman.,? \\Q2000\\E", "shortCiteRegEx": "Dasgupta and Schulman.", "year": 2000}, {"title": "Cluster analysis of multivariate data: Efficiency vs. interpretability of classification", "author": ["E. Forgey"], "venue": null, "citeRegEx": "Forgey.,? \\Q1965\\E", "shortCiteRegEx": "Forgey.", "year": 1965}, {"title": "The spectral method for general mixture models", "author": ["R. Kannan", "H. Salmasian", "S. Vempala"], "venue": "In COLT,", "citeRegEx": "Kannan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2005}, {"title": "Mixture Models:Theory", "author": ["B.G. Lindsey"], "venue": "Geometry and Applications. IMS,", "citeRegEx": "Lindsey.,? \\Q1996\\E", "shortCiteRegEx": "Lindsey.", "year": 1996}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Lloyd.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd.", "year": 1982}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J.B. MacQueen"], "venue": "In Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "MacQueen.,? \\Q1967\\E", "shortCiteRegEx": "MacQueen.", "year": 1967}, {"title": "Improved smoothed analysis of the k-means method", "author": ["B. Manthey", "H. R\u00f6glin"], "venue": "In SODA,", "citeRegEx": "Manthey and R\u00f6glin.,? \\Q2009\\E", "shortCiteRegEx": "Manthey and R\u00f6glin.", "year": 2009}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "In FOCS,", "citeRegEx": "Ostrovsky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ostrovsky et al\\.", "year": 2006}, {"title": "Strong consistency of k-means clustering", "author": ["D. Pollard"], "venue": "Annals of Statistics,", "citeRegEx": "Pollard.,? \\Q1981\\E", "shortCiteRegEx": "Pollard.", "year": 1981}, {"title": "Mixture densities, maximum likelihood and the em algorithm", "author": ["R. Redner", "H. Walker"], "venue": "SIAM Review,", "citeRegEx": "Redner and Walker.,? \\Q1984\\E", "shortCiteRegEx": "Redner and Walker.", "year": 1984}, {"title": "An investigation of computational and informational limits in gaussian mixture clustering", "author": ["N. Srebro", "G. Shakhnarovich", "S.T. Roweis"], "venue": "In ICML,", "citeRegEx": "Srebro et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2006}, {"title": "k-means takes exponentially many iterations even in the plane", "author": ["A. Vattani"], "venue": "SoCG,", "citeRegEx": "Vattani.,? \\Q2009\\E", "shortCiteRegEx": "Vattani.", "year": 2009}, {"title": "A spectral algorithm for learning mixtures of distributions", "author": ["V. Vempala", "G. Wang"], "venue": "In FOCS,", "citeRegEx": "Vempala and Wang.,? \\Q2002\\E", "shortCiteRegEx": "Vempala and Wang.", "year": 2002}, {"title": "On convergence properties of the em algorithm for gaussian mixtures", "author": ["L. Xu", "M.I. Jordan"], "venue": "Neural Computation,", "citeRegEx": "Xu and Jordan.,? \\Q1996\\E", "shortCiteRegEx": "Xu and Jordan.", "year": 1996}, {"title": "Assaoud, fano and le cam", "author": ["B. Yu"], "venue": null, "citeRegEx": "Yu.,? \\Q1997\\E", "shortCiteRegEx": "Yu.", "year": 1997}], "referenceMentions": [], "year": 2009, "abstractText": "One of the most popular algorithms for clustering in Euclidean space is the k-means algorithm; k-means is difficult to analyze mathematically, and few theoretical guarantees are known about it, particularly when the data is well-clustered. In this paper, we attempt to fill this gap in the literature by analyzing the behavior of k-means on well-clustered data. In particular, we study the case when each cluster is distributed as a different Gaussian \u2013 or, in other words, when the input comes from a mixture of Gaussians. We analyze three aspects of the k-means algorithm under this assumption. First, we show that when the input comes from a mixture of two spherical Gaussians, a variant of the 2-means algorithm successfully isolates the subspace containing the means of the mixture components. Second, we show an exact expression for the convergence of our variant of the 2-means algorithm, when the input is a very large number of samples from a mixture of spherical Gaussians. Our analysis does not require any lower bound on the separation between the mixture components. Finally, we study the sample requirement of k-means; for a mixture of 2 spherical Gaussians, we show an upper bound on the number of samples required by a variant of 2-means to get close to the true solution. The sample requirement grows with increasing dimensionality of the data, and decreasing separation between the means of the Gaussians. To match our upper bound, we show an information-theoretic lower bound on any algorithm that learns mixtures of two spherical Gaussians; our lower bound indicates that in the case when the overlap between the probability masses of the two distributions is small, the sample requirement of k-means is near-optimal.", "creator": "LaTeX with hyperref package"}}}