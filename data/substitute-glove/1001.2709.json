{"id": "1001.2709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2010", "title": "Kernel machines with two layers and multiple kernel learning", "abstract": "In this paper, the fundamental result polynomial machines with dozen layers is expanded, oversimplified classical kernel methods. The while knowledge methodology obtain up meetings connection over processes primitives with multiple silt and the parody of ubuntu learning where range regularization strategies. First, a representer linear for followed - layer interactive only presented, displayed probably i.e. embedding simple in tortilla on well layer are desirable next-generation things its corresponding purpose solve certain variational likely the flawlessly user-space Hilbert clustered (RKHS ). The signal - output topographical desire took these widgets pretty on over actually higher although turned suitable single - layer spline machines present or came ubuntu determining latter but talking from for data. Recently, the well - follow multiple kernel learning methods still viewed considerable latest to own computer learning literature. In this paper, individuals cryptographic different suggest are shown go rarely context cases brought kernel cellphones with out layers through which full second layer same linear. Finally, was typical some aim limited kernel learning method once RLS2 (reexamined 600 tables with well layers) though introduced, and first playing on with learning concerned them being analyzed. An top source MATLAB compacting to passengers for vouch RLS2 newer with a Graphic User Interface the include.", "histories": [["v1", "Fri, 15 Jan 2010 15:10:39 GMT  (388kb,DS)", "http://arxiv.org/abs/1001.2709v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["francesco dinuzzo"], "accepted": false, "id": "1001.2709"}, "pdf": {"name": "1001.2709.pdf", "metadata": {"source": "CRF", "title": "Kernel machines with two layers and multiple kernel learning", "authors": ["Francesco Dinuzzo"], "emails": ["francesco.dinuzzo@unipv.it."], "sections": [{"heading": "1 Introduction", "text": "Learning by minimizing costs in functional spaces has proven to be an important approach to better understand many estimation problems. Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods. One of the most appealing properties of kernel methods is optimality according to a variety of representer theorems. These results are usually presented within the theory of RKHS [5], and formalize the intuition that optimal learning machines trained with a finite number of data must be expressed by\n\u2217Francesco Dinuzzo is with Department of Mathematics, University of Pavia, Pavia, Italy e-mail: francesco.dinuzzo@unipv.it.\nar X\niv :1\n00 1.\n27 09\nv1 [\ncs .L\nG ]\n1 5\nJa n\na finite number of parameters, even when the hypothesis space is infinite dimensional. Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].\nRecently, has been pointed out that standard kernel machines are somehow limited in their ability to approximate complex functional classes, as a consequence of being shallow architectures. In addition, existing representer theorems only apply to single-layer architectures, though an extension of the theory to include multi-layer networks would be useful to better understand the behavior of current multi-layer architectures and characterize new methods with more flexible approximation capabilities. Such extension is also suggested by complexity theory of circuits [8] as well as by biological motivated learning models, [43]. In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32]. Indeed, the difficulty of choosing a good hypothesis space with little available a-priori knowledge is significantly reduced when the kernel is also learned from the data. The flexibility of algorithms implementing the framework of kernel learning makes also possible to address important machine learning issues such as feature selection, learning from heterogeneous sources of data, and multiscale approximation. A major extension to the framework of classical kernel methods, based on the concept of hyper-kernels, has been introduced in [33], encompassing many convex kernel learning algorithms. In this paper, the connection between learning the kernel in standard (single layer) kernel machines and learning in multi-layer architectures is analyzed. We introduce the framework of kernel machines with two layers, that also encompasses classical single layer kernel methods as well as many kernel learning algorithms.\nConsider a generic architecture whose input-output behavior can be described as a function composition of two layers\nf = f2 \u25e6 f1, f1 : X \u2192 Z, f2 : Z \u2192 Y, (1)\nwhere X is a generic set while Z and Y are two Hilbert spaces. The problem of learning simultaneously the two layers f1 and f2 from input-output data pairs (xi, yi) \u2208 X \u00d7 Y can be formalized in a functional analytic setting. Indeed, in section 2 it is shown that a representer theorem holds: even if f1 and f2 are searched into infinite dimensional function spaces, optimal learning architectures are finite linear combination of kernel functions on each layer, where optimality is measured according to general regularization functionals. Remarkably, such representer theorem also imply that, upon training, architecture (1) can be equivalently regarded as a standard kernel machine in which the kernel function has been learned from the data. After discussing the general result on the solution representation for two non-linear layers, the attention is focused on the case in which the second layer is linear. In section 3, we introduce a regularization framework that turns out to be equivalent to a general class of methods to perform multiple kernel learning, that is the simultaneous supervised learning of a predictor and the associated kernel as a convex combination\nof basis kernels. The general problem of learning the kernel is receiving a lot of attention in recent years, both from functional analytic point of view and from pure optimization perspectives. Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6]. In section 4, a method called RLS2 (regularized least squares with two layers) based on regularized multiple kernel learning with the square loss function is introduces and studied. Along the line of recent advances in multiple kernel learning [46, 35, 6], it is shown that the involved optimization can be efficiently carried out using a two-step procedure. For RLS2, two-step optimization turns out to be especially simple and computationally appealing, alternating between the solution of a linear system and a constrained least squares problem. The application of RLS2 on a variety of learning problems is analyzed in section 5. State of the art generalization performances are achieved on several datasets, including multi-class classification of genomic data. An open source MATLAB toolbox to train and validate RLS2 models with a Graphic User Interface is available at http://www.mloss.org. All the proof of Theorems and Lemmas are given in the Appendix."}, {"heading": "2 A representer theorem for architectures with", "text": "two layers\nA learning architecture with two layers can be formalized as a map f : X \u2192 Y expressed as a function composition as in equation (1). Introduce two RKHS H1 and H2 of vector-valued functions [31] defined over X and Z respectively, with (operator-valued) kernel functions K1 and K2, and consider the following problem:\nProblem 1\nmin f1\u2208H1, f2\u2208H2\n\u2211\u0300\ni=1\nLi ((f2 \u25e6 f1)(xi)) +R1(\u2016f1\u2016H1) +R2(\u2016f2\u2016H2).\nHere, Li : Y \u2192 R+ are loss functions measuring the approximation of training data, while R1, R2 : R+ \u2192 R+ \u222a {+\u221e} are two extended-valued nondecreasing functions (not identically +\u221e) that play the role of regularization terms. Problem 1 is outside the scope of standard representer theorems [40] due to the presence of the composition (f2 \u25e6 f1). Nevertheless, it still holds that linear combinations of a finite number of kernel functions are optimal solutions, as soon as there exist minimizers.\nTheorem 1 If the functional of Problem 1 admit minimizers, then there exist optimal solutions of Problem 1 in the form\nf1(x) = \u2211\u0300\ni=1\nK1(x, xi)ai, f2(z) = \u2211\u0300\ni=1\nK2(f1(xi), z)bi.\nTherefore, there exists optimal learning architectures in the following inputoutput form:\nf(x) = (f2 \u25e6 f1)(x) = \u2211\u0300\ni=1\nK(xi, x)bi, K(x1, x2) := K 2(f1(x1), f1(x2)). (2)\nTheorem 1 is a restriction theorem: the search for solutions of Problem 1 can be restricted to kernel machines with two layers involving a finite number of kernel functions, even when H1 and H2 are infinite dimensional spaces. Notice that Theorem 1 is not an existence theorem, since existence of minimizers is one of the hypotheses. As shown in the next sections, existence can be ensured under mild additional conditions on Li, R1, R2. Under the general hypothesis of Theorem 1, uniqueness of minimizers in Problem 1 is also not guaranteed, even when loss functions Li are strictly convex. Notice also that Theorem 1 do admit the presence of optimal solutions not in the form of finite kernel expansions. However, if such solutions exist, then their projections over the finite dimensional span of kernel sections are optimal as well, so that one can restrict the attention to kernel machines with two layers also in this case. Finally, when R1 and R2 are strictly increasing, it holds that every optimal solution of Problem 1 can be expressed as a kernel machine with two layers."}, {"heading": "3 Multiple kernel learning as a kernel machine", "text": "with two layers\nTheorem 1 shows that training an architecture with two layers is equivalent to train simultaneously a single-layer kernel network and the kernel function, see equation (2). In this section, it is shown that multiple kernel learning, consisting in simultaneous learning of a finite linear combination of kernels and the associated predictor, can be interpreted as a specific instance of kernel architecture with two layers. Introduce a set of m positive kernels K\u0303i defined on X \u00d7 X, called basis kernels and consider the following choice for H1 and H2.\n\u2022 H1 is an RKHS of vector valued functions f : X \u2192 Rm associated with the matrix-valued kernel function K1 such that\nK1(x1, x2) = diag { K\u03031(x1, x2), . . . , K\u0303m(x1, x2) } .\n\u2022 H2 is the RKHS of real valued functions f : Rm \u2192 R associated with the linear kernel\nK2 (z1, z2) = z T 1 Sz2,\nwhere S is a diagonal scaling matrix:\nS = diag {s1, . . . , sm} > 0.\nFor any f \u2208 H1, let f i, (i = 1, . . . ,m) denote its components. Introduce the indicator function I of the interval [0, 1] defined as\nI(x) = { 0, 0 \u2264 x \u2264 1 +\u221e, x > 1 ,\nlet Li : Y \u2192 R+ denote lower semi-continuous convex loss functions and \u03bb > 0. In the following, we analyze the particular case of Problem 1 in which R1 is a square regularization and R2 is the indicator function regularization:\nR1(x) = \u03bb\n2 x2, R2(x) = I(x).\nProblem 2\nmin f1\u2208H1, f2\u2208H2\n[\u2211\u0300\ni=1\nLi ((f2 \u25e6 f1)(xi)) + \u03bb\n2 \u2016f1\u20162H1 + I (\u2016f2\u2016H2)\n] .\nLet\u2019s briefly discuss the choice of regularizers. First of all, notice that both R1 and R2 are convex functions. Since Li are convex loss functions and f2 is linear, the problem is separately convex in both f1 and f2. Apparently, regularizing with the indicator function I (\u2016f2\u2016H2) is equivalent to impose the constraint \u2016f2\u2016H2 \u2264 1. Lemma 1 below shows that minimization into the unitary ball can be carried out without any loss of generality.\nLemma 1 Let (f\u22171 , f \u2217 2 ) denote an optimal solution of the following problem:\nmin f1\u2208H1, f2\u2208H2\n[\u2211\u0300\ni=1\nLi ((f2 \u25e6 f1)(xi)) + \u03b1\n2 \u2016f1\u20162H1 + \u03b3 \u00b7 I ( \u2016f2\u2016H2 \u03b2 )] .\nThen, (f1, f2) = (\u03b2f \u2217 1 , f \u2217 2 /\u03b2) is an an optimal solution of Problem 2 with \u03bb = \u03b1/\u03b22 and satisfy f = f2 \u25e6 f1 = f\u22172 \u25e6 f\u22171 . (3)\nThanks to the scaling properties coming from the linearity of the second layer and the use of the indicator function, the introduction of an additional regularization parameter can be avoided, thus significantly reducing the complexity of model selection. The next Theorem characterizes optimal solutions of Problem 2.\nTheorem 2 There exist optimal solutions f1 and f2 of Problem 2 in the form\nf i1(x) = siwi \u2211\u0300\nj=1\ncjK\u0303i(xj , x), f2(z) = z TSw.\nLetting di := siw 2 i , optimal coefficients (c, d) solves the multiple kernel learning Problem 3 below, where\nQ(z) := \u2211\u0300\nj=1\nLj(zj). (4)\nFinally, the solution of Problem 2 can be written as in equation (2), where the kernel K satisfies\nK(x, y) =\nm\u2211\ni=1\ndiKi(x, y), Ki(x, y) = \u2211\u0300\nj1=1\n\u2211\u0300\nj2=1\ncj1cj2K\u0303i(xj1 , x)K\u0303i(xj2 , y).\n(5)\nProblem 3\nmin c\u2208R`,d\u2208Rm\n( Q(R(d)c) + \u03bb\n2 cTR(d)c\n)\nsubject to\nRkij = skK\u0303k(xi, xj), R(d) =\nm\u2211\nk=1\ndkR k, dk \u2265 0,\nm\u2211\nk=1\ndk \u2264 1. (6)\nTheorem 2 shows that the variational Problem 2 for a two-layer kernel machine is equivalent to the multiple kernel learning Problem 3. The non-negativity constraints dk \u2265 0 leads to a sparse selection of a subset of basis kernels. In standard formulations, multiple kernel learning problems feature the equality constraint \u2211m k=1 dk = 1, instead of the inequality in (6). Nevertheless, Lemma 2 below shows that there always exist optimal solutions of Problem 4 satisfying the equality, so that the two optimization problems are equivalent.\nA few comments on certain degeneracies in Problem 2 are in order. First of all, observe that the absolute value of optimal coefficients wi characterizing the two layer kernel machine is given by |wi| = \u221a di/si, but sign(wi) is undeter-\nmined. Then, without loss of generality, it is possible to choose wi = \u221a di/si. Second, observe that the objective functional of Problem 3 depends on c through the product Rc. When R is singular, the optimal vector c is not unique (independently of Q). In particular, if v belongs to the null space of R, then c+ \u03b3v achieves the same objective value of c, for any \u03b3 \u2208 R. This case also occurs in standard (single-layer) kernel methods. One possible way to break the indetermination, again without any loss of generality, is to constrain c to belong to the range of R. With such additional constraint, there exists z such that\nc = R\u2020z, (7)\nwhere \u2020 denote the Moore-Penrose pseudo-inverse (notice that, in general, z might be different from Rc). Remarkably, the introduction of such change of variable makes also possible to derive an addition formulation of Problem 3, which can be shown to be a convex optimization problem. Indeed, by rewriting Problem 3 as a function of (z, d), the following problem is obtained:\nProblem 4\nmin z\u2208R`,d\u2208Rm\n( Q(z) + \u03bb\n2 zTR\u2020(d)z\n) , subject to (6).\nLemma 2 Problem 4 is a convex optimization problem and there exists an optimal vector d satisfying the equality constraint\nm\u2211\nk=1\ndk = 1. (8)\nLemma 2 completes the equivalence between the specific kernel machines with two layers obtained by solving Problem 2 and multiple kernel learning algorithms. The Lemma also gives another important insight into the structure of Problem 3: local minimizers are also global minimizers, a property that directly transfer from Problem 4 through the change of variable (7)."}, {"heading": "3.1 Linear machines", "text": "In applications of standard kernel methods involving high-dimensional input data, the linear kernel on RN\nK(x1, x2) = x T 1 x2 (9)\nplays an important role. Optimization algorithms for linear machines are being the subject of a renewed attention in the literature, due to some important experimental findings. First, it turns out that linear models are already enough flexible to achieve state of the art classification performances in application domains such as text document classification, word-sense disambiguation, and drug design, see e.g. [24]. Second, linear machines can be trained using extremely efficient and scalable algorithms [23, 44, 16]. Finally, linear methods can be also used to solve certain non-linear problems (by using non-linear feature maps), thus ensuring a good trade-off between flexibility and computational convenience.\nLinear kernels are also meaningful in the context of multiple kernel learning methods. Indeed, when the input set X is a subset of RN , a possible choice for the set of basis kernels K\u0303k is given by linear kernels on each component:\nK\u0303k(x1, x2) = x k 1x k 2 . (10)\nSuch a choice makes the input output map (2) a linear function:\nf(x) =\nm\u2211\nj=1\n( djsj \u2211\u0300\ni=1\ncix j i\n) xj = aTx, (11)\nwhere\naj := djsjzj , zj := \u2211\u0300\ni=1\ncix j i . (12)\nHere, an important benefit is sparsity in the vector of weights a, that follows immediately from sparsity of vector d. In this way, linear multiple kernel learning\nalgorithms can simultaneously perform regularization and linear feature selection. Such property is apparently linked to the introduction of the additional layer in the architecture, since standard kernel machines with one layer are not able to perform any kind of automatic feature selection. From the user\u2019s point of view, linear kernel machines with two layers behave similarly to sparse `1 regularization methods such as the Lasso [50], performing feature selection by varying with continuity a shrinking parameter. However, it seems that `1 regularization methods cannot be interpreted as kernel machines (not even with two layers) and these two classes of algorithms are thus distinct. An instance of linear regularization methods with two layers is proposed in subsection 4.2 and analyzed in the experimental section 5."}, {"heading": "4 Regularized least squares with two layers", "text": "Algorithm 1 Alternate optimization for RLS2\ni\u2190 arg maxk=1,...,m yTRky d\u2190 ei B \u2190 {i} while (stopping criterion is not met) do R\u2190 0 for j \u2208 B do R\u2190 R+ djRj\nend for c\u2190 Solution of the linear system (R+ \u03bbI) c = y u\u2190 ( y \u2212 \u03bbc2 ) for i = 1, . . . ,m do vi \u2190 Ric end for d\u2190 Solution of Problem (7). B \u2190 {j : dj 6= 0}\nend while\nIn the previous section, a general class of convex optimization problems to learn finite linear combinations of kernels is shown to be equivalent to a two-layer kernel machine. As for standard kernel machines, different choices of loss functions Li lead to a variety of learning algorithms. For instance, from the results of the previous section it follows that the two-layer version of standard Support Vector Machines with \u201chinge\u201d loss functions Li(z) = (1\u2212 yiz)+ is equivalent to the SILP (Semi-Infinite Linear Programming) multiple kernel learning problem studied in [46], whose solution can be computed, for instance, by using gradient descent or SimpleMKL [35].\nIn this section, attention is focussed on square loss functions Li(z) = (yi \u2212 z)2/2 and the associated kernel machine with two layers. As we shall show, coefficients cj and dj defining the architecture as well as the \u201cequivalent input-\noutput kernel\u201d K can be computed by solving a very simple optimization problem. Such problem features the minimization of a quartic functional in (c, d), that is separately quadratic in both c and d. It is worth noticing that the square loss function can be used to solve regression problems as well as classification ones. Indeed, generalization performances of regularized least squares classifiers have been shown to be comparable to that of Support Vector Machines on many dataset, see [38, 17] and references therein.\nProblem 5 (Regularized least squares with two layers (RLS2))\nmin c\u2208R`,d\u2208Rm\n( 1\n2 \u2016y \u2212R(d)c\u20162 + \u03bb 2 cTR(d)c\n) , subject to (6).\nLet \u2206m denote the standard (m\u2212 1)-simplex in Rm:\n\u2206m :=\n{ d \u2208 Rm : d \u2265 0, m\u2211\ni=1\ndi = 1\n} .\nFor any fixed d, Problem 5 is an unconstrained quadratic optimization problem with respect to c. It is then possible to solve for the optimal c\u2217 in closed form as a function of d:\nc\u2217(d) =\n( m\u2211\ni=1\ndiR i + \u03bbI )\u22121 y. (13)\nAs shown in Lemma 3 below, Problem 5 can be reduced to the following Problem in d only.\nProblem 6\nmin d\u2208\u2206m\n\u03bb 2 yT c\u2217(d),\nLemma 3 The pair (c\u2217, d\u2217) is an optimal solution of Problem 5 if and only if equation (13) holds and d\u2217 is an optimal solution of Problem 6.\nAlong the lines of recent developments in multiple kernel learning optimization [46, 35], we propose a two-step minimization procedure that alternates between kernel and predictor optimization. The specific structure of our problem allows for exact optimization in each of the two phases of the optimization process. Let\nV := ( v1 \u00b7 \u00b7 \u00b7 vm ) = ( R1c \u00b7 \u00b7 \u00b7 Rmc ) , u := ( y \u2212 \u03bbc\n2\n) .\nFor any fixed c, minimization with respect to d boils down to the following simplex-constrained least squares problem, as ensured by the subsequent Lemma 4.\nProblem 7 min d\u2208\u2206m \u2016V d\u2212 u\u20162.\nLemma 4 For any fixed c, the optimal coefficient vector d of Problem 5 can be obtained as the solution of Problem 7.\nOptimal coefficients can be computed using an iterative two-step procedure such as the Algorithm 1, that alternates between minimization with respect to c obtained through the solution of the linear system (13), and the solution of the simplex-constrained least squares Problem 7 in d. The non-negativity constraint induce sparsity in the vector d, and thus also in the input-output kernel expansion. To understand the initialization of coefficients c and d in Algorithm 1, consider the limiting solution of the optimization problem when the regularization parameter tends to infinity. Such solution is the most natural starting point for a regularization path because optimal coefficients can be computed in closed form.\nLemma 5 The limiting solution of Problem 5 when \u03bb\u2192 +\u221e is given by\n(c\u221e, d\u221e) = (0, ei) , i \u2208 arg max k=1,...,m yTRky.\nAs shown in subsection 4.3, the result of Lemma 5 can be also used to give important insights into the choice of the scaling S in the second layer."}, {"heading": "4.1 A Bayesian maximum a posteriori interpretation of RLS2", "text": "The equivalence between regularization Problem 2 and the multiple kernel learning optimization Problem 8 can be readily exploited to give a Bayesian MAP (maximum a posteriori) interpretation of RLS2. To specify the probabilistic model, we need to put a prior distribution over the set of functions from X into R and define the data generation model (likelihood). In the following, N(\u00b5, \u03c32) denote a real Gaussian distribution with mean \u00b5 and variance \u03c32, GM(f,K) a Gaussian measure on the set of functions from X into Rm with mean f and covariance function K, and U(\u2126) the uniform distribution in Rm over a set \u2126 of positive finite measure. Let f : X \u2192 R be such that\nf = wTSf1,\nwhere f1 : X \u2192 Rm is distributed according to a Gaussian measure over H1 with zero mean and (matrix-valued) covariance function K1, and w is a random vector independent of f1, distributed according to an uniform distribution over the ellipsoid ES := {w \u2208 Rm : wTSw \u2264 1}:\nf1 \u223c GM ( 0,K1 ) , w \u223c U(ES).\nRegarding the likelihood, assume that the data set D := {(xi, yi)}`i=1, is generated by drawing pairs (xi, yi) independently and identically distributed according to the usual additive Gaussian noise model:\nyi|(xi, f) \u223c N(f(xi), \u03c32).\nWhen H1 is a finite dimensional space, the MAP estimate of f is:\nf\u2217 = w\u2217TSf\u22171 ,\nwhere (f\u22171 , w \u2217) maximize the posterior density:\np(f |D) \u221d p(D|f)p(f1)p(w).\nSpecifically, we have\np(D|f) \u221d \u220f\u0300\ni=1\nexp ( \u2212 ( yi \u2212 wTSf1(xi) )2\n2\u03c32\n) ,\np(f1) \u221d exp ( \u2212 \u2016f1\u20162H1\n2\n) ,\np(w) \u221d {\n1, x \u2208 ES 0, else .\nIt follows that w\u2217 \u2208 ES and, by taking the negative logarithm of p(f |D), that the MAP estimate coincides with the solution of the regularization Problem 2 with square loss functions and \u03bb = \u03c32. When H1 is an infinite-dimensional function space, the Gaussian measure prior for f1 do not admit a probability density. Nevertheless, the regularization Problem 2 can be still recovered by understanding MAP estimate as a maximal point of the posterior probability measure, as described in [21]."}, {"heading": "4.2 Linear regularized least squares with two layers", "text": "As described in subsection 3.1, when the input set X is a subset of Rm the linear choice of basis kernels (10) produces linear machines with feature selection capabilities. First of all, recall that standard regularized least squares with the linear kernel (9) boils down to finite-dimensional Tikhonov regularization [51] also known as ridge regression [22]:\nmin a\u2208Rm\n( \u2016y \u2212Ha\u20162 + \u03bb\u2016a\u20162 ) ,\nwhere H \u2208 R`\u00d7m denote the matrix of inputs such that Hij = xji . Lemma 6 below states that the linear version of RLS2 is equivalent to a \u201cscaled\u201d ridge regression problem, in which the optimal scaling is also estimated from the data. Let\n\u0393(d) := diag {s1d1, . . . , smdm} . (14)\nFor any fixed d, let n(d) be the number of non-zero coefficients di, \u0393\u0303(d) \u2208 Rn(d)\u00d7n(d) denote the diagonal sub-matrix of \u0393 containing all the strictly positive coefficients. Moreover, let H\u0303 denote the scaled sub-matrix of selected features H\u0303 := H\u0393\u0303.\nProblem 8\nmin z\u0303\u2208Rn(d), d\u2208\u2206m\n\u2016y \u2212 H\u0303z\u0303\u20162 + \u03bb\u2016\u0393\u03031/2(d)z\u0303\u20162, subject to (14).\nLemma 6 When basis kernels are as in (10), the optimal solution of Problem 5 can be written as in (11)-(12), where (z, d) solves Problem 8.\nFrom Problem 8, one can easily see that when d is fixed to its optimal value, the optimal z\u0303 in Problem 8 is given by the familiar expression:\nz\u0303 = ( H\u0303T H\u0303 + \u03bb\u0393\u0303 )\u22121 H\u0303T y.\nThe result of Lemma 6 can be also used to give an interesting interpretation of linear RLS2. In fact, the coefficient sidi can be interpreted as a quantity proportional to the inverse variance of the i-th coefficient zi. Hence, Problem 8 can be seen as a Bayesian MAP estimation with Gaussian residuals, Gaussian prior on the coefficients and uniform prior over a suitable simplex on the vector of inverse coefficients\u2019s variances.\nIt is useful to introduce a notion of \u201cdegrees of freedom\u201d, see e.g. [15, 20]. Degrees of freedom is an index more interpretable than the regularization parameter, and can be also used to choose the regularization parameter according to tuning criteria such as Cp [28], AIC [1], BIC [42], GCV [11]. A general expression for the effective degrees of freedom of non-linear kernel regression methods with one layer, based on the SURE (Stein\u2019s Unbiased Risk Estimator) approximation [48] has been recently derived in [13]. For linear RLS2, the following quantity seems an appropriate approximation of the degrees of freedom:\nd\u0302f(\u03bb) = tr ( H\u0303 ( H\u0303T H\u0303 + \u03bb\u0393\u0303 )\u22121 H\u0303T ) . (15)\nExpression (15) corresponds to the equivalent degrees of freedom of a linear\nregularized estimator with regressors fixed to H\u0303 and diagonal regularization \u03bb\u0393\u0303. Notice that (15) neglects the non-linear dependence of matrix H\u0303 on the output data and does not coincide with the SURE estimator of the degrees of freedom. Nevertheless, the property 0 \u2264 d\u0302f(\u03bb) \u2264 m holds, so that d\u0302f can be conveniently used to interpret the complexity of the linear RLS2 model (see subsection 5.1 for an example)."}, {"heading": "4.3 Choice of the scaling and feature/kernel selection", "text": "The ability of RLS2 to select features or basis kernels is highly influenced by the scaling S in the second layer. In this subsection, we analyze certain scaling rules that are connected with popular statistical indices, often used as \u201cfilters\u201d for feature selection [19]. Since the issue of scaling still needs further investigation, it is not excluded that new rules different from those mentioned in this subsection may work better on specific problems.\nA key observation is the following: according to Lemma 5, RLS2 with heavy regularization favors basis kernels that maximizes the quantity Ak = y\nTRky, that represents a kind of alignment between the kernel Rk and the outputs. This means that RLS2 tends to select kernels that are highly aligned with the outputs. Since each alignment Ak is proportional to the scaling factor sk, an effective way to choose the scaling is one that makes the alignment a meaningful quantity to maximize. First of all, we discuss the choice of scaling for the linear RLS2 algorithm introduces in subsection 4.2. The generalization to the case of non-linear basis kernels easily follows by analyzing the associated feature maps.\nIn the linear case, we have Rk = skx kxkT , where xk is the k-th feature\nvector, so that Ak = sk(y Txk)2.\nBy choosing sk = (\u2016y\u2016\u2016xk\u2016)\u22122,\nthe alignment becomes the squared cosine of the angle between the k-th feature vector and the output vector:\nAk =\n( yTxk\n\u2016y\u2016\u2016xk\u2016\n)2 = cos2 \u03b8k.\nIn particular, when the outputs y and the features xk are centered to have zero mean, Ak coincides with the squared Pearson correlation coefficient between the outputs and the k-th feature, also known as coefficient of determination. This means that RLS2 with heavy regularization selects the features that mostly correlates to the outputs. Since the term \u2016y\u20162 is common to all the factors, one can also use\nsk = \u2016xk\u2016\u22122, (16)\nwithout changing the profile of solutions along a regularization path (though, the scale of regularization parameters is shifted). Observe that rule (16) may also make sense when data are not centered or centered around values other than the mean. In fact, for some datasets, performances are better without any centering (this is the case, for instance, of Experiment 1 in subsection 5.1). Notice also that (16) only uses training inputs whereas, in a possible variation, one can replace xk with the vector containing values of the k-th feature for both training and test data (when available). The latter procedure sometimes work better than scaling using training inputs only, and will be referred to as transductive scaling in the following. For binary classification with labels \u00b11, the choice (16) with or without centering still make sense, but other rules are also possible. Let `+ and `\u2212 denote the number of samples in the positive and negative class, respectively, and mk+ and m k \u2212 denote the within-class mean values of the k-th feature:\nmk+ = 1\n`+\n\u2211\ni:yi=1\nxki , m k \u2212 =\n1\n`\u2212\n\u2211\ni:yi=\u22121 xki .\nBy choosing\nsk = 1\n(\u03c3k+) 2 + (\u03c3k\u2212)2\n, (17)\nwhere \u03c3k+ and \u03c3 k \u2212 denote the within class standard deviations of the k-th feature, one obtain\nAk = (`+m\nk + \u2212 `\u2212mk\u2212)2\n(\u03c3k+) 2 + (\u03c3k\u2212)2\n.\nWhen the two classes are balanced (`+ = `\u2212 = `/2), Ak boils down to a quantity proportional to the classical Fisher criterion (or signal-to-interference ratio):\nAk = `2\n4\n(mk+ \u2212mk\u2212)2\n(\u03c3k+) 2 + (\u03c3k\u2212)2\n.\nRules (16) and (17) can be generalized to the case of non-linear basis kernels, by observing that non-linear kernels can be always seen as linear upon mapping the data in a suitable feature space. A rule that generalizes (16) is the following [?, see e.g.]]Rakotomamonjy08:\nsk =\n(\u2211\u0300\ni=1\nK\u0303k(xi, xi)\n)\u22121 , (18)\nthat amounts to scale each basis kernel by the trace of the kernel matrix, and reduces exactly to (16) in the linear case. Also (18) can be applied with or without centering. A typical centering is normalization in feature space, that amounts to subtract 1/` \u2211 i,j K\u0303k(xi, xj) to the basis kernel K\u0303k, before computing (18). A transductive scaling rule can be obtained by extending the sum to both training and test inputs, namely computing the inverse trace of the overall kernel matrix, as in [27]. Finally, a non-linear generalization of (17) is given by the following:\nsk =\n[ \u2211\u0300\ni:yi=1\n K\u0303k(xi, xi)\n`+ \u2212 \u2211\nj:yj=1\nK\u0303k(xi, xj)\n`2+\n +\n\u2211\u0300\ni:yi=\u22121\n K\u0303k(xi, xi)\n`\u2212 \u2212\n\u2211\nj:yj=\u22121\nK\u0303k(xi, xj)\n`2\u2212\n  ]\u22121\n."}, {"heading": "5 Experiments", "text": "In this section, the behavior of linear and non-linear RLS2 on several learning problems is analyzed. In subsection 5.1, an illustrative analysis of linear RLS2 is proposed, whose goal is to study the feature selection capabilities and the dependence on the regularization parameter of the algorithm in simple experimental settings. RLS2 with non-linear kernels is analyzed in subsection 5.2, where an\nextensive benchmark on several regression and classification problems from UCI repository is carried out. Finally, multi-class classification of microarray data is considered in subsection 5.3.\nComputations are carried out in a Matlab environment and the sub-problem (Problem 7) is solved using an SMO-like (Sequential Minimal Optimization) algorithm [34]. Current implementation features conjugate gradient to solve linear systems and a sophisticated variable shrinking technique to reduce gradient computations. The stopping criterion for Algorithm 1 used in all the experiments is the following test on the normalized residual of linear system (13):\n\u2016 (R+ \u03bbI) c\u2212 y\u2016 \u2264 \u03b4\u2016y\u2016.\nThe choice \u03b4 = 10\u22122 turns out to be sufficient to make all the coefficients stabilize to a good approximation of their final values. A full discussion of optimization details is outside the scope of the paper. All the experiments have been run on a Core 2 Duo T7700 2.4 GHz, 800 MHz FSB, 4 MB L2 cache, 2 GB RAM."}, {"heading": "5.1 Linear RLS2: illustrative experiments", "text": "In this subsection, we perform two experiments to analyze the behavior of linear RLS2. In the first experiment, a synthetic dataset is used to investigate the ability of linear RLS2 to perform feature selection. The dependence of generalization performances of RLS2 and other learning algorithms on the training set size is analyzed by means of learning curves. The goal of the second experiment is to illustrate the qualitative dependence of coefficients on the regularization parameter and give an idea of the predictive potentiality of the algorithm.\nExperiment 1 (Binary strings data) In the first experiment, a synthetic Binary strings dataset has been generated: 250 random binary strings xi \u2208 {0, 1}100 are obtained by independently sampling each bit from a Bernoulli distribution with p = 0.5. Then, the outputs have been generated as\nyi = x 1 i + x 2 i + x 3 i + i,\nwhere i \u223c N(0, \u03c32) are small independent Gaussian noises with zero mean and standard deviation \u03c3 = 0.01. In this way, the outputs only depend on the first three bits of the input binary string. The dataset has been divided into a training set of 150 input output pairs and a test set containing the remaining 100 data pairs. We compare the RMSE (root mean squared error) learning curves obtained by varying the training set size using five different methods:\n1. RLS (regularized least squares) with \u201cideal\u201d kernel:\nK(x1, x2) = x 1 1x 1 2 + x 2 1x 2 2 + x 3 1x 3 2. (19)\n2. RLS with linear kernel (9) (ridge regression).\n3. RLS with Gaussian RBF kernel\nK(x1, x2) = exp\n( \u22120.01\u2016x1 \u2212 x2\u2016 2\n2\n) .\n4. RLS2 with linear basis kernels (10) and scaling (16).\n5. Lasso regression.\nThe goal here is to assess the overall quality of regularization paths associated with different regularization algorithms, independently of model selection procedures. To this end, we compute the RMSE on the test data as a function of the training set size and evaluate the lower bounds of the learning curves with respect to variation of the regularization parameter. Results are shown in Figure 3, whose top plot reports the lower bounds of learning curves for all the five algorithms with training set sizes between 1 and 150. Notice that all the five methods are able to learn, asymptotically, the underlying \u201cconcept\u201d, up to the precision limit imposed by the noise, but methods that exploits coefficients sparsity are faster to reach the asymptotic error rate. Not surprisingly, the best method is RLS with the \u201cideal kernel\u201d (19), which incorporates a strong prior knowledge: the dependence of the outputs on the first three bits only. Though knowing in advance the optimal features is not realistic, this method can be used as a reference. The slowest learning curve is that associated to RLS with Gaussian RBF kernel, which only incorporates a notion of smoothness. A good compromise is RLS with linear kernel, which uses the knowledge of linearity of the underlying function, and reaches a good approximation of the asymptotic error rate after seeing about 100 strings. The remaining two methods (Lasso and linear RLS2) incorporate the knowledge of both linearity and sparsity. They are able to learn the underlying concept after seeing only 12 examples, despite the presence of the noise. Since after the 12-th example Lasso and linear RLS2 are basically equivalent, is it interesting to see what happen for very small sample sizes. The bottom plot of Figure 3 is a zoomed version of the top plot with training set sizes between 1 and 30, showing only the learning curves for the three methods that impose sparsity. Until the 8-th example, the Lasso learning curve stays lower than the RLS2 learning curve. After the 8-th example, the RLS2 learning curve stays uniformly lower than the Lasso, indicating an high efficiency in learning noisy sparse linear combinations. Since the multiple kernel learning interpretation of RLS2 suggests that the algorithm is being learning the \u201cideal\u201d kernel (19) simultaneously with the predictor, it might be interesting to analyze the asymptotic values of kernel coefficients di. Indeed, after the first 12 training examples, RLS2 sets to zero all the coefficients di except the first three, which are approximately equal to 1/3.\nExperiment 2 (Prostate Cancer data) Linear RLS2 is applied to the Prostate Cancer dataset, a regression problem whose goal is to predict the level of prostate-specific antigen on the basis of a number of clinical measures in men who were about to receive a radical prostatectomy [47]. These data\nare used in the textbook [20] to compare different feature selection and shrinkage methods, and have been obtained from the web site http://www-stat. stanford.edu/ElemStatLearn/. Data have been preprocessed by normalizing all the inputs to zero mean and unit standard deviation. The dataset is divided into a training set of 67 examples and a test set of 30 examples. To choose the regularization parameter, the 10-fold cross-validation score has been computed for different values of \u03bb in the interval [ 10\u22124, 104 ] on a logarithmic scale. The scaling coefficients si are chosen as in (16), thus normalizing each training feature to have unit norm. An intercept term equal to the average of training outputs has been subtracted to the outputs before estimating the other coefficients. For each of the dataset splits, the MSE (mean squared error) has been computed on the validation data. Figure 2 reports average and standard error bands for validation MSE along a regularization path. Following [20], we pick the value of \u03bb corresponding to the least complex model within one standard error of the best validation score.\nIn a second phase, the whole training set (67 examples) is used to compute the RLS2 solution with different values of \u03bb. Figure 3 reports the profile of RLS2 coefficients aj , see equation (12), along the whole regularization path as a function of the degrees of freedom defined as in (15). RLS2 does a continuous feature selection that may resemble that of the Lasso. However, the dependence of coefficients on the regularization parameter is rather complex and the profile in Figure 2 is not piecewise linear. In correspondence with the value of \u03bb chosen in the validation phase, RLS2 selects 5 input variables out of 8. Table 1 reports the value of coefficients estimated by RLS2 together with the test error and his standard error. For comparison, Table 1 also reports models and results taken from [20] associated with LS (Least Squares), Best subset regression, Ridge Regression, Lasso regression, PCR (Principal Component Regression), PLS (Partial Least Squares). The best model on these data is PCR, but RLS2 achieves the second lowest test error by using only 5 variables."}, {"heading": "5.2 Non-linear RLS2: regression and classification benchmark", "text": "In this subsection, benchmark experiments on four regression and six classification problems from UCI repository are illustrated (Table 2). RLS2 has been run on 100 random dataset splits with two different training/test ratios: 60/40 and 70/30. For each dataset split, an approximate regularization path with 30 values of \u03bb on a logarithmic scale in the interval [ 10\u22126, 106 ] has been computed. To speed-up the regularization path computation, a warm-start technique is employed: the value of \u03bb is iteratively decreased, while kernel-expansion coefficients di are initialized to their optimal values obtained with the previous value of \u03bb. Performances are measured by accuracy for classification and RMSE (root mean squared error) for regression. For each dataset split and value of the regularization parameter, the following quantities are computed: prediction performance on the test set, number of selected kernels (number of non-zero di), training time in seconds and number of iterations to compute the whole regularization\npath. Datasets have been pre-processed by removing examples with missing features and converting categorical features to binary indicators. For some of the datasets (see Table 2) input features have been standardized to have zero mean and unitary standard deviation. For classification, output labels are \u00b11 and predictions are given by the sign of (f2 \u25e6 f1). For regression, an intercept term equal to the mean of training outputs is subtracted to the training data. Basis kernel matrices are pre-computed and the scaling matrix S is chosen according to the rule (18) with transductive scaling.\nTo better compare the results to similar benchmarks for multiple kernel learning, see e.g. [35], the same set of basis kernels for all the datasets has been chosen. We remark that such agnostic approach is not representative of a realistic application of the algorithm, in which the choice of basis kernels K\u0303k should reflect a-priori knowledge about the learning task to be solved. The set of basis kernels contains the following:\n\u2022 Polynomial kernels K\u0303k(x1, x2) = (1 + x T 1 x2) n\nwith n = 1, 2, 3.\n\u2022 Gaussian RBF kernels\nK\u0303k(x1, x2) = exp ( \u2212\u03b3\u2016x1 \u2212 x2\u20162 ) ,\nwith 10 different values of \u03b3 chosen on a logarithmic scale between 10\u22123 and 103.\nKernels on each single feature and on all the features are considered, so that the number of basis kernels is an affine function of the number of features (recall that categorical features have been converted to binary indicators). More precisely, we have m = 13(N + 1).\nAll the profiles of test prediction performance, number of kernels and number of iterations for the 70/30 dataset split in correspondence with different values of the regularization parameter are reported in Figures 4-8. From the top plots, it can be seen that test performances are relatively stable to variations of the regularization parameter around the optimal value \u03bb\u2217, indicating that RLS2 is robust with respect to the use of different model selection procedures. For regression datasets such as Cpu, Servo, or Housing, optimal performances seems to be reached in correspondence with the un-regularized solution \u03bb \u2192 0+. Lines in light color are associated with single dataset splits, while thick lines are the averages over different dataset splits. The vertical dotted line corresponds to the value of the regularization parameter with best average test performance. The average number of selected kernels vary quite smoothly with respect to the regularization parameter. For large values of \u03bb, RLS2 chooses only one basis kernel. For small values of \u03bb, the number of selected kernels grows and exhibits an higher variability. The bottom plots in Figures 4-8 give an idea of the computation burden required by alternate optimization for RLS2 in correspondence with different values of \u03bb. In correspondence with high values\nof the regularization parameter, the algorithm converges in a single iteration. This occurs also for the very first value on the regularization path, meaning that the initialization rule is effective. With low values of \u03bb, RLS2 also converges in a single iteration, since the second layer doesn\u2019t change much from an iteration to the next.\nTest performances for regression and classification are summarized in Table 3, where the average and standard deviation with respect to the 100 dataset splits of either RMSE (regression) or accuracy (classification) in correspondence with to the best value of \u03bb are reported. Performances of other kernel learning algorithms on some of these datasets can be found in [27, 33, 35] and references therein. Another benchmark study that might be useful for comparison is [29]. Comparisons should be handled with care due to the use of different experimental procedures and optimization problems. For instance, [27] uses an 80/20 dataset split ratio, [33] uses 60/40, while [35] uses 70/30. Also, different numbers of dataset splits have been used. Individuating what kind of datasets are better suited to what algorithm is a complex issue, that is certainly worth further investigation. These experiments shows that RLS2 results are competitive and complexity of the model is well controlled by regularization. In particular, state of the art performances are reached on Servo, Housing, Hearth, Pima, Ionosphere. Finally, it should be observed that, although multiple kernel learning machines have been used as black box methods, the use of basis kernels on single features sometimes also selects a subset of relevant features. Such property is remarkable since standard single-layer kernel methods are not able to perform \u201cembedded\u201d feature selection.\nTable 4 reports the average and standard deviation of number of selected kernels in correspondence with the optimal value of \u03bb, number of iterations and training time needed to compute a regularization path for all the regression and classification datasets studied in this subsection. From the columns of selected kernels, it can be seen that a considerable fraction of the overall number of basis kernels is filtered out by the algorithm in correspondence with the optimal value of the regularization parameter. By looking at the number of iterations needed to compute the path with 30 values of the regularization parameter, one can see that the average number of iterations to compute the solution for a single value of \u03bb is in between 1 and 3, indicating that the warm-start procedure is rather effective at exploiting the continuity of solutions with respect to the regularization parameter. As a matter of fact, most of the optimization work is spent in correspondence with a central interval of values of \u03bb, as shown in the bottom plots of Figures 4-8. Finally, from the last column, reporting average and standard deviation of training times, it can be seen that, with the current implementation of RLS2, regularization paths for all the datasets in this subsection can be computed in less than one minute in the average (see the introduction of this section for experimental details). Although the current implementation of RLS2 has been designed to be efficient, it is believed that there\u2019s still considerable margin for further computational improvements. This issue may well be the subject of future developments."}, {"heading": "5.3 RLS2: multi-class classification of microarray data", "text": "RLS2 can be applied to multi-class classification problems by solving several binary classification problems and combining their outcomes. A possible way to combine binary classifiers is the OVA (one versus all) approach, in which each class is compared to all the others and test labels are assigned to the class maximizing the confidence (the real-valued output) of the corresponding binary classifier.\nLinear RLS2 with OVA has been applied to the 14 Cancers dataset [36], a delicate multi-class classification problem whose goal is to discriminate between 14 different types of cancer, on the basis of microarray measurements of 16063 gene expressions. Gene measurements and type of cancer (labels) are available for 198 patients, the dataset being already divided in a training set of 144 patients, and a test set of 54 patients. Another important goal in this problem is to individuate a small subset of genes which is relevant to discriminate between the different kind of cancer. [20] reports several results for these data using a variety of classification methods. Algorithms such as the Support Vector Classifier uses all the genes to compute the classification boundaries, while others\nsuch as Lasso or Elastic Net are also able to select a subset of relevant genes. Since the feature selection experiment in subsection 5.1 suggests that RLS2 may be very efficient at selecting relevant features from noisy examples, a microarray dataset seems to be an appropriate choice for testing the algorithm.\nGene expressions for each patient have been firstly standardized to have zero mean and variance one. For each binary classifier, coefficients si are chosen as si = ( \u03c32+ + \u03c3 2 \u2212 )\u22121/2 , where \u03c32+ and \u03c3 2 \u2212 are the within-class sample variances computed using all the training data. Such scaling gives more weight to genes whose expressions exhibits small within-class variability, and seems to slightly improve classification performances. A validation accuracy profile has been computed using stratified 8-fold cross validation, where the folds are organized to approximately preserve the class proportions1. For the final model, we pick the highest value of \u03bb maximizing the validation accuracy. Figure 9 reports the profiles of training accuracy, cross-validation accuracy with corresponding standard error bands, and test accuracy for 50 logarithmically spaced values of the regularization parameter. Table 5 reports the number of test errors and selected genes in correspondence with the value of \u03bb chosen in the validation phase, for RLS2 and other methods from [20]. Test errors in Table 5 are averages of test errors for different classifiers associated with all the different values of the regularization parameter that maximizes the cross-validation score (this explains the presence of non-integer values). For linear RLS2, such procedure yields a value of about 9.8. In correspondence with the least complex model maximizing the cross-validation accuracy, one obtain 10 test errors using 855 genes. Although the test set size is too small to draw significative conclusions from this comparison, linear RLS2 seems to work rather well on this problem and achieve the best test performances. Such good performance also confirm effectiveness of the OVA multi-class approach for RLS2.\n1We thank Trevor Hastie for kindly providing the folds used in their experiments."}, {"heading": "6 Conclusions", "text": "The connection between learning with a two-layer network and the problem of learning the kernel has been analyzed. While architectures with more than one layer are justified by a representer theorem, an alternative perspective to look at the problem of kernel learning is proposed. Such perspective makes clear that these two methodologies aim both at increasing the approximation power of standard single layer methods by using machines that can adaptively select functions with a variety of shapes when little prior knowledge is available. In particular, the multiple kernel learning framework is shown to be an important specific case of a more general two-layer architecture. We also introduce RLS2, a new method to perform multiple kernel learning based on regularization with the square loss function and alternate optimization. RLS2 exhibits state of the art performances on several learning problems, including multi-class classification of microarray data. An open source set of MATLAB scripts for RLS2 and linear RLS2 is available at http://www.mloss.org and also includes a Graphic User Interface."}, {"heading": "Appendix (proofs)", "text": "Proof of Theorem 1 For any fixed f1, let zi := f1(xi). By fixing an optimal f1, Problem 1 can be written as a function of f2 alone as\nmin f2\u2208H2\n(\u2211\u0300\ni=1\nLi (f2(zi)) +R2(\u2016f2\u2016H2)\n) .\nBy standard representer theorems for vector valued functions (see [31] and the remark on monotonicity in [40] after Theorem 1), there exists an optimal f2 in the form\nf2(z) = \u2211\u0300\ni=1\nK2(zi, z)bi.\nThen, by fixing on optimal f2 in this form, Problem 1 can be written as a function of f1 alone as\nmin f1\u2208H1\n(\u2211\u0300\ni=1\nL\u0303i (f1(xi)) +R1(\u2016f1\u2016H1)\n) ,\nwhere L\u0303i (z) := Li (f2(z)). Notice that the new loss functions L\u0303i depends on f2. Again, by the single-layer representer theorem the finite kernel expansion for f1 follows. Finally, it is immediate to see that the overall input-output relation f2 \u25e6 f1 can be written as in (2). Proof of Lemma 1\nBy linearity of f2, it is immediate to see that (3) holds. It follows that\n\u2211\u0300\ni=1\nLi ((f2 \u25e6 f1)(xi)) = \u2211\u0300\ni=1\nLi ((f \u2217 2 \u25e6 f\u22171 )(xi)) .\nIn addition,\n\u03b1 2 \u2016f\u22171 \u20162H1 = \u03b1 \u03b222 \u2016f1\u20162H1 , \u03b3 \u00b7 I ( \u2016f\u22172 \u2016H2 \u03b2 ) = I (\u2016f2\u2016H2) .\nIn the last equation, we exploit the fact that \u03b3I(x) = I(x), for any positive \u03b3, a property that is satisfied only by indicator functions. The thesis follows by letting \u03bb = \u03b1/\u03b22. Proof of Theorem 2\nProblem 2 is a specific instance of Problem 1. The functional to minimize is bounded below, lower semi-continuous and radially-unbounded with respect to (f1, f2). Existence of minimizers follows by weak-compactness of the unit ball in H1 and H2. By Theorem 1, there exists an optimal f1 in the form\nf1(x) = \u2211\u0300\nj=1\nK1(x, xj)aj = \u2211\u0300\nj=1\ndiag { K\u03031(x, xj), . . . , K\u0303m(x, xj) } aj .\nIntroduce the matrix A \u2208 Rm\u00d7` whose rows are denoted by (ci)T and whose columns are aj . Then, the i-th component of f1 can be written as:\nf i1(x) = \u2211\u0300\nj=1\ncijK\u0303i(x, xj).\nBy Theorem 1, there exists an optimal f2 such that\nf2(z) = \u2211\u0300\nj=1\nbjK 2(f1(xj), z) =\n\u2211\u0300\nj=1\nbjz TSf1(xj) = z TS \u2211\u0300\nj=1\nbjf1(xj) = z TSw,\nwhere\nw := \u2211\u0300\nj=1\nbjf1(xj).\nLetting matrices Rk \u2208 R`\u00d7` like in (6) and Q as in (4), Problem 2 can be rewritten as\nmin c1,...cm\u2208R`,w\u2208Rm\n[ Q ( m\u2211\nk=1\nwkR kck ) + \u03bb\n2\nm\u2211\nk=1\n(ckTRkck)\nsk\n] , subject to wTSw \u2264 1.\nBy optimizing with respect to vectors ci, we have\n0 \u2208 \u2212siwiRi\u2202Q\n( m\u2211\nk=1\nwkR kck ) + \u03bbRici,\nwhere \u2202 is the sub-differential of a convex function [39]. Now, letting\nc \u2208 1 \u03bb \u2202Q\n( m\u2211\nk=1\nwkR kck ) ,\nwe obtain ci = siwic.\nLetting di := siw 2 i and R := \u2211m i=1 diR\ni, Problem 2 boils down to Problem 3. By Theorem 1 again, the overall input-output relation can be written as in equation (2), where the kernel K satisfy\nK(x1, x2) = K 2(f1(x1), f1(x2)) = f1(x1) TSf1(x2) =\nm\u2211\ni=1\nsif i 1(x1)f i 1(x2)\n=\nm\u2211\ni=1\nsiw 2 i\n\u2211\u0300\nj1=1\n\u2211\u0300\nj2=1\ncj1cj2K\u0303i(xj1 , x1)K\u0303i(xj2 , x2)\n=\nm\u2211\ni=1\ndiKi(x1, x2),\nand Ki are as in (5). Proof of Lemma 2 Problem 4 can be rewritten as\nmin z\u2208R`,R\u2208S+m\n( Q(z) + \u03bb\n2 zTR\u2020z\n) , (20)\nsubject to (6), where S+m denotes the cone of m \u00d7 m positive semi-definite matrices. This problem can be seen to be jointly convex in (z,R) using an argument due to [25]: the term zTR\u2020z is a matrix-fractional function (see e.g. [9], Example 3.4), which is a jointly convex function of the pair (z,R). This easily follows by noticing that its epi-graph is a convex set:\nzTR\u2020z \u2264 \u03b1 \u21d4 ( \u03b1 zT\nz R\n) \u2208 S+m+1.\nSince Q is a convex function, the overall functional (20) is convex. Minimization in (20) subject to linear constraints (6) is thus a convex optimization problem. Since R is a linear function of d, Problem 4 is also convex.\nTo prove (8), assume that (z\u2217, d\u2217) is an optimal pair for Problem 4. Without loss of generality, we can assume d\u2217 6= 0. Indeed, if there\u2019s an optimal solution with d\u2217 = 0, then z = 0, d 6= 0 is optimal as well. Now, let \u03b3 := \u2211m i=1 d \u2217 i , and notice that 0 < \u03b3 \u2264 1. Introducing the new pair (z, d) = (z\u2217, d\u2217/\u03b3), the value of the objective functional in correspondence with (z, d) is\nQ(z) + \u03bb 2 zTR\u2020(d)z = Q(z\u2217) + \u03bb\u03b3 2 (z\u2217)TR\u2020(d\u2217)z\u2217 \u2264 Q(z\u2217) + \u03bb 2 (z\u2217)TR\u2020(d\u2217)z\u2217,\nso that the new pair is optimal as well. Proof of Lemma 3\nBy Lemma 2, minimization with respect to d can be restricted to the standard simplex \u2206m. For any fixed d, the functional of Problem 5 is a convex quadratic function of c. If c\u2217(d) satisfy equation (13), then the gradient of the objective functional with respect to c is zero in c\u2217, meaning that c\u2217 is optimal. Dropping the dependence on d, equation (13) can be rewritten as\ny \u2212Rc\u2217 = \u03bbc\u2217.\nIn correspondence with such optimal c\u2217, we have\n1 2 \u2016y \u2212Rc\u2217\u20162 + \u03bb 2 c\u2217TRc\u2217 =\n\u03bb2\n2 \u2016c\u2217\u20162 + \u03bb 2 c\u2217T (y \u2212 \u03bbc\u2217) = \u03bb 2 yT c\u2217.\nProof of Lemma 4 By Lemma 2, minimization with respect to d can be restricted to the standard simplex \u2206m. In addition, we have\n1 2 \u2016y \u2212Rc\u20162 + \u03bb 2 cTRc = 1 2 \u2225\u2225\u2225\u2225u\u2212Rc+ \u03bbc 2 \u2225\u2225\u2225\u2225 2 + \u03bb 2 cTRc\n= 1 2 \u2016u\u2212Rc\u20162 + \u03bb 2 cT ( \u03bb 2 c+ u ) = 1 2 \u2016u\u2212Rc\u20162 + \u03bb 2 cT y,\nwhere \u03bbcT y/2 does not depend on R (and thus does not depend on d). Now, recalling that\nR(d) =\nm\u2211\ni=1\ndiR i,\nwe have\nRc =\nm\u2211\ni=1\ndiR ic =\nm\u2211\ni=1\ndivi = V d.\nProof of Lemma 5 From equation (13), we have\nc\u221e = lim \u03bb\u2192+\u221e\n(R(d) + \u03bbI) \u22121 y = 0.\nSince R(d) is a continuous function of d defined over the compact set \u2206m, by fixing any matrix norm \u2016 \u00b7 \u2016 there exists a sufficiently large \u03bb such that\nmax d\u2208\u2206m\n\u2016R(d)\u2016 < \u03bb.\nFor \u03bb > \u03bb, the expansion\n(R(d)/\u03bb+ I) \u22121 = I \u2212R(d)/\u03bb+ o(1/\u03bb2),\nholds. By Lemma 3, it follows that\nd\u2217(\u03bb) = arg min d\u2208\u2206m yT (R(d)/\u03bb+ I) \u22121 y\n= arg min d\u2208\u2206m\n[ \u03bb\u2016y\u20162/2\u2212 (yTR(d)y) + o(1/\u03bb) ]\n= arg min d\u2208\u2206m\n[ \u2016y\u20162/2\u2212 (yTR(d)y)/\u03bb+ o(1/\u03bb2) ]\n= arg max d\u2208\u2206m\n[ yTR(d)y \u2212 o(1/\u03bb) ] .\nHence, d\u221e = lim\u03bb\u2192+\u221e d\u2217(\u03bb) solves the following linear program\nmax d\u2208\u2206m\nm\u2211\ni=1\ndi(y TRiy).\nThen, it is easy to see that d = ek, k \u2208 arg maxi=1,...,m yTRiy, is an optimal solution of the linear program, where k is any index maximizing the \u201ckernel alignment\u201d yTRiy. Proof of Lemma 6\nWhen basis kernel are chosen as in (10), f(x) can be written as in (11)-(12), and we have\nR(d) = H\u0393HT .\nBy letting z := HT c, it follows that\nRc = H\u0393z = H\u0303z\u0303\ncTRc = cTH\u0393HT c = \u2016\u03931/2z\u20162 = \u2016\u0393\u03031/2z\u0303\u20162\nHence, Problem 5 reduces to Problem 8."}], "references": [{"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Akaike"], "venue": "B. N. Petrov and F. Cs\u00e1ki, editors, Second International Symposium on Information Theory. Acad\u00e9miai Kiad\u00f3, Budapest,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1973}, {"title": "A DC-programming algorithm for kernel selection", "author": ["A. Argyriou", "R. Hauser", "C.A. Micchelli", "M. Pontil"], "venue": "ICML \u201906: Proceedings of the 23rd international conference on Machine learning, pages 41\u201348, New York, NY, USA,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning convex combinations of continuously parameterized basic kernels", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil"], "venue": "Proc. Conf. on Learning Theory (COLT\u201905),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "When there is a representer theorem? Vector versus matrix regularizers", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, 10:2507\u20132529,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Theory of reproducing kernels", "author": ["N. Aronszajn"], "venue": "Transactions of the American Mathematical Society, 68(3):337\u2013404,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1950}, {"title": "Consistency of the Group Lasso and multiple kernel learning", "author": ["F.R. Bach"], "venue": "Journal of Machine Learning Research, 9:1179\u20131225,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiple kernel learning, conic duality, and the SMO algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "ICML \u201904: Proceedings of the twenty-first international conference on Machine learning, page 6, New York, NY, USA,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Scaling learning algorithms towards AI", "author": ["Y. Bengio", "Y. LeCun"], "venue": "L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large-Scale Kernel Machines. MIT Press,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Asymptotic analysis of penalized likelihood and related estimators", "author": ["D. Cox", "F. O\u2019 Sullivan"], "venue": "The Annals of Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "Smoothing noisy data with spline functions", "author": ["P. Craven", "G. Wahba"], "venue": "Numerische Mathematik, 31:377\u2013403,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1979}, {"title": "Some properties of regularized kernel methods", "author": ["E. De Vito", "L. Rosasco", "A. Caponnetto", "M. Piana", "A. Verri"], "venue": "Journal of Machine Learning Research, 5:1363\u2013 1390,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "An algebraic characterization of the optimum of regularized kernel methods", "author": ["F. Dinuzzo", "G. De Nicolao"], "venue": "Machine Learning, 74(3):315\u2013345,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "On the representer theorem and equivalent degrees of freedom of SVR", "author": ["F. Dinuzzo", "M. Neve", "G. De Nicolao", "U.P. Gianazza"], "venue": "Journal of Machine Learning Research, 8:2467\u20132495,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "The estimation of prediction error: Covariance penalties and crossvalidation", "author": ["B. Efron"], "venue": "Journal of the American Statistical Association, 99(14):619\u2013632,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research, 9:1871\u20131874,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Multicategory proximal support vector machine classifiers", "author": ["G.M. Fung", "O.L. Mangasarian"], "venue": "Machine Learning, 59(1-2):77\u201397,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Regularization theory and neural networks architectures", "author": ["F. Girosi", "M. Jones", "T. Poggio"], "venue": "Neural Computation, 7(2):219\u2013269,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "Feature Extraction: Foundations and Applications (Studies in Fuzziness and Soft Computing)", "author": ["I. Guyon", "S. Gunn", "M. Nikravesh", "L.A. Zadeh"], "venue": "Springer- Verlag New York, Inc., Secaucus, NJ, USA,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "The Elements of Statistical Learning", "author": ["T.J. Hastie", "R.J. Tibshirani", "J. Friedman"], "venue": "Data Mining, Inference and Prediction. Springer-Verlag, Canada, 2nd edition,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximate maximum a posteriori with Gaussian process priors", "author": ["M. Hengland"], "venue": "Constructive Approximation, 26:205\u2013224,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Ridge regression: biased estimation for nonorthogonal problems", "author": ["A.E. Hoerl", "R. Kennard"], "venue": "Technometrics, 12:55\u201367,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1970}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["C. Hsieh", "K.W. Chang", "C.J. Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "Proceedings of the Twenty Fifth International Conference on Machine Learning (ICML), pages 408\u2013 415, Helsinki, Finland,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), pages 217\u2013226, Philadelphia, PA, USA,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning the kernel via convex optimization", "author": ["S.J. Kim", "A. Zymnis", "A. Magnani", "K. Koh", "S. Boyd"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 1997\u20132000, April", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Some results on Tchebycheffian spline functions", "author": ["G. Kimeldorf", "G. Wahba"], "venue": "Journal of Mathematical Analysis and Applications, 33(1):82\u201395,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1971}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "L. El Ghaoui", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, 5:27\u201372,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Some comments on Cp", "author": ["C. Mallows"], "venue": "Technometrics, 15:661\u2013675,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1973}, {"title": "The support vector machine under test", "author": ["D. Meyer", "F. Leisch", "K. Hornik"], "venue": "Neurocomputing, 55(1-2):169\u2013186,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning the kernel function via regularization", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, 6:1099\u20131125,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "On learning vector-valued functions", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Neural Computation, 17:177\u2013204,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Feature space perspectives for learning the kernel", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Machine Learning, 66:297\u2013319,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning the kernel with hyperkernels", "author": ["C.S. Ong", "A.J. Smola", "R.C. Williamson"], "venue": "Journal of Machine Learning Research, 6:1043\u20131071,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J. Platt"], "venue": "B. Sch\u00f6lkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press, Cambridge (MA),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1998}, {"title": "SimpleMKL", "author": ["A. Rakotomamonjy", "F.R. Bach", "S. Canu", "Y. Grandvalet"], "venue": "Journal of Machine Learning Research, 9:2491\u20132521,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiclass cancer diagnosis using tumor gene expression signatures", "author": ["S Ramaswamy", "P Tamayo", "R Rifkin", "S Mukherjee", "C H Yeang", "M Angelo", "C Ladd", "M Reich", "E Latulippe", "J P Mesirov", "T Poggio", "W Gerald", "M Loda", "E S Lander", "T R Golub"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, 98:15149\u201315154,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2001}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "The MIT Press,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularized least squares classification", "author": ["R. Rifkin", "G. Yeo", "T. Poggio"], "venue": "Suykens, Horvath, Basu, Micchelli, and Vandewalle, editors, Advances in Learning Theory: Methods, Model and Applications, volume 190 of NATO Science Series III: Computer and Systems Sciences, chapter 7, pages 131\u2013154. VIOS Press, Amsterdam,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": "Princeton University Press, Princeton, NJ, USA,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1970}, {"title": "A generalized representer theorem", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": "Neural Networks and Computational Learning Theory, 81:416\u2013426,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "(Adaptive Computation and Machine Learning). The MIT Press,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2001}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The Annals of Statistics, 6:461\u2013464,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1978}, {"title": "A quantitative theory of immediate visual recognition", "author": ["T. Serre", "G. Kreiman", "M. Kouh", "C. Cadieu", "U. Knoblich", "T. Poggio"], "venue": "Progress in Brain Research, Computational Neuroscience: Theoretical Insights into Brain Function, 165:33\u201356,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "PEGASOS: Primal Estimated sub- GrAdient SOlver for Svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "ICML \u201907: Proceedings of the 24th international conference on Machine learning, pages 807\u2013814, New York, NY, USA,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge University Press,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2004}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research, 7:1531\u20131565,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2006}, {"title": "Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate ii radical prostatectomy treated patients", "author": ["T. Stamey", "J. Kabalin", "J. McNeal", "I. Johnstone", "F. Freiha", "E. Redwine", "N. Yang"], "venue": "Journal of Urology, 16:1076\u20131083,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1989}, {"title": "Estimation of the mean of a multivariate normal distribution", "author": ["C. Stein"], "venue": "The Annals of Statistics, 9:1135\u20131151,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1981}, {"title": "Sparseness of support vector machines", "author": ["I. Steinwart"], "venue": "Journal of Machine Learning Research, 4:1071\u20131105,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2003}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B, 58(1):267\u2013288,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1996}, {"title": "Solutions of Ill Posed Problems", "author": ["A.N. Tikhonov", "V.Y. Arsenin"], "venue": "W. H. Winston, Washington, D. C.,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1977}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "Wiley, New York, NY, USA,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1998}, {"title": "Kernel extrapolation", "author": ["S.V.N. Vishwanathan", "K.M. Borgwardt", "O. Guttman", "A.J. Smola"], "venue": "Neurocomputing, 69(7-9):721\u2013729,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2006}, {"title": "Spline Models for Observational Data", "author": ["G. Wahba"], "venue": "SIAM, Philadelphia, USA,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1990}, {"title": "Multi-kernel regularized classifiers", "author": ["Q. Wu", "Y. Ying", "D. Zhou"], "venue": "Journal of Complexity, 23(1):108\u2013134,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 53, "context": "Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods.", "startOffset": 187, "endOffset": 211}, {"referenceID": 17, "context": "Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods.", "startOffset": 187, "endOffset": 211}, {"referenceID": 40, "context": "Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods.", "startOffset": 187, "endOffset": 211}, {"referenceID": 51, "context": "Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods.", "startOffset": 187, "endOffset": 211}, {"referenceID": 44, "context": "Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods.", "startOffset": 187, "endOffset": 211}, {"referenceID": 36, "context": "Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods.", "startOffset": 187, "endOffset": 211}, {"referenceID": 4, "context": "These results are usually presented within the theory of RKHS [5], and formalize the intuition that optimal learning machines trained with a finite number of data must be expressed by \u2217Francesco Dinuzzo is with Department of Mathematics, University of Pavia, Pavia, Italy e-mail: francesco.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 39, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 48, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 11, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 30, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 52, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 13, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 3, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 25, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 132, "endOffset": 136}, {"referenceID": 7, "context": "Such extension is also suggested by complexity theory of circuits [8] as well as by biological motivated learning models, [43].", "startOffset": 66, "endOffset": 69}, {"referenceID": 42, "context": "Such extension is also suggested by complexity theory of circuits [8] as well as by biological motivated learning models, [43].", "startOffset": 122, "endOffset": 126}, {"referenceID": 6, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 26, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 32, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 29, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 2, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 54, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 31, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 32, "context": "A major extension to the framework of classical kernel methods, based on the concept of hyper-kernels, has been introduced in [33], encompassing many convex kernel learning algorithms.", "startOffset": 126, "endOffset": 130}, {"referenceID": 26, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 24, "endOffset": 32}, {"referenceID": 29, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 24, "endOffset": 32}, {"referenceID": 1, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 88, "endOffset": 106}, {"referenceID": 45, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 88, "endOffset": 106}, {"referenceID": 31, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 88, "endOffset": 106}, {"referenceID": 34, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 88, "endOffset": 106}, {"referenceID": 5, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 88, "endOffset": 106}, {"referenceID": 45, "context": "Along the line of recent advances in multiple kernel learning [46, 35, 6], it is shown that the involved optimization can be efficiently carried out using a two-step procedure.", "startOffset": 62, "endOffset": 73}, {"referenceID": 34, "context": "Along the line of recent advances in multiple kernel learning [46, 35, 6], it is shown that the involved optimization can be efficiently carried out using a two-step procedure.", "startOffset": 62, "endOffset": 73}, {"referenceID": 5, "context": "Along the line of recent advances in multiple kernel learning [46, 35, 6], it is shown that the involved optimization can be efficiently carried out using a two-step procedure.", "startOffset": 62, "endOffset": 73}, {"referenceID": 30, "context": "Introduce two RKHS H1 and H2 of vector-valued functions [31] defined over X and Z respectively, with (operator-valued) kernel functions K and K, and consider the following problem:", "startOffset": 56, "endOffset": 60}, {"referenceID": 39, "context": "Problem 1 is outside the scope of standard representer theorems [40] due to the presence of the composition (f2 \u25e6 f1).", "startOffset": 64, "endOffset": 68}, {"referenceID": 0, "context": "Introduce the indicator function I of the interval [0, 1] defined as", "startOffset": 51, "endOffset": 57}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Second, linear machines can be trained using extremely efficient and scalable algorithms [23, 44, 16].", "startOffset": 89, "endOffset": 101}, {"referenceID": 43, "context": "Second, linear machines can be trained using extremely efficient and scalable algorithms [23, 44, 16].", "startOffset": 89, "endOffset": 101}, {"referenceID": 15, "context": "Second, linear machines can be trained using extremely efficient and scalable algorithms [23, 44, 16].", "startOffset": 89, "endOffset": 101}, {"referenceID": 49, "context": "From the user\u2019s point of view, linear kernel machines with two layers behave similarly to sparse `1 regularization methods such as the Lasso [50], performing feature selection by varying with continuity a shrinking parameter.", "startOffset": 141, "endOffset": 145}, {"referenceID": 45, "context": "For instance, from the results of the previous section it follows that the two-layer version of standard Support Vector Machines with \u201chinge\u201d loss functions Li(z) = (1\u2212 yiz)+ is equivalent to the SILP (Semi-Infinite Linear Programming) multiple kernel learning problem studied in [46], whose solution can be computed, for instance, by using gradient descent or SimpleMKL [35].", "startOffset": 280, "endOffset": 284}, {"referenceID": 34, "context": "For instance, from the results of the previous section it follows that the two-layer version of standard Support Vector Machines with \u201chinge\u201d loss functions Li(z) = (1\u2212 yiz)+ is equivalent to the SILP (Semi-Infinite Linear Programming) multiple kernel learning problem studied in [46], whose solution can be computed, for instance, by using gradient descent or SimpleMKL [35].", "startOffset": 371, "endOffset": 375}, {"referenceID": 37, "context": "Indeed, generalization performances of regularized least squares classifiers have been shown to be comparable to that of Support Vector Machines on many dataset, see [38, 17] and references therein.", "startOffset": 166, "endOffset": 174}, {"referenceID": 16, "context": "Indeed, generalization performances of regularized least squares classifiers have been shown to be comparable to that of Support Vector Machines on many dataset, see [38, 17] and references therein.", "startOffset": 166, "endOffset": 174}, {"referenceID": 45, "context": "Along the lines of recent developments in multiple kernel learning optimization [46, 35], we propose a two-step minimization procedure that alternates between kernel and predictor optimization.", "startOffset": 80, "endOffset": 88}, {"referenceID": 34, "context": "Along the lines of recent developments in multiple kernel learning optimization [46, 35], we propose a two-step minimization procedure that alternates between kernel and predictor optimization.", "startOffset": 80, "endOffset": 88}, {"referenceID": 20, "context": "Nevertheless, the regularization Problem 2 can be still recovered by understanding MAP estimate as a maximal point of the posterior probability measure, as described in [21].", "startOffset": 169, "endOffset": 173}, {"referenceID": 50, "context": "First of all, recall that standard regularized least squares with the linear kernel (9) boils down to finite-dimensional Tikhonov regularization [51] also known as ridge regression [22]:", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "First of all, recall that standard regularized least squares with the linear kernel (9) boils down to finite-dimensional Tikhonov regularization [51] also known as ridge regression [22]:", "startOffset": 181, "endOffset": 185}, {"referenceID": 14, "context": "[15, 20].", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[15, 20].", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "Degrees of freedom is an index more interpretable than the regularization parameter, and can be also used to choose the regularization parameter according to tuning criteria such as Cp [28], AIC [1], BIC [42], GCV [11].", "startOffset": 185, "endOffset": 189}, {"referenceID": 0, "context": "Degrees of freedom is an index more interpretable than the regularization parameter, and can be also used to choose the regularization parameter according to tuning criteria such as Cp [28], AIC [1], BIC [42], GCV [11].", "startOffset": 195, "endOffset": 198}, {"referenceID": 41, "context": "Degrees of freedom is an index more interpretable than the regularization parameter, and can be also used to choose the regularization parameter according to tuning criteria such as Cp [28], AIC [1], BIC [42], GCV [11].", "startOffset": 204, "endOffset": 208}, {"referenceID": 10, "context": "Degrees of freedom is an index more interpretable than the regularization parameter, and can be also used to choose the regularization parameter according to tuning criteria such as Cp [28], AIC [1], BIC [42], GCV [11].", "startOffset": 214, "endOffset": 218}, {"referenceID": 47, "context": "A general expression for the effective degrees of freedom of non-linear kernel regression methods with one layer, based on the SURE (Stein\u2019s Unbiased Risk Estimator) approximation [48] has been recently derived in [13].", "startOffset": 180, "endOffset": 184}, {"referenceID": 12, "context": "A general expression for the effective degrees of freedom of non-linear kernel regression methods with one layer, based on the SURE (Stein\u2019s Unbiased Risk Estimator) approximation [48] has been recently derived in [13].", "startOffset": 214, "endOffset": 218}, {"referenceID": 18, "context": "In this subsection, we analyze certain scaling rules that are connected with popular statistical indices, often used as \u201cfilters\u201d for feature selection [19].", "startOffset": 152, "endOffset": 156}, {"referenceID": 26, "context": "A transductive scaling rule can be obtained by extending the sum to both training and test inputs, namely computing the inverse trace of the overall kernel matrix, as in [27].", "startOffset": 170, "endOffset": 174}, {"referenceID": 33, "context": "Computations are carried out in a Matlab environment and the sub-problem (Problem 7) is solved using an SMO-like (Sequential Minimal Optimization) algorithm [34].", "startOffset": 157, "endOffset": 161}, {"referenceID": 46, "context": "Experiment 2 (Prostate Cancer data) Linear RLS2 is applied to the Prostate Cancer dataset, a regression problem whose goal is to predict the level of prostate-specific antigen on the basis of a number of clinical measures in men who were about to receive a radical prostatectomy [47].", "startOffset": 279, "endOffset": 283}, {"referenceID": 19, "context": "Results for methods other than RLS2 are taken from [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "are used in the textbook [20] to compare different feature selection and shrinkage methods, and have been obtained from the web site http://www-stat.", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "Following [20], we pick the value of \u03bb corresponding to the least complex model within one standard error of the best validation score.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "For comparison, Table 1 also reports models and results taken from [20] associated with LS (Least Squares), Best subset regression, Ridge Regression, Lasso regression, PCR (Principal Component Regression), PLS (Partial Least Squares).", "startOffset": 67, "endOffset": 71}, {"referenceID": 34, "context": "[35], the same set of basis kernels for all the datasets has been chosen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Performances of other kernel learning algorithms on some of these datasets can be found in [27, 33, 35] and references therein.", "startOffset": 91, "endOffset": 103}, {"referenceID": 32, "context": "Performances of other kernel learning algorithms on some of these datasets can be found in [27, 33, 35] and references therein.", "startOffset": 91, "endOffset": 103}, {"referenceID": 34, "context": "Performances of other kernel learning algorithms on some of these datasets can be found in [27, 33, 35] and references therein.", "startOffset": 91, "endOffset": 103}, {"referenceID": 28, "context": "Another benchmark study that might be useful for comparison is [29].", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "For instance, [27] uses an 80/20 dataset split ratio, [33] uses 60/40, while [35] uses 70/30.", "startOffset": 14, "endOffset": 18}, {"referenceID": 32, "context": "For instance, [27] uses an 80/20 dataset split ratio, [33] uses 60/40, while [35] uses 70/30.", "startOffset": 54, "endOffset": 58}, {"referenceID": 34, "context": "For instance, [27] uses an 80/20 dataset split ratio, [33] uses 60/40, while [35] uses 70/30.", "startOffset": 77, "endOffset": 81}, {"referenceID": 35, "context": "Linear RLS2 with OVA has been applied to the 14 Cancers dataset [36], a delicate multi-class classification problem whose goal is to discriminate between 14 different types of cancer, on the basis of microarray measurements of 16063 gene expressions.", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "[20] reports several results for these data using a variety of classification methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Results for methods other than RLS2 are taken from [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "Table 5 reports the number of test errors and selected genes in correspondence with the value of \u03bb chosen in the validation phase, for RLS2 and other methods from [20].", "startOffset": 163, "endOffset": 167}, {"referenceID": 30, "context": "By standard representer theorems for vector valued functions (see [31] and the remark on monotonicity in [40] after Theorem 1), there exists an optimal f2 in the form", "startOffset": 66, "endOffset": 70}, {"referenceID": 39, "context": "By standard representer theorems for vector valued functions (see [31] and the remark on monotonicity in [40] after Theorem 1), there exists an optimal f2 in the form", "startOffset": 105, "endOffset": 109}, {"referenceID": 38, "context": "where \u2202 is the sub-differential of a convex function [39].", "startOffset": 53, "endOffset": 57}, {"referenceID": 24, "context": "This problem can be seen to be jointly convex in (z,R) using an argument due to [25]: the term zTR\u2020z is a matrix-fractional function (see e.", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "[9], Example 3.", "startOffset": 0, "endOffset": 3}], "year": 2010, "abstractText": "In this paper, the framework of kernel machines with two layers is introduced, generalizing classical kernel methods. The new learning methodology provide a formal connection between computational architectures with multiple layers and the theme of kernel learning in standard regularization methods. First, a representer theorem for two-layer networks is presented, showing that finite linear combinations of kernels on each layer are optimal architectures whenever the corresponding functions solve suitable variational problems in reproducing kernel Hilbert spaces (RKHS). The input-output map expressed by these architectures turns out to be equivalent to a suitable single-layer kernel machines in which the kernel function is also learned from the data. Recently, the so-called multiple kernel learning methods have attracted considerable attention in the machine learning literature. In this paper, multiple kernel learning methods are shown to be specific cases of kernel machines with two layers in which the second layer is linear. Finally, a simple and effective multiple kernel learning method called RLS2 (regularized least squares with two layers) is introduced, and his performances on several learning problems are extensively analyzed. An open source MATLAB toolbox to train and validate RLS2 models with a Graphic User Interface is available.", "creator": "TeX"}}}