{"id": "1002.2050", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2010", "title": "Intrinsic dimension estimation of data by principal component analysis", "abstract": "Estimating invulnerability inharmonicity over signals neither a classic problem of different recognition many report. Principal Component Analysis (PCA) that each making systems in discovering tetration, data sets turn hand parameters structure; being, likely, indeed inherently when indicators although a nonlinear pattern. In far papers, we plan single american PCA - based formulation give surveyed intrinsic dimension this reporting with many-body structures. Our method used subsequently first finding next minimal all same the surveys key, then musicians PCA locally on each approximate in made cover and began even the estimation of by checking got since numbers variance during could small populated areas. The proposed alternative front-end the putting data into to preliminary instead qualities dimension and. providing for simplification work. In addition, our house PCA procedure can infrared stopped noise between signal took farthest supposed a stable estimation have next neighborhood restive sized rising. Experiments on synthetic taking real world account make nbc viability of only proposed experiments.", "histories": [["v1", "Wed, 10 Feb 2010 10:16:57 GMT  (988kb,S)", "http://arxiv.org/abs/1002.2050v1", "8 pages, submitted for publication"]], "COMMENTS": "8 pages, submitted for publication", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["mingyu fan", "nannan gu", "hong qiao", "bo zhang"], "accepted": false, "id": "1002.2050"}, "pdf": {"name": "1002.2050.pdf", "metadata": {"source": "CRF", "title": "Intrinsic dimension estimation of data by principal component analysis", "authors": ["Mingyu Fan", "Nannan Gu", "Bo Zhang"], "emails": ["fanmingyu@amss.ac.cn,", "b.zhang@amt.ac.cn)", "nan@gmail.com,", "hong.qiao@ia.ac.cn)"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 2.\n20 50\nv1 [\ncs .C\nV ]\n1 0\nFe b\n20 10\nIndex Terms\u2014Pattern recognition; Principal component analysis; Intrinsic dimensionality estimation.\nI. INTRODUCTION\nIntrinsic dimensionality (ID) of data is a key priori knowledge in pattern recognition and statistics, such as time series analysis, classification and neural networks, to improve their performance. In time series analysis [1], the domain of attraction of a nonlinear dynamic system has a very complex geometric structure, and study on the geometry of the attraction domain is closely related to the fractal geometry. Fractal dimension is an important tool to characterize certain geometric properties of complex sets. In neural network design [2], the number of hidden units in the encoding middle layer should be chosen according to the ID of data. In classification tasks [3], in order to balance the generalization ability and the empirical risk value, the complexity of the function should also be related to the ID of data.\nM. Fan and B. Zhang are with LSEC and the Institute of Applied Mathematics, AMSS, Chinese Academy of Sciences, Beijing 100190, China (email: fanmingyu@amss.ac.cn, b.zhang@amt.ac.cn)\nN. Gu and H. Qiao are with the Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China (email: gunannan@gmail.com, hong.qiao@ia.ac.cn)\nCorresponding author: Bo Zhang\nRecently, manifold learning, an important approach for nonlinear dimensionality reduction, has drawn great interests. Important manifold learning algorithms include isometric feature mapping (Isomap) [4], locally linear embedding (LLE) [5] and Laplacian eigenmaps (LE) [6]. They all assume data to distribute on an intrinsically lowdimensional sub-manifold [7] and reduce the dimensionality of data by investigating the intrinsic structure of data. However, all manifold learning algorithms require the ID of data as a key parameter for implementation.\nPrevious ID estimation methods can be categorized mainly into three groups: projection approach, geometric approach and probabilistic approach. The projection approach [9]\u2013[11] finds ID by checking up the lowdimensional embedding of data. The geometric method [22] finds ID by investigating the intrinsic geometric structure of data. The probabilistic technique [19] builds estimators by making distribution assumptions on data. These approaches will be briefly introduced in Section II.\nIn this paper, we propose a new PCA-based method for ID estimation which is called the C-PCA method. The proposed method first finds a minimal cover of the data set, and each subset in the cover is considered as a small subregion of the data manifold. Then, on each subset, a revised PCA procedure is applied to examine the local structure. The revised PCA method can filter out noise in data and leads to a stable and convergent ID estimation with the increase of the subregion size, as shown by the experimental results. This is an advantage over the traditional PCA method which is very sensitive to noise, outliers and the choice of the subregion size. Further analysis shows that the revised PCA procedure can efficiently reduce the running time complexity and utilize all data samples for ID estimation. We should remark that our ID estimation method is also applicable to incremental learning for consecutive data. Our method is compared with the maximum likelihood estimation (MLE) method [19], the manifold adaptive method (which is referred to as the k-k/2 NN method in this paper) [18] and the k-nearest neighbor graph (kNNG) method [26], [27] through experiments.\nThe rest of the paper is organized as follows. In\n2 Section II, previous ID estimation methods are briefly reviewed. In Section III, the new ID estimation method (C-PCA) is introduced. In Section IV, experiments are conducted on synthetic and real world data sets to show the effectiveness of the proposed algorithm. Conclusion is made in Section V."}, {"heading": "II. PREVIOUS ALGORITHMS ON ID ESTIMATION", "text": "Previously, there are mainly three approaches to estimate the ID of data: projection, geometric and probabilistic approaches.\nThe projection approach first projects data into a low-dimensional space and then determine the ID by verifying the low-dimensional representation of data. PCA is a classical projection method which finds ID by counting the number of significant eigenvalues. However, the traditional PCA only works on data lying in a linear subspace but becomes ineffective when data distribute on a nonlinear manifold. To overcome this limitation, localPCA [9] and OTPMs PCA [10] have been proposed and can discover the ID of data lying on nonlinear manifolds by performing the PCA method locally. The Isomap algorithm yields ID of data by inspecting the elbow of residual variance curve [4]. Cheng et al. gave an efficient procedure to compute eigenvalues and eigenvectors in PCA [24].\nGeometric approaches make use of the geometric structure of data to build ID estimators. Fractal-based methods have been well developed and used in time series analysis. For example, the correlation dimension (a kind of fractal dimensions) was used in [13] to estimate the ID, whilst the method of packing numbers was proposed in [14] to find the ID. Other fractalbased methods include the kernel correlation method [23] and the quantization estimator [21]. A good survey on fractal-based methods can be found in [22]. There are also many methods based on techniques from computational geometry. Lin [15] and Cheng [25] suggested to construct simplices to find the ID, while the nearest neighbor approach uses the distances between data points with their nearest neighbors to build ID estimators such as the estimator proposed by Pettis et al. [29], the kNNG method [26], [27] and the incising ball method [17]. A comparison of the local-PCA method with that introduced by Pettis et al. was made in [16].\nProbabilistic methods are based on probabilistic assumptions of data and have been tested on various data sets with stable performance. The MLE-method [19] is a representative method of this approach, whose final global estimator is given by averaging the local\nestimators:\nd\u0302k(xi) =\n\n\n1\nk \u2212 1\nk\u22121 \u2211\nj=1\nlog Tk(xi)\nTj(xi)\n\n\n\u22121\n, for i = 1, \u00b7 \u00b7 \u00b7 , N,\nwhere Tk(xi) is the distance between xi and its k-th nearest neighbor. MacKay and Ghahramani [20] pointed out that compared with averaging the local estimators directly, it is more sensible to average their inverses d\u0302\u22121k (xi), i = 1, \u00b7 \u00b7 \u00b7 , N for the maximum likelihood purpose. The recommended final estimator is\nd\u0302\u22121k = 1\nN\nN \u2211\ni=1\nd\u0302\u22121k (xi),\nwhere dk is the estimated ID corresponding to the neighborhood size k.\nIII. ID ESTIMATION USING PCA WITH COVER SETS: C-PCA\nBasically, there are two kinds of definitions of ID that are commonly used. One is based on the fractal dimension, such as the Hausdorff dimension and the packing dimension that are usually real positive numbers. The other kind of definition is based on the embedding manifold whose ID is always an integer.\nDefinition 3.1 (Embedding manifold and dimension): Let d < D and let \u2126 be a compact open set in Rd. Assume that span{\u2126 \u2212 \u222b\n\u2126 d\u00b5} = Rd and \u03c6 : \u2126 \u2192 RD\nis a smooth function. The set X = \u03c6(\u2126) is called an embedding manifold with d its embedding dimension.\nMore and more real world data are proved to have nonlinear intrinsic structures and may possibly distribute on nonlinear embedding manifolds [7]. Therefore, estimation of embedding ID of data becomes an important problem [17]. In this paper, we focus on estimation of embedding dimensions."}, {"heading": "A. PCA-based methods for ID estimation", "text": "The traditional PCA can find a subspace on which data projections have maximum variance. Given a data set X = {x1, \u00b7 \u00b7 \u00b7 , xN} with xi \u2208 RD. Let X = [x1, \u00b7 \u00b7 \u00b7 , xN ] and x\u0304 = 1N \u2211N i=1 xi. The covariance matrix of X is given by\nC = 1\nN\nN \u2211\ni=1\n(xi \u2212 x\u0304)(xi \u2212 x\u0304) T .\nSince C is a positive semi-definite matrix, we can assume that \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbN \u2265 0 are the eigenvalues of C with \u03bd1, \u00b7 \u00b7 \u00b7 , \u03bdN the corresponding orthonormal eigenvectors, respectively. The eigen-decomposition of\n3 matrix C is denoted as C = \u0393D\u0393T , where D is a diagonal matrix with Dii = \u03bbi and \u0393 = [\u03bd1, \u00b7 \u00b7 \u00b7 , \u03bdN ]. The eigenvector \u03bdi is the i-th principal direction (PD) and, for any variable x, yi = \u03bdix is defined as the i-th principal component (PC). By the definition, we have the variance var(yi) = \u03bbi and the covariance cov(yi, yj) = 0.\nIf the data set X distributes on a linear subspace, then the d primary PDs should be able to span the subspace and the corresponding PCs can account for most of the variations contained in X . On the other hand, the variance of PCs on \u03bdd+1, \u00b7 \u00b7 \u00b7 , \u03bdN (i.e., the PDs which are orthogonal to the linear subspace of dimension d) will be trivial. The most commonly-used criterions for ID estimation with the PCA method are\nmin i=1,\u00b7\u00b7\u00b7 ,d (var(yi))\nmax j=d+1,\u00b7\u00b7\u00b7 ,N\n(var(yj)) > \u03b1 \u226b 1 (1)\nand the percentage of the accounted variance \u2211d\ni=1 var(yi) \u2211N\ni=1 var(yi) > \u03b2, 0 < \u03b2 < 1. (2)\nIn this paper, the ID, d, is determined if the condition (1) or (2) is satisfied."}, {"heading": "B. Filtering out the noise of data", "text": "There are two challenges for PCA-based ID estimation methods. The first one is how to filter out the noise in data, while the second one is how to choose the size of subregions on the manifold. Previously, the ID estimation of data obtained with PCA-based methods always increases with the size of subregions so the methods can not converge to give a stable ID estimation. In order to address these two limitations, we propose the following noise filtering procedure which can efficiently filter out the noise in data and make PCA-based methods to converge.\nConsider the effect of additive white noise \u00b5 in data with E(\u00b5) = 0 and var(\u00b5) = \u03c32. The covariance matrix of the noise corrupted data is given by\nC \u2032 = var(X + \u00b5) = C + \u03c32I,\nwhere C is the covariance matrix of the data X . It can be seen that the PDs of C \u2032 are identical to those of C and the eigenvalues of C \u2032 are \u03bb\u2032i = \u03bbi + \u03c3\n2. If \u03c3 is relatively large, then the ID criterions (1) and (2) will be ineffective.\nThe variance of data projections on the PDs that are orthogonal to the intrinsic embedding subspace is very small, and the most part of the variance is produced by\nnoise. Therefore, it is possible to calculate the variance of noise by projecting data on the orthogonal PDs. Given a real number P which is very close to 1 (P is taken to be 0.95 in this paper), the noise part of data is determined by \u2211r\u22121\ni=1 var(yi) \u2211N\ni=1 var(yi) < P and\n\u2211r i=1 var(yi) \u2211N i=1 var(yi) > P.\nThus, the variance of noise contained in data can be estimated as\n\u03c3\u03022 = 1\nN \u2212 r + 1\nN \u2211\ni=r\nvar(yi). (3)\nOur new ID estimation criterions make use of the updated variance on PDs: var(yi) = \u03bbi \u2212 \u03c3\u03022.\nRemark 3.1: Noise is typically different from outliers. Noise affects every data points independently, while outliers are referred to data points that are at least at a certain distance from the data points on manifold. The proposed procedure is very robust to both noise and outliers, as shown in experiments. On the other hand, the traditional PCA procedure can handle limited noise but is very sensitive to outliers."}, {"heading": "C. The local region selection method", "text": "An embedding manifold can be approximated locally by linear subspaces. The dimensionality of each linear subspace should be equal to the ID of the embedding manifold. Therefore, it is possible to estimate the ID of a nonlinear manifold by checking it locally. A cover is referred to a set whose elements are subsets of the data set satisfying that the union of all subsets in the cover contains the whole data set.\nDefinition 3.2 (The set cover problem): Given a universe X of N elements and a collection F of subsets of X , where F = {F1, \u00b7 \u00b7 \u00b7 , FN}. Set cover is concerned with finding a minimum sub-collection of F that covers all data points.\nUsing a minimum cover has two advantages. First, it can find the minimal number of subregions, which helps save the computational time. Secondly, the result of ID estimation that utilizes the whole data set is more reliable. However, searching such a minimal cover is an NP-hard problem. In the following, we introduce an algorithm which can approximately find a minimal cover of a data set.\nGiven the parameter, an integer k or a real number \u03b5, there are two ways to define the neighborhood of any data point x:\n1) The k-NN method: any data point xi that is one of k nearest data points of x is in the neighborhood of x;\n4 2) The \u03b5-NN method: any data point xi in the region {y : \u2016y \u2212 x\u2016 < \u03b5} is in the neighborhood of x.\nWithout loss of generality, we may assume that the index of data points is independent of their locations.\nAlgorithm 1 (Minimum set cover algorithm) Input: Neighborhood size k (integer) or \u03b5 (real number), distance matrix D\u0302 = (\u2016xi \u2212 xj\u2016) Output: Minimum cover F = {(Fi, ri), i = 1, \u00b7 \u00b7 \u00b7 , S}.\n1: for i=1 to N do 2: Identify the neighbors {xi1 , \u00b7 \u00b7 \u00b7 , xiPi} of xi by the\nk-NN or \u03b5-NN method. Let Fi = {i, i1, \u00b7 \u00b7 \u00b7 , iPi} be the index set of the neighborhood and let D be the 0\u2212 1 incidence matrix.\n3: end for 4: Let F = {(Fi, ri = 0), i = 1, \u00b7 \u00b7 \u00b7 , N} 5: for i = 1 to N do 6: Let the frequency of xi be computed by Qi =\n\u2211N j=1Dij .\n7: end for 8: for i = 1 to N do 9: if Qi, Qi1 , \u00b7 \u00b7 \u00b7 , QiPi > 1 then\n10: Remove (Fi, ri) from the cover set F and set Qi = Qi \u2212 1, Qi1 = Qi1 \u2212 1, \u00b7 \u00b7 \u00b7 , QiPi = QiPi \u2212 1. 11: else 12: Let ri = max\nj=1,\u00b7\u00b7\u00b7 ,Pi \u2016xi \u2212 xij\u2016\n13: end if 14: end for\nUsing the above approximation algorithm, a cover F = {(Fi, ri), i = 1, \u00b7 \u00b7 \u00b7 , S} of the data set X can be found. Compared with the local region selection algorithm used in [9], our algorithm above has a low time complexity and avoids the supervised process to choose the neighborhood. Intuitively, the cardinality S of the cover F satisfies that N/k < S < N/2k, where k is the average number of neighbors."}, {"heading": "D. The proposed ID estimation algorithm", "text": "We now present the proposed ID estimation algorithm using local PCA on the minimal set cover: the C-PCD algorithms, which are summarized below for both batch and incremental data, respectively.\nIn many cases, consecutive data are collected incrementally. This requires an incremental learning algorithm to inspect the change of the data structure on time. The incremental C-PCA algorithm is presented as follows.\nRemark 3.2: Our method is different from the LocalPCA [9] in many aspects. First, the centers and the\nAlgorithm 2 (The C-PCA algorithm for batch data) Step 1. Given a parameter k or \u03b5, compute a minimal cover of X by Algorithm III-C. Without loss of generality, F = {(Fi, ri) : i = 1, \u00b7 \u00b7 \u00b7 , S} is assumed to be the constructed minimal set cover. Step 2. Perform the PCA algorithm proposed in Subsections III-A and III-B on subsets Fi, i = 1 \u00b7 \u00b7 \u00b7 , S. The local ID estimations {d\u0302i}Si=1 are then obtained. Step 3. Let \u03bbij be the j-th eigenvalue on the ith subset in the decreasing order. \u03bbj = \u2211\ni \u03bbij is considered as the variance of X on its j-th PD. Subsequently, the global ID estimation d\u0302 can be derived using the criterions (1) or (2).\nAlgorithm 3 (The incremental C-PCA algorithm) Step 1. The new data point is assumed to be x. Let {x1, \u00b7 \u00b7 \u00b7 , xS} be the centers of the subsets in the cover. Find the nearest center xq of x: xq = arg min\ni=1,\u00b7\u00b7\u00b7 ,S \u2016x\u2212 xi\u2016.\nStep 2. If \u2016xq \u2212 x\u2016 > rq, then the data point x is considered as an outlier and the remaining part of the algorithm will not be performed on x. Otherwise, go to Step 3. Step 3. Performs PCA on Fq = Fq \u22c3\n{x}. Let \u03bb\u2032qj be the j-th eigenvalue. Update \u03bbj by \u03bbj = \u03bbj + \u03bb \u2032 qj \u2212 \u03bbqj . Then let \u03bbqj = \u03bb \u2032\nqj . Step 4. Update the local ID, d\u0302q, and the global ID, d\u0302, of X .\nlocal regions are determined simultaneously by using one parameter - the neighborhood size, whilst, in [9], the centers and neighborhood sizes are determined by two parameters. Secondly, our approach finds the subregions by approximating a minimum cover of the data set, while the local-PCA in [9] does not guarantee whether or not the selected subregions cover the whole data set."}, {"heading": "E. Computational complexity analysis", "text": "The computational complexity of our algorithms is one of the most important issues for its application. The batch mode ID estimation can be divided into two parts. In the first part, computing the distance matrix needs O(N2) time, searching the nearest neighbors for every data point needs O(kN2) time and finding an approximate minimum cover of X needs O(kN) time. Therefore, the first part needs O((K + 1)N2 + kN) running time. In the second part, performing PCA locally needs k3 \u00d7 (N/k) \u2248 O(k2N) running time. To sum\n5 up, the total running time needed for the batch mode algorithm is O((k+1)N2+(k2+k)N). If the proposed method is embedded in a manifold learning algorithm, then the running time complexity can be reduced to O((k2 + k)N) in the case when the distance matrix and the neighborhood are already defined. This is a relatively small increase in the time complexity of a manifold learning algorithm which is always as high as O(N3).\nFor incremental learning, the neighborhood identification step needs O(N/k) running time, whilst the local PCA consumes O((k + 1)3) running time. Therefore, the total time complexity for incremental learning is O((k + 1)3 +N/k)."}, {"heading": "IV. EXPERIMENTS", "text": "The proposed algorithm was implemented with parameters \u03b1 = 10 and \u03b2 = 0.8 for all the experiments.\nIn practice, it is found that noise contained in data is of low-dimension, except an additive white noise which is assumed to be in every component of the data vectors in RD. Thus, in practice, we only use variances of the first min(10, N \u2212 r+1) PCs in the noise part of data to estimate the variance of noise (see Eq. (3)).\nComparison is made among the k-k/2 NN method [18], the k-NNG method [26], the revised MLE (MLE in short) method [20], the C-PCA method and the L-PCA method, where the L-PCA method stands for the C-PCA method without the noise filtering procedure proposed in Subsection III-B. It should be noted that the results obtained by the MLE, k-k/2 NN and k-NNG methods are positive real numbers, while the L-PCA and C-PCA methods produce only integer ID estimations. In order to make a comparison among these results, we average the local ID estimations obtained with the C-PCA and L-PCA methods to provide a real ID estimation: d\u0302 = 1 S \u2211S i=1 d\u0302i.\nA. 10-Mobius data\nThe first data set is a 10-Mobius ring embedded in R 3. Fig. 1(a) shows the scatter plot of the Mobius ring data set. As can be seen, the Mobius data points are lying on a highly nonlinear manifold with 1200 points uniformly distributing on the surface. Fig. 1(b) shows the results obtained by the five ID estimation algorithms against the neighborhood size ranging from 4 to 40. The MLE method is the most stable and accurate algorithm for all neighborhood sizes. All algorithms converge to the correct estimation. It seems that the L-PCA method does not diverge on this data set. This is possibly because the original dimensionality of data is low.\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n\u22120.5 0\n0.5\nx\ny\nz\n(a)"}, {"heading": "B. Real world data sets", "text": "Our algorithm is compared with the MLE, k-k/2 NN and k-NNG methods on some benchmark real world data sets: the Isoface data set [4], the LLEface data set [5] and the MNIST \u20190\u2019 and \u20191\u2019 data sets [28].\nThe Isoface data set is comprised of 698 images of a head with the resolution 64 \u00d7 64. Some samples of the Isoface data set are shown in Fig. 2(a). In the experiments, each image is reshaped to a 4096-dimensional vector. It can be seen that the Isoface data set is under a three-dimensional movement: up-down, left-right and lighting changes. In [4], the Isomap algorithm estimated its ID as 3 using the projection approach. As can be seen from Fig. 2(b), corresponding to the neighborhood sizes from 4 to 40, the C-PCA estimator ranges from 2.3 to 3.5 and the MLE estimator ranges from 3.5 to 4.5. The estimation given by the k-NNG and k-k/2 NN methods is oscillating badly with the neighborhood sizes, so they are bot unstable. Since the L-PCA method can not filter out noise contained in data, it tends to overestimate the ID as the neighborhood size increases. This means that our noise filtering process plays a key role in the convergence of the C-PCA method.\nThe second data set is the LLEface data set, which contains 1965 samples in a 560-dimensional space (see Fig. 3(a) for some samples). From Fig. 3(b), it is seen that both the C-PCA and the MLE methods give a convergent ID estimation with the increase of the neigh-\nborhood size, while the L-PCA, k-k/2-NN and k-NNG methods seem not convergent when the neighborhood size is increasing. The ID estimation given by the C-PCA method changes between 2.8 and 4.7 with the convergent estimation being 4.7, while the estimation result obtained by the MLE method changes gradually from 5.2 to 5.8 with a convergent estimation of 5.8.\nWe now consider two MNIST data sets: the set \u20190\u2019 and the set \u20191\u2019 (see Fig. 4(a) and Fig. 5(a) for some samples of these two data, respectively). The data set \u20190\u2019 contains\n980 data points, while the data set \u20191\u2019 contains 1135 data points. It can be seen from Fig. 4(b) and Fig. 5(b) that all methods, except the L-PCA and k-NNG methods, converge with the increase of the neighborhood size. For the data set \u20190\u2019, it can be seen from Fig. 4(b) that the ID estimation given by the C-PCA method converges to 5.8 and the estimation given by both the MLE estimator and the k-k/2-NN estimator converges to 10. For the data set \u20191\u2019, Fig. 5(b) shows that the ID estimation obtained by\nthe C-PCA method converges to 5.5 and the estimation provided with both the MLE method and the k-k/2-NN method converges to 7.2. Note that the result given by our method is in a big disagreement with the results given by other methods for the ID estimation of the data sets \u20190\u2019 and \u20191\u2019. A digit \u20190\u2019 is usually represented as an ellipse which can be determined by the coordinates of its focus and its major and minor axes, so the ID of the data set \u20190\u2019 is likely to be 5. The number \u20191\u2019 can be considered as a line segment, which rotates from left to right, so a sensible ID estimation for the data set \u20191\u2019 may be between 4 and 5."}, {"heading": "C. Noisy data sets", "text": "The traditional PCA algorithm is very sensitive to outliers, and the performance of PCA-based algorithms deteriorate rapidly if data points are sparse on a manifold such as the hand rotation data set 1. As can be seen from Fig. 6(a), the hand is under a one-dimensional movement, so the data points can be considered as lying on a one-dimensional curve. The data set contains 481 image samples, and each sample is a vector in a 512480- dimensional space. Many outliers can be seen from its low-dimensional embedding by the Isomap algorithm (see Fig. 6(b)). Its ID estimation results with different methods are shown in Fig. 6(c).\nBoth the k-k/2 NN and k-NNG methods are sensitive to the choice of the neighborhood size and tend to overestimate the ID as the neighborhood size increases. On the other hand, the MLE estimator is more stable (see Fig. 6(c)). However, the minimum estimation of MLE method is 1.75, which is still higher than the ID of this data set. L-PCA method has the worst performance due to the outliers contained in the data set. The estimation\n1CMU database: http://vasc.vi.cmu.edu/idb/html/motion/hand/index.html\nof the C-PCA method, which changes between 1.5 and 1.2, is the closest one to the correct ID of this data set.\nWe now transform the original 10-Mobius data in a 4-dimensional space using an Euclidean transformation. A random noise with mean 0 and variance 0.2 is added to the transformed data. The ID estimation results with different algorithms are given in Fig. 7. As can be seen from Fig. 7, the ID estimation given by the C-PCA method is the closest one to the correct ID of this noised 10-Mobius data set. The other algorithms tend to overestimate the ID of the noised data set. The estimation obtained by the L-PCA method is a little higher than that given by the C-PCA due to the effect of noise.\n8 0 5 10 15 20 25 30 35 40 1.5 2 2.5 3 3.5 4\nNeighborhood size\nE st\nim at\ned ID\nC\u2212PCA k\u2212k/2 NN L\u2212PCA MLE k\u2212NNG\nFig. 7. ID estimations of the noised Mobius data set."}, {"heading": "V. CONCLUSION", "text": "In this paper, we proposed a new ID estimation method based on PCA. The proposed algorithm is simple to implement and gives a convergent ID estimation corresponding to a wide range of neighborhood sizes. It is also convenient for incremental learning. Experiments have shown that the new algorithm has a robust performance."}, {"heading": "ACKNOWLEDGMENT", "text": "The work of H. Qiao was supported in part by the National Natural Science Foundation (NNSF) of China under grant no. 60675039 and 60621001 and by the Outstanding Youth Fund of the NNSF of China under grant no. 60725310. The work of B. Zhang was supported in part by the 863 Program of China under grant no. 2007AA04Z228, by the 973 Program of China under grant no. 2007CB311002 and by the NNSF of China under grant no. 90820007."}], "references": [{"title": "Characterising experimental time series using local intrinsic dimension", "author": ["T.M. Buzuga", "J.V. Stammb", "G. Pfister"], "venue": "Physics Letters A202 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": "Oxford Univ. Press, Oxford", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "The Elements of Statistical Learning - Data Mining", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Inference and Prediction, Springer, Berlin", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "V", "author": ["J.B. Tenenbaum"], "venue": "de Sliva, J.C. Landford, A global geometric framework for nonlinear dimensionality reduction, Science 290 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science 290 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Laplacian eigenmaps for dimensionality reduction and representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation 15 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "The manifold ways of perception", "author": ["H.S. Seung", "D.D. Lee"], "venue": "Science 290 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Principal Component Analysis", "author": ["I.T. Jolliffe"], "venue": "Springer, Berlin", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1989}, {"title": "An algorithm for finding intrinsic dimensionality of data", "author": ["K. Fukunaga", "D.R. Olsen"], "venue": "IEEE Transactions on Computers 20 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1971}, {"title": "Intrinsic dimension estimation with optimally topology preserving maps", "author": ["J. Bruske", "G. Sommer"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 20 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Principal curves", "author": ["T. Hastie", "W. Stuetzle"], "venue": "Journal of the American Statistical Association 84 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1988}, {"title": "Chaos", "author": ["S. Chatterjee", "M.R. Yilmag"], "venue": "fractals and statistics, Statistical Science 7 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "Measuring the strangeness of strange attractors", "author": ["P. Grassberger", "I. Procaccia"], "venue": "Physica D9 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1983}, {"title": "Intrinsic dimension estimation using packing numbers", "author": ["B. Kegl"], "venue": "Advances in Neural Information Processing Systems 16 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Riemannian manifold learning", "author": ["T. Lin", "H. Zha"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 30 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "An evaluation of intrinsic dimensionality estimators", "author": ["P.J. Verveer", "R.P.W. Duin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 17 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Intrinsic dimension estimation of manifolds by incising balls", "author": ["M. Fan", "H. Qiao", "B. Zhang"], "venue": "Pattern Recognition 42 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Manifoldadaptive dimension estimation", "author": ["A.M. Farahmand", "C. Szepesvari", "J.Y. Audibert"], "venue": "in: Proceedings of the 24th Annual International Conference on Machine Learning", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Maximum likelihood estimation of intrinsic dimension", "author": ["E. Levina", "P.J. Bickel"], "venue": "Advances in Neural Information Processing Systems 18 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Comments on \u2019Maximum likelihood estimation of intrinsic dimension\u2019 by E", "author": ["D.J.C. MacKay", "Z. Ghahramani"], "venue": "Levina and P. Bickel, see http://www.inference.phy.cam.ac.uk/mackay/dimension/", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimation of intrinsic dimensionality using high-rate vector quantization", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "Advances in Neural Information Processing Systems 19 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Data dimensionality estimation methods: a survey", "author": ["F. Camastra"], "venue": "Pattern Recognition 36 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Intrinsic dimensionality estimation of submanifolds in R", "author": ["M. Hein", "J.Y. Audibert"], "venue": "in: Proceedings of the 22nd International Conference on Machine Learning (ed. Morgan Kaufmann)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Provable dimension detection using principal component analysis", "author": ["S.W. Cheng", "Y.J. Wang", "Z.Z. Wu"], "venue": "in: Proceedings of the 21th Annual Symposium on Computational Geometry", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Dimension detection via slivers", "author": ["S.W. Cheng", "M.K. Chiu"], "venue": "in: Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete Algorithms", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Geodesic entropic graphs for dimension and entropy estimation in manifold learning", "author": ["J.A. Costa", "A.O. Hero"], "venue": "IEEE Transactions on Signal Processing 52 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Estimating local intrinsic dimension with k-nearest neighbor graphs", "author": ["J.A. Costa", "A.O. Hero"], "venue": "IEEE Transactions on Statistical Signal Processing 30 (23) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Le Cun", "L. Bottou", "Y. Bengio", "H. Patrick"], "venue": "Proceedings of the IEEE 86 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "An intrinsic dimensionality estimator from near-neighbor information", "author": ["K.W. Pettis", "T.A. Bailey", "A.K. Jain", "R.C. Dubes"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 1 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1979}], "referenceMentions": [{"referenceID": 0, "context": "In time series analysis [1], the domain of attraction of a nonlinear dynamic system has a very complex geometric structure, and study on the geometry of the attraction domain is closely related to the fractal geometry.", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "In neural network design [2], the number of hidden units in the encoding middle layer should be chosen according to the ID of data.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "In classification tasks [3], in order to balance the generalization ability and the empirical risk value, the complexity of the function should also be related to the ID of data.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "Important manifold learning algorithms include isometric feature mapping (Isomap) [4], locally linear embedding (LLE) [5] and Laplacian eigenmaps (LE) [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "Important manifold learning algorithms include isometric feature mapping (Isomap) [4], locally linear embedding (LLE) [5] and Laplacian eigenmaps (LE) [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 5, "context": "Important manifold learning algorithms include isometric feature mapping (Isomap) [4], locally linear embedding (LLE) [5] and Laplacian eigenmaps (LE) [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 6, "context": "They all assume data to distribute on an intrinsically lowdimensional sub-manifold [7] and reduce the dimensionality of data by investigating the intrinsic structure of data.", "startOffset": 83, "endOffset": 86}, {"referenceID": 8, "context": "The projection approach [9]\u2013[11] finds ID by checking up the lowdimensional embedding of data.", "startOffset": 24, "endOffset": 27}, {"referenceID": 10, "context": "The projection approach [9]\u2013[11] finds ID by checking up the lowdimensional embedding of data.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "The geometric method [22] finds ID by investigating the intrinsic geometric structure of data.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "The probabilistic technique [19] builds estimators by making distribution assumptions on data.", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "Our method is compared with the maximum likelihood estimation (MLE) method [19], the manifold adaptive method (which is referred to as the k-k/2 NN method in this paper) [18] and the k-nearest neighbor graph (kNNG) method [26], [27] through experiments.", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "Our method is compared with the maximum likelihood estimation (MLE) method [19], the manifold adaptive method (which is referred to as the k-k/2 NN method in this paper) [18] and the k-nearest neighbor graph (kNNG) method [26], [27] through experiments.", "startOffset": 170, "endOffset": 174}, {"referenceID": 25, "context": "Our method is compared with the maximum likelihood estimation (MLE) method [19], the manifold adaptive method (which is referred to as the k-k/2 NN method in this paper) [18] and the k-nearest neighbor graph (kNNG) method [26], [27] through experiments.", "startOffset": 222, "endOffset": 226}, {"referenceID": 26, "context": "Our method is compared with the maximum likelihood estimation (MLE) method [19], the manifold adaptive method (which is referred to as the k-k/2 NN method in this paper) [18] and the k-nearest neighbor graph (kNNG) method [26], [27] through experiments.", "startOffset": 228, "endOffset": 232}, {"referenceID": 8, "context": "To overcome this limitation, localPCA [9] and OTPMs PCA [10] have been proposed and can discover the ID of data lying on nonlinear manifolds by performing the PCA method locally.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "To overcome this limitation, localPCA [9] and OTPMs PCA [10] have been proposed and can discover the ID of data lying on nonlinear manifolds by performing the PCA method locally.", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "The Isomap algorithm yields ID of data by inspecting the elbow of residual variance curve [4].", "startOffset": 90, "endOffset": 93}, {"referenceID": 23, "context": "gave an efficient procedure to compute eigenvalues and eigenvectors in PCA [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "For example, the correlation dimension (a kind of fractal dimensions) was used in [13] to estimate the ID, whilst the method of packing numbers was proposed in [14] to find the ID.", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "For example, the correlation dimension (a kind of fractal dimensions) was used in [13] to estimate the ID, whilst the method of packing numbers was proposed in [14] to find the ID.", "startOffset": 160, "endOffset": 164}, {"referenceID": 22, "context": "Other fractalbased methods include the kernel correlation method [23] and the quantization estimator [21].", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "Other fractalbased methods include the kernel correlation method [23] and the quantization estimator [21].", "startOffset": 101, "endOffset": 105}, {"referenceID": 21, "context": "A good survey on fractal-based methods can be found in [22].", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Lin [15] and Cheng [25] suggested to construct simplices to find the ID, while the nearest neighbor approach uses the distances between data points with their nearest neighbors to build ID estimators such as the estimator proposed by Pettis et al.", "startOffset": 4, "endOffset": 8}, {"referenceID": 24, "context": "Lin [15] and Cheng [25] suggested to construct simplices to find the ID, while the nearest neighbor approach uses the distances between data points with their nearest neighbors to build ID estimators such as the estimator proposed by Pettis et al.", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": "[29], the kNNG method [26], [27] and the incising ball method [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[29], the kNNG method [26], [27] and the incising ball method [17].", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "[29], the kNNG method [26], [27] and the incising ball method [17].", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "[29], the kNNG method [26], [27] and the incising ball method [17].", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "was made in [16].", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "The MLE-method [19] is a representative method of this approach, whose final global estimator is given by averaging the local estimators:", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "MacKay and Ghahramani [20] pointed out that compared with averaging the local estimators directly, it is more sensible to average their inverses d\u0302 k (xi), i = 1, \u00b7 \u00b7 \u00b7 , N for the maximum likelihood purpose.", "startOffset": 22, "endOffset": 26}, {"referenceID": 6, "context": "More and more real world data are proved to have nonlinear intrinsic structures and may possibly distribute on nonlinear embedding manifolds [7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 16, "context": "Therefore, estimation of embedding ID of data becomes an important problem [17].", "startOffset": 75, "endOffset": 79}, {"referenceID": 8, "context": "Compared with the local region selection algorithm used in [9], our algorithm above has a low time complexity and avoids the supervised process to choose the neighborhood.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "2: Our method is different from the LocalPCA [9] in many aspects.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "local regions are determined simultaneously by using one parameter - the neighborhood size, whilst, in [9], the centers and neighborhood sizes are determined by two parameters.", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "Secondly, our approach finds the subregions by approximating a minimum cover of the data set, while the local-PCA in [9] does not guarantee whether or not the selected subregions cover the whole data set.", "startOffset": 117, "endOffset": 120}, {"referenceID": 17, "context": "Comparison is made among the k-k/2 NN method [18], the k-NNG method [26], the revised MLE (MLE in short) method [20], the C-PCA method and the L-PCA method, where the L-PCA method stands for the C-PCA method without the noise filtering procedure proposed in Subsection III-B.", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "Comparison is made among the k-k/2 NN method [18], the k-NNG method [26], the revised MLE (MLE in short) method [20], the C-PCA method and the L-PCA method, where the L-PCA method stands for the C-PCA method without the noise filtering procedure proposed in Subsection III-B.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "Comparison is made among the k-k/2 NN method [18], the k-NNG method [26], the revised MLE (MLE in short) method [20], the C-PCA method and the L-PCA method, where the L-PCA method stands for the C-PCA method without the noise filtering procedure proposed in Subsection III-B.", "startOffset": 112, "endOffset": 116}, {"referenceID": 3, "context": "Our algorithm is compared with the MLE, k-k/2 NN and k-NNG methods on some benchmark real world data sets: the Isoface data set [4], the LLEface data set [5] and the MNIST \u20190\u2019 and \u20191\u2019 data sets [28].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Our algorithm is compared with the MLE, k-k/2 NN and k-NNG methods on some benchmark real world data sets: the Isoface data set [4], the LLEface data set [5] and the MNIST \u20190\u2019 and \u20191\u2019 data sets [28].", "startOffset": 154, "endOffset": 157}, {"referenceID": 27, "context": "Our algorithm is compared with the MLE, k-k/2 NN and k-NNG methods on some benchmark real world data sets: the Isoface data set [4], the LLEface data set [5] and the MNIST \u20190\u2019 and \u20191\u2019 data sets [28].", "startOffset": 194, "endOffset": 198}, {"referenceID": 3, "context": "In [4], the Isomap algorithm estimated its ID as 3 using the projection approach.", "startOffset": 3, "endOffset": 6}], "year": 2010, "abstractText": "Estimating intrinsic dimensionality of data is a classic problem in pattern recognition and statistics. Principal Component Analysis (PCA) is a powerful tool in discovering dimensionality of data sets with a linear structure; it, however, becomes ineffective when data have a nonlinear structure. In this paper, we propose a new PCA-based method to estimate intrinsic dimension of data with nonlinear structures. Our method works by first finding a minimal cover of the data set, then performing PCA locally on each subset in the cover and finally giving the estimation result by checking up the data variance on all small neighborhood regions. The proposed method utilizes the whole data set to estimate its intrinsic dimension and is convenient for incremental learning. In addition, our new PCA procedure can filter out noise in data and converge to a stable estimation with the neighborhood region size increasing. Experiments on synthetic and real world data sets show effectiveness of the proposed method.", "creator": "LaTeX with hyperref package"}}}