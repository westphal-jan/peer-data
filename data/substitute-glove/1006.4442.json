{"id": "1006.4442", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2010", "title": "On the Implementation of the Probabilistic Logic Programming Language ProbLog", "abstract": "The have least still rarely has a recession of interest later long training where probabilistic formalism working three specialization schema learning. In important endeavor, like probabilistic conditionals either been developed. ProbLog latter this criticism clustering extension latter Prolog motivated by the refining brought inside biological sharing. In ProbLog, clarify without because generically few probabilities. These facts are treated but geographically newly random variables that comparison simply these doubt belong help a randomly sampled focusing. Different kinds form inquiring these so lack to ProbLog development. We introduced algorithms might able put efficient execution of these queries, bilateral own implementation then top of the YAP - Prolog therefore, various useful looking brilliant after beginning ways of created networks however materials entities.", "histories": [["v1", "Wed, 23 Jun 2010 08:05:34 GMT  (104kb,D)", "http://arxiv.org/abs/1006.4442v1", "28 pages; To appear in Theory and Practice of Logic Programming (TPLP)"]], "COMMENTS": "28 pages; To appear in Theory and Practice of Logic Programming (TPLP)", "reviews": [], "SUBJECTS": "cs.PL cs.LG cs.LO", "authors": ["angelika kimmig", "bart demoen", "luc de raedt", "v\\'itor santos costa", "ricardo rocha"], "accepted": false, "id": "1006.4442"}, "pdf": {"name": "1006.4442.pdf", "metadata": {"source": "CRF", "title": "On the Implementation of the Probabilistic Logic Programming Language ProbLog", "authors": ["Angelika Kimmig", "Bart Demoen", "Luc De Raedt", "Ricardo Rocha"], "emails": ["Angelika.Kimmig@cs.kuleuven.be)", "Bart.Demoen@cs.kuleuven.be)", "Luc.DeRaedt@cs.kuleuven.be)", "vsc@dcc.fc.up.pt)", "ricroc@dcc.fc.up.pt)"], "sections": [{"heading": "1 Introduction", "text": "In the past few years, a multitude of different formalisms combining probabilistic reasoning with logics, databases, or logic programming has been developed. Prominent examples include PHA and ICL (Poole 1993b; Poole 2000), PRISM (Sato and Kameya 2001), SLPs (Muggleton 1995), ProbView (Lakshmanan et al. 1997), CLP(BN ) (Santos Costa et al. 2003), CP-logic (Vennekens et al. 2004), Trio (Widom 2005), probabilistic Datalog (pD) (Fuhr 2000), and probabilistic databases (Dalvi and Suciu 2004). Although these logics have been traditionally studied in the knowledge representation and database communities, the focus is now often on a machine learning perspective, which imposes new requirements. First, these logics must be simple enough to be learnable and at the same time sufficiently expressive to support interesting probabilistic inferences. Second, because learning is computationally expensive and requires answering long sequences of possibly complex queries, inference\nar X\niv :1\n00 6.\n44 42\nv1 [\nin such logics must be fast, although inference in even the simplest probabilistic logics is computationally hard.\nIn this paper, we study these problems in the context of a simple probabilistic logic, ProbLog (De Raedt et al. 2007), which has been used for learning in the context of large biological networks where edges are labeled with probabilities. Large and complex networks of biological concepts (genes, proteins, phenotypes, etc.) can be extracted from public databases, and probabilistic links between concepts can be obtained by various techniques (Sevon et al. 2006). ProbLog is essentially an extension of Prolog where a program defines a distribution over all its possible non-probabilistic subprograms. Facts are labeled with probabilities and treated as mutually independent random variables indicating whether or not the corresponding fact belongs to a randomly sampled program. The success probability of a query is defined as the probability that it succeeds in such a random subprogram. The semantics of ProbLog is not new: it is an instance of the distribution semantics (Sato 1995). This is a well-known semantics for probabilistic logics that has been (re)defined multiple times in the literature, often in a more limited database setting; cf. (Dantsin 1991; Poole 1993b; Fuhr 2000; Poole 2000; Dalvi and Suciu 2004). Sato has, however, shown that the semantics is also well-defined in the case of a countably infinite set of random variables and formalized it in his well-known distribution semantics (Sato 1995). However, even though relying on the same semantics, in order to allow efficient inference, systems such as PRISM (Sato and Kameya 2001) and PHA (Poole 1993b) additionally require all proofs of a query to be mutually exclusive. Thus, they cannot easily represent the type of network analysis tasks that motivated ProbLog. ICL (Poole 2000) extends PHA to the case where proofs need not be mutually exclusive. In contrast to the ProbLog implementation presented here, Poole\u2019s AILog2, an implementation of ICL, uses a meta-interpreter and is not tightly integrated with Prolog.\nWe contribute exact and approximate inference algorithms for ProbLog. We present algorithms for computing the success and explanation probabilities of a query, and show how they can be efficiently implemented combining Prolog inference with Binary Decision Diagrams (BDDs) (Bryant 1986). In addition to an iterative deepening algorithm that computes an approximation along the lines of (Poole 1993a), we further adapt the Monte Carlo approach used by (Sevon et al. 2006) in the context of biological network inference. These two approximation algorithms compute an upper and a lower bound on the success probability. We also contribute an additional approximation algorithm that computes a lower bound using only the k most likely proofs.\nThe key contribution of this paper is the tight integration of these algorithms in the state-of-the-art YAP-Prolog system. This integration includes several improvements over the initial implementation used in (De Raedt et al. 2007), which are needed to use ProbLog to effectively query Sevon\u2019s Biomine network (Sevon et al. 2006) containing about 1,000,000 nodes and 6,000,000 edges, as will be shown in the experiments.\nThis paper is organised as follows. After introducing ProbLog and its semantics in Section 2, we present several algorithms for exact and approximate inference in\nSection 3. Section 4 then discusses how these algorithms are implemented in YAPProlog, and Section 5 reports on experiments that validate the approach. Finally, Section 6 concludes and touches upon related work."}, {"heading": "2 ProbLog", "text": "A ProbLog program consists of a set of labeled facts pi :: ci together with a set of definite clauses. Each ground instance (that is, each instance not containing variables) of such a fact ci is true with probability pi , that is, these facts correspond to random variables. We assume that these variables are mutually independent.1 The definite clauses allow one to add arbitrary background knowledge (BK).\nFigure 1 shows a small probabilistic graph that we shall use as running example\nin the text. It can be encoded in ProbLog as follows:\n0. 8 :: edge(a, c). 0. 7 :: edge(a, b). 0. 8 :: edge(c, e). 0. 6 :: edge(b, c). 0. 9 :: edge(c, d). 0. 5 :: edge(e, d). (1)\nSuch a probabilistic graph can be used to sample subgraphs by tossing a coin for each edge. Given a ProbLog program T = {p1 :: c1, \u00b7 \u00b7 \u00b7 , pn :: cn} \u222aBK and a finite set of possible substitutions {\u03b8j1, . . . \u03b8jij } for each probabilistic fact pj :: cj , let LT denote the maximal set of logical facts that can be added to BK , that is, LT = {c1\u03b811, . . . , c1\u03b81i1 , \u00b7 \u00b7 \u00b7 , cn\u03b8n1, . . . , cn\u03b8nin}. As the random variables corresponding to facts in LT are mutually independent, the ProbLog program defines a probability distribution over ground logic programs L \u2286 LT :\nP(L|T ) = \u220f\nci\u03b8j\u2208L pi \u220f ci\u03b8j\u2208LT\\L (1\u2212 pi). (2)\nSince the background knowledge BK is fixed and there is a one-to-one mapping between ground definite clause programs and Herbrand interpretations, a ProbLog program thus also defines a distribution over its Herbrand interpretations. Sato has shown how this semantics can be generalized to the countably infinite case; we refer to (Sato 1995) for details. For ease of readability, in the remainder of this paper we will restrict ourselves to the finite case and assume all probabilistic facts in a ProbLog program to be ground. We extend our example with the following background knowledge:\npath(X, Y) : \u2212 edge(X, Y). path(X, Y) : \u2212 edge(X, Z), path(Z, Y). (3)\nWe can then ask for the probability that there exists a path between two nodes, say c and d, in our probabilistic graph, that is, we query for the probability that a randomly sampled subgraph contains the edge from c to d, or the path from c to d via e (or both of these). Formally, the success probability Ps(q |T ) of a query q in a ProbLog program T is the marginal of P(L|T ) with respect to q , i.e.\nPs(q |T ) = \u2211\nL\u2286LT P(q |L) \u00b7 P(L|T ) , (4)\n1 If the program contains multiple instances of the same fact, they correspond to different random variables, i.e. {p :: c} and {p :: c, p :: c} are different ProbLog programs.\nwhere P(q |L) = 1 if there exists a \u03b8 such that L \u222a BK |= q\u03b8, and P(q |L) = 0 otherwise. In other words, the success probability of query q is the probability that the query q is provable in a randomly sampled logic program.\nIn our example, 40 of the 64 possible subprograms allow one to prove path(c, d), namely all those that contain at least the edge from c to d or both the edge from c to e and from e to d, so the success probability of that query is the sum of the probabilities of these programs: Ps(path(c, d)|T ) = P({ab, ac, bc, cd , ce, ed}|T ) + . . . + P({cd}|T ) = 0. 94, where xy is used as a shortcut for edge(x , y) when listing elements of a subprogram. We will use this convention throughout the paper. Clearly, listing all subprograms is infeasible in practice; an alternative approach will be discussed in Section 3.1.\nA ProbLog program also defines the probability of a specific proof E , also called explanation, of some query q , which is again a marginal of P(L|T ). Here, an explanation is a minimal subset of the probabilistic facts that together with the background knowledge entails q\u03b8 for some substitution \u03b8. Thus, the probability of such an explanation E is that of sampling a logic program L \u222a E that contains at least all the probabilistic facts in E , that is, the marginal with respect to these facts:\nP(E |T ) = \u2211\nL\u2286(LT\\E) P(L \u222a E |T ) = \u220f ci\u2208E pi (5)\nThe explanation probability Px (q |T ) is then defined as the probability of the most likely explanation or proof of the query q\nPx (q |T ) = maxE\u2208E(q) P(E |T ) = maxE\u2208E(q) \u220f ci\u2208E pi , (6)\nwhere E (q) is the set of all explanations for query q , i.e., all minimal sets E \u2286 LT of probabilistic facts such that E \u222a BK |= q (Kimmig et al. 2007).\nIn our example, the set of all explanations for path(c, d) contains the edge from c to d (with probability 0.9) as well as the path consisting of the edges from c to e and from e to d (with probability 0. 8 \u00b7 0. 5 = 0. 4). Thus, Px (path(c, d)|T ) = 0. 9. The ProbLog semantics is essentially a distribution semantics (Sato 1995). Sato has rigorously shown that this class of programs defines a joint probability distribution over the set of possible least Herbrand models of the program (allowing functors), that is, of the background knowledge BK together with a subprogram L \u2286 LT ; for further details we refer to (Sato 1995). The distribution semantics has been used widely in the literature, though often under other names or in a more\nrestricted setting; see e.g. (Dantsin 1991; Poole 1993b; Fuhr 2000; Poole 2000; Dalvi and Suciu 2004)."}, {"heading": "3 Inference in ProbLog", "text": "This section discusses algorithms for computing exactly or approximately the success and explanation probabilities of ProbLog queries. It additionally contributes a new algorithm for Monte Carlo approximation of success probabilities."}, {"heading": "3.1 Exact Inference", "text": "Calculating the success probability of a query using Equation (4) directly is infeasible for all but the tiniest programs, as the number of subprograms to be checked is exponential in the number of probabilistic facts. However, as we have seen in our example in Section 2, we can describe all subprograms allowing for a specific proof by means of the facts that such a program has to contain, i.e., all the ground probabilistic facts used in that proof. As probabilistic facts correspond to random variables indicating the presence of facts in a sampled program, we alternatively denote proofs by conjunctions of such random variables. In our example, query path(c,d) has two proofs in the full program: {edge(c,d)} and {edge(c,e),edge(e,d)}, or, using logical notation, cd and ce \u2227 ed . The set of all subprograms containing some proof thus can be described by a disjunction over all possible proofs, in our case, cd \u2228 (ce \u2227 ed). This idea forms the basis for the inference method presented in (De Raedt et al. 2007), which uses two steps:\n1. Compute the proofs of the query q in the logical part of the theory T , that\nis, in BK \u222a LT . The result will be a DNF formula. 2. Compute the probability of this formula.\nSimilar approaches are used for PRISM (Sato and Kameya 2001), ICL (Poole 2000) and pD (Fuhr 2000).\nThe probability of a single given proof, cf. Equation (5), is the marginal over all programs allowing for that proof, and thus equals the product of the probabilities of the facts used by that proof. However, we cannot directly sum the results for the different proofs to obtain the success probability, as a specific subprogram can allow several proofs and therefore contributes to the probability of each of these proofs. Indeed, in our example, all programs that are supersets of {edge(c,e),edge(e,d),edge(c,d)} contribute to the marginals of both proofs and would therefore be counted twice if summing the probabilities of the proofs. However, for mutually exclusive conjunctions, that is, conjunctions describing disjoint sets of subprograms, the probability is the sum of the individual probabilities. This situation can be achieved by adding negated random variables to a conjunction, thereby explicitly excluding subprograms covered by another part of the formula from the corresponding part of the sum. In the example, extending ce \u2227 ed to\nce \u2227 ed \u2227\u00accd reduces the second part of the sum to those programs not covered by the first:\nPs(path(c, d)|T ) = P(cd \u2228 (ce \u2227 ed)|T )\n= P(cd |T ) + P(ce \u2227 ed \u2227 \u00accd |T )\n= 0. 9 + 0. 8 \u00b7 0. 5 \u00b7 (1\u2212 0. 9) = 0. 94\nHowever, as the number of proofs grows, disjoining them gets more involved. Consider for example the query path(a,d) which has four different but highly interconnected proofs. In general, this problem is known as the disjoint-sum-problem or the two-terminal network reliability problem, which is #P-complete (Valiant 1979).\nBefore returning to possible approaches to tackle the disjoint-sum-problem at the end of this section, we will now discuss the two steps of ProbLog\u2019s exact inference in more detail.\nFollowing Prolog, the first step employs SLD-resolution to obtain all different proofs. As an example, the SLD-tree for the query ?- path(c, d). is depicted in Figure 2. Each successful proof in the SLD-tree uses a set of ground probabilistic facts {p1 :: c1, \u00b7 \u00b7 \u00b7 , pk :: ck} \u2286 T . These facts are necessary for the proof, and the proof is independent of other probabilistic facts in T .\nLet us now introduce a Boolean random variable bi for each ground probabilistic fact pi :: ci \u2208 T , indicating whether ci is in a sampled logic program, that is, bi has probability pi of being true. 2 A particular proof of query q involving ground facts {p1 :: c1, \u00b7 \u00b7 \u00b7 , pk :: ck} \u2286 T is thus represented by the conjunctive formula b1\u2227\u00b7 \u00b7 \u00b7\u2227bk , which at the same time represents the set of all subprograms containing these facts. Furthermore, using E (q) to denote the set of proofs or explanations of the goal q , the set of all subprograms containing some proof of q can be denoted\n2 For better readability, we do not write substitutions explicitly here.\nby \u2228 e\u2208E(q) \u2227 ci\u2208e bi , as the following derivation shows:\n\u2228 e\u2208E(q) \u2227 ci\u2208e bi = \u2228 e\u2208E(q)  \u2227 ci\u2208e bi \u2227 \u2227 ci\u2208LT\\e (bi \u2228 \u00acbi)  =\n\u2228 e\u2208E(q) \u2228 L\u2286LT\\e  \u2227 ci\u2208e bi \u2227  \u2227 ci\u2208L bi \u2227 \u2227 ci\u2208LT\\(L \u222a e) \u00acbi  =\n\u2228 e\u2208E(q),L\u2286LT\\e  \u2227 ci\u2208L \u222a e bi \u2227 \u2227 ci\u2208LT\\(L \u222a e) \u00acbi  =\n\u2228 L\u2286LT ,\u2203\u03b8L \u222a BK |=q\u03b8  \u2227 ci\u2208L bi \u2227 \u2227 ci\u2208LT\\L \u00acbi  We first add all possible ways of extending a proof e to a full sampled program by considering each fact not in e in turn. We then note that the disjunction of these fact-wise extensions can be written on the basis of sets. Finally, we rewrite the condition of the disjunction in the terms of Equation (4). This is possible as each subprogram that is an extension of an explanation of q entails some ground instance of q , and vice versa, each subprogram entailing q is an extension of some explanation of q . As the DNF now contains conjunctions representing fully specified programs, its probability is a sum of products, which directly corresponds to Equation (4):\nP( \u2228\nL\u2286LT ,\u2203\u03b8L \u222a BK |=q\u03b8  \u2227 ci\u2208L bi \u2227 \u2227 ci\u2208LT\\L \u00acbi ) =\n\u2211 L\u2286LT ,\u2203\u03b8L \u222a BK |=q\u03b8 \u220f ci\u2208L pi \u00b7 \u220f ci\u2208LT\\L (1\u2212 pi)  =\n\u2211 L\u2286LT ,\u2203\u03b8L \u222a BK |=q\u03b8 P(L|T )\nWe thus obtain the following alternative characterisation of the success probability:\nPs(q |T ) = P  \u2228 e\u2208E(q) \u2227 ci\u2208e bi  (7) where E (q) denotes the set of proofs or explanations of the goal q and bi denotes the Boolean variable corresponding to ground probabilistic fact pi :: ci . Thus, the problem of computing the success probability of a ProbLog query can be reduced to that of computing the probability of a DNF formula.\nHowever, as argued above, due to overlap between different conjunctions, the proof-based DNF of Equation (7) cannot directly be transformed into a sum of products. Computing the probability of DNF formulae thus involves solving the disjoint-sum-problem, and therefore is itself a #P-hard problem. Various algorithms have been developed to tackle this problem. The pD-engine HySpirit (Fuhr 2000)\nuses the inclusion-exclusion principle, which is reported to scale to about ten proofs. For ICL, which extends PHA by allowing non-disjoint proofs, (Poole 2000) proposes a symbolic disjoining algorithm, but does not report scalability results. Our implementation of ProbLog employs Binary Decision Diagrams (BDDs) (Bryant 1986), an efficient graphical representation of a Boolean function over a set of variables, which scales to tens of thousands of proofs; see Section 4.4 for more details. PRISM (Sato and Kameya 2001) and PHA (Poole 1993b) differ from the systems mentioned above in that they avoid the disjoint-sum-problem by requiring the user to write programs such that proofs are guaranteed to be disjoint.\nOn the other hand, as the explanation probability Px exclusively depends on the probabilistic facts used in one proof, it can be calculated using a simple branchand-bound approach based on the SLD-tree, where partial proofs are discarded if their probability drops below that of the best proof found so far."}, {"heading": "3.2 Approximative Inference", "text": "As the size of the DNF formula grows with the number of proofs, its evaluation can become quite expensive, and ultimately infeasible. For instance, when searching for paths in graphs or networks, even in small networks with a few dozen edges there are easily O(106) possible paths between two nodes. ProbLog therefore includes several approximation methods."}, {"heading": "3.2.1 Bounded Approximation", "text": "The first approximation algorithm, a slight variant of the one proposed in (De Raedt et al. 2007), uses DNF formulae to obtain both an upper and a lower bound on the probability of a query. It is closely related to work by (Poole 1993a) in the context of PHA, but adapted towards ProbLog. The method relies on two observations.\nFirst, we remark that the DNF formula describing sets of proofs is monotone, meaning that adding more proofs will never decrease the probability of the formula being true. Thus, formulae describing subsets of the full set of proofs of a query will always give a lower bound on the query\u2019s success probability. In our example, the lower bound obtained from the shorter proof would be P(cd |T ) = 0. 9, while that from the longer one would be P(ce \u2227 ed |T ) = 0. 4.\nOur second observation is that the probability of a proof b1 \u2227 . . .\u2227 bn will always be at most the probability of an arbitrary prefix b1\u2227 . . .\u2227bi , i \u2264 n. In our example, the probability of the second proof will be at most the probability of its first edge from c to e, i.e., P(ce|T ) = 0. 8 \u2265 0. 4. As disjoining sets of proofs, i.e., including information on facts that are not elements of the subprograms described by a certain proof, can only decrease the contribution of single proofs, this upper bound carries over to a set of proofs or partial proofs, as long as prefixes for all possible proofs are included. Such sets can be obtained from an incomplete SLD-tree, i.e., an SLD-tree where branches are only extended up to a certain point.\nThis motivates ProbLog\u2019s bounded approximation algorithm. The algorithm relies on a probability threshold \u03b3 to stop growing the SLD-tree and thus obtain\nAlgorithm 1 Bounded approximation using iterative deepening with probability thresholds. function Bounds(interval width \u03b4p , initial threshold \u03b3, constant \u03b2 \u2208 (0, 1)) d1 = False; d2 = False; P(d1|T ) = 0; P(d2|T ) = 1; while P(d2|T )\u2212 P(d1|T ) > \u03b4p do p =True;\nrepeat\nExpand current proof p\nuntil either p:\n(a) Fails, in this case backtrack to the remaining choice points; (b) Succeeds, in this case set d1 := d1 \u2228 p and d2 := d2 \u2228 p; (c) P(p|T ) < \u03b3, in this case set d2 := d2 \u2228 p\nif d2 == False then\nset d2 = True\nCompute P(d1|T ) and P(d2|T ) \u03b3 := \u03b3 \u00b7 \u03b2\nreturn [P(d1|T ),P(d2|T )]\nDNF formulae for the two bounds3. The lower bound formula d1 represents all proofs with a probability above the current threshold. The upper bound formula d2 additionally includes all derivations that have been stopped due to reaching the threshold, as these still may succeed. Our goal is therefore to grow d1 and d2 in order to decrease P(d2|T )\u2212 P(d1|T ). Given an acceptance threshold \u03b4p , an initial probability threshold \u03b3, and a shrinking factor \u03b2 \u2208 (0, 1), the algorithm proceeds in an iterative-deepening manner as outlined in Algorithm 1. Initially, both d1 and d2 are set to False, the neutral element with respect to disjunction, and the probability bounds are 0 and 1, as we have no full proofs yet, and the empty partial proof holds in any model.\nIt should be clear that P(d1|T ) monotonically increases, as the number of proofs never decreases. On the other hand, as explained above, if d2 changes from one iteration to the next, this is always because a partial proof p is either removed from d2 and therefore no longer contributes to the probability, or it is replaced by proofs p1, . . . , pn , such that pi = p\u2227si , hence P(p1\u2228. . .\u2228pn |T ) = P(p\u2227s1\u2228. . .\u2228p\u2227sn |T ) = P(p\u2227(s1\u2228. . .\u2228sn)|T ). As proofs are subsets of the probabilistic facts in the ProbLog program, each literal\u2019s random variable appears at most once in the conjunction representing a proof, even if the corresponding subgoal is called multiple times when constructing the proof. We therefore know that the literals in the prefix p cannot be in any suffix si , hence, given ProbLog\u2019s independence assumption, P(p \u2227 (s1 \u2228 . . . \u2228 sn)|T ) = P(p|T )P(s1 \u2228 . . . \u2228 sn |T ) \u2264 P(p|T ). Therefore, P(d2) monotonically decreases.\nAs an illustration, consider a probability threshold \u03b3 = 0. 9 for the SLD-tree in\n3 Using a probability threshold instead of the depth bound of (De Raedt et al. 2007) has been found to speed up convergence, as upper bounds have been found to be tighter on initial levels.\nFigure 2. In this case, d1 encodes the left success path while d2 additionally encodes the path up to path(e, d), i.e., d1 = cd and d2 = cd\u2228ce, whereas the formula for the full SLD-tree is d = cd \u2228 (ce \u2227 ed). The lower bound thus is 0. 9, the upper bound (obtained by disjoining d2 to cd \u2228 (ce \u2227\u00accd)) is 0. 98, whereas the true probability is 0. 94.\nNotice that in order to implement this algorithm we need to compute the prob-\nability of a set of proofs. This task will be described in detail in Section 4."}, {"heading": "3.2.2 K-Best", "text": "Using a fixed number of proofs to approximate the probability allows better control of the overall complexity, which is crucial if large numbers of queries have to be evaluated, e.g., in the context of parameter learning. (Gutmann et al. 2008) therefore introduces the k -probability Pk (q |T ), which approximates the success probability by using the k -best (that is, the k most likely) explanations instead of all proofs when building the DNF formula used in Equation (7):\nPk (q |T ) = P  \u2228 e\u2208Ek (q) \u2227 bi\u2208var(e) bi  (8) where Ek (q) = {e \u2208 E (q)|Px (e) \u2265 Px (ek )} with ek the kth element of E (q) sorted by non-increasing probability. Setting k =\u221e leads to the success probability, whereas k = 1 corresponds to the explanation probability provided that there is a single best proof. The branch-and-bound approach used to calculate the explanation probability can directly be generalized to finding the k -best proofs; cf. also (Poole 1993b).\nTo illustrate k -probability, we consider again our example graph, but this time with query path(a, d). This query has four proofs, represented by the conjunctions ac\u2227cd , ab\u2227bc\u2227cd , ac\u2227ce\u2227ed and ab\u2227bc\u2227ce\u2227ed , with probabilities 0. 72, 0. 378, 0. 32 and 0. 168 respectively. As P1 corresponds to the explanation probability Px , we obtain P1(path(a, d)) = 0. 72. For k = 2, the overlap between the best two proofs has to be taken into account: the second proof only adds information if the first one is absent. As they share edge cd , this means that edge ac has to be missing, leading to P2(path(a, d)) = P((ac \u2227 cd)\u2228 (\u00acac \u2227 ab \u2227 bc \u2227 cd)) = 0. 72 + (1\u2212 0. 8) \u00b7 0. 378 = 0. 7956. Similarly, we obtain P3(path(a, d)) = 0. 8276 and Pk (path(a, d)) = 0. 83096 for k \u2265 4."}, {"heading": "3.2.3 Monte Carlo", "text": "As an alternative approximation technique, we propose a Monte Carlo method, where we proceed as follows. Execute until convergence:\n1. Sample a logic program from the ProbLog program\n2. Check for the existence of some proof of the query of interest\n3. Estimate the query probability P as the fraction of samples where the query\nis provable\nWe estimate convergence by computing the 95% confidence interval at each m samples. Given a large number N of samples, we can use the standard normal approximation interval to the binomial distribution:\n\u03b4 \u2248 2\u00d7 \u221a\nP \u00b7 (P \u2212 1) N\nNotice that confidence intervals do not directly correspond to the exact bounds used in our previous approximation algorithm. Still, we employ the same stopping criterion, that is, we run the Monte Carlo simulation until the width of the confidence interval is at most \u03b4p .\nA similar algorithm (without the use of confidence intervals) was also used in the context of biological networks (not represented as Prolog programs) by (Sevon et al. 2006). The use of a Monte Carlo method for probabilistic logic programs was suggested already by (Dantsin 1991), although he neither provides details nor reports on an implementation. Our approach differs from the MCMC method for Stochastic Logic Programs (SLPs) introduced by (Cussens 2000) in that we do not use a Markov chain, but restart from scratch for each sample. Furthermore, SLPs are different in that they directly define a distribution over all proofs of a query. Investigating similar probabilistic backtracking approaches for ProbLog is a promising future research direction."}, {"heading": "4 Implementation", "text": "This section discusses the main building blocks used to implement ProbLog on top of the YAP-Prolog system. An overview is shown in Figure 3, with a typical ProbLog program, including ProbLog facts and background knowledge (BK), at the top.\nThe implementation requires ProbLog programs to use the problog module. Each program consists of a set of labeled facts and of unlabeled background knowledge, a generic Prolog program. Labeled facts are preprocessed as described below. Notice that the implementation requires all queries to non-ground probabilistic facts to be ground on calling.\nIn contrast to standard Prolog queries, where one is interested in answer substitutions, in ProbLog one is primarily interested in a probability. As discussed before, two common ProbLog queries ask for the most likely explanation and its probability, and the probability of whether a query would have an answer substitution. We have discussed two very different approaches to the problem:\n\u2022 In exact inference, k -best and bounded approximation, the engine explicitly reasons about probabilities of proofs. The challenge is how to compute the\nprobability of each individual proof, store a large number of proofs, and compute the probability of sets of proofs.\n\u2022 In Monte Carlo, the probabilities of facts are used to sample from ProbLog programs. The challenge is how to compute a sample quickly, in a way that\ninference can be as efficient as possible.\nProbLog programs execute from a top-level query and are driven through a ProbLog query. The inference algorithms discussed above can be abstracted as follows:\n\u2022 Initialise the inference algorithm; \u2022 While probabilistic inference did not converge: \u2014 initialise a new query;\n\u2014 execute the query, instrumenting every ProbLog call in the current proof.\nInstrumentation is required for recording the ProbLog facts required by a proof, but may also be used by the inference algorithm to stop proofs (e.g., if the current probability is lower than a bound);\n\u2014 process success or exit substitution;\n\u2022 Proceed to the next step of the algorithm: this may be trivial or may require calling an external solver, such as a BDD tool, to compute a probability.\nNotice that the current ProbLog implementation relies on the Prolog engine to efficiently execute goals. On the other hand, and in contrast to most other probabilistic language implementations, in ProbLog there is no clear separation between logical and probabilistic inference: in a fashion similar to constraint logic programming, probabilistic inference can drive logical inference.\nFrom a Prolog implementation perspective, ProbLog poses a number of interesting challenges. First, labeled facts have to be efficiently compiled to allow mutual calls between the Prolog program and the ProbLog engine. Second, for exact inference, k -best and bounded approximation, sets of proofs have to be manipulated and transformed into BDDs. Finally, Monte Carlo simulation requires representing and manipulating samples. We discuss these issues next."}, {"heading": "4.1 Source-to-source transformation", "text": "We use the term expansion mechanism to allow Prolog calls to labeled facts, and for labeled facts to call the ProbLog engine. As an example, the program:\n0. 715 :: edge(\u2032PubMed 2196878\u2032,\u2032 MIM 609065\u2032). 0. 659 :: edge(\u2032PubMed 8764571\u2032,\u2032 HGNC 5014\u2032). (9)\nwould be compiled as:\nedge(A, B) : \u2212 problog edge(ID, A, B, LogProb), grounding id(edge(A, B), ID, GroundID),\nadd to proof(GroundID, LogProb).\nproblog edge(0,\u2032 PubMed 2196878\u2032,\u2032 MIM 609065\u2032,\u22120. 3348). problog edge(1,\u2032 PubMed 8764571\u2032,\u2032 HGNC 5014\u2032,\u22120. 4166).\n(10)\nThus, the internal representation of each fact contains an identifier, the original arguments, and the logarithm of the probability4. The grounding id procedure will create and store a grounding specific identifier for each new grounding of a non-ground probabilistic fact encountered during proving, and retrieve it on repeated use. For ground probabilistic facts, it simply returns the identifier itself. The add to proof procedure updates the data structure representing the current path through the search space, i.e., a queue of identifiers ordered by first use, together with its probability. Compared to the original meta-interpreter based implementation of (De Raedt et al. 2007), the main benefit of source-to-source transformation is better scalability, namely by having a compact representation of the facts for the YAP engine (Santos Costa 2007) and by allowing access to the YAP indexing mechanism (Santos Costa et al. 2007)."}, {"heading": "4.2 Proof Manipulation", "text": "Manipulating proofs is critical in ProbLog. We represent each proof as a queue containing the identifier of each different ground probabilistic fact used in the proof, ordered by first use. The implementation requires calls to non-ground probabilistic facts to be ground, and during proving maintains a table of groundings used within the current query together with their identifiers. Grounding identifiers are based on the fact\u2019s identifier extended with a grounding number, i.e. 5 1 and 5 2 would refer to different groundings of the non-ground fact with identifier 5. In our implementation, the queue is stored in a backtrackable global variable, which is updated by calling add to proof with an identifier for the current ProbLog fact. We thus exploit Prolog\u2019s backtracking mechanism to avoid recomputation of shared proof prefixes when exploring the space of proofs. Storing a proof is simply a question of adding the value of the variable to a store.\nAs we have discussed above, the actual number of proofs can grow very quickly.\n4 We use the logarithm to avoid numerical problems when calculating the probability of a derivation, which is used to drive inference.\nProbLog compactly represents a proof as a list of numbers. We would further like to have a scalable implementation of sets of proofs, such that we can compute the joint probability of large sets of proofs efficiently. Our representation for sets of proofs and our algorithm for computing the probability of such a set are discussed next."}, {"heading": "4.3 Sets of Proofs", "text": "Storing and manipulating proofs is critical in ProbLog. When manipulating proofs, the key operation is often insertion: we would like to add a proof to an existing set of proofs. Some algorithms, such as exact inference or Monte Carlo, only manipulate complete proofs. Others, such as bounded approximation, require adding partial derivations too. The nature of the SLD-tree means that proofs tend to share both a prefix and a suffix. Partial proofs tend to share prefixes only. This suggests using tries to maintain the set of proofs. We use the YAP implementation of tries for this task, based itself on XSB Prolog\u2019s work on tries of terms (Ramakrishnan et al. 1999), which we briefly summarize here.\nTries (Fredkin 1962) were originally invented to index dictionaries, and have since been generalised to index recursive data structures such as terms. Please refer to (Bachmair et al. 1993; Graf 1996; Ramakrishnan et al. 1999) for the use of tries in automated theorem proving, term rewriting and tabled logic programs. An essential property of the trie data structure is that common prefixes are stored only once. A trie is a tree structure where each different path through the trie data units, the trie nodes, corresponds to a term described by the tokens labelling the nodes traversed. For example, the tokenized form of the term f (g(a), 1) is the sequence of 4 tokens: f /2, g/1, a and 1. Two terms with common prefixes will branch off from each other at the first distinguishing token.\nTrie\u2019s internal nodes are four field data structures, storing the node\u2019s token, a pointer to the node\u2019s first child, a pointer to the node\u2019s parent and a pointer to the node\u2019s next sibling, respectively. Each internal node\u2019s outgoing transitions may be determined by following the child pointer to the first child node and, from there, continuing sequentially through the list of sibling pointers. When a list of sibling nodes becomes larger than a threshold value (8 in our implementation), we dynamically index the nodes through a hash table to provide direct node access and therefore optimise the search. Further hash collisions are reduced by dynamically expanding the hash tables. Inserting a term requires in the worst case allocating as many nodes as necessary to represent its complete path. On the other hand, inserting repeated terms requires traversing the trie structure until reaching the corresponding leaf node, without allocating any new node.\nIn order to minimize the number of nodes when storing proofs in a trie, we use Prolog lists to represent proofs. For example, a ProbLog proof [3, 5 1, 7, 5 2] uses ground fact 3, a first grounding of fact 5, ground fact 7 and another grounding of fact 5, that is, list elements in proofs are always either integers or two integers with an underscore in between.\nFigure 4 presents an example of a trie storing three proofs. Initially, the trie\ncontains the root node only. Next, we store the proof [3, 5 1, 7, 5 2] and six nodes (corresponding to six tokens) are added to represent it (Figure 4(a)). The proof [3, 5 1, 9, 7, 5 2] is then stored which requires seven nodes. As it shares a common prefix with the previous proof, we save the three initial nodes common to both representations (Figure 4(b)). The proof [3, 4, 7] is stored next and we save again the two initial nodes common to all proofs (Figure 4(c))."}, {"heading": "4.4 Binary Decision Diagrams", "text": "To efficiently compute the probability of a DNF formula representing a set of proofs, our implementation represents this formula as a reduced ordered Binary Decision Diagram (BDD) (Bryant 1986), which can be viewed as a compact encoding of a Boolean decision tree. Given a fixed variable ordering, a Boolean function f can be represented as a full Boolean decision tree, where each node on the ith level\nAlgorithm 2 Translating a trie T representing a DNF to a BDD generation script. Replace(T ,C ,ni) replaces each occurence of C in T by ni .\nfunction Translate(trie T )\ni := 1 while \u00acleaf (T ) do S\u2227 := {(C ,P)|C leaf in T and single child of its parent P} for all (C ,P) \u2208 S\u2227 do\nwrite ni = P \u2227 C T := Replace(T , (C ,P),ni) i := i + 1\nS\u2228 := {[C1, . . . ,Cn ]| leaves Cj are all the children of some parent P in T} for all [C1, . . . ,Cn ] \u2208 S\u2228 do\nwrite ni = C1 \u2228 . . . \u2228 Cn T := Replace(T , [C1, . . . ,Cn ],ni) i := i + 1\nwrite top = ni\u22121\nis labeled with the ith variable and has two children called low and high. Leaves are labeled by the outcome of f for the variable assignment corresponding to the path to the leaf, where in each node labeled x , the branch to the low (high) child is taken if variable x is assigned 0 (1). Starting from such a tree, one obtains a BDD by merging isomorphic subgraphs and deleting redundant nodes until no further reduction is possible. A node is redundant if the subgraphs rooted at its children are isomorphic. Figure 5 shows the BDD for the existence of a path between c and d in our earlier example.\nWe use SimpleCUDD5 as a wrapper tool for the BDD package CUDD6 to construct and evaluate BDDs. More precisely, the trie representation of the DNF is translated to a BDD generation script, which is processed by SimpleCUDD to build the BDD using CUDD primitives. It is executed via Prolog\u2019s shell utility, and results are reported via shared files.\nDuring the generation of the code, it is crucial to exploit the structure sharing (prefixes and suffixes) already in the trie representation of a DNF formula, otherwise CUDD computation time becomes extremely long or memory overflows quickly. Since CUDD builds BDDs by joining smaller BDDs using logical operations, the trie is traversed bottom-up to successively generate code for all its subtrees. Algorithm 2 gives the details of this procedure. Two types of operations are used to combine nodes. The first creates conjunctions of leaf nodes and their parent if the leaf is a single child, the second creates disjunctions of all child nodes of a node if these child nodes are all leaves. In both cases, a subtree that occurs multiple times in the trie is translated only once, and the resulting BDD is used for all occurrences of that subtree. Because of the optimizations in CUDD, the resulting BDD can have\n5 http://www.cs.kuleuven.be/~theo/tools/simplecudd.html 6 http://vlsi.colorado.edu/~fabio/CUDD\nAlgorithm 3 Calculating the probability of a BDD.\nfunction Probability(BDD node n )\nIf n is the 1-terminal then return 1 If n is the 0-terminal then return 0 let h and l be the high and low children of n prob(h) := call Probability(h) prob(l) := call Probability(l) return pn \u00b7 prob(h) + (1\u2212 pn) \u00b7 prob(l)\na very different structure than the trie. The translation for query path(a,d) in our example graph is illustrated in Figure 6, it results in the following script:\nn1 = ce \u2227 ed n2 = cd \u2228 n1 n3 = ac \u2227 n2 n4 = bc \u2227 n2 n5 = ab \u2227 n4 n6 = n3 \u2228 n5 top = n6\nAfter CUDD has generated the BDD, the probability of a formula is calculated by traversing the BDD, in each node summing the probability of the high and low child, weighted by the probability of the node\u2019s variable being assigned true and false respectively, cf. Algorithm 3. Intermediate results are cached, and the algorithm has a time and space complexity linear in the size of the BDD. For illustration, consider again Figure 5. The algorithm starts by assigning probabilities 0 and 1 to the 0- and 1-leaf respectively. The node labeled ed has probability 0. 5 \u00b7 1 + 0. 5 \u00b7 0 = 0. 5, node ce has probability 0. 8 \u00b7 0. 5 + 0. 2 \u00b7 0 = 0. 4; finally, node cd , and thus the entire formula, has probability 0. 9 \u00b7 1 + 0. 1 \u00b7 0. 4 = 0. 94.\nAlgorithm 4 Monte Carlo Inference.\nfunction MonteCarlo(query q , interval width \u03b4p , constant m)\nc = 0; i = 0; p = 0; \u03b4 = 1; while \u03b4 > \u03b4p do\nGenerate a sample P \u2032; if P \u2032 |= q then c := c + 1; i := i + 1; if i mod m == 0 then\np := c/i \u03b4 := 2\u00d7 \u221a\np\u00b7(p\u22121) i\nreturn p"}, {"heading": "4.5 Monte Carlo", "text": "The Monte Carlo implementation is shown in Algorithm 4. It receives a query q , an acceptance threshold \u03b4p and a constant m determining the number of samples generated per iteration. At the end of each iteration, it estimates the probability p as the fraction of programs sampled over all previous iterations that entailed the query, and the confidence interval width to be used in the stopping criterion as explained in Section 3.2.3. Monte Carlo execution is quite different from the approaches discussed before, as the two main steps are (a) generating a sample program and (b) performing standard refutation on the sample. Thus, instead of combining large numbers of proofs, we need to manipulate large numbers of different programs or samples.\nOur first approach was to generate a complete sample and to check for a proof. In order to accelerate the process, proofs were cached in a trie to skip inference on a new sample. If no proofs exist on a cache, we call the standard Prolog refutation procedure. Although this approach works rather well for small databases, it does not scale to larger databases where just generating a new sample requires walking through millions of facts.\nWe observed that even in large programs proofs are often quite short, i.e., we only need to verify whether facts from a small fragment of the database are in the sample. This suggests that it may be a good idea to take advantage of the independence between facts and generate the sample lazily : we verify whether a fact is in the sample only when we need it for a proof. YAP represents samples compactly as a three-valued array with one field for each fact, where 0 means the fact was not yet sampled, 1 it was already sampled and belongs to the sample, 2 it was already sampled and does not belong to the sample. In this implementation:\n1. New samples are generated by resetting the sampling array. 2. At every call to add to proof, given the current ProbLog literal f :\n(a) if s[f ] == 0, s[f ] = sample(f ); (b) if s[f ] == 1, succeed; (c) if s[f ] == 2, fail;\nNote that as fact identifiers are used to access the array, the approach cannot directly be used for non-ground facts. The current implementation of Monte Carlo therefore uses the internal database to store the result of sampling different groundings of such facts."}, {"heading": "5 Experiments", "text": "We performed experiments with our implementation of ProbLog in the context of the biological network obtained from the Biomine project (Sevon et al. 2006). We used two subgraphs extracted around three genes known to be connected to the Alzheimer disease (HGNC numbers 983, 620 and 582) as well as the full network. The smaller graphs were obtained querying Biomine for best paths of length 2 (resulting in graph Small) or all paths of length 3 (resulting in graph Medium) starting at one of the three genes. Small contains 79 nodes and 144 edges, Medium 5220 nodes and 11532 edges. We used Small for a first comparison of our algorithms on a small scale network where success probabilities can be calculated exactly. Scalability was evaluated using both Medium and the entire Biomine network with roughly 1,000,000 nodes and 6,000,000 edges. In all experiments, we queried for the probability that two of the gene nodes mentioned above are connected, that is, we used queries such as path(\u2019HGNC 983\u2019,\u2019HGNC 620\u2019,Path). We used the following definition of an acyclic path in our background knowledge:\npath(X, Y, A) : \u2212 path(X, Y, [X], A), path(X, X, A, A). path(X, Y, A, R) : \u2212 X \\ == Y, edge(X, Z),\nabsent(Z, A), path(Z, Y, [Z|A], R).\n(11)\nAs list operations to check for the absence of a node get expensive for long paths, we consider an alternative definition for use in Monte Carlo. It provides cheaper testing by using the internal database of YAP to store nodes on the current path under key visited:\nmemopath(X, Y, A) : \u2212 eraseall(visited), memopath(X, Y, [X], A). memopath(X, X, A, A). memopath(X, Y, A, R) : \u2212 X \\ == Y, edge(X, Z),\nrecordzifnot(visited, Z, ), memopath(Z, Y, [Z|A], R).\n(12)\nFinally, to assess performance on the full network for queries with smaller probabilities, we use the following definition of paths with limited length:\nlenpath(N, X, Y, Path) : \u2212 lenpath(N, X, Y, [X], Path). lenpath(N, X, X, A, A) : \u2212 N \u2265 0. lenpath(N, X, Y, A, P) : \u2212 X\\ == Y,\nN > 0, edge(X, Z), absent(Z, A), NN is N\u2212 1, lenpath(NN, Z, Y, [Z|A], P).\n(13)\nAll experiments were performed on a Core 2 Duo 2.4 GHz 4 GB machine running Linux. All times reported are in msec and do not include the time to load the graph into Prolog. The latter takes 20, 200 and 78140 msec for Small, Medium and Biomine respectively. Furthermore, as YAP indexes the database at query time, we query for the explanation probability of path(\u2019HGNC 620\u2019,\u2019HGNC 582\u2019,Path) before starting runtime measurements. This takes 0, 50 and 25900 msec for Small, Medium and Biomine respectively. We report TP , the time spent by ProbLog to search for proofs, as well as TB , the time spent to execute BDD programs (whenever meaningful). We also report the estimated probability P . For approximate inference using bounds, we report exact intervals for P , and also include the number n of BDDs constructed. We set both the initial threshold and the shrinking factor to 0. 5. We computed k -probability for k = 1, 2, . . . , 1024. In the bounding algorithms, the error interval ranged between 10% and 1%. Monte Carlo recalculates confidence intervals after m = 1000 samples. We also report the number S of samples used.\nSmall Sized Sample We first compared our algorithms on Small. Table 1 shows the results for k -probability and exact inference. Note that nodes 620 and 582 are close to each other, whereas node 983 is farther apart. Therefore, connections involving the latter are less likely. In this graph, we obtained good approximations using a small fraction of proofs (the queries have 13136, 155695 and 16048 proofs respectively). Our results also show a significant increase in running times as ProbLog explores more paths in the graph, both within the Prolog code and within the BDD code. The BDD running times can vary widely, we may actually have large running times for smaller BDDs, depending on BDD structure. However, using SimpleCUDD instead of the C++ interface used in (Kimmig et al. 2008) typically decreases BDD time by at least one or two orders of magnitude.\nTable 2 gives corresponding results for bounded approximation. The algorithm converges quickly, as few proofs are needed and BDDs remain small. Note however that exact inference is competitive for this problem size. Moreover, we observe large speedups compared to the implementation with meta-interpreters used in (De Raedt et al. 2007), where total runtimes to reach \u03b4 = 0. 01 for these queries were 46234, 206400 and 307966 msec respectively. Table 3 shows the performance of the Monte Carlo estimator. On Small, Monte Carlo is the fastest approach. Already within the first 1000 samples a good approximation is obtained.\nThe experiments on Small thus confirm that the implementation on top of YAP-\nProlog enables efficient probabilistic inference on small sized graphs.\nMedium Sized Sample For graph Medium with around 11000 edges, exact inference is no longer feasible. Table 4 again shows results for the k -probability. Comparing these results with the corresponding values from Table 1, we observe that the estimated probability is higher now: this is natural, as the graph has both more\nnodes and is more connected, therefore leading to many more possible explanations. This also explains the increase in running times. Approximate inference using bounds only reached loose bounds (with differences> 0. 2) on queries involving node \u2019HGNC 983\u2019, as upper bound formulae with more than 10 million conjunctions were encountered, which could not be processed.\nThe Monte Carlo estimator using the standard definition of path/3 on Medium did not complete the first 1000 samples within one hour. A detailed analysis shows that this is caused by some queries backtracking too heavily. Table 5 therefore reports results using the memorising version memopath/3. With this improved definition, Monte Carlo performs well: it obtains a good approximation in a few seconds. Requiring tighter bounds however can increase runtimes significantly.\nBiomine Database The Biomine Database covers hundreds of thousands of entities and millions of links. On Biomine, we therefore restricted our experiments to the approximations given by k -probability and Monte Carlo. Given the results on Medium, we directly used memopath/3 for Monte Carlo. Tables 6 and 7 show the results on the large network. We observe that on this large graph, the number of possible paths is tremendous, which implies success probabilities practically equal to 1. Still, we observe that ProbLog\u2019s branch-and-bound search to find the best\nsolutions performs reasonably also on this size of network. However, runtimes for obtaining tight confidence intervals with Monte Carlo explode quickly even with the improved path definition. Given that sampling a program that does not entail the query is extremely unlikely for the setting considered so far, we performed an additional experiment on Biomine, where we restrict the number of edges on the path connecting two nodes to a maximum of 2 or 3. Results are reported in Table 8. As none of the resulting queries have more than 50 proofs, exact inference is much faster than Monte Carlo, which needs a higher number of samples to reliably estimate probabilities that are not close to 1.\nAltogether, the experiments confirm that our implementation provides efficient inference algorithms for ProbLog that scale to large databases. Furthermore, compared to the original implementation of (De Raedt et al. 2007), we obtain large speedups in both the Prolog and the BDD part, thereby opening new perspectives for applications of ProbLog."}, {"heading": "6 Conclusions", "text": "ProbLog is a simple but elegant probabilistic logic programming language that allows one to explicitly represent uncertainty by means of probabilistic facts denoting\nindependent random variables. The language is a simple and natural extension of the logic programming language Prolog. We presented an efficient implementation of the ProbLog language on top of the YAP-Prolog system that is designed to scale to large sized problems. We showed that ProbLog can be used to obtain both explanation and (approximations of) success probabilities for queries on a large database. To the best of our knowledge, ProbLog is the first example of a probabilistic logic programming system that can execute queries on such large databases. Due to the use of BDDs for addressing the disjoint-sum-problem, the initial implementation of ProbLog used in (De Raedt et al. 2007) already scaled up much better than alternative implementations such as Fuhr\u2019s pD engine HySpirit (Fuhr 2000). The tight integration in YAP-Prolog presented here leads to further speedups in runtime of several orders of magnitude.\nAlthough we focused on connectivity queries and Biomine in this work, similar problems are found across many domains; we believe that the techniques presented apply to a wide variety of queries and databases because ProbLog provides a clean separation between background knowledge and what is specific to the engine. As shown for Monte Carlo inference, such an interface can be very useful to improve performance as it allows incremental refinement of background knowledge, e.g., graph procedures. Initial experiments with Dijkstra\u2019s algorithm for finding the explanation probability are very promising.\nProbLog is closely related to some alternative formalisms such as PHA and ICL (Poole 1993b; Poole 2000), pD (Fuhr 2000) and PRISM (Sato and Kameya 2001) as their semantics are all based on Sato\u2019s distribution semantics even though there exist also some subtle differences. However, ProbLog is \u2013 to the best of the authors\u2019 knowledge \u2013 the first implementation that tightly integrates Sato\u2019s original distribution semantics (Sato 1995) in a state-of-the-art Prolog system without\nmaking additional restrictions (such as the exclusive explanation assumption made in PHA and PRISM). As ProbLog, both PRISM and the ICL implementation AILog2 use a two-step approach to inference, where proofs are collected in the first phase, and probabilities are calculated once all proofs are known. AILog2 is a meta-interpreter implemented in SWI-Prolog for didactical purposes, where the disjoint-sum-problem is tackled using a symbolic disjoining technique (Poole 2000). PRISM, built on top of B-Prolog, requires programs to be written such that alternative explanations for queries are mutually exclusive. PRISM uses a meta-interpreter to collect proofs in a hierarchical datastructure called explanation graph. As proofs are mutually exclusive, the explanation graph directly mirrors the sum-of-products structure of probability calculation (Sato and Kameya 2001). ProbLog is the first probabilistic logic programming system using BDDs as a basic datastructure for probability calculation, a principle that receives increased interest in the probabilistic logic learning community, cf. for instance (Riguzzi 2007; Ishihata et al. 2008).\nFurthermore, as compared to SLPs (Muggleton 1995), CLP(BN ) (Santos Costa et al. 2003), and BLPs (Kersting and De Raedt 2008), ProbLog is a much simpler and in a sense more primitive probabilistic programming language. Therefore, the relationship between probabilistic logic programming and ProbLog is, in a sense, analogous to that between logic programming and Prolog. From this perspective, it is our hope and goal to further develop ProbLog so that it can be used as a general purpose programming language with an efficient implementation for use in statistical relational learning (Getoor and Taskar 2007) and probabilistic programming (De Raedt et al. 2008). One important use of such a probabilistic programming language is as a target language in which other formalisms can be efficiently compiled. For instance, it has already been shown that CP-logic (Vennekens et al. 2004), a recent elegant probabilistic knowledge representation language based on a probabilistic extension of clausal logic, can be compiled into ProbLog (Riguzzi 2007) and it is well-known that SLPs (Muggleton 1995) can be compiled into Sato\u2019s PRISM, which is closely related to ProbLog. Further evidence is provided in (De Raedt et al. 2008).\nAnother, related use of ProbLog is as a vehicle for developing learning and mining algorithms and tools (Kimmig et al. 2007; De Raedt et al. 2008; Gutmann et al. 2008; Kimmig and De Raedt 2009; De Raedt et al. 2009). In the context of probabilistic representations (Getoor and Taskar 2007; De Raedt et al. 2008), one typically distinguishes two types of learning: parameter estimation and structure learning. In parameter estimation in the context of ProbLog and PRISM, one starts from a set of queries and the logical part of the program and the problem is to find good estimates of the parameter values, that is, the probabilities of the probabilistic facts in the program. (Gutmann et al. 2008) introduces a gradient descent approach to parameter learning for ProbLog that extends the BDD-based methods discussed here. In structure learning, one also starts from queries but has to find the logical part of the program as well. Structure learning is therefore closely related to inductive logic programming. The limiting factor in statistical relational learning and probabilistic logic learning is often the efficiency of inference, as learning requires repeated computation of the probabilities of many queries. Therefore,\nimprovements on inference in probabilistic programming implementations have an immediate effect on learning. The above compilation approach also raises the interesting and largely open question whether not only inference problems for alternative formalisms can be compiled into ProbLog but whether it is also possible to compile learning problems for these logics into learning problems for ProbLog.\nFinally, as ProbLog, unlike PRISM and PHA, deals with the disjoint-sum-problem,\nit is interesting to study how program transformation and analysis techniques could be used to optimize ProbLog programs, by detecting and taking into account situations where some conjunctions are disjoint. At the same time, we currently investigate how tabling, one of the keys to PRISM\u2019s efficiency, can be incorporated in ProbLog (Mantadelis and Janssens 2009; Kimmig et al. 2009)."}, {"heading": "Acknowledgements", "text": "We would like to thank Hannu Toivonen, Bernd Gutmann and Kristian Kersting for their many contributions to ProbLog, the Biomine team for the application, and Theofrastos Mantadelis for the development of SimpleCUDD. This work is partially supported by the GOA project 2008/08 Probabilistic Logic Learning. Angelika Kimmig is supported by the Research Foundation-Flanders (FWO-Vlaanderen). V\u0131\u0301tor Santos Costa and Ricardo Rocha are partially supported by the research projects STAMPA (PTDC/EIA/67738/2006) and JEDI (PTDC/ EIA/66924/2006) and by Fundac\u0327a\u0303o para a Cie\u0302ncia e Tecnologia."}], "references": [{"title": "Associative Commutative Discrimination Nets", "author": ["L. Bachmair", "T. Chen", "I.V. Ramakrishnan"], "venue": "International Joint Conference on Theory and Practice of Software Development. LNCS, vol. 668. Springer, 61\u201374.", "citeRegEx": "Bachmair et al\\.,? 1993", "shortCiteRegEx": "Bachmair et al\\.", "year": 1993}, {"title": "Graph-based algorithms for boolean function manipulation", "author": ["R.E. Bryant"], "venue": "IEEE Trans. Computers 35, 8, 677\u2013691.", "citeRegEx": "Bryant,? 1986", "shortCiteRegEx": "Bryant", "year": 1986}, {"title": "Stochastic logic programs: Sampling, inference and applications", "author": ["J. Cussens"], "venue": "Uncertainty in Artificial Intelligence, C. Boutilier and M. Goldszmidt, Eds. Morgan Kaufmann, 115\u2013122.", "citeRegEx": "Cussens,? 2000", "shortCiteRegEx": "Cussens", "year": 2000}, {"title": "Efficient query evaluation on probabilistic databases", "author": ["N.N. Dalvi", "D. Suciu"], "venue": "International Conference on Very Large Databases, M. A. Nascimento, M. T. \u00d6zsu, D. Kossmann, R. J. Miller, J. A. Blakeley, and K. B. Schiefer, Eds. Morgan Kaufmann, 864\u2013875.", "citeRegEx": "Dalvi and Suciu,? 2004", "shortCiteRegEx": "Dalvi and Suciu", "year": 2004}, {"title": "Probabilistic logic programs and their semantics", "author": ["E. Dantsin"], "venue": "Russian Conference on Logic Programming, A. Voronkov, Ed. LNCS, vol. 592. Springer, 152\u2013164.", "citeRegEx": "Dantsin,? 1991", "shortCiteRegEx": "Dantsin", "year": 1991}, {"title": "Towards Digesting the Alphabet-Soup of Statistical Relational Learning", "author": ["L. De Raedt", "B. Demoen", "D. Fierens", "B. Gutmann", "G. Janssens", "A. Kimmig", "N. Landwehr", "T. Mantadelis", "W. Meert", "R. Rocha", "V. Santos Costa", "I. Thon", "J. Vennekens"], "venue": "NIPS Workshop on Probabilistic Programming.", "citeRegEx": "Raedt et al\\.,? 2008", "shortCiteRegEx": "Raedt et al\\.", "year": 2008}, {"title": "Probabilistic Inductive Logic Programming - Theory and Applications", "author": ["L. De Raedt", "P. Frasconi", "K. Kersting", "S. Muggleton", "Eds."], "venue": "LNCS, vol. 4911.", "citeRegEx": "Raedt et al\\.,? 2008", "shortCiteRegEx": "Raedt et al\\.", "year": 2008}, {"title": "Compressing probabilistic Prolog programs", "author": ["L. De Raedt", "K. Kersting", "A. Kimmig", "K. Revoredo", "H. Toivonen"], "venue": "Machine Learning 70, 2-3, 151\u2013168.", "citeRegEx": "Raedt et al\\.,? 2008", "shortCiteRegEx": "Raedt et al\\.", "year": 2008}, {"title": "Probabilistic inductive querying using ProbLog", "author": ["L. De Raedt", "A. Kimmig", "B. Gutmann", "K. Kersting", "V. Santos Costa", "H. Toivonen"], "venue": "Tech. Rep. CW 552, Department of Computer Science, Katholieke Universiteit Leuven.", "citeRegEx": "Raedt et al\\.,? 2009", "shortCiteRegEx": "Raedt et al\\.", "year": 2009}, {"title": "ProbLog: A probabilistic Prolog and its application in link discovery", "author": ["L. De Raedt", "A. Kimmig", "H. Toivonen"], "venue": "International Joint Conference on Artificial Intelligence, M. M. Veloso, Ed. 2462\u20132467.", "citeRegEx": "Raedt et al\\.,? 2007", "shortCiteRegEx": "Raedt et al\\.", "year": 2007}, {"title": "Trie Memory", "author": ["E. Fredkin"], "venue": "Communications of the ACM 3, 490\u2013499.", "citeRegEx": "Fredkin,? 1962", "shortCiteRegEx": "Fredkin", "year": 1962}, {"title": "Probabilistic Datalog: Implementing logical information retrieval for advanced applications", "author": ["N. Fuhr"], "venue": "Journal of the American Society for Information Science (JASIS) 51, 2, 95\u2013110.", "citeRegEx": "Fuhr,? 2000", "shortCiteRegEx": "Fuhr", "year": 2000}, {"title": "Statistical Relational Learning", "author": ["L. Getoor", "B. Taskar", "Eds."], "venue": "The MIT press.", "citeRegEx": "Getoor et al\\.,? 2007", "shortCiteRegEx": "Getoor et al\\.", "year": 2007}, {"title": "Term Indexing", "author": ["P. Graf"], "venue": "LNAI, vol. 1053. Springer.", "citeRegEx": "Graf,? 1996", "shortCiteRegEx": "Graf", "year": 1996}, {"title": "Parameter learning in probabilistic databases: A least squares approach", "author": ["B. Gutmann", "A. Kimmig", "K. Kersting", "L. De Raedt"], "venue": "European Conference on Machine Learning, W. Daelemans, B. Goethals, and K. Morik, Eds. LNCS, vol. 5211. Springer, 473\u2013488.", "citeRegEx": "Gutmann et al\\.,? 2008", "shortCiteRegEx": "Gutmann et al\\.", "year": 2008}, {"title": "Propositionalizing the EM algorithm by BDDs", "author": ["M. Ishihata", "Y. Kameya", "T. Sato", "S. ichi Minato"], "venue": "In Proceedings of Inductive Logic Programming (ILP", "citeRegEx": "Ishihata et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ishihata et al\\.", "year": 2008}, {"title": "Basic principles of learning bayesian logic programs", "author": ["K. Kersting", "L. De Raedt"], "venue": "Probabilistic Inductive Logic Programming, L. De Raedt, P. Frasconi, K. Kersting, and S. Muggleton, Eds. LNCS, vol. 4911. Springer, 189\u2013221.", "citeRegEx": "Kersting and Raedt,? 2008", "shortCiteRegEx": "Kersting and Raedt", "year": 2008}, {"title": "Local query mining in a probabilistic Prolog", "author": ["A. Kimmig", "L. De Raedt"], "venue": "International Joint Conference on Artificial Intelligence, C. Boutilier, Ed. 1095\u20131100.", "citeRegEx": "Kimmig and Raedt,? 2009", "shortCiteRegEx": "Kimmig and Raedt", "year": 2009}, {"title": "Probabilistic explanation based learning", "author": ["A. Kimmig", "L. De Raedt", "H. Toivonen"], "venue": "European Conference on Machine Learning, J. N. Kok, J. Koronacki, R. L. de M\u00e1ntaras, S. Matwin, D. Mladenic, and A. Skowron, Eds. LNCS, vol. 4701. Springer, 176\u2013187.", "citeRegEx": "Kimmig et al\\.,? 2007", "shortCiteRegEx": "Kimmig et al\\.", "year": 2007}, {"title": "Trading memory for answers: Towards tabling ProbLog", "author": ["A. Kimmig", "B. Gutmann", "V. Santos Costa"], "venue": "International Workshop on Statistical Relational Learning.", "citeRegEx": "Kimmig et al\\.,? 2009", "shortCiteRegEx": "Kimmig et al\\.", "year": 2009}, {"title": "On the Efficient Execution of ProbLog Programs", "author": ["A. Kimmig", "V. Santos Costa", "R. Rocha", "B. Demoen", "L. De Raedt"], "venue": "International Conference on Logic Programming, M. G. de la Banda and E. Pontelli, Eds. Number 5366 in LNCS. Springer, 175\u2013189.", "citeRegEx": "Kimmig et al\\.,? 2008", "shortCiteRegEx": "Kimmig et al\\.", "year": 2008}, {"title": "ProbView: A flexible probabilistic database system", "author": ["L.V.S. Lakshmanan", "N. Leone", "R.B. Ross", "V.S. Subrahmanian"], "venue": "ACM Transactions on Database Systems 22, 3, 419\u2013469.", "citeRegEx": "Lakshmanan et al\\.,? 1997", "shortCiteRegEx": "Lakshmanan et al\\.", "year": 1997}, {"title": "Tabling relevant parts of SLD proofs for ground goals in a probabilistic setting", "author": ["T. Mantadelis", "G. Janssens"], "venue": "International Colloquium on Implementation of Constraint and LOgic Programming Systems.", "citeRegEx": "Mantadelis and Janssens,? 2009", "shortCiteRegEx": "Mantadelis and Janssens", "year": 2009}, {"title": "Stochastic logic programs", "author": ["S. Muggleton"], "venue": "Advances in ILP, L. De Raedt, Ed.", "citeRegEx": "Muggleton,? 1995", "shortCiteRegEx": "Muggleton", "year": 1995}, {"title": "Logic programming, abduction and probability", "author": ["D. Poole"], "venue": "New Generation Computing 11, 377\u2013400.", "citeRegEx": "Poole,? 1993a", "shortCiteRegEx": "Poole", "year": 1993}, {"title": "Probabilistic Horn abduction and Bayesian networks", "author": ["D. Poole"], "venue": "Artificial Intelligence 64, 81\u2013129.", "citeRegEx": "Poole,? 1993b", "shortCiteRegEx": "Poole", "year": 1993}, {"title": "Abducing through negation as failure: stable models within the independent choice logic", "author": ["D. Poole"], "venue": "Journal of Logic Programming 44, 1-3, 5\u201335.", "citeRegEx": "Poole,? 2000", "shortCiteRegEx": "Poole", "year": 2000}, {"title": "A top down interpreter for LPAD and CP-logic", "author": ["F. Riguzzi"], "venue": "Congress of the Italian Association for Artificial Intelligence (AI*IA), R. Basili and M. T. Pazienza, Eds. LNCS, vol. 4733. Springer, 109\u2013120.", "citeRegEx": "Riguzzi,? 2007", "shortCiteRegEx": "Riguzzi", "year": 2007}, {"title": "Prolog performance on larger datasets", "author": ["V. Santos Costa"], "venue": "Practical Aspects of Declarative Languages, 9th International Symposium, PADL 2007, Nice, France, January 14-15, 2007., M. Hanus, Ed. LNCS, vol. 4354. Springer, 185\u2013199.", "citeRegEx": "Costa,? 2007", "shortCiteRegEx": "Costa", "year": 2007}, {"title": "CLP(BN): constraint logic programming for probabilistic knowledge", "author": ["V. Santos Costa", "D. Page", "M. Qazi", "J. Cussens"], "venue": "Conference on Uncertainty in Artificial Intelligence, C. Meek and U. Kj\u00e6rulff, Eds. Morgan Kaufmann, 517\u2013524.", "citeRegEx": "Costa et al\\.,? 2003", "shortCiteRegEx": "Costa et al\\.", "year": 2003}, {"title": "Demand-driven indexing of prolog clauses", "author": ["V. Santos Costa", "K. Sagonas", "R. Lopes"], "venue": "International Conference on Logic Programming, V. Dahl and I. Niemel\u00e4, Eds. LNCS, vol. 4670. Springer, 305\u2013409.", "citeRegEx": "Costa et al\\.,? 2007", "shortCiteRegEx": "Costa et al\\.", "year": 2007}, {"title": "A statistical learning method for logic programs with distribution semantics", "author": ["T. Sato"], "venue": "International Conference on Logic Programming, L. Sterling, Ed. MIT Press, 715\u2013729.", "citeRegEx": "Sato,? 1995", "shortCiteRegEx": "Sato", "year": 1995}, {"title": "Parameter learning of logic programs for symbolicstatistical modeling", "author": ["T. Sato", "Y. Kameya"], "venue": "Journal of Artificial Intelligence Research (JAIR) 15, 391\u2013454.", "citeRegEx": "Sato and Kameya,? 2001", "shortCiteRegEx": "Sato and Kameya", "year": 2001}, {"title": "Link discovery in graphs derived from biological databases", "author": ["P. Sevon", "L. Eronen", "P. Hintsanen", "K. Kulovesi", "H. Toivonen"], "venue": "Data Integration in the Life Sciences, U. Leser, F. Naumann, and B. A. Eckman, Eds. LNCS, vol. 4075. Springer, 35\u201349.", "citeRegEx": "Sevon et al\\.,? 2006", "shortCiteRegEx": "Sevon et al\\.", "year": 2006}, {"title": "The complexity of enumeration and reliability problems", "author": ["L.G. Valiant"], "venue": "SIAM Journal on Computing 8, 3, 410\u2013421.", "citeRegEx": "Valiant,? 1979", "shortCiteRegEx": "Valiant", "year": 1979}, {"title": "Logic programs with annotated disjunctions", "author": ["J. Vennekens", "S. Verbaeten", "M. Bruynooghe"], "venue": "International Conference on Logic Programming, B. Demoen and V. Lifschitz, Eds. LNCS, vol. 3132. Springer, 431\u2013445.", "citeRegEx": "Vennekens et al\\.,? 2004", "shortCiteRegEx": "Vennekens et al\\.", "year": 2004}, {"title": "Trio: A system for integrated management of data, accuracy, and lineage", "author": ["J. Widom"], "venue": "Conference on Innovative Data Systems Research. 262\u2013276.", "citeRegEx": "Widom,? 2005", "shortCiteRegEx": "Widom", "year": 2005}], "referenceMentions": [{"referenceID": 25, "context": "Prominent examples include PHA and ICL (Poole 1993b; Poole 2000), PRISM (Sato and Kameya 2001), SLPs (Muggleton 1995), ProbView (Lakshmanan et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 26, "context": "Prominent examples include PHA and ICL (Poole 1993b; Poole 2000), PRISM (Sato and Kameya 2001), SLPs (Muggleton 1995), ProbView (Lakshmanan et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 32, "context": "Prominent examples include PHA and ICL (Poole 1993b; Poole 2000), PRISM (Sato and Kameya 2001), SLPs (Muggleton 1995), ProbView (Lakshmanan et al.", "startOffset": 72, "endOffset": 94}, {"referenceID": 23, "context": "Prominent examples include PHA and ICL (Poole 1993b; Poole 2000), PRISM (Sato and Kameya 2001), SLPs (Muggleton 1995), ProbView (Lakshmanan et al.", "startOffset": 101, "endOffset": 117}, {"referenceID": 21, "context": "Prominent examples include PHA and ICL (Poole 1993b; Poole 2000), PRISM (Sato and Kameya 2001), SLPs (Muggleton 1995), ProbView (Lakshmanan et al. 1997), CLP(BN ) (Santos Costa et al.", "startOffset": 128, "endOffset": 152}, {"referenceID": 35, "context": "2003), CP-logic (Vennekens et al. 2004), Trio (Widom 2005), probabilistic Datalog (pD) (Fuhr 2000), and probabilistic databases (Dalvi and Suciu 2004).", "startOffset": 16, "endOffset": 39}, {"referenceID": 36, "context": "2004), Trio (Widom 2005), probabilistic Datalog (pD) (Fuhr 2000), and probabilistic databases (Dalvi and Suciu 2004).", "startOffset": 12, "endOffset": 24}, {"referenceID": 11, "context": "2004), Trio (Widom 2005), probabilistic Datalog (pD) (Fuhr 2000), and probabilistic databases (Dalvi and Suciu 2004).", "startOffset": 53, "endOffset": 64}, {"referenceID": 3, "context": "2004), Trio (Widom 2005), probabilistic Datalog (pD) (Fuhr 2000), and probabilistic databases (Dalvi and Suciu 2004).", "startOffset": 94, "endOffset": 116}, {"referenceID": 33, "context": ") can be extracted from public databases, and probabilistic links between concepts can be obtained by various techniques (Sevon et al. 2006).", "startOffset": 121, "endOffset": 140}, {"referenceID": 31, "context": "The semantics of ProbLog is not new: it is an instance of the distribution semantics (Sato 1995).", "startOffset": 85, "endOffset": 96}, {"referenceID": 4, "context": "(Dantsin 1991; Poole 1993b; Fuhr 2000; Poole 2000; Dalvi and Suciu 2004).", "startOffset": 0, "endOffset": 72}, {"referenceID": 25, "context": "(Dantsin 1991; Poole 1993b; Fuhr 2000; Poole 2000; Dalvi and Suciu 2004).", "startOffset": 0, "endOffset": 72}, {"referenceID": 11, "context": "(Dantsin 1991; Poole 1993b; Fuhr 2000; Poole 2000; Dalvi and Suciu 2004).", "startOffset": 0, "endOffset": 72}, {"referenceID": 26, "context": "(Dantsin 1991; Poole 1993b; Fuhr 2000; Poole 2000; Dalvi and Suciu 2004).", "startOffset": 0, "endOffset": 72}, {"referenceID": 3, "context": "(Dantsin 1991; Poole 1993b; Fuhr 2000; Poole 2000; Dalvi and Suciu 2004).", "startOffset": 0, "endOffset": 72}, {"referenceID": 31, "context": "Sato has, however, shown that the semantics is also well-defined in the case of a countably infinite set of random variables and formalized it in his well-known distribution semantics (Sato 1995).", "startOffset": 184, "endOffset": 195}, {"referenceID": 32, "context": "However, even though relying on the same semantics, in order to allow efficient inference, systems such as PRISM (Sato and Kameya 2001) and PHA (Poole 1993b) additionally require all proofs of a query to be mutually exclusive.", "startOffset": 113, "endOffset": 135}, {"referenceID": 25, "context": "However, even though relying on the same semantics, in order to allow efficient inference, systems such as PRISM (Sato and Kameya 2001) and PHA (Poole 1993b) additionally require all proofs of a query to be mutually exclusive.", "startOffset": 144, "endOffset": 157}, {"referenceID": 26, "context": "ICL (Poole 2000) extends PHA to the case where proofs need not be mutually exclusive.", "startOffset": 4, "endOffset": 16}, {"referenceID": 1, "context": "We present algorithms for computing the success and explanation probabilities of a query, and show how they can be efficiently implemented combining Prolog inference with Binary Decision Diagrams (BDDs) (Bryant 1986).", "startOffset": 203, "endOffset": 216}, {"referenceID": 24, "context": "In addition to an iterative deepening algorithm that computes an approximation along the lines of (Poole 1993a), we further adapt the Monte Carlo approach used by (Sevon et al.", "startOffset": 98, "endOffset": 111}, {"referenceID": 33, "context": "In addition to an iterative deepening algorithm that computes an approximation along the lines of (Poole 1993a), we further adapt the Monte Carlo approach used by (Sevon et al. 2006) in the context of biological network inference.", "startOffset": 163, "endOffset": 182}, {"referenceID": 33, "context": "2007), which are needed to use ProbLog to effectively query Sevon\u2019s Biomine network (Sevon et al. 2006) containing about 1,000,000 nodes and 6,000,000 edges, as will be shown in the experiments.", "startOffset": 84, "endOffset": 103}, {"referenceID": 31, "context": "Sato has shown how this semantics can be generalized to the countably infinite case; we refer to (Sato 1995) for details.", "startOffset": 97, "endOffset": 108}, {"referenceID": 18, "context": ", all minimal sets E \u2286 LT of probabilistic facts such that E \u222a BK |= q (Kimmig et al. 2007).", "startOffset": 71, "endOffset": 91}, {"referenceID": 31, "context": "The ProbLog semantics is essentially a distribution semantics (Sato 1995).", "startOffset": 62, "endOffset": 73}, {"referenceID": 31, "context": "Sato has rigorously shown that this class of programs defines a joint probability distribution over the set of possible least Herbrand models of the program (allowing functors), that is, of the background knowledge BK together with a subprogram L \u2286 LT ; for further details we refer to (Sato 1995).", "startOffset": 286, "endOffset": 297}, {"referenceID": 4, "context": "(Dantsin 1991; Poole 1993b; Fuhr 2000; Poole 2000; Dalvi and Suciu 2004).", "startOffset": 0, "endOffset": 72}, {"referenceID": 25, "context": "(Dantsin 1991; Poole 1993b; Fuhr 2000; Poole 2000; Dalvi and Suciu 2004).", "startOffset": 0, "endOffset": 72}, {"referenceID": 11, "context": "(Dantsin 1991; Poole 1993b; Fuhr 2000; Poole 2000; Dalvi and Suciu 2004).", "startOffset": 0, "endOffset": 72}, {"referenceID": 26, "context": "(Dantsin 1991; Poole 1993b; Fuhr 2000; Poole 2000; Dalvi and Suciu 2004).", "startOffset": 0, "endOffset": 72}, {"referenceID": 3, "context": "(Dantsin 1991; Poole 1993b; Fuhr 2000; Poole 2000; Dalvi and Suciu 2004).", "startOffset": 0, "endOffset": 72}, {"referenceID": 32, "context": "Similar approaches are used for PRISM (Sato and Kameya 2001), ICL (Poole 2000) and pD (Fuhr 2000).", "startOffset": 38, "endOffset": 60}, {"referenceID": 26, "context": "Similar approaches are used for PRISM (Sato and Kameya 2001), ICL (Poole 2000) and pD (Fuhr 2000).", "startOffset": 66, "endOffset": 78}, {"referenceID": 11, "context": "Similar approaches are used for PRISM (Sato and Kameya 2001), ICL (Poole 2000) and pD (Fuhr 2000).", "startOffset": 86, "endOffset": 97}, {"referenceID": 34, "context": "In general, this problem is known as the disjoint-sum-problem or the two-terminal network reliability problem, which is #P-complete (Valiant 1979).", "startOffset": 132, "endOffset": 146}, {"referenceID": 11, "context": "The pD-engine HySpirit (Fuhr 2000)", "startOffset": 23, "endOffset": 34}, {"referenceID": 26, "context": "For ICL, which extends PHA by allowing non-disjoint proofs, (Poole 2000) proposes a symbolic disjoining algorithm, but does not report scalability results.", "startOffset": 60, "endOffset": 72}, {"referenceID": 1, "context": "Our implementation of ProbLog employs Binary Decision Diagrams (BDDs) (Bryant 1986), an efficient graphical representation of a Boolean function over a set of variables, which scales to tens of thousands of proofs; see Section 4.", "startOffset": 70, "endOffset": 83}, {"referenceID": 32, "context": "PRISM (Sato and Kameya 2001) and PHA (Poole 1993b) differ from the systems mentioned above in that they avoid the disjoint-sum-problem by requiring the user to write programs such that proofs are guaranteed to be disjoint.", "startOffset": 6, "endOffset": 28}, {"referenceID": 25, "context": "PRISM (Sato and Kameya 2001) and PHA (Poole 1993b) differ from the systems mentioned above in that they avoid the disjoint-sum-problem by requiring the user to write programs such that proofs are guaranteed to be disjoint.", "startOffset": 37, "endOffset": 50}, {"referenceID": 24, "context": "It is closely related to work by (Poole 1993a) in the context of PHA, but adapted towards ProbLog.", "startOffset": 33, "endOffset": 46}, {"referenceID": 14, "context": "(Gutmann et al. 2008) therefore introduces the k -probability Pk (q |T ), which approximates the success probability by using the k -best (that is, the k most likely) explanations instead of all proofs when building the DNF formula used in Equation (7): Pk (q |T ) = P \uf8eb\uf8ed \u2228 e\u2208Ek (q) \u2227", "startOffset": 0, "endOffset": 21}, {"referenceID": 25, "context": "also (Poole 1993b).", "startOffset": 5, "endOffset": 18}, {"referenceID": 33, "context": "A similar algorithm (without the use of confidence intervals) was also used in the context of biological networks (not represented as Prolog programs) by (Sevon et al. 2006).", "startOffset": 154, "endOffset": 173}, {"referenceID": 4, "context": "The use of a Monte Carlo method for probabilistic logic programs was suggested already by (Dantsin 1991), although he neither provides details nor reports on an implementation.", "startOffset": 90, "endOffset": 104}, {"referenceID": 2, "context": "Our approach differs from the MCMC method for Stochastic Logic Programs (SLPs) introduced by (Cussens 2000) in that we do not use a Markov chain, but restart from scratch for each sample.", "startOffset": 93, "endOffset": 107}, {"referenceID": 10, "context": "Tries (Fredkin 1962) were originally invented to index dictionaries, and have since been generalised to index recursive data structures such as terms.", "startOffset": 6, "endOffset": 20}, {"referenceID": 0, "context": "Please refer to (Bachmair et al. 1993; Graf 1996; Ramakrishnan et al. 1999) for the use of tries in automated theorem proving, term rewriting and tabled logic programs.", "startOffset": 16, "endOffset": 75}, {"referenceID": 13, "context": "Please refer to (Bachmair et al. 1993; Graf 1996; Ramakrishnan et al. 1999) for the use of tries in automated theorem proving, term rewriting and tabled logic programs.", "startOffset": 16, "endOffset": 75}, {"referenceID": 1, "context": "To efficiently compute the probability of a DNF formula representing a set of proofs, our implementation represents this formula as a reduced ordered Binary Decision Diagram (BDD) (Bryant 1986), which can be viewed as a compact encoding of a Boolean decision tree.", "startOffset": 180, "endOffset": 193}, {"referenceID": 33, "context": "We performed experiments with our implementation of ProbLog in the context of the biological network obtained from the Biomine project (Sevon et al. 2006).", "startOffset": 135, "endOffset": 154}, {"referenceID": 20, "context": "However, using SimpleCUDD instead of the C++ interface used in (Kimmig et al. 2008) typically decreases BDD time by at least one or two orders of magnitude.", "startOffset": 63, "endOffset": 83}, {"referenceID": 11, "context": "2007) already scaled up much better than alternative implementations such as Fuhr\u2019s pD engine HySpirit (Fuhr 2000).", "startOffset": 103, "endOffset": 114}, {"referenceID": 25, "context": "ProbLog is closely related to some alternative formalisms such as PHA and ICL (Poole 1993b; Poole 2000), pD (Fuhr 2000) and PRISM (Sato and Kameya 2001) as their semantics are all based on Sato\u2019s distribution semantics even though there exist also some subtle differences.", "startOffset": 78, "endOffset": 103}, {"referenceID": 26, "context": "ProbLog is closely related to some alternative formalisms such as PHA and ICL (Poole 1993b; Poole 2000), pD (Fuhr 2000) and PRISM (Sato and Kameya 2001) as their semantics are all based on Sato\u2019s distribution semantics even though there exist also some subtle differences.", "startOffset": 78, "endOffset": 103}, {"referenceID": 11, "context": "ProbLog is closely related to some alternative formalisms such as PHA and ICL (Poole 1993b; Poole 2000), pD (Fuhr 2000) and PRISM (Sato and Kameya 2001) as their semantics are all based on Sato\u2019s distribution semantics even though there exist also some subtle differences.", "startOffset": 108, "endOffset": 119}, {"referenceID": 32, "context": "ProbLog is closely related to some alternative formalisms such as PHA and ICL (Poole 1993b; Poole 2000), pD (Fuhr 2000) and PRISM (Sato and Kameya 2001) as their semantics are all based on Sato\u2019s distribution semantics even though there exist also some subtle differences.", "startOffset": 130, "endOffset": 152}, {"referenceID": 31, "context": "However, ProbLog is \u2013 to the best of the authors\u2019 knowledge \u2013 the first implementation that tightly integrates Sato\u2019s original distribution semantics (Sato 1995) in a state-of-the-art Prolog system without", "startOffset": 150, "endOffset": 161}, {"referenceID": 26, "context": "AILog2 is a meta-interpreter implemented in SWI-Prolog for didactical purposes, where the disjoint-sum-problem is tackled using a symbolic disjoining technique (Poole 2000).", "startOffset": 160, "endOffset": 172}, {"referenceID": 32, "context": "As proofs are mutually exclusive, the explanation graph directly mirrors the sum-of-products structure of probability calculation (Sato and Kameya 2001).", "startOffset": 130, "endOffset": 152}, {"referenceID": 27, "context": "for instance (Riguzzi 2007; Ishihata et al. 2008).", "startOffset": 13, "endOffset": 49}, {"referenceID": 15, "context": "for instance (Riguzzi 2007; Ishihata et al. 2008).", "startOffset": 13, "endOffset": 49}, {"referenceID": 23, "context": "Furthermore, as compared to SLPs (Muggleton 1995), CLP(BN ) (Santos Costa et al.", "startOffset": 33, "endOffset": 49}, {"referenceID": 35, "context": "For instance, it has already been shown that CP-logic (Vennekens et al. 2004), a recent elegant probabilistic knowledge representation language based on a probabilistic extension of clausal logic, can be compiled into ProbLog (Riguzzi 2007) and it is well-known that SLPs (Muggleton 1995) can be compiled into Sato\u2019s PRISM, which is closely related to ProbLog.", "startOffset": 54, "endOffset": 77}, {"referenceID": 27, "context": "2004), a recent elegant probabilistic knowledge representation language based on a probabilistic extension of clausal logic, can be compiled into ProbLog (Riguzzi 2007) and it is well-known that SLPs (Muggleton 1995) can be compiled into Sato\u2019s PRISM, which is closely related to ProbLog.", "startOffset": 154, "endOffset": 168}, {"referenceID": 23, "context": "2004), a recent elegant probabilistic knowledge representation language based on a probabilistic extension of clausal logic, can be compiled into ProbLog (Riguzzi 2007) and it is well-known that SLPs (Muggleton 1995) can be compiled into Sato\u2019s PRISM, which is closely related to ProbLog.", "startOffset": 200, "endOffset": 216}, {"referenceID": 18, "context": "Another, related use of ProbLog is as a vehicle for developing learning and mining algorithms and tools (Kimmig et al. 2007; De Raedt et al. 2008; Gutmann et al. 2008; Kimmig and De Raedt 2009; De Raedt et al. 2009).", "startOffset": 104, "endOffset": 215}, {"referenceID": 14, "context": "Another, related use of ProbLog is as a vehicle for developing learning and mining algorithms and tools (Kimmig et al. 2007; De Raedt et al. 2008; Gutmann et al. 2008; Kimmig and De Raedt 2009; De Raedt et al. 2009).", "startOffset": 104, "endOffset": 215}, {"referenceID": 14, "context": "(Gutmann et al. 2008) introduces a gradient descent approach to parameter learning for ProbLog that extends the BDD-based methods discussed here.", "startOffset": 0, "endOffset": 21}, {"referenceID": 22, "context": "At the same time, we currently investigate how tabling, one of the keys to PRISM\u2019s efficiency, can be incorporated in ProbLog (Mantadelis and Janssens 2009; Kimmig et al. 2009).", "startOffset": 126, "endOffset": 176}, {"referenceID": 19, "context": "At the same time, we currently investigate how tabling, one of the keys to PRISM\u2019s efficiency, can be incorporated in ProbLog (Mantadelis and Janssens 2009; Kimmig et al. 2009).", "startOffset": 126, "endOffset": 176}], "year": 2010, "abstractText": "The past few years have seen a surge of interest in the field of probabilistic logic learning and statistical relational learning. In this endeavor, many probabilistic logics have been developed. ProbLog is a recent probabilistic extension of Prolog motivated by the mining of large biological networks. In ProbLog, facts can be labeled with probabilities. These facts are treated as mutually independent random variables that indicate whether these facts belong to a randomly sampled program. Different kinds of queries can be posed to ProbLog programs. We introduce algorithms that allow the efficient execution of these queries, discuss their implementation on top of the YAP-Prolog system, and evaluate their performance in the context of large networks of biological entities. To appear in Theory and Practice of Logic Programming (TPLP)", "creator": "LaTeX with hyperref package"}}}