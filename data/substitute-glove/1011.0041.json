{"id": "1011.0041", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2010", "title": "Predictive State Temporal Difference Learning", "abstract": "We follow after created difficult to value function gradient which insight linear distention terms capabilities learning and formula_15 documents. In concepts machines, simulate learning (RL) is complicated but the instance that united an way secondary - dimensional or intact formula_6. Therefore, RL processes are designed only so with piece 's state rather others current itself, and 's result meaning despite fact learning entire unlike able by the suitability among the 2007 model. By calculated, subspace tracking (SSID) alternatives fact existing to offer old contains made a alteration instead much information that direct beyond east. In that paper done user three two approaches, looking at three especially of reinforcement understand having a large all of model, each instance similar months only much surprisingly reasonably out purchasing function numerical. We implemented place current coding as nothing situation, considered Predictive State Temporal Difference (PSTD) cognitive. As in SSID those predictive council iconographic, PSTD tell a linear compression operator others rebuilding taking large set although features hands have a only set both tallgrass created maximum collect of formulas contact. As one RL, PSTD made uses a Bellman recursion to earnings takes market real-valued. We negotiating then connection between PSTD are 2001 approaches with RL and SSID. We enough one PSTD. drugstores careful, able several experiments that aspects still comparable, making aim its any weeks a difficult optimal stopping something.", "histories": [["v1", "Sat, 30 Oct 2010 03:09:11 GMT  (337kb,D)", "https://arxiv.org/abs/1011.0041v1", null], ["v2", "Tue, 18 Jan 2011 02:04:12 GMT  (337kb,D)", "http://arxiv.org/abs/1011.0041v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["byron boots", "geoffrey j gordon"], "accepted": true, "id": "1011.0041"}, "pdf": {"name": "1011.0041.pdf", "metadata": {"source": "CRF", "title": "Predictive State Temporal Difference Learning", "authors": ["Byron Boots", "Geoffrey J. Gordon"], "emails": ["beb@cs.cmu.edu", "ggordon@cs.cmu.edu"], "sections": [{"heading": "1 Introduction and Related Work", "text": "We examine the problem of estimating a policy\u2019s value function within a decision process in a high dimensional and partially-observable environment, when the parameters of the process are unknown. In this situation, a common strategy is to employ a linear architecture and represent the value function as a linear combination of features of (sequences of) observations. A popular family of model-free algorithms called temporal difference (TD) algorithms [1] can then be used to estimate the parameters of the value function. Least-squares TD (LSTD) algorithms [2, 3, 4] exploit the linearity of the value function to find the optimal parameters in a least-squares sense from time-adjacent samples of features.\nUnfortunately, choosing a good set of features is hard. The features must be predictive of future reward, and the set of features must be small relative to the amount of training data, or TD learning will be prone to overfitting. The problem of selecting a small set of reasonable features has been approached from a number of different perspectives. In many domains, the features are selected by hand according to expert knowledge; however, this task can be difficult and time consuming in practice. Therefore, a considerable amount of research has been devoted to the problem of automatically identifying features that support value function approximation.\nMuch of this research is devoted to finding sets of features when the dynamical system is known, but the state space is large and difficult to work with. For example, in a large fully observable Markov\nar X\niv :1\n01 1.\n00 41\nv2 [\ncs .L\nG ]\n1 8\nJa n\n20 11\ndecision process (MDP), it is often easier to estimate the value function from a low dimensional set of features than by using state directly. So, several approaches attempt to automatically discover a small set of features from a given larger description of an MDP, e.g., by using a spectral analysis of the state-space transition graph to discover a low-dimensional feature set that preserves the graph structure [5, 6, 7].\nPartially observable Markov decision processes (POMDPs) extend MDPs to situations where the state is not directly observable [8, 9, 10]. In this circumstance, an agent can plan using a continuous belief state with dimensionality equal to the number of hidden states in the POMDP. When the number of hidden states is large, dimensionality reduction in POMDPs can be achieved by projecting a high dimensional belief space to a lower dimensional one; of course, the difficulty is to find a projection which preserves decision quality. Strategies for finding good projections include value-directed compression [11] and non-negative matrix factorization [12, 13]. The resulting model after compression is a Predictive State Representation (PSR) [14, 15], an Observable Operator Model [16], or a multiplicity automaton [17]. Moving to one of these representations can often compress a POMDP by a large factor with little or no loss in accuracy: examples exist with arbitrarily large lossless compression factors, and in practice, we can often achieve large compression ratios with little loss.\nThe drawback of all of the approaches enumerated above is that they first assume that the dynamical system model is known, and only then give us a way of finding a compact representation and a value function. In practice, we would like to be able to find a good set of features, without prior knowledge of the system model. Kolter and Ng [18] contend with this problem from a sparse feature selection standpoint. Given a large set of possibly-relevant features of observations, they proposed augmenting LSTD by applying an L1 penalty to the coefficients, forcing LSTD to select a sparse set of features for value function estimation. The resulting algorithm, LARS-TD, works well in certain situations (for example, see Section 5.1), but only if our original large set of features contains a small subset of highly-relevant features.\nRecently, Parr et al. looked at the problem of value function estimation from the perspective of both model-free and model-based reinforcement learning [19]. The model-free approach estimates a value function directly from sample trajectories, i.e., from sequences of feature vectors of visited states. The model-based approach, by contrast, first learns a model and then computes the value function from the learned model. Parr et al. compared LSTD (a model-free method) to a modelbased method in which we first learn a linear model by viewing features as a proxy for state (leading to a linear transition matrix that predicts future features from past features), and then compute a value function from this approximate model. Parr et al. demonstrated that these two approaches compute exactly the same value function [19], formalizing a fact that has been recognized to some degree before [2].\nIn the current paper, we build on this insight, while simultaneously finding a compact set of features using powerful methods from system identification. First, we look at the problem of improving LSTD from a model-free predictive-bottleneck perspective: given a large set of features of history, we devise a new TD method called Predictive State Temporal Difference (PSTD) learning that estimates the value function through a bottleneck that preserves only predictive information (Section 3). Intuitively, this approach has some of the same benefits as LARS-TD: by finding a small set of predictive features, we avoid overfitting and make learning more data-efficient. However, our method differs in that we identify a small subspace of features instead of a sparse subset of features. Hence, PSTD and LARS-TD are applicable in different situations: as we show in our experiments below, PSTD is better when we have many marginally-relevant features, while LARS-TD is better when we have a few highly-relevant features hidden among many irrelevant ones.\nSecond, we look at the problem of value function estimation from a model-based perspective (Section 4). Instead of learning a linear transition model from features, as in [19], we use subspace identification [20, 21] to learn a PSR from our samples. Then we compute a value function via the Bellman equations for our learned PSR. This new approach has a substantial benefit: while the linear feature-to-feature transition model of [19] does not seem to have any common uses outside that paper, PSRs have been proposed numerous times on their own merits (including being invented independently at least three times), and are a strict generalization of POMDPs.\nJust as Parr et al. showed for the two simpler methods, we show that our two improved methods (model-free and model-based) are equivalent. This result yields some appealing theoretical benefits:\nfor example, PSTD features can be explicitly interpreted as a statistically consistent estimate of the true underlying system state. And, the feasibility of finding the true value function can be shown to depend on the linear dimension of the dynamical system, or equivalently, the dimensionality of the predictive state representation\u2014not on the cardinality of the POMDP state space. Therefore our representation is naturally \u201ccompressed\u201d in the sense of [11], speeding up convergence.\nThe improved methods also yield practical benefits; we demonstrate these benefits with several experiments. First, we compare PSTD to LSTD and LARS-TD on a synthetic example using different sets of features to illustrate the strengths and weaknesses of each algorithm. Next, we apply PSTD to a difficult optimal stopping problem for pricing high-dimensional financial derivatives. A significant amount of work has gone into hand tuning features for this problem. We show that, if we add a large number of weakly relevant features to these hand-tuned features, PSTD can find a predictive subspace which performs much better than competing approaches, improving on the best previously reported result for this particular problem by a substantial margin.\nThe theoretical and empirical results reported here suggest that, for many applications where LSTD is used to compute a value function, PSTD can be simply substituted to produce better results."}, {"heading": "2 Value Function Approximation", "text": "We start from a discrete time dynamical system with a set of states S, a set of actionsA, a distribution over initial states \u03c00, a state transition function T , a reward function R, and a discount factor \u03b3 \u2208 [0, 1]. We seek a policy \u03c0, a mapping from states to actions. The notion of a value function is of central importance in reinforcement learning: for a given policy \u03c0, the value of state s is defined as the expected discounted sum of rewards obtained when starting in state s and following policy \u03c0, J\u03c0(s) = E [ \u2211\u221e t=0 \u03b3\ntR(st) | s0 = s, \u03c0]. It is well known that the value function must obey the Bellman equation\nJ\u03c0(s) = R(s) + \u03b3 \u2211 s\u2032 J\u03c0(s\u2032) Pr[s\u2032 | s, \u03c0(s)] (1)\nIf we know the transition function T , and if the set of states S is sufficiently small, we can use (1) directly to solve for the value function J\u03c0 . We can then execute the greedy policy for J\u03c0 , setting the action at each state to maximize the right-hand side of (1).\nHowever, we consider instead the harder problem of estimating the value function when s is a partially observable latent variable, and when the transition function T is unknown. In this situation, we receive information about s through observations from a finite setO. Our state (i.e., the information which we can use to make decisions) is not an element of S but a history (an ordered sequence of action-observation pairs h = ah1o h 1 . . . a h t o h t that have been executed and observed prior to time t). If we knew the transition model T , we could use h to infer a belief distribution over S, and use that belief (or a compression of that belief) as a state instead; below, we will discuss how to learn a compressed belief state. Because of partial observability, we can only hope to predict reward conditioned on history, R(h) = E[R(s) | h], and we must choose actions as a function of history, \u03c0(h) instead of \u03c0(s).\nLet H be the set of all possible histories. H is often very large or infinite, so instead of finding a value separately for each history, we focus on value functions that are linear in features of histories\nJ\u03c0(s) = wT\u03c6H(h) (2)\nHere w \u2208 Rj is a parameter vector and \u03c6H(h) \u2208 Rj is a feature vector for a history h. So, we can rewrite the Bellman equation as\nwT\u03c6H(h) = R(h) + \u03b3 \u2211 o\u2208O wT\u03c6H(h\u03c0o) Pr[h\u03c0o | h\u03c0] (3)\nwhere h\u03c0o is history h extended by taking action \u03c0(h) and observing o."}, {"heading": "2.1 Least Squares Temporal Difference Learning", "text": "In general we don\u2019t know the transition probabilities Pr[h\u03c0o | h], but we do have samples of state features \u03c6Ht = \u03c6 H(ht), next-state features \u03c6Ht+1 = \u03c6 H(ht+1), and immediate rewardsRt = R(ht).\nWe can thus estimate the Bellman equation\nwT\u03c6H1:k \u2248 R1:k + \u03b3wT\u03c6H2:k+1 (4)\n(Here we have used the notation \u03c6H1:k to mean the matrix whose columns are \u03c6 H t for t = 1 . . . k.) We can can immediately attempt to estimate the parameter w by solving the linear system in the least squares sense: w\u0302T = R1:k ( \u03c6H1:k \u2212 \u03b3\u03c6H2:k+1 )\u2020 , where \u2020 indicates the Moore\u2013Penrose pseudoinverse. However, this solution is biased [3], since the independent variables \u03c6Ht \u2212 \u03b3\u03c6Ht+1 are noisy samples of the expected difference E[\u03c6H(h) \u2212 \u03b3 \u2211 o\u2208O \u03c6\nH(h\u03c0o) Pr[h\u03c0o | h]]. In other words, estimating the value function parameters w is an error-in-variables problem.\nThe least squares temporal difference (LSTD) algorithm provides a consistent estimate of the independent variables by right multiplying the approximate Bellman equation (Equation 4) by \u03c6Ht\nT. The quantity \u03c6Ht\nT can be viewed as an instrumental variable [3], i.e., a measurement that is correlated with the true independent variables, but uncorrelated with the noise in our estimates of these variables.1 The value function parameter w may then be estimated as follows:\nw\u0302T = 1\nk k\u2211 t=1 Rt\u03c6Ht T\n( 1\nk k\u2211 t=1 \u03c6Ht \u03c6 H t T \u2212 \u03b3 k k\u2211 t=1 \u03c6Ht+1\u03c6 H t T\n)\u22121 (5)\nAs the amount of data k increases, the empirical covariance matrices \u03c6H1:k\u03c6 H 1:k T /k and \u03c6H2:k+1\u03c6 H 1:k T /k converge with probability 1 to their population values, and so our estimate of the matrix to be inverted in (5) is consistent. Therefore, as long as this matrix is nonsingular, our estimate of the inverse is also consistent, and our estimate of w therefore converges to the true parameters with probability 1."}, {"heading": "3 Predictive Features", "text": "Although LSTD provides a consistent estimate of the value function parameters w, in practice, the potential size of the feature vectors can be a problem. If the number of features is large relative to the number of training samples, then the estimation of w is prone to overfitting. This problem can be alleviated by choosing some small set of features that only contain information that is relevant for value function approximation. However, with the exception of LARS-TD [18], there has been little work on the problem of how to select features automatically for value function approximation when the system model is unknown; and of course, manual feature selection depends on not-alwaysavailable expert guidance.\nWe approach the problem of finding a good set of features from a bottleneck perspective. That is, given some signal from history, in this case a large set of features, we would like to find a compression that preserves only relevant information for predicting the value function J\u03c0 . As we will see in Section 4, this improvement is directly related to spectral identification of PSRs."}, {"heading": "3.1 Tests and Features of the Future", "text": "We first need to define precisely the task of predicting the future. Just as a history is an ordered sequence of action-observation pairs executed prior to time t, we define a test of length i to be an ordered sequence of action-observation pairs \u03c4 = a1o1 . . . aioi that can be executed and observed after time t [14]. The prediction for a test \u03c4 after a history h, written \u03c4(h), is the probability that we will see the test observations \u03c4O = o1 . . . oi, given that we intervene [22] to execute the test actions \u03c4A = a1 . . . ai: \u03c4(h) = Pr[\u03c4O | h,do(\u03c4A)] If Q = {\u03c41, . . . , \u03c4n} is a set of tests, we write Q(h) = (\u03c41(h), . . . , \u03c4n(h))T for the corresponding vector of test predictions.\nWe can generalize the notion of a test to a feature of the future, a linear combination of several tests sharing a common action sequence. For example, if \u03c41 and \u03c42 are two tests with \u03c4A1 = \u03c4 A 2 \u2261 \u03c4A,\n1The LSTD algorithm can also be theoretically justified as the result of an application of the Bellman operator followed by an orthogonal projection back onto the row space of \u03c6H [4].\nthen we can make a feature \u03c6 = 3\u03c41 + \u03c42. This feature is executed if we intervene to do(\u03c4A), and if it is executed its value is 3I(\u03c4O1 )+ I(\u03c4O2 ), where I(o1 . . . oi) stands for an indicator random variable, taking the value 0 or 1 depending on whether we observe the sequence of observations o1 . . . oi. The prediction of \u03c6 given h is \u03c6(h) \u2261 E(\u03c6 | h,do(\u03c4A)) = 3\u03c41(h) + \u03c42(h). While linear combinations of tests may seem restrictive, our definition is actually very expressive: we can represent an arbitrary function of a finite sequence of future observations. To do so, we take a collection of tests, each of which picks out one possible realization of the sequence, and weight each test by the value of the function conditioned on that realization. For example, if our observations are integers 1, 2, . . . , 10, we can write the square of the next observation as \u221110 o=1 o\n2I(o), and the mean of the next two observations as \u221110 o=1 \u221110 o\u2032=1 1 2 (o+ o \u2032)I(o, o\u2032).\nThe restriction to a common action sequence is necessary: without this restriction, all the tests making up a feature could never be executed at once. Once we move to feature predictions, however, it makes sense to lift this restriction: we will say that any linear combination of feature predictions is also a feature prediction, even if the features involved have different action sequences.\nAction sequences raise some problems with obtaining empirical estimates of means and covariances of features of the future: e.g., it is not always possible to get a sample of a particular feature\u2019s value on every time step, and the feature we choose to sample at one step can restrict which features we can sample at subsequent steps. In order to carry out our derivations without running into these problems repeatedly, we will assume for the rest of the paper that we can reset our system after every sample, and get a new history independently distributed as ht \u223c \u03c9 for some distribution \u03c9. (With some additional bookkeeping we could remove this assumption [23], but this bookkeeping would unnecessarily complicate our derivations.)\nFurthermore, we will introduce some new language, again to keep derivations simple: if we have a vector of features of the future \u03c6T , we will pretend that we can get a sample \u03c6Tt in which we evaluate all of our features starting from a single history ht, even if the different elements of \u03c6T require us to execute different action sequences. When our algorithms call for such a sample, we will instead use the following trick to get a random vector with the correct expectation (and somewhat higher variance, which doesn\u2019t matter for any of our arguments): write \u03c4A1 , \u03c4 A 2 , . . . for the different action sequences, and let \u03b61, \u03b62, . . . > 0 be a probability distribution over these sequences. We pick a single action sequence \u03c4Aa according to \u03b6, and execute \u03c4 A a to get a sample \u03c6\u0302\nT of the features which depend on \u03c4Aa . We then enter \u03c6\u0302\nT /\u03b6a into the corresponding coordinates of \u03c6Tt , and fill in zeros everywhere else. It is easy to see that the expected value of our sample vector is then correct: the probability of selection \u03b6a and the weighting factor 1/\u03b6a cancel out. We will write E(\u03c6T | ht, do(\u03b6)) to stand for this expectation.\nNone of the above tricks are actually necessary in our experiments with stopping problems: we simply execute the \u201ccontinue\u201d action on every step, and use only sequences of \u201ccontinue\u201d actions in every test and feature."}, {"heading": "3.2 Finding Predictive Features Through a Bottleneck", "text": "In order to find a predictive feature compression, we first need to determine what we would like to predict. Since we are interested in value function approximation, the most relevant prediction is the value function itself; so, we could simply try to predict total future discounted reward given a history. Unfortunately, total discounted reward has high variance, so unless we have a lot of data, learning will be difficult.\nWe can reduce variance by including other prediction tasks as well. For example, predicting individual rewards at future time steps, while not strictly necessary to predict total discounted reward, seems highly relevant, and gives us much more immediate feedback. Similarly, future observations hopefully contain information about future reward, so trying to predict observations can help us predict reward better. Finally, in any specific RL application, we may be able to add problem-specific prediction tasks that will help focus our attention on relevant information: for example, in a pathplanning problem, we might try to predict which of several goal states we will reach (in addition to how much it will cost to get there).\nWe can represent all of these prediction tasks as features of the future: e.g., to predict which goal we will reach, we add a distinct observation at each goal state, or to predict individual rewards, we add individual rewards as observations.2 We will write \u03c6Tt for the vector of all features of the \u201cfuture at time t,\u201d i.e., events starting at time t+ 1 and continuing forward.\nSo, instead of remembering a large arbitrary set of features of history, we want to find a small subspace of features of history that is relevant for predicting features of the future. We will call this subspace a predictive compression, and we will write the value function as a linear function of only the predictive compression of features.\nTo find our predictive compression, we will use reduced-rank regression [24]. We define the following empirical covariance matrices between features of the future and features of histories:\n\u03a3\u0302T ,H = 1\nk k\u2211 t=1 \u03c6Tt \u03c6 H t T \u03a3\u0302H,H = 1 k k\u2211 t=1 \u03c6Ht \u03c6 H t T (6)\nLet LH be the lower triangular Cholesky factor of \u03a3\u0302H,H. Then we can find a predictive compression of histories by a singular value decomposition (SVD) of the weighted covariance: write\nUDVT \u2248 \u03a3\u0302T ,HL\u2212TH (7) for a truncated SVD [25] of the weighted covariance, where U are the left singular vectors, VT are the right singular vectors, and D is the diagonal matrix of singular values. The number of columns of U , V , or D is equal to the number of retained singular values.3 Then we define\nU\u0302 = UD1/2 (8) to be the mapping from the low-dimensional compressed space up to the high-dimensional space of features of the future.\nGiven U\u0302 , we would like to find a compression operator V that optimally predicts features of the future through the bottleneck defined by U\u0302 . The least squares estimate can be found by minimizing the loss\nL(V ) = \u2225\u2225\u2225\u03c6T1:k \u2212 U\u0302V \u03c6H1:k\u2225\u2225\u22252\nF (9)\nwhere \u2016 \u00b7 \u2016F denotes the Frobenius norm. We can find the minimum by taking the derivative of this loss with respect to V , setting it to zero, and solving for V (see Appendix, Section A for details), giving us:\nV\u0302 = arg min V L(V ) = U\u0302T\u03a3\u0302T ,H(\u03a3\u0302H,H)\u22121 (10)\nBy weighting different features of the future differently, we can change the approximate compression in interesting ways. For example, as we will see in Section 4.2, scaling up future reward by a constant factor results in a value-directed compression\u2014but, unlike previous ways to find value-directed compressions [11], we do not need to know a model of our system ahead of time. For another example, define LT to be the lower triangular Cholesky factor of the empirical covariance of future features \u03a3\u0302T ,T . Then, if we scale features of the future by L\u2212TT , the singular value decomposition will preserve the largest possible amount of mutual information between features of the future and features of history. This is equivalent to canonical correlation analysis [26, 27], and the matrix D becomes a diagonal matrix of canonical correlations between futures and histories.\n2If we don\u2019t wish to reveal extra information by adding additional observations, we can instead add the corresponding feature predictions as observations; these predictions, by definition, reveal no additional information. To save the trouble of computing these predictions, we can use realized feature values rather than predictions in our learning algorithms below, at the cost of some extra variance: the expectation of the realized feature value is the same as the expectation of the predicted feature value.\n3If our empirical estimate \u03a3\u0302T ,H were exact, we could keep all nonzero singular values to find the smallest possible compression that does not lose any predictive power. In practice, though, there will be noise in our estimate, and \u03a3\u0302T ,HL\u2212TH will be full rank. If we know the true rank n of \u03a3T ,H, we can choose the first n singular values to define a subspace for compression. Or, we can choose a smaller subspace that results in an approximate compression: by selectively dropping columns of U corresponding to small singular values, we can trade off compression against predictive power. Directions of larger variance in features of the future correspond to larger singular values in the SVD, so we minimize prediction error by truncating the smallest singular values. By contrast with an SVD of the unscaled covariance, we do not attempt to minimize reconstruction error for features of history, since features of history are standardized when we multiply by the inverse Cholesky factor."}, {"heading": "3.3 Predictive State Temporal Difference Learning", "text": "Now that we have found a predictive compression operator V\u0302 via Equation 10, we can replace the features of history \u03c6Ht with the compressed features V\u0302 \u03c6 H t in the Bellman recursion, Equation 4. Doing so results in the following approximate Bellman equation:\nwTV\u0302 \u03c6H1:k \u2248 R1:k + \u03b3wTV\u0302 \u03c6H2:k+1 (11)\nThe least squares solution for w is still prone to an error-in-variables problem. The variable \u03c6H is still correlated with the true independent variables and uncorrelated with noise, and so we can again use it as an instrumental variable to unbias the estimate of w. Define the additional empirical covariance matrices:\n\u03a3\u0302R,H = 1\nk k\u2211 t=1 Rt\u03c6Ht T\n\u03a3\u0302H+,H = 1\nk k\u2211 t=1 \u03c6Ht+1\u03c6 H t T (12)\nThen, the corrected Bellman equation is:\nw\u0302TV\u0302 \u03a3\u0302H,H = \u03a3\u0302R,H + \u03b3w\u0302 TV\u0302 \u03a3\u0302H+,H\nand solving for w\u0302 gives us the Predictive State Temporal Difference (PSTD) learning algorithm:\nw\u0302T = \u03a3\u0302R,H ( V\u0302 \u03a3\u0302H,H \u2212 \u03b3V\u0302 \u03a3\u0302H+,H )\u2020 (13)\nSo far we have provided some intuition for why predictive features should be better than arbitrary features for temporal difference learning. Below we will show an additional benefit: the modelfree algorithm in Equation 13 is, under some circumstances, equivalent to a model-based value function approximation method which uses subspace identification to learn Predictive State Representations [20, 21]."}, {"heading": "4 Predictive State Representations", "text": "A predictive state representation (PSR) [14] is a compact and complete description of a dynamical system. Unlike POMDPs, which represent state as a distribution over a latent variable, PSRs represent state as a set of predictions of tests.\nFormally, a PSR consists of five elements \u3008A,O,Q, s1, F \u3009. A is a finite set of possible actions, and O is a finite set of possible observations. Q is a core set of tests, i.e., a set whose vector of predictions Q(h) is a sufficient statistic for predicting the success probabilities of all tests. F is the set of functions f\u03c4 which embody these predictions: \u03c4(h) = f\u03c4 (Q(h)). And, m1 = Q( ) is the initial prediction vector. In this work we will restrict ourselves to linear PSRs, in which all prediction functions are linear: f\u03c4 (Q(h)) = rT\u03c4Q(h) for some vector r\u03c4 \u2208 R|Q|. Finally, a core set Q for a linear PSR is said to be minimal if the tests in Q are linearly independent [16, 15], i.e., no one test\u2019s prediction is a linear function of the other tests\u2019 predictions.\nSince Q(h) is a sufficient statistic for all tests, it is a state for our PSR: i.e., we can remember just Q(h) instead of h itself. After action a and observation o, we can update Q(h) recursively: if we write Mao for the matrix with rows rTao\u03c4 for \u03c4 \u2208 Q, then we can use Bayes\u2019 Rule to show:\nQ(hao) = MaoQ(h)\nPr[o |h, do(a)] =\nMaoQ(h)\nmT\u221eMaoQ(h) (14)\nwhere m\u221e is a normalizer, defined by mT\u221eQ(h) = 1 for all h.\nIn addition to the above PSR parameters, we need a few additional definitions for reinforcement learning: a reward function R(h) = \u03b7TQ(h) mapping predictive states to immediate rewards, a discount factor \u03b3 \u2208 [0, 1] which weights the importance of future rewards vs. present ones, and a policy \u03c0(Q(h)) mapping from predictive states to actions. (Specifying a reward in terms of the core test predictions Q(h) is fully general: e.g., if we want to add a unit reward for some test \u03c4 6\u2208 Q, we can instead equivalently set \u03b7 := \u03b7 + r\u03c4 , where r\u03c4 is defined (as above) so that \u03c4(h) = rT\u03c4Q(h).)\nInstead of ordinary PSRs, we will work with transformed PSRs (TPSRs) [20, 21]. TPSRs are a generalization of regular PSRs: a TPSR maintains a small number of sufficient statistics which are\nlinear combinations of a (potentially very large) set of test probabilities. That is, a TPSR maintains a small number of feature predictions instead of test predictions. TPSRs have exactly the same predictive abilities as regular PSRs, but are invariant under similarity transforms: given an invertible matrix S, we can transformm1 \u2192 Sm1,mT\u221e \u2192 mT\u221eS\u22121, andMao \u2192 SMaoS\u22121 without changing the corresponding dynamical system, since pairs S\u22121S cancel in Eq. 14. The main benefit of TPSRs over regular PSRs is that, given any core set of tests, low dimensional parameters can be found using spectral matrix decomposition and regression instead of combinatorial search. In this respect, TPSRs are closely related to the transformed representations of LDSs and HMMs found by subspace identification [28, 29, 27, 30]."}, {"heading": "4.1 Learning Transformed PSRs", "text": "Let Q be a minimal core set of tests for a dynamical system, with cardinality n = |Q| equal to the linear dimension of the system. Then, let T be a larger core set of tests (not necessarily minimal, and possibly even with |T | countably infinite). And, letH be the set of all possible histories. (|H| is finite or countably infinite, depending on whether our system is finite-horizon or infinite-horizon.)\nAs before, write \u03c6Ht \u2208 R` for a vector of features of history at time t, and write \u03c6Tt \u2208 R` for a vector of features of the future at time t. Since T is a core set of tests, by definition we can compute any test prediction \u03c4(h) as a linear function of T (h). And, since feature predictions are linear combinations of test predictions, we can also compute any feature prediction \u03c6(h) as a linear function of T (h). We define the matrix \u03a6T \u2208 R`\u00d7|T | to embody our predictions of future features: that is, an entry of \u03a6T is the weight of one of the tests in T for calculating the prediction of one of the features in \u03c6T . Below we define several covariance matrices, Equation 15(a\u2013d), in terms of the observable quantities \u03c6Tt , \u03c6 H t , at, and ot, and show how these matrices relate to the parameters of the underlying PSR. These relationships then lead to our learning algorithm, Eq. 17 below.\nFirst we define \u03a3H,H, the covariance matrix of features of histories, as E[\u03c6Ht \u03c6Ht T | ht \u223c \u03c9]. Given k samples, we can approximate this covariance:\n[\u03a3\u0302H,H]i,j = 1\nk k\u2211 t=1 \u03c6Hit\u03c6 H jt =\u21d2 \u03a3\u0302H,H = 1 k \u03c6H1:k\u03c6 H 1:k T . (15a)\nAs k \u2192\u221e, the empirical covariance \u03a3\u0302H,H converges to the true covariance \u03a3H,H with probability 1. Next we define \u03a3S,H, the cross covariance of states and features of histories. Writing st = Q(ht) for the (unobserved) state at time t, let\n\u03a3S,H = E [ 1\nk s1:k\u03c6\nH 1:k T \u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t)]\nWe cannot directly estimate \u03a3S,H from data, but this matrix will appear as a factor in several of the matrices that we define below.\nNext we define \u03a3T ,H, the cross covariance matrix of the features of tests and histories: \u03a3T ,H \u2261 E[\u03c6Tt \u03c6Ht T | ht \u223c \u03c9, do(\u03b6)]. The true covariance is the expectation of the sample covariance \u03a3\u0302T ,H:\n[\u03a3\u0302T ,H]i,j \u2261 1\nk k\u2211 t=1 \u03c6Ti,t\u03c6 H j,t\n[\u03a3T ,H]i,j = E\n[ 1\nk k\u2211 t=1 \u03c6Ti,t\u03c6 H j,t \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t), do(\u03b6) (\u2200t) ]\n= E\n[ 1\nk k\u2211 t=1 E [ \u03c6Ti,t | ht, do(\u03b6) ] \u03c6Hj,t \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t), do(\u03b6) (\u2200t) ]\n= E\n[ 1\nk k\u2211 t=1 \u2211 \u03c4\u2208T \u03a6Ti,\u03c4\u03c4(ht)\u03c6 H j,t \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t) ]\n= E\n[ 1\nk k\u2211 t=1 \u2211 \u03c4\u2208T \u03a6Ti,\u03c4r T \u03c4Q(ht)\u03c6 H j,t \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t) ]\n= \u2211 \u03c4\u2208T \u03a6Ti,\u03c4r T \u03c4 E\n[ 1\nk k\u2211 t=1 Q(ht)\u03c6 H j,t \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t) ]\n= \u2211 \u03c4\u2208T \u03a6Ti,\u03c4r T \u03c4 E\n[ 1\nk k\u2211 t=1 st\u03c6 H j,t \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t) ]\n=\u21d2 \u03a3T ,H = \u03a6T R\u03a3S,H (15b)\nwhere the vector r\u03c4 is the linear function that specifies the probability of the test \u03c4 given the probabilities of tests in the core set Q, and the matrix R has all of the r\u03c4 vectors as rows.\nThe above derivation shows that, because of our assumptions about the linear dimension of the system, the matrix \u03a3T ,H has factors R \u2208 R|T |\u00d7n and \u03a3S,H \u2208 Rn\u00d7`. Therefore, the rank of \u03a3T ,H is no more than n, the linear dimension of the system. We can also see that, since the size of \u03a3T ,H is fixed but the number of samples k is increasing, the empirical covariance \u03a3\u0302T ,H converges to the true covariance \u03a3T ,H with probability 1.\nNext we define \u03a3H,ao,H, a set of matrices, one for each action-observation pair, that represent the covariance between features of history before and after taking action a and observing o. In the following, It(o) is an indicator variable for whether we see observation o at step t.\n\u03a3\u0302H,ao,H \u2261 1\nk k\u2211 t=1 \u03c6Ht+1It(o)\u03c6Ht T\n\u03a3H,ao,H \u2261 E [ \u03a3\u0302H,ao,H \u2223\u2223\u2223ht \u223c \u03c9 (\u2200t), do(a) (\u2200t)] = E [ 1\nk k\u2211 t=1 \u03c6Ht+1It(o)\u03c6Ht T \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t), do(a) (\u2200t) ]\n(15c)\nSince the dimensions of each \u03a3\u0302H,ao,H are fixed, as k \u2192\u221e these empirical covariances converge to the true covariances \u03a3H,ao,H with probability 1.\nFinally we define \u03a3R,H \u2261 E[Rt\u03c6Ht T | ht \u223c \u03c9], and approximate the covariance (in this case a vector) of reward and features of history:\n\u03a3\u0302R,H \u2261 1\nk k\u2211 t=1 Rt\u03c6Ht T\n\u03a3R,H \u2261 E [ \u03a3\u0302R,H \u2223\u2223\u2223ht \u223c \u03c9 (\u2200t)] = E [ 1\nk k\u2211 t=1 Rt\u03c6Ht T \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t) ]\n= E\n[ 1\nk k\u2211 t=1 \u03b7TQ(ht)\u03c6 H t T \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t) ]\n= \u03b7TE\n[ 1\nk k\u2211 t=1 st\u03c6 H t T \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t) ]\n= \u03b7T\u03a3S,H (15d)\nAgain, as k \u2192\u221e, \u03a3\u0302R,H converges to \u03a3R,H with probability 1. We now wish to use the above-defined matrices to learn a TPSR from data. To do so we need to make a somewhat-restrictive assumption: we assume that our features of history are rich enough to determine the state of the system, i.e., the regression from \u03c6H to s is exact: st = \u03a3S,H\u03a3\u22121H,H\u03c6 H t . We discuss how to relax this assumption below in Section 4.3. We also need a matrix U such that UT\u03a6T R is invertible; with probability 1 a random matrix satisfies this condition, but as we will see below, it is useful to choose U via SVD of a scaled version of \u03a3T ,H as described in Sec. 3.2.\nUsing our assumptions we can show a useful identity for \u03a3H,ao,H:\n\u03a3S,H\u03a3 \u22121 H,H\u03a3H,ao,H = E\n[ 1\nk k\u2211 t=1 \u03a3S,H\u03a3 \u22121 H,H\u03c6 H t+1It(o)\u03c6Ht T \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t), do(a) (\u2200t) ]\n= E\n[ 1\nk k\u2211 t=1 st+1It(o)\u03c6Ht T \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t), do(a) (\u2200t) ]\n= E\n[ 1\nk k\u2211 t=1 Maost\u03c6 H t T \u2223\u2223\u2223\u2223\u2223ht \u223c \u03c9 (\u2200t) ]\n= Mao\u03a3S,H (16)\nThis identity is at the heart of our learning algorithm: it shows that \u03a3H,ao,H contains a hidden copy of Mao, the main TPSR parameter that we need to learn. We would like to recover Mao via Eq. 16, Mao = \u03a3S,H\u03a3 \u22121 H,H\u03a3H,ao,H\u03a3 \u2020 S,H; but of course we do not know \u03a3S,H. Fortunately, though, it turns out that we can use UT\u03a3T ,H as a stand-in, as described below, since this matrix differs from \u03a3S,H only by an invertible transform (Eq. 15b).\nWe now show how to recover a TPSR from the matrices \u03a3T ,H, \u03a3H,H, \u03a3R,H, \u03a3H,ao,H, and U . Since a TPSR\u2019s predictions are invariant to a similarity transform of its parameters, our algorithm\nonly recovers the TPSR parameters to within a similarity transform.\nbt \u2261 UT\u03a3T ,H(\u03a3H,H)\u22121\u03c6Ht = UT\u03a6T R\u03a3S,H(\u03a3H,H) \u22121\u03c6Ht\n= (UT\u03a6T R)st (17a)\nBao \u2261 UT\u03a3T ,H(\u03a3H,H)\u22121\u03a3H,ao,H(UT\u03a3T ,H)\u2020\n= UT\u03a6T R\u03a3S,H(\u03a3H,H) \u22121\u03a3H,ao,H(U T\u03a3T ,H) \u2020\n= (UT\u03a6T R)Mao \u03a3S,H(U T\u03a3T ,H) \u2020\n= (UT\u03a6T R)Mao(U T\u03a6T R)\u22121(UT\u03a6T R)\u03a3S,H(U T\u03a3T ,H) \u2020\n= (UT\u03a6T R)Mao(U T\u03a6T R)\u22121 (17b)\nbT\u03b7 \u2261 \u03a3R,H(UT\u03a3T ,H)\u2020\n= \u03b7T\u03a3S,H(U T\u03a3T ,H)\n\u2020\n= \u03b7T(UT\u03a6T R)\u22121(UT\u03a6T R)\u03a3S,H(U T\u03a3T ,H) \u2020\n= \u03b7T(UT\u03a6T R)\u22121 (17c)\nOur PSR learning algorithm is simple: simply replace each true covariance matrix in Eq. 17 by its empirical estimate. Since the empirical estimates converge to their true values with probability 1 as the sample size increases, our learning algorithm is clearly statistically consistent."}, {"heading": "4.2 Predictive State Temporal Difference Learning (Revisited)", "text": "Finally, we are ready to show that the model-free PSTD learning algorithm introduced in Section 3.3 is equivalent to a model-based algorithm built around PSR learning. For a fixed policy \u03c0, a TPSR\u2019s value function is a linear function of state, J\u03c0(s) = wTb, and is the solution of the TPSR Bellman equation [31]: for all b, wTb = bT\u03b7 b+ \u03b3 \u2211 o\u2208O w TB\u03c0ob, or equivalently,\nwT = bT\u03b7 + \u03b3 \u2211 o\u2208O wTB\u03c0o\nIf we substitute in our learned PSR parameters from Equations 17(a\u2013c), we get\nw\u0302T = \u03a3\u0302R,H(U T\u03a3\u0302T ,H) \u2020 + \u03b3 \u2211 o\u2208O w\u0302TUT\u03a3\u0302T ,H(\u03a3\u0302H,H) \u22121\u03a3\u0302H,\u03c0o,H(U T\u03a3\u0302T ,H) \u2020\nw\u0302TUT\u03a3\u0302T ,H = \u03a3\u0302R,H + \u03b3w\u0302 TUT\u03a3\u0302T ,H(\u03a3\u0302H,H) \u22121\u03a3\u0302H+,H\nsince, by comparing Eqs. 15c and 12, we can see that \u2211 o\u2208O \u03a3\u0302H,\u03c0o,H = \u03a3\u0302H+,H. Now, suppose that we define U\u0302 and V\u0302 by Eqs. 8 and 10, and let U = U\u0302 as suggested above in Sec. 4.1. Then UT\u03a3\u0302T ,H = V\u0302 \u03a3\u0302H,H, and\nw\u0302TV\u0302 \u03a3\u0302H,H = \u03a3\u0302R,H + \u03b3w\u0302 TV\u0302 \u03a3\u0302H+,H\nw\u0302T = \u03a3\u0302R,H ( V\u0302 \u03a3\u0302H,H \u2212 \u03b3V\u0302 \u03a3\u0302H+,H )\u2020 (18)\nEq. 18 is exactly the PSTD algorithm (Eq. 13). So, we have shown that, if we learn a PSR by the subspace identification algorithm of Sec. 4.1 and then compute its value function via the Bellman equation, we get the exact same answer as if we had directly learned the value function via the model-free PSTD method. In addition to adding to our understanding of both methods, an important corollary of this result is that PSTD is a statistically consistent algorithm for PSR value function approximation\u2014to our knowledge, the first such result for a TD method.\nPSTD learning is related to value-directed compression of POMDPs [11]. If we learn a TPSR from data generated by a POMDP, then the TPSR state is exactly a linear compression of the POMDP state [15, 20]. The compression can be exact or approximate, depending on whether we include enough features of the future and whether we keep all or only some nonzero singular values in our bottleneck. If we include only reward as a feature of the future, we get a value-directed compression\nin the sense of Poupart and Boutilier [11]. If desired, we can tune the degree of value-directedness of our compression by scaling the relative variance of our features: the higher the variance of the reward feature compared to other features, the more value-directed the resulting compression will be. Our work significantly diverges from previous work on POMDP compression in one important respect: prior work assumes access to the true POMDP model, while we make no such assumption, and learn a compressed representation directly from data."}, {"heading": "4.3 Insights from Subspace Identification", "text": "The close connection to subspace identification for PSRs provides additional insight into the temporal difference learning procedure. In Equation 17 we made the assumption that the features of history are rich enough to completely determine the state of the dynamical system. In fact, using theory developed in [21], it is possible to relax this assumption and instead assume that state is merely correlated with features of history. In this case, we need to introduce a new set of covariance matrices \u03a3T ,ao,H \u2261 E[\u03c6Tt It(o)\u03c6Ht\nT | ht \u223c \u03c9, do(a, \u03b6)], one for each actionobservation pair, that represent the covariance between features of history before and features of tests after taking action a and observing o. We can then estimate the TPSR transition matrices as B\u0302ao = U\u0302T\u03a3\u0302T ,ao,H(U\u0302T\u03a3\u0302T ,H)\u2020 (see [21] for proof details). The value function parameter w can be estimated as w\u0302T = \u03a3\u0302R,H(U\u0302T\u03a3\u0302T ,H)\u2020(I \u2212 \u2211 o\u2208O U\u0302 T\u03a3\u0302T ,ao,H(U\u0302 T\u03a3\u0302T ,H) \u2020)\u2020 = \u03a3\u0302R,H(U\u0302 T\u03a3\u0302T ,H \u2212 \u2211 o\u2208O U\u0302 T\u03a3\u0302T ,ao,H) \u2020 (the proof is similar to Equation 18). Since we no longer assume that state is completely specified by features of history, we can no longer apply the learned value function to U\u0302\u03a3T ,H(\u03a3H,H)\u22121\u03c6t at each time t. Instead we need to learn a full PSR model and filter with the model to estimate state. Details on this procedure can be found in [21]."}, {"heading": "5 Experimental Results", "text": "We designed several experiments to evaluate the properties of the PSTD learning algorithm. In the first set of experiments we look at the comparative merits of PSTD with respect to LSTD and LARS-TD when applied to the problem of estimating the value function of a reduced-rank POMDP. In the second set of experiments, we apply PSTD to a benchmark optimal stopping problem (pricing a fictitious financial derivative), and show that PSTD outperforms competing approaches."}, {"heading": "5.1 Estimating the Value Function of a RR-POMDP", "text": "We evaluate the PSTD learning algorithm on a synthetic example derived from [32]. The problem is to find the value function of a policy in a partially observable Markov decision Process (POMDP). The POMDP has 4 latent states, but the policy\u2019s transition matrix is low rank: the resulting belief distributions can be represented in a 3-dimensional subspace of the original belief simplex. A reward of 1 is given in the first and third latent state and a reward of 0 in the other two latent states (see Appendix, Section B). The system emits 2 possible observations, conflating information about the latent states.\nWe perform 3 experiments, comparing the performance of LSTD, LARS-TD, PSTD, and PSTD as formulated in Section 4.3 (which we call PSTD2) when different sets of features are used. In each case we compare the value function estimated by each algorithm to the true value function computed by J\u03c0 = R(I \u2212 \u03b3T\u03c0)\u22121. In the first experiment we execute the policy \u03c0 for 1000 time steps. We split the data into overlapping histories and tests of length 5, and sample 10 of these histories and tests to serve as centers for Gaussian radial basis functions. We then evaluate each basis function at every remaining sample. Then, using these features, we learned the value function using LSTD, LARS-TD, PSTD with linear dimension 3, and PSTD2 with linear dimension 3 (Figure 1(A)).4 In this experiment, PSTD and PSTD2 both had lower mean squared error than the other approaches. For the second experiment, we added 490 random features to the 10 good features and then attempted to learn the value function with each of the 3 algorithms (Figure 1(B)). In this case, LSTD and PSTD both had difficulty fitting\n4Comparing LSTD and PSTD is straightforward; the two methods differ only by the compression operator V\u0302 .\nthe value function due to the large number of irrelevant features in both tests and histories and the relatively small amount of training data. LARS-TD, designed for precisely this scenario, was able to select the 10 relevant features and estimate the value function better by a substantial margin. Surprisingly, in this experiment PSTD2 not only outperformed PSTD but bested even LARS-TD. For the third experiment, we increased the number of sampled features from 10 to 500. In this case, each feature was somewhat relevant, but the number of features was relatively large compared to the amount of training data. This situation occurs frequently in practice: it is often easy to find a large number of features that are at least somewhat related to state. PSTD and PSTD2 both outperform LARS-TD and each of these subspace and subset selection methods outperform LSTD by a large margin by efficiently estimating the value function (Figure 1(C))."}, {"heading": "5.2 Pricing A High-dimensional Financial Derivative", "text": "Derivatives are financial contracts with payoffs linked to the future prices of basic assets such as stocks, bonds and commodities. In some derivatives the contract holder has no choices, but in more complex cases, the contract owner must make decisions\u2014e.g., with early exercise the contract holder can decide to terminate the contract at any time and receive payments based on prevailing market conditions. In these cases, the value of the derivative depends on how the contract holder acts. Deciding when to exercise is therefore an optimal stopping problem: at each point in time, the contract holder must decide whether to continue holding the contract or exercise. Such stopping problems provide an ideal testbed for policy evaluation methods, since we can easily collect a single data set which is sufficient to evaluate any policy: we just choose the \u201ccontinue\u201d action forever. (We can then evaluate the \u201cstop\u201d action easily in any of the resulting states, since the immediate reward is given by the rules of the contract, and the next state is the terminal state by definition.)\nWe consider the financial derivative introduced by Tsitsiklis and Van Roy [33]. The derivative generates payoffs that are contingent on the prices of a single stock. At the end of a given day, the holder may opt to exercise. At exercise the owner receives a payoff equal to the current price of the stock divided by the price 100 days beforehand. We can think of this derivative as a \u201cpsychic call\u201d: the owner gets to decide whether s/he would like to have bought an ordinary 100-day European call option, at the then-current market price, 100 days ago.\nIn our simulation (and unknown to the investor), the underlying stock price follows a geometric Brownian motion with volatility \u03c3 = 0.02 and continuously compounded short term growth rate\n\u03c1 = 0.0004. Assuming stock prices fluctuate only on days when the market is open, these parameters correspond to an annual growth rate of \u223c 10%. In more detail, if wt is a standard Brownian motion, then the stock price pt evolves as \u2207pt = \u03c1pt\u2207t + \u03c3pt\u2207wt, and we can summarize relevant state at the end of each day as a vector xt \u2208 R100, with xt = ( pt\u221299 pt\u2212100 , pt\u221298pt\u2212100 , . . . , pt pt\u2212100 )T . The ith dimension xt(i) represents the amount a $1 investment in a stock at time t \u2212 100 would grow to at time t \u2212 100 + i. This process is Markov and ergodic [33, 34]: xt and xt+100 are independent and identically distributed. The immediate reward for exercising the option is G(x) = x(100), and the immediate reward for continuing to hold the option is 0. The discount factor \u03b3 = e\u2212\u03c1 is determined by the growth rate; this corresponds to assuming that the risk-free interest rate is equal to the stock\u2019s growth rate, meaning that the investor gains nothing in expectation by holding the stock itself.\nThe value of the derivative, if the current state is x, is given by V \u2217(x) = supt E[\u03b3tG(xt) | x0 = x]. Our goal is to calculate an approximate value function V (x) = wT\u03c6H(x), and then use this value function to generate a stopping time min{t |G(xt) \u2265 V (xt)}. To do so, we sample a sequence of 1,000,000 states xt \u2208 R100 and calculate features \u03c6H of each state. We then perform policy iteration on this sample, alternately estimating the value function under a given policy and then using this value function to define a new greedy policy \u201cstop if G(xt) \u2265 wT\u03c6H(xt).\u201d Within the above strategy, we have two main choices: which features do we use, and how do we estimate the value function in terms of these features. For value function estimation, we used LSTD, LARS-TD, or PSTD. In each case we re-used our 1,000,000-state sample trajectory for all iterations: we start at the beginning and follow the trajectory as long as the policy chooses the \u201ccontinue\u201d action, with reward 0 at each step. When the policy executes the \u201cstop\u201d action, the reward is G(x) and the next state\u2019s features are all 0; we then restart the policy 100 steps in the future, after the process has fully mixed. For feature selection, we are fortunate: previous researchers have hand-selected a \u201cgood\u201d set of 16 features for this data set through repeated trial and error (see Appendix, Section B and [33, 34]). We greatly expand this set of features, then use PSTD to synthesize a small set of highquality combined features. Specifically, we add the entire 100-step state vector, the squares of the components of the state vector, and several additional nonlinear features, increasing the total number of features from 16 to 220. We use histories of length 1, tests of length 5, and (for comparison\u2019s sake) we choose a linear dimension of 16. Tests (but not histories) were value-directed by reducing the variance of all features except reward by a factor of 100.\nFigure 1D shows results. We compared PSTD (reducing 220 to 16 features) to LSTD with either the 16 hand-selected features or the full 220 features, as well as to LARS-TD (220 features) and to a simple thresholding strategy [33]. In each case we evaluated the final policy on 10,000 new random trajectories. PSTD outperformed each of its competitors, improving on the next best approach, LARS-TD, by 1.75 percentage points. In fact, PSTD performs better than the best previously reported approach [33, 34] by 1.24 percentage points. These improvements correspond to appreciable fractions of the risk-free interest rate (which is about 4 percentage points over the 100 day window of the contract), and therefore to significant arbitrage opportunities: an investor who doesn\u2019t know the best strategy will consistently undervalue the security, allowing an informed investor to buy it for below its expected value."}, {"heading": "6 Conclusion", "text": "In this paper, we attack the feature selection problem for temporal difference learning. Although well-known temporal difference algorithms such as LSTD can provide asymptotically unbiased estimates of value function parameters in linear architectures, they can have trouble in finite samples: if the number of features is large relative to the number of training samples, then they can have high variance in their value function estimates. For this reason, in real-world problems, a substantial amount of time is spent selecting a small set of features, often by trial and error [33, 34].\nTo remedy this problem, we present the PSTD algorithm, a new approach to feature selection for TD methods, which demonstrates how insights from system identification can benefit reinforcement learning. PSTD automatically chooses a small set of features that are relevant for prediction and value function approximation. It approaches feature selection from a bottleneck perspective, by finding a small set of features that preserves only predictive information. Because of the focus on predictive information, the PSTD approach is closely connected to PSRs: under appropriate\nassumptions, PSTD\u2019s compressed set of features is asymptotically equivalent to TPSR state, and PSTD is a consistent estimator of the PSR value function.\nWe demonstrate the merits of PSTD compared to two popular alternative algorithms, LARS-TD and LSTD, on a synthetic example, and argue that PSTD is most effective when approximating a value function from a large number of features, each of which contains at least a little information about state. Finally, we apply PSTD to a difficult optimal stopping problem, and demonstrate the practical utility of the algorithm by outperforming several alternative approaches and topping the best reported previous results."}, {"heading": "Acknowledgements", "text": "Byron Boots was supported by the NSF under grant number EEEC-0540865. Byron Boots and Geoffrey J. Gordon were supported by ONR MURI grant number N00014-09-1-1052."}, {"heading": "A Determining the Compression Operator", "text": "We find a compression operator V that optimally predicts test-features through the CCA bottleneck defined by U\u0302 . The least squares estimate can be found by minimizing the following loss\nL(V ) = \u2225\u2225\u2225\u03c6T1:k \u2212 U\u0302V \u03c6H1:k\u2225\u2225\u22252\nF\nV\u0302 = arg min V L(V )\nwhere \u2016 \u00b7 \u2016F denotes the Frobenius norm. We can find V\u0302 by taking a derivative of this loss L with respect to V , setting it to zero, and solving for V\nL = 1 k\ntr ( (\u03c6T1:k \u2212 U\u0302V \u03c6H1:k)(\u03c6T1:k \u2212 U\u0302V \u03c6H1:k)T )\n= 1 k tr ( \u03c6T1:k T \u03c6T1:k \u2212 2\u03c6T1:k T U\u0302V \u03c6H1:k + \u03c6 H 1:k T V TU\u0302TU\u0302V \u03c6H1:k ) =\u21d2 dL = \u22122tr ( \u03c6H1:k T dV TU\u0302T\u03c6T1:k ) + 2tr ( \u03c6H1:k T dV TU\u0302TU\u0302V \u03c6\u0302H1:k\n) =\u21d2 dL = \u22122tr ( dV TU\u0302T\u03c6T1:k\u03c6 H 1:k T ) + 2tr ( dV TU\u0302TU\u0302V \u03c6\u0302H1:k\u03c6 H 1:k T )\n=\u21d2 dL = \u22122tr ( dV TU\u0302T\u03a3\u0302T ,H ) + 2tr ( dV TU\u0302TU\u0302V \u03a3\u0302H,H ) =\u21d2 dL\ndV T = \u22122tr\n( U\u0302T\u03a3\u0302T ,H ) + 2tr ( U\u0302TU\u0302V \u03a3\u0302H,H ) =\u21d2 0 = \u2212U\u0302T\u03a3\u0302T ,H + U\u0302TU\u0302V \u03a3\u0302H,H\n=\u21d2 V\u0302 = (U\u0302TU\u0302)\u22121U\u0302T\u03a3\u0302T ,H(\u03a3\u0302H,H)\u22121\n= U\u0302T\u03a3\u0302T ,H(\u03a3\u0302H,H) \u22121"}, {"heading": "B Experimental Results", "text": "B.1 RR-POMDP\nThe RR-POMDP parameters are:\n[m = 4 hidden states, n = 2 observations, k = 3 transition matrix rank].\nT\u03c0 =  0.7829 0.1036 0.0399 0.07360.1036 0.4237 0.4262 0.04650.0399 0.4262 0.4380 0.0959 0.0736 0.0465 0.0959 0.7840  O = [ 1 0 1 00 1 0 1 ]\nThe discount factor is \u03b3 = 0.9.\nB.2 Pricing a financial derivative\nBasis functions The fist 16 are the basis functions suggested by Van Roy; for full description and justification see [33, 34]. The first functions consist of a constant, the reward, the minimal and\nmaximal returns, and how long ago they occurred:\n\u03c61(x) = 1\n\u03c62(x) = G(x)\n\u03c63(x) = min i=1,...,100\nx(i)\u2212 1\n\u03c64(x) = max i=1,...,100\nx(i)\u2212 1\n\u03c65(x) = arg min i=1,...,100\nx(i)\u2212 1\n\u03c66(x) = arg max i=1,...,100\nx(i)\u2212 1\nThe next set of basis functions summarize the characteristics of the basic shape of the 100 day sample path. They are the inner product of the path with the first four Legendre polynomial degrees. Let j = i/50\u2212 1.\n\u03c67(x) = 1\n100 100\u2211 i=1 x(i)\u2212 1\u221a 2\n\u03c68(x) = 1\n100 100\u2211 i=1 x(i)\n\u221a 3\n2 j\n\u03c69(x) = 1\n100 100\u2211 i=1 x(i)\n\u221a 5\n2\n( 3j2 \u2212 1\n2\n)\n\u03c610(x) = 1\n100 100\u2211 i=1 x(i)\n\u221a 7\n2\n( 5j3 \u2212 3j\n2 ) Nonlinear combinations of basis functions:\n\u03c611(x) = \u03c62(x)\u03c63(x)\n\u03c612(x) = \u03c62(x)\u03c64(x)\n\u03c613(x) = \u03c62(x)\u03c67(x)\n\u03c614(x) = \u03c62(x)\u03c68(x)\n\u03c615(x) = \u03c62(x)\u03c69(x)\n\u03c616(x) = \u03c62(x)\u03c610(x)\nIn order to improve our results, we added a large number of additional basis functions to these hand-picked 16. PSTD will compress these features for us, so we can use as many additional basis functions as we would like. First we defined 4 additional basis functions consisting of the inner products of the 100 day sample path with the 5th and 6th Legende polynomials and we added the corresponding nonlinear combinations of basis functions:\n\u03c617(x) = 1\n100 100\u2211 i=1 x(i)\n\u221a 9\n2\n( 35j4 \u2212 30x2 + 3\n8\n)\n\u03c618(x) = 1\n100 100\u2211 i=1 x(i)\n\u221a 11\n2\n( 63j5 \u2212 70j3 + 15j\n8 ) \u03c619(x) = \u03c62(x)\u03c617(x)\n\u03c620(x) = \u03c62(x)\u03c618(x)\nFinally we added the the entire sample path and the squared sample path:\n\u03c621:120 = x1:100\n\u03c6121:220 = x 2 1:100"}], "references": [{"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1988}, {"title": "Least-squares temporal difference learning", "author": ["Justin A. Boyan"], "venue": "In Proc. Intl. Conf. Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["Steven J. Bradtke", "Andrew G. Barto"], "venue": "In Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Least-squares policy iteration", "author": ["Michail G. Lagoudakis", "Ronald Parr"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Representation policy iteration", "author": ["Sridhar Mahadevan"], "venue": "In Proceedings of the Proceedings of the Twenty-First Conference Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Samuel meets amarel: automating value function approximation using global state space analysis", "author": ["Sridhar Mahadevan"], "venue": "Proceedings of the 20th national conference on Artificial intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Compact spectral bases for value function approximation using kronecker factorization", "author": ["Jeff Johns", "Sridhar Mahadevan", "Chang Wang"], "venue": "Proceedings of the 22nd national conference on Artificial intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Optimal control of Markov decision processes with incomplete state estimation", "author": ["K.J. Astr\u00f6m"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1965}, {"title": "The Optimal Control of Partially Observable Markov Processes", "author": ["E.J. Sondik"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1971}, {"title": "Acting optimally in partially observable stochastic domains", "author": ["Anthony R. Cassandra", "Leslie P. Kaelbling", "Michael R. Littman"], "venue": "In Proc. AAAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Value-directed compression of pomdps", "author": ["Pascal Poupart", "Craig Boutilier"], "venue": "In NIPS, pages 1547\u20131554,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "A novel orthogonal nmf-based belief compression for pomdps", "author": ["Xin Li", "William K.W. Cheung", "Jiming Liu", "Zhili Wu"], "venue": "Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Compressing pomdps using locality preserving non-negative matrix factorization", "author": ["Georgios Theocharous", "Sridhar Mahadevan"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Predictive representations of state", "author": ["Michael Littman", "Richard Sutton", "Satinder Singh"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Predictive state representations: A new theory for modeling dynamical systems", "author": ["Satinder Singh", "Michael James", "Matthew Rudary"], "venue": "In Proc. UAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Observable operator models for discrete stochastic time series", "author": ["Herbert Jaeger"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Planning in pomdps using multiplicity automata", "author": ["Y. Eyal Even-dar"], "venue": "Proceedings of 21st Conference on Uncertainty in Artificial Intelligence (UAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Regularization and feature selection in least-squares temporal difference learning", "author": ["J. Zico Kolter", "Andrew Y. Ng"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning", "author": ["Ronald Parr", "Lihong Li", "Gavin Taylor", "Christopher Painter-Wakefield", "Michael L. Littman"], "venue": "In ICML \u201908: Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Learning low dimensional predictive representations", "author": ["Matthew Rosencrantz", "Geoffrey J. Gordon", "Sebastian Thrun"], "venue": "In Proc. ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["Byron Boots", "Sajid M. Siddiqi", "Geoffrey J. Gordon"], "venue": "In Proceedings of Robotics: Science and Systems VI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Causality: models, reasoning, and inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "Learning predictive state representations using non-blind policies", "author": ["Michael Bowling", "Peter McCracken", "Michael James", "James Neufeld", "Dana Wilkinson"], "venue": "In Proc. ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Multivariate Reduced-rank Regression: Theory and Applications", "author": ["Gregory C. Reinsel", "Rajabather Palani Velu"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Matrix Computations", "author": ["Gene H. Golub", "Charles F. Van Loan"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1996}, {"title": "The most predictable criterion", "author": ["Harold Hotelling"], "venue": "Journal of Educational Psychology,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1935}, {"title": "Dynamic data factorization", "author": ["S. Soatto", "A. Chiuso"], "venue": "Technical report,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Subspace Identification for Linear Systems: Theory, Implementation, Applications", "author": ["P. Van Overschee", "B. De Moor"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1996}, {"title": "Subspace Methods for System Identification", "author": ["Tohru Katayama"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["Daniel Hsu", "Sham Kakade", "Tong Zhang"], "venue": "In COLT,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Improving approximate value iteration using memories and predictive state representations", "author": ["Michael R. James", "Ton Wessling", "Nikos A. Vlassis"], "venue": "In AAAI,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Reduced-rank hidden Markov models", "author": ["Sajid Siddiqi", "Byron Boots", "Geoffrey J. Gordon"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Optimal stopping of markov processes: Hilbert space theory, approximation algorithms, and an application to pricing high-dimensional financial derivatives", "author": ["John N. Tsitsiklis", "Benjamin Van Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "A popular family of model-free algorithms called temporal difference (TD) algorithms [1] can then be used to estimate the parameters of the value function.", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "Least-squares TD (LSTD) algorithms [2, 3, 4] exploit the linearity of the value function to find the optimal parameters in a least-squares sense from time-adjacent samples of features.", "startOffset": 35, "endOffset": 44}, {"referenceID": 2, "context": "Least-squares TD (LSTD) algorithms [2, 3, 4] exploit the linearity of the value function to find the optimal parameters in a least-squares sense from time-adjacent samples of features.", "startOffset": 35, "endOffset": 44}, {"referenceID": 3, "context": "Least-squares TD (LSTD) algorithms [2, 3, 4] exploit the linearity of the value function to find the optimal parameters in a least-squares sense from time-adjacent samples of features.", "startOffset": 35, "endOffset": 44}, {"referenceID": 4, "context": ", by using a spectral analysis of the state-space transition graph to discover a low-dimensional feature set that preserves the graph structure [5, 6, 7].", "startOffset": 144, "endOffset": 153}, {"referenceID": 5, "context": ", by using a spectral analysis of the state-space transition graph to discover a low-dimensional feature set that preserves the graph structure [5, 6, 7].", "startOffset": 144, "endOffset": 153}, {"referenceID": 6, "context": ", by using a spectral analysis of the state-space transition graph to discover a low-dimensional feature set that preserves the graph structure [5, 6, 7].", "startOffset": 144, "endOffset": 153}, {"referenceID": 7, "context": "Partially observable Markov decision processes (POMDPs) extend MDPs to situations where the state is not directly observable [8, 9, 10].", "startOffset": 125, "endOffset": 135}, {"referenceID": 8, "context": "Partially observable Markov decision processes (POMDPs) extend MDPs to situations where the state is not directly observable [8, 9, 10].", "startOffset": 125, "endOffset": 135}, {"referenceID": 9, "context": "Partially observable Markov decision processes (POMDPs) extend MDPs to situations where the state is not directly observable [8, 9, 10].", "startOffset": 125, "endOffset": 135}, {"referenceID": 10, "context": "Strategies for finding good projections include value-directed compression [11] and non-negative matrix factorization [12, 13].", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "Strategies for finding good projections include value-directed compression [11] and non-negative matrix factorization [12, 13].", "startOffset": 118, "endOffset": 126}, {"referenceID": 12, "context": "Strategies for finding good projections include value-directed compression [11] and non-negative matrix factorization [12, 13].", "startOffset": 118, "endOffset": 126}, {"referenceID": 13, "context": "The resulting model after compression is a Predictive State Representation (PSR) [14, 15], an Observable Operator Model [16], or a multiplicity automaton [17].", "startOffset": 81, "endOffset": 89}, {"referenceID": 14, "context": "The resulting model after compression is a Predictive State Representation (PSR) [14, 15], an Observable Operator Model [16], or a multiplicity automaton [17].", "startOffset": 81, "endOffset": 89}, {"referenceID": 15, "context": "The resulting model after compression is a Predictive State Representation (PSR) [14, 15], an Observable Operator Model [16], or a multiplicity automaton [17].", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "The resulting model after compression is a Predictive State Representation (PSR) [14, 15], an Observable Operator Model [16], or a multiplicity automaton [17].", "startOffset": 154, "endOffset": 158}, {"referenceID": 17, "context": "Kolter and Ng [18] contend with this problem from a sparse feature selection standpoint.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "looked at the problem of value function estimation from the perspective of both model-free and model-based reinforcement learning [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "demonstrated that these two approaches compute exactly the same value function [19], formalizing a fact that has been recognized to some degree before [2].", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "demonstrated that these two approaches compute exactly the same value function [19], formalizing a fact that has been recognized to some degree before [2].", "startOffset": 151, "endOffset": 154}, {"referenceID": 18, "context": "Instead of learning a linear transition model from features, as in [19], we use subspace identification [20, 21] to learn a PSR from our samples.", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "Instead of learning a linear transition model from features, as in [19], we use subspace identification [20, 21] to learn a PSR from our samples.", "startOffset": 104, "endOffset": 112}, {"referenceID": 20, "context": "Instead of learning a linear transition model from features, as in [19], we use subspace identification [20, 21] to learn a PSR from our samples.", "startOffset": 104, "endOffset": 112}, {"referenceID": 18, "context": "This new approach has a substantial benefit: while the linear feature-to-feature transition model of [19] does not seem to have any common uses outside that paper, PSRs have been proposed numerous times on their own merits (including being invented independently at least three times), and are a strict generalization of POMDPs.", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "Therefore our representation is naturally \u201ccompressed\u201d in the sense of [11], speeding up convergence.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "We start from a discrete time dynamical system with a set of states S, a set of actionsA, a distribution over initial states \u03c00, a state transition function T , a reward function R, and a discount factor \u03b3 \u2208 [0, 1].", "startOffset": 208, "endOffset": 214}, {"referenceID": 2, "context": "However, this solution is biased [3], since the independent variables \u03c6t \u2212 \u03b3\u03c6t+1 are noisy samples of the expected difference E[\u03c6H(h) \u2212 \u03b3 \u2211 o\u2208O \u03c6 H(h\u03c0o) Pr[h\u03c0o | h]].", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "The quantity \u03c6t T can be viewed as an instrumental variable [3], i.", "startOffset": 60, "endOffset": 63}, {"referenceID": 17, "context": "However, with the exception of LARS-TD [18], there has been little work on the problem of how to select features automatically for value function approximation when the system model is unknown; and of course, manual feature selection depends on not-alwaysavailable expert guidance.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "aioi that can be executed and observed after time t [14].", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "oi, given that we intervene [22] to execute the test actions \u03c4 = a1 .", "startOffset": 28, "endOffset": 32}, {"referenceID": 3, "context": "For example, if \u03c41 and \u03c42 are two tests with \u03c4 1 = \u03c4 A 2 \u2261 \u03c4, The LSTD algorithm can also be theoretically justified as the result of an application of the Bellman operator followed by an orthogonal projection back onto the row space of \u03c6H [4].", "startOffset": 240, "endOffset": 243}, {"referenceID": 22, "context": "(With some additional bookkeeping we could remove this assumption [23], but this bookkeeping would unnecessarily complicate our derivations.", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "To find our predictive compression, we will use reduced-rank regression [24].", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "Then we can find a predictive compression of histories by a singular value decomposition (SVD) of the weighted covariance: write UDV \u2248 \u03a3\u0302T ,HL H (7) for a truncated SVD [25] of the weighted covariance, where U are the left singular vectors, V are the right singular vectors, and D is the diagonal matrix of singular values.", "startOffset": 169, "endOffset": 173}, {"referenceID": 10, "context": "2, scaling up future reward by a constant factor results in a value-directed compression\u2014but, unlike previous ways to find value-directed compressions [11], we do not need to know a model of our system ahead of time.", "startOffset": 151, "endOffset": 155}, {"referenceID": 25, "context": "This is equivalent to canonical correlation analysis [26, 27], and the matrix D becomes a diagonal matrix of canonical correlations between futures and histories.", "startOffset": 53, "endOffset": 61}, {"referenceID": 26, "context": "This is equivalent to canonical correlation analysis [26, 27], and the matrix D becomes a diagonal matrix of canonical correlations between futures and histories.", "startOffset": 53, "endOffset": 61}, {"referenceID": 19, "context": "Below we will show an additional benefit: the modelfree algorithm in Equation 13 is, under some circumstances, equivalent to a model-based value function approximation method which uses subspace identification to learn Predictive State Representations [20, 21].", "startOffset": 252, "endOffset": 260}, {"referenceID": 20, "context": "Below we will show an additional benefit: the modelfree algorithm in Equation 13 is, under some circumstances, equivalent to a model-based value function approximation method which uses subspace identification to learn Predictive State Representations [20, 21].", "startOffset": 252, "endOffset": 260}, {"referenceID": 13, "context": "A predictive state representation (PSR) [14] is a compact and complete description of a dynamical system.", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "Finally, a core set Q for a linear PSR is said to be minimal if the tests in Q are linearly independent [16, 15], i.", "startOffset": 104, "endOffset": 112}, {"referenceID": 14, "context": "Finally, a core set Q for a linear PSR is said to be minimal if the tests in Q are linearly independent [16, 15], i.", "startOffset": 104, "endOffset": 112}, {"referenceID": 0, "context": "In addition to the above PSR parameters, we need a few additional definitions for reinforcement learning: a reward function R(h) = \u03b7Q(h) mapping predictive states to immediate rewards, a discount factor \u03b3 \u2208 [0, 1] which weights the importance of future rewards vs.", "startOffset": 207, "endOffset": 213}, {"referenceID": 19, "context": ") Instead of ordinary PSRs, we will work with transformed PSRs (TPSRs) [20, 21].", "startOffset": 71, "endOffset": 79}, {"referenceID": 20, "context": ") Instead of ordinary PSRs, we will work with transformed PSRs (TPSRs) [20, 21].", "startOffset": 71, "endOffset": 79}, {"referenceID": 27, "context": "In this respect, TPSRs are closely related to the transformed representations of LDSs and HMMs found by subspace identification [28, 29, 27, 30].", "startOffset": 128, "endOffset": 144}, {"referenceID": 28, "context": "In this respect, TPSRs are closely related to the transformed representations of LDSs and HMMs found by subspace identification [28, 29, 27, 30].", "startOffset": 128, "endOffset": 144}, {"referenceID": 26, "context": "In this respect, TPSRs are closely related to the transformed representations of LDSs and HMMs found by subspace identification [28, 29, 27, 30].", "startOffset": 128, "endOffset": 144}, {"referenceID": 29, "context": "In this respect, TPSRs are closely related to the transformed representations of LDSs and HMMs found by subspace identification [28, 29, 27, 30].", "startOffset": 128, "endOffset": 144}, {"referenceID": 30, "context": "For a fixed policy \u03c0, a TPSR\u2019s value function is a linear function of state, J(s) = wb, and is the solution of the TPSR Bellman equation [31]: for all b, wb = b\u03b7 b+ \u03b3 \u2211 o\u2208O w B\u03c0ob, or equivalently, w = b\u03b7 + \u03b3 \u2211", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "PSTD learning is related to value-directed compression of POMDPs [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "If we learn a TPSR from data generated by a POMDP, then the TPSR state is exactly a linear compression of the POMDP state [15, 20].", "startOffset": 122, "endOffset": 130}, {"referenceID": 19, "context": "If we learn a TPSR from data generated by a POMDP, then the TPSR state is exactly a linear compression of the POMDP state [15, 20].", "startOffset": 122, "endOffset": 130}, {"referenceID": 10, "context": "in the sense of Poupart and Boutilier [11].", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "In fact, using theory developed in [21], it is possible to relax this assumption and instead assume that state is merely correlated with features of history.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "We can then estimate the TPSR transition matrices as B\u0302ao = \u00db\u03a3\u0302T ,ao,H(\u00db\u03a3\u0302T ,H) (see [21] for proof details).", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "Details on this procedure can be found in [21].", "startOffset": 42, "endOffset": 46}, {"referenceID": 31, "context": "We evaluate the PSTD learning algorithm on a synthetic example derived from [32].", "startOffset": 76, "endOffset": 80}, {"referenceID": 32, "context": "The optimal threshold strategy (sell if price is above a threshold [33]) is in black, LSTD (16 canonical features) is in blue, LSTD (on the full 220 features) is cyan, LARS-TD (feature selection from set of 220) is in green, and PSTD (16 dimensions, compressing 220 features (16 + 204)) is in red.", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": ") We consider the financial derivative introduced by Tsitsiklis and Van Roy [33].", "startOffset": 76, "endOffset": 80}, {"referenceID": 32, "context": "This process is Markov and ergodic [33, 34]: xt and xt+100 are independent and identically distributed.", "startOffset": 35, "endOffset": 43}, {"referenceID": 32, "context": "For feature selection, we are fortunate: previous researchers have hand-selected a \u201cgood\u201d set of 16 features for this data set through repeated trial and error (see Appendix, Section B and [33, 34]).", "startOffset": 189, "endOffset": 197}, {"referenceID": 32, "context": "We compared PSTD (reducing 220 to 16 features) to LSTD with either the 16 hand-selected features or the full 220 features, as well as to LARS-TD (220 features) and to a simple thresholding strategy [33].", "startOffset": 198, "endOffset": 202}, {"referenceID": 32, "context": "In fact, PSTD performs better than the best previously reported approach [33, 34] by 1.", "startOffset": 73, "endOffset": 81}, {"referenceID": 32, "context": "For this reason, in real-world problems, a substantial amount of time is spent selecting a small set of features, often by trial and error [33, 34].", "startOffset": 139, "endOffset": 147}], "year": 2011, "abstractText": "We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.", "creator": "LaTeX with hyperref package"}}}