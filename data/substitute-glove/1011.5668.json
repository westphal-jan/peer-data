{"id": "1011.5668", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2010", "title": "On Theorem 2.3 in \"Prediction, Learning, and Games\" by Cesa-Bianchi and Lugosi", "abstract": "The letter follows turned integrated contained such good unexpected cargo that its substantially weighted 90 nhc same could - varying determined. The reiterate thus of the algorithm because southern - bounded by sqrt {n ln (N) } (uniformly since n ), elsewhere N is saw were which according own n if within number is steps.", "histories": [["v1", "Thu, 25 Nov 2010 18:52:30 GMT  (2kb)", "http://arxiv.org/abs/1011.5668v1", "3 pages; excerpt fromarXiv:1005.1918, simplified and rewritten using the notation of the monograph by Cesa-Bianchi and Lugosi"]], "COMMENTS": "3 pages; excerpt fromarXiv:1005.1918, simplified and rewritten using the notation of the monograph by Cesa-Bianchi and Lugosi", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alexey chernov"], "accepted": false, "id": "1011.5668"}, "pdf": {"name": "1011.5668.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Alexey Chernov"], "emails": ["chernov@cs.rhul.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n01 1.\n56 68\nv1 [\ncs .L\nG ]\n2 5\nN ov\n2 01\n0\nOn Theorem 2.3 in \u201cPrediction, Learning, and\nGames\u201d by Cesa-Bianchi and Lugosi.\nAlexey Chernov\u2217\nComputer Learning Research Centre and Dept Computer Science\nRoyal Holloway, University of London, Egham, Surrey TW20 0EX, UK\nchernov@cs.rhul.ac.uk\nThis note proves a loss bound for the exponentially weighted average forecaster with time-varying potential, see [1, \u00a7 2.3] for context and definitions. The present proof gives a better constant in the regret term than Theorem 2.3 in [1]. This proof first appeared in [2] (Theorem 2), where a more general algorithm is considered. Here the proof is rewritten using the notation of [1].\nTheorem 1. Assume that the loss function \u2113 is convex in the first argument and \u2113(p, y) \u2208 [0, 1] for all p \u2208 D and y \u2208 Y. For any positive reals \u03b71 \u2265 \u03b72 \u2265 . . ., for any n \u2265 1 and for any y1, . . . , yn \u2208 Y, the regret of the exponentially weighted average forecaster with time-varying learning rate \u03b7t satisfies\nL\u0302n \u2212 min i=1,...,N\nLi,n \u2264 lnN\n\u03b7n +\n1\n8\nn\u2211\nt=1\n\u03b7t . (1)\nIn particular, for \u03b7t = \u221a 4 lnN t , t = 1, . . . , n, we have\nL\u0302n \u2212 min i=1,...,N\nLi,n \u2264 \u221a n lnN .\nProof. The forecaster at step t predicts p\u0302t = \u2211N\ni=1 wi,t\u22121 Wt\u22121 fi,t, where wi,t\u22121 =\ne\u2212\u03b7tLi,t\u22121 and Wt\u22121 = \u2211N j=1wj,t\u22121. Due to convexity of \u2113 we have\n\u2113(p\u0302t, yt) \u2264 N\u2211\ni=1\nwi,t\u22121 Wt\u22121 \u2113(fi,t, yt) .\n\u2217Supported by EPSRC grant EP/F002998/1.\nUsing the Hoeffding inequality ([1, Lemma A.1]), we get\ne \u2212\u03b7t\n\u2211N i=1 wi,t\u22121\nWt\u22121 \u2113(fi,t,yt) \u2265\nN\u2211\ni=1\nwi,t\u22121 Wt\u22121 e\u2212\u03b7t\u2113(fi,t,yt)\u2212\u03b7 2 t /8\nand thus\ne\u2212\u03b7t\u2113(p\u0302t,yt) \u2265 N\u2211\ni=1\nwi,t\u22121 Wt\u22121 e\u2212\u03b7t\u2113(fi,t,yt)\u2212\u03b7 2 t /8 . (2)\nConsider the values\nsi,t\u22121 = e \u2212\u03b7t\u22121Li,t\u22121+\u03b7t\u22121L\u0302t\u22121\u2212\n1 8 \u03b7t\u22121 \u2211t\u22121 k=1 \u03b7k\nand note that\nwi,t\u22121 Wt\u22121\n= 1 N (si,t\u22121)\n\u03b7t \u03b7t\u22121\n\u2211N j=1 1 N (sj,t\u22121) \u03b7t \u03b7t\u22121 . (3)\nLet us show that \u2211N\nj=1 1 N sj,t \u2264 1 by induction over t. For t = 0 this is\ntrivial, since sj,0 = 1 for all j. Assume that \u2211N j=1 1 N sj,t\u22121 \u2264 1. Then\nN\u2211\nj=1\n1\nN (sj,t\u22121)\n\u03b7t \u03b7t\u22121 \u2264\n( N\u2211\nj=1\n1\nN sj,t\u22121\n) \u03b7t \u03b7t\u22121\n\u2264 1 , (4)\nsince the function x 7\u2192 x\u03b1 is concave and monotone for x \u2265 0 and \u03b1 \u2208 [0, 1] and since \u03b7t\u22121 \u2265 \u03b7t > 0. Using (4) to bound the right-hand side of (3), we get\nwi,t\u22121 Wt\u22121 \u2265 1 N (si,t\u22121)\n\u03b7t \u03b7t\u22121 ; and combining with (2), we get\ne\u2212\u03b7t\u2113(p\u0302t,yt) \u2265 N\u2211\ni=1\n1\nN (si,t\u22121)\n\u03b7t \u03b7t\u22121 e\u2212\u03b7t\u2113(fi,t,yt)\u2212\u03b7 2 t /8 .\nIt remains to note that\nsi,t = (si,t\u22121) \u03b7t \u03b7t\u22121 e\u2212\u03b7t\u2113(fi,t,yt)+\u03b7t\u2113(p\u0302t,yt)\u2212\u03b7 2 t /8\nand we get \u2211N\ni=1 1 N si,t \u2264 1.\nFor any i, we have 1 N si,n \u2264 \u2211N j=1 1 N sj,n \u2264 1, thus\n\u2212\u03b7nLi,n + \u03b7nL\u0302n \u2212 1\n8 \u03b7n\nn\u2211\nk=1\n\u03b7k \u2264 lnN ,\nand (1) follows.\nTheorem 1 recommends the learning rate \u03b7t = \u221a (4 lnN)/t instead of\u221a\n(8 lnN)/t used in Theorem 2.3 in [1] and achieves the regret term \u221a n lnN\ninstead of \u221a 2n lnN + \u221a 0.125 lnN .\nTo compare the bounds for arbitrary learning rates, let us observe that the proof of Theorem 2.3 in [1] actually implies (under the assumptions of Theorem 1):\nL\u0302n \u2212 min i=1,...,N\nLi,n \u2264 ( 2\n\u03b7n \u2212 1 \u03b71\n) lnN + 1\n8\nn\u2211\nt=1\n\u03b7t .\nThe right-hand side of this inequality is larger than the right-hand side of (1) if \u03b7n 6= \u03b71. If \u03b7t are equal for all t, the bounds coincide and give the bound of Theorem 2.2 in [1].\nReferences\n[1] N. Cesa-Bianchi, G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, Cambridge, England, 2006.\n[2] A. Chernov, F. Zhdanov. Prediction with expert advice under discounted loss. Proc. of ALT 2010, LNCS 6331, pp. 255-269. See also: arXiv:1005.1918v1 [cs.LG]."}], "references": [{"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Prediction with expert advice under discounted loss", "author": ["A. Chernov", "F. Zhdanov"], "venue": "Proc. of ALT 2010,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1918}], "referenceMentions": [{"referenceID": 0, "context": "3 in [1].", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "This proof first appeared in [2] (Theorem 2), where a more general algorithm is considered.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "Here the proof is rewritten using the notation of [1].", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "Assume that the loss function l is convex in the first argument and l(p, y) \u2208 [0, 1] for all p \u2208 D and y \u2208 Y.", "startOffset": 78, "endOffset": 84}, {"referenceID": 0, "context": "since the function x 7\u2192 x is concave and monotone for x \u2265 0 and \u03b1 \u2208 [0, 1] and since \u03b7t\u22121 \u2265 \u03b7t > 0.", "startOffset": 68, "endOffset": 74}, {"referenceID": 0, "context": "3 in [1] and achieves the regret term \u221a n lnN instead of \u221a 2n lnN + \u221a 0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "3 in [1] actually implies (under the assumptions of Theorem 1): L\u0302n \u2212 min i=1,.", "startOffset": 5, "endOffset": 8}], "year": 2010, "abstractText": null, "creator": "LaTeX with hyperref package"}}}