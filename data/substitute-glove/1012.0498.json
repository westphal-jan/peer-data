{"id": "1012.0498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2010", "title": "Estimating Probabilities in Recommendation Systems", "abstract": "Recommendation utilize are emerging as an important focused application when resulting economic risk. Currently popular telecommunication separate Amazon ' s biographical submitted, Netflix ' s best recommendations, some Pandora ' ld guitar carefully. In possible volumes else address on but of overstatement calculating both with pending forms analyzed techniques non - dynamics server smoothing. In keeping estimation done evaluate missing items having assigning authenticated observations others obtained enabling computation undertaken using minimization listed of generating formula_1. We acknowledge ability change full several hearing studies implicated certainly world animated memo applications. The results can reduced with state - of - from - art methodologies instead also receiving approximation satisfy estimates outside the subject instance create recommender systems.", "histories": [["v1", "Thu, 2 Dec 2010 17:04:19 GMT  (653kb)", "http://arxiv.org/abs/1012.0498v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mingxuan sun", "guy lebanon", "paul kidwell"], "accepted": false, "id": "1012.0498"}, "pdf": {"name": "1012.0498.pdf", "metadata": {"source": "CRF", "title": "Estimating Probabilities in Recommendation Systems", "authors": ["Mingxuan Sun", "Guy Lebanon", "Paul Kidwell"], "emails": ["msun3@gatech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n01 2.\n04 98\nv1 [\ncs .L\nG ]\n2 D\nec 2"}, {"heading": "1 Introduction", "text": "Recommendation systems are emerging as an important business application with significant economic impact. The data in such systems are collections of incomplete tied preferences across n items associated with m different users. Given an incomplete tied preference associated with an additional m+1 user, the system recommends unobserved items to that user based on the preference relations of the m+ 1 users. Currently deployed recommendation systems include book recommendations at amazon.com, movie recommendations at netflix.com, and music recommendations at pandora.com. Constructing accurate recommendation systems (that recommend to users items that are truly preferred over other items) is important for assisting users as well as increasing business profitability. It is an important unsolved goal in machine learning and data mining.\nIn most cases of practical interest the number of items n indexed by the system (items may be books, movies, songs, etc.) is relatively high in the 103 \u2212 104 range. Perhaps due the size of n, it is almost always the case that each user observes only a small subset of the items, typically in the range 10-100. As a result the preference relations expressed by the users are over a small subset of the n items.\nFormally, we have m users providing incomplete tied preference relations on n items\nS1 : A1,1 \u227aA1,2 \u227a \u00b7 \u00b7 \u00b7 \u227a A1,k(1)\nS2 : A2,1 \u227aA2,2 \u227a \u00b7 \u00b7 \u00b7 \u227a A2,k(2)\n... (1)\nSm : Am,1 \u227aAm,2 \u227a \u00b7 \u00b7 \u00b7 \u227a Am,k(m)\nwhere Ai,j \u2282 {1, . . . , n} are sets of items (wlog we identify items with integers 1, . . . , n) defined by the following interpretation: user i prefers all items in Ai,j to all items in Ai,j+1. The notation k(i) above is\n\u2217Corresponding author: msun3@gatech.edu\nthe number of such sets provided by user i. The data (1) is incomplete since not all items are necessarily observed by each user i.e., \u22c3k(i)\nj=1 Ai,j ( {1, . . . , n} and may contain ties since some items are left uncompared, i.e., |Ai,j | > 1. Recommendation systems recommend items to a new user, denoted as m+1, based on their preference\nSm+1 : Am+1,1 \u227aAm+1,2 \u227a \u00b7 \u00b7 \u00b7 \u227a Am+1,k(m+1) (2)\nand its relation to the preferences of the m users (1). As an illustrative example, assuming n = 9,m = 3, the data\nS1 : 1, 8, 9 \u227a 4 \u227a 2, 3, 7\nS2 : 4 \u227a 2, 3 \u227a 8\nS3 : 4, 8 \u227a 2, 6, 9\ncorresponds to A1,1 = {1, 8, 9}, A1,2 = {4}, A1,3 = {2, 3, 7}, A2,1 = {4}, A2,2 = {2, 3}, A2,3 = {8}, A3,1 = {4, 8}, A3,2 = {2, 6, 9}, and k(1) = k(2) = 3, k(3) = 2. From the data we may guess that item 4 is relatively popular across the board while some users like item 8 (users 1, 3) and some hate it (user 2). Given a new m+1 user issuing the preference 1 \u227a 2, 3, 7 we might observe a similar pattern of preference or taste as user 1 and recommend to the user item 8. We may also recommend item 4 which has broad appeal resulting in the augmentation\n1 \u227a 2, 3, 7 7\u2192 1, 4, 8 \u227a 2, 3, 7.\nWe note that in some cases the preference relations (1) arise from users providing numeric scores to items. For example, if the users assign 1-5 stars to movies, the set Ai,j contains all movies that user i assigned 6\u2212 j stars to and k(i) = 5 (assuming some movies were assigned to each of the 1, 2, 3, 4, 5 star levels). As pointed out by a wide variety of studies in economics and social sciences, such numeric scores are inconsistent among different users. We therefore proceed to interpret such data as ordinal rather than numeric.\nA substantial body of literature in computer science has addressed the problem of constructing recommendation systems. We have attempted to outline the most important and successful approaches in the related work section towards the end of this paper. However, none of these previous approaches are fully satisfactory from a statistical perspective: there are no reasonable probability models assumed to generate the data and no clear meaningful statistical estimation procedures. We substantiate this argument more fully in the related work section.\nIn this paper we describe a non-parametric statistical technique for estimating probabilities on preferences based on the data (1). This technique may be used in recommendation systems in different ways. Its principal usage may be to provide a statistically meaningful estimation framework for issuing recommendations (in conjunction with decision theory). However, it also leads to other important applications including mining association rules, exploratory data analysis, and clustering items and users. Two key observations that we make are: (i) incomplete tied preference data may be interpreted as randomly censored permutation data, and (ii) using generating functions we are able to provide a computationally efficient scheme for computing the estimator in the case of triangular smoothing.\nWe proceed in the next sections to describe notations and our assumptions and estimation procedure, and follow with case studies demonstrating our approach on real world recommendation systems data."}, {"heading": "2 Definitions and Estimation Framework", "text": "We describe the following notations and conventions for permutations, which are taken from [3] where more detail may be found. We denote a permutation by listing the items from most preferred to least separated by a \u227a or | symbol: \u03c0\u22121(1) \u227a \u03c0\u22121(2) \u227a \u00b7 \u00b7 \u00b7 \u227a \u03c0\u22121(n), e.g. \u03c0(1) = 2, \u03c0(2) = 3, \u03c0(3) = 1 is 3 \u227a 1 \u227a 2. Ranking with ties occur when judges do not provide enough information to construct a total order. In particular, we define tied rankings as a partition of {1, . . . , n} to k < n disjoint subsets A1, . . . , Ak \u2282 {1, . . . , n} such that all items in Ai are preferred to all items in Ai+1 but no information is provided concerning the relative\npreference of the items among the sets Ai. We denote such rankings by separating the items in Ai and Ai+1 with a \u227a or | notation. For example, the tied ranking A1 = {3}, A2 = {2}, A3 = {1, 4} (items 1 and 4 are tied for last place) is denoted as 3 \u227a 2 \u227a 1, 4 or 3|2|1, 4.\nRanking with missing items occur when judges omit certain items from their preference information altogether. For example assuming a set of items {1, . . . , 4}, a judge may report a preference 3 \u227a 2 \u227a 4, omitting altogether item 1 which the judge did not observe or experience. This case is very common in situations involving a large number of items n. In this case judges typically provide preference only for the l \u226a n items that they observed or experienced. For example, in movie recommendation systems we may have n \u223c 103 and l \u223c 101.\nRankings can be full (permutations), with ties, with missing items, or with both ties and missing items. In either case we denote the rankings using the \u227a or | notation or using the disjoint sets A1, . . . , Ak notation. We also represent tied and incomplete rankings by the set of permutations that are consistent with it. For example,\n3 \u227a 2 \u227a 1, 4 = {3 \u227a 2 \u227a 1 \u227a 4} \u222a {3 \u227a 2 \u227a 4 \u227a 1}\n3 \u227a 2 \u227a 4 = {1 \u227a 3 \u227a 2 \u227a 4} \u222a {3 \u227a 1 \u227a 2 \u227a 4} \u222a {3 \u227a 2 \u227a 1 \u227a 4} \u222a {3 \u227a 2 \u227a 4 \u227a 1}\nare sets of two and four permutations corresponding to tied and incomplete rankings, respectively. It is hard to directly posit a coherent probabilistic model on incomplete tied data such as (1). Different preferences relations are not unrelated to each other: they may subsume one another (for example 1 \u227a 2 \u227a 3 and 1 \u227a 3), represent disjoint events (for example 1 \u227a 3 and 3 \u227a 1), or interact in more complex ways (for example 1 \u227a 2 \u227a 3 and 1 \u227a 4 \u227a 3). A valid probabilistic framework needs to respect the constraints resulting from the axioms of probability, e.g., p(1 \u227a 2 \u227a 3) \u2264 p(1 \u227a 3).\nOur approach is to consider the incomplete tied preferences as censored permutations. That is, we assume a distribution p(\u03c0) over permutations \u03c0 \u2208 Sn (Sn is the symmetric group of permutations of order n) that describes the complete without-ties preferences in the population. The data available to the recommender system (1) is sampled by drawing m iid permutations from p: \u03c01, . . . , \u03c0m iid \u223c p, followed by censoring to result in the observed preferences S1, . . . , Sm\n\u03c0i \u223c p(\u03c0), Si \u223c p(S|\u03c0i), i = 1, . . . ,m+ 1 (3)\np(\u03c0|S) = I(\u03c0 \u2208 S)p(\u03c0) \u2211\n\u03c3\u2208S p(\u03c3) (4)\np(S|\u03c0) = p(\u03c0|S)p(S)/p(\u03c0) = I(\u03c0 \u2208 S)p(\u03c0)p(S)\np(\u03c0) \u2211 \u03c3\u2208S p(\u03c3) =\nI(\u03c0 \u2208 S)p(S) \u2211\n\u03c3\u2208S p(\u03c3) (5)\nwhere p(S) is the probability of observing the censoring S (specifically, it is not equal to \u2211\n\u03c3\u2208S p(\u03c3)). Although many approaches for estimating p given S1, . . . , Sm are possible, experimental evidence point to the fact that in recommendation systems with high n, the distribution p does not follow a simple parametric form such as the Mallows, Bradley-Terry, or Thurstone models [12] (see Figure 1 for a demonstration how parametric assumptions break down with increasing n). Instead, the distribution p tends to be diffuse and multimodal with different probability mass regions corresponding to different types of judges (for example in movie preferences probability modes may correspond to genre as fans of drama, action, comedy, etc. having similar preferences).\nWe therefore propose to estimate the underlying distribution p on permutations using non-parametric kernel smoothing. The standard kernel smoothing formula applies to the permutation setting as\np\u0302(\u03c0) = 1\nm\nm \u2211\ni=1\nKh(T (\u03c0, \u03c0i))\nwhere \u03c01, . . . , \u03c0m iid \u223c p, T a distance on permutations such as Kendall\u2019s distance and Kh(r) = h \u22121K(r/h) a normalized unimodal function. In the case at hand, however, the observed preferences \u03c0i as well as \u03c0 are\nreplaced with permutations sets S1, . . . , Sm, R representing incomplete tied preferences\np\u0302(R) = \u2211\n\u03c0\u2208R\np\u0302(\u03c0) = 1\nm\nm \u2211\ni=1\n\u2211\n\u03c0\u2208R\n\u2211\n\u03c3\u2208Si\nq(\u03c3|Si)Kh(T (\u03c0, \u03c3)) (6)\nwhere q(\u03c3|Si) serves as a surrogate for the unknown p(\u03c3|Si) \u221d I(\u03c3 \u2208 Si)p(\u03c3) (see (4)). Selecting q(\u03c3|Si) = p(\u03c3|Si) would lead to consistent estimation of p(R) in the limit h \u2192 0, m \u2192 \u221e assuming positive p(\u03c0), p(S). Such a selection, however, is generally impossible since p(\u03c0) and therefore p(\u03c3|Si) are unknown.\nIn general the specific choice of the surrogate q(\u03c3|S) is important as it may influence the estimated probabilities. Furthermore, it may cause underestimation or overestimation of p\u0302(R) in the limit of large data. An exception occurs when the sets S1, . . . , Sm are either subsets of R or disjoint from R. In this case limh\u21920 Kh(\u03c0, \u03c3) = I(\u03c0 = \u03c3) resulting in the following limit (with probability 1 by the strong law of large numbers)\nlim m\u2192\u221e lim h\u21920 p\u0302(R) = lim m\u2192\u221e\n1\nm\nm \u2211\ni=1\nI(Si \u2282 R) \u2211\n\u03c3\u2208Si\nq(\u03c3|Si) (7)\n= lim m\u2192\u221e\n1\nm\nm \u2211\ni=1\nI(Si \u2282 R) = lim m\u2192\u221e\n1\nm\nm \u2211\ni=1\nI(\u03c0i \u2208 R) = p(R). (8)\nThus, if we our data is comprised of preferences Si that are either disjoint or a subset of R we have consistency regardless of the choice of the surrogate q. Such a situation is more realistic when the preference R involves a small number of items and the preferences Si, i = 1, . . . ,m involve a larger number of items. This is often the case for recommendation systems where individuals report preferences over 10-100 items and we are mostly interested in estimating probabilities of preferences over fewer items such as i \u227a j, k or i \u227a j, k \u227a l (see experiment section).\nThe main difficulty with the estimator above is the computation of \u2211\n\u03c0\u2208R\n\u2211\n\u03c3\u2208Si q(\u03c3|Si)Kh(T (\u03c0, \u03c3)).\nIn the case of high n and only a few observed items k the sets Si, R grow factorially as (n \u2212 k)! making\na naive computation of (6) intractable for all but the smallest n. In the next section we explore efficient computations of these sums for a triangular kernel Kh and a uniform q(\u03c0|S)."}, {"heading": "3 Computationally Efficient Kernel Smoothing", "text": "In previous work [10] the estimator (6) is proposed for tied (but complete) rankings. That work derives closed form expressions and efficient computation for (6) assuming a Mallows kernel [11]\nKh(T (\u03c0, \u03c3)) = exp\n(\n\u2212 T (\u03c0, \u03c3)\nh\n) n \u220f\nj=1\n1\u2212 e\u22121/h 1\u2212 e\u2212j/h (9)\nwhere T is Kendall\u2019s Tau distance on permutations (below I(x) = 1 for x > 0 and 0 otherwise)\nT (\u03c0, \u03c3) = n\u22121 \u2211\ni=1\n\u2211\nl>i\nI(\u03c0\u03c3\u22121(i)\u2212 \u03c0\u03c3\u22121(l)). (10)\nUnfortunately these simplifications do not carry over to the case of incomplete rankings where the sets of consistent permutations S1, . . . , Sm are not cosets of the symmetric group. As a result the problem of probability estimation in recommendation systems where n is high and many items are missing is particularly challenging. However, as we show below replacing the Mallows kernel (9) with a triangular kernel leads to efficient computation in some cases. Specifically, the triangular kernel on permutation is\nKh(T (\u03c0, \u03c3)) = (1 \u2212 h \u22121T (\u03c0, \u03c3)) I(h\u2212 T (\u03c0, \u03c3)) /C (11)\nwhere the bandwidth parameter h represent both the support (the kernel is 0 for all larger distances) and the inverse slope of the triangle. As we show below the normalization term C is a function of h and may be efficiently computed using generating functions. Figure 2 (right panel) displays the linear decay of (11) for the simple case of permutations over n = 3 items.\nCombinatorial Generating Function\nGenerating functions, a tool from enumerative combinatorics, allow efficient computation of (6) by concisely expressing the distribution of distances between permutations. Kendall\u2019s tau T (\u03c0, \u03c3) is the total number of discordant pairs or inversions between \u03c0, \u03c3 [20] and thus its computation becomes a combinatorial counting problem. We associate the following generating function with the symmetric group of order n permutations\nGn(z) = n\u22121 \u220f\nj=1\nj \u2211\nk=0\nzk. (12)\nAs shown for example in [20] the coefficient of zk of Gn(z), which we denote as [z k]Gn(z), corresponds to the number of permutations \u03c3 for which T (\u03c3, \u03c0\u2032) = k. For example, the distribution of Kendall\u2019s tau T (\u00b7, \u03c0\u2032) over all permutations of 3 items is described by G3(z) = (1 + z)(1 + z + z\n2) = 1z0 + 2z1 + 2z2 + 1z3 i.e., there is one permutation \u03c3 with T (\u03c3, \u03c0\u2032) = 0, two permutations \u03c3 with T (\u03c3, \u03c0\u2032) = 1, two with T (\u03c3, \u03c0\u2032) = 2 and one with T (\u03c3, \u03c0\u2032) = 3. Another important generating function is\nHn(z) = Gn(z)\n1\u2212 z = (1 + z + z2 + z3 + \u00b7 \u00b7 \u00b7 )Gn(z)\nwhere [zk]Hn(z) represents the number of permutations \u03c3 for which T (\u03c3, \u03c0 \u2032) \u2264 k.\nProposition 1. The normalization term C(h) is given by C(h) = [zh]Hn(z)\u2212 h \u22121[zh\u22121] G\u2032n(z) 1\u2212z .\nProof. The proof factors the non-normalized triangular kernelCKh(\u03c0, \u03c3) to I(h\u2212T (\u03c0, \u03c3)) and h \u22121T (\u03c0, \u03c3)I(h\u2212 T (\u03c0, \u03c3)) and making the following observations. First we note that summing the first factor over all permutations may be counted by [zh]Hn(z). The second observation is that [z\nk\u22121]G\u2032n(z) is the number of permutations \u03c3 for which T (\u03c3, \u03c0\u2032) = k, multiplied by k. Since we want to sum over that quantity for all permutations whose distance is less than h we extract the h \u2212 1 coefficient of the generating function G\u2032n(z) \u2211 k\u22650 z k = G\u2032n(z)/(1\u2212 z). We thus have\nC = \u2211\n\u03c3:T (\u03c0\u2032,\u03c3)\u2264h\n1\u2212 h\u22121 \u2211\n\u03c3:T (\u03c0\u2032,\u03c3)\u2264h\nT (\u03c0\u2032, \u03c3) = [zh]Hn(z)\u2212 h \u22121[zh\u22121]\nG\u2032n(z) 1\u2212 z .\nProposition 2. The complexity of computing C(h) is O(n4).\nProof. We describe a dynamic programming algorithm to compute the coefficients of Gn by recursively computing the coefficients of Gk from the coefficients of Gk\u22121, k = 1, . . . , n. The generating function Gk(z) has k(k+1)/2 non-zero coefficients and computing each of them (using the coefficients of Gk\u22121) takes O(k). We thus have O(k3) to compute Gk from Gk\u22121 which implies O(n\n4) to compute Gk, n = 1, . . . , n. We conclude the proof by noting that once the coefficients of Gn are computed the coefficients of Hn(z) and Gn(z)/(1 \u2212 z) are computable in O(n\n2) as these are simply cumulative weighted sums of the coefficients of Gn.\nNote that computing C(h) for one or many h values may be done offline prior to the arrival of the rankings and the need to compute the estimated probabilities.\nDenoting by k the number of items ranked in either S or R or both, the computation of p\u0302(\u03c0) in (6) requires O(k2) online and O(n4) offline complexity if either non-zero smoothing is performed over the entire data i.e., max\u03c0\u2208R max n i=1 max\u03c3\u2208Si T (\u03c3, \u03c0) < h or alternatively, we use the modified triangular kernel K \u2217 h(\u03c0, \u03c3) \u221d (1\u2212 h\u22121)T (\u03c0, \u03c3) which is allowed to take negative values for the most distant permutations (normalization still applies though).\nProposition 3. For two sets of permutations S,R corresponding to tied-incomplete rankings\n1\n|S||R|\n\u2211\n\u03c0\u2208S\n\u2211\n\u03c3\u2208R\nT (\u03c0, \u03c3) = n(n\u2212 1)\n4 \u2212\n1\n2\nn\u22121 \u2211\ni=1\nn \u2211\nj=i+1\n(1 \u2212 2pij(S))(1 \u2212 2pij(R)) (13)\npij(U) =\n\n    \n    \nI(\u03c4U (j)\u2212 \u03c4U (i)) i and j are ranked in U with \u03c4U (i) 6= \u03c4U (j) 1\u2212 \u03c4U (i)+ \u03c6U (i)\u22121 2\nk+1 only i is ranked in U \u03c4U (j)+ \u03c6U (j)\u22121\n2\nk+1 only j is ranked in U\n1/2 otherwise\n.\nwith \u03c4U (i) = min\u03c0\u2208U \u03c0(i), and \u03c6U (i) being the number of items that are tied to i in U .\nProof. We note that (13) is an expectation with respect to the uniform measure. We thus start by computing the probability pij(U) that i is preferred to j for U = S and U = R under the uniform measure. Five scenarios exist for each of pij(U) corresponding to whether each of i and j are ranked by S,R. Starting with the case that i is not ranked and j is ranked, we note that i is equally likely to be preferred to any item or to be preferred to. Given the uniform distribution over compatible rankings item j is equally likely to appear in positions \u03c4U (j), . . . , \u03c4U (j) + \u03c6U (j)\u2212 1. Thus\npij = 1\n\u03c6U (j)\n\u03c4U (j) k + 1 + \u00b7 \u00b7 \u00b7+ 1\n\u03c6U (j)\n\u03c4U (j) + \u03c6U (j)\u2212 1\nk + 1 =\n\u03c4U (j) + \u03c6U (j)\u22121\n2\nk + 1 (14)\nSimilarly, if j is unknown and i is known then pij + pji = 1. If both i and j are unknown either ordering must be equally likely given the uniform distribution making pij = 1/2. Finally, if both i and j are known pij = 1, 1/2, 0 depending on their preference. Given pij , linearity of expectation, and the independence between rankings, the change in the expected number of inversions relative to the uniform expectation n(n\u2212 1)/4 can be found by considering each pair separately,\nET (i, j) = 1\n2 P (i and j disagree)\u2212\n1 2 P (i and j agree)\n= 1\n2 (pij(\u03c3)(1 \u2212 pij(\u03c0)) + (1\u2212 pij(\u03c3))pij(\u03c0)) \u2212 pij(\u03c3)pij(\u03c0)\u2212 (1\u2212 pij(\u03c3))(1 \u2212 pij(\u03c0)))\n= \u22121\n2 (1\u2212 2pij(\u03c3)) (1\u2212 2pij(\u03c0)) .\nSumming the n(n\u2212 1)/2 components yields the desired quantity.\nCorollary 1. Denoting the number of items ranked by either S or R or both as k, and assuming either h > max\u03c0\u2208R max n i=1 max\u03c3\u2208Si T (\u03c3, \u03c0) or that the modified triangular kernel K \u2217 h(\u03c0, \u03c3) \u221d (1 \u2212 h\n\u22121)T (\u03c0, \u03c3) is used, the complexity of computing p\u0302(R) in (6) (assuming uniform q(\u03c0|Si)) is O(k 2) online and O(n4) offline.\nProof. The proof follows from noting that (6) reduces to O(n4) offline computation of the normalization term and O(k2) online computation of the form (13)."}, {"heading": "4 Applications and Case Studies", "text": "We divide our experimental study to three parts. In the first we examine the task of predicting probabilities. The remaining two parts use these probabilities for rank prediction and rule discovery.\nIn our experiments we used three datasets. The Movielens dataset1 contains one million ratings from 6040 users over 3952 movies. The EachMovie dataset2 contains 2.6 million ratings from 74424 users over 1648 movies. The Netflix dataset 3 contains 100 million movie ratings from 480189 users on 17770. In all of these datasets users typically rated only a small number of items. Histograms of the distribution of the number of votes per user, number of votes per item, and vote distribution appear in Figure 3."}, {"heading": "4.1 Estimating Probabilities", "text": "We consider here the task of estimating p\u0302(R) where R is a set of permutations corresponding to a tied incomplete ranking. Such estimates may be used to compute conditional estimates P\u0302 (R|Sm+1) which are used to predict which augmentations R of Sm+1 are highly probable. For example, given an observed preference 3 \u227a 2 \u227a 5 we may want to compute p\u0302(8 \u227a 3 \u227a 2 \u227a 5|3 \u227a 2 \u227a 5) = p\u0302(8 \u227a 3 \u227a 2 \u227a 5)/p\u0302(3 \u227a 2 \u227a 5) to see whether item 8 should be recommended to the user.\n1http://www.grouplens.org 2 http://www.grouplens.org/node/76 3http://www.netflixprize.com/community\nMovielens Netflix EachMovie\nFor simplicity we focus in this section on probabilities of simple events such as i \u227a j or i \u227a j \u227a k. The next section deals with more complex events. In our experiment, we estimate the probability of i \u227a j for the n = 53 most rated movies in Netflix and m = 10000 users who rate most of these movies. The probability matrix of the pairs is shown in Figure 4 where each cell corresponds to the probability of preference between a pair of movies determined by row j and column i. In the top left panel the rows and columns are ordered by average probability of a movie being preferred to others r(i) = \u2211 j p\u0302(i\u227aj)\nn with the most preferred movie in row and column 1 (top right panel indicates the ordering according to r(i)). In the bottom left panel the movies were ordered first by popularity of genres and then by r(i). The bottom right panel indicates that ordering. The names, genres, and both orderings of all 53 movies appear in Figure 6.\nThe three highest movies in terms of r(i) are Lord of the Rings: The Return of the King, Finding Nemo, and Lord of the Rings: The Two Towers. The three lowest movies are Maid in Manhattan, Anger Management, and The Royal Tenenbaums. Examining the genre (colors in right panels of Figure 4) we see that family and science fiction are generally preferred to others movies while comedy and romance generally receive lower preferences. The drama, action genres are somewhere in the middle.\nAlso interesting is the variance of the movie preferences within specific genres. Family movies are generally preferred to almost all other movies. Science fiction movies, on the other hand, enjoy high preference overall but exhibit a larger amount of variability as a few movies are among the least preferred. Similarly, the preference probabilities of action movies are widely spread with some movies being preferred to others and others being less preferred. More specifically (see bottom left panel of Figure 4) we see that the decay of r(i) within genres is linear for family and romance and nonlinear for science fiction, action, drama, and comedy. In these last three genres there are a few really \u201cbad\u201d movies that are substantially lower than the rest of the curve. Figure 6 shows the full information including titles, genres and orderings of the 53 most popular movies in Netflix.\nWe plot the individual values of p\u0302(i \u227a j) for three movies: Shrek (family), Catch Me If You Can (drama) and Napoleon Dynamite (comedy) (Figure 5). Comparing the three stem plots we observe that Shrek is preferred to almost all other movies, Napoleon Dynamite is less preferred than most other movies, and Catch Me If You Can is preferred to some other movies but less preferred than others. Also interesting is the linear increase of the stem plots for Catch Me If You Can and Napoleon Dynamite and the non-linear increase of the stem plot for Shrek. This is likely a result of the fact that for very popular movies there are only a few comparable movies with the rest being very likely to be less preferred movies (p\u0302(i \u227a j) close to 1).\nIn a second experiment (see Figure 7) we compare the predictive behavior of the kernel smoothing estimator with that of a parametric model (Mallows model) and the empirical measure (frequency of event occurring in the m samples). We evaluate the predictive performance of a probability estimator by separating the data to two parts: a training set that is used to construct the estimator and a testing set used for evaluation via its loglikelihood. A higher test set loglikelihood indicates that the model assigns high probability to events that occurred. Mathematically, this corresponds to approximating the KL divergence between nature and the model. Since the Mallows model is intractable for large n we chose in this experiment small values of n: 3, 4, 5.\nWe observe that the kernel estimator consistently achieves higher test set loglikelihood than the Mallows model and the empirical measure. The former is due to the breakdown of parametric assumptions as indicated by Figure 1 (note that this happens even for n as low as 3). The latter is due to the superior statistical performance of the kernel estimator over the empirical measure."}, {"heading": "4.2 Rank Prediction", "text": "Our task here is to predict ranking of new unseen items for users. We follow the standard procedure in collaborative filtering: the set of users is partitioned to two sets, a training set and a testing set. For each of the test set users we further split the observed items into two sets: one set used for estimating preferences (together with the preferences of the training set users) and the second set to evaluate the performance of the prediction [14]. Given a loss function L(i, j) which measures the loss of predicting rank i when true rank is j (rank here refers to the number of sets of equivalent items that are more or less preferred\nthan the current item) we evaluate a prediction rule by the expected loss. We focus on three loss functions: L0(i, j) = 0 if i = j and 1 otherwise, L1(i, j) = |i\u2212j| which reduces to the standard CF evaluation technique described in [14], and an asymmetric loss function (rows correspond to estimated number of stars (0-5) and columns to actual number of stars (0-5)\nLe =\n\n       0 0 0 3 4 5 0 0 0 2 3 4 0 0 0 1 2 3 9 4 1.5 0 0 0 12 6 3 0 0 0 15 8 4.5 0 0 0\n\n       . (15)\nIn contrast to the L0 and L1 loss, Le captures the fact that recommending bad movies as good movies is worse than recommending good movies as bad.\nFor example, consider a test user whose observed preference is 3 \u227a 4, 5, 6 \u227a 10, 11, 12 \u227a 23 \u227a 40, 50, 60 \u227a 100, 101. We may withhold the preferences of items 4, 11 for evaluation purposes. The recommendation systems then predict a rank of 1 for item 4 and a rank of 4 for item 11. Since the true ranking of these items are 2 and 3 the absolute value loss is |1\u2212 2| = 1 and |3\u2212 4| = 1 respectively.\nIn our experiment, we use the kernel estimator p\u0302 to predict ranks that minimize the posterior loss and thus adapts to customized loss functions such as Le. This is an advantage of a probabilistic modeling approach over more ad-hoc rule based recommendation systems.\nFigure 8 compares the performance of our estimator to several standard baselines in the collaborative filtering literature: two older memory based methods vector similarity (sim1), correlation (sim2) e.g., [2], and a recent state-of-the-art non-negative matrix (NMF) factorization (gnmf) [9]. The kernel smoothing estimate performed similar to the state-of-the-art but substantially better than the memory based methods to which it is functionally similar."}, {"heading": "4.3 Rule Discovery", "text": "In the third task, we used the estimator p\u0302 to detect noteworthy association rules of the type i \u227a j \u21d2 k \u227a l (if i is preferred to j than it is probably the case that k is preferred to l). Such association rules are important for both business analytics (devising marketing and manufacturing strategies) and recommendation system engineering. Specifically, we used p\u0302 to select sets of four items i, j, k, l for which the mutual information I(i \u227a j ; k \u227a l) is maximized. After these sets are identified we detected the precise shape of the rule (i.e., i \u227a j \u21d2 k \u227a l rather than j \u227a i \u21d2 k \u227a l by examining the summands in the mutual information expectation).\nFigure 9 (top) shows the top 10 rules that were discovered. These rules nicely isolate viewer preferences for genres such as fantasy, romantic comedies, animation, and action (note however that genre information was not used in the rule discovery). To quantitatively evaluate the rule discovery process we judge a rule i \u227a j \u21d2 k \u227a l to be good if i, k are of the same genre and j, l are of the same genre. This quantitative evaluation appears in Figure 9 (bottom) where it is contrasted with the same rule discovery process (maximizing mutual information) based on the empirical measure.\nIn another rule discovery experiment, we used p\u0302 to detect association rules of the form i ranked highest \u21d2 j ranked second highest by selecting i, j that maximize the score p(\u03c0(i)=1,\u03c0(j)=2)p(\u03c0(i)=1)p(\u03c0(j)=2) between pairs of movies in the Netflix data. We similarly detected rules of the form i ranked highest \u21d2 j ranked lowest by maximizing the scores p(\u03c0(i)=1,\u03c0(j)=last)p(\u03c0(i)=1)p(\u03c0(j)=last) between pairs of movies.\nThe left panel of Figure 10 shows the top 9 rules of 100 most rated movies, which nicely represents movie preference of similar type, e.g. romance, comedies, and action. The right of Figure 10 shows the top 9 rules which represents like and dislike of different movie types, e.g. like of romance leads to dislike of action/thriller.\nIn a third experiment, we used p\u0302 to construct an undirected graph where vertices are items (Netflix movies) and two nodes i,j are connected by an edge if the average score of the rule i ranked highest \u21d2\nj ranked second highest and the rule j ranked highest \u21d2 i ranked second highest is higher than a certain threshold. Figure 11 shows the graph for the 100 most rated movies in Netflix (only movies with vertex degree greater than 0 are shown). The clusters in the graph corresponding to vertex color and numbering were obtained using a graph partitioning algorithm and the graph is embedded in a 2-D plane using standard graph visualization technique. Within each of the identified clusters movies are clearly similar with respect to genre, while an even finer separation can be observed when looking at specific clusters. For example, clusters 6 and 9 both contain comedy movies, where as cluster 6 tends toward slapstick humor and cluster 9 contains romantic comedies."}, {"heading": "5 Related Work", "text": "Collaborative filtering or recommendation system has been an active research area in computer science since the 1990s. The earliest efforts made a prediction for the rating of items based on the similarity of the test user and the training users [17, 2, 6]. Specifically, these attempts used similarity measures such as Pearson correlation [17] and Vector cosine similarity [2, 6] to evaluate the similarity level between different users.\nMore recent work includes user and movie clustering [2, 21, 22], item-item similarities [18], Bayesian networks [2], dependence network [5] and probabilistic latent variable models [14, 7, 13].\nMost recently, the state of the art methods including the winner of the Netflix competition are based on non-negative matrix factorization of the partially observed user-rating matrix. The factorized matrix can be used to fill out the unobserved entries in a way similar to latent factor analysis [4, 16, 9, 8].\nEach of the above methods focuses exclusively on user ratings. In some cases item information is available (movie genre, actors, directors, etc) which have lead to several approaches that combine voting information with item information e.g., [1, 15, 19].\nOur method differs from the methods above in that it constructs a full probabilistic model on preferences,\nit is able to handle heterogeneous preference information (not all users must specify the same number of preference classes) and does not make any parametric assumptions. In contrast to previous approaches it enables not only the prediction of item ratings, but also the discovery of association rules and the estimation of probabilities of interesting events."}, {"heading": "6 Summary", "text": "Estimating distributions from tied and incomplete data is a central task in many applications with perhaps the most obvious one being collaborative filtering. An accurate estimator p\u0302 enables going beyond the traditional item-rank prediction task. It can be used to compute probabilities of interest, find association rules, and perform a wide range of additional data analysis tasks.\nWe demonstrate the first non-parametric estimator for such data that is computationally tractable i.e., polynomial rather than exponential in n. The computation is made possible using generating function and dynamic programming techniques.\nWe examine the behavior of the estimator p\u0302 in three sets of experiments. The first set of experiments involves estimating probabilities of interest such as p(i \u227a j). The second set of experiments involves predicting preferences of held-out items which is directly applicable in recommendation systems. In this task, our estimator outperforms other memory based methods (to which it is similar functionally) and performs similarly to state-of-the-art methods that are based on non-negative matrix factorization. In the third set of experiments we examined the usage of the estimator in discovering association rules such as i \u227a j \u21d2 k \u227a l."}], "references": [{"title": "Recommendation as classification: Using social and content-based information in recommendation", "author": ["C. Basu", "H. Hirsh", "W. Cohen"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Empirical analysis of predictive algorithms for collaborative filtering", "author": ["J. Breese", "D. Heckerman", "C. Kadie"], "venue": "In Proc. of Uncertainty in Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Group Representations in Probability and Statistics", "author": ["P. Diaconis"], "venue": "Institute of Mathematical Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1988}, {"title": "Eigentaste: A constant time collaborative filtering algorithm", "author": ["K. Goldberg", "T. Roeder", "D. Gupta", "C. Perkins"], "venue": "Information Retrieval,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Dependency networks for inference, collaborative filtering, and data visualization", "author": ["David Heckerman", "David Maxwell Chickering", "Christopher Meek", "Robert Rounthwaite", "Carl Kadie"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "An algorithmic framework for performing collaborative filtering", "author": ["J.L. Herlocker", "J.A. Konstan", "A. Borchers", "J. Riedl"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Latent semantic models for collaborative filtering", "author": ["T. Hofmann"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Factor in the neighbors: Scalable and accurate collaborative filtering", "author": ["Y. Koren"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Non-linear matrix factorization with gaussian processes", "author": ["N.D. Lawrence", "R. Urtasun"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Non-parametric modeling of partially ranked data", "author": ["G. Lebanon", "Y. Mao"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Non-null ranking models", "author": ["C.L. Mallows"], "venue": "Biometrika, 44:114\u2013130,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1957}, {"title": "Analyzing and modeling rank data", "author": ["J.I. Marden"], "venue": "CRC Press,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Modeling user rating profiles for collaborative filtering", "author": ["B. Marlin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Collaborative filtering by personality diagnosis: A hybrid memory- and model-based approach", "author": ["D.M. Pennock", "E. Horvitz", "S. Lawrence", "C.L. Giles"], "venue": "In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Probabilistic models for unified collaborative and content-based recommendation in sparse-data environments", "author": ["A. Popescul", "L.H. Ungar", "D.M. Pennock", "S. Lawrence"], "venue": "In Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "author": ["J.D.M. Rennie", "N. Srebro"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Grouplens: an open architecture for collaborative filtering of netnews", "author": ["P. Resnick", "N. Iacovou", "M. Suchak", "P. Bergstrom", "J. Riedl"], "venue": "In Proceedings of the Conference on Computer Supported Cooperative Work,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["B. Sarwar", "G. Karypis", "J. Konstan", "J. Reidl"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Methods and metrics for cold-start recommendations", "author": ["A.I. Schein", "A. Popescul", "L.H. Ungar", "D.M. Pennock"], "venue": "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Enumerative Combinatorics, volume 1", "author": ["R.P. Stanley"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Clustering methods for collaborative filtering", "author": ["L.H. Ungar", "D.P. Foster"], "venue": "In AAAI Workshop on Recommendation Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Scalable collaborative filtering using cluster-based smoothing", "author": ["G.R. Xue", "C. Lin", "Q. Yang", "W.S. Xi", "H.J. Zeng", "Y. Yu", "Z. Chen"], "venue": "In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "2 Definitions and Estimation Framework We describe the following notations and conventions for permutations, which are taken from [3] where more detail may be found.", "startOffset": 130, "endOffset": 133}, {"referenceID": 11, "context": ", Sm are possible, experimental evidence point to the fact that in recommendation systems with high n, the distribution p does not follow a simple parametric form such as the Mallows, Bradley-Terry, or Thurstone models [12] (see Figure 1 for a demonstration how parametric assumptions break down with increasing n).", "startOffset": 219, "endOffset": 223}, {"referenceID": 9, "context": "3 Computationally Efficient Kernel Smoothing In previous work [10] the estimator (6) is proposed for tied (but complete) rankings.", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "That work derives closed form expressions and efficient computation for (6) assuming a Mallows kernel [11] Kh(T (\u03c0, \u03c3)) = exp (", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "Kendall\u2019s tau T (\u03c0, \u03c3) is the total number of discordant pairs or inversions between \u03c0, \u03c3 [20] and thus its computation becomes a combinatorial counting problem.", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "As shown for example in [20] the coefficient of z of Gn(z), which we denote as [z ]Gn(z), corresponds to the number of permutations \u03c3 for which T (\u03c3, \u03c0) = k.", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "For each of the test set users we further split the observed items into two sets: one set used for estimating preferences (together with the preferences of the training set users) and the second set to evaluate the performance of the prediction [14].", "startOffset": 245, "endOffset": 249}, {"referenceID": 13, "context": "We focus on three loss functions: L0(i, j) = 0 if i = j and 1 otherwise, L1(i, j) = |i\u2212j| which reduces to the standard CF evaluation technique described in [14], and an asymmetric loss function (rows correspond to estimated number of stars (0-5) and columns to actual number of stars (0-5)", "startOffset": 157, "endOffset": 161}, {"referenceID": 1, "context": ", [2], and a recent state-of-the-art non-negative matrix (NMF) factorization (gnmf) [9].", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": ", [2], and a recent state-of-the-art non-negative matrix (NMF) factorization (gnmf) [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 16, "context": "The earliest efforts made a prediction for the rating of items based on the similarity of the test user and the training users [17, 2, 6].", "startOffset": 127, "endOffset": 137}, {"referenceID": 1, "context": "The earliest efforts made a prediction for the rating of items based on the similarity of the test user and the training users [17, 2, 6].", "startOffset": 127, "endOffset": 137}, {"referenceID": 5, "context": "The earliest efforts made a prediction for the rating of items based on the similarity of the test user and the training users [17, 2, 6].", "startOffset": 127, "endOffset": 137}, {"referenceID": 16, "context": "Specifically, these attempts used similarity measures such as Pearson correlation [17] and Vector cosine similarity [2, 6] to evaluate the similarity level between different users.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "Specifically, these attempts used similarity measures such as Pearson correlation [17] and Vector cosine similarity [2, 6] to evaluate the similarity level between different users.", "startOffset": 116, "endOffset": 122}, {"referenceID": 5, "context": "Specifically, these attempts used similarity measures such as Pearson correlation [17] and Vector cosine similarity [2, 6] to evaluate the similarity level between different users.", "startOffset": 116, "endOffset": 122}, {"referenceID": 1, "context": "More recent work includes user and movie clustering [2, 21, 22], item-item similarities [18], Bayesian networks [2], dependence network [5] and probabilistic latent variable models [14, 7, 13].", "startOffset": 52, "endOffset": 63}, {"referenceID": 20, "context": "More recent work includes user and movie clustering [2, 21, 22], item-item similarities [18], Bayesian networks [2], dependence network [5] and probabilistic latent variable models [14, 7, 13].", "startOffset": 52, "endOffset": 63}, {"referenceID": 21, "context": "More recent work includes user and movie clustering [2, 21, 22], item-item similarities [18], Bayesian networks [2], dependence network [5] and probabilistic latent variable models [14, 7, 13].", "startOffset": 52, "endOffset": 63}, {"referenceID": 17, "context": "More recent work includes user and movie clustering [2, 21, 22], item-item similarities [18], Bayesian networks [2], dependence network [5] and probabilistic latent variable models [14, 7, 13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 1, "context": "More recent work includes user and movie clustering [2, 21, 22], item-item similarities [18], Bayesian networks [2], dependence network [5] and probabilistic latent variable models [14, 7, 13].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "More recent work includes user and movie clustering [2, 21, 22], item-item similarities [18], Bayesian networks [2], dependence network [5] and probabilistic latent variable models [14, 7, 13].", "startOffset": 136, "endOffset": 139}, {"referenceID": 13, "context": "More recent work includes user and movie clustering [2, 21, 22], item-item similarities [18], Bayesian networks [2], dependence network [5] and probabilistic latent variable models [14, 7, 13].", "startOffset": 181, "endOffset": 192}, {"referenceID": 6, "context": "More recent work includes user and movie clustering [2, 21, 22], item-item similarities [18], Bayesian networks [2], dependence network [5] and probabilistic latent variable models [14, 7, 13].", "startOffset": 181, "endOffset": 192}, {"referenceID": 12, "context": "More recent work includes user and movie clustering [2, 21, 22], item-item similarities [18], Bayesian networks [2], dependence network [5] and probabilistic latent variable models [14, 7, 13].", "startOffset": 181, "endOffset": 192}, {"referenceID": 3, "context": "The factorized matrix can be used to fill out the unobserved entries in a way similar to latent factor analysis [4, 16, 9, 8].", "startOffset": 112, "endOffset": 125}, {"referenceID": 15, "context": "The factorized matrix can be used to fill out the unobserved entries in a way similar to latent factor analysis [4, 16, 9, 8].", "startOffset": 112, "endOffset": 125}, {"referenceID": 8, "context": "The factorized matrix can be used to fill out the unobserved entries in a way similar to latent factor analysis [4, 16, 9, 8].", "startOffset": 112, "endOffset": 125}, {"referenceID": 7, "context": "The factorized matrix can be used to fill out the unobserved entries in a way similar to latent factor analysis [4, 16, 9, 8].", "startOffset": 112, "endOffset": 125}, {"referenceID": 0, "context": ", [1, 15, 19].", "startOffset": 2, "endOffset": 13}, {"referenceID": 14, "context": ", [1, 15, 19].", "startOffset": 2, "endOffset": 13}, {"referenceID": 18, "context": ", [1, 15, 19].", "startOffset": 2, "endOffset": 13}], "year": 2010, "abstractText": "Recommendation systems are emerging as an important business application with significant economic impact. Currently popular systems include Amazon\u2019s book recommendations, Netflix\u2019s movie recommendations, and Pandora\u2019s music recommendations. In this paper we address the problem of estimating probabilities associated with recommendation system data using non-parametric kernel smoothing. In our estimation we interpret missing items as randomly censored observations and obtain efficient computation schemes using combinatorial properties of generating functions. We demonstrate our approach with several case studies involving real world movie recommendation data. The results are comparable with state-of-the-art techniques while also providing probabilistic preference estimates outside the scope of traditional recommender systems.", "creator": "LaTeX with hyperref package"}}}