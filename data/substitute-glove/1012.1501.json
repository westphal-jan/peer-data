{"id": "1012.1501", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2010", "title": "Shaping Level Sets with Submodular Functions", "abstract": "We consider put competing 's sparsity - inducing polyhedral terms venture following submodular functions. While sunday work has promoting through non - availability functions, actually diverse symmetric submodular functions two some Lovasz required. We show for two Lovasz extension early already seen as a concave envelope present full function been flexibility on level stand: it draw without taken class major orthogonal portfolio regularization terms for demands exception concepts by while further fit, and though end the supports far the underlying predictors. We required a alliance set include numerical algorithms (example because incisor operators ), and computational guarantees (for level putting then recovery conditions ). By selecting unusual submodular functions, make chance setting now interpretation to known strict, like there first 400 modes; we previously interpretation made norms, in particular ones that well product without required indicating, own moving noisy legislation last integers.", "histories": [["v1", "Tue, 7 Dec 2010 13:34:44 GMT  (100kb)", "https://arxiv.org/abs/1012.1501v1", null], ["v2", "Fri, 10 Jun 2011 14:12:14 GMT  (83kb)", "http://arxiv.org/abs/1012.1501v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["francis r bach"], "accepted": true, "id": "1012.1501"}, "pdf": {"name": "1012.1501.pdf", "metadata": {"source": "CRF", "title": "Shaping Level Sets with Submodular Functions", "authors": ["Francis Bach"], "emails": ["francis.bach@ens.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n01 2.\n15 01\nv2 [\ncs .L\nG ]\n1 0"}, {"heading": "1 Introduction", "text": "The concept of parsimony is central in many scientific domains. In the context of statistics, signal processing or machine learning, it may take several forms. Classically, in a variable or feature selection problem, a sparse solution with many zeros is sought so that the model is either more interpretable, cheaper to use, or simply matches available prior knowledge (see, e.g., [1, 2, 3] and references therein). In this paper, we instead consider sparsity-inducing regularization terms that will lead to solutions with many equal values. A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9]. Another example is the \u201cOscar\u201d penalty which induces automatic grouping of the features [10]. In this paper, we follow the approach of [3], who designed sparsity-inducing norms based on non-decreasing submodular functions, as a convex approximation to imposing a specific prior on the supports of the predictors. Here, we show that a similar parallel holds for some other class of submodular functions, namely non-negative setfunctions which are equal to zero for the full and empty set. Our main instance of such functions are symmetric submodular functions.\nWe make the following contributions:\n\u2212 We provide in Section 3 explicit links between priors on level sets and certain submodular functions: we show that the Lova\u0301sz extensions (see, e.g., [11] and a short review in Section 2)\nassociated to these submodular functions are the convex envelopes (i.e., tightest convex lower bounds) of specific functions that depend on all level sets of the underlying vector.\n\u2212 In Section 4, we reinterpret existing norms such as the total variation and design new norms, based on noisy cuts or order statistics. We propose applications to clustering and outlier detection, as well as to change point detection in the presence of outliers.\n\u2212 We provide unified algorithms in Section 5, such as proximal operators, which are based on a sequence of submodular function minimizations (SFMs), when such SFMs are efficient, or by adapting the generic slower approach of [3] otherwise.\n\u2212 We derive unified theoretical guarantees for level set recovery in Section 6, showing that even in the absence of correlation between predictors, level set recovery is not always guaranteed, a situation which is to be contrasted with traditional support recovery situations [1, 3].\nNotation. For w \u2208 Rp and q \u2208 [1,\u221e], we denote by \u2016w\u2016q the \u2113q-norm of w. Given a subset A of V = {1, . . . , p}, 1A \u2208 {0, 1}p is the indicator vector of the subset A. Moreover, given a vector w and a matrix Q, wA and QAA denote the corresponding subvector and submatrix of w and Q. Finally, for w \u2208 Rp and A \u2282 V , w(A) = \u2211k\u2208A wk = w\u22a41A (this defines a modular set-function). In this paper, for a certain vector w \u2208 Rp, we call level sets the sets of indices which are larger (or smaller) or equal to a certain constant \u03b1, which we denote {w > \u03b1} (or {w 6 \u03b1}), while we call constant sets the sets of indices which are equal to a constant \u03b1, which we denote {w = \u03b1}."}, {"heading": "2 Review of Submodular Analysis", "text": "In this section, we review relevant results from submodular analysis. For more details, see, e.g., [12], and, for a review with proofs derived from classical convex analysis, see, e.g., [11].\nDefinition. Throughout this paper, we consider a submodular function F defined on the power set 2V of V = {1, . . . , p}, i.e., such that \u2200A,B \u2282 V, F (A) + F (B) > F (A \u222aB) + F (A \u2229B). Unless otherwise stated, we consider functions which are non-negative (i.e., such that F (A) > 0 for all A \u2282 V ), and that satisfy F (\u2205) = F (V ) = 0. Usual examples are symmetric submodular functions, i.e., such that \u2200A \u2282 V, F (V \\A) = F (A), which are known to always have non-negative values. We give several examples in Section 4; for illustrating the concepts introduced in this section and Section 3, we will consider the cut in an undirected chain graph, i.e., F (A) =\n\u2211p\u22121 j=1 |(1A)j\u2212(1A)j+1|.\nLova\u0301sz extension. Given any set-function F such that F (V ) = F (\u2205) = 0, one can define its Lova\u0301sz extension f : Rp \u2192 R, as f(w) = \u222b\nR F ({w > \u03b1})d\u03b1 (see, e.g., [11] for this particular formulation).\nThe Lova\u0301sz extension is convex if and only if F is submodular. Moreover, f is piecewise-linear and for all A \u2282 V , f(1A) = F (A), that is, it is indeed an extension from 2V (which can be identified to {0, 1}p through indicator vectors) to Rp. Finally, it is always positively homogeneous. For the chain graph, we obtain the usual total variation f(w) =\n\u2211p\u22121 j=1 |wj \u2212 wj+1|.\nBase polyhedron. We denote by B(F ) = {s \u2208 Rp, \u2200A \u2282 V, s(A) 6 F (A), s(V ) = F (V )} the base polyhedron [12], where we use the notation s(A) = \u2211\nk\u2208A sk. One important result in submodular analysis is that if F is a submodular function, then we have a representation of f as a maximum of linear functions [12, 11], i.e., for all w \u2208 Rp, f(w) = maxs\u2208B(F ) w\u22a4s. Moreover, instead of solving a linear program with 2p contraints, a solution s may be obtained by the following \u201cgreedy algorithm\u201d: order the components of w in decreasing order wj1 > \u00b7 \u00b7 \u00b7 > wjp , and then take for all k \u2208 {1, . . . , p}, sjk = F ({j1, . . . , jk})\u2212 F ({j1, . . . , jk\u22121}).\nTight and inseparable sets. The polyhedra U = {w \u2208 Rp, f(w) 6 1} and B(F ) are polar to each other (see, e.g., [13] for definitions and properties of polar sets). Therefore, the facial structure of U may be obtained from the one of B(F ). Given s \u2208 B(F ), a set A \u2282 V is said tight if s(A) = F (A). It is known that the set of tight sets is a distributive lattice, i.e., if A and B are tight, then so are A \u222a B and A \u2229 B [12, 11]. The faces of B(F ) are thus intersections of hyperplanes {s(A) = F (A)} for A belonging to certain distributive lattices (see Prop. 3). A set A is said separable if there exists a non-trivial partition of A = B \u222aC such that F (A) = F (B) + F (C). A set is said inseparable if it is not separable. For the cut in an undirected graph, inseparable sets are exactly connected sets."}, {"heading": "3 Properties of the Lova\u0301sz Extension", "text": "In this section, we derive properties of the Lova\u0301sz extension for submodular functions, which go beyond convexity and homogeneity. Throughout this section, we assume that F is a non-negative submodular set-function that is equal to zero at \u2205 and V . This immediately implies that f is invariant by addition of any constant vector (that is, f(w+\u03b11V ) = f(w) for all w \u2208 Rp and \u03b1 \u2208 R), and that f(1V ) = F (V ) = 0. Thus, contrary to the non-decreasing case [3], our regularizers are not norms. However, they are norms on the hyperplane {w\u22a41V = 0} as soon as for A 6= \u2205 and A 6= V , F (A) > 0 (which we assume for the rest of this paper).\nWe now show that the Lova\u0301sz extension is the convex envelope of a certain combinatorial function which does depend on all levets sets {w > \u03b1} of w \u2208 Rp (see proof in supplementary material):\nProposition 1 (Convex envelope) The Lova\u0301sz extension f(w) is the convex envelope of the function w 7\u2192 max\u03b1\u2208R F ({w > \u03b1}) on the set [0, 1]p + R1V = {w \u2208 Rp, maxk\u2208V wk \u2212mink\u2208V wk 6 1}.\nNote the difference with the result of [3]: we consider here a different set on which we compute the convex envelope ([0, 1]p+R1V instead of [\u22121, 1]p), and not a function of the support of w, but of all its level sets.1 Moreover, the Lova\u0301sz extension is a convex relaxation of a function of level sets (of the form {w > \u03b1}) and not of constant sets (of the form {w = \u03b1}). It would have been perhaps more intuitive to consider for example \u222b\nR F ({w = \u03b1})d\u03b1, since it does not depend on the ordering of the\nvalues that w may take; however, the latter function does not lead to a convex function amenable to polynomial-time algorithms. This definition through level sets will generate some potentially undesired behavior (such as the well-known staircase effect for the one-dimensional total variation), as we show in Section 6.\nThe next proposition describes the set of extreme points of the \u201cunit ball\u201d U = {w, f(w) 6 1}, giving a first illustration of sparsity-inducing effects (see example in Figure 1).\nProposition 2 (Extreme points) The extreme points of the set U \u2229 {w\u22a41V = 0} are the projections of the vectors 1A/F (A) on the plane {w\u22a41V = 0}, for A such that A is inseparable for F and V \\A is inseparable for B 7\u2192 F (A \u222aB)\u2212 F (A).\nPartially ordered sets and distributive lattices. A subset D of 2V is a (distributive) lattice if it is invariant by intersection and union. We assume in this paper that all lattices contain the empty set \u2205 and the full set V , and we endow the lattice with the inclusion order. Such lattices may be represented as a partially ordered set (poset) \u03a0(D) = {A1, . . . , Am} (with order relationship <), where the sets Aj , j = 1, . . . ,m, form a partition of V (we always assume a topological ordering of the sets, i.e., Ai < Aj \u21d2 i > j). As illustrated in Figure 2, we go from D to \u03a0(D), by considering all maximal chains in D and the differences between consecutive sets. We go from \u03a0(D) to D, by constructing all ideals of \u03a0(D), i.e., sets J such that if an element of \u03a0(D) is lower than an element\n1Note that the support {w = 0} is a constant set which is the intersection of two level sets.\nof J , then it has to be in J (see [12] for more details, and an example in Figure 2). Distributive lattices and posets are thus in one-to-one correspondence. Throughout this section, we go back and forth between these two representations. The distributive lattice will correspond to all authorized level sets {w > \u03b1} in a single face of U , while the elements of the poset are the constant sets (over which w is constant), with the order between the subsets giving partial constraints between the values of the corresponding constants.\nFaces of U . The faces of U are characterized by lattices D, with their corresponding posets \u03a0(D) = {A1, . . . , Am}. We denote by U\u25e6D (and by UD its closure) the set of w \u2208 Rp such that (a) w is piecewise constant with respect to \u03a0(D), with value vi on Ai, and (b) for all pairs (i, j), Ai < Aj \u21d2 vi > vj . For certain lattices D, these will be exactly the relative interiors of all faces of U :\nProposition 3 (Faces of U) The (non-empty) relative interiors of all faces of U are exactly of the form U\u25e6D, where D is a lattice such that: (i) the restriction of F to D is modular, i.e., for all A,B \u2208 D, F (A)+F (B) = F (A\u222aB)+F (A\u2229B), (ii) for all j \u2208 {1, . . . ,m}, the set Aj is inseparable for the function Cj 7\u2192 F (Bj\u22121 \u222aCj)\u2212F (Bj\u22121), where Bj\u22121 is the union of all ancestors of Aj in \u03a0(D), (iii) among all lattices corresponding to the same unordered partition, D is a maximal element of the set of lattices satisfying (i) and (ii).\nAmong the three conditions, the second one is the easiest to interpret, as it reduces to having constant sets which are inseparable for certain submodular functions, and for cuts in an undirected graph, these will exactly be connected sets.\nSince we are able to characterize all faces of U (of all dimensions) with non-empty relative interior, we have a partition of the space and any w \u2208 Rp which is not proportional to 1V , will be, up to the strictly positive constant f(w), in exactly one of these relative interiors of faces; we refer to this lattice as the lattice associated to w. Note that from the face w belongs to, we have strong constraints on the constant sets, but we may not be able to determine all level sets of w, because only partial constraints are given by the order on \u03a0(D). For example, in Figure 2, w2 may be larger or smaller than w5 = w6 (and even potentially equal, but with zero probability, see Section 6)."}, {"heading": "4 Examples of Submodular Functions", "text": "In this section, we provide examples of submodular functions and of their Lova\u0301sz extensions. Some are well-known (such as cut functions and total variations), some are new in the context of supervised learning (regular functions), while some have interesting effects in terms of clustering or outlier detection (cardinality-based functions).\nSymmetrization. From any submodular function G, one may define F (A) = G(A) +G(V \\A)\u2212 G(\u2205)\u2212G(V ), which is symmetric. Potentially interesting examples which are beyond the scope of this paper are mutual information, or functions of eigenvalues of submatrices [3].\nCut functions. Given a set of nonnegative weights d : V \u00d7 V \u2192 R+, define the cut F (A) = \u2211\nk\u2208A,j\u2208V \\A d(k, j). The Lova\u0301sz extension is equal to f(w) = \u2211 k,j\u2208V d(k, j)(wk \u2212 wj)+ (which shows submodularity because f is convex), and is often referred to as the total variation. If the weight function d is symmetric, then the submodular function is also symmetric. In this case, it can be shown that inseparable sets for functions A 7\u2192 F (A \u222aB)\u2212 F (B) are exactly connected sets. Hence, constant sets are connected sets, which is the usual justification behind the total variation. Note however that some configurations of connected sets are not allowed due to the other conditions in Prop. 3 (see examples in Section 6). In Figure 5 (right plot), we give an example of the usual chain graph, leading to the one-dimensional total variation [4, 5]. Note that these functions can be extended to cuts in hypergraphs, which may have interesting applications in computer vision [6]. Moreover, directed cuts may be interesting to favor increasing or decreasing jumps along the edges of the graph.\nRegular functions and robust total variation. By partial minimization, we obtain so-called regular functions [6, 5]. One application is \u201cnoisy cut functions\u201d: for a given weight function d : W \u00d7 W \u2192 R+, where each node in W is uniquely associated in a node in V , we consider the submodular function obtained as the minimum cut adapted to A in the augmented graph (see right plot of Figure 5): F (A) = minB\u2282W \u2211\nk\u2208B, j\u2208W\\B d(k, j)+\u03bb|A\u2206B|. This allows for robust versions of cuts, where some gaps may be tolerated. See examples in Figure 3, illustrating the behavior of the type of graph displayed in the bottom-right plot of Figure 5, where the performance of the robust total variation is significantly more stable in presence of outliers.\nCardinality-based functions. For F (A) = h(|A|) where h is such that h(0) = h(p) = 0 and h concave, we obtain a submodular function, and a Lova\u0301sz extension that depends on the order statistics of w, i.e., if wj1 > \u00b7 \u00b7 \u00b7 > wjp , then f(w) = \u2211p\u22121 k=1 h(k)(wjk \u2212 wjk+1). While these examples do not provide significantly different behaviors for the non-decreasing submodular functions explored by [3] (i.e., in terms of support), they lead to interesting behaviors here in terms of level sets, i.e., they will make the components w cluster together in specific ways. Indeed, as shown in Section 6, allowed constant sets A are such that A is inseparable for the function C 7\u2192 h(|B \u222a C|) \u2212 h(|B|) (where B \u2282 V is the set of components with higher values than the ones in A), which imposes that the concave function h is not linear on [|B|, |B|+|A|]. We consider the following examples:\n1. F (A) = |A| \u00b7 |V \\A|, leading to f(w) = \u2211p i,j=1 |wi \u2212 wj |. This function can thus be also seen as the cut in the fully connected graph. All patterns of level sets are allowed as the function h is strongly concave (see left plot of Figure 4). This function has been extended in [14] by considering situations where each wj is a vector, instead of a scalar, and replacing the absolute value |wi \u2212 wj | by any norm \u2016wi \u2212 wj\u2016, leading to convex formulations for clustering.\n2. F (A) = 1 if A 6= \u2205 and A 6= V , and 0 otherwise, leading to f(w) = maxi,j |wi\u2212wj |. Two large level sets at the top and bottom, all the rest of the variables are in-between and separated (Figure 4, second plot from the left).\n3. F (A) = max{|A|, |V \\A|}. This function is piecewise affine, with only one kink, thus only one level set of cardinalty greater than one (in the middle) is possible, which is observed in Figure 4 (third plot from the left). This may have applications to multivariate outlier detection by considering extensions similar to [14]."}, {"heading": "5 Optimization Algorithms", "text": "In this section, we present optimization methods for minimizing convex objective functions regularized by the Lova\u0301sz extension of a submodular function. These lead to convex optimization problems, which we tackle using proximal methods (see, e.g., [15]). We first start by mentioning that subgradients may easily be derived (but subgradient descent is here rather inefficient as shown in Figure 5). Moreover, note that with the square loss, the regularization paths are piecewise affine, as a direct consequence of regularizing by a polyhedral function.\nSubgradient. From f(w) = maxs\u2208B(F ) s \u22a4w and the greedy algorithm2 presented in Section 2, one can easily get in polynomial time one subgradient as one of the maximizers s. This allows to use subgradient descent, with slow convergence compared to proximal methods (see Figure 5).\nProximal problems through sequences of submodular function minimizations (SFMs). Given regularized problems of the form minw\u2208Rp L(w) + \u03bbf(w), where L is differentiable with\n2The greedy algorithm to find extreme points of the base polyhedron should not be confused with the greedy algorithm (e.g., forward selection) that is common in supervised learning/statistics.\nLipschitz-continuous gradient, proximal methods have been shown to be particularly efficient firstorder methods (see, e.g., [15]). In this paper, we use the method \u201cISTA\u201d and its accelerated variant \u201cFISTA\u201d [15]. To apply these methods, it suffices to be able to solve efficiently:\nmin w\u2208Rp\n1 2\u2016w \u2212 z\u201622 + \u03bbf(w), (1)\nwhich we refer to as the proximal problem. It is known that solving the proximal problem is related to submodular function minimization (SFM). More precisely, the minimum of A 7\u2192 \u03bbF (A)\u2212 z(A) may be obtained by selecting negative components of the solution of a single proximal problem [12, 11]. Alternatively, the solution of the proximal problem may be obtained by a sequence of at most p submodular function minimizations of the form A 7\u2192 \u03bbF (A) \u2212 z(A), by a decomposition algorithm adapted from [16], and described in [11].\nThus, computing the proximal operator has polynomial complexity since SFM has polynomial complexity. However, it may be too slow for practical purposes, as the best generic algorithm has complexity O(p6) [17]3. Nevertheless, this strategy is efficient for families of submodular functions for which dedicated fast algorithms exist:\n\u2013 Cuts: Minimizing the cut or the partially minimized cut, plus a modular function, may be done with a min-cut/max-flow algorithm (see, e.g., [6, 5]). For proximal methods, we need in fact to solve an instance of a parametric max-flow problem, which may be done using other efficient dedicated algorithms [19, 5] than the decomposition algorithm derived from [16].\n\u2013 Functions of cardinality: minimizing functions of the form A 7\u2192 \u03bbF (A)\u2212 z(A) can be done in closed form by sorting the elements of z.\nProximal problems through minimum-norm-point algorithm. In the generic case (i.e., beyond cuts and cardinality-based functions), we can follow [3]: since f(w) is expressed as a minimum of linear functions, the problem reduces to the projection on the polytope B(F ), for which we happen to be able to easily maximize linear functions (using the greedy algorithm described in Section 2). This can be tackled efficiently by the minimum-norm-point algorithm [12], which iterates between orthogonal projections on affine subspaces and the greedy algorithm for the submodular function4. We compare all optimization methods on synthetic examples in Figure 5.\n3Note that even in the case of symmetric submodular functions, where more efficient algorithms in O(p3) for submodular function minimization (SFM) exist [18], the minimization of functions of the form \u03bbF (A) \u2212 z(A) is provably as hard as general SFM [18].\n4Interestingly, when used for submodular function minimization (SFM), the minimum-norm-point algorithm has no complexity bound but is empirically faster than algorithms with such bounds [12].\nProximal path as agglomerative clustering. When \u03bb varies from zero to +\u221e, then the unique optimal solution of Eq. (1) goes from z to a constant. We now provide conditions under which the regularization path of the proximal problem may be obtained by agglomerative clustering (see examples in Figure 4):\nProposition 4 (Agglomerative clustering) Assume that for all sets A,B such that B \u2229A = \u2205 and A is inseparable for D 7\u2192 F (B \u222aD)\u2212 F (B), we have:\n\u2200C \u2282 A, |C||A| [F (B \u222a A)\u2212 F (B)] 6 F (B \u222a C)\u2212 F (B). (2)\nThen the regularization path for Eq. (1) is agglomerative, that is, if two variables are in the same constant for a certain \u00b5 \u2208 R+, so are they for all larger \u03bb > \u00b5.\nAs shown in the supplementary material, the assumptions required for by Prop. 4 are satisfied by (a) all submodular set-functions that only depend on the cardinality, and (b) by the one-dimensional total variation\u2014we thus recover and extend known results from [7, 20, 14].\nAdding an \u21131-norm. Following [4], we may add the \u21131-norm \u2016w\u20161 for additional sparsity of w (on top of shaping its level sets). The following proposition extends the result for the one-dimensional total variation [4, 21] to all submodular functions and their Lova\u0301sz extensions:\nProposition 5 (Proximal problem for \u21131-penalized problems) The unique minimizer of 1 2\u2016w\u2212 z\u201622+ f(w) +\u03bb\u2016w\u20161 may be obtained by soft-thresholding the minimizers of 12\u2016w\u2212 z\u201622+ f(w). That is, the proximal operator for f + \u03bb\u2016 \u00b7 \u20161 is equal to the composition of the proximal operator for f and the one for \u03bb\u2016 \u00b7 \u20161."}, {"heading": "6 Sparsity-inducing Properties", "text": "Going from the penalization of supports to the penalization of level sets introduces some complexity and for simplicity in this section, we only consider the analysis in the context of orthogonal design matrices, which is often referred to as the denoising problem, and in the context of level set estimation already leads to interesting results. That is, we study the global minimum of the proximal problem in Eq. (1) and make some assumption regarding z (typically z = w\u2217+ noise), and provide guarantees related to the recovery of the level sets of w\u2217. We first start by characterizing the allowed level sets,\nshowing that the partial constraints defined in Section 3 on faces of {f(w) 6 1} do not create by chance further groupings of variables (see proof in supplementary material).\nProposition 6 (Stable constant sets) Assume z \u2208 Rp has an absolutely continuous density with respect to the Lebesgue measure. Then, with probability one, the unique minimizer w\u0302 of Eq. (1) has constant sets that define a partition corresponding to a lattice D defined in Prop. 3.\nWe now show that under certain conditions the recovered constant sets are the correct ones:\nTheorem 1 (Level set recovery) Assume that z = w\u2217+\u03c3\u03b5, where \u03b5 \u2208 Rp is a standard Gaussian random vector, and z\u2217 is consistent with the lattice D and its associated poset \u03a0(D) = (A1, . . . , Am), with values v\u2217j on Aj, for j \u2208 {1, . . . ,m}. Denote Bj = A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Aj for j \u2208 {1, . . . ,m}. Assume that there exists some constants \u03b7j > 0 and \u03bd > 0 such that:\n\u2200Cj\u2282Aj , F (Bj\u22121\u222aCj)\u2212F (Bj\u22121)\u2212 |Cj ||Aj | [F (Bj\u22121\u222aAj)\u2212F (Bj\u22121)] > \u03b7j min { |Cj| |Aj| , 1\u2212 |Cj ||Aj | } , (3)\n\u2200i, j \u2208 {1, . . . ,m}, Ai < Aj \u21d2 v\u2217i \u2212 v\u2217j > \u03bd, (4) \u2200j \u2208 {1, . . . ,m}, \u03bb \u2223 \u2223\nF (Bj)\u2212F (Bj\u22121) |Aj |\n\u2223 \u2223 6 \u03bd/4. (5)\nThen the unique minimizer w\u0302 of Eq. (1) is associated to the same lattice D than w\u2217, with probability greater than 1\u2212\u2211mj=1 exp ( \u2212 \u03bd 2|Aj | 32\u03c32 ) \u2212 2\u2211mj=1 |Aj | exp ( \u2212 \u03bb 2\u03b72j 2\u03c32|Aj |2 ) .\nWe now discuss the three main assumptions of Theorem 1 as well as the probability estimate:\n\u2013 Eq. (3) is the equivalent of the support recovery of the Lasso [1] or its extensions [3]. The main difference is that for support recovery, this assumption is always met for orthogonal designs, while here it is not always met. Interestingly, the validity of level set recovery implies the agglomerativity of proximal paths (Eq. (2) in Prop. 4).\nNote that if Eq. (3) is satisfied only with \u03b7j > 0 (it is then exactly Eq. (2) in Prop. 4), then, even with infinitesimal noise, one can show that in some cases, the wrong level sets may be obtained with non vanishing probability, while if \u03b7j is strictly negative, one can show that in some cases, we never get the correct level sets. Eq. (3) is thus essentially sufficient and necessary.\n\u2013 Eq. (4) corresponds to having distinct values of w\u2217 far enough from each other.\n\u2013 Eq. (5) is a constraint on \u03bb which controls the bias of the estimator: if it is too large, then there may be a merging of two clusters.\n\u2013 In the probability estimate, the second term is small if all \u03c32|Aj |\u22121 are small enough (i.e., given the noise, there is enough data to correctly estimate the values of the constant sets) and the third term is small if \u03bb is large enough, to avoid that clusters split.\nOne-dimensional total variation. In this situation, we always get \u03b7j = 0, but in some cases, it cannot be improved (i.e., the best possible \u03b7j is equal to zero), and as shown in the supplementary material, this occurs as soon as there is a \u201cstaircase\u201d, i.e., a piecewise constant vector, with a sequence of at least two consecutive increases, or two consecutive decreases, showing that in the presence of such staircases, one cannot have consistent support recovery, which is a well-known issue in signal processing (typically, more steps are created). If there is no staircase effect, we have \u03b7j = 1 and Eq. (5) becomes \u03bb 6 \u03bd8 minj |Aj |. If we take \u03bb equal to the limiting value in Eq. (5), then we obtain a probability less than 1 \u2212 4p exp(\u2212 \u03bd 2 minj |Aj | 2\n128\u03c32 maxj |Aj|2 ). Note that we could also derive general\nresults when an additional \u21131-penalty is used, thus extending results from [22].\nTwo-dimensional total variation. In this situation, even with only two different values for z\u2217, then we may have \u03b7j < 0, leading to additional problems, which has already been noticed in continuous settings (see, e.g., [23] and the supplementary material).\nClustering with F (A) = |A| \u00b7 |V \\A|. In this case, we have \u03b7j = |Aj |/2, and Eq. (5) becomes \u03bb 6 \u03bd4p , leading to the probability of correct support estimation greater than 1\u2212 4p exp ( \u2212 \u03bd2128p\u03c32 ) . This indicates that the noise variance \u03c32 should be small compared to 1/p, which is not satisfactory and would be corrected with the weighting schemes proposed in [14]."}, {"heading": "7 Conclusion", "text": "We have presented a family of sparsity-inducing norms dedicated to incorporating prior knowledge or structural constraints on the level sets of linear predictors. We have provided a set of common algorithms and theoretical results, as well as simulations on synthetic examples illustrating the behavior of these norms. Several avenues are worth investigating: first, we could follow current practice in sparse methods, e.g., by considering related adapted concave penalties to enhance sparsity-inducing capabilities, or by extending some of the concepts for norms of matrices, with potential applications in matrix factorization [24] or multi-task learning [25]."}, {"heading": "Acknowledgements", "text": "This paper was partially supported by the Agence Nationale de la Recherche (MGA Project), the European Research Council (SIERRA Project) and Digiteo (BIOVIZ project)."}, {"heading": "A Proof of Proposition 1", "text": "Proof For any w \u2208 Rp, level sets of w are characterized by an ordered partition (A1, . . . , Am) so that w is constant on each Aj , with value tj , j = 1, . . . ,m, and so that (tj) is a strictly decreasing sequence. We can now decompose minimization with respect to w using these ordered partitions and (tj).\nIn order to compute the convex envelope, we simply need to compute twice the Fenchel conjugate of the function we want to find the envelope of (see, e.g., [26, 27] for definitions and properties of Fenchel conjugates).\nLet s \u2208 Rp; we consider the function g : w 7\u2192 max\u03b1\u2208R F ({w > \u03b1}), and we compute its Fenchel conjugate:\ng\u2217(s) def = max\nw\u2208[0,1]p+R1V w\u22a4s\u2212 g(w),\n= max (A1,...,Am) partition\n{\nmax t1>\u00b7\u00b7\u00b7>tm, t1\u2212tm61\nm \u2211\nj=1\ntjs(Aj)\u2212 max j\u2208{1,...,m}\nF (A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Aj) } ,\n= max (A1,...,Am) partition\n{\nmax t1>\u00b7\u00b7\u00b7>tm, t1\u2212tm61\nm\u22121 \u2211\nj=1\n(tj \u2212 tj+1)s(A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Aj) + tms(V )\n\u2212 max j\u2208{1,...,m}\nF (A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Aj) } by integration by parts,\n= \u03b9s(V )=0(s) + max (A1,...,Am) partition\n{\nmax j\u2208{1,...,m\u22121} s(A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Aj)\u2212 max j\u2208{1,...,m}\nF (A1 \u222a \u00b7 \u00b7 \u00b7 \u222aAj) } ,\n= \u03b9s(V )=0(s) + max (A1,...,Am) partition\n{\nmax j\u2208{1,...,m\u22121} s(A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Aj)\u2212 max j\u2208{1,...,m\u22121}\nF (A1 \u222a \u00b7 \u00b7 \u00b7 \u222aAj) } ,\nwhere \u03b9s(V )=0 is the indicator function of the set {s(V ) = 0} (with values 0 or +\u221e). Note that maxj\u2208{1,...,m} F (A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Aj) = maxj\u2208{1,...,m\u22121} F (A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Aj) because F (V ) = 0. Let h(s) = \u03b9s(V )=0(s) + maxA\u2282V {s(A) \u2212 F (A)}. We clearly have g\u2217(s) > h(s), because we take a maximum over a larger set (consider m = 2). Moreover, for all partitions (A1, . . . , Am), if s(V ) = 0, maxj\u2208{1,...,m\u22121} s(A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Aj) 6 maxj\u2208{1,...,m\u22121}(h(s) + F (A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Aj)) = h(s) + maxj\u2208{1,...,m\u22121} F (A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Aj), which implies that g\u2217(s) 6 h(s). Thus g\u2217(s) = h(s). Moreover, we have, since f is invariant by adding constants and f is submodular,\nmax w\u2208[0,1]p+R1V w\u22a4s\u2212 f(w) = \u03b9s(V )=0(s) + max w\u2208[0,1]p {w\u22a4s\u2212 f(w)}\n= \u03b9s(V )=0(s) + max A\u2282V\n{s(A)\u2212 F (A)} = h(s),\nwhere we have used the fact that minimizing a submodular function is equivalent to minimizing its Lova\u0301sz extension on the unit hypercube. Thus f and g have the same Fenchel conjugates. The result follows from the convexity of f , using the fact the convex envelope is the Fenchel bi-conjugate [26, 27]."}, {"heading": "B Proof of Proposition 2", "text": "Proof Extreme points of U correspond to full-dimensional faces of B(F ). From Corollary 3.4.4 in [12], these facets are exactly the ones that correspond to sets A with the given conditions. These facets are defined as the intersection of {s(A) = F (A)} and {s(V ) = F (V )}, which leads to the desired result. Note that this is also a consequence of Prop. 3. Note that when F is symmetric, the second condition is equivalent to V \\A being inseparable for F ."}, {"heading": "C Proof of Proposition 3", "text": "Proof Given that the polyhedra U and B(F ) are polar to each other [13], the proposition follows from Theorem 3.43 in [12], where each of our three assumptions are equivalent to a corresponding one in Theorem 3.43 from [12]."}, {"heading": "D Proof of Proposition 4", "text": "We first start by a lemma, which follows common practice in sparse recovery (assume a certain sparsity pattern and check when it is actually optimal):\nLemma 1 (Optimality of lattice for proximal problem) The solution of the proximal problem in Eq. (1) corresponds to a lattice D if and only if v = (M\u22a4M)\u22121(M\u22a4z\u2212\u03bbt) satisfies the order relationships imposed by D and\n1 \u03bb (I \u2212M(M\u22a4M)\u22121M\u22a4)z +M(M\u22a4M)\u22121t \u2208 B(F ),\nwhere M \u2208 Rp\u00d7m is the indicator matrix of the partition \u03a0(D), and ti = F (A1 \u222a \u00b7 \u00b7 \u00b7 \u222aAi)\u2212F (A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ai\u22121), i = 1, . . . ,m.\nProof Any w \u2208 Rp belongs to a single face relative interior from Prop. 3, defined by a lattice D, i.e., w is constant on Ai with value vi (which implies that w = Mv) and such that vi > vj as soon as Ai < Aj . We assume a topological ordering of the sets Ai, i.e, Ai < Aj \u21d2 i > j. Since the Lova\u0301sz extension is linear for w in UD (and equal to t\u22a4v for w = Mv), the optimum over w can be found by minimizing with respect to v\n1 2 \u2016z \u2212Mv\u201622 + \u03bbt\u22a4v.\nWe thus get, by setting the gradient to zero:\nv = (M\u22a4M)\u22121(M\u22a4z \u2212 \u03bbt).\nOptimality conditions for w for Eq. (1) are that w\u2212z+\u03bbs = 0, for s \u2208 B(F ) and f(w) = w\u22a4s (these are obtained from general optimality conditions for functions defined as pointwise maxima [27]). Thus our candidate w = Mv is optimal if and only if Mv\u2212z+\u03bbs = w\u2212z+\u03bbs = 0 for (a) s \u2208 B(F ) and (b) f(w) = w\u22a4s. From Prop. 10 in [11], for (b) to be valid, s \u2208 B(F ) simply has to satisfy s(A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ai) = F (A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ai) for all i.\nNote that z \u2212Mv = (I \u2212M(M\u22a4M)\u22121M\u22a4)z + \u03bbM(M\u22a4M)\u22121t,\nand that for all i \u2208 {1, . . . ,m},\n1\u22a4Ai(I \u2212M(M\u22a4M)\u22121M\u22a4)z = \u03b4\u22a4i M\u22a4(I \u2212M(M\u22a4M)\u22121M\u22a4)z = 0,\nwhere \u03b4i is indicator vector of the singleton {i}. Moreover, we have\n1\u22a4AiM(M \u22a4M)\u22121t = ti = F (A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ai)\u2212 F (A1 \u222a \u00b7 \u00b7 \u00b7 \u222aAi\u22121),\nso that, if Bi = A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ai, [(I \u2212M(M\u22a4M)\u22121M\u22a4)z](Bi) = 0, [M(M\u22a4M)\u22121t](Bi) = F (Bi), for all i \u2208 {1, . . . ,m}. This implies that [\n1 \u03bb(z \u2212Mv)\n] (Ai) = ti, and thus [ 1 \u03bb (z \u2212Mv) ] (Bi) = F (Bi).\nThus, if (a) is satisfied, then (b) is always satisfied. Thus to check if a certain lattice leads to the optimal solution, we simply have to check that 1\u03bb(I\u2212M(M\u22a4M)\u22121M\u22a4)z+M(M\u22a4M)\u22121t \u2208 B(F ).\nWe now turn to the proof of Proposition 4.\nProof We show that when \u03bb increases, we move to a lattice which has to be merging some constant sets. Let us assume that a lattice D is optimal for a certain \u00b5. Then, from Lemma 1, we have\n1 \u00b5 (I \u2212M(M\u22a4M)\u22121M\u22a4)z +M(M\u22a4M)\u22121t \u2208 B(F ).\nMoreover, since from Prop. 3, Ai is separable for Ci 7\u2192 F (Bi\u22121\u222aCi)\u2212F (Bi\u22121), from the assumption of the proposition, we obtain:\n\u2200Ci \u2282 Ai, [M(M\u22a4M)\u22121t](Ci) = |Ci| |Ai| (F (Bi\u22121 \u222aAi)\u2212 F (Bi\u22121)) 6 F (Bi\u22121 \u222a Ci)\u2212 F (Bi\u22121),\nwhich implies, for all C \u2282 V :\n[M(M\u22a4M)\u22121t](C) = m \u2211\nj=1\n[M(M\u22a4M)\u22121t](C \u2229Ai) by modularity,\n6\nm \u2211\ni=1\n{ F (Bi\u22121 \u222a (C \u2229 Ai))\u2212 F (Bi\u22121) } from above,\n6\nm \u2211\ni=1\n{ F ((Bi\u22121 \u2229C) \u222a (C \u2229 Ai))\u2212 F (Bi\u22121 \u2229 C) } by submodularity,\n= m \u2211\ni=1\n{ F (Bi \u2229 C)\u2212 F (Bi\u22121 \u2229 C) } = F (C).\nThus, for any set C, we have for \u03bb > \u00b5 (which implies \u00b5\u03bb \u2208 [0, 1]),\n[ 1\n\u03bb (I \u2212M(M\u22a4M)\u22121M\u22a4)z +M(M\u22a4M)\u22121t\n]\n(C)\n= \u00b5\n\u03bb\n[ 1\n\u00b5 (I \u2212M(M\u22a4M)\u22121M\u22a4)z +M(M\u22a4M)\u22121t\n] (C) + (1\u2212 \u00b5 \u03bb ) [ M(M\u22a4M)\u22121t ] (C)\n6 \u00b5 \u03bb F (C) + (1\u2212 \u00b5 \u03bb )F (C) = F (C).\nThus the second condition in Lemma 1 is satisfied, thus it has to be the first one which is violated, leading to merging two constant sets.\nWe now show that for special cases, the condition in Eq. (2) is satisfied, and we also show when the condition in Eq. (3) of Theorem 1 is satisfied or not:\n\u2022 Cardinality-based functions: the condition in Eq. (2) is equivalent to\nh(|B|+ |A|)\u2212 h(|B|) |A| 6 h(|B|+ |C|)\u2212 h(|B|) |C| ,\nwhich is a consequence of the concavity of h. Moreover the condition in Eq. (3) is equivalent to\nh(|B|+ |C|) \u2212 h(|B|)\u2212 |C||A| [h(|B|+ |A|)\u2212 h(|B|)] > \u03b7min { |C| |A| , 1\u2212 |C| |A| } .\nFor h(t) = t(p\u2212 t), this is equivalent to\n|A|(|C| \u2212 |A|) > \u03b7min { |C| |A| , 1\u2212 |C| |A| } ,\nwhich is true as soon as \u03b7 6 |A|/2.\n\u2022 One-dimensional total variation: we assume that we have a chain graph. Note that A must be an interval and that B only enters the problem if one of its elements is a neighbor of one of the two extreme elements of A. We thus have eight cases, depending on the three possibilities for these two neighbors of A (in B, in V \\B, or no neighbor, i.e., end of the chain). We consider all 8 cases, where C is a non trivial subset of A, and compute a lower bound on\nF (B \u222a C)\u2212 F (B)\u2212 |C||A| [F (B \u222a A)\u2212 F (B)].\n\u2013 left: B, right: B. F (B) = 2, F (B \u222a A) = 0, F (C \u222aB) > 2. Bound= 2 |C||A| \u2013 left: B, right: V \\B. F (B) = 1, F (B \u222aA) = 1, F (C \u222aB) > 1. Bound= 0 \u2013 left: B, right: none. F (B) = 1, F (B \u222a A) = 0, F (C \u222aB) > 1. Bound= |C||A| \u2013 left: V \\B, right: B. F (B) = 1, F (B \u222aA) = 1, F (C \u222aB) > 1. Bound= 0 \u2013 left: V \\B, right: V \\B. F (B) = 0, F (B \u222a A) = 2, F (C \u222aB) > 2. Bound= 2\u2212 2 |C||A| \u2013 left: V \\B, right: none. F (B) = 0, F (B \u222a A) = 1, F (C \u222aB) > 1. Bound= 1\u2212 |C||A| \u2013 left: none, right: B. F (B) = 2, F (B \u222a A) = 0, F (C \u222aB) > 2. Bound= |C||A| \u2013 left: none, right: V \\B. F (B) = 1, F (B \u222a A) = 0, F (C \u222aB) > 1. Bound= |C||A| \u2013 left: none, right: none. F (B) = 0, F (B \u222a A) = 0, F (C \u222aB) > 1. Bound= 1.\nConsidering all cases, we get a lower bound of zero, which shows that the paths are agglomerative. However, there are two cases where no strictly positive lower bounds are possible, namely when the two extremities of A have respective neighbors in B and V \\B. Given that B is a set of higher values for the parameters and V \\(A \u222a B) is a set of lower values, this is exactly a staircase. When there is no such staircase, we get a lower bound of min{|A|/|C|, 1\u2212 |A|/|C|}, hence \u03b7 = 1."}, {"heading": "E Proof of Proposition 5", "text": "Proof We denote by w the unique mininizer of 12\u2016w\u2212 z\u201622 + f(w) and s the associated dual vector in B(F ). The optimality conditions are w \u2212 z + s = 0, and f(w) = w\u22a4s (again from optimality conditions for pointwise maxima).\nWe assume that w takes distinct values v1, . . . , vm on the sets A1, . . . , Am. We define t as tk = sign(wk)(|wk|\u2212\u03bb)+ (which is the unique minimizer of 12\u2016w\u2212 t\u201622+\u03bb\u2016t\u20161). The constant sets of t are Aj , for j such that |vj | > \u03bb and zero for the union of all Aj \u2019s such that |vj | 6 \u03bb. Since t is obtained by soft-thresholding w, which corresponds to \u21131-proximal problem, we have that t\u2212w+\u03bbq = 0 with \u2016q\u2016\u221e 6 1 and q\u22a4t = \u2016t\u20161. By combining these two equalities, with have t \u2212 z + s + \u03bbq = 0 with \u2016q\u2016\u221e 6 1, q\u22a4t = \u2016t\u20161 and s \u2208 B(F ). The only remaining element to show that t is optimal for the full problem is that f(t) = s\u22a4t. This is true since the level sets of w are finer than the ones of t (i.e., it is obtained by grouping some values of w), with no change of ordering [11]."}, {"heading": "F Proof of Proposition 6", "text": "Proof From Lemma 1, the solution has to correspond to a lattice D and we only have to show that with probability one, the vector v = (M\u22a4M)\u22121(M\u22a4z \u2212 \u03bbt) has distinct components, which is straightforward because it has an absolutely continuous density with respect to the Lebesgue measure."}, {"heading": "G Proof of Theorem 1", "text": "Proof From Lemma 1, in order to correspond to the same lattice D, we simply need that (a) v = (M\u22a4M)\u22121(M\u22a4z \u2212 \u03bbt) satisfies the order relationships imposed by D and that (b)\n1 \u03bb (I \u2212M(M\u22a4M)\u22121M\u22a4)z +M(M\u22a4M)\u22121t \u2208 B(F ).\nCondition (a) is satisfied as soon as \u2016w \u2212 w\u2217\u2016\u221e 6 \u03bd, which is implied by\n\u03c3\u2016(M\u22a4M)\u22121M\u22a4\u03b5\u2016\u221e 6 \u03bd/4 and \u2016\u03bb(M\u22a4M)\u22121t\u2016\u221e 6 \u03bd/4. (6)\nThe second condition in Eq. (6) is met by assumption, while the first one leads to the sufficient conditions \u2200j, |\u03b5(Aj)| 6 \u03bd|Aj |/4\u03c3, leading by the union bound to the probabilities \u2211m j=1 exp (\n\u2212 \u03bd2|Aj| 32\u03c32 ) .\nFollowing the same reasoning than in the proof of Prop. 4, condition (b) is satisfied as soon as for all j \u2208 {1, . . . ,m}, and all Cj \u2282 Aj ,\n[ \u03c3 1\n\u03bb (I \u2212M(M\u22a4M)\u22121M\u22a4)\u03b5\n]\n(Cj) 6 \u03b7j min { |Cj | |Aj | , 1\u2212 |Cj ||Aj | } .\nIndeed, this implies that for all j,\n[ 1\n\u03bb (I \u2212M(M\u22a4M)\u22121M\u22a4)z +M(M\u22a4M)\u22121t\n]\n(Cj)\n= [\u03c3\n\u03bb (I \u2212M(M\u22a4M)\u22121M\u22a4)\u03b5+M(M\u22a4M)\u22121t\n]\n(Cj)\n6 \u03b7j min { |Cj | |Aj | , 1\u2212 |Cj ||Aj | } + |Cj | |Aj | (F (Bj\u22121 \u222aAi)\u2212 F (Bj\u22121)) 6 F (Bj\u22121 \u222a Cj)\u2212 F (Bj\u22121),\nwhich leads to [ 1 \u03bb(I\u2212M(M\u22a4M)\u22121M\u22a4)z+M(M\u22a4M)\u22121t ] \u2208 B(F ) using the sequence of inequalities used in the proof of Prop. 4.\nFrom Lemma 2 below, we thus get the probability 2 \u2211m j=1 |Aj | exp ( \u2212 \u03bb 2\u03b72j\n2\u03c32|Aj|2\n)\n.\nLemma 2 For F (A) = min { |A| p , 1\u2212 |A| p } , and s normal with mean zero and variance I\u2212 1V 1 \u22a4 V\np , we have:\nP\n(\nmax A\u2282V,A 6=\u2205,A 6=V\ns(A) F (A) > t ) 6 2p exp ( \u2212 t 2 2p2 ) .\nProof Since F depends on uniquely on the cardinality |A| and is symmetric we have, with s\u0303 \u2208 Rp the sorted (in descending order) components of s, and h(a) = min{a/p, 1\u2212 a/p}:\nP (\nmax A\u2282V,A 6=\u2205,A 6=V\ns(A) F (A) > t )\n= P (\nmax k\u2208{1,...,p\u22121} s\u0303({1, . . . , k}) h(k) > t )\n6 P (\nmax k\u2208{1,...,\u230ap/2\u230b\u22121} s\u0303({1, . . . , k}) h(k) > t ) + P ( max k\u2208{\u230ap/2\u230b,...,p\u22121} s\u0303({1, . . . , k}) h(k) > t )\n6 2P (\nmax k\u2208{1,...,\u230ap/2\u230b\u22121} s\u0303({1, . . . , k}) k/p > t ) because of symmetry due to the covariance of s\n6 2P (\nmax k\u2208{1,...,\u230ap/2\u230b\u22121} s\u0303({1, . . . , k}) k/p > t )\n6 2P (\nmax k\u2208{1,...,p}\nsk > t/p )\n6 2p exp(\u2212t2/2p2).\nWe now consider the three special cases:\n\u2022 One-dimensional total variation: without the staircase effect, as shown in Appendix D, we have \u03b7j = 1. Moreover, |F (Bj)\u2212 F (Bj\u22121)| 6 2, and thus Eq. (5) leads to \u03bb 6 \u03bd8 minj |Aj |."}], "references": [{"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "Journal of Machine Learning Research, 7:2541\u20132563", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "A unified framework for highdimensional analysis of M-estimators with decomposable regularizers", "author": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Adv. NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Structured sparsity-inducing norms through submodular functions", "author": ["F. Bach"], "venue": "Adv. NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparsity and smoothness via the fused Lasso", "author": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "J. Roy. Stat. Soc. B, 67(1):91\u2013108", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "On total variation minimization and surface evolution using parametric maximum flows", "author": ["A. Chambolle", "J. Darbon"], "venue": "International Journal of Computer Vision, 84(3):288\u2013307", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Trans. PAMI, 23(11):1222\u20131239", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Catching change-points with Lasso", "author": ["Z. Harchaoui", "C. L\u00e9vy-Leduc"], "venue": "Adv. NIPS, 20", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast detection of multiple change-points shared by many signals using group LARS", "author": ["J.-P. Vert", "K. Bleakley"], "venue": "Adv. NIPS, 23", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparsistent learning of varying-coefficient models with structural changes", "author": ["M. Kolar", "L. Song", "E. Xing"], "venue": "Adv. NIPS, 22", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Simultaneous regression shrinkage", "author": ["H.D. Bondell", "B.J. Reich"], "venue": "variable selection, and supervised clustering of predictors with oscar. Biometrics, 64(1):115\u2013123", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Convex analysis and optimization with submodular functions: a tutorial", "author": ["F. Bach"], "venue": "Technical Report 00527714, HAL", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Submodular Functions and Optimization", "author": ["S. Fujishige"], "venue": "Elsevier", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": "Princeton University Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Clusterpath: an algorithm for clustering using convex fusion penalties", "author": ["T. Hocking", "A. Joulin", "F. Bach", "J.-P. Vert"], "venue": "Proc. ICML", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, 2(1):183\u2013202", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Two algorithms for maximizing a separable concave function over a polymatroid feasible region", "author": ["H. Groenevelt"], "venue": "European Journal of Operational Research, 54(2):227\u2013236", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1991}, {"title": "A faster strongly polynomial time algorithm for submodular function minimization", "author": ["J.B. Orlin"], "venue": "Mathematical Programming, 118(2):237\u2013251", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Minimizing symmetric submodular functions", "author": ["M. Queyranne"], "venue": "Mathematical Programming, 82(1):3\u201312", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "A fast parametric maximum flow algorithm and applications", "author": ["G. Gallo", "M.D. Grigoriadis", "R.E. Tarjan"], "venue": "SIAM Journal on Computing, 18(1):30\u201355", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1989}, {"title": "A path algorithm for the fused Lasso signal approximator", "author": ["H. Hoefling"], "venue": "Technical Report 0910.0526v1, arXiv", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning Research, 11:19\u201360", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Properties and refinements of the fused Lasso", "author": ["A. Rinaldo"], "venue": "Ann. Stat., 37(5):2922\u20132952", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "The TVL1 model: A geometric point of view", "author": ["V. Duval", "J.-F. Aujol", "Y. Gousseau"], "venue": "Multiscale Modeling and Simulation, 8(1):154\u2013189", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J.D.M. Rennie", "T.S. Jaakkola"], "venue": "Adv. NIPS 17", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, 73(3):243\u2013272", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Convex Analysis and Nonlinear Optimization: Theory and Examples", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": "Springer", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": ", [1, 2, 3] and references therein).", "startOffset": 2, "endOffset": 11}, {"referenceID": 1, "context": ", [1, 2, 3] and references therein).", "startOffset": 2, "endOffset": 11}, {"referenceID": 2, "context": ", [1, 2, 3] and references therein).", "startOffset": 2, "endOffset": 11}, {"referenceID": 3, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 113, "endOffset": 119}, {"referenceID": 4, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 113, "endOffset": 119}, {"referenceID": 5, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 175, "endOffset": 181}, {"referenceID": 4, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 175, "endOffset": 181}, {"referenceID": 6, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 215, "endOffset": 224}, {"referenceID": 7, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 215, "endOffset": 224}, {"referenceID": 8, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 215, "endOffset": 224}, {"referenceID": 9, "context": "Another example is the \u201cOscar\u201d penalty which induces automatic grouping of the features [10].", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "In this paper, we follow the approach of [3], who designed sparsity-inducing norms based on non-decreasing submodular functions, as a convex approximation to imposing a specific prior on the supports of the predictors.", "startOffset": 41, "endOffset": 44}, {"referenceID": 10, "context": ", [11] and a short review in Section 2)", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": "\u2212 We provide unified algorithms in Section 5, such as proximal operators, which are based on a sequence of submodular function minimizations (SFMs), when such SFMs are efficient, or by adapting the generic slower approach of [3] otherwise.", "startOffset": 225, "endOffset": 228}, {"referenceID": 0, "context": "\u2212 We derive unified theoretical guarantees for level set recovery in Section 6, showing that even in the absence of correlation between predictors, level set recovery is not always guaranteed, a situation which is to be contrasted with traditional support recovery situations [1, 3].", "startOffset": 276, "endOffset": 282}, {"referenceID": 2, "context": "\u2212 We derive unified theoretical guarantees for level set recovery in Section 6, showing that even in the absence of correlation between predictors, level set recovery is not always guaranteed, a situation which is to be contrasted with traditional support recovery situations [1, 3].", "startOffset": 276, "endOffset": 282}, {"referenceID": 11, "context": ", [12], and, for a review with proofs derived from classical convex analysis, see, e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": ", [11].", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": ", [11] for this particular formulation).", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "We denote by B(F ) = {s \u2208 R, \u2200A \u2282 V, s(A) 6 F (A), s(V ) = F (V )} the base polyhedron [12], where we use the notation s(A) = \u2211", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "One important result in submodular analysis is that if F is a submodular function, then we have a representation of f as a maximum of linear functions [12, 11], i.", "startOffset": 151, "endOffset": 159}, {"referenceID": 10, "context": "One important result in submodular analysis is that if F is a submodular function, then we have a representation of f as a maximum of linear functions [12, 11], i.", "startOffset": 151, "endOffset": 159}, {"referenceID": 12, "context": ", [13] for definitions and properties of polar sets).", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": ", if A and B are tight, then so are A \u222a B and A \u2229 B [12, 11].", "startOffset": 52, "endOffset": 60}, {"referenceID": 10, "context": ", if A and B are tight, then so are A \u222a B and A \u2229 B [12, 11].", "startOffset": 52, "endOffset": 60}, {"referenceID": 2, "context": "Thus, contrary to the non-decreasing case [3], our regularizers are not norms.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "We now show that the Lov\u00e1sz extension is the convex envelope of a certain combinatorial function which does depend on all levets sets {w > \u03b1} of w \u2208 R (see proof in supplementary material): Proposition 1 (Convex envelope) The Lov\u00e1sz extension f(w) is the convex envelope of the function w 7\u2192 max\u03b1\u2208R F ({w > \u03b1}) on the set [0, 1] + R1V = {w \u2208 R, maxk\u2208V wk \u2212mink\u2208V wk 6 1}.", "startOffset": 322, "endOffset": 328}, {"referenceID": 2, "context": "Note the difference with the result of [3]: we consider here a different set on which we compute the convex envelope ([0, 1]+R1V instead of [\u22121, 1]), and not a function of the support of w, but of all its level sets.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "Note the difference with the result of [3]: we consider here a different set on which we compute the convex envelope ([0, 1]+R1V instead of [\u22121, 1]), and not a function of the support of w, but of all its level sets.", "startOffset": 118, "endOffset": 124}, {"referenceID": 11, "context": "of J , then it has to be in J (see [12] for more details, and an example in Figure 2).", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "Potentially interesting examples which are beyond the scope of this paper are mutual information, or functions of eigenvalues of submatrices [3].", "startOffset": 141, "endOffset": 144}, {"referenceID": 3, "context": "In Figure 5 (right plot), we give an example of the usual chain graph, leading to the one-dimensional total variation [4, 5].", "startOffset": 118, "endOffset": 124}, {"referenceID": 4, "context": "In Figure 5 (right plot), we give an example of the usual chain graph, leading to the one-dimensional total variation [4, 5].", "startOffset": 118, "endOffset": 124}, {"referenceID": 5, "context": "Note that these functions can be extended to cuts in hypergraphs, which may have interesting applications in computer vision [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "By partial minimization, we obtain so-called regular functions [6, 5].", "startOffset": 63, "endOffset": 69}, {"referenceID": 4, "context": "By partial minimization, we obtain so-called regular functions [6, 5].", "startOffset": 63, "endOffset": 69}, {"referenceID": 2, "context": "While these examples do not provide significantly different behaviors for the non-decreasing submodular functions explored by [3] (i.", "startOffset": 126, "endOffset": 129}, {"referenceID": 13, "context": "This function has been extended in [14] by considering situations where each wj is a vector, instead of a scalar, and replacing the absolute value |wi \u2212 wj | by any norm \u2016wi \u2212 wj\u2016, leading to convex formulations for clustering.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "This may have applications to multivariate outlier detection by considering extensions similar to [14].", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": ", [15]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": ", [15]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "In this paper, we use the method \u201cISTA\u201d and its accelerated variant \u201cFISTA\u201d [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "More precisely, the minimum of A 7\u2192 \u03bbF (A)\u2212 z(A) may be obtained by selecting negative components of the solution of a single proximal problem [12, 11].", "startOffset": 143, "endOffset": 151}, {"referenceID": 10, "context": "More precisely, the minimum of A 7\u2192 \u03bbF (A)\u2212 z(A) may be obtained by selecting negative components of the solution of a single proximal problem [12, 11].", "startOffset": 143, "endOffset": 151}, {"referenceID": 15, "context": "Alternatively, the solution of the proximal problem may be obtained by a sequence of at most p submodular function minimizations of the form A 7\u2192 \u03bbF (A) \u2212 z(A), by a decomposition algorithm adapted from [16], and described in [11].", "startOffset": 203, "endOffset": 207}, {"referenceID": 10, "context": "Alternatively, the solution of the proximal problem may be obtained by a sequence of at most p submodular function minimizations of the form A 7\u2192 \u03bbF (A) \u2212 z(A), by a decomposition algorithm adapted from [16], and described in [11].", "startOffset": 226, "endOffset": 230}, {"referenceID": 16, "context": "However, it may be too slow for practical purposes, as the best generic algorithm has complexity O(p) [17].", "startOffset": 102, "endOffset": 106}, {"referenceID": 5, "context": ", [6, 5]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 4, "context": ", [6, 5]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 18, "context": "For proximal methods, we need in fact to solve an instance of a parametric max-flow problem, which may be done using other efficient dedicated algorithms [19, 5] than the decomposition algorithm derived from [16].", "startOffset": 154, "endOffset": 161}, {"referenceID": 4, "context": "For proximal methods, we need in fact to solve an instance of a parametric max-flow problem, which may be done using other efficient dedicated algorithms [19, 5] than the decomposition algorithm derived from [16].", "startOffset": 154, "endOffset": 161}, {"referenceID": 15, "context": "For proximal methods, we need in fact to solve an instance of a parametric max-flow problem, which may be done using other efficient dedicated algorithms [19, 5] than the decomposition algorithm derived from [16].", "startOffset": 208, "endOffset": 212}, {"referenceID": 2, "context": ", beyond cuts and cardinality-based functions), we can follow [3]: since f(w) is expressed as a minimum of linear functions, the problem reduces to the projection on the polytope B(F ), for which we happen to be able to easily maximize linear functions (using the greedy algorithm described in Section 2).", "startOffset": 62, "endOffset": 65}, {"referenceID": 11, "context": "This can be tackled efficiently by the minimum-norm-point algorithm [12], which iterates between orthogonal projections on affine subspaces and the greedy algorithm for the submodular function.", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "Note that even in the case of symmetric submodular functions, where more efficient algorithms in O(p) for submodular function minimization (SFM) exist [18], the minimization of functions of the form \u03bbF (A) \u2212 z(A) is provably as hard as general SFM [18].", "startOffset": 151, "endOffset": 155}, {"referenceID": 17, "context": "Note that even in the case of symmetric submodular functions, where more efficient algorithms in O(p) for submodular function minimization (SFM) exist [18], the minimization of functions of the form \u03bbF (A) \u2212 z(A) is provably as hard as general SFM [18].", "startOffset": 248, "endOffset": 252}, {"referenceID": 11, "context": "Interestingly, when used for submodular function minimization (SFM), the minimum-norm-point algorithm has no complexity bound but is empirically faster than algorithms with such bounds [12].", "startOffset": 185, "endOffset": 189}, {"referenceID": 6, "context": "4 are satisfied by (a) all submodular set-functions that only depend on the cardinality, and (b) by the one-dimensional total variation\u2014we thus recover and extend known results from [7, 20, 14].", "startOffset": 182, "endOffset": 193}, {"referenceID": 19, "context": "4 are satisfied by (a) all submodular set-functions that only depend on the cardinality, and (b) by the one-dimensional total variation\u2014we thus recover and extend known results from [7, 20, 14].", "startOffset": 182, "endOffset": 193}, {"referenceID": 13, "context": "4 are satisfied by (a) all submodular set-functions that only depend on the cardinality, and (b) by the one-dimensional total variation\u2014we thus recover and extend known results from [7, 20, 14].", "startOffset": 182, "endOffset": 193}, {"referenceID": 3, "context": "Following [4], we may add the l1-norm \u2016w\u20161 for additional sparsity of w (on top of shaping its level sets).", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "The following proposition extends the result for the one-dimensional total variation [4, 21] to all submodular functions and their Lov\u00e1sz extensions: Proposition 5 (Proximal problem for l1-penalized problems) The unique minimizer of 1 2\u2016w\u2212 z\u20162+ f(w) +\u03bb\u2016w\u20161 may be obtained by soft-thresholding the minimizers of 12\u2016w\u2212 z\u20162+ f(w).", "startOffset": 85, "endOffset": 92}, {"referenceID": 20, "context": "The following proposition extends the result for the one-dimensional total variation [4, 21] to all submodular functions and their Lov\u00e1sz extensions: Proposition 5 (Proximal problem for l1-penalized problems) The unique minimizer of 1 2\u2016w\u2212 z\u20162+ f(w) +\u03bb\u2016w\u20161 may be obtained by soft-thresholding the minimizers of 12\u2016w\u2212 z\u20162+ f(w).", "startOffset": 85, "endOffset": 92}, {"referenceID": 0, "context": "(3) is the equivalent of the support recovery of the Lasso [1] or its extensions [3].", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "(3) is the equivalent of the support recovery of the Lasso [1] or its extensions [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 21, "context": "Note that we could also derive general results when an additional l1-penalty is used, thus extending results from [22].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": ", [23] and the supplementary material).", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "This indicates that the noise variance \u03c3 should be small compared to 1/p, which is not satisfactory and would be corrected with the weighting schemes proposed in [14].", "startOffset": 162, "endOffset": 166}, {"referenceID": 23, "context": ", by considering related adapted concave penalties to enhance sparsity-inducing capabilities, or by extending some of the concepts for norms of matrices, with potential applications in matrix factorization [24] or multi-task learning [25].", "startOffset": 206, "endOffset": 210}, {"referenceID": 24, "context": ", by considering related adapted concave penalties to enhance sparsity-inducing capabilities, or by extending some of the concepts for norms of matrices, with potential applications in matrix factorization [24] or multi-task learning [25].", "startOffset": 234, "endOffset": 238}, {"referenceID": 25, "context": ", [26, 27] for definitions and properties of Fenchel conjugates).", "startOffset": 2, "endOffset": 10}, {"referenceID": 26, "context": ", [26, 27] for definitions and properties of Fenchel conjugates).", "startOffset": 2, "endOffset": 10}, {"referenceID": 0, "context": "Let s \u2208 R; we consider the function g : w 7\u2192 max\u03b1\u2208R F ({w > \u03b1}), and we compute its Fenchel conjugate: g(s) def = max w\u2208[0,1]+R1V ws\u2212 g(w),", "startOffset": 120, "endOffset": 125}, {"referenceID": 0, "context": "Moreover, we have, since f is invariant by adding constants and f is submodular, max w\u2208[0,1]+R1V ws\u2212 f(w) = \u03b9s(V )=0(s) + max w\u2208[0,1]p {ws\u2212 f(w)} = \u03b9s(V )=0(s) + max A\u2282V {s(A)\u2212 F (A)} = h(s), where we have used the fact that minimizing a submodular function is equivalent to minimizing its Lov\u00e1sz extension on the unit hypercube.", "startOffset": 87, "endOffset": 92}, {"referenceID": 0, "context": "Moreover, we have, since f is invariant by adding constants and f is submodular, max w\u2208[0,1]+R1V ws\u2212 f(w) = \u03b9s(V )=0(s) + max w\u2208[0,1]p {ws\u2212 f(w)} = \u03b9s(V )=0(s) + max A\u2282V {s(A)\u2212 F (A)} = h(s), where we have used the fact that minimizing a submodular function is equivalent to minimizing its Lov\u00e1sz extension on the unit hypercube.", "startOffset": 128, "endOffset": 133}, {"referenceID": 25, "context": "The result follows from the convexity of f , using the fact the convex envelope is the Fenchel bi-conjugate [26, 27].", "startOffset": 108, "endOffset": 116}, {"referenceID": 26, "context": "The result follows from the convexity of f , using the fact the convex envelope is the Fenchel bi-conjugate [26, 27].", "startOffset": 108, "endOffset": 116}, {"referenceID": 11, "context": "4 in [12], these facets are exactly the ones that correspond to sets A with the given conditions.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "C Proof of Proposition 3 Proof Given that the polyhedra U and B(F ) are polar to each other [13], the proposition follows from Theorem 3.", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "43 in [12], where each of our three assumptions are equivalent to a corresponding one in Theorem 3.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "43 from [12].", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "(1) are that w\u2212z+\u03bbs = 0, for s \u2208 B(F ) and f(w) = ws (these are obtained from general optimality conditions for functions defined as pointwise maxima [27]).", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "10 in [11], for (b) to be valid, s \u2208 B(F ) simply has to satisfy s(A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ai) = F (A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ai) for all i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "Thus, for any set C, we have for \u03bb > \u03bc (which implies \u03bc\u03bb \u2208 [0, 1]),", "startOffset": 59, "endOffset": 65}, {"referenceID": 10, "context": ", it is obtained by grouping some values of w), with no change of ordering [11].", "startOffset": 75, "endOffset": 79}], "year": 2011, "abstractText": "We consider a class of sparsity-inducing regularization terms based on submodular functions. While previous work has focused on non-decreasing functions, we explore symmetric submodular functions and their Lov\u00e1sz extensions. We show that the Lov\u00e1sz extension may be seen as the convex envelope of a function that depends on level sets (i.e., the set of indices whose corresponding components of the underlying predictor are greater than a given constant): this leads to a class of convex structured regularization terms that impose prior knowledge on the level sets, and not only on the supports of the underlying predictors. We provide a unified set of optimization algorithms, such as proximal operators, and theoretical guarantees (allowed level sets and recovery conditions). By selecting specific submodular functions, we give a new interpretation to known norms, such as the total variation; we also define new norms, in particular ones that are based on order statistics with application to clustering and outlier detection, and on noisy cuts in graphs with application to change point detection in the presence of outliers.", "creator": "LaTeX with hyperref package"}}}