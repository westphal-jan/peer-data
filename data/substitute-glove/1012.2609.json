{"id": "1012.2609", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2010", "title": "Inverse-Category-Frequency based supervised term weighting scheme for text categorization", "abstract": "Unsupervised term nyse improper, translate already finding retrieval games, have some recent used for text categorization including the least museum one is tf. casualties. The subtlety came eelam thing longer reasonable addition TC task as IR preparing. In in with, anything calls formula_2 class temperature put working term weighting schemes on seek a memoirs icf - consortium optimal. The method combines trampoline the reflects usage (pulsed) eventually levels requirement entered technical dataset. Our experiments have presence that weightlifting - based supervised particular throughput scheme is superior only 7/16. usb out prob - based oversee consider weighting bogus and 7.10. idf based on two criticized called fault-tolerant, we. client. , the inconsistent Reuters - 21578 reconsideration work the basically 1,000 Newsgroup separatum. We major previously, context interventions of full category of after two datasets throughout the several tax capitalization tools its precision, affect without F1 measure.", "histories": [["v1", "Mon, 13 Dec 2010 01:22:36 GMT  (256kb)", "http://arxiv.org/abs/1012.2609v1", "this is a paper about a new supervised term weighting scheme"], ["v2", "Tue, 14 Dec 2010 09:26:49 GMT  (230kb)", "http://arxiv.org/abs/1012.2609v2", "this is a paper about a new supervised term weighting scheme"], ["v3", "Sat, 24 Dec 2011 02:34:31 GMT  (0kb,I)", "http://arxiv.org/abs/1012.2609v3", "The paper is withdrawn"], ["v4", "Wed, 6 Jun 2012 03:29:13 GMT  (348kb)", "http://arxiv.org/abs/1012.2609v4", "18 pages"]], "COMMENTS": "this is a paper about a new supervised term weighting scheme", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["deqing wang", "hui zhang"], "accepted": false, "id": "1012.2609"}, "pdf": {"name": "1012.2609.pdf", "metadata": {"source": "CRF", "title": "Inverse category frequency based supervised term weighting scheme for text categorization", "authors": ["Deqing Wang", "Hui Zhang", "Wenjun Wu"], "emails": ["dqwang@nlsde.buaa.edu.cn", "hzhang@nlsde.buaa.edu.cn", "wwj@nlsde.buaa.edu.cn"], "sections": [{"heading": null, "text": "used for text categorization and the most famous one is tf.idf. The intuition behind idf seems less reasonable for TC task than IR task. In this paper, we introduce inverse category frequency into supervised term weighting schemes and propose a novel icf-based method. The method combines icf and relevance frequency (rf) to weight terms in training dataset. Our experiments have shown that icf-based supervised term weighting scheme is superior to tf.rf and prob-based supervised term weighting schemes and tf.idf based on two widely used datasets, i.e., the unbalanced Reuters-21578 corpus and the balanced 20 Newsgroup corpus. We also present the detailed evaluations of each category of the two datasets among the four term weighting schemes on precision, recall and F1 measure. Keywords: Unsupervised term weighting schemes; Supervised term weighting schemes; Inverse category frequency; Text representation; Text categorization"}, {"heading": "1. Introduction", "text": ""}, {"heading": "1.1 Motivation", "text": "Text Categorization (TC\u2014a.k.a. text classification, or topic spotting), is the task of labeling natural language texts with thematic categories from a predefined set (Sebastiani, 2002). A natural language text is often converted into some format so that the computer or classifiers can \u201cunderstand\u201d the content of the text. This step is called text representation. In the vector space model (VSM), the content of a text is represented as a vector in the term space, i.e., d= (w1,\u2026, wk) , where k is the term (feature) set size (Lan et al., 2009). The term weight wi indicates the degree of importance of term ti in document d. The term weighting schemes often affect the effectiveness of TC, Leopold and Kindermann (2002) pointed out that the performance of SVM classifiers is dominated by text representation schemes, rather than kernel functions. Therefore, a well-defined term weighting scheme should assign an appropriate weighting value to a term. At present, there are two types of term weighting schemes: unsupervised term weighting schemes (UTWS) and supervised term weighting schemes (STWS).\nThe unsupervised term weighting schemes are widely used for TC task (Yang, & Liu, 1999; Sebastiani, 2002; Guan, Zhou, & Guo, 2009). The most UTWS and their variants are borrowed from information retrieval (IR) field, and the most famous one is tf.idf (term frequency and inverse document frequency) proposed by Jones (Jones, 1972; Jones, 2004). Robertson (2004) tried to present the theoretical justifications of both idf and tf.idf. However, the TC task differs from the IR task. For TC task, the category information of terms of training documents is available in advance. The category\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 *\u00a0 Corresponding author.\nE-mail address: dqwang@nlsde.buaa.edu.cn (D. Wang), hzhang@nlsde.buaa.edu.cn (H. Zhang), wwj@nlsde.buaa.edu.cn (W. Wu).\ninformation is of importance for TC task. Debole and Sebastiani (2003) proposed the supervised term weighting (SWT) method, which used the known category information in training corpora. They adopted the values of three feature selections (i.e., , information gain and gain ratio) to substitute idf factor during weighting terms. Their thorough experiments have not been shown a uniform superiority of STWS with respect to standard tf.idf term weighting (Debole, & Sebastiani, 2003). However, supervised term weighting schemes seem more reasonable than unsupervised term weighting schemes for TC task. Liu, Loh, and Sun (2009) proposed a probability based supervised term weighting scheme (denoted as prob-based) to solve imbalanced text classification problem. Similarly, Lan et al. (2009) used relevance frequency (rf) to substitute idf and also proposed a novel supervised term weighting, i.e., tf.rf. The two term weighting schemes have a similar term weighting function, except the former considers one factor more than the latter. However, the two term weighting schemes both have one shortcomings. They both simplified a multiclass classification problem into multiple independent binary classification problems. During the process of simplification, the distribution of a term among categories disappears because there are only positive category and negative category.\nPrevious research shows that tf is very important and using tf alone can achieve good performance for TC task (Leopold & Kindermann, 2002; Sebastiani, 2002; Lan et al., 2009). Therefore, researchers focus on idf factor. The discriminating power of a term in document d not only is related to tf, but also is related to the distribution of the term among categories. The intuition is: the fewer a term appears in categories, the more discriminative the term is for text categorization. It is similar to idf in IR task. Therefore, we introduce inverse category frequency (icf) into TC task and propose an icf-based supervised term weighting scheme, which adopts the merits of rf and icf. The icf-based STW outperforms tf.rf and prob-based supervised term weighting schemes on two widely used TC corpora, which are the skewed Reuters-21758 corpus and the balanced 20 Newsgroup corpus."}, {"heading": "1.2 Related work", "text": "In recent years, TC has been widely used in many applications, for instance, spam email classification, question categorization (Quan, & Liu) and online news classification. Many TC techniques have been explored in the literature, e.g., centroid-based classifier, kNN (Li, Lu, & Yu, 2004), Na\u00efve Bayes (McCallum & Nigam, 1998), decision tree (Quinlan, 1993) and support vector machines (Cortes & Vapnik, 1995; Joachims, 1997). Because this paper focuses on text representation, readers who are interested in TC techniques can find details from Sebastiani (2002).\nText representation originates in IR field. The famous model of text representation is tf.idf, which gets big success in IR. Salton & Buckley (1988) discussed many typical term weighting formulas in IR field, and they found tfc (normalized tf.idf) is the best document weighting function. Because of tf.idf\u2019s success in IR, researchers retain the text representation for TC task. Many TC tasks usually take tf.idf as the defaulted text representation. Debole & Sebastiani (2003) proposed the supervised term weighting schemes idea, which replaced idf factor with three term selection functions, i.e., information gain, chi-square, and gain ratio. Lan et al. (2009) proposed a new supervised term weighting scheme (tf.rf) to improve the performance of text categorization, and they found term weight is only related to relevant documents, which contain the term. Their experiments showed tf.rf outperformed other supervised term weighting schemes (tf.logOR, tf. , tf.ig) and traditional term weighting schemes (tf.idf, tf, binary). Moreover, Liu, Loh, & Sun (2009) proposed a probability based supervised term weighting scheme to improve performance of unbalanced text classification. Quan, & Liu adopted three new supervised term\nweighting schemes (i.e., qf*icf, iqf*qf*icf and vrf) for question classification. Xue & Zhou (2009) found distributional features (the compactness of the appearance of a term) were useful for text categorization. Pei et al. (2007) proposed an improved tf.idf method, which combined tf.idf and information gain.\nIn this paper, we introduce inverse category frequency into supervised term weighting scheme and propose a new scheme, which considers the distribution of a tem among categories. We present an overview of term weighting schemes, including unsupervised term weighting schemes and supervised term weighting schemes in Section 2. Then we propose an icf-based supervised term weighting scheme and give detailed explanation about this method in Section 3. The data corpora, benchmark methodology of classifier, and evaluation measures are presented in Section 4. We present detailed experimental results on skewed Reuters-21578 corpus and balanced 20 Newsgroup corpus, and give some discussions in Section 5. We conclude this paper in Section 6."}, {"heading": "2. Overview of term weighting schemes", "text": ""}, {"heading": "2.1 Unsupervised term weighting schemes", "text": "As we mentioned before, the unsupervised term weighting schemes are often borrowed from IR field. The intuition of idf in IR filed is that a query term which occurs in many documents is not a good discriminator, and should be give less weight than one which occurs in a few documents, and the measures is an heuristics implementation of this intuition (Robertson, 2004). Debole & Sebastiani (2003) concluded three assumptions about tf.idf model, i.e., (\u0456) idf assumption\uff1arare terms are no less important than frequent terms; (ii) tf assumption: multiple occurrences of a term in a document are no less important than single occurrence; (iii) normalization assumption: long documents are no more important than short documents for the same quantity of term matching. According to these assumptions, researchers proposed many variants of tf.idf model. Here we present its standard \u201cltc\u201d variant (Salton & Buckley, 1988).\n, , | | #\n(1)\nwhere | |, # denote the total number of documents, the number of documents containing term ti in training set Tr, respectively. , has various variants such as raw tem frequency, binary (0 or 1) or log(1+tf). In our study, we use raw term frequency as , . To eliminate the effect of document length, the cosine normalization is performed to normalize the term weight obtained by Eq. (1).\n,\n\u2211 ,| | (2)\nwhere |T| denotes the total number of unique terms contained in training set Tr. Many researchers believe that term weighting schemes in the form as tf.idf can also be applied into TC task. The unsupervised term weighting schemes often contain two factors, i.e., local weight and global weight. Local weight\u2014the tf term, specifies the weight of term ti within a specific document dj, which is estimated based on the frequency or relative frequency of ti within the document. Global weight\u2014the idf term, is based on counting the number of documents containing the term ti in the collection. The intuition of idf in IR filed seems reasonable. However, it deserves to be carefully weighed for TC task. TC task is different from IR task. For TC task, a term, which occurs in many documents, maybe is a good discriminator, especially when it occurs in a few categories or only one\ncategory. Therefore, we should consider the distribution of a term among categories, rather than the distribution of the term in documents. This is our first motivation to revise the term weighting scheme for TC task."}, {"heading": "2.2 Supervised term weighting schemes", "text": "Researchers have noted tf.idf maybe is not the best text representation model for TC task. Therefore, Debole & Sebastiani (2003) proposed the supervised term weighting schemes. They introduced term (feature) selection into term weighting schemes and made the phases of term weighting to be an activity of supervised learning, in which information on membership of training documents in categories is used. Because later research showed using feature selection metrics (such as , information gain, gain ratio, Odds Ratio, and so on) to weight term is not superior to traditional tf.idf model (Liu, Loh, and Sun, 2009; Lan et al, 2009), we do not give detailed discussion about this method, readers who are interested in the method can obtain details from Debole & Sebastiani (2003).\nRecently, two new supervised term weighting schemes are proposed, i.e., tf.rf and prob-based. The two methods both adopt \u201clocal policy\u201d, that is, in each experiment, a chosen category c is tagged as positive category, and the rest categories in training corpus are combined together as the negative category (Lan et al, 2009). This is a binary classification and the fundamental information elements used for term weighting scheme are shown in Table 1. Lan et al. (2009) proposed tf.rf supervised term weighting scheme and its formula is expressed as\n. 2 ,\n(3)\nIn addition, Liu, Loh, & Sun (2009) proposed prob-based supervised term weighting scheme and its formula is expressed as\n,\n1 (4)\nwhere is the maximum frequency of a term in document . The two supervised term weighting schemes have a common factor, i.e., a/c. It means the weight of term ti is related to relevant documents that contain this term ti (Lan et al. 2009). There are four different factors between Eq. (3) and Eq. (4), i.e., (\u0456) The base of logarithmic operation of rf factor in Eq. (3) is 2, whereas the base is e (natural number) in Eq. (4); (ii) the constant is 2, 1 respectively in Eq.(3) and Eq. (4); (iii) tf factor in Eq. (4) is divided by , whereas tf in Eq. (3) is raw term frequency; (iv) Eq. (4) considers the ratio between relevant documents and irrelevant documents in category , i.e., a/b.\nAs we know, prob-based supervised term weighting scheme is proposed to deal with imbalanced text classification, so a/b is considered to balance the rare categories. For instance, given two categories\n(major category, which contains 100 documents) and (minor category, which just contains 3\ndocuments.), if = 50, = 40 and = 2, 1, then\n. The term weighting value in\nis improved by the factor a/b. This will improve the probability of assigning a document to . However, introducing the parameter b (the number of documents which do not contain term ti in specified category ) has no any intuitive explanation. Just as the authors presented, prob-based supervised term weighting scheme just gives more weight to terms occurred in minor category. In our experiments on skewed Reuters-21578, prob-based supervised term weighting scheme obtained worse performance. However, after removing the factor a/b, having the similar form as tf.rf, the macro-F1 and micro-F1 on unbalanced Reuters-21578 corpus improve obviously. Through our experiments, we verify the a/b factor is less important than a/c and it maybe affects the performance of a classifier.\nTf.rf supervised term weighting scheme outperforms prob-based because it just considers the frequency of relevant documents. However, tf.rf favors terms which frequently occurs in positive category. It is opposite to idf, which favors rare terms. For instance, if a term ti just occurs once in\ncategory , ti is a good discriminator for category . However, the rf ti 2 , 3.\nFollowing Zipf\u2019s law of regular words (Zipf, 1949), the frequency of many terms in a big corpus should be one, and rf assigns less weight to these terms. This will result in the discriminating power of these words decreases or disappears.\nBesides, tf.rf and prob-based supervised term weighting schemes both have one shortness, i.e., the distribution of a term among categories disappears during partitioning training corpus into positive and negative categories. Just as we know, if a term occurs in many categories, then it is not a good discriminator. This is similar to idf assumption. Tf.rf and prob-based supervised term weighting schemes do not consider the factor."}, {"heading": "3. A novel ICF-based supervised term weighting scheme", "text": "In this paper, we introduce inverse category frequency (icf) into supervised term weighting scheme for TC task. Two concepts are defined as\nCategory frequency (cf): the number of categories in which term ti occurs. Inverse category frequency (icf): the formula of icf is similar to idf and it is expressed as\nti | | (5)\nwhere |C| denotes the total number of categories in the training corpus. The icf have been widely used in many TC tasks. For instance, Quan, & Liu used a variant of icf as one factor in their question categorization problem. Meanwhile, Guan, Zhou, & Guo (2009) adopted icf to construct centroid vector of category for centroid-based classifier and their Class-Feature-Centroid classifier outperformed SVM classifiers on closed test. The intuition behind icf is: the less categories a term occurs in, the more discriminating power the term contributes to text categorization. This assumption is named icf assumption. Icf favors rare terms and biases against popular terms.\nTherefore, we introduce icf factor into tf.rf and propose a novel icf-based supervised term weighting scheme. Its formula is expressed as\n2 ,\n| | (6)\nIcf-based supervised term weighting scheme contains three factors. Tf factor is the raw term frequency; rf factor measures the distribution of term ti between positive category and negative category; icf factor measures the distribution of term ti among categories. The icf information can be acquired before partitioning training corpus.\nFor future clarification of two supervised term weighting schemes, consider the terms t1, t2 having distributions given in Table 2, and given t1 occurs in fewer categories than t2. According to Eq. (3), we know tf.rf ( t1) = tf.rf ( t2). Tf.rf just considers the distribution of a term in positive and negative categories. However, the discriminating power of t1 should be larger than that of t2, because t1 occurs in fewer categories than t2. According to Eq. (6), we can find icf-based ( t1) > icf-based ( t2). It means icf-based supervised term weighting scheme seems more reasonable than tf.rf. Our experiments on real-world datasets verify our starting point."}, {"heading": "4. Data corpora and benchmark methodology", "text": ""}, {"heading": "4.1 Reuters-21578 (unbalanced corpus) & 20Newsgroup (balanced corpus)", "text": "Reuters-21578: The Reuters-21578 dataset\u2020 are widely used benchmarking collection. Our dataset are based on the Trinity College Dublin version, which changed documents from the original SGML format into XML format. According to the ModApte split, we got a collection of 52 categories after removing unlabeled documents and documents with more than one class labels. There are 6532 training documents and 2568 testing documents left. In order to compare to the results of Lan et al. (2009), we select the top 10 largest categories. The training documents and testing documents in the top 10 categories are shown in Table 3. One issue of the Reuters corpus is the skewed category distribution problem. Among the top 10 categories, the most common category (earn) accounts for 43% of the whole\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2020 http://ronaldo.cs.tcd.ie/esslli07/sw/step01.tgz\ntraining set, whereas the most rare category (coffee) just contains 99 training instances, accounting for 1.5%. 319 stop words, punctuation and numbers are removed; all letters have been converted into lowercase; word stemming is not applied. The final vocabulary has 27953 words.\n20 Newsgroup: The dataset\u2021 consists of 19905 documents, uniformly distributing in 20 categories. We randomly select 33% instances from each category as testing instances and the rest texts as training instances. There are 13330 training instances and 6575 testing instances. We only keep \u201cSubject\u201d, \u201cKeywords\u201d, and \u201cContent\u201d. Other information, such as \u201cPath\u201d, \u201cFrom\u201d, \u201cMessage-ID\u201d, \u201cSender\u201d, \u201cOrganization\u201d, \u201cReferences\u201d, \u201cDate\u201d, \u201cLines\u201d, and email addresses, are filtered out. The stop words list (McCallum, 2002) has 823 words and we keep words that occur at least twice. All letters are converted into lowercase and word stemming is not applied. The total number of unique terms is 24180. The source code of text parsing for 20 Newsgroup is provided by Guan, Zhou & Guo (2009)."}, {"heading": "4.2 Classifier: Support Vector Machines", "text": "We choose the state-of-art algorithm, i.e., SVM-based classifier, as our defaulted classifier, because SVM almost achieves top-notch performance among the widely used classification algorithms (Joachims, 1997; Sebastiani, 2002; Lan et al., 2009). Lan et al. also adopted kNN, but their experiments showed kNN is inferior to SVM on both macro-F1 and micro-F1. We adopt radial basis function (RBF) as the kernel function of SVM classifier implemented in the LibSVM package (Chang, & Lin, 2001), whereas Lan et al. (2009) adopted linear kernel. Our experimental results have shown RBF kernel performs better than linear kernel. In each experiment, we use the grid.py (a Python script) to find the optimal parameters C and \u03b3. C is the penalty parameter and is the kernel parameter of RBF."}, {"heading": "4.3 Performance measures", "text": "We measure the effectiveness in terms of precision (p) and recall (r) defined in the usual way. The two measures are popular performance measures for TC task. As a measure of effectiveness that combines the contributions of p and r we use the well-known F1 function (Lewis, 1995), defined as\n1 (7)\nUsually, F1 is estimated from two ways, i.e., macro-averaging F1 (macro-F1) and micro-averaging F1 (micro-F1). Macro-F1 gives the same weight to all categories, thus is mainly influenced by the F1 of rare categories for skewed Reuters-25718 corpus. On the contrary, micro-F1 will be dominated by the performance of common categories for skewed Reuters-25718 corpus. Therefore, the macro-F1 and micro-F1 of Reuters-25718 may give quite different results. Because of the balance of 20 Newsgroup, the macro-F1 and micro-F1 of 20 Newsgroup are quite similar."}, {"heading": "5. Results and discussions", "text": "We have constructed 80 groups of experiments (40 groups of binary classification experiments for Reuters-21578 top 10 categories corpus and 40 groups of binary classification experiments for 20 Newsgroup corpus) to test the validation of the icf-based supervised term weighting scheme. For each category, we adopt \u201clocal policy\u201d to construct training set and testing set. In each experiment, a chosen category c is tagged as positive category, and the rest categories in training corpus are combined\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2021\u00a0 http://kdd.ics.uci.edu/databases/20newsgroup\u00a0\ntogether as the negative category"}, {"heading": "5.1 Overall performance", "text": "Fig. 1. reports the overall performance of four term weighting schemes (tf.idf, prob-based, tf.rf, tf.idf, and icf-based) on macro-F1 and micro-F1 on Reuters-21578 top 10 categories corpus. The best macro-F1 and micro-F1 on Reuters-21578 corpus using icf-based supervised term weighting scheme reach up to 88.93%, 95.08%, respectively. The macro-F1 and micro-F1 using tf.rf are 88.55% and 94.06% in our experiments. As shown in Fig. 1.,we can conclude tf.rf and icf-based are superior to prob-based and tf.idf on macro-F1 and micro-F1. The macro-F1 of tf.rf and icf-based on Reuters-21578 corpus has been improved up to 4.95%, 5.33%, respectively, compared with prob-based. In addition, the micro-F1 of tf.rf and icf-based has been improved 3.15%, 4.17%, respectively. However, our experiments have shown that tf.idf is better than prob-based on macro-F1 and micro-F1. Icf-based is a little superior to tf.rf on macro-F1. On micro-F1, icf-based improves 1.02% more than tf.rf. These results indicate the order of overall performance is icf-based > tf.rf > tf.idf > prob-based on Reuters-21578 top 10 categories corpus.\nFig. 2. shows the performance of icf-based and tf.rf supervised term weighting schemes on macro-F1 and micro-F1 on balanced 20 Newsgroup corpus. We explained the reason why prob-based SWT was ignored in subsection 5.3. The best macro-F1 using icf-based reaches 82.08%, whereas the macro-F1 using tf.rf is 81.73%, which is a little lower than the former. The micro-F1 of icf-based and tf.rf reaches 79. 92%, 79.88%, respectively. The difference of micro-F1 between two weighting schemes is 0.04%. The same conclusion is obtained by Lan et al. (2009). On balanced 20 Newsgroup corpus, icf-based method and tf.rf method obtain very close performance."}, {"heading": "5.2 Results on Reuters-21578", "text": "To give detailed exhibition for top 10 categories on Reuters-21578, Table 4 reports the results on precision, recall and F1. We can conclude that icf-based SWT and tf.rf SWT outperform prob-based SWT on F1 measure, because only F1value of crude category of prob-based is very close to that of tf.rf and icf-based, and the rest F1 values of prob-based supervised term weighting scheme are all inferior to tf.rf and icf-based. Meanwhile, because Reuters-21578 top 10 categories corpus is unbalanced, we expected prob-based supervised term weighting scheme could obtain better performance. However, we did not get the expectation. There are two same categories (i.e., crude and ship) in our corpus and that used in Liu, Loh, & Sun (2009), but it is hard to compare them, because we used different training corpus and kernel functions. However, according to our experimental results, prob-based obtained close F1 values on top four categories and bottom two categories compared with tf.rf and icf-based, but worse F1 values on middle four categories. The results maybe reflect the starting point of prob-based, which is improving term weighting of minor categories.\nThe F1 values of tf.idf are very close to those of tf.rf and icf-based, except money-fx, ship and sugar categories. However, the F1 value of tf.idf is larger than that of prob-based on each category. It shows that traditional tf.idf also obtain better performance than some supervised term weighting schemes. This\nconclusion is consistent with that by Debole & Sebastiani (2003). It is difficult to find out which is the better term weighting scheme between tf.rf and icf-based based on a specified category. According to macro-F1 and micro-F1, we have found icf-based is better than tf.rf. Here we count the number of categories whose difference of F1 is greater than 2%. Then we find icf-based supervised term weighting scheme is superior to tf.rf supervised term weighting scheme on three categories, i.e., money-fx, ship and sugar, and tf.rf is better than icf-based only on one category, i.e., interest. Besides, the largest improvement of F1 reaches up to 8.79%, finished by icf-based on ship category."}, {"heading": "5.3 Results on 20 Newsgroup", "text": "Because prob-based supervised term weighting scheme has the same form as tf.rf when the corpus is balanced and partial results (we randomly selected six categories from 20 Newsgroups and constructed six groups of experiments using prob-based SWT) have shown its performance is worse than that of tf.rf. Meanwhile, we do not give comparisons between icf-based and tf.idf on balanced 20 Newsgroup corpus, because tf.rf performs consistently the best according to the experiments of Lan et al. (2009). They have present many comparative results between tf.rf and tf.idf. Here we just give the detailed comparison between icf-based SWT and tf.rf SWT on balanced 20 Newsgroup corpus, as shown in Table 5. We can observe that the F1 values between tf.rf and icf-based are very close, except category 19. Perhaps our preprocessing of this corpus results in this phenomenon. Because we just select terms which happen twice within a document and only 24180 words are left to construct text vector. The distributions of these words among categories have no significant difference according our statistics."}, {"heading": "5.4 Discussions", "text": "Through above experiments we can find that our icf-based and tf.rf supervised term weighting schemes have improved the performance of text categorization, compared with tf.idf and prob-based. However, traditional tf.idf also has its own superiority and outperforms some supervised term weighting schemes (e.g., prob-based, tf. , and tf.ig). We should point out that the conclusions above are made in combination with SVM classifier in terms of macro-F1 and micro-F1 and other controlled settings.\nLan et al.(2009) did not present detailed results of each category, here we present the results of each category on precision, recall and F1 measure and express some discussions about these results, which provide later researcher detailed data when they focus on supervised term weighting schemes on Reuters-21578 corpus and 20 Newsgroup corpus."}, {"heading": "6. Conclusion and future work", "text": "Compared with unsupervised term weighting, supervised term weighting for TC task has become an important research point. The known category information of terms can be applied to text categorization. This paper combines inverse category frequency and relevance frequency (rf is proposed by Lan et al., 2009) and proposes a novel icf-based supervised term weighting scheme. The introduction of icf can assigns less weight to terms occurring in many categories. Our experimental results and extensive comparisons based on two common corpora, i.e., skewed Reuters-21578 and balanced 20 Newsgroup, have shown icf-based supervised term weighting scheme performs the better performance than tf.rf and prob-based two supervised term weighting schemes and a traditional tf.idf term weighting scheme.\nIn our future work, we will introduce inverse category frequency into unsupervised term weighting\nschemes, such as tf.icf. According to our preliminary experiments, tf.icf has shown its superiority on SVM classifier and centroid-based classifier, compared with other unsupervised term weighting schemes, such as tf.idf, idf. This work will be presented in the future."}, {"heading": "Acknowledgment", "text": "We thank Guan Hu et al. for sharing their software, which helps us parse the 20 Newsgroup corpus. The source code of their software is available at http://epcc.sjtu.edu.cn/~hzhou/research/cfc. References Chang, C. C., & Lin, C. J. (2001). LIBSVM: A Library for Support Vector Machines, http://www.csie.ntu.edu.tw/ cjlin/libsvm.\nCortes, C., & Vapnik, V. (1995). Support-Vector Networks. Machine Learning, 20, 273-297.\nDebole, F., & Sebastiani, F. (2003). Supervised term weighting for automated text categorization. In Proceedings of the 2003 ACM\nsymposium on applied computing (pp. 784\u2013788). Melbourne, Florida, USA.\nGuan, H., Zhou, J., & Guo, M. (2009). A class-feature-centroid classifier for text categorization. In Proceedings of the 18th\ninternational conference on World wide web, 201-210.\nJoachims, T. (1997). Text categorization with support vector machines. Technical report, LS VIII Number 23, University of\nDortmund,. ftp://ftp-ai.informatik.uni-dortmund.de/pub/Reports/report23.ps.Z.\nJones, K.S. (1972). A Statistical Interpretation of Term Specificity and Its Application in Retrieval. J. Documentation, 28(1),\n11-21.\nJones, K.S. (2004). A Statistical Interpretation of Term Specificity and Its Application in Retrieval. J. Documentation, 60(5),\n493-502.\nLan, M., Tan C. L., Su, J., & Lu, Y. (2009). Supervised and traditional term weighting methods for automatic text categorization.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 31(4), 721-735.\nLeopold, E., & Kindermann, J. (2002). Text categorization with support vector machines \u2013 How to represent texts in input space.\nMachine Learning, 46(1\u20133), 423\u2013444. Lewis, D. D. (1995). Evaluating and optimizing autonomous text classification systems. In Proceedings of 18th ACM International\nConference on Research and Development in Information Retrieval, 246\u2013254.Seattle, US.\nLi, B.Y., Lu, Q., & Yu, S. W. (2004). An adaptive k-nearest neighbor text categorization strategy. ACM Transactions on Asian\nLanguage Information Processing (TALIP), 3(4), 215\u2013226.\nMcCallum, A., & Nigam, K. (1998) . A Comparison of Event Models for Naive Bayes Text Classification. In Proc. AAAI\nWorkshop Learning for Text Categorization.\nMcCallum, A. (2002). Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.\nPei, Z. L., Shi X. H., Marchese, M., & Liang Y. C. (2007). An enhanced text categorization method based on improved text\nfrequency approach and mutual information algorithm. PROGRESS IN NATURAL SCIENCE, 17(12), 1494-1500.\nQuan, X. J., & Liu, W. Y. Term weighting schemes for question categorization. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, to be published.\nQuinlan, R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.\nRobertson, S.E. (2004). Understanding Inverse Document Frequency: On Theoretical Arguments for IDF. J. Documentation, 60(5),\n503-520.\nSalton, G., & Buckley, C. (1988). Term-weighting approaches in automatic text retrieval. Information Processing & Management,"}, {"heading": "24 (5), 513\u2013523.", "text": "Sebastiani, F. (2002). Machine learning in automated text categorization. ACM Computing Surveys (CSUR), 34(1), 1\u201347.\nXue, X. B., Zhou, Z. H. (2009). Distributional Features for Text Categorization. IEEE Transactions on Knowledge and Data\nEngineering, 21(3), 428-442.\nYang, Y., Liu, X. (1999). A re-examination of text categorization methods. In Proc. of Annual Int. ACM SIGIR Conf. on Research\nand Development in Information Retrieval(SIGIR), ACM Press New York, NY, USA, 42-49.\nZipf, G. K. (1949). Human behavior and the principle of least effort, Addison-Wesley, Reading, Ma."}], "references": [{"title": "LIBSVM: A Library for Support Vector Machines, http://www.csie.ntu.edu.tw/ cjlin/libsvm", "author": ["C.C. Chang", "C.J. Lin"], "venue": "Support-Vector Networks. Machine Learning,", "citeRegEx": "Chang and Lin,? \\Q2001\\E", "shortCiteRegEx": "Chang and Lin", "year": 2001}, {"title": "A Statistical Interpretation of Term Specificity and Its Application in Retrieval", "author": ["K.S. Jones"], "venue": "J. Documentation,", "citeRegEx": "Jones,? \\Q2004\\E", "shortCiteRegEx": "Jones", "year": 2004}, {"title": "Text categorization with support vector machines \u2013 How to represent texts in input space", "author": ["E. Leopold", "J. Kindermann"], "venue": "Machine Learning,", "citeRegEx": "Leopold and Kindermann,? \\Q2002\\E", "shortCiteRegEx": "Leopold and Kindermann", "year": 2002}, {"title": "frequency approach and mutual information algorithm", "author": ["X. J", "W.Y. Liu"], "venue": "PROGRESS IN NATURAL SCIENCE,", "citeRegEx": "J. and Liu,? \\Q1993\\E", "shortCiteRegEx": "J. and Liu", "year": 1993}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Information Processing & Management,", "citeRegEx": "Salton and Buckley,? \\Q1988\\E", "shortCiteRegEx": "Salton and Buckley", "year": 1988}, {"title": "A re-examination of text categorization methods", "author": ["Y. Yang", "X. Liu"], "venue": "In Proc. of Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval(SIGIR),", "citeRegEx": "Yang and Liu,? \\Q1999\\E", "shortCiteRegEx": "Yang and Liu", "year": 1999}], "referenceMentions": [{"referenceID": 1, "context": "idf (term frequency and inverse document frequency) proposed by Jones (Jones, 1972; Jones, 2004).", "startOffset": 70, "endOffset": 96}, {"referenceID": 1, "context": "The term weighting schemes often affect the effectiveness of TC, Leopold and Kindermann (2002) pointed out that the performance of SVM classifiers is dominated by text representation schemes, rather than kernel functions.", "startOffset": 65, "endOffset": 95}, {"referenceID": 1, "context": "idf (term frequency and inverse document frequency) proposed by Jones (Jones, 1972; Jones, 2004). Robertson (2004) tried to present the theoretical justifications of both idf and tf.", "startOffset": 64, "endOffset": 115}], "year": 2010, "abstractText": "Unsupervised term weighting schemes, borrowed from information retrieval field, have been widely used for text categorization and the most famous one is tf.idf. The intuition behind idf seems less reasonable for TC task than IR task. In this paper, we introduce inverse category frequency into supervised term weighting schemes and propose a novel icf-based method. The method combines icf and relevance frequency (rf) to weight terms in training dataset. Our experiments have shown that icf-based supervised term weighting scheme is superior to tf.rf and prob-based supervised term weighting schemes and tf.idf based on two widely used datasets, i.e., the unbalanced Reuters-21578 corpus and the balanced 20 Newsgroup corpus. We also present the detailed evaluations of each category of the two datasets among the four term weighting schemes on precision, recall and F1 measure.", "creator": "PScript5.dll Version 5.2"}}}