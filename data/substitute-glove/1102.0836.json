{"id": "1102.0836", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2011", "title": "EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning", "abstract": "It another a challenging task put committees approximates constraint in a west dimensional installation. To policy this make, another elastic net has been designs and successfully limits it many digital. Despite well great achieved, . muscles net does believe explicitly direct rational information circuitry in embedded to take correlated variables. To overcome indeed limitations, sure present given epic Bayesian plug-in model, the \\ romine, suggested embedded next eigenstructures now web making an generalized selection. Specifically, it interface little uniformly obtain classed features turned a pmma comparison hiding variable confounding however well principled Bayesian facilitate. We reparameterize three suvs new months created eigenspace making avoiding overfiting those to increase saw biomedical efficiency similar its MCMC mono. Furthermore, we provide while example present to the \\ eig then with semigroups grasp: the \\ quon new an hypermedia dpc - currently composite regularizer, through naturally approximated the $ l_ {october / 2} $ regularizer as same the elastic quarter. Experiments start soluble out unfortunately data even that first \\ eig markedly outperforms the lasso, instead melanin gross, being the Bayesian lasso took terms also prediction knowledge, poor when been handful though classes tissue thought preferred alone now additionally with variables.", "histories": [["v1", "Fri, 4 Feb 2011 04:40:07 GMT  (104kb,D)", "https://arxiv.org/abs/1102.0836v1", null], ["v2", "Tue, 8 Feb 2011 04:03:50 GMT  (104kb,D)", "http://arxiv.org/abs/1102.0836v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuan qi", "feng yan 0003"], "accepted": true, "id": "1102.0836"}, "pdf": {"name": "1102.0836.pdf", "metadata": {"source": "CRF", "title": "EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning", "authors": ["Yuan Qi", "Feng Yan"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this paper we consider the problem of selecting correlated variables in a high dimensional space. Among many variable selection methods, the lasso and the elastic net are two popular choices [Tibshirani, 1994, Zou & Hastie, 2005]. The lasso uses a l1 regularizer on model parameters. This regularizer shrinks the parameters towards zero, removing irreverent variables and yielding a sparse model [Tibshirani, 1994]. However, the l1 penalty may lead to oversparisification: given many correlated variables, the lasso often only select a few of them. This not only degenerates its prediction accuracy but also affects the interpretability of the estimated model. For example, based on high-throughput\nar X\niv :1\n10 2.\n08 36\nv2 [\ncs .L\nG ]\n8 F\nbiological data such as gene expression and RNA-seq data, it is highly desirable to select multiple correlated genes specific to a phenotype since it may reveal underlying biological pathways. Due to its over-sparsification, lasso may not be suitable for this task.\nTo address this issue, the elastic net has been developed to encourage a grouping effect, where strongly correlated variables tend to be in or out of the model together [Zou & Hastie, 2005]. However, the grouping effect is just the result of its composite l1 and l2 regularizer; the elastic net does not explicitly incorporate correlation information among variables in its model.\nIn this paper, we propose a new sparse Bayesian hybrid model, called the EigenNet. Unlike the previous sparse models, it uses the eigen information from the data covariance matrix to guide the selection of correlated variables. Specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlation in a principle Bayesian framework [Lasserre et al., 2006]. The hybrid model enables identification of groups of correlated variables guided by the eigenstructures. Also, it passes the information from the conditional model to the generative model, selecting informative eigenvectors for the classification task. Unlike frequentist approaches, the Bayesian hybrid model can reveal correlations between classifier weights via their joint posterior distribution.\nWe reparameterize the model in the eigenspace of the data. When the number of predictor variables (i.e., input features), (p), is bigger than the number of training samples (n), this reparameterization restricts the model in the data subspace, which not only reduces overfitting, but also allows us to develop efficient Markov Chain Monte Carlo sampler.\nFrom the regularization perspective, the EigenNet naturally generalizes the elastic net by using a composite regularizer adaptive to the data eigenstructures. It contains a l1 sparsity regularizer and a directional regularizer that encourages selecting variables associated with eigenvectors chosen by the model. When the variables are independent of each other, the eigenvectors are parallel to the axes and this composite regularizer reduces to the l1/2 regularizer used by the elastic net; when some of the input variables are strongly correlated, the regularizer will encourage the classifier aligned with eigenvectors selected by the model. On one hand, our model is like the elastic net to retain \u2018all the big fish\u2019. On the other hand, our model is different from the elastic net by using the eigenstructure. Hence the name EigenNet.\nExperiments on synthetic and real data are presented in Section 7. They demonstrate that the EigenNet significantly outperforms the lasso, the elastic net, and the Bayesian lasso [Park et al., 2008, Hans, 2009] in terms of prediction accuracy, especially when the number of training samples is smaller than the number of features."}, {"heading": "2 Background: lasso and elastic net", "text": "We denote n independent and identically distributed samples as\nD = {(x1, y1), . . . , (xn, yn)}\n, where xi is a p dimensional input features (i.e., explanatory variables) and yi is a scalar label (i.e., response). Also, we denote [x1, . . . ,xn] by X and (y1, . . . ,yn) by y. In this paper, we consider the binary classification problem (yi \u2208 {\u22121, 1}), but our analysis and the proposed models can be extended to regression and other problems.\nFor classification, we use a logistic function as the data likelihood function: p(y|X,w, b) = \u220f i \u03c3(yi(w Txi + b)) (1)\nwhere \u03c3(z) = 11+exp(\u2212z) , and w and b define the classifier.\nTo identify relevant variables for high dimensional problems, the lasso [Tibshirani, 1994] uses a l1 penalty, effectively shrinking w and b towards zero and pruning irrelevant variables. In a probabilistic framework this penalty corresponds to a Laplace prior distribution:\np(w) = \u220f j \u03bb exp(\u2212\u03bb|wj |) (2)\nwhere \u03bb is a hyperparameter that controls the sparsity of the estimated model. The larger the hyperparameter \u03bb, the sparser the model.\nAs described in Section 1, the lasso may over-penalize relevant variables and hurt its predictive performance, especially when there are strongly correlated variables. To address this issue, the elastic net [Zou & Hastie, 2005] combines l1 and l2 regularizers to avoid the over-penalization. The combined regularizer corresponds to the following prior distribution:\np(w) \u221d \u220f j exp(\u2212\u03bb1|wj | \u2212 \u03bb2w2j ) (3)\nwhere \u03bb1 and \u03bb2 are hyperparameters. While it is well known that the elastic net tends to select strongly correlated variables together, it does not uses correlation information embedded in the data. The selection of correlated variables is merely the result of a less aggressive regularizer for sparisty.\nBesides the elastic net, there are many variants (and extensions) to the lasso, such as the bridge [Frank & Friedman, 1993] and smoothly clipped absolute deviation [Fan & Li, 2001]. These variants modify the l1 penalty to choose variables, but again do not explicitly use correlation information in data."}, {"heading": "3 EigenNet: eigenstructure-guided variable se-", "text": "lection\nIn this section, we propose to use covariance structures in data to guide the sparse estimation of model parameters.\nFirst, let us consider the following toy examples."}, {"heading": "3.1 Toy examples", "text": "Figure 1(a) shows samples from two classes. Clearly the variables x1 and x2 are not correlated. The lasso or the elastic net can successfully select the relevant variable x1 to classify the data. For the samples in Figure 1(b), the variables x1 and x2 are strongly correlated. Despite the strong correlation, the lasso would select only x1 and ignore x2. The elastic net may select both x1 and x2 if the regularization weight \u03bb1 is small and \u03bb2 is big, so that the elastic net behaves like l2 regularized classifier. The elastic net, however, does not explore the fact that x1 and x2 are correlated.\nSince the eigenstructure of the data covariance matrix captures correlation information between variables, we propose to not only regularize the classifier to be sparse, but also encourage it to be aligned with certain eigenvector(s) that are helpful for the classification task. Since our new model uses the eigen information, we name it the EigenNet.\nFor the data in Figure 1(a), since the two eigenvectors are parallel with the horizontal and vertical axes, the EigenNet essentially reduces to the elastic net and selects x1. For the data in Figure 1(b), however, the eigenvectors (in particular, the principle eigenvector) will guide the EigenNet to select both x1 and x2.\nWe use a Bayesian framework to materialize the above ideas in the EigenNet, as shown in the following section."}, {"heading": "3.2 Bayesian hybrid of conditional and generative models", "text": "The EigenNet is a hybrid of conditional and generative models. The conditional component allows us to learn the classifier via \u201ddiscriminative\u201d training; the generative component captures the correlations between variables; and these two models are glued together via a joint prior distribution, so that the correlation information is used to guide the estimation of the classifier and the classification task is used to choose or scale relevant eigenvectors. Our approach is based on the general Bayesian framework proposed by Lasserre et al. [2006]), which allows one to combine conditional and generative models in an elegant principled way.\nSpecifically, for the conditional model we have the same likelihood as (1), p(y|X,w, b) = \u220f i \u03c3(yi(w\nTxi + b)). To sparsify the classifier, we can use a Laplace prior on w,\np(w) = \u220f j \u03bb1 exp{\u2212\u03bb1|wj |}. (4)\nTo encourage the classifier aligned with certain eigenvectors, we use the following generative model:\np(Vs|w\u0303) \u221d exp(\u2212\u03bb2 2\n\u2211 \u03b7j ||w\u0303 \u2212 sivi||2+) (5)\nwhere\n||w\u0303 \u2212 sivi||2+\n\u2261\u2212 1 2 \u03bb2 \u2211 j \u03b7j(||w\u0303||2 \u2212 2sj |w\u0303Tvj |+ s2j ||vj ||2\n=\u2212 1 2 \u03bb2 \u2211 j \u03b7j(||w\u0303||2 \u2212 2sj |w\u0303Tvj |+ s2j , (6)\ns are nonnegative continuous variables, vi and \u03b7i are the i-th eigenvector and eigenvalue of the data covariance matrix, respectively. The reason we use absolute values of w\u0303Tvj in (6) is because we only care about the alignment of w\u0303 and vi, not the sign of their product. Overall, the above model encourages the classifier to more aligned with the major eigenvectors with bigger eigenvalues. But the variables s allow us to scale or select individual eigenvectors to remove irrelevant ones.\nTo integrate the conditional and generative models, we use a joint prior on w and w\u0303:\np(w, w\u0303) \u221d exp(\u2212\u03bb1|w|1) exp(\u2212 \u03bb3 2 ||w \u2212 w\u0303||2). (7)\ni.e., we have\np(w, w\u0303) = \u03bb1 exp(\u2212\u03bb1|w|1)N (w\u0303|w, \u03bb3\u22121). (8)\nFinally we can assign Gamma priors on all the hyperparameters, \u03bb1, \u03bb2, and \u03bb3. The whole model is depicted in the graphical model in Figure 2."}, {"heading": "3.3 Reparameterization and constraint in Eigenspace", "text": "In this section we reparameterize the model in the eigenspace:\nw = V\u03b1 w\u0303 = V\u03b2 (9)\nwhere V \u2261 [v1, . . . ,vm] (m = min{n, p}), and \u03b1 and \u03b2 are the projections of w and w\u0303 on the eigenvectors, respectively.\nThe reparameterization restricts w in the vector space spanned by {v1, . . . ,vm}, which is equivalent to the data space C(X), spanned by the data points {x1, . . . ,xn}. When the number of features is bigger than the number of training points, i.e., p > n, it effectively reduces the number of free parameters in the model, helping avoid overfitting. Furthermore, it provides significant computational advantage when p >> n.\nGiven p(w, w\u0303) and the relationship between (w, w\u0303) and (\u03b1,\u03b2), we obtain p(\u03b1,\u03b2) (Please see Appendix for the details):\np(\u03b1,\u03b2) \u221d exp(\u2212\u03bb1|V\u03b1|1) exp(\u2212 \u03bb3 2 ||\u03b1\u2212 \u03b2||2) (10)\nBased on the new reparameterization, the likelihood for the conditional model becomes\np(y|X,\u03b1, b) = \u220f i \u03c3(yi(x T i V\u03b1 + b)). (11)\nSimilarly, the likelihood for the generative model becomes\np(V, s|\u03b2) \u221d exp(\u22121 2 \u03bb2 \u2211 j \u03b7j(||V\u03b2||2\n\u2212 2sj |V\u03b2Tvj |+ ||vj ||2)\n\u221d exp(\u22121 2 \u03bb2 \u2211 j (\u03b22j \u2212 2\u03b7jsj |\u03b2j |+ \u03b7js2j )) (12)\nThe second equation holds since V is an orthonormal matrix. Combining (10), (11) and (12), we obtain a complete model. We use Markov Chain Monte Carlo with a random walk proposal to estimate the model parameters s, w, and w\u0303."}, {"heading": "4 Alternative view: composite regularization", "text": "In this section, we provide an alternative view to the EigenNet by considering the limiting case of \u03bb3 \u2192 0. For such as case the prior p(\u03b1,\u03b2) becomes\np(\u03b1,\u03b2) = p(\u03b1)\u03b4(\u03b1\u2212 \u03b2)\nThis forces \u03b1 = \u03b2. From a regularization perspective, this prior is equivalent to a composite regularizer:\n\u03bb1|w|+ \u03bb2 2\n\u2211 \u03b7j ||w \u2212 sjvj ||2+ (13)\n=\u03bb1|w|+ \u03bb2 2\n\u2211 \u03b7j(||w||2 \u2212 2sj |wTvj |+ s2j ) (14)\nClearly, when si = 0 for all i\u2019s, the above regularizer reduces to the l1/2 regularizer used by the elastic net 1. When si 6= 0 then the regularizer is adaptive\n1A subtle difference is that we also constrain w in the data space for our model.\nbased on the eigenvector vi: First, if the elements of vi all have reasonably large values, then all the variables in w will very likely to be selected. This effect is visualized in Figure 3(b). Second, if this eigenvector has only several large elements, the corresponding variables in w\u0303 and w are likely to be selected jointly. Unlike the l1/2 regularizer that encourages the selection of groups of variables from all the variables, our regularizer directly targets at specific groups of variables corresponding to the sparse eigenvector. Third, if all the variables are independent of each other, then the eigenvectors are parallel to the axes and each of them contains only one nonzero element. In this case |wTvj | reduces |wj |, a l1 regularizer. Figure 3(a) visualizes the eigen regularizer when variables are independent of each other.\nIn summary, the EigenNet can be viewed as an adaptive generalization of the elastic net by selecting groups of correlated variables based on eigenvectors of the data covariance matrix."}, {"heading": "5 Related work", "text": "The EigenNet can be viewed as an extension of the classical eigenface approaches [Turk & Pentland, 1991, Sirovich & Kirby, 1987]. The eigenface approach uses PCA coefficients of samples to train a classifier. Naturally the major eigenvectors are often associated with large PCA coefficients and the classifier is constrained in the data subspace when the number of features is smaller than the number of training samples. The EigenNet essentially extends the eigenface approach by combining generative and conditional models in a Bayesian framework and performs sparse learning in an adaptive eigenspace (since the model selects or scales relevant eigenvectors based on sj).\nThere are Bayesian versions of the lasso and the elastic net. Bayesian lasso [Park et al., 2008] puts a hyper-prior on the regularization coefficient and use a Gibbs sampler to jointly sample both regression weights and the regularization coefficient. Using a similar treatment to Bayesian lasso, Bayesian elastic net [Li & Lin, 2010] samples the two regularization coefficients simultaneously, potentially avoiding the \u201cdouble shrinkage\u201d problem described in the original elastic net paper [Zou & Hastie, 2005]. As the EigenNet, these methods are grounded in a Bayesian framework, sharing the benefits of obtaining posterior distributions for handling estimation uncertainty. However, Bayesian lasso and Bayesian elastic net are presented to handle regression problems (though certainly they can be generalized for classification problems) and sample in the original parameter space, not using the eigen information embedded in data. The EigenNet, by contrast, works in the eigenspace and uses eigen information to guide classification."}, {"heading": "6 Experimental results", "text": "We evaluate the new sparse Bayesian model, the EigenNet, on both synthetic and real data and compare it with three representative state-of-the-art variable selection methods, including the lasso, the elastic net, and the Bayesian lasso modified for classification problems. For the lasso and the elastic net we use the Glmnet software package that uses cyclical coordinate descent in a pathwise fashion2. The original Bayesian lasso was developed for regression and uses Gibbs sampling. For the classification tasks we consider, we change its Gaussian regression likelihood to the logistic likelihood (1) while keeping its Laplace prior distributions. We used Markov Chain Monte Carlo, instead of Gibbs sampler, to estimate the classifier for the Bayesian lasso. Bayesian approaches are capable of estimating all the hyperparameters from data. However, for easy and objective comparisons, we simply use cross-validation to tune the hyperparameters, \u03bbi, for all methods. For the Bayesian lasso and the EigenNet, we draw the 300,000 MCMC samples and use the last 150,000 samples to estimate the posterior mean of the classifiers, which are used for predicting the labels of test samples. We measure the prediction performance of all methods on test samples in terms of their average test error rate (e.g., the 0.2 error rate indicates 20% errors) and report the standard error of the error rates (except for the following visualization example)."}, {"heading": "6.1 Visualization of estimated classifiers", "text": "First, we test these methods on synthetic data that contain correlated features. We sample 40 dimensional data points, each of which contains two groups of correlated variables. The correlation coefficient between variables in each group is 0.81 and there are 4 variables in each group. We set the values of the classifier weights in one group as 5 and in the other group as -5. We also generate the bias term randomly from a standard Gaussian distribution. We set the number of training points to 80. Figure 4 shows the estimated classifiers and the true classifier. It is not surprising that the elastic net identifies more features than the lasso. What is interesting is that EigenNet does not suppress many the irrelevant features to be exactly 0, but it clearly identifies all the relevant one, which dominate the irrelevant ones. To save space, we did not show the estimated classifier by the Bayesian lasso. Similar to the EigenNet, its classifier also contains many small, but nonzero weights. On this dataset, the test error rates of the lasso, the elastic net, the Bayesian lasso, and the EigenNet are 0.297, 0.245, 0.251, and 0.137.\nAn advantage of the Bayesian treatment for feature selection over frequentist approaches is to possibly uncover the correlations between the classifier weights. These correlations can be revealed by the covariance matrices of the joint posterior distribution over the classifier weights. In Figure 5, we visualize the quantized covariance matrices estimated by the Bayesian lasso and the\n2http://www-stat.stanford.edu/ tibs/glmnet-matlab/\nEigenNet. As shown in 5(a) and 5(b), while the Bayesian lasso suggests some correlation structures among features, they are fairly noisy. By contrast, the EigenNet shows the two groups of correlated features much more clearly."}, {"heading": "6.2 Classification of synthetic data", "text": "Now we systematically compare these methods on synthetic datasets containing correlated features and datasets containing independent features. For this first case, we use a similar procedure as in the visualization example: we sample 40 dimensional data points, each of which contains two groups of correlated variables. The correlation coefficient between variables in each group is 0.81 and there are 4 variables in each group. However, unlike for the previous example where the classifier weights are the same for the correlated variables, now we set the weights within the same group to have the same sign, but with different random values. We vary the number of training points, ranging from 10 to 80, and test all these methods. For the datasets with independent features, we follow the same procedure except that the features are independently sampled.\nWe run the experiments 10 times. Figure 6 shows the error rates averaged over 10 runs. We do not plot the standard errors of the test error rates, since\nthey have very small values: the biggest one is less than 0.0183 for the results on data with correlated features, and for the results on data with independent features, the biggest one is less than 0.030. We report the numerical values of both the averaged error rates and the standard errors in the supplemental materials.\nFor the datasets with independent features, the EigenNet outperforms the alternative methods when the number of training samples are smaller than 40, the number of features (i.e., p > n). Since in this case the eigenstructures of the datasets are uninformative, we expect the improved prediction accuracy is the result of the subspace constraint used by the EigenNet. And once the number of training samples are not bigger than the data dimension, all these methods perform quite similarly.\nFor the datasets with correlated features, the EigenNet significantly outperforms the alternative methods consistently, not only when the number of training samples are smaller than 40 (p > n) but also when it is not. We believe this is because the EigenNet uses the valuable eigen information revealing the feature correlations to train its classifiers. Note that although the result of the elastic net appear to overlaps with those of the lasso. Actually for the data with correlated features, the elastic net often slightly outperforms the lasso (Please their numerical values in the supplemental materials)."}, {"heading": "6.3 Classification of real data", "text": "Besides the synthetic data, we also test all these methods on UCI benchmark datasets, two high-dimensional gene expression datasets, leukaemia and colon cancer, and a spambase dataset with relatively lower dimension but a lot more training samples.\nFor the leukaemia dataset, the task is to distinguish acute myeloid leukaemia\n(AML) from acute lymphoblastic leukaemia (ALL). The whole dataset has 47 and 25 samples of type ALL and AML respectively with 7129 features per sample. The dataset was randomly split 20 times into 37 training and 35 test samples.\nFor the colon cancer dataset, the task is to discriminate tumor from normal tissues using microarray data. The dataset has 22 normal and 40 cancer samples with 2000 features per sample. We randomly split the dataset into 31 training and 31 test samples 10 times.\nFor the spambase datast, the task is to detect spam emails, i.e., unsolicited commercial emails. We use 57 features indicating whether a particular word or character was frequently occurring in the emails. We randomly split the dataset into 1533 training and 3066 test samples 10 times. Note that we do not use any kernel here and the results on this dataset are meant to examine how the performance of these methods compares to each other when there are more samples than features. Using a nonlinear basis function, e.g., a radial basis function, is expected to boost the predictive performance of all these methods.\nFigure 7 summarizes the average test error rates and the standard errors of these methods on the three datasets. Again, the EigenNet significantly outperforms the alternative methods on three datasets. Note that for the leukaemia and colon cancer datasets Bayesian lasso does not perform much worse than the other methods. The reason, we believe, is that these two high dimensional datasets contain thousands of features and Bayesian lasso directly draws sam-\nples in such high dimensional spaces, leading to very slow mixing rates. By contrast, the EigenNet draws samples efficiently in a much smaller eigenspace, not only leading to faster mixing rates but also greatly saving the computing cost for obtaining each sample."}, {"heading": "7 Conclusions", "text": "In this paper, we have presented a novel sparse Bayesian hybrid model, the EigenNet. It integrates a sparse conditional classification model with a generative model capturing the feature correlations. It also generalizes the elastic net by explicitly exploring correlations between features. Compared with several state-of-the art methods, the EigenNet achieves significantly improved prediction accuracy on several benchmark datasets.\nWe plan to extend our hybrid model by utilizing other probabilistic generative models, such as sparse principle component analysis and related projection methods [Guan & Dy, Archambeau & Bach, 2009] and independent component\nanalysis models. Compared to the classical PCA models, these models could be used to better guide the selection of interdependent sparse features."}, {"heading": "Acknowledgement", "text": "Thanks to Jyotishka Datta for his help on software implementation and to Tommi Jaakkola for stimulating discussion."}], "references": [{"title": "Sparse probabilistic projections", "author": ["Archambeau", "C\u00e9dric", "Bach", "Francis"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Archambeau et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Archambeau et al\\.", "year": 2009}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["Fan", "Jianqing", "Li", "Runze"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Fan et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2001}, {"title": "Principled hybrids of generative and discriminative models", "author": ["Lasserre", "Julia A", "Bishop", "Christopher M", "Minka", "Thomas P"], "venue": "In Proc. of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Lasserre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lasserre et al\\.", "year": 2006}, {"title": "The Bayesian Elastic Net", "author": ["Li", "Qing", "Lin", "Nan"], "venue": "Bayesian Analysis,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "The Bayesian Lasso", "author": ["Park", "Trevor", "Casella", "George"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Park et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Park et al\\.", "year": 2008}, {"title": "The matrix cookbook", "author": ["Petersen", "Kaare Brandt", "Pedersen", "Michael Syskind"], "venue": null, "citeRegEx": "Petersen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Petersen et al\\.", "year": 2008}, {"title": "Low-dimensional procedure for the characterization of human faces", "author": ["L. Sirovich", "M. Kirby"], "venue": "J. Opt. Soc. Am. A,", "citeRegEx": "Sirovich and Kirby,? \\Q1987\\E", "shortCiteRegEx": "Sirovich and Kirby", "year": 1987}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Tibshirani and Robert.,? \\Q1994\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1994}, {"title": "Eigenfaces for recognition", "author": ["Turk", "Matthew", "Pentland", "Alex"], "venue": "J. Cognitive Neuroscience,", "citeRegEx": "Turk et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Turk et al\\.", "year": 1991}, {"title": "Regularization and variable selection via the Elastic Net", "author": ["Zou", "Hui", "Hastie", "Trevor"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Zou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "Specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlation in a principle Bayesian framework [Lasserre et al., 2006].", "startOffset": 159, "endOffset": 182}, {"referenceID": 2, "context": "Our approach is based on the general Bayesian framework proposed by Lasserre et al. [2006]), which allows one to combine conditional and generative models in an elegant principled way.", "startOffset": 68, "endOffset": 91}, {"referenceID": 4, "context": "Bayesian lasso [Park et al., 2008] puts a hyper-prior on the regularization coefficient and use a Gibbs sampler to jointly sample both regression weights and the regularization coefficient.", "startOffset": 15, "endOffset": 34}], "year": 2013, "abstractText": "It is a challenging task to select correlated variables in a high dimensional space. To address this challenge, the elastic net has been developed and successfully applied to many applications. Despite its great success, the elastic net does not explicitly use correlation information embedded in data to select correlated variables. To overcome this limitation, we present a novel Bayesian hybrid model, the EigenNet, that uses the eigenstructures of data to guide variable selection. Specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlations in a principled Bayesian framework. We reparameterize the hybrid model in the eigenspace to avoid overfiting and to increase the computational efficiency of its MCMC sampler. Furthermore, we provide an alternative view to the EigenNet from a regularization perspective: the EigenNet has an adaptive eigenspace-based composite regularizer, which naturally generalizes the l1/2 regularizer used by the elastic net. Experiments on synthetic and real data show that the EigenNet significantly outperforms the lasso, the elastic net, and the Bayesian lasso in terms of prediction accuracy, especially when the number of training samples is smaller than the number of variables.", "creator": "LaTeX with hyperref package"}}}