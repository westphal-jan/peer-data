{"id": "1102.2831", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2011", "title": "The effect of linguistic constraints on the large scale organization of language", "abstract": "This products studying for effective include vocabulary ensures before the few dramatic joint among speakers. It metaphor the properties of historiography networks renamed well texts of written origins, been words approximations. These spaces are 35 to those obtained its a new built about the text in ecosystem order. It thought observed been several \" probability \" satellite too scenes making - world and involves - stopping particular. They also performance a school degree of clustering. This has suggest a surprising result - six that has not been calling individually since both literature. We hypothesize question example related similar network statistics ago here born are early fact linear including second existing all time characteristics data now which and systems is owned including may not be expectation bringing saw this of the reasons network.", "histories": [["v1", "Mon, 14 Feb 2011 17:29:24 GMT  (1422kb,D)", "https://arxiv.org/abs/1102.2831v1", null], ["v2", "Tue, 15 Feb 2011 03:33:51 GMT  (1422kb,D)", "http://arxiv.org/abs/1102.2831v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["madhav krishna", "ahmed hassan", "yang liu", "dragomir radev"], "accepted": false, "id": "1102.2831"}, "pdf": {"name": "1102.2831.pdf", "metadata": {"source": "CRF", "title": "The effect of linguistic constraints on the large scale organization of language", "authors": ["Madhav Krishna", "Ahmed Hassan", "Yang Liu", "Dragomir Radev"], "emails": ["radev@umich.edu"], "sections": [{"heading": null, "text": "This paper studies the effect of linguistic constraints on the large scale organization of language. It describes the properties of linguistic networks built using texts of written language with the words randomized. These properties are compared to those obtained for a network built over the text in natural order. It is observed that the \u201crandom\u201d networks too exhibit small-world and scale-free characteristics. They also show a high degree of clustering. This is indeed a surprising result - one that has not been addressed adequately in the literature. We hypothesize that many of the network statistics reported here studied are in fact functions of the distribution of the underlying data from which the network is built and may not be indicative of the nature of the concerned network."}, {"heading": "1 Introduction", "text": "Human language is a good example of a naturally occurring self-organizing complex system. Language, when modeled as a network, has been shown to exhibit small-world and scale-free properties [15]. A network is said to be scale-free if its degree distribution follows a a power-law [1]. Moreover, it has also been suggested that the network evolves over time by following the rule of preferential attachment. That is, a new word being introduced into the system tends to associate with a pre-existing word that is highly frequent [4]. However, probably the first ever mathematical law that hinted at the complex nature of language was given by Zipf [22]. Zipf\u2019s law gives us the following relationship between the frequency of a word, F (r), in a language and its rank r, when words are ranked in order of their frequencies of occurrence (most frequent word assigned rank 1):\nF (r) = C\nr\u03b1 (1)\nZipf observed this \u201claw\u201d for English (with C \u2248 0.1 and \u03b1 \u2248 1) [17], [16] but it has been shown to be obeyed by other natural languages [2]. It seems surprising at first that Zipf\u2019s law is also obeyed by random texts [17], [16]. Random texts may be produced artificially, composed of symbols assigned prior probabilities of occurrence, the latter may be unequal. [11] demonstrates, through numerical simulation, that random texts follow Zipf\u2019s law because of the choice of rank as the independent variable in that relationship. Therefore, this paper concluded that Zipf\u2019s law is not an intrinsic property of natural language of any major consequence. Recent work [8] has shown that this is not the case.\nIn [13], the authors state that a linguistic network formed from a random permutation of words also exhibits a power-law. They attribute this to the nature of the linguistic network created which is essentially a co-occurrence network. A co-occurrence network, for our purposes, is a directed graph of words which are linked if they are adjacent to each other in a sentence. The authors reason that in such a network, the degree of a word is proportional to its frequency within the text from which the network is constructed. Thus, they say, permuting a text has no impact on the degree distribution of a co-occurrence network. This is not true unless duplicate edges can be added to the graph (which is clearly not the case). An illustration is the dummy sentence A B C D E B C (degrees are A=1, B=3, C=2, D=2, E=2 ) and one of its random permutations D B E C B A C (degrees are A=2, B=4, C=3,\nar X\niv :1\n10 2.\n28 31\nv2 [\ncs .C\nL ]\n1 5\nFe b\n20 11\n2 D=1, E=2 ). The identities of the words that co-occur with a word, and whether the word itself occurs at the end or the beginning of a sentence clearly affect its degrees in the graph.\nIn this work we analyze in detail the topology of the language networks at different levels of linguistic constraints. We find that networks based on randomized text exhibit small-world and scale-free characteristics. We also show that many of the network statistics we study are functions of the distribution of the underlying data from which the network is built. This study tries to shed light on several aspects of human language. Language is a complex structure where words act as simple elements that come together to form language. We try to understand why is it rather easy to link such simple elements to form sentences, novels,books,..etc. We are also interested in studying the structural properties of word networks that would provide humans with easy and fast word production. We study such word networks at different levels of linguistics constraints to better understand the role of such constraints in terms of making mental navigation through words easy. In this study, we conduct experiments with four different languages (English, French, Spanish, and Chinese). This allows us to find out the features of word networks that are common to all languages. The experiments we will describe throughout this paper were able to find some answers that will ultimately lead to the answers to those harder questions.\nWe study the properties of linguistic networks at different levels of linguistic constraints. We study the networks formed by randomizing the underlying text, and how they compare to the properties of networks formed from plain English Broadly, two types of networks are built in this paper - frequent bigrams (should we call these collocations?) and co-occurrence networks. A frequent bigram can be defined as a sequence of two or more words that is statistically idiosyncratic. That is, in a certain context, the constituent words of a frequent bigram co-occur with a significant frequency. In [12], a collocation (frequent bigram) is defined as a conventional way of saying something. Library card, phone booth and machine translation are examples of collocations. A frequent bigrams network, simply, is a graph of words as vertices such that there is an edge between two words provided they form a frequent bigram. Frequent bigrams networks are built from unscrambled text for the purpose of general study. Here, the method presented in [6] is improved upon by employing a Fisher\u2019s exact test to extract frequent bigrams [19]. A sample from the frequent bigrams graph constructed in this study is shown in figure 1.\nThe contributions of this paper include: (1) we used uniformly distributed permutation algorithms for scrambling the corpus and added the Fisher exact test for detecting significantly correlated word pairs, (2) we obtained good results on networks induced from random corpus. Randomization of the corpus reduces linguistic constraints. Linguistic constraints reduce the small-worldness of the network!, (3) we used network randomization algorithms that preserve the degree distribution, and (4) we used lemmatized words and considered multiple languages.\nThe paper is organized as follows: in section 2, we review some related work and put our work in context with respect to related work. Section 3 describes our methodology and how the different networks are created. Finally in section 4, we analyze the different properties of all networks."}, {"heading": "2 Related Work", "text": "Network models of language have been studied by many researchers in the past [6, 21]. One of the earliest such studies [6] constructed a \u201crestricted\u201d (frequent bigrams) network and an \u201cunrestricted\u201d (co-occurrence) network from a subset of the British National Corpus [3]. These networks were undirected. In that work, a bigram, wiwi+1, is treated as a frequent bigram if the probability of its occurrence, pi,i+1, is found to be greater than pi \u00b7 pi+1, under the assumption of the constituent words being independent. This is a simplistic and unreliable filter for word dependence [12]. [19] demonstrates the suitability of Fisher\u2019s (right-sided) Exact test for this purpose. In the unrestricted network, words are linked if they co-occur in at least one sentence within a window of two words. Both these networks were shown to possess small-world characteristics with the average minimum distance between vertices \u2248 3. They were also shown to be scale-free, with degree distributions following composite power-laws\n3\nwith exponents \u03b31 = \u22121.50 and \u03b32 = \u22122.70 respectively. The latter exponent is approximately equal to \u03b3 = \u22123, obtained for a random graph constructed by employing the Barabasi-Albert (BA) model with preferential attachment. Also studied was a \u201ckernel\u201d network of the 5000 most connected words extracted from the restricted network. This network too was shown to follow a power-law degree distribution with \u03b3 = \u22123. The authors go on to suggest that power-law coefficients obtained are indicative of language having evolved following the law of preferential attachment. They also suggest that the kernel network constructed here is representative of human mental lexicons; these lexicons possess the small-world feature which facilitates quick navigation from one word to another.\nBeyond universal regularities such as Zipf\u2019s law, [20] recently examined burstiness, topicality, semantic similarity distribution and their interrelation and modeled them with two mechanisms, namely frequency ranking with dynamic reordering and memory across documents. Besides, large web datasets were used to validate the model. This paper and several other papers focused on modeling human written text with specific mechanisms.\nNetwork theory has been used in several studies about the structure of syntactic dependency networks. In [7], the author overview-ed the past studies on linguistic networks and discussed the possibilities and advantages of network analysis of syntactic dependency networks. In [10], network properties such as small world structure, heterogeneity, hierarchical organization, betweenness centrality and assortativeness etc were examined for the syntactic networks from Czech, Romanian and German corpora. Seven corpora were examined by similar complex network analysis methods in [9]. Several common patterns of syntactic dependency networks were found. These patterns include high clustering coefficient of each vertex, the presence of a hierarchical network organization, disassortative mixing of vertices. In [5], spectral methods were introduced to cluster the words of the same class in a syntactic dependency network.\nIn [14], the authors examined the structural properties of two weighted networks, a linguistic network and a scientific collaboration network. The linguistic network in this paper is simply a co-occurrence network instead of syntactic dependency network. The weight of edges between vertices were considered\n4\nin the paper. The networks built from shuffled text were used as the null hypothesis to compare with the real network in order to find the characteristic of the real ones. Through the analysis of differences between the real network and a shuffled network, they proved that the scale free degree distribution are induced by Zipf\u2019s law.\nIn [4], the authors model the evolution of a network of language based upon preferential attachment. That is, a new word is connected to a word in the network with probability proportional to the latter\u2019s degree. The model that they develop, almost astonishingly, agrees very well with the empirical results obtained in [6]. The degree distribution of the theoretical network follows a composite power-law with exponents exactly equal to those obtained by [6]. This work further validates the scale-free nature of human language.\nA stochastic model of language was created on the basis of combined local and global preferential attachment (PA) in [13]. A new word attaches to the nearest neighbor with highest degree in the local PA scheme, whereas it attaches to the word with the highest degree in the entire graph in global PA. They find that their model produces a network with scale-free properties with various statistics in agreement with empirical evidence. They argue that plain PA schemes don\u2019t necessarily take into account the hierarchical nature of networks as is hinted at by Zipf\u2019s law and hence their combined model, which follows a mixed local-global growth is more suitable for the desired purpose.\n5"}, {"heading": "3 Methodology", "text": "In this article we consider the two aspects that have not been previously studied in depth in word cooccurrence networks: (1) the effect of the strength of links and (2) the word forms that vertices stand for.\nWe consider four levels of cooccurrence constraints (from less to more constraints):\n1. RANDOC Words were permuted randomly within each document. Two vertices are linked if the corresponding words are adjacent in the permuted text. sentence boundaries were respected. That is, the length of the original sentences was kept constant\n2. RANSEN Words were permuted randomly within each sentence. Two vertices are linked if the corresponding words are adjacent in the permuted text.\n3. PLAIN Two vertices are linked if the corresponding words are adjacent in the original text.\n4. COLL Two vertices are linked if the corresponding words form a highly associative bigram. Fisher\u2019s Exact Test was used to extract highly associative bigrams (significance value \u2264 0.01) from the unscrambled English text. Fisher\u2019s Exact Test is used because it is considered a more suitable test for determining word associativeness [19] We assume that the frequency associated with a bigram < word1 >< word2 > is stored in a 2x2 contingency table:\nword2 \u00acword2 word1 n11 n12 \u00acword1 n21 n22\nwhere n11 is the number of times < word1 >< word2 > occur together, n12 is the number of times < word1 > occurs with some word other than word2, and so on. Fisher\u2019s exact test is calculated by fixing the marginal totals and computing the hypergeometric probabilities for all the possible contingency tables.\nWe also consider two kinds of vertices\n6 1. RAW Words are used in their raw forms.\n2. LEMM A lemmatized form of words is used.\nFor each corpus, a different cooccurrence network is built for each level of constraints and type of vertex. This results in 8 different networks for each language. We assume that that our graphs are undirected and that loops are allowed\nWe define the structure of an undirected graph of n vertices through a binary adjacency matrix A = {aij}, where aij = 1 if the vertex i and the vertex j are linked and aij = 0 otherwise. Notice that the matrix is symmetric (aij = aik) and aii = 1 is possible. We define ki, the degree of the i-th vertex, as\nki = n\u2211 j=1 aij . (2)\nThe English text we used to construct all the networks comes from a random subset of the British National Corpus. The subset had 7.5M words. We used the Fisher-Yates shuffle algorithm for creating random permutations within sentences and documents. This algorithm produces each possible permutation with equal probability.\nWe report several general network statistics including average degree, diameter, average shortest path, global and local clustering coefficient. The diameters and average shortest paths were calculated using the maximum connected components while the other statistics were calculated used the whole network.\nIn addition to English, we consider several other languages. We used a subset of the Spanish, French, and Chinese Gigaword corpora. We built four different networks for each language (RANDOC, RANSENT, PLAIN, and COLL). More statistics regarding the datasets used for constructing the networks is shown in Table 6."}, {"heading": "4 Results", "text": ""}, {"heading": "4.1 Degree Distribution", "text": "The degree distributions for all four networks ENG-COLL, ENG, ENG-RANSEN and ENG-RANDOC are plotted in figure 2. It is worth mentioning that all figures are on a base 10 log-log scale. All four distributions show characteristics of a power-law. The tails of the corresponding cumulative degree\n7\n8\n9\n10\n11\ndistributions are plotted in figure 4 and are fitted by lines. The power-law coefficient is fitted with the maximum likelihood method as recommended in [18]. The coefficients thus obtained from the plots are \u03b3ENG\u2212COLL = 2.0, \u03b3ENG = 2.3, \u03b3ENG\u2212RANSEN = 2.1 and \u03b3ENG\u2212RANDOC = 2.0. These are all very similar values and are also close to the values obtained for co-occurrence and frequent bigrams (\u03b3 = 2.7 each) networks in [6].\nFor Spanish, French and Chinese experiment, the degree distributions are plotted in Figure. 5. The corresponding cumulative degree distributions are plotted in Figure. 6. All distributions show characteristics of a power-law. The coefficients for the networks are \u03b3SPA\u2212COLL = 2.1, \u03b3FRE\u2212COLL = 2.1, \u03b3CHI\u2212COLL = 2.5 \u03b3SPA = 2.3, \u03b3FRE = 2.2, \u03b3CHI = 2.8, \u03b3SPA\u2212RANSEN = 2.2, \u03b3FRE\u2212RANSEN = 2.2, \u03b3CHI\u2212RANSEN = 2.3, \u03b3SPA\u2212RANDOC = 2.1, \u03b3FRE\u2212RANDOC = 2.1, \u03b3CHI\u2212RANDOC = 2.7. These are similar to the English dataset (\u03b3ENG = 2.3, \u03b3RANSEN = 2.1).\nThe distributions of the networks based on the lemmatized form of the words are pretty similar to the original ones. The degree distributions for stemmed ENG-COLL, ENG and ENG-RANSEN are plotted in Figure 7 and Figure 8. The coefficients are \u03b3STEMENG\u2212COLL = 2.0, \u03b3STEMENG = 2.2, \u03b3STEMENG\u2212RANSEN = 2.2 and \u03b3STEMENG\u2212RANDOC = 2.1. The degree distributions for stemmed FRE-COLL, FRE, FRE-RANSEN and FRE-RANDOC are plotted in Figure 9 and Figure 10. The coefficients are \u03b3STEMFRE\u2212COLL = 2.3, \u03b3STEMFRE = 2.1, \u03b3STEMFRE\u2212RANSEN = 2.2 and \u03b3STEMFRE\u2212RANDOC = 2.1. The degree distributions for stemmed SPA-COLL, SPA, SPA-RANSEN and SPA-RANDOC are plotted in Figure 11 and Figure 12. The coefficients are \u03b3STEMSPA\u2212COLL = 2.4, \u03b3STEMSPA = 2.3, \u03b3STEMSPA\u2212RANSEN = 2.1 and \u03b3STEMSPA\u2212RANDOC = 2.3.\n12"}, {"heading": "4.2 Link density", "text": "As expected, the more cooccurrence constraints, the lower the density of links. In particular, we find a perfect negative rank correlation between the mean degree of vertices and the level of constraints for all the networks of the same language and the same kind of vertex. More formally, we have that the mean degree k\u0304 obeys k\u0304COLL < k\u0304PLAIN < k\u0304RANSEN < k\u0304RANDOC . Knowing that there are not ties between values of k\u0304 for the same language and kind of vertex, the probability that the expected ordering is produced by chance is 1/4! \u2248 0.041. Thus, the perfect correlation between the level of constraints is statistically significant at a significance level of 0.05.\nSince the number of vertices of the network is the same for all networks of the same language (corpus) and the same kind of vertex, this perfect negative correlation is also equivalent to a perfect negative correlation between link density and level of constraints. The link density \u03b4 of a network where loops are allowed is defined as the proportion of linkable pairs of vertices that are linked. For the particular case of an undirected network where loops are allowed, we have\n\u03b4 = 1( n+1 2 ) n\u2211 i=1 n\u2211 j=i aij , (3)\n= k\u0304\nn+ 1 , (4)\nwhere n is the number of vertices (if loops where not allowed this would be \u03b4 = k\u0304/(n \u2212 1). The link density of the network of raw words is smaller than that of the network of stemmed words with the same\n14\n15\nlevel of constraints, i.e. k\u0304 < k\u0304STEMMED or equivalently, \u03b4 < \u03b4STEMMED."}, {"heading": "4.3 Small-worldness", "text": "We define d as the shortest distance between a pair of nodes. The average shortest path is the average of all such distances. The diameter of a network is the number of links in the shortest path between the furthest pair of nodes. The diameter and the average shortest path for all networks are shown in Table 8 and Table ?? respectively. The diameters for all four networks, as shown in Table 8, are small. These networks are small worlds. What is surprising is that the diameters for RANSEN and RANDOC are smaller than that for ENG. This may be attributed to the randomization produces which forms links between words which are otherwise not connected in ENG. This results in a network that is even faster to navigate.\nThe mean shortest vertex-vertex distance, d\u0304 obeys,\n1. d\u0304 > d\u0304STEMMED for all languages and constrain levels.\n2. d\u0304COLL < d\u0304RANDOC < d\u0304RANSEN < d\u0304PLAIN in all languages except Chinese, where we have d\u0304RANDOC < d\u0304COLL < d\u0304RANSEN < d\u0304PLAIN .\nThe diameter, i.e. the longest shortest path, dmax obeys\n1. dmax \u2265 dmaxSTEMMED for all languages and constraint levels (we would have dmax > dmaxSTEMMED if COLL was excluded).\n2. dmaxCOLL < d max RANDOC < d max RANSEN < d max PLAIN\n16\n17"}, {"heading": "4.4 Clustering", "text": "We examined two clustering coefficients for all networks The local clustering is defined as the mean of the vertex clustering,\nC = 1\nn n\u2211 i=1 Ci, (5)\nwhere Ci is the clustering of the i-th vertex and n is the number of vertices of the network. Ci is the proportion of linkable pairs of adjacent vertices to vertex i that are linked .\nThe global clustering coefficient is based on triplets of nodes. A triplet could be either open or closed. In an open triplet, the three nodes are connected by two edges. Where as in a closed triplet, they are connected with three edges. The global clustering coefficient is computing by calculating the ratio between the number of closed triplets and the total number of triplets. Both local and global clustering coefficients ignore the loops and the isolated vertices. View the networks as undirected. The local and global clustering coefficients for all networks are shown in Tables 11 and 10 respectively."}, {"heading": "4.5 Degree correlations", "text": "Assortative mixing is a bias in favor of connections between network nodes with similar characteristics. In other words the higher the degree of a vertex, the higher the degree of its neighbors. On the other hand, disassortative mixing refers to the phenomenon where the higher the degree of a vertex the lower the degree of its neighbors.\nFigures 13,14,15,16,17,18 and 19 show the relation between the degree of a vertex k and the normalized mean degree of the nearest neighbors of vertices of degree k. We notice that the normalized mean degree of the nearest neighbors of vertices of degree k, shrinks as k grows for all networks, (i.e. vertices with large degree tend to connect with vertices with low degree). The figures show that the networks exhibit disassortative mixing pattern for all languages and levels of constraint and regardless of the kind of\n18\nTable 9: Average Shortest Paths (d\u0304) for all networks. Standard deviation is shown in parenthesis Statistic COLL PLAIN RANSEN RANDOC\nENG-RAW 2.48 (0.54) 3.08 (0.72) 2.95 (0.63) 2.90 (0.56) ENG-LEMM 2.48 (0.54) 3.02 (0.70) 2.89 (0.60) 2.83 (0.54) FRE-RAW 2.49 (0.65) 3.41 (0.83) 3.07 (0.69) 2.96 (0.58) FRE-LEMM 2.45 (0.58) 3.30 (0.83) 2.99 (0.69) 2.86 (0.57) SPA-RAW 2.50 (0.60) 3.69 (0.98) 3.06 (0.68) 3.00 (0.62) SPA-LEMM 2.45 (0.59) 3.19 (0.79) 2.96 (0.68) 2.88 (0.59) CHI 2.30 (0.55) 2.62 (0.71) 2.32 (0.52) 2.26 (0.54)\nthe vertices (raw or lemmatized words). The relationship between knn and k is a consequence of word frequencies. The network randomization algorithms destroys the pattern for all complexity levels."}, {"heading": "4.6 The relationship between vertex frequency and degree", "text": "Please, put a table showing the 5 or 10 most frequent words in the English corpus and the 5 or 10 most connected words in each of the networks: COLL, PLAIN, RANSEN and RANDOC.\nTo examine the relationship between vertex frequency and degree, we use linear regression after excluding some outlier with frequency > 105. The result of linear regression for the real network shows a strong correlation between degree and frequency. The slope is 2.86, the p value is less than 2 \u2217 10\u221216, R2 is 0.89. For the random network, we also observe a high correlation between degree and frequency. The slope is 2.11, the p value of the correlation between two variables is less than 2 \u2217 10\u221216, R2 is 0.89. In addition, we averaged the degree of words of a certain frequency and then make a plot for all the frequency range for all networks(Fig. 20, Fig. 21, Fig. 22, Fig. 23, Fig. 24, Fig. 25, Fig. 26).\nWe notice that the higher the frequency of a word, the higher its degree. We also notice that the hubs of the networks are high frequency words. Lists of the top ten frequent words and the top ten most connected words in all English networks are shown in Tables 1, 2, 3, 4, and 5."}, {"heading": "5 Conclusion", "text": "We studied the topological properties of linguistics networks at different levels of linguistic constraints. We found out that the networks produced from randomized data exihibit small worlds and scale-free characteristics. One possible explanation would be that degree distributions are functions of the word frequencies. However human language is a very complex \u201csystem\u201d and there is no simple way to explain this observation. We also find out hat the degree distribution of a co-occurrence graph does change under randomization Further, network statistics such as diameter and clustering coefficient too seem to depend on the degree distributions of the underlying network.\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29"}, {"heading": "Acknowledgements", "text": "The authors would like to thank Ramon Ferrer i Cancho for his valuable comments and contributions."}], "references": [{"title": "Topology of evolving networks: local events and universality", "author": ["Reka Albert", "Albert L. Barabasi"], "venue": "Phys. Rev. Lett., pages 5234\u20135237,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Zipf and Heaps laws coefficients depend on language", "author": [], "venue": "CICLing", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Complex networks and human language", "author": ["S.N. Dorogovtsev", "J.F. Mendes"], "venue": "Advanced Physics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Spectral methods cluster words of the same class in a syntactic dependency network", "author": ["R. Ferrer-i-Cancho", "A. Capocci", "G. Caldarelli"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "The small world of human language", "author": ["R. Ferrer-i-Cancho", "R.V. Sole"], "venue": "Proceedings of The Royal Society of London. Series B, Biological Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "The structure of syntactic dependency networks: insights from recent advances in network theory", "author": ["Ramon Ferrer-i-Cancho"], "venue": "Problems of Quantitative Linguistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Random texts do not exhibit the real Zipf\u2019s law-like rank distribution", "author": ["Ramon Ferrer-i Cancho", "Elvev Brita"], "venue": "PLoS ONE, 5(3):e9411,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Correlations in the organization of large-scale syntactic dependency networks. TextGraphs-2: Graph-Based Algorithms for Natural Language", "author": ["Ramon Ferrer-i-Cancho", "Alexander Mehler", "Olga Pustylnikov", "Albert Diaz-Guilera"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Patterns in syntactic dependency networks", "author": ["Ramon Ferrer-i-Cancho", "Ricard V. Sole", "Reinhard Kohler"], "venue": "Phys. Rev. E,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Random texts exhibit Zipf\u2019s-law-like word frequency distribution", "author": ["Wentian Li"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["Christopher Manning", "Hinrich Schuetze"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Network properties of written human language", "author": ["A.P. Masucci", "G.J. Rodgers"], "venue": "Physical Review,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Differences between normal and shuffled texts: structural properties of weighted networks", "author": ["A.P. Masucci", "G.J. Rodgers"], "venue": "Advances in Complex Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Large text networks as an object of corpus linguistic studies", "author": ["A. Mehler"], "venue": "Corpus linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Finitary models of language users", "author": ["George Miller", "Noam Chomsky"], "venue": "Handbook of Mathematical Psychology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1963}, {"title": "Some effects of intermittent silence", "author": ["George A. Miller"], "venue": "American Journal of Psychology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1957}, {"title": "Power laws, pareto distributions and Zipf\u2019s law", "author": ["Mark Newman"], "venue": "American Journal of Psychology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Fishing for exactness", "author": ["Ted Pedersen"], "venue": "In Proceedings of the South Central SAS User\u2019s Group (SCSUG-96) Conference,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1996}, {"title": "Modeling statistical properties of written text", "author": ["M.A. Serrano", "A. Flammini", "F. Menczer"], "venue": "PLoS ONE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth", "author": ["Mark Steyvers", "Joshua B. Tenenbaum"], "venue": "Cognitive Science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Selective studies and the principle of relative frequency in language, 1932", "author": ["George Zipf"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1932}], "referenceMentions": [{"referenceID": 13, "context": "Language, when modeled as a network, has been shown to exhibit small-world and scale-free properties [15].", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "A network is said to be scale-free if its degree distribution follows a a power-law [1].", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "That is, a new word being introduced into the system tends to associate with a pre-existing word that is highly frequent [4].", "startOffset": 121, "endOffset": 124}, {"referenceID": 20, "context": "However, probably the first ever mathematical law that hinted at the complex nature of language was given by Zipf [22].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "1 and \u03b1 \u2248 1) [17], [16] but it has been shown to be obeyed by other natural languages [2].", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "1 and \u03b1 \u2248 1) [17], [16] but it has been shown to be obeyed by other natural languages [2].", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "1 and \u03b1 \u2248 1) [17], [16] but it has been shown to be obeyed by other natural languages [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 15, "context": "It seems surprising at first that Zipf\u2019s law is also obeyed by random texts [17], [16].", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "It seems surprising at first that Zipf\u2019s law is also obeyed by random texts [17], [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 9, "context": "[11] demonstrates, through numerical simulation, that random texts follow Zipf\u2019s law because of the choice of rank as the independent variable in that relationship.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Recent work [8] has shown that this is not the case.", "startOffset": 12, "endOffset": 15}, {"referenceID": 11, "context": "In [13], the authors state that a linguistic network formed from a random permutation of words also exhibits a power-law.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In [12], a collocation (frequent bigram) is defined as a conventional way of saying something.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "Here, the method presented in [6] is improved upon by employing a Fisher\u2019s exact test to extract frequent bigrams [19].", "startOffset": 30, "endOffset": 33}, {"referenceID": 17, "context": "Here, the method presented in [6] is improved upon by employing a Fisher\u2019s exact test to extract frequent bigrams [19].", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "Network models of language have been studied by many researchers in the past [6, 21].", "startOffset": 77, "endOffset": 84}, {"referenceID": 19, "context": "Network models of language have been studied by many researchers in the past [6, 21].", "startOffset": 77, "endOffset": 84}, {"referenceID": 4, "context": "One of the earliest such studies [6] constructed a \u201crestricted\u201d (frequent bigrams) network and an \u201cunrestricted\u201d (co-occurrence) network from a subset of the British National Corpus [3].", "startOffset": 33, "endOffset": 36}, {"referenceID": 10, "context": "This is a simplistic and unreliable filter for word dependence [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 17, "context": "[19] demonstrates the suitability of Fisher\u2019s (right-sided) Exact test for this purpose.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Beyond universal regularities such as Zipf\u2019s law, [20] recently examined burstiness, topicality, semantic similarity distribution and their interrelation and modeled them with two mechanisms, namely frequency ranking with dynamic reordering and memory across documents.", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "In [7], the author overview-ed the past studies on linguistic networks and discussed the possibilities and advantages of network analysis of syntactic dependency networks.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [10], network properties such as small world structure, heterogeneity, hierarchical organization, betweenness centrality and assortativeness etc were examined for the syntactic networks from Czech, Romanian and German corpora.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "Seven corpora were examined by similar complex network analysis methods in [9].", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "In [5], spectral methods were introduced to cluster the words of the same class in a syntactic dependency network.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "In [14], the authors examined the structural properties of two weighted networks, a linguistic network and a scientific collaboration network.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [4], the authors model the evolution of a network of language based upon preferential attachment.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "The model that they develop, almost astonishingly, agrees very well with the empirical results obtained in [6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "The degree distribution of the theoretical network follows a composite power-law with exponents exactly equal to those obtained by [6].", "startOffset": 131, "endOffset": 134}, {"referenceID": 11, "context": "A stochastic model of language was created on the basis of combined local and global preferential attachment (PA) in [13].", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "Fisher\u2019s Exact Test is used because it is considered a more suitable test for determining word associativeness [19] We assume that the frequency associated with a bigram < word1 >< word2 > is stored in a 2x2 contingency table:", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "The power-law coefficient is fitted with the maximum likelihood method as recommended in [18].", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "7 each) networks in [6].", "startOffset": 20, "endOffset": 23}], "year": 2016, "abstractText": "This paper studies the effect of linguistic constraints on the large scale organization of language. It describes the properties of linguistic networks built using texts of written language with the words randomized. These properties are compared to those obtained for a network built over the text in natural order. It is observed that the \u201crandom\u201d networks too exhibit small-world and scale-free characteristics. They also show a high degree of clustering. This is indeed a surprising result one that has not been addressed adequately in the literature. We hypothesize that many of the network statistics reported here studied are in fact functions of the distribution of the underlying data from which the network is built and may not be indicative of the nature of the concerned network.", "creator": "LaTeX with hyperref package"}}}