{"id": "1102.4021", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2011", "title": "Privacy Preserving Spam Filtering", "abstract": "We not current perspective to training given formula_1 day-to-day disorientation pseudoprime in early similar being where training data keeping came be kept private. We provide has comparative analysis of the taken while this restriction and experimental results no three cause of huge scale spam fingerprint. High both transmits mesh presumably use character formula_1 - 62 known design which far in large lush parameterized might which applying our wto because is not logical. We future groups dimensionality reduction and single-molecule discussion and aid a biographical analysis following own speed and objective exchanges - off. Our results seeing clear we can achieve the accuracy of legislative country taken art subliminal vapor 30 equivalent training working training take other generally - private parody of logistic regression.", "histories": [["v1", "Sat, 19 Feb 2011 20:40:56 GMT  (206kb,D)", "http://arxiv.org/abs/1102.4021v1", "9 pages"], ["v2", "Sun, 18 Sep 2011 05:37:43 GMT  (208kb,D)", "http://arxiv.org/abs/1102.4021v2", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["manas a pathak", "mehrbod sharifi", "bhiksha raj"], "accepted": false, "id": "1102.4021"}, "pdf": {"name": "1102.4021.pdf", "metadata": {"source": "CRF", "title": "Privacy Preserving Spam Filtering", "authors": ["Manas A. Pathak", "Mehrbod Sharifi", "Bhiksha Raj"], "emails": ["manasp@cs.cmu.edu", "mehrbod@cs.cmu.edu", "bhiksha@cs.cmu.edu"], "sections": [{"heading": null, "text": "General Terms Data Privacy, Spam Filtering, Logistic Regression"}, {"heading": "1. INTRODUCTION", "text": "Email is a private medium of communication with the message intended to be read only by the recipients. Due to the sensitive nature of the information in email, there might be personal, strategic, and legal constraints against sharing emails. This is a problem in many applications such as spam filtering.\nOver the years, spam has become a major problem: 79.55% of all emails sent in January 2011 were spam [13]. Email users can benefit from using accurate spam filters, which could greatly reduce the loss of time and productivity due to spam email. The accuracy of spam filters based on statistical classification models can be improved by training on aggregates of data from a large number of email users. However, the training and application of spam filters should not be at the expense of user privacy, with users being required to make their emails available to the spam filtering provider.\nIn this paper, we propose a solution that enables users to share their private email data to train and apply spam filters based on a logistic regression classifier, while satisfying\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 2010 ACM X-XXXXX-XX-X/XX/XX ...$10.00.\nprivacy constraints. The specific form of learning procedure that we consider is an online procedure: in a practical setting neither users nor the provider of a classifier are likely to revisit spam to update a classifier, when there is a continuously arriving stream of messages available. Although primarily directed at spam filtering, the proposed solution is also applicable to any form of private text classification and in general to any binary classification setting where privacy is important, e.g., predicting the likelihood of disease based on a person\u2019s private medical records, and also extends to batch processing scenarios where the learning procedure must iterate over data.\nFormally, we consider two parties\u201cAlice\u201dand\u201cBob\u201d, where Alice has access to emails and Bob is interested in training a classification model using Alice\u2019s data. The privacy constraint of our protocol is that Bob should not be able to observe the contents of the emails belonging to Alice, and Alice should not be able to observe the parameters of the model trained by Bob. While the motivation behind Alice keeping her emails private is more obvious, Bob might want to keep his classification model private in situations where the technology behind learning model might constitute as his intellectual property, or he wants to use the model as a restricted pay per use spam filtering service. We further show that our protocol can also be used in the case where Bob is able to learn the classification model from the private email data belonging to multiple parties. We also present an evaluation protocol where Bob is able to apply his model to classify the email data belonging to another party while maintaining the same privacy constraints.\nOur privacy preserving protocol falls into the broad class of secure multiparty computation (SMC) algorithms [17]. In the SMC framework, multiple parties desire to compute a function that combines their individual inputs. The privacy constraint is that no party should learn anything about inputs belonging to any other party besides what can be inferred from the value of the function. We construct our protocol using a cryptosystem satisfying homomorphic encryption [11], in which operations on encrypted data correspond to operations on the original unencrypted data (Section 3.2). We further augment our protocol with additive and multiplicative randomization, and present an information theoretic analysis of the security of the protocol.\nThe benefit of training and evaluating a spam filtering classifier privately comes with an additional burden of encryption and data transmission costs. We find that these costs are linear in the number of training data instances and the data dimensionality. As the size of our character four-\nar X\niv :1\n10 2.\n40 21\nv1 [\ncs .L\nG ]\n1 9\nFe b\n20 11\ngram feature representation of the text data is extremely large (e.g., one million features), direct application of our protocol becomes prohibitively expensive. Towards this, we apply suitable dimensionality reduction techniques to make the training protocol computationally feasible for practical settings. We present extensive evaluation of our protocol on a large scale email dataset from the CEAS 2008 spam filtering challenge. We demonstrate that our protocol is able to achieve state of the art performance in a feasible amount of running time."}, {"heading": "2. RELATED WORK", "text": "Email spam filtering is a well established area of research. The accuracy of the best systems in the 2007 spam filter competition by the CEAS was better than 0.9999 [3]. Our implementation is an online logistic regression classifier implementation inspired by [7] which on application to binary character four-gram features was shown to have near state of the art accuracy [3]. There has also been a much recent work on constructing privacy preserving protocols for other data mining tasks such as decision trees [14], set matching [5], clustering [9], naive Bayes [15], support vector machines [16], etc.\nThe application of privacy preserving techniques to large scale real world problems of practical importance, such as spam filtering, is an emerging area of research. Li, et al. [8] present a distributed framework for privacy aware spam filtering. Their method is based on applying a one-way fingerprinting transformation [1] to the message text and comparing two emails using a Hamming distance metric and does not involve any form of statistical learning. It also requires that the spam emails belonging to all users should be revealed which does not match our privacy criteria because we consider all emails, spam or not, to be private.\nTo the best of our knowledge, this paper is the first to describe a practical privacy-preserving framework for logistic regression with a real world application to spam filtering."}, {"heading": "3. PRELIMINARIES", "text": ""}, {"heading": "3.1 Classification Model: Logistic Regression in the Batch and Online Settings", "text": "The training dataset consisting of n documents classified by the user as spam or ham (i.e., not spam) are represented as the labeled data instances (x, y) = {(x1, y1), . . . , (xn, yn)} where xi \u2208 Rd and yi \u2208 {\u22121, 1}. In the batch learning setting, we assume that the complete dataset is available at a given time. In the logistic regression classification algorithm, we model the class probabilities by a sigmoid function\nP (yi = 1|xi, w) = 1\n1 + e\u2212yiwT xi .\nWe denote the log-likelihood for the weight vector w computed over the data instances (x, y) by L(w, x, y). Assuming the data instances to be i.i.d., the data log-likelihood L(w, x, y) is equal to\nL(w, x, y) = log \u220f i\n1\n1 + e\u2212yiwT xi = \u2212 \u2211 i log[1 + e\u2212yiw T xi ].\nWe maximize the data log-likelihood L(w, x, y) to obtain the classifier with the optimal weight vector w\u2217. L(w, x, y) is a convex and differentiable function of w, however, as there is\nno closed form solution for w\u2217, we use the gradient ascent algorithm. Starting with a uniformly initialized vector w(0), in the tth iteration, we update w(t) as w(t+1) = w(t) + \u03b7\u2207L(w(t), x, y) = w(t) + \u03b7 \u2211 i yix T i 1 + e yiw T (t) xi ,\n(1)\nwhere \u03b7 is the pre-defined step size. We terminate the procedure on convergence between consecutive values of w(t).\nIn the online learning setting, the data instances are obtained incrementally rather than being completely available at a given instance of time. In this case, we start with a model with the uniformly random weight vector w(0). A model w(t) learned using the first t instances, is updated after observing a small block of k instances. with the gradient of the log-likelihood computed over that block."}, {"heading": "3.2 Homomorphic Encryption", "text": "In a homomorphic cryptosystem, operations performed on encrypted data (\u201cciphertext\u201d) map to the corresponding operations performed on the original unencrypted data (\u201cplaintext\u201d). If + and \u00b7 are two operators and x and y are two plaintexts, a homomorphic encryption function E satisfies\nE[x] \u00b7 E[y] = E[x+ y].\nThis allows one party to encrypt the data using a homomorphic encryption scheme and another party to perform operations without being able to observe the plaintext data. This property forms the fundamental building block of our privacy preserving protocol.\nIn this work, we use the additively homomorphic Paillier cryptosystem [11] which also satisfies semantic security. The Paillier key generation algorithm produces a pair of N -bit numbers (n, g) constituting the public key corresponding to the encryption function E : Zn 7\u2192 Zn2 and another pair of N -bit numbers (\u03bb, \u00b5) constituting the public key corresponding to the decryption function D : Zn2 7\u2192 Zn.\nGiven a plaintext x \u2208 Zn, the encrypted text is given by:\nE[x] = gxrn mod n2,\nwhere r is a random number sampled uniformly from Zn. Using a different value of the random number r provides semantic security \u2013 two different encryptions of a number x say, E[x; r1] and E[x; r2] will have different values but decrypting each of them will result in the same number x. It can be easily verified that the above encryption function satisfies the following properties:\n1. For any two ciphertexts E[x] and E[y],\nE[x] E[y] = E[x+ y mod n2].\n2. And as a corollary, for any ciphertext E[x] and plaintext y,\nE[x]y = E[x y mod n2].\nExtending the Encryption Function to Real Numbers Paillier encryption as most other cryptosystems is defined over the finite field Zn = {0, . . . , n \u2212 1}. However, in our protocol we need to encrypt real numbers, such as the training data and model parameters. We make the following modifications to the encryption function to support this.\n1. Real numbers are converted to a fixed precision floating point representation. For a large constant C, a real number x is represented as bCxc = x\u0304.\nE[x\u0304] = E[bCxc], D[E[x\u0304]] = E[bCxc]/C = x.\n2. The encryption of a negative integer is represented by the encryption of its modular additive inverse. If \u2212x is a negative integer,\nE[\u2212x] = E[n\u2212 x].\n3. Exponentiation of an encrypted number by a negative integer is represented as the exponentiation of the modular multiplicative inverse of the encrypted number by the corresponding positive integer. We represent the exponentiation of the ciphertext E[x] by a negative integer \u2212y as\nE[x]\u2212y = E[x\u22121 mod n2]y.\nRepresenting real numbers by a fixed precision number introduces a small error due to the truncation which is directly proportional to the value of C. This representation also reduces the domain of the encryption function from 0, . . . , n\u22121 to 0, . . . , bn\u22121\nC c. We need to ensure that the result of homo-\nmorphic operations on encrypted functions do not overflow the range, so we need to increase the bit-size N of the encryption keys proportionally with C. As the computational cost of the encryption operations is also proportional to N , this creates a trade-off between accuracy and computation cost.\nThe representation of negative integers on the other hand does not introduce any error but further halves the domain of the encryption function from 0, . . . , bn\u22121\nC c to 0, . . . , bn\u22121 2C c."}, {"heading": "4. PRIVACY PRESERVING CLASSIFIER TRAINING AND EVALUATION", "text": ""}, {"heading": "4.1 Data Setup and Privacy Conditions", "text": "We define two parties \u201cAlice\u201d and \u201cBob\u201d. Alice has access to a sequence of labeled training data instances (x, y) = {(x1, y1), . . . , (xn, yn)}. Bob is interested in training a logistic regression classifier with weight vector w \u2208 Rd over (x, y) as discussed in Section 3.1. In the online learning setting, multiple users can interact with Bob at one time to update his classifier using their training data. As any of these parties play the same role as Alice in their interactions with Bob, we represent them by her.\nThe privacy constraint implies that Alice should not be able to observe w and Bob should not be able to observe (xi, yi). The parties are assumed to be semi-malicious, i.e., they correctly execute the steps of the protocol and do not attempt to cheat by using fraudulent data as input in order to extract additional information about the other parties. The parties are assumed to be curious, i.e., they keep a transcript of all intermediate results and can use that to gain as much information as possible."}, {"heading": "4.2 Private Training Protocol", "text": "Bob generates a public and private key pair for an N -bit Paillier cryptosystem and provides the public key to Alice.\nIn this cryptosystem, Bob is able to perform both encryption and decryption operations while Alice can perform only decryption.\nAs mentioned before, we use the homomorphic properties of Paillier encryption to allow the parties to perform computations using private data. The update rule requires Bob to compute the gradient of the data log-likelihood function \u2207L(w(t), x, y) which involves exponentiation and division and cannot be done using only homomorphic additions and multiplications. We supplement the homomorphic operations with Bob performing those operations on multiplicative shares to maintain the privacy constraints. As mentioned in Section 3.2, the domain of the encryption function is {0, . . . , bn\u22121\n2C c}, which we denote by D. We sample the\nrandomizations uniformly from this set. Bob initiates the protocol with a uniform w(0) and the gradient update step \u03b7 is publicly known. We describe the tth iteration of the protocol below.\nInput: Alice has (x, y) and the encryption key, Bob has w(t) and both encryption and decryption keys. Output: Bob has w(t+1).\n1. Bob encrypts w(t) and transfers E[w(t)] to Alice.\n2. For each training instance xi, i = 1, . . . , n, Alice computes\nd\u220f j=1 E[w(t)j ] yixij = E [ d\u2211 j=1 yiw(t)jxij ] = E [ yiw T (t)xi ] .\n3. Alice samples n numbers r1, . . . , rn uniformly from 1, . . . , N \u2212 1 where N is the size of the encryption key and computes\nE [ yiw T (t)xi ] \u00b7 E[\u2212ri] = E [ yiw T (t)xi \u2212 ri ] .\nAlice transfers E [ yiw T (t)xi \u2212 ri ] to Bob.\n4. Bob decrypts this to obtain yiw T (t)xi \u2212 ri. In this way,\nAlice and Bob have additive shares of the inner products yiw T (t)xi.\n5. Bob exponentiates and encrypts his shares of the inner products. He transfers E [ e yiw T (t)xi\u2212ri ] to Alice.\n6. Alice homomorphically multiplies the quantities she obtained from Bob by the exponentiations of her corresponding random shares to obtain the encryption of the exponentiations of the inner products.\nE [ e yiw T (t)xi\u2212ri ]eri = E [ e yiw T (t)xi ] .\nAlice homomorphically adds E[1] to these quantities to obtain E [ 1 + e yiw T (t)xi ] .\n7. Alice samples n numbers q1, . . . , qn from D using a bounded Power law distribution1. We analyze the reasons for this in Section 5.2. Alice then homomorphically computes\nE [ 1 + e yiw T (t)xi ]qi = E [ qi ( 1 + e yiw T (t)xi )] .\n1We require that q has the pdf P (q) \u221d 1 q for 1 \u2264 q \u2264 |D|. q can be generated using inverse transform sampling.\nShe transfers these quantities to Bob.\n8. Bob decrypts these quantities and computes the reciprocal 1\nqi\n( 1+e yiw T (t) xi ) . He then encrypts the reciprocals and sends them to Alice.\n9. Alice homomorphically multiplies qi with the encrypted reciprocals to cancel out her multiplicative share.\nE  1 qi ( 1 + e yiw T (t) xi ) qi = E [ 1 1 + e yiw T (t) xi ] .\n10. Alice then homomorphically multiplies the encrypted reciprocal by each component of yix T i to obtain the\nencrypted d-dimensional vector\nE\n[ 1\n1 + e yiw\nT (t) xi\n]yixTi = E [ yix T i\n1 + e yiw\nT (t) xi\n] .\nShe homomorphically adds each encrypted component to obtain\u220f\ni\nE\n[ yix T i\n1 + e yiw\nT (t) xi\n] = E [\u2211 i\nyix T i 1 + e yiw T (t) xi\n] .\nThis is the encrypted gradient vector E [ \u2207L(w(t), x, y) ] .\n11. Alice homomorphically updates the encrypted weight vector she obtained in Step 1 with the gradient.\nE[w(t+1)] = E[w(t)] E [ \u2207L(w(t), x, y) ]\u03b7 = E [ w(t) + \u03b7\u2207L(w(t), x, y) ] .\n12. Alice then sends the updated weight vector E[w(t+1)] to Bob who then decrypts it to obtain his output.\nIn this way, Bob is able to update his weight vector using Alice\u2019s data while maintaining the privacy constraints. In the batch setting, Alice and Bob repeat Steps 2 to 11 to perform the iterative gradient descent. Bob can check for convergence in the value of w between iterations by performing Step 12. In the online setting, Alice and Bob execute the protocol only once with using Alice using a typically small block of k data instances as input.\nExtensions to the Training Protocol 1. Training on data belonging to multiple parties.\nIn the online setting, we do not make any assumption about which data holding party is participating in the protocol. Alice1 can use her data to update w privately and Alice2 can then use her data to perform the same online update.\nIn the batch setting, multiple parties can execute one iteration of the protocol individually with Bob to compute the encrypted gradient on their own data. Finally, Bob can receive the encrypted gradients from all the parties and update the weight vector as follows.\nw(t+1) = w(t) + \u03b7 \u2211 k \u2207L(w(t), xk, yk),\nwhere (x1, y1), . . . , (xK , yK) are the individual datasets belonging to K parties.\n2. Training a regularized classifier.\nThe protocol can easily be extended to introduce `2 regularization, which is a commonly used method to prevent over-fitting. In this case, the update rule becomes\nw(t+1) = w(t) + \u03b7\u2207L(w(t), x, y) + 2\u03bbw(t),\nwhere \u03bb is a regularization constant.\nThis can be accommodated by Alice homomorphically adding the term 2\u03bbw(t) to the gradient in Step 11.\nIn order to identify the appropriate value of \u03bb to use, Alice and Bob can perform m-fold cross-validation by repeatedly executing the private training and evaluation protocols over different subsets of data belonging to Alice."}, {"heading": "4.3 Private Evaluation Protocol", "text": "Another party \u201cCarol\u201d having one test data instance x\u2032 \u2208 Rd is interested in applying the classification model with weight vector w belonging to Bob. Here, the privacy constraint require that Bob should not be able to observe x\u2032 and Carol should not be able to observe w. Similar to the training protocol, Bob generates a public and private key pair for an N -bit Paillier cryptosystem and provides the public key to Carol.\nIn order to label the data instance as y\u2032 = 1, Carol needs to check if P (y\u2032 = 1|x\u2032, w) = 1\n1+e\u2212wT x\u2032 > 1 2 and vice-versa\nfor y\u2032 = \u22121. This is equivalent to checking if wTx\u2032 > 0. We develop the following protocol towards this purpose.\nInput: Bob has w and generates a public-private key pair. Carol has x\u2032 and Bob\u2019s public key. Output: Carol knows if wTx\u2032 > 0.\n1. Bob encrypts w and transfers E[w] to Carol.\n2. Carol homomorphically computes the encrypted inner product.\nd\u220f j=1 E[w]x \u2032 j = E [ d\u2211 j=1 wjx \u2032 j ] = E [ wTx\u2032 ] .\n3. Carol generates a random number r and sends E [ wTx\u2032 ] \u2212\nr to Bob.\n4. Bob decrypts it to obtain his additive share wTx\u2032 \u2212 r. Let us denote it by \u2212s, so that r \u2212 s = wTx\u2032.\n5. Bob and Carol execute a variant of the secure millionaire protocol [17] to with inputs r and s and both learn whether r > s. If r > s, Carol concludes wTx\u2032 > 0 and if r < s, she concludes wTx\u2032 < 0.\nIn this way, Carol and Bob are able to perform the classification operation while maintaining the privacy constraints. If Bob has to repeatedly execute the same protocol, he can pre-compute E[w] to be used in Step 1.\n5. ANALYSIS"}, {"heading": "5.1 Correctness", "text": "The private training protocol does not alter any of the computations of the original training algorithm and therefore results in the same output. The additive randomization ri introduced in Step 3 is removed in Step 6 leaving the results unchanged. Similarly, the multiplicative randomization qi introduced in Step 7 is removed in Step 9.\nAs discussed in Section 3.2, the only source of error is the truncation of less significant digits in the finite precision representation of real numbers. In practice, we observe that the error in computing the weight vector w is negligibly small and does not result in any loss of accuracy."}, {"heading": "5.2 Security", "text": "The principal requirement of a valid secure multiparty computation (SMC) protocol is that any party must not learn anything about the input data provided by the other parties apart from what can be inferred from the result of the computation itself. As we mentioned earlier, we assume that the parties are semi-malicious. From this perspective, it can be seen that the private training protocol (Section 4.2) is demonstrably secure.\nAlice/Carol: In the private training protocol, Alice can only observe encrypted inputs from Bob and hence she does not learn anything about the weight vector used by Bob. In the private classifier evaluation protocol, the party Carol with the test email only receives the final outcome of the classifier in plaintext. Thus, the only additional information available to her is the output of the classifier itself, which being the output is permissible under the privacy criteria of the problem.\nBob: In the training stage, Bob receives unencrypted data from Alice in Steps 3, 8 and 12.\n\u2022 Step 3: Bob receives yiwT(t)xi\u2212 ri. Let us denote this quantity by v and ywTx by z, giving us v = z \u2212 ri. Since ri is drawn from a uniform distribution over the entire finite field Zn, for any v and for every value of z there exists a unique value of ri such that v = z \u2212 ri. Thus, Pz(z|v) \u221d Pz(z)Pr(z \u2212 v) = Pz(z). The conditional entropy H(z|v) = H(z), i.e., Bob receives no information from the operation.\n\u2022 Step 8: A similar argument can be made for this step. Here Bob receives v = qz, where z = 1 + e yiw T (t)xi . It\ncan be shown that for any value v that Bob receives, Pz(z|v) \u221d Pz(z)Pq(v/z)z . Since q is drawn from a power law distribution, i.e. Pq(q) \u221d 1/q, for all v < |D| Pz(z|v) = Pz(z). Once again, the conditional entropy H(z|v) = H(z), i.e., Bob receives no information from the operation.\n\u2022 Step 12: The information Bob receives in this step is the updated weight vector, which is the result of the computation that Bob is permitted to receive by the basic premise of the SMC protocol.\nInformation Revealed by the Output We specifically assume that Alice is agreeable with Bob receiving the updated classifier at the end of the training protocol \u2013 this is the premise behind her participating in the\nprotocol to start with. Nevertheless, this output does reveal significant information about Alice\u2019s data, and we explain below how this can be minimized.\nAt the end of Step 12 in each iteration, Bob receives the update weight vector wt+1 = wt+\u03b7\u2207L(w(t), x, y). As he also has the previous weight vector wt, he effectively observes the\ngradient \u2207L(w(t), x, y) = \u2211 i yix T i ( 1 + e yiw T (t)xi )\u22121 .\nIn the online setting, we normally use one training data instance at a time to update the classifier. If Alice participates in the training protocol using only one document (x1, y1),\nthe gradient observed by Bob will be y1x1 ( 1 + e y1w T (t)xi )\u22121 ,\nwhich is simply a scaling of the data vector y1x1. As Bob knows w(t) he effectively knows y1x1. In particular, if x1 is a vector of non-negative counts as is the case for n-grams, the knowledge of y1x1 is equivalent to knowing x1. Although the protocol itself is secure, the output reveals Alice\u2019s data completely.\nAlice can prevent this by updating the classifier using blocks of K document vectors (x, y) at a time. The protocol ensures that for each block of K vectors Bob only receives the gradient computed over them\n\u2207L(w(t), x, y) = K\u2211 i=1 yix T i ( 1 + e yiw T (t)xi )\u22121 =\nK\u2211 i=1 g(w(t), xi, yi)xi,\nwhere g(w(t), xi, yi) is a scalar function of the data instance such that g(w(t), xi, yi)xi has a one-to-one mapping to xi. Assuming that all data vectors xi are IID, using Jensen\u2019s inequality, we can show that the conditional entropy\nH [ xi|\u2207L(w(t), x, y) ] \u2264 K \u2212 1\nK H[xi] + log(K). (2)\nIn other words, while Bob gains some information about the data belonging to Alice, the amount of this information is inversely proportional to the block size. In the online learning setting, choosing a large block size decreases the accuracy of the classifier. Therefore, the choice of the block size effectively becomes a parameter that Alice can control to trade off giving away some information about her data with the accuracy of the classifier. In Section 6.2, we empirically analyze the performance of the classifier for varying batch sizes. We observe that in practice, the accuracy of the classifier is not reduced even after choosing substantially large batches of 1000 documents, which would hardly cause any loss of information as given by Equation 2."}, {"heading": "5.3 Complexity", "text": "We analyze the encryption/decryption and the data transmission costs for a single execution of the protocol as these consume a vast majority of the time.\nThere are 6 steps of the protocol where encryption or decryption operations are carried out.\n1. In Step 1, Bob encrypts the d-dimensional vector w(t).\n2. In Step 3, Alice encrypts the n random numbers ri.\n3. In Step 4, Bob decrypts the n inner products obtained from Alice.\n4. In Step 5, Bob encrypts the exponentiation of the n inner products.\n5. In Step 8, Bob decrypts, takes a reciprocal, and encrypts the n multiplicatively scaled quantities.\n6. In Step 12, Bob decrypts the d dimensional updated weight vector obtained from Alice.\nTotal: 3n+ 2d encryptions and decryptions.\nSimilarly, there are 6 steps of the protocol where Alice and Bob transfer data to each other.\n1. In Step 1, Bob transfers the d-dimensional vector w(t) to Alice.\n2. In Step 3, Alice transfers n randomized innner products to Bob.\n3. In Step 5, Bob transfers the n encrypted exponentials to Alice.\n4. In Step 7, Alice transfers n scaled quantities to Bob.\n5. In Step 8, Bob transfers the n encrypted reciprocals to Alice.\n6. In Step 11, Alice transfers the d dimensional encrypted updated weight vector to Bob.\nTotal: Transmitting 4n+ 2d elements.\nThe speed of performing the encryption and decryption operations depends directly on the size of the key of the cryptosystem. Similarly, when we are transfering encrypted data, the size of an individual element also depends on the size of the encryption key. As the security of the encryption function is largely determined by the size of the encryption key, this reflects a direct trade-off between security and efficiency."}, {"heading": "6. EXPERIMENTS", "text": "We provide an experimental evaluation of our approach for the task of email spam filtering. The privacy preserving training protocol requires a substantially larger running time as compared to the non-private algorithm. In this section, we analyze the training protocol for running time and accuracy also provide solutions on how it can be used in practice by effectively trading off between accuracy and the computation time.\nAs it is conventional in spam filtering research, we report AUC2 scores. It is considered to be a more appropriate metric for this task as compared to other metrics such as classification accuracy or F-measure because it averages the performance of the classifier in different precision-recall points which correspond to different thresholds on the prediction confidence of the classifier. The AUC score of a random classifier is 0.5 and that for the perfect classifier is 1.\nWe compared AUC performance of the classifier given by the privacy preserving training protocol with the non-private training algorithm and in all cases the numbers were identical up to the five significant digits. Therefore, the error due to the finite precision representation mentioned in Section 5.1 is negligible for all practical purposes.\n2Area under the ROC curve"}, {"heading": "6.1 Email Spam Dataset", "text": "We used the public spam email corpus from the CEAS 2008 spam filtering challenge.3 For generality, we refer to emails as documents. Performance of various algorithms on this dataset is reported in [12]. The dataset consists of 3,067 training and 206,207 testing documents manually labeled as spam or ham (i.e., not spam). To simplify the benchmark calculations, we used the first 3000 documents from each set (Table 1). Accuracy of the baseline majority classifier which labels all documents as spam is 0.79433."}, {"heading": "6.2 Spam Filter Implementation", "text": "Our classification approach is based on online logistic regression [7], as described in Section 3.1. The features are overlapping character four-grams which are extracted from the documents by a sliding window of four characters. The feature are binary indicating the presence or absence of the given four-gram. The documents are in ASCII or UTF-8 encoding which represents each character in 8 bits, therefore the space of possible four-gram features is 232. Following the previous work, we used modulo 106 to reduce the fourgram feature space to one million features and only the first 35 KB of the documents is used to compute the features. For all experiments, we use a step size of \u03b7 = 0.001 and no regularization is used."}, {"heading": "6.3 Protocol Implementation", "text": "We created a prototype implementation of the protocol in C++ and used the variable precision arithmetic libraries provided by OpenSSL [10] to implement the Paillier cryptosystem. We used the GSL libraries [6] for matrix operations. We performed the experiments on a 3.2 GHz Intel\n3The dataset is available at http://plg.uwaterloo.ca/ ~gvcormac/ceascorpus/ The part of the dataset we have used corresponds to pretrain-nofeedback task."}, {"heading": "9, 10 1.81 8.33", "text": "Pentium 4 machine with 2 GB RAM and running 64-bit Ubuntu.\nIn Table 2, we provide a comparison between the time required to train a simple logistic regression classifier and privacy preserving logistic regression classifier. It can be seen that the protocol is slower than non-private version by a factor of 104 mainly due to the encryption in each step of the protocol. To further analyze the behavior of various steps of the protocol, in Table 4 we report the running time of individual steps of the protocol outlined in Section 4.2 on two test datasets of random vectors. It can be observed that encryption is the main bottle neck among the other operations in the protocol.\nFor all experiments, we used the Paillier cryptosystem with 256-bit keys. However, the recommended key size for Paillier encryption is 1024-bits for state of the art security [4]. As shown in Table 3, using 1024-bit encryption keys result in a slowdown by a factor of about 50 as compared to using 256-bit encryption keys. This is a constant factor which can be applied to all our timing results if the stronger level of security provided by 1024-bit keys is desired.\nThe original dataset has 106 features as described in Section 6.2. Similar to the complexity analysis of the training protocol (Section 5.1), we observed that time required for the training protocol is linear in number of documents and number of features. Table 2 compares the running time of two algorithms for one document and the entire training dataset.\nUsing a pre-computed value of the encrypted weight vector E[w], the private evaluation protocol took 210.956 seconds for one document using 106 features and 2.059 seconds for one document using 104 features."}, {"heading": "6.4 Dimensionality Reduction", "text": "Since the time complexity of the privacy preserving protocol is linearly related to the dimensionality of the data, we can improve it by reducing data dimensionality. Data with fewer dimensions will require fewer encryptions and decryptions. On the other hand, reducing the dimensionality of the features, particularly for sparse features such as n-gram counts, can adversely affect the classification performance of the classifier.\nWe experimented with six different dimensionality reduction techniques, and compared the running time and AUC of the classifier learned by the training protocol for features of various reduced dimensionalities.\n1. Principal Component Analysis (PCA): PCA is\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\n0 10 20 30 40 50\nA U\nC\nTime in seconds\nComparison of Dimensionality Reduction Methods\nPCA LSH Hash Space\nSample Multinomial Sample Uniform Document Frequency\nWe ran each of these algorithms on 6000 documents of 106 dimensions. Table 6 summarizes the time and space requirement of each algorithm for reducing dimensions to 104. We trained the logistic regression classifier on 3000 training documents with various reduced dimensions and measured the running time and AUC of the learned classifier on the 3000 test documents. The results are shown in Figure 1.\nClassifier Performance for Varying Batch Size As explained in Section 5.2, another important requirement of our protocol is to train in batches rather than training on one document at a time. We have shown that the extra information gained by Bob about Alice\u2019s data decreases with the increasing batch size. On the other hand, increasing the batch size causes the optimization procedure of the training algorithm to have fewer chances of correcting itself in a single pass over the entire training dataset. In Figure 2, we see that the trade-off in AUC is negligible even with batch sizes of around 1000 documents."}, {"heading": "6.5 Parallel Processing", "text": "Another approach to address the performance issue is parallelization. We experimented with a multi-threaded implementation of the algorithm. On average, we observed 6.3% speed improvement on a single core machine. We expect the improvement to be more significant on a multi-core architecture. A similar scheme can be used to parallelize the protocol across a cluster of machines, such as in a MapReduce framework. In both of these cases, the accuracy of the online algorithms will decrease slightly as the number of threads or machines increase because the gradient \u2207L(w(t), x, y) computed in each of the parallel processes is based on an older value of the weight vector w(t).\nA more promising approach which does not impact the accuracy is encrypting vectors in parallel. In the protocol, we need to encrypt vectors together and the procedure used for the individual elements is identical. We can potentially reduce the encryption time of a feature vector substantially by using a parallel processing infrastructure. We leave the experiments with this type of parallelization to future work."}, {"heading": "7. CONCLUSION", "text": "We presented protocols for training and evaluating a spam filter over private emails based on a logistic regression classifier in both batch and online learning settings. We used character four-grams on textual data as features for training. We presented an information theoretic analysis of the security of the protocol and also found that both the encryption/decryption and data transmission costs of the protocol are linear in the the number of training instances and the dimensionality of the data. We also experimented with a prototype implementation of the protocol on a large scale email dataset and demonstrate that our protocol is able to achieve state of the art performance in a feasible amount of execution time.\nThe future directions of this work includes deploying the privacy preserving spam filtering classifier in a real world email system. We also plan to extend our protocols to make extensive use of parallel architectures to further increase the speed and scalability."}, {"heading": "8. REFERENCES", "text": "[1] A. Z. Broder. Some applications of Rabin\u2019s\nfingerprinting method. Sequences II: Methods in Communications, Security, and Computer Science, pages 143\u2013152, 1993.\n[2] M. Charikar. Similarity estimation techniques from rounding algorithms. In 34th Annual ACM Symposium on Theory of Computing, 2002.\n[3] G. V. Cormack. TREC 2007 spam track overview. In Text REtrieval Conference TREC, 2007.\n[4] Cryptographic key length recommendation. http://keylength.com, 2009.\n[5] M. Freedman, K. Nissim, and B. Pinkas. Efficient private matching and set intersection. In Advances in Cryptology - EUROCRYPT, pages 1\u201319. Springer, 2004.\n[6] M. Galassi, J. Davies, J. Theiler, B. Gough, G. Jungman, P. Alken, M. Booth, and F. Rossi. GNU Scientific Library Reference Manual (v1.12). Network Theory Ltd., third edition, 2009.\n[7] J. Goodman and W. Yih. Online discriminative spam filter training. In Conference on Email and Anti-Spam CEAS, 2006.\n[8] K. Li, Z. Zhong, and L. Ramaswamy. Privacy-aware collaborative spam filtering. IEEE Transactions on Parallel and Distributed Systems, 20(5):725\u2013739, 2009.\n[9] X. Lin, C. Clifton, and M. Y. Zhu. Privacy-preserving clustering with distributed EM mixture modeling. Knowledge and Information Systems, 8(1):68\u201381, 2005.\n[10] http://www.openssl.org/docs/crypto/bn.html.\n[11] P. Paillier. Public-key cryptosystems based on composite degree residuosity classes. In EUROCRYPT, 1999.\n[12] D. Sculley and G. V. Cormack. Going mini: Extreme lightweight spam filters. In Conference on Email and Anti-Spam CEAS, 2008.\n[13] The state of spam & phishing. http://www.symantec. com/business/theme.jsp?themeid=state_of_spam, Jan 2011.\n[14] J. Vaidya, C. Clifton, M. Kantarcioglu, and S. Patterson. Privacy-preserving decision trees over vertically partitioned data. TKDD, 2(3), 2008.\n[15] J. Vaidya, M. Kantarcioglu, and C. Clifton. Privacy-preserving naive Bayes classification. VLDB J, 17(4):879\u2013898, 2008.\n[16] J. Vaidya, H. Yu, and X. Jiang. Privacy-preserving SVM classification. Knowledge and Information Systems, 14(2):161\u2013178, 2008.\n[17] A. Yao. Protocols for secure computations. In IEEE Symposium on Foundations of Computer Science, 1982."}], "references": [{"title": "Some applications of Rabin\u2019s fingerprinting method. Sequences II: Methods in Communications, Security, and Computer Science, pages", "author": ["A.Z. Broder"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M. Charikar"], "venue": "In 34th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "TREC 2007 spam track overview", "author": ["G.V. Cormack"], "venue": "In Text REtrieval Conference TREC,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Efficient private matching and set intersection", "author": ["M. Freedman", "K. Nissim", "B. Pinkas"], "venue": "In Advances in Cryptology - EUROCRYPT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "GNU Scientific Library Reference Manual (v1.12)", "author": ["M. Galassi", "J. Davies", "J. Theiler", "B. Gough", "G. Jungman", "P. Alken", "M. Booth", "F. Rossi"], "venue": "Network Theory Ltd., third edition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Online discriminative spam filter training", "author": ["J. Goodman", "W. Yih"], "venue": "In Conference on Email and Anti-Spam CEAS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Privacy-aware collaborative spam filtering", "author": ["K. Li", "Z. Zhong", "L. Ramaswamy"], "venue": "IEEE Transactions on Parallel and Distributed Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Privacy-preserving clustering with distributed EM mixture modeling", "author": ["X. Lin", "C. Clifton", "M.Y. Zhu"], "venue": "Knowledge and Information Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Public-key cryptosystems based on composite degree residuosity classes", "author": ["P. Paillier"], "venue": "In EUROCRYPT,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Going mini: Extreme lightweight spam filters", "author": ["D. Sculley", "G.V. Cormack"], "venue": "In Conference on Email and Anti-Spam CEAS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Privacy-preserving decision trees over vertically partitioned data", "author": ["J. Vaidya", "C. Clifton", "M. Kantarcioglu", "S. Patterson"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Privacy-preserving naive Bayes classification", "author": ["J. Vaidya", "M. Kantarcioglu", "C. Clifton"], "venue": "VLDB J,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Privacy-preserving SVM classification", "author": ["J. Vaidya", "H. Yu", "X. Jiang"], "venue": "Knowledge and Information Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Protocols for secure computations", "author": ["A. Yao"], "venue": "In IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1982}], "referenceMentions": [{"referenceID": 13, "context": "Our privacy preserving protocol falls into the broad class of secure multiparty computation (SMC) algorithms [17].", "startOffset": 109, "endOffset": 113}, {"referenceID": 8, "context": "We construct our protocol using a cryptosystem satisfying homomorphic encryption [11], in which operations on encrypted data correspond to operations on the original unencrypted data (Section 3.", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "9999 [3].", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "Our implementation is an online logistic regression classifier implementation inspired by [7] which on application to binary character four-gram features was shown to have near state of the art accuracy [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "Our implementation is an online logistic regression classifier implementation inspired by [7] which on application to binary character four-gram features was shown to have near state of the art accuracy [3].", "startOffset": 203, "endOffset": 206}, {"referenceID": 10, "context": "There has also been a much recent work on constructing privacy preserving protocols for other data mining tasks such as decision trees [14], set matching [5], clustering [9], naive Bayes [15], support vector machines [16], etc.", "startOffset": 135, "endOffset": 139}, {"referenceID": 3, "context": "There has also been a much recent work on constructing privacy preserving protocols for other data mining tasks such as decision trees [14], set matching [5], clustering [9], naive Bayes [15], support vector machines [16], etc.", "startOffset": 154, "endOffset": 157}, {"referenceID": 7, "context": "There has also been a much recent work on constructing privacy preserving protocols for other data mining tasks such as decision trees [14], set matching [5], clustering [9], naive Bayes [15], support vector machines [16], etc.", "startOffset": 170, "endOffset": 173}, {"referenceID": 11, "context": "There has also been a much recent work on constructing privacy preserving protocols for other data mining tasks such as decision trees [14], set matching [5], clustering [9], naive Bayes [15], support vector machines [16], etc.", "startOffset": 187, "endOffset": 191}, {"referenceID": 12, "context": "There has also been a much recent work on constructing privacy preserving protocols for other data mining tasks such as decision trees [14], set matching [5], clustering [9], naive Bayes [15], support vector machines [16], etc.", "startOffset": 217, "endOffset": 221}, {"referenceID": 6, "context": "[8] present a distributed framework for privacy aware spam filtering.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Their method is based on applying a one-way fingerprinting transformation [1] to the message text and comparing two emails using a Hamming distance metric and does not involve any form of statistical learning.", "startOffset": 74, "endOffset": 77}, {"referenceID": 8, "context": "In this work, we use the additively homomorphic Paillier cryptosystem [11] which also satisfies semantic security.", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "Alice homomorphically adds E[1] to these quantities", "startOffset": 28, "endOffset": 31}, {"referenceID": 13, "context": "Bob and Carol execute a variant of the secure millionaire protocol [17] to with inputs r and s and both learn whether r > s.", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "Performance of various algorithms on this dataset is reported in [12].", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "Our classification approach is based on online logistic regression [7], as described in Section 3.", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "We used the GSL libraries [6] for matrix operations.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "Locality Sensitive Hashing (LSH): In LSH [2], we choose K random hyper-planes in the original d dimensional space which represent each dimension in the target space.", "startOffset": 41, "endOffset": 44}], "year": 2017, "abstractText": "We present an approach to training a binary logistic regression classifier in the setting where the training data needs to be kept private. We provide a theoretical analysis of the security of this procedure and experimental results for the problem of large scale spam detection. High performance spam filters often use character n-grams as features which result in large sparse vectors to which applying our protocol directly is not feasible. We explore various dimensionality reduction and parallelization approaches and provide a detailed analysis of speed and accuracy trade-off. Our results show that we can achieve the accuracy of state of the art spam filters at comparable training and testing time of nonprivate version of logistic regression.", "creator": "LaTeX with hyperref package"}}}