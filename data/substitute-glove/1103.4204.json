{"id": "1103.4204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2011", "title": "Parallel Online Learning", "abstract": "In this doing need study lithium-6 form online training, a core pattern having device aspects. In from connecting environment come known course for platform subscription learning double to delayed saturday, where to model although listing be out - of - date information. In the worst found, or turn examples like co-existed correlated, possible get have followed hardly severe certain on on skills formula_3. Here, you categorize having those preliminary empirical results on any sets of learning x86 for on a show sharding merely that present categories bewildering present agreeing, learning of parallelism, unitary influence however inferences shown.", "histories": [["v1", "Tue, 22 Mar 2011 04:54:35 GMT  (97kb,D)", "http://arxiv.org/abs/1103.4204v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel hsu", "nikos karampatziakis", "john langford", "alex smola"], "accepted": false, "id": "1103.4204"}, "pdf": {"name": "1103.4204.pdf", "metadata": {"source": "CRF", "title": "Parallel Online Learning", "authors": ["Daniel Hsu", "Nikos Karampatziakis", "John Langford", "Alex J. Smola"], "emails": [], "sections": [{"heading": "Parallel Online Learning", "text": "Daniel Hsu, Nikos Karampatziakis, John Langford and Alex J. Smola\nar X\niv :1\n10 3.\n42 04\nv1 [\ncs .L\nG ]\n2 2\nM ar\n2 01\n1\n0.1 Online Learning 3\n0.1 Online Learning\nOne well-known general approach to machine learning is to repeatedly greedily update a partially learned system using a single labeled data instance. A canonical example of this is provided by the perceptron algorithm (Rosenblatt, 1958) which modifies a weight vector by adding or subtracting the features of a misclassified instance. More generally, typical methods compute the gradient of the prediction\u2019s loss with respect to the weight vector\u2019s parameters, and then update the system according to the negative gradient. This basic approach has many variations and extensions, as well as at least two names. In the neural network literature, this approach is often called \u201cstochastic gradient descent\u201d , while in the learning theory literature it is typically called \u201conline gradient descent\u201d. For the training of complex nonlinear prediction systems, the stochastic gradient descent approach was described long ago and has been standard practice for at least two decades (Bryson and Ho, 1969; Rumelhart et al., 1986; Amari, 1967).\nAlgorithm 1 describes the basic gradient descent algorithm we consider here. The core algorithm uses a differentiable loss function ` = `(\u00b7, y) to measure the quality of a prediction y\u0302 with respect to a correct prediction y, and a sequence of learning rates (\u03b7t). Qualitatively, a \u201clearning rate\u201d is the degree to which the weight parameters are adjusted to predict in accordance with a data instance. For example, a common choice is squared loss where `(y\u0302, y) = (y \u2212 y\u0302)2 and a common learning rate sequence is \u03b7t = 1/ \u221a t.\nAlgorithm 1 Gradient Descent\ninput loss function l, learning rate schedule \u03b7t initialize for all i \u2208 {1, . . . , n}, weight wi := 0 for t = 1 to T do\nGet next feature vector x \u2208 Rn Compute prediction y\u0302 := \u3008w, x\u3009 Get corresponding label y\nFor i \u2208 {1, . . . , n} compute gradient gi := \u2202`(y\u0302,y)\u2202wi ( = \u2202`(\u3008w,x\u3009,y)\u2202wi ) For i \u2208 {1, . . . , n} update wi := wi \u2212 \u03b7tgi\nend for\nThere are several basic observations regarding efficiency of online learning\napproaches.\n\u2022 At a high level, many learning system make a sequence of greedy improvements. For such systems, it is difficult to reduce these improvements to one\nor just a few steps of greedy improvement, simply because the gradient\n4 provides local information relevant only to closely similar parameteriza-\ntions, while successful prediction is a global property. This observation applies to higher order gradient information such as second derivatives as well. An implication of this observation is that multiple steps must be taken, and the most efficient way to make multiple steps is to take a step after each instance is observed. \u2022 If the same instance occurs twice in the data, it\u2019s useful to take advantage of data as it arrives. Take the extreme case where every instance is\nreplicated n times. Here an optimization algorithm using fractions of 1/n of the data at a time would enjoy an n-fold speedup relative to an algorithm using full views of the data for optimization. While in practice it is difficult to ascertain these properties beforehand, it is highly desirable to have algorithms which can take advantage of redundancy and similarity as data arrives. \u2022 The process of taking a gradient step is generally amortized by prediction itself. For instance, with the square loss `(y\u0302, y) = 12(y\u0302\u2212y) 2, the gradient is\ngiven by (y\u0302\u2212y)xi for i \u2208 {1, . . . , n}, so the additional cost of the gradient step over the prediction is roughly just a single multiply and store per feature. Similar amortization can also be achieved with complex nonlinear circuit-based functions, for instance, when they are compositions of linear predictors. \u2022 The process of prediction can often be represented in vectorial form such that highly optimized linear algebra routines can be applied to yield an\nadditional performance improvement.\nBoth the practice of machine learning and the basic observations above suggest that gradient descent learning techniques are well suited to address large scale machine learning problems. Indeed, the techniques are so effective, and modern computers are so fast, that we might imagine no challenge remains. After all, a modern computer might have 8 cores operating at 3GHz, each core capable of 4 floating point operations per clock cycle, providing a peak performance of 96GFlops. A large dataset by today\u2019s standards is about webscale, perhaps 1010 instances, each 104 features in size. Taking the ratio, this suggests that a well-implemented learning algorithm might be able to process such a dataset in under 20 minutes. Taking into account that GPUs are capable of delivering at least one order of magnitude more computation and that FPGAs might provide another order of magnitude, this suggests no serious effort should be required to scale up learning algorithms, at least for simple linear predictors.\nHowever, considering only floating point performance is insufficient to\n0.2 Limits due to Bandwidth and Latency 5\ncapture the constraints imposed by real systems: the limiting factor is not computation, but rather network limits on bandwidth and latency. This chapter is about dealing with these limits in the context of gradient descent learning algorithms. We take as our baseline gradient descent learning algorithm a simple linear predictor, which we typically train to minimize squared loss. Nevertheless, we believe our findings with respect to these limitations qualitatively apply to many other learning algorithms operating according to gradient descent on large datasets.\nAnother substantial limit is imposed by label information\u2014it\u2019s difficult in general to cover the cost of labeling 109 instances. For large datasets relevant to this work, it\u2019s typically the case that label information is derived in some automated fashion\u2014for example a canonical case is web advertisement where we might have 1010 advertisements displayed per day, of which some are clicked on and some are not.\n0.2 Limits due to Bandwidth and Latency\nThe bandwidth limit is well-illustrated by the Stochastic Gradient Descent (SGD) implementation (Bottou, 2008). Leon Bottou released it as a reference implementation along with a classification problem with 781K instances and 60M total (non-unique) features derived from RCV1 (Lewis et al., 2004). On this dataset, the SGD implementation might take 20 seconds to load the dataset into memory and then learn a strong predictor in 0.4 seconds. This illustrates that the process of loading the data from disk at 15MB/s is clearly the core bottleneck.\nBut even if that bottleneck were removed we would still be far from peak performance: 0.4 seconds is about 100 times longer than expected given the peak computational limits of a modern CPU. A substantial part of this slowdown is due to the nature of the data, which is sparse. With sparse features, each feature might incur the latency to access either cache or RAM (typically a 10x penalty), imposing many-cycle slowdowns on the computation. Thus, performance is sharply limited by bandwidth and latency constraints which in combination slow down learning substantially.\nLuckily, gradient descent style algorithms do not require loading all data into memory. Instead one data instance can be loaded, a model updated, and then the instance discarded. A basic question is: Can this be done rapidly enough to be an effective strategy? For example, a very reasonable fear is that the process of loading and processing instances one at a time induces too much latency, slowing the overall approach unacceptably.\nThe Vowpal Wabbit (VW) software (Langford et al., 2007) provides an\n6 existence proof that it is possible to have a fast fully online implementation which loads data as it learns. On the dataset above, VW can load and learn on the data simultaneously in about 3 seconds, an order of magnitude faster than SGD. A number of tricks are required to achieve this, including a good choice of cache format, asynchronous parsing, and pipelining of the computation. A very substantial side benefit of this style of learning is that we are no longer limited to datasets which fit into memory. A dataset can be either streamed from disk or over the network, implying that the primary bottleneck is bandwidth, and the learning algorithm can handle datasets with perhaps 1012 non-unique features in a few hours.\nThe large discrepancy between bandwidth and available computation suggests that it should be possible to go beyond simple linear models without a significant computational penalty: we can compute nonlinear features of the data and build an extended linear model based on those features. For instance, we may use the random kitchen sink features (Rahimi and Recht, 2008) to obtain prediction performance comparable with Gaussian RBF kernel classes. Furthermore, while general polynomial features are computationally infeasible it is possible to obtain features based on the outer product of two sets of features efficiently by explicitly expanding such features on the fly. These outer product features can model interaction between two sources of information, for example the interaction of (query,result) feature pairs is often relevant in internet search settings.\nVW allows the implicit specification of these outer product features via specification of the elements of the pairs. The outer product features thus need not be read from disk, implying that the disk bandwidth limit is not imposed. Instead, a new limit arises based on random memory access latency and to a lesser extent on bandwidth constraints. This allows us to perform computation in a space of up to 1013 features with a throughput in the order of 108 features/second. Note that VW can additionally reduce the dimensionality of each instance using feature hashing (Shi et al., 2009; Weinberger et al., 2009) , which is essential when the (expanded) feature space is large, perhaps even exceeding memory size. The core idea here is to use a hash function which sometimes collides features. The learning algorithm learns to deal with these collisions, and the overall learning and evaluation process happens much more efficiently due to substantial space savings.\nThis quantity remains up to two orders of magnitude below the processing limit imposed by a modern CPU (we have up to 100 Flops available per random memory access). This means that there is plenty of room to use more sophisticated learning algorithms without substantially slowing the learning process. Nevertheless, it also remains well below the size of the\n0.3 Parallelization Strategies 7\nlargest datasets, implying that our pursuit of a very fast efficient algorithm is not yet complete.\nTo make matters more concrete assume we have datasets of 10TB size (which is not uncommon for web applications). If we were to stream this data from disk we cannot expect a data stream of more than 100MB/s per disk (high performance arrays might achieve up to 5x this throughput, albeit often at a significant CPU utilization). This implies that we need to wait at least 105 seconds, i.e. 30 hours to process this data on a single computer. This is assuming an optimal learning algorithm which needs to see each instance only once and a storage subsystem which is capable of delivering sustained peak performance for over a day. Even with these unrealistic assumptions this is often too slow.\n0.3 Parallelization Strategies\nCreating an online algorithm to process large amounts of data directly limits the designs possible. In particular, it suggests decomposition of the data either in terms of instances or in terms of features as depicted in Figure 0.1. Decomposition in terms of instances automatically reduces the load per computer since we only need to process and store a fraction of the data on each computer. We refer to this partitioning as \u201cinstance sharding\u201d1.\nAn alternative is to decompose data in terms of its features. While it does not reduce the number of instances per computer, it reduces the data per computer by reducing the number of features associated with an instance for each computer, thus increasing the potential throughput per computer.\nA typical instance shard scheme runs the learning algorithm on each shard, combines the results in some manner, and then runs a learning algorithm again (perhaps with a different initialization) on each piece of the data. An extreme case of the instance shard approach is given by parallelizing statistical query algorithms (Chu et al., 2007), which compute statistics for various queries over the entire dataset, and then update the learned model based on these queries, but there are many other variants as well (Mann et al., 2009; McDonald et al., 2010). The instance shard approach has a great virtue\u2014it\u2019s straightforward and easy to program.\nA basic limitation of the instance shard approach is the \u201ccombination\u201d operation which does not scale well with model complexity. When a predictor is iteratively built based on statistics, it is easy enough to derive an aggregate statistic. When we use an online linear predictor for each instance\n1 In the context of data, \u201cshard\u201d is typically used to define a partition without any particular structure other than size.\n8 Data\nData\nData\nData\nC\nC\nC\nC\nCCCC\nInstance based sharding feature based sharding\nFigure 0.1 Two approaches to data splitting. Left: instance shards, Right: feature shards.\nshard, some averaging or weighted averaging style of operation is provably sensible. However, when a nonlinear predictor is learned, it is unclear how to combine the results. Indeed, when a nonlinear predictor has symmetries, and the symmetries are broken differently on different instance shards, a simple averaging approach might cancel the learning away. An example of a symmetry is provided by a two-layer neural network with 2 hidden nodes. By swapping the weights in the first hidden node with the weights of the second hidden node, and similarly swapping the weights in the output node we can build a representationally different predictor with identical predictions. If these two neural networks then have their weights averaged, the resulting predictor can perform very poorly.\nWe have found a feature shard approach more effective after the (admittedly substantial) complexity of programming has been addressed. The essential idea in a feature shard approach is that a learning algorithm runs on a subset of the features of each instance, then the predictions on each shard are combined to make an overall prediction for each instance. In effect, the parameters of the global model are partitioned over different machines. One simple reason why the feature shard approach works well is due to caching effects\u2014any learned model is distributed across multiple nodes and hence better fits into the cache of each node. This combination process can be a simple addition, or the predictions from each shard can be used as features for a final prediction process, or the combination could even be carried out in a hierarchical fashion. After a prediction is made, a gradient based update can be made to weights at each node in the process. Since we are concerned with datasets less than 1012 in size, the bandwidth required to pass a few bytes per instance around is not prohibitive.\n0.4 Delayed Update Analysis 9\nOne inevitable side effect of either the instance shard or the feature shard approach is a delayed update, as explained below. Let m be the number of instances and n be the number of computation nodes. In the instance shard approach the delay factor is equal to m/n, because m/n updates can occur before information from a previously seen instance is incorporated into the model. With the feature shard approach, the latency is generally smaller, but more dependent on the network architecture. In the asymptotic limit when keeping the bandwidth requirements of all nodes constant, the latency grows as O(log(n)) when the nodes are arranged in a binary tree hierarchy; in this case, the prediction and gradient computations are distributed in a divide-and-conquer fashion and is completed in time proportional to the depth of the recursion, which is O(log(n)). In the current implementation of VW, a maximum latency of 2048 instances is allowed. It turns out that any delay can degrade performance substantially, at least when instances arrive adversarially, as we outline next.\n0.4 Delayed Update Analysis\nWe have argued that both instance sharding and feature sharding approaches require delayed updates in a parallel environment. Here we state some analysis of the impact of delay, as given by the delayed gradient descent algorithm in Algorithm 2. We assume that at time t we observe some instance x with associated label y. Given the instance x we generate some prediction \u3008w, x\u3009. Based on this we incur a loss `(\u3008w, x\u3009 , y) such as 12(y \u2212 \u3008w, x\u3009) 2.\nGiven this unified representation we consider the following optimization algorithm template. It differs from Algorithm 1 because the update is delayed by \u03c4 rounds. This aspect models the delay due to the parallelization strategy for implementing the gradient descent computation.\n0.4.1 Guarantees\nWe focus on the impact of delay on the convergence rate of the weight vector learned by the algorithm. Convergence rate is a natural performance criterion for online learning algorithms, as it characterizes the trade-off between running time and learning accuracy (measured specifically in number of instances versus error rate).\nIntroducing delay between data presentation and updates can lead to a substantial increase in error rate. Consider the case where we have a delay of \u03c4 between the time we see an instance and when we are able to update w based on the instance. If we are shown \u03c4 duplicates of the same data,\n10\nAlgorithm 2 Delayed Gradient Descent\nInput: loss function l, learning rate \u03b7t and delay \u03c4 \u2208 N initialize for all i \u2208 {1, . . . , n}, weight wi := 0 Set x1 . . . , x\u03c4 := 0 and compute corresponding gt for `(0, 0). for t = \u03c4 + 1 to T + \u03c4 do\nGet next feature vector x \u2208 Rn Compute prediction y\u0302 := \u3008w, x\u3009 Get corresponding label y For i \u2208 {1, . . . , n} compute gradient gt,i := \u2202`(y\u0302,y)\u2202wi For i \u2208 {1, . . . , n} update wi := wi \u2212 \u03b7tgt\u2212\u03c4,i\nend for\ni.e. xt, . . . , xt+\u03c4\u22121 = x\u0304 in sequence we have no chance of responding to x\u0304 in time and the algorithm cannot converge to the best weight vector any faster than 1\u03c4 times the rate of an algorithm which is able to respond instantly. Note that this holds even if we are told beforehand that we will see the same instance \u03c4 times.\nThis simple reasoning shows that for an adversarially chosen sequence of instances the regret (defined below) induced by a delay of \u03c4 can never be better than that of the equivalent no-delay algorithm whose convergence speed is reduced by a factor of 1\u03c4 . It turns out that these are the very rates we are able to obtain in the adversarial setting. On the other hand, in the non-adversarial setting, we are able to obtain rates which match those of no-delay algorithms, albeit with a sizable additive constant which depends on the delay.\nThe guarantees we provide are formulated in terms of a regret , i.e. as a discrepancy relative to the best possible solution w\u2217 defined with knowledge of all events. Formally, we measure the performance of the algorithm in terms of\nReg[w1, . . . , wT\ufe38 \ufe37\ufe37 \ufe38 =:W ] := T\u2211 t=1 [`(y\u0302t, yt)\u2212 `(y\u0302\u2217t , yt)] (0.1)\nwhere y\u2217t = \u2329 xt, argmin\nw\nT\u2211 t\u2032=1 `(y\u0302, yt\u2032) \u232a Theorem 1 (Worst case guarantees for delayed updates; Langford et al., 2009) If \u2016w\u2217\u2016 \u2264 R2 and the norms of the gradients \u2207w`(\u3008w, x\u3009 , y) are bounded by L, then\nReg[W ] \u2264 4RL \u221a \u03c4T (0.2)\n0.4 Delayed Update Analysis 11\nwhen we choose the learning rate \u03b7t = R L \u221a 2\u03c4t . If, in addition, `(\u3008w, x\u3009 , y) is strongly convex with modulus of convexity c we obtain the guarantee\nReg[W ] \u2264 L 2\nc [\u03c4 + 0.5] log T + C(\u03c4, L, c)\nwith learning rate \u03b7t = 1\nc(t\u2212\u03c4) , where C is a function independent of T .\nIn other words the average error of the algorithm (as normalized by the number of seen instances) converges at rate O( \u221a \u03c4/T ) whenever the loss gradients are bounded and at rate O(\u03c4 log T/T ) whenever the loss function is strongly convex. This is exactly what we would expect in the worst case: an adversary may reorder instances so as to maximally slow down progress. In this case a parallel algorithm is no faster than a sequential code. While such extreme cases hardly occur in practice, we have observed experimentally that for sequentially correlated instances, delays can rapidly degrade learning.\nIf subsequent instances are only weakly correlated or IID, it is possible to prove tighter bounds where the delay does not directly harm the update (Langford et al., 2009). The basic structure of these bounds is that they have a large delay-dependent initial regret after which the optimization essentially degenerates into an averaging process for which delay is immaterial. These bounds have many details, but a very crude alternate form of analysis can be done using sample complexity bounds. In particular, if we have a set H of predictors and at each timestep t choose the best predictor on the first t\u2212 \u03c4 timesteps, we can bound the regret to the best predictor h according to the following:\nTheorem 2 (IID Case for delayed updates) If all losses are in {0, 1}, for all IID data distributions D over features and labels, for any \u03b4 in (0, 1), with probability 1\u2212 \u03b4\nmin h\u2208H T\u2211 t=1 [`(h(xt), yt)\u2212 `(ht(xt), yt)] \u2264 \u03c4 + \u221a T ln 3|H|T \u03b4 + \u221a T ln 3\u03b4 2 (0.3)\nProof The proof is a straightforward application of the Hoeffding bound. At every timestep t, we have t \u2212 \u03c4 labeled data instances. Applying the Hoeffding bound for every hypothesis h, we have that with probability\n2\u03b4/3|H|T , \u2223\u2223\u2223 1t\u2212\u03c4 \u2211t\u2212\u03c4i=1 `(h(xt), yt)\u2212 E(x,y)\u223cD`(h(x), y)\u2223\u2223\u2223 \u2264 \u221a ln 3|H|T/\u03b42(t\u2212\u03c4) . Applying a union bound over all hypotheses and timesteps implies the same holds with probability at least 2\u03b4/3. The algorithm which chooses the best predictor in hindsight therefore chooses a predictor with expected loss at\nmost \u221a\n2 ln 3|H|T/\u03b4 t\u2212\u03c4 worse than the best. Summing over T timesteps, we get:\n12 \u03c4 + \u221a 2 ln 3|H|T/\u03b4 \u2211T\u2212\u03c4\nt=1 1\u221a t \u2264 \u03c4 +\n\u221a 2T ln 3|H|T/\u03b4. This is a bound on an\nexpected regret. To get a bound on the actual regret, we can simply apply a Hoeffding bound again yielding the theorem result.\n0.5 Parallel Learning Algorithms\nWe have argued that delay is generally bad when doing online learning (at least in an adversarial setting), and that it is also unavoidable when parallelizing. This places us in a bind: How can we create an effective parallel online learning algorithm? We\u2019ll discuss two approaches based on multicore and multinode parallelism.\n0.5.1 Multicore Feature Sharding\nA multicore processor consists of multiple CPUs which operate asynchronously in a shared memory space. It should be understood that because multicore parallelization does not address the primary bandwidth bottleneck, its usefulness is effectively limited to those datasets and learning algorithms which require substantial computation per raw instance used. In the current implementation, this implies the use of feature pairing, but there are many learning algorithms more complex than linear prediction where this trait may also hold.\nThe first version of Vowpal Wabbit used an instance sharding approach for multicore learning, where the set of weights and the instance source was shared between multiple identical threads which each parsed the instance, made a prediction, and then did an update to the weights. This approach was effective for two threads yielding a near factor-of-2 speedup since parsing of instances required substantial work. However, experiments with more threads on more cores yielded no further speedups due to lock contention. Before moving on to a feature sharding approach, we also experimented with a dangerous parallel programming technique: running with multiple threads that do not lock the weight vector. This did yield improved speed, but at a cost in reduced learning rate and nondeterminism which was unacceptable.\nThe current implementation of Vowpal Wabbit uses an asynchronous parsing thread which prepares instances into just the right format for learning threads, each of which computes a sparse-dense vector product on a disjoint subset of the features. The last thread completing this sparse-dense vector product adds together the results and computes an update which is then sent to all learning threads to update their weights, and then the process repeats. Aside from index definition related to the core hashing representation (Shi\n0.5 Parallel Learning Algorithms 13\nxF1 xF2 xF3 . . . xFn\n. . .\ny\u03021 y\u03022 y\u03023 y\u0302n\ny\u0302\nFigure 0.2 Architecture for no-delay multinode feature sharding.\net al., 2009; Weinberger et al., 2009) Vowpal Wabbit employs, the resulting algorithm is identical to the single thread implementation. It should be noted that although processing of instances is fully synchronous there is a small amount of nondeterminism between runs due to order-of-addition ambiguities between threads. In all our tests, this method of multicore parallelization yielded virtually identical prediction performance with negligible overhead compared to non-threaded code and sometimes substantial speedups. For example, with 4 learning threads, about a factor of 3 speedup is observed.\nWe anticipate that this approach to multicore parallelization will not scale to large numbers of cores, because the very tight coupling of parallelization requires low latency between the different cores. Instead, we believe that multinode parallelization techniques will ultimately need to be used for multicore parallelization, motivating the next section.\n0.5.2 Multinode Feature Sharding\nThe primary distinction between multicore and multinode parallelization is latency, with the latency between nodes many orders of magnitude larger than for cores. In particular, the latency between nodes is commonly much larger than the time to process an individual instance, implying that any per-instance blocking operation, as was used for multicore parallelization, is unacceptable.\nThis latency also implies a many-instance delayed update which, as we have argued, incurs a risk of substantially degrading performance. In an experiment to avoid this risk, we investigated the use of updates based on information available to only one node in the computation, where there is no delay. Somewhat surprisingly, this worked better than our original predictor.\n14"}, {"heading": "Tree Architectures", "text": "Our strategy is to employ feature sharding across several nodes, each of which updates its parameters online as a single-node learning algorithm would. So, ignoring the overhead due to creating and distributing the feature shards (which can be minimized by reorganizing the dataset), we have so far fully decoupled the computation. The issue now is that we have n independent predictors each using just a subset of the features (where n is the number of feature shards), rather than a single predictor utilizing all of the features. We reconcile this in the following manner: (i) we require that each of these nodes compute and transmit a prediction to a master node after receiving each new instance (but before updating its parameters); and (ii) we use the master node to treat these n predictions as features, from which the master node learns to predict the label in an otherwise symmetric manner. Note that the master node must also receive the label corresponding to each instance, but this can be handled in various ways with minimal overhead (e.g., it can be piggybacked with one of the subordinate node\u2019s predictions). The end result, illustrated in Figure 0.2, is a two-layer architecture for online learning with reduced latency at each node and no delay in parameter updates.\nNaturally, the strategy described above can be iterated to create multilayered architectures that further reduce the latency at each node. At the extreme, the architecture becomes a (full) binary tree: each leaf node (at the bottom layer) predicts using just a single feature, and each internal node predicts using the predictions of two subordinate nodes in the next lower layer as features (see Figure 0.3). Note that each internal node may incur delay proportional to its fan-in (in-degree), so reducing fan-in is desirable; however, this comes at the cost of increased depth and thus prediction latency. Therefore, in practice the actual architecture that is deployed may be somewhere in between the binary tree and the two-layer scheme. Nevertheless, we will study the binary tree structure further because it illustrates the distinctions relative to a simple linear prediction architecture."}, {"heading": "Convergence Time vs Representation Power", "text": "The price of the speed-up that comes with the no-delay approach (even with the two-layer architecture) is paid in representation power. That is, the no-delay approach learns restricted forms of linear predictors relative to what can be learned by ordinary (delayed) gradient descent. To illustrate this, we compare the resulting predictors from the no-delay approach with the binary tree architecture and the single-node linear architecture. Let x =\n0.5 Parallel Learning Algorithms 15\nx1 x2 x3 x4\nw (0) 1 w (0) 2 w (0) 3 w (0) 4\nw (1) 1 w (1) 2 w (1) 3 w (1) 4\nw (2) 12 w (2) 34\nFigure 0.3 Hierarchical architecture for no-delay multinode feature sharding. Each edge is associated with a weight learned by the node at the arrow head.\n(x1, . . . , xn) \u2208 Rn be a random vector (note that the subscripts now index the features) and y \u2208 R be a random variable. Gradient descent using a linear architecture converges toward the least-squares linear predictor of y from x, i.e.,\nw\u2217 := arg min w\u2208Rn\nE [ 1\n2 (\u3008x,w\u3009 \u2212 y)2\n] = \u03a3\u22121b \u2208 Rn\nwhere\n\u03a3 := E[xx>] \u2208 Rn\u00d7n and b := E[xy] \u2208 Rn,\nin time roughly linear in the number of features n (Kivinen and Warmuth, 1995).\nThe gradient descent strategy with the binary tree architecture, on the other hand, learns weights locally at each node; the weights at each node therefore converge to weights that are locally optimal for the input features supplied to the node. The final predictor is linear in the input features but can differ from the least-squares solution. To see this, note first that the leaf nodes learn weights w (0) 1 , . . . , w (0) n , where\nw (0) i := bi \u03a3i,i \u2208 R.\nThen, the (k+ 1)th layer of nodes learns weights from the predictions of the kth layer; recursively, a node whose input features are the predictions of the ith and jth nodes from layer k learns the weights (w (k+1) i , w (k+1) j ) \u2208 R2. By induction, the prediction of the ith node in layer k is linear in the subset\n16\nSi of variables that are descendants of this node in the binary tree. Let w (k) Si \u2208 R|Si| denote these weights and xSi \u2208 R|Si| denote the corresponding feature vector. Then (w (k+1) i , w\n(k+1) j ) \u2208 R2 can be expressed as[\nw (k+1) i w (k+1) j\n] = \u2329w(k)Si , \u03a3Si,Siw(k)Si \u232a \u2329w(k)Si , \u03a3Si,Sjw(k)Sj \u232a\u2329 w\n(k) Sj , \u03a3Sj ,Siw (k) Si\n\u232a \u2329 w\n(k) Sj , \u03a3Sj ,Sjw (k) Sj\n\u232a\u22121 \u2329w(k)Si , bSi\u232a\u2329 w\n(k) Sj , bSj \u232a where \u03a3Si,Sj = E[xSix>Sj ] and bSi = E[xSiy]. Then, the prediction at this particular node in layer k + 1 is\nw (k+1) i\n\u2329 w\n(k) Si , xSi\n\u232a + w\n(k+1) j\n\u2329 w\n(k) Sj , xSj\n\u232a ,\nwhich is linear in (xSi , xSj ). Therefore, the overall prediction is linear in x, with the weight attached to xi being a product of weights at the different levels. However, these weights can differ significantly from w\u2217 when the features are highly correlated, as the tree architecture only ever considers correlations between (say) xSi and xSj through the scalar summary\u2329 w\n(k) Si , \u03a3Si,Sjw (k) Sj\n\u232a . Thus, the representational expressiveness of the binary\ntree architecture is constrained by the local training strategy.\nThe tree predictor can represent solutions with complexities between Na\u0308\u0131ve Bayes and a linear predictor. Na\u0308\u0131ve Bayes learns weights identical to the bottom layer of the binary tree, but stops there and combines the n individual predictions with a trivial sum: w (0) 1 x1 + . . .+w (0) n xn. The advantage of Na\u0308\u0131ve Bayes is its convergence time: because the weights are learned independently, a union bound argument implies convergence in just O(log n) time, which is exponentially faster than the O(n) convergence time using the linear architecture!\nThe convergence time of gradient descent with the binary tree architecture is roughly O(log2 n). To see this, note that the kth layer converges in roughly O(log(n/2k)) time since there are n/2k parameters that need to converge, plus the time for the (k\u2212 1)th layer to converge. Inductively, this is O(log n+log(n/2)+ . . .+log(n/2k)) = O(k log n). Thus, all of the weights have converged by the time the final layer (k = log2 n) converges; this gives an overall convergence time of O(log2 n). This is slightly slower than Na\u0308\u0131ve Bayes, but still significantly faster than the single-node linear architecture.\nThe advantage of the binary tree architecture over Na\u0308\u0131ve Bayes is that it can account for variability in the prediction power of various feature shards, as the following result demonstrates.\nProposition 3 There exists a data distribution for which the binary tree\n0.5 Parallel Learning Algorithms 17\narchitecture can represent the least-squares linear predictor but Na\u0308\u0131ve Bayes cannot.\nProof Suppose the data comes from a uniform distribution over the following four points:\nx1 x2 x3 y\nPoint 1 +1 +1 \u22121/2 +1 Point 2 +1 \u22121 \u22121 \u22121 Point 3 \u22121 \u22121 \u22121/2 +1 Point 4 \u22121 +1 +1 +1\nNa\u0308\u0131ve Bayes yields the weights w = (\u22121/2, 1/2, 2/5), which incurs mean squared-error 0.8. On the other hand, gradient descent with the binary tree architecture learns additional weights:\nx1 x2 x3\n\u221212 1 2\n2 5\n1 1 1\n3 \u22125\nwhich ultimately yields an overall weight vector of\n(\u22121/2 \u00b7 1 \u00b7 3, 1/2 \u00b7 1 \u00b7 3, 2/5 \u00b7 1 \u00b7 \u22125) = (\u22123/2, 3/2, \u22122)\nwhich has zero mean squared-error.\nIn the proof example, the features are, individually, equally correlated with the label y. However, the feature x3 is correlated with the two individually uncorrelated features x1 and x2, but Na\u0308\u0131ve Bayes is unable to discover this whereas the binary tree architecture can compensate for it.\nOf course, as mentioned before, the binary tree architecture (and Na\u0308\u0131ve Bayes) is weaker than the single-node linear architecture in expressive power due to its limited accounting of feature correlation.\nProposition 4 There exists a data distribution for which neither the bi-\n18\nnary tree architecture nor Na\u0308\u0131ve Bayes can represent the least-squares linear predictor.\nProof Suppose the data comes from a uniform distribution over following four points:\nx1 x2 x3 y\nPoint 1 +1 \u22121 \u22121 \u22121 Point 2 \u22121 +1 \u22121 \u22121 Point 3 +1 +1 \u22121 +1 Point 4 +1 +1 \u22121 +1\nThe optimal least-squares linear predictor is the all-ones vector w\u2217 = (1, 1, 1) and incurs zero squared-error (since 1 \u00b7x1 +1 \u00b7x2 +1 \u00b7x3 = y for every point). However, both Na\u0308\u0131ve Bayes and the binary tree architecture yield weight vectors in which zero weight is assigned to x3, since x3 is uncorrelated with y; any linear predictor that assigns zero weight to x3 has expected squarederror at least 1/2.\n0.5.3 Experiments\nHere, we detail experimental results conducted on a medium size proprietary ad display dataset. The task associated with the dataset is to derive a good policy for choosing an ad given user, ad, and page display features. This is accomplished via pairwise training concerning which of two ads was clicked on and element-wise evaluation with an offline policy evaluator (Langford et al., 2008). There are several ways to measure the size of this dataset\u2014it is about 100Gbytes when gzip compressed, has around 10M instances, and about 125G non-unique nonzero features. In the experiments, VW was run with 224 ' 16M weights, which is substantially smaller than the number of unique features. This discrepancy is accounted for by the use of a hashing function, with 224 being chosen because it is large enough such that a larger numbers of weights do not substantially improve results.\nIn the experimental results, we report the ratio of progressive validation squared losses (Blum et al., 1999) and wall clock times to a multicore parallelized version of Vowpal Wabbit running on the same data and the same machines. Here, the progressive validation squared loss is the average over t of (yt \u2212 y\u0302t)2 where critically, y\u0302t is the prediction just prior to an update. When data is independent, this metric has deviations similar to the average loss computed on held-out evaluation data.\nEvery node has 8 CPU cores and is connected via gigabit Ethernet. All\n0.5 Parallel Learning Algorithms 19\na) FeaturesLabel b) FeaturesLabel\nFeature Shard\nc)\nPredict & Learn\nPredictions\nFeaturesLabel\nFeature Shard\nd)\nPredict & Learn\nPredictions\nPredict & Learn\nFeaturesLabel\nFeature Shard\nFigure 0.4 Diagram of the parallel algorithm used in the experiments. Step (a) starts with a full data instance. Step (b) splits the instance\u2019s features across each shard while replicating the label to each shard. In our experiments, the number of feature shards varies between 1 and 8. Step (c) does prediction and learning at each feature shard using only local information. Step (d) combines these local predictions treating them as features for a final output prediction.\nlearning results are obtained with single pass learning on the dataset using learning parameters optimized to control progressive validation loss. The precise variant of the multinode architecture we experimented with is detailed in Figure 0.4. In particular, note that we worked with a flat hierarchy using 1-8 feature shards (internal nodes). All code is available in the current Vowpal Wabbit open source code release.\nResults are reported in Figure 0.5. The first thing to note in Figure 0.5(a) is that there is essentially no loss in time and precisely no loss in solution quality for using two machines (shard count = 1): one for a no-op shard (used just for sending data to the other nodes) and the other for learning. We also note that the running time does not decrease linearly in the number of shards, which is easily explained by saturation of the network by the no-op sharding node. Luckily, this is not a real bottleneck, because the process of sharding instances is stateless and (hence) completely parallelizable.\n20\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1 2 4 8\nre la\ntiv e\nsq ua\nre d\nlo ss\no r\ntim e\nshard count\nSharding & Training\nr. squared loss r. time\n0\n0.2\n0.4\n0.6\n0.8\n1\n1 2 4 8\nre la\ntiv e\nsq ua\nre d\nlo ss\no r\ntim e\nshard count\nTraining & Combining\nr. squared loss r. time\n(a) (b)\nFigure 0.5 Plots of running time and loss versus shard count. (a) Ratio of time and progressive squared loss of the shard and local train steps to a multicore single-machine instance of VW. Here the squared loss is reported is the average of the squared losses at each feature shard, without any aggregation at the final output node. (b) Ratio of time and squared loss for learning at the local nodes and passing information to the final output node where a final prediction is done.\nAs expected, the average solution quality across feature shards also clearly degrades with the shard count. This is because the increase in shard count implies a decrease in the number of features per nodes, which means each node is able to use less information on which to base its predictions.\nUpon examination of Figure 0.5(b), we encounter a major surprise\u2014the quality of the final solution substantially improves over the single node solution since the relative squared loss is less than 1. We have carefully verified this. It is most stark when there is only one feature shard, where we know that the solution on that shard is identical to the single node solution. This output prediction is then thresholded to the interval [0, 1] (as the labels are either 0 or 1) and passed to a final prediction node which uses the prediction as a feature and one (default) constant feature to make a final prediction. This very simple final prediction step is where the large improvement in prediction quality occurs. Essentially, because there are only two features (one is constant!), the final output node performs a very careful calibration which substantially improves the squared loss.\nNote that one may have the false intuition that because each node does linear prediction the final output is equivalent to a linear predictor. This is in fact what was suggested in the previous description of the binary tree architecture. However, this is incorrect due to thresholding of the final prediction of each node to the interval [0,1].\nFigure 0.5(b) shows that the improved solution quality degrades mildly\n0.6 Global Update Rules 21\nwith the number of feature shards and the running time is again not decreasing linearly. We believe this failure to scale linearly is due to limitations of Ethernet where the use of many small packets can result in substantially reduced bandwidth.\nA basic question is: How effective is this algorithm in general? Further experiments on other datasets (below) show that the limited representational capacity does degrade performance on many other datasets, motivating us to consider global update rules.\n0.6 Global Update Rules\nSo far we have outlined an architecture that lies in-between Na\u0308\u0131ve Bayes and a linear model. In this section, we investigate various trade-offs between the efficiency of local training procedure of the previous section and the richer representation power of a linear model trained on a single machine. Before we describe these trade-offs, let us revisit the proof of Proposition 4. In that example, the node which gets the feature that is uncorrelated with the label learns a zero weight because its objective is to minimize its own loss, not the loss incurred by the final prediction at the root of the tree. This can be easily fixed if we are willing to communicate more information on each link. In particular, when the root of the tree has received all the information from its children, it can send back to them some information about its final prediction. Once a node receives some information from its master, it can send a similar message to its children. In what follows we show several different ways in which information can be propagated and dealt with on each node. We call these updates global because, in contrast to the local training of the previous section, they use information about the final prediction of the system, to mitigate the problems that may arise from pure local training.\n0.6.1 Delayed Global Update\nAn extreme example of global training is to avoid local training altogether and simply rely on the update from the master. At time t the subordinate node sends to its master a prediction pt using its current weights and does not use the label until time t + \u03c4 when the master replies with the final prediction y\u0302t of the system. At this point the subordinate node computes the gradient of the loss as if it had made the final prediction itself (i.e.\n22\nit computes gdg = \u2202`\n\u2202\u3008w,x\u3009 \u2223\u2223\u2223 \u3008w,x\u3009=y\u0302t x, where x are the node\u2019s features) and\nupdates its weights using this gradient.\n0.6.2 Corrective Update\nAnother approach to global training is to allow local training when an instance is received but use the global training rule and undo the local training as soon as the final prediction is received. More formally, at time t the subordinate node sends a prediction pt to its master and then updates its weights using the gradient g = \u2202`\u2202\u3008w,x\u3009 \u2223\u2223\u2223 \u3008w,x\u3009=pt x. At time t+ \u03c4 it receives the final prediction y\u0302t and updates its weights using gcor = \u2202`\n\u2202\u3008w,x\u3009 \u2223\u2223\u2223 \u3008w,x\u3009=y\u0302t x \u2212\n\u2202` \u2202\u3008w,x\u3009 \u2223\u2223\u2223 \u3008w,x\u3009=pt x. The rationale for using local training is that it might be\nbetter than doing nothing while waiting for the master, as in the case of the delayed global update. However, once the final prediction is available, there is little reason to retain the effect of local training and the update makes sure it is forgotten.\n0.6.3 Delayed Backpropagation\nOur last update rule treats the whole tree as a composition of linear functions and uses the chain rule of calculus to compute the gradients in each layer of the architecture. For example, the tree of Figure 0.3 computes the function\nf(x) = w (2) 12 f12(x1, x2) + w (2) 34 f34(x3, x4)\nf12(x1, x2) = w (1) 1 f1(x1) + w (1) 2 f2(x2) f34(x3, x4) = w (1) 3 f3(x3) + w (1) 4 f4(x4)\nfj(xj) = w (0) j xj j = 1, 2, 3, 4.\nAs before let y\u0302 = f(x) and `(y\u0302, y) be our loss. Then partial derivatives of ` with respect to any parameter w (j) i can be obtained by the chain rule as shown in the following examples:\n\u2202`\n\u2202w (1) 3\n= \u2202`\n\u2202f\n\u2202f\n\u2202f34\n\u2202f34\n\u2202w (1) 3\n= \u2202`(y\u0302, y)\n\u2202y\u0302 w\n(2) 34 f3\n\u2202`\n\u2202w (0) 3\n= \u2202`\n\u2202f\n\u2202f\n\u2202f34 \u2202f34 \u2202f3 \u2202f3\n\u2202w (0) 3\n= \u2202`(y\u0302, y)\n\u2202y\u0302 w\n(2) 34 w (1) 3 x3\n0.6 Global Update Rules 23\nNotice here the modularity implied by the chain rule: once the node that outputs f34 has computed \u2202`\n\u2202w (1) 3\nit can send to its subordinate nodes the\nproduct \u2202`(y\u0302,y)\u2202y\u0302 w (2) 34 as well as the weight it uses to weigh their predictions (i.e. w (1) 3 in the case of the node that outputs f3). The subordinate nodes then have all the necessary information to compute partial derivatives with respect to their own weights. The chain rule suggests that nodes whose predictions are important for the next level are going to be updated more aggressively than nodes whose predictions are effectively ignored in the next level.\nThe above procedure is essentially the same as the backpropagation procedure, the standard way of training with many layers of learned transformations as in multi-layer neural networks. In that case the composition of simple nonlinear functions yields improved representational power. Here the gain from using a composition of linear functions is not in representational power, as f(x) remains linear in x, but in the improved scalability of the system.\nAnother difference from the backpropagation procedure is the inevitable delay between the time of the prediction and the time of the update. In particular, at time t the subordinate node performs local training and then sends a prediction p\u0304t using the updated weights. At time t + \u03c4 it receives from the master the gradient of the loss with respect to p\u0304t: g = \u2202` \u2202p\u0304t \u2223\u2223\u2223 \u3008w,x\u3009=y\u0302t . It then computes the gradient of the loss with respect to its weights using the chain rule: gbp = g \u00b7 \u2202p\u0304t\u2202w . Finally the weights are updated using this gradient.\n0.6.4 Minibatch Gradient Descent\nAnother class of delay-tolerant algorithms is \u201cminibatch\u201d approaches which aggregate predictions from several (but not all) examples before making an aggregated update. Minibatch has even been advocated over gradient descent itself (see Shalev-Shwartz et al., 2007), with the basic principle being that a less noisy update is possible after some amount of averaging.\nA minibatch algorithm could be implemented either on an example shard organized data (as per Dekel et al., 2010) or on feature shard organized data. On an example shard based system, minibatch requires transmitting and aggregating the gradients of all features for an example. In terms of bandwidth requirements, this is potentially much more expensive than a minibatch approach on a feature shard system regardless of whether the\n24\nfeatures are sparse or dense. On the latter only a few bytes/example are required to transmit individual and joint predictions at each node. Specifically, the minibatch algorithms use global training without any delay: once the master has sent all the gradients in the minibatch to his subordinate nodes they perform an update and the next minibatch is processed.\nProcessing the examples in minibatches reduces the variance of the used gradient by a factor of b (the minibatch size) compared to computing the gradient based on one example. However, the model is updated only once every b examples, slowing convergence.\nOnline gradient descent has two properties that might make it insensitive\nto the advantage provided by the minibatch gradient:\n\u2022 Gradient descent is a somewhat crude method: it immediately forgets the gradient after it uses it. Contrast this with, say, bundle methods (Teo\net al., 2009) which use the gradients to construct a global approximation of the loss. \u2022 Gradient descent is very robust. In other words, gradient descent converges even if provided with gradient estimates of bounded variance.\nOur experiments, in the next section, confirm our suspicions and show that, for simple gradient descent, the optimal minibatch size is b = 1.\n0.6.5 Minibatch Conjugate Gradient\nThe drawbacks of simple gradient descent suggest that a gradient computed on a minibatch might be more beneficial to a more refined learning algorithm. An algorithm that is slightly more sophisticated than gradient descent is the nonlinear conjugate gradient (CG) method. Nonlinear CG can be thought as gradient descent with momentum where principled ways for setting the momentum and the step sizes are used. Empirically, CG can converge much faster than gradient descent when noise does not drive it too far astray.\nApart from the weight vector wt, nonlinear CG maintains a direction\nvector dt and updates are performed in the following way:\ndt = \u2212gt + \u03b2tdt\u22121 wt+1 = wt + \u03b1tdt\nwhere gt = \u2211 \u03c4\u2208m(t)\u2207w`(\u3008w, x\u03c4 \u3009 , y\u03c4 ) \u2223\u2223 w=wt is the gradient computed on the t-th minibatch of examples, denoted by m(t). We set \u03b2t according to a widely\n0.6 Global Update Rules 25\nused formula (Gilbert and Nocedal, 1992):\n\u03b2t = max { 0, \u3008gt, gt \u2212 gt\u22121\u3009 ||gt\u22121||2 } which most of the time is maximized by the second term known as the PolakRibie\u0300re update. Occasionally \u03b2t = 0 effectively reverts back to gradient descent. Finally, \u03b1t is set by minimizing a quadratic approximation of the loss, given by its Taylor expansion at the current point:\n\u03b1t = \u2212 \u3008gt, dt\u3009 \u3008dt, Htdt\u3009\nwhere Ht is the Hessian of the loss at wt on the t-th minibatch. This procedure avoids an expensive line search and takes advantage of the simple form of the Hessian of a decomposable loss which allows fast computation of the\ndenominator. In general Ht = \u2211 \u03c4\u2208m(t) ` \u2032\u2032 \u03c4x\u03c4x > \u03c4 where ` \u2032\u2032 \u03c4 = \u22022`(y\u0302,y\u03c4 ) \u2202y\u03022 \u2223\u2223 y\u0302=\u3008wt,x\u03c4 \u3009 is the second derivative of the loss with respect to the prediction for the \u03c4 -th example in the minibatch m(t). Hence the denominator is simply \u3008dt, Htdt\u3009 =\u2211 \u03c4\u2208m(t) ` \u2032\u2032 \u03c4 \u3008dt, x\u03c4 \u3009 2.\nAt first glance it seems that updating wt will be an operation involving two dense vectors. However we have worked out a way to perform these operations in a lazy fashion so that all updates are sparse. To see how this could work assume for now that \u03b2t = \u03b2 is fixed throughout the algorithm and that the i-th element of the gradient is non-zero at times t0 and t1 > t0, and zero for all times \u03c4 in between. We immediately see that\ndi,\u03c4 = \u03c4\u220f\ns=t0\n\u03b2sdi,t0 = di,t0\u03b2 \u03c4\u2212t0 .\nHence, we can compute the direction at any time by storing a timestamp for each weight recording its last modification time. To handle the case of varying \u03b2, we first conceptually split the algorithm\u2019s run in phases. A new phase starts whenever \u03b2t = 0, which effectively restarts the CG method. Hence, within each phase \u03b2t 6= 0. To compute the direction, we need to keep track of Bt the cumulative product of the \u03b2\u2019s from the beginning of the\nphase up to time t and use \u220f\u03c4 s=t0 \u03b2s = B\u03c4 Bt0\n. Next, because each direction dt changes w by a different amount \u03b1t in each iteration, we must keep track of\nAt = \u2211t s=1 \u03b1sBs. Finally, at time t the update for a weight whose feature i was last seen at time \u03c4 is:\nwt,i = w\u03c4,i + At \u2212A\u03c4\u22121\nB\u03c4 d\u03c4,i.\n26\nName # training data # testing data\nRCV1 780K 23K Webspam 300K 50K\nTable 0.1 Description of data sets in global experiments.\n0.6.6 Determinizing the Updates\nIn all of the above updates, delay plays an important role. Because of the physical constraints of the communication, the delay \u03c4 can be different for each instance and for each node. This can have an adverse effect on the reproducibility of our results. To see this it suffices to think about the first time a leaf node receives a response. If that varies, then the number of instances for which this node will send a prediction of zero to its master varies too. Hence the weights that will be learned are going to be different. To alleviate this problem and ensure reproducible results our implementation takes special care to impose a deterministic schedule of updates. This has also helped in the development and debugging of our implementation. Currently, the subordinate node switches between local training on new instances and global training on old instances in a round robin fashion, after an initial period of local training only, that maintains \u03c4 = 1024 (which is half the size of the node\u2019s buffer). In other words, the subordinate node will wait for a response from its master if doing otherwise would cause \u03c4 > 1024. It would also wait for instances to become available if doing otherwise would cause \u03c4 < 1024, unless the node is processing the last 1024 instances in the training set.\n0.7 Experiments\nHere we experimentally compare the predictive performance of the local, the global, and the centralized update rules. We derived classification tasks from the two data sets described in Table 0.1, trained predictors using each training algorithm, and then measured performance on separate test sets. For each algorithm, we perform a separate search for the best learning rate schedule of the form \u03b7t = \u03bb\u221a t+t0\nwith \u03bb \u2208 {2i}9i=0, t0 \u2208 {10i}6i=0. We report results with the best learning rate we found for each algorithm and task. For the minibatch case we report a minibatch size of 1024 but we also tried smaller sizes even though there is little evidence that they can be parallelized efficiently. Finally we report the performance of a centralized stochastic gradient descent (SGD) which corresponds to minibatch gradient descent with a batch size of 1.\n0.8 Conclusion 27\nWe omit results for the Delayed Global and Corrective update rules because they have serious issues with delayed feedback. Imagine trying to control a system (say, driving a car) that responds to actions after much delay. Every time an action is taken (such as steering in one direction) it is not clear how much it has affected the response of the system. If our strategy is to continue performing the same action until its effect is noticeable, it is likely that by the time we receive all the delayed feedback, we will have produced an effect much larger than the desired. To reduce the effect we can try to undo our action which of course can produce an effect much smaller than what was desirable. The system then oscillates around the desired state and never converges there. This is exactly what happens with the delayed global and corrective update rules. Delayed backpropagation is less susceptible to this problem because the update is based on both the global and the local gradient. Minibatch approaches completely sidestep this problem because the information they use is always a gradient at the current weight vector.\nIn Figure 0.6 we report our results on each data set. We plot the test accuracy of each algorithm under different settings. \u201cBackprop x8\u201d is the same as backprop where the gradient from the master is multiplied by 8 (we also tried 2, 4, and 16 and obtained qualitatively similar results)\u2014we tried this variant as a heuristic way to balance the relative importance of the backprop update and that of the local update. In the first row of Figure 0.6, we show that the performance of both local and global learning rules degrades as the degree of parallelization (number of workers) increases. However, this effect is somewhat lessened with multiple passes through the training data and is milder for the delayed backprop variants, as shown in in the second row for the case of 16 passes. In the third and fourth rows, we show how performance improves with the number of passes through the training data, using 1 worker and 16 workers. Notice that SGD, Minibatch, and CG are not affected by the number of workers as they are global-only methods. Among these methods SGD dominates CG which in turn dominates minibatch. However, SGD is not parallelizable while minibatch CG is.\n0.8 Conclusion\nOur core approach to scaling up and parallelizing learning is to first take a very fast learning algorithm, and then speed it up even more. We found that a core difficulty with this is dealing with the problem of delay in online learning. In adversarial situations, delay can reduce convergence speed by the delay factor, with no improvement over the original serial learning algorithm.\nWe addressed these issues with parallel algorithms based on feature shard-\n28\n0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99\n1\n1 2 4 8 16\nac cu\nra cy\nworker number\nRCV1, 1 pass\n0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99\n1\n1 2 4 8 16\nac cu\nra cy\nworker number\nWebspam, 1 pass\nLocal Backprop Backprop x8 SGD\nMini1024 CG1024\n0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99\n1\n1 2 4 8 16\nac cu\nra cy\nworker number\nRCV1, 16 passes\n0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99\n1\n1 2 4 8 16\nac cu\nra cy\nworker number\nWebspam, 16 passes\n0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99\n1\n1 2 4 8 16\nac cu\nra cy\npass number\nRCV1, 1 worker\n0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99\n1\n1 2 4 8 16\nac cu\nra cy\npass number\nWebspam, 1 worker\n0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99\n1\n1 2 4 8 16\nac cu\nra cy\npass number\nRCV1, 16 workers\n0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99\n1\n1 2 4 8 16\nac cu\nra cy\npass number\nWebspam, 16 workers\nFigure 0.6 Experimental results compare global to local learning rules. In the first two rows, we see how performance degrades for various rules as the number of workers increases. In the last two rows, we show how performance changes with multiple passes. All plots share the same legend, shown in the top right plot.\n0.8 Conclusion 29\ning. The first is simply a very fast multicore algorithm which manages to avoid any delay in weight updates by virtue of the low latency between cores. The second approach, designed for multinode settings, addresses the latency issue by trading some loss of representational power for local-only updates, with the big surprise that this second algorithm actually improved performance in some cases. The loss of representational power can be addressed by incorporating global updates either based on backpropagation on top of the local updates or using a minibatch conjugate gradient method; experimentally, we observed that the combination of local and global updates can improve performance significantly over the local-only updates.\nThe speedups we have found so far are relatively mild due to working with a relatively small number of cores, and a relatively small number of nodes. Given that we are starting with an extraordinarily fast baseline algorithm, these results are unsurprising. A possibility does exist that great speedups can be achieved on a large cluster of machines, but this requires further investigation."}], "references": [{"title": "Random Features for Large-Scale Kernel Machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In: Platt, J.C., Koller, D., Singer, Y., and Roweis, S. (eds), Advances in Neural Information Processing Systems 20. Cambridge, MA: MIT Press.", "citeRegEx": "Rahimi and Recht,? 2008", "shortCiteRegEx": "Rahimi and Recht", "year": 2008}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review, 65(6), 386\u2013408.", "citeRegEx": "Rosenblatt,? 1958", "shortCiteRegEx": "Rosenblatt", "year": 1958}, {"title": "Learning Internal Representations by Error Propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Chap. 8, pages 318\u2013362 of: Parallel Distributed Processing. Cambridge, MA: MIT Press.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Pegasos: Primal Estimated sub-GrAdient Solver for SVM", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan."], "venue": "In: Proc. Intl. Conf. Machine Learning.", "citeRegEx": "Shalev.Shwartz et al\\.,? 2007", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Hash Kernels", "author": ["Shi", "Qinfeng", "Petterson", "James", "Dror", "Gideon", "Langford", "John", "Smola", "Alex", "Strehl", "Alex", "S.V.N. Vishwanathan"], "venue": "In: Welling, Max, and van Dyk, David (eds), Proc. Intl. Workshop on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics.", "citeRegEx": "Shi et al\\.,? 2009", "shortCiteRegEx": "Shi et al\\.", "year": 2009}, {"title": "Bundle Methods for Regularized Risk Minimization", "author": ["Teo", "Choon Hui", "S.V.N. Vishwanthan", "Smola", "Alex J.", "Le", "Quoc V."], "venue": "J. Mach. Learn. Res. Submitted in February 2009.", "citeRegEx": "Teo et al\\.,? 2009", "shortCiteRegEx": "Teo et al\\.", "year": 2009}, {"title": "Feature Hashing for Large Scale Multitask Learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Attenberg", "J. Langford", "A.J. Smola"], "venue": "In: Bottou, L., and Littman, M. (eds), International Conference on Machine Learning.", "citeRegEx": "Weinberger et al\\.,? 2009", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "A canonical example of this is provided by the perceptron algorithm (Rosenblatt, 1958) which modifies a weight vector by adding or subtracting the features of a misclassified instance.", "startOffset": 68, "endOffset": 86}, {"referenceID": 2, "context": "For the training of complex nonlinear prediction systems, the stochastic gradient descent approach was described long ago and has been standard practice for at least two decades (Bryson and Ho, 1969; Rumelhart et al., 1986; Amari, 1967).", "startOffset": 178, "endOffset": 236}, {"referenceID": 0, "context": "For instance, we may use the random kitchen sink features (Rahimi and Recht, 2008) to obtain prediction performance comparable with Gaussian RBF kernel classes.", "startOffset": 58, "endOffset": 82}, {"referenceID": 4, "context": "Note that VW can additionally reduce the dimensionality of each instance using feature hashing (Shi et al., 2009; Weinberger et al., 2009) , which is essential when the (expanded) feature space is large, perhaps even exceeding memory size.", "startOffset": 95, "endOffset": 138}, {"referenceID": 6, "context": "Note that VW can additionally reduce the dimensionality of each instance using feature hashing (Shi et al., 2009; Weinberger et al., 2009) , which is essential when the (expanded) feature space is large, perhaps even exceeding memory size.", "startOffset": 95, "endOffset": 138}, {"referenceID": 5, "context": "Contrast this with, say, bundle methods (Teo et al., 2009) which use the gradients to construct a global approximation of the loss.", "startOffset": 40, "endOffset": 58}], "year": 2011, "abstractText": null, "creator": "LaTeX with hyperref package"}}}