{"id": "1104.5071", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2011", "title": "Attacking and Defending Covert Channels and Behavioral Models", "abstract": "In similar well we actually systems now attacking the defending $ k $ - h2o csa analysis techniques own represent machine, provided not, in network traffic review as involvement airwaves triggers. The main made result is our anti-government several could to use a behavior ' s should initiate ' $ o $ - allowing statistics from its put stochastic begun that part giving this $ k $ - effectively stationary compiling but uncommon primarily, deliberately built, $ (k + respectively) $ - must statistics take desired. Such a model happen next \" synanthedon \" an the focus or orientation which a defender n't might to monitor whether gives bystanders unlike balancing along manner. By manipulated strategies constructed $ (k + 1) $ - unless cues, , heskey will fill to say if among self-destructive are similar days the monitor. We also develop three-dimensional more source codes actually appreciate the $ formula_5 $ - should indicate of a solutions again encoding transferring data. One regarding circumstances of addition results any that no particular significant thinking analyses pioneered come thrown soon of {\\ bks pulling race} for into clarity that then success how to one campaigning for to enough innovative resources practice allowed the why.", "histories": [["v1", "Wed, 27 Apr 2011 04:12:47 GMT  (441kb,DS)", "http://arxiv.org/abs/1104.5071v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["valentino crespi", "george cybenko", "annarita giani"], "accepted": false, "id": "1104.5071"}, "pdf": {"name": "1104.5071.pdf", "metadata": {"source": "CRF", "title": "Attacking and Defending Covert Channels and Behavioral Models", "authors": ["Valentino Crespi", "George Cybenko", "Annarita Giani"], "emails": ["vcrespi@calstatela.edu.", "gvc@dartmouth.edu.", "agiani@eecs.berkeley.edu."], "sections": [{"heading": null, "text": "In this paper we present methods for attacking and defending k-gram statistical analysis techniques that are used, for example, in network traffic analysis and covert channel detection. The main new result is our demonstration of how to use a behavior\u2019s or process\u2019 k-order statistics to build a stochastic process that has those same k-order stationary statistics but possesses different, deliberately designed, (k + 1)- order statistics if desired. Such a model realizes a \u201ccomplexification\u201d of the process or behavior which a defender can use to monitor whether an attacker is shaping the behavior. By deliberately introducing designed (k + 1)-order behaviors, the defender can check to see if those behaviors are present in the data. We also develop constructs for source codes that respect the k-order statistics of a process while encoding covert information. One fundamental consequence of these results is that certain types of behavior analyses techniques come down to an arms race in the sense that the advantage goes to the party that has more computing resources applied to the problem.\nPoints of view in this document are those of the authors and do not necessarily represent the official position of the sponsoring\nagencies or the U.S. Government.\nV. Crespi is with the Department of Computer Science, California State University at Los Angeles, Los Angeles CA, 90032 USA. email: vcrespi@calstatela.edu. Crespi\u2019s work was partially supported by AFOSR Grant FA9550-07-1-0421 and by NSF Grant HRD-0932421.\nG. Cybenko is with the Thayer School of Engineering, Dartmouth College, Hanover NH 03755. email: gvc@dartmouth.edu. Cybenko\u2019s work was partially supported by Air Force Research Laboratory contracts FA8750-10-1-0045, FA8750-09-1-0174, AFOSR contract FA9550-07-1-0421, U.S. Department of Homeland Security Grant 2006-CS-001-000001 and DARPA Contract HR001-06-1-0033\nA. Giani is with the Department of EECS, University of California at Berkeley, Berkeley CA 94720. email: agiani@eecs.berkeley.edu. Giani\u2019s\u2019s work was partially supported by U.S. Department of Homeland Security Grant 2006-CS001-000001 and DARPA Contract HR001-06-1-0033 when she was a Ph.D. student at Dartmouth\nApril 28, 2011 DRAFT\nar X\niv :1\n10 4.\n50 71\nv1 [\ncs .L\nG ]\n2 7\nA pr\n2 01\nIndex Terms\nCovert Channels, Exfiltration, Probabilistic Automata, Cognitive Attack, Anomaly Detection.\nI. INTRODUCTION\nComputer security researchers have been investigating statistical behavioral modeling techniques as a means for determining whether a machine, a network or data packet contents are behaving \u201cnormally\u201d or not. These are so-called behavior analysis techniques and implicitly model stochastic processes at some level of fidelity.\nConsider for example, the problem of detecting covert channels. Some existing approaches assume that an adversary has installed an exfiltrating agent, or Trojan, which operates by encoding data in a way that introduces detectable regularities in some network traffic statistics. For example, Giani et al. [1] and Cabuk et al. [2] estimate certain first order statistics of packet inter-arrival delays in order to determine whether a time covert channel is being used. Dainotti et al. [3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies. Other techniques are based on various analyses of n-gram statistics [9]. In fact, some have called techniques that match n-gram statistics \u201cmimicry attacks\u201d and while techniques have been developed for detecting certain simple types of mimicry, techniques for building mimicry attacks as described in the present paper appear to be novel [9].\nGeneral discussions of covert channels and their taxonomies, existence and modeling have been published [10], [11], [12], [13], [14]. The design, implementation and experimental evaluation of several specific covert channel attacks in real systems is of specific interest [14]. That work presents threat models, achievable bit rates, noise properties and channel capacities for covert channels.\nThe existence and successful use of a covert channel is based on the assumption that the covert channel code does not perturb the measured statistical properties of behavior so that, over time, a covert transmission does not introduce discernible patterns which are different than expected, at least with respect to what is measured. In this paper we assume the ability to learn a k-gram type model of \u201cnormal behavior.\u201d This is simply done by counting the occurrences of k-grams and then normalizing to produce frequencies or probabilities. It is important to note that researchers often talk about entropy as a channel statistic [10], [15] but entropy is typically\nApril 28, 2011 DRAFT\ncalculated from k-order statistics so that our methods for preserving k-order statistics preserves all lower order statistics and will also preserve the entropy.\nWe present a technique for encoding messages that respects these k-order statistics. Both attacker and defender can use this coding technique. The attacker could exfiltrate coded information while the defender could embed an encoded reference message or carrier to detect manipulations of the channel by an adversary attempting covert communications. That is, for any order k, an attacker or a defender can encode covert messages while otherwise respecting the k-order statistics of the traffic.\nAlso, we show how a defender can create a process of order k + 1 which has the same k-order statistics but specifically designed (k + 1)-order statistics that the defender can easily monitor to see if the (k+1)-order statistics have been changed. Researchers have recently started to develop systematic taxonomies and examples of attacks against statistical machine learning techniques [16]. In that spirit, the present work develops specific techniques to both attack and defend using certain statistical approaches.\nWe discuss these methods in the context of behaviors that have a finite set of observable symbols (the alphabet). Interpacket arrival times, packet sizes, header fields, packet contents and so on are examples of such observables if quantized into a finite number of bins. Our approach models the observables as a stationary stochastic process X [17]. After estimating the k-order statistics, we build a Probabilistic Deterministic Finite Automata model (PDFA) [18], [19], [20], [21] that realizes the k-order statistics.\nUsing that PDFA, we show that:\n1) an adversary can encode messages covertly while respecting the k-order statistics; 2) the defender can encode reference messages or a carrier while respecting the k-order\nstatistics and;\n3) the defender can build a more complex process which has the same k-order statistics but\npossesses deliberately designed (k + 1)-order statistics.\nExamples of such covert channels in network traffic include, but are not restricted to:\n\u2022 Timing Channels: The observable symbols are the inter-packet time delays, appropriately\nquantized;\n\u2022 Size Channels: The observable symbols are quantized sizes of the packets; \u2022 Header Channels: The observable symbols are various header fields in TCP/IP packets\nApril 28, 2011 DRAFT\nwhich can be manipulated by the transmitting entity without violating protocol semantics. Several such fields are known to exist [22].\nIt is important to clarify right away what we mean by a k-order statistic and a k-gram. Suppose we have an alphabet consisting of {\u03b1, \u03b2, \u03b3} and we observe a sequence comprised of that alphabet, say\n\u03b1\u03b1\u03b1\u03b2\u03b1\u03b2\u03b3\u03b3.\nThe first order statistics are [1/2 1/4 1/4] indicating that 1/2 of the symbols are \u03b1\u2019s, 1/4 are \u03b2\u2019s and 1/4 are \u03b3\u2019s. The 1-grams are merely {\u03b1, \u03b2, \u03b3}.\nThe 2-grams observed in this sequence are \u03b1\u03b1, \u03b1\u03b1, \u03b1\u03b2, \u03b2\u03b1, \u03b1\u03b2, \u03b2\u03b3, \u03b3\u03b3 and the 2-order\nstatistics for the 9 possible 2-grams\n\u03b1\u03b1, \u03b1\u03b2, \u03b1\u03b3, \u03b2\u03b1, \u03b2\u03b2, \u03b2\u03b3, \u03b3\u03b1, \u03b3\u03b2, \u03b3\u03b3\nare respectively\n[2/7 2/7 0 1/7 0 1/7 0 0 1/7].\nThat is, our k-grams are obtained by moving a sliding window of width k across the data one symbol at a time. This is not to be confused with moving that window across the data sequence k symbols at a time.\nThe following discussion shows how a timing covert channel can be constructed based on a beacon and argues that a naive encoding of covert messages based on packet inter-arrival times produces a clearly detectable distortion of the 1-order statistics of those time intervals in network traffic [23], [1].\nFigure 1 describes the setup. Machine A sends a regularly timed beacon to machine D. (Such a beacon can be a time server request or a stay alive beacon for instance.) The inter-packet delays seen at machine B are not regular due to internal routing delays in the LAN. (These statistics were actually measured from a regularly timed beacon traveling several hops.) An intruder was able to compromise and control machine B which is inside the local network and a relay for the traffic between A and D. (B could be a proxy server, border router or other device for example.) Assume we set up a machine C outside the internal network perimeter to check for timing covert channels. C has seen a certain distribution of inter-packet delays coming from A going to D.\nApril 28, 2011 DRAFT\nIn this paper, we show how machine B can encode covert messages in the inter-packet delays in such a way that the first order statistics as seen by C remain unchanged from the original distribution. Conversely, we can deliberately defend against such channels by encoding messages so that any manipulation of the delays will be detectable on the outside at machine C, because the covert message will not be received at C.\nFigure 2 shows the number of packets received with a given delay in two scenarios. The horizontal axis reports the inter-arrival time in seconds, and the vertical axis the number of packets received with those delays. In the left graph of Figure 2 are the observed inter-packet delays resulting from a regularly timed beacon traversing multiple hops in a LAN. On the right hand graph, we depict a naive covert timing channel using two time intervals to encode a message. It is evident from the data that the naive covert communication in the right graph can be easily detected if the 1-order statistics of normal traffic have been measured and are those on\nApril 28, 2011 DRAFT\nthe left. However, the 1-order distribution on the left can be generated either by normal traffic, as it was obtained, or by a covert channel, as we will show.\nIn this contribution, we develop a more sophisticated approach than the naive approach insofar as we consider also statistics of arbitrarily higher order, i.e. k > 1, and our results effectively show that, for any k, defenders and attackers both have technical approaches for, respectively, attacking or defending a k-order behavior with respect to covert communications. Consequently, the situation is an arms race in the sense that whichever side has the ability to learn the highest order statistics wins."}, {"heading": "A. Outline of the Paper", "text": "In Section II we present an illustrative example. In Section III we describe our method and show how to manipulate a behavior\u2019s statistics with Probabilistic Automata. In Section IV we provide a numerical example. In Section V we show how to use Probabilistic Automata to build a channel code that respects the statistics of traffic up to some predecided order. Finally, Section VI contains some conclusions and future work references.\nApril 28, 2011 DRAFT"}, {"heading": "II. A SIMPLE ILLUSTRATIVE EXAMPLE", "text": "To illustrate these concepts, consider a simple binary observable with values, 0 and 1. It is assumed that these observables are irrelevant to the normal operation of the underlying system and its semantics. For example, the observables could be quantized inter-arrival times or unused packet header fields.\nAssume that the 1-order statistics of these observables are r0 > 0 and r1 > 0 with r0 +r1 = 1. This means that the relative frequency of 0\u2019s and 1\u2019s as observed in the behavior are r0 and r1 respectively. Now suppose an attacker has estimated these probabilities and seeks to exfiltrate messages while respecting these probabilities. This is possible and, later in this paper, we review standard source coding ideas that allow the attacker to create such codes efficiently.\nIn fact, if the messages to be sent are binary and Bernoulli with p = 0.5 (such as for encrypted and/or compressed messages), then there are codes that use 1/H(r0) = 1/H(r1) bits in the covert channel per original message symbol where H(x) = \u2212(x log2 x + (1 \u2212 x) log2(1 \u2212 x)) is the entropy function. We show how to construct such codes to respect k-order statistics as well.\nBy the same token, the defender can encode a reference signal, also respecting the first order\nstatistics as above, which can be decoded and verified at the receiving end.\nNote that no specific second order statistics r00, r01, r10 and r11 have been modeled so far, but if the process is modeled by a Bernoulli process with p = r1 then the second order statistics would be ri \u00b7 rj = rij = Prob(ij) = Prob(ji) by independence.\nHowever, the defender can construct a second order process with second order statistics r00, r01, r10 and r11 for which rij 6= ri \u00b7 rj while satisfying the required first order statistics, namely r0 and r1. If the attacker exploits the channel through a purely first order process, the constructed second order statistics rij will likely not be observed by the defender who could then conclude that the traffic is being shaped by an adversary.\nTo illustrate this 2-order construction, consider an automaton with two states, Q = {0, 1}, corresponding to the two 1-grams of observables. Let X be the matrix of the transition probabilities\nX =  p00 p01 p10 p11  . We seek PDFAs that have the stationary distribution \u03c0 = [r0 1\u2212 r0] = [1\u2212 r1 r1] = [1\u2212 r r].\nApril 28, 2011 DRAFT\nSpecifically, we seek X that satisfies\n[1\u2212 r r] \u00b7X = \u03c0X = \u03c0 = [1\u2212 r r]\nwith X being a stochastic matrix (non-negative with row sums equal to 1). The class of PDFAs that are 1-order equivalent to the given process is therefore determined by a set of linear equality and inequality constraints as follows:\n(1\u2212 r) \u00b7 p00 + r \u00b7 p01 = 1\u2212 r\n(1\u2212 r) \u00b7 p10 + r \u00b7 p11 = r\np00 + p01 = 1\np10 + p11 = 1\npij \u2265 0.\nThe four equations are linearly dependent and we can reduce them to the three equations and\nconstraints\nr \u00b7 p11 \u2212 (1\u2212 r) \u00b7 p00 = 2 \u00b7 r \u2212 1\np00 + p01 = 1\np10 + p11 = 1\n0 \u2264 p00 , p11 \u2264 1.\nThere are an infinite number of solutions according to\np11 = 1\u2212 r r p00 + 2r \u2212 1 r , 0 \u2264 p00 , p11 \u2264 1. (1)\nFor example, if r = 0.3, 1\u2212 r = 0.7 then the constraints become:\np11 = 0.7p00 \u2212 0.4\n0.3\n0 \u2264 p00 , p11 \u2264 1\nso letting p00 = 0.8 we get p11 = 0.160.3 = 0.53\u0304 and therefore p01 = 0.2 and p10 = 0.46\u0304. This\nApril 28, 2011 DRAFT\nyields 2-order statistics of:\nr00 = p00\u03c01 = 0.8 \u00b7 0.7 = 0.56\nr01 = p01\u03c01 = 0.2 \u00b7 0.7 = 0.14\nr10 = p10\u03c02 = 0.46\u0304 \u00b7 0.3 = 0.14\nr11 = p11\u03c02 = 0.53\u0304 \u00b7 0.3 = 0.16.\nNotice that r01 = r10 = 0.14, r01 + r00 = r0 = \u03c01 = 0.3 and r01 + r11 = r1 = \u03c02 = 0.7 as required.\nAnother, equivalent way to derive these relations is to note that there are two trivial solutions for X , namely X1 = I2 (the 2 by 2 identity matrix) and X2 = 1 \u00b7 \u03c0 where 1 = [1 1]T is the column vector whose entries are all 1\u2019s. These two solutions are always different. Moreover, we can see that any convex combination \u03c1X1 + (1 \u2212 \u03c1)X2 for 0 \u2264 \u03c1 \u2264 1 is also a solution to all the constraints and in fact yields the same class of solutions as above.\nThe point of this example is that we can shape the second order statistics of the observables without changing the first order statistics. In particular, multiple choices for p00 (and so for r00) are possible, all of which lead to the same 1-order statistics. A defender can shape the second order statistics so that if an attacker only obeys the first order statistics, the defender can detect that the expected second order statistics are wrong.\nNote that the second order process in this example satisfies an additional constraint - namely, the marginal distributions must agree with the first order process, namely r01 + r11 = r1 and so on. Moreover, r01 = r10 must be true as well (this is a symmetry which arises from considering the 0 to 1 and 1 to 0 transitions in the observed sequence which must be equal). For higher order processes, the construction involves identifying and dealing with additional constraints and finding realizations which satisfy them. These generalizations to higher orders are one of the main contributions of this paper.\nTo apply this construction to the empirical data shown in Figure 2, normalize the counts into frequencies or probabilities by dividing by the total packet count. This yields a vector of probabilities:\nR = [ 0.0029 0.0144 0.0734 0.1453 0.3094 0.1295 0.1151 0.1079 0.1007 0 0 0 0.0014 ] (2)\nApril 28, 2011 DRAFT\nwhere the coordinates 1 through 13 correspond to delays of 0.01 through 0.13 in increments of 0.01.\nWe seek to construct a Markov Chain whose states correspond to observable inter-packet delays and whose transition probabilities, P , describe the probability that one delay follows another. As explained above, P must satisfy two matrix equations (capturing the facts that R is a stationary vector for P and that P is row stochastic)"}, {"heading": "R \u2217 P = R and P \u2217 1 = 1", "text": "where 1 is the column vector of all ones. Moreover, the entries of P are all non-negative.\nIn this simple case, there are two solutions which are simple to identify, namely\nPB = 1 \u2217R and PD = I (3)\nwhere I is the 13 by 13 identity matrix. The reader can easily check that both these matrices satisfy the two required matrix equations. This construct is simple for 1-grams but becomes more complex for general k-grams as shown below.\nMoreover, for any 0 \u2264 \u03b1 \u2264 1, P\u03b1 = \u03b1PB + (1\u2212\u03b1)PD is also a solution. Whereas PB defines a Bernoulli process and PD describes a completely disconnected Markov Chain with an infinite number of fixed distributions, P\u03b1 defines a Markov Chain that is irreducible, aperiodic and not a Bernoulli process for any 0 < \u03b1 < 1 . Therefore, P\u03b1 can be used by a defender to create specific second order statistics which an attacker would have to first model and then respect."}, {"heading": "III. CONSTRUCTING THE AUTOMATA", "text": "In this section, we show how to construct automata that can reproduce observed statistics\ncomputed from data.\nLet \u03a3 = {a, b, c, ...} be the finite observable alphabet and \u03c3 = |\u03a3| < \u221e be the number of observables. We are assuming that we have sequences of observables from which we compute the relative frequencies of k-grams (k \u2265 1):\n0 \u2264 R(x) \u2264 1, \u2211 x\u2208\u03a3k R(x) = 1.\nHere \u03a3k is the set of k-grams; that is, the set of all possible sequences of length k drawn from the alphabet \u03a3.\nApril 28, 2011 DRAFT\nRoughly speaking, if s0s1...sn\u22121 = S0:n\u22121 is an observed data sequence of length n > k, R(x) is approximated by the number of occurrences of the substring x in S0:n\u22121 divided by the total number of substrings of length k in S0:n\u22121, namely n \u2212 k + 1. The set of R(x)\u2019s is precisely what we mean by the k-order statistics of the observations.\nThese statistics must satisfy certain regularity conditions required by the proposed construction\nso some care must be taken in their computation. Specifically, the identity\u2211 a\u2208\u03a3 R(ay) = \u2211 b\u2208\u03a3 R(yb) = R(y) should hold for every y \u2208 \u03a3k\u22121. This can be accomplished by appending S0:n\u22121 with s0s1...sk\u22122 as a suffix, creating a periodic string effectively, and counting occurrences in the periodic string.\nMoreover, this can be repeated for every 1 < j < k by using a circular buffer appending\ns0s1...sj\u22122. All marginal distributions\u2211 w\u2208\u03a3k\u2212j R(wy) = \u2211 w\u2208\u03a3k\u2212j R(yw) = R(y) will hold for all y \u2208 \u03a3j then. (Details are left to the reader.)\nWe will now construct a special type of Markov Chain in which \u03a3k are the states and the semantics of the k-grams are preserved so that if x = ay \u2208 \u03a3k is an observed k-gram, then P (ay, yb) is the probability of transitioning to state yb where both a, b \u2208 \u03a3. Such transitions are the only ones possible in the Markov Chain k-gram model. Such models are called kth-order Markov Models, k Markov Chains or k-gram models by different authors [19], [24].\nLet \u03c0 be the vector of measured k-gram statistics, R(x), and let P be the desired Markov\nChain transition probabilities:\nP = (P (x, x\u2032))\nwhere the entries of both \u03c0 and P are indexed by x, x\u2032 \u2208 \u03a3k.\nThe stationary probabilities of the desired Markov Chain are precisely \u03c0 when the equation \u03c0P = \u03c0 is satisfied. This matrix equation consists of \u03c3k equations and the stochasticity requirement on P is another \u03c3k equations resulting in the following 2\u03c3k equations overall:\n\u2211 x\u2208\u03a3k P (x, x\u2032)R(x) = R(x\u2032), \u2200x\u2032 \u2208 \u03a3k , (stationary probability conditions) (4)\n\u2211 x\u2032\u2208\u03a3k P (x, x\u2032) = 1, \u2200x \u2208 \u03a3k (probability requirements) (5)\nApril 28, 2011 DRAFT\nwhere P (x, x\u2032) \u2265 0 as well.\nBecause of the relationship between k-grams and the Markov Chain that we are seeking to construct, we can only have P (x, x\u2032) 6= 0 when x = ay and x\u2032 = yb for some a, b \u2208 \u03a3 and y \u2208 \u03a3k\u22121. That is, y is the suffix of the state x = ay and we can only transition to states x\u2032 = yb which have y as a prefix and some suffix b \u2208 \u03a3. Accordingly, for every y \u2208 \u03a3k\u22121, we have the 2\u03c3 equations \u2211 a\u2208\u03a3\nP (ay, yb)R(ay) = R(yb), \u2200b \u2208 \u03a3 , (6) \u2211 b\u2208\u03a3 P (ay, yb) = 1, \u2200a \u2208 \u03a3 (7)\nP (ay, yb) \u2265 0 (8)\nwhich are completely decoupled from the equations corresponding to (k \u2212 1)-grams other than y. Accordingly, we can solve each system independently.\nNoting that the k-grams statistics, R(x), satisfy the marginalization relations\u2211 a\u2208\u03a3 R(ay) = \u2211 b\u2208\u03a3 R(yb) = R(y), \u2200y \u2208 \u03a3k\u22121,\nsumming over b in the equations (6), we get\u2211 b\u2208\u03a3 \u2211 a\u2208\u03a3 P (ay, yb)R(ay) = \u2211 a\u2208\u03a3 \u2211 b\u2208\u03a3 P (ay, yb)R(ay) = \u2211 a\u2208\u03a3 R(ay) = \u2211 b\u2208\u03a3 R(yb) = R(y) which is an identity not involving the unknown P (ay, yb).\nAccordingly, there are no more than 2\u03c3 \u2212 1 linearly independent equations in (6). In fact, if we define pre(y) to be the number of nonzero R(ay) and post(y) be the number of nonzero R(yb), there are in fact no more than pre(y) \u00b7 post(y) unknown probabilities, P (ay, yb), and no more than pre(y) + post(y)\u2212 1 independent equations altogether."}, {"heading": "A. The Standard Solution", "text": "One solution to the equations, which we call the Standard Solution, is P\u0304 (ay, yb) = R(yb)/R(y)\nbecause then\u2211 a\u2208\u03a3 P\u0304 (ay, yb)R(ay) = \u2211 a\u2208\u03a3 R(ay)R(yb)/R(y) = R(yb)/R(y) \u2211 a\u2208\u03a3 R(ay) = R(yb) and \u2211 b\u2208\u03a3 P\u0304 (ay, yb) = \u2211 b\u2208\u03a3 R(yb)/R(y) = R(y)/R(y) = 1.\nApril 28, 2011 DRAFT\nThis specific solution has pre(y) \u00b7 post(y) nonzero probabilities, P (ay, yb), for the substring y \u2208 \u03a3k\u22121 by construction.\nBy construction, this Markov Chain is irreducible because we have constructed the transition probabilities from a circular buffer so that there is a nonzero probability of going from any state with nonzero probability, namely R(x), to any other state with nonzero probability. If additionally the constructed Standard Solution Markov Chain is aperiodic, its unique stationary distribution is precisely R(x) and its entropy rate is\nH(P\u0304 ) = HP (Xk+1|X k 1 ) = \u2212 \u2211 a\u2208\u03a3 \u2211 y\u2208\u03a3k\u22121 \u2211 b\u2208\u03a3 R(ay)P\u0304 (ay, yb) log(P\u0304 (ay, yb)). (9)"}, {"heading": "B. Extended Solutions", "text": "If pre(y) and post(y) are both strictly greater than 1, then pre(y) \u00b7 post(y) > pre(y) + post(y) \u2212 1. From the theory of linear programming, there are feasible solutions to the linear program defined by (6), (7) and (8) which have no more than pre(y) + post(y) \u2212 1 nonzero coordinates, namely the Basic Feasible Solutions [25].\nLet such a Basic Feasible Solution be P\u0302 (ay, yb). As derived above, there are solutions with exactly pre(y) \u00b7 post(y) nonzero coordinates, namely the Standard Solutions, P\u0304 (ay, yb). Note that strict convex combinations of P\u0302 with P\u0304 , Pu = uP\u0302 + (1 \u2212 u)P\u0304 with 0 < u < 1, define a continuum of solutions to (6), (7) and (8), with each solution corresponding to an irreducible Markov Chain. This is the case because every state is reachable from every other state with nonzero probability due to the construction of the Standard Solution.\nMoreover, when pre(y) and post(y) are both strictly greater than 1, P\u0302 and P\u0304 are different. As an aside, we have observed that Basic Feasible Solutions typically result in reducible chains because those solutions involve a minimal number of nonzero transition probabilities."}, {"heading": "IV. NUMERICAL EXAMPLES", "text": "In this section we demonstrate the constructions described above.\n1) We consider data generated by the automata depicted in Figure 3 which is a Hidden Markov\nModel (HMM), M = {A(0), A(1)}, defined by the two transition matrices\nA(0) =  0.5 0.5 0 0.5  , A(1) =  0 0 0.5 0  . April 28, 2011 DRAFT\n!\" #\"\n$\"%\"$&'\"\n$\"%\"$&'\"\n!\"%\"$&'\"\n$\"%\"$&'\"\nand  R(000) = 0.338 R(001) = 0.174 R(010) = 0.244 R(011) = 0.000 R(100) = 0.174 R(101) = 0.070 R(110) = 0.000\nR(111) = 0.000\n .\nObserve that R(01) = R(10) which is a necessary regularity that follows from the marginalization property: \u2211 a R(ay) = \u2211 b R(yb) = R(y) . In order to be sure that the estimates verify those consistency conditions we have treated the data stream as a circular buffer as described previously. 3) We built the Standard Solution, P , where P (ay, yb) = R(yb)/R(y), and then we computed\na different numerical solution, P\u0302 , of the linear program (6), (7) and (8).1 The two solutions are summarized below: P (00, 00) = 0.678 P (00, 01) = 0.322 P (10, 00) = 0.678 P (10, 01) = 0.322 P (01, 10) = 1.000 P (01, 11) = 0.000 P (11, 10) = 1.000\nP (11, 11) = 0.000\n ,  P\u0302 (00, 00) = 1.000 P\u0302 (00, 01) = 0.000 P\u0302 (10, 00) = 0.000 P\u0302 (10, 01) = 1.000 P\u0302 (01, 10) = 1.000 P\u0302 (01, 11) = 0.000 P\u0302 (11, 10) = 0.000\nP\u0302 (11, 11) = 1.000\n .\nNote that the Basic Feasible Solution, P\u0302 , has a maximal number of zeros and results in a reducible chain with three communicating classes, namely 00, {01, 10}, 11. By convexity\n1P\u0302 is a Basic Feasible Solution obtained by employing the Matlab linprog function.\nApril 28, 2011 DRAFT\nPu = u \u00b7 P + (1 \u2212 u) \u00b7 P\u0302 is also a solution, for any 0 < u < 1, so that for u = 0.5 and u = 0.2 we obtain respectively the following two different 2-grams: P0.5(00, 00) = 0.839 P0.5(00, 01) = 0.161 P0.5(10, 00) = 0.339 P0.5(10, 01) = 0.661 P0.5(01, 10) = 1.000 P0.5(01, 11) = 0.000 P0.5(11, 10) = 0.500\nP0.5(11, 11) = 0.500\n ,  P0.2(00, 00) = 0.936 P0.2(00, 01) = 0.064 P0.2(10, 00) = 0.136 P0.2(10, 01) = 0.864 P0.2(01, 10) = 1.000 P0.2(01, 11) = 0.000 P0.2(11, 10) = 0.200\nP0.2(11, 11) = 0.800\n .\n4) Now compare the original 2-order statistics specified by M with the statistics specified by\nthe two new models, namely P0.5 and P0.2 as above: R(00) = 0.513 R(01) = 0.244 R(10) = 0.244\nR(11) = 0.000\n ,  R0.5(00) = 0.513 R0.5(01) = 0.244 R0.5(10) = 0.244\nR0.5(11) = 0.000\n ,  R0.2(00) = 0.513 R0.2(01) = 0.244 R0.2(10) = 0.244\nR0.2(11) = 0.000\n .\nThey are numerically identical as expected. Finally we verify that the 3-order statistics are all different from each other and from the 3-order statistics of the original data, R, previously listed. R(000) = 0.348 R(001) = 0.165 R(010) = 0.244 R(011) = 0.000 R(100) = 0.165 R(101) = 0.079 R(110) = 0.000\nR(111) = 0.000\n ,  R\u0302(000) = 0.513 R\u0302(001) = 0.000 R\u0302(010) = 0.244 R\u0302(011) = 0.000 R\u0302(100) = 0.000 R\u0302(101) = 0.244 R\u0302(110) = 0.000\nR\u0302(111) = 0.000\n ,  R0.5(000) = 0.430 R0.5(001) = 0.083 R0.5(010) = 0.244 R0.5(011) = 0.000 R0.5(100) = 0.083 R0.5(101) = 0.161 R0.5(110) = 0.000\nR0.5(111) = 0.000\n ,  R0.2(000) = 0.480 R0.2(001) = 0.033 R0.2(010) = 0.244 R0.2(011) = 0.000 R0.2(100) = 0.033 R0.2(101) = 0.211 R0.2(110) = 0.000\nR0.2(111) = 0.000\n .\nThese 3-order statistics are calculated using the relationships\nR\u0303(ayb) = R(ay) \u00b7 P\u0303 (ay, yb)\nfor the various R\u0303, a, y, b. Moreover, the Ru are the same convex combinations as the the various Pu\u2019s. This example illustrates the various constructions we have described in complete generality in the previous section.\nApril 28, 2011 DRAFT"}, {"heading": "V. A COVERT CHANNEL CODING TECHNIQUE", "text": "In the previous section, we showed that given observed string frequencies, R(z), z \u2208 \u03a3k we can construct multiple Markov Chains, M , whose states are the k-grams (z \u2208 \u03a3k), transition probabilities are P (ay, yb), a \u2208 \u03a3, y \u2208 \u03a3k\u22121 and whose stationary distributions are precisely the observed R.\nWe now show how to use such a Markov Chain to encode messages while preserving the statistics, R, of the channel. This means that someone monitoring the channel will observe the same k-gram statistics in spite of the fact that covert messages can be communicated within that channel. As noted before, this can be exploited by either attacker or defender.\nConceptually, the coding concept is the opposite of the classical Shannon Source Coding Theorem [17] in the sense that traditionally we start with a stochastic source with entropy rate H that we seek to compress into binary strings whereas in this case we start with a collection of 2r messages which we wish to efficiently encode using the dynamics and statistics of the given stochastic process. Because we have to respect the statistics of the channel, the encoding will typically not be be compressing but expanding the number of bits needed. Nonetheless, we still seek efficiency with respect to observing the channel\u2019s k-gram statistics.\nIn this work, we will assume, for simplicity, that the communication covert channel is noiseless noting that the results can be extended to noisy channels in the traditional way. A more thorough analysis is deferred to a future study in which the Shannon capacity of noisy channels will be considered.\nThis construction involves several steps:\n1) Compute the entropy of the irreducible Markov Chain M , HM , specified by transition\nprobabilities, PM , and stationary distribution, RM :\nHM = \u2212 \u2211 ay\u2208\u03a3k RM(ay) \u2211 b\u2208\u03a3 PM(ay, yb) log2(PM(ay, yb)).\nNote that we construct the Markov Chains to have a given stationary distribution, R, so only PM is different for the different models. For the examples developed in the previous section, we have computed:\nHP = 0.6863 , HP\u0302 = 0 , HP0.2 = 0.3165 , HP0.5 = 0.5520.\nNote that P\u0302 is entirely deterministic and so has zero entropy.\nApril 28, 2011 DRAFT\nSince we constructed these Markov Chains so that different transitions from a state correspond to different observables (that is, be DPFA\u2019s), knowledge of the initial state of the Markov Chain results in a one-to-one correspondence between state sequences and observation sequences. Hence the entropy rates of both the Markov Chain state sequences and resulting observation sequences are the same. Let Ys represent the stochastic process of observations produced by the constructed Markov Chain, M , starting in state s \u2208 M . All states in M are recurrent by construction so the entropy rate of each process Ys is the same and equal to HM . 2) Apply the Shannon-MacMillan-Breiman Asymptotic Equipartition Property (AEP) The-\norem [17] to each Ys showing that for large n there are approximately 2nHM typical sequences of length n of Ys and each occurs with probability approximately (1/2)nHM . Consequently, in order to encode 2r covert messages, say Ci with 1 \u2264 i \u2264 2r we must have r \u2264 nHM or equivalently n \u2265 r/HM so n is selected to encode 2r different covert message sequences accordingly. 3) Construct length n typical sequences of Ys by starting in state s and then performing a\nrandom walk of length n in M according to the probabilities PM . Such random walks define observation sequences of length n in \u03a3n. Produce 2r \u2264 2nHM unique sequences for each state s, labeling them as Ys(i) where 1 \u2264 i \u2264 2r \u2264 2nHM . (If a random walk produces a sequence already generated, simply repeat until a novel random walk is produced.) 4) Note that the k-gram frequencies of each z \u2208 \u03a3k within the Ys(i) approach the original\nR(z) as n \u2192 \u221e because R is the stationary distribution of the Markov Chain and Ys(i) is produced by taking a random walk in the chain. 5) For each state, s, assign the covert message Ci to Ys(i). Pick a random initial state s(0)\nand assign a sequences of covert messages Ci1Ci2 ...Cim to\nYs(0)(i1)Ys(1)(i2)...Ys(m\u22121)(im)\nwhere s(j) is recursively defined as the state in which Ys(j\u22121)(ij) ended.\nBecause each random walk in the sequence thus constructed starts in the state in which the previous random walk ended, the concatenated sequence of random walks is also a legal random walk in the Markov Chain, obeying all the transition probabilities. Moreover, the k-gram statistics in the overall concatenated sequence of mn observations is approximately R and approaches R\nApril 28, 2011 DRAFT\nas n\u2192\u221e. The encoded sequence is uniquely decodable by the receiver as well.\nTo illustrate this construction, consider the example presented in the left of Figure 2 where we use the 1-order statistics as in equation (2). We take the convex combination (see Section II)\nP = 0.75 \u00b7 PB + 0.25 \u00b7 PD,\nwhich results in an entropy of HP = 0.004 as computed from (9). We build a (2r, n) codebook as described above with r = 8 and n = dr/HP e = 1995. That is, this encodes binary sequences of length 8 into inter-packet delays of length 1995. We encoded 16 blocks of 8 random source bits each into 16 \u00b71995 = 31920 symbols from the alphabet \u03a3 = {1, 2, . . . , 13} which correspond to the delays in the left graph of Figure 2.\nThe obtained 1-order statistics of the resulting 31920 long concatenated codeword are\nR\u2032 = [0.0029 0.0144 0.0734 0.1453 0.3094 0.1295 0.1151 0.1079 0.1007 0 0 0 0.0014];\nand are depicted in Figure 4. Note the empirical frequencies and graphs are identical to the displayed precision.\nThis illustrates empirically the effectiveness of the construction described in this paper. Matlab\ncode for reproducing these results is available upon request.\nApril 28, 2011 DRAFT"}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "This paper has demonstrated that covert channels can exist even when arbitrarily high order statistics about a channel are estimated and monitored. The resulting covert channels can be used to either exploit or defend the channel and the advantage goes to the party that has the ability to estimate the highest order statistics.\nThe adversarial nature of this situation falls within the scope of cognitive attacks [26], [27]. It can be described in abstract as follows: the environment (for example, inter-packet delays) is modeled as a stochastic process X (such as a Hidden Markov Model, Markov Chain or other formalism). Both the attacker, A, and the defender, D, monitor the environment through functions fA \u2208 F and fD \u2208 F respectively (for example, fD(X ) could be the probability distribution of k-grams produced by X ).\nThe attacker guesses fD and manipulates X in order to produce a new process, A(X ), so that covert communications can be performed while respecting the behavior that the defender expects; namely, fD(X ) = fD(A(X )).\nOn the other hand, the defender, by anticipating the attacker\u2019s guess of fD, picks a different\nf\u0303D and manipulates X to produce a new process D(X ) so that:\n1) fD(A(D(X ))) = fD(D(X )) = fD(X ) = fD(A(X )): the defensive shaping action is\nimperceptible to the defender who uses fD;\n2) f\u0303D(A(D(X ))) 6= f\u0303D(D(X )): the attacker\u2019s action (that is, creation of a covert channel) is\ndetectable by the defender.\nThe game consists of attacker and defender guessing and then exploiting each other\u2019s monitoring strategy and manipulating the environment accordingly. The common objective of the players is to alter the environment in a manner that would be imperceptible to the opponent in order to perform a secret task (covert communication or covert channel detection).\nThis work raises some questions which are deferred to future work. In particular, the following\ndirections are worthy of future investigation:\n\u2022 Inter-packet delays involve real-world time so the question of stability when shaping the\nchannel must be considered. That is, packets can be delayed by certain times only if there are packets in the queue to be delayed. Discussions of such queuing aspects of timing channels and the possibility of jamming them have been studied [28]. Relating this work\nApril 28, 2011 DRAFT\nto timing channel jamming will be investigated.\n\u2022 We used a circular buffer in Section IV to numerically estimate k-gram statistics so that the\nstatistics have the required marginalization properties. A single pass, online algorithm for implementing this circular buffer only requires storing the first and last k symbols of the data. In the absence of such a buffer, the empirical statistics will not in general obey the marginalization identities and so some additional processing would be required. The use of singular value decompositions, non-negative matrix factorizations or other decomposition methods for imposing the regularity might be worth exploring further as alternatives to the circular buffer approach. \u2022 In principle, one can attempt to build automata smaller than the Markov Chains we construct.\nIn particular, Probabilistic Finite Automata (PFA) [29], [30], [31] could implement Markov Chains based on k-grams but using fewer states. Unlike k-gram based Markov Chains, kPSAs have states that are labeled with input sequences of length at most k. So they can be seen as \u201cvariable length\u201d k-gram Markov Chains. They can be learned efficiently in the KL-PAC sense [32], [33], [34] and are generally smaller than k-gram based Markov chains (by having fewer states). \u2022 Within the space of possible Markov Chains that realize given k-gram statistics, it would\nbe good to select the \u201cbest\u201d chain from the point of maximizing entropy so that the covert channel coding is as efficient as possible. Our experiments suggest that the so-called Standard Solutions presented in Section III-A have the largest entropy although we have not been able to prove that analytically. \u2022 It is reasonable to ask how our results relate to the use of Hidden Markov Models for\nmodeling traffic, as for example in [3]. It is known that a Hidden Markov Model with n states is completely determined by the 2n-grams produced by the model so that reproducing 2n-gram statistics will result in the same n state Hidden Markov Model [8]."}], "references": [{"title": "Detection of covert channel encoding in network packet delays", "author": ["V. Berk", "A. Giani", "G. Cybenko"], "venue": "Proc. of FloCon 2005 Pittsburgh, PA, 2005.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "IP covert timing channels: design and detection", "author": ["S. Cabuk", "C. Brodley", "C. Shields"], "venue": "Proceedings of the 11th ACM Conference on Computer and Communications Security, 2004. April 28, 2011  DRAFT  CRESPI, CYBENKO, GIANI  22", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Internet traffic modeling by means of Hidden Markov Models", "author": ["A. Dainotti", "A. Pescap\u00e9", "P.S. Rossi", "F. Palmieri", "G. Ventre"], "venue": "Computer Networks, vol. 52, pp. 2645\u20132662, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Hidden Markov Processes", "author": ["Y. Ephraim"], "venue": "IEEE Transactions on Information Theory, vol. 48, no. 6, pp. 1518\u20131569, 2002.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "A tutorial on Hidden Markov Models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceeding of the IEEE, vol. 77, no. 2, pp. 257\u2013286, 1989.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1989}, {"title": "Approximate Nonnegative Matrix Factorization via alternating minimization", "author": ["L. Finesso", "P. Spreij"], "venue": "Proceedings of Mathematical Theory of Networks and Systems, Leuven, Belgium, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Nonnegative Matrix Factorixation and I-divergence alternating minimization", "author": ["\u2014\u2014"], "venue": "Linear Algebra and its Applications, vol. 416, pp. 270\u2013287, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning Hidden Markov Models using Nonnegative Matrix Factorization", "author": ["G. Cybenko", "V. Crespi"], "venue": "IEEE Transactions on Information Theory, 2011, to appear. [Online]. Available: http://arxiv.org/abs/0809.4086", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Anagram: a content anomaly detector resistant to mimicry attack", "author": ["K. Wang", "J.J. Parekh", "S.J. Stolfo"], "venue": "Springer Lecture Notes in Computer Science, Recent Advances in Intrusion Detection, vol. 4219, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "On the limits of steganography", "author": ["R. Anderson", "F. Petitcolas"], "venue": "Selected Areas in Communications, IEEE Journal on, vol. 16, no. 4, pp. 474 \u2013481, May 1998.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Shared Resource Matrix Methodology: an approach to identifying storage and timing channel", "author": ["R. Kemmerer"], "venue": "ACM Transactions on Computer Systems, vol. 1, no. 3, pp. 256\u2013277, August 1983.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1983}, {"title": "New constructive approach to covert channel modeling and channel capacity estimation", "author": ["Z. Wang", "R.B. Lee"], "venue": "Information Security, ser. Lecture Notes in Computer Science, J. Zhou, J. Lopez, R. H. Deng, and F. Bao, Eds. Springer Berlin / Heidelberg, 2005, vol. 3650, pp. 498\u2013505.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Problems of modeling in the analysis of covert channels", "author": ["A. Grusho", "N. Grusho", "E. Timonina"], "venue": "Computer Network Security, ser. Lecture Notes in Computer Science, I. Kotenko and V. Skormin, Eds. Springer Berlin / Heidelberg, 2010, vol. 6258, pp. 118\u2013124.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Design, implementation and evaluation of covert channel attacks", "author": ["H. Okhravi", "S. Bak", "S.T. King"], "venue": "HST \u201910: Proceedings of IEEE Conference on Technologies for Homeland Security, Oct 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Covert channels\u2013here to stay?", "author": ["I. Moskowitz", "M. Kang"], "venue": "Computer Assurance,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1994}, {"title": "Open problems in the security of learning", "author": ["M. Barreno", "P.L. Bartlett", "F.J. Chi", "A.D. Joseph", "B. Nelson", "B.I.P. Rubinstein", "U. Saini", "J.D. Tygar"], "venue": "Conference on Computer and Communications Security, Proceedings of the 1st ACM Workshop on Workshop on AISec. Alexandria, Virginia, USA: Assocation for Computing Machinery, 2008, pp. 19\u201326.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic Finite State Machines \u2013 Part I", "author": ["E. Vidal", "F. Thollard", "C. de la Higuera", "F. Casacuberta", "R. Carrasco"], "venue": "PAMI, vol. 27, no. 7, pp. 1013\u20131025, July 2005.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Probabilistic Finite State Machines \u2013 Part II", "author": ["\u2014\u2014"], "venue": "PAMI, vol. 27, no. 7, pp. 1026\u20131039, July 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning deterministic linear languages", "author": ["C. De La Higuera", "J. Oncina"], "venue": "Lecture Notes in Computer Science - Lecture Notes in Artificial Intelligence, vol. 2375, pp. 185\u2013200, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning stochastic finite automata", "author": ["C. de la Higuera", "J. Oncina"], "venue": "0302-9743 - Lecture Notes in Computer Science - Lecture Notes in Artificial Intelligence, vol. 3264, no. 3264, pp. 175\u2013186, 2004. April 28, 2011  DRAFT  CRESPI, CYBENKO, GIANI  23", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Embedding covert channels into TCP/IP", "author": ["S.J. Murdoch", "S. Lewis"], "venue": "7th Information Hiding Workshop, Barcelona, Catalonia (Spain), June 2005.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Data exfiltration and covert channels", "author": ["A. Giani", "V.H. Berk", "G. Cybenko"], "venue": "Proceedings of the SPIE Vol. 6201, Sensors, and Command, Control, Communications, and Intelligence (C3I) Technologies for Homeland Security and Homeland Defense IV Orlando, Florida, April 2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Statistical methods for speech recognition", "author": ["F. Jelinek"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "Combinatorial optimization : algorithms and complexity", "author": ["C.H. Papadimitriou", "K. Steiglitz"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Cognitive Hacking", "author": ["G. Cybenko", "A. Giani", "P. Thompson"], "venue": "Advances in Computers, vol. 60, pp. 36\u201375, 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Cognitive Hacking: a battle for the mind", "author": ["\u2014\u2014"], "venue": "IEEE Computer, vol. 35, no. 8, pp. 50\u201356, 2002.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "An information\u2013theoretic and game\u2013theoretic study of timing channels", "author": ["J. Giles", "B. Hajek"], "venue": "Information Theory, IEEE Transactions on, vol. 48, no. 9, pp. 2455\u20132477, sep 2002.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "The power of amnesia", "author": ["D. Ron", "Y. Singer", "N. Tishby"], "venue": "Advances in Neural Information Processing Systems, vol. 6, 1993.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning probabilistic automata with variable memory length", "author": ["\u2014\u2014"], "venue": "Proceedings of the Workshop on Computational Learning Theory, 1994.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1994}, {"title": "PAC-learnability of probabilistic deterministic finite state automata in terms of variation distance", "author": ["N. Palmer", "P.W. Goldberg"], "venue": "Theor. Comput. Sci., vol. 387, no. 1, pp. 18\u201331, 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "PAC-learnability of probabilistic deterministic finite state automata", "author": ["A. Clark", "F. Thollard"], "venue": "Journal of Machine Learning Research, vol. 5, pp. 437\u2013497, 2004.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "On the computational complexity of approximating distributions by probabilistic automata", "author": ["N. Abe", "M. Warmuth"], "venue": "Machine Learning, vol. 9, pp. 205\u2013260, 1992.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 0, "context": "[1] and Cabuk et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] estimate certain first order statistics of packet inter-arrival delays in order to determine whether a time covert channel is being used.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "[3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "[3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies.", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "[3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies.", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "[3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies.", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "Other techniques are based on various analyses of n-gram statistics [9].", "startOffset": 68, "endOffset": 71}, {"referenceID": 8, "context": "In fact, some have called techniques that match n-gram statistics \u201cmimicry attacks\u201d and while techniques have been developed for detecting certain simple types of mimicry, techniques for building mimicry attacks as described in the present paper appear to be novel [9].", "startOffset": 265, "endOffset": 268}, {"referenceID": 9, "context": "General discussions of covert channels and their taxonomies, existence and modeling have been published [10], [11], [12], [13], [14].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "General discussions of covert channels and their taxonomies, existence and modeling have been published [10], [11], [12], [13], [14].", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "General discussions of covert channels and their taxonomies, existence and modeling have been published [10], [11], [12], [13], [14].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "General discussions of covert channels and their taxonomies, existence and modeling have been published [10], [11], [12], [13], [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 13, "context": "General discussions of covert channels and their taxonomies, existence and modeling have been published [10], [11], [12], [13], [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "The design, implementation and experimental evaluation of several specific covert channel attacks in real systems is of specific interest [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 9, "context": "It is important to note that researchers often talk about entropy as a channel statistic [10], [15] but entropy is typically", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "It is important to note that researchers often talk about entropy as a channel statistic [10], [15] but entropy is typically", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "Researchers have recently started to develop systematic taxonomies and examples of attacks against statistical machine learning techniques [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "After estimating the k-order statistics, we build a Probabilistic Deterministic Finite Automata model (PDFA) [18], [19], [20], [21] that realizes the k-order statistics.", "startOffset": 109, "endOffset": 113}, {"referenceID": 17, "context": "After estimating the k-order statistics, we build a Probabilistic Deterministic Finite Automata model (PDFA) [18], [19], [20], [21] that realizes the k-order statistics.", "startOffset": 115, "endOffset": 119}, {"referenceID": 18, "context": "After estimating the k-order statistics, we build a Probabilistic Deterministic Finite Automata model (PDFA) [18], [19], [20], [21] that realizes the k-order statistics.", "startOffset": 121, "endOffset": 125}, {"referenceID": 19, "context": "After estimating the k-order statistics, we build a Probabilistic Deterministic Finite Automata model (PDFA) [18], [19], [20], [21] that realizes the k-order statistics.", "startOffset": 127, "endOffset": 131}, {"referenceID": 20, "context": "Several such fields are known to exist [22].", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "The following discussion shows how a timing covert channel can be constructed based on a beacon and argues that a naive encoding of covert messages based on packet inter-arrival times produces a clearly detectable distortion of the 1-order statistics of those time intervals in network traffic [23], [1].", "startOffset": 294, "endOffset": 298}, {"referenceID": 0, "context": "The following discussion shows how a timing covert channel can be constructed based on a beacon and argues that a naive encoding of covert messages based on packet inter-arrival times produces a clearly detectable distortion of the 1-order statistics of those time intervals in network traffic [23], [1].", "startOffset": 300, "endOffset": 303}, {"referenceID": 0, "context": "Another, equivalent way to derive these relations is to note that there are two trivial solutions for X , namely X1 = I2 (the 2 by 2 identity matrix) and X2 = 1 \u00b7 \u03c0 where 1 = [1 1] is the column vector whose entries are all 1\u2019s.", "startOffset": 175, "endOffset": 180}, {"referenceID": 0, "context": "Another, equivalent way to derive these relations is to note that there are two trivial solutions for X , namely X1 = I2 (the 2 by 2 identity matrix) and X2 = 1 \u00b7 \u03c0 where 1 = [1 1] is the column vector whose entries are all 1\u2019s.", "startOffset": 175, "endOffset": 180}, {"referenceID": 17, "context": "Such models are called k-order Markov Models, k Markov Chains or k-gram models by different authors [19], [24].", "startOffset": 100, "endOffset": 104}, {"referenceID": 22, "context": "Such models are called k-order Markov Models, k Markov Chains or k-gram models by different authors [19], [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "From the theory of linear programming, there are feasible solutions to the linear program defined by (6), (7) and (8) which have no more than pre(y) + post(y) \u2212 1 nonzero coordinates, namely the Basic Feasible Solutions [25].", "startOffset": 220, "endOffset": 224}, {"referenceID": 24, "context": "The adversarial nature of this situation falls within the scope of cognitive attacks [26], [27].", "startOffset": 85, "endOffset": 89}, {"referenceID": 25, "context": "The adversarial nature of this situation falls within the scope of cognitive attacks [26], [27].", "startOffset": 91, "endOffset": 95}, {"referenceID": 26, "context": "Discussions of such queuing aspects of timing channels and the possibility of jamming them have been studied [28].", "startOffset": 109, "endOffset": 113}, {"referenceID": 27, "context": "In particular, Probabilistic Finite Automata (PFA) [29], [30], [31] could implement Markov Chains based on k-grams but using fewer states.", "startOffset": 51, "endOffset": 55}, {"referenceID": 28, "context": "In particular, Probabilistic Finite Automata (PFA) [29], [30], [31] could implement Markov Chains based on k-grams but using fewer states.", "startOffset": 57, "endOffset": 61}, {"referenceID": 29, "context": "In particular, Probabilistic Finite Automata (PFA) [29], [30], [31] could implement Markov Chains based on k-grams but using fewer states.", "startOffset": 63, "endOffset": 67}, {"referenceID": 30, "context": "They can be learned efficiently in the KL-PAC sense [32], [33], [34] and are generally smaller than k-gram based Markov chains (by having fewer states).", "startOffset": 52, "endOffset": 56}, {"referenceID": 31, "context": "They can be learned efficiently in the KL-PAC sense [32], [33], [34] and are generally smaller than k-gram based Markov chains (by having fewer states).", "startOffset": 58, "endOffset": 62}, {"referenceID": 2, "context": "\u2022 It is reasonable to ask how our results relate to the use of Hidden Markov Models for modeling traffic, as for example in [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "It is known that a Hidden Markov Model with n states is completely determined by the 2n-grams produced by the model so that reproducing 2n-gram statistics will result in the same n state Hidden Markov Model [8].", "startOffset": 207, "endOffset": 210}], "year": 2011, "abstractText": "In this paper we present methods for attacking and defending k-gram statistical analysis techniques that are used, for example, in network traffic analysis and covert channel detection. The main new result is our demonstration of how to use a behavior\u2019s or process\u2019 k-order statistics to build a stochastic process that has those same k-order stationary statistics but possesses different, deliberately designed, (k + 1)order statistics if desired. Such a model realizes a \u201ccomplexification\u201d of the process or behavior which a defender can use to monitor whether an attacker is shaping the behavior. By deliberately introducing designed (k + 1)-order behaviors, the defender can check to see if those behaviors are present in the data. We also develop constructs for source codes that respect the k-order statistics of a process while encoding covert information. One fundamental consequence of these results is that certain types of behavior analyses techniques come down to an arms race in the sense that the advantage goes to the party that has more computing resources applied to the problem. Points of view in this document are those of the authors and do not necessarily represent the official position of the sponsoring agencies or the U.S. Government. V. Crespi is with the Department of Computer Science, California State University at Los Angeles, Los Angeles CA, 90032 USA. email: vcrespi@calstatela.edu. Crespi\u2019s work was partially supported by AFOSR Grant FA9550-07-1-0421 and by NSF Grant HRD-0932421. G. Cybenko is with the Thayer School of Engineering, Dartmouth College, Hanover NH 03755. email: gvc@dartmouth.edu. Cybenko\u2019s work was partially supported by Air Force Research Laboratory contracts FA8750-10-1-0045, FA8750-09-1-0174, AFOSR contract FA9550-07-1-0421, U.S. Department of Homeland Security Grant 2006-CS-001-000001 and DARPA Contract HR001-06-1-0033 A. Giani is with the Department of EECS, University of California at Berkeley, Berkeley CA 94720. email: agiani@eecs.berkeley.edu. Giani\u2019s\u2019s work was partially supported by U.S. Department of Homeland Security Grant 2006-CS001-000001 and DARPA Contract HR001-06-1-0033 when she was a Ph.D. student at Dartmouth April 28, 2011 DRAFT ar X iv :1 10 4. 50 71 v1 [ cs .L G ] 2 7 A pr 2 01 1 CRESPI, CYBENKO, GIANI 2", "creator": "LaTeX with hyperref package"}}}