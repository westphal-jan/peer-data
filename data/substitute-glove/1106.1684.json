{"id": "1106.1684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2011", "title": "Max-Margin Stacking and Sparse Regularization for Linear Classifier Combination and Selection", "abstract": "The consists principle the cubicles generalization (or Stacking) is form example second - level generalizer with salt itself template is artillery classifiers in an ensemble. In this paper, we documents among combination vary under the stacking framework; established weighted increment (WS ), addition - particular percentages revenue (CWS) for formula_2 lacquered generalization (LSG ). For studies put bending, realize requiring allowing demised calculation danger minimization three following conceivably loss. In addition, we propose well parent sparsity for riemann to ensure codice_40 selection. We originally studies using up depending jazz setups another preference heterogeneity new (it - world infographics. Results there then which another regularized studies few while rests loss determines. Using sparsely spinneret, we people might means reduce still similar well representatives fungicides of several diverse ensemble without abiding durability. With given most - create conductors, we even value reliability when average other make sparse four-dimensional.", "histories": [["v1", "Wed, 8 Jun 2011 23:03:47 GMT  (277kb,D)", "http://arxiv.org/abs/1106.1684v1", "8 pages, 3 figures, 6 tables, journal"]], "COMMENTS": "8 pages, 3 figures, 6 tables, journal", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mehmet umut sen", "hakan erdogan"], "accepted": false, "id": "1106.1684"}, "pdf": {"name": "1106.1684.pdf", "metadata": {"source": "CRF", "title": "Max-Margin Stacking and Sparse Regularization for Linear Classifier Combination and Selection", "authors": ["Mehmet Umut Sen", "Hakan Erdogan"], "emails": ["umutsen@sabanciuniv.edu,", "haerdogan@sabanciuniv.edu."], "sections": [{"heading": null, "text": "Index Terms\u2014classifier combination, classifier selection, regularized empirical risk minimization, hinge loss, group sparsity"}, {"heading": "1 INTRODUCTION", "text": "Classifier ensembles aim to increase efficiency of classifier systems in terms of accuracy at the expense of increased complexity and they are shown to obtain greater performance than single-expert systems for a broad range of applications. Among all theoretical and practical reasons to prefer using ensembles, which are categorized as statistical, computational and representational in [1], the most important ones are the statistical reasons. Since we are looking for the generalization performance (error in the test data) in pattern recognition problems, it is often very difficult to find the \u201cperfect classifier\u201d, but by combining multiple classifiers probability of getting closer to the perfect classifier is increased. An ensemble may not always beat the performance of the best single classifier obtained, but it will surely decrease the variance of the classification error. Some other reasons besides statistical reasons can be found in [1], [2].\nThe straightforward method to obtain an ensemble is using different classifier types or different parameters. Also training base classifiers with different subsets or samplings of data or features is used to obtain ensembles which will result in more diverse ensembles. There\nThe authors are with Sabanci University, Istanbul Turkey. Email addresses: umutsen@sabanciuniv.edu, haerdogan@sabanciuniv.edu.\nare different measures of diversity of an ensemble, but diversity simply means that base classifiers make errors on different examples. Diverse ensembles result in better performance with a reasonable combiner. In this work, we are not interested in the methods of obtaining the ensemble, but we investigate various linear combination types for a given set of base classifiers.\nBase classifiers produce either label outputs or continuous valued outputs. For the former, combiners like majority voting or weighted majority voting are used. In the latter case, base classifiers produce continuous scores for each class that represent the degree of support for each class. They can be interpreted as confidences in the suggested labels or estimates of the posterior probabilities for the classes [3]. Former thinking is more reasonable since for most of the classifier types, support values may not be very close to the actual posterior probabilities even if the data is dense, because classifiers generally do not try to estimate the posterior probabilities, but try to classify the data instances correctly so they usually only try to force the true class\u2019 score to be the maximum. In this paper, we deal with the combination of continuous valued outputs.\nCombination rules can be grouped into trainable vs. non-trainable (or supervised vs. unsupervised). Simple average (mean), product, trimmed mean, minimum, maximum and median rules are some examples to nontrainable combiners. Learning the combiner from training data is shown to give better accuracy than nontrainable combiners. Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]. The idea of Stacking is to use the confidence scores that are obtained from base classifiers as attributes in a new training set with the original class labels and training a meta-classifier (This classifier is called level-1 generalizer in [4]) with this new dataset. Considering the speed and complexity advantage of linear meta-classifiers over nonlinear ones, they are usually preferred in the literature. When initially introduced, stacking is used to combine ar X\niv :1\n10 6.\n16 84\nv1 [\ncs .L\nG ]\n8 J\nun 2\n01 1\n2 the class predictions of the base classifiers [4]. Ting & Witten used confidence scores of base classifiers as input features and improved stacking\u2019s performance [6], [8]. Merz used stacking and correspondence analysis to model the relationship between the learning examples and their classification by a collection of learned models and used nearest neighbor classifier as meta learner. Dzeroski & Zenko used multi-response model trees as the meta-learner [11]. Seewald introduced StackingC, which improves Stacking\u2019s performance further and reduces the computational cost by introducing class-conscious combination. [9]. Sill, incorporated meta-features with the posterior scores of base classifiers to improve accuracy [12]. Ledezma, used genetic algorithms to search for good Stacking configurations [15]. Tang, re-ranked all possible class labels according to the scores and obtained a learner which outperforms all base classifiers [16].\nSince training the base classifiers and the combiner with the same data samples will result in overfitting, a sophisticated cross-validation is applied to obtain the training data of the combiner (level-1 data). This procedure, called internal cross-validation, is described in section 2. After obtaining level-1 data, there are two main problems remaining for a linear combination: (1.) Which type of combination method should be used? (2.) Given a combination type, how should we learn the parameters of the combiner? For the former problem, Ueda [17] defined three linear combination types namely type-1, type-2 and type-3. In this paper, we use the descriptive names weighted sum (WS), class-dependent weighted sum (CWS) and linear stacked generalization (LSG) for these types of combinations respectively and investigate all of them. In [7], [8], LSG is used and CWS combination is proposed in [6]. For the latter problem, Ting & Witten proposed a multi-response linear regression algorithm for learning the weights [6]. Ueda in [17] proposed using minimum classification error (MCE) criterion for estimating optimal weights, which increased the accuracies. MCE criterion is an approximation to the zero-one loss function which is not convex, so finding a global optimizer is not always possible. Ueda derived algorithms for different types of combinations with MCE loss using stochastic gradient methods. Both of these studies ignored \u201cregularization\u201d which has a huge effect on the performance, especially if the number of base classifiers is large. Reid & Grudic in [13] regularized the standard linear least squares estimation of the weights with CWS and improved the performance of stacking. They applied l2 norm penalty, l1 norm penalty and combination of the two (elastic net regression). In this work, we propose maximum margin algorithms for learning the optimal weights. We work with the regularized empirical risk minimization framework [18] and use the hinge loss function with l2 regularization, which corresponds to the support vector machines (SVM). We do not derive algorithms for the solutions of the minimization problems, but state-of-the-art solutions of SVM in the literature can be modified for our problem.\nAnother issue, recently addressed in [19], is combination with a sparse weight vector so that we do not use all of the ensemble. Since we do not have to use classifiers which have zero weight on the test phase, overall test time will be much less. Zhang formulated this problem as a linear programming problem for only the WS combination type [19]. Reid used l1 norm regularization for CWS combination [13]. In this paper, we investigate sparsity issues for all three combination types: WS, CWS and LSG. We use both l1 norm and l1 \u2212 l2 norm for regularization in the objective function for CWS and LSG. Latter regularization results in group sparsity, which is deeply investigated and successfully applied to various problems recently.\nThroughout the paper, we used m for the classifier subscript, n for the class subscript, i for the data instance subscript, M , N and I for the number of classifiers, classes and data instances respectively. Datapoint subscript i is sometimes dropped for simplicity. In section 2 we explain the cross-validation technique used in stacked generalization. In section 3, we define the classifier combination problem formally and define three different combination types used in the literature, namely WS, CWS and LSG. In section 4, we explain how the weights are learned using regularized empirical risk minimization framework with hinge loss and a regularization function. In section 5, we define sparse regularization functions to enforce classifier selection. In section 6, we describe the experiment setups we build. In section 7, we show the results of our experiments and discuss them."}, {"heading": "2 STACKED GENERALIZATION", "text": "A novel approach has been introduced in 1992 known as stacked generalization or stacking [4]. The basic idea is applying a meta-level (or level-1) generalizer to the outputs of base classifiers (or level-0 classifiers). For training the level-1 classifier, we need the confidence scores (Level-1 Data) of the training data, but training the combiner with the same data instances which are used for training the base classifiers will lead to overfitting the database and eventually result in poor generalization performance. Stacking deals with this problem by a sophisticated cross-validation method (internal CV), in which training data is divided into k parts and each part of the data is tested with the base classifiers that are trained with the other k \u2212 1 parts of data. So at the end, each training instance\u2019s score is obtained from the base classifiers whose training data does not contain that particular instance. This procedure is repeated for each base classifier in the ensemble. We apply this procedure for the three different linear combination types. Wolpert combined only the class predictions with this framework, Ting & Witten improved the performance of stacking by combining continuous valued predictions [6].\n3"}, {"heading": "3 COMBINATION TYPES", "text": ""}, {"heading": "3.1 Problem Formulation", "text": "In the classifier combination problem, input to the combiner are the posterior scores belonging to different classes obtained from the base classifiers. Let pnm be the posterior score of class n obtained from classifier m for any data instance. Let pm = [p 1 m, p 2 m, . . . , p N m]\nT , then the input to the combiner is f = [pT1 ,p T 2 , . . . ,p T M ]\nT , where N is the number of classes and M is the number of classifiers. Outputs of the combiner are N different scores representing the degree of support for each class. Let rn be the combined score of class n and let r = [r1, . . . , rN ]T ; then in general the combiner is defined as a function g : RMN \u2192 RN such that r = g(f). Let I be the number of training data instances, fi contain the scores for training data point i obtained from base classifiers with stacking and yi be the corresponding class label; then our aim is to learn the g function using data {(fi, yi)}Ii=1. On the test phase, label of a data instance is assigned as follows:\ny\u0302 = argmax n\u2208[N ]\nrn, (1)\nwhere [N ] = {1, . . . , N}. Among combination types, linear ones are shown to be powerful for the classifier combination problem. For linear combiners, g function has the following form:\ng(f) = Wf + b. (2)\nIn this case, we aim to learn the elements of W \u2208 RN\u00d7MN and b \u2208 RN . So, the number of parameters to be learned is MN2+N . This type of combination is the most general form of linear combiners and called type-3 combination in [17]. In the framework of stacking, we call it linear stacked generalization (LSG) combination. One disadvantage of this type of combination is that, since the number of parameters is high, learning the combiner takes a lot of time and may require a large amount of training data. To overcome this disadvantage, simpler but still strong combiner types are introduced with the help of the knowledge that pnm is the posterior score of class n. We call these methods weighted sum (WS) rule and class-dependent weighted sum (CWS) rule. These types are categorized as class-conscious combinations in [3]."}, {"heading": "3.2 Linear Combination Types", "text": "In this section, we describe and analyze three combination types, namely weighted sum rule (WS), class-dependent weighted sum rule (CWS) and linear stacked generalization (LSG) where LSG is already defined in (2)."}, {"heading": "3.2.1 Weighted Sum Rule", "text": "In this type of combination, each classifier is given a weight, so there are totally M different weights. Let um\nbe the weight of classifier m, then the final score of class n is estimated as follows:\nrn = M\u2211 m=1 ump n m = u T fn , n = 1, . . . , N, (3)\nwhere fn contains the scores of class n: fn = [pn1 , . . . , p n M ]\nT and u = [u1, . . . , uM ]T . For the framework given in (2), WS combination can be obtained by letting b = 0 and W to be the concatenation of constant diagonal matrices:\nW = [u1IN | . . . |uM IN ], (4)\nwhere IN is the N \u00d7 N identity matrix. We expect to obtain higher weights for stronger base classifiers after learning the weights from the database."}, {"heading": "3.2.2 Class-Dependent Weighted Sum Rule", "text": "The performances of base classifiers may differ for different classes and it may be better to use a different weight distribution for each class. We call this type of combination CWS rule. Let vnm be the weight of classifier m for class n, then the final score of class n is estimated as follows:\nrn = M\u2211 m=1 vnmp n m = v T n f n , n = 1, . . . , N, (5)\nwhere vn = [vn1 , . . . , vnM ] T . There are MN parameters in a CWS combiner. For the framework given in (2), CWS combination can be obtained by letting b = 0 and W to be the concatenation of diagonal matrices; but unlike in WS, diagonals are not constant:\nW = [W1|W2| . . . |WM ], (6)\nwhere Wm \u2208 RN\u00d7N are diagonal for m = 1, . . . ,M ."}, {"heading": "3.2.3 Linear Stacked Generalization", "text": "This type of combination is the most general form of supervised linear combinations and is already defined in (2). With LSG, score of class n is estimated as follows:\nrn = wTn f + bn , n = 1, . . . , N, (7)\nwhere wn \u2208 RMN is the nth row of W and bn is the nth element of b. LSG can be interpreted as feeding the base classifiers\u2019 outputs to a linear multi-class classifier as a new set of features. This type of combination may result in overfitting to the database and may give lower accuracy then WS and CWS combination when there is not enough data. From this point of view, WS and CWS combination can be treated as regularized versions of LSG. A crucial disadvantage of LSG is that the number of parameters to be learned is MN2+N which will result in a long training period.\nThere is not a single superior one among these three combination types since results are shown to be data dependent [20]. A convenient way of choosing the combination type is selecting the one that gives the best performance in cross-validation.\n4"}, {"heading": "4 LEARNING THE COMBINER", "text": "We use the regularized empirical risk minimization (RERM) framework [18] for learning the weights. In this framework, learning is formulated as an unconstrained minimization problem and the objective function consists of a summation of empirical risk function over data instances and a regularization function. Empirical risk is obtained as a sum of \u201closs\u201d values obtained from each sample. Different choices of loss functions and regularization functions correspond to different classifiers. Using hinge loss function with l2 norm regularization is equivalent to support vector machines (SVM). It has been shown in studies that the hinge loss function yields much better classification performance as compared to the least-squares(LS) loss function in general. Earlier classifier combination literature uses LS loss function [6], [8], [13], which is suboptimal as compared to the hinge loss that we promote and use in this paper. Using leastsquares with l2 regularization is equivalent to applying least-squares support vector machine (LS-SVM) [21]. We use an adaptation of SVM in multiclass problems defined in [22]. With this adaptation, we find the linear separating hyper-plane that maximizes the margin between true class and the most offending wrong class. For LSG, we have the following objective function:\n\u03c6LSG(W,b) = 1\nI I\u2211 i=1 (1\u2212 ryii +max n 6=yi rni )+ + \u03bbRLSG(W),\n(8) where RLSG(W) is the regularization function, (x)+ = max(0, x) and the posterior score of data instance i for class n, rni , is given as follows:\nrni = w T n fi + bn. (9)\n\u03bb \u2208 R in (8) is the regularization parameter which is usually learned by cross validation. Objective function given in (8) encourages the distance between the true class\u2019 score and the most offending wrong class\u2019 score to be larger than one. A conventional regularization function is the Frobenius norm of W:\nRLSG(W) = ||W||F = N\u2211\nn=1\n||wn||22, (10)\nEquation (8) is given for LSG but it can be modified for other types of combinations using the unifying framework described in [20]. But we also give objective functions for WS and CWS explicitly. Objective function of WS is as follows:\n\u03c6WS(u) = 1\nI I\u2211 i=1 (1\u2212 uT fyii +max n 6=yi (uT fni ))+ + \u03bbRWS(u).\n(11) For regularization, we use l2 norm of u: RWS = ||u||2. For CWS, we have the following objective function:\n\u03c6CWS(V) = 1\nI I\u2211 i=1 (1\u2212 vTyif yi i +max n 6=yi (vTn f n i ))++\u03bbRCWS(V),\n(12)\nwhere V \u2208 RM\u00d7N contains the weights for different classes: V = [v1, . . . ,vN ]. As for LSG, conventional regularization function for CWS is the Frobenious norm of V: RCWS(V) = ||V||F ."}, {"heading": "5 SPARSE REGULARIZATION", "text": "In this section, we define a set of regularization functions for enforcing sparsity on the weights so that the resulting combiner will not use all the base classifiers leading to a shorter test time. This method can be seen as a classifier selection algorithm, but here classifiers are selected automatically and we cannot determine the number of selected classifiers beforehand. But we can lower this number by increasing the weight of the regularization function (\u03bb), and vice versa. With sparse regularization, \u03bb has two main effects on the resulting combiner. First, it will determine how much the combiner should fit the data. Decreasing \u03bb results in more fitting the training data and decreasing it too much results in overfitting, on the other hand, increasing it too much prevents the combiner to learn from the data and the accuracy drops dramatically. Second, as mentioned before, it will determine the number of selected classifiers. As \u03bb increases, the number of selected classifiers decreases."}, {"heading": "5.1 Regularization with the l1 Norm", "text": "The most successful approach for inducing sparsity is using the l1 norm of the weight vector for WS. For CWS and LSG, in which the combiner consists of matrices, we can concatenate the weights in a vector and take the l1 norm or equivalently we can take the l1 \u2212 l1 norm of the weight matrices. We have the following sparse regularization functions for WS, CWS and LSG respectively:\nRWS(u) = ||u||1, (13)\nRCWS(V) = ||V||1,1 = N\u2211\nn=1\n||vn||1, (14)\nRLSG(W) = ||W||1,1 = N\u2211\nn=1\n||wn||1. (15)\nIf all weights of a classifier are zero, that classifier will be eliminated and we do not have to use that base classifier for a test instance, so that testing will be faster. But the problem with l1-norm regularizations for CWS and LSG is that we are not able to use all the information from a selected base classifier, because a classifier may receive both zero and non-zero weights. To overcome this problem, we propose to use group sparsity, as explained in the next section.\n5"}, {"heading": "5.2 Regularization with Group Sparsity", "text": "We define another set of regularization functions which are embedded by group sparsity for LSG and CWS to enforce classifier selection. The main principle of the group sparsity is enforcing all elements that belong to a group to be zero altogether. Grouping of the elements are done before learning. In classifier combination, posterior scores obtained from each base classifier form a group. The following regularization function yields group sparsity for LSG:\nRLSG(W) = M\u2211\nm=1\n||Wm||F . (16)\nFor CWS, we use the following regularization:\nRCWS(V) = ||V||1,2 = M\u2211\nm=1\n||vm||2, (17)\nwhere vm is the mth row of V, so it contains the weights of the classifier m. After the learning process, the elements of vm for any m are either all zero or all non-zero. This leads to better performance than l1 regularization for automatic classifier selection, as we show in section 7. In the next section, we describe the setup of the experiments."}, {"heading": "6 EXPERIMENTAL SETUPS", "text": "We have performed extensive experiments in eight realworld datasets from the UCI repository [23]. For a summary of the characteristics of the datasets, see Table 1. In order to obtain statistically significant results, we applied 5x2 cross-validation [24] which is based on 5 iterations of 2-fold cross-validation (CV). In this method, for each CV, data is randomly split into two stacks as training and testing resulting in overall 10 stacks for each database.\nWe constructed two ensembles which differ in their diversity. In the first ensemble, we construct 10 different subsets randomly which contains 80% of the original data. Then, 13 different classifiers are trained with each subset resulting in a total of 130 base classifiers. We used PR-Tools [25] and Libsvm toolbox [26] for obtaining the base classifiers. These 13 different classifiers are: normal densities based linear classifier, normal densities based quadratic classifier, nearest mean classifier, knearest neighbor classifier, polynomial classifier, general kernel/dissimilarity based classification, normal densities based classifier with independent features, parzen classifier, binary decision tree classifier, linear perceptron, SVM with linear kernel, polynomial kernel and radial basis function (RBF) kernel. We used default parameters of the toolboxes. In the second ensemble setup, we trained a total of 154 SVM\u2019s with different kernel functions and parameters. Latter method produces less diverse base classifiers with respect to the former one. Training data of the combiner is obtained by 4-fold stacked generalization. For each stack in 5\u00d72 CV, 2-fold CV is used to obtain the optimal \u03bb in the regularization\nfunction, i.e. \u03bb which gives the best average accuracy in CV 1. For the minimization of the objective functions, we used the CVX-toolbox [27]. We use the Wilcoxon signed-rank test for identifying statistical significance of the results with one-tailed significant level \u03b1 = 0.05 [28]."}, {"heading": "7 RESULTS", "text": "First, we investigate the performance of regularized learning of the weights with the hinge loss compared to the conventional least squares loss [13] and the multiresponse linear regression method which does not contain regularization [6] with the diverse ensemble setup described in section 6. It should be noted that results shown here and in [13], [6] are not directly comparable since construction of the ensembles is different. Error percentages of these three different learning algorithms for WS, CWS and LSG are given in Table 2. Results for the simple sum rule, which is equivalent to using equal weights in the WS, are also given in the column titled EW. The first entries in the boxes are the means of error percentages over 5\u00d7 2 CV stacks and the second entries are the standard deviations. For five datasets, the lowest error means are obtained with the hinge loss function and for two datasets lowest error means are obtained with the least-squares loss function. On yeast dataset, simple averaging works better than the supervised learners. On all datasets, MLR method results in higher error percentages compared to other methods, and this shows the power of regularized learning, especially if the number of base classifiers is high. It should be noted that in [6], 3 base classifiers are used and here we use 130 base classifiers. According to the pairwise Wilcoxon signed-ranks test [28], hinge loss function outperforms least squares loss function at one-tailed significant level \u03b1 = 0.05 for WS and CWS combination types and at \u03b1 = 0.0525 for LSG combination.\n1. We searched for \u03bb in {10\u221211, 10\u22129, 10\u22127, 10\u22125, 10\u22123, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10}\n2. The full name of Segment dataset is \u201cImage Segmentation\u201d 3. The full name of Waveform dataset is \u201cWaveform Database Generator (Version 1)\u201d 4. The full name of Robot dataset is \u201cWall-Following Robot Navigation Data\u201d 5. The full name of Statlog dataset is \u201cStatlog (Vehicle Silhouettes)\u201d 6. The full name of Vowel dataset is \u201cConnectionist Bench (Vowel Recognition - Deterding Data)\u201d 7. The full name of Steel dataset is \u201cSteel Plates Faults\u201d\n6\nFig. 1: Accuracy and Number of selected classifiers vs. \u03bb for WS combination of Robot data with the diverse ensemble setup\n0\n20\n40\n60\n80\n100\n120\n140\n0.935\n0.94\n0.945\n0.95\n0.955\n0.96\n0.965\n0.97\n1.00E-07 1.00E-06 1.00E-05 1.00E-04 1.00E-03 1.00E-02 1.00E-01 1.00E+00\nN u\nm b\ner o\nf Se\nle ct\ned C\nla ss\nif ie\nrs\nA cc\nu ra\ncy\nl2 accuracy l1-l2 accuracy l1 accuracy\nl1-l2 # of classifiers l1 # of classifiers\nFig. 2: Accuracy and Number of selected classifiers vs. \u03bb for CWS combination of Robot data with the diverse ensemble setup\nWe also investigated the performance of sparse regularization with the hinge loss function. We used two different ensemble setups described in the beginning of this section. Regularization parameter \u03bb given in the objective functions (8,11,12) is an important parameter and if we minimize the objective functions also over \u03bb, the combiner will overfit the training data, which will result in poor generalization performance. Therefore, we used 2-fold cross-validation to learn the optimal parameter. We plot the relation of \u03bb with accuracies and the number of selected classifiers for different regularizations with WS, CWS and LSG for Robot dataset in Figures 1, 2 and 3 respectively. In these figures, dashed lines correspond to the number of selected classifiers and solid lines\ncorrespond to accuracies. The l1 \u2212 l2 label represents group sparsity. In all sparse regularizations, the best accuracies are obtained when most of the base classifiers are eliminated. For all regularizations, accuracies make a peak at \u03bb values between 0.001 and 0.1. For l1 norm regularization, accuracies drop dramatically with a small increase in \u03bb. However, with group sparsity regularization, accuracies remain high in a larger range for \u03bb than that with the l1 norm regularization. Thus the performance of l1 regularization is more sensitive to the selection of \u03bb. So we can say that the l1 \u2212 l2 norm regularization is more robust than l1 norm regularization. As the number of selected classifiers decrease, accuracies increase in general, but this increase in the accuracy cannot be attributed to the classifier selection, because \u03bb also determines how much the combiner should fit the data as discussed in section 5.\nNext, we show the test results for all combination types with various regularization functions. Error percentages (mean\u00b1 standard deviation) are shown in Table 3 for the diverse ensemble setup and corresponding number of selected classifiers are shown in Table 4.\nIn general, we are able to use much less base classifiers with sparse regularizations with the cost of a small decrease in the accuracies. For CWS, group sparsity regularization outperforms l1 norm regularization at onetailed significance level \u03b1 = 0.005. For LSG, average error percentage of group sparsity is a little less than that of the l1 norm regularization which is not statistically significant. But the number of selected base classifiers is much less. So if classifier selection is desired, we suggest\n7\nto use either CWS or LSG combination with l1 \u2212 l2 regularization. If training time is also crucial, CWS with l1 \u2212 l2 regularization seems to be the best option.\nError percentages and number of selected classifiers for the non-diverse ensembles are given in Tables 5 and 6 respectively. With the non-diverse ensembles we are even able to increase the accuracy with much less number of base classifiers with sparse regularization in CWS and LSG. On the average, l1 \u2212 l2 regularization results in lower error percentages for both CWS and LSG, but the results are not statistically significant. But, the number of selected classifiers is much less with l1\u2212l2 regularization than that of l1 regularization. Except statlog dataset, lowest error percentages are obtained with the sparse combinations with much less base classifiers than that of l2 regularization which uses 154 base classifiers. If we compare different combination types with the l2 norm, on the average we see that, unlike in the diverse ensemble setup, WS and CWS outperforms LSG in four databases. We can conclude that if the posterior scores obtained from base classifiers are correlated, noncomplex combiners are more powerful since complex combiners may result in overfitting."}, {"heading": "8 CONCLUSION", "text": "In this paper, we suggested using hinge loss function with regularization to learn the parameters (or weights) of linear combiners in stacked generalization. We are able to obtain better accuracies with the hinge loss function than conventional least-squares estimation of the weights. Results also indicate the importance of the regularized learning of the weights. We also proposed l1 \u2212 l2 norm regularization (or group sparsity) to obtain a reduced number of base classifiers so that the test time is shortened. Results indicate that we can use smaller number of base classifiers with a small sacrifice in the accuracy with the diverse ensemble. We show that l1\u2212 l2 regularization outperforms l1 regularization in terms of\nboth accuracy and the number of selected classifiers. With the non-diverse ensemble setup, we even obtain better accuracies using sparse regularizations. If training time is crucial, we suggest using CWS type combination. And if test time is also important, we suggest using group sparsity regularization."}, {"heading": "9 ACKNOWLEDGMENTS", "text": "This research is supported by The Scientific and Technological Research Council of Turkey (TUBITAK) under the scientific and technological research support program (code 1001), project number 107E015 entitled \u201cNovel Approaches in Audio Visual Speech Recognition\u201d."}], "references": [{"title": "Ensemble methods in machine learning,", "author": ["T.G. Dietterich"], "venue": "in International Workshop on Multiple Classifier Systems. Springer- Verlag,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Ensemble based systems in decision making,", "author": ["R. Polikar"], "venue": "Ieee Circuits And Systems Magazine,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Combining Pattern Classifiers: Methods and Algorithms", "author": ["L.I. Kuncheva"], "venue": "Wiley-Interscience", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Stacked generalization,", "author": ["D.H. Wolpert"], "venue": "Neural Netw.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Combining the results of several neural network classifiers,", "author": ["G. Rogova"], "venue": "Neural Netw.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Issues in stacked generalization,", "author": ["K.M. Ting", "I.H. Witten"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Combining estimates in regression and classification,", "author": ["M. LeBlanc", "R. Tibshirani"], "venue": "Journal of the American Statistical Association, Tech. Rep.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Stacking bagged and dagged models,", "author": ["K.M. Ting", "I.H. Witten"], "venue": "Proc. 14th International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "How to make stacking better and faster while also taking care of an unknown weakness,", "author": ["A.K. Seewald"], "venue": "Proceedings of the Nineteenth International Conference on Machine Learning, ser. ICML \u201902", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Using correspondence analysis to combine classifiers,", "author": ["C.J. Merz", "S. Stolfo"], "venue": "Machine Learning. Kluwer Academic Publishers,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "and D", "author": ["J. Sill", "G. Tak\u00e1cs", "L. Mackey"], "venue": "Lin, \u201cFeature-weighted linear stacking,\u201d CoRR, vol. abs/0911.0460", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Regularized linear models in stacked generalization,", "author": ["S. Reid", "G. Grudic"], "venue": "Proceedings of the 8th International Workshop on Multiple Classifier Systems, ser. MCS \u201909. Berlin, Heidelberg: Springer-Verlag,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Deroski, \u201cCombining multiple models with meta decision trees,\u201d in Principles of Data Mining and Knowledge Discovery, ser", "author": ["S.L. Todorovski"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Ga-stacking: Evolutionary stacked generalization,", "author": ["A. Ledezma", "R. Aler", "A. Sanchis", "D. Borrajo"], "venue": "Intell. Data Anal.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "and X", "author": ["B. Tang", "Q. Chen", "X. Wang"], "venue": "Wang, \u201cReranking for stacking ensemble learning,\u201d in Proceedings of the 17th international conference on Neural information processing: theory and algorithms - Volume Part I, ser. ICONIP\u201910. Berlin, Heidelberg: Springer-Verlag", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimal linear combination of neural networks for improving classification performance,", "author": ["N. Ueda"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 22,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "and B", "author": ["Y. Lecun", "S. Chopra", "R. Hadsell", "F.J. Huang", "G. Bakir", "T. Hofman", "B. Schlkopf", "A. Smola"], "venue": "T. (eds, \u201cA tutorial on energy-based learning,\u201d in Predicting Structured Data. MIT Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Sparse ensembles using weighted combination methods based on linear programming,", "author": ["L. Zhang", "W.-D. Zhou"], "venue": "Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "A unifying framework for learning the linear combiners for classifier ensembles,", "author": ["H. Erdogan", "M. Sen"], "venue": "in Pattern Recognition (ICPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Least squares support vector machine classifiers,", "author": ["J. Suykens", "J. Vandewalle"], "venue": "Neural Processing Letters,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Ultraconservative online algorithms for multiclass problems,", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "UCI machine learning repository,", "author": ["A. Asuncion", "D. Newman"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Approximate statistical tests for comparing supervised classification learning algorithms,", "author": ["T.G. Dietterich"], "venue": "Neural Computation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "PRTools4.1, A Matlab Toolbox for Pattern Recognition, 2007, delft University of Technology", "author": ["R.P.W.J.P. D", "P.P.E. P", "D. d. R", "D.M.J. T", "S. V"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "LIBSVM: a library for support vector machines, 2001, software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "CVX: Matlab software for disciplined convex programming, version 1.21,", "author": ["M. Grant", "S. Boyd"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Statistical comparisons of classifiers over multiple data sets,", "author": ["J. Dem\u0161ar"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Among all theoretical and practical reasons to prefer using ensembles, which are categorized as statistical, computational and representational in [1], the most important ones are the statistical reasons.", "startOffset": 147, "endOffset": 150}, {"referenceID": 0, "context": "Some other reasons besides statistical reasons can be found in [1], [2].", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "Some other reasons besides statistical reasons can be found in [1], [2].", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "They can be interpreted as confidences in the suggested labels or estimates of the posterior probabilities for the classes [3].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 211, "endOffset": 214}, {"referenceID": 5, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 216, "endOffset": 219}, {"referenceID": 6, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 221, "endOffset": 224}, {"referenceID": 7, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 226, "endOffset": 229}, {"referenceID": 8, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 231, "endOffset": 234}, {"referenceID": 9, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 236, "endOffset": 240}, {"referenceID": 10, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 248, "endOffset": 252}, {"referenceID": 11, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 254, "endOffset": 258}, {"referenceID": 12, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 260, "endOffset": 264}, {"referenceID": 13, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 266, "endOffset": 270}, {"referenceID": 3, "context": "The idea of Stacking is to use the confidence scores that are obtained from base classifiers as attributes in a new training set with the original class labels and training a meta-classifier (This classifier is called level-1 generalizer in [4]) with this new dataset.", "startOffset": 241, "endOffset": 244}, {"referenceID": 3, "context": "the class predictions of the base classifiers [4].", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "Ting & Witten used confidence scores of base classifiers as input features and improved stacking\u2019s performance [6], [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 7, "context": "Ting & Witten used confidence scores of base classifiers as input features and improved stacking\u2019s performance [6], [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Sill, incorporated meta-features with the posterior scores of base classifiers to improve accuracy [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "Ledezma, used genetic algorithms to search for good Stacking configurations [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "Tang, re-ranked all possible class labels according to the scores and obtained a learner which outperforms all base classifiers [16].", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": ") Given a combination type, how should we learn the parameters of the combiner? For the former problem, Ueda [17] defined three linear combination types namely type-1, type-2 and type-3.", "startOffset": 109, "endOffset": 113}, {"referenceID": 6, "context": "In [7], [8], LSG is used and CWS combination is proposed in [6].", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "In [7], [8], LSG is used and CWS combination is proposed in [6].", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "In [7], [8], LSG is used and CWS combination is proposed in [6].", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "For the latter problem, Ting & Witten proposed a multi-response linear regression algorithm for learning the weights [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 15, "context": "Ueda in [17] proposed using minimum classification error (MCE) criterion for estimating optimal weights, which increased the accuracies.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "Reid & Grudic in [13] regularized the standard linear least squares estimation of the weights with CWS and improved the performance of stacking.", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "We work with the regularized empirical risk minimization framework [18] and use the hinge loss function with l2 regularization, which corresponds to the support vector machines (SVM).", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "Another issue, recently addressed in [19], is combination with a sparse weight vector so that we do not use all of the ensemble.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "Zhang formulated this problem as a linear programming problem for only the WS combination type [19].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "Reid used l1 norm regularization for CWS combination [13].", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "A novel approach has been introduced in 1992 known as stacked generalization or stacking [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Wolpert combined only the class predictions with this framework, Ting & Witten improved the performance of stacking by combining continuous valued predictions [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 15, "context": "This type of combination is the most general form of linear combiners and called type-3 combination in [17].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "These types are categorized as class-conscious combinations in [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 18, "context": "There is not a single superior one among these three combination types since results are shown to be data dependent [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "We use the regularized empirical risk minimization (RERM) framework [18] for learning the weights.", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "Earlier classifier combination literature uses LS loss function [6], [8], [13], which is suboptimal as compared to the hinge loss that we promote and use in this paper.", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "Earlier classifier combination literature uses LS loss function [6], [8], [13], which is suboptimal as compared to the hinge loss that we promote and use in this paper.", "startOffset": 69, "endOffset": 72}, {"referenceID": 11, "context": "Earlier classifier combination literature uses LS loss function [6], [8], [13], which is suboptimal as compared to the hinge loss that we promote and use in this paper.", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "Using leastsquares with l2 regularization is equivalent to applying least-squares support vector machine (LS-SVM) [21].", "startOffset": 114, "endOffset": 118}, {"referenceID": 20, "context": "We use an adaptation of SVM in multiclass problems defined in [22].", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "Equation (8) is given for LSG but it can be modified for other types of combinations using the unifying framework described in [20].", "startOffset": 127, "endOffset": 131}, {"referenceID": 21, "context": "We have performed extensive experiments in eight realworld datasets from the UCI repository [23].", "startOffset": 92, "endOffset": 96}, {"referenceID": 22, "context": "In order to obtain statistically significant results, we applied 5x2 cross-validation [24] which is based on 5 iterations of 2-fold cross-validation (CV).", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "We used PR-Tools [25] and Libsvm toolbox [26] for obtaining the base classifiers.", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": "We used PR-Tools [25] and Libsvm toolbox [26] for obtaining the base classifiers.", "startOffset": 41, "endOffset": 45}, {"referenceID": 25, "context": "For the minimization of the objective functions, we used the CVX-toolbox [27].", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "05 [28].", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "First, we investigate the performance of regularized learning of the weights with the hinge loss compared to the conventional least squares loss [13] and the multiresponse linear regression method which does not contain regularization [6] with the diverse ensemble setup described in section 6.", "startOffset": 145, "endOffset": 149}, {"referenceID": 5, "context": "First, we investigate the performance of regularized learning of the weights with the hinge loss compared to the conventional least squares loss [13] and the multiresponse linear regression method which does not contain regularization [6] with the diverse ensemble setup described in section 6.", "startOffset": 235, "endOffset": 238}, {"referenceID": 11, "context": "It should be noted that results shown here and in [13], [6] are not directly comparable since construction of the ensembles is different.", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "It should be noted that results shown here and in [13], [6] are not directly comparable since construction of the ensembles is different.", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "It should be noted that in [6], 3 base classifiers are used and here we use", "startOffset": 27, "endOffset": 30}, {"referenceID": 26, "context": "According to the pairwise Wilcoxon signed-ranks test [28], hinge loss function outperforms least squares loss function at one-tailed significant level \u03b1 = 0.", "startOffset": 53, "endOffset": 57}], "year": 2011, "abstractText": "The main principle of stacked generalization (or Stacking) is using a second-level generalizer to combine the outputs of base classifiers in an ensemble. In this paper, we investigate different combination types under the stacking framework; namely weighted sum (WS), class-dependent weighted sum (CWS) and linear stacked generalization (LSG). For learning the weights, we propose using regularized empirical risk minimization with the hinge loss. In addition, we propose using group sparsity for regularization to facilitate classifier selection. We performed experiments using two different ensemble setups with differing diversities on 8 real-world datasets. Results show the power of regularized learning with the hinge loss function. Using sparse regularization, we are able to reduce the number of selected classifiers of the diverse ensemble without sacrificing accuracy. With the non-diverse ensembles, we even gain accuracy on average by using sparse regularization.", "creator": "LaTeX with hyperref package"}}}