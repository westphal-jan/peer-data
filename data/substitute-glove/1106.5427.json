{"id": "1106.5427", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2011", "title": "Theory and Algorithms for Partial Order Based Reduction in Planning", "abstract": "Search is new major technique for services. It raw to exploring a oregon solar of working domains provide resembles part hand directed parameter. However, prohibitively large standard including of search space makes search putting. Developing they inference nodes so been the branch ingenious all improving find improved. Nevertheless, critical academic have presented that improve pre-treatment seeing all certain basic limits making sectors agents improve. Recently, after new step of research suggested slight purpose based contribute (POR) the been initiatives taken critical alternative to deteriorating interfacing. POR has performance promise in speeding end videos.", "histories": [["v1", "Mon, 27 Jun 2011 16:06:27 GMT  (185kb)", "http://arxiv.org/abs/1106.5427v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["you xu", "yixin chen", "qiang lu", "ruoyun huang"], "accepted": false, "id": "1106.5427"}, "pdf": {"name": "1106.5427.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n10 6.\n54 27\nv1 [\ncs .A\nI] 2\n7 Ju\nn 20\n11\nTheory and Algorithms for Partial Order Based\nReduction in Planning\nYou Xu and Yixin Chen\nWashington University in St. Louis\nand\nQiang Lu\nUniversity of Science and Technology of China\nand\nRuoyun Huang\nWashington University in St. Louis\nSearch is a major technique for planning. It amounts to exploring a state space of planning domains typically modeled as a directed graph. However, prohibitively large sizes of the search space make search expensive. Developing better heuristic functions has been the main technique for improving search efficiency. Nevertheless, recent studies have shown that improving heuristics alone has certain fundamental limits on improving search efficiency. Recently, a new direction of research called partial order based reduction (POR) has been proposed as an alternative to improving heuristics. POR has shown promise in speeding up searches.\nPOR has been extensively studied in model checking research and is a key enabling technique for scalability of model checking systems. Although the POR theory has been extensively studied in model checking, it has never been developed systematically for planning before. In addition, the conditions for POR in the model checking theory are abstract and not directly applicable in planning. Previous works on POR algorithms for planning did not establish the connection between these algorithms and existing theory in model checking.\nIn this paper, we develop a theory for POR in planning. The new theory we develop connects the stubborn set theory in model checking and POR methods in planning. We show that previous POR algorithms in planning can be explained by the new theory. Based on the new theory, we propose a new, stronger POR algorithm. Experimental results on various planning domains show further search cost reduction using the new algorithm.\nCategories and Subject Descriptors: I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search\u2014Graph and tree search strategies\nGeneral Terms: AI planning, State-space search, Partial order reduction, Stubborn set"}, {"heading": "1. INTRODUCTION", "text": "State space search is a fundamental and pervasive approach to artificial intelligence in general and planning in particular. It is among the most successful approaches to planning. A major concern with state space search is that it has a high time and space cost since the state space that needs to be explored is usually very large.\nMuch research on classical planning has focused on the design of better heuristic functions. For example, new heuristic functions have recently been developed by analyzing the domain transition graphs (DTGs) and causal graphs on top of the SAS+ formalism [Briel et al. 2007; Helmert and Ro\u0308ger 2008]. Despite the suc-\nACM Journal Name, Vol. V, No. N, Month 20YY, Pages 1\u20130??.\n2 \u00b7\ncess of using domain-independent heuristics for classic planning, heuristic planners still face scalability challenges for large-scale problems. As shown by recent work, search even with almost perfect heuristic guidance may still lead to very high search cost [Helmert and Ro\u0308ger 2008]. Therefore, it is important to improve other components of the search algorithm that are orthogonal to the development of heuristics.\nRecently, partial order based reduction (POR), a new way to reduce the search cost from an orthogonal perspective, has been studied for classical planning [Chen et al. 2009; Chen and Yao 2009]. POR as a method to reduce search space has been extensively studied in model checking with solid theoretical investigation. However, the theoretical properties of POR in planning have still not been fully investigated. There are three key questions.\n1) POR algorithms have been extensively studied in model checking. In fact, POR is an enabling technique for modeling checking, which will not be practical without POR due to its high time complexity. Extensive research has been developed for the theory of POR in model checking. What are the relationships between the previous POR methods designed for model checking and existing work for planning? Understanding these relationships can not only help us understand both problems better, but can also potentially lead to better POR algorithms for planning.\n2) In essence, all POR based algorithms reduce the search space by restricting certain actions from expanding at each state. Although these POR algorithms all look similar, what are the differences in the quality of reduction that significantly affect search efficiency? We think it is important to investigate the reduction powers of different POR algorithms.\n3) Given the fact that there is more than one POR reduction algorithm for planning, are there other, stronger POR algorithms? To answer this question, in essence, we need to find the sufficient and/or necessary conditions for partialorder based pruning. There are sufficient conditions for POR in model checking. Nevertheless, those conditions are abstract and not directly applicable in planning.\nThe main contribution of this work is to establish the relationship between the POR methods for model checking and those for planning. We leverage on the existing POR theory for model checking and develop a counterpart theory for planning. This new theory allows existing POR algorithms for planning to be explained in a unified framework. Moreover, based on the conditions given by this theory, we develop a new POR algorithm for planning that is stronger than previous ones. Experimental results also show that our proposed algorithm leads to more reduction.\nThis paper is organized as follows. We first give basic definitions in Section 2. In Section 3, we present a general theory that gives sufficient conditions for POR in planning. In Section 4, we use the new theory to explain two previous POR algorithms. Based on the theory, in Section 5, we propose a new POR algorithm for planning which is different and stronger than previous ones. We report experimental results in Section 7, review some related work in Section 8, and give conclusions in Section 9.\nACM Journal Name, Vol. V, No. N, Month 20YY.\n\u00b7 3"}, {"heading": "2. BACKGROUND", "text": "Planning is a core area of artificial intelligence. It entails arranging a course of actions to achieve certain goals under given constraints. Classical planning is the most fundamental form of planning, which deals with only propositional logic. In this paper, we work on the SAS+ formalism [Jonsson and Ba\u0308ckstro\u0308m 1998] of classical planning. SAS+ formalism has recently attracted a lot of attention due to a number of advantages it has over the traditional STRIPS formalism. In the following, we review this formalism and introduce our notations.\nDefinition 1. A SAS+ planning task \u03a0 is defined as a tuple of four elements, \u03a0 = {X,O, S, sI , sG}.\n\u2014X = {x1, \u00b7 \u00b7 \u00b7 , xN} is a set of multi-valued state variables, each with an associated finite domain Dom(xi).\n\u2014O is a set of actions and each action o \u2208 O is a tuple (pre(o), eff(o)), where both pre(o) and eff(o) define some partial assignments of state variables in the form xi = vi, vi \u2208 Dom(xi). sG is a partial assignment that defines the goal.\n\u2014S is the set of states. A state s \u2208 S is a full assignment to all the state variables. sI \u2208 S is the initial state. A state s is a goal state if sG \u2286 s.\nDefinition 2. Two partial assignment sets are conflict-free if and only if they do not assign different values to the same state variable.\nFor a SAS+ planning task, for a given state s and an action o, when all variable assignments in pre(o) are met in state s, action o is applicable in state s. After applying o to s, the state variable assignment will be changed to a new state s\u2032 according to eff(o): the state variables that appear in eff(o) will be changed to the assignments in eff(o) while other state variables remain the same. We denote the resulting state after applying an applicable action o to s as s\u2032 = apply(s, o). apply(s, o) is undefined if o is not applicable in s. The planning task is to find a path, or a sequence of actions, that transits the initial state sI to a goal state that includes sG. An important structure for a given SAS+ task is the domain transition graph defined as follows:\nDefinition 3. For a SAS+ planning task, each state variable xi (i = 1, \u00b7 \u00b7 \u00b7 , N) corresponds to a domain transition graph (DTG) Gi, a directed graph with a vertex set V (Gi) = Dom(xi) \u222a v0, where v0 is a special vertex, and an edge set E(Gi) determined by the following.\n\u2014If there is an action o such that (xi = vi) \u2208 pre(o) and (xi = v\u2032i) \u2208 eff(o), then (vi, v \u2032 i) belongs to E(Gi) and we say that o is associated with the edge\nei = (vi, v \u2032 i) (denoted as o \u22a2 ei). It is conventional to call the edges in DTGs transitions.\n\u2014If there is an action o such that (xi = v \u2032 i) \u2208 eff(o) and no assignment to xi is in\npre(o), then (v0, v \u2032 i) belongs to E(Gi) and we say that o is associated with the transition ei = (v0, v \u2032 i) (denoted as o \u22a2 ei).\nACM Journal Name, Vol. V, No. N, Month 20YY.\n4 \u00b7\nIntuitively, a SAS+ task can be decomposed into multiple objects, each corresponding to one DTG, which models the transitions of the possible values of that object.\nDefinition 4. For a SAS+ planning task, an action o is associated with a DTG Gi (denoted as o \u22a2 Gi) if eff(o) contains an assignment to xi.\nDefinition 5. For a SAS+ planning task, a DTG Gi is goal-related if the partial assignments in sG that define the goal states include an assignment xi = gi in Gi. A goal-related DTG is unachieved in state s if xi = vi in s and vi 6= gi.\nA SAS+ planning task can also specify a preference that needs to be optimized. A preference is a mapping from a path p to a numerical value. In this paper we assume an action set invariant preference. A preference is action set invariant if two paths have the same preference whenever they contain the same set of actions (possibly in different orders). Most popular preferences, such as plan length and total action cost, are action set invariant."}, {"heading": "3. PARTIAL ORDER REDUCTION THEORY FOR PLANNING", "text": "Partial order based reduction (POR) algorithms have been extensively studied for model checking [Varpaaniemi 2005; Clarke et al. 2000], which also requires examining a state space in order to prove certain properties. POR is a technique that allows a search to explore only part of the entire search space and still maintain completeness and/or optimality. Without POR, model checking would be too expensive to be practical [Holzmann 1997]. However, POR has not been studied systematically for planning. In this section, we will first introduce the concept of search reduction. Then, we will present a general POR theory for planning, which gives sufficient conditions that guide the design of practical POR algorithms."}, {"heading": "3.1 Search reduction for planning", "text": "We first introduce the concept of search reduction. A standard search, such as breath-first search (BFS), depth-first search, or A\u2217 search, needs to explore a state space graph. A reduction algorithm is an algorithm that reduces the state space graph into a subgraph, so that a search will be performed on the subgraph instead of the original one. We first define the state space graph. In our presentation, for any graph G, we use V (G) to denote the set of vertices and E(G) the set of edges. For a directed graph G, for any vertex s \u2208 V (G), a vertex s\u2032 \u2208 V (G) is its successor if and only if (s, s\u2032) \u2208 E(G). For a SAS+ planning task, a state space graph for the task is a directed graph G in which each state s is a vertex and each directed edge (s, s\u2032) represents an action that will be explored during a search process. Most search algorithms work on the original state space graph as defined below.\nDefinition 6. For a SAS+ planning task, its original state space graph is a directed graph G in which each state s is a vertex and there is a directed edge (s, s\u2032) if and only if there exists an action o such that apply(s, o) = s\u2032. We say that action o marks the edge (s, s\u2032).\nACM Journal Name, Vol. V, No. N, Month 20YY.\n\u00b7 5\nDefinition 7. For a SAS+ planning task, for a state space graph G, the successor set of a state s , denoted by succG(s), is the set of all the successor states of s. The expansion set of a state s, denoted by expandG(s), is the set of actions\nexpandG(s) = {o | o marks (s, s \u2032), (s, s\u2032) \u2208 E(G)}.\nIntuitively, the successor set of a state s includes all the successor states that shall be generated by a search upon expanding s, while the expansion set includes all the actions to be expanded at s. In general, a reduction method is a method that maps each input state space graph G to a subgraph of G. The POR algorithms we study remove edges from G. More specifically, each state s is only connected to a subset of all its successors in the reduced subgraph. We note that, by removing edges, a POR algorithm may also reduce the number of vertices that are reachable from the initial state, hence reducing the number of nodes examined by a search. The decision whether a successor state s\u2032 would still be a successor in the reduced subgraph can be made locally by checking certain conditions related to the current state and some precomputed information. Hence, a POR algorithm can be combined with various search algorithms. For a SAS+ task, a solution sequence in its state space graph G is a pair (s0, p), where s0 is a non-goal state, p = (a1, . . . , ak) is a sequence of actions, and, let si = apply(si\u22121, ai), i = 1, . . . , k, (s\ni\u22121, si) is an edge in G for i = 1, . . . , k and sk is a goal state. We now define some generic properties of reduction methods.\nDefinition 8. For a SAS+ planning task, a reduction method is completenesspreserving if for any solution sequence (s0, p) in the state space graph, there also exists a solution sequence (s0, p\u2032) in the reduced state space graph.\nDefinition 9. For a SAS+ planning task, a reduction method is optimalitypreserving if, for any solution sequence (s0, p) in the state space graph, there also exists a solution sequence (s0, p\u2032) in the reduced state space graph satisfying that p\u2032 has the same preference that p does.\nDefinition 10. For a SAS+ planning task, a reduction method is action-preserving if, for any solution sequence (s0, p) in the state space graph, there also exists a solution sequence (s0, p\u2032) in the reduced state space graph satisfying that the actions in p\u2032 is a permutation of the actions in p.\nClearly, being action-preserving is a sufficient condition for being completenesspreserving. When the preference is action set invariant, being action-preserving is also a sufficient condition for being optimality-preserving."}, {"heading": "3.2 Stubborn set theory for planning", "text": "Although there are many variations of POR methods, a popular and representative POR algorithm is the stubborn set method [Valmari 1988; 1989; 1990; 1998; 1991; 1993], used for model checking based on Petri nets. The basic idea is to form a stubborn set of applicable actions for each state and expand only the actions in the stubborn set during search. By expanding a small subset of applicable actions in each state, stubborn set methods can reduce the search space without compromising completeness.\nACM Journal Name, Vol. V, No. N, Month 20YY.\n6 \u00b7\nSince planning also examines a large search space, we propose to develop a stubborn set theory for planning. To achieve this, we need to handle various subtle issues arising from the differences between model checking and planning. We first define the concept of stubborn sets for planning, adapted from the concepts in model checking.\nDefinition 11 Stubborn Set for Planning. For a SAS+ planning task, a set of actions T (s) is a stubborn set at state s if and only if\nA1) For any action b \u2208 T (s) and actions b1, \u00b7 \u00b7 \u00b7 , bk /\u2208 T (s), if (b1, \u00b7 \u00b7 \u00b7 , bk, b) is a prefix of a path from s to a goal state, then (b, b1, \u00b7 \u00b7 \u00b7 , bk) is a valid path from s and leads to the same state that (b1, \u00b7 \u00b7 \u00b7 , bk, b) does; and\nA2) Any valid path from s to a goal state contains at least one action in T (s).\nThe above definition is schematically illustrated in Figure 1. Once we define the stubborn set T (s) at each state s, we in effect reduce the state space graph to a subgraph: only the edges corresponding to actions in the stubborn sets are kept in the subgraph.\nDefinition 12. For a SAS+ planning task, given a stubborn set T (s) defined at each state s, the stubborn set method reduces its state space graph G to a subgraph Gr such that V (Gr) = V (G) and there is an edge (s, s\u2032) in E(Gr) if and only if there exists an action o \u2208 T (s) such that s\u2032 = apply(s, o).\nA stubborn set method for planning is a reduction method that reduces the original state space graph G to a subgraph Gr according to Definition 12. In other words, a stubborn set method expands actions only in a stubborn set in each state. In the sequel, we show that such a reduction method preserves actions, hence, it also preserves completeness and optimality.\nTheorem 1. Any stubborn set method for planning is action-preserving.\nProof. We prove that for any solution sequence (s0, p) in the original state space graph G, there exists a solution sequence (s0, p\u2032) in the reduced state space graph Gr resulting from the stubborn set method, such that p\u2032 is a permutation of actions in p. We prove this fact by induction on k, the length of p. When k = 1, let a be the only action in p, according to the second condition in Definition 12, a is in T (s0). Thus, (s 0, p) is also a solution sequence in Gr. The EC method is action-preserving in the base case.\nACM Journal Name, Vol. V, No. N, Month 20YY.\n\u00b7 7\nWhen k > 1, the induction assumption is that any path in G with length less than or equal to k \u2212 1 has a permutation in Gr that leads to the same final state. Now we consider a solution sequence (s0, p) in G: p = (a1, . . . , ak). Let si = apply(si\u22121, ai), i = 1, . . . , k. If a1 \u2208 T (s), we can invoke the induction assumption for the state s1 and prove our induction assumption for k. We now consider the case where a1 /\u2208 T (s). Let aj be the first action in p such that aj \u2208 T (s). Such an action must exist because of the condition A2 in Definition 11. Consider the sequence p\u2217 = (aj , a1, \u00b7 \u00b7 \u00b7 , aj\u22121, aj+1, \u00b7 \u00b7 \u00b7 , ak). According to condition A1 in Definition 12, (aj , a1, \u00b7 \u00b7 \u00b7 , aj\u22121) is also a valid sequence from s0 which leads to the same state that (a1, \u00b7 \u00b7 \u00b7 , aj) does. Hence, we know that (s0, p\u2217) is also a solution path. Therefore, let s\u2032 = apply(s0, aj), we know (a1, \u00b7 \u00b7 \u00b7 , aj\u22121) is an executable action sequence starting from s\u2032. Let p\u2217\u2217 = (a1, \u00b7 \u00b7 \u00b7 , aj\u22121, aj+1, \u00b7 \u00b7 \u00b7 , ak), (s\u2032, p\u2217\u2217) is a solution sequence in G. From the induction assumption, we know there is a sequence p\u2032 which is a permutation of p\u2217\u2217, such that (s\u2032, p\u2032) is a solution sequence in Gr. Since aj \u2208 T (s 0), we know that aj followed by p \u2032 is a solution sequence from s0 and is a permutation of actions in p\u2217, which is a permutation of actions in p. Thus, the stubborn set method is action-preserving.\nSince being action-preserving is a sufficient condition for being completenesspreserving and optimality-preserving, when the preference is action set invariant, we have the following result.\nCorollary 1. A stubborn set method for planning is completeness-preserving. In addition, it is optimality-preserving when the preference is action set invariant."}, {"heading": "3.3 Left commutativity in SAS+ planning", "text": "Note that although Theorem 1 provides an important result for reduction, it is not directly applicable since the conditions in Definition 11 are abstract and not directly implementable in algorithms. We need to find sufficient conditions for Definition 11 that can facilitate the design of reduction algorithms. In the following, we define several concepts that can lead to sufficient conditions for Definition 11.\nDefinition 13 State-Dependent Left Commutativity. For a SAS+ planning task, an ordered action pair (a, b), a, b \u2208 O is left commutative in state s, if (a, b) is a valid path at s, and (b, a) is also a valid path at s and results in the same state. We denote such a relationship by s : b \u21d2 a.\nDefinition 14 State-Independent Left Commutativity. For a SAS+ planning task, an ordered action pair (a, b), a, b \u2208 O is left commutative if, for any state s, it is true that s : b \u21d2 a. We denote such a relationship by b \u21d2 a.\nNote the following. 1) Left commutativity is not a symmetric relationship. b \u21d2 a does not imply a \u21d2 b. 2) The order in the notation b \u21d2 a suggests that we should always try only (b, a) during the search instead of trying both (a, b) and (b, a). Also, not every state-independent left commutative action pair is state-dependent left commutative. For instance, in a SAS+ planning task with three state variables {x1, x2, x3}, action a with pre(a) = {x1 = 0}, eff (a) = {x2 = 1} and action b with pre(b) = {x2 = 1, x3 = 2}, eff (b) = {x3 = 3} are left commutative in state\nACM Journal Name, Vol. V, No. N, Month 20YY.\ns1 = {x1 = 0, x2 = 1, x3 = 3} but not in state s2 = {x1 = 0, x2 = 0, x3 = 2} as b is not applicable in state s2. We introduce state-independent left commutativity as it can be used to derive sufficient conditions for finding stubborn sets.\nDefinition 15 State-Independent Left Commutative Set. For a SAS+ planning task, a set of actions T (s) is a left commutative set at a state s if and only if\nL1) For any action b \u2208 T (s) and any action a \u2208 O \u2212 T (s), if there exists a valid path from s to a goal state that contains both a and b, then it is the case that b \u21d2 a; and\nA2) Any valid path from s to a goal state contains at least one action in T (s).\nTheorem 2. For a SAS+ planning task, for a state s, if a set of actions T (s) is a state-independent left commutative set, it is also a stubborn set.\nProof. We only need to prove that L1 in Definition 15 implies A1 in Definition 11. The proof strategy is schematically shown in Figure 2. For an action b \u2208 T (s) and actions b1, \u00b7 \u00b7 \u00b7 , bk /\u2208 T (s), if (b1, \u00b7 \u00b7 \u00b7 , bk, b) is a prefix of a path from s to a goal state, then according to L1, we see that b \u21d2 bi, for i = 1, \u00b7 \u00b7 \u00b7 , k. According to the definition of left commutativity, we see that bk and b can be swapped and that the resulting path (b1, \u00b7 \u00b7 \u00b7 , b, bk) is still a valid path that leads to the same state that (b1, \u00b7 \u00b7 \u00b7 , bk, b) does. We can subsequently swap b with bk\u22121, \u00b7 \u00b7 \u00b7 , and b1 to obtain equivalent paths, before finally obtaining (b, b1, \u00b7 \u00b7 \u00b7 , bk), as shown in the schematic illustration in the right part of Figure 2. Hence, we have shown that if p = (b1, \u00b7 \u00b7 \u00b7 , bk, b) is a prefix of a path from s to a goal state, then p\u2032 = (b, b1, \u00b7 \u00b7 \u00b7 , bk) is a also valid path from s that leads to the same state that p does, which is exactly the condition A1 in Definition 11.\nFrom the above proof, we see that the requirement of state-independent left commutativity in Definition 15 is unnecessarily strong. Instead, only certain statedependent left commutativity is necessary. In fact, when we change (b1, \u00b7 \u00b7 \u00b7 , bk, b) to (b1, \u00b7 \u00b7 \u00b7 , b, bk), we only require s\u2032 : b \u21d2 bk where s\u2032 is the state after bk\u22121 is executed. Similarly, when we change (b1, \u00b7 \u00b7 \u00b7 , bk, b) to (b1, \u00b7 \u00b7 \u00b7 , b, bk\u22121, bk), we only\nACM Journal Name, Vol. V, No. N, Month 20YY.\n\u00b7 9\nrequire s\u2032\u2032 : b \u21d2 bk\u22121 where s\u2032\u2032 is the state after bk\u22122 is executed. Based on the above analysis, we can refine the sufficient conditions.\nDefinition 16 State-Dependent Left Commutative Set. For a SAS+ planning task, a set of actions T (s) is a left commutative set at a state s if and only if\nL1\u2019) For any action b \u2208 T (s) and actions b1, \u00b7 \u00b7 \u00b7 , bk /\u2208 T (s), if (b1, \u00b7 \u00b7 \u00b7 , bk, b) is a prefix of a path from s to a goal state, then s\u2032 : b \u21d2 bk, where s\u2032 is the state after (b1, \u00b7 \u00b7 \u00b7 , bk\u22121) is executed; and A2) Any valid path from s to a goal state contains at least one action in T (s).\nWe only need to slightly modify the proof to Theorem 2 in order to prove the following theorem.\nTheorem 3. For a SAS+ planning task, for a state s, if a set of actions T (s) is a state-dependent left commutative set, it is also a stubborn set.\nThe above result gives sufficient conditions for finding stubborn sets in planning. The concept of state-dependent left commutative set requires a less stringent condition than the state-independent left commutative set. Such a nuance actually leads to different previous POR algorithms with varying performances. Therefore, it will result in smaller T (s) sets and stronger reduction. Next, we present our algorithm for finding such a set at each state to satisfy these conditions."}, {"heading": "3.4 Determining left commutativity", "text": "Theorem 3 provides a key result for POR. However, the conditions in Definition 13 are still abstract and not directly implementable. The key issue is to efficiently find left commutative action pairs. Now we give necessary and sufficient conditions for Definition 13 that can practically determine left commutativity and facilitate the design of reduction algorithms.\nTheorem 4. For a SAS+ planning task, for a valid action path (a, b) in state s, we have s : b \u21d2 a if and only if pre(a) and eff(b), pre(b) and eff(a), eff(a) and eff(b) are all conflict-free and b is applicable at s.\nProof. First, from the definition of s : b \u21d2 a, we know that action b is applicable in state apply(s, a). This implies that pre(b) and eff (a) are conflict-free. Symmetrically, since action a is applicable in state apply(s, b), pre(a) and eff (b) are also conflict-free. Now we prove eff (a) and eff (b) are conflict-free by contradiction. If eff (a) and eff (b) are not conflict-free, without loss of generality, we can assume that eff(a) contains xi = vi and eff(b) contains xi = v \u2032 i 6= vi. Thus, the value of xi is vi for state sab = apply(apply(s, a), b) and v \u2032 i for state sba = apply(apply(s, b), a), i.e., sab is different than sba. This contradicts our assumption that a and b are left commutative. Thus, eff(a) and eff(b) are conflict-free. Second, if pre(a) and eff(b), eff(a) and pre(b), eff(a) and eff(b) are all conflictfree, since a is applicable in s, a is also applicable in state apply(s, b) as pre(a) and eff (b) are conflict-free. Hence, (b, a) is a valid path at s. Also, for any state variable xi, its value in states sab = apply(apply(s, a), b) and sba = apply(apply(s, b), a) are the same, because eff (a) and eff (b) are conflict-free. Therefore, we have sab = sba. Hence, we have s : b \u21d2 a.\nACM Journal Name, Vol. V, No. N, Month 20YY.\n10 \u00b7\nTheorem 4 gives necessary and sufficient conditions for deciding whether two actions are left-commutative or not. Based on this result, we later develop practical POR algorithms that find stubborn sets using left commutativity."}, {"heading": "4. EXPLANATION OF PREVIOUS POR ALGORITHMS", "text": "Previously, we have proposed two POR algorithms for planning: expansion core (EC) [Chen and Yao 2009] and stratified planning (SP) [Chen et al. 2009], both of which showed good performance in reducing the search space. However we did not have a unified explanation for them. We now explain how these two algorithms can be explained by our theory. Full details of the two algorithms can be found in our papers [Chen and Yao 2009; Chen et al. 2009]."}, {"heading": "4.1 Explanation of EC", "text": "Expansion core (EC) algorithm is a POR-based reduction algorithm for planning. We will see that, in essence, the EC algorithm exploits the SAS+ formalism to find a left commutative set for each state. To describe the EC algorithm, we need the following definitions.\nDefinition 17. For a SAS+ task, for each DTG Gi, i = 1, . . . , N , for a vertex v \u2208 V (Gi), an edge e \u2208 E(Gi) is a potential descendant edge of v (denoted as v \u2701 e) if 1) Gi is goal-related and there exists a path from v to the goal state in Gi that contains e; or 2) Gi is not goal-related and e is reachable from v.\nDefinition 18. For a SAS+ task, for each DTG Gi, i = 1, . . . , N , for a vertex v \u2208 V (Gi), a vertex w \u2208 V (Gi) is a potential descendant vertex of v (denoted as v \u2701w) if 1) Gi is goal-related and there exists a path from v to the goal state in Gi that contains w; or 2) Gi is not goal-related and w is reachable from v.\nDefinition 19. For a SAS+ task, given a state s = (s1, \u00b7 \u00b7 \u00b7 , sN ), for any 1 \u2264 i, j \u2264 N, i 6= j, we call si a potential precondition of the DTG Gj if there exist o \u2208 O and ej \u2208 E(Gj) such that\nsj \u2701 ej, o \u22a2 ej , and si \u2208 pre(o) (1)\nACM Journal Name, Vol. V, No. N, Month 20YY.\n\u00b7 11\nDefinition 20. For a SAS+ task, given a state s = (s1, . . . , sN ), for any 1 \u2264 i, j \u2264 N, i 6= j, we call si a potential dependent of the DTG Gj if there exists o \u2208 O, ei = (si, s\u2032i) \u2208 E(Gi) and wj \u2208 V (Gj) such that\nsj \u2701 wj , o \u22a2 ei, and wj \u2208 pre(o) (2)\nDefinition 21. For a SAS+ task, for a state s = (s1, . . . , sN ), its potential dependency graph PDG(s) is a directed graph in which each DTG Gi, i = 1, \u00b7 \u00b7 \u00b7 , N corresponds to a vertex, and there is an edge from Gi to Gj, i 6= j, if and only if si is a potential precondition or potential dependent of Gj .\nFigure 3 illustrates the above definitions. In PDG(s), G1 points to G2 as s1 is a potential precondition of G2 and G2 points to G1 as s2 is a potential dependent of G1.\nDefinition 22. For a directed graph H, a subset C of V (H) is a dependency closure if there do not exist v \u2208 C and w \u2208 V (H)\u2212 C such that (v, w) \u2208 E(H).\nIntuitively, a DTG in a dependency closure may depend on other DTGs in the closure but not those DTGs outside of the closure. In Figure 3, G1 and G2 form a dependency closure of PDG(s). The EC algorithm is defined as follows:\nDefinition 23 Expansion Core Algorithm. For a SAS+ planning task, the EC method reduces its state space graph G to a subgraph Gr such that V (Gr) = V (G) and for each vertex (state) s \u2208 V (G), it expands actions in the following set T (s) \u2286 O:\nT (s) = \u22c3\ni\u2208C(s)\n{\no\n\u2223 \u2223 \u2223 \u2223 o \u2208 exec(s) \u2227 o \u22a2 Gi } , (3)\nwhere exec(s) is the set of executable actions in s and C(s) \u2286 {1, \u00b7 \u00b7 \u00b7 , N} is an index set satisfying:\nEC1) The DTGs {Gi, i \u2208 C(s)} form a dependency closure in PDG(s); and\nEC2) There exists i \u2208 C(s) such that Gi is goal-related and si is not the goal state in Gi.\nIntuitively, the EC method can be described as follows. To reduce the original state-space graph, for each state, instead of expanding actions in all the DTGs, it only expands actions in DTGs that belong to a dependency closure of PDG(s) under the condition that at least one DTG in the dependency closure is goal-related and not at a goal state. The set C(s) can always be found for any non-goal state s since PDG(s) itself is always such a dependency closure. If there is more than one such closure, theoretically any dependency closure satisfying the above conditions can be used in EC. In practice, when there are multiple such dependency closures, EC picks the one with less actions in order to get stronger reduction. EC has adopted the following scheme to find the dependency closure for any state s. Given a PDG(s), EC first finds its strongly connected components (SCCs). If each SCC is contracted to a single vertex, the resulting graph is a directed acyclic\nACM Journal Name, Vol. V, No. N, Month 20YY.\n12 \u00b7\ngraph S. Note that each vertex in S with a zero out-degree corresponds to a dependency closure. It then topologically sorts all the vertices in S to get a sequence of SCCs: S1, S2, \u00b7 \u00b7 \u00b7 , and picks the minimum m such that Sm includes a goal-related DTG that is not in its goal state. It chooses all the DTGs in S1, \u00b7 \u00b7 \u00b7 , Sm as the dependency closure. Now we explain the EC algorithm using the POR theory we developed in Section 3. We show that the EC algorithm can be viewed as an algorithm for finding a state-dependent left-commutative set in each state.\nLemma 1. For a SAS+ planning task, the EC algorithm defines a state-dependent left commutative set for each state.\nProof. Consider the set of actions T (s) expanded by the EC algorithm in each state s, as defined in (3). We prove that T (s) satisfies conditions L1\u2019 and A2 in Definition 16. Consider an action b \u2208 T (s) and actions b1, \u00b7 \u00b7 \u00b7 , bk /\u2208 T (s) such that (b1, \u00b7 \u00b7 \u00b7 , bk, b) is a prefix of a path from s to a goal state, we show that s\u2032 : b \u21d2 bk, where s\u2032 is the state after (b1, \u00b7 \u00b7 \u00b7 , bk\u22121) is applied to s. Let C(s) be the index set of the DTGs that form a dependency closure, as used in in (3). Since b \u2208 T (s), there must exist m \u2208 C(s) such that b \u22a2 Gm. Let the state after applying (b1, \u00b7 \u00b7 \u00b7 , bk) to s be s\u2217. We see that we must have s\u2217m = sm because otherwise there must exist a bj , 1 \u2264 j \u2264 m that changes the assignment of state variable xm. However, that would imply that bk \u2208 T (s). Since b is applicable in s\u2217, we see that sm = s \u2217 m \u2208 pre(b).\nIf there exists a state variable xi such that an assignment to xi is in both eff (bk) and pre(b), then Gm will point to the DTG Gi as sm is a potential dependent of Gi, forcing Gi to be included in the dependency closure, i.e. i \u2208 C(s). However, as bk \u22a2 Gi, it will violate our assumption that bk /\u2208 T (s). Hence, none of the precondition assignments of b is added by bk. Therefore, since b is applicable in apply(s\u2032, bk), it is also applicable in s\n\u2032. On the other hand, if bk has a precondition assignment in a DTG that b is associated with, then Gm will point to that DTG since sm is a potential precondition of bk, forcing that DTG to be in C(s), which contradicts the assumption that bk /\u2208 T (s). Hence, b does not alter any precondition assignment of bk. Therefore, since bk is applicable in s\n\u2032, it is also applicable in the state apply(s\u2032, b). Finally, if there exists a state variable xi such that an assignment to xi is altered by both b and bk, then we know b \u22a2 Gi and bk \u22a2 Gi. In this case, Gm will point to Gi since sm is a potential precondition of Gi, making bk \u2208 T (s), which contradicts our assumption. Hence, eff (b) and eff (bk) correspond to assignments to distinct sets of state variables. Therefore, applying (bk, b) and (b, bk) to s\n\u2032 will lead to the same state. From the above, we see that b is applicable in s\u2032, bk is applicable in apply(s\n\u2032, b), and hence (b, bk) is applicable in s\n\u2032. Further we see that (b, bk) leads to the same state as (bk, b) does when applied to s\n\u2032. We conclude that s\u2032 : b \u21d2 bk and T (s) satisfies L1\u2019. Moreover, for any goal-related DTG Gi, if in a state s, its assignment si is not the goal state in Gi, then some actions associated with Gi have to be executed in any solution path from s. Since T (s) includes all the actions in at least one goal-related\nACM Journal Name, Vol. V, No. N, Month 20YY.\n\u00b7 13\nDTG Gi, any solution path must contain at least one action in T (s). Therefore, T (s) also satisfies A2 and it is indeed a state-dependent left commutative set.\nFrom Lemma 1 and Theorem 3, we obtain the following result, which shows that EC fits our framework as a stubborn set method for planning.\nTheorem 5. For any SAS+ planning task, the EC algorithm defines a stubborn set in each state."}, {"heading": "4.2 Explanation of SP", "text": "The stratified planning (SP) algorithm exploits commutativity of actions directly [Chen et al. 2009]. To describe the SP algorithm, we need the following definitions first.\nDefinition 24. Given a SAS+ planning task \u03a0 with state variable set X, the causal graph (CG) is a directed graph CG(\u03a0) = (X,E) with X as the vertex set. There is an edge (x, x\u2032) \u2208 E if and only if x 6= x\u2032 and there exists an action o such that x \u2208 eff(o) and x\u2032 \u2208 pre(o) or eff(o).\nDefinition 25. For a SAS+ task \u03a0, a stratification of the causal graph CG(\u03a0) as (X,E) is a partition of the node set X: X = (X1, \u00b7 \u00b7 \u00b7 , Xk) in such a way that there exists no edge e = (x, y) where x \u2208 Xi, y \u2208 Xj and i > j.\nBy stratification, each state variable is assigned a level L(x), where L(x) = i if x \u2208 Xi, 1 \u2264 i \u2264 k. Subsequently, each action o is assigned a level L(o), 1 \u2264 L(o) \u2264 k. L(o) is the level of the state variable(s) in eff (o). Note that all state variables in the same eff (o) must be in the same level, hence, our L(o) is well-defined.\nDefinition 26 Follow-up Action. For a SAS+ task \u03a0, an action b is a followup action of a (denoted as a \u22b2 b) if eff(a) \u2229 pre(b) 6= \u2205 or eff(a) \u2229 eff(b) 6= \u2205.\nThe SP algorithm can be combined with standard search algorithms, such as breadth-first search, depth-first search, and best-first search (including A\u2217). During the search, for each state s that is going to be expanded, the SP algorithm examines the action a that leads to s. Then, for each applicable action b in state S, SP makes the following decisions.\nDefinition 27 Stratified Planning Algorithm. For a SAS+ planning task, in any non-initial state s, assuming a is the action that leads directly to s, and b is an applicable action in s, then SP does not expand b if L(b) < L(a) and b is not a follow-up action of a. Otherwise, SP expands b. In the initial state s0, SP expands all applicable actions.\nThe following result shows the relationship between the SP algorithm and our new POR theory.\nLemma 2. If an action b is not SP-expandable after a, and state s is the state before action a, then s : b \u21d2 a.\nProof. Since b is not SP-expandable after a, following the SP algorithm, we have L(a) > L(b) and b is not a follow-up action of a. According to Definition 26, we have eff(a)\u2229pre(b) = eff(a)\u2229 eff(b) = \u2205. These imply that eff (a) and pre(b) are conflict-free, and that eff (a) and eff(b) are conflict-free. Also, since b is applicable\nACM Journal Name, Vol. V, No. N, Month 20YY.\n14 \u00b7\nin apply(s, a) and eff (a) and pre(b) are conflict-free, b must be applicable in s (Otherwise eff (a) must change the value of at least one variable in pre(b), which means eff (a) and pre(b) are not conflict-free). Now we prove that pre(a) and eff (b) are conflict-free by showing pre(a)\u2229eff(b) = \u2205. If their intersection is non-empty, we assume a state variable x is assigned by both pre(a) and eff (b). By the definition of stratification, x is in layer L(b). However, since x is assigned by pre(a), there must be an edge from layer L(a) to layer L(x) = L(b) since L(a) 6= L(b). In this case, we know that L(a) < L(b) from the definition of stratification. Nevertheless, this contradicts with the assumption that L(a) > L(b). Thus, pre(a) \u2229 eff(b) = \u2205, and pre(a) and eff (b) are conflict-free. With all three conflict-free pairs, we have s : b \u21d2 a according to Theorem 2.\nAlthough SP reduces the search space by avoiding the expansion of certain actions, it is in fact not a stubborn set based reduction algorithm. We have the following theorem for the SP algorithm.\nDefinition 28. For a SAS+ planning task S, a valid path pa = (a1, \u00b7 \u00b7 \u00b7 , an) is an SP-path if and only if pa is a path in the search space of the SP algorithm applied to S.\nTheorem 6. For a SAS+ planning task S, for any initial s0 and any valid path pa = (a1, \u00b7 \u00b7 \u00b7 , an) from s0, there exists a path pb = (b1, \u00b7 \u00b7 \u00b7 , bn) from s0 such that pb is an SP-path, and both pa and pb lead to the same state from s0, and pb is a permutation of actions in pa.\nProof. We prove by induction on the number of actions. When n = 1, since there is no action before s0, any valid path (a1) will also be a valid path in the search space of the SP algorithm. Now we assume this proposition is true of for n = k, k \u2265 1 and prove the case when n = k + 1. For a valid path p0 = (a1, \u00b7 \u00b7 \u00b7 , ak, ak+1), by our induction hypothesis, we can rearrange the first k actions to obtain a path (a11, a 1 2, \u00b7 \u00b7 \u00b7 , a 1 k).\nNow we consider a new path p1 = (a11, \u00b7 \u00b7 \u00b7 , a 1 k, ak+1). There are two cases. First,\nif L(ak+1) < L(a 1 k), or L(ak+1) > L(a 1 k) and ak+1 is a follow-up action of a 1 k, then p1 is already an SP-path. Otherwise, we have L(ak+1) > L(a 1 k) and ak+1 is not a follow-up action of a1k. In this case, by Lemma 2, path p 1\u2032 = (a11, \u00b7 \u00b7 \u00b7 , a 1 k\u22121, ak+1, a 1 k is also a valid path that leads s to the same state as pa does. By the induction hypothesis, if p1 \u2032\nis still not an SP-path, we can rearrange the first k actions in p1 \u2032\nto get a new path p2 = (a21, \u00b7 \u00b7 \u00b7 , a 2 k, a 1 k). Otherwise we let\np2 = p1 \u2032 . Comparing p1 and p2, we know L(ak+1) > L(a 1 k), namely, the level value of the last action in p1 is strictly larger than that in p2. We can repeat the above process to generate p3, \u00b7 \u00b7 \u00b7 , pm, \u00b7 \u00b7 \u00b7 as long as pj(j \u2208 Z+) is not an SP-path. Our transformation from pj to pj+1 also ensures that every pj is a valid path from s and leads to the same state that pa does. Since we know that the layer value of the last action in each pj is monotonically decreasing as j increases, such a process must stop after a finite number of iterations. Suppose it finally stops at pm = (a\u20321, a \u2032 2, \u00b7 \u00b7 \u00b7 , a \u2032 k, a \u2032 k+1, we must have that L(a \u2032 k+1) \u2264 L(a\u2032k) or L(a \u2032 k+1) > L(a \u2032 k) and a \u2032 k+1 is a follow-up action of ak\u2032 . Hence, p\nm now is an SP-path. We then assign pm to pb and the induction step is proved.\nACM Journal Name, Vol. V, No. N, Month 20YY.\n\u00b7 15\nTheorem 6 shows that the SP algorithm cannot reduce the number of states expanded in the search space. The reason is as follows: for any state in the original search space that is reachable from the initial state s0 via a path p, there is still an SP-path that reaches s. Therefore, every reachable state in the search space is still reachable by the SP algorithm. In other words, SP reduces the number of generated states, but not the number of expanded states. SP is not a stubborn set based reduction algorithm. This can be illustrated by the following example. Assuming a SAS+ planning task S that contains two state variables x1 and x2, where both x1 and x2 have domain {0, 1}, with the initial state as {x1 = 0, x2 = 0} and the goal as {x1 = 1, x2 = 1}. Actions a and b are two actions in S where pre(a) is {x1 = 0} and eff (a) is {x1 = 1} and pre(b) is {x2 = 0} and eff (b) is {x2 = 1}. It is easy to see that a and b are not follow-up actions of each other, and that x1, x2 will be in different layers after stratification. Without loss of generality, we can assume L(a) = L(x1) > L(x2) = L(b). Therefore, we know that action b will not be expanded after action a in state s : {x1 = 1, x2 = 0}. However, apply(s, b) is the goal. Not expanding b in state s violates condition A2 in Definition 11 where any valid path from s to a goal state has to contain at least one action in the expansion set of s. We can also see in the above example that the search space explored by SP contains four states, namely, the initial state s0, apply(s0, a), apply(s0, b) and the goal state. Meanwhile, under the EC algorithm, in state s0, the DTGs for x1 and x2 are not in each other\u2019s dependency closures. This implies that in s0, EC expands either action a or b, but not both. Therefore, EC expands three states while SP expands four. This illustrates our conclusion in Theorem 6 that the SP algorithm cannot reduce the number of expanded states."}, {"heading": "5. A NEW POR ALGORITHM FRAMEWORK FOR PLANNING", "text": "We have developed a POR theory for planning and explained two previous POR algorithms using the theory. Now, based on the theory, we propose a new POR algorithm which is stronger than the previous EC algorithm. Our theory shows in Theorem 3 that the condition for enabling POR reduction is strongly related to left commutativity of actions. In fact, constructing a stubborn set can be reduced to finding a left commutativity set. As we show in Theorem 5, the EC algorithm follows this idea. However, the basic unit of reduction in EC is DTG (i.e. either all actions in a DTG are expanded or none of them are), which is not necessary according to our theory. Based on this insight, we propose a new algorithm that operates with the granularity of actions instead of DTGs.\nDefinition 29. For a state s, an action set L is a landmark action set if and only if any valid path starting from s to a goal state contains at least one action in L.\nDefinition 30. For a SAS+ task, an action a \u2208 O is supported by an action b if and only if pre(a) \u2229 eff(b) 6= \u2205.\nDefinition 31. For a state s, its action support graph (ASG) at s is defined as a directed graph in which each vertex is an action, and there is an edge from a\nACM Journal Name, Vol. V, No. N, Month 20YY.\n16 \u00b7\nto b if and only if a is not applicable in s and a is supported by b.\nThe above definition of ASG is a direct extension of the definition of a causal graph. Instead of having domains as basic units, here we directly use actions as basic units.\nDefinition 32. For an action a and a state s, the action core of a at s, denoted by ACs(a), is the set of actions that are in the transitive closure of a in ASG(s). The action core for a given set of actions A is the union of action cores of every action in A.\nLemma 3. For a state s, if an action a is not applicable in s and there is a valid path p starting from s whose last action is a, then p contains an action b, b 6= a, b \u2208 ACs(a).\nProof. We prove this by induction on the length of p. In the base case where |p| = 2, we assume p = (b, a). Since a is not applicable in s, it must be supported by b. Thus, b \u2208 ACs(a). Suppose this lemma is true for 2 \u2264 |p| \u2264 k \u2212 1, we prove the case for |p| = k. For a valid path p = (o1, . . . , ok), again there exists an action b before a that supports a. If b is applicable in s, then b \u2208 ACs(a). Otherwise, we have a path p\u2032 = (o1, . . . , b) with 2 \u2264 |p\u2032| \u2264 k\u22121. Thus, by the induction assumption, p\u2032 contains at least one action in ACs(b), which is a subset of ACs(a), according to Definition 31 and 32.\nDefinition 33. Given a SAS+ planning task \u03a0 with O as set of all actions O, for a state s and a set of action A, the action closure of action set A at s, denoted by by Cs(A), is a subset of O and a super set of A such that for any applicable action a \u2208 Cs(A) at s and any action b \u2208 O\\Cs(A), eff(a) and eff(b) are conflict-free. In addition, if pre(b) \u2208 S, eff(a) and pre(b) are conflict-free.\nIntuitively, actions in Cs(A) can be executed without affecting the completeness and optimality of search. Specifically, because any applicable action in Cs(A) and any action not in Cs(A) will not assign different values to the same state variable, for action a \u2208 Cs(A) and action b \u2208 O\\Cs(A) at s, path (a, b) will lead to the same state that (b, a) does. Additionally, because pre(b) and eff(a) are conflict-free when pre(b) \u2208 s, executing action a will not affect the applicability of action b in future. Therefore, actions in Cs(A) can be safely expanded first during the search, while actions outside it can be expanded later. A simple procedure, shown in Algorithm 1, can be used to find the action closure for a given action set A. The proposed POR algorithm, called stubborn action core (SAC), works as follows. At any given state s, the expansion set E(s) of state s is determined by Algorithm 2. There are various ways to find a landmark action set for a given state. Here we give one example that is used in our current implementation. To find a landmark action set L at s, we utilize the DTGs associated with the SAS+ formalism. We first find a transition set that includes all possible transitions (si, vi) in an unachieved goal-related DTG Gi where si is the current state of Gi in s. It is easy to see that all actions that mark transitions in this set make up a landmark action set, because\nACM Journal Name, Vol. V, No. N, Month 20YY.\n\u00b7 17\ninput : A SAS+ task with action set O, an action set A \u2286 Q, and a state s output: An action closure C(A) of A\nC(A) \u2190 A; repeat\nforeach action a in C(A) applicable in s do foreach action b in O\\C(A) do\nif pre(b) \u2229 s 6= \u2205 and pre(b) and eff(a) are not conflict-free then C(A) \u2190 C(A) \u222a {b} ;\nend if eff(b) and eff(a) are not conflict-free then C(A) \u2190 C(A) \u222a {b} ;\nend\nend\nend\nuntil C(A) is not changing; return C(A) ;\nAlgorithm 1: A procedure to find action closure\ninput : A SAS+ planning task and state s output: The expansion set E(s)\nFind a landmark action set L at s ; Calculate the action core ACs(L) of L using Algorithm 1; Use ACs(L) as E(s) ;\nAlgorithm 2: The SAC algorithm\nGi is unachieved and at least one action starting from si has to be performed in any solution plan. There are also other ways to find a landmark action set. For instance, the preprocessor in the LAMA planner [Richter et al. 2008] can be used to find landmark facts, and all actions that lead to these landmark facts also make up a landmark action set.\nTheorem 7. For a state s, the expansion set E(s) defined by the SAC algorithm is a stubborn set at s.\nProof. We first prove that our expansion set E(s) satisfies condition A1 in Definition 11, namely, for any action b \u2208 E(S), and actions b1, \u00b7 \u00b7 \u00b7 , bk /\u2208 E(s), if (b1, \u00b7 \u00b7 \u00b7 , bk, b) is a valid path from s, then (b, b1, \u00b7 \u00b7 \u00b7 , bk) is also a valid path, and leads to the same state that (b1, \u00b7 \u00b7 \u00b7 , bk, b) does. To simplify this proof, we can treat action sequence (b1, \u00b7 \u00b7 \u00b7 , bk) as a \u201cmacro\u201d action B where an assignment xt = vt in pre(B) if and only if xt = vt is in the precondition of some bi \u2208 B and xt = vt is not in the effects of a previous action bj(j < i), and an assignment xt = vt is in eff(B) if and only if xt = vt is in the effect set of some bi \u2208 B, and xt is not assigned to any value other than vt in the effects of later action bj(j > i). In the following proof, we use the macro action B in place of the path (b1, \u00b7 \u00b7 \u00b7 , bk).\nACM Journal Name, Vol. V, No. N, Month 20YY.\n18 \u00b7\nTo prove A1, we only need to prove that if (B, b) is a valid path, then s : b \u21d2 B. According to Theorem 4, s : b \u21d2 B if and only if the following four propositions are true. a) Action b must be applicable in s. We prove this by contradiction. Let s\u2032 = apply(s,B), if b is not applicable in s, but applicable in s\u2032, then B supports b. Since all effects of B are from actions in the path (b1, \u00b7 \u00b7 \u00b7 , bk), there exists an action bi \u2208 {b1, \u00b7 \u00b7 \u00b7 , bk} such that bi supports b. However, according to Definition 32, bi is in the transitive closure of b in ASG(s). According to our algorithm, bi should be in E(s). This contradicts with our assumption that bi /\u2208 E(s). Thus, b must be applicable at s. b) pre(B) and eff(b) are conflict-free. We prove this proposition by contradiction. If pre(B) and eff(b) are not conflict-free, we assume that pre(B) has xt = vt that conflicts with an assignment in eff(b). According to the way we define B, there exists an action bi \u2208 (b1, \u00b7 \u00b7 \u00b7 , bk), such that xt = vt. Also, since B is applicable in s, we know that xt takes the value vt at s also. Therefore, we know that pre(bi) and eff(b) are not conflict-free. However, according to Definition 33 and Algorithm 1, bi is in E(s). This contradicts with our assumption that bi is not in E(s). Thus, pre(B) and eff(b) are conflict-free. c) eff (B) and eff(b) are conflict-free. The proof of this proposition is very similar to the one above. If they are not conflict-free, we must have action bi \u2208 (b1, \u00b7 \u00b7 \u00b7 , bk), such that eff(b) and eff(bi) are not conflict-free. However, according to Definition 33 and Algorithm 1, bi is in E(s). This contradicts with our assumption that bi is not in E(s). Thus, eff(B) and eff(b) are conflict-free. d) pre(b) and eff(B) are conflict-free. This proposition is true as we assumed in condition A1 that (B, b) is a valid path from s. Thus, from Theorem 4, we see that s : b \u21d2 B and that condition A1 in Definition 11 is true. Now we verify condition A2 by showing that any solution path p from s contains at least one action in E(s). From the definition of landmark action sets, we know that there exists an action l \u2208 L such that p contains l. From Lemma 3 we know that ACs(l) contains at least one action, applicable in s, in p. Thus, E(s) indeed contains at least one action in p. Since E(s) satisfies conditions A1 and A2 in Definition 11, E(s) is a stubborn set in state s."}, {"heading": "5.1 SAC vs. EC", "text": "SAC gives stronger reduction than the previous EC algorithm, since it is based on actions, which have a finer granularity than DTGs do. Specifically, SAC gives more reduction than EC for two reasons. First, applicable actions that are not associated with landmark transitions, even if they are in the same DTG, are expanded by EC but not by SAC. Second, applicable actions that do not support any actions in the landmark action set, even if they are in the same DTG, are expanded by EC but not by SAC. To give an example, in Figure 4a, G1, G2, G3 are three DTGs. The goal assignment is marked as an unfilled circle in G1. a, b, c, d, e are actions. Dashed arrows denote the preconditions of actions. For instance, the lower dashed arrow means\nACM Journal Name, Vol. V, No. N, Month 20YY.\n\u00b7 19\nthat b requires a precondition x3 = w. In this example, according to EC, G1 is a goal DTG and G2 and G3 are in the dependency closure of G1. Thus, before executing a, EC expands every applicable action in G1, G2 and G3 at any state. SAC, on the other hand, starts with a singleton set {a} as the initial landmark action set and ignores action e. Applicable action c is also not included in the action closure in state s since it does not support a. The search graphs are compared in Figure 4 and we see that SAC gives stronger reduction."}, {"heading": "6. SYSTEM IMPLEMENTATION", "text": "We adopt the Fast Downward (FD) planning system [Helmert 2006] as our code base. The overall architecture of FD is described in Figure 5. A complete FD system contains three parts corresponding to three phases in execution: translation, knowledge compilation and search. Translation module will convert planning tasks\nACM Journal Name, Vol. V, No. N, Month 20YY.\n20 \u00b7\ndescribed into a SAS+ planning task. The knowledge compilation module will generate domain transition graphs and causal graph for the SAS+ planning task. The search module implements various state-space-search algorithms as well as heuristic functions. All these three modules communicate by temporary files. We make two additions to the above system to implement our SAC planning system, as shown in Figure 5. First, we add a \u201ccommutativity analysis\u201d module into the knowledge compilation step to identify commutativity between actions. Second, we add a \u201cspace reduction\u201d module to the search module to conduct state space reduction. The commutativity analysis module is used to build left commutativity relations between actions and build the action support graph. It reads action information from the output of knowledge compilation module and determines the left commutativity relations between actions according to conditions in Theorem 3. In addition, this module also determines if one action is supported by another and builds the action support graph defined in Definition 31. The reduction module for search is used to generate a stubborn set of a given state. We implement the SAC algorithm in this module. Starting from a landmark action set L as the target action set, we find the action closure ACs(L) iteratively add actions that support actions in the target action set to the target action set until it is not changing. We then use the applicable actions in the action closure as the set of actions to expand at s. In other words, in our SAC system, during the search, for any given state s, instead of using successor generator provided by FD to generate a set of applicable operators, we use the reduction module to generate a stubborn set in state s and use it as the expansion set. It is easy to see that the overall time complexity of determining left commutativity relationships between actions is O(|A|2) where |A| is the number of actions. We implement this module in Python. Since the number of actions |A| is usually not large, in most of the cases, the commutativity analysis module takes less than 1 second to finish. This module only runs once for solving a planning problem. Therefore, the commutativity analysis module amounts to an insignificant amount of overhead to the system. Theoretically, the worst case time complexity for finding the action closure is O(|A|2) where |A| is the number of actions. However, in practice, by choosing the landmark action set L that associated with transitions in an unarchived goal-related DTG starting from current state, the procedure of finding action closure terminates quickly after about 4 to 5 iterations. Therefore, adding the reduction module does not increase the overall search overhead significantly either. We implement this module in C++ and incorporate it into the search module of FD."}, {"heading": "7. EXPERIMENTAL RESULTS", "text": "We test our algorithm on problems in the recent International Planning Competitions (IPCs): IPC4, and IPC5. We implemented our algorithm on top of the Fast Downward (FD) planner [Helmert 2006]. We only modified the state expansion part. We have implemented our SAC algorithm and tested it along with Fast Downward and its combination with the EC extension on a Red Hat Linux server with 2Gb memory and one 2.0GHz CPU. The admissible HSP hmax heuristic [Bonet and\nACM Journal Name, Vol. V, No. N, Month 20YY.\n\u00b7 21\nGeffner 2001] and inadmissible Fast Forward (FF) heuristic [Hoffmann and Nebel 2001] are used in our experiments. First, we apply our SAC algorithm to A\u2217 search with the HSP hmax heuristic [Bonet and Geffner 2001]. We also turn off the option of preferred operators [Helmert 2006] since it compromises the optimality of A\u2217 search. Table I shows the detailed results on node expansion and generation during the search. We also compare the solving times of these three algorithms. As we can clearly see from\nTable I, the numbers of expanded nodes of the SAC-enhanced A\u2217 algorithm are consistently lower than those of the baseline A\u2217 algorithm and the EC-enhanced A\u2217 algorithm. There are some cases where the generated nodes of the SAC-enhanced algorithm are slightly larger than those of the baseline A\u2217 or EC-enhanced A\u2217 algorithm. This is possible due to the tie-breaking of states with equal heuristic values during search.\nWe can also see that the computational overhead of SAC is low. For instance, in the Freecell domain, the running time of the SAC-enhanced algorithm is only slightly higher than the baseline and lower than the EC-enhanced algorithm, despite their equal number of expanded and generated nodes. Aside from the A\u2217 algorithm, we also test SAC on best-first search algorithms. Although POR preserves completeness and optimality, it can also be combined with suboptimal searches such as best-first search to reduce their search space. In this comparison, we turned off the option of preferred operators in our experiment for FD. Preferred operator is another space reduction method that does not preserve completeness, and using it with EC or SAC will lead to worse performance. We will investigate how to find synergy between these two approaches in our future work. We summarize the performance of three algorithms, original Fast Downward (FD), FD with EC, and FD with SAC, in Table II by presenting the number of problem instances in a planning domain that can be solved within 1800 seconds by each solver. We also ignore small problem instances with solving time less than 0.01 seconds. All there solvers uses inadmissible Fast Forward (FF) heuristic. As we can see from Table II, when combined with a best-first-search algorithm, SAC can still reduce the number of generated and expanded nodes compared to the baseline FD algorithm and the EC-enhanced algorithm. In many problems (e.g. pipesworld18, tpp15, truck13), the saving on the number of expanded states can be of orders of magnitude."}, {"heading": "8. RELATED WORK", "text": "We discuss some related work in this section."}, {"heading": "8.1 Symmetry", "text": "Symmetry detection is another way for reducing the search space [Fox and Long 1999]. From the view of node expansion on a SAS+ formalism for planning, we can see that symmetry removal is different from SAC. For example, consider a domain with three objects A1, A2, and B, where A1 and A2 are symmetric, and actions associated with B have no conflict with any actions associated with A1 or A2. In this case, symmetry removal will expand actions associated with (A1 and B) or (A2 and B), whereas SAC will only expand actions associated with the DTG for B if we pick a landmark action set based on it. This is because both the action\nACM Journal Name, Vol. V, No. N, Month 20YY.\n22 \u00b7\ncore and the action closure will not include any actions associated with A1 or A2 as they have no conflict with actions in B.\nIntuitively, symmetric removal finds that it is not important whether A1 or A2 is used since they are symmetric, whereas SAC finds that it is not important whether actions associated with B is used before or after actions associated with Ai, i = 1, 2 since there is no conflict. In fact, SAC can also detect stronger relationships such as the fact that it is safe to use actions associated with B before those associated with Ai, since any path that uses actions associated with Ai before those associated with B corresponds to another valid path with the same cost.\nFurther, there is limited research on domain-independent detection and removal of symmetry. The method by Fox and Long [Fox and Long 1999] detects symmetry from the specification of initial and goal states and may miss many symmetries."}, {"heading": "8.2 Factored planning", "text": "Factored planning [Amir and Engelhardt 2003; Brafman and Domshlak 2006; Kelareva et al. 2007] is a class of search algorithms that exploits the decomposition of state space. In essence, factored planning finds all the subplans for each individual subgraph and tries to merge them. There are some limitations of factored planning.\nFirst, for some problems with dense subgraphs, the number of subplans in each subgraph may be very large, making the search very expensive. What is worse is that there are many subgraphs in which the goal is not specified, leading to more subplans that need to be considered. We have done some empirical study on this matter. For example, for pipesworld20, there are 96 DTGs, 18 out of which have goal facts. Even if we only consider one DTG and apply the canonicality assumption that each state can be included at most once in any subplan, the number of subplans from the initial state to the goal can be as high as 1.96\u00d7109 in DTG #16 generated by Fast Downward. The number is high because there are multiple transition paths, and each transition can be associated with many actions. If we multiply the numbers of possible subplans of the 18 DTGs containing goals, the number will approximately be of the order of 10120. Thus, the search space will be extremely large if we consider all the 96 DTGs (78 of which do not even have a goal state) and remove the canonicality assumption. Of course, techniques such as tree search and pruning [Kelareva et al. 2007] can speed up the process but the potential speedup is largely unknown.\nSecond, since the canonicality assumption is generally not true for many domains, and there are potentially infinite number of subplans without restriction on the subplan length, the factored planning algorithm needs to use certain schemes such as iterative deepening [Brafman and Domshlak 2006] to restrict the subplan length. These schemes further increase the complexity and may compromise the global optimality of the resulting plan [Kelareva et al. 2007].\nIn summary, although factored planning has shown potential on some domaindependent studies, its practicality for general domain-independent planning has not been established yet. We note that POR algorithms we studied in this paper are not exclusive to factored planning and it is possible that POR can be integrated into factored planning to reduce the cost of search.\nACM Journal Name, Vol. V, No. N, Month 20YY.\n\u00b7 23"}, {"heading": "8.3 Planning utilizing the SAS+ decomposition", "text": "Our POR method is based on the SAS+ representation [Helmert 2006]. Recently there has been increasing interest in utilizing the SAS+ representation. The Fast Downward planner [Helmert 2006] develops its heuristic function by analyzing the causal graphs on top of the SAS+ models. Another SAS+ based heuristic for optimal planning is recently obtained via a linear programming model encoding DTGs [van den Briel et al. 2007]. The LAMA planner derives inadmissible heuristic values by analyzing landmarks in SAS+ models [Richter et al. 2008]. An admissible version of it is proposed in [Karpas and Domshlak 2009] by using action cost partitioning. Yet another admissible heuristic called \u2018mergeand-shrink\u2019 is developed based on abstraction of domain transitions [Helmert et al. 2007], which strictly dominates the admissible landmark heuristics [Helmert and Domshlak 2009]. Moreover, long-distance mutual exclusion constraints based on a DTG analysis is proposed and shown to be effective in speeding up SAT-based optimal planners [Chen et al. 2009]. The DTG-Plan planner searches directly on the space of DTGs in a hierarchical decomposition fashion [Chen et al. 2008]. The algorithm is shown to be fast but is not complete or optimal. Comparing to the above recent work, POR offers a completely new approach to exploit the state-space decomposition in the SAS+ representation. It is orthogonal to the design of better heuristics and it provides a systematical, theoretically sound way to reduce search costs. POR is most effective for problems where the action support graphs are directional and the inter-action dependencies are not dense. It may not be useful for problems where the actions are strongly connected and there is a high degree of inter-action dependencies. For example, it is not useful for the 15-puzzle where each action on each piece is supported by surrounding actions, which makes the action support graph strongly connected. In this case, POR cannot give reductions during the search."}, {"heading": "9. CONCLUSIONS AND FUTURE WORK", "text": "Previous work in both model checking and AI planning has demonstrated that POR is a powerful method for reducing search costs. POR is an enabling technique for modeling checking, which will not be practical without POR due to its high complexity. Although POR has been extensively studied for model checking, its theory has not been developed for AI planning. In this paper, we developed a new POR theory for planning that is parallel to the stubborn set theory in model checking. In addition, by analyzing the structure of actions in planning problems, we derived a practical criterion that defines left commutativity between actions. Based on the notion of left commutativity, we developed sufficient conditions for finding stubborn sets during search for planning. Furthermore, we applied our theory to explain two previous POR algorithms for planning. The explanation provided useful insights that lead to a stronger and more efficient POR algorithm called SAC. Compared to previous POR algorithms, SAC finds stubborn sets based on a finer granularity for checking left commutativity, leading to strong reduction. We compared the performance of SAC to the previously\nACM Journal Name, Vol. V, No. N, Month 20YY.\n24 \u00b7\nproposed EC algorithm on both optimal and non-optimal state space searches. Experimental results showed that the proposed SAC algorithm led to significantly stronger node reduction and less overhead. In our future work, we plan to develop stronger POR algorithms for planning based on our theoretical framework and study its interaction with other search reduction techniques such as preferred operators [Helmert 2006], abstraction heuristics [Helmert et al. 2007], landmarks [Richter et al. 2008], and symmetry detection [Fox and Long 1999]."}], "references": [{"title": "Factored planning", "author": ["E. Amir", "B. Engelhardt"], "venue": "Proc. IJCAI.", "citeRegEx": "Amir and Engelhardt,? 2003", "shortCiteRegEx": "Amir and Engelhardt", "year": 2003}, {"title": "Planning as heuristic search", "author": ["B. Bonet", "H. Geffner"], "venue": "Artificial Intelligence, Special issue on Heuristic Search 129, 1.", "citeRegEx": "Bonet and Geffner,? 2001", "shortCiteRegEx": "Bonet and Geffner", "year": 2001}, {"title": "Factored planning: How, when, and when not", "author": ["R. Brafman", "C. Domshlak"], "venue": "Proc. AAAI.", "citeRegEx": "Brafman and Domshlak,? 2006", "shortCiteRegEx": "Brafman and Domshlak", "year": 2006}, {"title": "An lp-based heuristic for optimal planning", "author": ["M.V.D. Briel", "J. Benton", "S. Kambhampati"], "venue": "13th International Conference on Principles and Practice of Constraint Programming. 651\u2013665.", "citeRegEx": "Briel et al\\.,? 2007", "shortCiteRegEx": "Briel et al\\.", "year": 2007}, {"title": "Fast planning by search in domain transition graphs", "author": ["Y. Chen", "R. Huang", "W. Zhang"], "venue": "Proc. AAAI.", "citeRegEx": "Chen et al\\.,? 2008", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "Stratified planning", "author": ["Y. Chen", "Y. Xu", "G. Yao"], "venue": "Proc. IJCAI.", "citeRegEx": "Chen et al\\.,? 2009", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Completeness and optimality preserving reduction for planning", "author": ["Y. Chen", "G. Yao"], "venue": "Proc. IJCAI.", "citeRegEx": "Chen and Yao,? 2009", "shortCiteRegEx": "Chen and Yao", "year": 2009}, {"title": "Long distance mutual exclusion for planning", "author": ["Y. Chen", "X. Zhao", "W. Zhang"], "venue": "Artificial Intelligence 173, 2, 365\u2013391.", "citeRegEx": "Chen et al\\.,? 2009", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Model Checking", "author": ["E.M. Clarke", "O. Grumberg", "D.A. Peled"], "venue": "The MIT Press.", "citeRegEx": "Clarke et al\\.,? 2000", "shortCiteRegEx": "Clarke et al\\.", "year": 2000}, {"title": "The detection and exploitation of symmetry in planning problems", "author": ["M. Fox", "D. Long"], "venue": "Proc. IJCAI.", "citeRegEx": "Fox and Long,? 1999", "shortCiteRegEx": "Fox and Long", "year": 1999}, {"title": "The Fast Downward planning system", "author": ["M. Helmert"], "venue": "Journal of Artificial Intelligence Research 26, 191\u2013246.", "citeRegEx": "Helmert,? 2006", "shortCiteRegEx": "Helmert", "year": 2006}, {"title": "Landmarks, critical paths and abstractions: What\u2019s the difference anyway? In Proc", "author": ["M. Helmert", "C. Domshlak"], "venue": "ICAPS.", "citeRegEx": "Helmert and Domshlak,? 2009", "shortCiteRegEx": "Helmert and Domshlak", "year": 2009}, {"title": "Flexible abstraction heuristics for optimal sequential planning", "author": ["M. Helmert", "P. Haslum", "J. Hoffmann"], "venue": "Proc. ICAPS. 176\u2013183.", "citeRegEx": "Helmert et al\\.,? 2007", "shortCiteRegEx": "Helmert et al\\.", "year": 2007}, {"title": "How good is almost perfect", "author": ["M. Helmert", "G. R\u00f6ger"], "venue": "Proc. AAAI.", "citeRegEx": "Helmert and R\u00f6ger,? 2008", "shortCiteRegEx": "Helmert and R\u00f6ger", "year": 2008}, {"title": "The FF planning system: Fast plan generation through heuristic search", "author": ["J. Hoffmann", "B. Nebel"], "venue": "Journal of Artificial Intelligence Research 14, 253\u2013302.", "citeRegEx": "Hoffmann and Nebel,? 2001", "shortCiteRegEx": "Hoffmann and Nebel", "year": 2001}, {"title": "The model checker spin", "author": ["G.J. Holzmann"], "venue": "IEEE Trans. on Software Engineering 23, 279\u2013295.", "citeRegEx": "Holzmann,? 1997", "shortCiteRegEx": "Holzmann", "year": 1997}, {"title": "State-variable planning under structural restrictions: Algorithms and complexity", "author": ["P. Jonsson", "C. B\u00e4ckstr\u00f6m"], "venue": "Artificial Intelligence 100, 1-2, 125\u2013176.", "citeRegEx": "Jonsson and B\u00e4ckstr\u00f6m,? 1998", "shortCiteRegEx": "Jonsson and B\u00e4ckstr\u00f6m", "year": 1998}, {"title": "Cost-optimal planning with landmarks", "author": ["E. Karpas", "C. Domshlak"], "venue": "Proceedings of the 21st international jont conference on Artifical intelligence. Morgan Kaufmann Publishers Inc., 1728\u20131733.", "citeRegEx": "Karpas and Domshlak,? 2009", "shortCiteRegEx": "Karpas and Domshlak", "year": 2009}, {"title": "Factored planning using decomposition trees", "author": ["E. Kelareva", "O. Buffet", "J. Huang", "S. Thi\u00e9baux"], "venue": "Proc. IJCAI.", "citeRegEx": "Kelareva et al\\.,? 2007", "shortCiteRegEx": "Kelareva et al\\.", "year": 2007}, {"title": "Landmarks revisited", "author": ["S. Richter", "M. Helmert", "M. Westphal"], "venue": "Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence. AAAI Press.", "citeRegEx": "Richter et al\\.,? 2008", "shortCiteRegEx": "Richter et al\\.", "year": 2008}, {"title": "State space generation: Efficiency and practicality", "author": ["A. Valmari"], "venue": "Ph.D. thesis, Tampere University of Technology Publications.", "citeRegEx": "Valmari,? 1988", "shortCiteRegEx": "Valmari", "year": 1988}, {"title": "Stubborn sets for reduced state space generation", "author": ["A. Valmari"], "venue": "Proceedings of the 10th International Conference on Applications and Theory of Petri Nets.", "citeRegEx": "Valmari,? 1989", "shortCiteRegEx": "Valmari", "year": 1989}, {"title": "A stubborn attack on state explosion", "author": ["A. Valmari"], "venue": "Proceedings of the 1990 International Workshop on Computer Aided Verification.", "citeRegEx": "Valmari,? 1990", "shortCiteRegEx": "Valmari", "year": 1990}, {"title": "Stubborn sets of coloured petri nets", "author": ["A. Valmari"], "venue": "Proceedings of the 12th International Conference on Application and Theory of Petri Nets. 102\u2013121.", "citeRegEx": "Valmari,? 1991", "shortCiteRegEx": "Valmari", "year": 1991}, {"title": "On-the-fly verification with stubborn sets", "author": ["A. Valmari"], "venue": "CAV \u201993: Proceedings of the 5th International Conference on Computer Aided Verification. Springer-Verlag, London, UK, 397\u2013408.", "citeRegEx": "Valmari,? 1993", "shortCiteRegEx": "Valmari", "year": 1993}, {"title": "The state explosion problem", "author": ["A. Valmari"], "venue": "Lectures on Petri Nets I: Basic Models, Lecture Notes in Computer Science 1491, 429\u2013528.", "citeRegEx": "Valmari,? 1998", "shortCiteRegEx": "Valmari", "year": 1998}, {"title": "An LP-based heuristic for optimal planning", "author": ["M. van den Briel", "J. Benson", "S. Kambhampati", "T. Vossen"], "venue": "In Proc. Constraint Programming Conference", "citeRegEx": "Briel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Briel et al\\.", "year": 2007}, {"title": "On stubborn sets in the verification of linear time temporal properties", "author": ["K. Varpaaniemi"], "venue": "Formal Methods in System Design 26, 1, 45\u201367.", "citeRegEx": "Varpaaniemi,? 2005", "shortCiteRegEx": "Varpaaniemi", "year": 2005}], "referenceMentions": [{"referenceID": 3, "context": "For example, new heuristic functions have recently been developed by analyzing the domain transition graphs (DTGs) and causal graphs on top of the SAS+ formalism [Briel et al. 2007; Helmert and R\u00f6ger 2008].", "startOffset": 162, "endOffset": 205}, {"referenceID": 13, "context": "For example, new heuristic functions have recently been developed by analyzing the domain transition graphs (DTGs) and causal graphs on top of the SAS+ formalism [Briel et al. 2007; Helmert and R\u00f6ger 2008].", "startOffset": 162, "endOffset": 205}, {"referenceID": 13, "context": "As shown by recent work, search even with almost perfect heuristic guidance may still lead to very high search cost [Helmert and R\u00f6ger 2008].", "startOffset": 116, "endOffset": 140}, {"referenceID": 5, "context": "Recently, partial order based reduction (POR), a new way to reduce the search cost from an orthogonal perspective, has been studied for classical planning [Chen et al. 2009; Chen and Yao 2009].", "startOffset": 155, "endOffset": 192}, {"referenceID": 6, "context": "Recently, partial order based reduction (POR), a new way to reduce the search cost from an orthogonal perspective, has been studied for classical planning [Chen et al. 2009; Chen and Yao 2009].", "startOffset": 155, "endOffset": 192}, {"referenceID": 16, "context": "In this paper, we work on the SAS+ formalism [Jonsson and B\u00e4ckstr\u00f6m 1998] of classical planning.", "startOffset": 45, "endOffset": 73}, {"referenceID": 27, "context": "PARTIAL ORDER REDUCTION THEORY FOR PLANNING Partial order based reduction (POR) algorithms have been extensively studied for model checking [Varpaaniemi 2005; Clarke et al. 2000], which also requires examining a state space in order to prove certain properties.", "startOffset": 140, "endOffset": 178}, {"referenceID": 8, "context": "PARTIAL ORDER REDUCTION THEORY FOR PLANNING Partial order based reduction (POR) algorithms have been extensively studied for model checking [Varpaaniemi 2005; Clarke et al. 2000], which also requires examining a state space in order to prove certain properties.", "startOffset": 140, "endOffset": 178}, {"referenceID": 15, "context": "Without POR, model checking would be too expensive to be practical [Holzmann 1997].", "startOffset": 67, "endOffset": 82}, {"referenceID": 20, "context": "2 Stubborn set theory for planning Although there are many variations of POR methods, a popular and representative POR algorithm is the stubborn set method [Valmari 1988; 1989; 1990; 1998; 1991; 1993], used for model checking based on Petri nets.", "startOffset": 156, "endOffset": 200}, {"referenceID": 6, "context": "EXPLANATION OF PREVIOUS POR ALGORITHMS Previously, we have proposed two POR algorithms for planning: expansion core (EC) [Chen and Yao 2009] and stratified planning (SP) [Chen et al.", "startOffset": 121, "endOffset": 140}, {"referenceID": 5, "context": "EXPLANATION OF PREVIOUS POR ALGORITHMS Previously, we have proposed two POR algorithms for planning: expansion core (EC) [Chen and Yao 2009] and stratified planning (SP) [Chen et al. 2009], both of which showed good performance in reducing the search space.", "startOffset": 170, "endOffset": 188}, {"referenceID": 6, "context": "Full details of the two algorithms can be found in our papers [Chen and Yao 2009; Chen et al. 2009].", "startOffset": 62, "endOffset": 99}, {"referenceID": 5, "context": "Full details of the two algorithms can be found in our papers [Chen and Yao 2009; Chen et al. 2009].", "startOffset": 62, "endOffset": 99}, {"referenceID": 5, "context": "2 Explanation of SP The stratified planning (SP) algorithm exploits commutativity of actions directly [Chen et al. 2009].", "startOffset": 102, "endOffset": 120}, {"referenceID": 19, "context": "For instance, the preprocessor in the LAMA planner [Richter et al. 2008] can be used to find landmark facts, and all actions that lead to these landmark facts also make up a landmark action set.", "startOffset": 51, "endOffset": 72}, {"referenceID": 10, "context": "SYSTEM IMPLEMENTATION We adopt the Fast Downward (FD) planning system [Helmert 2006] as our code base.", "startOffset": 70, "endOffset": 84}, {"referenceID": 10, "context": "We implemented our algorithm on top of the Fast Downward (FD) planner [Helmert 2006].", "startOffset": 70, "endOffset": 84}, {"referenceID": 14, "context": "Geffner 2001] and inadmissible Fast Forward (FF) heuristic [Hoffmann and Nebel 2001] are used in our experiments.", "startOffset": 59, "endOffset": 84}, {"referenceID": 1, "context": "First, we apply our SAC algorithm to A search with the HSP hmax heuristic [Bonet and Geffner 2001].", "startOffset": 74, "endOffset": 98}, {"referenceID": 10, "context": "We also turn off the option of preferred operators [Helmert 2006] since it compromises the optimality of A search.", "startOffset": 51, "endOffset": 65}, {"referenceID": 9, "context": "1 Symmetry Symmetry detection is another way for reducing the search space [Fox and Long 1999].", "startOffset": 75, "endOffset": 94}, {"referenceID": 9, "context": "The method by Fox and Long [Fox and Long 1999] detects symmetry from the specification of initial and goal states and may miss many symmetries.", "startOffset": 27, "endOffset": 46}, {"referenceID": 0, "context": "2 Factored planning Factored planning [Amir and Engelhardt 2003; Brafman and Domshlak 2006; Kelareva et al. 2007] is a class of search algorithms that exploits the decomposition of state space.", "startOffset": 38, "endOffset": 113}, {"referenceID": 2, "context": "2 Factored planning Factored planning [Amir and Engelhardt 2003; Brafman and Domshlak 2006; Kelareva et al. 2007] is a class of search algorithms that exploits the decomposition of state space.", "startOffset": 38, "endOffset": 113}, {"referenceID": 18, "context": "2 Factored planning Factored planning [Amir and Engelhardt 2003; Brafman and Domshlak 2006; Kelareva et al. 2007] is a class of search algorithms that exploits the decomposition of state space.", "startOffset": 38, "endOffset": 113}, {"referenceID": 18, "context": "Of course, techniques such as tree search and pruning [Kelareva et al. 2007] can speed up the process but the potential speedup is largely unknown.", "startOffset": 54, "endOffset": 76}, {"referenceID": 2, "context": "Second, since the canonicality assumption is generally not true for many domains, and there are potentially infinite number of subplans without restriction on the subplan length, the factored planning algorithm needs to use certain schemes such as iterative deepening [Brafman and Domshlak 2006] to restrict the subplan length.", "startOffset": 268, "endOffset": 295}, {"referenceID": 18, "context": "These schemes further increase the complexity and may compromise the global optimality of the resulting plan [Kelareva et al. 2007].", "startOffset": 109, "endOffset": 131}, {"referenceID": 10, "context": "3 Planning utilizing the SAS+ decomposition Our POR method is based on the SAS+ representation [Helmert 2006].", "startOffset": 95, "endOffset": 109}, {"referenceID": 10, "context": "The Fast Downward planner [Helmert 2006] develops its heuristic function by analyzing the causal graphs on top of the SAS+ models.", "startOffset": 26, "endOffset": 40}, {"referenceID": 19, "context": "The LAMA planner derives inadmissible heuristic values by analyzing landmarks in SAS+ models [Richter et al. 2008].", "startOffset": 93, "endOffset": 114}, {"referenceID": 17, "context": "An admissible version of it is proposed in [Karpas and Domshlak 2009] by using action cost partitioning.", "startOffset": 43, "endOffset": 69}, {"referenceID": 12, "context": "Yet another admissible heuristic called \u2018mergeand-shrink\u2019 is developed based on abstraction of domain transitions [Helmert et al. 2007], which strictly dominates the admissible landmark heuristics [Helmert and Domshlak 2009].", "startOffset": 114, "endOffset": 135}, {"referenceID": 11, "context": "2007], which strictly dominates the admissible landmark heuristics [Helmert and Domshlak 2009].", "startOffset": 67, "endOffset": 94}, {"referenceID": 5, "context": "Moreover, long-distance mutual exclusion constraints based on a DTG analysis is proposed and shown to be effective in speeding up SAT-based optimal planners [Chen et al. 2009].", "startOffset": 157, "endOffset": 175}, {"referenceID": 4, "context": "The DTG-Plan planner searches directly on the space of DTGs in a hierarchical decomposition fashion [Chen et al. 2008].", "startOffset": 100, "endOffset": 118}, {"referenceID": 10, "context": "In our future work, we plan to develop stronger POR algorithms for planning based on our theoretical framework and study its interaction with other search reduction techniques such as preferred operators [Helmert 2006], abstraction heuristics [Helmert et al.", "startOffset": 204, "endOffset": 218}, {"referenceID": 12, "context": "In our future work, we plan to develop stronger POR algorithms for planning based on our theoretical framework and study its interaction with other search reduction techniques such as preferred operators [Helmert 2006], abstraction heuristics [Helmert et al. 2007], landmarks [Richter et al.", "startOffset": 243, "endOffset": 264}, {"referenceID": 19, "context": "2007], landmarks [Richter et al. 2008], and symmetry detection [Fox and Long 1999].", "startOffset": 17, "endOffset": 38}, {"referenceID": 9, "context": "2008], and symmetry detection [Fox and Long 1999].", "startOffset": 30, "endOffset": 49}], "year": 2011, "abstractText": "Search is a major technique for planning. It amounts to exploring a state space of planning domains typically modeled as a directed graph. However, prohibitively large sizes of the search space make search expensive. Developing better heuristic functions has been the main technique for improving search efficiency. Nevertheless, recent studies have shown that improving heuristics alone has certain fundamental limits on improving search efficiency. Recently, a new direction of research called partial order based reduction (POR) has been proposed as an alternative to improving heuristics. POR has shown promise in speeding up searches. POR has been extensively studied in model checking research and is a key enabling technique for scalability of model checking systems. Although the POR theory has been extensively studied in model checking, it has never been developed systematically for planning before. In addition, the conditions for POR in the model checking theory are abstract and not directly applicable in planning. Previous works on POR algorithms for planning did not establish the connection between these algorithms and existing theory in model checking. In this paper, we develop a theory for POR in planning. The new theory we develop connects the stubborn set theory in model checking and POR methods in planning. We show that previous POR algorithms in planning can be explained by the new theory. Based on the new theory, we propose a new, stronger POR algorithm. Experimental results on various planning domains show further search cost reduction using the new algorithm.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}