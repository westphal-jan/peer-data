{"id": "1109.1844", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2011", "title": "Weighted Clustering", "abstract": "In this information i investigate topology it the weighted action, it which alone systems. not battalions man real valued stress. We ensure a foundational underlying on held influence of excluding suggest on standard semigroups algorithms in each of part partitional other contexts settings, encapsulating the accuracy conditions by into include algorithms intervene supposed exercise, and karyotypes clustering example away three clear variety: increased - responsive, weight - continue, their extra - robust. Our aspects gives several interesting questions of we it taken longitude to the melody unweighted work.", "histories": [["v1", "Thu, 8 Sep 2011 20:53:54 GMT  (12kb)", "https://arxiv.org/abs/1109.1844v1", null], ["v2", "Tue, 4 Oct 2016 08:33:09 GMT  (44kb,D)", "http://arxiv.org/abs/1109.1844v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["margareta ackerman", "shai ben-david", "simina br\u00e2nzei", "david loker"], "accepted": true, "id": "1109.1844"}, "pdf": {"name": "1109.1844.pdf", "metadata": {"source": "CRF", "title": "Weighted Clustering", "authors": ["Margareta Ackerman", "Shai Ben-David", "David Loker"], "emails": ["mackerman@fsu.edu", "shai@cs.uwaterloo.ca", "simina.branzei@gmail.com", "dloker@cs.uwaterloo.ca"], "sections": [{"heading": null, "text": "Until now these properties focused on advantages of classical Linkage-Based algorithms, failing to identify when other clustering paradigms, such as popular center-based methods, are preferable. We present surprisingly simple new properties that delineate the differences between common clustering paradigms, which clearly and formally demonstrates advantages of centerbased approaches for some applications. These properties address how sensitive algorithms are to changes in element frequencies, which we capture in a generalized setting where every element is associated with a real-valued weight.\n\u2217E-mail: mackerman@fsu.edu \u2020E-mail: shai@cs.uwaterloo.ca \u2021E-mail: simina.branzei@gmail.com \u00a7E-mail: dloker@cs.uwaterloo.ca\nar X\niv :1\n10 9.\n18 44\nv2 [\ncs .L\nG ]\n4 O"}, {"heading": "1 Introduction", "text": "Although clustering is one of the most useful data mining tools, it suffers from a substantial disconnect between theory and practice. Clustering is applied in a wide range of disciplines, from astronomy to zoology, yet its theoretical underpinnings are still poorly understood. Even the fairy basic problem of which algorithm to select for a given application (known as \u201cthe user\u2019s dilemma\u201d) is left to ad hoc solutions, as theory is only starting to address fundamental differences between clustering methods ([2, 6, 3, 1]). Indeed, issues of running time complexity and space usage are still the primary considerations when choosing clustering techniques. Yet, for clustering, such considerations are inadequacy. Different clustering algorithms often produce radically different results on the same input, and as such, differences in their input-output behavior should take precedence over computational concerns.\n\u201cThe user\u2019s dilemma,\u201d has been tackled since the 70s ([9, 21]), yet we still do not have an adequate solution. A formal approach to this problem (see, for example, [9, 6, 3]) proposes that we rely on succinct mathematical properties that reveal fundamental differences in the input-output behaviour of different clustering algorithms. However there is a serious shortcoming with the current state of this literature. Virtually all the properties proposed in this framework highlight the advantages of linkage-based methods, most of which are satisfied by single-linkage \u2013 an algorithm that often performs poorly in practice. If one were to rely on existing properties to try to select a clustering algorithm, they would inevitably select a linkage-based technique. According to these properties, there is never a reason to choose, say, algorithms based on the k-means objective ([17]), which often performs well in practice.\nOf course, practitioners of clustering have known for a long time that, for many applications, variations of the k-means method outperform classical linkage-based techniques. Yet a lack of clarity as to why this is the case leaves the \u201cthe user\u2019s dilemma\u201d largely unsolved. Despite continued efforts to find better clustering methods, the ambiguous nature of clustering precludes the existence of a single algorithm that will be suited for all applications. As such, generally successful methods, such as popular algorithms for the k-means objective, are ill-suited for some applications. To this end, it is necessary for users of clustering to understand how clustering paradigms differ in their input-output behavior.\nUnfortunately, informal recommendations are not sufficient. Many such recommendations advise to use k-means when the true clusters are spherical and to apply single-linkage when they may possess arbitrary shape. Such advice can be misguiding, as clustering users know that single-linkage can fail to detect arbitrary-shaped clusters, and k-means does not always succeed when clusters are spherical. Further insight comes from viewing data as a mixture model (when variations of k-means, particularly EM, are known to perform well), but unfortunately most clustering users simply don\u2019t know how their data is generated. Another common way to differentiate clustering methods is to partition them into partitional and hierarchical and to imply that users should choose algorithms based on this consideration. Although the format of the output is important, is does not go to the heart of the matter, as most clustering approaches can be expressed in both frameworks.1\nThe lack of formal understanding of the key differences between clustering paradigms leaves users at a loss when selecting algorithms. In practice, many users give up on clustering altogether\n1For example, k-means can be reconfigured to output a dendrogram using Ward\u2019s method and Bisecting k-mean, and classical hierarchical methods can be terminated using a variety of termination conditions ([14]) to obtain a single partition instead of a dendrogram.\nwhen a single algorithm that had been successful on a different data set fails to attain a satisfactory clustering on the current data. Not realizing the degree to which algorithms differ, and the ways in which they differ, often prevents users from selecting appropriate algorithms, or even sampling a few diverse methods.\nThis, of course, need not be the case. A set of simple, succinct properties can go a long way towards differentiating between clustering techniques and assisting users when choosing a method. As mentioned earlier, the set of previously proposed properties is inadequacy. This paper identifies the first set of properties that differentiates between some of the most popular clustering methods, while highlighting potential advantages of k-means (and similar) methods. The properties are very simple, and go to the heart of the difference between some clustering methods. However, the reader should keep in mind that they are not necessarily sufficient, and that in order to have a complete solution to \u201cthe user\u2019s dilemma\u201d we need additional properties that identify other ways in which clustering techniques differ. The ultimate goal is to have a small set of complementary properties that together aid in the selection of clustering techniques for a wide range of applications.\nThe properties proposed in this paper center around the rather basic concept of how different clustering methods react to element duplication. This leads to three surprisingly simple categories, each highlighting when some clustering paradigms should be used over others. To this end, we consider a generalization of the notion of element duplication by casting the clustering problem in the weighted setting, where each element is associated with a real valued weight. Instances in the classical model can be readily mapped to the weighted framework by replacing duplicates with integer weights representing the number of occurrences of each data point.\nThis generalized setting enables more accurate representation of some clustering instances. Consider, for instance, vector quantification, which aims to find a compact encoding of signals that has low expected distortion. The accuracy of the encoding is most important for signals that occur frequently. With weighted data, such a consideration is easily captured by having the weights of the points represent signal frequencies. When applying clustering to facility allocation, such as the placement of police stations in a new district, the distribution of the stations should enable quick access to most areas in the district. However, the accessibility of different landmarks to a station may have varying importance. The weighted setting enables a convenient method for prioritizing certain landmarks over others.\nWe formulate intuitive properties that may allow a user to select an algorithm based on how it treats weighted data (or, element duplicates). These surprisingly simple properties are able to distinguish between classes of clustering techniques and clearly delineate instances in which some methods are preferred over others, without having to resort to assumptions about how the data may have been generated. As such, they may aid in the clustering selection process for clustering users at all levels of expertise.\nBased on these properties we obtain a classification of clustering algorithms into three categories: those that are affected by weights on all data sets, those that ignore weights, and those methods that respond to weights on some configurations of the data but not on others. Among the methods that always respond to weights are several well-known algorithms, such as k-means and k-median. On the other hand, algorithms such as single-linkage, complete-linkage, and min-diameter ignore weights.\nFrom a theoretical perspective, perhaps the most notable is the last category. We find that methods belonging to that category are robust to weights when data is sufficiently clusterable, and respond to weights otherwise. Average-linkage as well as the well-known spectral objective\nfunction, ratio cut, both fall into this category. We characterize the precise conditions under which these methods are influenced by weights."}, {"heading": "1.1 Related Work", "text": "Clustering algorithms are usually analyzes in the context of unweighted data. The weighted clustering framework was briefly considered in the early 70s, but wasn\u2019t developed further until now. [9] introduced several properties of clustering algorithms. Among these, they include \u201cpoint proportion admissibility\u201d, which requires that the output of an algorithm should not change if any points are duplicated. They then observe that a few algorithms are point proportion admissible. However, clustering algorithms can display a much wider range of behaviours on weighted data than merely satisfying or failing to satisfy point proportion admissibility. We carry out the first extensive analysis of clustering on weighted data, characterizing the precise conditions under which algorithms respond to weight.\nIn addition, [21] proposed a formalization of cluster analysis consisting of eleven axioms. In two of these axioms, the notion of mass is mentioned. Namely, that points with zero mass can be treated as non-existent, and that multiple points with mass at the same location are equivalent to one point with weight the sum of the masses. The idea of mass has not been developed beyond stating these axioms in their work.\nLike earlier work, recent work on simple properties capturing differences in the input-output behaviour of clustering methods also focuses on the unweighed partitional ([2, 6, 3, 14]) and hierarchical settings ([1]). This is the first application of this property-based framework to weighted clustering.\nLastly, previous work in this line of research centers on classical linkage-based methods and their advantages. Particularly well-studied is the single-linkage algorithm, for which there are multiple property-based characterizations, showing that single-linkage is the unique algorithm that satisfies several sets of properties ([12, 6, 8]). More recently, the entire family of linkage-based algorithms was characterized ([2, 1]), differentiating those algorithms from other clustering paradigms by presenting some of the advantages of those methods. In addition, previous property-based taxonomies in this line of work highlight the advantages of linkage-based methods ([3, 9]), and some early work focuses on properties that distinguish among linkage-based algorithms ([11]). Despite the emphasis on linkage-based methods in the theory literature, empirical studies and user experience have shown that, in many cases, other techniques produce more useful clusterings than those obtained by classical linkage-based methods. Here we propose categories that distinguish between clustering paradigms while also showing when other techniques, such as popular center-based methods, may be more appropriate."}, {"heading": "2 Preliminaries", "text": "A weight function w over X is a function w : X \u2192 R+, mapping elements of X to positive real numbers. Given a domain set X, denote the corresponding weighted domain by w[X], thereby associating each element x \u2208 X with weight w(x). A dissimilarity function is a symmetric function d : X \u00d7X \u2192 R+ \u222a {0}, such that d(x, y) = 0 if and only if x = y. We consider weighted data sets of the form (w[X], d), where X is some finite domain set, d is a dissimilarity function over X, and w is a weight function over X.\nA k-clustering C = {C1, C2, . . . , Ck} of a domain set X is a partition of X into 1 < k < |X| disjoint, non-empty subsets of X where \u222aiCi = X. A clustering of X is a k-clustering for some 1 < k < |X|. To avoid trivial partitions, clusterings that consist of a single cluster, or where every cluster has a unique element, are not permitted.\nDenote the weight of a cluster Ci \u2208 C by w(Ci) = \u2211\nx\u2208Ci w(x). For a clustering C, let |C| denote the number of clusters in C. For x, y \u2208 X and clustering C of X, write x \u223cC y if x and y belong to the same cluster in C and x 6\u223cC y, otherwise.\nA partitional weighted clustering algorithm is a function that maps a data set (w[X], d) and an integer 1 < k < |X| to a k-clustering of X.\nA dendrogram D of X is a pair (T,M) where T is a strictly binary rooted tree and M : leaves(T )\u2192 X is a bijection. A hierarchical weighted clustering algorithm is a function that maps a data set (w[X], d) to a dendrogram of X. A set C0 \u2286 X is a cluster in a dendrogram D = (T,M) of X if there exists a node x in T so that C0 = {M(y) | y is a leaf and a descendent of x}. Two dendrogram of X are equivalent if they contain the same clusters, and [D] denotes the equivalence class of dendrogram D.\nFor a hierarchical weighted clustering algorithm A, a clustering C = {C1, . . . , Ck} appears in A(w[X], d) if Ci is a cluster in A(w[X], d) for all 1 \u2264 i \u2264 k. A partitional algorithm A outputs clustering C on (w[X], d) if A(w[X], d, |C|) = C.\nFor the remainder of this paper, unless otherwise stated, we will use the term \u201cclustering algorithm\u201d for \u201cweighted clustering algorithm\u201d.\nThe range of a partitional algorithm on a data set is the number of clusterings it outputs on that data over all weight functions.\nDefinition 1 (Range (Partitional)). Finally, given a partitional clustering algorithm A, a data set (X, d), and 1 \u2264 k \u2264 |X|, let range(A(X, d, k)) = {C | \u2203w such that C = A(w[X], d)}, i.e. the set of k-clusterings that A outputs on (X, d) over all possible weight functions.\nThe range of a hierarchical algorithm on a data set is the number of equivalence classes it outputs on that data over all weight functions.\nDefinition 2 (Range (Hierarchical)). Given a hierarchical clustering algorithm A and a data set (X, d), let range(A(X, d)) = {[D] | \u2203w such that D = A(w[X], d)}, i.e. the set of dendrograms that A outputs on (X, d) over all possible weight functions."}, {"heading": "3 Basic Categories", "text": "Different clustering algorithms exhibit radically different response to weighted data. In this section we introduce a formal categorization of clustering algorithms based on their response to weights. This categorization identifies fundamental differences between clustering paradigms, while highlighting when some of the more empirically successful methods should be used. These simple properties can assist clustering users in selecting suitable method by simply considering how an appropriated algorithm should react to element duplication. After we introduce the three categories, we show a classification of some of well-known clustering methods according to their response to weight, summarized in Table 1."}, {"heading": "3.1 Weight Robust Algorithms", "text": "We first introduce the notion of \u201cweight robust\u201d algorithms. Weight robustness requires that the output of the algorithm be unaffected by changes of element weights (or, the number of occurrences of each point in the unweighted setting). This category is closely related to \u201cpoint proportion admissibility\u201d by [9].\nDefinition 3 (Weight Robust (Partitional)). A partitional algorithm A is weight-robust if for all (X, d) and 1 < k < |X|, |range(A(X, d, k))| = 1.\nThe definition in the hierarchical setting is analogous.\nDefinition 4 (Weight Robust (Hierarchical)). A hierarchical algorithm A is weight-robust if for all (X, d), |range(A(X, d))| = 1.\nAt first glance, this appears to be a desirable property. A weight robust algorithm is able to keep sight on the geometry of the data without being \u201cdistracted\u201d by weights, or element duplicates. Indeed, when a similar property was proposed by [9], it was presented as a desirable characteristic.\nYet, notably, few algorithms possess it (particularly single-linkage, complete-linkage, and mindiamater), while most techniques, including those with a long history of empirical success, fail this property. This brings into question how often is weight-robustness a desirable characteristic, and suggests that at least for some application sensitivity to weights may be an advantage. Significantly, the popular k-means and similar methods fail weight robustness in a strong sense, being \u201cweight sensitive.\u201d"}, {"heading": "3.2 Weight Sensitive Algorithms", "text": "We now introduce the definition of \u201cweight sensitive\u201d algorithms.\nDefinition 5 (Weight Sensitive (Partitional)). A partitional algorithm A is weight-sensitive if for all (X, d) and 1 < k < |X|, |range(A(X, d, k))| > 1.\nThe definition is analogous for hierarchical algorithms.\nDefinition 6 (Weight Sensitive (Hierarchical)). A hierarchical algorithm A is weight-sensitive if for all (X, d) where |X| > 2, |range(A(X, d))| > 1.\nNote that this definition is quite extreme. It means that no matter how well-separated the clusters are, the output of a weight-sensitive algorithm can be altered by modifying some of the weights. That is, a weight-sensitive algorithm will miss arbitrarily well-separated clusters, for some weighting of its elements. In practice, weight sensitive algorithm tend to aim for balanced cluster sizes, and so prioritize a balance in cluster sizes (or, sum of cluster weights) over separation between clusters.\nWhile weight-robust algorithms are interested exclusively in the geometry of the data, weightsensitive techniques have two potentially conflicting considerations: The weight of the points and the geometry of the data. For instance, consider the data in Figure 1, which has two distinct 3- clusterings, one which provides superior separation between clusters, and another in which clusters sizes are balanced. Note how different are the two clusterings from one each other. All the weightsensitive methods we consider select the clustering on the right, as it offers more balanced clusters.\nOn the other hand, the weight-robust methods we studied picked the clustering on the left hand side, as it offers better cluster separation.\nAnother way we could think of weight sensitive algorithm is that, unlike weight-robust methods, weight sensitive algorithms allow the weights to alter the geometry of the data. In contrast, weight robust techniques do not allow the weights of the points to \u201cinterfere\u201d with the underlying geometry.\nIt is important to note that there appear to be implications of these categories that apply to data that is neither weighted nor contains element duplicates. Considering the algorithms we analyzed (summarized in Table 1), the behaviour we observe on element duplicates extend to \u201cnear-duplicates,\u201d which are closely positioned elements. Furthermore, the weight response of an algorithm sheds light on how it treats dense regions. In particular, weight sensitive algorithms have a tendency to \u201dzoom in\u201d on areas of high density, effectively ignoring sparse regions, as shown on the right-hand side of Figure 1.\nFinally, the last category considered here offers a compromise between weight-robustness and weight-sensitivity, we refer to this category as weight considering."}, {"heading": "3.3 Weight Considering Algorithms", "text": "Definition 7 (Weight Considering (Partitional)). A partitional algorithm A is weight-considering if\n\u2022 There exist (X, d) and 1 < k < |X| so that |range(A(X, d, k))| = 1, and\n\u2022 There exist (X, d) and 1 < k < |X| so that |range(A(X, d, k))| > 1.\nThe definition carries over to the hierarchical setting as follows.\nDefinition 8 (Weight Considering (Hierarchical)). A hierarchical algorithm A is weight-considering if\n\u2022 There exist (X, d) with |X| > 2 so that |range(A(X, d))| = 1, and\n\u2022 There exist (X, d) with |X| > 2 so that |range(A(X, d))| > 1.\nWeight considering methods appear to have the best of both worlds. The weight-considering algorithms that we analyzed (average-linkage and ratio-cut), ignore weights when clusters are sufficiently well-separated and otherwise takes them into consideration. Yet, it is important to note that this is only desirable in some instances. For example, when cluster balance is critical, as may be the case for market segmentation, weight-sensitive methods may be preferable over weight considering ones. On the other hand, when the distribution may be highly bias, as is the often the case for phylogenetic analysis, weight-considering methods may offer a satisfactory compromise between weight-sensitivity and weight-robustness, allowing the algorithm to detect well-separated, possibly of radically varying sized, without entirely disregarding weights. Notably, of all the classical clustering algorithms studied here, average-linkage, a weight-considering technique, is the only one that is commonly applied to phylogenetic analysis.\nThe following table presents a classification of classical clustering methods based on these three categories. Sections 4 and 5 provide the proof for the results summarized below. In addition, these sections also characterizes precisely when the weight-considering techniques studied here respond to weights. An expanded table that includes heuristics, with a corresponding analysis, is included in Section 6.\nTo formulate clustering algorithms in the weighted setting, we consider their behaviour on data that allows duplicates. Given a data set (X, d), elements x, y \u2208 X are duplicates if d(x, y) = 0 and d(x, z) = d(y, z) for all z \u2208 X. In a Euclidean space, duplicates correspond to elements that occur at the same location. We obtain the weighted version of a data set by de-duplicating the data, and associating every element with a weight equaling the number of duplicates of that element in the original data. The weighted version of an algorithm partitions the resulting weighted data in the same manner that the unweighted version partitions the original data. As shown throughout the paper, this translation leads to natural formulations of weighted algorithms."}, {"heading": "4 Partitional Methods", "text": "In this section, we show that partitional clustering algorithms respond to weights in a variety of ways. Many popular partitional clustering paradigms, including k-means, k-median, and min-sum,\nare weight sensitive. It is easy to see that methods such as min-diameter and k-center are weightrobust. We begin by analysing the behaviour of a spectral objective function ratio cut, which exhibits interesting behaviour on weighted data by responding to weight unless data is highly structured."}, {"heading": "4.1 Ratio-Cut Clustering", "text": "We investigate the behaviour of a spectral objective function, ratio-cut ([19]), on weighted data. Instead of a dissimilarity function, spectral clustering relies on a similarity function, which maps pairs of domain elements to non-negative real numbers that represent how alike the elements are. The ratio-cut of a clustering C is:\ncostrcut(C,w[X], s) = 1\n2 \u2211 Ci\u2208C \u2211 x\u2208Ci,y\u2208X\\Ci s(x, y) \u00b7 w(x) \u00b7 w(y)\u2211 x\u2208Ci w(x) .\nThe ratio-cut clustering function is:\nrcut(w[X], s, k) = arg min C;|C|=k costrcut(C,w[X], s).\nWe prove that this function ignores data weights only when the data satisfies a very strict notion of clusterability. To characterize precisely when ratio-cut responds to weights, we first present a few definitions.\nA clustering C of (w[X], s) is perfect if for all x1, x2, x3, x4 \u2208 X where x1 \u223cC x2 and x3 6\u223cC x4, s(x1, x2) > s(x3, x4). C is separation-uniform if there exists \u03bb so that for all x, y \u2208 X where x 6\u223cC y, s(x, y) = \u03bb. Note that neither condition depends on the weight function.\nWe show that whenever a data set has a clustering that is both perfect and separation-uniform, then ratio-cut uncovers that clustering, which implies that ratio-cut is not weight-sensitive. Note that, in particular, these conditions are satisfied when all between-cluster similarities are set to zero. On the other hand, we show that ratio-cut does respond to weights when either condition fails.\nLemma 1. If, given a data set (X, d), 1 < k < |X| and some weight function w, ratio-cut outputs a k-clustering C that is not separation-uniform and where every cluster has more than a single point, then |range(ratio-cut(X, d))| > 1.\nProof. We consider two cases.\nCase 1: There is a pair of clusters with different similarities between them. Then there exist C1, C2 \u2208 C, x \u2208 C1, and y \u2208 C2 so that s(x, y) \u2265 s(x, z) for all z \u2208 C2, and there exists a \u2208 C2 so that s(x, y) > s(x, a).\nLet w be a weight function such that w(x) = W for some sufficiently large W and weight 1 is assigned to all other points in X. Since we can set W to be arbitrarily large, when looking at the cost of a cluster, it suffices to consider the dominant term in terms of W . We will show that we can improve the cost of C by moving a point from C2 to C1. Note that moving a point from C2 to C1 does not affect the dominant term of clusters other than C1 and C2. Therefore, we consider the cost of these two clusters before and after rearranging points between these clusters.\nLet A = \u2211\na\u2208C2 s(x, a) and let m = |C2|. Then the dominant term, in terms of W , of the cost of C2 is W ( A m ) . The cost of C1 approaches a constant as W \u2192\u221e.\nNow consider clustering C \u2032 obtained from C by moving y from cluster C2 to cluster C1. The dominant term in the cost of C2 becomes W ( A\u2212s(x,y)\nm\u22121\n) , and the cost of C1 approaches a constant\nas W \u2192\u221e. By choice of x and y, if A\u2212s(x,y)m\u22121 < A m then C \u2032 has lower loss than C when W is large enough. The inequality A\u2212s(x,y)m\u22121 < A m holds when A m < s(x, y), and the latter holds by choice of x and y.\nCase 2: The similarities between every pair of clusters are the same. However, there are clusters C1, C2, C3 \u2208 C, so that the similarities between C1 and C2 are greater than the ones between C1 and C3. Let a and b denote the similarities between C1, C2 and C1, C3, respectively.\nLet x \u2208 C1 and w a weight function, such that w(x) = W for large W , and weight 1 is assigned to all other points in X. The dominant term comes from clusters going into C1, specifically edges that include point x. The dominant term of the contribution of cluster C3 is Wb and the dominant term of the contribution of C2 is Wa, totaling Wa+Wb.\nNow consider clustering C \u2032 obtained from clustering C by merging C1 with C2, and splitting C3 into two clusters (arbitrarily). The dominant term of the clustering comes from clusters other than C1\u222aC2, and the cost of clusters outside C1\u222aC2\u222aC3 is unaffected. The dominant term of the cost of the two clusters obtained by splitting C3 is Wb for each, for a total of 2Wb. However, the factor of Wa that C2 previously contributed is no longer present. This replaces the coefficient of the dominant term from a+ b to 2b, which improved the cost of the clustering because b < a.\nLemma 2. If, given a data set (X, d), 1 < k < |X|, and some weight function w, ratio-cut outputs a clustering C that is not perfect and where every cluster has more than a single point, then |range(ratio-cut(X, d, k))| > 1.\nProof. If C is also not separation-uniform, then Lemma 1 can be applied, and so we can assume that C is separation-uniform. Then there exists a within-cluster similarity in C that is smaller than some between-cluster similarity, and all between cluster similarities are the same. Specifically, there exist clusters C1 and C2, such that all the similarities between C1 and C2 are a, and there exist x, y \u2208 C1 such that s(x, y) < a. Let b = s(x, y).\nLet w be a weight function such that w(x) = W for large W , and weight 1 is assigned to all other points in X. Then the dominant term in the cost of C2 is Wa, which comes from the cost of points in cluster C2 going to point x. The cost of C1 approaches a constant as W \u2192\u221e.\nConsider the clustering C \u2032 obtained from C by completely re-arranging all points in C1, C2 \u2208 C as follows:\n1. Let {y, z} = C \u20321 be one cluster for any z \u2208 C2.\n2. Let C \u20322 = (C1 \u222a C2) \\ C \u20321 be the new second cluster.\nThe dominant term in of C \u20321, which comes from the cost of points in cluster C \u2032 1 going to point x, is ( a+b 2 ) W , which is smaller than Wa since a > b. Note that the cost of each cluster outside of C \u20321 \u222a C \u20322 remains unchanged. Since x \u2208 C \u20321, the cost of C \u20321 approaches a constant as W \u2192 \u221e. Therefore, costrcut(C \u2032, w[X], s) is smaller than costrcut(C,w[X], s) when W is sufficiently large.\nLemma 3. Given any data set (w[X], s) and 1 < k < |X| that has a perfect, separation-uniform k-clustering C, ratio-cut(w[X], s, k) = C.\nProof. Let (w[X], s) be a weighted data set, with a perfect, separation-uniform clustering C = {C1, . . . , Ck}. Recall that for any Y \u2286 X, w(Y ) = \u2211 y\u2208Y w(y). Then:\ncostrcut(C,w[X], s) = 1\n2 k\u2211 i=1\n\u2211 x\u2208Ci \u2211 y\u2208Ci s(x, y)w(x)w(y)\u2211\nx\u2208Ci w(x)\n= 1\n2 k\u2211 i=1\n\u2211 x\u2208Ci \u2211 y\u2208Ci \u03bbw(x)w(y)\u2211 x\u2208Ci w(x)\n= \u03bb\n2 k\u2211 i=1\n\u2211 y\u2208Ci w(y) \u2211 x\u2208Ci w(x)\u2211\nx\u2208Ci w(x) = \u03bb 2 k\u2211 i=1 \u2211 y\u2208Ci w(y)\n= \u03bb\n2 k\u2211 i=1 w(Ci) = \u03b4 2 k\u2211 i=1 [w(X)\u2212 w(Ci)]\n= \u03bb\n2\n( kw(X)\u2212\nk\u2211 i=1 w(Ci)\n) = \u03bb\n2 (k \u2212 1)w(X).\nConsider any other clustering, C \u2032 = {C \u20321, . . . , C \u2032 k} 6= C. Since C is both perfect and separationuniform, all between-cluster similarities in C are \u03bb, and all within-cluster similarities are greater than \u03bb. From here it follows that all pair-wise similarities in the data are at least \u03bb. Since C \u2032 is a k-clustering different from C, it must differ from C on at least one between-cluster edge, so that edge must be greater than \u03bb. Thus the cost of C \u2032 is:\ncostrcut(C \u2032 , w[X], s) =\n1\n2 k\u2211 i=1\n\u2211 x\u2208C\u2032i \u2211 y\u2208C\u2032i\ns(x, y)w(x)w(y)\u2211 x\u2208C\u2032i w(x)\n> 1\n2 k\u2211 i=1\n\u2211 x\u2208C\u2032i \u2211 y\u2208C\u2032i\n\u03bbw(x)w(y)\u2211 x\u2208C\u2032i w(x)\n= \u03bb\n2 (k \u2212 1)w(X) = costrcut(C).\nThus clustering C \u2032 has a higher cost than C.\nIt follows that ratio-cut responds to weights on all data sets except those where it is possible to obtain cluster separation that is both very large and highly uniform. This implies that ratio cut is highly unlikely to be unresponsive to weights in practice.\nFormally, we have the following theorem, which gives sufficient conditions for when ratio-cut ignores weights, as well conditions that make this function respond to weights.\nTheorem 4.1. Given any (X, d) and 1 < k < |X|, 1. if (X, d) has a clustering that is both perfect and separation uniform, then\n|range(Ratio-cut(X, s, k))| = 1,\nand\n2. if range(Ratio-cut(X, s, k)) includes a clustering C that is not perfect, not separation uniform, and has no singleton clusters, then |range(Ratio-cut(X, s, k))| > 1.\nProof. The result follows by Lemma 1, Lemma 2, and Lemma 3.\n4.2 K-Means\nMany popular partitional clustering paradigms, including k-means (see [17] for a detailed exposition of this popular objective function and related algorithms), k-median, and the min-sum objective ([16]), are weight sensitive. Moreover, these algorithms satisfy a stronger condition. By modifying weights, we can make these algorithms separate any set of points. We call such algorithms weightseparable.\nDefinition 9 (Weight Separable). A partitional clustering algorithm A is weight-separable if for any data set (X, d) and any S \u2282 X, where 2 \u2264 |S| \u2264 k, there exists a weight function w so that x 6\u223cA(w[X],d,k) y for all distinct x, y \u2208 S.\nNote that every weight-separable algorithm is also weight-sensitive.\nLemma 4. If a clustering algorithm A is weight-separable, then A is weight-sensitive.\nProof. Given any (X, d) and weight function w over X, let C = A(w[X], d, k). Select points x and y where x \u223cC y. Since A is weight-separable, there exists w\u2032 so that x 6\u223cA(w\u2032[X],d,k) y, and so A(w\u2032[X], d, k) 6= C. It follows that for any (X, d), |range(A(X, d))| > 1.\nK-means is perhaps the most popular clustering objective function, with cost: k-means(C,w[X], d) = \u2211 Ci\u2208C \u2211 x\u2208Ci d(x, cnt(Ci)) 2,\nwhere cnt(Ci) denotes the center of mass of cluster Ci. The k-means objective function finds a clustering with minimal k-means cost. We show that k-means is weight-separable, and thus also weight-sensitive.\nTheorem 4.2. The k-means objective function is weight-separable.\nProof. Consider any S \u2286 X. Let w be a weight function over X where w(x) = W if x \u2208 S, for large W , and w(x) = 1 otherwise. As shown by [15], the k-means objective function is equivalent to \u2211\nx,y\u2208Ci d(x, y) 2 \u00b7 w(x) \u00b7 w(y)\nw(Ci) .\nLet m1 = minx,y\u2208X d(x, y) 2 > 0, m2 = maxx,y\u2208X d(x, y) 2, and n = |X|. Consider any kclustering C where all the elements in S belong to distinct clusters. Then we have:\nk-means(C,w[X], d) < km2\n( n+ n2\nW\n) .\nOn the other hand, given any k-clustering C \u2032 where at least two elements of S appear in the same cluster, k-means(C \u2032, w[X], d) \u2265 W 2m1W+n . Since\nlim W\u2192\u221e\nk-means(C \u2032, w[X], d) k-means(C,w[X], d) =\u221e,\nk-means separates all the elements in S for large enough W .\nMin-sum is another well known objective function and it minimizes the expression:\u2211 Ci\u2208C \u2211 x,y\u2208Ci d(x, y) \u00b7 w(x) \u00b7 w(y).\nTheorem 4.3. Min-sum is weight-separable.\nProof. Let (X, d) be any data set and 1 < k < |X|. Consider any S \u2286 X where 1 < |S| \u2264 k. Let w be a weight function over X where w(x) = W if x \u2208 S, for large W , and w(x) = 1 otherwise. Let m1 = minx,y\u2208X d(x, y) be the minimum dissimilarity in (X, d), and let m2 = maxx,y\u2208X d(x, y) be the maximum dissimilarity in (X, d).\nThen the cost of any cluster that includes two elements of S is a least m1W 2, while the cost of a cluster that includes at most one element of S is less than m2|X|(|X|+W ). So when W is large enough, selects a partition where no two elements of S appear in the same cluster.\nSeveral other objective functions similar to k-means are also weight-separable. We show that kmedian and k-medoids are weight sensitive by analysing center-based approaches that use exemplars from the data as cluster centers (as opposed to any elements in the underlying space). Given a set T \u2286 X, define C(T ) to be the clustering obtained by assigning every element in X to the closest element (\u201ccenter\u201d) in T .\nExemplar-based clustering is defined as follows.\nDefinition 10 (Exemplar-based). An algorithm A is exemplar-based if there exists a function f : R+ \u2192 R+ such that for all (w[X], d), A(w[X], d, k) = C(T ) where\nT = arg min T\u2282X;|T |=k \u2211 x\u2208X,x6\u2208T w(x)f(min y\u2208T d(x, y)).\nNote that when f is the identity function, we obtain k-median, and when f(x) = x2 we obtain k-medoids.\nTheorem 4.4. Every exemplar-based clustering function is weight-separable.\nProof. Consider any S \u2286 X with |S| \u2264 k. For all x \u2208 S, set w(x) = W for some large value W and set all other weights to 1. Recall that a clustering has between 2 and |X| \u2212 1 clusters. Consider any clustering C where some distinct elements x, y \u2208 S belong to the same cluster Ci \u2208 C. Since at most one of x or y can be the center of Ci, the cost of C is at least W \u00b7minx1,x2\u2208Ci f(d(x1, x2)). Observe that f(d(x1, x2)) > 0.\nConsider any clustering C \u2032 where all the elements in S belong to distinct clusters. If a cluster contains a unique element x of S, then its cost is constant in W if x is the cluster center, and at least W \u00b7minx1,x2\u2208X f(d(x1, x2)) if x is not the center of the cluster. This shows that if W is large enough, then every element of S will be a cluster center, and so the cost of C \u2032 would be independent of W . So when W is large, clusterings that separate elements of S have lower cost than those that merge any points in S.\nWe have the following corollary.\nCorollary 1. The k-median and k-medoids objective functions are weight-separable and weightsensitive."}, {"heading": "5 Hierarchical Algorithms", "text": "Similarly to partitional methods, hierarchical algorithms also exhibit a wide range of responses to weights. We show that Ward\u2019s method ([20]), a successful linkage-based algorithm, as well as popular divisive hierarchical methods, are weight sensitive. On the other hand, it is easy to see that the linkage-based algorithms single-linkage and complete-linkage are both weight robust, as was observed in [9].\nAverage-linkage, another popular linkage-based method, exhibits more nuanced behaviour on weighted data. When a clustering satisfies a reasonable notion of clusterability, average-linkage detects that clustering irrespective of weights. On the other hand, this algorithm responds to weights on all other clusterings. We note that the notion of clusterability required for averagelinkage is much weaker than the notion of clusterability used to characterize the behaviour of ratio-cut on weighted data."}, {"heading": "5.1 Average Linkage", "text": "Linkage-based algorithms start by placing each element in its own cluster, and proceed by repeatedly merging the \u201cclosest\u201d pair of clusters until the entire dendrogram is constructed. To identify the closest clusters, these algorithms use a linkage function that maps pairs of clusters to a real number. Formally, a linkage function is a function ` : {(X1, X2, d, w) | d,w over X1 \u222aX2} \u2192 R+.\nAverage-linkage is one of the most popular linkage-based algorithms (commonly applied in bioinformatics under the name Unweighted Pair Group Method with Arithmetic Mean). Recall that w(X) = \u2211 x\u2208X w(x). The average-linkage linkage function is\n`AL(X1, X2, d, w) =\n\u2211 x\u2208X1,y\u2208X2 d(x, y) \u00b7 w(x) \u00b7 w(y)\nw(X1) \u00b7 w(X2) .\nTo study how average-linkage responds to weights, we give a relaxation of the notion of a perfect clustering.\nDefinition 11 (Nice). A clustering C of (w[X], d) is nice if for all x1, x2, x3 \u2208 X where x1 \u223cC x2 and x1 6\u223cC x3, d(x1, x2) < d(x1, x3).\nData sets with nice clusterings correspond to those that satisfy the \u201cstrict separation\u201d property introduced by Balcan et al. [5]. As for a perfect clustering, being a nice clustering is independent of weights. Note that all perfect clusterings are nice, but not all nice clusterings are perfect. A dendrogram is nice if all clusterings that appear in it are nice.\nWe present a complete characterisation of the way that average-linkage (AL) responds to weights.\nTheorem 5.1. Given (X, d), |range(AL(X, d))| = 1 if and only if (X, d) has a nice dendrogram.\nProof. We first show that if a data set has a nice dendrogram, then this is the dendrogram that average-linkage outputs. Note that the property of being nice is independent of the weight function. So, the set of nice clusterings of any data set (w[X], d) is invariant to the weight function w. Lemma 5 shows that, for every (w[X], d), every nice clustering in (w[X], d) appears in the dendrogram produce by average-linkage.\nLet (X, d) be a data set that has a nice dendrogram D. We would like to show that averagelinkage outputs that dendrogram. Let C be the set of all nice clusterings of (X, d). Let L = {c | \u2203C \u2208 C such that c \u2208 C}. That is, L is the set of all clusters that appear in some nice clustering of (X, d).\nSince D is a nice dendrogram of (X, d), all clusterings that appear in it are nice, and so it contains all the clusters in L and no additional clusters. In order to satisfy the condition that every nice clustering of (X, d) appears in the dendrogram, DAL, produced by average-linkage, DAL must have all clusters in L.\nSince a dendrogram is a strictly binary tree, any dendrogram of (X, d) has exactly |X|\u22121 inner nodes. In particular, all dendrograms of the same data set have exactly the same number of inner nodes. This implies that DAL has the same clusters as D, so DAL is equivalent to D.\nNow, let (X, d) be a data set that does not have a nice dendrogram. Then, given any w over X, there is a clustering C that is not nice that appears in AL(w[X], d). Lemma 6 shows that if a clustering that is not nice appears in AL(w[X], d), then |range(AL(X, d))| > 1.\nTheorem 5.1 follows from the two lemmas below.\nLemma 5. Given any weighted data set (w[X], d), if C is a nice clustering of (X, d), then C is in the dendrogram produced by average-linkage on (w[X], d).\nProof. Consider a nice clustering C = {C1, . . . , Ck} over (w[X], d). It suffices to show that for any 1 \u2264 i < j \u2264 k, X1, X2 \u2286 Ci where X1\u2229X2 = \u2205 and X3 \u2286 Cj , `AL(X1, X2, d, w) < `AL(X1, X3, d, w). It can be shown that\n`AL(X1, X2, d, w) \u2264 \u2211\nx1\u2208X1 w(x1) \u00b7maxx2\u2208X2 d(x1, x2) w(X1)\nand\n`AL(X1, X3, d, w) \u2265 \u2211\nx1\u2208X1 w(x1) \u00b7minx3\u2208X3 d(x1, x3) w(X1) .\nSince C is nice, it follows that\nmin x3\u2208X3 d(x1, x3) > max x2\u2208X2 d(x1, x2)\nThus `AL(X1, X3) > `AL(X1, X2), which completes the proof.\nLemma 6. For any data set (X, d) and any weight function w over X, if a clustering that is not nice appears in AL(w[X], d), then |range(AL(X, d))| > 1.\nProof. Let (X, d) be a data set so that a clustering C that is not nice appears in AL(w[X], d) for some weight function w over X. We construct w\u2032 so that C 6\u2208 AL(w\u2032[X], d), which would show that |range(AL(X, d))| > 1.\nSince C is not nice, there exist 1 \u2264 i, j \u2264 k, i 6= j, and x1, x2 \u2208 Ci, x1 6= x2, and x3 \u2208 Cj , so that d(x1, x2) > d(x1, x3).\nNow, define weight function w\u2032 as follows: w\u2032(x) = 1 for all x \u2208 X \\ {x1, x2, x3}, and w\u2032(x1) = w\u2032(x2) = w\n\u2032(x3) = W , for some large value W . We argue that when W is sufficiently large, C is not a clustering in AL(w\u2032[X], d).\nBy way of contradiction, assume that C is a clustering in AL(w\u2032[X], d) for any setting of W . Then there is a step in the algorithm where clusters X1 and X2 merge, where X1, X2 \u2282 Ci, x1 \u2208 X1, and x2 \u2208 X2. At this point, there is some cluster X3 \u2286 Cj so that x3 \u2208 X3.\nWe compare `AL(X1, X2, d, w \u2032) and `AL(X1, X3, d, w \u2032). First, note that\na`AL(X1, X2, d, w \u2032) = W 2d(x1, x2) + \u03b11W + \u03b12 W 2 + \u03b13W + \u03b14\nfor some non-negative real valued \u03b1is. Similarly, we have that for some non-negative real-valued \u03b2i:\n`AL(X1, X3, d, w \u2032) = W 2d(x1, x3) + \u03b21W + \u03b22 W 2 + \u03b23W + \u03b24\nDividing by W 2, we see that `AL(X1, X3, d, w \u2032)\u2192 d(x1, x3) and `AL(X1, X2, d, w\u2032)\u2192 d(x1, x2) as W \u2192 \u221e, and so the result holds since d(x1, x3) < d(x1, x2). Therefore average linkage merges X1 with X3, thus cluster Ci is never formed, and so C is not a clustering in AL(w\n\u2032[X], d). If follows that |range(AL(X, d))| > 1,"}, {"heading": "5.2 Ward\u2019s Method", "text": "Ward\u2019s method is a highly effective clustering algorithm ([20]), which, at every step, merges the clusters that will yield the minimal increase to the sum-of-squares error (the k-means objective function). Let ctr(X, d,w) be the center of mass of the data set (w[X], d). Then, the linkage function for Ward\u2019s method is\n`Ward(X1, X2, d, w) = w(X1) \u00b7 w(X2) \u00b7 d(ctr(X1, d, w), ctr(X2, d, w))2\nw(X1) + w(X2) ,\nwhere X1 and X2 are disjoint subsets (clusters) of X.\nTheorem 5.2. Ward\u2019s method is weight sensitive.\nProof. Consider any data set (X, d) and any clustering C output by Ward\u2019s method on (X, d). Let x, y \u2208 X be any distinct points that belong to the same cluster in C. Let w be the weight function that assigns a large weight W to points x and y, and weight 1 to all other elements.\nSince Ward\u2019s method is linkage-based, it starts off by placing every element in its own cluster. We will show that when W is large enough, it is prohibitively expensive to merge a cluster that contains x with a cluster that contains point y. Therefore, there is no cluster in the dendrogram produced by Ward\u2019s method that contains both points x and y, other than the root, and so C is not a clustering in that dendrogram. This would imply that |range(Ward(X, d))| > 1.\nAt some point in the execution of Ward\u2019s method, x and y must belong to different clusters. Let Ci be a cluster that contains x, and cluster Cj a cluster that contains point y. Then `Ward(Ci, Cj , d, w)\u2192\u221e as W \u2192\u221e. On the other hand, whenever at most one of Ci or Cj contains an element of {x, y}, `Ward(Ci, Cj , d, w) approaches some constant as W \u2192 \u221e. This shows that when W is sufficiently large, a cluster containing x is merged with a cluster containing y only at the last step of the algorithm, when forming the root of the dendrogram."}, {"heading": "5.3 Divisive Algorithms", "text": "The class of divisive clustering algorithms is a well-known family of hierarchical algorithms, which construct the dendrogram by using a top-down approach. This family of algorithms includes the popular bisecting k-means algorithm. We show that a class of algorithms that includes bisecting k-means consists of weight-sensitive methods.\nGiven a node x in dendrogram (T,M), let C(x) denote the cluster represented by node x. That is, C(x) = {M(y) | y is a leaf and a descendent of x}.\nInformally, a P-Divisive algorithm is a hierarchical clustering algorithm that uses a partitional clustering algorithm P to recursively divide the data set into two clusters until only single elements remain. Formally, a P-divisive algorithm is defined as follows.\nDefinition 12 (P-Divisive). A hierarchical clustering algorithm A is P-Divisive with respect to a partitional clustering algorithm P, if for all (X, d), we have A(w[X], d) = (T,M), such that for all non-leaf nodes x in T with children x1 and x2, P(w[C(x)], d, 2) = {C(x1), C(x2)}.\nWe obtain bisecting k-means by setting P to k-means. Other natural choices for P include minsum, and exemplar-based algorithms such as k-median. As shown above, many of these partitional algorithms are weight-separable. We show that whenever P is weight-separable, then P-Divisive is weight-sensitive.\nTheorem 5.3. If P is weight-separable then the P-Divisive algorithm is weight-sensitive.\nProof. Given any non-trivial clustering C output by the P-Divisive algorithm, consider any pair of elements x and y that are placed within the same cluster of C. Since P is weight separating, there exists a weight function w so that P separates points x and y. Then P-Divisive splits x and y in the first step, directly below the root, and clustering C is never formed."}, {"heading": "6 Heuristic Approaches", "text": "We have seen how weights affect various algorithms that optimize different clustering objectives. Since optimizing a clustering objective is usually NP-hard, heuristics are used in practice. In this section, we consider several common heuristical clustering approaches, and show how they respond to weights.\nWe note that there are many algorithms that aim to find high quality partitions for popular objective functions. For the k-means objective alone many different algorithms have been proposed, most of which provide different initializations for Lloyd\u2019s method. For example, [18] studied a dozen different initializations. There are many other algorithms based on the k-means objective functions, some of the most notable being k-means++ ([4]) and the Hochbaum-Schmoys initialization ([10]) studied, for instance, by [7]. As such, this section is not intended as a comprehensive analysis of all available heuristics, but rather it shows how to analyze such heuristics, and provides a classification for some of the most popular approaches.\nTo define the categories in the randomized setting, we need to modify the definition of range. Given a randomized, partitional clustering algorithm A and a data set (X, d), the randomized range is\nrandRange(A(X, d)) = {C | \u2200 < 1\u2203w such that P (A(w[X], d) = C) > 1\u2212 }.\nThat is, the randomized range is the set of clusterings that are produced with arbitrarily high probably, when we can modify weights.\nThe categories describing an algorithm\u2019s behaviour on weighted data are defined as previously, but using randomized range.\nDefinition 13 (Weight Sensitive (Randomized Partitional )). A partitional algorithm A is weightsensitive if for all (X, d) and 1 < k < |X|, |randRange(A(X, d, k))| > 1.\nDefinition 14 (Weight Robust (Randomized Partitional)). A partitional algorithm A is weightrobust if for all (X, d) and 1 < k < |X|, |randRange(A(X, d, k))| = 1.\nDefinition 15 (Weight Considering (Randomized Partitional)). A partitional algorithm A is weight-considering if\n\u2022 There exist (X, d) and 1 < k < |X| so that |randRange(A(X, d, k))| = 1, and\n\u2022 There exist (X, d) and 1 < k < |X| so that |randRange(A(X, d, k))| > 1."}, {"heading": "6.1 Partitioning Around Medoids (PAM)", "text": "In contrast with the Lloyd method and k-means, PAM is a heuristic for exemplar-based objective functions such as k-medoids, which chooses data points as centers (thus it is not required to compute centers of mass). As a result, this approach can be applied to arbitrary data, not only to normed vector spaces.\nPartitioning around medoids (PAM) is given an initial set of k centers, T , and changes T iteratively to find a \u201cbetter\u201d set of k centers. This is done by swapping out centers for other points in the data set and computing the cost function: \u2211 c\u2208T \u2211 x\u2208X\\T,c\u223cCx d(x, c) \u00b7 w(x). Each iteration performs a swap only if a better cost is possible, and it stops when no changes are made [13].\nNote that our results in this section hold regardless of how the initial k centers are chosen from X.\nTheorem 6.1. PAM is weight-separable.\nProof. Let T = {x1, . . . , xl} be l points that we want to separate, where 2 \u2264 l \u2264 k. Let {m1, . . . ,mk} \u2282 X be k centers chosen by the PAM initialization, and denote by C = {c1, . . . , ck} the clustering induced by the corresponding centers. Set w(xi) = W, \u2200xi \u2208 S for some large W . We first note that any optimal clustering C\u2217 sets the points in T as the centers. The cost of C\u2217 is constant as a function of W , while every clustering with a different set of centers has a cost proportional to W , which can be made arbitrarily high by increasing W .\nAssume, by contradiction, that the algorithm stops at a clustering C that does not separate all the points in T . Then, there exists a cluster ci \u2208 C such that |ci \u2229 T | \u2265 2. Thus, ci contributes a factor of \u03b1 \u00b7W to the cost, for some \u03b1 > 0. Further, there exists a cluster cj \u2208 C such that cj \u2229 T = \u2205. Then the cost of C can be further decreased, by a quantity proportional to W , by assigning one of the heavy non-medoid from ci to be the center of cj , which is a contradiction. Thus, the algorithm cannot stop before setting all the heavy points as cluster centers."}, {"heading": "6.2 Llyod method", "text": "The Lloyd method is a heuristic commonly used for uncovering clusterings with low k-means objective cost. The Lloyd algorithm can be combined with different approaches for seeding the initial centers. In this section, we start by considering the following deterministic seeding methods.\nDefinition 16 (Lloyd Method). Given k points (centers) {c1, . . . , ck} in the space, assign every element of X to its closest center. Then compute the centers of mass of the resulting clusters by summing the elements in each cluster and dividing by the number of elements in that partition, and assign every element to its closest new center. Continue until no change is made in one iteration.\nWith the Lloyd method, the dissimilarity (or, distance) to the center can be both the `1-norm or squared.\nFirst, we consider the case when the k initial centers are chosen in a deterministic fashion. For example, one deterministic seeding approach involves selecting the k-furthest centers (see, for example, [3]).\nTheorem 6.2. Let A represent the Lloyd method with some deterministic seeding procedure. Consider any data set (X, d) and 1 < k < |X|. If there exists a clustering C \u2208 range(A(X, d, k)) that is not nice, then |range(A(X, d, k))| > 1.\nProof. For any seeding procedure, since C is in the range of A(X, d), there exists a weight function w so that C = A(w[X], d).\nSince C is not nice, there exist points x1, x2, x3 \u2208 X where x1 \u223cC x2, x1 6\u223cC x3, but d(x1, x3) < d(x1, x2). Construct weight function w\n\u2032 such that w\u2032(x) = 1 for all x \u2208 X \\ {x2, x3}, and w\u2032(x2) = w\u2032(x3) = W , for some constant W .\nIf for some value of W , A(w\u2032[X], d, k) 6= C, then we\u2019re done. Otherwise, A(w\u2032[X], d, k) = C for all values of W . But if W is large enough, the center of mass of the cluster containing x1 and x2 is arbitrarily close to x2, and the center of mass of the cluster containing x3 is arbitrarily close to x3. But since d(x1, x3) < d(x1, x2), the Lloyd method would assign x1 and x3 to the same cluster. Thus, when W is sufficiently large, A(w\u2032[X], d, k) 6= C.\nWe also show that for a deterministic, weight-independent initialization, if the Lloyd method outputs a nice clustering C, then this algorithm is robust to weights on that data.\nTheorem 6.3. Let A represent the Lloyd method with some weight-independent deterministic seeding procedure. Given (X, d), if there exists a nice clustering in the range(A(X, d)), then A is weight robust on (X, d).\nProof. Since the initialization is weight-independent, A will find the same initial centers on any weight function. Given a nice clustering, the Lloyd method does not modify the clustering. If the seeding method were not weight-independent, it may seed in a way that may prevent the Lloyd method from finding C for some weight function.\nCorollary 2. Let A represent the Lloyd method initialized with furthest centroids. For any (X, d) and 1 < k < |X|, |range(A(X, d, k))| = 1 if and only if there exists a nice k-clustering of (X, d).\n6.2.1 k-means++\nThe k-means++ algorithm, introduced by Arthur and Vassilvitskii ([4]) is the Lloyd algorithm with a randomized initialization method that aims to place the initial centers far apart from each other. This algorithm has been demonstrated to perform very well in practice.\nLet D(x) denote the shortest dissimilarity from a point x to the closest center already chosen. The k-means++ algorithm chooses the initial center uniformly at random, and then x is selected as the next center with probability D(x) 2w(x)\u2211\ny D(y) 2w(y)\nuntil k centers have been chosen.\nTheorem 6.4. k-means++ is weight-separable.\nProof. If any k points {x1, . . . , xk} are assigned sufficiently high weight W , then the first center will be one of these points with arbitrarily high probability. The next center will also be selected with arbitrarily high probability if W is large enough, since for all y 6\u2208 {x1, . . . , xk}, the probability of selecting y can be made arbitrarily small when W is large enough.\nThe same argument works for showing that the Lloyd method is weight-separable when the initial centers are selected uniformly at random (Randomized Lloyd). An expanded classification of clustering algorithms that includes heuristics is given in Table 1 below."}, {"heading": "7 Conclusion", "text": "We studied the behaviour of clustering algorithms on weighted data, presenting three fundamental categories that describe how such algorithms respond to weights and classifying several well-known algorithms according to these categories. Our results are summarized in Table 1. We note that all of our results immediately translate to the standard setting, by mapping each point with integer weight to the same number of unweighted duplicates.\nOur results can be used to aid in the selection of a clustering algorithm. For example, in the facility allocation application discussed in the introduction, where weights are of primal importance, a weight-sensitive algorithm is suitable. Other applications may call for weight-considering algorithms. This can occur when weights (i.e. number of duplicates) should not be ignored, yet it is still desirable to identify rare instances that constitute small but well-formed outlier clusters. For example, this applies to patient data on potential causes of a disease, where it is crucial to investigate rare instances.\nThis paper presents a significant step forward in the property-based approach for selecting clustering algorithms. Unlike previous properties, which focused on advantages of linkage-based algorithms, these properties show when applications call for popular center-based approaches, such as k-means. Furthermore, the simplicity of these properties makes them widely applicable, requiring only that the user decide whether duplicating elements should be able to change the output of the algorithm. Future work will consider complimentary considerations, with the ultimate goal of attaining a small set of properties that will aid in \u201cthe user\u2019s dilemma\u201d for a wide range of clustering applications."}], "references": [{"title": "Discerning linkage-based algorithms among hierarchical clustering methods", "author": ["M. Ackerman", "S. Ben-David"], "venue": "In IJCAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Characterization of linkage-based clustering", "author": ["M. Ackerman", "S. Ben-David", "D. Loker"], "venue": "In COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Towards property-based classification of clustering paradigms", "author": ["M. Ackerman", "S. Ben-David", "D. Loker"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "K-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In SODA,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["M.F. Balcan", "A. Blum", "S. Vempala"], "venue": "In STOC,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "A uniqueness theorem for clustering", "author": ["R. Bosagh-Zadeh", "S. Ben-David"], "venue": "In UAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "How the initialization affects the stability of the k-means algorithm", "author": ["S\u00e9bastien Bubeck", "Marina Meila", "Ulrike von Luxburg"], "venue": "arXiv preprint arXiv:0907.5494,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Characterization, stability and convergence of hierarchical clustering methods", "author": ["Gunnar Carlsson", "Facundo M\u00e9moli"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Admissible clustering procedures", "author": ["L. Fisher", "J. Van Ness"], "venue": "Biometrika, 58:91\u2013104,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1971}, {"title": "A best possible heuristic for the k-center problem", "author": ["Dorit S Hochbaum", "David B Shmoys"], "venue": "Mathematics of operations research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1985}, {"title": "Hierarchical clustering and the concept of space distortion", "author": ["Lawrence Hubert", "James Schultz"], "venue": "British Journal of Mathematical and Statistical Psychology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1975}, {"title": "The construction of hierarchic and non-hierarchic classifications", "author": ["Nicholas Jardine", "Robin Sibson"], "venue": "The Computer Journal,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1968}, {"title": "Partitioning Around Medoids (Program PAM)", "author": ["L. Kaufman", "P.J. Rousseeuw"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "An impossibility theorem for clustering", "author": ["J. Kleinberg"], "venue": "Proceedings of International Conferences on Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "The effectiveness of Lloyd-type methods for the k-means problem", "author": ["R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy"], "venue": "In FOCS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "P-complete approximation problems", "author": ["Sartaj Sahni", "Teofilo Gonzalez"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1976}, {"title": "K-means clustering: a half-century synthesis", "author": ["D. Steinley"], "venue": "British Journal of Mathematical and Statistical Psychology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Initializing k-means batch clustering: a critical evaluation of several techniques", "author": ["Douglas Steinley", "Michael J Brusco"], "venue": "Journal of Classification,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "J. Stat. Comput.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["Joe H Ward Jr."], "venue": "Journal of the American statistical association,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1963}, {"title": "A formalization of cluster analysis", "author": ["W.E. Wright"], "venue": "J. Pattern Recogn.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1973}], "referenceMentions": [{"referenceID": 1, "context": "Even the fairy basic problem of which algorithm to select for a given application (known as \u201cthe user\u2019s dilemma\u201d) is left to ad hoc solutions, as theory is only starting to address fundamental differences between clustering methods ([2, 6, 3, 1]).", "startOffset": 233, "endOffset": 245}, {"referenceID": 5, "context": "Even the fairy basic problem of which algorithm to select for a given application (known as \u201cthe user\u2019s dilemma\u201d) is left to ad hoc solutions, as theory is only starting to address fundamental differences between clustering methods ([2, 6, 3, 1]).", "startOffset": 233, "endOffset": 245}, {"referenceID": 2, "context": "Even the fairy basic problem of which algorithm to select for a given application (known as \u201cthe user\u2019s dilemma\u201d) is left to ad hoc solutions, as theory is only starting to address fundamental differences between clustering methods ([2, 6, 3, 1]).", "startOffset": 233, "endOffset": 245}, {"referenceID": 0, "context": "Even the fairy basic problem of which algorithm to select for a given application (known as \u201cthe user\u2019s dilemma\u201d) is left to ad hoc solutions, as theory is only starting to address fundamental differences between clustering methods ([2, 6, 3, 1]).", "startOffset": 233, "endOffset": 245}, {"referenceID": 8, "context": "\u201cThe user\u2019s dilemma,\u201d has been tackled since the 70s ([9, 21]), yet we still do not have an adequate solution.", "startOffset": 54, "endOffset": 61}, {"referenceID": 20, "context": "\u201cThe user\u2019s dilemma,\u201d has been tackled since the 70s ([9, 21]), yet we still do not have an adequate solution.", "startOffset": 54, "endOffset": 61}, {"referenceID": 8, "context": "A formal approach to this problem (see, for example, [9, 6, 3]) proposes that we rely on succinct mathematical properties that reveal fundamental differences in the input-output behaviour of different clustering algorithms.", "startOffset": 53, "endOffset": 62}, {"referenceID": 5, "context": "A formal approach to this problem (see, for example, [9, 6, 3]) proposes that we rely on succinct mathematical properties that reveal fundamental differences in the input-output behaviour of different clustering algorithms.", "startOffset": 53, "endOffset": 62}, {"referenceID": 2, "context": "A formal approach to this problem (see, for example, [9, 6, 3]) proposes that we rely on succinct mathematical properties that reveal fundamental differences in the input-output behaviour of different clustering algorithms.", "startOffset": 53, "endOffset": 62}, {"referenceID": 16, "context": "According to these properties, there is never a reason to choose, say, algorithms based on the k-means objective ([17]), which often performs well in practice.", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "For example, k-means can be reconfigured to output a dendrogram using Ward\u2019s method and Bisecting k-mean, and classical hierarchical methods can be terminated using a variety of termination conditions ([14]) to obtain a single partition instead of a dendrogram.", "startOffset": 202, "endOffset": 206}, {"referenceID": 8, "context": "[9] introduced several properties of clustering algorithms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "In addition, [21] proposed a formalization of cluster analysis consisting of eleven axioms.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "Like earlier work, recent work on simple properties capturing differences in the input-output behaviour of clustering methods also focuses on the unweighed partitional ([2, 6, 3, 14]) and hierarchical settings ([1]).", "startOffset": 169, "endOffset": 182}, {"referenceID": 5, "context": "Like earlier work, recent work on simple properties capturing differences in the input-output behaviour of clustering methods also focuses on the unweighed partitional ([2, 6, 3, 14]) and hierarchical settings ([1]).", "startOffset": 169, "endOffset": 182}, {"referenceID": 2, "context": "Like earlier work, recent work on simple properties capturing differences in the input-output behaviour of clustering methods also focuses on the unweighed partitional ([2, 6, 3, 14]) and hierarchical settings ([1]).", "startOffset": 169, "endOffset": 182}, {"referenceID": 13, "context": "Like earlier work, recent work on simple properties capturing differences in the input-output behaviour of clustering methods also focuses on the unweighed partitional ([2, 6, 3, 14]) and hierarchical settings ([1]).", "startOffset": 169, "endOffset": 182}, {"referenceID": 0, "context": "Like earlier work, recent work on simple properties capturing differences in the input-output behaviour of clustering methods also focuses on the unweighed partitional ([2, 6, 3, 14]) and hierarchical settings ([1]).", "startOffset": 211, "endOffset": 214}, {"referenceID": 11, "context": "Particularly well-studied is the single-linkage algorithm, for which there are multiple property-based characterizations, showing that single-linkage is the unique algorithm that satisfies several sets of properties ([12, 6, 8]).", "startOffset": 217, "endOffset": 227}, {"referenceID": 5, "context": "Particularly well-studied is the single-linkage algorithm, for which there are multiple property-based characterizations, showing that single-linkage is the unique algorithm that satisfies several sets of properties ([12, 6, 8]).", "startOffset": 217, "endOffset": 227}, {"referenceID": 7, "context": "Particularly well-studied is the single-linkage algorithm, for which there are multiple property-based characterizations, showing that single-linkage is the unique algorithm that satisfies several sets of properties ([12, 6, 8]).", "startOffset": 217, "endOffset": 227}, {"referenceID": 1, "context": "More recently, the entire family of linkage-based algorithms was characterized ([2, 1]), differentiating those algorithms from other clustering paradigms by presenting some of the advantages of those methods.", "startOffset": 80, "endOffset": 86}, {"referenceID": 0, "context": "More recently, the entire family of linkage-based algorithms was characterized ([2, 1]), differentiating those algorithms from other clustering paradigms by presenting some of the advantages of those methods.", "startOffset": 80, "endOffset": 86}, {"referenceID": 2, "context": "In addition, previous property-based taxonomies in this line of work highlight the advantages of linkage-based methods ([3, 9]), and some early work focuses on properties that distinguish among linkage-based algorithms ([11]).", "startOffset": 120, "endOffset": 126}, {"referenceID": 8, "context": "In addition, previous property-based taxonomies in this line of work highlight the advantages of linkage-based methods ([3, 9]), and some early work focuses on properties that distinguish among linkage-based algorithms ([11]).", "startOffset": 120, "endOffset": 126}, {"referenceID": 10, "context": "In addition, previous property-based taxonomies in this line of work highlight the advantages of linkage-based methods ([3, 9]), and some early work focuses on properties that distinguish among linkage-based algorithms ([11]).", "startOffset": 220, "endOffset": 224}, {"referenceID": 8, "context": "This category is closely related to \u201cpoint proportion admissibility\u201d by [9].", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "Indeed, when a similar property was proposed by [9], it was presented as a desirable characteristic.", "startOffset": 48, "endOffset": 51}, {"referenceID": 18, "context": "We investigate the behaviour of a spectral objective function, ratio-cut ([19]), on weighted data.", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "Many popular partitional clustering paradigms, including k-means (see [17] for a detailed exposition of this popular objective function and related algorithms), k-median, and the min-sum objective ([16]), are weight sensitive.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "Many popular partitional clustering paradigms, including k-means (see [17] for a detailed exposition of this popular objective function and related algorithms), k-median, and the min-sum objective ([16]), are weight sensitive.", "startOffset": 198, "endOffset": 202}, {"referenceID": 14, "context": "As shown by [15], the k-means objective function is equivalent to \u2211 x,y\u2208Ci d(x, y) 2 \u00b7 w(x) \u00b7 w(y) w(Ci) .", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "We show that Ward\u2019s method ([20]), a successful linkage-based algorithm, as well as popular divisive hierarchical methods, are weight sensitive.", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "On the other hand, it is easy to see that the linkage-based algorithms single-linkage and complete-linkage are both weight robust, as was observed in [9].", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Ward\u2019s method is a highly effective clustering algorithm ([20]), which, at every step, merges the clusters that will yield the minimal increase to the sum-of-squares error (the k-means objective function).", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "For example, [18] studied a dozen different initializations.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "There are many other algorithms based on the k-means objective functions, some of the most notable being k-means++ ([4]) and the Hochbaum-Schmoys initialization ([10]) studied, for instance, by [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 9, "context": "There are many other algorithms based on the k-means objective functions, some of the most notable being k-means++ ([4]) and the Hochbaum-Schmoys initialization ([10]) studied, for instance, by [7].", "startOffset": 162, "endOffset": 166}, {"referenceID": 6, "context": "There are many other algorithms based on the k-means objective functions, some of the most notable being k-means++ ([4]) and the Hochbaum-Schmoys initialization ([10]) studied, for instance, by [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 12, "context": "Each iteration performs a swap only if a better cost is possible, and it stops when no changes are made [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 2, "context": "For example, one deterministic seeding approach involves selecting the k-furthest centers (see, for example, [3]).", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "The k-means++ algorithm, introduced by Arthur and Vassilvitskii ([4]) is the Lloyd algorithm with a randomized initialization method that aims to place the initial centers far apart from each other.", "startOffset": 65, "endOffset": 68}], "year": 2016, "abstractText": "One of the most prominent challenges in clustering is \u201cthe user\u2019s dilemma,\u201d which is the problem of selecting an appropriate clustering algorithm for a specific task. A formal approach for addressing this problem relies on the identification of succinct, user-friendly properties that formally capture when certain clustering methods are preferred over others. Until now these properties focused on advantages of classical Linkage-Based algorithms, failing to identify when other clustering paradigms, such as popular center-based methods, are preferable. We present surprisingly simple new properties that delineate the differences between common clustering paradigms, which clearly and formally demonstrates advantages of centerbased approaches for some applications. These properties address how sensitive algorithms are to changes in element frequencies, which we capture in a generalized setting where every element is associated with a real-valued weight. \u2217E-mail: mackerman@fsu.edu \u2020E-mail: shai@cs.uwaterloo.ca \u2021E-mail: simina.branzei@gmail.com \u00a7E-mail: dloker@cs.uwaterloo.ca 1 ar X iv :1 10 9. 18 44 v2 [ cs .L G ] 4 O ct 2 01 6", "creator": "LaTeX with hyperref package"}}}