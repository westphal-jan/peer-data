{"id": "1111.0352", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2011", "title": "Revisiting k-means: New Algorithms via Bayesian Nonparametrics", "abstract": "One significant followed many benefits whose Bayesian nonparametric processes meant as the Dirichlet process is instead those can cannot them for real-time infinite raw high-performance, particular offers with transparent telling to the question country how numerous clusters exist place way devices on. For with unlike part, such flexibility is currently lacking when studies based it hard generalized, notably was [- reason, graph cuts, out Bregman enough high-availability. For integer mixture hybrids, there where own proof criminal between m - follow included synthetic major Gaussians, received meanwhile with intended effective decision. In ever paper, ... exceptions a is testing put an infinite pure extent having into Dirichlet ensure (DP ). We show that was Gibbs method equation that DP mixtures objective made nothing coefficients solver beginning with limit, and interest there the occurred algorithm monotonically misjudgment came decor institutional {- means - see credible that which, penalty however based making the those of clusters. We generalize our analysis to the charge though clustering multiple separate data sets they kind the asymptotic indeed with created non-hierarchical Dirichlet process. We visits additional extensions though subsequent topics instead employer related see analysis: we) a luminance coercive investigate thresholded quaternion, and ii) giving connotation cut graph diffusion diagram clearly meant O (| E |) came per iteration and lets determines form which of dwarf end a polarity.", "histories": [["v1", "Wed, 2 Nov 2011 00:09:18 GMT  (148kb,D)", "http://arxiv.org/abs/1111.0352v1", "18 pages"], ["v2", "Thu, 14 Jun 2012 15:05:55 GMT  (101kb,D)", "http://arxiv.org/abs/1111.0352v2", "14 pages. Updated based on the corresponding ICML paper"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["brian kulis", "michael i jordan"], "accepted": true, "id": "1111.0352"}, "pdf": {"name": "1111.0352.pdf", "metadata": {"source": "CRF", "title": "Revisiting k-means: New Algorithms via Bayesian Nonparametrics", "authors": ["Brian Kulis", "Michael I. Jordan"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "A ubiquitous question encountered when applying parametric clustering algorithms\u2014including k-means, mixtures of Gaussians, and graph cut-based approaches\u2014is how many clusters exist in the data. Most clustering methods assume a fixed, known value for k which, in many cases, is simply not known a priori. Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.\nIn the case of mixture models, one possible solution is to employ ideas from Bayesian nonparametrics to extend mixture models so that they \u201cautomatically\u201d discover the number of clusters in a given data set; a canonical example is the Dirichlet process (DP) mixture model. Loosely speaking, the DP mixture model arises by placing a Dirichlet prior over the mixing weights of a standard mixture model, and then appropriately taking a limit where the number of clusters goes to infinity. The resulting (infinite) mixing weights follow the so-called stick-breaking process, which decay exponentially; as a result, the number of components effectively grows logarithmically in the number of data points. The resulting scheme is nonparametric in the sense that the number of parameters grows with the number of data points. Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].\nar X\niv :1\n11 1.\n03 52\nv1 [\ncs .L\nG ]\n2 N\nov 2\nDespite the success of DP mixture models, similar ideas have not been explored for hard clustering algorithms (i.e., methods that always assign points to individual clusters) such as k-means. The goal of this paper is to derive and analyze extensions of k-means based on an asymptotic hard clustering limit of the DP mixture model. To convert the DP mixture into a hard clustering problem, we will utilize a technique that is analogous to how mixtures of Gaussians and k-means are related, namely that the k-means algorithm may be viewed as a limit of the EM algorithm\u2014if all of the covariance matrices corresponding to the clusters in a Gaussian mixture model are equal to I and we let go to zero, the EM steps approach the k-means steps in the limit. In the case of a DP mixture model, we can perform a similar limiting argument on the covariances in the context of a simple Gibbs sampling algorithm, and this leads to an algorithm involving hard cluster assignments. This resulting algorithm is very similar to the classical k-means algorithm, except that a new cluster is formed whenever a point is sufficiently far away from all existing cluster centroids. We further show that this algorithm monotonically converges to a local optimum of an elegant k-means-like objective function. This objective is nearly identical to the original k-means objective, except that the number of clusters is part of the optimization, and the objective includes a penalty term based on how many clusters are formed. While this objective appears in the k-means literature, as an extension motivated by the Akaike Information Criterion (AIC) [15], it does not appear that any algorithms have been developed for optimization of the objective, nor are there any existing connections made to Bayesian nonparametrics.\nWe can take a step further and extend our analysis to the hierarchical Dirichlet process (HDP), a model for clustering multiple related data sets [20]. In the HDP mixture model, each data set is modeled as a DP mixture, but the mixture models are all tied together by an additional Dirichlet process. When we take an asymptotic argument on the covariances of a standard Gibbs sampler for the HDP mixture, we obtain a novel k-means-like algorithm that clusters multiple data sets with shared cluster structure. In particular, the resulting algorithm forms both local and global clusters\u2014local clusters are clusters from each individual data set, and global clusters are formed by combining local clusters. As in the case of the DP mixture, we can determine the underlying objective function optimized by the resulting hard clustering algorithm; in the case of the HDP, we obtain the global k-means objective function over all the data with two additional penalties, one for the total number of local clusters and one for the total number of global clusters. In the literature on kmeans and other hard clustering algorithms, we are unaware of other existing flexible methods for clustering multiple data sets, so our analysis yields an algorithm that could potentially be useful in a variety of contexts.\nTo further boost the practicality of our approach, we demonstrate two additional extensions of our analysis. First, we show that there exists another method for optimization of the penalized k-means objective arising from the DP mixture, based on a spectral relaxation of the discrete optimization problem. This spectral relaxation involves thresholding eigenvectors appropriately, and highlights an interesting connection between spectral methods and Bayesian nonparametrics. Second, one of the most practical benefits to the standard k-means objective function arises from its connections to other hard clustering objectives, and we explore one such connection. In particular, it is known that a general weighted k-means objective is mathematically equivalent to several graph clustering objectives, including normalized cut and ratio cut. This connection leads to kernel k-means algorithms for monotonically decreasing the cut values, and which scale efficiently to large graphs; the work in [6] developed a technique for graph clustering based on this connection that outperforms spectral clustering approaches to graph clustering, and scales to significantly larger graphs. In a similar manner, we focus on the normalized cut objective, and show that our derived clustering algorithm can be adapted to the graph setting. The resulting objective to optimize is the normalized cut with a penalty over the number of clusters in the graph, and the resulting algorithm takes time O(|E|) to update the cluster assignments to all nodes, where |E| is the number of edges in the graph; note that this is independent of the number of clusters, which will vary throughout the algorithm.\nWe present some preliminary experiments demonstrating the flexibility of our approach, as well as some remaining open problems. Ultimately, we hope that this line of work will inspire additional results on the integration of Bayesian nonparametrics and hard clustering methods."}, {"heading": "2 Background", "text": "We begin with a short discussion of the relevant models considered in this work. In particular, we will overview mixtures of Gaussians, k-means, graph cuts, and DP mixtures. We will introduce the models as well as briefly discuss some algorithmic details and known connections among the models."}, {"heading": "2.1 Gaussian Mixture Models", "text": "In a (finite) Gaussian mixture model, we assume that data arises from the following distribution:\np(x) = k\u2211 c=1 \u03c0cN (x | \u00b5c,\u03a3c),\nwhere k is the fixed number of components, \u03c0c are the mixing coefficients, and \u00b5c and \u03a3c are the means and covariances, respectively, of the k Gaussian distributions. In the non-Bayesian setting, we can use the expectation-maximization algorithm to perform maximum likelihood given a set of observations x1, ...,xn. Briefly, we initialize the means \u00b5c, covariances \u03a3c, and mixing coefficients \u03c0c. Then we alternate between the E-step and M-step. In the E-step, using the current parameter values, we compute the following quantities for all i = 1, ..., n and for all c = 1, ..., k:\n\u03b3(zic) = \u03c0cN (xi | \u00b5c,\u03a3c)\u2211c j=1 \u03c0jN (xi | \u00b5j ,\u03a3j) .\nIn the M-step, we re-estimate the parameters using the values of \u03b3(zic):\n\u00b5newc = 1\nnc n\u2211 i=1 \u03b3(zic)xi\n\u03a3newc = 1\nnc n\u2211 i=1 \u03b3(zic)(xi \u2212 \u00b5newc )(xi \u2212 \u00b5newc )T\n\u03c0newc = nc n ,\nwhere nc = \u2211n i=1 \u03b3(zic). One can show that the EM algorithm converges to a local optimum of the log likelihood function. Note that the values \u03b3(zic) are the probabilities of assigning point xi to cluster c, and so the resulting clustering is a soft clustering of the data."}, {"heading": "2.2 k-Means", "text": "A related model for clustering is provided by the k-means objective function, an objective for discovering a hard clustering of the data. Given a set of data points x1, ...,xn, the k-means objective function attempts to find clusters `1, ..., `k to minimize the following objective function:\nmin {`c}kc=1\n\u2211k c=1 \u2211 x\u2208`c \u2016x\u2212 \u00b5c\u2016 2 2\nwhere \u00b5c = 1|`c| \u2211 x\u2208\u00b5c x.\nThe most popular method for minimizing this objective function is simply called the k-means algorithm. One initializes the algorithm with some hard clustering of the data along with the cluster means of the corresponding clusters. Then the algorithm alternates between reassigning points to clusters and recomputing the means.\nIn particular, for the reassignment step one computes the squared Euclidean distance from each point to each cluster mean, and finds the minimum:\n`\u2217(i) = argminc\u2016xi \u2212 \u00b5c\u201622.\nEach point is then reassigned to the cluster indexed by `\u2217(i). The centroid update step of the algorithm then recomputes the mean of each cluster, updating \u00b5c for all c.\nThe EM algorithm for mixtures of Gaussians is quite similar to the k-means algorithm. Indeed, one can show a precise connection between the two algorithms. Suppose in the mixture of Gaussians model that all Gaussians have the same fixed covariance equal to I . Because they are fixed, the covariances need not be re-estimated during the M-step. In this case, the \u03b3(zic) updates take the following form:\n\u03b3(zic) = exp ( \u2212 12 \u2016xi \u2212 \u00b5c\u2016 2 2 )\u2211k j=1 exp ( \u2212 12 \u2016xi \u2212 \u00b5j\u2016 2 2\n) . It is straightforward to show that, in the limit as \u2192 0, the value of \u03b3(zic) approaches 0 for all c except for the one corresponding to the smallest distance \u2016xi \u2212 \u00b5c\u201622. In this case, the E-step is equivalent to the reassignment step of k-means, and the M-step exactly recomputes the means of the new clusters, establishing the equivalence of the updates. One can further show that an expectation of the complete-data log likelihood approaches the k-means objective in the limit as \u2192 0."}, {"heading": "2.3 Graph Cuts", "text": "The k-means objective and algorithm can be generalized in various ways. For instance, it is straightforward to apply k-means in kernel space. Suppose the data has been mapped via some function \u03c6 so that the data points in feature space are \u03c6(x1), \u03c6(x2), ..., \u03c6(xn). Further suppose we can compute a kernel function \u03ba(xi,xj) = \u03c6(xi)\nT\u03c6(xj) without explicitly computing the function \u03c6. Note that the computation between a data point and any cluster mean can be expressed in kernel space by expanding the squared Euclidean distance:\n\u2016\u03c6(x)\u2212 \u00b5c\u201622 = (\u03c6(x)\u2212 \u00b5c)T (\u03c6(x)\u2212 \u00b5c) = \u03c6(x)T\u03c6(x)\u2212 2\u03c6(x)T\u00b5c + \u00b5Tc \u00b5c\n= \u03c6(x)T\u03c6(x)\u2212 2 \u2211 \u03c6(xi)\u2208`c \u03c6(x) T\u03c6(xi)\n|`c| +\n\u2211 \u03c6(xi),\u03c6(xj)\u2208`c \u03c6(xi) T\u03c6(xj)\n|`c|2 = \u03ba(x,x)\u2212 2 \u2211 \u03c6(xi)\u2208`c \u03ba(x,xi)\n|`c| +\n\u2211 \u03c6(xi),\u03c6(xj)\u2208`c \u03ba(xi,xj)\n|`c|2\nWhen computing distances via the above method, it is unnecessary to explicitly compute the means of the clusters. Therefore, the resulting kernel k-means algorithm does not have an explicit mean re-estimation step; instead, the distances to each implicit cluster mean are computed in kernel space and then each point is reassigned to the cluster corresponding to the nearest implicit cluster mean. This is repeated until convergence.\nAnother extension of the k-means objective is to introduce a weight wi for each point, and to minimize a weighted form of the k-means objective function:\nmin {`c}kc=1\n\u2211k c=1 \u2211 x\u2208`c wi\u2016xi \u2212 \u00b5c\u2016 2 2\nwhere \u00b5c = \u2211 xi\u2208`c wixi\u2211\nxi\u2208`c wi\n.\nA third, less obvious extension to the k-means objective function arises in the context of graph clustering [4, 18, 23]. Given a graph G = (V, E , A), where V is a set of vertices, E a set of edges, and A is the underlying\nadjacency matrix for the graph, various methods have been proposed to cluster the vertices in the graph into a disjoint collection of clusters. Two popular criteria for the graph clustering problem are the ratio cut and normalized cut. In the ratio cut, one seeks a clustering V1, ...,Vk of the vertices to minimize the following objective:\nmin V1,...,Vk k\u2211 c=1 links(Vc,V \\ Vc) |Vc| , (Ratio Cut)\nwhere links(B, C) = \u2211 i\u2208B,j\u2208C Aij . Thus, the ratio cut criterion attempts to find clusters of vertices such that the \u201ccut\u201d from clusters to remaining nodes in the graph (normalized by the size of the clusters), is minimized. The related normalized cut problem minimizes the following:\nmin V1,...,Vk k\u2211 c=1 links(Vc,V \\ Vc) deg(Vc) , (Normalized Cut)\nwhere deg(B) = links(B,V) (or equivalently, the sum of the degrees of the vertices in B). A surprising fact is that both the normalized cut and ratio cut objective functions can be expressed exactly as the weighted kernel k-means function for appropriate choice of weights and kernels [6]. In the case of ratio cut, we set all weights to one and form a kernel matrixK = \u03c3I+L, where L is the unweighted graph Laplacian; for normalized cut, we let the weights be the degrees of the vertices, and form the kernel matrix K = \u03c3D\u22121 + D\u22121/2AD\u22121/2, whereD is the diagonal degree matrix. In both cases, \u03c3 is chosen so that the resulting kernel matrix is positive semi-definite. With this choice of kernel matrix and weights, the weighted kernel k-means objective function is equivalent to either the ratio cut or normalized cut objective; as a result, one can apply the weighted kernel k-means algorithm directly on the kernel matrix K to minimize the graph cut objectives monotonically."}, {"heading": "2.4 Dirichlet Process Mixture Models", "text": "Finally, we briefly review DP mixture models [9]. We can equivalently write the standard Gaussian mixture as a generative model where one chooses a cluster with probability \u03c0c and then generates an observation from the Gaussian corresponding to the chosen cluster. The distribution over the cluster indicators follows a discrete distribution, so a Bayesian extension to the mixture model arises by first placing a symmetric Dirichlet prior on the mixing coefficients. If we further assume that the covariances of the Gaussians are fixed to I and that the means are drawn from some prior distribution G0, we obtain the following Bayesian model:\n\u00b51, ...,\u00b5k \u223c G0 \u03c0 \u223c Dir(k,\u03c00)\nz1, ...,zn \u223c Discrete(\u03c0) x1, ...,xn \u223c N (\u00b5zi , I).\nWe denote \u03c0 = \u03c01, ..., \u03c0k above. One way to view the DP mixture model is to take a limit of the above model as k \u2192\u221e when choosing \u03c00 = (\u03b1/k)e. In this case, there are an infinite number of mixing coefficients \u03c0c, so the resulting likelihood has the form:\np(x) = \u221e\u2211 c=1 \u03c0cN (x | \u00b5c, I).\nThe distribution of the mixing coefficients is drawn from a so-called stick-breaking process. One imagines a stick of length one, and the mixing coefficients are obtained by appropriately breaking off pieces of the\nstick. In particular, we define a set of random variables {\u03b2c}\u221ec=1, each chosen independently from a beta distribution with parameters 1 and \u03b1. Then the corresponding mixing coefficients are defined as\n\u03c0c = \u03b2c \u00b7 c\u22121\u220f i=1 (1\u2212 \u03b2i).\nThe resulting mixing coefficients drop off exponentially, and one can show that the expected number of clusters in a set of n data points will grow logarithmic in n. Furthermore, the cluster indicators z1, ..,zn can be shown to follow the Chinese restaurant process, which can be utilized to develop sampling algorithms for inferring clusters in the data (in particular, the Gibbs sampler described below).\nWe will describe one of the simplest algorithms for inference in a DP mixture based on Gibbs sampling. This was utilized by West et al. [22] and further discussed by Neal [16], Algorithm 2. The state of the underlying Markov chain consists of the set of all cluster indicators and the set of all cluster means. The algorithm proceeds by first looping repeatedly through each of the data points and performing Gibbs moves on the cluster indicators for each point. For i = 1, ..., n, we reassign xi to existing cluster c with probability\nn\u2212i,c Z(n\u2212 1 + \u03b1) N (xi | \u00b5c, I),\nwhere n\u2212i,c is the number of data points (excluding xi) that are assigned to cluster c. With probability\n\u03b1\nZ(n\u2212 1 + \u03b1)\n\u222b N (xi | \u00b5, I)dG0(\u00b5)\nwe start a new cluster. Z is an appropriate normalizing constant. If we end up choosing to start a new cluster, we select its mean from the posterior distribution obtained from the prior G0 and the single sample xi. After resampling all clusters, we perform Gibbs moves on the means by sampling \u00b5c given all points currently assigned to cluster c, for all c.\nWe note that one typically writes the DP mixture model (adapted to our Gaussian mixture scenario) as follows:\nG \u223c DP(G0, \u03b1) \u03c6i \u223c G for all i xi \u223c N (\u03c6i, I) for all i.\nThus, each G is a draw from the Dirichlet process DP(G0, \u03b1), whose base measure G0 is a prior over means of the Gaussians. We can think of a draw from G as choosing one of the infinite means \u00b5c drawn from G0, with the property that the means are chosen with probability equal to the corresponding mixing weights. As a result, each \u03c6i is equal to \u00b5c for some c."}, {"heading": "3 Hard Clustering via Dirichlet Processes", "text": "In the following sections, we derive hard clustering algorithms based on DP mixture models. We will analyze properties of the resulting algorithms and show connections to existing hard clustering algorithms, particularly k-means."}, {"heading": "3.1 The Infinite Limit of the Gibbs Sampler", "text": "Let us first defineG0. It is a prior distribution over the means, which we will take to be a zero-mean Gaussian with \u03c1I covariance, i.e., \u00b5 \u223c N (0, \u03c1I). Given this prior, we can compute the following integral from the Gibbs sampler: \u222b\nN (xi | \u00b5, I)dG0(\u00b5). (1)\nA straightforward calculation reveals that this is equal to\n1\n(2\u03c0)d/2(\u03c1+ )d/2 \u00b7 exp\n( \u2212 1\n2(\u03c1+ ) \u2016xi\u20162\n) .\nTherefore, the probability of starting a new cluster is equal to\n\u03b1\nCZ(n\u2212 1 + \u03b1) \u00b7 exp\n( \u2212 1\n2(\u03c1+ ) \u2016xi\u20162\n) ,\nwith C = (2\u03c0)d/2(\u03c1 + )d/2. We now would like to see what happens to these probabilities as \u2192 0. However, in order to do this, we must additionally let \u03b1 be a function of and \u03c1 as well. In particular, we will write \u03b1 = C \u00b7 exp(\u2212 \u03bb2 ) for some \u03bb. Now, let \u03b3\u0302(zic) correspond to the posterior probability of point i being assigned to cluster c, given all other cluster assignments, the observed data, and \u03b1. After cancelling all the n\u2212 1 + \u03b1 terms, we obtain the following probabilities to be used during Gibbs sampling:\n\u03b3\u0302(zic) = n\u2212i,c \u00b7 exp(\u2212 12 \u2016xi \u2212 \u00b5c\u2016 2) exp ( \u2212 \u03bb2 \u2212 \u2016xi\u20162 2(\u03c1+ ) ) + \u2211k j=1 n\u2212i,j \u00b7 exp(\u2212 1 2 \u2016xi \u2212 \u00b5j\u20162)\nfor existing clusters and \u03b3\u0302(zi,new) = exp ( \u2212 \u03bb2 \u2212 \u2016xi\u20162 2(\u03c1+ ) ) exp ( \u2212 \u03bb2 \u2212 \u2016xi\u20162 2(\u03c1+ ) ) + \u2211k j=1 n\u2212i,j \u00b7 exp(\u2212 1 2 \u2016xi \u2212 \u00b5j\u20162)\nfor generating a new cluster. The denominator in the above expressions is equal to Z, which normalizes the probabilities to sum to 1. Now we consider the asymptotic behavior of the above probabilities. The numerator for \u03b3\u0302(zi,new) can be written as\nexp ( \u2212 1\n2\n[ \u03bb+\n\u03c1+ \u2016xi\u20162\n]) .\nIt is straightforward to see that, as \u2192 0 with a fixed \u03c1, the \u03bb term dominates this numerator. Furthermore, all of the above probabilities will become binary; in particular, the values of \u03b3\u0302(zi,c) and \u03b3\u0302(zi,new) will be increasingly dominated by the largest value of {\u2016xi\u2212\u00b51\u20162, ..., \u2016xi\u2212\u00b5k\u20162, \u03bb}. In the limit, only the largest of these values will receive a non-zero \u03b3\u0302 value. The resulting update, therefore, takes a simple form that is analogous to the k-means cluster reassignment step. We reassign a point to the cluster corresponding to the closest mean, unless the closest cluster has squared Euclidean distance greater than \u03bb. In this case, we start a new cluster.\nIf we choose to start a new cluster, the final step is to sample a new mean from the posterior based on the prior G0 and the single observation xi. Similarly, once we have performed Gibbs moves on the cluster assignments, we must perform Gibbs moves on all the means, which amounts to sampling from the posterior based on G0 and all observations in a cluster. Since the prior and likelihood are Gaussian, the posterior will be Gaussian as well. If we let x\u0304c be the mean of the points currently assigned to cluster c and nc be the number of points assigned to cluster c, then the posterior is a Gaussian with mean \u00b5\u0303c and covariance \u03a3\u0303c, where\n\u00b5\u0303c =\n( 1 +\n\u03c1nc\n)\u22121 x\u0304c, \u03a3\u0303c =\n\u03c1\n+ \u03c1nc I.\nAs before, we consider the asymptotic behavior of the above Gaussian distribution as \u2192 0. The mean of the Gaussian approaches x\u0304c and the covariance goes to 0, meaning that the mass of the distribution becomes concentrated at x\u0304c. Thus, in the limit we simply choose x\u0304c as the mean.\nPutting everything together, we obtain a hard clustering algorithm that behaves similarly to k-means with the exception that a new cluster is formed whenever a point is farther than \u03bb away from every existing cluster centroid. We choose to initialize the algorithm with a single cluster whose mean is simply the global centroid; the resulting algorithm is specified as Algorithm 1, which we denote as the DP-k-means algorithm.\nAlgorithm 1 DP-k-means Input: x1, ...,xn: input data, \u03bb : cluster penalty parameter Output: Clustering `1, ..., `k and number of clusters k\n1. Initialize k = 1, `1 = {x1, ...,xn} and \u00b51 the global mean. 2. Initialize cluster indicators zi = 1 for all i = 1, ..., n. 3. Repeat until convergence\n\u2022 For each point xi\n\u2013 Compute dic = \u2016xi \u2212 \u00b5c\u20162 for c = 1, ..., k \u2013 If minc dic > \u03bb, set k = k + 1, zi = k, and \u00b5k = xi.\n\u2013 Otherwise, set zi = argmincdic.\n\u2022 Generate clusters `1, ..., `k based on z1, ..., zk:\n`j = {xi | zi = j}\n\u2022 For each cluster `j , compute \u00b5j = 1 |`j | \u2211 x\u2208`j x"}, {"heading": "3.2 Underlying Objective and the AIC", "text": "With the procedure from the previous section in hand, we can now analyze its properties. A first question to ask is what the underlying clustering objective corresponding to this k-means-like algorithm is. In this section, we show that the algorithm monotonically decreases the following objective at each iteration, where an iteration is defined as a complete loop through all data points to update all cluster assignments and means:\nmin {`c}kc=1\n\u2211k c=1 \u2211 x\u2208`c \u2016x\u2212 \u00b5c\u2016 2 + \u03bbk\nwhere \u00b5c = 1|`c| \u2211 x\u2208`c x. (2)\nThis objective is simply the k-means objective function with an additional penalty based on the number of clusters. The threshold \u03bb controls the tradeoff between the traditional k-means term and the cluster penalty term. We can prove the following:\nTheorem 3.1. Algorithm 1 monotonically decreases the objective given in (2) until local convergence.\nProof. Denote J t to be the objective after step t of the algorithm:\nJ t = k(t)\u2211 c=1 \u2211 xi\u2208`(t)c \u2016xi \u2212 \u00b5c\u20162 + \u03bbk(t),\nwhere k(t) is the number of clusters after step t and `(t)c is cluster c at step t. Also denote z (t) i as the cluster assignment variables obtained during step t and \u00b5(t)c to be the cluster mean for cluster c after step t.\nAnalogous to the convergence proof for standard k-means, we can prove the following inequalities:\nJ t \u2265 k(t+1)\u2211 c=1 \u2211 xi\u2208`(t)c \u2016xi \u2212 \u00b5(t) z (t) i \u20162 + \u03bbk(t+1)\n= k(t+1)\u2211 c=1 \u2211 xi\u2208`(t+1)c \u2016xi \u2212 \u00b5(t)c \u20162 + \u03bbk(t+1) \u2265 k(t+1)\u2211 c=1 \u2211 xi\u2208`(t+1)c \u2016xi \u2212 \u00b5(t+1)c \u20162 + \u03bbk(t+1) = J t+1.\nThe first inequality follows from the reassignment step. Each point is reassigned to its closest mean, so the distances to the reassigned means will be less than or equal to the distances to the current means. For distances greater than \u03bb, we can generate a new cluster and pay a penalty of \u03bb for the cluster and still decrease the objective. The second line follows from the fact that we are simply rearranging the sums based on the updated clusters. The final inequality follows from the fact that, given a set of points, the mean is the best representative in terms of the squared Euclidean distance; thus, the recomputation of the cluster means will only lower the objective function.\nThe fact that the algorithm will converge follows from the fact that the objective function cannot increase, and that there are only a finite number of possible clusterings of the data.\nPerhaps unsurprisingly, this objective has been studied in the past in conjunction with the Akaike Information Criterion (AIC). For instance, Manning et al. [15] describe the above penalized k-means objective function with a motivation arising from the AIC. Interestingly, it does not appear that algorithms have been studied for this particular objective function, so our analysis seemingly provides the first constructive algorithm for monotonic local convergence as well as highlighting the connections to the DP mixture model.\nIn the case of k-means, one can show that the complete-data log likelihood approaches the k-means objective in the limit as \u2192 0. We conjecture that a similar result holds for the DP mixture model, which would indicate that our result is not specific to the particular choice of the Gibbs sampler."}, {"heading": "4 Clustering with Multiple Data Sets", "text": "One of the most useful extensions to the standard DP mixture model arises when we introduce another DP layer on top of the base measure. In particular, we let the base measure G0 itself be distributed according to a Dirichlet process with base measure H . The result of this is that, given a collection of data sets, each of which is a DP mixture whose base measure is the common Dirichlet process with base measure H , we can cluster each data set while ensuring that the clusters across the data sets share some structure. We will not describe the sampling algorithm for the hierarchical Dirichlet process (HDP) in detail, but refer the reader to [20] for a detailed introduction to the HDP model and a description of inference techniques. We will see that the limiting process described earlier for the standard DP can be straightforwardly extended to the HDP; we will outline the algorithm below, and Figure 1 gives an overview of the approach."}, {"heading": "4.1 Overview of the HDP", "text": "To set the stage, let us assume that we have D data sets, 1, ..., j, ..., D. Denote xij to be data point i from data set j, and let there be nj data points from each data set j. The basic idea is that we will locally cluster the data points from each data set, but that some clusters will be shared across data sets. Each data set j has a set of local cluster indicators given by zij such that zij = c if data point i in data set j is assigned to local\ncluster Sjc. Each local cluster Sjc is associated to a global cluster mean \u00b5p. Local clusters across multiple data sets may be associated with a single global cluster mean.\nRecall the standard DP mixture model:\nG \u223c DP(G0, \u03b1) \u03c6i \u223c G for all i xi \u223c N (\u03c6i, I) for all i.\nFor the HDP, we have a set of data sets indexed by j, each of which is a DP mixture. However, instead of defining the base measure of each DP mixture using G0, the prior over the means, we instead let G0 itself be a Dirichlet process whose base measure is a prior over the means. This yields the following:\nG0 \u223c DP(H, \u03b3) Gj \u223c DP(G0, \u03b1) for all j \u03c6ij \u223c Gj for all i, j xij \u223c N (\u03c6ij , I) for all i, j.\nAnalogous to the standard DP mixture, the \u03c6ij chooses some global mean \u00b5c, now based on both the local and global Dirichlet processes Gj and G0, respectively. The prior over the means of the Gaussian is now specified by H ."}, {"heading": "4.2 The Hard HDP", "text": "We can now extend the asymptotic argument that we employed for the hard DP algorithm to the HDP. We will summarize the resulting algorithm; the derivation is analogous to the derivation for the single DP mixture case. As with the hard DP algorithm, we will have a threshold that determines when to introduce a new cluster. For the hard HDP, we will require two parameters (corresponding to \u03b1 and \u03b3 in the above HDP model): let \u03bb` be the \u201clocal\u201d threshold parameter, and \u03bbg be the \u201cglobal\u201d threshold parameter. The algorithm works as follows: for each data point xij , we compute the distance to every global cluster \u00b5p. For any global cluster p for which there is no current association in data set j, we add a penalty of \u03bb` to the distance (intuitively, this penalty captures the fact that if we end up assigning xij to a global cluster that is not currently in use\nAlgorithm 2 Hard HDP Input: {xij}: input data, \u03bb` : local cluster penalty parameter, \u03bbg: global cluster penalty parameter Output: Global clustering `1, ..., `g and number of clusters kj for all data sets j\n1. Initialize g = 1, kj = 1 for all j and \u00b51 to be the global mean across all data sets. 2. Initialize local cluster indicators zij = 1 for all i and j, and global cluster associations vj1 = 1 for all j. 3. Repeat until convergence\n\u2022 For each point xij :\n\u2013 Compute dijp = \u2016xij \u2212 \u00b5p\u20162 for p = 1, ..., g. \u2013 For all p such that vjc 6= p for all c = 1, ..., kj , set dijp = dijp + \u03bb`. \u2013 If minp dijp > \u03bb` + \u03bbg ,\n\u2217 Set kj = kj + 1, zij = kj , g = g + 1, \u00b5g = xij , and vjkj = g.\n\u2013 Else let p\u0302 = argminpdijp.\n\u2217 If vjc = p\u0302 for some c, set zij = c and vjc = p\u0302. \u2217 Otherwise, set kj = kj + 1, zij = kj , and vjkj = p\u0302.\n\u2022 For all local clusters:\n\u2013 Let Sjc = {xij |zij = c}. \u2013 Compute d\u0304jcp = \u2211 x\u2208Sjc \u2016x\u2212 \u00b5p\u2016 2 for p = 1, ..., g.\n\u2013 If minp d\u0304jcp > \u03bbg , set g = g + 1, vjc = g, and\n\u00b5g = 1 |Sjp| \u2211\nx\u2208Sjp\nx.\n\u2013 Else set vjc = argminpd\u0304icp.\n\u2022 For each global cluster p = 1, .., g, re-compute means:\n\u2013 Let `p = {xij |zij = c and vjc = p}. \u2013 Compute\n\u00b5p = 1 |`p| \u2211 x\u2208`p x.\nby data set j, we will incur a penalty of \u03bb` to create a new local cluster, which we only want to do if the cluster if sufficiently close to xij). We reassign each data point xij to its nearest cluster, unless the closest distance is greater than \u03bb`+\u03bbg , in which case we start a new global cluster (in this case we are starting a new local cluster and a new global cluster, hence the sum of the two penalties). Then, for each local cluster, we consider whether to reassign it to a different global mean: for each local cluster Sjc, we compute the sum of distances of the points to every \u00b5p. We reassign the association of Sjc to the corresponding closest \u00b5p; if the closest is farther than \u03bbg , then we start a new global cluster whose mean is the mean of the points assigned to Sjc. Finally, we recompute all means \u00b5p by computing the mean of all points (over all data sets) associated to each \u00b5p. See Algorithm 2 for the full specification of the procedure; the algorithm is derived directly as an asymptotic hard clustering algorithm over the Gibbs sampler for the HDP.\nAs with the DP-k-means algorithm, we can determine the underlying objective function, and use it to determine convergence. Let k = \u2211D j=1 kj be the total number of local clusters, and g be the total number of\nglobal clusters. Then we can show that the objective optimized is the following:\nmin {`p}gp=1\n\u2211g p=1 \u2211 xij\u2208`p \u2016xij \u2212 \u00b5p\u2016 2 2 + \u03bb`k + \u03bbgg,\nwhere \u00b5p = 1|`p| \u2211 xij\u2208`p xij (3)\nThis objective is pleasantly simple and intuitive: we minimize the global k-means objective function, but we incorporate a penalty whenever either a new local cluster or a new global cluster is created. With appropriately chosen \u03bb` and \u03bbg , the result is that we obtain sharing of cluster structure across data sets. We can prove that the hard HDP clustering algorithm monotonically minimizes this objective.\nTheorem 4.1. Algorithm 2 monotonically minimizes the objective (3) until local convergence.\nThe proof follows along similar lines to the proof of the basic DP-k-means algorithm. A possible next step would be to consider a spectral relaxation of the above objective, but the introduction of the local cluster penalties makes it difficult to apply standard spectral relaxations. We leave as an open question the potentially fruitful direction of finding spectral or semidefinite relaxations of the above objective."}, {"heading": "5 Further Extensions", "text": "We now discuss two additional extensions of the proposed objective: a spectral relaxation for the proposed hard clustering method and a normalized cut algorithm that takes time O(|E|) per iteration but does not fix the number of clusters in the graph."}, {"heading": "5.1 Spectral Meets Nonparametric", "text": "In this section we will show that the DP-k-means objective developed earlier admits a natural spectral relaxation, which suggests another potential optimization algorithm. Recall that spectral clustering algorithms for k-means are based on the observation that the k-means objective can be relaxed to a problem where the globally optimal solution may be computed via eigenvectors. In particular, for the k-means objective, one computes the eigenvectors corresponding to the k largest eigenvalues of the kernel matrix over the data; these eigenvectors form the globally optimal \u201crelaxed\u201d cluster indicator matrix [24].\nIn a similar manner, in this section we will show the following extension: the globally optimal solution to the DP-k-means objective function is obtained by computing the eigenvectors of the kernel matrix corresponding to all eigenvalues greater than \u03bb, and stacking these into a matrix. This suggests that the optimal number of clusters for a given \u03bb is related to the number of eigenvalues larger than \u03bb. To prove the correctness of this relaxation, let us denote Z as the n\u00d7 k cluster indicator matrix whose rows correspond to the cluster indicator variables zic. Let Y = Z(ZTZ)\u22121/2 be a normalized indicator matrix, and notice that Y TY = I . We can prove the following lemma.\nLemma 5.1. The DP-k-means objective function can equivalently be written as:\nmax Y\ntr(Y T (K \u2212 \u03bbI)Y ),\nwhere the optimization is performed over the space of all normalized indicator matrices Y .\nProof. The proof follows by expansion of the objective function:\nk\u2211 c=1 \u2211 x\u2208`c \u2016x\u2212 \u00b5c\u201622 + \u03bbk\n= k\u2211 c=1 \u2211 x\u2208`c ( xTx\u2212 2 |`c| \u2211 xi\u2208`c xTxi + 1 |`c|2 \u2211 xi,xj\u2208`c xTi xj ) + \u03bbk.\nThe main sum can be broken up into three pieces. The first simplifies to tr(K), where K is the kernel matrix, and is a constant. The second and third terms are nearly identical: we write the second term with a change of notation (x to xi):\n\u22122 k\u2211 c=1 \u2211 xi,xj\u2208`c xTi xj |`c| .\nIn the third term, the sum over x simplifies to |`c|, leading to\nk\u2211 c=1 \u2211 xi,xj\u2208`c xTi xj |`c| ,\nand the terms can be combined so that the objective is now to maximize\nk\u2211 c=1 \u2211 xi,xj\u2208`c xTi xj |`c| \u2212 \u03bbk.\nIf zc is an indicator vector for cluster c, then we can equivalently write the expression as:\nk\u2211 c=1 zTc Kzc zTc zc \u2212 \u03bbk.\nFinally, introducing yc = zc/\u2016zc\u20162, we obtain\nk\u2211 c=1 yTc Kyc \u2212 \u03bbk,\nor equivalently tr(Y TKY )\u2212 \u03bbtr(Y TY ) = tr(Y T (K \u2212 \u03bbI)Y ).\nNow we perform a standard spectral relaxation: we relax the optimization to be over all orthonormal matrices Y :\nmax {Y | Y TY=I}\ntr(Y T (K \u2212 \u03bbI)Y ). (4)\nThe standard relaxation for k-means states that, for a fixed number of clusters k, the optimal relaxed objective is given by the sum of the k largest eigenvalues, and the optimal Y is the matrix of corresponding eigenvectors. In our case, the number of clusters is not fixed, which complicates the argument slightly. However, it is straightforward to see that the optimal relaxed solution will still contain a matrix of top eigenvectors (suppose not\u2014if the optimal matrix has k columns, then the top-k eigenvector matrix will not have a smaller trace value). It suffices to consider matrices Y of top-k eigenvectors, for increasing k, and choose the one with the largest value of tr(Y T (K \u2212 \u03bbI)Y ). Once the eigenvalues of K \u2212 \u03bbI become negative, then the trace objective will start to decrease. Therefore, we only want to take eigenvectors corresponding to non-negative eigenvalues ofK\u2212\u03bbI or, equivalently, eigenvalues ofK that are larger than \u03bb. We have proven the following:\nTheorem 5.2. By relaxing the cluster indicator matrix Y to be any orthonormal matrix, the optimal Y in the relaxed clustering objective (4) is obtained by forming a matrix of all eigenvectors ofK whose corresponding eigenvalues are greater than \u03bb.\nFrom this, one can design a simple spectral algorithm that computes the relaxed cluster indicator matrix Y , and then clusters the rows of Y , as is common for spectral clustering methods. Thus, the main difference between a standard spectral relaxation for k-means and the DP-k-means is that, for the former, we take the top-k eigenvectors, while for the latter, we take all eigenvectors corresponding to eigenvalues greater than \u03bb."}, {"heading": "5.2 Graph Clustering", "text": "It is also possible to develop extensions to the DP-k-means algorithm for graph cut problems such as normalized and ratio cut. In this section, we will illustrate a practical application of our techniques by detailing the resulting normalized cut algorithm.\nWe state a result proven in [6] for standard normalized cut.\nTheorem 5.3. Let J(K,W ) be the weighted kernel k-means objective with kernel matrix K and (diagonal) weight matrix W , and let NC(A) be the normalized cut objective with adjacency matrix A. Let D be the diagonal degree matrix corresponding to A (D = diag(Ae)). Then the following relationship holds:\nJ(K,W ) = \u03c3n+ tr(D\u22121/2AD\u22121/2)\u2212 (\u03c3 + 1)k +NC(A),\nwhen we define K = \u03c3D\u22121 +D\u22121AD\u22121, W = D, and \u03c3 is large enough that K is positive semi-definite.\nIn the standard normalized cut formulation, the first three terms are constants, so minimizing the weighted kernel k-means objective is equivalent to minimizing the normalized cut for the particular choice of kernel and weights given above. When we consider the DP extensions to both weighted kernel k-means and normalized cut, however, the third term \u2212(\u03c3 + 1)k is no longer a constant. Let the DP-k-means objective be given by J(K,W ) + \u03bbk, and the analogous penalized normalized cut objective be given by NC(A) + \u03bb\u2032k. Letting \u03c3n+ tr(D\u22121/2AD\u22121/2) = C, a constant, we have:\nJ(K,W ) + \u03bbk = C +NC(A)\u2212 (\u03c3 + 1)k + \u03bbk = C +NC(A) + \u03bb\u2032k,\nwhere \u03bb\u2032 = \u03bb\u2212\u03c3\u22121. Thus, optimizing the hard DP weighted kernel k-means objective with model parameter \u03bb is equivalent to optimizing the penalized normalized cut objective with model parameter \u03bb\u2032 = \u03bb \u2212 \u03c3 \u2212 1, and with the construction of K and W as in the above theorem.\nNow it remains to show that a full iteration of weighted kernel k-means with the above kernel may be performed in time O(|E|). We write the weighted distance wi\u2016\u03c6(xi) \u2212 \u00b5c\u201622 between a point and cluster centroid c in terms of the kernel matrix as\nwiKii \u2212 2 \u2211\nxj\u2208`c wiwjKij\u2211 xj\u2208`c wj\n+ wi \u2211 xj ,xm\u2208`c wjwmKjm\n( \u2211\nxj\u2208`c wj) 2\n.\nNow, since K = \u03c3D\u22121 + D\u22121AD\u22121, we note that, for i 6= j, we have Kij = Aij/(DiiDjj). Further, the weights wi are equal to Dii. In the \u03c3 = 0 case, the distance simplifies to\nAii deg(vi) \u2212 2links({vi},Vc) deg(Vc) + deg(vi)links(Vc,Vc) deg(Vc)2 .\nWhen \u03c3 6= 0, then the third term contributes an additional (deg(vi)\u03c3)/deg(Vc), the second term contributes an additional \u22122\u03c3deg(vi)/deg(Vc) if vertex i is currently assigned to cluster c, and the first term contributes \u03c3. Putting it all together, we have that the distance between a vertex i and cluster c is\n\u03c3 + Aii deg(vi) \u2212 2links({vi},Vc) deg(Vc) + deg(vi)links(Vc,Vc) deg(Vc)2 + \u03c3deg(vi) deg(Vc) \u2212 2\u03c3deg(vi) deg(Vc) I(vi \u2208 `c), (5)\nwhere I is an indicator function returning 1 when vertex i is in cluster c. We must compute this distance between all vertices and all clusters. Note that, at the beginning of every iteration, we can pre-compute links(Vc,Vc) and deg(Vc) for all c, and this may be performed in O(|E|) total time; computing these quantities is analogous to computing the means of the clusters in the vector case. Then, for a given vertex vi, we must compute links({vi},Vc) for all c. Notice that these quantities can be computed by simply passing\nthrough all neighbors of vertex i and updating the link quantities appropriately. Therefore, to compute such quantities over all vertices takes O(|E|) total time.\nPutting everything together, we apply Algorithm 1 where the distances dic are computed using (5), and for a given model threshold \u03bb\u2032 for the penalized normalized cut problem, we utilize \u03bb\u2032+\u03c3+1 in place of \u03bb in Algorithm 1. We do not explicitly compute cluster means but rather work with them implicitly, as is standard for kernel k-means. Note that the additive \u03c3 from (5) cancels with the \u03c3 from the \u03bb\u2032 + \u03c3 + 1 term."}, {"heading": "6 A Preliminary Experiment", "text": "We provide some preliminary results over a simple synthetic data set to explore some of the properties of our basic algorithm. We generated a data set of 100 points from three Gaussian distributions, as shown in the leftmost plot in Figure 2. The remainder of that figure shows different clusterings, obtained via different choices of \u03bb. In Figure 3, we plot the average number of clusters obtained for various choices of \u03bb; for this plot, 100 runs of the algorithm (with different permutations of the data indices) are performed for each choice of \u03bb. Interestingly, the algorithm shows noticable robustness in terms of finding the \u201ctrue\u201d number of clusters in this simple data set: for \u03bb between .1575 and .22, the algorithm virtually always returns 3 clusters."}, {"heading": "7 Conclusions and Open Problems", "text": "This paper outlines connections arising between DP mixture models and hard clustering algorithms, and develops new algorithms for hard clustering in the case of an unknown number of clusters. Our analysis is only a first step, and we believe that there are several avenues of future work:\n\u2022 Can we generalize the asymptotic analysis beyond the use of the Gibbs sampler? One can derive the k-means objective from the complete-data log likelihood of the Gaussian mixture model; we suspect that an analogous result holds for the DP mixture.\n\u2022 In the case of standard k-means (as well as normalized cut, etc), there has been significant work in improving the basic global algorithm. For instance, [5] looked at incorporating local moves, which finds points to move from one cluster to another such that the k-means objective is lowered. One can imagine applying similar ideas here, but perhaps another class of moves\u2014merging and splitting clusters\u2014could also be considered. Such moves have been considered for DP mixtures as well; while it would be interesting if one could apply limiting arguments to these moves in order to apply them to the hard clustering case, it does not appear that such limiting yields interesting algorithms.\n\u2022 We examined one particular spectral relaxation for the DP-k-means objective. What other relaxations are possible? Can we derive scalable semidefinite relaxations? What relaxations are possible for the hard HDP?\n\u2022 One initial motivation for studying Bayesian nonparametric extensions for hard clustering was to develop a scalable graph clustering algorithm that could be applied on large real-world networks, particularly those with power-law structure. An example is in image segmentation, where natural scene statistics yield clusters whose sizes follow power laws. Pitman-Yor processes [10, 17] are a definite candidate for developing the appropriate extensions, but further work is needed to determine if they can be applied asymptotically to the hard clustering case.\n\u2022 The work of [1] considers a more general connection between mixture models and k-means-type algorithms. In particular, the authors show that one can generalize the k-means algorithm to utilize any Bregman divergence. Furthermore, there is a bijection between exponential family distributions and Bregman divergences, so a limiting argument on an exponential family mixture model yields a particular Bregman clustering algorithm. Does a similar result hold in the case of the DP mixtures?\nFinally, while this paper is focused on an asymptotic analysis for the DP mixture, a significant amount of empirical work is still required to determine the practical impact of these algorithms. We are currently testing the derived algorithms on a number of real-world problems. A practical consideration concerns the selection of \u03bb: whereas the choice of k in k-means provides the user with exact knowledge of the number of outputted clusters, it is difficult to ascertain roughly how many clusters corresponds to a particular choice of \u03bb. Deriving heuristics for this choice could have a significant practical impact on results, and is a further issue under consideration."}], "references": [{"title": "Clustering with Bregman divergences", "author": ["A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh"], "venue": "Journal of Machine Learning Research, 6:1705\u20131749", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "MDL principle for robust vector quantisation", "author": ["H. Bischof", "A. Leonardis", "A. Selb"], "venue": "Pattern Analysis and Applications, 2(1):59\u201372", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Variational inference for Dirichlet process mixtures", "author": ["D. Blei", "M. Jordan"], "venue": "Journal of Bayesian Analysis, 1(1):121\u2013144", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Spectral k-way ratio cut partitioning", "author": ["P. Chan", "M. Schlag", "J. Zien"], "venue": "IEEE Trans. CAD-Integrated Circuits and Systems, 13:1088\u20131096", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Iterative clustering of high dimensional text data augmented by local search", "author": ["I.S. Dhillon", "Y. Guan", "J. Kogan"], "venue": "Proceedings of the 2nd IEEE International Conference on Data Mining, pages 131\u2013138", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Weighted graph cuts without eigenvectors: A multilevel approach", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(11):1944\u20131957", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "How many clusters? Which clustering method? Answers via model-based cluster analysis", "author": ["C. Fraley", "A.E. Raftery"], "venue": "Computer Journal, 41(8):578\u2013588", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning the k in k-means", "author": ["G. Hamerly", "C. Elkan"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Bayesian Nonparametrics: Principles and Practice", "author": ["N. Hjort", "C. Holmes", "P. Mueller", "S. Walker"], "venue": "Cambridge University Press, Cambridge, UK", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Gibbs sampling methods for stick-breaking priors", "author": ["H. Ishwaran", "L.F. James"], "venue": "Journal of the American Statistical Association, 96(453):161\u2013173", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model", "author": ["S. Jain", "R.M. Neal"], "venue": "Journal of Computational and Graphical Statistics, 13(1):158\u2013182", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "The application of cluster analysis in strategic management research: an analysis and critique", "author": ["D.J. Ketchen", "C . L. Shook"], "venue": "Strategic Management Journal,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "A permutation-augmented sampler for DP mixture models", "author": ["P. Liang", "M. Jordan", "B. Taskar"], "venue": "International Conference on Machine Learning", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Selecting variables for k-means cluster analysis by using a genetic algorithm that optimizes the silhouettes", "author": ["R. Lleti", "M.C. Ortiz", "L.A. Sarabia", "M.S. Sanchez"], "venue": "Analytica Chimica Acta, 515:87\u2013100", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "Cambridge University Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Markov chain sampling methods for Dirichlet process mixture models", "author": ["R.M. Neal"], "venue": "Journal of Computational and Graphical Statistics, 9:249\u2013265", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Combinatorial Stochastic Processes", "author": ["J. Pitman"], "venue": "Springer-Verlag", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888\u2013905", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Finding the number of clusters in a data set: An information theoretic approach", "author": ["C.A. Sugar", "G.M. James"], "venue": "Journal of the American Statistical Association, 98:750\u2013763", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Hierarchical Dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association, 101(476):1566\u20131581", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Estimating the number of clusters in a data set via the gap statistic", "author": ["R. Tibshirani", "G. Walther", "T. Hastie"], "venue": "Journal of the Royal Statistical Society Series B, 63:411\u2013423", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Hierarchical priors and mixture models", "author": ["M. West", "P. M\u00fcller", "M.D. Escobar"], "venue": "with application in regression and density estimation. In P. R. Freeman and A. F. M. Smith, editors, Aspects of Uncertainty, pages 363\u2013386. John Wiley", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Multiclass spectral clustering", "author": ["S.X. Yu", "J. Shi"], "venue": "Proceedings of the International Conference on Computer Vision", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Spectral relaxation for k-means clustering", "author": ["H. Zha", "X. He", "C. Ding", "H. Simon", "M. Gu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 1, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 6, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 7, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 11, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 13, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 18, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 20, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 15, "context": "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].", "startOffset": 214, "endOffset": 222}, {"referenceID": 21, "context": "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].", "startOffset": 214, "endOffset": 222}, {"referenceID": 15, "context": "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].", "startOffset": 254, "endOffset": 258}, {"referenceID": 10, "context": "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].", "startOffset": 282, "endOffset": 286}, {"referenceID": 2, "context": "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].", "startOffset": 310, "endOffset": 313}, {"referenceID": 12, "context": "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].", "startOffset": 350, "endOffset": 354}, {"referenceID": 14, "context": "While this objective appears in the k-means literature, as an extension motivated by the Akaike Information Criterion (AIC) [15], it does not appear that any algorithms have been developed for optimization of the objective, nor are there any existing connections made to Bayesian nonparametrics.", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": "We can take a step further and extend our analysis to the hierarchical Dirichlet process (HDP), a model for clustering multiple related data sets [20].", "startOffset": 146, "endOffset": 150}, {"referenceID": 5, "context": "This connection leads to kernel k-means algorithms for monotonically decreasing the cut values, and which scale efficiently to large graphs; the work in [6] developed a technique for graph clustering based on this connection that outperforms spectral clustering approaches to graph clustering, and scales to significantly larger graphs.", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": "A third, less obvious extension to the k-means objective function arises in the context of graph clustering [4, 18, 23].", "startOffset": 108, "endOffset": 119}, {"referenceID": 17, "context": "A third, less obvious extension to the k-means objective function arises in the context of graph clustering [4, 18, 23].", "startOffset": 108, "endOffset": 119}, {"referenceID": 22, "context": "A third, less obvious extension to the k-means objective function arises in the context of graph clustering [4, 18, 23].", "startOffset": 108, "endOffset": 119}, {"referenceID": 5, "context": "A surprising fact is that both the normalized cut and ratio cut objective functions can be expressed exactly as the weighted kernel k-means function for appropriate choice of weights and kernels [6].", "startOffset": 195, "endOffset": 198}, {"referenceID": 8, "context": "4 Dirichlet Process Mixture Models Finally, we briefly review DP mixture models [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 21, "context": "[22] and further discussed by Neal [16], Algorithm 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[22] and further discussed by Neal [16], Algorithm 2.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "[15] describe the above penalized k-means objective function with a motivation arising from the AIC.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "We will not describe the sampling algorithm for the hierarchical Dirichlet process (HDP) in detail, but refer the reader to [20] for a detailed introduction to the HDP model and a description of inference techniques.", "startOffset": 124, "endOffset": 128}, {"referenceID": 23, "context": "In particular, for the k-means objective, one computes the eigenvectors corresponding to the k largest eigenvalues of the kernel matrix over the data; these eigenvectors form the globally optimal \u201crelaxed\u201d cluster indicator matrix [24].", "startOffset": 231, "endOffset": 235}, {"referenceID": 5, "context": "We state a result proven in [6] for standard normalized cut.", "startOffset": 28, "endOffset": 31}, {"referenceID": 4, "context": "For instance, [5] looked at incorporating local moves, which finds points to move from one cluster to another such that the k-means objective is lowered.", "startOffset": 14, "endOffset": 17}, {"referenceID": 9, "context": "Pitman-Yor processes [10, 17] are a definite candidate for developing the appropriate extensions, but further work is needed to determine if they can be applied asymptotically to the hard clustering case.", "startOffset": 21, "endOffset": 29}, {"referenceID": 16, "context": "Pitman-Yor processes [10, 17] are a definite candidate for developing the appropriate extensions, but further work is needed to determine if they can be applied asymptotically to the hard clustering case.", "startOffset": 21, "endOffset": 29}, {"referenceID": 0, "context": "\u2022 The work of [1] considers a more general connection between mixture models and k-means-type algorithms.", "startOffset": 14, "endOffset": 17}], "year": 2017, "abstractText": "One of the many benefits of Bayesian nonparametric processes such as the Dirichlet process is that they can be used for modeling infinite mixture models, thus providing a flexible answer to the question of how many clusters exist in a data set. For the most part, such flexibility is currently lacking in techniques based on hard clustering, such as k-means, graph cuts, and Bregman hard clustering. For finite mixture models, there is a precise connection between k-means and mixtures of Gaussians, obtained by an appropriate limiting argument. In this paper, we apply a similar technique to an infinite mixture arising from the Dirichlet process (DP). We show that a Gibbs sampling algorithm for DP mixtures approaches a hard clustering algorithm in the limit, and further that the resulting algorithm monotonically minimizes an elegant underlying k-means-like objective that includes a penalty term based on the number of clusters. We generalize our analysis to the case of clustering multiple related data sets through a similar asymptotic argument with the hierarchical Dirichlet process. We discuss additional extensions that further highlight the benefits of our analysis: i) a spectral relaxation involving thresholded eigenvectors, and ii) a normalized cut graph clustering algorithm that requires O(|E|) time per iteration and automatically determines the number of clusters in a graph.", "creator": "LaTeX with hyperref package"}}}