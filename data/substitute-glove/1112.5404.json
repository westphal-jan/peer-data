{"id": "1112.5404", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2011", "title": "Similarity-based Learning via Data Driven Embeddings", "abstract": "We any the result under classification various similarity / track functions between computer. Specifically, we propose a concept have reality given humility although a (dis) similarity define including rights to place present educational key of propose algorithms. both grants correlation properties when being with such good functions. Our fundamental initializes while pre-dates three frameworks proposed by [Balcan - Blum ICML 1972] various [Wang vers hassan ICML 2007 ]. An stronger collection especially we framework is though robustness to inventory - always did not aimed a fixed notion now essence but especially wait provide rules for. We show, . seeking perspectives apply that three goodness criterion best incredibly to old example why which however learned as makes without better applicable made entire features an domains and problems. We amend called landmarking - based view instead denying means end-point from especially learned wholeness criteria. We briefly besides up novel diversity well asymptotic to undergo task - putting selection for landmark points instead to different selection. We demonstrate been capability of our selflessness criteria develop e.g. from been as the landmark selection pc-based set a unusual for inflection - based learning predefined and benchmark UCI datasets the some not method fairly outshines existing path has a significant tally.", "histories": [["v1", "Thu, 22 Dec 2011 18:08:27 GMT  (113kb,S)", "http://arxiv.org/abs/1112.5404v1", "To appear in the proceedings of NIPS 2011, 14 pages"]], "COMMENTS": "To appear in the proceedings of NIPS 2011, 14 pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["purushottam kar", "prateek jain 0002"], "accepted": true, "id": "1112.5404"}, "pdf": {"name": "1112.5404.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["purushot@cse.iitk.ac.in", "prajain@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n11 2.\n54 04\nv1 [\ncs .L\nG ]\n2 2\nD ec"}, {"heading": "1 Introduction", "text": "Machine learning algorithms have found applications in diverse domains such as computer vision, bio-informatics and speech recognition. Working in such heterogeneous domains often involves handling data that is not presented as explicit features embedded into vector spaces. However in many domains, for example co-authorship graphs, it is natural to devise similarity/distance functions over pairs of points. While classical techniques like decision tree and linear perceptron cannot handle such data, several modern machine learning algorithms such as support vector machine (SVM) can be kernelized and are thereby capable of using kernels or similarity functions.\nHowever, most of these algorithms require the similarity functions to be positive semi-definite (PSD), which essentially implies that the similarity stems from an (implicit) embedding of the data into a Hilbert space. Unfortunately in many domains, the most natural notion of similarity does not satisfy this condition - moreover, verifying this condition is usually a non-trivial exercise. Take for example the case of images on which the most natural notions of distance (Euclidean, Earth-mover) [3] do not form PSD kernels. Co-authorship graphs give another such example.\nConsequently, there have been efforts to develop algorithms that do not make assumptions about the PSD-ness of the similarity functions used. One can discern three main approaches in this area. The first approach tries to coerce a given similarity measure into a PSD one by either clipping or shifting the spectrum of the kernel matrix [4, 5]. However, these approaches are mostly restricted to transductive settings and are not applicable to large scale problems due to eigenvector computation requirements. The second approach consists of algorithms that either adapt classical methods like\nk-NN to handle non-PSD similarity/distance functions and consequently offer slow test times [5], or are forced to solve non-convex formulations [6, 7].\nThe third approach, which has been investigated recently in a series of papers [1, 2, 8, 9], uses the similarity function to embed the domain into a low dimensional Euclidean space. More specifically, these algorithms choose landmark points in the domain which then give the embedding. Assuming a certain \u201cgoodness\u201d property (that is formally defined) for the similarity function, these models offer both generalization guarantees in terms of how well-suited the similarity function is to the classification task as well as the ability to use fast algorithmic techniques such as linear SVM [10] on the landmarked space. The model proposed by Balcan-Blum in [1] gives sufficient conditions for a similarity function to be well suited to such a landmarking approach. Wang et al. in [2] on the other hand provide goodness conditions for dissimilarity functions that enable landmarking algorithms.\nInformally, a similarity (or distance) function can be said to be good if points of similar labels are closer to each other than points of different labels in some sense. Both the models described above restrict themselves to a fixed goodness criterion, which need not hold for the underlying data. We observe that this might be too restrictive in many situations and present a framework that allows us to tune the goodness criterion itself to the classification problem at hand. Our framework consequently unifies and generalizes those presented in [1] and [2]. We first prove generalization bounds corresponding to landmarked embeddings under a fixed goodness criterion. We then provide a uniform-convergence bound that enables us to learn the best goodness criterion for a given problem. We further generalize our framework by giving the ability to incorporate any Lipschitz loss function into our goodness criterion which allows us to give guarantees for the use of various algorithms such as C-SVM and logistic regression on the landmarked space.\nNow similar to [1, 2], our framework requires random sampling of training points to create the embedding space1. However in practice, random sampling is inefficient and requires sampling of a large number of points to form a useful embedding, thereby increasing training and test time. To address this issue, [2] proposes a heuristic to select the points that are to be used as landmarks. However their scheme is tied to their optimization algorithm and is computationally inefficient for large scale data. In contrast, we propose a general heuristic for selecting informative landmarks based on a novel notion of diversity which can then be applied to any instantiation of our model.\nFinally, we apply our methods to a variety of benchmark datasets for similarity learning as well as ones from the UCI repository. We empirically demonstrate that our learning model and landmark selection heuristic consistently offers significant improvements over the existing approaches. In particular, for small number of landmark points, which is a practically important scenario as it is expensive to compute similarity function values at test time, our method provides, on an average, accuracy boosts of upto 5% over existing methods. We also note that our methods can be applied on top of any strategy used to learn the similarity measure (eg. MKL techniques [11]) or the distance measure (eg. [12]) itself. Akin to [1], our techniques can also be extended to learn a combination of (dis)similarity functions but we do not explore these extensions in this paper."}, {"heading": "2 Methodology", "text": "Let D be a fixed but unknown distribution over the labeled input domain X and let \u2113 : X \u2192 {\u22121,+1} be a labeling over the domain. Given a (potentially non-PSD) similarity function2 K : X \u00d7 X \u2192 R, the goal is to learn a classifier \u2113\u0302 : X \u2192 {\u22121,+1} from a finite number of i.i.d. samples from D that has bounded generalization error over D. Now, learning a reasonable classifier seems unlikely if the given similarity function does not have any inherent \u201cgoodness\u201d property. Intuitively, the goodness of a similarity function should be its suitability to the classification task at hand. For PSD kernels, the notion of goodness is defined in terms of the margin offered in the RKHS [13]. However, a more basic requirement is that the similarity function should preserve affinities among similarly labeled points - that is to say, a good similarity function should not, on an average, assign higher similarity values to dissimilarly labeled points than to similarly labeled points. This intuitive notion of goodness turns out to be rather robust\n1Throughout the paper, we use the terms embedding space and landmarked space interchangeably. 2Results described in this section hold for distance functions as well; we present results with respect to\nsimilarity functions for sake of simplicity.\nin the sense that all PSD kernels that offer a good margin in their respective RKHSs satisfy some form of this goodness criterion as well [14].\nRecently there has been some interest in studying different realizations of this general notion of goodness and developing corresponding algorithms that allow for efficient learning with similarity/distance functions. Balcan-Blum in [1] present a goodness criteria in which a good similarity function is considered to be one that, for most points, assigns a greater average similarity to similarly labeled points than to dissimilarly labeled points. More specifically, a similarity function is (\u01eb, \u03b3)-good if there exists a weighing function w : X \u2192 R such that, at least a (1 \u2212 \u01eb) probability mass of examples x \u223c D satisfies:\nE x\u2032\u223cD [w (x\u2032)K(x, x\u2032)|\u2113(x\u2032) = \u2113(x)] \u2265 E x\u2032\u223cD [w (x\u2032)K(x, x\u2032)|\u2113(x\u2032) 6= \u2113(x)] + \u03b3. (1)\nwhere instead of average similarity, one considers an average weighted similarity to allow the definition to be more general.\nWang et al in [2] define a distance function d to be good if a large fraction of the domain is, on an average, closer to similarly labeled points than to dissimilarly labeled points. They allow these averages to be calculated based on some distribution distinct from D, one that may be more suited to the learning problem. However it turns out that their definition is equivalent to one in which one again assigns weights to domain elements, as done by [1], and the following holds\nE x\u2032,x\u2032\u2032\u223cD\u00d7D\n[w(x\u2032)w(x\u2032\u2032) sgn (d(x, x\u2032\u2032)\u2212 d(x, x\u2032)) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)] > \u03b3 (2)\nAssuming their respective goodness criteria, [1] and [2] provide efficient algorithms to learn classifiers with bounded generalization error. However these notions of goodness with a single fixed criterion may be too restrictive in the sense that the data and the (dis)similarity function may not satisfy the underlying criterion. This is, for example, likely in situations with high intra-class variance. Thus there is need to make the goodness criterion more flexible and data-dependent.\nTo this end, we unify and generalize both the above criteria to give a notion of goodness that is more data dependent. Although the above goodness criteria (1) and (2) seem disparate at first, they can be shown to be special cases of a generalized framework where an antisymmetric function is used to compare intra and inter-class affinities. We use this observation to define our novel goodness criterion using arbitrary bounded antisymmetric functions which we refer to as transfer functions. This allows us to define a family of goodness criteria of which (1) and (2) form special cases ((1) uses the identity function and (2) uses the sign function as transfer function). Moreover, the resulting definition of a good similarity function is more flexible and data dependent. In the rest of the paper we shall always assume that our similarity functions are normalized i.e. for the domain of interest X , sup\nx,y\u2208X K(x, y) \u2264 1.\nDefinition 1 (Good Similarity Function). A similarity function K : X \u00d7 X \u2192 R is said to be an (\u01eb, \u03b3, B)-good similarity for a learning problem where \u01eb, \u03b3, B > 0 if for some antisymmetric transfer function f : R \u2192 R and some weighing function w : X \u00d7X \u2192 [\u2212B,B], at least a (1\u2212 \u01eb) probability mass of examples x \u223c D satisfies\nE x\u2032,x\u2032\u2032\u223cD\u00d7D\n[w (x\u2032, x\u2032\u2032) f (K(x, x\u2032)\u2212K(x, x\u2032\u2032)) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)] \u2265 Cf\u03b3 (3)\nwhere Cf = sup x,x\u2032\u2208X f(K(x, x\u2032))\u2212 inf x,x\u2032\u2208X f(K(x, x\u2032))\nAs mentioned before, the above goodness criterion generalizes the previous notions of goodness 3 and is adaptive to changes in data as it allows us, as shall be shown later, to learn the best possible criterion for a given classification task by choosing the most appropriate transfer function from a parameterized family of functions. We stress that the property of antisymmetry for the transfer function is crucial to the definition in order to provide a uniform treatment to points of all classes as will be evident in the proof of Theorem 2.\nAs in [1, 2], our goodness criterion lends itself to a simple learning algorithm which consists of choosing a set of d random pairs of points from the domain P = {(\nx+i , x \u2212 i\n)}d\ni=1 (which we refer to\n3We refer the reader to the appendix for a discussion.\nas landmark pairs) and defining an embedding of the domain into a landmarked space using these landmarks : \u03a6L : X \u2192 Rd,\u03a6L(x) = ( f(K(x, x+i )\u2212K(x, x\u2212i )) )d i=1 \u2208 Rd. The advantage of performing this embedding is the guaranteed existence of a large margin classifier in the landmarked space as shown below.\nTheorem 2. If K is an (\u01eb, \u03b3, B)-good similarity with respect to transfer function f and weight function w then for any \u01eb1 > 0, with probability at least 1 \u2212 \u03b4 over the choice of d = (8/\u03b32) ln(2/\u03b4\u01eb1) positive and negative samples, {\nx+i }d i=1 \u2282 D+ and { x\u2212i }d i=1 \u2282 D\u2212 respectively, the classifier\nh(x) = sgn[g(x)] where g(x) = 1 d \u2211d i=1 w(x + i , x \u2212 i )f\n( K(x, x+i )\u2212K(x, x\u2212i ) )\nhas error no more than \u01eb + \u01eb1 at margin \u03b3 2 .\nProof. We shall prove that with probability at least 1\u2212 \u03b4, at least a 1 \u2212 \u01eb1 fraction of points x that satisfy Equation 3 are classified correctly by the classifier h(x). Overestimating the error by treating the points that do not satisfy Equation 3 as always being misclassified will give us the desired result.\nFor any fixed x \u2208 X+ that satisfies Equation 3, we have E\nx\u2032,x\u2032\u2032\u223cD\u00d7D [w(x\u2032, x\u2032\u2032)f (K(x, x\u2032)\u2212K(x, x\u2032\u2032)) |\u2113(x\u2032) = 1, \u2113(x\u2032\u2032) = \u22121] \u2265 Cf\u03b3\nhence the Hoeffding Bounds give us\nPr [ g(x) < \u03b3\n2\n]\n= Pr\n[\n1\nd\nd \u2211\ni=1\nw(x+i , x \u2212 i )f\n( K(x, x+i )\u2212K(x, x\u2212i ) )\n< \u03b3\n2\n]\n\u2264 2 exp ( \u2212\u03b3 2d\n8\n)\nSimilarly, for any fixed x \u2208 X\u2212 that satisfies Equation 3, we have E\nx\u2032,x\u2032\u2032\u223cD\u00d7D [w(x\u2032, x\u2032\u2032)f (K(x, x\u2032)\u2212K(x, x\u2032\u2032)) |\u2113(x\u2032) = \u22121, \u2113(x\u2032\u2032) = 1] \u2265 Cf\u03b3\nhence the Hoeffding Bounds give us\nPr [ g(x) > \u03b3\n2\n]\n= Pr\n[\n1\nd\nd \u2211\ni=1\nw(x+i , x \u2212 i )f\n( K(x, x+i )\u2212K(x, x\u2212i ) )\n> \u03b3\n2\n]\n= Pr\n[\n1\nd\nd \u2211\ni=1\nw(x+i , x \u2212 i )f\n( K(x, x\u2212i )\u2212K(x, x+i ) )\n< \u03b3\n2\n]\n\u2264 2 exp ( \u2212\u03b3 2d\n8\n)\nwhere in the second step we have used antisymmetry of f .\nSince we have shown that this result holds true individually for any point x that satisfies Equation 3, the expected error (where the expectation is both over the choice of domain points as well as choice of the landmark points) itself turns out to be less than 2 exp (\n\u2212 \u03b32d8 )\n\u2264 \u01eb1\u03b4. Applying Markov\u2019s inequality gives us that the probability of obtaining a set of landmarks such that the error on points satisfying Equation 3 is greater than \u01eb1 is at most \u03b4.\nAssuming, as mentioned earlier, that the points not satisfying Equation 3 can always be misclassified proves our desired result.\nHowever, there are two hurdles to obtaining this large margin classifier. Firstly, the existence of this classifier itself is predicated on the use of the correct transfer function, something which is unknown. Secondly, even if an optimal transfer function is known, the above formulation cannot be converted into an efficient learning algorithm for discovering the (unknown) weights since the formulation seeks to minimize the number of misclassifications which is an intractable problem in general.\nWe overcome these two hurdles by proposing a nested learning problem. First of all we assume that for some fixed loss function L, given any transfer function and any set of landmark pairs, it is possible to obtain a large margin classifier in the corresponding landmarked space that minimizes L. Having made this assumption, we address below the issue of learning the optimal transfer function for a given learning task. However as we have noted before, this assumption is not valid for arbitrary loss functions. This is why, subsequently in Section 2.2, we shall show it to be valid for a large class of loss functions by incorporating surrogate loss functions into our goodness criterion."}, {"heading": "2.1 Learning the transfer function", "text": "In this section we present results that allow us to learn a near optimal transfer function from a family of transfer functions. We shall assume, for some fixed loss function L, the existence of an efficient routine which we refer to as TRAIN that shall return, for any landmarked space indexed by a set of landmark pairs P , a large margin classifier minimizing L. The routine TRAIN is allowed to make use of additional training data to come up with this classifier.\nAn immediate algorithm for choosing the best transfer function is to simply search the set of possible transfer functions (in an algorithmically efficient manner) and choose the one offering lowest training error. We show here that given enough landmark pairs, this simple technique, which we refer to as FTUNE (see Algorithm 2) is guaranteed to return a near-best transfer function. For this we prove a uniform convergence type guarantee on the space of transfer functions.\nLet F \u2282 [\u22121, 1]R be a class of antisymmetric functions and W = [\u2212B,B]X\u00d7X be a class of weight functions. For two real valued functions f and g defined on X , let \u2016f \u2212 g\u2016\u221e := sup\nx\u2208X |f(x)\u2212 g(x)|.\nLet B\u221e(f, r) := {f \u2032 \u2208 F | \u2016f \u2212 f \u2032\u2016\u221e < r}. Let L be a CL-Lipschitz loss function. Let P = {(\nx+i , x \u2212 i\n)}d\ni=1 be a set of (random) landmark pairs. For any f \u2208 F , w \u2208 W , define\nG(f,w)(x) = E x\u2032,x\u2032\u2032\u223cD\u00d7D\n[w (x\u2032, x\u2032\u2032) f (K(x, x\u2032)\u2212K(x, x\u2032\u2032)) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)]\ng(f,w)(x) = 1\nd\nd \u2211\ni=1\nw ( x+i , x \u2212 i ) f ( K(x, x+i )\u2212K(x, x\u2212i ) )\nTheorem 7 (see Section 2.2) guarantees us that for any fixed f and any \u01eb1 > 0, if d is large enough then E\nx\n[ L(g(f,w)(x)) ] \u2264 E x [ L(G(f,w)(x)) ] + \u01eb1. We now show that a similar result holds even if\none is allowed to vary f . Before stating the result, we develop some notation.\nFor any transfer function f and arbitrary choice of landmark pairs P , let w(g,f) be the best weighing function for this choice of transfer function and landmark pairs i.e. let w(g,f) = argmin w\u2208[\u2212B,B]d E x\u223cD [ L ( g(f,w)(x) )] 4. Similarly, let w(G,f) be the best weighing function corresponding\nto G i.e. w(G,f) = argmin w\u2208W E x\u223cD\n[ L ( G(f,w)(x) )] . Then we can ensure the following :\nTheorem 3. Let F be a compact class of transfer functions with respect to the infinity norm and \u01eb1, \u03b4 > 0. Let N (F , r) be the size of the smallest \u01eb-net over F with respect to the infinity norm at scale r = \u01eb14CLB . Then if one chooses d = 64B2C2L \u01eb21 ln ( 16B\u00b7N (F ,r) \u03b4\u01eb1 ) random landmark pairs then we have the following with probability greater than (1\u2212 \u03b4)\nsup f\u2208F\n[\u2223\n\u2223 \u2223 E\nx\u223cD\n[ L ( g(f,w(g,f))(x) )]\n\u2212 E x\u223cD\n[ L ( G(f,w(G,f))(x) )]\u2223 \u2223 \u2223 ] \u2264 \u01eb1\nWe shall prove the theorem in two parts. As we shall see, one of the parts is fairly simple to prove. To prove the other part, we shall exploit the Lipschitz properties of the loss function as well as the fact that the class of transfer functions chosen form a compact set. Let us call a given set of landmark pairs to be good with respect to a fixed transfer function f \u2208 F if for the corresponding g, E\nx [L(g(x))] \u2264 E x [L(G(x))] + \u01eb1 for some small fixed \u01eb1 > 0.\nWe will first prove, using Lipschitz properties of the loss function that if a given set of landmarks is good with respect to a given transfer function, then it is also good with respect to all transfer functions in its neighborhood. Having proved this, we will apply a standard covering number argument in which we will ensure that a large enough set of landmarks is good with respect to a set of transfer functions that form an \u01eb-net over F and use the previous result to complete the proof. We first prove a series of simple results which will be used in the first part of the proof. In the following f and f \u2032 are two transfer functions such that f \u2032 \u2208 B\u221e(f, r) \u2229 F . Lemma 4. The following results are true\n4Note that the function g(f,w)(x) is dictated by the choice of the set of landmark pairs P\n1. For any fixed f \u2208 F , E x\u223cD\n[ L ( G(f,w(G,f))(x) )]\n\u2264 E x\u223cD\n[ L ( G(f,w)(x) )] for all w \u2208 W .\n2. For any fixed f \u2208 F , any fixed g obtained by an arbitrary choice of landmark pairs, E\nx\u223cD\n[ L ( g(f,w(g,f))(x) )]\n\u2264 E x\u223cD\n[ L ( g(f,w)(x) )] for all w \u2208 W .\n3. For any f \u2032 \u2208 B\u221e(f, r)\u2229F , \u2223 \u2223 \u2223 E\nx\u223cD\n[ L ( G(f,w(G,f))(x) )]\n\u2212 E x\u223cD\n[ L ( G(f \u2032,w(G,f\u2032))(x) )]\u2223 \u2223 \u2223 \u2264\nCLrB.\n4. For any fixed g obtained by an arbitrary choice of landmark pairs, f \u2032 \u2208 B\u221e(f, r) \u2229 F , \u2223\n\u2223 \u2223 E\nx\u223cD\n[ L ( g(f,w(g,f))(x) )]\n\u2212 E x\u223cD\n[ L ( g(f \u2032,w(g,f\u2032))(x) )]\n\u2223 \u2223 \u2223 \u2264 CLrB.\nProof. We prove the results in order,\n1. Immediate from the definition of w(G,f).\n2. Immediate from the definition of w(g,f).\n3. We have E x\u223cD\n[ L ( G(f \u2032,w(G,f\u2032))(x) )]\n\u2264 E x\u223cD\n[ L ( G(f \u2032,w(G,f))(x) )] by an application of\nLemma 4.1 proven above. For sake of simplicity let us denote w(G,f) = w for the next set of calculations. Now we have\nG(f \u2032,w)(x) = E x\u2032,x\u2032\u2032\u223cD\u00d7D\n[w (x\u2032, x\u2032\u2032) f \u2032 (K(x, x\u2032)\u2212K(x, x\u2032\u2032)) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)]\n\u2264 E x\u2032,x\u2032\u2032\u223cD\u00d7D [w (x\u2032, x\u2032\u2032) (f (K(x, x\u2032)\u2212K(x, x\u2032\u2032)) + r) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)]\n= E x\u2032,x\u2032\u2032\u223cD\u00d7D\n[w (x\u2032, x\u2032\u2032) f (K(x, x\u2032)\u2212K(x, x\u2032\u2032)) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)]\n+ r \u00b7 E x\u2032,x\u2032\u2032\u223cD\u00d7D [w (x\u2032, x\u2032\u2032) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)]\n\u2264 G(f,w)(x) + rB\nwhere in the second inequality we have used the fact that \u2016f \u2212 f \u2032\u2016\u221e \u2264 r and in the fourth inequality we have used the fact that w \u2208 W . Thus we have G(f \u2032,w)(x) \u2264 G(f,w)(x) + rB. Using the Lipschitz properties of L we can now get E\nx\u223cD\n[ L ( G(f \u2032,w)(x) )] \u2264\nE x\u223cD\n[ L ( G(f,w)(x) )]\n+ CLrB. Thus we have E x\u223cD\n[ L ( G(f \u2032,w(G,f\u2032))(x) )] \u2264\nE x\u223cD\n[ L ( G(f \u2032,w(G,f))(x) )]\n\u2264 E x\u223cD\n[ L ( G(f,w(G,f))(x) )] + CLrB.\nSimilarly we can also prove E x\u223cD\n[ L ( G(f,w(G,f))(x) )]\n\u2264 E x\u223cD\n[ L ( G(f \u2032,w(G,f\u2032))(x) )] +\nCLrB. This gives us the desired result.\n4. The proof follows in a manner similar to the one for Lemma 4.3 proven above.\nUsing the above results we get a preliminary form of the first part of our proof as follows :\nLemma 5. Suppose a set of landmarks is (\u01eb1/2)-good for a particular landmark f \u2208 F (i.e. E\nx\u223cD\n[ L ( g(f,w(G,f))(x) )]\n< E x\u223cD\n[ L ( G(f,w(G,f))(x) )] + \u01eb1/2), then the same set of\nlandmarks are also \u01eb1-good for any f \u2032 \u2208 B\u221e(f, r) \u2229 F (i.e. for all f \u2032 \u2208 B\u221e(f, r) \u2229 F , E x\u223cD [ L ( g(f \u2032,w(g,f\u2032))(x) )] \u2264 E x\u223cD [ L ( G(f \u2032,w(G,f\u2032))(x) )] + \u01eb1) for some r = r (\u01eb1).\nProof. Theorem 7 proven below guarantees that for any fixed f \u2208 F , with probability 1 \u2212 \u03b4 that E x\u223cD [ L ( g(f,w(G,f))(x) )] < E x\u223cD [ L ( G(f,w(G,f))(x) )] + \u01eb1/2. This can be achieved with d =\n(64B2C2L/\u01eb 2 1) ln(8B/\u03b4\u01eb1). Now assuming that the above holds, using the above results we can get the following for any f \u2032 \u2208 B\u221e(f, r) \u2229 F .\nE x\u223cD\n[ L ( g(f \u2032,w(g,f\u2032))(x) )] \u2264 E x\u223cD [ L ( g(f,w(g,f))(x) )] + CLrB\n(using Lemma 4.4)\n\u2264 E x\u223cD\n[ L ( g(f,w(G,f))(x) )] + CLrB\n(using Lemma 4.2)\n\u2264 E x\u223cD\n[ L ( G(f,w(G,f))(x) )] + \u01eb1/2 + CLrB\n(using Theorem 7)\n\u2264 E x\u223cD\n[ L ( G(f \u2032,w(G,f\u2032))(x) )] + \u01eb1/2 + 2CLrB\n(using Lemma 4.3)\nSetting r = \u01eb14CLB gives us the desired result.\nProof. (of Theorem 3) As mentioned earlier we shall prove the theorem in two parts as follows :\n1. (Part I) In this part we shall prove the following :\nsup f\u2208F\n[\nE x\u223cD\n[ L ( g(f,w(g,f))(x) )]\n\u2212 E x\u223cD\n[ L ( G(f,w(G,f))(x) )]] \u2264 \u01eb1\nWe first set up an \u01eb-net over F at scale r = \u01eb14CLB . Let there be N (F , r) elements in this net. Taking d = (64B2C2L/\u01eb 2 1) ln(8B \u00b7 N (F , r) /\u03b4\u01eb1) landmarks should ensure that the landmarks, with very high probability, are good for all functions in the net by an application of union bound. Since every function in F is at least r-close to some function in the net, Lemma 5 tells us that the same set of landmarks are, with very high probability, good for all the functions in F . This proves the first part of our result.\n2. (Part II) In this part we shall prove the following :\nsup f\u2208F\n[\nE x\u223cD\n[ L ( G(f,w(G,f))(x) )]\n\u2212 E x\u223cD\n[ L ( g(f,w(g,f))(x) )]] \u2264 \u01eb1\nThis part is actually fairly simple to prove. Intuitively, since one can imagine G as being the output of an algorithm that is allowed to take the entire domain as its landmark set,\nwe should expect E x\u223cD\n[ L ( G(f,w(G,f))(x) )]\n\u2264 E x\u223cD\n[ L ( g(f,w(g,f))(x) )] to hold uncon-\nditionally for every f . For a formal argument, let us build up some more notation. As we have said before, for any transfer function f and arbitrary choice of d landmark pairs P , we let w(g,f) \u2208 [\u2212B,B]d be the best weighing function for this choice of transfer function and landmark pairs. Now let w(g,f) be the best possible extension of w(g,f) to the entire domain. More formally, for any w\u2217 \u2208 [\u2212B,B]d let w\u2217 = argmin w\u2208W,w|P=w\u2217 E x\u223cD [ L ( G(f,w)(x) )] .\nNow Lemma 4.1 tells us that for any f \u2208 F and any choice of landmark pairs P , E x\u223cD [ L ( G(f,w(G,f))(x) )] \u2264 E x\u223cD [ L ( G(f,w(g,f))(x) )] . Furthermore, since w(g,f) is cho-\nsen to be the most beneficial extension of w(g,f), we also have E x\u223cD\n[ L ( G(f,w(g,f))(x) )] \u2264\nE x\u223cD\n[ L ( g(f,w(g,f))(x) )] . Together, these two inequalities give us the second part of the\nproof.\nThis result tells us that in a large enough landmarked space, we shall, for each function f \u2208 F , recover close to the best classifier possible for that transfer function. Thus, if we iterate over the set of transfer functions (or use some gradient-descent based optimization routine), we are bound to select a transfer function that is capable of giving a classifier that is close to the best."}, {"heading": "2.2 Working with surrogate loss functions", "text": "The formulation of a good similarity function suggests a simple learning algorithm that involves the construction of an embedding of the domain into a landmarked space on which the existence of a large margin classifier having low misclassification rate is guaranteed. However, in order to exploit this guarantee we would have to learn the weights w (\nx+i , x \u2212 i\n)\nassociated with this classifier by minimizing the empirical misclassification rate on some training set.\nUnfortunately, not only is this problem intractable but also hard to solve approximately [15, 16]. Thus what we require is for the landmarked space to admit a classifier that has low error with respect to a loss function that can also be efficiently minimized on any training set. In such a situation, minimizing the loss on a random training set would, with very high probability, give us weights that give similar performance guarantees as the ones used in the goodness criterion.\nWith a similar objective in mind, [1] offers variants of its goodness criterion tailored to the hinge loss function which can be efficiently optimized on large training sets (for example LIBSVM [17]). Here we give a general notion of goodness that can be tailored to any arbitrary Lipschitz loss function.\nDefinition 6. A similarity function K : X \u00d7 X \u2192 R is said to be an (\u01eb, B)-good similarity for a learning problem with respect to a loss function L : R \u2192 R+ where \u01eb > 0 if for some transfer function f : R \u2192 R and some weighing function w : X \u00d7X \u2192 [\u2212B,B], E\nx\u223cD [L(G(x))] \u2264 \u01eb where\nG(x) = E x\u2032,x\u2032\u2032\u223cD\u00d7D\n[w (x\u2032, x\u2032\u2032) f (K(x, x\u2032)\u2212K(x, x\u2032\u2032)) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)]\nOne can see that taking the loss functions as L(x) = 1x<Cf\u03b3 gives us Equation 3 which defines a good similarity under the 0\u22121 loss function. It turns out that we can, for any Lipschitz loss function, give similar guarantees on the performance of the classifier in the landmarked space.\nTheorem 7. If K is an (\u01eb, B)-good similarity function with respect to a CL-Lipschitz loss function L then for any \u01eb1 > 0, with probability at least 1 \u2212 \u03b4 over the choice of d = (16B2C2L/\u01eb 2 1) ln(4B/\u03b4\u01eb1) positive and negative samples from D+ and D\u2212 respectively, the expected loss of the classifier g(x) with respect to L satisfies E x [L(g(x))] \u2264 \u01eb + \u01eb1 where g(x) = 1 d \u2211d i=1 w ( x+i , x \u2212 i ) f ( K(x, x+i )\u2212K(x, x\u2212i ) ) .\nProof. For any x \u2208 X , we have, by an application of Hoeffding bounds Pr g [|G(x) \u2212 g(x)| > \u01eb1] < 2 exp ( \u2212 \u01eb 2 1d\n2B2\n)\nsince |g(x)| \u2264 B. Here the notation Pr g signifies that the probability is over the\nchoice of the landmark points. Thus for d > 4B 2 \u01eb21 ln ( 2 \u03b4 ) , we have Pr g [|G(x) \u2212 g(x)| > \u01eb1] < \u03b42. For sake of simplicity let us denote by BAD (x) the event |G(x) \u2212 g(x)| > \u01eb1. Thus we have, for every x \u2208 X , E\ng\n[\n1BAD(x)\n]\n< \u03b42. Since this is true for every x \u2208 X , this also holds in expectation i.e. E x E g [ 1BAD(x) ]\n< \u03b42. The expectation over x is with respect to the problem distribution D. Applying Fubini\u2019s Theorem gives us E\ng E x\n[\n1BAD(x)\n]\n< \u03b42 which upon application of Markov\u2019s inequality gives\nus Pr g\n[\nE x\n[\n1BAD(x)\n] > \u03b4 ] < \u03b4. Thus, with very high probability we would always choose landmarks\nsuch thatPr x [BAD(x)] < \u03b4. Thus we have, in such a situation, E x [|G(x) \u2212 g(x)|] \u2264 (1\u2212\u03b4)\u01eb1+\u03b4\u00b72B since sup x\u2208X |G(x)\u2212 g(x)| \u2264 2B. For small enough \u03b4 we have E x [|G(x) \u2212 g(x)|] \u2264 2\u01eb1.\nThus we have E x [L(g(x))] \u2212 E x [L(G(x))] = E x [L(g(x))\u2212 L(G(x))] \u2264 E x [CL \u00b7 |g(x)\u2212G(x)|] = CL \u00b7 E x [|g(x)\u2212G(x)|] \u2264 2CL\u01eb1 where we used the Lipschitz properties of the loss function L to arrive at the second inequality. Putting \u01eb1 = \u01eb\u20321\n2CL we have E x [L(g(x))] \u2264 E x [L(G(x))]+ \u01eb\u20321 \u2264 \u01eb+ \u01eb\u20321\nwhich gives us our desired result.\nAlgorithm 1 DSELECT\nRequire: A training set T , landmarking size d. Ensure: A set of d landmark pairs/singletons.\n1: \u0141\u2190 get-random-element(T ), PFTUNE \u2190 \u2205 2: for j = 2 to d do 3: z \u2190 argmin\nx\u2208T\n\u2211\nx\u2032\u2208\u0141 K(x, x\u2032).\n4: \u0141\u2190 \u0141 \u222a {z}, T \u2190 T\\{z} 5: end for 6: for j = 1 to d do 7: Sample z1, z2, s.t., \u2113(z1) = 1, \u2113(z2) = \u22121 randomly from \u0141 with replacement 8: PFTUNE \u2190 PFTUNE \u222a {(z1, z2)} 9: end for\n10: return \u0141 (for BBS), PFTUNE (for FTUNE)\nAlgorithm 2 FTUNE\nRequire: A family of transfer functions F , a similarity function K and a loss function L Ensure: An optimal transfer function f\u2217 \u2208 F . 1: Select d landmark pairs P . 2: for all f \u2208 F do 3: wf \u2190 TRAIN(P , L), Lf \u2190 L (wf ) 4: end for 5: f\u2217 \u2190 argmin\nf\u2208F\nLf\n6: return (f\u2217, wf\u2217).\nActually we can prove something stronger since \u2223 \u2223\n\u2223 E x [L(g(x))] \u2212 E x [L(G(x))]\n\u2223 \u2223 \u2223 =\n\u2223 \u2223 \u2223 E x [L(g(x))\u2212 L(G(x))] \u2223 \u2223 \u2223 \u2264 E x [|L(g(x))\u2212 L(G(x))|] \u2264 E x [CL \u00b7 |g(x) \u2212G(x)|] \u2264 \u01eb\u20321. Thus we have \u01eb\u2212 \u01eb\u20321 \u2264 E x [L(g(x))] \u2264 \u01eb+ \u01eb\u20321.\nIf the loss function is hinge loss at margin \u03b3 then CL = 1\u03b3 . The 0 \u2212 1 loss function and the loss function L(x) = 1x<\u03b3 (implicitly used in Definition 1 and Theorem 2) are not Lipschitz and hence this proof technique does not apply to them."}, {"heading": "2.3 Selecting informative landmarks", "text": "Recall that the generalization guarantees we described in the previous section rely on random selection of landmark pairs from a fixed distribution over the domain. However, in practice, a totally random selection might require one to select a large number of landmarks, thereby leading to an inefficient classifier in terms of training as well as test times. For typical domains such as computer vision, similarity function computation is an expensive task and hence selection of a small number of landmarks should lead to a significant improvement in the test times. For this reason, we propose a landmark pair selection heuristic which we call DSELECT (see Algorithm 1). The heuristic generalizes naturally to multi-class problems and can also be applied to the classification model of Balcan-Blum that uses landmark singletons instead of pairs.\nAt the core of our heuristic is a novel notion of diversity among landmarks. Assuming K is a normalized similarity kernel, we call a set of points S \u2282 X diverse if the average inter-point similarity is small i.e 1|S|(|S|\u22121) \u2211\nx,y\u2208S,x 6=y K(x, y) \u226a 1 (in case we are working with a distance kernel we would require large inter-point distances). The key observation behind DSELECT is that a nondiverse set of landmarks would cause all data points to receive identical embeddings and linear separation would be impossible. Small inter-landmark similarity, on the other hand would imply that the landmarks are well-spread in the domain and can capture novel patterns in the data.\nSimilar notions of diversity have been used in the past for ensemble classifiers [18] and k-NN classifiers [5]. Here we use this notion to achieve a better embedding into the landmarked space. Experimental results demonstrate that the heuristic offers significant performance improvements over random landmark selection (see Figure 1). One can easily extend Although Algorithm 1 to multiclass problems by selecting a fixed number of landmarks from each class."}, {"heading": "3 Empirical results", "text": "In this section, we empirically study the performance of our proposed methods on a variety of benchmark datasets. We refer to the algorithmic formulation presented in [1] as BBS and its augmentation using DSELECT as BBS+D. We refer to the formulation presented in [2] as DBOOST. We refer to our transfer function learning based formulation as FTUNE and its augmentation using DSELECT as FTUNE+D. In multi-class classification scenarios we will use a one-vs-all formulation which\npresents us with an opportunity to further exploit the transfer function by learning separate transfer function per class (i.e. per one-vs-all problem). We shall refer to our formulation using a single (resp. multiple) transfer function as FTUNE+D-S (resp. FTUNE+D-M). We take the class of ramp functions indexed by a slope parameter as our set of transfer functions. We use 6 different values of the slope parameter {1, 5, 10, 50, 100, 1000}. Note that these functions (approximately) include both the identity function (used by [1]) and the sign function (used by [2]).\nOur goal in this section is two-fold: 1) to show that our FTUNE method is able to learn a more suitable transfer function for the underlying data than the existing methods BBS and DBOOST and 2) to show that our diversity based heuristic for landmark selection performs better than random selection. To this end, we perform experiments on a few benchmark datasets for learning with similarity (non-PSD) functions [5] as well as on a variety of standard UCI datasets where the similarity function used is the Gaussian kernel function.\nFor our experiments, we implemented our methods FTUNE and FTUNE+D as well as BBS and BBS+D using MATLAB while using LIBLINEAR [10] for SVM classification. For DBOOST, we use the C++ code provided by the authors of [2]. On all the datasets we randomly selected a fixed percentage of data for training, validation and testing. Except for DBOOST , we selected the SVM penalty constantC from the set {1, 10, 100, 1000}using validation. For each method and dataset, we report classification accuracies averaged over 20 runs. We compare accuracies obtained by different methods using t-test at 95% significance level."}, {"heading": "3.1 Similarity learning datasets", "text": "First, we conduct experiments on a few similarity learning datasets [5]; these datasets provide a (non-PSD) similarity matrix along with class labels. For each of the datasets, we randomly select 70% of the data for training, 10% for validation and the remaining for testing purposes. We then apply our FTUNE-S, FTUNE+D-S, BBS+D methods along with BBS and DBOOST with varying number of landmark pairs. Note that we do not apply our FTUNE-M method to these datasets as it overfits heavily to these datasets as typically they are small in size.\nWe first compare the accuracy achieved by FTUNE+D-S with the existing methods. Table 1 compares the accuracies achieved by our FTUNE+D-S method with those of BBS and DBOOST over different datasets when using landmark sets of sizes 30 and 300. Numbers in brackets denote standard deviation over different runs. Note that in both the tables FTUNE+D-S is one of the best methods (upto 95% significance level) on all but one dataset. Furthermore, for datasets with large number of classes such as Amazon47 and FaceRec our method outperforms BBS and DBOOST by at least 20% percent. Also, note that some of the datasets have multiple bold faced methods, which means that the two sample t-test (at 95% level) rejects the hypothesis that their mean is different.\nNext, we evaluate the effectiveness of our landmark selection criteria for both BBS and our method. Figure 1 shows the accuracies achieved by various methods on four different datasets with increasing number of landmarks. Note that in all the datasets, our diversity based landmark selection criteria increases the classification accuracy by around 5\u2212 6% for small number of landmarks."}, {"heading": "3.2 UCI benchmark datasets", "text": "We now compare our FTUNE method against existing methods on a variety of UCI datasets [19]. We ran experiments with FTUNE and FTUNE+D but the latter did not provide any advantage. So for lack of space we drop it from our presentation and only show results for FTUNE-S (FTUNE with a single transfer function) and FTUNE-M (FTUNE with one transfer function per class). Similar to [2], we use the Gaussian kernel function as the similarity function for evaluating our method. We set the \u201cwidth\u201d parameter in the Gaussian kernel to be the mean of all pair-wise training data distances, a standard heuristic. For all the datasets, we randomly select 50% data for training, 20% for validation and the remaining for testing. We report accuracy values averaged over 20 runs for each method with varying number of landmark pairs.\nTable 2 compares the accuracies obtained by our FTUNE-S and FTUNE-M methods with those of BBS and DBOOST when applied to different UCI benchmark datasets. Note that FTUNE-S is one of the best on most of the datasets for both the landmarking sizes. Also, BBS performs reasonably well for small landmarking sizes while DBOOST performs well for large landmarking sizes. In contrast, our method consistently outperforms the existing methods in both the scenarios.\nNext, we study accuracies obtained by our method for different landmarking sizes. Figure 2 shows accuracies obtained by various methods as the number of landmarks selected increases. Note that the accuracy curve of our method dominates the accuracy curves of all the other methods, i.e. our method is consistently better than the existing methods for all the landmarking sizes considered."}, {"heading": "3.3 Discussion", "text": "We note that since FTUNE selects its output by way of validation, it is susceptible to over-fitting on small datasets but at the same time, capable of giving performance boosts on large ones. We observe a similar trend in our experiments \u2013 on smaller datasets (such as those in Table 1 with average dataset size 660), FTUNE over-fits and performs worse than BBS and DBOOST. However, even in these cases, DSELECT (intuitively) removes redundancies in the landmark points thus allowing FTUNE to recover the best transfer function. In contrast, for larger datasets like those in Table 2 (average\nsize 13200), FTUNE is itself able to recover better transfer functions than the baseline methods and hence both FTUNE-S and FTUNE-M perform significantly better than the baselines. Note that DSELECT is not able to provide any advantage here since the datasets sizes being large, greedy selection actually ends up hurting the accuracy."}, {"heading": "Acknowledgments", "text": "We thank the authors of [2] for providing us with C++ code of their implementation. P. K. is supported by Microsoft Corporation and Microsoft Research India under a Microsoft Research India Ph.D. fellowship award. Most of this work was done while P. K. was visiting Microsoft Research Labs India, Bangalore."}, {"heading": "A Comparison with the models of Balcan-Blum and Wang et al", "text": "In [2], Wang et al consider a model of learning with distance functions. Their model is similar to our but for the difference that they restrict themselves to the use of a single transfer function namely the sign function f = sgn(). More formally they have the following notion of a good distance function.\nDefinition 8 ([1] Definition 4). A distance functionX , d : X\u00d7X \u2192 R is said to be an (\u01eb, \u03b3, B)-good distance for a learning problem where \u01eb, \u03b3, B > 0 if there exist two class conditional probability distributions D\u0303(x|\u2113(x) = 1) and D\u0303(x|\u2113(x) = \u22121) such that for all x \u2208 X , D\u0303(x|\u2113(x)=1)D(x|\u2113(x)=1) < \u221a B and D\u0303(x|\u2113(x)=\u22121)D(x|\u2113(x)=\u22121) < \u221a B where D(x|\u2113(x) = 1) and D(x|\u2113(x) = \u22121) are the class conditional probability distributions of the problem, such that at least a 1 \u2212 \u01eb probability mass of examples x \u223c D satisfies\nD\u0303 x\u2032,x\u2032\u2032\u223cD\u0303\u00d7D\u0303 [d(x, x\u2032) < d(x, x\u2032\u2032)|\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)] \u2265 1 2 + \u03b3 (4)\nIt can be shown (and is implicit in the proof of Theorem 5 in [2]) that the above condition is equivalent to\nE x\u2032,x\u2032\u2032\u223cD\u00d7D\n[\nw\u2113(x)(x \u2032)w\u2212\u2113(x)(x \u2032\u2032) sgn (d(x, x\u2032\u2032)\u2212 d(x, x\u2032)) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x) ] \u2265 2\u03b3\nwhere w1(x) := D\u0303(x|\u2113(x)=1) D(x|\u2113(x)=1) and w\u22121(x) := D\u0303(x|\u2113(x)=\u22121) D(x|\u2113(x)=\u22121) . Now define \u031f(x \u2032, x\u2032\u2032) := w\u2113(x\u2032)(x \u2032)w\u2113(x\u2032\u2032)(x\n\u2032\u2032) and take f = sgn() as the transfer function in our model. We have, for a 1\u2212 \u01eb fraction of points,\nE x\u2032,x\u2032\u2032\u223cD\u00d7D\n[\u031f (x\u2032, x\u2032\u2032) f (K(x, x\u2032)\u2212K(x, x\u2032\u2032)) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)] \u2265 Cf\u03b3\nwhich is clearly seen to be equivalent to\nE x\u2032,x\u2032\u2032\u223cD\u00d7D\n[\nw\u2113(x)(x \u2032)w\u2212\u2113(x)(x \u2032\u2032) sgn (K(x, x\u2032)\u2212K(x, x\u2032\u2032)) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x) ] \u2265 \u03b3\nsince Cf = 1 for the sgn() function. Thus the Wang et al model of learning is an instantiation of our proposed model.\nIn [1], Balcan-Blum present a model of learning with similarity functions. Their model does not consider landmark pairs, just singletons. Accordingly, instead of assigning a weight to each landmark pair, one simply assigns a weight to each element of the domain. Consequently one arrives at the following notion of a good similarity.\nDefinition 9 ([2], Definition 3). A similarity measure K : X \u00d7 X \u2192 R is said to be an (\u01eb, \u03b3)-good similarity for a learning problem where \u01eb, \u03b3 > 0 if for some weighing function w : X \u2192 [\u22121, 1], at least a 1\u2212 \u01eb probability mass of examples x \u223c D satisfies\nE x\u2032\u223cD [w (x\u2032)K(x, x\u2032)|\u2113(x\u2032) = \u2113(x)] \u2265 E x\u2032\u223cD [w (x\u2032)K(x, x\u2032)|\u2113(x\u2032) 6= \u2113(x)] + \u03b3 (5)\nNow define w+ := E x\u223cD [w (x) |\u2113(x) = 1] and w\u2212 := E x\u223cD [w (x) |\u2113(x) = \u22121]. Furthermore, take \u031f(x\u2032, x\u2032\u2032) = w(x\u2032)w(x\u2032\u2032) as the weight function and f = id() as the transfer function in our model. Then we have, for a 1\u2212 \u01eb fraction of the points,\nE x\u2032,x\u2032\u2032\u223cD\u00d7D\n[\u031f (x\u2032, x\u2032\u2032) f (K(x, x\u2032)\u2212K(x, x\u2032\u2032)) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)] \u2265 Cf\u03b3\n\u2261 E x\u2032,x\u2032\u2032\u223cD\u00d7D [\u031f (x\u2032, x\u2032\u2032) (K(x, x\u2032)\u2212K(x, x\u2032\u2032)) |\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)] \u2265 \u03b3\n\u2261 E x\u2032,x\u2032\u2032\u223cD\u00d7D [\u031f (x\u2032, x\u2032\u2032)K(x, x\u2032)|\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)] \u2265\nE x\u2032,x\u2032\u2032\u223cD\u00d7D\n[\u031f (x\u2032, x\u2032\u2032)K(x, x\u2032\u2032)|\u2113(x\u2032) = \u2113(x), \u2113(x\u2032\u2032) 6= \u2113(x)] + \u03b3\n\u2261 w\u2212\u2113(x) E x\u2032\u223cD [w(x\u2032)K(x, x\u2032)|\u2113(x\u2032) = \u2113(x)] \u2265 w\u2113(x) E x\u2032\u223cD [w(x\u2032)K(x, x\u2032)|\u2113(x\u2032) 6= \u2113(x)] + \u03b3 \u2261 E x\u2032\u223cD [w\u2032(x\u2032)K(x, x\u2032)|\u2113(x\u2032) = \u2113(x)] \u2265 E x\u2032\u223cD [w\u2032(x\u2032)K(x, x\u2032)|\u2113(x\u2032) 6= \u2113(x)] + \u03b3\nwhere Cf = 1 for the id() function and w\u2032(x) = w(x)w\u2212\u2113(x). Note that this again guarantees a classifier with margin \u03b3 in the landmarked space. Thus the Balcan-Blum model can also be derived in our model."}], "references": [{"title": "On a Theory of Learning with Similarity Functions", "author": ["Maria-Florina Balcan", "Avrim Blum"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "On Learning with Dissimilarity Functions", "author": ["Liwei Wang", "Cheng Yang", "Jufu Feng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Fast Image Retrieval via Embeddings", "author": ["Piotr Indyk", "Nitin Thaper"], "venue": "In International Workshop Statistical and Computational Theories of Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "On Combining Dissimilarity Representations", "author": ["El\u017cbieta P\u0229kalska", "Robert P.W. Duin"], "venue": "In Multiple Classifier Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Similarity-based Classification: Concepts and Algorithms", "author": ["Yihua Chen", "Eric K. Garcia", "Maya R. Gupta", "Ali Rahimi", "Luca Cazzanti"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Learning with non-positive Kernels", "author": ["Cheng Soon Ong", "Xavier Mary", "St\u00e9phane Canu", "Alexander J. Smola"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Feature Space Interpretation of SVMs with Indefinite Kernels", "author": ["Bernard Haasdonk"], "venue": "IEEE Transactions on Pattern Analysis and Machince Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Classification on Pairwise Proximity Data", "author": ["Thore Graepel", "Ralf Herbrich", "Peter Bollmann-Sdorra", "Klaus Obermayer"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Improved Guarantees for Learning via Similarity Functions", "author": ["Maria-Florina Balcan", "Avrim Blum", "Nathan Srebro"], "venue": "In 21st Annual Conference on Computational Learning Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "LIBLINEAR: A Library for Large Linear Classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "More Generality in Efficient Multiple Kernel Learning", "author": ["Manik Varma", "Bodla Rakesh Babu"], "venue": "In 26th Annual International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Metric and Kernel Learning using a Linear Transformation", "author": ["Prateek Jain", "Brian Kulis", "Jason V. Davis", "Inderjit S. Dhillon"], "venue": "To appear, Journal of Machine Learning (JMLR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Kernels as Features: On Kernels, Margins, and Low-dimensional Mappings", "author": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "How Good Is a Kernel When Used as a Similarity Measure", "author": ["Nathan Srebro"], "venue": "In 20th Annual Conference on Computational Learning Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Computers and Intractability: A Guide to the theory of NP-Completeness", "author": ["M.R. Garey", "D.S. Johnson"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1979}, {"title": "The Hardness of Approximate Optima in Lattices, Codes, and Systems of Linear Equations", "author": ["Sanjeev Arora", "L\u00e1szl\u00f3 Babai", "Jacques Stern", "Z. Sweedyk"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Designing classifiers for fusion-based biometric verification", "author": ["Krithika Venkataramani", "B.V.K. Vijaya Kumar"], "venue": "In Plataniotis Boulgouris and Micheli-Tzankou, editors, Biometrics: Theory, Methods and Applications. Springer,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Our framework unifies and generalizes the frameworks proposed by [1] and [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "Our framework unifies and generalizes the frameworks proposed by [1] and [2].", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "Take for example the case of images on which the most natural notions of distance (Euclidean, Earth-mover) [3] do not form PSD kernels.", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "The first approach tries to coerce a given similarity measure into a PSD one by either clipping or shifting the spectrum of the kernel matrix [4, 5].", "startOffset": 142, "endOffset": 148}, {"referenceID": 4, "context": "The first approach tries to coerce a given similarity measure into a PSD one by either clipping or shifting the spectrum of the kernel matrix [4, 5].", "startOffset": 142, "endOffset": 148}, {"referenceID": 4, "context": "k-NN to handle non-PSD similarity/distance functions and consequently offer slow test times [5], or are forced to solve non-convex formulations [6, 7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 5, "context": "k-NN to handle non-PSD similarity/distance functions and consequently offer slow test times [5], or are forced to solve non-convex formulations [6, 7].", "startOffset": 144, "endOffset": 150}, {"referenceID": 6, "context": "k-NN to handle non-PSD similarity/distance functions and consequently offer slow test times [5], or are forced to solve non-convex formulations [6, 7].", "startOffset": 144, "endOffset": 150}, {"referenceID": 0, "context": "The third approach, which has been investigated recently in a series of papers [1, 2, 8, 9], uses the similarity function to embed the domain into a low dimensional Euclidean space.", "startOffset": 79, "endOffset": 91}, {"referenceID": 1, "context": "The third approach, which has been investigated recently in a series of papers [1, 2, 8, 9], uses the similarity function to embed the domain into a low dimensional Euclidean space.", "startOffset": 79, "endOffset": 91}, {"referenceID": 7, "context": "The third approach, which has been investigated recently in a series of papers [1, 2, 8, 9], uses the similarity function to embed the domain into a low dimensional Euclidean space.", "startOffset": 79, "endOffset": 91}, {"referenceID": 8, "context": "The third approach, which has been investigated recently in a series of papers [1, 2, 8, 9], uses the similarity function to embed the domain into a low dimensional Euclidean space.", "startOffset": 79, "endOffset": 91}, {"referenceID": 9, "context": "Assuming a certain \u201cgoodness\u201d property (that is formally defined) for the similarity function, these models offer both generalization guarantees in terms of how well-suited the similarity function is to the classification task as well as the ability to use fast algorithmic techniques such as linear SVM [10] on the landmarked space.", "startOffset": 304, "endOffset": 308}, {"referenceID": 0, "context": "The model proposed by Balcan-Blum in [1] gives sufficient conditions for a similarity function to be well suited to such a landmarking approach.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "in [2] on the other hand provide goodness conditions for dissimilarity functions that enable landmarking algorithms.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Our framework consequently unifies and generalizes those presented in [1] and [2].", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "Our framework consequently unifies and generalizes those presented in [1] and [2].", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "Now similar to [1, 2], our framework requires random sampling of training points to create the embedding space1.", "startOffset": 15, "endOffset": 21}, {"referenceID": 1, "context": "Now similar to [1, 2], our framework requires random sampling of training points to create the embedding space1.", "startOffset": 15, "endOffset": 21}, {"referenceID": 1, "context": "To address this issue, [2] proposes a heuristic to select the points that are to be used as landmarks.", "startOffset": 23, "endOffset": 26}, {"referenceID": 10, "context": "MKL techniques [11]) or the distance measure (eg.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "[12]) itself.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Akin to [1], our techniques can also be extended to learn a combination of (dis)similarity functions but we do not explore these extensions in this paper.", "startOffset": 8, "endOffset": 11}, {"referenceID": 12, "context": "For PSD kernels, the notion of goodness is defined in terms of the margin offered in the RKHS [13].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "in the sense that all PSD kernels that offer a good margin in their respective RKHSs satisfy some form of this goodness criterion as well [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "Balcan-Blum in [1] present a goodness criteria in which a good similarity function is considered to be one that, for most points, assigns a greater average similarity to similarly labeled points than to dissimilarly labeled points.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "Wang et al in [2] define a distance function d to be good if a large fraction of the domain is, on an average, closer to similarly labeled points than to dissimilarly labeled points.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "However it turns out that their definition is equivalent to one in which one again assigns weights to domain elements, as done by [1], and the following holds E x\u2032,x\u2032\u2032\u223cD\u00d7D [w(x)w(x) sgn (d(x, x)\u2212 d(x, x)) |l(x) = l(x), l(x) 6= l(x)] > \u03b3 (2) Assuming their respective goodness criteria, [1] and [2] provide efficient algorithms to learn classifiers with bounded generalization error.", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "However it turns out that their definition is equivalent to one in which one again assigns weights to domain elements, as done by [1], and the following holds E x\u2032,x\u2032\u2032\u223cD\u00d7D [w(x)w(x) sgn (d(x, x)\u2212 d(x, x)) |l(x) = l(x), l(x) 6= l(x)] > \u03b3 (2) Assuming their respective goodness criteria, [1] and [2] provide efficient algorithms to learn classifiers with bounded generalization error.", "startOffset": 286, "endOffset": 289}, {"referenceID": 1, "context": "However it turns out that their definition is equivalent to one in which one again assigns weights to domain elements, as done by [1], and the following holds E x\u2032,x\u2032\u2032\u223cD\u00d7D [w(x)w(x) sgn (d(x, x)\u2212 d(x, x)) |l(x) = l(x), l(x) 6= l(x)] > \u03b3 (2) Assuming their respective goodness criteria, [1] and [2] provide efficient algorithms to learn classifiers with bounded generalization error.", "startOffset": 294, "endOffset": 297}, {"referenceID": 0, "context": "As in [1, 2], our goodness criterion lends itself to a simple learning algorithm which consists of choosing a set of d random pairs of points from the domain P = {( x+i , x \u2212 i d i=1 (which we refer to We refer the reader to the appendix for a discussion.", "startOffset": 6, "endOffset": 12}, {"referenceID": 1, "context": "As in [1, 2], our goodness criterion lends itself to a simple learning algorithm which consists of choosing a set of d random pairs of points from the domain P = {( x+i , x \u2212 i d i=1 (which we refer to We refer the reader to the appendix for a discussion.", "startOffset": 6, "endOffset": 12}, {"referenceID": 14, "context": "Unfortunately, not only is this problem intractable but also hard to solve approximately [15, 16].", "startOffset": 89, "endOffset": 97}, {"referenceID": 15, "context": "Unfortunately, not only is this problem intractable but also hard to solve approximately [15, 16].", "startOffset": 89, "endOffset": 97}, {"referenceID": 0, "context": "With a similar objective in mind, [1] offers variants of its goodness criterion tailored to the hinge loss function which can be efficiently optimized on large training sets (for example LIBSVM [17]).", "startOffset": 34, "endOffset": 37}, {"referenceID": 16, "context": "With a similar objective in mind, [1] offers variants of its goodness criterion tailored to the hinge loss function which can be efficiently optimized on large training sets (for example LIBSVM [17]).", "startOffset": 194, "endOffset": 198}, {"referenceID": 17, "context": "Similar notions of diversity have been used in the past for ensemble classifiers [18] and k-NN classifiers [5].", "startOffset": 81, "endOffset": 85}, {"referenceID": 4, "context": "Similar notions of diversity have been used in the past for ensemble classifiers [18] and k-NN classifiers [5].", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "We refer to the algorithmic formulation presented in [1] as BBS and its augmentation using DSELECT as BBS+D.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "We refer to the formulation presented in [2] as DBOOST.", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "Note that these functions (approximately) include both the identity function (used by [1]) and the sign function (used by [2]).", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Note that these functions (approximately) include both the identity function (used by [1]) and the sign function (used by [2]).", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "To this end, we perform experiments on a few benchmark datasets for learning with similarity (non-PSD) functions [5] as well as on a variety of standard UCI datasets where the similarity function used is the Gaussian kernel function.", "startOffset": 113, "endOffset": 116}, {"referenceID": 9, "context": "For our experiments, we implemented our methods FTUNE and FTUNE+D as well as BBS and BBS+D using MATLAB while using LIBLINEAR [10] for SVM classification.", "startOffset": 126, "endOffset": 130}, {"referenceID": 1, "context": "For DBOOST, we use the C++ code provided by the authors of [2].", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "1 Similarity learning datasets First, we conduct experiments on a few similarity learning datasets [5]; these datasets provide a (non-PSD) similarity matrix along with class labels.", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "Similar to [2], we use the Gaussian kernel function as the similarity function for evaluating our method.", "startOffset": 11, "endOffset": 14}], "year": 2013, "abstractText": "We consider the problem of classification using similarity/distance functions over data. Specifically, we propose a framework for defining the goodness of a (dis)similarity function with respect to a given learning task and propose algorithms that have guaranteed generalization properties when working with such good functions. Our framework unifies and generalizes the frameworks proposed by [1] and [2]. An attractive feature of our framework is its adaptability to data we do not promote a fixed notion of goodness but rather let data dictate it. We show, by giving theoretical guarantees that the goodness criterion best suited to a problem can itself be learned which makes our approach applicable to a variety of domains and problems. We propose a landmarking-based approach to obtaining a classifier from such learned goodness criteria. We then provide a novel diversity based heuristic to perform task-driven selection of landmark points instead of random selection. We demonstrate the effectiveness of our goodness criteria learning method as well as the landmark selection heuristic on a variety of similarity-based learning datasets and benchmark UCI datasets on which our method consistently outperforms existing approaches by a significant margin.", "creator": "LaTeX with hyperref package"}}}