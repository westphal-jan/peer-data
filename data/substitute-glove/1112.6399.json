{"id": "1112.6399", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2011", "title": "Two-Manifold Problems", "abstract": "Recently, same though with unfortunately returns in spectral context to example chiral - - - yet - any disk eigenmap purposes. These methodology without two some notably, even into applicability if limited should supposed are make robust only seismic. To address comes permitting, so putting at the - manifold woes, 2001 which we simultaneously reconstruct two involved insoluble, over representative a various fact which then. report. By relate these niches learning such keep are free search have flow continues them, half - manifold method are take only succeed where a non - enable consistent allow accept: each view allows us to suppress sounds in the or, reducing misconduct when also same do nothing be improvisations parameter entry but eventually remove profiling into short {linear} time-span reduction problem. We propose now both of constructs on two - manifold needs, based from spectral convolution of off - coefficients customers as Hilbert craft. Finally, we resolved situations town out - generalized possibility are useful, often demonstrate that complicated a two - manifold nothing necessarily support in learning a approximation dynamical system eventually commercial data.", "histories": [["v1", "Thu, 29 Dec 2011 19:52:14 GMT  (7479kb,D)", "http://arxiv.org/abs/1112.6399v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["byron boots", "geoffrey j gordon"], "accepted": false, "id": "1112.6399"}, "pdf": {"name": "1112.6399.pdf", "metadata": {"source": "CRF", "title": "Two-Manifold Problems", "authors": ["Byron Boots", "Geoffrey J. Gordon"], "emails": ["beb@cs.cmu.edu", "ggordon@cs.cmu.edu"], "sections": [{"heading": null, "text": "Recently, there has been much interest in spectral approaches to learning manifolds\u2014 so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together and allowing information to flow between them, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias in the same way that an instrumental variable allows us to remove bias in a linear dimensionality reduction problem. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space. Finally, we discuss situations where two-manifold problems are useful, and demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data."}, {"heading": "1 Introduction", "text": "Manifold learning algorithms are non-linear methods for embedding a set of data points into a lowdimensional space while preserving local geometry. Recently, there has been a great deal of interest in spectral approaches to learning manifolds. These kernel eigenmap methods include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], Maximum Variance Unfolding (MVU) [4], and Maximum Entropy Unfolding (MEU) [5]. These\napproaches can be viewed as kernel principal component analysis [6] with specific choices of manifold kernels [7]: they seek a small set of latent variables that, through a nonlinear mapping, explains the observed high-dimensional data.\nDespite the popularity of kernel eigenmap methods, they are limited in one important respect: they generally only perform well when there is little or no noise. Several authors have attacked this problem using methods including neighborhood smoothing [8] and robust principal components analysis [9, 10], with some success under limited noise. Unfortunately, the problem is fundamentally ill posed without some sort of side information about the true underlying signal: by design, manifold methods will recover extra latent dimensions which \u201cexplain\u201d the noise.\nWe take a different approach to the problem of learning manifolds from noisy observations. We assume access to a set of instrumental variables: variables that are correlated with the true latent variables, but uncorrelated with the noise in observations. Such instrumental variables can be used to separate signal from noise, as described in Section 3. Instrumental variables have been used to allow consistent estimation of model parameters in many statistical learning problems, including linear regression [11], principal component analysis [12], and temporal difference learning [13]. Here we extend the scope of this technique to manifold learning. We will pay particular attention to the case of observations from two manifolds, each of which can serve as instruments for the other. We call such problems two-manifold problems.\nIn one-manifold problems, when the noise variance is significant compared to the manifold\u2019s geometry, kernel eigenmap methods are biased: they will fail to recover the true manifold, even in the limit of infinite data. Two-manifold methods, by contrast, can succeed in this case: we propose algorithms based on spectral decompositions related to cross-covariance operators in a tensor product space of two different manifold\nar X\niv :1\n11 2.\n63 99\nv1 [\ncs .L\nG ]\n2 9\nD ec\n2 01\nkernels, and show that the instrumental variable idea suppresses noise in practice. We also examine some theoretical properties of two-manifold methods: we argue consistency for a special case in Sec. 3.2.3, but we leave a full theoretical analysis of these algorithms to future work.\nThe benefits of the spectral approach to two-manifold problems are significant: first, the spectral decomposition naturally solves the manifold alignment problem by finding low-dimensional manifolds (defined by the left and right eigenvectors) that explain both sets of observations. Second, the connection between manifolds and covariance operators opens the door to solving more-sophisticated machine learning problems with manifolds, including nonlinear analogs of canonical correlations analysis (CCA) and reduced-rank regression (RRR).\nAs an example of this last point, subspace identification approaches to learning non-linear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinitedimensional) covariance operators [14, 15, 16, 17, 18]. Two-manifold problems are a natural fit: by relating the spectral decomposition to our two-manifold method, subspace identification techniques can be forced to identify a manifold state space, and consequently, to learn a dynamical system that is both accurate and interpretable, outperforming the current state of the art."}, {"heading": "2 Preliminaries", "text": "We begin by looking at two well-known classes of nonlinear dimensionality reduction: kernel principal component analysis (kernel PCA) and manifold learning."}, {"heading": "2.1 Kernel PCA", "text": "Kernel PCA [6] is a generalization of principal component analysis [12]: we first map our d-dimensional inputs x1, . . . , xn \u2208 Rd to a higher-dimensional feature space F using a feature mapping \u03c6 : Rd \u2192 F , and then find the principal components in this new space. If the features are sufficiently expressive, kernel PCA can find structure that regular PCA misses. However, if F is high- or infinite-dimensional, the straightforward approach to PCA, via an eigendecomposition of a covariance matrix, is in general intractable. Kernel PCA overcomes this problem by assuming that F is a reproducing-kernel Hilbert space (RKHS), and that the feature mapping \u03c6 is implicitly defined via an efficiently-computable kernel function K(x,x\u2032) = \u3008\u03c6(x), \u03c6(x\u2032)\u3009F . Popular kernels include the linear kernel K(x,x\u2032) = x \u00b7x\u2032 (which identifies the fea-\nture space with the input space) and the RBF kernel K(x,x\u2032) = exp(\u2212\u03b3\u2016x\u2212 x\u2032\u20162/2).\nConceptually, if we define an infinitely-tall \u201cmatrix\u201d with columns \u03c6(xi), \u03a6 = (\u03c6(x1), . . . , \u03c6(xn)), our goal is to recover the eigenvalues and eigenvectors of the centered covariance operator \u03a3\u0302XX = 1 n\u03a6H\u03a6 T, where H is the centering matrix H = In \u2212 1n11 T. To avoid working with the high- or infinite-dimensional covariance operator, we instead define the Gram matrix G = 1n\u03a6 T\u03a6. The nonzero eigenvalues of the centered covariance \u03a3\u0302XX are the same as those of the centered Gram matrix HGH. And, the corresponding unitlength eigenvectors of \u03a3\u0302XX are given by \u03a6Hvi\u03bb \u22121/2 i , where \u03bbi and vi are the eigenvalues and eigenvectors of HGH [6]. If data weights P (a diagonal matrix) are present, we can instead decompose the weighted centered Gram matrix PHGHP. (Note that, perhaps confusingly, we center first and multiply by the weights only after centering; the reason for this order will become clear below, in Section 2.2.)"}, {"heading": "2.2 Manifold Learning", "text": "Spectral algorithms for manifold learning, sometimes called kernel eigenmap methods, include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], and Maximum Variance Unfolding (MVU) [4]. These methods seek a nonlinear function that maps a high-dimensional set of data points to a lower-dimensional space while preserving the manifold on which the data lies. The main insight behind these methods is that large distances in input space are often meaningless due to the large-scale curvature of the manifold; so, ignoring these distances can lead to a significant improvement in dimensionality reduction by \u201cunfolding\u201d the manifold.\nInterestingly, these algorithms can be viewed as special cases of kernel PCA where the Gram matrix G is constructed over the finite domain of the training data in a particular way [7]. Specifically, kernel eigenmap methods first induce a neighborhood structure on the data, capturing a notion of local geometry via a graph, where nodes are data points and edges are neighborhood relations. Then they solve an eigenvalue or related problem based on the graph to embed the data into a lower dimensional space while preserving the local relationships. Here we just describe LE; the other methods have a similar intuition, although the details and performance characteristics differ.\nIn LE, neighborhoods are summarized by an adjacency matrix W, computed by nearest neighbors: element wi,j is nonzero whenever the ith point is one of the nearest neighbors of the jth point, or vice versa. Non-zero weights are typically either set to\n1 or computed according to a Gaussian RBF kernel: wi,j = exp(\u2212\u03b3\u2016xi \u2212 xj\u20162/2). Now let Si,i = \u2211 j wi,j , and set C = S\u22121/2(W \u2212 S)S\u22121/2. Finally, eigendecompose C to get a low dimensional embedding of the data points, discarding the top eigenvector (which is trivial): if C = V\u039bVT, the embedding is S1/2V2:k+1. To relate LE to kernel PCA, note that G = W \u2212 S is already centered, i.e., G = HGH. So, we can view LE as performing weighted kernel PCA, with Gram matrix G and data weights P = S\u22121/2.1,2,3\nThe weighted Gram matrix C can be related to a random walk on the graph defined by W: if we add the identity and make a similarity transform (both operations that preserve the eigensystem), we get S\u22121/2(C + I)S 1/2\n= S\u22121W, a stochastic transition matrix for the walk. So, Laplacian eigenmaps can be viewed as trying to keep points close together when they are connected by many short paths in our graph."}, {"heading": "3 Bias and Instrumental Variables", "text": "Kernel eigenmap methods are very good at dimensionality reduction when the original data points sample a high-dimensional manifold relatively densely, and when the noise in each sample is small compared to the local curvature of the manifold (or when we don\u2019t care about recovering curvature on a scale smaller than the noise). In practice, however, observations are frequently noisy. Depending on the nature of the noise, manifold-learning algorithms applied to these datasets produce biased embeddings. See Figures 1\u20132, the \u201cnoisy swiss rolls,\u201d for an example of how noise can bias manifold learning algorithms.\nTo see why, we examine PCA, a special case of manifold learning methods, and look at why it produces biased embeddings in the presence of noise. We first show how overcome this problem in the linear case, and\n1The original algorithm actually looks for the smallest eigenvalues of \u2212C, which is equivalent. The matrix \u2212G = S\u2212W is the graph Laplacian for our neighborhood graph, and \u2212C is the weighted graph Laplacian\u2014hence the algorithm name \u201cLaplacian eigenmaps.\u201d\n2The centering step is vestigial: we can include it, C = S\u22121/2H(W \u2212 S)HS\u22121/2, but it has no effect. The discarded eigenvector (which corresponds to eigenvalue 0 and to a constant coordinate in the embedding) similarly is vestigial: we can discard it or not, and it doesn\u2019t affect pairwise distances among embedded points. Interestingly, previous papers on the connection between Laplacian eigenmaps and the other algorithms mentioned here seem to contain an imprecision: they typically connect dropping the top (trivial) eigenvector with the centering step\u2014e.g., see [5].\n3A minor difference is that LE does not scale its embedding using the eigenvalues of G; the related diffusion maps algorithm [19] does.\nthen use these same ideas to fix kernel PCA, a nonlinear algorithm. Finally, in Sec. 4, we extend these ideas to fully general kernel eigenmap methods."}, {"heading": "3.1 Bias in Finite-Dimensional Linear Models", "text": "Suppose that xi is a noisy view of some underlying low-dimensional latent variable zi: xi = Mzi + i for a linear transformation M and i.i.d. zero-mean noise term i. Without loss of generality, we assume that xi and zi are centered, and that Cov[zi] and M both have full column rank: any component of zi in the nullspace of M doesn\u2019t affect xi.\nIn this case, PCA on X will generally fail to recover Z: the expectation of \u03a3\u0302XX = 1 nXX T is M Cov[zi] M T + Cov[ i], while we need M Cov[zi] M T to be able to recover a transformation of M or Z. The unwanted term Cov[ i] will, in general, affect all eigenvalues and eigenvectors of \u03a3\u0302XX , causing us to recover a biased answer even in the limit of infinite data."}, {"heading": "3.1.1 Instrumental Variables", "text": "We can fix this problem for linear embeddings: instead of plain PCA, we can use what might be called two-subspace PCA. This method finds a statistically consistent solution through the use of an instrumental variable [11, 12], an observation yi that is correlated with the true latent variables, but uncorrelated with the noise in xi.\nImportantly, picking an instrumental variable is not merely a statistical aid, but rather a value judgement about the nature of the latent variable and the noise in the observations. In particular, we are defining the noise to be that part of the variability which is uncorrelated with the instrumental variable, and the signal to be that part which is correlated.\nIn our example above, a good instrumental variable yi is a different (noisy) view of the same underlying low-dimensional latent variable: yi = Nzi + \u03b6i for some full-column-rank linear transformation N and i.i.d. zero-mean noise term \u03b6i. The expectation of the empirical cross covariance \u03a3\u0302XY = 1 nXY\nT is then M Cov(zi) N\nT: the noise terms, being independent and zero-mean, cancel out. (And the variance of each element of \u03a3\u0302XY goes to 0 as n\u2192\u221e.)\nIn this case, we can identify the embedding by computing the singular value decomposition (SVD) of the covariance: let UDVT = \u03a3\u0302XY , where U and V are orthonormal and D is diagonal. To reduce noise, we can keep just the columns of U, D, and V which correspond to the top k largest singular values (diagonal entries of D): \u3008U,D,V\u3009 = SVD(\u03a3\u0302XY , k). If we set k to be the true dimension of z, then as n\u2192\u221e, U will\nconverge to an orthonormal basis for the range of M, and V will converge to an orthonormal basis for the range of N. The corresponding embeddings are then given by UTX and VTY.\nInterestingly, we can equally well view xi as an instrumental variable for yi: we simultaneously find consistent embeddings of both xi and yi, using each to unbias the other."}, {"heading": "3.1.2 Whitening: Reduced-Rank Regression and Cannonical Correlation Analysis", "text": "Going beyond two-subspace PCA, there are a number of interesting spectral decompositions of crosscovariance matrices that involve transforming the variables xi and yi before applying a singular value decomposition [12]. For example, in reduced-rank regression [20, 12], we want to estimate E [xi | yi]. Define \u03a3\u0302Y Y = 1 nYY T and \u03a3\u0302XY = 1 nXY T. Then the ordinary regression of Y on X is \u03a3\u0302XY (\u03a3\u0302Y Y + \u03b7I) \u22121Y, where the regularization term \u03b7I (\u03b7 > 0) ensures that the matrix inverse is well-defined. For a reduced-rank regression, we instead project onto the set of rank-k matrices: \u3008U,D,V\u3009 = SVD(\u03a3\u0302XY (\u03a3\u0302Y Y + \u03b7I)\u22121Y, k). In our example above, so long as we let \u03b7 \u2192 0 as n\u2192\u221e, U will again converge to an orthonormal basis for the range of M.\nTo connect back to two-subspace PCA, we can show that RRR is the same as two-subspace PCA if we first whiten Y. That is, we can equivalently define U for RRR to be the top k left singular vectors of the covariance between X and the whitened instruments Yw = (\u03a3\u0302Y Y + \u03b7I)\n\u22121/2Y. (Here, A1/2 stands for the symmetric square root of A, which is guaranteed to exist and be invertible if A is symmetric and positive definite.) Whitening means transforming a covariance matrix toward the identity: if \u03b7 = 0 and \u03a3\u0302Y Y has full rank, then E[YwYTw] = \u03a3\u0302 \u22121/2 Y Y \u03a3\u0302Y Y \u03a3\u0302 \u22121/2 Y Y = I, while if \u03b7 is near 0, then E[YwYTw] \u2248 I.\nFor symmetry, we can whiten both X and Y before computing their covariance. An SVD of the resulting doubly-whitened cross-covariance matrix (\u03a3\u0302XX + \u03b7I)\u22121/2\u03a3\u0302XY (\u03a3\u0302Y Y + \u03b7I)\n\u22121/2 is called canonical correlation analysis [21], and the resulting singular values are called canonical correlations."}, {"heading": "3.2 Bias in Learning Nonlinear Models", "text": "We now extend the analysis of Section 3.1 to nonlinear models. We assume noisy observations xi = f(zi)+ i, where zi is the desired low-dimensional latent variable, i is an i.i.d. noise term, and f is a smooth function with smooth inverse (so that f(zi) lies on a manifold). Our goal is to recover f and zi up to identifiability.\nKernel PCA (Sec. 2.1) is a common approach to this problem. In the realizable case, kernel PCA gets the right answer: that is, suppose that zi has dimension k, and that we have at least k independent samples. And, suppose that \u03c6(f(z)) is a linear function of z. Then, the Gram matrix or the covariance \u201cmatrix\u201d will have rank k, and we can reconstruct a basis for the range of \u03c6 \u25e6 f from the top k eigenvectors of the Gram matrix. (Similarly, if \u03c6 \u25e6 f is near linear and the variance of i is small, we can expect kernel PCA to work well, if not perfectly.)\nHowever, just as PCA recovers a biased answer in the finite dimensional case when the variance of i is nonzero, kernel PCA will also recover a biased answer in this case, even in the limit of infinite data. The bias of kernel PCA follows immediately from the example at the beginning of Section 3: if we use a linear kernel, kernel PCA will simply reproduce the bias of ordinary PCA."}, {"heading": "3.2.1 Instrumental Variables", "text": "By analogy to two-subspace PCA, a a natural generalization of kernel PCA is two-subspace kernel PCA, which we can accomplish via a kernelized SVD of a cross-covariance operator in Hilbert space. Given a joint distribution P[X,Y ] over two variables X on X and Y on Y, with feature maps \u03c6 and \u03c5 (corresponding to kernels Kx and Ky), the cross-covariance operator \u03a3XY is E[\u03c6(x) \u2297 \u03c5(y)]. The cross-covariance operator reduces to an ordinary cross-covariance matrix in the finite-dimensional case; in the infinite-dimensional case, it can be viewed as a kernel mean map descriptor [22] for the joint distribution P[X,Y ].\nThe concept of a cross-covariance operator is helpful because it allows us to extend the methods of instrumental variables to infinite dimensional RKHSs. In our example above, a good instrumental variable yi is a different (noisy) view of the same underlying lowdimensional latent variable: yi = g(zi) + \u03b6i for some smoothly invertible function g and i.i.d. zero-mean noise term \u03b6i.\nWe proceed now to derive the kernel SVD for a crosscovariance operator.4 We show below (Sec. 3.2.3) that the kernel SVD of the cross-covariance operator leads to a consistent estimate of the shared latent variables in two-manifold problems which satisfy appropriate assumptions.\n4The kernel SVD algorithm previously appeared as an intermediate step in [17, 23]; here we generalize the algorithm to weighted cross covariance operators in Hilbert space and give a more complete description, both because the method is interesting in its own right, and because this generalized SVD will serve as a step in our two-manifold algorithms.\nConceptually, our inputs are \u201cmatrices\u201d \u03a6 and \u03a5 whose columns are respectively \u03c6(xi) and \u03c5(yi), along with data weights PX and PY . The centered empirical covariance operator is then \u03a3\u0302XY = 1 n (\u03a6H)(\u03a5H) T. The goal of the kernel SVD is then to factor \u03a3\u0302XY (or possibly the weighted variant \u03a3\u0302 P XY = 1 n (\u03a6H)PXPY (\u03a5H)\nT) so that we can recover the desired bases for \u03c6(xi) and \u03c5(yi).\nHowever, this conceptual algorithm is impractical, since \u03a3\u0302XY can be high- or infinite-dimensional. Instead, we can perform an SVD on the covariance operator in Hilbert space via a trick analogous to kernel PCA. We first show how to perform an SVD on a covariance matrix using Gram matrices in finitedimensional space, and then we extend the method to infinite dimensional spaces in Section 3.2.3."}, {"heading": "3.2.2 SVD via Gram matrices", "text": "We start by looking at a Gram matrix formulation of finite dimensional SVD. In standard SVD, the singular values of \u03a3\u0302XY = 1 n (XH)(YH) T are the square roots of the eigenvalues of \u03a3\u0302XY \u03a3\u0302Y X (where \u03a3\u0302Y X = \u03a3\u0302 T XY ), and the left singular vectors are defined to be the corresponding eigenvectors. We can find identical eigenvectors and eigenvalues through centered Gram matrices BX = 1 n (XH) T(XH) and BY = 1 n (YH)\nT(YH). Let vi be the right eigenvector of BY BX so that BY BXvi = \u03bbivi. Premultiplying by (XH) yields\n1\nn2 (XH)(YH)T(YH)(XH)T(XH)vi = \u03bbi(XH)vi\nand regrouping terms gives us \u03a3\u0302XY \u03a3\u0302Y Xwi = \u03bbiwi where wi = (XH)vi. So, \u03bbi is an eigenvalue of \u03a3\u0302XY \u03a3\u0302Y X , \u221a \u03bbi is a singular value of \u03a3\u0302XY , and (XH)vi\u03bb \u22121/2 i is the corresponding unit length left singular vector. An analogous argument shows that, if w\u2032i is a unit-length right singular vector of \u03a3\u0302XY , then w\u2032i = (YH)v \u2032 i\u03bb \u22121/2 i , where v \u2032 i is a unit-length left eigenvector of BY BX . Furthermore, we can easily incorporate weights into SVD by using weighted matrices CX = PXBXPX and CY = PY BY PY in place of BX and BY ."}, {"heading": "3.2.3 Two-subspace PCA in RKHSs", "text": "The machinery developed in Section 3.2.2 allows us to solve the two-subspace kernel PCA problem by computing the singular values of the weighted empirical covariance operator \u03a3\u0302PXY . We define GX and GY to be the Gram matrices whose elements are Kx(xi,xj) and Ky(yi,yj) respectively, and then compute the eigendecomposition of CY CX = (PY HGY HPY )(PXHGXHPX). This method avoids any computations in infinite-\ndimensional spaces; and, it gives us compact representations of the left and right singular vectors. E.g., if vi is a right eigenvector of CY CX , then the corresponding singular vector is wi = \u2211 j vi,j \u03c5(xj \u2212 x\u0304)pxj , where pxj is the weight assigned to data point xj .\nUnder appropriate assumptions, we can show that the SVD of the empirical cross-covariance operator \u03a3\u0302XY = 1 n\u03a6H\u03a5\nT converges to the desired value. Suppose that E[\u03c6(xi) | zi] is a linear function of zi, and similarly, that E[\u03c5(yi) | zi] is a linear function of zi.\n5 The noise terms \u03c6(xi) \u2212 E[\u03c6(xi) | zi] and \u03c5(yi) \u2212 E[\u03c5(yi) | zi] are by definition zero-mean; and they are independent of each other, since the first depends only on i and the second only on \u03b6i. So, the noise terms cancel out, and the expectation of \u03a3\u0302XY is the true covariance \u03a3XY . If we additionally assume that the noise terms have finite variance, the product-RKHS norm of the error \u03a3\u0302XY \u2212 \u03a3XY vanishes as n\u2192\u221e.\nThe remainder of the proof follows from the proof of Theorem 1 in [17] (the convergence of the empirical estimator of the kernel covariance operator). In particular, the top k left singular vectors of \u03a3\u0302XY converge to a basis for the range of E[\u03c6(xi) | zi] (considered as a function of zi); similarly, the top right singular vectors of \u03a3\u0302XY converge to a basis for the range of E[\u03c5(yi) | zi]."}, {"heading": "3.2.4 Whitening and Kernel SVD", "text": "Just as one can use kernel SVD to solve the twosubspace kernel PCA problem for high- or infinitedimensional feature space, one can also compute the kernel versions of CCA and RRR. (Kernel CCA is a well-known algorithm, though our formulation here is different than in [23], and kernel RRR is to our knowledge novel.) Again, these problems can be solved by pre-whitening the feature space before applying kernel SVD.\nTo compute a RRR from centered covariates H\u03a5 in Hilbert space to centered responses H\u03a6 in Hilbert space, we find the kernel SVD of the H\u03a5-whitened covariance matrix: first we define BX = HGXH and BY = HGY H; next we compute BXBY (B 2 Y + \u03b7I)\u22121BY ; and, finally, we perform kernel SVD by finding the singular value decomposition of BXBY (B 2 Y + \u03b7I)\u22121BY .\n5The assumption of linearity is restrictive, but appears necessary: in order to learn a representation of a manifold using factorization-based methods, we need to pick a kernel which flattens out the manifold into a subspace. This is why kernel eigenmap methods are generally more successful than plain kernel PCA: by learning an appropriate kernel, they are able to adapt their nonlinearity to the shape of the target manifold.\nSimilarly, for CCA in Hilbert space, we find the kernel SVD of the H\u03a5- and H\u03a6-whitened covariance matrix: BX(B 2 X + \u03b7I) \u22121BXBY (B 2 Y + \u03b7I) \u22121BY .\nIf data weights are present, we use instead weighted centered Gram matrices CX = PXBXPX and CY = PY BY PY . Then, to generalize RRR and CCA to RKHSs, we can compute the eigendecompositions of Equations 1a\u2013b, respectively:\nCXCY (C 2 Y + \u03b7I) \u22121CY (1a)\nCX(C 2 X + \u03b7I) \u22121CXCY (C 2 Y + \u03b7I) \u22121CY (1b)\nThe consistency of both approaches follows directly from the consistency of kernel SVD, so long as we let \u03b7 \u2192 0 as n\u2192\u221e [24]."}, {"heading": "4 Two-Manifold Problems", "text": "Now that we have extended the instrumental variable idea to RKHSs, we can also expand the scope of manifold learning to two-manifold problems, where we want to simultaneously learn two manifolds for two covarying lists of observations, each corrupted by uncorrelated noise.6 The idea is simple: we view manifold learners as constructing Gram matrices, then apply the RKHS instrumental variable idea of Sec. 3. As we will see below, this procedure allows us to regain good performance when observations are noisy.\nSuppose we are given two set of observations residing on (or near) two different manifolds: x1, . . . ,xn \u2208 Rd1 on a manifoldMX and y1, . . . ,yn \u2208 Rd2 on a manifold MY . Further suppose that both xi and yi are noisy functions of a latent variable zi, itself residing on a latent k-dimensional manifold MZ : xi = f(zi) + i and yi = g(zi) + \u03b6i. We assume that the functions f and g are smooth, so that f(zi) and g(zi) trace out submanifolds f(MZ) \u2286 MX and g(MZ) \u2286 MY . We further assume that the noise terms i and \u03b6i move xi and yi within their respective manifoldsMX andMY : this assumption is without loss of generality, since we can can always increase the dimension of the manifolds MX and MY to allow an arbitrary noise term. See Figure 1 for an example.\nIf the variance of the noise terms i and \u03b6i is too high, or if MX and MY are higher-dimensional than the latent MZ manifold (i.e., if the noise terms move xi and yi away from f(MZ) and g(MZ)), then it may be\n6The uncorrelated noise assumption is extremely mild: if some latent variable causes correlated changes in our measurements on the two manifolds, then we are making the definition that it is part of the desired signal to be recovered. No other definition seems reasonable: if there is no difference in statistical behavior between signal and noise, then it is impossible to use a statistical method to separate signal from noise.\ndifficult to reconstruct f(MZ) or g(MZ) separately from xi or yi. Our goal, therefore, is to use xi and yi together to reconstruct both manifolds simultaneously: the extra information from the correspondence between xi and yi will make up for noise, allowing success in the two-manifold problem where the individual one-manifold problems are intractable.\nGiven samples of n i.i.d. pairs {xi,yi}ni=1 from two manifolds, we propose a two-step spectral learning algorithm for two-manifold problems: first, use either a given kernel or an ordinary one-manifold algorithm such as LE or LLE to compute weighted centered Gram matrices CX and CY from xi and yi separately. Second, use use one of the cross-covariance methods from Section 3.2.4, such as kernel SVD, to recover the embedding of points inMZ . The procedure, called instrumental eigenmaps, is summarized in Algorithm 1.\nFor example, if we are using LE (Section 2.2), then CX = S \u22121/2 X (WX\u2212SX)S \u22121/2 X and CY = S \u22121/2 Y (WY \u2212 SY )S \u22121/2 Y , where WX and SX are computed from xi, and WY and SY are computed from yi. We then perform a singular value decomposition and truncate to the top k singular values:\n\u3008U,\u039b,VT\u3009 = SVD(CXCY , k)\nAlgorithm 1 Instrumental Eigenmaps In: n i.i.d. pairs of observations {xi,yi}ni=1 Out: embeddings EX and EY\n1: Compute centered Gram matrices: BX and BY and data weights PX and PY from x1:n and y1:n 2: Compute weighted, centered Gram matrices: CX = PXBXPX and CY = PY BY PY 3: Perform a singular value decomposition and truncate the top k singular values: \u3008U,\u039b,VT\u3009 = SVD(CXCY , k) 4: Find the embeddings from the singular values:\nEX = P 1/2 X U2:k+1\u039b 1/2 2:k+1 and EY = P 1/2 Y V2:k+1\u039b 1/2 2:k+1\nThe embeddings for xi and yi are then given by\nS 1/2 X U2:k+1\u039b 1/2 2:k+1 and S 1/2 Y V2:k+1\u039b 1/2 2:k+1.\nComputing eigenvalues of CXCY instead of CX or CY alone will alter the eigensystem: it will promote directions within each individual learned manifold that are useful for predicting coordinates on the other learned manifold, and demote directions that are not useful. As shown in Figure 2, this effect strengthens our ability to recover relevant dimensions in the face of noise."}, {"heading": "5 Two-Manifold Detailed Example: Learning Dynamical Systems", "text": "A longstanding goal in machine learning and robotics has been to learn accurate, economical models of dynamical systems directly from observations. This task requires two related subtasks: 1) learning a low dimensional state space, which is often known to lie on a manifold ; and 2) learning the system dynamics. We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.\nAny of the above-cited spectral learning algorithms can be combined with two-manifold methods. Here, we focus on one specific example: we show how to combine HSE-HMMs [17], a powerful nonparametric approach to modeling dynamical systems, with manifold learning. Specifically, we look at one important step in the HSE-HMM learning algorithm: the original approach uses kernel SVD to discover a low-dimensional state space, and we propose replacing the kernel SVD with a two-manifold method. We demonstrate that the resulting manifold HSE-HMM can outperform standard HSE-HMMs (and many other well-known methods for learning dynamical systems) on a difficult realworld example: the manifold HSE-HMM accurately\ndiscovers a curved low-dimensional manifold which contains the state space, while other methods discover only a (potentially much higher-dimensional) subspace which contains this manifold."}, {"heading": "5.1 Hilbert Space Embeddings of HMMs", "text": "The key idea behind spectral learning of dynamical systems is that a good latent state is one that lets us predict the future. HSE-HMMs implement this idea by finding a low-dimensional embedding of the conditional probability distribution of sequences of future observations, and using the embedding coordinates as state. Song et al. [17] suggest finding this low-dimensional state space as a subspace of an infinite dimensional RKHS.\nIntuitively, we might think that we could find the best state space by performing PCA or kernel PCA of sequences of future observations. That is, we would sample n sequences of future observations x1, . . . ,xn \u2208 Rd1 from a dynamical system. (If our training data consists of a single long sequence of observations, we can collect our sample by picking n random time steps t1, . . . , tn. Each sample xi is then a sequence of obser-\nvations starting at time ti.) We would then construct a Gram matrix GX , whose (i, j) element is Kx(xi,xj). Finally, we would find the eigendecomposition of the centered Gram matrix BX = HGXH as in Section 2.1. The resulting embedding coordinates would be tuned to predict future observations well, and so could be viewed as a good state space.\nHowever, the state space found by kernel PCA is biased : it typically includes noise, information that cannot be predicted from past observations. We would like instead to find a low dimensional state space that embeds the probability distribution of possible futures conditioned on the past. In particular, we want to find a state space that is uncorrelated with the noise in the future observations. From a two-manifold perspective, we view features of the past as instrumental variables to unbias the future.\nTherefore, in addition to sampling sequences of future observations, we sample corresponding sequences of past observations y1, . . . ,yn \u2208 Rd2 : sequence yi ends at time ti\u2212 1. We then construct a Gram matrix GY , whose (i, j) element is Ky(yi,yj). From GY we construct the centered Gram matrix BY = HGY H. Finally, we identify the state space using a kernel SVD as in Section 3.2.3: \u3008U,\u039b,VT\u3009 = SVD(BXBY , k) (there are no data weights for HSE-HMMs). The left singular \u201cvectors\u201d (reconstructed from U as in Section 3.2.3) now identify a subspace in which the system evolves. From this subspace, we can proceed to identify the parameters of the dynamical system as in Song et al. [17].7"}, {"heading": "5.2 Manifold HSE-HMMs", "text": "In contrast with HSE-HMMs, we are interested in modeling a dynamical system whose state space lies on a low-dimensional manifold, even if this manifold is curved to occupy a higher-dimensional subspace (an example is given in Section 5.3, below). We want to use this additional knowledge to constrain the learning algorithm and produce a more accurate model for a given amount of training data.\nTo do so, we replace the kernel SVD by a two-manifold method. That is, we learn weighted centered Gram matrices CX and CY for the future and past observations, using a manifold method like LE or LLE (see Section 2.2). Then we apply a SVD to CXCY in order to recover the latent state space.\n7Other approaches such as CCA and RRR have also been successfully used for finite-dimensional spectral learning algorithms [18], suggesting that kernel SVD could be replaced by kernel CCA or kernel RRR (Section 3.2.4)."}, {"heading": "5.3 Slotcar: A Real-World Dynamical System", "text": "Here we look at the problem of tracking and predicting the position of a slotcar with attached inertial measurement unit (IMU) racing around a track. The setup consisted of a track and a miniature car (1:32 scale model) guided by a slot cut into the track. Figure 3(A) shows setup. At a rate of 10Hz, we extracted the estimated 3-D acceleration and angular velocity of the car. An overhead camera also supplied estimates of the 2-dimensional location of the car.\nWe collected 3000 successive observations while the slot car circled the track controlled by a constant policy (with varying speeds). The goal was to learn a dynamical model of the noisy IMU data, and, after filtering, to predict current and future 2-dimensional locations. We used the first 2000 data points as training data, and held out the last 500 data points for testing the learned models. We trained four models, and evaluated these models based on prediction accuracy, and, where appropriate, the learned latent state.\nFirst, we trained a 20-dimensional embedded HMM with the spectral algorithm of Song et al. [17], using sequences of 150 consecutive observations. Following Song et al., we used Gaussian RBF kernels, setting the bandwidth parameter with the \u201cmedian trick\u201d, and using regularization parameter \u03bb = 10\u22124. Second, we trained a similar 20-dimensional embedded HMM with LE kernels. The number of nearest neighbors was selected to be 50, and the other parameters were set to be identical to the first model. (So, the only difference is that the first model performs a kernel SVD to find the subspace on which the dynamical system evolves, while the second model solves a two-manifold problem.) Third, for comparison\u2019s sake, we trained a 20-dimensional Kalman filter using the N4SID algorithm [26] with Hankel matrices of 150 time steps; and finally, we learned a 20-state discrete HMM (with 400 levels of discretization for observations) using the EM algorithm.\nBefore investigating the predictive accuracy of the learned dynamical systems, we looked at the learned state space of the first three models. These models differ mainly in their kernel: Gaussian RBF, learned manifold from Laplacian Eigenmaps, or linear. As a test, we tried to reconstruct the 2-dimensional locations of the car from each of the three latent state spaces: the more accurate the learned state space, the better we expect to be able to reconstruct the locations. Results are shown in Figure 3(B); the learned manifold is the clear winner.\nFinally we examined the prediction accuracy of each model. We performed filtering for different extents t1 = 100, . . . , 350, then predicted the car location for\na further t2 steps in the future, for t2 = 1, . . . , 100. The root mean squared error of this prediction in the 2-dimensional location space is plotted in Figure 3(C). The Manifold HMM learned by the method detailed in Section 5.2 consistently yields lower prediction error for the duration of the prediction horizon. This is important: not only does the manifold HSE-HMM provide a model that better visualizes the data that we want to predict, but it is significantly more accurate when filtering and predicting than classic and state-ofthe-art alternatives."}, {"heading": "6 Related Work", "text": "While preparing this manuscript, we learned of the simultaneous and independent work of Mahadevan et al. [27]. This paper defines one particular two-manifold algorithm, maximum covariance unfolding (MCU), by extending maximum variance unfolding; but, it does not discuss how to extend other one-manifold methods. It also does not discuss any asymptotic properties of the MCU method, such as consistency.\nA similar problem to the two-manifold problem is manifold alignment [28, 29], which builds connections between two or more data sets by aligning their underlying manifolds. Generally, manifold alignment algorithms either first learn the manifolds separately and then attempt to align them based on their lowdimensional geometric properties, or they take the union of several manifolds and attempt to learn a\nlatent space that preserves the geometry of all of them [29]. Our aim is different: we assume paired data, where manifold alignments do not; and, we focus on learning algorithms that simultaneously discover manifold structure (as kernel eigenmap methods do) and connections between manifolds (as provided by, e.g., a top-level learning problem defined between two manifolds).\nThis type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34]. RRR is a linear regression method that attempts to estimate a set of coefficients \u03b2 to predict response vectors yi from covariate vectors xi under the constraint that \u03b2 is low rank (and can therefore be factored). In SDR, the goal is to find a linear subspace of covariates xi that makes response vectors yi conditionally independent of the xis. The formulation is in terms of conditional independence, and, unlike in RRR, no assumption is made on the form of the regression from xi to yi. Unfortunately, SDR pays a price for the relaxation of the linear assumption: the solution to SDR problems usually requires a difficult non-linear non-convex optimization.\nManifold kernel dimension reduction [35], finds an embedding of covariates xi using a kernel eigenmap method, and then attempts to find a linear transformation of some of the dimensions of the embedded points to predict response variables yi. The response variables are constrained to be linear in the manifold,\nso the problem is quite different from a two-manifold problem.\nThere has also been some work on finding a mapping between manifolds [36] and learning a dynamical system on a manifold [37]; however, in both of these cases it was assumed that the manifold was known.\nFinally, some authors have focused on the problem of combining manifold learning algorithms with system identification algorithms. For example, Lewandowski et al. [38] introduces Laplacian eigenmaps that accommodate time series data by picking neighbors based on temporal ordering. Lin et al. [39] and Li et al. [40] propose learning a piecewise linear model that approximates a non-linear manifold and then attempt to learn the dynamics in the low-dimensional space. Although these methods do a qualitatively reasonable job of constructing a manifold, none of these papers compares the predictive accuracy of its model to state-of-the-art dynamical system identification algorithms."}, {"heading": "7 Conclusion", "text": "In this paper we propose a class of problems called twomanifold problems, where two sets of corresponding data points, generated from a single latent manifold and corrupted by noise, lie on or near two different higher dimensional manifolds. We design algorithms by relating two-manifold problems to cross-covariance operators in RKHSs, and show that these algorithms result in a significant improvement over standard manifold learning approaches in the presence of noise. This is an appealing result: manifold learning algorithms typically assume that observations are (close to) noiseless, an assumption that is rarely satisfied in practice.\nFurthermore, we demonstrate the utility of twomanifold problems by extending a recent dynamical system identification algorithm to learn a system with a state space that lies on a manifold. The resulting algorithm learns a model that outperforms the current state-of-the-art in terms of predictive accuracy. To our knowledge this is the first combination of system identification and manifold learning that accurately identifies a latent time series manifold and is competitive with the best system identification algorithms at learning accurate predictive models."}, {"heading": "Acknowledgements", "text": "Byron Boots and Geoffrey J. Gordon were supported by ONR MURI grant number N00014-09-1-1052. Byron Boots was supported by the NSF under grant number EEEC-0540865."}], "references": [{"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B. Tenenbaum", "Vin De Silva", "John Langford"], "venue": "Science, 290:2319\u20132323,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T. Roweis", "Lawrence K. Saul"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Learning a kernel matrix for nonlinear dimensionality reduction", "author": ["Kilian Q. Weinberger", "Fei Sha", "Lawrence K. Saul"], "venue": "Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Spectral dimensionality reduction via maximum entropy", "author": ["Neil D. Lawrence"], "venue": "In Proc. AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["Bernhard Sch\u00f6lkopf", "Alex J. Smola", "Klaus- Robert M\u00fcller"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "A kernel view of the dimensionality reduction of manifolds", "author": ["Jihun Ham", "Daniel D. Lee", "Sebastian Mika", "Bernhard Schlkopf"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Neighborhood smoothing embedding for noisy manifold learning", "author": ["Guisheng Chen", "Junsong Yin", "Deyi Li"], "venue": "In GrC,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Robust local tangent space alignment via iterative weighted PCA", "author": ["Yubin Zhan", "Jianping Yin"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1985}, {"title": "Causality: models, reasoning, and inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Principal component analysis", "author": ["I.T. Jolliffe"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["Steven J. Bradtke", "Andrew G. Barto"], "venue": "In Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["Daniel Hsu", "Sham Kakade", "Tong Zhang"], "venue": "In COLT,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Reduced-rank hidden Markov models", "author": ["Sajid Siddiqi", "Byron Boots", "Geoffrey J. Gordon"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["Byron Boots", "Sajid M. Siddiqi", "Geoffrey J. Gordon"], "venue": "In Proceedings of Robotics: Science and Systems VI,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Hilbert space embeddings of hidden Markov models", "author": ["L. Song", "B. Boots", "S.M. Siddiqi", "G.J. Gordon", "A.J. Smola"], "venue": "In Proc. 27th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Predictive state temporal difference learning", "author": ["Byron Boots", "Geoff Gordon"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Diffusion maps, spectral clustering and eigenfunctions of fokkerplanck operators", "author": ["Boaz Nadler", "St\u00e9phane Lafon", "Ronald R. Coifman", "Ioannis G. Kevrekidis"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Multivariate Reduced-rank Regression: Theory and Applications", "author": ["Gregory C. Reinsel", "Rajabather Palani Velu"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "The most predictable criterion", "author": ["Harold Hotelling"], "venue": "Journal of Educational Psychology,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1935}, {"title": "A Hilbert space embedding for distributions", "author": ["A.J. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "Algorithmic Learning Theory, Lecture Notes on Computer Science. Springer,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Consistency of kernel canonical correlation analysis", "author": ["K. Fukumizu", "F. Bach", "A. Gretton"], "venue": "Technical Report 942,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Hilbert space embeddings of conditional distributions", "author": ["L. Song", "J. Huang", "A. Smola", "K. Fukumizu"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "An online spectral learning algorithm for partially observable nonlinear dynamical systems", "author": ["Byron Boots", "Sajid Siddiqi", "Geoffrey Gordon"], "venue": "In Proceedings of the 25th National Conference on Artificial Intelligence (AAAI-2011),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Subspace Identification for Linear Systems: Theory, Implementation, Applications", "author": ["P. Van Overschee", "B. De Moor"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1996}, {"title": "Maximum covariance unfolding: Manifold learning for bimodal data", "author": ["Vijay Mahadevan", "Chi Wah Wong", "Jose Costa Pereira", "Tom Liu", "Nuno Vasconcelos", "Lawrence Saul"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Semisupervised alignment of manifolds", "author": ["Jihun Ham", "Daniel Lee", "Lawrence Saul"], "venue": "10th International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "A general framework for manifold alignment", "author": ["Chang Wang", "Sridhar Mahadevan"], "venue": "In Proc. AAAI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Estimating linear restrictions on regression coefficients for multivariate normal distributions", "author": ["T.W. Anderson"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1951}, {"title": "Reduced-rank regression for the multivariate linear model", "author": ["Alan Julian Izenman"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1975}, {"title": "Sliced inverse regression for dimension reduction", "author": ["Ker-Chau Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1991}, {"title": "Theory and methods: Special invited paper: Dimension reduction and visualization in discriminant analysis (with discussion)", "author": ["R. Dennis Cook", "Xiangrong Yin"], "venue": "Australian and New Zealand Journal of Statistics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces", "author": ["Kenji Fukumizu", "Francis R. Bach", "Michael I. Jordan", "Chris Williams"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Regression on manifolds using kernel dimension reduction", "author": ["Jens Nilsson", "Fei Sha", "Michael I. Jordan"], "venue": "In ICML,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Nonparametric regression between general riemannian manifolds", "author": ["Florian Steinke", "Matthias Hein", "Bernhard Sch\u00f6lkopf"], "venue": "SIAM J. Imaging Sciences,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "A recursive filter for linear systems on riemannian manifolds", "author": ["Ambrish Tyagi", "James W. Davis"], "venue": "Computer Vision and Pattern Recognition, IEEE Computer Society Conference on,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "Temporal extension of laplacian eigenmaps for unsupervised dimensionality reduction of time series", "author": ["Michal Lewandowski", "Jes\u00fas Mart\u0301\u0131nez del Rinc\u00f3n", "Dimitrios Makris", "Jean-Christophe Nebel"], "venue": "In ICPR,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Learning nonlinear manifolds from time series", "author": ["Ruei sung Lin", "Che bin Liu", "Ming hsuan Yang", "Narendra Ahuja", "Stephen Levinson"], "venue": "In In Proc. ECCV,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Simultaneous learning of nonlinear manifold and dynamical models for high-dimensional time series", "author": ["Rui Li", "Stan Sclaroff Phd", "Margrit Betke Phd", "David J. Fleet. S"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "These kernel eigenmap methods include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], Maximum Variance Unfolding (MVU) [4], and Maximum Entropy Unfolding (MEU) [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "These kernel eigenmap methods include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], Maximum Variance Unfolding (MVU) [4], and Maximum Entropy Unfolding (MEU) [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "These kernel eigenmap methods include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], Maximum Variance Unfolding (MVU) [4], and Maximum Entropy Unfolding (MEU) [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "These kernel eigenmap methods include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], Maximum Variance Unfolding (MVU) [4], and Maximum Entropy Unfolding (MEU) [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 4, "context": "These kernel eigenmap methods include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], Maximum Variance Unfolding (MVU) [4], and Maximum Entropy Unfolding (MEU) [5].", "startOffset": 190, "endOffset": 193}, {"referenceID": 5, "context": "These approaches can be viewed as kernel principal component analysis [6] with specific choices of manifold kernels [7]: they seek a small set of latent variables that, through a nonlinear mapping, explains the observed high-dimensional data.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "These approaches can be viewed as kernel principal component analysis [6] with specific choices of manifold kernels [7]: they seek a small set of latent variables that, through a nonlinear mapping, explains the observed high-dimensional data.", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "Several authors have attacked this problem using methods including neighborhood smoothing [8] and robust principal components analysis [9, 10], with some success under limited noise.", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "Several authors have attacked this problem using methods including neighborhood smoothing [8] and robust principal components analysis [9, 10], with some success under limited noise.", "startOffset": 135, "endOffset": 142}, {"referenceID": 9, "context": "Instrumental variables have been used to allow consistent estimation of model parameters in many statistical learning problems, including linear regression [11], principal component analysis [12], and temporal difference learning [13].", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "Instrumental variables have been used to allow consistent estimation of model parameters in many statistical learning problems, including linear regression [11], principal component analysis [12], and temporal difference learning [13].", "startOffset": 191, "endOffset": 195}, {"referenceID": 11, "context": "Instrumental variables have been used to allow consistent estimation of model parameters in many statistical learning problems, including linear regression [11], principal component analysis [12], and temporal difference learning [13].", "startOffset": 230, "endOffset": 234}, {"referenceID": 12, "context": "As an example of this last point, subspace identification approaches to learning non-linear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinitedimensional) covariance operators [14, 15, 16, 17, 18].", "startOffset": 243, "endOffset": 263}, {"referenceID": 13, "context": "As an example of this last point, subspace identification approaches to learning non-linear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinitedimensional) covariance operators [14, 15, 16, 17, 18].", "startOffset": 243, "endOffset": 263}, {"referenceID": 14, "context": "As an example of this last point, subspace identification approaches to learning non-linear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinitedimensional) covariance operators [14, 15, 16, 17, 18].", "startOffset": 243, "endOffset": 263}, {"referenceID": 15, "context": "As an example of this last point, subspace identification approaches to learning non-linear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinitedimensional) covariance operators [14, 15, 16, 17, 18].", "startOffset": 243, "endOffset": 263}, {"referenceID": 16, "context": "As an example of this last point, subspace identification approaches to learning non-linear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinitedimensional) covariance operators [14, 15, 16, 17, 18].", "startOffset": 243, "endOffset": 263}, {"referenceID": 5, "context": "Kernel PCA [6] is a generalization of principal component analysis [12]: we first map our d-dimensional inputs x1, .", "startOffset": 11, "endOffset": 14}, {"referenceID": 10, "context": "Kernel PCA [6] is a generalization of principal component analysis [12]: we first map our d-dimensional inputs x1, .", "startOffset": 67, "endOffset": 71}, {"referenceID": 5, "context": "length eigenvectors of \u03a3\u0302XX are given by \u03a6Hvi\u03bb \u22121/2 i , where \u03bbi and vi are the eigenvalues and eigenvectors of HGH [6].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "Spectral algorithms for manifold learning, sometimes called kernel eigenmap methods, include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], and Maximum Variance Unfolding (MVU) [4].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "Spectral algorithms for manifold learning, sometimes called kernel eigenmap methods, include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], and Maximum Variance Unfolding (MVU) [4].", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "Spectral algorithms for manifold learning, sometimes called kernel eigenmap methods, include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], and Maximum Variance Unfolding (MVU) [4].", "startOffset": 166, "endOffset": 169}, {"referenceID": 3, "context": "Spectral algorithms for manifold learning, sometimes called kernel eigenmap methods, include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], and Maximum Variance Unfolding (MVU) [4].", "startOffset": 208, "endOffset": 211}, {"referenceID": 6, "context": "Interestingly, these algorithms can be viewed as special cases of kernel PCA where the Gram matrix G is constructed over the finite domain of the training data in a particular way [7].", "startOffset": 180, "endOffset": 183}, {"referenceID": 4, "context": ", see [5].", "startOffset": 6, "endOffset": 9}, {"referenceID": 17, "context": "A minor difference is that LE does not scale its embedding using the eigenvalues of G; the related diffusion maps algorithm [19] does.", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "This method finds a statistically consistent solution through the use of an instrumental variable [11, 12], an observation yi that is correlated with the true latent variables, but uncorrelated with the noise in xi.", "startOffset": 98, "endOffset": 106}, {"referenceID": 10, "context": "This method finds a statistically consistent solution through the use of an instrumental variable [11, 12], an observation yi that is correlated with the true latent variables, but uncorrelated with the noise in xi.", "startOffset": 98, "endOffset": 106}, {"referenceID": 10, "context": "Going beyond two-subspace PCA, there are a number of interesting spectral decompositions of crosscovariance matrices that involve transforming the variables xi and yi before applying a singular value decomposition [12].", "startOffset": 214, "endOffset": 218}, {"referenceID": 18, "context": "For example, in reduced-rank regression [20, 12], we want to estimate E [xi | yi].", "startOffset": 40, "endOffset": 48}, {"referenceID": 10, "context": "For example, in reduced-rank regression [20, 12], we want to estimate E [xi | yi].", "startOffset": 40, "endOffset": 48}, {"referenceID": 19, "context": "An SVD of the resulting doubly-whitened cross-covariance matrix (\u03a3\u0302XX + \u03b7I)\u03a3\u0302XY (\u03a3\u0302Y Y + \u03b7I) \u22121/2 is called canonical correlation analysis [21], and the resulting singular values are called canonical correlations.", "startOffset": 139, "endOffset": 143}, {"referenceID": 20, "context": "The cross-covariance operator reduces to an ordinary cross-covariance matrix in the finite-dimensional case; in the infinite-dimensional case, it can be viewed as a kernel mean map descriptor [22] for the joint distribution P[X,Y ].", "startOffset": 192, "endOffset": 196}, {"referenceID": 15, "context": "The kernel SVD algorithm previously appeared as an intermediate step in [17, 23]; here we generalize the algorithm to weighted cross covariance operators in Hilbert space and give a more complete description, both because the method is interesting in its own right, and because this generalized SVD will serve as a step in our two-manifold algorithms.", "startOffset": 72, "endOffset": 80}, {"referenceID": 21, "context": "The kernel SVD algorithm previously appeared as an intermediate step in [17, 23]; here we generalize the algorithm to weighted cross covariance operators in Hilbert space and give a more complete description, both because the method is interesting in its own right, and because this generalized SVD will serve as a step in our two-manifold algorithms.", "startOffset": 72, "endOffset": 80}, {"referenceID": 15, "context": "The remainder of the proof follows from the proof of Theorem 1 in [17] (the convergence of the empirical estimator of the kernel covariance operator).", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "(Kernel CCA is a well-known algorithm, though our formulation here is different than in [23], and kernel RRR is to our knowledge novel.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "The consistency of both approaches follows directly from the consistency of kernel SVD, so long as we let \u03b7 \u2192 0 as n\u2192\u221e [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.", "startOffset": 108, "endOffset": 132}, {"referenceID": 13, "context": "We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.", "startOffset": 108, "endOffset": 132}, {"referenceID": 14, "context": "We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.", "startOffset": 108, "endOffset": 132}, {"referenceID": 16, "context": "We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.", "startOffset": 108, "endOffset": 132}, {"referenceID": 23, "context": "We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.", "startOffset": 108, "endOffset": 132}, {"referenceID": 15, "context": "We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.", "startOffset": 108, "endOffset": 132}, {"referenceID": 15, "context": "Here, we focus on one specific example: we show how to combine HSE-HMMs [17], a powerful nonparametric approach to modeling dynamical systems, with manifold learning.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "[17] suggest finding this low-dimensional state space as a subspace of an infinite dimensional RKHS.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Other approaches such as CCA and RRR have also been successfully used for finite-dimensional spectral learning algorithms [18], suggesting that kernel SVD could be replaced by kernel CCA or kernel RRR (Section 3.", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "[17], using sequences of 150 consecutive observations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": ") Third, for comparison\u2019s sake, we trained a 20-dimensional Kalman filter using the N4SID algorithm [26] with Hankel matrices of 150 time steps; and finally, we learned a 20-state discrete HMM (with 400 levels of discretization for observations) using the EM algorithm.", "startOffset": 100, "endOffset": 104}, {"referenceID": 25, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "A similar problem to the two-manifold problem is manifold alignment [28, 29], which builds connections between two or more data sets by aligning their underlying manifolds.", "startOffset": 68, "endOffset": 76}, {"referenceID": 27, "context": "A similar problem to the two-manifold problem is manifold alignment [28, 29], which builds connections between two or more data sets by aligning their underlying manifolds.", "startOffset": 68, "endOffset": 76}, {"referenceID": 27, "context": "Generally, manifold alignment algorithms either first learn the manifolds separately and then attempt to align them based on their lowdimensional geometric properties, or they take the union of several manifolds and attempt to learn a latent space that preserves the geometry of all of them [29].", "startOffset": 291, "endOffset": 295}, {"referenceID": 28, "context": "This type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34].", "startOffset": 127, "endOffset": 139}, {"referenceID": 29, "context": "This type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34].", "startOffset": 127, "endOffset": 139}, {"referenceID": 18, "context": "This type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34].", "startOffset": 127, "endOffset": 139}, {"referenceID": 30, "context": "This type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34].", "startOffset": 181, "endOffset": 193}, {"referenceID": 31, "context": "This type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34].", "startOffset": 181, "endOffset": 193}, {"referenceID": 32, "context": "This type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34].", "startOffset": 181, "endOffset": 193}, {"referenceID": 33, "context": "Manifold kernel dimension reduction [35], finds an embedding of covariates xi using a kernel eigenmap method, and then attempts to find a linear transformation of some of the dimensions of the embedded points to predict response variables yi.", "startOffset": 36, "endOffset": 40}, {"referenceID": 34, "context": "There has also been some work on finding a mapping between manifolds [36] and learning a dynamical system on a manifold [37]; however, in both of these cases it was assumed that the manifold was known.", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "There has also been some work on finding a mapping between manifolds [36] and learning a dynamical system on a manifold [37]; however, in both of these cases it was assumed that the manifold was known.", "startOffset": 120, "endOffset": 124}, {"referenceID": 36, "context": "[38] introduces Laplacian eigenmaps that accommodate time series data by picking neighbors based on temporal ordering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[39] and Li et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[40] propose learning a piecewise linear model that approximates a non-linear manifold and then attempt to learn the dynamics in the low-dimensional space.", "startOffset": 0, "endOffset": 4}], "year": 2011, "abstractText": "Recently, there has been much interest in spectral approaches to learning manifolds\u2014 so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together and allowing information to flow between them, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias in the same way that an instrumental variable allows us to remove bias in a linear dimensionality reduction problem. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space. Finally, we discuss situations where two-manifold problems are useful, and demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.", "creator": "LaTeX with hyperref package"}}}