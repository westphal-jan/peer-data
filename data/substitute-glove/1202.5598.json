{"id": "1202.5598", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2012", "title": "Clustering using Max-norm Constrained Optimization", "abstract": "We suggest using the cooper - carlson as full ellipse preschool constraints for generalized. We success everything actually yields a go precisely consist recovery ensures than 2003 although nuclear - norm fluidity, including study came mobility own mind formulation, has other some convex hastiness, rise to other clustering approaches.", "histories": [["v1", "Sat, 25 Feb 2012 02:10:20 GMT  (3366kb,D)", "https://arxiv.org/abs/1202.5598v1", null], ["v2", "Sun, 18 Mar 2012 22:09:44 GMT  (3366kb,D)", "http://arxiv.org/abs/1202.5598v2", null], ["v3", "Wed, 11 Apr 2012 20:56:40 GMT  (3371kb,D)", "http://arxiv.org/abs/1202.5598v3", null], ["v4", "Fri, 13 Apr 2012 06:52:44 GMT  (3371kb,D)", "http://arxiv.org/abs/1202.5598v4", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ali jalali", "nathan srebro"], "accepted": true, "id": "1202.5598"}, "pdf": {"name": "1202.5598.pdf", "metadata": {"source": "CRF", "title": "Clustering using Max-norm Constrained Optimization", "authors": ["Ali Jalali", "Nathan Srebro"], "emails": ["alij@mail.utexas.edu", "nati@uchicago.edu"], "sections": [{"heading": "1 Introduction", "text": "Clustering as the problem of partitioning data into clusters with strong similarity inside the clusters and strong dissimilarity across different clusters is one of the main problems in machine learning. In this paper, we consider the problem of cut-based, or correlation, clustering [4] that has received a lot of attention recently [1, 22, 3]: Given G(V, E) on n nodes with normalized symmetric affinity matrix A (for all u, v \u2208 V: 0 \u2264 Auv \u2264 1 and Auu = 1), we want to partition V into clusters C = {C1, . . . , Ck} so as to minimize the total disagreement\nD(C) = k\u2211 i=1 \u2211 u,v\u2208Ci (1\u2212Auv) + k\u2211 i 6=j=1 \u2211 u\u2208Ci,v\u2208Cj Auv.\nThe first term, captures the internal disagreement inside clusters, and the second term captures the external agreement between nodes in different clusters. In an ideal cluster, the affinities between all members of the same cluster are 1 and the affinities between members of two different clusters are zero and hence the objective is zero. This objective does not require the number of clusters to be known ahead of time\u2014we may decide to use any number of clusters, and this is accounted for in the objective. Unfortunately, finding a clustering minimizing the disagreement D(C) is NP-Hard [4].\nWe formulate this problem as an optimization of a convex disagreement objective over a non-convex set of valid clustering matrices (Section 2) and then consider convex relaxations of this constraint. Recently, Jalali et al. [16] suggested a trace-norm (aka nuclear-norm) relaxation, casting the problem as minimizing an `1 loss and a trace-norm penalty, and providing conditions under which the true underlying clustering is recovered. Instead of trace-norm, we propose using the max-norm (aka \u03b32 : `1 \u2192 `\u221e norm) [30], which is a tighter convex relaxation than the trace-norm. Accordingly, we establish an exact recovery guarantee for our max-norm based formulation that is strictly better then the trace-norm based guarantee. We show that if the affinity matrix is a corruption of an \u201cideal\u201d clustering matrix, with a certain bound on the corruption, then the optimal solution of the max-norm bounded optimization problem is exactly the ideal clustering (Section 3.1). We also discuss even tighter convex relaxations related to the max-norm, and suggest augmenting the convex relaxation with a single-linkage post-processing step in case of non-exact recovery, showing the empirical advantages of these approaches (Section 5).\nThe approach we suggests relies on optimizing an `1 objective subject to a max-norm constraint. A similar optimization problem with a trace-norm constraint (or trace-norm regularization) has recently been the subject of some interest in the context of \u201crobust PCA\u201d [8, 33] and recovering the structure of graphical models with latent variables [10]. As with the trace-norm regularized variant, the `1 + max-norm problem can be formulated as an SDP and solved using standard solvers, but this is only applicable to fairly small scale problems. In Section 4, we discuss various optimization approaches to this problems, including approaches which preserve the sparsity of the solution.\nar X\niv :1\n20 2.\n55 98\nv4 [\ncs .L\nG ]\n1 3\nA pr\n2 01\n2"}, {"heading": "1.1 Relationship to the Goemans Willimason SDP Relaxation", "text": "Our convex relaxation approach is related to the classic SDP relaxations of max-cut [13] and more generally the cut-norm [2]. In fact, if we are interested in a partition to exactly two clusters, the correlation clustering problem is essentially a max-cut problem, though with both positive and negative weights (i.e. a symmetric cut-norm problem), and our relaxation is essentially the classic SDP relaxation of these problems. Our approach and results differ in several ways.\nFirst, we deal with problems with multiple clusters, and even when the number of clusters is not predetermined. If the number of clusters k is pre-determined, the correlation clustering problem can be written as an integer quadratic program, with a k variables per node, and can be relaxed to an SDP. But this SDP will be very different from ours, and will involve a matrix of size nk \u00d7 nk, unlike our relaxation where the matrix is of size n\u00d7 n regardless of the number of clusters. Consequently, the rounding techniques based on (random) projections typically employed for classic SDP relaxations do not seem relevant here. Instead, we employ a single-linkage post-processing as a form of \u201crounding\u201d imperfect solutions.\nSecond, the type of guarantees we provide are very different from those in the Theory of Computation literature. Most of the SDP relaxation work we are aware of (including the classical work cited above) focuses on worst case constant factor approximation guarantees. On one hand, this means the guarantee needs to hold even on \u201ccrazy\u201d inputs where there is really no reasonable clustering anyway, and second, and on the other hand it is not clear how approximating the objective to within a constant factor translates to recovering an underlying clustering. Instead, we prove that when the affinity matrix is close enough to following some underlying \u201ctrue\u201d clustering, the true clustering will be recovered exactly. This type of guarantee is more in the spirit of compressed sensing, which where exact recovery of a support set is guaranteed subject to conditions on the input [16]."}, {"heading": "1.2 Other Clustering Approaches", "text": "There are several classes of clustering algorithms with different objectives. In hierarchical clustering algorithms such as UPGMA [28], SLINK [27] and CLINK [11] the goal is to generate a sequence of clusterings by produce a sequence of clustering by merging/splitting two clusters at each step of the sequence according to a local disagreement objective as opposed to our global D(C). Because of this locality, these methods are known to be very sensetive to outliers.\nCut-based clustering algorithms such as k-means/medians [31, 15], ratio association [26], ratio cut [9] and normalized cut [34] try to optimize an objective function globally. The main issue with these objectives is that they are typically NP-Hard and need to know the number of clusters ahead of time, since these objectives are monotone in the number of clusters.\nIn contrast, spectral clustering algorithms[32] try to find the first k principal component of the affinity matrix or a transformed version of that [24]. These methods require the number of clusters in advance and has been shown to be tractable (convex) relaxations to NP-Hard cut-based algorithms [12]. These methods are again very sensitive to outliers as they might change the principal components dramatically."}, {"heading": "2 Problem Setup", "text": "Our approach is based on representing a clustering C through its incidence matrix K(C) \u2208 Rn\u00d7n where Kuv = 1 iff u and v belong to the same cluster in C (i.e. u, v \u2208 Ci for some i), and Kuv = 0 otherwise (i.e. if u and v belong to different clusters). The matrix K(C) is thus a permuted block-diagonal matrix, and can also be thought of as the edge incidence matrix of a graph with cliques corresponding to clusters in C. We will say that a matrix K is a valid clustering matrix, or sometimes simply valid, if it can be written as K = K(C) for some clustering C (i.e. if it is a permuted block diagonal matrix, with 1s in the diagonal blocks).\nThe disagreement can then be written as either: D(C) = \u2016A\u2212K(C)\u20161 = \u2211 u,v |Auv \u2212K(C)uv| (1)\nor as: D(C) = \u2211 u,v K(C)uv(1\u2212 2Auv) + \u2211 uv Auv , (2)\nwhere the term \u2211 uv Auv does not depend on the clustering C and can thus be dropped.\nWe now phrase the correlation clustering problem as matrix problem, where we would like to solve\nmin K D(K) s.t. K is a valid clustering matrix. (3)\nThe problem is that even though the objectives (1) and (2) are convex, the constraint that K is valid is certainly not constraint. Our approach to correlation clustering will thus be to relax this non-convex constraint (the validity of K) to a convex constraint.\nWe note that although both the absolute error objective (1) and the linear objective (2) agree on valid clustering matrices (or more generally, on binary matrices K), they can differ when K is fractional, and especially when A is also fractional. The choice of objective can thus be important when relaxing the validity constraint to a convex constraint. More specifically, as long as A is binary (i.e. Auv \u2208 {0, 1}), and 0 \u2264 Kuv \u2264 1, even if K is fractional, the two objectives agree. Non-negativity of Kuv is ensured in some, but not all, of the convex relaxations we study. When non-negativity is not ensured, the absolute error objective (1) would tend to avoid negative values, but the linear objective might certainly prefer them. More importantly, once the affinities Auv are also fractional, the two objectives differ even for 0 \u2264 Kuv \u2264 1. While the linear objective would tend to not care much about entries with affinities close to 1/2, the absolute error objective would tend to encourage fractional values in thees cases.\nThe linear objective also has some optimization advantages over the absolute function as well. From a numerical optimization point of view, dealing with the linear objective function is easier since we do not need to compute the sub-gradients of the `1-norm."}, {"heading": "3 Max-Norm Relaxation", "text": "As discussed in the previous Section, we are interested in optimizing over the non-convex set of valid clustering matrices. The approach we discuss here is to relaxing this set to the set of matrices with bounded max-norm [30]. The max-norm of a matrix K is defined as\n\u2016K\u2016max = min K=RLT \u2016R\u2016\u221e,2\u2016L\u2016\u221e,2\nwhere, \u2016 \u00b7 \u2016\u221e,2 is the maximum of the `2 norm of the rows, and the minimization is over factorization of any internal dimensionality. It is not hard to see that if K is a valid clustering matrix, with K = K(C), then \u2016K\u2016max = 1. This is achieved, e.g., by a factorization with R = L, and where each row Ru of R is a (unit norm) indicator vector with Rui = 1 for u \u2208 Ci and zero elsewhere.\nRelaxing the validity constraint to a max-norm constraint, and using the absolute error objective, we obtain the following convex relaxation of the correlation clustering problem:\nK\u0302 = arg min K \u2016A\u2212K\u20161 s.t. \u2016K\u2016max \u2264 1. (4)\nAlternatively, we could have used the linear objective (2) instead. In any case, after finding K\u0302, it is easy to check whether it is valid, and if so recover the clustering from its block structure. If K\u0302 is valid, we are assured the corresponding clustering is a globally optimal solution of the correlation clustering problem."}, {"heading": "3.1 Theoretical Guarantee", "text": "Assuming there exists an underlying true clustering, we provide a worst-case (deterministic) guarantee for exact recovery of that clustering in the presence of noise when the affinity matrix A is a binary 0\u2212 1 matrix using absolute objective. The flavor of our result is similar to [16] for trace-norm, except that we show the max-norm constraint problem recovers the underlying clustering with larger noise comparing to tracenorm constraint. This matches our intuition that max-norm is a tighter relaxation than trace-norm for valid clustering matrices.\nTo present our theoretical result, we start by introducing an important quantity that our main result is based upon. Suppose C\u2217 = {C\u22171 , . . . , C\u2217k} is the underlying true clustering. For a node u and a cluster C\u2217i , let du,C\u2217i = \u2211 v\u2208C\u2217 i Au,v |C\u2217i | if u /\u2208 C\u2217i and du,C\u2217i = 1\u2212 \u2211 v\u2208C\u2217 i Au,v |C\u2217i | otherwise and\nDmax(A,K) \u2261 Dmax(A,K(C\u2217)) = max u,i du,C\u2217i\nFigure 1: Theorem 1 guarantee region of the noise level Dmax vs the unbalanceness parameter\n1 k\u2217\n\u2211\ni\n(\n|C\u2217i | |Cmin|\n)2\n.\nbe the maximum of the disagreement ratios on the adjacency matrix. This definition is inspired by [16] but is slightly different. Notice that the larger Dmax(A,K) is, the more noisy (comparing to ideal clusters) the graph is; and hence, the harder the clustering becomes. In particular for ideal clusters (fully connected inside and fully disconnected outside clusters), we have Dmax(A,K) = 0.\nWe would like to ensure that when Dmax(A,K) is small enough, our method can recover K. The following lemma helps us understand the information theoretic limit of Dmax(A,K), i.e. what value of Dmax is certainly not enough to ensure recovery, even information theoretically: Lemma 1. For any clustering C = {C1, . . . , Ck} and for all \u03b3 > 25+r with r = n2\u2211\ni |Ci|2 , there exists an\naffinity matrix A such that Dmax(A,K(C)) = \u03b3 and the combinatorial program (3) does not output C.\nNote that the minimum of 25+r is attained when all clusters have equal sizes. If we have k \u2217 clusters of\nsize nk\u2217 , then r = k \u2217 and the bound in Lemma 1 asserts that if Dmax(A,K) > 2 k\u2217+5 , then there are examples for which the original clustering cannot be recovered by the combinatorial program (3). This implies that Dmax(A,K) cannot be scaled better than \u0398( 1 k\u2217 ) in general even without convex relaxation.\nSuppose there exist a true underlying clustering C\u2217 with k\u2217 clusters. Let Cmin be the smallest size underlying true cluster and we are given an affinity matrix A with Dmax = Dmax(A,K(C\u2217)). Introducing lagrange multiplier \u00b5, we consider the optimization problem\nK\u0302\u00b5 = arg min K 1\u2212 \u00b5 n2 \u2016A\u2212K\u20161 + \u00b5 \u2016K\u2016max. (5)\nThe following theorem characterizes the noise regime under which the simple max-norm relaxation (5) recovers C\u2217.\nTheorem 1. For binary 0 \u2212 1 matrix A, if Dmax < 1k\u2217+1 is small enough to satisfy 1 k\u2217 \u2211 i ( |C\u2217i | |Cmin| )2 \u2264\n(1\u22123Dmax)2 (1+Dmax)Dmax then, for any \u00b50 satisfying (1+Dmax) (1\u22123Dmax)|Cmin|2 < (1\u2212\u00b50)k\u2217 \u00b50n2 < (1\u22123Dmax)k \u2217 Dmax \u2211 i |C\u2217i |2 , the matrix K\u0302\u00b50 (the solution to (5)) is unique and equal to the matrix K\u2217 = K(C\u2217) (the solution to (3)). Remark 1: Consider the parameter 1k\u2217 \u2211 i ( |C\u2217i | |Cmin| )2 in the theorem. Notice that for a balanced underlying clustering (k\u2217 clusters of size n/k\u2217), this parameter is 1 and as the underlying clustering gets more and more unbalanced, this parameter increases. That motivates to call it unbalanceness of the clustering. It is clear that as unbalanceness parameter increases, the region of Dmax for which our theorem guarantees the clustering recovery shrinks. We plot the admissible region of Dmax due to unabalanceness in Fig 1. Remark 2: According to the Lemma 1, the bound on Dmax is order-wise tight and can be only improved by a constant in general."}, {"heading": "3.2 Comparison to Single-Linkage Algorithm", "text": "Considering single-linkage algorithm (SLINK) [27] as a baseline for clustering, we compare the power our algorithm in cluster recovery with that. SLINK generates a hierarchy of clusterings starting with each node\nas a cluster. At each iteration, SLINK measures the similarity of all pairs of clusters and combines the most similar pair of clusters to a new cluster. We consider the closedness of the columns Ai and Aj as the similarity measure of nodes i and j.\nConsider the graph shown in Fig. 2. With exhaustive search, one can show that the non-convex problem (3) outputs two clusters as shown. Running SLINK on this graph, the algorithm first finds two cliques of size 17 and nodes A and B as four separate clusters in the hierarchy. Next, it combines nodes A and B as a separate cluster since they are more similar to each other than to their own clusters. This means that single linkage algorithm will never find the correct clustering. However, it can be easily checked that our proposed max-norm constrained algorithm will recover the solution of (3)."}, {"heading": "3.3 Comparison to Trace-Norm Constrained Clustering", "text": "Since the max-norm constraint is strictly a tighter relaxation to the trace-norm constraint, we expect the max-norm algorithm to perform better. Our theorem shows improvement over the guarantees provided for trace-norm clustering. Comparing to the result of [16] on trace-norm (Dmax \u2264 |Cmin|4n ), the max-norm tolerates more noise. To see this, consider a balanced clustering, then trace-norm requires Dmax \u2264 14k\u2217 and max-norm requires Dmax \u2264 min( 1k\u2217+1 , 0.1789) which is larger than 1 4k\u2217 for all k\n\u2217. The difference gets more clear for unbalanced clustering. Suppose we have one small cluster of constant size |Cmin| and other clusters are approximately of size nk\u2217 . As (n, k \u2217) scales, trace-norm guarantee requires that Dmax = o( 1 n ) which is inverse proportional to the size of the smallest cluster, whereas, max-norm guarantee requires Dmax = o( k\u2217\nn ) which is inverse proportional to the size of the largest cluster. This is a huge theoretical advantage in our theorem.\nBesides comparing the provided guarantees, we compare max-norm clustering with trace-norm clustering both deterministically and probabilistically. Running Trace-Norm constrained minimization [16] on the graph shown in Fig. 2, the resulting clustering consists of two clusters and node B belongs to the correct cluster. However, node A belongs to both clusters! \u2013 The clustering matrix contains two blocks of ones and the row/column corresponding to the node A contain all ones. Also, the diagonal entry corresponding to node A is larger than one and the diagonal entry corresponding to the node B is less than one. In short, this algorithm is confused as of which cluster the node A belongs to.\nFurther, we compare our algorithm with trace-norm algorithm [16] and SLINK on a probabilistic setup. Start from two different ideal clusters on 100 nodes: a) Balanced clusters: four ideal clusters of size 25, b) Unbalanced clusters: three ideal clusters of size 30 and one ideal cluster of size 10. Then, gradually increase Dmax on both graphs and run all algorithms and report the probability of success in exact recovery of the underlying clusters. Although our theoretical guarantee is for binary affinity matrices, here, we run the same experiment for fractional affinity matrix. We run all experiments for both absolute and linear objectives.\nFig. 3.1 shows that in all cases max-norm outperforms the trace-norm and the improvement is more significant for unbalanced clustering with fractional affinity matrix. Moreover, this experiments reveal that the absolute objective has slight advantage if the affinity matrix is binary and clusters are balanced; otherwise, the linear objective is better.\n4 Max-norm + `1-norm Optimization\nIn this Section we consider optimization problems of the form (4). This problem recovers a sparse and low-rank matrix from their sum, considering max-norm as a proxy to rank. In Section 4.1, we discuss how (4) can be formulated as an SDP, allowing us to easily solve it using standard SDP solvers, as long as the problem size is relatively small. We then propose three other methods to numerically solve the optimization problem (4)."}, {"heading": "4.1 Semi-Definite Programming Method", "text": "Following Srebro et al. [30], we introduce dummy variables L,R \u2208 Rn\u00d7n and reformulate (4) as the following SDP problem\nK\u0302 = arg min K,L,R\n\u2016A\u2212K\u20161\ns.t. [ L K KT R ] 0 and Lii, Rii \u2264 1\nThese constraints are equivalent to the condition \u2016K\u2016max \u2264 1. This SDP can be solved using generic SDP solvers, though is very slow and is not scalable to large problems."}, {"heading": "4.2 Factorization Method", "text": "Motivated by Lee et al. [20], we introduce dummy variables L,R \u2208 Rn\u00d7n and let K = LRT . With this change of variable, we can reformulate (4) as\nK\u0302 = L\u0302R\u0302T = arg min L,R \u2016A\u2212 LRT \u20161\ns.t. \u2016L\u2016\u221e,2, \u2016R\u2016\u221e,2 \u2264 1.\nThis problem is not convex, but it is guaranteed to have no local minima for large enough size of the problem [7]. Furthermore, if we now the optimal solution K\u0302 has rank at most r, we can take L,R to be Rn\u00d7(r+1). In practice, we truncate to some reasonably high rank r even without a known gurantee on the rank of the optimal solution. To solve this problem iteratively, Lee et al. [20] suggest the following update[\nL R ] k+1 = Pmax ([ L R ] k + \u03c4\u221a k [ Sign(A\u2212 LRT ) R Sign(A\u2212 LRT )T L ] k ) .\nThe projection Pmax(\u00b7) operates on rows of L and R; if `2-norm of a row is less than one, it remains unchanged, otherwise it will be rescaled so that the `2-norm becomes one.\nA possible problem with the above formulation is the lack of \u201csparsity\u201d in the following sense: The `1 objective is likely to yield and optimal solution K\u2217 with many non-zeros in A\u2212K\u2217, i.e. where K\u2217 is exactly equal to A on some of the entries. However, gradient steps on the factorization are not likely to end up in exactly sparse solutions, and we are not likely to see any such sparsity in solutions obtained by the above method."}, {"heading": "4.3 Loss Function Method", "text": "There are gradient methods such as truncated gradient [18] that produce sparse solution, however, these methods cannot be applied to this problem. We introduce a surrogate optimization problem to (4) by adding a loss function. For some large \u03bb \u2208 R, solve\nK\u0302 = A\u2212 Z\u0302 = arg min Z,L,R \u2016Z\u20161 + \u03bb\u2016A\u2212 Z \u2212 LRT \u201622\ns.t. \u2016L\u2016\u221e,2, \u2016R\u2016\u221e,2 \u2264 1.\nHere, the matrix Z is sparse and includes the disagreements. For sufficiently large values of \u03bb, the loss function ensures that the matrix A\u2212 Z is close to the matrix LRT that is a bounded max-norm matrix. To solve this problem iteratively, we use the following update\nZk+1 = P`1 ( Zk +\n\u03c4\u03bb\u221a k\n(A\u2212 Z \u2212 LRT )k )\n[ L R ] k+1 = Pmax ([ L R ] k + \u03c4\u03bb\u221a k [ (A\u2212 Z \u2212 LRT ) R (A\u2212 Z \u2212 LRT )T L ] k ) .\nHere, P`1(\u00b7) operates on entries; if an entry has the same sign before and after the update, it remains unchanged; otherwise, it will be set to zero. Solving directly for large values of \u03bb might cause some problems due to the finite numerical precision. In practice, we start with some small value say \u03bb = 1 and double the value of \u03bb after some iterations. This way, we gradually put more and more emphasis on the loss function as we get closer to the optimal point."}, {"heading": "4.4 Dual Decomposition Method", "text": "Inspired by Rockafellar [25], we first reformulate (4) by introducing a dummy variable Z \u2208 Rn\u00d7n as follows\nK\u0302 = arg min Z,K\n\u2016A\u2212K\u20161\ns.t. \u2016Z\u2016max \u2264 1 and Z = K.\nThen, introducing a Lagrange multiplier \u039b \u2208 Rn\u00d7n, we propose the following equivalent problem:\nK\u0302 = arg max \u039b min Z,K\n\u2016A\u2212K\u20161 + \u3008\u3008\u039b, K \u2212 Z\u3009\u3009\ns.t. \u2016Z\u2016max \u2264 1.\nHere, \u3008\u3008\u00b7, \u00b7\u3009\u3009 is the trace of the product. This problem is a saddle-point convex problem in (Z,K,\u039b). To solve this, we iteratively fix \u039b and optimize over (K,Z) and then, using those optimal values of (K,Z), update \u039b.\nFor a fixed \u039b, the problem can be separated into two optimization problems over K and Z as\nK\u0302(\u039b) = arg min K \u2016A\u2212K\u20161 + \u3008\u3008\u039b, K\u3009\u3009\nwhich can be solved using factorization method discussed above, and\nZ\u0302(\u03bb) = arg min Z \u2212\u3008\u3008\u039b, Z\u3009\u3009\ns.t. \u2016Z\u2016max \u2264 1.\nwhich is a soft thresholding; if |\u039bij | > 1 then, K\u0302(\u039b)ij = \u2212Sign(\u039bij); otherwise K\u0302(\u039b)ij = Aij . Using K\u0302(\u039bk) and Z\u0302(\u039bk), we update \u039b as follows\n\u039bk+1 = \u039bk \u2212 \u03c4\u221a k (K\u0302(\u039bk)\u2212 Z\u0302(\u039bk))\nuntil it converges. One criterion for the convergence of this method is to round both matrices K\u0302, Z\u0302 and check if they are equal. To use this criterion, we need to initialize the two matrices very differently to avoid the stopping due to the initialization."}, {"heading": "4.5 Numerical Comparison", "text": "We compare the performance of these methods. For three ideal clusters of size 20 with noise level Dmax, we run all three algorithms for 2000 iterations. We consider an initial step size \u03c4 = 1 for all methods, and, for the loss function method, we doubel \u03bb every 100 iterations. For the dual method, we update \u039b for 20 times and run 100 iterations of the factorization method for the max-norm sub-problem at each update. We report the sparsity of the solution A\u2212 K\u0302 as well as the `1-norm of the error \u2016K\u0302\u2212K\u2217\u20161 for each algorithm in Fig 4. This result shows that there is a trade-off between sparsity and the error \u2013 the dual optimization method provides consistently a sparse solution, where, factorization and loss function methods provide small error. The sparsity of loss function method gets worse as the noise increases."}, {"heading": "5 Tighter Relaxations", "text": "In this section, we improve our basic algorithm in two ways: first, we use a tighter relaxation for valid clustering constraint and second, we add a single-linkage step after we recovered the clustering matrix. Although max-norm is a tighter relaxation comparing to trace-norm, we would like to go further and introduce tighter relaxations. Figure 5 summarizes different possible relaxations based on max-norm. The arrows in this figure indicated the strict subset relations among these relaxations. The tightest relaxation we suggest is {K = RRT : \u2016R\u2016\u221e,2 \u2264 1, R \u2265 0} based on the intuition that a clustering matrix is symmetric and has a\ntrivial factorization R \u2208 Rn\u00d7k, where, Rij is non-zero if node i belongs to cluster j. Next lemma formalizes this result.\nLemma 2. All relaxation sets shown in Fig. 5 are convex and the strict subset relations hold.\nThis suggests using the tightest convex relaxation, that is constraining to K such that there exists R >= 0, \u2016R\u2016\u221e,2 <= 1 with K = RRT (the set of matrices K with a factorization K = RRT , R >= 0 is called the set of completely positive matrices and is convex [5]). We optimize over this relaxation by solving the following optimization problem over R:\nR\u0302 = arg min R \u2016A\u2212RRT \u20161\ns.t. \u2016R\u2016\u221e,2 \u2264 1 & R \u2265 0. (6)\nand setting K\u0302 = R\u0302R\u0302T . Although the constraint on K\u0302 is convex, the optimization problem (6) is not convex in R."}, {"heading": "5.1 Single-linkage Post Processing", "text": "The matrix K\u0303 extracted from (6) might diverge from a valid clustering matrix in two ways: firstly, it might not have the structure of a valid clustering and secondly, even if it has the structure, the values might not be integer. We run SLINK on K\u0303 as a \u201crounding scheme\u201d to fix both of the above problems. SLINK gives a sequence of clusterings C1, . . . , Cn. To pick the best clustering, we choose\nK\u0302 = arg min i \u2016A\u2212K(Ci)\u20161. (7)\nThe matrix K\u0303 can be viewed as a refined version of the affinity matrix A and hence the second step of the algorithm can be replaced by other hierarchical clustering algorithms. The criterion of choosing the best clustering in the hierarchy comes naturally from the correlation clustering formulation."}, {"heading": "5.2 Comparison with Other Algorithms", "text": "We compare our enhanced algorithm with the trace-norm algorithm [16] followed by SLINK and SLINK itself. In all cases we pick a clustering from SLINK hierarchy using (7). The setup is identical to the experiment\nexplained in Section 3.3. Fig 3.1 summarizes the results and shows that our enhanced algorithm outperforms all competitive methods significantly.\nBesides the exact recovery of the underlying clustering, we would like to investigate that as noise level Dmax increases, how bad the output of our algorithm get. Using \u201cvariation of information\u201d [23] as a distance measure for clusterings, we compare our algorithm with linear objective with trace-norm counterpart, SLINK and spectral clustering[32] for both balanced and unbalanced clusterings described before. For the spectral clustering method, we first find the largest k = 4 principal components of A and then, run SLINK on principal components. Fig 5 shows the result indicating that max-norm, even when the noise level is high and no method can recover the exact clustering, outputs a clustering that is not far from the true underlying clustering in our metric."}, {"heading": "5.3 MNIST Dataset", "text": "To demonstrate our method in a realistic and larger scale data set, we run our enhanced algorithm, trace-norm and spectral clustering on MNIST Dataset [19]. For each experiment, we pick a total of n data points from 10 different classes (n/10 from each class) and construct the affinities using Gaussian kernel as explained in [6]. We report the time complexities and clustering errors as previous experiment in Fig 5.2. For the spectral clustering, we take SVD using Matlab and pick the top 10 principal components followed by k-means."}, {"heading": "A Proof of Lemma 2", "text": "Provided equivalences (1) and (2), it is clear that {K = LRT : \u2016L\u2016\u221e,2 \u2264 1, \u2016R\u2016\u221e,2 \u2264 1} and {K = RRT : \u2016R\u2016\u221e,2 \u2264 1} are both convex sets. Since {K = RRT : \u2016R\u2016\u221e,2 \u2264 1, R \u2265 0} is the intersection of two sets {K = RRT : \u2016R\u2016\u221e,2 \u2264 1} and CP{K = RRT : R \u2265 0}, it suffices to show that CP is a convex set. The set CP is called the set of completely positive matrices and has been shown to be a closed convex cone (see Theorem 2.2 in [5] for details).\nFor the proof of equivalence (1) see Lemma 15 in [29]. To prove equivalence (2), it is clear that {K = RRT : \u2016R\u2016\u221e,2 \u2264 1} \u2286 {K : \u2016K\u2016max \u2264 1,K 0}. Now, suppose K0 \u2208 {K : \u2016K\u2016max \u2264 1,K 0}; let R0 = \u221a K0 and in contrary, assume that \u2016R0\u2016\u221e,2 > 1. This implies that at least one element on the diagonal of K0 exceeds 1 and hence \u2016K0\u2016max > 1. This is a contradiction and hence the equivalence (2) follows. To show the relation (3), it suffices to show that the sub-set relation is strict, since the sub-set relation itself is trivial. By counter-example provided in [14], the sub-set relation is strict (i.e., there is a positive semi-definite and positive entry K0 that does not belong to CP)."}, {"heading": "B Proof of Lemma 1", "text": "We construct an example with Dmax = 2\nn2\u2211 i |Ci|2 +5 that cannot be recovered. Consider the clustering shown\nin Fig. 8(a). It is clear that for this clustering, we have Dmax = \u03b3 and\nB(C1) = \u03b32 k\u2211 i=1 |Ci|2 + \u03b32 2 k\u2211 i=1 |Ci|(n\u2212 |Ci|).\nNow, consider the alternative clustering shown in Fig. 8(b). For this alternative clustering, we have\nB(C2) = \u03b3(1\u2212 2\u03b3) k\u2211 i=1 |Ci|2.\nIt is clear that B(C2) < B(C1) (the alternative is a better clustering) for \u03b3 > 2n2\u2211 i |Ci|2 +5 ."}, {"heading": "C Proof of Theorem 1", "text": "The proof has two main steps; in the first step, we characterize a sufficient optimality condition set based on the existence of a dual variable and in the second step, we construct such dual variable. For the sake of the\nproof, we consider a useful equivalent definition [21] of the max norm as\n\u2016K\u2016max = max X:\u2016X\u20162\u22641 \u2016K \u25e6X\u20162 (8)\nwhere, \u2016\u00b7\u20162 is the spectral norm (maximum eigenvalue) of the matrix and \u201c \u25e6 \u201d is the Hadamard element-wise product.\nC.1 Notation\nIn this section, we introduce our notation and definitions used throughout the paper.\nC.1.1 Residual Matrix Notations\nIn general, we do not expect the residual matrix B\u2217 = A\u2212K\u2217 to be sparse unless we threshold the affinity matrix (or we have adjacency matrix). However, to provide a guarantee, we need to characterize the subgradient of the `1-norm and hence distinguish between zeros and non-zeros of B \u2217. Let\n\u2126 = {B \u2208 Rn\u00d7n : B = BT ,Supp(B) \u2286 Supp(B\u2217)}, (9)\nwhere, Supp(\u00b7) is the index set of non-zero entries. The orthogonal projection of a matrix M to this space is defined to be a matrix of the same size with P\u2126(M)ij = Mij if (i, j) \u2208 Supp(B\u2217) and zero otherwise. The orthogonal complement of this space is denoted by \u2126\u22a5 and the projection is defined as P\u2126\u22a5(M) = M\u2212P\u2126(M).\nC.1.2 Clustering Matrix Notations\nLet U \u2208 Rn\u00d7k\u2217 be constructed as\nU =  1\u221a |C1| 1|C1| 1\u221a |C2| 1|C2|\n\u00b7 1\u221a |Ck\u2217 | 1|Ck\u2217 |\n . (10)\nDefine T = {UXT + Y UT : X,Y \u2208 Rn\u00d7k\u2217} to be the space of matrices sharing either row or column space with U . The orthogonal projection to this space can be defined as\nPT (M) = UUTM +MUUT \u2212 UUTMUUT ,\nwhere,\nUUT =  1 |C1|1|C1|\u00d7|C1| 1 |C2|1|C2|\u00d7|C2|\n\u00b7 1 |Ck\u2217 | 1|Ck\u2217 |\u00d7|Ck\u2217 |  . Denote the orthogonal complement of the space T by T \u22a5 equipped with projection PT \u22a5(M) = M \u2212PT (M). Let \u03b1 = 2Dmax be the contraction between the ideal clusters and disagreements (See Lemma 5 for more details on this definition). Under the assumption of the theorem, we have \u03b1 < 1 and hence, T \u2229 \u2126 = {0}.\nUsing definitions in (11), let X\u2217 = W (Z\u2217) + V (UUT ),\nwhere,\nZ\u2217 = P\u2126  1 |C1|1|C1|\u00d7|C1|\n1\u221a |C1| |C2| 1|C1|\u00d7|C2| \u00b7 1\u221a|C1| |Ck\u2217 | 1|C1|\u00d7|Ck\u2217 |\n1\u221a |C2| |C1| 1|C2|\u00d7|C1| 1 |C2|1|C2|\u00d7|C2| \u00b7\n1\u221a |C2| |Ck\u2217 | 1|C2|\u00d7|Ck\u2217 |\n\u00b7 \u00b7 \u00b7 \u00b7 1\u221a\n|Ck\u2217 | |C1| 1|Ck\u2217 |\u00d7|C1| 1\u221a |Ck\u2217 | |C2| 1|Ck\u2217 |\u00d7|C2| \u00b7 1 |Ck\u2217 | 1|Ck\u2217 |\u00d7|Ck\u2217 |\n .\nNotice that PT (X\u2217) = UUT and hence X\u2217 \u2212 UUT \u2208 T \u22a5. If we show that X\u2217 \u2212 UUT has spectral norm less than 1, then it is immediate that X\u2217 \u2208 arg maxX:\u2016X\u20162\u22641 \u2016K\u2217 \u25e6 X\u20162. Also, we have an eigenvalue decomposition K\u2217 \u25e6 X\u2217 = [U V ]\u03a3[U V ]T , where, U is as defined above and contains the eigenvector(s) corresponding to the maximum magnitude eigenvalue +1 (with k\u2217 repetitions). To bound the spectral norm of X\u2217 \u2212 UUT , consider\n\u2016X\u2217 \u2212 UUT \u20162 = \u2016W (P\u2126(Z\u2217 \u2212 UUT ))\u20162\n\u2264 Dmax 1\u2212 \u03b1 (k\u2217 \u2212 1) < 1.\nThe first inequality follows from Lemma 6. We make assumptions so that the last inequality holds.\nWe use the variational form (8) to characterize the sub-gradient of the max-norm at the point K\u2217.\nLemma 3. For a matrix M \u2208 Rn\u00d7n, we have M \u2208 \u2202\u2016K\u2217\u2016max if M = (USUT +W ) \u25e6X\u2217, for some diagonal positive semi-definite matrix S \u2208 Rr\u00d7r with Trace (S) = 1 and for some matrix W \u2208 Rn\u00d7n with PT (W ) = 0 and \u2016W\u2016\u2217 < 1.\nProof. Using the variational form (8) and theorem 4.4.2 in [17] on the sub-gradient of the maximum of convex functions, we have\n\u2202\u2016K\u2217 \u25e6X\u2217\u20162 \u2286 \u2202\u2016K\u2217\u2016max.\nThus, it suffices to show that M \u2208 \u2202\u2016K\u2217 \u25e6X\u2217\u20162 (which is the case).\nC.2 Sufficient Optimality Conditions\nWe provide similar optimality conditions to those provided in `1 plus trace norm minimization in the literature. The main difference here is the existence of the auxiliary variable X\u2217 in the conditions. The following lemma characterizes a sufficient optimality condition set.\nLemma 4 (Sufficient Optimality Condition.). K\u2217 = K\u0302\u00b5 (Problem (3)\u2261 Problem (5)), if T \u2229 \u2126 = {0} and there exists a dual matrix Q such that\n(a) P\u2126(Q \u25e6X\u2217) = \u2212 1\u2212\u00b5n2 Sign(A\u2212K \u2217)\n(b) \u2016P\u2126\u22a5(Q \u25e6X\u2217)\u2016\u221e < 1\u2212\u00b5 n2\n(c) PT (Q) = USUT , for some diagonal matrix S 0 with Trace(S) = \u00b5.\n(d) \u2016PT \u22a5(Q)\u2016\u2217 < \u00b5.\nProof. Notice that since X\u2217 by construction has no zero entry (except for the very corner case where there are only two clusters both of size 2), the matrix Q \u25e6X\u2217 can take any value/sign on each entry by choosing the values of Q properly. Under these conditions, Q \u25e6X\u2217 \u2208 \u2202\u2016A\u2212K\u2217\u20161 and also Q \u25e6X\u2217 \u2208 \u2202\u2016K\u2217\u2016max and the result follows from the standard first order optimality argument and zero duality gap of both `1 and max norms.\nC.3 Dual Variable Construction\nFirst notice that under the assumption of the theorem, we have \u03b1 < 1 and hence, by Lemma 5, we have T \u2229 \u2126 = {0} and also \u00b5 = \u00b50 is feasible. Second, we construct Q by using alternating projections. Consider the infinite sums\nW (M) = M \u2212 PT (M) + P\u2126(PT (M))\u2212 PT (P\u2126(PT (M))) + . . . V (N) = N \u2212 P\u2126(N) + PT (P\u2126(N)) \u2212 P\u2126(PT (P\u2126(N))) + . . .\n(11)\nBy the proof of the Lemma 5, these sums converge geometrically with parameter \u03b1 (See Lemma 5 in [16] for the proof). Denoting element-wise division with \u201c/\u201d (and 00 = 0), let\nQ = \u22121\u2212 \u00b5 n2 W (Sign(A\u2212K\u2217)/X\u2217) + \u00b5 k\u2217 V (UUT ).\nIt is easy to check that conditions (a) and (c) in lemma 4 are both satisfied for S = 1k\u2217 I. To show condition (b), first notice that \u2016P\u2126\u22a5PT P\u2126(M)\u2016\u221e \u2264 Dmax\u2016P\u2126\u22a5(M)\u2016\u221e and hence, we have\n\u2016P\u2126\u22a5(Q \u25e6X\u2217)\u2016\u221e \u2264 1\n(1\u2212Dmax)2 \u2225\u2225\u2225\u2225P\u2126\u22a5 (1\u2212 \u00b5n2 PT (Sign(A\u2212K\u2217)/X\u2217) + \u00b5k\u2217UUT ) \u25e6 P\u2126\u22a5 ( UUT \u2212 PT (Z\u2217) )\u2225\u2225\u2225\u2225 \u221e\n= max i\n1\n(1\u2212Dmax)2\n( (1\u2212 \u00b5)|Ci|\nn2 Dmax +\n\u00b5 k\u2217 1 |Ci| ) 1 +Dmax |Ci|\n= 1\n(1\u2212Dmax)2 ( (1\u2212 \u00b5) n2 (1 +Dmax)Dmax + \u00b5 k\u2217 1 +Dmax |Cmin|2 ) < 1\u2212 \u00b5 n2 .\nThe last inequality holds for (1\u2212\u00b5)k \u2217\n\u00b5n2 > (1+Dmax) (1\u22123Dmax)|Cmin|2 . For the condition (d), we have\n\u2016PT\u22a5 (Q)\u2016\u2217 \u2264 1\n1\u2212 \u03b1 \u2225\u2225\u2225\u2225PT\u22a5 (1\u2212 \u00b5n2 Sign(A\u2212K\u2217)/X\u2217 + \u00b5k\u2217P\u2126(UUT ) )\u2225\u2225\u2225\u2225 \u2217\n\u2264 1\n1\u2212 \u03b1 \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 P\u2126  ( \u00b5 k\u2217 1 |C1| \u2212 1\u2212\u00b5 n |C1| n ) 1|C1|\u00d7|C1| \u2212 1\u2212\u00b5 n \u221a |C1| |C2| n 1|C1|\u00d7|C2| \u00b7 \u2212 1\u2212\u00b5 n \u221a |C1| |Ck\u2217 | n 1|C1|\u00d7|Ck\u2217 | \u2212 1\u2212\u00b5 n \u221a |C2| |C1| n 1|C2|\u00d7|C1| ( \u00b5 k\u2217 1 |C2| \u2212 1\u2212\u00b5 n |C2| n ) 1|C2|\u00d7|C2| \u00b7 \u2212 1\u2212\u00b5 n \u221a |C2| |Ck\u2217 | n 1|C2|\u00d7|Ck\u2217 | \u00b7 \u00b7 \u00b7 \u00b7\n\u2212 1\u2212\u00b5 n\n\u221a |Ck\u2217 | |C1|\nn 1|Ck\u2217 |\u00d7|C1| \u2212 1\u2212\u00b5 n\n\u221a |Ck\u2217 | |C2|\nn 1|Ck\u2217 |\u00d7|C2| \u00b7 ( \u00b5 k\u2217 1 |Ck\u2217 | \u2212 1\u2212\u00b5 n |Ck\u2217 | n ) 1|Ck\u2217 |\u00d7|Ck\u2217 |\n \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u2217\n\u2264 Dmax 1\u2212 \u03b1 1\u2212 \u00b5 n \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225  |C1| n 1|C1|\u00d7|C1|\n\u221a |C1| |C2| n 1|C1|\u00d7|C2| \u00b7 \u221a |C1| |Ck\u2217 | n 1|C1|\u00d7|Ck\u2217 |\u221a\n|C2| |C1| n 1|C2|\u00d7|C1| |C2| n 1|C2|\u00d7|C2| \u00b7 \u221a |C2| |Ck\u2217 | n 1|C2|\u00d7|Ck\u2217 |\n\u00b7 \u00b7 \u00b7 \u00b7\u221a |Ck\u2217 | |C1|\nn 1|Ck\u2217 |\u00d7|C1|\n\u221a |Ck\u2217 | |C2|\nn 1|Ck\u2217 |\u00d7|C2| \u00b7\n|Ck\u2217 | n 1|Ck\u2217 |\u00d7|Ck\u2217 |\n \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u2217\n+ Dmax\n1\u2212 \u03b1 \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225  \u00b5 k\u2217 1 |C1| 1|C1|\u00d7|C1| 0 \u00b7 0 0 \u00b5 k\u2217 1 |C2| 1|C2|\u00d7|C2| \u00b7 0 \u00b7 \u00b7 \u00b7 \u00b7 0 0 \u00b7 \u00b5\nk\u2217 1 |Ck\u2217 | 1|Ck\u2217 |\u00d7|Ck\u2217 |  \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u2217\n= Dmax\n1\u2212 \u03b1 ( 1\u2212 \u00b5 n \u2211 i |Ci|2 n + \u00b5 ) < \u00b5.\nThe last inequality holds for (1\u2212\u00b5)k \u2217 \u00b5n2 < (1\u2212\u03b1\u2212Dmax)k\u2217 Dmax \u2211 i |Ci|2 as assumed.\nLemma 5. If \u03b1 < 1 then T \u2229 \u2126 = {0}.\nProof. We show that the projection PT P\u2126(\u00b7) has a norm \u03b1 strictly less than one. Then, if there exists a non-zero matrix M \u2208 T \u2229 \u2126, then \u2016M\u2016\u221e = \u2016PT P\u2126(M)\u2016\u221e \u2264 \u03b1\u2016M\u2016\u221e < \u2016M\u2016\u221e is a trivial contradiction. Let M \u2208 \u2126 and consider\n\u2016PT (M)\u2016\u221e = maxi,j \u2225\u2225\u2225\u2225 1|Ci|1|Ci|\u00d7|Ci|MCi,Cj + 1|Cj |MCi,Cj1|Cj |\u00d7|Cj | \u2212 1|Ci| |Cj |1|Ci|\u00d7|Ci|MCi,Cj1|Cj |\u00d7|Cj | \u2225\u2225\u2225\u2225 \u221e\n\u2264 2Dmax\u2016M\u2016\u221e = \u03b1\u2016M\u2016\u221e.\nThe last step is attained by optimizing over |Ci| and |Cj |. This concludes the proof of the lemma.\nLemma 6. \u2016W (P\u2126(Z\u2217 \u2212 UUT ))\u20162 \u2264 Dmax1\u2212\u03b1 (k \u2217 \u2212 1).\nProof. For M \u2208 \u2126, we have \u2016M\u20162 \u2264 \u2016M\u03c3\u20162, where, M\u03c3 \u2208 Rk \u2217\u00d7k\u2217 with (M\u03c3)i,j = \u2016MCi,Cj\u20162. By definition of Dmax, we have \u2016MCi,Cj\u20162 \u2264 Dmax \u221a |Ci| |Cj |\u2016MCi,Cj\u2016\u221e. Thus,\n\u2016P\u2126(Z\u2217 \u2212 UUT )\u20162 \u2264 Dmax \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225  0 1 \u00b7 1 1 0 \u00b7 1 \u00b7 \u00b7 \u00b7 \u00b7 1 1 \u00b7 0  \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\n2\n= Dmax(k \u2217 \u2212 1).\nThe rest of the proof is straight forward as follows\n\u2016W (P\u2126(Z\u2217 \u2212 UUT ))\u20162 = \u2225\u2225\u2225\u2225\u2225PT \u22a5P\u2126 ( \u221e\u2211 i=0 (PT P\u2126)i(P\u2126(Z\u2217 \u2212 UUT )) )\u2225\u2225\u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225\u2225\u2225P\u2126 ( \u221e\u2211 i=0 (PT P\u2126)i(P\u2126(Z\u2217 \u2212 UUT )) )\u2225\u2225\u2225\u2225\u2225 2 = Dmax 1\u2212 \u03b1 (k\u2217 \u2212 1).\nThis concludes the proof of the lemma."}], "references": [{"title": "An improved algorithm for bipartite correlation clustering", "author": ["N. Ailon", "N. Avigdor-Elgrabli", "E. Liberty"], "venue": "European Symposium on Algorithms", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Approximating the cut-norm via grothendieck\u2019s inequality", "author": ["N. Alon", "A. Noar"], "venue": "SIAM Journal on Computing ,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Large scale correlation clustering optimization", "author": ["S. Bagon", "M. Galun"], "venue": "Arxiv preprint arXiv:1112.2903", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Correlation clustering", "author": ["N. Bansal", "A. Blum", "S. Chawla"], "venue": "In Proceedings of the 43rd Symposium on Foundations of Computer Science", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Completely Positive Matrices. World Scientific Publication", "author": ["A. Berman", "N. Shaked-Monderer"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Spectral clustering based on the graph p-laplacian", "author": ["T. B\u00fchler", "M. Hein"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Computational enhancements in low-rank semidefinite programming", "author": ["S. Burer", "C. Choi"], "venue": "Optimization Methods and Software,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Robust principal component analysis", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of ACM ,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Spectral k-way ratio cut partitioning", "author": ["P. Chan", "M. Schlag", "J. Zien"], "venue": "IEEE Trans. CAD- Integrated Circuits and Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Latent variable graphical model selection via convex optimization. arXiv:1008.1290", "author": ["V. Chandrasekaran", "P.A. Parrilo", "A.S. Willsky"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "An efficient algorithm for a complete link method", "author": ["D. Defays"], "venue": "The Computer Journal (British Computer Society),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1977}, {"title": "A unified view of kernel k-means, spectral clustering and graph cuts", "author": ["I. Dhillon", "Y. Guan", "B. Kulis"], "venue": "UTCS Technical Report TR-04-25,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["M. Goemans", "D. Williamson"], "venue": "Journal of ACM ,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1995}, {"title": "Nonnegative factorization of positive semidefinite nonnegative matrices", "author": ["L. Gray", "D. Wilson"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1980}, {"title": "Algorithms for Clustering Data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1981}, {"title": "Clustering partially observed graphs via convex optimization", "author": ["A. Jalali", "Y. Chen", "S. Sanghavi", "H. Xu"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Convex Analysis and Minimization Algorithms I", "author": ["J.B", "H.-U. C"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1991}, {"title": "Sparse online learning via truncated gradient", "author": ["J. Langford", "L. Li", "T. Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE ,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Practical large-scale optimization for max-norm regularization", "author": ["J. Lee", "B. Recht", "R. Salakhutdinov", "N. Srebro", "J. Tropp"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "A direct product theorem for discrepancy", "author": ["T. Lee", "A. Shraibman", "R. Spalek"], "venue": "In Proceedings of the IEEE 23rd Annual Conference on Computational Complexity", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Correlation clustering with noisy input", "author": ["C. Mathieu", "W. Schudy"], "venue": "In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Comparing clusterings\u2014an information based distance", "author": ["M. Meil\u01ce"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Learning segmentation by random walks", "author": ["M. Meil\u01ce", "J. Shi"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Slink: an optimally efficient algorithm for the single-link cluster method", "author": ["R. Sibson"], "venue": "The Computer Journal (British Computer Society),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1973}, {"title": "Learning with Matrix Factorizations", "author": ["N. Srebro"], "venue": "Ph.D. thesis, Massachusetts Institute of Technology,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakkola"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "A tutorial on spectral clustering", "author": ["U. von Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Robust pca via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "IEEE Transactions on Information Theory ", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "In this paper, we consider the problem of cut-based, or correlation, clustering [4] that has received a lot of attention recently [1, 22, 3]: Given G(V, E) on n nodes with normalized symmetric affinity matrix A (for all u, v \u2208 V: 0 \u2264 Auv \u2264 1 and Auu = 1), we want to partition V into clusters C = {C1, .", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "In this paper, we consider the problem of cut-based, or correlation, clustering [4] that has received a lot of attention recently [1, 22, 3]: Given G(V, E) on n nodes with normalized symmetric affinity matrix A (for all u, v \u2208 V: 0 \u2264 Auv \u2264 1 and Auu = 1), we want to partition V into clusters C = {C1, .", "startOffset": 130, "endOffset": 140}, {"referenceID": 21, "context": "In this paper, we consider the problem of cut-based, or correlation, clustering [4] that has received a lot of attention recently [1, 22, 3]: Given G(V, E) on n nodes with normalized symmetric affinity matrix A (for all u, v \u2208 V: 0 \u2264 Auv \u2264 1 and Auu = 1), we want to partition V into clusters C = {C1, .", "startOffset": 130, "endOffset": 140}, {"referenceID": 2, "context": "In this paper, we consider the problem of cut-based, or correlation, clustering [4] that has received a lot of attention recently [1, 22, 3]: Given G(V, E) on n nodes with normalized symmetric affinity matrix A (for all u, v \u2208 V: 0 \u2264 Auv \u2264 1 and Auu = 1), we want to partition V into clusters C = {C1, .", "startOffset": 130, "endOffset": 140}, {"referenceID": 3, "context": "Unfortunately, finding a clustering minimizing the disagreement D(C) is NP-Hard [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 15, "context": "[16] suggested a trace-norm (aka nuclear-norm) relaxation, casting the problem as minimizing an `1 loss and a trace-norm penalty, and providing conditions under which the true underlying clustering is recovered.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Instead of trace-norm, we propose using the max-norm (aka \u03b32 : `1 \u2192 `\u221e norm) [30], which is a tighter convex relaxation than the trace-norm.", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "A similar optimization problem with a trace-norm constraint (or trace-norm regularization) has recently been the subject of some interest in the context of \u201crobust PCA\u201d [8, 33] and recovering the structure of graphical models with latent variables [10].", "startOffset": 169, "endOffset": 176}, {"referenceID": 29, "context": "A similar optimization problem with a trace-norm constraint (or trace-norm regularization) has recently been the subject of some interest in the context of \u201crobust PCA\u201d [8, 33] and recovering the structure of graphical models with latent variables [10].", "startOffset": 169, "endOffset": 176}, {"referenceID": 9, "context": "A similar optimization problem with a trace-norm constraint (or trace-norm regularization) has recently been the subject of some interest in the context of \u201crobust PCA\u201d [8, 33] and recovering the structure of graphical models with latent variables [10].", "startOffset": 248, "endOffset": 252}, {"referenceID": 12, "context": "1 Relationship to the Goemans Willimason SDP Relaxation Our convex relaxation approach is related to the classic SDP relaxations of max-cut [13] and more generally the cut-norm [2].", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "1 Relationship to the Goemans Willimason SDP Relaxation Our convex relaxation approach is related to the classic SDP relaxations of max-cut [13] and more generally the cut-norm [2].", "startOffset": 177, "endOffset": 180}, {"referenceID": 15, "context": "This type of guarantee is more in the spirit of compressed sensing, which where exact recovery of a support set is guaranteed subject to conditions on the input [16].", "startOffset": 161, "endOffset": 165}, {"referenceID": 25, "context": "In hierarchical clustering algorithms such as UPGMA [28], SLINK [27] and CLINK [11] the goal is to generate a sequence of clusterings by produce a sequence of clustering by merging/splitting two clusters at each step of the sequence according to a local disagreement objective as opposed to our global D(C).", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "In hierarchical clustering algorithms such as UPGMA [28], SLINK [27] and CLINK [11] the goal is to generate a sequence of clusterings by produce a sequence of clustering by merging/splitting two clusters at each step of the sequence according to a local disagreement objective as opposed to our global D(C).", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "Cut-based clustering algorithms such as k-means/medians [31, 15], ratio association [26], ratio cut [9] and normalized cut [34] try to optimize an objective function globally.", "startOffset": 56, "endOffset": 64}, {"referenceID": 24, "context": "Cut-based clustering algorithms such as k-means/medians [31, 15], ratio association [26], ratio cut [9] and normalized cut [34] try to optimize an objective function globally.", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "Cut-based clustering algorithms such as k-means/medians [31, 15], ratio association [26], ratio cut [9] and normalized cut [34] try to optimize an objective function globally.", "startOffset": 100, "endOffset": 103}, {"referenceID": 28, "context": "In contrast, spectral clustering algorithms[32] try to find the first k principal component of the affinity matrix or a transformed version of that [24].", "startOffset": 43, "endOffset": 47}, {"referenceID": 23, "context": "In contrast, spectral clustering algorithms[32] try to find the first k principal component of the affinity matrix or a transformed version of that [24].", "startOffset": 148, "endOffset": 152}, {"referenceID": 11, "context": "These methods require the number of clusters in advance and has been shown to be tractable (convex) relaxations to NP-Hard cut-based algorithms [12].", "startOffset": 144, "endOffset": 148}, {"referenceID": 27, "context": "The approach we discuss here is to relaxing this set to the set of matrices with bounded max-norm [30].", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "The flavor of our result is similar to [16] for trace-norm, except that we show the max-norm constraint problem recovers the underlying clustering with larger noise comparing to tracenorm constraint.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "This definition is inspired by [16] but is slightly different.", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "2 Comparison to Single-Linkage Algorithm Considering single-linkage algorithm (SLINK) [27] as a baseline for clustering, we compare the power our algorithm in cluster recovery with that.", "startOffset": 86, "endOffset": 90}, {"referenceID": 15, "context": "Comparing to the result of [16] on trace-norm (Dmax \u2264 |Cmin| 4n ), the max-norm tolerates more noise.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "Running Trace-Norm constrained minimization [16] on the graph shown in Fig.", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "Further, we compare our algorithm with trace-norm algorithm [16] and SLINK on a probabilistic setup.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "[30], we introduce dummy variables L,R \u2208 Rn\u00d7n and reformulate (4) as the following SDP problem K\u0302 = arg min K,L,R \u2016A\u2212K\u20161", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20], we introduce dummy variables L,R \u2208 Rn\u00d7n and let K = LR .", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "This problem is not convex, but it is guaranteed to have no local minima for large enough size of the problem [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 19, "context": "[20] suggest the following update [ L R ]", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "3 Loss Function Method There are gradient methods such as truncated gradient [18] that produce sparse solution, however, these methods cannot be applied to this problem.", "startOffset": 77, "endOffset": 81}, {"referenceID": 4, "context": "This suggests using the tightest convex relaxation, that is constraining to K such that there exists R >= 0, \u2016R\u2016\u221e,2 <= 1 with K = RR (the set of matrices K with a factorization K = RR , R >= 0 is called the set of completely positive matrices and is convex [5]).", "startOffset": 257, "endOffset": 260}, {"referenceID": 15, "context": "2 Comparison with Other Algorithms We compare our enhanced algorithm with the trace-norm algorithm [16] followed by SLINK and SLINK itself.", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "Using \u201cvariation of information\u201d [23] as a distance measure for clusterings, we compare our algorithm with linear objective with trace-norm counterpart, SLINK and spectral clustering[32] for both balanced and unbalanced clusterings described before.", "startOffset": 33, "endOffset": 37}, {"referenceID": 28, "context": "Using \u201cvariation of information\u201d [23] as a distance measure for clusterings, we compare our algorithm with linear objective with trace-norm counterpart, SLINK and spectral clustering[32] for both balanced and unbalanced clusterings described before.", "startOffset": 182, "endOffset": 186}, {"referenceID": 18, "context": "3 MNIST Dataset To demonstrate our method in a realistic and larger scale data set, we run our enhanced algorithm, trace-norm and spectral clustering on MNIST Dataset [19].", "startOffset": 167, "endOffset": 171}, {"referenceID": 5, "context": "For each experiment, we pick a total of n data points from 10 different classes (n/10 from each class) and construct the affinities using Gaussian kernel as explained in [6].", "startOffset": 170, "endOffset": 173}], "year": 2012, "abstractText": "We suggest using the max-norm as a convex surrogate constraint for clustering. We show how this yields a better exact cluster recovery guarantee than previously suggested nuclear-norm relaxation, and study the effectiveness of our method, and other related convex relaxations, compared to other clustering approaches.", "creator": "LaTeX with hyperref package"}}}