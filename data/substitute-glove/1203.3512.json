{"id": "1203.3512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Exact and Approximate Inference in Associative Hierarchical Networks using Graph Cuts", "abstract": "Markov Networks are mainly used before out memory vision such machine improve. An major base-2 are the Associative Markov Networks which are well in next wide variety of applications. For because users a good approximate terms increase create are be treated devote using notation next source coming making algorithms changes as \u03b2 - acquisition. Recently part similar experimental even largely package, the manipulatives clustering aol, which provides came natural oligomer included the Associative Markov Network three greater permanent cliques (my. [. cliques size greater for two ). This devised management took... model for simple class pluralization likely 2004 computer vision. Within this materials even by describe the associative constructionist creating and provide a shrewder cheap calculation not approximate syllogism largest without combinatorics delay. Our theory ensemble without a backbone paper homes between thirty beyond constraint, and higher order external any consequently before monied e.g. billions under countless however formula_33. Due although seen size of specifically problems standard linear creative techniques already self-contradictory. We that that everything method has a flying of april for the logical of general associative hierarchical network made arbitrary unipotent component noting clearly few determine opened accuracy exist same the achieve of readability all Markov Networks with higher order cliques.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (451kb)", "http://arxiv.org/abs/1203.3512v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["chris russell", "l'ubor ladicky", "pushmeet kohli", "philip h s torr"], "accepted": false, "id": "1203.3512"}, "pdf": {"name": "1203.3512.pdf", "metadata": {"source": "CRF", "title": "Exact and Approximate Inference in Associative Hierarchical Networks using Graph Cuts", "authors": ["Chris Russell", "Pushmeet Kohli", "Philip H.S. Torr"], "emails": [], "sections": [{"heading": null, "text": "Markov Networks are widely used through out computer vision and machine learning. An important subclass are the Associative Markov Networks which are used in a wide variety of applications. For these networks a good approximate minimum cost solution can be found efficiently using graph cut based move making algorithms such as alphaexpansion. Recently a related model has been proposed, the associative hierarchical network, which provides a natural generalisation of the Associative Markov Network for higher order cliques (i.e. clique size greater than two). This method provides a good model for object class segmentation problem in computer vision.\nWithin this paper we briefly describe the associative hierarchical network and provide a computationally efficient method for approximate inference based on graph cuts. Our method performs well for networks containing hundreds of thousand of variables, and higher order potentials are defined over cliques containing tens of thousands of variables. Due to the size of these problems standard linear programming techniques are inapplicable. We show that our method has a bound of 4 for the solution of general associative hierarchical network with arbitrary clique size noting that few results on bounds exist for the solution of labelling of Markov Networks with higher order cliques."}, {"heading": "1 INTRODUCTION", "text": "The last few decades have seen the emergence of Markov networks or random fields as the most widely used probabilistic model for formulating problems in\nmachine learning and computer vision. This interest has led to a large amount of work on the problem of estimating the maximum a posteriori (map) solution of a random field (Szeliski et al., 2006). However, most of this research effort has focused on inference over pairwise Markov networks. Of particular interest are the families of associative pairwise potentials (Taskar et al., 2004), in which connected variables are assumed to be more likely than not to share the same label. Inference algorithms targeting these associative potentials, which include truncated convex costs (Kumar and Torr, 2008), metrics (Boykov et al., 2001), and semi metrics (Kumar and Koller, 2009), often carry bounds which guarantee the cost of the solution found must lie within a bound, specified as a fixed factor of n of the cost of the minimal solution.\nAlthough higher order Markov networks (i.e. those with a clique size greater than two) have been used to obtain impressive results for a number of challenging problems in computer vision (Roth and Black, 2005; Komodakis and Paragios, 2009; Vicente et al., 2009; Ladicky et al., 2010), the problem of bounded higher order inference has been largely ignored.\nIn this paper, we address the problem of performing graph cut based inference in a new model: the Associative Hierarchical Networks (ahns) (Ladicky et al., 2009), which includes the higher order Associative Markov Networks (amns) (Taskar et al., 2004) or Pn potentials (Kohli et al., 2007) and the Robust Pn (Kohli et al., 2008) model as special cases, and derive a bound of 4.\nThis family of ahns have been successfully applied to diverse problems such as object class recognition, document classification and texture based video segmentation, where they obtain state of the art results. Note that in our earlier work Ladicky et al. (2009), the problem of inference is not discussed at all; it shows how these hierarchical models can be used for scene understanding, and how learning is possible under the assumption that the model is tractable.\nFor a set of variables x(1) ahns are characterised by energies (or costs) of the form:\nE(x(1)) = E\u2032(x(1)) + min xa Ea(x(1),xa) (1)\nwhere E\u2032 and Ea are pairwise amns and xa is a set of auxiliary variables. The ahn is a amn containing higher order cliques, defined only in terms of x(1), but can also be seen as a pairwise amn defined in terms of x(1) and xa. We propose new move making algorithms over the pairwise energy E\u2032(x(1)) +Ea(x(1),xa) which have the important property of transformational optimality.\nMove making algorithms function by efficiently searching through a set of candidate labellings and proposing the optimal candidate i.e. the one with the lowest energy to move to. The set of candidates is then updated, and the algorithm repeats till convergence.\nWe call a move making algorithm transformationally optimal if and only if any proposed move (x\u2217,xa) satisfies the property:\nE(x\u2217) = E\u2032(x\u2217) + Ea(x\u2217,xa). (2)\nExperimentally, our transformationally optimal algorithms converge faster, and to better solutions than standard approaches, such as \u03b1-expansion. Moreover, unlike standard approaches, our transformationally optimal algorithms always find the exact solution for binary ahns.\nOutline of the paper In section 2 we introduce the notation used in the rest of the paper. Existing models generalised by the associative hierarchical network, and the full definition of ahns are given in section 3. In section 4 we discuss work on efficient inference, and show how the pairwise form of associative hierarchical networks can be minimised using the \u03b1-expansion algorithm, and derive bounds for our approach. Section 5 discusses the application of novel move making algorithms to such energies, and we show that under our formulation the moves of the robust Pn model become equivalent to a more general form of range moves over unordered sets. We derive transformational optimality results over hierarchies of these potentials, guaranteeing the optimality of the moves proposed. We experimentally verify the effectiveness of our approach against other methods in section 6, and conclude in section 7."}, {"heading": "2 NOTATION", "text": "Consider an amn defined over a set of latent variables x = {xi|i \u2208 V} where V = {1, 2, ..., n}. Each random variable xi can take a label from the label set\nL = {l1, l2, ..., lk}. Let C represent a set of subsets of V (i.e., cliques), over which the amn is defined. The map solution of a amn can be found by minimising an energy function E : Ln \u2192 R. These energy functions can typically be written as a sum of potential functions: E(x) = \u2211 c\u2208C \u03c8c(xc), where xc represents the set of variables included in any clique c \u2208 C. We refer to functions defined over cliques of size one as unary potentials and denote them by \u03c8i : L \u2192 R where the subscript i denotes the index of the variable over which the potential is defined. Similarly, functions defined over cliques of size two are referred to as pairwise potentials and denoted: \u03c8ij : L2 \u2192 R.\nPotentials defined over cliques of size greater than two i.e. \u03c8c : L|c| \u2192 R, |c| > 2 will be called higher order potentials, where |c| represents the number of variables included in the clique (also called the clique order). We will call an energy function pairwise if it contains no potentials defined over cliques of size greater than 2.\nAt points within the paper, we will want to distinguish between the original variables of the energy function, whose optimal values we are attempting to find, and the auxiliary variables which we will introduce to convert our higher order function into a pairwise one. We refer to the original variables as the base layer x(1) (as they lie at the bottom of the hierarchical network). All auxiliary variables at any level h of the hierarchy are denoted by x(h). The set of indices of variables constituting level h of the hierarchy is denote by Vh. Similarly, the set of all pairwise interactions at level h is denoted by Eh."}, {"heading": "3 ASSOCIATIVE HIERARCHICAL NETWORKS", "text": "Existing higher-order models Taskar et al. (2004) proposed the use of higher order potentials that encourage the entirety of a clique to take some label, and discusses how they can be applied to predicting protein interactions and document classification. These potentials were introduced into computer vision along with an efficient graph cut based method of inference, as the strict Pn Potts model (Kohli et al., 2007).\nA generalisation of this approach was proposed by Kohli et al. (2008), who observed that in the image labelling problem, most (but not all) pixels belonging to image segments computed using an unsupervised clustering/segmentation algorithm take the same object label. They proposed a higher order mrf over segment based cliques. The energy took the form:\nE(x) = \u2211 i\u2208V \u03c8i(xi) + \u2211 ij\u2208E \u03c8ij(xi, xj) + \u2211 c\u2208C \u03c8c(xc), (3)\nwhere \u03c8c(xc) = min l\u2208L\n( \u03b3maxc , \u03b3 l c +\n\u2211 i\u2208c kic\u2206(xi 6= l)\n) . (4)\nThe potential function parameters kic, \u03b3 l c, and \u03b3 max c are subject to the restriction that kic \u2265 0 and \u03b3lc \u2264 \u03b3maxc ,\u2200l \u2208 L. \u2206 denotes the Kronecker delta, an indicator function taking a value of 1 if the statement following it is true and 0 if false.\nThese potentials can be understood as a truncated majority voting scheme on the base layer. Where possible, they encourage the entirety of the clique to assume one consistent labelling. However, beyond a certain threshold of disagreement they implicitly recognise that no consistent labelling is likely to occur, and no further penalty is paid for increasing heterogeneity.\nWe now demonstrate that the higher order potentials \u03c8c(xc) of the Robust P n model (4) can be represented by an equivalent pairwise function \u03c8c(x (1) c , x (2) c ) defined over a two level hierarchical network with the addition of a single auxiliary variable x (2) c for every clique c \u2208 C. This auxiliary variable take values from an extended label set Le = L \u222a {LF }, where LF , the \u2018free\u2019 label of the auxiliary variables, allows its child variables to take any label without paying a pairwise penalty.\nIn general, every higher order cost function can be converted to a 2\u2212layer associative hierarchical network by taking an approach analogous to that of factor graphs (Kschischang et al., 2001) and adding a single multi-state auxiliary variable. However, to do this for general higher order functions requires the addition of an auxiliary variable with an exponential sized label set (Wainwright and Jordan, 2008). Fortunately, the class of higher order potentials we are concerned with can be compactly described as ahns with auxiliary variables that take a similar sized label set to the base layer, permitting fast inference.\nThe corresponding higher order function can be written as:\n\u03c8c(x (1) c ) = min\nx (2) c\n\u03c8c(x (1) c , x (2) c )\n= min x (2) c\n[ \u03c6c(x (2) c ) + \u2211 i\u2208c \u03c6ic(x (1) i , x (2) c ) ] .(5)\nThe unary potentials \u03c6c(x (2) c ) defined on the auxiliary variable x (2) c assign the cost \u03b3l if x (2) c = l \u2208 L, and \u03b3max if x (2) c = LF . The pairwise potential \u03c6ic(xi, x (2) c ) is defined as:\n\u03c6ic(xi, x (2) c ) =\n{ 0 if x (2) c = LF , or x (2) c = xi.\nkic if x (2) c = l \u2208 L, and xi 6= l.\n(6)\nGeneral Formulation The scheme described above can be extended by allowing pairwise and higher order potentials to be defined over x(2) and further over x(i), which corresponds to higher order potentials defined over the layer x(i\u22121). The higher order energy corresponding to the general hierarchical network can be written using the following recursive function:\nE(1)(x(1)) = \u2211 i\u2208V \u03c8 (1) i (x (1) i ) + \u2211 ij\u2208E(1) \u03c8 (1) ij (x (1) i , x (1) j )\n+ min x(2)\nE(2)(x(1),x(2)) (7)\nwhere E(2)(x(1),x(2)) is recursively defined as:\nE(n)(x(n\u22121),x(n)) = \u2211\nc\u2208V (n) \u03c6c(x\n(n) c ) + \u2211 c\u2208V(n)i\u2208c \u03c6 (n) ic (x (n\u22121) i , x (n) c )\n+ \u2211\ncd\u2208E(n) \u03c8 (n) cd (x (n) c , x (n) d ) + min x(n+1) E(n+1)(x(n),x(n+1))\n(8)\nand x(n) = {x(n)c |c \u2208 Vn} denotes the set of variables at the nth level of the hierarchy, E(n) represents the edges at this layer, and \u03c8\n(n) ic (x (n\u22121) c , x (n) c ) denotes\nthe inter-layer potentials defined over variables of layer n\u2212 1 and n.\nWhile the hierarchical formulation of both Taskar\u2019s and Kohli\u2019s models can be understood as a mathematical convenience that allows for fast and efficient bounded inference, our earlier work (Ladicky et al., 2009) used it for true multi-scale inference, modelling constraints defined over many quantisations of the image."}, {"heading": "4 INFERENCE", "text": "Inference in Pairwise Networks Although the problem of map inference is NP-hard for most associative pairwise functions defined over more than two labels, in real world problems many conventional algorithms provide near optimal solutions over grid connected networks (Szeliski et al., 2006). However, the dense structure of hierarchical networks results in frustrated cycles and makes traditional reparameterisation based message passing algorithms for map inference such as loopy belief propagation (Weiss and Freeman, 2001) and tree-reweighted message passing (Kolmogorov, 2006) slow to converge and unsuitable (Kolmogorov and Rother, 2006). Many of these frustrated cycles can be eliminated via the use of cycle inequalities (Sontag et al., 2008; Werner, 2009), but only by significantly increasing the run time of the algorithm. Graph cut based move making algorithms do not suffer from this problem and have been successfully used for minimising pairwise functions defined over densely connected networks encountered in vision.\nExamples of move making algorithms include \u03b1expansion which can only be applied to metrics, \u03b1\u03b2 swap which can be applied to semi-metrics (Boykov et al., 2001), and range moves (Kumar and Torr, 2008; Veksler, 2007) for truncated convex potentials. These moves differ in the size of the space searched for the optimal move. While expansion and swap search a space of size at most 2n while minimising a function of n variables, the range moves explores a much larger space of Kn where K is a parameter of the energy (see Veksler (2007) for more details). Of these move making approaches, only \u03b1\u03b2 swap can be directly applied to associative hierarchical networks as the term \u03c6ic(xi, xc), is not a metric nor truncated convex.\nThese methods start from an arbitrary initial solution of the problem and proceed by making a series of changes each of which leads to a solution of the same or lower energy (Boykov et al., 2001). At each step, the algorithms project a set of candidate moves into a Boolean space, along with their energy function. If the resulting projected energy function (also called the move energy) is both submodular and pairwise, it can be exactly minimised in polynomial time by solving an equivalent st-mincut problem. These optima can then be mapped back into the original space, returning the optimal move within the move set. The move algorithms run this procedure until convergence, iteratively picking the best candidate as different choices of range are cycled through.\nMinimising Higher Order Functions A number of researchers have worked on the problem of map inference in higher order amns. Lan et al. (2006) proposed approximation methods for bp to make efficient inference possible in higher order mrfs. This was followed by the recent works of Potetz and Lee (2008); Tarlow et al. (2008, 2010) in which they showed how belief propagation can be efficiently performed in networks containing moderately large cliques. However, as these methods were based on bp, they were quite slow and took minutes or hours to converge, and lack bounds.\nTo perform inference in the Pn models, Kohli et al. (2007, 2008), first showed that certain projection of the higher order Pn model can be transformed into submodular pairwise functions containing auxiliary variables. This was used to formulate higher order expansion and swap move making algorithms The only existing work that addresses the problem of bounded higher order inference is (Gould et al., 2009) which showed how theoretical bounds could be derived given move making algorithms that proposed optimal moves by exactly solving some sub-problem. In application they used approximate moves which do not exactly\nsolve the sub-problems proposed. Consequentially, the bounds they derive do not hold for the methods they propose. However, their analysis can be applied to the Pn (Kohli et al., 2007) model and inference techniques, which do propose optimal moves, and it is against these bounds that we compare our results."}, {"heading": "4.1 INFERENCE WITH \u03b1-EXPANSION", "text": "We show that by restricting the form of the inter-layer potentials \u03c8 (n) c (x (n\u22121) c , x (n) c ) to that of the weighted Robust Pn model (Kohli et al., 2008) (see (4)), we can apply \u03b1-expansion to the pairwise form of the ahn.\nThis requires a transform of all functions in the pairwise representation, so that they can be representable as a metric (Boykov et al., 2001). This transformation is non-standard and should be considered a contribution of this work.\nWe alter the form of the potentials in two ways. First, we assume that all variables in the hierarchy take values from the same label set Le = L \u222a {LF }. Where this is not true \u2014 original variables x(1) at the base of the hierarchy can not take label LF \u2014 we artificially augment the label set with the label LF and associate an infinite unary cost with it. Secondly, we make the inter-layer pairwise potentials symmetric by performing a local reparameterisation operation.\nLemma 1. The inter-layer pairwise functions\n\u03c6 (n) ic (x (n\u22121) i , x (n) c ) = 0 if x (n) c = LF or x (n) c = x (n\u22121) i\nkic if x (n) c = l \u2208 L and x(n\u22121)i 6= l\n(9) of (8) can be written as:\n\u03c6 (n) ic (x (n\u22121) i , x (n) c ) = \u03c8 (n\u22121) i (x (n\u22121) i ) + \u03c8 (n) c (x (n) c )\n+ \u03a6 (n) ic (x (n\u22121) i , x (n) c ), (10)\nwhere \u03a6 (n) ic (x (n\u22121) i , x (n) c ) =  0 if x (n\u22121) i = x (n) c kic/2 if x (n\u22121) i = LF or x (n) c = LF and x (n\u22121) i 6= x (n) c\nkic otherwise,\n(11) and\n\u03c8(n)c (x (n) c ) = 0 if x (n) c \u2208 L\n\u2212kic/2 otherwise, (12)\n\u03c8 (n\u22121) i (x (n\u22121) i ) = 0 if x (n\u22121) i \u2208 L\nkic/2 otherwise. (13)\nProof Consider a clique containing only one variable, the general case will follow by induction. Note that if\nno variables take state LF the costs are invariant to reparameterisation. This leaves three cases:\nx (n) c = LF,x (n\u22121) i \u2208 L\n\u03c8c(x (n) c ) + \u03c8ic(x (n) c , x (n\u22121) i ) = \u2212k/2 + k/2 = 0\nx (n) c \u2208 L,x(n\u22121)i = LF\n\u03c8i(x (n\u22121) i ) + \u03c8ic(x (n\u22121) i , x (n) c ) = k/2 + k/2 = k\nx (n) c = LF,x (n\u22121) i = LF\n\u03c8i(x (n\u22121) i ) + \u03c8ic(x (n\u22121) i , x (n) c ) + \u03c8c(x (n) c ) = k\u2212k 2 = 0\n(14)\nBounded Higher Order Inference We now prove bounds for \u03b1-expansion over an ahn.\n1. The pairwise function of lemma 1, is positive definite, symmetric, and satisfies the triangle inequality\n\u03c8a,b(x, z) \u2264 \u03c8a,b(x, y)+\u03c8a,b(y, z)\u2200x, y, z \u2208 L\u222a{LF }. (15) Hence it is a metric, and the algorithms \u03b1\u03b2 swap and \u03b1-expansion can be used to minimise it.\n2. By the work of Boykov et al. (2001), the \u03b1-expansion algorithm is guaranteed to find a solution within a factor of 2 max ( 2,maxE\u2208E1 maxxi,xj\u2208L \u03c8E(xi,xj)\nminxi,xj\u2208L \u03c8E(xi,xj)\n) (i.e. 4\nwhere the potentials defined over the base layer of hierarchy take the form of a Potts model) of the global optima.\n3. The following two properties hold:\nmin x(1) E(x(1)) = min x(1),xa E\u2032(x(1)) + Ea(x(1),xa),\n(16)\nE(x(1)) \u2264 E\u2032(x(1)) + Ea(x(1),xa), (17)\nHence, if there exists a labelling (x\u2032,x\u2217)) such that\nE\u2032(x\u2032)+Ea(x\u2032,x\u2217) \u2264 k min x(1),xa E\u2032(x(1))+Ea(x(1),xa). (18) then\nE(x\u2032) \u2264 kmin x(1) E(x(1)). (19)\nConsequentially, the bound is preserved in the transformation that maps from the pairwise energy back to its higher order form.\nBy way of comparison, the work of Gould et al. (2009) provides a bound of 2|c| for the higher order potentials of the strict Pn model (Kohli et al., 2007), where c is the largest clique in the network. Using their approach, no bounds are possible for the general class of Robust Pn models or for associative hierarchical networks.\nThe moves of our new range-move algorithm (see next section) strictly contain those considered by \u03b1expansion and thus our approach automatically inherits the above approximation bound."}, {"heading": "5 NOVEL MOVES AND TRANSFORMATIONAL OPTIMALITY", "text": "In this section we propose a novel graph cut based move making algorithm for minimising the hierarchical pairwise energy function defined in the previous section.\nLet us consider a generalisation of the swap and expansion moves proposed in Boykov et al. (2001). In a standard swap move, the set of all moves considered is those in which a subset of the variables currently taking label \u03b1 or \u03b2 change labels to either \u03b2 or \u03b1. In our range-swap the moves considered allow any variables taking labels \u03b1,LF or \u03b2 to change their state to any of \u03b1,LF or \u03b2. Similarly, while a normal \u03b1 expansion move allows any variable to change to some state \u03b1, our range expansion allows any variable to change to states \u03b1 or LF .\nThis approach can be seen as a variant on the ordered range moves proposed in Veksler (2007); Kumar and Torr (2008), however while these works require that an ordering of the labels {l1, l2, . . . , ln} exist such that moves over the range {li, li+1 . . . li+j} are convex for some j \u2265 2 and for all 0 < i \u2264 n\u2212 j, our range moves function despite no such ordering existing.\nWe now show that the problem of finding the optimal swap move can be solved exactly in polynomial time. Consider a label mapping function f\u03b1,\u03b2 : L \u2192 {1, 2, 3} defined over the set {\u03b1,LF , \u03b2} that maps \u03b1 to 1, LF to 2 and \u03b2 to 3. Given this function, it is easy to see that the reparameterised inter-layer potential \u03a6 (n) ic (x (n\u22121) i , x (n) c ) defined in lemma 1 can be written as a convex function of f\u03b1,\u03b2(x (n\u22121) i )\u2212 f\u03b1,\u03b2(x (n) c ) over the range \u03b1,LF , \u03b2. Hence, we can use the Ishikawa construct (Ishikawa, 2003) to minimise the swap move energy to find the optimal move. A similar proof can be constructed for the range-expansion move described above.\nThe above defined move algorithm gives improved solutions for the hierarchical energy function used for formulating the object segmentation problem. We can improve further upon this algorithm. Our novel construction for computing the optimal moves explained in the following section, is based upon the original energy function (before reparameterisation) and has a strong transformational optimality property. We first\ndescribe the construction of a three label range move over the hierarchical network, and then show in section 5.2 that under a set of reasonable assumptions, our methods are equivalent to a swap or expansion move that exactly minimises the equivalent higher order energy defined over the base variables E(x(1)) of the hierarchical network (as defined in (7))."}, {"heading": "5.1 CONSTRUCTION OF THE RANGE MOVE", "text": "We now explain the construction of the submodular quadratic pseudo boolean (qpb) move function for range expansion. The construction of the swap based move function can be derived from this range move.\nIn essence, we demonstrate that the cost function of (9) over the range xc \u2208 {\u03b2, LF , \u03b1}, xi \u2208 {\u03b4, LF , \u03b1} where \u03b2 may or may not equal \u03b4 is expressible as a submodular qpb potential. To do this, we create a qpb function defined on 4 variables c1, c2, i1 and i2. We associate the states i1 = 1, i2 = 1 with xi taking state \u03b1, i1 = 0, i2 = 0 with the current state of xi = \u03b4, and i1 = 1, i2 = 0 with state LF . We prohibit the state i1 = 0, i2 = 1 by incorporating the pairwise term \u221e(1\u2212 i1)i2 which assigns an infinite cost to the state i1 = 0, i2 = 1, and do the same respectively with xc and c1 and c2. To simplify the resulting equation, we write I instead of \u2206(\u03b2 6= \u03b4), k\u03b4 for \u03c8i,c(LF , \u03b4) and k\u03b1 for \u03c8i,c(LF , \u03b1) then\n\u03c8i,c(xi, xc) = (1\u2212I)k\u03b4c2(1\u2212x2)\u2212Ik\u03b4c2+k\u03b1(1\u2212c1)x1 (20)\nover the range xc \u2208 {\u03b2, LF , \u03b1}, xi \u2208 {\u03b4, LF , \u03b1}.\nThe proof follows from inspection of the function. Note that c2 = 1 if and only if xc = \u03b2 while c1 = 0 if and only if c = \u03b1. If xc = LF then c2 = 0 and c1 = 1 and the cost is always 0. If xc = \u03b1 the first two terms take cost 0, and the third term has a cost of k\u03b1 associated with it unless xi = \u03b1. Similarly, if xc = \u03b2 there is a cost of k\u03b2 associated with it, unless xi also takes label \u03b2."}, {"heading": "5.2 OPTIMALITY", "text": "Note that both variants of unordered range moves are guaranteed to find the global optima if the label space of x(1) contains only two states. This is not the case for the standard forms of \u03b1 expansion or \u03b1\u03b2 swap as auxiliary variables may take one of three states.\nTransformational optimality Consider an energy function defined over the variables x = {x(h), h \u2208 {1, 2, . . . ,H}} of a hierarchy with H levels. We call a move making algorithm transformationally optimal if and only if any proposed move x\u2217 = {x(h)\u2217 , h \u2208\n{1, 2, . . . ,H}} satisfies the property:\nE(x\u2217) = min xaux\u2217\nE(x (1) \u2217 ,x aux \u2217 ) (21)\nwhere xaux\u2217 = \u22c3 h\u22082,...,H x (h) \u2217 represents the labelling of all auxiliary variables in the hierarchy. Note that any move proposed by transformationally optimal algorithms minimises the original higher order energy (7). We now show that when applied to hierarchical networks, the range moves are transformationally optimal.\nMove Optimality To guarantee transformational optimality we need to constrain the set of higher order potentials. Consider a clique c with an associated auxiliary variable x (i) c . Let xl be a labelling such that x (i) c = l \u2208 L and xLF be a labelling that only differs from it that the variable x (i) c takes label LF . We say a clique potential is hierarchically consistent if it satisfies the constraint:\nE(xl) \u2265 E(xLF ) =\u21d2 \u2211 i\u2208c k\ni c\u2206(xi = l)\u2211 i\u2208c k i c > 0.5. (22)\nThe property of hierarchical consistency is also required in computer vision for the cost associated with the hierarchy to remain meaningful. The labelling of an auxiliary variable within the hierarchy should be reflected in the state of the clique associated with it. If an energy is not hierarchically consistent, it is possible that the optimal labelling of regions of the hierarchy will not reflect the labelling of the base layer.\nThe constraint (22) is enforced by construction, weighting the relative magnitude of \u03c8i(l) and \u03c8i,j(bj , x (i) c ) to guarantee that:\n\u03c8i(l) + \u2211\nj\u2208Ni/c\nmax xj\u2208L\u222a{Lf}\n\u03c8i,j(bj , x (i) c ) < 0.5 \u2211 Xi\u2208c ki\u2200l \u2208 L.\n(23)\nIf this holds, in the degenerate case where there are only two levels in the hierarchy, and no pairwise connections between the auxiliary variables, our network is exactly equivalent to the Pn model.\nAt most one l \u2208 L at a time can satisfy (22), assuming the hierarchy is consistent. Given a labelling for the base layer of the hierarchy x(1), an optimal labelling for an auxiliary variable in x(2) associated with some clique must be one of two labels: LF and some l \u2208 L. By induction, the choice of labelling of any clique in x(j) must also be a decision between at most two labels: LF and some l \u2208 L."}, {"heading": "5.3 TRANSFORMATIONAL OPTIMALITY UNDER UNORDERED RANGE MOVES", "text": "Swap range moves\nSwap based optimality requires an additional constraint to that of (22), namely that there are no pairwise connections between variables in the same level of the hierarchy, except in the base layer. From (6) if an auxiliary variable xc may take label \u03b3 or LF , and one of its children xi|i \u2208 c take label \u03b4 or LF , the cost associated with assigning label \u03b3 or LF to xc is independent of the label of xi with respect to a given move.\nUnder a swap move, a clique currently taking label \u03b4 6\u2208 {\u03b1, \u03b2} will continue to do so. This follows from (4) as the cost associated with taking label \u03b4 is only dependent upon the weighted average of child variables taking state \u03b4, and this remains constant. Hence the only clique variables that may have a new optimal labelling under the swap are those currently taking state \u03b1,LF or \u03b2, and these can only transform to one of the states \u03b1,LF or \u03b2. As the range moves map exactly this set of transformations, the move proposed must be transformationally optimal, and consequently the best possible \u03b1\u03b2 swap over the energy (7).\nExpansion Moves\nIn the case of a range-expansion move, we can maintain transformational optimality while incorporating pairwise connections into the hierarchy \u2014 provided condition (22) holds, and the energy can be exactly represented in our submodular moves.\nIn order for this to be the case, the pairwise connections must be both convex over any range \u03b1,LF , \u03b2 and a metric. The only potentials that satisfy this are linear over the ordering \u03b1,LF , \u03b2 \u2200\u03b1, \u03b2. Hence all pairwise connections must be of the form:\n\u03c8i,j(xi, xj) =  0 if xi = xj\n\u03bb/2 if xi = LF or xj = LF and xi 6= xj \u03bb otherwise.\n(24) where \u03bb \u2208 R+0 . By lemma 1, it can be readily seen that the connections in the hierarchical network are a constrained variant of this form.\nA similar argument to that of the optimality of \u03b1\u03b2 swap can be made for \u03b1-expansion. As the label \u03b1 is \u2018pushed\u2019 out across the base layer, the optimal labelling of some x(n) where n \u2265 2 must either remain constant or transition to one of the labels LF or \u03b1. Again, the range moves map exactly this set of transforms and the suggested move is both transformation-\nally optimal, and the best expansion of label \u03b1 over the higher order energy of (7)."}, {"heading": "6 EXPERIMENTS", "text": "We evaluate \u03b1-expansion, \u03b1\u03b2 swap, trw-s, Belief Propagation, Iterated Conditional Modes, and both the expansion and swap based variants of our unordered range moves on the problem of object class segmentation over the MSRC data-set (Shotton et al., 2006), in which each pixel within an image must be assigned a label representing its class, such as grass, water, boat or cow. We express the problem as a three layer hierarchy. Each pixel is represented by a random variables of the base layer. The second layer is formed by performing multiple unsupervised segmentations over the image, and associating one auxiliary variable with each segment. The children of each of these variables in x(2) are the variables contained within the segment, and pairwise connections are formed between adjacent segments. The third layer is formed in the same manner as the second layer by clustering the image segments. Further details are given in Ladicky et al. (2009).\nWe tested each algorithm on 295 test images, with an average of 70,000 pixels/variables in the base layer and up to 30,000 variables in a clique, and ran them either until convergence, or for a maximum of 500 iterations. In the table in figure 1 we compare the final energies obtained by each algorithm, showing the number of times they achieved an energy lower than or equal to all other methods, the average difference E(method)\u2212 E(min) and average ratio E(method)/E(min). Empirically, the message passing algorithms trw-s and bp appear ill-suited to inference over these dense hierarchical networks. In comparison to the graph cut based move making algorithms, they had higher resulting energy, higher memory usage, and exhibited slower convergence.\nWhile it may appear unreasonable to test message passing approaches on hierarchical energies when higher order formulations such as (Komodakis and Paragios, 2009; Potetz and Lee, 2008) exist, we note that for the simplest hierarchy that contains only one additional layer of nodes and no pairwise connections in this second layer, higher order and hierarchical message-passing approaches will be equivalent, as inference over the trees that represent higher order potentials is exact. Similar relative performance by message passing schemes was observed in these cases. Further, application of such approaches to the general form of (7) would require the computation of the exact min-marginals of E(2), a difficult problem in itself.\nIn all tested images both \u03b1-expansion variants outper-\nMethod Best E(meth)\u2212 E(min) E(meth) E(min)\nTime\nRange-exp 265 74.747887 1.000368 6.1s Range-swap 137 9033.847065 1.058777 19.8s \u03b1-expansion 109 255.500278 1.001604 6.3s\n\u03b1\u03b2 swap 42 9922.084163 1.060385 41.6s trw-s 12 38549.214994 1.239831 8.3min\nbp 6 13455.569713 1.081627 2min icm 5 45954.670836 1.277519 25.3s\nObra\u0301zek 1: Left Typical behaviour of all methods along with the lower bound obtained from trw-s an image from MSRC (Shotton et al., 2006) data set. The dashed lines at the right of the graph represent final converged solutions. Right Comparison of methods on 295 testing images. From left to right the columns show the number of times they achieved the best energy (including ties), the average difference (E(method) \u2212 E(min)), the average ratio (E(method)/E(min)) and the average time taken. All three approaches proposed by this paper: \u03b1-expansion under the reparameterisation of section 5, and the transformationally optimal range expansion and swap significantly outperformed existing inference methods both in speed and accuracy. See the supplementary materials for more examples.\nformed trw-s, bp and icm. These later methods only obtained minimal cost labellings in images in which the optimal solution found contained only one label i.e. they were entirely labelled as grass or water. The comparison also shows that unordered range move variants usually outperform vanilla move making algorithms. The higher number of minimal labellings found by the range-move variant of \u03b1\u03b2 swap in comparison to those of vanilla \u03b1-expansion can be explained by the large number of images in which two labels strongly dominate, as unlike standard \u03b1-expansion both range move algorithms are guaranteed to find the global optima of such a two label sub-problem (see section 5.2). The typical behaviour of all methods alongside the lower bound of trw-s can be seen in figure 1 and further, alongside qualitative results, in the supplementary materials."}, {"heading": "7 CONCLUSION", "text": "This paper shows that higher order amns are intimately related to pairwise hierarchical networks. This observation allowed us to characterise higher order potentials which can be solved under a novel reparameterisation using conventional move making expansion and swap algorithms, and derive bounds for such approaches. We also gave a new transformationally optimal family of algorithms for performing efficient inference in higher order amn that inherits such bounds.\nWe have demonstrated the usefulness of our algorithms on the problem of object class segmentation where they have been shown to outperform state of the art approaches over challenging data sets (Ladicky et al., 2009) both in speed and accuracy.\nReference Boykov, Y., Veksler, O. and Zabih, R. (2001), \u2018Fast approximate energy\nminimization via graph cuts\u2019, IEEE Transactions on Pattern Analysis and Machine Intelligence 23, 2001. 1, 4, 5\nGould, S., Amat, F. and Koller, D. (2009), Alphabet soup: A framework for approximate energy minimization, pp. 903\u2013910. 4, 5\nIshikawa, H. (2003), \u2018Exact optimization for markov random fields with convex priors\u2019, IEEE Transactions on Pattern Analysis and Machine Intelligence 25(10), 1333\u20131336. 5 Kohli, P., Kumar, M. and Torr, P. (2007), P3 and beyond: Solving energies with higher order cliques, in \u2018CVPR\u2019. 1, 2, 4, 5\nKohli, P., Ladicky, L. and Torr, P. (2008), Robust higher order potentials for enforcing label consistency, in \u2018CVPR\u2019. 1, 2, 4 Kolmogorov, V. (2006), \u2018Convergent tree-reweighted message passing for energy minimization.\u2019, IEEE Trans. Pattern Anal. Mach. Intell. 28(10), 1568\u20131583. 3 Kolmogorov, V. and Rother, C. (2006), C.: Comparison of energy minimization algorithms for highly connected graphs. in: Eccv, in \u2018In Proc. ECCV\u2019, pp. 1\u201315. 3 Komodakis, N. and Paragios, N. (2009), Beyond pairwise energies: Efficient optimization for higher-order mrfs, in \u2018CVPR09\u2019, pp. 2985\u20132992. 1, 7 Kschischang, F. R., Member, S., Frey, B. J. and andrea Loeliger, H. (2001), \u2018Factor graphs and the sum-product algorithm\u2019, IEEE Transactions on Information Theory 47, 498\u2013519. 3 Kumar, M. P. and Koller, D. (2009), MAP estimation of semi-metric MRFs via hierarchical graph cuts, in \u2018Proceedings of the Conference on Uncertainity in Artificial Intelligence\u2019. 1 Kumar, M. P. and Torr, P. H. S. (2008), Improved moves for truncated convex models, in \u2018Proceedings of Advances in Neural Information Processing Systems\u2019. 1, 4, 5 Ladicky, L., Russell, C., Kohli, P. and Torr, P. H. (2009), Associative hierarchical crfs for object class image segmentation, in \u2018International Conference on Computer Vision\u2019. 1, 3, 7, 8 Ladicky, L., Russell, C., Sturgess, P., Alahri, K. and Torr, P. (2010), What, where and how many? combining object detectors and crfs, in \u2018ECCV\u2019, IEEE. 1 Lan, X., Roth, S., Huttenlocher, D. and Black, M. (2006), Efficient belief propagation with learned higher-order markov random fields., in \u2018ECCV (2)\u2019, pp. 269\u2013282. 4 Potetz, B. and Lee, T. S. (2008), \u2018Efficient belief propegation for higher order cliques using linear constraint nodes\u2019. 4, 7 Roth, S. and Black, M. (2005), Fields of experts: A framework for learning image priors., in \u2018CVPR\u2019, pp. 860\u2013867. 1 Shotton, J., Winn, J., Rother, C. and Criminisi, A. (2006), TextonBoost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation., in \u2018ECCV\u2019, pp. 1\u201315. 7, 8 Sontag, D., Meltzer, T., Globerson, A., Jaakkola, T. and Weiss, Y. . (2008), Tightening lp relaxations for map using message passing, in \u2018UAI\u2019. 3 Szeliski, R., Zabih, R., Scharstein, D., Veksler, O., Kolmogorov, V., Agarwala, A., Tappen, M. and Rother, C. (2006), A comparative study of energy minimization methods for markov random fields., in \u2018ECCV (2)\u2019, pp. 16\u201329. 1, 3 Tarlow, D., Givoni, I. and Zemel, R. (2010), Hop-map: Efficient message passing with high order potentials, in \u2018Artificial Intelligence and Statistics\u2019. 4 Tarlow, D., Zemel, R. and Frey, B. (2008), Flexible priors for exemplarbased clustering, in \u2018Uncertainty in Artificial Intelligence (UAI)\u2019. 4 Taskar, B., Chatalbashev, V. and Koller, D. (2004), Learning associative markov networks, in \u2018Proc. ICML\u2019, ACM Press, p. 102. 1, 2 Veksler, O. (2007), Graph cut based optimization for mrfs with truncated convex priors, pp. 1\u20138. 4, 5 Vicente, S., Kolmogorov, V. and Rother, C. (2009), Joint optimization of segmentation and appearance models, in \u2018ICCV\u2019, IEEE. 1 Wainwright, M. and Jordan, M. (2008), \u2018Graphical Models, Exponential Families, and Variational Inference\u2019, Foundations and Trends in Machine Learning 1(1-2), 1\u2013305. 3 Weiss, Y. and Freeman, W. (2001), \u2018On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs.\u2019, Transactions on Information Theory . 3 Werner, T. (2009), High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf), in \u2018CVPR\u2019. 3"}], "references": [{"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Boykov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2001}, {"title": "Alphabet soup: A framework for approximate energy minimization", "author": ["S. Gould", "F. Amat", "D. Koller"], "venue": "pp. 903\u2013910", "citeRegEx": "Gould et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gould et al\\.", "year": 2009}, {"title": "Exact optimization for markov random fields with convex priors", "author": ["H. Ishikawa"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Ishikawa,? \\Q2003\\E", "shortCiteRegEx": "Ishikawa", "year": 2003}, {"title": "Robust higher order potentials for enforcing label consistency, in \u2018CVPR", "author": ["P. Kohli", "L. Ladicky", "P. Torr"], "venue": null, "citeRegEx": "Kohli et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kohli et al\\.", "year": 2008}, {"title": "Convergent tree-reweighted message passing for energy minimization.", "author": ["V. Kolmogorov"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 28(10),", "citeRegEx": "Kolmogorov,? \\Q2006\\E", "shortCiteRegEx": "Kolmogorov", "year": 2006}, {"title": "Comparison of energy minimization algorithms for highly connected graphs", "author": ["V. Kolmogorov", "Rother", "C. C"], "venue": "in: Eccv, in \u2018In Proc. ECCV\u2019,", "citeRegEx": "Kolmogorov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kolmogorov et al\\.", "year": 2006}, {"title": "Beyond pairwise energies: Efficient optimization for higher-order mrfs, in \u2018CVPR09", "author": ["N. Komodakis", "N. Paragios"], "venue": "pp. 2985\u20132992", "citeRegEx": "Komodakis and Paragios,? \\Q2009\\E", "shortCiteRegEx": "Komodakis and Paragios", "year": 2009}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "S. Member", "B.J. Frey", "H. andrea Loeliger"], "venue": "IEEE Transactions on Information Theory 47,", "citeRegEx": "Kschischang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kschischang et al\\.", "year": 2001}, {"title": "MAP estimation of semi-metric MRFs via hierarchical graph cuts, in \u2018Proceedings of the Conference on Uncertainity in Artificial Intelligence", "author": ["M.P. Kumar", "D. Koller"], "venue": null, "citeRegEx": "Kumar and Koller,? \\Q2009\\E", "shortCiteRegEx": "Kumar and Koller", "year": 2009}, {"title": "Improved moves for truncated convex models, in \u2018Proceedings of Advances in Neural Information", "author": ["M.P. Kumar", "P.H.S. Torr"], "venue": "Processing Systems\u2019", "citeRegEx": "Kumar and Torr,? \\Q2008\\E", "shortCiteRegEx": "Kumar and Torr", "year": 2008}, {"title": "Associative hierarchical crfs for object class image segmentation, in \u2018International", "author": ["L. Ladicky", "C. Russell", "P. Kohli", "P.H. Torr"], "venue": "Conference on Computer Vision\u2019", "citeRegEx": "Ladicky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ladicky et al\\.", "year": 2009}, {"title": "What, where and how many? combining object detectors and crfs, in \u2018ECCV", "author": ["L. Ladicky", "C. Russell", "P. Sturgess", "K. Alahri", "P. Torr"], "venue": null, "citeRegEx": "Ladicky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ladicky et al\\.", "year": 2010}, {"title": "Efficient belief propagation with learned higher-order markov random fields., in \u2018ECCV", "author": ["X. Lan", "S. Roth", "D. Huttenlocher", "M. Black"], "venue": null, "citeRegEx": "Lan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2006}, {"title": "Efficient belief propegation for higher order cliques using linear constraint nodes", "author": ["B. Potetz", "T.S. Lee"], "venue": null, "citeRegEx": "Potetz and Lee,? \\Q2008\\E", "shortCiteRegEx": "Potetz and Lee", "year": 2008}, {"title": "Fields of experts: A framework for learning image priors., in \u2018CVPR", "author": ["S. Roth", "M. Black"], "venue": null, "citeRegEx": "Roth and Black,? \\Q2005\\E", "shortCiteRegEx": "Roth and Black", "year": 2005}, {"title": "TextonBoost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation", "author": ["J. Shotton", "J. Winn", "C. Rother", "A. Criminisi"], "venue": "in \u2018ECCV\u2019,", "citeRegEx": "Shotton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shotton et al\\.", "year": 2006}, {"title": "Tightening lp relaxations for map using message passing, in \u2018UAI", "author": ["D. Sontag", "T. Meltzer", "A. Globerson", "T. Jaakkola", "Y. Weiss"], "venue": null, "citeRegEx": "Sontag et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2008}, {"title": "A comparative study of energy minimization methods for markov random fields", "author": ["R. Szeliski", "R. Zabih", "D. Scharstein", "O. Veksler", "V. Kolmogorov", "A. Agarwala", "M. Tappen", "C. Rother"], "venue": "\u2018ECCV", "citeRegEx": "Szeliski et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Szeliski et al\\.", "year": 2006}, {"title": "Hop-map: Efficient message passing with high order potentials, in \u2018Artificial Intelligence and Statistics", "author": ["D. Tarlow", "I. Givoni", "R. Zemel"], "venue": null, "citeRegEx": "Tarlow et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tarlow et al\\.", "year": 2010}, {"title": "Flexible priors for exemplarbased clustering, in \u2018Uncertainty in Artificial Intelligence (UAI)", "author": ["D. Tarlow", "R. Zemel", "B. Frey"], "venue": null, "citeRegEx": "Tarlow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tarlow et al\\.", "year": 2008}, {"title": "Learning associative markov networks, in \u2018Proc", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller"], "venue": "ICML\u2019, ACM Press,", "citeRegEx": "Taskar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "Graph cut based optimization for mrfs with truncated convex priors", "author": ["O. Veksler"], "venue": "pp. 1\u20138", "citeRegEx": "Veksler,? \\Q2007\\E", "shortCiteRegEx": "Veksler", "year": 2007}, {"title": "Joint optimization of segmentation and appearance models, in \u2018ICCV", "author": ["S. Vicente", "V. Kolmogorov", "C. Rother"], "venue": null, "citeRegEx": "Vicente et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Vicente et al\\.", "year": 2009}, {"title": "Graphical Models, Exponential Families, and Variational Inference", "author": ["M. Wainwright", "M. Jordan"], "venue": "Foundations and Trends in Machine Learning 1(1-2),", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs.", "author": ["Y. Weiss", "W. Freeman"], "venue": "Transactions on Information Theory ", "citeRegEx": "Weiss and Freeman,? \\Q2001\\E", "shortCiteRegEx": "Weiss and Freeman", "year": 2001}, {"title": "High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf), in \u2018CVPR", "author": ["T. Werner"], "venue": null, "citeRegEx": "Werner,? \\Q2009\\E", "shortCiteRegEx": "Werner", "year": 2009}], "referenceMentions": [{"referenceID": 17, "context": "This interest has led to a large amount of work on the problem of estimating the maximum a posteriori (map) solution of a random field (Szeliski et al., 2006).", "startOffset": 135, "endOffset": 158}, {"referenceID": 20, "context": "Of particular interest are the families of associative pairwise potentials (Taskar et al., 2004), in which connected variables are assumed to be more likely than not to share the same label.", "startOffset": 75, "endOffset": 96}, {"referenceID": 9, "context": "Inference algorithms targeting these associative potentials, which include truncated convex costs (Kumar and Torr, 2008), metrics (Boykov et al.", "startOffset": 98, "endOffset": 120}, {"referenceID": 0, "context": "Inference algorithms targeting these associative potentials, which include truncated convex costs (Kumar and Torr, 2008), metrics (Boykov et al., 2001), and semi metrics (Kumar and Koller, 2009), often carry bounds which guarantee the cost of the solution found must lie within a bound, specified as a fixed factor of n of the cost of the minimal solution.", "startOffset": 130, "endOffset": 151}, {"referenceID": 8, "context": ", 2001), and semi metrics (Kumar and Koller, 2009), often carry bounds which guarantee the cost of the solution found must lie within a bound, specified as a fixed factor of n of the cost of the minimal solution.", "startOffset": 26, "endOffset": 50}, {"referenceID": 14, "context": "those with a clique size greater than two) have been used to obtain impressive results for a number of challenging problems in computer vision (Roth and Black, 2005; Komodakis and Paragios, 2009; Vicente et al., 2009; Ladicky et al., 2010), the problem of bounded higher order inference has been largely ignored.", "startOffset": 143, "endOffset": 239}, {"referenceID": 6, "context": "those with a clique size greater than two) have been used to obtain impressive results for a number of challenging problems in computer vision (Roth and Black, 2005; Komodakis and Paragios, 2009; Vicente et al., 2009; Ladicky et al., 2010), the problem of bounded higher order inference has been largely ignored.", "startOffset": 143, "endOffset": 239}, {"referenceID": 22, "context": "those with a clique size greater than two) have been used to obtain impressive results for a number of challenging problems in computer vision (Roth and Black, 2005; Komodakis and Paragios, 2009; Vicente et al., 2009; Ladicky et al., 2010), the problem of bounded higher order inference has been largely ignored.", "startOffset": 143, "endOffset": 239}, {"referenceID": 11, "context": "those with a clique size greater than two) have been used to obtain impressive results for a number of challenging problems in computer vision (Roth and Black, 2005; Komodakis and Paragios, 2009; Vicente et al., 2009; Ladicky et al., 2010), the problem of bounded higher order inference has been largely ignored.", "startOffset": 143, "endOffset": 239}, {"referenceID": 10, "context": "In this paper, we address the problem of performing graph cut based inference in a new model: the Associative Hierarchical Networks (ahns) (Ladicky et al., 2009), which includes the higher order Associative Markov Networks (amns) (Taskar et al.", "startOffset": 139, "endOffset": 161}, {"referenceID": 20, "context": ", 2009), which includes the higher order Associative Markov Networks (amns) (Taskar et al., 2004) or P potentials (Kohli et al.", "startOffset": 76, "endOffset": 97}, {"referenceID": 3, "context": ", 2007) and the Robust P (Kohli et al., 2008) model as special cases, and derive a bound of 4.", "startOffset": 25, "endOffset": 45}, {"referenceID": 10, "context": "Note that in our earlier work Ladicky et al. (2009), the problem of inference is not discussed at all; it shows how these hierarchical models can be used for scene understanding, and how learning is possible under the assumption that the model is tractable.", "startOffset": 30, "endOffset": 52}, {"referenceID": 19, "context": "Existing higher-order models Taskar et al. (2004) proposed the use of higher order potentials that encourage the entirety of a clique to take some label, and discusses how they can be applied to predicting protein interactions and document classification.", "startOffset": 29, "endOffset": 50}, {"referenceID": 3, "context": "A generalisation of this approach was proposed by Kohli et al. (2008), who observed that in the image labelling problem, most (but not all) pixels belonging to image segments computed using an unsupervised clustering/segmentation algorithm take the same object label.", "startOffset": 50, "endOffset": 70}, {"referenceID": 7, "context": "In general, every higher order cost function can be converted to a 2\u2212layer associative hierarchical network by taking an approach analogous to that of factor graphs (Kschischang et al., 2001) and adding a single multi-state auxiliary variable.", "startOffset": 165, "endOffset": 191}, {"referenceID": 23, "context": "However, to do this for general higher order functions requires the addition of an auxiliary variable with an exponential sized label set (Wainwright and Jordan, 2008).", "startOffset": 138, "endOffset": 167}, {"referenceID": 10, "context": "While the hierarchical formulation of both Taskar\u2019s and Kohli\u2019s models can be understood as a mathematical convenience that allows for fast and efficient bounded inference, our earlier work (Ladicky et al., 2009) used it for true multi-scale inference, modelling constraints defined over many quantisations of the image.", "startOffset": 190, "endOffset": 212}, {"referenceID": 17, "context": "Inference in Pairwise Networks Although the problem of map inference is NP-hard for most associative pairwise functions defined over more than two labels, in real world problems many conventional algorithms provide near optimal solutions over grid connected networks (Szeliski et al., 2006).", "startOffset": 267, "endOffset": 290}, {"referenceID": 24, "context": "However, the dense structure of hierarchical networks results in frustrated cycles and makes traditional reparameterisation based message passing algorithms for map inference such as loopy belief propagation (Weiss and Freeman, 2001) and tree-reweighted message passing (Kolmogorov, 2006) slow to converge and unsuitable (Kolmogorov and Rother, 2006).", "startOffset": 208, "endOffset": 233}, {"referenceID": 4, "context": "However, the dense structure of hierarchical networks results in frustrated cycles and makes traditional reparameterisation based message passing algorithms for map inference such as loopy belief propagation (Weiss and Freeman, 2001) and tree-reweighted message passing (Kolmogorov, 2006) slow to converge and unsuitable (Kolmogorov and Rother, 2006).", "startOffset": 270, "endOffset": 288}, {"referenceID": 16, "context": "Many of these frustrated cycles can be eliminated via the use of cycle inequalities (Sontag et al., 2008; Werner, 2009), but only by significantly increasing the run time of the algorithm.", "startOffset": 84, "endOffset": 119}, {"referenceID": 25, "context": "Many of these frustrated cycles can be eliminated via the use of cycle inequalities (Sontag et al., 2008; Werner, 2009), but only by significantly increasing the run time of the algorithm.", "startOffset": 84, "endOffset": 119}, {"referenceID": 0, "context": "Examples of move making algorithms include \u03b1expansion which can only be applied to metrics, \u03b1\u03b2 swap which can be applied to semi-metrics (Boykov et al., 2001), and range moves (Kumar and Torr, 2008; Veksler, 2007) for truncated convex potentials.", "startOffset": 137, "endOffset": 158}, {"referenceID": 9, "context": ", 2001), and range moves (Kumar and Torr, 2008; Veksler, 2007) for truncated convex potentials.", "startOffset": 25, "endOffset": 62}, {"referenceID": 21, "context": ", 2001), and range moves (Kumar and Torr, 2008; Veksler, 2007) for truncated convex potentials.", "startOffset": 25, "endOffset": 62}, {"referenceID": 0, "context": "Examples of move making algorithms include \u03b1expansion which can only be applied to metrics, \u03b1\u03b2 swap which can be applied to semi-metrics (Boykov et al., 2001), and range moves (Kumar and Torr, 2008; Veksler, 2007) for truncated convex potentials. These moves differ in the size of the space searched for the optimal move. While expansion and swap search a space of size at most 2 while minimising a function of n variables, the range moves explores a much larger space of K where K is a parameter of the energy (see Veksler (2007) for more details).", "startOffset": 138, "endOffset": 531}, {"referenceID": 0, "context": "These methods start from an arbitrary initial solution of the problem and proceed by making a series of changes each of which leads to a solution of the same or lower energy (Boykov et al., 2001).", "startOffset": 174, "endOffset": 195}, {"referenceID": 12, "context": "Lan et al. (2006) proposed approximation methods for bp to make efficient inference possible in higher order mrfs.", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "Lan et al. (2006) proposed approximation methods for bp to make efficient inference possible in higher order mrfs. This was followed by the recent works of Potetz and Lee (2008); Tarlow et al.", "startOffset": 0, "endOffset": 178}, {"referenceID": 1, "context": "This was used to formulate higher order expansion and swap move making algorithms The only existing work that addresses the problem of bounded higher order inference is (Gould et al., 2009) which showed how theoretical bounds could be derived given move making algorithms that proposed optimal moves by exactly solving some sub-problem.", "startOffset": 169, "endOffset": 189}, {"referenceID": 3, "context": "potentials \u03c8 (n) c (x (n\u22121) c , x (n) c ) to that of the weighted Robust P model (Kohli et al., 2008) (see (4)), we can apply \u03b1-expansion to the pairwise form of the ahn.", "startOffset": 81, "endOffset": 101}, {"referenceID": 0, "context": "This requires a transform of all functions in the pairwise representation, so that they can be representable as a metric (Boykov et al., 2001).", "startOffset": 121, "endOffset": 142}, {"referenceID": 0, "context": "By the work of Boykov et al. (2001), the \u03b1-expansion algorithm is guaranteed to find a solution within a factor of 2 max ( 2,maxE\u2208E1 maxxi,xj\u2208L \u03c8E(xi,xj) minxi,xj\u2208L \u03c8E(xi,xj) ) (i.", "startOffset": 15, "endOffset": 36}, {"referenceID": 1, "context": "By way of comparison, the work of Gould et al. (2009) provides a bound of 2|c| for the higher order potentials of the strict P model (Kohli et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 0, "context": "Let us consider a generalisation of the swap and expansion moves proposed in Boykov et al. (2001). In a standard swap move, the set of all moves considered is those in which a subset of the variables currently taking label \u03b1 or \u03b2 change labels to either \u03b2 or \u03b1.", "startOffset": 77, "endOffset": 98}, {"referenceID": 20, "context": "This approach can be seen as a variant on the ordered range moves proposed in Veksler (2007); Kumar and Torr (2008), however while these works require that an ordering of the labels {l1, l2, .", "startOffset": 78, "endOffset": 93}, {"referenceID": 9, "context": "This approach can be seen as a variant on the ordered range moves proposed in Veksler (2007); Kumar and Torr (2008), however while these works require that an ordering of the labels {l1, l2, .", "startOffset": 94, "endOffset": 116}, {"referenceID": 2, "context": "Hence, we can use the Ishikawa construct (Ishikawa, 2003) to minimise the swap move energy to find the optimal move.", "startOffset": 41, "endOffset": 57}, {"referenceID": 15, "context": "We evaluate \u03b1-expansion, \u03b1\u03b2 swap, trw-s, Belief Propagation, Iterated Conditional Modes, and both the expansion and swap based variants of our unordered range moves on the problem of object class segmentation over the MSRC data-set (Shotton et al., 2006), in which each pixel within an image must be assigned a label representing its class, such as grass, water, boat or cow.", "startOffset": 232, "endOffset": 254}, {"referenceID": 10, "context": "Further details are given in Ladicky et al. (2009).", "startOffset": 29, "endOffset": 51}, {"referenceID": 6, "context": "While it may appear unreasonable to test message passing approaches on hierarchical energies when higher order formulations such as (Komodakis and Paragios, 2009; Potetz and Lee, 2008) exist, we note that for the simplest hierarchy that contains only one additional layer of nodes and no pairwise connections in this second layer, higher order and hierarchical message-passing approaches will be equivalent, as inference over the trees that represent higher order potentials is exact.", "startOffset": 132, "endOffset": 184}, {"referenceID": 13, "context": "While it may appear unreasonable to test message passing approaches on hierarchical energies when higher order formulations such as (Komodakis and Paragios, 2009; Potetz and Lee, 2008) exist, we note that for the simplest hierarchy that contains only one additional layer of nodes and no pairwise connections in this second layer, higher order and hierarchical message-passing approaches will be equivalent, as inference over the trees that represent higher order potentials is exact.", "startOffset": 132, "endOffset": 184}, {"referenceID": 15, "context": "Obr\u00e1zek 1: Left Typical behaviour of all methods along with the lower bound obtained from trw-s an image from MSRC (Shotton et al., 2006) data set.", "startOffset": 115, "endOffset": 137}, {"referenceID": 10, "context": "We have demonstrated the usefulness of our algorithms on the problem of object class segmentation where they have been shown to outperform state of the art approaches over challenging data sets (Ladicky et al., 2009) both in speed and accuracy.", "startOffset": 194, "endOffset": 216}, {"referenceID": 21, "context": ", Veksler, O. and Zabih, R. (2001), \u2018Fast approximate energy minimization via graph cuts\u2019, IEEE Transactions on Pattern Analysis and Machine Intelligence 23, 2001.", "startOffset": 2, "endOffset": 35}, {"referenceID": 4, "context": ", Kolmogorov, V., Agarwala, A., Tappen, M. and Rother, C. (2006), A comparative study of energy minimization methods for markov random fields.", "startOffset": 2, "endOffset": 65}, {"referenceID": 4, "context": ", Kolmogorov, V. and Rother, C. (2009), Joint optimization of segmentation and appearance models, in \u2018ICCV\u2019, IEEE.", "startOffset": 2, "endOffset": 39}], "year": 2010, "abstractText": "Markov Networks are widely used through out computer vision and machine learning. An important subclass are the Associative Markov Networks which are used in a wide variety of applications. For these networks a good approximate minimum cost solution can be found efficiently using graph cut based move making algorithms such as alphaexpansion. Recently a related model has been proposed, the associative hierarchical network, which provides a natural generalisation of the Associative Markov Network for higher order cliques (i.e. clique size greater than two). This method provides a good model for object class segmentation problem in computer vision. Within this paper we briefly describe the associative hierarchical network and provide a computationally efficient method for approximate inference based on graph cuts. Our method performs well for networks containing hundreds of thousand of variables, and higher order potentials are defined over cliques containing tens of thousands of variables. Due to the size of these problems standard linear programming techniques are inapplicable. We show that our method has a bound of 4 for the solution of general associative hierarchical network with arbitrary clique size noting that few results on bounds exist for the solution of labelling of Markov Networks with higher order cliques.", "creator": "LaTeX with hyperref package"}}}