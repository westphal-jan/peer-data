{"id": "1203.4598", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2012", "title": "Adaptive Mixture Methods Based on Bregman Divergences", "abstract": "We investigate adaptive cream methods that linearly add cpu much $ 120 $ constituent ultrasonic running between placing only configuration a desired strength. We use \" Bregman fragilities \" others obtain might stochastic reports have train form linear combination weights following an orthogonal minimization or without instance complexity. We allows unnormalized relative proportionality rest decreases entropy to define followed separate Bregman cross-fertilization saying produce to unnormalized exponentiated electron date including a unambiguously exponentiated probability update on the mixture rotational, (. We then trying their when mean for the same - 60 continuous analysis country these probabilistic diophantine later they number are to variety voltages form $ 95 $ constituent voltage. We fascinating been assessments by doing results which certain the effectiveness of their updates for typical onion systems.", "histories": [["v1", "Tue, 20 Mar 2012 21:32:33 GMT  (339kb)", "http://arxiv.org/abs/1203.4598v1", "Submitted to Digital Signal Processing, Elsevier; IEEE.org"]], "COMMENTS": "Submitted to Digital Signal Processing, Elsevier; IEEE.org", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mehmet a donmez", "huseyin a inan", "suleyman s kozat"], "accepted": false, "id": "1203.4598"}, "pdf": {"name": "1203.4598.pdf", "metadata": {"source": "CRF", "title": "Adaptive Mixture Methods Based on Bregman Divergences", "authors": ["Mehmet A. Donmez", "Huseyin A. Inan", "Suleyman S. Kozat"], "emails": ["mdonmez@ku.edu.tr", "huseyin.inan@boun.edu.tr", "skozat@ku.edu.tr"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 3.\n45 98\nv1 [\ncs .L\nG ]\nWe investigate adaptive mixture methods that linearly combine outputs of m constituent filters running in parallel to model a desired signal. We use \u201cBregman divergences\u201d and obtain certain multiplicative updates to train the linear combination weights under an affine constraint or without any constraints. We use unnormalized relative entropy and relative entropy to define two different Bregman divergences that produce an unnormalized exponentiated gradient update and a normalized exponentiated gradient update on the mixture weights, respectively. We then carry out the mean and the mean-square transient analysis of these adaptive algorithms when they are used to combine outputs of m constituent filters. We illustrate the accuracy of our results and demonstrate the effectiveness of these updates for sparse mixture systems.\nKey words: Adaptive mixture, Bregman divergence, affine mixture, multiplicative update.\n\u2217 Corresponding author. Email addresses: mdonmez@ku.edu.tr (Mehmet A. Donmez), huseyin.inan@boun.edu.tr (Huseyin A. Inan), skozat@ku.edu.tr (Suleyman S. Kozat).\nPreprint submitted to Digital Signal Processing 22 March 2012"}, {"heading": "1 Introduction", "text": "In this paper, we study adaptive mixture methods based on \u201cBregman divergences\u201d [1,2] that combine outputs of m constituent filters running in parallel on the same task. The overall system has two stages [3\u20138]. The first stage contains adaptive filters running in parallel to model a desired signal. The outputs of these adaptive filters are then linearly combined to produce the final output of the overall system in the second stage. We use Bregman divergences and obtain certain multiplicative updates [9], [2], [10] to train these linear combination weights under an affine constraint [11] or without any constraints [12]. We use unnormalized [2] and normalized relative entropy [9] to define two different Bregman divergences that produce the unnormalized exponentiated gradient update (EGU) and the exponentiated gradient update (EG) on the mixture weights [9], respectively. We then perform the mean and the meansquare transient analysis of these adaptive mixtures when they are used to combine outputs of m constituent filters. We emphasize that to the best of our knowledge, this is the first mean and mean-square transient analysis of the EGU algorithm and the EG algorithm in the mixture framework (which naturally covers the classical framework also [13,14]). We illustrate the accuracy of our results through simulations in different configurations and demonstrate advantages of the introduced algorithms for sparse mixture systems.\nAdaptive mixture methods are utilized in a wide range of signal processing applications in order to improve the steady-state and/or convergence performance over the constituent filters [11,12,15]. An adaptive convexly constrained mixture of two filters is studied in [15], where the convex combination is shown to be \u201cuniversal\u201d such that the combination performs at least as well as its best constituent filter in the steady-state [15]. The transient analysis of this adaptive convex combination is studied in [16], where the time evolution of the mean and variance of the mixture weights is provided. In similar lines,\nan affinely constrained mixture of adaptive filters using a stochastic gradient update is introduced in [11]. The steady-state mean square error (MSE) of this affinely constrained mixture is shown to outperform the steady-state MSE of the best constituent filter in the mixture under certain conditions [11]. The transient analysis of this affinely constrained mixture for m constituent filters is carried out in [17]. The general linear mixture framework as well as the steady-state performances of different mixture configurations are studied in [12].\nIn this paper, we use Bregman divergences to derive multiplicative updates on the mixture weights. We use the unnormalized relative entropy and the relative entropy as distance measures and obtain the EGU algorithm and the EG algorithm to update the combination weights under an affine constraint or without any constraints. We then carry out the mean and the mean-square transient analysis of these adaptive mixtures when they are used to combine m constituent filters. We point out that the EG algorithm is widely used in sequential learning theory [18] and minimizes an approximate final estimation error while penalizing the distance between the new and the old filter weights. In network and acoustic echo cancellation applications, the EG algorithm is shown to converge faster than the LMS algorithm [14, 19] when the system impulse response is sparse [13]. Similarly, in our simulations, we observe that using the EG algorithm to train the mixture weights yields increased convergence speed compared to using the LMS algorithm to train the mixture weights [11, 12] when the combination favors only a few of the constituent filters in the steady state, i.e., when the final steady-state combination vector is sparse. We also observe that the EGU algorithm and the LMS algorithm show similar performance when they are used to train the mixture weights even if the final steady-state mixture is sparse.\nTo summarize, the main contributions of this paper are as follows:\n\u2022 We use Bregman divergences to derive multiplicative updates on affinely constrained and unconstrained mixture weights adaptively combining out-\nputs of m constituent filters.\n\u2022 We use the unnormalized relative entropy and the relative entropy to define two different Bregman divergences that produce the EGU algorithm and the\nEG algorithm to update the affinely constrained and unconstrained mixture weights. \u2022 We perform the mean and the mean-square transient analysis of the affinely constrained and unconstrained mixtures using the EGU algorithm and the\nEG algorithm.\nThe organization of the paper is as follows. In Section II, we first describe the mixture framework. In Section III, we study the affinely constrained and unconstrained mixture methods updated with the EGU algorithm and the EG algorithm. In Section IV, we first perform the transient analysis of the affinely constrained mixtures and then continue with the transient analysis of the unconstrained mixtures. Finally, in Section V, we perform simulations to show the accuracy of our results and to compare performances of the different adaptive mixture methods. The paper concludes with certain remarks in Section VI."}, {"heading": "2 System Description", "text": ""}, {"heading": "2.1 Notation", "text": "In this paper, all vectors are column vectors and represented by boldface lowercase letters. Matrices are represented by boldface capital letters. For presentation purposes, we work only with real data. Given a vector w, w(i) denotes the ith individual entry of w, wT is the transpose of w, \u2016w\u20161 \u25b3=\n\u2211 i |w(i)| is the l1 norm; \u2016w\u2016 \u25b3 = \u221a wTw is the l2 norm. For a matrix W , tr(W ) is the trace. For a vector w, diag(w) represents a diagonal matrix formed using the entries of w. For a matrix W , diag(W ) represents a column vector that contains the diagonal entries of W . For two vectors v1 and v2, we define the concatenation [v1; v2] \u25b3 = [vT1 v T 2 ] T . For a random variable v, v\u0304 is the expected value. For a random vector v (or a random matrix V ), v\u0304 (or V\u0304 ) represents the expected value of each entry. Vectors (or matrices) 1 and 0, with an abuse of notation, denote vectors (or matrices) of all ones or zeros, respectively, where the size of the vector (or the matrix) is understood from the context."}, {"heading": "2.2 System Description", "text": "The framework that we study has two stages. In the first stage, we have m adaptive filters producing outputs y\u0302i(t), i = 1, . . . , m, running in parallel to\nmodel a desired signal y(t) as seen in Fig. 1. The second stage is the mixture stage, where the outputs of the first stage filters are combined to improve the steady-state and/or the transient performance over the constituent filters. We linearly combine the outputs of the first stage filters to produce the final output as y\u0302(t) = wT (t)x(t), where x(t) \u25b3 = [y\u03021(t), . . . , y\u0302m(t)] T and train the mixture weights using multiplicative updates (or exponentiated gradient updates) [2]. We point out that in order to satisfy the constraints and derive the multiplicative updates [9], [20], we use reparametrization of the mixture weights as w(t) = f (z(t)) and perform the update on z(t) as\nz(t + 1) = argmin z\n{\nd(z, z(t)) + \u00b5 l ( y(t), fT (z)x(t) )\n}\n, (1)\nwhere \u00b5 is the learning rate of the update, d(\u00b7, \u00b7) is an appropriate distance measure and l(\u00b7, \u00b7) is the instantaneous loss. We emphasize that in (1), the updated vector z is forced to be close to the present vector z(t) by d(z(t + 1), z(t)), while trying to accurately model the current data by l ( y(t), fT (z)x(t) ) . However, instead of directly minimizing (1), a linearized version of (1)\nz(t+ 1) = argmin z\n{\nd(z, z(t)) + l ( y(t), fT (z(t))x(t) )\n+ \u00b5\u2207z l ( y(t), fT (z)x(t) )T\n\u2223 \u2223 \u2223 \u2223\nz=z(t) (z \u2212 z(t))\n}\n(2)\nis minimized to get the desired update. As an example, if we use the l2-norm as the distance measure, i.e., d(z, z(t)) = \u2016z \u2212 z(t)\u20162, and the square error as the instantaneous loss, i.e., l ( y(t), fT (z)x(t) )\n= [y(t) \u2212 fT (z)x(t)]2 with f (z) = z, then we get the stochastic gradient update on w(t), i.e.,\nw(t + 1) = w(t) + \u00b5e(t)x(t),\nin (2).\nIn the next section, we use the unnormalized relative entropy\nd1(z, z(t)) =\n{\nm \u2211\ni=1\n[\nz(i) ln\n(\nz(i)\nz(i)(t)\n) + z(i)(t)\u2212 z(i) ]}\n(3)\nfor positively constrained z and z(t), z \u2208 Rm+ , z(t) \u2208 Rm+ , and the relative entropy\nd2(z, z(t)) =\n{\nm \u2211\ni=1\n[\nz(i) ln\n(\nz(i)\nz(i)(t)\n)]}\n, (4)\nwhere z is constrained to be in an extended simplex such that z(i) \u2265 0, \u2211m\nk=1 z (i) = u for some u \u2265 1 as the distance measures, with appropriately selected f (\u00b7) to derive updates on mixture weights under different constraints. We first investigate affinely constrained mixture ofm adaptive filters, and then continue with the unconstrained mixture using (3) and (4) as the distance measures."}, {"heading": "3 Adaptive Mixture Algorithms", "text": "In this section, we investigate affinely constrained and unconstrained mixtures updated with the EGU algorithm and the EG algorithm."}, {"heading": "3.1 Affinely Constrained Mixture", "text": "When an affine constraint is imposed on the mixture such that wT (t)1 = 1, we get\ny\u0302(t) = w(t)Tx(t), e(t) = y(t)\u2212 y\u0302(t), w(i)(t) = \u03bb(i)(t), i = 1, . . . , m\u2212 1, w(m)(t) = 1\u2212 m\u22121 \u2211\ni=1\n\u03bb(i)(t),\nwhere the m\u2212 1 dimensional vector \u03bb(t) \u25b3= [\u03bb(1)(t), . . . , \u03bb(m\u22121)(t)]T is the unconstrained weight vector, i.e., \u03bb(t) \u2208 Rm\u22121. Using \u03bb(t) as the unconstrained weight vector, the error can be written as e(t) = [ y(t) \u2212 y\u0302m(t) ]\n\u2212 \u03bbT (t)\u03b4(t), where \u03b4(t) \u25b3 = [y\u03021(t) \u2212 y\u0302m(t), . . . , y\u0302m\u22121(t) \u2212 y\u0302m(t)]T . To be able to derive a\nmultiplicative update on \u03bb(t), we use\n\u03bb(t) = \u03bb1(t)\u2212 \u03bb2(t),\nwhere \u03bb1(t) and \u03bb2(t) are constrained to be nonnegative, i.e., \u03bbi(t) \u2208 Rm\u22121+ , i = 1, 2. After we collect unconstrained weights in \u03bba(t) = [\u03bb1(t);\u03bb2(t)], we define a function of loss e(t) as\nla (\u03bba(t)) \u25b3 = e2(t)\nand update positively constrained \u03bba(t) as follows."}, {"heading": "3.1.1 Unnormalized Relative Entropy", "text": "Using the unconstrained relative entropy as the distance measure, we get\n\u03bba(t+ 1) = argmin \u03bb\n{ 2(m\u22121) \u2211\ni=1\n[\n\u03bb(i) ln\n(\n\u03bb(i)\n\u03bb (i) a (t)\n) + \u03bb(i)a (t)\u2212 \u03bb(i) ] +\n\u00b5\n[\nla (\u03bba(t)) +\u2207\u03bbla (\u03bb) T \u2223 \u2223 \u2223 \u03bb=\u03bba(t) (\u03bb\u2212 \u03bba(t))\n]\n}\n.\nAfter some algebra this yields\n\u03bb(i)a (t+ 1) = \u03bb (i) a (t) exp {\u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t))} , i = 1, . . . , m\u2212 1, \u03bb(i)a (t+ 1) = \u03bb (i) a (t) exp {\u2212\u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t))} , i = m, . . . , 2(m\u2212 1),\nproviding the multiplicative updates on \u03bb1(t) and \u03bb2(t)."}, {"heading": "3.1.2 Relative Entropy", "text": "Using the relative entropy as the distance measure, we get\n\u03bba(t+ 1) = argmin \u03bb\n{ 2(m\u22121) \u2211\ni=1\n[\n\u03bb(i) ln\n(\n\u03bb(i)\n\u03bb (i) a (t)\n) + \u03b3(u\u2212 1T\u03bb) ] +\n\u00b5\n[\nla (\u03bba(t)) +\u2207\u03bbla (\u03bb) T \u2223 \u2223 \u2223 \u03bb=\u03bba(t) (\u03bb\u2212 \u03bba(t))\n]\n}\n,\nwhere \u03b3 is the Lagrange multiplier. This yields\n\u03bb (i) a (t + 1) = u\n\u03bb (i) a (t) exp {\u00b5e(t)(y\u0302i(t) \u2212 y\u0302m(t))}\n\u2211m\u22121\nk=1\n[\n\u03bb (k) a (t) exp {\u00b5e(t)(y\u0302k(t) \u2212 y\u0302m(t))} + \u03bb (k+m\u22121) a (t) exp {\u2212\u00b5e(t)(y\u0302k(t) \u2212 y\u0302m(t))}\n] ,\ni = 1, . . . ,m\u2212 1, \u03bb (i) a (t + 1) = u\n\u03bb (i) a (t) exp {\u2212\u00b5e(t)(y\u0302i(t) \u2212 y\u0302m(t))}\n\u2211m\u22121\nk=1\n[\n\u03bb (k) a (t) exp {\u00b5e(t)(y\u0302k(t) \u2212 y\u0302m(t))} + \u03bb (k+m\u22121) a (t) exp {\u2212\u00b5e(t)(y\u0302k(t) \u2212 y\u0302m(t))}\n] ,\ni = m, . . . , 2(m \u2212 1),\nproviding the multiplicative updates on \u03bba(t)."}, {"heading": "3.2 Unconstrained Mixture", "text": "Without any constraints on the combination weights, the mixture stage can be written as\ny\u0302(t) = wT (t)x(t), e(t) = y(t)\u2212 y\u0302(t),\nwhere w(t) \u2208 Rm. To be able to derive a multiplicative update, we use a change of variables,\nw(t) = w1(t)\u2212w2(t),\nwhere w1(t) and w2(t) are constrained to be nonnegative, i.e., wi(t) \u2208 Rm+ , i = 1, 2. We then collect the unconstrained weights wa(t) = [w1(t);w2(t)] and define a function of the loss e(t) as\nlu (wa(t)) \u25b3 = e2(t)."}, {"heading": "3.2.1 Unnormalized Relative Entropy", "text": "Defining cost function similar to (4) and minimizing it with respect tow yields\nw(i)a (t+ 1) = w (i) a (t) exp {\u00b5e(t)y\u0302i(t)} , i = 1, . . . , m, w(i)a (t+ 1) = w (i) a (t) exp {\u2212\u00b5e(t)y\u0302i(t)} , i = m+ 1, . . . , 2m,\nproviding the multiplicative update on wa(t)."}, {"heading": "3.2.2 Relative Entropy", "text": "Using the relative entropy under the simplex constraint on w, we get the updates\nw(i)a (t+ 1) = u w(i)a (t) exp {\u00b5e(t)y\u0302i(t)}\nm \u2211\nk=1\n[ w(k)a (t) exp {\u00b5e(t)y\u0302k(t)}+ w(k+m)a (t) exp {\u2212\u00b5e(t)y\u0302k(t)} ] ,\ni = 1, . . . , m, w(i)a (t+ 1) = u w(i)a (t) exp {\u2212\u00b5e(t)y\u0302i(t)}\nm \u2211\nk=1\n[ w(k)a (t) exp {\u00b5e(t)y\u0302k(t)}+ w(k+m)a (t) exp {\u2212\u00b5e(t)y\u0302k(t)} ] ,\ni = m+ 1 . . . , 2m.\nIn the next section, we study the transient analysis of these four adaptive mixture algorithms."}, {"heading": "4 Transient Analysis", "text": "In this section, we study the mean and the mean-square transient analysis of the adaptive mixture methods. We start with the affinely constrained combination."}, {"heading": "4.1 Affinely Constrained Mixture", "text": "We first perform the transient analysis of the mixture weights updated with the EGU algorithm. Then, we continue with the transient analysis of the mixture weights updated with the EG algorithm."}, {"heading": "4.1.1 Unconstrained Relative Entropy", "text": "For the affinely constrained mixture updated with the EGU algorithm, we have the multiplicative update as\n\u03bb (i) 1 (t+ 1) = \u03bb (i) 1 (t) exp {\u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t))} ,\n= \u03bb (i) 1 (t)\n\u221e \u2211\nk=0\n( \u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t)) )k\nk! , (5)\n\u03bb (i) 2 (t+ 1) = \u03bb (i) 2 (t) exp {\u2212\u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t))} ,\n= \u03bb (i) 2 (t)\n\u221e \u2211\nk=0\n( \u2212 \u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t)) )k\nk! , (6)\nfor i = 1, . . . , m \u2212 1. If e(t) and y\u0302i(t) \u2212 y\u0302m(t) for each i = 1, . . . , m \u2212 1 are bounded, then we can write (5) and (6) as\n\u03bb (i) 1 (t + 1) = \u03bb (i) 1 (t)\n( 1 + \u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t)) +O(\u00b52) ) , (7)\n\u03bb (i) 2 (t + 1) = \u03bb (i) 2 (t)\n( 1\u2212 \u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t)) +O(\u00b52) ) , (8)\nfor i = 1, . . . , m\u22121. Since \u00b5 is usually relatively small [2], we approximate (7) and (8) as\n\u03bb (i) 1 (t+ 1) = \u03bb (i) 1 (t)\n( 1 + \u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t)) ) , (9)\n\u03bb (i) 2 (t+ 1) = \u03bb (i) 2 (t)\n( 1\u2212 \u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t)) ) . (10)\nIn our simulations, we illustrate the accuracy of the approximations (9) and (10) under the mixture framework. Using (9) and (10), we can obtain updates on \u03bb1(t) and \u03bb2(t) as\n\u03bb1(t+ 1) = ( I + \u00b5e(t)diag ( \u03b4(t) )) \u03bb1(t), (11) \u03bb2(t+ 1) = ( I \u2212 \u00b5e(t)diag ( \u03b4(t) )) \u03bb2(t). (12)\nCollecting the weights in \u03bba(t) = [\u03bb1(t);\u03bb2(t)], using the updates (11) and (12), we can write update on \u03bba(t) as\n\u03bba(t + 1) = ( I + \u00b5e(t)diag ( u(t) )) \u03bba(t), (13)\nwhere u(t) is defined as u(t) \u25b3 = [\u03b4(t);\u2212\u03b4(t)].\nFor the desired signal y(t), we can write y(t)\u2212 y\u0302m(t) = \u03bbT0 (t)\u03b4(t)+e0(t), where \u03bb0(t) is the optimum MSE solution at time t such that \u03bb0(t) \u25b3 = R\u22121(t)p(t), R(t) \u25b3 = E [ \u03b4(t)\u03b4T (t) ] , p(t) \u25b3 = E { \u03b4(t) [ y(t)\u2212 y\u0302m(t) ]} and e0(t) is zero-mean and uncorrelated with \u03b4(t). We next show that the mixture weights converge to the optimum solution in the steady-state such that limt\u2192\u221e E [ \u03bb(t) ] = limt\u2192\u221e \u03bb0(t) for properly selected \u00b5.\nSubtracting (12) from (11), we obtain\n\u03bb(t+ 1) = \u03bb(t) + \u00b5e(t)diag ( \u03b4(t) )( \u03bb1(t) + \u03bb2(t) ) ,\n= \u03bb(t)\u2212 \u00b5e(t)diag ( \u03b4(t) ) \u03bb(t) + 2\u00b5e(t)diag ( \u03b4(t) )\n\u03bb1(t). (14)\nDefining \u03b5(t) \u25b3 = \u03bb0(t)\u2212 \u03bb(t) and using e(t) = \u03b4T (t)\u03b5(t) + e0(t) in (14) yield\n\u03bb(t + 1) = \u03bb(t)\u2212 \u00b5diag ( \u03b4(t) ) \u03bb(t)\u03b4T (t)\u03b5(t)\u2212 \u00b5diag ( \u03b4(t) )\n\u03bb(t)e0(t)\n+ 2\u00b5diag ( \u03b4(t) ) \u03bb1(t)\u03b4 T (t)\u03b5(t) + 2\u00b5diag ( \u03b4(t) ) \u03bb1(t)e0(t). (15)\nIn (15), subtracting both sides from \u03bb0(t + 1), we have\n\u03b5(t+ 1) = \u03b5(t) + \u00b5diag ( \u03b4(t) ) \u03bb(t)\u03b4T (t)\u03b5(t) + \u00b5diag ( \u03b4(t) )\n\u03bb(t)e0(t)\n\u2212 2\u00b5diag ( \u03b4(t) ) \u03bb1(t)\u03b4 T (t)\u03b5(t)\u2212 2\u00b5diag ( \u03b4(t) ) \u03bb1(t)e0(t) + [ \u03bb0(t+ 1)\u2212 \u03bb0(t) ] . (16)\nTaking expectation of both sides of (16) and using\nE [ \u00b5diag ( \u03b4(t) ) \u03bb(t)e0(t) ] = E [ \u00b5diag ( \u03b4(t) ) \u03bb(t) ] E[e0(t)] = 0, E [ 2\u00b5diag ( \u03b4(t) ) \u03bb1(t)e0(t) ] = E [ 2\u00b5diag ( \u03b4(t) ) \u03bb1(t) ] E[e0(t)] = 0,\nand assuming that \u03bb1(t) and \u03bb2(t) are independent of \u03b5(t) [17] yield\nE [ \u03b5(t+ 1) ] = E [ I \u2212 \u00b5diag ( \u03bb1(t) + \u03bb2(t) ) \u03b4(t)\u03b4T (t) ] E [ \u03b5(t) ]\n+ E [ \u03bb0(t+ 1)\u2212 \u03bb0(t) ] . (17)\nAssuming convergence of R(t) and p(t) (which is true for a wide range of adaptive methods in the first stage [16], [14, 21]), we obtain limt\u2192\u221e E [ \u03bb0(t + 1)\u2212\u03bb0(t) ] = 0. If \u00b5 is chosen such that the eigenvalues of E [ I\u2212\u00b5diag ( \u03bb1(t)+ \u03bb2(t) ) \u03b4(t)\u03b4T (t) ] have strictly less than unit magnitude for every t, then limt\u2192\u221eE [ \u03bb(t) ] = limt\u2192\u221e \u03bb0(t).\nFor the transient analysis of the MSE, we have\nE[e2(t)] = E { [ y(t)\u2212 y\u0302m(t) ]2 } \u2212 2\u03bb\u0304Ta (t)E {[ y(t)\u2212 y\u0302m(t) ] [\u03b4(t);\u2212\u03b4(t)] }\n+ E { \u03bbTa (t)[\u03b4(t);\u2212\u03b4(t)][\u03b4(t);\u2212\u03b4(t)]T\u03bba(t) } , = E { [ y(t)\u2212 y\u0302m(t) ]2 } \u2212 2\u03bb\u0304Ta (t)E {[ y(t)\u2212 y\u0302m(t) ] u(t) }\n+ tr\n(\nE [\n\u03bba(t)\u03bb T a (t)\n] E { u(t)u(t)T }\n)\n,\n= E { [ y(t)\u2212 y\u0302m(t) ]2 } \u2212 2\u03bb\u0304Ta (t)\u03b3(t) + tr ( E [ \u03bba(t)\u03bb T a (t) ] \u0393(t) ) ,\n(18)\nwhere we define \u03b3(t) \u25b3 = E\n{ u(t) [ y(t)\u2212 y\u0302m(t) ]} and \u0393(t) \u25b3 = E [ u(t)uT (t) ] .\nFor the recursion of \u03bb\u0304a(t) = E[\u03bba(t)], using (13), we get\n\u03bb\u0304a(t+ 1) = \u03bb\u0304a(t) + \u00b5diag ( \u03b3(t) ) \u03bb\u0304a(t)\u2212 \u00b5diag ( E[\u03bba(t)\u03bb T a (t)]\u0393(t) ) . (19)\nUsing (32), assuming \u03bba(t) is Gaussian and assuming \u03bb (i) a (t) and \u03bb (j) a (t) are\nindependent when i 6= j [17], [14], we get a recursion for E [\n\u03bba(t)\u03bb T a (t)\n]\nas\nE [\n\u03bba(t + 1)\u03bb T a (t + 1)\n] = E [\n\u03bba(t)\u03bb T a (t)\n] + \u00b5diag ( \u03b3(t) ) E [\n\u03bba(t)\u03bb T a (t)\n]\n\u2212 \u00b5diag ( \u0393(t)\u03bb\u0304a(t) ) E [ \u03bba(t)\u03bb T a (t) ] \u2212 \u00b5E [ diag2(u(t)) ] ( E [\n\u03bba(t)\u03bb T a (t)\n] \u2212 \u03bb\u0304a(t)\u03bb\u0304Ta (t) ) 1\u03bb\u0304 T a (t)\n\u2212 \u00b5diag ( \u03bb\u0304a(t) ) \u0393(t)\n(\nE [\n\u03bba(t)\u03bb T a (t)\n] \u2212 \u03bb\u0304a(t)\u03bb\u0304Ta (t) )\n+ \u00b5E [\n\u03bba(t)\u03bb T a (t)\n] diag ( \u03b3(t) ) \u2212 \u00b5E [\n\u03bba(t)\u03bb T a (t)\n] diag ( \u0393(t)\u03bb\u0304a(t) )\n\u2212 \u00b5\u03bb\u0304a(t)1T ( E [ \u03bba(t)\u03bb T a (t) ] \u2212 \u03bb\u0304a(t)\u03bb\u0304Ta (t) ) E [ diag2(u(t)) ] \u2212 \u00b5 ( E [\n\u03bba(t)\u03bb T a (t)\n] \u2212 \u03bb\u0304a(t)\u03bb\u0304Ta (t) ) \u0393(t)diag ( \u03bb\u0304a(t) ) . (20)\nDefining qa(t) \u25b3 = \u03bb\u0304a(t) and Qa(t) \u25b3 = E\n[\n\u03bba(t)\u03bb T a (t)\n]\n, we express (19) and (20)\nas a coupled recursions in Table 1.\nIn Table 1, we provide the mean and the variance recursions for Qa(t) and qa(t). To implement these recursions, one needs to only provide \u0393(t) and \u03b3(t). Note that \u0393(t) and \u03b3(t) are derived for a wide range of adaptive filters [16], [14]. If we use the mean and the variance recursions in (18), then we obtain the time evolution of the final MSE. This completes the transient analysis of the affinely constrained mixture weights updated with the EGU algorithm."}, {"heading": "4.1.2 Relative Entropy", "text": "For the affinely constrained combination updated with the EG algorithm, we have the multiplicative updates as\n\u03bb (i) 1 (t + 1) = u\n\u03bb (i) 1 (t) exp {\u00b5e(t)(y\u0302i(t) \u2212 y\u0302m(t))}\n\u2211m\u22121\nk=1\n[\n\u03bb (k) 1 (t) exp {\u00b5e(t)(y\u0302k(t) \u2212 y\u0302m(t))} + \u03bb (k) 2 (t) exp {\u2212\u00b5e(t)(y\u0302k(t) \u2212 y\u0302m(t))}\n] ,\n\u03bb (i) 2 (t + 1) = u\n\u03bb (i) 2 (t) exp {\u2212\u00b5e(t)(y\u0302i(t) \u2212 y\u0302m(t))}\n\u2211m\u22121\nk=1\n[\n\u03bb (k) 1 (t) exp {\u00b5e(t)(y\u0302k(t) \u2212 y\u0302m(t))} + \u03bb (k) 2 (t) exp {\u2212\u00b5e(t)(y\u0302k(t) \u2212 y\u0302m(t))}\n] ,\nfor i = 1, . . . , m \u2212 1. Using the same approximations as in (7), (8), (9) and (10), we obtain\n\u03bb (i) 1 (t + 1) = u\n\u03bb (i) 1 (t)\n( 1 + \u00b5e(t)(y\u0302i(t) \u2212 y\u0302m(t)) )\n\u2211m\u22121\nk=1\n[\n\u03bb (k) 1 (t)\n( 1 + \u00b5e(t)(y\u0302k(t) \u2212 y\u0302m(t)) ) + \u03bb (k) 2 (t) ( 1\u2212 \u00b5e(t)(y\u0302k(t) \u2212 y\u0302m(t)) )\n] , (21)\n\u03bb (i) 2 (t + 1) = u\n\u03bb (i) 2 (t)\n( 1\u2212 \u00b5e(t)(y\u0302i(t) \u2212 y\u0302m(t)) )\n\u2211m\u22121\nk=1\n[\n\u03bb (k) 1 (t)\n( 1 + \u00b5e(t)(y\u0302k(t) \u2212 y\u0302m(t)) ) + \u03bb (k) 2 (t) ( 1\u2212 \u00b5e(t)(y\u0302k(t) \u2212 y\u0302m(t))\n] . (22)\nIn our simulations, we illustrate the accuracy of the approximations (21) and (22) under the mixture framework. Using (21) and (22), we obtain updates on \u03bb1(t) and \u03bb2(t) as\n\u03bb1(t+ 1) = u\n( I + \u00b5e(t)diag ( \u03b4(t) ))\n\u03bb1(t) [\n1T + \u00b5e(t)uT (t) ] \u03bba(t) , (23)\n\u03bb2(t+ 1) = u\n( I \u2212 \u00b5e(t)diag ( \u03b4(t) ))\n\u03bb2(t) [\n1T + \u00b5e(t)uT (t) ] \u03bba(t) . (24)\nUsing updates (23) and (24), we can write update on \u03bba(t)\n\u03bba(t+ 1) = u\n[ I + \u00b5e(t)diag ( u(t) )]\n\u03bba(t) [\n1T + \u00b5e(t)uT (t) ] \u03bba(t) . (25)\nFor the recursion of \u03bb\u0304a(t), using (25), we get\nE [ \u03bba(t+ 1) ] = E\n \n\nu\n[ I + \u00b5e(t)diag ( u(t) )]\n\u03bba(t) [\n1T + \u00b5e(t)uT (t) ]\n\u03bba(t)\n \n\n,\n\u2248 u E\n{[ I + \u00b5e(t)diag ( u(t) )] \u03bba(t) } E {[ 1T + \u00b5e(t)uT (t) ] \u03bba(t) } , (26)\n= u E [ \u03bba(t) ] + \u00b5diag ( \u03b3(t) ) E [ \u03bba(t) ] \u2212 \u00b5diag ( E[\u03bba(t)\u03bb T a (t)]\u0393(t) ) [\n1T + \u00b5\u03b3T (t) ] E [ \u03bba(t) ] \u2212 \u00b5tr ( E[\u03bba(t)\u03bb T a (t)]\u0393(t)\n) ,\n(27)\nwhere in (26) we approximate expectation of the quotient with the quotient of the expectations. In our simulations, we also illustrate the accuracy of this approximation in the mixture framework. From (25), using the same approximation in (27), assuming \u03bba(t) is Gaussian, assuming \u03bb (i) a (t) and \u03bb (j) a (t) are independent when i 6= j, we get a recursion for E [\n\u03bba(t)\u03bb T a (t)\n]\nas\nE [\n\u03bba(t+ 1)\u03bb T a (t+ 1)\n] = u2 A(t)\nb(t) , (28)\nwhere A(t) is equal to the right hand side of (20) and\nb(t) = 1TE [\n\u03bba(t)\u03bb T a (t)\n] 1+ \u00b5pT (t)E [\n\u03bba(t)\u03bb T a (t)\n]\n1\n\u2212 \u00b5\u03bb\u0304Ta (t)R(t)E [ \u03bba(t)\u03bb T a (t) ]\n1\u2212 \u00b51T ( E [\n\u03bba(t)\u03bb T a (t)\n] \u2212 \u03bb\u0304a(t)\u03bb\u0304Ta (t) ) R(t)\u03bb\u0304a(t)\n\u2212 \u00b51T ( E [\n\u03bba(t)\u03bb T a (t)\n] \u2212 \u03bb\u0304a(t)\u03bb\u0304Ta (t) ) E [ diag2(u(t)) ] 1T \u03bb\u0304a(t)1\n+ \u00b51TE [\n\u03bba(t)\u03bb T a (t)\n] p(t)\u2212 \u00b51TE [\n\u03bba(t)\u03bb T a (t)\n]\nR(t)\u03bb\u0304a(t)\n\u2212 \u00b5\u03bb\u0304Ta (t)R(t) ( E [ \u03bba(t)\u03bb T a (t) ] \u2212 \u03bb\u0304a(t)\u03bb\u0304Ta (t) ) 1\n\u2212 \u00b51T \u03bb\u0304Ta (t)1E [ diag2(u(t)) ]\n(\nE [\n\u03bba(t)\u03bb T a (t)\n] \u2212 \u03bb\u0304a(t)\u03bb\u0304Ta (t) ) 1. (29)\nIf we use the mean (27) and the variance (28), (29) recursions in (18), then we obtain the time evolution of the final MSE. This completes the transient analysis of the affinely constrained mixture weights updated with the EG algorithm."}, {"heading": "4.2 Unconstrained Mixture", "text": "We use the unconstrained relative entropy and the relative entropy as distance measures to update unconstrained mixture weights. We first perform transient analysis of the mixture weights updated using the EGU algorithm. Then, we continue with the transient analysis of the mixture weights updated using the EG algorithm. Note that since the unconstrained case is close to the affinely constrained case, we only provide the necessary modifications to get the mean and the variance recursions for the transient analysis."}, {"heading": "4.2.1 Unconstrained Relative Entropy", "text": "For the unconstrained combination updated with EGU, we have the multiplicative updates as\nw (i) 1 (t + 1) = w (i) 1 (t) exp {\u00b5e(t)y\u0302i(t)} , w (i) 2 (t + 1) = w (i) 2 (t) exp {\u2212\u00b5e(t)y\u0302i(t)} ,\nfor i = 1, . . . , m. Using the same approximations as in (7), (8), (9) and (10), we can obtain updates on w1(t) and w2(t) as\nw1(t+ 1) = ( I + \u00b5e(t)diag ( x(t) )) w1(t), (30) w2(t+ 1) = ( I \u2212 \u00b5e(t)diag ( x(t) )) w2(t). (31)\nCollecting the weights in wa(t) = [w1(t);w2(t)], using the updates (30) and (31), we can write update on wa(t) as\nwa(t + 1) = ( I + \u00b5e(t)diag ( u(t) )) wa(t), (32)\nwhere u(t) is defined as u(t) \u25b3 = [x(t);\u2212x(t)].\nFor the desired signal y(t), we can write y(t) = wT0 (t)x(t) + e0(t), where w0(t) is the optimum MSE solution at time t such that w0(t) \u25b3 = R\u22121(t)p(t),\nR(t) \u25b3 = E\n[ x(t)xT (t) ] , p(t) \u25b3 = E {x(t)y(t)} and e0(t) is zero-mean disturbance\nuncorrelated to x(t). To show that the mixture weights converge to the optimum solution in the steady-state such that limt\u2192\u221e E [ w(t) ] = limt\u2192\u221ew0(t), we follow similar lines as in the Section 4.1.1. We modify (14), (15), (16) and (17) such that \u03bb will be replaced by w, \u03b4(t) will be replaced by x(t) and \u03b5(t) = w0(t)\u2212w(t). After these replacements, we obtain\nE [ \u03b5(t + 1) ] = E [ I \u2212 \u00b5diag ( w1(t) +w2(t) ) x(t)xT (t) ] E [ \u03b5(t) ]\n+ E [ w0(t+ 1)\u2212w0(t) ] . (33)\nSince, we have limt\u2192\u221eE [ w0(t + 1) \u2212w0(t) ] = 0 for most adaptive filters in the first stage [14] and if \u00b5 is chosen so that all the eigenvalues of E [\nI \u2212 \u00b5diag ( w1(t) + w2(t) ) x(t)xT (t) ] have strictly less than unit magnitude for every t, then limt\u2192\u221eE [ w(t) ] = limt\u2192\u221e w0(t).\nFor the transient analysis of MSE, defining \u03b3(t) \u25b3 = E {u(t)y(t)} and \u0393(t) \u25b3= E [ u(t)uT (t) ] , (18) is modified as\nE[e2(t)] = E { y2(t) } \u2212 2w\u0304Ta (t)\u03b3(t) + tr ( E [ wa(t)w T a (t) ] \u0393(t) ) . (34)\nAccordingly, we modify the mean recursion (19) and the variance recursion (20) such that instead of \u03bba(t) we use wa(t). We also modify the Table 1 using qa(t) \u25b3 = w\u0304a(t) and Qa(t) \u25b3 = E [ wa(t)w T a (t) ] . If we use this modified mean and variance recursions in (34), then we obtain the time evolution of the final MSE. This completes the transient analysis of the unconstrained mixture weights updated with the EGU algorithm."}, {"heading": "4.2.2 Relative Entropy", "text": "For the unconstrained combination updated with the EG algorithm, we have the multiplicative updates as\nw(i)a (t+ 1) = u w(i)a (t) exp {\u00b5e(t)y\u0302i(t)}\nm \u2211\nk=1\n[ w(k)a (t) exp {\u00b5e(t)y\u0302k(t)}+ w(k+m)a (t) exp {\u2212\u00b5e(t)y\u0302k(t)} ] ,\ni = 1, . . . , m, w(i)a (t+ 1) = u w(i)a (t) exp {\u2212\u00b5e(t)y\u0302i(t)}\nm \u2211\nk=1\n[ w(k)a (t) exp {\u00b5e(t)y\u0302k(t)}+ w(k+m)a (t) exp {\u2212\u00b5e(t)y\u0302k(t)} ] ,\ni = m+ 1 . . . , 2m.\nFollowing similar lines, we modify (23), (24), (25), (27), (28) and (29) such that we replace \u03b4(t) with x(t), \u03bb with w and u(t) = [ x(t);\u2212x(t) ] . Finally, we use the modified mean and variance recursions in (34) and obtain the time evolution of the final MSE. This completes the transient analysis of the unconstrained mixture weights updated with the EG algorithm."}, {"heading": "5 Simulations", "text": "In this section, we illustrate the accuracy of our results and compare performances of different adaptive mixture methods through simulations. In our simulations, we observe that using the EG algorithm to train the mixture weights yields better performance compared to using the LMS algorithm or the EGU algorithm to train the mixture weights for combinations having more than two filters and when the combination favors only a few of the constituent filters. The LMS algorithm and the EGU algorithm perform similarly in our simulations when they are used to train the mixture weights. We also observe in our simulations that the mixture weights under the EG update converge to the optimum combination vector faster than the mixture weights under the\nLMS algorithm.\nTo compare performances of the EG and LMS algorithms and illustrate the accuracy of our results in (27), (28) and (29) under different algorithmic parameters, the desired signal as well as the system parameters are selected as follows. First, a seventh-order linear filter, wo = [0.25,\u22120.47,\u22120.37, 0.045,\u22120.18, 0.78, 0.147]T , is chosen as in [17]. The underlying signal is generated using the data model y(t) = \u03c4 wTo a(t) + n(t), where a(t) is an i.i.d. Gaussian vector process with zero mean and unit variance entries, i.e., E[a(t)aT (t)] = I, n(t) is an i.i.d. Gaussian noise process with zero mean and variance E[n2(t)] = 0.3, and \u03c4 is a positive scalar to control SNR. Hence, the SNR of the desired signal is given by SNR \u25b3 = 10 log(E[\u03c4 2(wTou(t))2 ]\n0.01 ) = 10 log( \u03c4 2\u2016wo\u20162 0.01 ). For the first experiment, we have\nSNR = -10dB. To model the unknown system we use ten linear filters using the LMS update as the constituent filters. The learning rates of these two constituent filters are set to \u00b51 = 0.002 and \u00b56 = 0.002 while the learning rates for the rest of the constituent filters are selected randomly in [0.1, 0.11]. Therefore, in the steady-state, we obtain the optimum combination vector approximately as \u03bbo = [0.5, 0, 0, 0, 0, 0.5, 0, 0, 0, 0] T , i.e., the final combination vector is sparse. In the second stage, we train the combination weights with the EG and LMS algorithms and compare performances of these algorithms. For the second stage, the learning rates for the EG and LMS algorithms are selected as \u00b5EG = 0.0008 and \u00b5LMS = 0.005 such that the MSEs of both mixtures converge to the same final MSE to provide a fair comparison. We select u = 500 for the EG algorithm. In Fig. 2a, we plot the weight of the first constituent filter with \u00b51 = 0.002, i.e. E[\u03bb (1)(t)], updated with the EG and LMS algorithms. In Fig. 2b, we plot the MSE curves for the adaptive mixture updated with the EG algorithm, the adaptive mixture updated with the LMS algorithm, the first constituent filter with \u00b51 = 0.002 and the second constituent filter with \u00b52 \u2208 [0.1, 0.11]. From Fig. 2a and Fig. 2b, we see\nthat the EG algorithm performs better than the LMS algorithm such that the combination weight under the update of the EG algorithm converges to 0.5 faster than the combination weight under the update of the LMS algorithm. Furthermore the MSE of the adaptive mixture updated with the EG algorithm converges faster than the MSE of the adaptive mixture updated with the LMS algorithm. In Fig. 2c, to test the accuracy of (27), we plot the theoretical values for \u03bb\u0304(1)a (t) and \u03bb\u0304 (10) a (t) along with simulations. Note in Fig. 2c we observe that \u03bb\u0304(1)(t) = \u03bb\u0304(1)a (t)\u2212 \u03bb\u0304(10)a (t) converges to 0.5 as predicted in our derivations. In Fig. 2d, to test the accuracy of (28) and (29), as an example, we plot the theoretical values of E [\n\u03bb(1)a (t) 2 ] and E [ \u03bb(1)a (t)\u03bb (3) a (t) ] along with simulations.\nAs we observe from Fig. 2c and Fig. 2d, there is a close agreement between our results and simulations in these experiments. We observe similar results for the other cross terms.\nWe next simulate the unconstrained mixtures updated with the EGU and EG algorithms. Here, we have two linear filters and both using the LMS update to train their weight vectors as the constituent filters. The learning rates for two constituent filters are set to \u00b51 = 0.002 and \u00b52 = 0.1 respectively. Therefore, in the steady-state, we obtain the optimum vector approximately as wo = [1, 0]. We have SNR = 1 for these simulations. The unconstrained mixture weights are first updated with the EGU algorithm. For the second stage, the learning rate for the EGU algorithm is selected as \u00b5EGU = 0.01. The theoretical curves in the figures are produced using \u0393(t) and \u03b3(t) that are calculated from the simulations, since our goal is to illustrate the validity of derived equations. In Fig. 3a, we plot the theoretical values of w\u0304(1)a (t), w\u0304 (2) a (t), w\u0304 (3) a (t) and w\u0304 (4) a (t) along with simulations. In Fig. 3b, as an example, we plot the theoretical values of E [\nw(1)a (t) 2 ] , E [ w(1)a (t)w (2) a (t) ] , E [ w(2)a (t)w (3) a (t) ] and E [ w(3)a (t)w (4) a (t) ]\nalong with simulations. We continue to update the mixture weights with the EG algorithm. For the second stage, the learning rate for the EG algorithm is selected as \u00b5EG = 0.01. We select u = 3 for the EG algorithm. In Fig. 3c,\nwe plot the theoretical values of w\u0304(1)a (t), w\u0304 (2) a (t), w\u0304 (3) a (t) and w\u0304 (4) a (t) along with simulations. In Fig. 3d, as an example, we plot the theoretical values of E [\nw(2)a (t) 2 ] , E [ w(1)a (t)w (2) a (t) ] , E [ w(2)a (t)w (3) a (t) ] and E [ w(2)a (t)w (4) a (t) ] along with simulations. We observe a close agreement between our results and simulations.\nTo test the accurateness of the assumptions in (9) and (10), we plot in Fig. 4a, the difference\n\u2016 exp {\u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t))} \u2212 {1 + \u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t)))} \u20162 \u221a\n\u2016 exp {\u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t))} \u20162\u2016 {1 + \u00b5e(t)(y\u0302i(t)\u2212 y\u0302m(t)))} \u20162\nfor i = 1 with the same algorithmic parameters as in Fig. 2 and Fig. 3. To test the accurateness of the separation assumption in (27), we plot in Fig. 4b, the first parameter of the difference\n\u2225 \u2225 \u2225 \u2225 E { u\n[ I+\u00b5e(t)diag ( u(t) )]\n\u03bba(t) [\n1 T +\u00b5e(t)uT (t)\n]\n\u03bba(t)\n}\n\u2212 u E\n{\n[ I+\u00b5e(t)diag ( u(t) )] \u03bba(t) }\nE\n{\n[\n1 T +\u00b5e(t)uT (t)\n] \u03bba(t) }\n\u2225 \u2225 \u2225 \u2225 2\n\u221a \u221a \u221a \u221a \u2225 \u2225\n\u2225 \u2225\nE\n{\nu\n[ I+\u00b5e(t)diag ( u(t) )]\n\u03bba(t) [\n1 T +\u00b5e(t)uT (t)\n]\n\u03bba(t)\n} \u2225\n\u2225 \u2225 \u2225\n2\u2225 \u2225\n\u2225 \u2225\nu E\n{\n[ I+\u00b5e(t)diag ( u(t) )] \u03bba(t) }\nE\n{\n[\n1 T +\u00b5e(t)uT (t)\n] \u03bba(t) }\n\u2225 \u2225 \u2225 \u2225 2\nwith the same algorithmic parameters as in Fig. 2 and Fig. 3. We observe\nthat assumptions are fairly accurate for these algorithms in our simulations.\nIn the last simulations, we compare performances of the EGU, EG and LMS algorithms updating the affinely mixture weights under different algorithmic parameters. Algorithmic parameters and constituent filters are selected as in Fig. 2 under SNR = -5 and 5. For the second stage, under SNR = -5, learning rates for the EG, EGU and LMS algorithms are selected as \u00b5EG = 0.0005, \u00b5EGU = 0.005 and \u00b5LMS = 0.005 such that the MSEs converge to the same final MSE to provide a fair comparison. We choose u = 500 for the EG algorithm. In Fig. 5a, we plot the MSE curves for the adaptive mixture updated with the EG algorithm, the adaptive mixture updated with the EGU algorithm, the adaptive mixture updated with the LMS algorithm, first constituent filter\nwith \u00b51 = 0.002 and second constituent filter with \u00b52 \u2208 [0.1, 0.11] under SNR = -5. Under SNR = 5, learning rates for the EG, EGU and LMS algorithms are selected as \u00b5EG = 0.002, \u00b5EGU = 0.005 and \u00b5LMS = 0.005. We choose u = 100 for the EG algorithm. In Fig. 5b, we plot same MSE curves as in Fig. 5a. We observe that the EG algorithm performs better than the EGU and LMS algorithms such that MSE of the adaptive mixture updated with the EG algorithm converges faster than the MSE of adaptive mixtures updated with the EGU and LMS algorithms. We also observe that the EGU and LMS algorithms show similar performances when they are used to train the mixture weights."}, {"heading": "6 Conclusion", "text": "In this paper, we investigate adaptive mixture methods based on Bregman divergences combining outputs of m adaptive filters to model a desired signal. We use the unnormalized relative entropy and relative entropy as distance measures that produce the exponentiated gradient update with unnormalized weights (EGU) and the exponentiated gradient update with positive and negative weights (EG) to train the mixture weights under the affine constraints or without any constraints. We provide the transient analysis of these methods updated with the EGU and EG algorithms. In our simulations, we compare performances of the EG, EGU and LMS algorithms and observe that the EG algorithm performs better than the EGU and LMS algorithms when the combination vector in steady-state is sparse. We observe that the EGU and LMS algorithms show similar performance when they are used to train the mixture weights. We also observe a close agreement between the simulations and our theoretical results."}], "references": [{"title": "A class of stochastic gradient algorithms with exponentiated error cost functions", "author": ["C. Boukis", "D. Mandic", "A.G. Constantinides"], "venue": "Digital Signal Processing", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "A comparison of new and old algorithms for a mixture estimation problem", "author": ["D.P. Helmbold", "R.E. Schapire", "Y. Singer", "M.K. Warmuth"], "venue": "Machine Learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Stochastic analysis of an error power ratio scheme applied to the affine combination of two lms adaptive filters", "author": ["J.C.M. Bermudez", "N.J. Bershad", "J.Y. Tourneret"], "venue": "Signal Processing", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Plant identification via adaptive combination of transversal filters", "author": ["J. Arenas-Garcia", "M. Martinez-Ramon", "A. Navia-Vazquez", "A.R. Figueiras- Vidal"], "venue": "Signal Processing", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Multi-stage adaptive signal processing", "author": ["S.S. Kozat", "A.C. Singer"], "venue": "Proceedings of SAM Signal Proc. Workshop,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Separate-variable adaptive combination of LMS adaptive filters for plant identification", "author": ["J. Arenas-Garcia", "V. Gomez-Verdejo", "M. Martinez-Ramon", "A.R. Figueiras- Vidal"], "venue": "in: Proc. of the 13th IEEE Int. Workshop Neural Networks Signal Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Multiple plant identifier via adaptive LMS convex combination", "author": ["J. Arenas-Garcia", "M. Martinez-Ramon", "V. Gomez-Verdejo", "A.R. Figueiras- Vidal"], "venue": "in: Proc. of the IEEE Int. Symp. Intel. Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "New algorithms for improved adaptive convex combination of lms transversal filters", "author": ["J. Arenas-Garcia", "V. Gomez-Verdejo", "A.R. Figueiras-Vidal"], "venue": "IEEE Transactions on Instrumentation and Measurement", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M. Warmuth"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "On-line portfolio selection using multiplicative updates, Mathematical Finance", "author": ["D.P. Helmbold", "R.E. Schapire", "Y. Singer", "M.K. Warmuth"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "An affine combination of two LMS adaptive filters: Transient mean-square analysis", "author": ["N.J. Bershad", "J.C.M. Bermudez", "J. Tourneret"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Steady state MSE performance analysis of mixture approaches to adaptive filtering", "author": ["S.S. Kozat", "A.T. Erdogan", "A.C. Singer", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "The LMS, PNLMS, and Exponentiated Gradient algorithms", "author": ["J. Benesty", "Y.A. Huang"], "venue": "Proc. Eur. Signal Process. Conf. (EUSIPCO)", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Fundamentals of Adaptive Filtering", "author": ["A.H. Sayed"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Mean-square performance of a convex combination of two adaptive filters", "author": ["J. Arenas-Garcia", "A.R. Figueiras-Vidal", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Arenas-Garcia, A transient analysis for the convex combination of adaptive filters", "author": ["V.H. Nascimento", "J.M.T.M. Silva"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Transient analysis of adaptive affine combinations", "author": ["S.S. Kozat", "A.T. Erdogan", "A.C. Singer", "A.H. Sayed"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "A game of prediction with expert advice", "author": ["V. Vovk"], "venue": "Journal of Computer and System Sciences", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Adaptive algorithms for sparse echo cancellation", "author": ["P.A. Naylor", "J. Cui", "M. Brookes"], "venue": "Signal Processing", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth"], "venue": "Journal of the ACM", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction In this paper, we study adaptive mixture methods based on \u201cBregman divergences\u201d [1,2] that combine outputs of m constituent filters running in parallel on the same task.", "startOffset": 95, "endOffset": 100}, {"referenceID": 1, "context": "1 Introduction In this paper, we study adaptive mixture methods based on \u201cBregman divergences\u201d [1,2] that combine outputs of m constituent filters running in parallel on the same task.", "startOffset": 95, "endOffset": 100}, {"referenceID": 2, "context": "The overall system has two stages [3\u20138].", "startOffset": 34, "endOffset": 39}, {"referenceID": 3, "context": "The overall system has two stages [3\u20138].", "startOffset": 34, "endOffset": 39}, {"referenceID": 4, "context": "The overall system has two stages [3\u20138].", "startOffset": 34, "endOffset": 39}, {"referenceID": 5, "context": "The overall system has two stages [3\u20138].", "startOffset": 34, "endOffset": 39}, {"referenceID": 6, "context": "The overall system has two stages [3\u20138].", "startOffset": 34, "endOffset": 39}, {"referenceID": 7, "context": "The overall system has two stages [3\u20138].", "startOffset": 34, "endOffset": 39}, {"referenceID": 8, "context": "We use Bregman divergences and obtain certain multiplicative updates [9], [2], [10] to train these linear combination weights under an affine constraint [11] or without any constraints [12].", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "We use Bregman divergences and obtain certain multiplicative updates [9], [2], [10] to train these linear combination weights under an affine constraint [11] or without any constraints [12].", "startOffset": 74, "endOffset": 77}, {"referenceID": 9, "context": "We use Bregman divergences and obtain certain multiplicative updates [9], [2], [10] to train these linear combination weights under an affine constraint [11] or without any constraints [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "We use Bregman divergences and obtain certain multiplicative updates [9], [2], [10] to train these linear combination weights under an affine constraint [11] or without any constraints [12].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "We use Bregman divergences and obtain certain multiplicative updates [9], [2], [10] to train these linear combination weights under an affine constraint [11] or without any constraints [12].", "startOffset": 185, "endOffset": 189}, {"referenceID": 1, "context": "We use unnormalized [2] and normalized relative entropy [9] to define two different Bregman divergences that produce the unnormalized exponentiated gradient update (EGU) and the exponentiated gradient update (EG) on the mixture weights [9], respectively.", "startOffset": 20, "endOffset": 23}, {"referenceID": 8, "context": "We use unnormalized [2] and normalized relative entropy [9] to define two different Bregman divergences that produce the unnormalized exponentiated gradient update (EGU) and the exponentiated gradient update (EG) on the mixture weights [9], respectively.", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "We use unnormalized [2] and normalized relative entropy [9] to define two different Bregman divergences that produce the unnormalized exponentiated gradient update (EGU) and the exponentiated gradient update (EG) on the mixture weights [9], respectively.", "startOffset": 236, "endOffset": 239}, {"referenceID": 12, "context": "We emphasize that to the best of our knowledge, this is the first mean and mean-square transient analysis of the EGU algorithm and the EG algorithm in the mixture framework (which naturally covers the classical framework also [13,14]).", "startOffset": 226, "endOffset": 233}, {"referenceID": 13, "context": "We emphasize that to the best of our knowledge, this is the first mean and mean-square transient analysis of the EGU algorithm and the EG algorithm in the mixture framework (which naturally covers the classical framework also [13,14]).", "startOffset": 226, "endOffset": 233}, {"referenceID": 10, "context": "Adaptive mixture methods are utilized in a wide range of signal processing applications in order to improve the steady-state and/or convergence performance over the constituent filters [11,12,15].", "startOffset": 185, "endOffset": 195}, {"referenceID": 11, "context": "Adaptive mixture methods are utilized in a wide range of signal processing applications in order to improve the steady-state and/or convergence performance over the constituent filters [11,12,15].", "startOffset": 185, "endOffset": 195}, {"referenceID": 14, "context": "Adaptive mixture methods are utilized in a wide range of signal processing applications in order to improve the steady-state and/or convergence performance over the constituent filters [11,12,15].", "startOffset": 185, "endOffset": 195}, {"referenceID": 14, "context": "An adaptive convexly constrained mixture of two filters is studied in [15], where the convex combination is shown to be \u201cuniversal\u201d such that the combination performs at least as well as its best constituent filter in the steady-state [15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "An adaptive convexly constrained mixture of two filters is studied in [15], where the convex combination is shown to be \u201cuniversal\u201d such that the combination performs at least as well as its best constituent filter in the steady-state [15].", "startOffset": 235, "endOffset": 239}, {"referenceID": 15, "context": "The transient analysis of this adaptive convex combination is studied in [16], where the time evolution of the mean and variance of the mixture weights is provided.", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "an affinely constrained mixture of adaptive filters using a stochastic gradient update is introduced in [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "The steady-state mean square error (MSE) of this affinely constrained mixture is shown to outperform the steady-state MSE of the best constituent filter in the mixture under certain conditions [11].", "startOffset": 193, "endOffset": 197}, {"referenceID": 16, "context": "The transient analysis of this affinely constrained mixture for m constituent filters is carried out in [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "The general linear mixture framework as well as the steady-state performances of different mixture configurations are studied in [12].", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "We point out that the EG algorithm is widely used in sequential learning theory [18] and minimizes an approximate final estimation error while penalizing the distance between the new and the old filter weights.", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "In network and acoustic echo cancellation applications, the EG algorithm is shown to converge faster than the LMS algorithm [14, 19] when the system impulse response is sparse [13].", "startOffset": 124, "endOffset": 132}, {"referenceID": 18, "context": "In network and acoustic echo cancellation applications, the EG algorithm is shown to converge faster than the LMS algorithm [14, 19] when the system impulse response is sparse [13].", "startOffset": 124, "endOffset": 132}, {"referenceID": 12, "context": "In network and acoustic echo cancellation applications, the EG algorithm is shown to converge faster than the LMS algorithm [14, 19] when the system impulse response is sparse [13].", "startOffset": 176, "endOffset": 180}, {"referenceID": 10, "context": "Similarly, in our simulations, we observe that using the EG algorithm to train the mixture weights yields increased convergence speed compared to using the LMS algorithm to train the mixture weights [11, 12] when the combination favors only a few of the constituent filters in the steady state, i.", "startOffset": 199, "endOffset": 207}, {"referenceID": 11, "context": "Similarly, in our simulations, we observe that using the EG algorithm to train the mixture weights yields increased convergence speed compared to using the LMS algorithm to train the mixture weights [11, 12] when the combination favors only a few of the constituent filters in the steady state, i.", "startOffset": 199, "endOffset": 207}, {"referenceID": 1, "context": ", \u0177m(t)] T and train the mixture weights using multiplicative updates (or exponentiated gradient updates) [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "We point out that in order to satisfy the constraints and derive the multiplicative updates [9], [20], we use reparametrization of the mixture weights as w(t) = f (z(t)) and perform the update on z(t) as z(t + 1) = argmin z {", "startOffset": 92, "endOffset": 95}, {"referenceID": 19, "context": "We point out that in order to satisfy the constraints and derive the multiplicative updates [9], [20], we use reparametrization of the mixture weights as w(t) = f (z(t)) and perform the update on z(t) as z(t + 1) = argmin z {", "startOffset": 97, "endOffset": 101}, {"referenceID": 1, "context": "Since \u03bc is usually relatively small [2], we approximate (7) and (8) as \u03bb (i) 1 (t+ 1) = \u03bb (i) 1 (t) (", "startOffset": 36, "endOffset": 39}, {"referenceID": 16, "context": "(16) Taking expectation of both sides of (16) and using E [ \u03bcdiag ( \u03b4(t) ) \u03bb(t)e0(t) ] = E [ \u03bcdiag ( \u03b4(t) ) \u03bb(t) ] E[e0(t)] = 0, E [ 2\u03bcdiag ( \u03b4(t) ) \u03bb1(t)e0(t) ] = E [ 2\u03bcdiag ( \u03b4(t) ) \u03bb1(t) ] E[e0(t)] = 0, and assuming that \u03bb1(t) and \u03bb2(t) are independent of \u03b5(t) [17] yield E [", "startOffset": 264, "endOffset": 268}, {"referenceID": 15, "context": "Assuming convergence of R(t) and p(t) (which is true for a wide range of adaptive methods in the first stage [16], [14, 21]), we obtain limt\u2192\u221e E [", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "Assuming convergence of R(t) and p(t) (which is true for a wide range of adaptive methods in the first stage [16], [14, 21]), we obtain limt\u2192\u221e E [", "startOffset": 115, "endOffset": 123}, {"referenceID": 16, "context": "independent when i 6= j [17], [14], we get a recursion for E [ \u03bba(t)\u03bb T a (t) ]", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "independent when i 6= j [17], [14], we get a recursion for E [ \u03bba(t)\u03bb T a (t) ]", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Note that \u0393(t) and \u03b3(t) are derived for a wide range of adaptive filters [16], [14].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Note that \u0393(t) and \u03b3(t) are derived for a wide range of adaptive filters [16], [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "= 0 for most adaptive filters in the first stage [14] and if \u03bc is chosen so that all the eigenvalues of E [ I \u2212 \u03bcdiag ( w1(t) + w2(t) ) x(t)x (t) ] have strictly less than unit magnitude for every t, then limt\u2192\u221eE [", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "147] , is chosen as in [17].", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "Therefore, in the steady-state, we obtain the optimum vector approximately as wo = [1, 0].", "startOffset": 83, "endOffset": 89}], "year": 2012, "abstractText": "We investigate adaptive mixture methods that linearly combine outputs of m constituent filters running in parallel to model a desired signal. We use \u201cBregman divergences\u201d and obtain certain multiplicative updates to train the linear combination weights under an affine constraint or without any constraints. We use unnormalized relative entropy and relative entropy to define two different Bregman divergences that produce an unnormalized exponentiated gradient update and a normalized exponentiated gradient update on the mixture weights, respectively. We then carry out the mean and the mean-square transient analysis of these adaptive algorithms when they are used to combine outputs of m constituent filters. We illustrate the accuracy of our results and demonstrate the effectiveness of these updates for sparse mixture systems.", "creator": "LaTeX with hyperref package"}}}