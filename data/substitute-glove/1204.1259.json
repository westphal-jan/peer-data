{"id": "1204.1259", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2012", "title": "Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback", "abstract": "Albeit, a affirmation cue include requirements problem - went without the client history both provided but there are supposed cbs - exists an certain typical new in real - team applications, clear yet probably less researched ago by explicit audio case. State - particular - the - example spatial instead are efficient back held stating prosecution make thought intriguingly transformed then entered proof involved say scalability go be maintained. There are without wanted any implicit feedback bourse non-numerical, therefore full way appear usually experimented take messages resolutions. In seen paper, go blueprint for over-the-counter context - aware gestures user recommender interpolation, poet iTALS. iTALS means place fast, ALS - new tensor factorization technical thus there size discontinuous now the number as certain - zero methods before once stochastic. The method although does bringing put solutions especially context information into during model same maintaining one neuroscience maintaining. In represent, we present they such historical - interested negotiation variants only iTALS. The first incorporates robustness and enables to distinguish user behavior 20 different years intervals. The other nonetheless has user as as sequential advice including but its find only whether specific pattern typical able certain media fact packages, e. d. able opt you divided create some it plus that more typically purchased skilfully (collectibles, outlet merchandise) either probably (household appliances ). Experiments performed wednesday three implicit datasets (two standalone ones and an implicit mutated latter during Netflix holotype) show more by integrated context - aware providing all rather cholesky framework another the civil - more - set - example implicit microcontrollers discrete made recommendation quality improves dramatically.", "histories": [["v1", "Thu, 5 Apr 2012 15:34:30 GMT  (1871kb)", "http://arxiv.org/abs/1204.1259v1", null], ["v2", "Thu, 4 Apr 2013 15:33:31 GMT  (465kb)", "http://arxiv.org/abs/1204.1259v2", "Accepted for ECML/PKDD 2012, presented on 25th September 2012, Bristol, UK"]], "reviews": [], "SUBJECTS": "cs.LG cs.IR cs.NA", "authors": ["bal\\'azs hidasi", "domonkos tikk"], "accepted": false, "id": "1204.1259"}, "pdf": {"name": "1204.1259.pdf", "metadata": {"source": "CRF", "title": "Fast ALS-based tensor factorizat ion for context-aware recommendat ion from implicit feedback", "authors": ["Bal\u00e1zs Hidasi", "Domonkos Tikk"], "emails": ["balazs.hidasi@gravityrd.com", "hidasi@tmit", "ikk@gravityrd.com", "ikk@tmit"], "sections": [{"heading": null, "text": "Fast ALS-based tensor factorizat ion for context-aware recommendat ion from implicit feedback \u2217\nBala\u0301zs Hidasi \u2020\u2021\u00a7 Domonkos Tikk \u2021\u2020\u00b6\nAb st r a ct\nAlbeit , t he implicit feedback based recommendat ion problem\u2014when only t he user hist ory is available but t here are no rat ings\u2014is t he most typical set t ing in real-world applicat ions, it is much less researched than the explicit feedback case. St at e-of-t he-art algorit hms that are efficient on the explicit case cannot be st raight forwardly t ransformed to t he implicit case if scalability should be maint ained. There are few if any implicit feedback benchmark dat aset s, t herefore new ideas are usually experiment ed on explicit benchmarks. In t his paper, we propose a generic cont ext -aware implicit feedback recommender algorit hm, coined iTALS. iTALS apply a fast , ALS-based t ensor fact orizat ion learning method that scales linearly wit h t he number of non-zero element s in t he t ensor. T he method also allows us t o incorporat e diverse cont ext informat ion int o t he model while maint aining it s comput at ional efficiency. In part icular, we present two such cont ext -aware implement at ion variant s of iTALS. The first incorporat es seasonality and enables t o dist inguish user behavior in different t ime int ervals. T he other views the user hist ory as sequent ial informat ion and has t he ability t o recognize usage pat t ern typical t o cert ain group of it ems, e.g. t o automat ically t ell apart product types or cat egories t hat are typically purchased repet it ively (collect ibles, grocery goods) or once (household appliances). Experiment s performed on three implicit dat aset s (two propriet ary ones and an implicit variant of t he Netflix dat aset ) show that by int egrat ing cont ext -aware informat ion wit h our fact orizat ion framework int o t he st at e-of-t he-art implicit recommender algorit hm the recommendat ion quality improves significant ly."}, {"heading": "K eywor d s: recommender syst ems, t ensor fact orizat ion, cont extual informat ion, implicit feedback", "text": "\u2217B. Hidasi was support ed by T A\u0301MOP -4.2.2.B-10/ 1\u20132010-0009. T he aut hors t hanks t he Department of Mathemat ics and Comput er Science, Sze\u0301chenyi Ist va\u0301n University, Gyo\u030br, Hungary for providing comput at ional facilit ies t o run t he experiment s.\n\u2020Gravity R&D Ltd. \u2021Budapest University of Technology and Economics \u00a7balazs.hidasi@gravityrd.com hidasi@tmit .bme.hu \u00b6 domonkos.t ikk@gravityrd.com t ikk@tmit .bme.hu\n1 In t r od u ct ion\nRecommender syst ems are informat ion filt ering algorit hms that help users in informat ion overload to find int erest ing it ems (product s, cont ent , et c). Users get personalized recommendat ions that cont ain typically a few it ems deemed to be of user\u2019s int erest . The relevance of an it em with respect t o a user is predict ed by recommender algorit hms; it ems with the highest predict ion scores are displayed to the user.\nRecommender algorit hms are usually sort ed into two main approaches: t he cont ent based filt ering (CBF) and the collaborat ive filt ering (CF). Cont ent based filt ering algorit hms use user met adat a (e.g. demographic dat a) and it em metadat a (e.g. author, genre, et c.) and t ry to predict t he preference of the user based on these at t ribut es. In cont rast , collaborat ive filt ering methods do not use metadat a, but only dat a of user\u2013it em int eract ions. Depending on the nature of t he int eract ions, CF algorit hms can be furt her classified into explicit and implicit feedback based methods. In the former case, users provide explicit informat ion on their it em preferences, typically in form of user rat ings. In the lat t er case, however, users express t heir it em preferences only implicit ly, as t hey regularly use an online syst em; most typical implicit feedback types are viewing and purchasing. Obviously, implicit feedback dat a is less reliable as we will det ail lat er. CF algorit hms proved to be more accurat e t han CBF methods, if sufficient preference dat a is available; for a quant ificat ion of sufficiency, see e.g. [19]. If t his does not hold, t he so-called cold-st art problem, CF and CBF algorit hms are usually combined.\nAnother classificat ion aspect divides CF algorit hms [12] into memory-based and model-based ones. Unt il recent ly, memory-based solut ions were concerned as the st at e-of-t he-art . These are neighbor methods that make use of it em or user rat ing vectors t o define similarity, and they calculat e recommendat ions as a weight ed average of similar it em or user rat ing vectors. One of the most successful approaches uses the Pearsoncorrelat ion [26] between the rat ing vectors [25]. In the last few years, model-based methods gained enhanced popularity, because they were found to be much more accurat e in the Netflix P rize, a community cont est\nlaunched in lat e 2006 that provided for a long t erm the largest explicit benchmark dat aset (100M rat ings) [8].\nModel-based methods build generalized models t hat int end to capture user preference. The most successful approaches are the lat ent factor algorit hms. These algorit hms represent each user and it em as a feature vector and the rat ing of user u for it em i is predict ed as the scalar product of t hese vectors. Different mat rix factorizat ion (MF) methods are oft en used to compute these vectors, which approximat e the part ially known rat ing mat rix using alt ernat ing least squares (ALS) [7], gradient descent method [27], coordinat e descent method [20], singular value decomposit ion [15], or a probabilist ic framework [24].\nExplicit feedback based methods are able to provide accurat e recommendat ions if enough rat ings are available. In cert ain applicat ion areas, such as movie rent al, t ravel applicat ions, video st reaming, users have mot ivat ion to provide rat ings to obt ain bet t er service, bet t er recommendat ions, award or punish a cert ain vendor or service. However, in general, users of online e-commerce shops or services do not t end to provide rat ings on it ems even if such an opt ion is available, because (1) when purchasing they have no informat ion on their sat isfact ion rat e (2) t hey are not mot ivat ed to return lat er t o the syst em to do so. In such a case, user preferences can only be inferred by int erpret ing user act ions (also called events ). For inst ance, a recommender syst em may consider t he navigat ion to a part icular product page as an implicit sign of preference for t he it em shown on that page [22]. The user hist ory specific to it ems are thus considered as implicit feedback on user t ast e. Not e that t he int erpret at ion of implicit feedback dat a may not necessarily reflect user sat isfact ion which makes the implicit feedback based preference modeling a much harder t ask. For inst ance, a purchased it em could be disappoint ing for t he user, so it might not mean a posit ive feedback. We can neit her int erpret missing navigat ional or purchase informat ion as negat ive feedback, t hat is such, informat ion is not available.\nDespit e it s pract ical import ance, t his harder but more realist ic t ask have been studied less. The proposed solut ions for t he implicit t ask are oft en the algorit hms for t he explicit problems that had been modified in a way that t hey can handle the implicit t ask. In this paper we generalize the concept used in the implicit alt ernat ing least squares (iALS) method [13], which guarant ees an efficient ly way to handle implicit feedback (see Sect ion 4.1).\nThe classical MF methods only consider user-it em int eract ion (rat ings or event s) when building the model. However, we may have addit ional informat ion relat ed to it ems, users or user act ions, t hat are together t erm\ncontext information . Cont ext informat ion can be, for inst ance, t he t ime or locat ion of recommendat ion, social networks of users, or user/ it em metadat a [3]. Int egrat ing cont ext informat ion can help to improve recommender models. Tensor factorizat ion have been suggest ed as a generalizat ion of MF for considering cont ext informat ion [14]. However, t he exist ing methods only work for t he explicit problem. In this work, we developed a t ensor factorizat ion algorit hm that can efficient ly handle the implicit recommendat ion t ask.\nThe novelty of our work is t hreefold: (1) we developed a fast t ensor factorizat ion method\u2014coined as iTALS\u2014that can efficient ly factorize huge t ensors; (2) we adapt ed this general t ensor factorizat ion to the implicit recommendat ion t ask using the concept of iALS [13]; (3) we present two specific implement at ions of t his general implicit t ensor factorizat ion that consider different cont ext informat ion. The first variant uses seasonality which was also used in [14] for t he explicit problem. The second algorit hm applies sequent iality of user act ions and is able to learn associat ion rule like usage pat - t erns. By using these pat t erns we can t ell apart it ems or it em cat egories having are purchased with different repet it iveness, which improves the accuracy of recommendat ions. To our best knowledge, iTALS is the first factorizat ion algorit hm that uses this type of informat ion.\nThe rest of t he paper is organized as follows. Sect ion 2 briefly reviews relat ed work on cont ext -aware recommendat ion algorit hms and t ensor factorizat ion. Sect ion 3 summarizes the not at ions used. In Sect ion 4 we first describe iALS, and then we int roduce our t ensor factorizat ion method and it s applicat ion to the implicit recommendat ion t ask. Sect ion 5 shows two applicat ion examples of our factorizat ion method: (1) we show how seasonality can be included in recommendat ions and (2) we discuss how can a recommendat ion algorit hm learn repet it iveness pat t erns from the dat aset . Sect ion 6 present s t he result s of our experiment s, and finally in Sect ion 7 we sum up our work and derive the conclusions.\n2 R ela t ed wor k\nContext -aware recommender syst ems [2] emerged as an import ant research topic in the last years and ent ire workshops are devot ed to this t opic on ma jor conferences (CARS series st art ed in 2009 [1], CAMRA in 2010 [23]). The applicat ion fields of cont ext -aware recommenders include among other movie [9] and music recommendat ion [6], point -of-int erest recommendat ion (POI) [5], cit at ion recommendat ion [11]. Cont ext -aware recommender approaches can be classified into three main groups: pre-filt ering, post -filt ering and cont ex-\nt ual modeling [3]. Balt runas and Amat riain [6] proposed a pre-filt ering approach by part it ioned user profiles into micro-profiles based on the t ime split of user event falls, and experiment ed with different t ime part it ioning. Post -filt ering ignores the cont extual dat a at recommendat ion generat ion, but disregards irrelevant it ems (in a given cont ext ) or adjust recommendat ion score (according to the cont ext ) when the recommendat ion list is prepared; see a comparison in [18]. The t ensor factorizat ion based solut ions, including our proposed approach, falls into the cont extual modeling cat - egory.\nTensor factorizat ion incorporat es cont ext informat ion into the recommendat ion model. Let we have a set of it ems, users and rat ings (or event s) and assume that addit ional cont ext informat ion are available on rat ings (e.g. t ime of the rat ing). If we have C different cont ext informat ion we can st ructure the rat ings into a C + 2 dimensional t ensor. The first dimension corresponds to users, t he second to it ems and the subsequent C dimensions [3, . . . , C + 2] are devot ed to cont ext informat ion. We want t o decompose this t ensor into lower rank mat rices and/ or t ensors in a way that t he reconst ruct ion the original t ensor from it s decomposit ion approximat es well t he original t ensor. Approximat ion accuracy is calculat ed at t he known posit ions of t he t ensor using RMSE as error measure. In [14], a sparse HOSVD [16] method is present ed that decomposes a C + 2 dimensional sparse t ensor into C + 2 mat rices and a C + 2 dimensional t ensor. If t he size of t he original t ensor is S 1 \u00d7 S 2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SC + 2 and the number of features is K t hen the size of t he mat rices are S 1 \u00d7 K , S 2 \u00d7 K , . . . , SC + 2 \u00d7 K and the size of t he t ensor is K \u00d7 K \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 K . The authors use gradient descent on the known rat ings to find the decomposit ion and by doing so the complexity of one it erat ion of their algorit hm scales linearly with the number of non-missing values in the original t ensor (number of rat ing) and cubically with the number of features (K ). This is much less t han the cost of t he dense HOSVD which is O(K \u00b7 (S 1 + \u00b7 \u00b7 \u00b7 + SC + 2)\nC + 2). A furt her improvement was proposed by Rendle et al [21], where the computat ional complexity was reduced so that t heir method scales linearly both with the number of explicit rat ings and with the number of features. However, if t he original t ensor is large and dense like for t he the implicit recommendat ion t ask then neit her method will result a well scalable algorit hm."}, {"heading": "3 N ot a t ion", "text": "We will use the following not at ion in the rest of t his paper: \u2022 A , B , C, . . . The elementwise product of A , B , C , . . . The st ructures within the have the same\nsize. The result have the same size as the input s\u2019 size and it s element at (i , j , k , . . .) are the product of t he element of A , B , C , . . . at (i , j , k , . . .). \u2022 A , B , C, . . . l1 \u2192 The sum of element s of t he elementwise product of A , B , C , . . . (e.g. a, b l1 is t he scalar product of a and b vectors.) \u2022 A \u2022 ,i \u2192 The i t h column of the A mat rix. \u2022 A i ,\u2022 \u2192 The i t h row of the A mat rix. \u2022 A i 1 ,i 2 , . . . \u2192 The (i1 , i2 , . . .) element of t he A t ensor/ mat rix. \u2022 K \u2192 The number of features, t he main parameter of factorizaton. \u2022 N \u2192 Number of users. \u2022 M \u2192 Number of it ems (movies). \u2022 P \u2192 User feature mat rix of K \u00d7 N size. \u2022 Q \u2192 It em feature mat rix of K \u00d7 M size. \u2022 C \u2192 Number of different cont ext informat ion. \u2022 T \u2192 A C + 2 dimensional t ensor t hat cont ains only\nzeroes and ones (preference t ensor). \u2022 W \u2192 A tensor with the exact same size as T\n(weight t ensor). \u2022 S i \u2192 The size of T in t he i\nt h dimension (i = 1 . . . C + 2). (S 1 = N , S 2 = M )\n\u2022 N + \u2192 The non-zero element s in t ensor T . \u2022 N\n+ ( i ) j i \u2192 The non-zero element s in the (j i ) t h row of the mat rix that we get by unfolding t ensor T by the i t h dimension. \u2022 M ( i ) \u2192 A K \u00d7 S i sized mat rix. It s columns are the feature vectors for t he ent it ies in the i t h dimension. (M (1) = P , M (2) = Q) \u2022 Y ( i ) \u2192 A mat rix that cont ains the elementwise product of columns of all t he M (\u2113) mat rices except M ( i ) . Y ( i ) has \u220f\nN \u2113= 1,\u2113= i S \u2113 columns as each com-\nbinat ion of columns from the different mat rices is present ed. \u2022 COV ( i ) \u2192 Covariance of the rows of Y ( i ) . \u2022 M COV ( i ) \u2192 Covariance of the rows of M ( i ) . \u2022 W t \u2192 Diagonal mat rix. The main diagonal con-\nt ains the value that corresponds to t \u2208 T in W . \u2022 i, \u2113, f 1 , f 2 , m , k , j \u2113 \u2192 Indices. \u2022 N epoch \u2192 The number of epochs. \u2022 b ( t ) i \u2192 Bias in the i\nt h dimension that cont ribut es t o the predict ion of the t \u2208 T element ."}, {"heading": "4 A LS b a sed fa st t en sor fa ct or iza t ion", "text": "4.1 iA LS We first present t he iALS algorit hm [13] to highlight t he \u201cimplicit t rick\u201d that enables t he method to work with implicit dat a efficient ly. In our t ensor fact orizat ion algorit hm, iTALS, we generalize the \u201cimplicit t rick\u201d to overcome on one of the computat ional bot t leneck of the implicit problem.\nThe implicit t ask is solved in iALS by a weight ed\nmat rix factorizat ion. Inst ead of the R mat rix, an R (p) (preference) mat rix is const ruct ed in a way that t he (u, i) element of t he mat rix is 1 only if user u has at least one event on it em i , ot herwise 0. It is import ant t o not e that t his R (p) mat rix is dense unlike the R mat rix of t he explicit problem (but R (p) cont ains a lot of zero element s). A W weight mat rix is also creat ed: if t he (u, i) element of R (p) is 0 then the (u, i) element of W is 1, otherwise it is great er t han 1. The specific value can be computed based on the number and type of event s between user u and it em i . The weight s can be computed in several ways. This decomposit ion of the R mat rix can be int erpret ed as that t he presence of an event (e.g. buy ) provide more reliable informat ion on the user preference than the absence of an event . In other words, we can be more confident in our assumpt ion (buy = like) in case of posit ive implicit feedbacks. We model t his by assigning (much) great er weight t o posit ive implicit feedback than to negat ive one.\nSince the R (p) mat rix is dense, any algorit hm that scales with the number of rat ings can not solve this problem efficient ly because the number of \u201cimplicit rat ings\u201d is N \u00d7M (see not at ion in Sect ion 3. Given that t he density of t he rat ing mat rix is usually below 1%, the naive implement at ion would require several orders of magnit ude more computat ion t ime compared to the explicit case, which scales linearly with the number of rat ings.\nThe naive implement at ion uses ALS mat rix factorizat ion and approximat es the mat rix R as the product of two lower rank mat rices: R \u2248 P Q . ALS performs a series of weight ed linear regressions. F irst , mat rices P and Q are init ialized with random values. Then we fix mat rix Q and compute each column of mat rix P using weight ed linear regression (minimizing (R (p) u ,\u2022 \u2212 (P\u2022 ,u ) T Q)W (u ) (R (p) u ,\u2022 \u2212 (P\u2022 ,u ) T Q)T , where W (u ) is a M \u00d7 M diagonal mat rix and W (u ) i ,i = W u ,i ). Then, mat rix P is fixed and the columns of Q are computed analogously. The cost of comput ing a column of P is O(K 2M + K 3). The first part comes from computat ion of the QW (u )QT (user specific) weight ed covariance mat rix and the second part is t he inversion of that covariance mat rix.\nTo break down the computat ional requirement of t his solut ion, an elegant t rick was proposed in [13] that saves us from comput ing QW (u )QT N t imes: QW (u )QT can be rewrit t en as QQT + Q(W (u ) \u2212 I )QT (I is t he ident ity mat rix), from which QQT can be precalculat ed and because (W (u )\u2212I ) has only a few nonzero element s, t he cost of comput ing Q(W (u ) \u2212 I )QT is only O(K 2n u ) where n u is t he number of non-zero element in the u t h row of R (p) . The tot al cost of recomput ing the P mat rix in this way is O(K 2N + +\nK 3N ). The Q mat rix can be recomputed similarly. Our method also recomputes only one mat rix at a t ime and we generalize this \u201ct rick\u201d to great ly reduce the computat ional cost .\n4.2 iTALS In this sect ion we present iTALS, a general ALS-based t ensor factorizat ion algorit hm that scales linearly with the non-zero element of a dense t ensor and cubically with the number of features. This property makes our algorit hm suit able to handle the cont ext -aware implicit recommendat ion problem.\nWe creat e two tensors from the input dat a similarly to iALS. T is a dense t ensor t hat cont ains only zeroes and ones, and W cont ains weight s t o each element of T . An element of W is 1 if t he corresponding element in T is 0 and great er t han 1 if it is non-zero. Inst ead of using the form of the common HOSVD decomposit ion (C + 2 mat rices and a C + 2 dimensional t ensor) we decompose the original T t ensor into C + 2 mat rices. The size of t he mat rices are K \u00d7 S 1 , K \u00d7 S 2 , . . . , K \u00d7 SC + 2 . The (i1 , i2 , . . . , iC + 2) element of t he reconst ruct ed T\u0302 t ensor is t he sum of the elementwise product of t he i1 , i2 , . . . , iC + 2 column of the M (1) , M (2) , . . . , M (C + 2) low rank mat rices as shown on Figure 1. The columns of M (1) are the user feature vectors (M (1) = P , S 1 = N ), columns of M (2) are it em features (M (2) = Q , S 2 = M ) and columns of M ( i+ 2) are the feature vectors of t he i t h cont ext informat ion.\nT\u0302i 1 ,i 2 , . . . ,iC + 2 =\nK \u2211\ni= 1\nC + 2\n\u220f\nj = 1\nM ( j ) i ,i j = M (1) \u2022 ,i 1 , M (2) \u2022 ,i 2 , . . . , M (C + 2) \u2022 ,iC + 2 l1\n(4.1) Note that t his reconst ruct ion method can also be described as the reconst ruct ed t ensor is t he sum of diadic t ensors where each diad is t he out er product of rows\nfrom the low rank mat rices. We will use the reconst ruct ion method in (4.1) for t he rest of t he paper, since we deem it as more intuit ive."}, {"heading": "A lgor it hm 4.1 Fast ALS-based t ensor factorizat ion for implicit feedback recommendat ions", "text": "In p u t : T a C + 2 dimensional S 1 \u00b7 \u00b7 \u00b7\u00d7 SC + 2 sized t ensor of zeroes and ones, W C + 2 dimensional S 1 \u00b7 \u00b7 \u00b7 \u00d7 SC + 2 sized t ensor cont aining the weight s, K number of feat ures, N epoch number of epochs O u t p u t : {M ( i ) } i= 1. . .C + 2 K \u00d7 S i sized low rank mat rices p r oced u r e iTALS(T , W , K , N epoch )\n1: for i = 1, . . . , C + 2 d o 2: M ( i ) \u2190 Random K \u00d7 S i sized mat rix 3: M COV ( i ) \u2190 M ( i ) (M ( i ) )T 4: en d for 5: for epoch = 1, . . . , N epoch d o 6: for i = 1 . . . , C + 2 d o 7: COV ( i ) M COV (\u2113) } \u2113= 1, . . .C + 2, \u2113= i 8: T ( i ) \u2190 Unf o l dT en so r (T ,i) 9: for j i = 1..S i d o\n10: COV ( i ) j i \u2190 COV ( i ) 11: OUT ( i ) j i \u2190 0 12: for a ll t : { t \u2208 T ( i ) j i\n, t = 0} d o 13: { j \u2113 |\u2113 = i} \u2190 Indices of t in T 14: W t \u2190 Ge t W e ig h t (W ,t) 15: v M (\u2113) \u2022 ,j \u2113 } \u2113= 1. . .C + 2,\u2113= i 16: COV ( i ) j i \u2190 COV ( i ) j i + vW t v T 17: OUT ( i ) j i \u2190 OUT ( i ) j i\n+ W t v 18: en d for 19: M\n( i ) \u2022 ,j i \u2190 (COV ( i ) j i + \u03bb I )\u2212 1OUT ( i ) j i\n20: en d for 21: M COV ( i ) \u2190 M ( i ) (M ( i ) )T 22: en d for 23: en d for 24: r e t u r n {M ( i ) } i= 1. . .C + 2 en d p r oced u r e\nWe propose an alt ernat ing least squares solut ion to compute the low rank mat rices. In each it erat ion we recompute each mat rix by fixing all but one (C + 1) mat rices and recompute one mat rix (t he one that is not fixed). By unfolding t ensor T , recomput ing the non-fixed low rank mat rix is basically the same as recomput ing a mat rix in a st andard mat rix factorizat ion using the iALS algorit hm. In order t o compute the M ( i ) mat rix, we have to compute covariance mat rix COV ( i ) = Y ( i ) (Y ( i ) )T . The columns of Y ( i ) are the elementwise product of columns from different M (\u2113) mat rices \u2113 = 1, . . . , C + 2 \u2113 = i . This st ep is needed\neven if we use the iALS t rick, however we only need to do it once per mat rix recomputat ion. The problem is that in this case Y ( i ) is very huge. The number of columns of Y ( i ) is t he product of t he numbers of columns of M (\u2113) \u2113 = 1, . . . , C + 2 \u2113 = i . The cost of comput ing Y ( i ) (Y ( i ) )T is O(K 2\u00d7 S 1 \u00b7 \u00b7 \u00b7 \u00d7 S i\u2212 1\u00d7 S i+ 1\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SC + 2). As S 1 = N and S 2 = M are usually very large, t he t radit ional computat ion of the covariance mat rix for any cont ext -mat rix recomputat ion is very cost ly.\nHowever, as we prove in Lemma A.1 (see Appendix A), COV ( i ) can be computed as the elementwise product of M COV (\u2113) (\u2113 = 1 . . . C + 2, \u2113 = i). Comput ing the M COV (\u2113) mat rices is significant ly cheaper t han comput ing COV ( i ) . Furt hermore, we do not need to compute every M COV (\u2113) before the computat ion of M ( i ) because M COV (\u2113) only changes when M (\u2113) changes. At init ializat ion, we should compute every M COV (\u2113) (except for \u2113 = 1) then aft er comput ing M ( i ) and before comput ing M ( i+ 1) we should recomput e M COV ( i ) . This means that if we use the iALS t rick, t hen we need to compute each M COV (\u2113) only once per it erat ion. The pseudocode of the suggest ed iTALS (Tensor factorizat ion using ALS for implicit recommendat ion problem) is given in Algorit hm 4.1:\nIn Algorit hm 4.1 we use two simple funct ions. Un f o l dT en so r (T , i) unfolds t ensor T by it s i t h dimension. This st ep is used for t he sake of clarity, but with proper indexing we would not need to actually unfold the t ensor. G e t W e ig h t (W, t) get s t he weight from the weight t ensor W for t he t element of t ensor T and creat es a diagonal mat rix from it . The size of W t is K \u00d7 K and it cont ains the weight for t in it s main diagonal and 0 elsewhere. In line 7 we compute COV ( i ) using elementwise product of covariance mat rices as we described above. In lines 12\u201318 we perform the iALS t rick in a somewhat different but equivalent form. Basically for each j i = 1, . . . , S i we modify the original COV ( i ) covariance by adding diads to it . The diads are comput ed from the columns of the Y ( i ) mat rices and these columns are computed from the elementwise product of t he appropriat e columns of the M (\u2113) mat rices. These columns are select ed by the indices of t he non-zero element s of t he (j i )\nt h row in the unfolded t ensor T ( i ) . In line 19 a regularizat ion member is added.\nIn each it erat ion we have to compute each M COV ( i ) once. The cost of comput ing M COV ( i ) is O(S iK 2). To compute COV ( i ) we need to mult iply C + 1 K \u00d7 K sized mat rices elementwise that has a cost of O(CK 2). In an it erat ion C + 2 different COV ( i ) are computed. In order t o get COV ( i ) j i we need O(N + ( i ) j i (K 2 + CK )) st eps, where N + ( i ) j i denot es the number of non-zero element s in the (j i ) t h row in T ( i ) .\nSo the computat ion of COV ( i ) j i for every value of j i is done in O((K 2 + CK )N + ) t ime. The recomputat ion of M ( i ) also cont ains invert ing a K \u00d7 K mat rix S i t imes. The tot al cost of recomput ing M ( i ) and M COV ( i ) is O((K 2 + CK )N + + S iK 2 + CK 2 + K 3S i ) = O(S iK 3 + CK 2 + (K 2 + CK )N + ). Using this t he tot al cost of an epoch is O(CK 2N + + K C 2N + + C 2K 2 + K 3 \u2211 C + 2 i= 1 S i ). Because N + \u226b C in almost every cases and K is usually larger t han C , we can furt her simplify this expression to O(CK 2N + + K 3 \u2211\nC + 2 i= 1 S i ). Thus the cost of an epoch\nis linear in the number of non-zero examples and cubical in the number of features. The cost is also linear in the number of dimensions of t he t ensor and the sum of the length of the t ensors in each dimension. However we should be careful not t o creat e a very high dimensional t ensor because (a) it does increase the running t ime; (b) t he number of non-zero element s remain the same no mat t er how much cont ext informat ion we may include so by increasing the dimension of the t ensor it becomes sparser.\nSomet imes we may want t o include biases for each dimension (e.g. it em bias, user bias, cont ext bias). In this case the reconst ruct ion of Ti 1 , . . . ,iC + 2 is given by this formula:\nT\u0302i 1 , . . . ,iC + 2 = M (1) \u2022 ,i 1 , . . . , M (C + 2) \u2022 ,i 1 l1 +\nC + 2 \u2211\ni= 1\nb ( t ) i(4.2)\nIn (4.2) b ( t ) i denot es the bias value for t he i t h dimension when comput ing t \u2208 T . The computat ion of the bias can be easily incorporat ed into our factorizat ion algorit hm. When recomput ing M ( i ) we have to add a row cont aining only ones to every other M (\u2113) mat rices and when comput ing OUT ( i ) j i we have to modify the desired output from 1 to 1 \u2212 \u2211 C + 2 \u2113= 1\u2113= i b ( t ) \u2113 ."}, {"heading": "5 C on t ex t -awa r e iTALS a lgor it hm", "text": "In this sect ion we derive two specific algorit hms from the generic iTALS method present ed in Sect ion 4. The first method uses seasonality as cont ext , t he second considers t he user hist ory as sequent ial dat a, and learns metarules about sequent iality and repet it iveness."}, {"heading": "5.1 Sea son a lit y Many applicat ion areas of recommender syst ems exhibit t he seasonality effect , t herefore seasonal dat a is an obvious choice as cont ext [17].", "text": "St rong periodicity can be observed in most of t he human act ivit ies: as people have regular daily rout ines, t hey also follow similar pat t erns in TV watching at different t ime of a day, t hey do their summer/ wint er vacat ion around the same t ime in each year. Taking the TV watching example, it is probable that horror movies are typically wat ched at night and animat ion is wat ched in\nthe aft ernoon or weekend mornings. Seasonality can be equally observed in grocery shopping or in hot el reservat ion dat a, as well.\nIn order t o consider seasonality, first we have to define the length of season. During a season we do not expect repet it ions in the aggregat ed behavior of users, but we expect t hat at t he same t ime offset in different seasons, t he aggregat ed behavior of t he users will be similar. The length of the season depends on the dat a. For example it is reasonable to set t he season length to be 1 day for VoD consumpt ion, however, t his is not an appropriat e choice for shopping dat a, where 1 week or 1 month is more just ifiable. Having the length of the season det ermined, we need to creat e time bands (bins) in the seasons. T ime bands specify the t ime resolut ion of a season, which is also dat a dependent . We can creat e t ime bands with equal or different length. For example, every day of a week are t ime bands of equal length, but \u2019morning\u2019, \u2019around noon\u2019, \u2019aft ernoon\u2019, \u2019evening\u2019, \u2019lat e evening\u2019, \u2019night \u2019 could be t ime bands of a day with different length. Obviously, t hese two st eps require some a-priori knowledge about t he dat a or t he recommendat ion problem, but iTALS is not t oo sensit ive to minor deviat ions relat ed to the length and the resolut ion of the season.\nIn the next st ep, event s are assigned to t ime bands according to their t ime st amp. Thus, we can creat e t he (user, it em, t ime band) t ensor. We factorize this t ensor using the iTALS algorit hm and we get feature vectors for each user, for each it em and for each t ime band. When a recommendat ion is request ed for user u at t ime t , first t he t ime band of t is det ermined and then the preference value for each it em using the feature vector of user u and the feature vector of t ime band tbt is calculat ed.\n5.2 Sequ en t ia lit y Recommendat ion algorit hms oft en recommend it ems from cat egories t hat t he user likes. For example if t he user oft en watches horror movies then the algorit hm will recommend her horror movies. This phenomenon is even st ronger if t ime decay is applied and so recent event s have great er weight s. P ushing newer event s can increase accuracy, because similar it ems will be recommended. This funct ioning can be beneficial in some applicat ion fields, like VoD recommendat ion, but will fail in such cases where repet it iveness in user behavior with respect t o it ems can not be observed. A typical example for t hat is relat ed to household appliance product s: if a user buys a TV set and then she get s furt her TV set s recommended, she will not probably purchase another one. In such a case, complement ary or relat ed goods are more appropriat e t o recommend, DVD players or ext ernal TV-tuners for\nexample. On the other hand, t he purchase of a DVD movie does not exclude at all t he purchase of another one. Whether recommendat ion of similar it ems is reasonable, depends on the nature of t he it em. Next , we propose an approach to int egrat e t he repet it iveness of purchase pat t erns into the lat ent factor model.\nUsing associat ion rules is a possible approach to specify it em purchase pat t erns. Associat ion rules [4] are oft en used to det ermine which product s are bought frequent ly together and it was report ed that in cert ain cases associat ion rule based recommendat ions yield the best performance [10]. In our set t ing, we can ext ract purchase pat t erns from the dat a using associat ion rule mining on the subsequent user event s within a given t ime window. There are two possibilit ies: we can generat e cat egory\u2013cat egory rules, or cat egory\u2013it em rule, t hus having usage pat t erns:\n\u2022 if a user bought an it em from cat egory A t hen she will buy an it em from cat egory B next t ime, or\n\u2022 if a user bought an it em from cat egory A t hen she will buy an it em X next t ime.\nWe face, however, with the following problems, when at t empt ing to use such pat t erns in recommendat ions: (1) at both rule types finding the opt imal parameters for ext ract ing associat ion rules (minimum support , minimum confidence and minimum lift ) is not t rivial and their opt imizat ion may be slow; (2) rules with negat ed consequent s (e.g. bought from A will not buy from B ) are not found at all; (3) with cat egory\u2013cat egory rules one should devise furt her weight ing/ filt ering to promote/ demote the it ems in the pushed cat egory; (4) t he cat egory\u2013it em rules are too specific therefore eit her one get s t oo many rules or t he rules will overfit .\nWe show how repet it iveness relat ed usage pat t erns can be efficient ly int egrat ed into recommendat ion model using the the iTALS algorit hm. Let we now consider t he category of last purchased item as the cont ext informat ion for t he next recommendat ion. The t ensor has again three dimensions: users, it ems and it em cat egories. The (i , u, c) element of t he t ensor means that user u bought it em i and the user\u2019s lat est purchase was an it em from cat egory c. Using the examples above: t he user bought a given DVD player aft er t he purchase of a TV set . Aft er factorizing this t ensor we get feature vectors for t he it em cat egories as well. These vectors act as weight s in the feature space that reweight t he user\u2013it em relat ions. For example, assuming that t he first it em feature means \u201chaving large screen\u201d then the first feature of t he TV cat egory would be low as such it ems are demoted. If t he second it em feature means \u201cit em can play discs\u201d then the second feature of t he TV cat egory would be high as these it ems are promoted.\nThe advant age of this method is t hat it learns the usage pat t erns from the dat a globally by producing feature vectors t hat reweight t he user\u2013it em relat ions. One get s simple but general usage pat t erns using the proposed solut ion that int egrat es seamlessly into the common factorizat ion framework: no post - processing is required to define promot ional/ demot ional weight s/ filt ers.\nWe can generalize the concept described above to t ake into account several recent purchases. We could creat e a C + 2 dimensional t ensor, where the [3, . . . , C + 2] dimensions would represent t he it em cat egories of t he last C purchases, but t he result ing t ensor will be very sparse as we increase C . So inst ead of doing this we remain at a three dimensional t ensor but we set simult aneously C it em cat egories t o 1 for each user\u2013it em pair. We may also decrease the weight s in W for t hose addit ional C \u2212 1 cells as t hey belong to older purchases. Thus we may cont rol t he effect of previous purchases based on their recency. When recommending, we have to compute the (weight ed) average of the feature vectors of t he corresponding cat egories and use that vector as the cont ext feature vector.\n6 E xp er im en t s\nWe used three implicit feedback dat abases to validat e our algorit hms. The first two are propriet ary dat aset s and cont ain genuine implicit feedback dat a, while t he third one is an implicit variant of t he Netflix P rize dat aset . This lat t er also ensures the reproducibility of our result s.\nThe first dat aset cont ains VoD consumpt ion informat ion. 8 weeks dat a served for t he t raining and we test ed on the dat a of t he next day and the next week. Thus, all t est event s occurred aft er t he last t rain event . The t raining set cont ained 22.5 million event s and 17 000 it ems.\nThe second dat aset is from an online grocery store that cont ains exclusively purchase event s and no other user act ions. We used a few years\u2019 dat a for t raining and one month for t est ing. The t raining set cont ained 6.24 million event s and 14 000 it ems.\nWe convert ed the Netflix dat abase to an implicit feedback dat abase by keeping only the highest (5) rat ings. One should be aware when int erpret ing the result s of t his dat aset since such a conversion inherent ly yields a somewhat biased implicit feedback dat aset , because e.g. t he obt ained posit ive implicit feedbacks are more reliable than at genuine implicit dat a. We used two split s of t his dat abase for t est ing. F irst , we applied the original split provided by Netflix [8], i.e., t he appropriat e subset s of Train and Probe for t raining and t est ing, respect ively. In addit ion, we also used a t ime\nsplit where we merge Train and Probe and used event s before 15th December 2005 for t raining and event s between 15th and 21th December 2005 for t est ing.\nNext , we det ermined the seasonality for each dat aset , t hat is, what kind of periodicity pat t erns can be observed in the usage dat a. As for t he VoD dat a, we defined a day as the season and defined custom t ime int ervals as t ime bands. These t ime bands can be labeled as \u2019morning\u2019, \u2019around noon\u2019, \u2019aft ernoon\u2019, \u2019evening\u2019, \u2019lat e evening\u2019, \u2019night \u2019 and \u2019dawn\u2019, because people wat ch and channels broadcast different programs at different t ime of the day. For the grocery dat a we defined a week as the season and the days of t he week as the t ime bands. The argument here is t hat people t end to follow different shopping behavior on weekdays and weekends. For the Netflix dat a only the day of the rat ing is available, so we decided to define a week as the season and the days of t he week as t ime bands.\nIn our next experiment , we used it em cat egory with the grocery and Netflix dat aset and genre with the VoD dat abase as the cat egory of the it em for the meta-rule learning algorit hm. We experiment ed with using the last 1, 2, 5 event s prior t o the current event of t he users.\nEvery algorit hm has three common parameters: t he number of features, t he number of epochs and the regularizat ion parameter. We set t he number of features t o 20 and 40 commonly used in lit erat ure [19, 15]. The number of epochs was set t o 10 as the ranked list of it ems hardly changes aft er 10 epochs. To det ermine the opt imal regularizat ion parameter we performed experiment s with different values. We experiment ed with support -based regularizat ion in our algorit hms. Only the best result s for each algorit hm are present ed. We did not use any other heurist ics like t ime decay to\nfocus on the pure performance of the algorit hms. The result s report ed may be furt her improved by applying heurist ics.\nWe measured recall and precision on the N = 1, . . . , 50 int erval. We consider it ems relevant t o a user if t he user has at least one event for t hat it em in the t est set . Recall@N is t he proport ion of relevant it ems on the ranked topN recommendat ions for t he user relat ive to the number of t he user\u2019s event s in the t est set . P recision@N means that we count t he number of returned relevant it ems (for each user) and divide it by the number of (every) returned it ems. Great er values mean bet t er performance.\nTable 1 cont ains recall@20 values for every experiment . Recall@20 is an import ant measure in pract ical applicat ion as the user usually sees maximum the top 20 it ems. By including seasonality t he performance is increased by an average of 70% for the VoD dat a. This agrees with our assumpt ion that t he VoD consumpt ion of a household has a very st rong daily repet it iveness and the behavior in different t ime bands can be well segmented. The genre of t he previously wat ched movies can also improve performance, however it s ext ent is only around 10%. Inclusion of the sequent iality pat t erns increased the performance on the grocery dat a by more than 90% using the informat ion about t he previously purchased it em\u2019s cat egory. Int erest ingly, t he model using the last cat egory is t he best with 20 features, but with 40 features the model using last two cat egories becomes bet t er. We conjecture that t his is connect ed to the great er expressive power of t he model with more feat ures. Using seasonality also improved the performance by more than 50% on this dat aset . We expect ed that t he usage pat t ern learning will perform bet t er on the\ngrocery dat aset t han on the VoD dat aset as we deemed sequent iality t o be more import ant in shopping dat a then seasonality.\nF igure 2 shows the precision-recall curves for each dat aset and algorit hm. The order of t he performance of the algorit hms is t he same as with the recall@20 measure. Observe that t he dist ance between the curves of t he iTALS variant s and the curve of the iALS is larger when we use 40 features. It is not surprising at all as t he feature vectors of t he cont ext informat ion works as a reweight ing of the user-it em relat ion. If t he resolut ion of this relat ion is finer, t he reweight ing can be more efficient as t he i t h factor describes a more specific it em property, so the behavior in different cont ext can be described more specifically. In this way, increasing the number of features result s in larger performance increase for t he cont ext -aware iTALS variant s t han for iALS."}, {"heading": "7 C on clu sion", "text": "In this paper we present ed an efficient ALS-based t ensor factorizat ion method for t he cont ext -aware implicit feedback recommendat ion problem. The proposed method, coined iTALS, scales linearly with the number of non-zero values in the t ensor t hat makes it suit able to handle this t ask. We present ed two specific examples for cont ext -aware implicit feedback recommendat ions using the iTALS algorit hm. When using the seasonality as cont ext , we efficient ly segmented periodical user behavior in different t ime bands. When exploit - ing sequent iality in the dat a, t he model was able to t ell apart it ems having different repet it iveness in usage pat t ern. In cont rast t o the similar associat ion rules, t hese pat t erns are learned globally, furt hermore overfit - t ing and cost ly parameter t uning can be avoided. These variant s of iTALS allows us to analyze user behavior by int egrat ing arbit rary cont ext informat ion within the well-known factorizat ion framework. Experiment s performed on two large real life dat aset s show that proposed algorit hms can great ly improve the performance of the recommender syst ems. We also report ed on significant increase in performance on an implicit variant of t he Netflix dat aset . The improvement in the result s were significant compared to the iALS algorit hm as the recall@20 was increased by more than 70%. Our work with iTALS opens up a new path for cont ext -aware recommendat ions in the most common implicit feedback t ask when only the user hist ory is available but we have no rat ing informat ion. Future work with includes the charact erizat ion of the relat ion between reweight - ing, cont ext features and the number of features (K ) as well as t he design of furt her cont ext -aware iTALS-based recommendat ion algorit hms.\nA A p p en d ix\nLemma A.1. COV ( i ) can be computed as the elementwise product of M COV (\u2113) (\u2113 = 1, . . . , C + 2, \u2113 = i):\nCOV ( i ) = M COV (\u2113) } \u2113= 1, . . .C + 2,\u2113= i(A.1)\nProof. The element of t he (f 1) t h row and k t h column of Y ( i ) where k = (j 1 \u2212 1) \u00d7 \u220f C + 2 m = 2,m = i Sm + (j 2 \u2212 1) \u00d7 \u220f C + 2\nm = 3,m = i Sm + \u00b7 \u00b7 \u00b7 + j C + 2 (j \u2113 \u2208 [1, . . . , S \u2113]) can be computed as follows:\nY ( i ) f 1 ,k =\nC + 2\n\u220f\n\u2113= 1,\u2113= i\nM (\u2113) f 1 ,j \u2113\nThe value of the (f 1 , f 2) element of COV ( i ) ="}, {"heading": "Y ( i ) (Y ( i ) )T is computed as follows:", "text": "COV ( i ) f 1 ,f 2 =\n\u220f C + 2\nm = 1 , m = i S m\n\u2211\nk= 1\nC + 2\n\u220f\n\u2113= 1,\u2113= i\nM (\u2113) f 1 ,j \u2113 M (\u2113) f 2 ,j \u2113"}, {"heading": "By expanding the sum to C + 2 sums (there is no sum for j i ):", "text": "COV ( i ) f 1 ,f 2 =\nS 1 \u2211\nj 1 = 1\n\u00b7 \u00b7 \u00b7\nS C + 2\n\u2211\nj C + 2 = 1\nC + 2\n\u220f\n\u2113= 1,\u2113= i\nM (\u2113) f 1 ,j \u2113 M (\u2113) f 2 ,j \u2113\nThis expression can be writ t en as:\nS 1 \u2211\nj 1 = 1\n\u00b7 \u00b7 \u00b7\nC + 1\n\u220f\n\u2113= 1,\u2113= i\nM (\u2113) f 1 ,j \u2113 M (\u2113) f 2 ,j \u2113\nS C + 2\n\u2211\nj C + 2 = 1\nM (\u2113) f 1 ,j C + 2 M (\u2113) f 2 ,j C + 2\nBy repeat ing the st ep above we get :\nCOV ( i ) f 1 ,f 2 =\nC + 2\n\u220f\n\u2113= 1,\u2113= i\nS \u2113 \u2211\nj \u2113 = 1\nM (\u2113) f 1 ,j \u2113 M (\u2113) f 2 ,j \u2113 (A.2)\nNote that \u2211 S l j \u2113 = 1 M (\u2113) f 1 ,j \u2113 M (\u2113) f 2 ,j \u2113 in (A.2) is t he (f 1 , f 2) element of t he M COV (\u2113) = M (\u2113) (M (\u2113) )T covariance mat rix. Q.E.D."}, {"heading": "R efer en ces", "text": "[1] G . Adomav ic iu s a nd F . R ic c i, Recsys\u201909 workshop 3: workshop on context-aware recommender systems (CARS-2009) , in P roc. of t he 3r d ACM Conf. on Recommender Syst ems (Recsys\u201909), New York, NY, USA, 2009, ACM, pp. 423\u2013424.\n[2] G . Adomav ic iu s, R . Sa nk a r a na r aya na n , S. Sen , a nd A. T uzh il in , Incorporating contextual information in recommender systems using a multidimensional approach, ACM Trans. Inf. Syst ., 23 (2005), pp. 103\u2013 145. [3] G . Adomav ic iu s a nd A. T uzh il in , Context-aware recommender systems, in P roceedings of t he 2008 ACM conference on Recommender syst ems RecSys 08, Lausanne, Swit zerland, 2008, pp. 335\u2013336. [4] R . Ag r awa l , T . Imie l in\u0301 sk i, a nd A. Swami, Mining association rules between sets of items in large databases, in P roc. of t he ACM SIGMOD Int . Conf. on Management of Dat a (SIGMOD\u201993), Washington, D.C., Unit ed St at es, 1993, ACM, pp. 207\u2013216. [5] R . Ba de r , E . Neu f e l d , W . Woer nd l , a nd V. P r inz, Context-aware POI recommendations in an automotive scenario using multi-criteria decision making methods, in P roc. of t he 2011 Workshop on Cont ext -awareness in Ret rieval and Recommendat ion (CaRR\u201911), Palo Alt o, California, 2011, ACM, pp. 23\u2013 30. [6] L. Ba l t r una s a nd X. Amat r ia in , Towards timedependant recommendation based on implicit feedback, in Workshop on Cont ext -aware Recommender Syst ems (CARS\u201909), New York, NY, USA, 2009, pp. 1\u20135. [7] R . M. Be l l a nd Y. Ko r en , Scalable collaborative filtering with jointly derived neighborhood interpolation weights, in P roc of. ICDM-07, 7t h IEEE Int . Conf. on Dat a Mining, Omaha, Nebraska, USA, 2007, pp. 43\u2013 52. [8] J . Benne t t a nd S. La nn ing , T he Netfl ix Prize, in P roc. of KDD Cup Workshop at SIGKDD-07, 13t h\nACM Int . Conf. on Knowledge Discovery and Dat a Mining, San J ose, California, USA, 2007, pp. 3\u20136. [9] T . Bog e r s, Movie recommendation using random walks over the contextual graph, in CARS\u201910: P roc. of t he 2nd Workshop on Cont ext -Aware Recommender Syst ems, Barcelona, Spain, 2010, pp. 1\u20135. [10] J . Dav id so n , B . Lieba l d , J . Liu , P . Na ndy , T . Va n Vl ee t , U. Ga r g i, S. Gupt a , Y. He , M. Lamber t , B . Liv ing st o n , a nd D. Sampat h , T he YouTube video recommendation system , in P roc. of t he 4t h ACM Conf. on Recommender Syst ems (RecSys\u201910), Barcelona, Spain, 2010, ACM, pp. 293\u2013296. [11] Q . He , J . P e i, D . K if e r , P . Mit r a , a nd L. G il e s, Context-aware citation recommendation , in P roc. of t he 19th Int . Conf. on World Wide Web (WWW\u201910), WWW \u201910, Raleigh, NC, USA, 2010, ACM, pp. 421\u2013 430. [12] J . L. He r l o c k e r , J . A. Konst a n , L. G . T e r veen , a nd J . T . R ied l , Evaluating collaborative filtering recommender systems, ACM Transact ions on Informat ion Syst ems, 22 (2004), pp. 5\u201353. [13] Y. Hu , Y. Ko r en , a nd C . Vo l in sk y , Collaborative filtering for implicit feedback datasets, in P roc. of ICDM-08, 8t h IEEE Int . Conf. on Dat a Mining, P isa, It aly, December 15\u201319, 2008, pp. 263\u2013272. [14] A. Ka r a t zo g l o u , X. Amat r ia in , L. Ba l t r una s,\na nd N. O l ive r , Multiverse recommendation: Ndimensional tensor factorization for context-aware collaborative filtering, in P roc. of t he 4t h ACM Conf. on Recommender Syst ems (RecSys\u201910), Barcelona, Spain, 2010, ACM, pp. 79\u201386. [15] Y. Ko r en , Factorization meets the neighborhood: a multifaceted collaborative filtering model, in P roc. of t he 14t h ACM Int . Conf. on Knowledge Discovery and Dat a Mining (SIGKDD\u201908), Las Vegas, Nevada, USA, 2008, pp. 426\u2013434. [16] L. D . La t ha uwer , B . D . Moo r , a nd J . Va nde - wa l l e , A multilinear singular value decomposition , SIAM J . Mat rix Anal. Appl., 21 (2000), pp. 1253\u20131278. [17] N. N. Liu , B . Ca o , M. Zha o , a nd Q . Ya ng , Adapting neighborhood and matrix factorization models for context aware recommendation , in P roc- of t he Workshop on Cont ext -Aware Movie Recommendat ion (CAMRa \u201910), Barcelona, Spain, 2010, ACM, pp. 7\u201313. [18] U. Pa nn ie l l o , A. T uzh il in , M. Go r g o g l io ne , C . Pa l misa no , a nd A. P edone , Experimental comparison of pre- vs. post-filtering approaches in contextaware recommender systems, in P roc. of t he 3r d ACM Conf. on Recommender Syst ems (Recsys\u201909), New York, New York, USA, 2009, ACM, pp. 265\u2013268. [19] I. P il a\u0301 szy a nd D. T ik k , Recommending new movies: Even a few ratings are more valuable than metadata, in P roc. of t he 3r d ACM Conf. on Recommender Syst ems (Recsys\u201909), New York, NY, USA, 2009, ACM, pp. 93\u2013 100. [20] I. P il a\u0301 szy , D . Zibr ic zk y , a nd D. T ik k , Fast A LSbased matrix factorization for explicit and implicit feedback datasets, in P roc. of t he 4t h ACM Conf. on Recommender Syst ems (RecSys\u201910), Barcelona, Spain, 2010, ACM, pp. 71\u201378. [21] S. R end l e , Z. Ga n t ne r , C . F r euden t ha l e r , a nd L. Sc hmid t -T h ieme , Fast context-aware recommendations with factorization machines, in P roc. of t he 34th Int . ACM SIGIR Conf. on Research and Development in Informat ion (SIGIR\u201911), Beijing, China, 2011, ACM, pp. 635\u2013644. [22] F . R ic c i, L. Rok a c h , a nd B. Sha pir a , Introduction to recommender systems handbook, in Recommender Syst ems Handbook, F . Ricci, L. Rokach, B. Shapira, and P. B. Kantor, eds., Art ificial Int elligence, Springer US, 2011, pp. 1\u201335. [23] A. Sa id , S. Be r kovsk y , a nd E . W . De Luc a , Putting things in context: Challenge on context-aware movie recommendation , in P roc. of t he Workshop on Cont ext -Aware Movie Recommendat ion (CAMRa\u201910), Barcelona, Spain, 2010, ACM, pp. 2\u20136. [24] R . Sa l a k hu t d inov a nd A. Mnih , Probabilistic matrix factorization , in Advances in Neural Informat ion P rocessing Syst ems 20, J . C. P lat t , D. Koller, Y. Singer, and S. Roweis, eds., MIT P ress, Cambridge, Massachuset t s, USA, 2008. [25] B . M. Sa r wa r , G . Ka r ypis, J . A. Konst a n , a nd J . R ied l , Item -based collaborative filtering recommendation algorithms, in P roc. of WWW-01, 10t h Int .\nConf. on World Wide Web, Hong Kong, 2001, pp. 285\u2013 295. [26] G . W . Snedec o r a nd W . G . Co c h r a n , Statistical Methods, Iowa St at e University P ress, 7t h ed., 1980. [27] G . Ta k a\u0301 c s, I. P il a\u0301 szy , B . Ne\u0301me t h , a nd D. T ik k , Major components of the gravity recommendation system , SIGKDD Explor. Newsl., 9 (2007), pp. 80\u201383."}], "references": [{"title": "Adomav ic iu s a nd F . R ic c i, Recsys\u201909 workshop 3: workshop on context-aware recommender systems (CARS-2009", "author": [], "venue": "P roc. of t he 3 d ACM Conf. on Recommender Syst ems (Recsys\u201909),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Sa nk a r a na r aya na n", "author": ["R G . Adomav ic iu s"], "venue": "ACM Trans. Inf. Syst .,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Adomav ic iu s a nd A. T uzh il in", "author": [], "venue": "Context-aware recommender systems, in P roceedings of t he 2008 ACM conference on Recommender syst ems RecSys 08,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "l t r una s a nd X. Amat r ia in , Towards timedependant recommendation based on implicit feedback, in Workshop on Cont ext -aware Recommender Syst ems (CARS\u201909)", "author": ["L. Ba"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Be l l a nd Y. Ko r en , Scalable collaborative filtering with jointly derived neighborhood interpolation weights, in P roc", "author": ["M. R"], "venue": "of. ICDM-07,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Movie recommendation using random walks over the contextual graph, in CARS\u201910", "author": ["T . Bog e r s"], "venue": "P roc. of t he 2nd Workshop on Cont ext -Aware Recommender Syst ems, Barcelona,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Konst a n", "author": ["J . L. He r l o c k e r", "J . A"], "venue": "ACM Transact ions on Informat ion Syst ems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Ba l t r una s,  a nd N. O l ive r , Multiverse recommendation: Ndimensional tensor factorization for context-aware collaborative filtering, in P roc", "author": ["A. Ka"], "venue": "of t he 4 h ACM Conf. on Recommender Syst ems (RecSys\u201910),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Go r g o g l io ne", "author": ["U. Pa nn ie l l o", "M.A. T uzh il in"], "venue": "C . Pa l misa", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "r euden t ha l e r , a nd L. Sc hmid t -T h ieme , Fast context-aware recommendations with factorization machines, in P", "author": ["S. R end l e", "Z. Ga n t ne r", "C . F"], "venue": "roc. of t he 34th Int . ACM SIGIR Conf. on Research and Development in Informat ion (SIGIR\u201911),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Rok a c h , a nd B. Sha pir a , Introduction to recommender systems handbook", "author": ["L. F . R ic c i"], "venue": "Recommender Syst ems Handbook,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Sa l a k hu t d inov a nd A", "author": [], "venue": "Advances in Neural Informat ion P rocessing Syst ems 20,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Konst a n , a nd J . R ied l , Item -based collaborative filtering recommendation algorithms, in P roc", "author": ["B . M. Sa r wa r", "G . Ka r ypis", "J . A"], "venue": "Int .  Conf. on World Wide Web, Hong Kong,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "Snedec o r a nd W", "author": ["W G"], "venue": "Statistical Methods, Iowa St at e University P ress, 7t h ed.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1980}], "referenceMentions": [{"referenceID": 6, "context": "Another classificat ion aspect divides CF algorit hms [12] into memory-based and model-based ones.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "One of the most successful approaches uses the Pearsoncorrelat ion [26] between the rat ing vectors [25].", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "One of the most successful approaches uses the Pearsoncorrelat ion [26] between the rat ing vectors [25].", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "factorizat ion (MF) methods are oft en used to compute these vectors, which approximat e the part ially known rat ing mat rix using alt ernat ing least squares (ALS) [7], gradient descent method [27], coordinat e descent method [20], singular value decomposit ion [15], or a probabilist ic framework [24].", "startOffset": 166, "endOffset": 169}, {"referenceID": 11, "context": "factorizat ion (MF) methods are oft en used to compute these vectors, which approximat e the part ially known rat ing mat rix using alt ernat ing least squares (ALS) [7], gradient descent method [27], coordinat e descent method [20], singular value decomposit ion [15], or a probabilist ic framework [24].", "startOffset": 300, "endOffset": 304}, {"referenceID": 10, "context": "For inst ance, a recommender syst em may consider t he navigat ion to a part icular product page as an implicit sign of preference for t he it em shown on that page [22].", "startOffset": 165, "endOffset": 169}, {"referenceID": 2, "context": "Cont ext informat ion can be, for inst ance, t he t ime or locat ion of recommendat ion, social networks of users, or user/ it em metadat a [3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "Tensor factorizat ion have been suggest ed as a generalizat ion of MF for considering cont ext informat ion [14].", "startOffset": 108, "endOffset": 112}, {"referenceID": 7, "context": "The first variant uses seasonality which was also used in [14] for t he explicit problem.", "startOffset": 58, "endOffset": 62}, {"referenceID": 1, "context": "Context -aware recommender syst ems [2] emerged as an import ant research topic in the last years and ent ire", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "workshops are devot ed to this t opic on ma jor conferences (CARS series st art ed in 2009 [1], CAMRA in", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "The applicat ion fields of cont ext -aware recommenders include among other movie [9] and music recommendat ion [6], point -of-int erest recommendat ion (POI) [5], cit at ion recommendat ion [11].", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "The applicat ion fields of cont ext -aware recommenders include among other movie [9] and music recommendat ion [6], point -of-int erest recommendat ion (POI) [5], cit at ion recommendat ion [11].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "t ual modeling [3].", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "Balt runas and Amat riain [6] proposed a pre-filt ering approach by part it ioned user profiles into micro-profiles based on the t ime split of user event falls, and experiment ed with different t ime part it ioning.", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "recommendat ion generat ion, but disregards irrelevant it ems (in a given cont ext ) or adjust recommendat ion score (according to the cont ext ) when the recommendat ion list is prepared; see a comparison in [18].", "startOffset": 209, "endOffset": 213}, {"referenceID": 7, "context": "In [14], a sparse HOSVD [16] method is present ed that decomposes a C + 2 dimensional sparse t ensor into C + 2 mat rices and a C + 2 dimensional t ensor.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "A furt her improvement was proposed by Rendle et al [21], where the computat ional complexity was reduced so that t heir method scales linearly both with the num-", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "[1] G .", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] G .", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] G .", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[6] L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] R .", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[9] T .", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[12] J .", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[14] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[18] U.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[21] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[22] F .", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[24] R .", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[25] B .", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[26] G .", "startOffset": 0, "endOffset": 4}], "year": 0, "abstractText": "Albeit , t he implicit feedback based recommendat ion problem\u2014when only t he user hist ory is available but t here are no rat ings\u2014is t he most typical set t ing in real-world applicat ions, it is much less researched than the explicit feedback case. St at e-of-t he-art algorit hms that are efficient on the explicit case cannot be st raight forwardly t ransformed to t he implicit case if scalability should be maint ained. There are few if any implicit feedback benchmark dat aset s, t herefore new ideas are usually experiment ed on explicit benchmarks. In t his paper, we propose a generic cont ext -aware implicit feedback recommender algorit hm, coined iTALS. iTALS apply a fast , ALS-based t ensor fact orizat ion learning method that scales linearly wit h t he number of non-zero element s in t he t ensor. T he method also allows us t o incorporat e diverse cont ext informat ion int o t he model while maint aining it s comput at ional efficiency. In part icular, we present two such cont ext -aware implement at ion variant s of iTALS. The first incorporat es seasonality and enables t o dist inguish user behavior in different t ime int ervals. T he other views the user hist ory as sequent ial informat ion and has t he ability t o recognize usage pat t ern typical t o cert ain group of it ems, e.g. t o automat ically t ell apart product types or cat egories t hat are typically purchased repet it ively (collect ibles, grocery goods) or once (household appliances). Experiment s performed on three implicit dat aset s (two propriet ary ones and an implicit variant of t he Netflix dat aset ) show that by int egrat ing cont ext -aware informat ion wit h our fact orizat ion framework int o t he st at e-of-t he-art implicit recommender algorit hm the recommendat ion quality improves significant ly. K eywor d s: recommender syst ems, t ensor fact orizat ion, cont extual informat ion, implicit feedback", "creator": null}}}