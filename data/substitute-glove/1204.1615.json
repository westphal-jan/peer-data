{"id": "1204.1615", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2012", "title": "Discrimination between Arabic and Latin from bilingual documents", "abstract": "july International Conference followed Communications, Computing while Control Applications (CCCA )", "histories": [["v1", "Sat, 7 Apr 2012 09:28:19 GMT  (91kb)", "http://arxiv.org/abs/1204.1615v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.IR", "authors": ["sofiene haboubi", "samia maddouri", "hamid amiri"], "accepted": false, "id": "1204.1615"}, "pdf": {"name": "1204.1615.pdf", "metadata": {"source": "CRF", "title": "Discrimination between Arabic and Latin from bilingual documents", "authors": ["Sofiene Haboubi", "Samia Snoussi Maddouri", "Hamid Amiri"], "emails": ["sofiene.haboubi@istmt.rnu.tn;", "hamid.amiri}@enit.rnu.tn"], "sections": [{"heading": null, "text": "Language identification;structural features; word extraction\nI. INTRODUCTION Discriminating between the languages a document image is a complex and challenging task. It has kept the scientists, working in this field, puzzled for quite some time now. Researchers have been emphasizing a lot of effort for pattern recognition since decades. Amongst the pattern recognition field Optical Character Recognition is the oldest sub field and has almost achieved a lot of success in the case of recognition of Monolingual Scripts.\nOne of the important tasks in machine learning is the electronic reading of documents. All documents can be converted to electronic form using a high performance Optical Character Recognizer (OCR). Recognition of bilingual documents can be approached by the recognition via script identification.\nThe digital processing of documents is a very varied field of research. Their goal is to make the machine able automatically to read the contents of a document, even of high complexity. Among the obtained results, we find the OCR (Optical characters recognition). This system makes to read the scripts form images, to convert them in numerical form. This system must know the language of script before the launching of process, to obtain good results. Also, the currently system does not treat two different languages in the same document. Like several researchers, our work is to introduce the Multilanguage to our OCR, and we took the differentiation between the Arab and Latin scripts our field of contribution. Existing methods for differentiation between scripts are presented by [14] in four principal classes according to analyzed levels of information: methods based on an analysis of text block,\nmethods based on the analysis of text line, methods based on the analysis of related objects and methods based on mixed analyses.\nII. BACKGROUND The preliminary study, shown that the majority of differentiation methods treat only the printed text documents. Among the latter, the method suggested by [1], develops a strategy of discrimination between Arabic and Latin scripts. This approach is based on Template-Matching, which makes it possible to decide between the identified language. In [5], the authors uses a supervised Multi-Classes for classification and the Gabor filter to identify the Latin script. The type of document used is printed with Latin scripts and mixed. Two methods are proposed by [4], with two approaches: Statistical and spectral by Gabor filter. This work is interested on Kannada and Latin scripts. The system of identification, proposed by [13], relates to Latin and not-Latin languages in printed documents. This method is based on the application of Gabor filter, the author classifiers for the identification of languages other than Latin. With statistical methods and on printed documents, [15] has interested by identification of Arabic, Chinese, Latin, Devanagari, and Bangla languages. The identification of the type of scripts (printed or handwritten) is treated by [12], on Korean language. This approach is based on an analysis of related components and contours. A spectral method is presented in [16]. This method is to classes the script to Chinese, Japanese, Korean or Latin language by Gabor filter. The Arabic script is cursive and present various diacritic.\nAn Arab word is a sequence of letters entirely disjoined and related entities. Contrary to the Latin script, the Arab characters are script from the right to left, and do not comprise capital letters. The characters form varies according to their position in the word: initial, median, final and insulated. In the case of handwritten, the characters, Arabic or Latin, can vary in their static and dynamic properties. The static variations relate to the size and the form, while the dynamic variations relate to the number of diacritic segments and their order. Theirs increases the complexity of distinction between the languages.\nIII. DISCRIMINATION BETWEEN LANGUAGES The propose method consists of several stages (Figure 1). From a printed document, we must pass a filter to remove the\ndiacritics dots. Then, extraction lines by horizontal projection. In the next step, we extract the words from lines; where each word will be treated separately.\nWe extracted the structural features for each word, and then passed to a stage of training or classification. In the training phase, we store all the results into a database, and in the classification phase, we use a classifier to determine the nature of language (Arabic or Latin)."}, {"heading": "A. Elimination of dots", "text": "The challenge is the presence of eight different types of diacritical marks, used to represent Arabic or Latin letters. In written text they are considered as special letters where each one is assigned a single code, as with normal letters. In fully diacriticized text a diacritical mark is added after each consonant of the Arabic word. These diacritical marks play a very important role in fixing the meaning of words.\nBut these diacritics can cause problems in the detection and extraction lines. Because in some writing styles, the diacritics may leave the upper limit of the line or the lower limit. This can cause poor detection of lines. Their diacritics will be removed by morphological erosion followed by a morphological dilation."}, {"heading": "B. Line detection", "text": "To separate words from text document, we must first start with the detection of lines. There are several methods proposed by authors such as [7, 11]. But since we work with the Arabic and Latin, and our basic document that consists of text (without images and background), we chose to use the horizontal projection method. This method is still reliable in the absence of the inclination of lines. Figure 2 shows the application of the horizontal projection."}, {"heading": "C. Word detection and separation", "text": "The detection and separation of words from a text document, is a conventional step in the automatic document processing. This step makes it difficult in the case of multilingual documents. This difficulty because of the large difference between the Arabic printed script and Latin printed script.\nThe Arabic handwriting is cursive. It is writing where the letters are linked to each other. This is not only in the case of handwriting; it is also the case of printed letters. The Latin alphabet is not cursive writing in the case of printed letters. In Arabic, the writing is always attached, even with print. This break is a problem in the step of separation between words; since both languages are present in the same text together.\nFigure 3 shows the difference in dispersal areas in the case of Arabic (AR) and Latin (LA) scripts. To see this difference we used a Latin text and its similarity in Arabic, they have almost the same amount of information. We measured the distance (in pixels) between characters, and we calculated the number of occurrences of each distance found. In the case of the Arabic script, there are not much of separators between the characters, because of its nature cursive. In contrary, in the case of the Latin script, the distance is the dominant separators between the characters of the same word. But these two types of scripts have a common point at the threshold between the distances \u201cbetween words\u201d with the distances \u201cbetween characters\u201d (e.g. 6 pixels).\nOur idea of separation of words is to use the Morphological image analysis to know the limits of the word (lower, upper, right and left). The structuring element for morphological dilation is a horizontal line form. The size of this line is the threshold that separates the distances between words and the distances between characters. In Figure 4 we show the impact of the order of dilation (size of the structuring element) on the number of connected components. The search for the ideal size of dilation is difficult with Figure 4.\nIn our case (Arabic and Latin text printed), the sequential dilation causes a decrease in numbers of connected components thereof (the characters of same word stick), then there is a stabilization, then there is a second decrease (the words stick). The difference between the two phases of reductions is stabilization, which is shown in Figure 5. Stability is the first value where the standard deviation of the variation in the number of related components vanishes.\nAfter choosing the size of the structuring element, and after dilation of the original image, we determine the boundaries of each word in the Arabic and Latin text. The figure 6 shows an example of results found."}, {"heading": "D. Features extraction", "text": "The extraction of structural features is based on three steps: pre-treatment, the determination of the baseline, and the detection of primitives.\nThe words to be recognized are extracted from their contexts check or postal letters. A stage analysis, segmentation and filtering of documents is required. This task is not part of our work. The words are supposed to be taken out of context without noise. Since our method of feature extraction is based mainly on the outline, the preprocessing step we have introduced in our system is the expansion in order to obtain a closed contour with the least points of intersections. Since the elimination of the slope may introduce additional distortions we have tried to avoid this step. It is for this reason that techniques of preprocessing, avoiding the inclination correction has emerged [2, 3, 8, 11].\n1) Determination of baselines From the word we can extract two baselines. A upper and lower baseline. These two baselines divide the word into three regions. The poles \"H\" and diacritical dots high \"P\" which are regions above the upper baseline. The jambs \"J\" and diacritical dots lower \"Q\" correspond to regions below the lower baseline. The body of the word is the region between the two baselines. In general, the loops are in the middle.\n2) Extraction of the poles and jambs A pole is all forms with a maximum above the upper baseline. Similarly, jambs and all maxima below the lower baseline. The distance between these extrema and the baseline is determined empirically. It corresponds to:\nMargeH = 2(lower baseline \u2013 upper baseline) for the poles\nMargeJ = (lower baseline \u2013 upper baseline) for the jambs.\n3) Detection of diacritical dots The diacritical points are extracted from the contour. Browsing through it, we can detect those that are closed. From these closed contours, we choose those with a number of contour points below a certain threshold. This threshold is derived from a statistical study to recognize the words taken from their context (checks, letters, mailing ...).\n4) Determination of loops from the contour The problems encountered during the extraction of loops are: - Some diacritical dots can be confused with the loops if they are intersecting with the baselines. -Some loops may be longer than 60 pixels and can not be taken into account.\nAfter a first selection step loops, a second step of verifying their inclusion in another closed loop is completed. This method involves: - Looking for word parts that can include the loop. - Stain the relevant section blank, if the contour points disappear when the latter is included in the word color and is part of the list of loops.\n5) Detection of PAWS Given the variability of the shape of characters according to their position, an Arabic word can be composed by more than one party called for PAW \"Pieces of Arabic Word.\" Detection of PAWS is useful information both in the recognition step in\nthe step of determining the position of structural features in the word.\n6) Position detection primitives The shape of an Arabic character depends on its position in the word. A character can have four different positions which depend on its position in the word. We can have single characters at the beginning, middle or end of a word. This position is detected during the primary feature extraction. Indeed, the extracted areas are defined by local minima. These minimums are from the vertical projection and contour. The number of black pixels is calculated in the vicinity of boundaries demarcated areas and between the two baselines above and below. If this number is greater than 0 at the left boundary and equal to 0 on the right, the position is the top \"D\", etc ...\nIV. TRAINING AND CLASSIFICATION The recognition of the language of the document is regarded as a preprocessing step; this step has become difficult in the case of bilingual document. We begin by discriminating between an Arabic text and a Latin text printed by the structural method. Considering the visual difference between writing Arabic and Latin script, we have chosen to discriminate between them based on the general structure of each, and the number of occurrences of the structural characteristics mentioned above. Indeed, in analyzing a text in Arabic and Latin text we can distinguish a difference in the cursivity, the number of presence of diacritical dots and leg in the Arabic script. To printed Latin script, it is composed mainly of isolated letters.\nThe first step in the process regardless of the Arabic script from a text document is extracted lines and words. The extraction of lines is done by determining the upper and lower limit using the horizontal projection. For each line, there are the words using the method of dilation. Each word will be awarded by a system for extracting structural features.\nFrom image of the word, we extract the feature vector that can discriminate between writing languages. This vector represents the number of occurrences of each feature in the word. It is to count the number of each feature: Hampe (H), Jamb (J), Upper (P) and Lower (Q) diacritic dots, Loop (B), Start (D), Middle (M), End (F) and Isolated (I). Our vector is composed of selected features\n\u2022 the number of PAWS and the number of characters {NbPAW, NBL}\n\u2022 the number of occurrences of each feature {H, J, B, P, Q, R, D, M, F , I}\n\u2022 the number of occurrence of each feature taking into account their position {HD, HM, HF, HI, JF, JI, PD, PM, PF, PI, QD, QM, QF, QI, BD, BM, BF, BI}.\nTo evaluate our method, we used 57 documents Arabic and Latin text. After the step of separation into words, we found 4229 words. From each word, we generate its feature vector. For learning and testing, we used the Multilayer Perceptron function (MLP). The learning database contains 80% of all\nwords, and the test database contains the rest. In practice, we used the WEKA1 [6, 9] software to evaluate our method.\nV. RESULTS In the test phase, we used 846 words (440 Arab and 406 Latin). After evaluating the test split, we found 798 words correctly classified and 48 words incorrectly classified. Who gave a classification rate equal to 94.32% and an error rate equal to 5.68%. From the confusion matrix, we found 37 Arabic words incorrectly classified and 11 Latin words incorrectly classified.\nVI. CONCLUSION According to the method of discrimination presented, we showed that discrimination between Arabic and Latin is possible. The text documents used are in Arabic and Latin at the same time. The techniques used for discrimination are: mathematical morphology for separation into words, and structural characteristics for identification, and neural networks for classification. The levels found are too motivating to treat the case of handwritten documents.\nREFERENCES\n[1] A. Elbaati, M. Charfi, and A. M. Alimi, Discrimination de documents imprims arabes et latins utilisant une approche de reconnaissance, in Journes GEI, Tunisie, Mars 2004\n[2] Caesar T., J. M Gloger, E. Mandler. Processing and Feature Extraction for a Handwriting Recognition System, ICDAR, 408-411. 1993\n[3] C\u00f4t\u00e9 M., M. Lecolinet, E. Cheriet et M. Suen. Automatic Reading of Cursive Scripts Using a Reading Model end Perceptual Concepts, International Journal on Document Analysis and Recognition, IJDAR, 1 : 3-17. 1998\n[4] D. Dhanya and A. G. Ramakrishnan, \u201cScript identification in printed bilingual documents,\u201d in DAS \u201902: Proceedings of the 5th International Workshop on Document Analysis Systems V. London, UK: SpringerVerlag, 2002, pp. 13\u201324.\n[5] H. Ma and D. Doermann, Gabor filter based multi-class classi?er for scanned document images, in ICDAR \u201903: Proceedings of the Seventh International Conference on Document Analysis and Recognition. Washington, DC, USA: IEEE Computer Society, 2003, p. 968.\n[6] Ian H. Witten, Eibe Frank. Data Mining: Practical Machine Learning Tools and Techniques (Second Edition).SBN 0-12-088407-0. 2005\n[7] Kaouthar Bouriel, Fadoua Bouafif Samoud, Haikal El Abed, Samia Snoussi Maddouri. Strat\u00e9gies d\u2019\u00e9valuation et de comparaison de m\u00e9thodes d\u2019extraction de la ligne de base de mots manuscrits arabes. Actes du dixi\u00e8me Colloque International Francophone sur l\u2019\u00c9crit et le Document (CIFED), France, 2008.\n[8] Madhvanath S., V. Krpasundar, Pruning Large Lexicons Using Generalised Woed Shape Descriptors (1997). ICDAR, 2 : 552-555. 1997\n[9] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, Ian H. Witten; The WEKA Data Mining Software: An Update; SIGKDD Explorations, Volume 11, Issue 1.2009\n[10] Negi A., K. S. Swaroop et A. Agarwal, A correspondence based approach to segmentation of cursive words, ICDAR, 1034-1037. 1995\n[11] Pechwitz M., M\u00e4rgner V., Baseline estimation for Arabic handwritten words, International Workshop on Frontiers in Handwriting Recognition (IWFHR), pp. 479\u2013484, 2002.\n1Weka: Data Mining Software. Weka is a collection of machine learning algorithms for data mining tasks. http://www.cs.waikato.ac.nz/ml/weka/\n[12] S. I. Jang, S. H. Jeong, and Y. S. Nam, \u201cClassification of machineprinted and handwritten addresses on korean mail piece images using geometric features,\u201d in ICPR \u201904: Proceedings of the Pattern Recognition, 17th International Conference on (ICPR\u201904) Volume 2. Washington, DC, USA: IEEE Computer Society, 2004, pp. 383\u2013386.\n[13] S. Jaeger, H. Ma, and D. Doermann, \u201cIdentifying script onword-level with informational confidence,\u201d in ICDAR \u201905: Proceedings of theEighth International Conference on Document Analysis and Recognition. Washington, DC, USA: IEEE Computer Society, 2005, pp. 416\u2013420.\n[14] S. Kanoun, A. Ennaji, Y. LeCourtier, and A. M. Alimi, Script and nature differentiation for arabic and latin text images, in IWFHR \u201902: Proceedings of the Eighth International Workshop on Frontiers in Handwriting Recognition (IWFHR\u201902). Washington, DC, USA: IEEE Computer Society, 2002, p. 309.\n[15] U. Pal, S. Sinha, and B. B. Chaudhuri, \u201cMulti-script line identification from indian documents,\u201d in ICDAR \u201903: Proceedings of the Seventh International Conference on Document Analysis and Recognition. Washington, DC, USA: IEEE Computer Society, 2003, p. 880.\n[16] W. M. Pan, C. Y. Suen, and T. D. Bui, \u201cScript identification using steerable gabor filters,\u201d in ICDAR \u201905: Proceedings of the Eighth International Conference on Document Analysis and Recognition. Washington, DC, USA: IEEE Computer Society, 2005, pp. 883\u2013887."}], "references": [{"title": "Discrimination de documents imprims arabes et latins utilisant une approche de reconnaissance, in Journes GEI, Tunisie", "author": ["A. Elbaati", "M. Charfi", "A.M. Alimi"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Processing and Feature Extraction for a Handwriting Recognition System, ICDAR", "author": ["Caesar T", "J. M Gloger", "E. Mandler"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "Automatic Reading of Cursive Scripts Using a Reading Model end Perceptual Concepts, International", "author": ["C\u00f4t\u00e9 M", "M. Lecolinet", "E. Cheriet et M. Suen"], "venue": "Journal on Document Analysis and Recognition, IJDAR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Script identification in printed bilingual documents", "author": ["D. Dhanya", "A.G. Ramakrishnan"], "venue": "DAS \u201902: Proceedings of the 5th International Workshop on Document Analysis Systems V. London, UK: Springer- Verlag, 2002, pp. 13\u201324.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Gabor filter based multi-class classi?er for scanned document images, in ICDAR \u201903", "author": ["H. Ma", "D. Doermann"], "venue": "Proceedings of the Seventh International Conference on Document Analysis and Recognition", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques (Second Edition).SBN 0-12-088407-0", "author": ["Ian H. Witten", "Eibe Frank"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Strat\u00e9gies d\u2019\u00e9valuation et de comparaison de m\u00e9thodes d\u2019extraction de la ligne de base de mots manuscrits arabes", "author": ["Kaouthar Bouriel", "Fadoua Bouafif Samoud", "Haikal El Abed", "Samia Snoussi Maddouri"], "venue": "Actes du dixie\u0300me Colloque International Francophone sur l\u2019E\u0301crit et le Document (CIFED), France,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "The WEKA Data Mining Software: An Update; SIGKDD Explorations", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "Volume 11,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "A correspondence based approach to segmentation of cursive words, ICDAR", "author": ["Negi A", "K.S. Swaroop et A. Agarwal"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "Baseline estimation for Arabic handwritten words, International Workshop on Frontiers in Handwriting Recognition (IWFHR)", "author": ["M. Pechwitz", "V. M\u00e4rgner"], "venue": "pp. 479\u2013484,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Classification of machineprinted and handwritten addresses on korean mail piece images using geometric features", "author": ["S.I. Jang", "S.H. Jeong", "Y.S. Nam"], "venue": "ICPR \u201904: Proceedings of the Pattern Recognition, 17th International Conference on (ICPR\u201904) Volume 2. Washington, DC, USA: IEEE Computer Society, 2004, pp. 383\u2013386.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Identifying script onword-level with informational confidence", "author": ["S. Jaeger", "H. Ma", "D. Doermann"], "venue": "ICDAR \u201905: Proceedings of theEighth International Conference on Document Analysis and Recognition. Washington, DC, USA: IEEE Computer Society, 2005, pp. 416\u2013420.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Script and nature differentiation for arabic and latin text images, in IWFHR \u201902: Proceedings of the Eighth International Workshop on Frontiers in Handwriting Recognition (IWFHR\u201902)", "author": ["S. Kanoun", "A. Ennaji", "Y. LeCourtier", "A.M. Alimi"], "venue": "IEEE Computer Society,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Multi-script line identification from indian documents", "author": ["U. Pal", "S. Sinha", "B.B. Chaudhuri"], "venue": "ICDAR \u201903: Proceedings of the Seventh International Conference on Document Analysis and Recognition. Washington, DC, USA: IEEE Computer Society, 2003, p. 880.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Script identification using steerable gabor filters", "author": ["W.M. Pan", "C.Y. Suen", "T.D. Bui"], "venue": "ICDAR \u201905: Proceedings of the Eighth International Conference on Document Analysis and Recognition. Washington, DC, USA: IEEE Computer Society, 2005, pp. 883\u2013887.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 12, "context": "Existing methods for differentiation between scripts are presented by [14] in four principal classes according to analyzed levels of information: methods based on an analysis of text block, methods based on the analysis of text line, methods based on the analysis of related objects and methods based on mixed analyses.", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "Among the latter, the method suggested by [1], develops a strategy of discrimination between Arabic and Latin scripts.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "In [5], the authors uses a supervised Multi-Classes for classification and the Gabor filter to identify the Latin script.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Two methods are proposed by [4], with two approaches: Statistical and spectral by Gabor filter.", "startOffset": 28, "endOffset": 31}, {"referenceID": 11, "context": "The system of identification, proposed by [13], relates to Latin and not-Latin languages in printed documents.", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "With statistical methods and on printed documents, [15] has interested by identification of Arabic, Chinese, Latin, Devanagari, and Bangla languages.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "The identification of the type of scripts (printed or handwritten) is treated by [12], on Korean language.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "A spectral method is presented in [16].", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "There are several methods proposed by authors such as [7, 11].", "startOffset": 54, "endOffset": 61}, {"referenceID": 9, "context": "There are several methods proposed by authors such as [7, 11].", "startOffset": 54, "endOffset": 61}, {"referenceID": 1, "context": "It is for this reason that techniques of preprocessing, avoiding the inclination correction has emerged [2, 3, 8, 11].", "startOffset": 104, "endOffset": 117}, {"referenceID": 2, "context": "It is for this reason that techniques of preprocessing, avoiding the inclination correction has emerged [2, 3, 8, 11].", "startOffset": 104, "endOffset": 117}, {"referenceID": 9, "context": "It is for this reason that techniques of preprocessing, avoiding the inclination correction has emerged [2, 3, 8, 11].", "startOffset": 104, "endOffset": 117}, {"referenceID": 5, "context": "In practice, we used the WEKA [6, 9] software to evaluate our method.", "startOffset": 30, "endOffset": 36}, {"referenceID": 7, "context": "In practice, we used the WEKA [6, 9] software to evaluate our method.", "startOffset": 30, "endOffset": 36}], "year": 2010, "abstractText": "One of the important tasks in machine learning is the electronic reading of documents. The discrimination between languages is one of the first steps in the problem of automatic documents text recognition. We are interested in this work to the printed document Arabic and Latin (mixed). Our method is based essentially on the extraction of words from any document. Extracting structural features of each word; and then the recognition of language writing from a classification step. We present the found results of classification step, with a discussion on possible improvements. Language identification;structural features; word extraction", "creator": "PScript5.dll Version 5.2"}}}