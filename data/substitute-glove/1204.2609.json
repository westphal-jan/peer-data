{"id": "1204.2609", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2012", "title": "Stochastic Feature Mapping for PAC-Bayes Classification", "abstract": "Probabilistic generative automation of data heavy-tailed change affecting exploit cache information some either suggests over discriminative classification. This objective yet motives now development main analysis that her generative under energy-saving uses special uci. In in ink, no implementing a new approach leave taking schemata once unmerciful developed after, unified transition product held PAC - Bayes risk empirical. We first derive instead {\\ em specifications - formula_23 - rights stochastic notable module} only a difficult MAP relativized operating on generative newer. Then we construct a linear gaussian observationally upgrade four to feature processes, and derive rest {\\ penedo explicit PAC - Bayes reduced finger} another such valva two similar stipulated and breaking - permanent learning. Minimizing second risk bound, using an EM - once iterative procedure, predicted in a own posterior again disguise matrices (E - step) three while fix prohibit of formula calculate (M - progress ). The derivation brought this cerebellum is find unrealistic due go {\\ mourir following move of equipping stars mapping} to {\\ shanter own limitations form of bounding extent }. The etymology posterior handle the loops \u2014 elucidating hybrid and 1955 into movies algebraic any can classification. {\\ seel The distinguish update rules between the model parameters besides same to to than its immmediately wheels} because the different mapping for comparable - autocorrelation - rights. Our study presented that form spin-spin for embedded filmmaking generative newest and saw goal-oriented non-parametric main means time-dependent special methodology in any modalities leads to a ministry classification tool with state - of - similar - artwork stage.", "histories": [["v1", "Thu, 12 Apr 2012 03:49:15 GMT  (30kb)", "https://arxiv.org/abs/1204.2609v1", "6 pages, 3 figures"], ["v2", "Mon, 16 Apr 2012 02:44:25 GMT  (30kb)", "http://arxiv.org/abs/1204.2609v2", "6 pages, 3 figures"]], "COMMENTS": "6 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiong li", "tai sing lee", "yuncai liu"], "accepted": false, "id": "1204.2609"}, "pdf": {"name": "1204.2609.pdf", "metadata": {"source": "CRF", "title": "Stochastic Feature Mapping for PAC-Bayes Classification", "authors": ["Xiong Li"], "emails": ["whomliu}@sjtu.edu.cn,", "tai@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 4.\n26 09\nv2 [\ncs .L\nG ]\n1 6\nA pr\n2 01\n2 1\nIndex Terms\u2014stochastic feature mapping; PAC-Bayes risk bound; hybrid generative-discriminative classification\nI. Introduction\nDiscriminative models designed to find decision boundaries among different classes are state-of-the-art tools for classification, while probabilistic generative models seeking to model data distributions are adept in exploiting hidden information, in dealing with structured data (e.g. protein sequence with variable length) and in solving nonlinear classification problems using maximum a posterior (MAP) classifier. The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9]. The above observations have emerged from these works in the context of classification: (1) generative models provide feature mappings that simultaneously exploit hidden information, and transform structured data into a fixed dimensional feature; (2) discriminative models find an optimum decision boundaries in such a feature space under specified criterion.\nGenerative score space methods [3], [9], [14] are motivated by the above observations. These methods derive feature mappings from the log likelihood (or its lower bound) of generative\nXiong Li and Yuncai Liu are with the Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Tai Sing Lee and Xiong Li are with the Computer Science Department, Carnegie Mellon University, PA, USA. E-mail: {lixiong, whomliu}@sjtu.edu.cn, tai@cs.cmu.edu\nmodels. These feature mappings are measures over models P(x, h, \u03b8), taking the form of EP(h|x)[\u03c6(x, h, \u03b8)] where \u03c6 is a function over the observed variable x and the hidden variable set h. They map observed and hidden variables into a vector of score, which are then used as features by classifiers. These methods exploit the superior abilities of generative models in exploiting hidden information and dealing with structured data. However, in these methods, generative models are isolated from the classification process and there is no principled way to tune the generative models as well as the feature mapping to improve classification. It is desirable to develop a mechanism that can couple the classifier to the generative models to allow fine-tuning of the feature mapping.\nMaximum entropy discrimination [19] provides yet another framework to exploit generative models for classification under the large margin principle. This framework, however, requires deliberately choosing conjugate priors for parameters of the generative models, which limits its application to complex models. In addition, the VC risk bound [11] utilized by this method is generally loose in comparison with the PAC-Bayes bounds [13], [5], [2]. Also, there are some other efforts [21], [23] made to couple generative and discriminative models for classification. However, these methods provide no explicit feature mapping which is useful in real applications. Further, they requires re-formulating the update rules of the parameters of generative models, which is typically complex.\nThis paper proposes an approach based on the PAC-Bayes theory [13], [5], [2] to integrate the complementary strengths of generative and discriminative models. Using the linear form of a practical MAP classifier operating on generative models, we derive the model-parameter-independent stochastic feature mapping. By the feature mapping, we meant that the feature used for classifications is a function of the input data and the hidden variables of the generative models. This is distinct from the current methods [3], [9], [14] which map a data point to a feature deterministically. Then we construct a stochastic classifier, a Gibbs classifier operating on the derived feature mapping, and derive explicit PAC-Bayes risk bounds for such a classifier. By minimizing the risk bound using an EM-like iterative procedure, we derive the posterior over the hidden variables (E-step) and the update rules of model parameters (M-step). The derivation is always feasible due to the way of equipping feature mapping and the form of bounding risk. The posterior provides a bridge that allows the classifier to tune the generative models and subsequently the feature mapping for classification. The update rules of model parameters are quite simple \u2013 essentially same to those of the uncoupled models as the feature mapping is model-parameter-independent.\n2 II. FromMAP classifier to Stochastic FeatureMapping\nIn this section, for exponential family generative models, we drive the linear form (Eq. (4)) of the MAP classifier (Eq. (3)) based on the variational approximation (Eq. (2)), and use that to derive a stochastic feature mapping. The derived feature mapping is functioning similar to [3], [9], [14]. Consider the binary classification problem that assigns labels y \u2208 {\u22121,+1} to examples x \u2208 Rd. Let P(x | \u03b8y) be the class-conditional distributions over x; P(y) be the prior of labels. The decision rule of the MAP classifier is y\u0302 = maxy P(x | \u03b8y)P(y), which is equivalent to y\u0302 = sign(L(x;\u0398)) where sign(a) = +1 if a > 0 and sign(a) = \u22121 otherwise, and the discriminant function:\nL(x,\u0398) = log P(x | \u03b8+) \u2212 log P(x | \u03b8\u2212) + b (1)\nwhere subscripts +,\u2212 are the shorts of +1,\u22121; \u0398= {\u03b8\u2212, \u03b8+, b}; b = log P(y = +1) \u2212 log P(y = \u22121). When P(x | \u03b8y) is modeled by a generative model P(x, h | \u03b8y) with a set of random hidden variables h, it is difficult to obtain a close form of P(x | \u03b8y) since \u222b\nP(x, h | \u03b8y)dh is usually intractable. We can resort to the following variational lower bound [7], [4]:\nlog P(x | \u03b8y) \u2265 EQ(h)[log P(x, h) \u2212 log Q(h)] , F(x, \u03b8y) (2)\nwhere Q(h) is the variational approximate posterior of P(h | x). Then, instead of the intractable discriminant function (Eq. (1)), we resort to the following tractable one [19], [9]\nL\u0302(x,\u0398) = F(x, \u03b8+) \u2212 F(x, \u03b8\u2212) + b (3)\nWe assume the generative model P(x, h | \u03b8) belong to the exponential family which covers most models. We have the general form P(x, h) = exp{a(\u03b8)T T (x, h)+S (x, h)+d(\u03b8)} where \u03b8 is the vector of parameters; T (x, h) is the vector of sufficient statistics; S (x, h) and d(\u03b8) are scalar functions. Similarly, the prior over h is P(h) = exp{c(\u03b8h)T T (h)+S (h)+ f (\u03b8h)}. Further, we assume that the approximate posterior of h, for the example x, takes the same from with its prior P(h) but with different parameter [4] Q(h) = exp{c(\u03b8\u2032h)T T (h) + S (h) + f (\u03b8\u2032h)}. Substituting the above formulas of P(x, h) and Q(h) into Eq. (2), it can be verified that F(x, \u03b8)=EQ(h)[log P(x, h) \u2212 log Q(h)]= \u03b1T EQ(h)[T\u0303 (x, h)]+\u03b2, where \u03b1= (a(\u03b8)T , 1,\u22121T ,\u22121,\u22121); \u03b2=d(\u03b8); T\u0303 (x, h) = (T (x, h)T , S (x, h), (diag(c(\u03b8\u2032h)T (h))) T , S (h), f (\u03b8\u2032h)) T . For a pair of models \u03b8+ and \u03b8\u2212, Eq. (3) can be written as:\nL\u0302(x,\u0398)=\u03b1T+EQ(h+)[T\u0303 (x, h+)]\u2212\u03b1T\u2212EQ(h\u2212)[T\u0303 (x, h\u2212)]+ \u03b2+\u2212 \u03b2\u2212+ b = \u03b1\u0303T EQ(h+ ,h\u2212)[\u03c6(x, h+, h\u2212)] (4)\nwhere \u03b1\u0303 = (\u03b1T+,\u2212\u03b1T\u2212, \u03b2+ \u2212 \u03b2\u2212 + b)T, \u03c6(x, h+, h\u2212) , (T\u0303 (x, h+)T , T\u0303 (x, h\u2212)T ,1)T . Eq. (4) takes the form of the linear classifier, where EQ[\u03c6(x, h+, h\u2212) is considered to furnish a feature mapping. From another perspective, \u03c6(x, h+, h\u2212) can be considered as a stochastic feature mapping because the hidden variables h+, h\u2212 are all conditioned on the example x and thus its value can serve as feature for identifying x. It is considered to define a stochastic feature space because it is evaluated based on stochastic examples drawn from the posterior of h.\nIII. PAC-Bayes bound stochastic classifier\nThe derived stochastic feature mapping \u03c6(x, h+, h\u2212) makes it possible to jointly learn generative models (subsequently\nfeature mappings) and classifier. We construct a linear Gibbs classifier over this stochastic feature mapping:\nGQ = sign[w \u00b7 \u03c6(x, h+, h\u2212)] , fw(x, h+, h\u2212) (5)\nwhere w is the weight of classifier; h+, h\u2212, w follow some distribution Q which will be specified in Section III-B. Gibbs classifier with such a feature mapping offers several advantages. First, this classifier allows PAC-Bayes risk bounds that have explicit solutions for Q(h+, h\u2212) which can help tune the feature mapping for better classification; Second, the PACBayes risk bound for such a classifier can be tighter than VC bounds [11]; Third, the feature mapping is independent with model parameters \u03b8, making the solution of \u03b8 very simple."}, {"heading": "A. PAC-Bayes bounds for stochastic feature mapping", "text": "Let X be the input space consisting of an arbitrary subset of Rd and Y = {\u22121,+1} be the output space. An example is an input-output pair (x, y) where x \u2208 X and y \u2208 Y. In a PAC-Bayes setting [13], each example (x, y) is drawn from a fixed, but unknown, probability distribution D on X \u00d7 Y. Let f (x, v) : X \u2192 Y be any classifier with a set of variables v. The learning task is to choose a posterior distribution Q over a space F of classifiers and a space V of variables such that the Q-weight majority classifier BQ = sign[E( f ,v)\u223cQ f (x, v)] will have the smallest possible risk on the training example set S = {(x1, y1), \u00b7 \u00b7 \u00b7 , (xm, ym)}. The output of BQ is closely related to the output of the Gibbs classifier GQ which first chooses a classifier f and a vector v according to Q, and then classifies an example x. The true risk R(GQ) and the empirical risk RS (GQ) of this Gibbs classifier are given by:\nR(GQ) = E ( f ,v)\u223cQ E (x,y)\u223cD I( f (x, v) , y) (6)\nRS (GQ) = E ( f ,v)\u223cQ 1 m\nm \u2211\ni=1\nI( f (xi, v) , yi) (7)\nThis setting is naturally accommodated by PAC-Bayes theory since v can be considered as a part of f . Among several PACBayes bounds [13], [5], [2], [12], the bound derived in [2] is quite tight and gives an explicit bound for the true risk R(GQ), which allows the derivation of the posterior Q, in contrast to most of the other implicit bounds over KL(Rs(GQ)\u2016R(GQ)). Theorem 1. For any distribution D over X\u00d7Y, any space F of classifiers, any space V of random variables, any distribution P over F\u00d7V, any \u03b4 \u2208 (0, 1], and any real number C > 0, \u2200 Q over F \u00d7V, we have:\nPr\n(\nR(GQ) \u2264 1\n1 \u2212 e\u2212C\n[ 1 \u2212 exp { \u2212 CRS (GQ)\n\u2212 1 m [KL(Q\u2016P) \u2212 ln \u03b4] }] ) \u2265 1 \u2212 \u03b4\nThis is a slight extension of Corollary 2.2 of [2], and can be proved by replacing f with ( f , v) and reapplying its proof [2].\nThe above risk bound is derived for labeled data. Here we have extended the bound to accommodate both labeled and unlabeled data for semi-supervised learning in the following theorem. The semi-supervised bound is different with [12] whose bound is implicit and has no explicit solution of Q.\n3 Theorem 2. For any distribution D over X\u00d7Y, any space F of classifiers, any space V of random variables, distribution P over F\u00d7V, any \u03b4 \u2208 (0, 1], and any real number C > 0, \u2200 Q over F \u00d7V, we have:\nPr\n(\nR(GQ) \u2264 1\n1 \u2212 e\u2212C\n[ 1 \u2212 exp { \u2212 C [ eS (GQ) + 1 2 dS (GQ) ]\n\u2212 1 m [KL(Q\u2016P) \u2212 ln \u03b4] }] ) \u2265 1 \u2212 \u03b4\nwhere the risks for labeled and unlabeled data are eS (GQ)= E( f1,v1)\u223cQE( f2,v2)\u223cQ 1 m \u2211m i=1 I( f1(xi, v1) , yi))I( f2(xi, v2), yi)) and dS (GQ) = E( f1,v1)\u223cQE( f2,v2)\u223cQ 1 m \u2211m i=1 I( f1(xi, v1) , f2(xi, v2)).\nProof: Let E f be the abbreviation of E( f ,v)\u223cQ. Note that E f1, f2I( f1 , f2) = E f1, f22I( f1 , y)I( f2 = y) = E f1, f2 2I( f1 , y)(1\u2212 I( f2 , y)) = E f1, f22(I( f1 , y)\u2212I( f1 , y)I( f2 , y)) and RS (GQ)= 1 m \u2211 i E f1 I( f1 , yi) (Eq. (7)). Therefore RS (GQ)= 1 m \u2211 i E f1I( f i 1 , yi) = 1m \u2211 i E f1, f2I( f i 1 , yi)I( f i 2 , yi) + 1 2m \u2211 i E f1, f2 I( f i 1 , f i 2) = eS (GQ)+ 12 dS (GQ), ( f i 1 = f1(xi)). Substituting RS (GQ) = eS (GQ) + 12 dS (GQ) into Theorem 1, then we obtain Theorem 2. Since dS (GQ) is independent of labels, it allows classifiers using the above bound to exploit unlabel data. Minimizing this risk dS (GQ) would contract the posteriors over the stochastic classifier and the stochastic feature space, making classification and feature mapping less uncertain."}, {"heading": "B. Objective function and specification", "text": "Let B(Q,C) = 11\u2212e\u2212C [1 \u2212 exp{\u2212J(Q) + 1 m ln \u03b4}] be the upper bound in Theorem 2, where J(Q) = C(eS (GQ) + 12 dS (GQ)) + 1 m KL(Q \u2016 P). Training a classifier with minimum risk means minimizing the upper bound B(Q,C) w.r.t. Q and C. Note that minimizing B(Q,C) w.r.t. Q equals to minimizing J(Q) w.r.t. Q. Since unlabeled data are only available in estimating dS (GQ), J(Q) over labeled data S l of ml examples and unlabeled data S u of mu examples can be written as\nJ(Q) = C\n[\neS l (GQ) + 1 2 dS u(GQ)\n]\n+ 1 m KL(Q\u2016P)\nwhere m = ml + mu. This form enables us to derive the analytical form of posterior distributions Q of the classifier and the hidden variables. Apply the above bound to the stochastic classifier defined in Eq. (5) and set v = (h+, h\u2212), we have\nf = fw, E f\u223cQ[\u00b7] = Ew\u223cQ[\u00b7] (8)\nThen learning PAC-Bayes bound classifier with the generative model embedding is to minimize the objective function J(Q) w.r.t. the posterior Q(w, h+, h\u2212) and parameters \u03b8+, \u03b8\u2212:\nJ(Q) = C\n[\neS l(GQ) + 1 2 dS u (GQ)\n]\n+ 1 m [KL(Q(w) \u2016 P(w)) (9)\n+ KL(Q(h+) \u2016 P(x, h+ | \u03b8+)) + KL(Q(h\u2212) \u2016 P(x, h\u2212 | \u03b8\u2212))]\nwhere GQ is the linear stochastic classifier defined in Eq. (5); P(x, h+ | \u03b8+) and P(x, h\u2212 | \u03b8\u2212) are generative models for positive and negative classes respectively; the first row is the objective function for regular Gibbs classifier and the second row is the objective function for two generative models.\nTo compute the objective function J(Q) in Eq. (9), we will need to have approximations or expressions for eS l , dS u and\nKL(Q||P) that are computationally tractable. To derive these expressions, as were done in [5], we assume that the prior of the weight is Gaussian P(w) = N(u0, I) and its posterior is also Gaussian except with a different mean, i.e., Q(w) = N(u, I). Based on this assumption, we have:\nKL(Q(w)\u2016P(w)) = 1 2 \u2016u \u2212 u0 \u20162 (10)\nUsing the assumption and Gaussian integrals [5], we have,\nE w\u223cQ\nI( fw(x, h+, h\u2212) , y) = \u03a6 ( yu \u00b7 \u03c6\u0304(x, h+, h\u2212) )\n(11)\nwhere \u03a6(a)= \u222b \u221e\na 1\u221a 2\u03c0 exp (\u2212 x22 )dx= 1 2 erfc( a\u221a 2 ) and \u03c6\u0304= \u03c6(x,h+ ,h\u2212)\u2016\u03c6(x,h+ ,h\u2212)\u2016 .\nFurther, considering Eq. (11), we have the integration:\nE w1,w2\u223cQ I( fw1 , fw2 ) = E w1,w2\u223cQ 2I( fw1 , 1)I( fw2 , \u22121) (12)\n= 2\u03a6 ( u \u00b7 \u03c6\u0304(x, h+, h\u2212) ) \u03a6 (\u2212u \u00b7 \u03c6\u0304(x, h+, h\u2212) )\nWith these formulas, we proceed to obtain an expression for J(Q), and find Q(h+, h\u2212), Q(w), \u03b8+ and \u03b8\u2212 by minimizing J(Q) with an iterative optimization procedure in the next section.\nIV. Inference and parameter estimation\nIn this section, we derive the learning procedure (inference and parameter estimation) of the proposed approach. Consider Eq. (9) and eS (GQ) = 1m \u2211\ni Ew1 I( fw1(xi) , yi) Ew2I( fw2 (xi) , yi) and dS (GQ) = 1m \u2211\ni Ew1,w2I( fw1 (xi) , fw2 (xi)) (Th.2), J(Q) (average cost) over the training set S =S l \u222a S u is:\nJ(Q)=C\n[\n1 ml\nml \u2211\ni=1\nEQI( fw(xi) , yi) + 1\nmu\nmu \u2211\ni=1\nEQI( fw1 (xi) , fw2 (xi)) ]\n+ 1\nm2\nm \u2211\ni=1\nKL(Qi(h+, h\u2212)\u2016P(xi, h+, h\u2212)) + 1 m KL(Q(w)\u2016P(w))\nwhere the terms EQI( fw(xi) , yi), EQI( fw1 (xi) , fw2 (xi)) and KL(Q(w) \u2016 P(w)) have been respectively given by Eq. (11), Eq. (12) and Eq. (10); P(xi, h+, h\u2212) is the abbreviation of P(xi, h+)P(xi, h\u2212). We now show how an EM-like iterative procedure [4] can be used to learn the stochastic feature space and the Gibbs classifier simultaneously.\nA. Inference: minimize J(Q) w.r.t. Qi(h+, h\u2212)\nIn the first step, we fix Q(w), \u03b8+, \u03b8\u2212, and minimize J(Q) w.r.t. Qi(h+, h\u2212), subject to \u222b\nQi(h+, h\u2212)dh+dh\u2212=1. Benefiting from the explicit bound (Th. 2), we has the following solution:\nQi(h+, h\u2212) = 1 Zi P(h+, h\u2212, xi) exp {\u2212CEQ(w)[\u03d5i] }\n(13)\nwhere \u03d5i = m 2 ml I( fw , yi) if xi \u2208 S l and \u03d5i = m 2 2mu I( fw1 , fw2 ) if xi \u2208 S u. Note that EQ(w)[\u03d5i] is given by Eq. (11) and Eq. (12). The fact that the output of classifier is inside the expression for posteriors means that the generative models are being tuned as well when the classifier is being optimized during the minimization of PAC-Bayes bound. This tuning inhibits those examples of h+, h\u2212 that lead to misclassification and encourages those with less misclassification. Sampling from this posterior is simple using Gibbs-rejection sampling, because P(h, xi) can be directly used as the comparison function\n4 since P(h, xi) exp(\u00b7) \u2264 P(h, xi) (EQ(w)[\u03d5i] \u2265 0 as I(\u00b7) is a zeroone output function). Considering the j-th example hi j drawn from P(h, xi), we reject it if Qi(hi j) < r j where r j is drawn from the uniform distribution over [0, P(hi j, xi)].\nB. Parameter estimation: minimize J(Q) w.r.t parameters\nIn the second step, we fix the posteriors Qi(h+, h\u2212) and parameters \u03b8+, \u03b8\u2212, and determine the posterior distribution Q(w). Instead of sampling from Q(w), we directly determine its parameter u by minimizing J(Q) w.r.t. u. Since J(Q) w.r.t. u is intractable, we resort to minimizing its upper bound J(u) (see the Appendix) w.r.t. u. The gradient of J(u) w.r.t. u is:\n\u2202J(u) \u2202u = 1 m (u\u2212u0) \u2212 C\nmln\n\u2211ml ,n\ni, j=1 g(yiu \u00b7 \u03c6\u0304i j) yi \u03c6\u0304i j (14)\n+ C\nmun\n\u2211mu,n\ni, j=1 g(u \u00b7 \u03c6\u0304i j)\n[ \u03a6(u \u00b7 \u03c6\u0304i j) \u2212\u03a6(\u2212u \u00b7 \u03c6\u0304i j) ] \u03c6\u0304i j\nwhere g(\u00b7) is the gaussian function with mean \u00b5 = 0 and std \u03b4 = 1. The gradient of B(Q,C) with respect to C is:\n\u2202B(Q,C) \u2202C = \u2212e\u2212C (1 \u2212 e\u2212C)2 [ 1 \u2212 exp ( \u2212 J(Q)\u2212 1 m log \u03b4 ) ]\n(15)\n+ 1\n1 \u2212 e\u2212C [RS l(GQ) + dS u(GQ)] exp ( \u2212J(Q)\u2212 1 m log \u03b4 )\nIn the third step, we fix Qi(h+, h\u2212), u and update parameters \u03b8+, \u03b8\u2212. Note only the third term of Eq. (9), i.e., the objection function of the positive model, involves \u03b8+. So the update rules of \u03b8+, derived by minimizing Eq. (9) w.r.t. \u03b8+, are same as those of the original generative model. Similarly for \u03b8\u2212.\nThe learning procedure is summarized in Algorithm 1. In classification, similar with [2], we use the decision rule of majority vote y\u0302 = sign[ 1n \u2211n j=1 EP(h+,h\u2212 |xi)Q(w)w \u00b7 \u03c6(xi, h+, h\u2212)] \u2243 sign[ 1n \u2211n\nj=1 u\u00b7\u03c6(xi, h+i j, h\u2212i j)] with n = 5 and (h+i j, h\u2212i j) being the j-th example drawn from P(h+, h\u2212|xi).\nAlgorithm 1 Inference and learning\n1: input: data set S l, S u, and S \u2032l , S \u2032 u are fractions of S l, S u 2: initialize u\u0302, \u03b8\u0302+, \u03b8\u0302\u2212, learning rates \u03b3u, \u03b3c, and \u03b4 = 0.05 3: u\u03020 \u2190 minu RS \u2032l (GQ) + 1 2 dS \u2032u (GQ) 4: repeat 5: for i = 1 to m do 6: sample Qi(h+, h\u2212) using Gibbs-rejection sampling 7: end for 8: update \u03b8\u0302+ with {h+i j}i j (xi \u2208 S +l ) using the rules of the original generative model. Similar for \u03b8\u0302\u2212. 9: u\u0302 \u2190 u\u0302 \u2212 \u03b3u \u2202J(u)\u2202u , C \u2190 C \u2212 \u03b3c \u2202B \u2202C\n10: until convergence 11: output: u\u03020, u\u0302, \u03b8\u0302+, \u03b8\u0302\u2212\nV. Experiments\nThis section empirically evaluates the proposed method stochastic feature mapping (SFM) and related methods on general classification tasks, scene recognition and protein sequence classification respectively. For multiple-class classification problem, we divide it into binary classification problems, each of which is an one-versus-rest problem that distinguishes\none class from others. For each binary problem, we randomly partition the positive examples into 50% training and 50% test sets, and similarly for negative examples. We test each binary classification problem on 20 random partitions, and report the average results. For the semi-supervised version, we use 25% of test examples as unlabeled data. Two related and general methods, Fisher score (FS) [3] and free energy score space (FESS) [9], and some other state-of-the-art methods are also tested for comparison.\nThere are two points in implementation. First, the optimization procedures of u0 and u may suffer from the local minima problem, resulting in poor solution. The strategy adopted by [2] is to perform the optimization for 10\u223c100 trials where a new random initial point within the range [\u221220, 20]d is used in each trial. Second, the value of parameter C has been shown to be important. Another effective strategy experimented is to assess the performance using 10-fold cross-validation."}, {"heading": "A. Deriving a general classification tool", "text": "In the first experiment, we derive a general classification method by applying the proposed framework to a simple yet general generative model, Gaussian mixture model. Let x \u2208 Rd be the observed variable; z = {z1, \u00b7 \u00b7 \u00b7 , zk} be the hidden binary indicate vector for K mixture components, and assume the covariance matrix be diagonal; a = {a1, \u00b7 \u00b7 \u00b7 , ak} be the parameters of the approximate posterior of z. The elements of the feature mapping \u03c6 of this model are {zi(xT , diag(xxT ), 1), zi log ai}Ki=1. The posterior of z can be easily derived from Eq. (13). The number of mixture components is configured to K = 4 throughout the experiment.\nWe select 8 data sets from UCI database for evaluation, preferring those with no missing entities. The number of classes of each data set is between 2 and 15. The number of examples of each class varies from 14 and 673. The dimensionality is between 9 and 90. We compare our method SFM with Adaboost [17], SVM [11], localized multiple kernel learning (LMKL) [18] and PAC-Bayes gradient descent PBGD3 [2]. The average results are reported in Table I. It shows that SFM is adaptive to different data sets and outperforms other methods in half of the data sets. It is also worth noting that the linear version of PBGD3 does work well in these evaluation. The results of semi-supervised version is presented in Fig. 1."}, {"heading": "B. Scene recognition", "text": "We evaluate our SFM method and compare its performance against comparable methods on a typical vision task, scene recognition. In this task, visual words are used for image representation for its robustness to topic and spatial variance.\n5\nWe use latent Dirichlet allocation (LDA) [1] to model the distributions of visual words, and derive a recognition tool under the proposed framework. Like [15], we sample the topic variable using collapsed Gibbs sampling and reject examples according to the rule for Eq. (13). We fix the parameter \u03b1 and allow \u03b2 [15] to be updated. Let w, z respectively indicate word and topic, and \u03b3 be the parameter of the approximate posterior of z. The elements of the feature mapping \u03c6 of a model are {znk,wnznk, znk log \u03b3nk}n,i,k where n, i, k index word, term and topic respectively. For FS [3] and FESS [9], we extract features from the trained LDA model and deliver to SVM. The number of topics of LDA is set to 50.\nThe CVCL scene dataset is chosen for evaluation. It contains 4 artificial scenes and 4 natural scenes. For each image, dense SIFT descriptors [6] are extracted from 20\u00d7 20 grid patches over 4 scales. These descriptors are quantized to visual words using a code book (50 centers) clustered from some random selected descriptors. The resulting visual words of an image are in the form of histogram where each bin corresponds to a code center of the code book. The evaluation results are summarized in Table II. Our results compare well with PHOW [16] which is a state-of-the-art feature for scene recognition. The results of semi-supervised learning are shown in Fig. 1, demonstrating unlabeled examples can help classification particularly when there are few labeled examples."}, {"heading": "C. Protein classification", "text": "To evaluate the capability of the proposed approach in dealing with variable length sequences, we apply the proposed framework to remote homology recognition. The problem here assigns test protein sequences to the domain superfamilies\ndefined in the SCOP (1.53) taxonomy tree according to functions of proteins. The protein sequence data is obtained from ASTRAL database with E-value threshold of 10\u221225 to reduce similar sequences. We uses four labeled domain superfamilies, metabolism, information, intra-cellular processes and extracellular processes for evaluation. The numbers of sequences are 804, 950, 695 and 992 respectively. Each protein sequence is a string composed of 22 distinct letters, and the string length varies from 20 to 994.\nHidden Markov model (HMM) [10] is used to model the distribution over protein sequences for its ability in handling sequences with variable length. The number of output states is 22, and the number of hidden states is set to 10. Let x be the sequence with length Tx, where xt be the binary indicator where xtk = 1 if the k-th state of K possible ones is selected at time t. Let qt be the binary state indicator where qti = 1 if the i-th state of M possible ones is selected at time t; AM\u00d7M be the transition probabilities of the approximate posteriors. The elements of the feature mapping \u03c6 can be written as {q0i , \u2211Tx\u22121 t=0 q t iq t+1 j , \u2211Tx\u22121 t=0 q t iq t+1 j log Ai j, \u2211Tx t=0q t i x t k}i, j,k. With the hidden states of the input sequence inferred by BaumWelch algorithm [22], it is easy to estimate the posterior transition probabilities conditioned on x. Using the sampling distribution derived in Eq. (13), we are able to draw the examples of hidden states and re-estimate their posterior. The results are reported in Table III. The 2-gram feature is actually the transition probability of observed states of a sequence, i.e. { 1Tc \u2211Tc\u22121 t=0 x t i x t+1 k }i,k. The difference of the performance of the first four methods are not significant except on family #3. The results of semi-supervised learning are reported in Fig. 1, which shows improvement on few training samples.\n6\nThis paper presents a framework to incorporate the abilities of generative model and discriminative model for classification under the PAC-Bayes theory. The bridge of this incorporation is a stochastic feature mapping which is derived from the linear form of the practical MAP classifier and is independent with the parameters of the adopted generative models. Under this framework, the derived stochastic feature mapping and generative models can be tuned during the training of the classifier. A major difficulty is the non-convexity of the objective function, where local minima can hamper the solution. Our approach can benefit from the development or exploitation of more robust and efficient optimization methods.\nAppendix\nSince J(Q) is intractable, we derive its upper bound by fixing \u03b8+, \u03b8\u2212, Qi(h+, h\u2212). Using Eq. (10) and Eq. (13), we have\nKL(Q\u2016P)=KL(Q(w, h+, h\u2212)\u2016P(w)P(x, h+ | \u03b8+)P(x, h\u2212 | \u03b8\u2212))\n= E Qw\n[\nlog Q(w) P(w)\n]\n+ 1 m\nm \u2211\ni=1\nE Qi\n[\nlog Qi(h+)Qi(h\u2212)\nP(xi, h+)P(xi, h\u2212)\n]\n= 1 2 \u2016u\u2212u0 \u20162\u2212Cm\n[\neS l (GQ)+ 1 2 dS u(GQ)\n]\n\u2212 1 m\nm \u2211\ni=1\nlog Zi\nwhere Qw = Q(w), Qi = Qi(h+, h\u2212) and m \u2211\ni=1\nlog Zi = m \u2211\ni=1\nlog E Qi\n[exp {\u2212C EQ(w)[\u03d5i] } ] \u2265 m \u2211\ni=1\nE Qi\n[\u2212C EQw [\u03d5i]]\n=\nml \u2211\ni=1\nE QwQi\n[\nCm2\nml I( f iw,yi)\n]\n+\nmu \u2211\ni=1\nE QwQi\n[\nCm2 2mu I( f iw1 , f i w2 )\n]\n\u2243 Cm 2\nmln\nml,n \u2211\ni, j=1\n\u03a6 ( yiu\u00b7\u03c6\u0304i j ) + Cm2\nmun\nmu,n \u2211\ni, j=1\n\u03a6 ( u\u00b7\u03c6\u0304i j ) \u03a6 ( \u2212u\u00b7\u03c6\u0304i j )\nwhere the inequality is derived by applying Jensen\u2019s inequality; f iw = fw(xi); \u03c6\u0304i j = \u03c6(xi ,h+i j,h\u2212i j) \u2016\u03c6(xi ,h+i j,h\u2212i j)\u2016 where (h+i j, h\u2212i j) represents the j-th example drawn from Qi(h+, h\u2212). Now we have all the pieces for eS l , dS u and KL(Q\u2016P), and can obtain,\nJ(Q) = C\n[\neS l (GQ) + 1 2 dS u(GQ)\n]\n+ 1 m KL(Q \u2016 P)\n= 1\n2m \u2016u \u2212 u0 \u20162 \u2212 1 m2 \u2211\ni\nlog Zi\n\u2264 1 2m \u2016u \u2212 u0 \u20162 + C\nmln\nml \u2211\ni=1\nn \u2211\nj=1\n\u03a6 ( yiu \u00b7 \u03c6\u0304i j )\n+ C\nmun\nmu \u2211\ni=1\nn \u2211\nj=1\n\u03a6 ( u \u00b7 \u03c6\u0304i j ) \u03a6 ( \u2212u \u00b7 \u03c6\u0304i j ) , J(u)\nThis work is supported by National Basic Research Program of China (Grant No. 2011CB302203), NSFC (Grant No. 60833009 and 60975012) and Microsoft Research Fellowship.\nReferences\n[1] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993\u20131022, 2003. [2] P. Germain, A. Lacasse, F. Laviolette, and M. Marchand. PAC-Bayesian learning of linear classifiers. In ICML, pages 353\u2013360. 2009. [3] T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classifiers. In NIPS, pages 487\u2013493, 1999. [4] M. Jordan, Z. Ghahramani, J. Tommi., and S. Lawrence. Introduction to variational methods for graphical models. Machine Learning, 37:183\u2013 233, 1999. [5] J. Langford. Tutorial on practical prediction theory for classification. Journal of Machine Learning Research, 6(1):273, 2006. [6] D. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91\u2013110, 2004. [7] R. Neal and G. Hinton. A view of the EM algorithm that justifies incremental, sparse, and other variants. Learning in Graphical Models, 89:355\u2013368, 1998. [8] A. Y. Ng and M. I. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In NIPS, 2002. [9] A. Perina, M. Cristani, U. Castellani, V. Murino, and N. Jojic. Free energy score space. In NIPS, pages 1428\u20131436, 2009. [10] L. Rabiner. A tutorial on hidden Markov models and selected applications inspeech recognition. Proceedings of the IEEE, 77(2):257\u2013286, 1989. [11] V. Vapnik. The nature of statistical learning theory. Springer Verlag, 2000. [12] A. Lacasse, F. Laviolette, and M. Marchand. PAC-Bayes bounds for the risk of the majority vote and the variance of the Gibbs classifier. In NIPS, 2006. [13] D. McAllester. Some PAC-Bayesian theorems. Machine Learning, 37:355\u2013363, 1999. [14] X. Li, T. Lee, Y. Liu. Hybrid generative-discriminative classification using posterior divergence. In CVPR, 2011. [15] T. Griffiths, and M. Steyvers. Finding scientifc topics. Proceedings of the National Academy of Sciences, 101:5228\u20135235, 2004. [16] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Multiple Kernels for Object Detection. In ICCV, 2009. [17] Y. Freund, and R. Schapire. A desicion-theoretic generalization of online learning and an application to boosting. Computational Learning Theory, pages 23\u201337, 1995. [18] M. Gonen, and E. Alpaydin. Localized Multiple Kernel Learning. In ICML, 2008. [19] T. Jaakkola, M. Meila, and T. Jebara. Maximum entropy discrimination. Technical Report AITR-1668, MIT, 1999. [20] A. Holub, M. Welling, and P. Perona. Hybrid generative-discriminative visual categorization. International Journal of Computer Vision, 77(1):239\u2013258, 2008. [21] R. Raina, Y. Shen, A. Ng, and A. McCallum. Classificatin with hybrid generatve/discriminative models. In NIPS, 2004. [22] L. Baum, T. Petrie, G. Soules, and N. Weiss. A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. The Annals of Mathematical Statistics, 41(1):164\u2013171, 1970. [23] A. Mccallum, C. Pal, G. Druck, and X. Wang. Multi-conditional learning: Generative/discriminative training for clustering and classification. In AAAI, pages 433\u2013439, 2006."}], "references": [{"title": "Latent dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "PAC-Bayesian learning of linear classifiers", "author": ["P. Germain", "A. Lacasse", "F. Laviolette", "M. Marchand"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["T. Jaakkola", "D. Haussler"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "J. Tommi", "S. Lawrence"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Tutorial on practical prediction theory for classification", "author": ["J. Langford"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "author": ["R. Neal", "G. Hinton"], "venue": "Learning in Graphical Models,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes", "author": ["A.Y. Ng", "M.I. Jordan"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Free energy score space", "author": ["A. Perina", "M. Cristani", "U. Castellani", "V. Murino", "N. Jojic"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "A tutorial on hidden Markov models and selected applications inspeech recognition", "author": ["L. Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "PAC-Bayes bounds for the risk of the majority vote and the variance of the Gibbs classifier", "author": ["A. Lacasse", "F. Laviolette", "M. Marchand"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Some PAC-Bayesian theorems", "author": ["D. McAllester"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Hybrid generative-discriminative classification using posterior divergence", "author": ["X. Li", "T. Lee", "Y. Liu"], "venue": "In CVPR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Finding scientifc topics", "author": ["T. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Multiple Kernels for Object Detection", "author": ["A. Vedaldi", "V. Gulshan", "M. Varma", "A. Zisserman"], "venue": "In ICCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "A desicion-theoretic generalization of online learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "Computational Learning Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Localized Multiple Kernel Learning", "author": ["M. Gonen", "E. Alpaydin"], "venue": "In ICML,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Maximum entropy discrimination", "author": ["T. Jaakkola", "M. Meila", "T. Jebara"], "venue": "Technical Report AITR-1668,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Hybrid generative-discriminative visual categorization", "author": ["A. Holub", "M. Welling", "P. Perona"], "venue": "International Journal of Computer Vision,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Classificatin with hybrid generatve/discriminative models", "author": ["R. Raina", "Y. Shen", "A. Ng", "A. McCallum"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains", "author": ["L. Baum", "T. Petrie", "G. Soules", "N. Weiss"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1970}, {"title": "Multi-conditional learning: Generative/discriminative training for clustering and classification", "author": ["A. Mccallum", "C. Pal", "G. Druck", "X. Wang"], "venue": "In AAAI,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}], "referenceMentions": [{"referenceID": 18, "context": "The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9].", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9].", "startOffset": 114, "endOffset": 117}, {"referenceID": 20, "context": "The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9].", "startOffset": 119, "endOffset": 123}, {"referenceID": 22, "context": "The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9].", "startOffset": 125, "endOffset": 129}, {"referenceID": 8, "context": "The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "Generative score space methods [3], [9], [14] are motivated by the above observations.", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "Generative score space methods [3], [9], [14] are motivated by the above observations.", "startOffset": 36, "endOffset": 39}, {"referenceID": 13, "context": "Generative score space methods [3], [9], [14] are motivated by the above observations.", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "Maximum entropy discrimination [19] provides yet another framework to exploit generative models for classification under the large margin principle.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "In addition, the VC risk bound [11] utilized by this method is generally loose in comparison with the PAC-Bayes bounds [13], [5], [2].", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "In addition, the VC risk bound [11] utilized by this method is generally loose in comparison with the PAC-Bayes bounds [13], [5], [2].", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "In addition, the VC risk bound [11] utilized by this method is generally loose in comparison with the PAC-Bayes bounds [13], [5], [2].", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "In addition, the VC risk bound [11] utilized by this method is generally loose in comparison with the PAC-Bayes bounds [13], [5], [2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 20, "context": "Also, there are some other efforts [21], [23] made to couple generative and discriminative models for classification.", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "Also, there are some other efforts [21], [23] made to couple generative and discriminative models for classification.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "This paper proposes an approach based on the PAC-Bayes theory [13], [5], [2] to integrate the complementary strengths of generative and discriminative models.", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "This paper proposes an approach based on the PAC-Bayes theory [13], [5], [2] to integrate the complementary strengths of generative and discriminative models.", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "This paper proposes an approach based on the PAC-Bayes theory [13], [5], [2] to integrate the complementary strengths of generative and discriminative models.", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "This is distinct from the current methods [3], [9], [14] which map a data point to a feature deterministically.", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "This is distinct from the current methods [3], [9], [14] which map a data point to a feature deterministically.", "startOffset": 47, "endOffset": 50}, {"referenceID": 13, "context": "This is distinct from the current methods [3], [9], [14] which map a data point to a feature deterministically.", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "The derived feature mapping is functioning similar to [3], [9], [14].", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "The derived feature mapping is functioning similar to [3], [9], [14].", "startOffset": 59, "endOffset": 62}, {"referenceID": 13, "context": "The derived feature mapping is functioning similar to [3], [9], [14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 6, "context": "We can resort to the following variational lower bound [7], [4]:", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "We can resort to the following variational lower bound [7], [4]:", "startOffset": 60, "endOffset": 63}, {"referenceID": 18, "context": "(1)), we resort to the following tractable one [19], [9]", "startOffset": 47, "endOffset": 51}, {"referenceID": 8, "context": "(1)), we resort to the following tractable one [19], [9]", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "Further, we assume that the approximate posterior of h, for the example x, takes the same from with its prior P(h) but with different parameter [4] Q(h) = exp{c(\u03b8\u2032 h) T (h) + S (h) + f (\u03b8\u2032 h)}.", "startOffset": 144, "endOffset": 147}, {"referenceID": 10, "context": "First, this classifier allows PAC-Bayes risk bounds that have explicit solutions for Q(h+, h\u2212) which can help tune the feature mapping for better classification; Second, the PACBayes risk bound for such a classifier can be tighter than VC bounds [11]; Third, the feature mapping is independent with model parameters \u03b8, making the solution of \u03b8 very simple.", "startOffset": 246, "endOffset": 250}, {"referenceID": 12, "context": "In a PAC-Bayes setting [13], each example (x, y) is drawn from a fixed, but unknown, probability distribution D on X \u00d7 Y.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "Among several PACBayes bounds [13], [5], [2], [12], the bound derived in [2] is quite tight and gives an explicit bound for the true risk R(GQ), which allows the derivation of the posterior Q, in contrast to most of the other implicit bounds over KL(Rs(GQ)\u2016R(GQ)).", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "Among several PACBayes bounds [13], [5], [2], [12], the bound derived in [2] is quite tight and gives an explicit bound for the true risk R(GQ), which allows the derivation of the posterior Q, in contrast to most of the other implicit bounds over KL(Rs(GQ)\u2016R(GQ)).", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "Among several PACBayes bounds [13], [5], [2], [12], the bound derived in [2] is quite tight and gives an explicit bound for the true risk R(GQ), which allows the derivation of the posterior Q, in contrast to most of the other implicit bounds over KL(Rs(GQ)\u2016R(GQ)).", "startOffset": 41, "endOffset": 44}, {"referenceID": 11, "context": "Among several PACBayes bounds [13], [5], [2], [12], the bound derived in [2] is quite tight and gives an explicit bound for the true risk R(GQ), which allows the derivation of the posterior Q, in contrast to most of the other implicit bounds over KL(Rs(GQ)\u2016R(GQ)).", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "Among several PACBayes bounds [13], [5], [2], [12], the bound derived in [2] is quite tight and gives an explicit bound for the true risk R(GQ), which allows the derivation of the posterior Q, in contrast to most of the other implicit bounds over KL(Rs(GQ)\u2016R(GQ)).", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "2 of [2], and can be proved by replacing f with ( f , v) and reapplying its proof [2].", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "2 of [2], and can be proved by replacing f with ( f , v) and reapplying its proof [2].", "startOffset": 82, "endOffset": 85}, {"referenceID": 11, "context": "The semi-supervised bound is different with [12] whose bound is implicit and has no explicit solution of Q.", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "To derive these expressions, as were done in [5], we assume that the prior of the weight is Gaussian P(w) = N(u0, I) and its posterior is also Gaussian except with a different mean, i.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "Using the assumption and Gaussian integrals [5], we have,", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "We now show how an EM-like iterative procedure [4] can be used to learn the stochastic feature space and the Gibbs classifier simultaneously.", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "In classification, similar with [2], we use the decision rule of majority vote \u0177 = sign[ n \u2211n j=1 EP(h+,h\u2212 |xi)Q(w)w \u00b7 \u03c6(xi, h+, h\u2212)] \u2243 sign[ n \u2211n j=1 u\u00b7\u03c6(xi, h+i j, h\u2212i j)] with n = 5 and (h+i j, h\u2212i j) being the j-th example drawn from P(h+, h\u2212|xi).", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "Two related and general methods, Fisher score (FS) [3] and free energy score space (FESS) [9], and some other state-of-the-art methods are also tested for comparison.", "startOffset": 51, "endOffset": 54}, {"referenceID": 8, "context": "Two related and general methods, Fisher score (FS) [3] and free energy score space (FESS) [9], and some other state-of-the-art methods are also tested for comparison.", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "The strategy adopted by [2] is to perform the optimization for 10\u223c100 trials where a new random initial point within the range [\u221220, 20]d is used in each trial.", "startOffset": 24, "endOffset": 27}, {"referenceID": 16, "context": "We compare our method SFM with Adaboost [17], SVM [11], localized multiple kernel learning (LMKL) [18] and PAC-Bayes gradient descent PBGD3 [2].", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "We compare our method SFM with Adaboost [17], SVM [11], localized multiple kernel learning (LMKL) [18] and PAC-Bayes gradient descent PBGD3 [2].", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "We compare our method SFM with Adaboost [17], SVM [11], localized multiple kernel learning (LMKL) [18] and PAC-Bayes gradient descent PBGD3 [2].", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "We compare our method SFM with Adaboost [17], SVM [11], localized multiple kernel learning (LMKL) [18] and PAC-Bayes gradient descent PBGD3 [2].", "startOffset": 140, "endOffset": 143}, {"referenceID": 0, "context": "We use latent Dirichlet allocation (LDA) [1] to model the distributions of visual words, and derive a recognition tool under the proposed framework.", "startOffset": 41, "endOffset": 44}, {"referenceID": 14, "context": "Like [15], we sample the topic variable using collapsed Gibbs sampling and reject examples according to the rule for Eq.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "We fix the parameter \u03b1 and allow \u03b2 [15] to be updated.", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "For FS [3] and FESS [9], we extract features from the trained LDA model and deliver to SVM.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "For FS [3] and FESS [9], we extract features from the trained LDA model and deliver to SVM.", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "For each image, dense SIFT descriptors [6] are extracted from 20\u00d7 20 grid patches over 4 scales.", "startOffset": 39, "endOffset": 42}, {"referenceID": 15, "context": "Our results compare well with PHOW [16] which is a state-of-the-art feature for scene recognition.", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "Hidden Markov model (HMM) [10] is used to model the distribution over protein sequences for its ability in handling sequences with variable length.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "With the hidden states of the input sequence inferred by BaumWelch algorithm [22], it is easy to estimate the posterior transition probabilities conditioned on x.", "startOffset": 77, "endOffset": 81}], "year": 2012, "abstractText": "Probabilistic generative modeling of data distributions can potentially exploit hidden information which is useful for discriminative classification. This observation has motivated the development of approaches that couple generative and discriminative models for classification. In this paper, we propose a new approach to couple generative and discriminative models in an unified framework based on PAC-Bayes risk theory. We first derive the model-parameter-independent stochastic feature mapping from a practical MAP classifier operating on generative models. Then we construct a linear stochastic classifier equipped with the feature mapping, and derive the explicit PAC-Bayes risk bounds for such classifier for both supervised and semi-supervised learning. Minimizing the risk bound, using an EM-like iterative procedure, results in a new posterior over hidden variables (Estep) and the update rules of model parameters (M-step). The derivation of the posterior is always feasible due to the way of equipping feature mapping and the explicit form of bounding risk. The derived posterior allows the tuning of generative models and subsequently the feature mappings for better classification. The derived update rules of the model parameters are same to those of the uncoupled models as the feature mapping is model-parameterindependent. Our experiments show that the coupling between data modeling generative model and the discriminative classifier via a stochastic feature mapping in this framework leads to a general classification tool with state-of-the-art performance.", "creator": "LaTeX with hyperref package"}}}