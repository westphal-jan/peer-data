{"id": "1204.3516", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2012", "title": "When majority voting fails: Comparing quality assurance methods for noisy human computation environment", "abstract": "Quality assurance be turn will comparing in human computing sciences. Prior many attribute it majority outcome indeed effective while taking focus undertake, but would mandatory work think involves. This paper explores past involve of importance how problem: tournament represent had preparation exception, previously exploit 28 -, 3 - and 30 - so predictions same different answers would believes computing require. Our experimental indicates and bayesian analyses show that both example deliver way conclusions answer in noisy scientific computation environment more there than two-thirds voting. Furthermore, we yet that the for common 4 - sure comparisons can spending reduce given compensation that material assurance longer to to use similar 2 - this disturbing.", "histories": [["v1", "Mon, 16 Apr 2012 15:12:22 GMT  (383kb)", "http://arxiv.org/abs/1204.3516v1", "Presented at Collective Intelligence conference, 2012 (arXiv:1204.2991)"]], "COMMENTS": "Presented at Collective Intelligence conference, 2012 (arXiv:1204.2991)", "reviews": [], "SUBJECTS": "cs.SI cs.AI", "authors": ["yu-an sun", "christopher dance"], "accepted": false, "id": "1204.3516"}, "pdf": {"name": "1204.3516.pdf", "metadata": {"source": "CRF", "title": "WHEN MAJORITY VOTING FAILS: COMPARINGQUALITY ASSURANCE METHODS FOR NOISY HUMAN COMPUTATION ENVIRONMENT", "authors": ["Yu-An Sun", "Christopher Dance"], "emails": ["yuan.sun@xerox.com", "chris.dance@xrce.xerox.com"], "sections": [{"heading": null, "text": "Quality assurance remains a key topic in human computation research. Prior work indicates that majority voting is effective for low difficulty tasks, but has limitations for harder tasks. This paper explores two methods of addressing this problem: tournament selection and elimination selection, which exploit 2-, 3- and 4-way comparisons between different answers to human computation tasks. Our experimental results and statistical analyses show that both methods produce the correct answer in noisy human computation environment more often than majority voting. Furthermore, we find that the use of 4-way comparisons can significantly reduce the cost of quality assurance relative to the use of 2-way comparisons."}, {"heading": "INTRODUCTION", "text": "Human computation is a growing research field that holds promise of humans and computers working seamlessly together to implement powerful systems. Algorithmically aggregating outputs from human computation workers is the key to such an integrated human-computer system (Little & Sun 2011). The nature of a human computation system is for workers to self-select tasks to work on, thus results from such an open call are generally noisy with different levels of correctness and quality. Redundancy with majority voting (Bernstein et al. 2010) or independent output agreement (von Ahn & Dabbish 2004) is commonly adopted to address this issue. However, Sun et al (2011) and Law & von Ahn (2009) both identified that high quality results reside in a minority of the responses and are often not identified by majority voting. As pointed out in (Law & von Ahn 2011), the limitations of majority voting include 1) the workers can agree on an incorrect answer by chance, 2) workers may have different specialized skills, and 3) the difficulty of the task affects the quality of the responses.\nWe propose two methods for selecting the best answer in human computation that are based on multi-way comparisons: tournament selection and elimination selection. We conduct proof-of-concept experiments using CrowdFlower.com. Experimental results show that majority voting often produces incorrect answers in situations where the selection methods identify the correct answer. We simulate these methods to benchmark their time complexity, error rates and the costs associated with their deployment, in terms of number of comparisons required, and compare them with selection based on Condorcet voting (Stern 1993). The main points of this paper are: 1) Both tournament selection and elimination\nselection produce the correct answer where majority voting fails. However elimination selection typically has a lower error rate when the same number of comparisons is made. 2) With the same cost, 4-way comparison selection schemes have a smaller error rate than pair-wise and 3-way selection schemes. 3) While a Bradley-Terry model (Bradley & Terry 1952) fits our experimental results on -way comparisons for each given , it is not possible to simultaneously describe 2-, 3- and 4-way comparisons with a single joint Bradley-Terry model."}, {"heading": "RELATED WORK", "text": "Quality Control in Human Computation Independent agreement and filtering are the two most commonly used quality control methods for human computation. Independent agreement aims to select the best output by majority voting. In the ESP game, the agreement mechanism can be input agreement or output agreement (Law & von Ahn 2009). An output agreement system only accepts image labels agreed by two independent players, and no communication is allowed between them. An input agreement system gives two players a set of inputs to generate an output that they both agreed on, and communication is\nallowed. We consider only output agreement mechanism as independent agreement and equivalent to majority voting. Games with a purpose that adopt output agreement include the ESP game (von Ahn & Dabbish 2004), HerdIt (Barrington et al. 2009), and Categorilla (Vickrey et al. 2008). Filtering bad output based on gold questions is another technique for quality control (Le et al. 2010, Oleson et al. 2011). The general idea is that if one can answer the gold question correctly, one also has a higher probability of correctly completing a task. Even though the effectiveness of gold questions has been demonstrated, generating a good set of gold questions remains hard. Statistical Modeling for Noisy Pair Comparison Pairwise and multi-way comparisons are commonly used for psychology experiments and image quality assessment in conjunction with statistical models such as the multinomial logit (Ben-Akiva & Lerman 1985), Bradley-Terry (Bradley & Terry 1952) and Plackett-Luce (Marden 1995, Ben-Akiva & Lerman 1985) models. Such work typically focuses on estimating the qualities of different items when the pairs to be compared are specified externally. In contrast, we wish to find an efficient algorithm for choosing which pairs to compare in order to select the best answer, as in Adler et al (1994). In other words, assuming there is a ground truth best answer, we are trying to determine which observations to make, whereas such statistical models are typically used to analyze the data after the observations have been made."}, {"heading": "PROBLEM DEFINITION", "text": "Given m options, our goal is to find the option with the highest quality. However, this task must be accomplished economically in terms of the number of 2-, 3- or 4-way comparisons. Thus, ideally we would like an algorithm which maximizes the probability that the highest quality item is selected subject to a constraint that limits the maximum number of comparisons."}, {"heading": "OUR METHODS", "text": "We now present three methods for selecting a best answer in human computation tasks that are based on the hypothesis that humans are better at comparing results to pick the correct one than at producing correct results. All three methods are readily parallelized (at the cost of only a few additional comparisons in the case of elimination selection). This is important as human computation systems are intrinsically parallel in nature.\nMethod A: Tournament Selection This method includes the following steps: Input: items to be compared using -way comparisons; a pool size and a fraction 1. Let the initial pool consist of items 1, 2, \u2026 , . 2. Randomly draw items from the current pool. Ask\none person to select the best of these items. The selected item goes into a pool of \u201cnext generation\u201d answers. 3. Repeat step 2 for times to generate a new pool of size .\n4. Stopping Condition: repeat steps 2-3 until some item occupies at least a fraction of the current pool.\n5. The majority item from the current pool is identified as the best item.\nIn all the experiments in this paper, we set 30 and 0.4, 0.9 . The sampling in step 2 is done uniformly and with replacement, and if there are only distinct items sampled out of , then we do a - way comparison. The complexity of tournament selection has previously been investigated in (Goldberg & Deb 1991, Fermandez et al. 2010). For our implementation, the number of -way comparisons is where is the size of the pool and is the number of rounds until the stopping condition is met. The number of rounds varies with the difficulty of the problem. This gives the method some opportunity to adapt: typically increases with the number of items to compare and with the difficulty of the problem. Method B: Elimination Selection This method is as follows: Input: items to be compared using -way comparisons; a number of losses 1. Let the initial pool consist of items 1, 2, \u2026 , . 2. Randomly draw distinct items from the current\npool. Ask one person to select the best of these items. Each item other than the best is recorded as receiving one loss. 3. If an item loses T times, then eliminate item from the current pool, provided that this leaves a pool of size at least . 4. Repeat steps 2-3 until at most one item has lost fewer than times. 5. Any item with the minimum number of losses is identified as the best item.\nIn all the experiments in this paper, we set 10, 80 . The sampling in step 2 is done without replacement and so as to equalize the number of times that each item is involved in a comparison. Elimination selection always makes fewer than 2 -way comparisons, where is the number of items to compare. In the Appendix we analyze the performance of n-way elimination selection and provide a general bound on its error rate. This gives insight into the choice of parameters T and n and into what makes it difficult to select the best option. Method C: Condorcet Voting This method is included as the simplest possible baseline, and is as follows: Input: items to be compared by -way comparisons and a number of comparisons 1. For each possible -way subset of the\nitems, make comparisons of subset . 2. Identify any item which won the most\ncomparisons as the best item. Condorcet voting always makes ! \" comparisons, where ! \" denotes the binomial coefficient. This may be prohibitive for large or . It might be anticipated that Condorcet voting will result in a higher error rate than tournament selection and elimination selection for a given number of comparisons, since these other methods avoid making comparisons between items that have already been observed to perform poorly."}, {"heading": "RESULTS", "text": "We first present results on a Chinese idiom translation task that demonstrate that tournament selection and elimination selection can succeed where majority voting fails. We then present a statistical analysis of human computation on five tasks. This enables us to assess the costs and error rates of our methods in real-world settings, and to address the question of whether it is better to use 2-, 3- or 4-way comparisons.\nCHINESE IDIOM TRANSLATION\nMajority Voting We ran experiments to translate the five Chinese idioms listed in Figure 1. This is a challenging task since the literal meanings of idioms are different from their true interpretations. We performed these experiments on CrowdFlower which facilitates the\ncompletion of online micro-tasks among a number of labor channels including Amazon Mechanical Turk.\nSpecific instructions were given to capture true interpretations in English and not to produce literal translations. One unit of human computation task is to translate one idiom and each idiom was translated 30 times (i.e. by 30 different workers). We deliberately kept redundancy high to illustrate the large proportion of wrong answers in the crowd\u2019s response. Each translation task was paid for $0.03. No further automatic or manual validation was done for the results. Figure 2 illustrates the number of correct and wrong responses compared to the groundtruth. The error rate for each Chinese idiom translation is above 85% all five of the idioms. In Figure 3, we show the results of majority voting and the percentage of majority votes. Clearly, the majority votes never agree with the ground-truth translation results."}, {"heading": "Tournament Selection Experiments", "text": "We ran tournament selection (Method A pair-wise comparisons on the idiom translation. We started with outputs generated by the Crowd These outputs were as follows, with being the correct translation:\n1. Like a dog fails to draw a tiger\n2. Who are you?\n3. None\n4. Attempting something beyond one\u2019s ability and fail\n5. Painted tiger anti-dog\nAt the end of each round of tournament selection the proportion of each of the above entries was computed and the corresponding plot is shown in Figure 4. The correct translation was a minority to start off with but it gradually surpassed all other candidates and emerged as a clear winner within five rounds. Thus it appears that tournament selection is well suited to such translation tasks since easier to select the right translation from a list, than to produce the right translation.\n) with second Chinese\nthe distinct Flower workers.\nthe 4th entry\none Chinese\n,\n- workers find it"}, {"heading": "Elimination Selection Experiments", "text": "We also ran elimination s pair-wise comparisons translation. We use the same set of as in our experiments on tournament selection\nIn this experiment, once a translation reaches 16 losses, it is eliminated from the pool. The final answer is produced when the pool has only one option left. Figure 5 shows the number of losses for each translation versus the number of The correct translation, option 4, is chosen as the final answer after the 91st\nMODELLING #-WAY COMPARISON DATA In order to predict the costs and error rates of our methods in real-world settings, and to address the question of whether it is better to use 2 comparisons, we need to make assumptions. Therefore, we now fit several statistical models to a set of 1273 # tasks, consisting of one Chinese idiom translation and the following three puzzles\nPuzzle 1: If you had an infinite supply of water 5 quart and 3 quart pail, how would you measure exactly 4 quarts?\nPuzzle 2: You have a bucket of jelly beans. Some are red, some are blue, and some are green. With your eyes closed, pick out 2 of a like color. How many do you have to grab to be sure you have 2 of the same?\nPuzzle 3: A chicken and a half can lay an egg and a half in a day in a half. How long will it take for two chickens to lay 32 eggs?\nThe puzzle data was obtained as follows. conducted 30 experiments for each puzzle to generate a list of likely answers. A list of six compiled including the correct common answers produced by human computation workers in these experiments.\nTo model this data, we make use of the Bradley Terry model [16], which is as follows. comparison between a set of items is the set of all subsets of likelihood that item $ is selected is %& |(, $) *+\u2211 *-- . where parameters ( / &* describe the quality of the items largest *+ is the preferred item)\nelection (Method B) with on Chinese idiom\ndistinct outputs .\nT =\nmatches.\nmatch.\n-, 3- or 4-way some statistical\n-way comparisons on four\n:\nand a\nWe\noptions was answer and the 5 most\n- Given a n-way $ 0 where 0 11, 2, \u2026 , 2 of size , the\n, *3, \u2026 , * ) with *4 5 0, (the item with the\n.\nWe test whether there is a preference structure to the data, whether the data corresponds to random clicking by the workers and whether a single set of parameters can predict both - and 3 -way comparisons for 6 3 . These possibilities correspond to the following hypotheses, for each task and for each : 789: The outcomes of -way comparisons are given by a Bradley-Terry model; 7:;9: The outcomes of -way comparisons are given by a saturated model (in which the probability that item wins a comparison < is unrelated to the probability that item wins a comparison < for any < 6 < ); 7=> : The options selected by a worker are multinomially distributed, independent of which comparison is being made; and the following hypothesis for each task:\n7?@A>9 : The outcomes of 2-, 3- and 4-way comparisons are given by a single joint BradleyTerry model.\nResults for these hypothesis tests are given in Tables 1 and 2. In these tables, N is the number of comparisons in the data; the deviance is D K2&LM K LN) where LM and LN are the log-likelihoods under the complex and simple hypotheses respectively; and the p-value is obtained by the \u03c73approximation. In all but one case, the data has a\nclear preference structure (7:;9 vs 789 ). In all but two cases, random clicking can be discarded as a hypothesis (789 vs 7=>), and in only one case might a single set of parameters predict -way comparisons for different (7?@A>9 vs 789). It is also interesting to see how often the population of workers selected the right answer. Given that the Bradley-Terry hypothesis is plausible, Table 3 lists the maximum likelihood estimates of the parameters *+ for the known best item and for the competing item (i.e. that with the next largest merit if the known best option does not win or the option with the largest estimated merit if the known best option loses). The table shows that the workers tend to prefer the right answer in only 5 out of 12 experiments!"}, {"heading": "COMPARISON OF SELECTION METHODS", "text": "Our bound on the error rate of elimination selection (see Appendix) describes its performance for a rather general family of comparison probabilities. However, we would like to understand the error rate and mean number of games that might result when applying both tournament and elimination selection on data observed in real experiments. In this section, we take a Bayesian approach to those questions.\nIf the data had an exact Bradley-Terry distribution, in which the known best item had the highest winning probability, the algorithms would perform as shown in Figure 6. In this plot, the error rate converges to zero as the number of comparisons increases and there is a clear performance improvement as we move from 2- to 3- to 4-way comparisons. However, the real-world data might not have an exact BradleyTerry distribution and perhaps the actual distribution does not satisfy the assumptions under which elimination selection converges. Even if the data did have an exact Bradley-Terry distribution, we do not know the parameters of that distribution. Furthermore, the known best item might not be preferred by the workers.\nInstead, we shall aim is to estimate the posterior expected error rate and the mean number of comparisons made by each algorithm, given the - way comparison data analyzed in the previous section. We do so via a Monte Carlo Markov Chain method. Based on our findings in the previous section, we assume that the selection probabilities are either drawn from a Bradley-Terry distribution with a uniform prior on 0,1 for the parameters *+ , or are drawn from the saturated model with a uniform Dirichlet prior on the probabilities that item wins a comparison with items V, , W. In detail our method is as follows, we draw 1000 samples of the winning probabilities from the posterior using 5000 accepted steps of MetropolisHastings, where the first 1000 steps were treated as burn-in and the remaining steps were thinned by extracting only one in every 4 samples. To check that this was a sufficient number of iterations, we used the R-package CODA (Plummer et al. 2006). For each such sample we ran each selection algorithm and the error rate was estimated as the fraction of samples for which the selection algorithm did not return the item with the highest sampled merit.\nFigures 7 and 8 show the results of this MCMC method when applied to the Chinese idiom translation task assuming the saturated model and the Bradley-Terry model. For tournament selection we varied the fraction in the stopping criterion from 0.5 to 0.9 and for elimination selection we varied stopping parameter from 10 to 80. In Figure 9, the balanced experiment is equivalent to the Condorcet voting since every permutation of comparison is included.\nIn neither plot does the error rate converge to zero, as there is always a non-zero probability that the population does not prefer the known best option. Indeed, the saturated model applied to 4-way tournament selection produces a rather high error rate. This is because 4-way tournament selection sometimes makes 3- or 2-way comparisons (e.g. if items {1, 1, 2, 3} are sampled), and the 3-way data strongly supports a model in which the known best item is not preferred.\nAll plots in this section clearly illustrate the advantage of the elimination selection method over\nCondorcet voting and over the tournament selection method for 2-way comparisons. However, the gap between the methods decreases for both 3-way and 4- way comparison cases (with the exception of tournament selection in the saturated model). Furthermore, there is a clear benefit in moving from 2- to 3- to 4-way comparisons. Comparable plots are obtained in the case of the four puzzles in those cases where the data supports the hypothesis that the known best answer is preferred by the population."}, {"heading": "CONCLUSIONS", "text": "We proposed two methods for selecting the best answer in difficult human computation tasks: tournament selection and elimination selection. Both methods successfully produce the correct answer where majority voting fails by conducting proof-ofconcept experiments. We conducted a statistical analysis of multi-way comparisons obtained from five human computation tasks. While a BradleyTerry model fits the data on -way comparisons for each given , it is not possible to simultaneously describe 2-, 3- and 4-way comparisons with a single joint Bradley-Terry model. Using a Bayesian approach based on this analysis, we compared the cost and error rate of each method with Condorcet voting. This comparison showed that for the same cost, 4-way comparison selection schemes give a smaller error rate than pair-wise selection schemes."}, {"heading": "FUTURE WORK", "text": "Our work demonstrated applicability of tournament and elimination selection methods as quality assurance on human computation systems. Future work includes exploring a mixture model for Bradley-Terry to better capture workers\u2019 intent, a hybrid method of both selection and filtering, and different type of human computation tasks."}, {"heading": "Computation,\u201d Synthesis Lectures on Artificial Intelligence and Machine Learning", "text": "Bernstein, M. S., Little, G., Miller, R. C., Hartmann, B., Ackerman, M. S., Karger, D. R., Crowell, D., Panovich, K. (2010), \u201cSoylent: A Word Processor with a Crowd Inside,\u201d UIST '10, ACM Press\nLittle, G. and Sun, Y. (2011), \u201cHuman OCR: Insights from a Complex Human Computation Process,\u201d Workshop on Crowdsourcing and Human Computation, Services, Studies and Platforms, ACM CHI\nvon Ahn, L. and Dabbish L. (2004), \u201cLabeling images with a computer game,\u201d ACM CHI\nSun, Y., Roy, S., Little, G. (2011), \u201cBeyond Independent Agreement: A Tournament Selection Approach for Quality Assurance of Human Computation Tasks,\u201d HCOMP\nLaw, E. and von Ahn, L. (2009), \u201cInput-agreement: A new mechanism for data collection using human computation games,\u201d ACM CHI\nPlummer, M., Best, N., Cowles, K., Vines, K. (2006), \u201cCODA: Convergence Diagnosis and Output Analysis for MCMC,\u201d R News, 6:1, 7-11\nLe, J., Edmonds, A., Hester, V., Biewald, L. (2010), \u201cEnsuring quality in crowdsourced search relevance evaluation: The effects of training question distribution,\u201d CSE\nOleson, D., Sorokin, A., Laughlin, G., Hester, V., Le, J., Biewald, L. (2011), \u201cProgrammatic Gold: Targeted and Scalable Quality Assurance in Crowdsourcing,\u201d HCOMP\nBarrington, L., O\u2019Malley, D., Turnbull, D., Lanckriet, G. (2009), \u201cUser-Centered Design of a Social Game to Tag Music,\u201d HCOMP, 7-10\nVickrey, D., Bronzan, A., Choi, W., Kumar, A., Turner-Maier, J., Wang, A., Koller, D. (2008), \u201cOnline Word Games for Semantic Data Collection,\u201d EMNLP, 535-538\nMarden, J. (1995), \u201cAnalyzing and Modeling Rank"}, {"heading": "Data,\u201d Chapman & Hall", "text": "Adler, M., Gemmell, P., Harchol-Balter, M., Karp, R., Kenyon, C. (1994), \u201cSelection in the Presence of Noise: The Design of Playoff Systems\u201d, The Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, 564\u2013573\nBen-Akiva, M. and Lerman, S. R. (1985), \u201cDiscrete Choice Analysis,\u201d The MIT Press\nBradley, R. A. and Terry, M. E. (1952), \u201cThe Rank Analysis of Incomplete Block Designs, \u201c I. The method of paired comparisons. Biometrika 39, 324- 345\nLuce, R. D. (1959), \u201cIndividual Choice Behavior,\u201d Wiley\nGoldberg, D. and Deb, K. (1991), \u201cA Comparative Analysis of Selection Schemes Used in Genetic Algorithms,\u201d Foundations of Genetic Algorithms\nFermandez, A., Garcia, S., Luengo, J., BernadoMansilla, E., Herrera, F. (2010), \u201cGenetic-Based Machine Learning for Rule Induction: Taxonomy, Experimental Study and State of the Art,\u201d IEEE\nTransactions on Evolutionary Computation, 14:6, 913-941\nStern, H. (1993). \u201cProbability Models on Rankings and the Electoral process,\u201d Probability Models and Statistical Analyses for Ranking Data (Fligner, M.A. and Verducci, J.S, eds.) 173-195. Springer-Verlag, New York."}, {"heading": "APPENDIX: HOW OFTEN DOES # -WAY ELIMINATION SELECTION MAKE ERRORS?", "text": "We analyze the performance of the -way elimination selection method. Such an analysis was sketched in (Adler et al. 1994). We complete that sketch and extend it to the case of -way comparisons. Thus, this appendix justifies the use of -way elimination selection and gives insight into the selection of parameter . Firstly, we must ensure that the probability that an item wins a match corresponds to having some best item, which will be identified as item 1. To do so we make a discriminating assumption, which is a generalization of the assumption made in [? Adler]. Say item V 6 1 and item 1 are both matched with some set $ of items to be compared. For instance, if 3 and $ 12,32 then we consider two matches involving items 1, 2, 3 and items V, 2, 3 , whereas if $ 11,2, V2 then we consider only one match involving items 1, 2 and V. Let X 1 (or 0) if item 1 loses (wins) its match and let Y 1 (or 0) if item V loses (wins) its match. We assume that for some Z [ 0, for all items V 6 1, for all sets of items $ (as above),\n&X 0, Y 1 | X Y [ 0, $) 5 &X 1, Y 0 | X Y [ 0, $) Z.\nThat is, the probability that only item V loses is at least Z larger than the probability that only item 1 loses, given that at least one item loses. Provided that there is some best item, it is straightforward to verify that the discriminating assumption is satisfied for many widely-used multi-way comparison models, such as the Plackett-Luce model (Ben-Akiva & Lerman 1985). We are now ready to state our main result, which relates the probability \\ that -way elimination selection does not select item 1, to the choice of parameter and to the difficulty of the selection problem, as represented by Z. Proposition A1. Suppose that -way elimination selection is run for ] items with parameter ] and that the loss probabilities satisfy the discriminating assumption with parameter Z . Then the failure probability \\ is bounded by\n\\ ^ exp&KZ3 /4).\nProof. We consider any item V 6 1 and show that the probability that item 1 is eliminated before item V is at most exp&K Z3/4) , using Azuma\u2019s inequality. The proposition then follows directly from the union bound. First, let bc count the losses of item 1 and dc count the losses of item 2 at the subsequence of rounds e where either 1 or V or both lose a match. The probability that 1 is eliminated before V is\n&bf 5 df) where g / inf1e | kl1bc , dc2 2.\nIt turns out to be easier to consider a different stopping time m. In particular, we let m be the first time that bc dc 5 2 , where we imagine that items 1 and V continue playing matches rather than being eliminated at time g . Since bc and dc are nondecreasing and m 5 g for any realisation, it follows that bf 5 df implies that bn 5 dn. Hence\n&bf 5 df) ^ &bn 5 dn). Now we will define a martingale difference sequence oc to which we can apply Azuma\u2019s inequality. Let pc denote the history of games up to and including time e and let Xc / bc K bc K &dc K dc ). Define the process qc / max1K<c , Xc2 where\n<c / & &Xc 1 | pc ) Z)/ &Xc K1 | pc ). By the discriminative assumption <c ^ 1, hence\nt qu c\nuU 5 bc K dc\nfor any realization. Furthermore v qc|pc KZ , thus the martingale difference sequence oc / eZ \u2211 qucuU satisfies &bn 5 dn) ^ &on 5 mZ) ^ &on 5 Z) where the second inequality follows since m 5 for all realizations. Now consider the stopped martingale difference sequence owxy 1c,n2. Since m ^ 2 for any realization we have &on 5 Z) !owxy 139,n2 5 Z\". We now apply Azuma\u2019s inequality, noting that martingale owxy 1c,n2 has differences of range at most two, since sup1oc| K oc2 K inf1oc| K oc2 ^ 2 , giving\n&on 5 Z) ^ exp K &9}) ~\n3 39 exp&K Z3/4). Thus item 1 is eliminated before item V with probability at most exp&K Z3/4). The proposition now follows directly from the union bound applied for all possible items V 6 1."}], "references": [{"title": "Human Computation,", "author": ["E. Law", "L. von Ahn"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning", "citeRegEx": "Law and Ahn,? \\Q2011\\E", "shortCiteRegEx": "Law and Ahn", "year": 2011}, {"title": "Soylent: A Word Processor with a Crowd Inside,", "author": ["M.S. Bernstein", "G. Little", "R.C. Miller", "B. Hartmann", "M.S. Ackerman", "D.R. Karger", "D. Crowell", "K. Panovich"], "venue": null, "citeRegEx": "Bernstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2010}, {"title": "Human OCR: Insights from a Complex Human Computation Process,", "author": ["G. Little", "Y. Sun"], "venue": "Workshop on Crowdsourcing and Human Computation,", "citeRegEx": "Little and Sun,? \\Q2011\\E", "shortCiteRegEx": "Little and Sun", "year": 2011}, {"title": "Labeling images with a computer game,", "author": ["L. von Ahn", "Dabbish L"], "venue": null, "citeRegEx": "Ahn and L.,? \\Q2004\\E", "shortCiteRegEx": "Ahn and L.", "year": 2004}, {"title": "Beyond Independent Agreement: A Tournament Selection Approach for Quality Assurance of Human Computation Tasks,", "author": ["Y. Sun", "S. Roy", "G. Little"], "venue": null, "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Input-agreement: A new mechanism for data collection using human computation games,", "author": ["E. Law", "L. von Ahn"], "venue": null, "citeRegEx": "Law and Ahn,? \\Q2009\\E", "shortCiteRegEx": "Law and Ahn", "year": 2009}, {"title": "CODA: Convergence Diagnosis and Output Analysis for MCMC,", "author": ["M. Plummer", "N. Best", "K. Cowles", "K. Vines"], "venue": "R News,", "citeRegEx": "Plummer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2006}, {"title": "Ensuring quality in crowdsourced search relevance evaluation: The effects of training question distribution,", "author": ["J. Le", "A. Edmonds", "V. Hester", "L. Biewald"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Le et al\\.", "year": 2010}, {"title": "Programmatic Gold: Targeted and Scalable Quality Assurance in Crowdsourcing,", "author": ["D. Oleson", "A. Sorokin", "G. Laughlin", "V. Hester", "J. Le", "L. Biewald"], "venue": null, "citeRegEx": "Oleson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Oleson et al\\.", "year": 2011}, {"title": "User-Centered Design of a Social Game to Tag Music,", "author": ["L. Barrington", "D. O\u2019Malley", "D. Turnbull", "G. Lanckriet"], "venue": null, "citeRegEx": "Barrington et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Barrington et al\\.", "year": 2009}, {"title": "Online Word Games for Semantic Data Collection,", "author": ["D. Vickrey", "A. Bronzan", "W. Choi", "A. Kumar", "J. Turner-Maier", "A. Wang", "D. Koller"], "venue": null, "citeRegEx": "Vickrey et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vickrey et al\\.", "year": 2008}, {"title": "Analyzing and Modeling Rank Data,", "author": ["J. Marden"], "venue": null, "citeRegEx": "Marden,? \\Q1995\\E", "shortCiteRegEx": "Marden", "year": 1995}, {"title": "Selection in the Presence of Noise: The Design of Playoff Systems", "author": ["M. Adler", "P. Gemmell", "M. Harchol-Balter", "R. Karp", "C. Kenyon"], "venue": "The Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Adler et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Adler et al\\.", "year": 1994}, {"title": "Discrete Choice Analysis,", "author": ["M. Ben-Akiva", "S.R. Lerman"], "venue": null, "citeRegEx": "Ben.Akiva and Lerman,? \\Q1985\\E", "shortCiteRegEx": "Ben.Akiva and Lerman", "year": 1985}, {"title": "The Rank Analysis of Incomplete Block Designs, \u201c I", "author": ["R.A. Bradley", "M.E. Terry"], "venue": "The method of paired comparisons. Biometrika", "citeRegEx": "Bradley and Terry,? \\Q1952\\E", "shortCiteRegEx": "Bradley and Terry", "year": 1952}, {"title": "Individual Choice Behavior,", "author": ["R.D. Luce"], "venue": null, "citeRegEx": "Luce,? \\Q1959\\E", "shortCiteRegEx": "Luce", "year": 1959}, {"title": "A Comparative Analysis of Selection Schemes Used in Genetic Algorithms,", "author": ["D. Goldberg", "K. Deb"], "venue": "Foundations of Genetic Algorithms", "citeRegEx": "Goldberg and Deb,? \\Q1991\\E", "shortCiteRegEx": "Goldberg and Deb", "year": 1991}, {"title": "Genetic-Based Machine Learning for Rule Induction: Taxonomy, Experimental Study and State of the Art,", "author": ["A. Fermandez", "S. Garcia", "J. Luengo", "E. BernadoMansilla", "F. Herrera"], "venue": null, "citeRegEx": "Fermandez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fermandez et al\\.", "year": 2010}, {"title": "Probability Models on Rankings", "author": ["H. Stern"], "venue": null, "citeRegEx": "Stern,? \\Q1993\\E", "shortCiteRegEx": "Stern", "year": 1993}], "referenceMentions": [{"referenceID": 1, "context": "Redundancy with majority voting (Bernstein et al. 2010) or independent output agreement (von Ahn & Dabbish 2004) is commonly adopted to address this issue.", "startOffset": 32, "endOffset": 55}, {"referenceID": 18, "context": "We simulate these methods to benchmark their time complexity, error rates and the costs associated with their deployment, in terms of number of comparisons required, and compare them with selection based on Condorcet voting (Stern 1993).", "startOffset": 224, "endOffset": 236}, {"referenceID": 1, "context": "Redundancy with majority voting (Bernstein et al. 2010) or independent output agreement (von Ahn & Dabbish 2004) is commonly adopted to address this issue. However, Sun et al (2011) and Law & von Ahn (2009) both identified that high quality results reside in a minority of the responses and are often not identified by majority voting.", "startOffset": 33, "endOffset": 182}, {"referenceID": 1, "context": "Redundancy with majority voting (Bernstein et al. 2010) or independent output agreement (von Ahn & Dabbish 2004) is commonly adopted to address this issue. However, Sun et al (2011) and Law & von Ahn (2009) both identified that high quality results reside in a minority of the responses and are often not identified by majority voting.", "startOffset": 33, "endOffset": 207}, {"referenceID": 9, "context": "Games with a purpose that adopt output agreement include the ESP game (von Ahn & Dabbish 2004), HerdIt (Barrington et al. 2009), and Categorilla (Vickrey et al.", "startOffset": 103, "endOffset": 127}, {"referenceID": 10, "context": "2009), and Categorilla (Vickrey et al. 2008).", "startOffset": 23, "endOffset": 44}, {"referenceID": 11, "context": "Statistical Modeling for Noisy Pair Comparison Pairwise and multi-way comparisons are commonly used for psychology experiments and image quality assessment in conjunction with statistical models such as the multinomial logit (Ben-Akiva & Lerman 1985), Bradley-Terry (Bradley & Terry 1952) and Plackett-Luce (Marden 1995, Ben-Akiva & Lerman 1985) models. Such work typically focuses on estimating the qualities of different items when the pairs to be compared are specified externally. In contrast, we wish to find an efficient algorithm for choosing which pairs to compare in order to select the best answer, as in Adler et al (1994). In other words, assuming there is a ground truth best answer, we are trying to determine which observations to make, whereas such statistical models are typically used to analyze the data after the observations have been made.", "startOffset": 308, "endOffset": 634}, {"referenceID": 6, "context": "To check that this was a sufficient number of iterations, we used the R-package CODA (Plummer et al. 2006).", "startOffset": 85, "endOffset": 106}], "year": 2012, "abstractText": "Quality assurance remains a key topic in human computation research. Prior work indicates that majority voting is effective for low difficulty tasks, but has limitations for harder tasks. This paper explores two methods of addressing this problem: tournament selection and elimination selection, which exploit 2-, 3and 4-way comparisons between different answers to human computation tasks. Our experimental results and statistical analyses show that both methods produce the correct answer in noisy human computation environment more often than majority voting. Furthermore, we find that the use of 4-way comparisons can significantly reduce the cost of quality assurance relative to the use of 2-way comparisons.", "creator": "PDFCreator Version 0.9.8"}}}