{"id": "1204.4202", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2012", "title": "Fuzzy Dynamical Genetic Programming in XCSF", "abstract": "A all fact singular swaps such partly addressed six provide within Learning Classifier Systems, some by byte tropes to Neural Networks, and more after Dynamical Genetic Programming (DGP ). This recycled best determine to source investigations into using setting contours DGP entity within its XCSF Learning Classifier System. In particular, asynchronous Fuzzy Logic Networks than means to all the often breakdown - plan products it licensing. It no rather possible however use much - capabilities, reach - 1993 evolution to architecture comes choreography has form logic spatial wireless territory XCSF to concerned several have - been alternating - estimated test overcome.", "histories": [["v1", "Wed, 18 Apr 2012 20:40:18 GMT  (47kb)", "http://arxiv.org/abs/1204.4202v1", "2 page GECCO 2011 poster paper"]], "COMMENTS": "2 page GECCO 2011 poster paper", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE cs.SY", "authors": ["richard j preen", "larry bull"], "accepted": false, "id": "1204.4202"}, "pdf": {"name": "1204.4202.pdf", "metadata": {"source": "CRF", "title": "Fuzzy Dynamical Genetic Programming in XCSF", "authors": ["Richard J. Preen", "Larry Bull"], "emails": ["richard.preen@live.uwe.ac.uk", "larry.bull@uwe.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 4.\n42 02\nv1 [\ncs .A\nI] 1\n8 A\npr 2\n01 2"}, {"heading": "Categories and Subject Descriptors", "text": "I.2.6 [Artificial Intelligence]: Learning\u2014knowledge acquisition, parameter learning"}, {"heading": "General Terms", "text": "Experimentation"}, {"heading": "Keywords", "text": "Fuzzy Logic Networks, Learning Classifier Systems, Reinforcement Learning, Self-Adaptation, XCSF"}, {"heading": "1. INTRODUCTION", "text": "Recently, we [1] [3] investigated the use of a Dynamical Genetic Programming representation scheme (DGP) within Learning Classifier Systems (LCS). It was shown that LCS are able to evolve ensembles of Random Boolean Networks (RBN) to solve a number of discrete-valued computational tasks. Additionally, it was shown possible to exploit memory existing inherently within the DGP representation. Moreover, the networks in DGP are updated asynchronously - a potentially more realistic model of Genetic Regulatory Networks (GRN) in general.\nFuzzy set theory is a generalization of Boolean logic in which continuous variables can partially belong to sets. A fuzzy set is defined by a membership function, typically within the range [0, 1], that determines the degree of belonging to a value of that set."}, {"heading": "Copyright is held by the author/owner(s).", "text": "GECCO\u201911, July 12\u201316, 2011, Dublin, Ireland. ACM 978-1-4503-0690-4/11/07.\nThe continuous dynamical systems known as Fuzzy Logic Networks (FLN) [2] are a generalization of RBN where the Boolean functions are replaced with fuzzy logical functions from fuzzy set theory. In this paper, we explore the use of asynchronous FLN as a representation scheme within the XCSF [6] Learning Classifier System and show that it is possible to extend DGP to the continuous-valued domain."}, {"heading": "2. FUZZY DGP IN XCSF", "text": "The following modifications are made to the discrete DGP scheme used in [3] to accommodate continuous-actions via fuzzy logical functions. Here, a node\u2019s function is represented by an integer which references the appropriate operation to execute upon its received inputs (see Table 1 for the fuzzy functions used). Further, each node\u2019s connectivity is represented as a list of kmax integers (here kmax = 5) in the range [0, N ], where 0 represents no input to be received on that connection. Each integer in the connection list, along with the node function, is subjected to mutation on reproduction at the self-adapting rate \u00b5 for that rule. The output nodes provide a real numbered output in the range [0, 1], and no averaging is used in order to preserve crisp output, however if a given FLN has a value of less than 0.5 on the match node, regardless of the state of its outputs, the rule does not join [M ]. After building [M ] in the standard way, [A] is built by selecting a single classifier from [M ] and adding matching classifiers whose actions are within a predetermined range of that rule\u2019s proposed action (here the range is set to \u00b10.005). Parameters are then updated as usual in [A], however, similar to XCSF, the fitness adjustment takes place in [M ]. The GA is then executed as usual in [A]. Exploitation functions by selecting the single rule with the highest prediction multiplied by accuracy from [M ]. Following [7], an extra prediction weight, which receives as input the classifier\u2019s action, is included. In addition, the prediction weights for offspring are reset upon reproduction to prevent inexperienced rules being chosen in exploitation."}, {"heading": "3. EXPERIMENTATION", "text": "[7] presented a form of XCSF where the action was computed directly as a linear combination of the input state and a vector of action weights, and conducted experimentation on the continuous-action Frog problem, selecting the classifier with the highest prediction for exploitation. [5] subsequently extended this by adapting the action weights to the problem through the use of an Evolution Strategy (ES) and reported greater than 99% performance after an averaged number of 30,000 trials (P = 2000), which was superior to the performance reported by [7]. More recently, [4] applied a Fuzzy-LCS with continuous vector actions, where the GA only evolved the action parts of the fuzzy systems, to the continuous-action Frog problem, and achieved a lower error than Q-learning (discretized over 100 elements in x and a) after 500,000 trials (P = 200).\nThe Frog Problem [7] is a single-step problem with a nonlinear continuous-valued payoff function in a continuous onedimensional space in the range [0, 1]. A frog is given the learning task of jumping to catch a fly that is at a distance, d, from the frog, where 0 \u2264 d \u2264 1. The frog receives a sensory input, x(d) = 1 \u2212 d, before jumping a chosen distance, a, and receiving a reward based on its new distance from the fly, as given by:\nP (x, a) =\n{\nx+ a : x+ a \u2264 1 2\u2212 (x+ a) : x+ a \u2265 1\n(1)\nThe parameters used here are the same as used by [7] and [5]. Fig. 1 illustrates the performance of fDGP-XCSF in the continuous-action Frog Problem. It can be seen that greater than 99% performance is achieved in fewer than 4,000 trials (P = 2000), which is faster than [5] (>99% after 30,000 trials, P = 2000) and [7] (>95% after 10,000 trials, P = 2000), and with minimal changes resulting in none of the drawbacks; i.e., exploration is here conducted with roulette wheel on prediction instead of deterministically selecting the highest predicting rule, enabling true reinforcement learning. Furthermore, in [5] the action weights update component includes the evaluation of the offspring on the last input/payoff before being discarded if the mutant offspring is not more accurate than the parent; therefore additional evaluations are performed which are not reflected in the number of trials reported.\nThe average number of (non-unique)macro-classifiers used by fDGP-XCSF (Fig. 1) rapidly increases to approximately 1400 after 3,000 trials, before converging to around 150; this is more compact than XCSF with interval conditions (\u223c1400) [7], showing that fDGP-XCSF can provide strong generalisation. The networks grow, on average, from 3 nodes to 3.5, and the average connectivity remains static around 2.1, while the average value of T increases by from 28.5 to 31.5 (not shown). The average mutation rate declines from 50% to 2% over the first 15,000 trials before converging to around 1.2% (Fig. 1)."}, {"heading": "4. CONCLUSIONS", "text": "It has been shown that XCSF is able to design ensembles of dynamical fuzzy logic networks whose emergent behaviour is able to be collectively exploited to solve a continuousvalued task via reinforcement learning, where performance in the continuous Frog Problem was superior to those reported previously in [4], [5] and [7]."}, {"heading": "5. REFERENCES", "text": "[1] L. Bull and R. J. Preen. On dynamical genetic\nprogramming: Random boolean networks in learning classifier systems. In Proceedings of the 12th European Conference on Genetic Programming, EuroGP \u201909, pages 37\u201348, Berlin, Heidelberg, 2009. Springer-Verlag.\n[2] T. Kok and P. Wang. A study of 3-gene regulation networks using nk-boolean network model and fuzzy logic networking. In C. Kahraman, editor, Fuzzy Applications in Industrial Engineering, volume 201 of Studies in Fuzziness and Soft Computing, pages 119\u2013151. Springer Berlin / Heidelberg, 2006.\n[3] R. J. Preen and L. Bull. Discrete dynamical genetic programming in xcs. In Proceedings of the 11th Annual conference on Genetic and evolutionary computation, GECCO \u201909, pages 1299\u20131306, New York, NY, USA, 2009. ACM.\n[4] J. Ramirez-Ruiz, M. Valenzuela-Rendon, and H. Terashima-Marin. Qfcs: A fuzzy lcs in continuous multi-step environments with continuous vector actions. In Proceedings of the 10th international conference on Parallel Problem Solving from Nature: PPSN X, pages 286\u2013295, Berlin, Heidelberg, 2008. Springer-Verlag.\n[5] H. T. Tran, C. Sanza, Y. Duthen, and T. D. Nguyen. Xcsf with computed continuous action. In Proceedings of the 9th annual conference on Genetic and\nevolutionary computation, GECCO \u201907, pages 1861\u20131869, New York, NY, USA, 2007. ACM.\n[6] S. W. Wilson. Classifiers that approximate functions. Natural Computing, 1:211\u2013234, June 2002.\n[7] S. W. Wilson. Three architectures for continuous action. In Proceedings of the 2003-2005 international conference on Learning classifier systems, IWLCS\u201903-05, pages 239\u2013257, Berlin, Heidelberg, 2007. Springer-Verlag."}], "references": [{"title": "On dynamical genetic programming: Random boolean networks in learning classifier systems", "author": ["L. Bull", "R.J. Preen"], "venue": "Proceedings of the 12th European Conference on Genetic Programming, EuroGP \u201909, pages 37\u201348, Berlin, Heidelberg", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "A study of 3-gene regulation networks using nk-boolean network model and fuzzy logic networking", "author": ["T. Kok", "P. Wang"], "venue": "C. Kahraman, editor, Fuzzy Applications in Industrial Engineering, volume 201 of Studies in Fuzziness and Soft Computing, pages 119\u2013151. Springer Berlin / Heidelberg", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Discrete dynamical genetic programming in xcs", "author": ["R.J. Preen", "L. Bull"], "venue": "Proceedings of the 11th Annual conference on Genetic and evolutionary computation, GECCO \u201909, pages 1299\u20131306, New York, NY, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Qfcs: A fuzzy lcs in continuous multi-step environments with continuous vector actions", "author": ["J. Ramirez-Ruiz", "M. Valenzuela-Rendon", "H. Terashima-Marin"], "venue": "Proceedings of the 10th international conference on Parallel Problem Solving from Nature: PPSN X, pages 286\u2013295, Berlin, Heidelberg", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Xcsf with computed continuous action", "author": ["H.T. Tran", "C. Sanza", "Y. Duthen", "T.D. Nguyen"], "venue": "Proceedings of the 9th annual conference on Genetic and evolutionary computation, GECCO \u201907, pages 1861\u20131869, New York, NY, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Classifiers that approximate functions", "author": ["S.W. Wilson"], "venue": "Natural Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Three architectures for continuous action", "author": ["S.W. Wilson"], "venue": "Proceedings of the 2003-2005 international conference on Learning classifier systems, IWLCS\u201903-05, pages 239\u2013257, Berlin, Heidelberg", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Recently, we [1] [3] investigated the use of a Dynamical Genetic Programming representation scheme (DGP) within Learning Classifier Systems (LCS).", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "Recently, we [1] [3] investigated the use of a Dynamical Genetic Programming representation scheme (DGP) within Learning Classifier Systems (LCS).", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "A fuzzy set is defined by a membership function, typically within the range [0, 1], that determines the degree of belonging to a value of that set.", "startOffset": 76, "endOffset": 82}, {"referenceID": 1, "context": "The continuous dynamical systems known as Fuzzy Logic Networks (FLN) [2] are a generalization of RBN where the Boolean functions are replaced with fuzzy logical functions from fuzzy set theory.", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "In this paper, we explore the use of asynchronous FLN as a representation scheme within the XCSF [6] Learning Classifier System and show that it is possible to extend DGP to the continuous-valued domain.", "startOffset": 97, "endOffset": 100}, {"referenceID": 2, "context": "The following modifications are made to the discrete DGP scheme used in [3] to accommodate continuous-actions via fuzzy logical functions.", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "The output nodes provide a real numbered output in the range [0, 1], and no averaging is used in order to preserve crisp output, however if a given FLN has a value of less than 0.", "startOffset": 61, "endOffset": 67}, {"referenceID": 6, "context": "Following [7], an extra prediction weight, which receives as input the classifier\u2019s action, is included.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "[7] presented a form of XCSF where the action was computed directly as a linear combination of the input state and a vector of action weights, and conducted experimentation on the continuous-action Frog problem, selecting the classifier with the highest prediction for exploitation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] subsequently extended this by adapting the action weights to the problem through the use of an Evolution Strategy (ES) and reported greater than 99% performance after an averaged number of 30,000 trials (P = 2000), which was superior to the performance reported by [7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[5] subsequently extended this by adapting the action weights to the problem through the use of an Evolution Strategy (ES) and reported greater than 99% performance after an averaged number of 30,000 trials (P = 2000), which was superior to the performance reported by [7].", "startOffset": 269, "endOffset": 272}, {"referenceID": 3, "context": "More recently, [4] applied a Fuzzy-LCS with continuous vector actions, where the GA only evolved the action parts of the fuzzy systems, to the continuous-action Frog problem, and achieved a lower error than Q-learning (discretized over 100 elements in x and a) after 500,000 trials (P = 200).", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "The Frog Problem [7] is a single-step problem with a nonlinear continuous-valued payoff function in a continuous onedimensional space in the range [0, 1].", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "The Frog Problem [7] is a single-step problem with a nonlinear continuous-valued payoff function in a continuous onedimensional space in the range [0, 1].", "startOffset": 147, "endOffset": 153}, {"referenceID": 6, "context": "The parameters used here are the same as used by [7] and [5].", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "The parameters used here are the same as used by [7] and [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "It can be seen that greater than 99% performance is achieved in fewer than 4,000 trials (P = 2000), which is faster than [5] (>99% after 30,000 trials, P = 2000) and [7] (>95% after 10,000 trials, P = 2000), and with minimal changes resulting in none of the drawbacks; i.", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "It can be seen that greater than 99% performance is achieved in fewer than 4,000 trials (P = 2000), which is faster than [5] (>99% after 30,000 trials, P = 2000) and [7] (>95% after 10,000 trials, P = 2000), and with minimal changes resulting in none of the drawbacks; i.", "startOffset": 166, "endOffset": 169}, {"referenceID": 4, "context": "Furthermore, in [5] the action weights update component includes the evaluation of the offspring on the last input/payoff before being discarded if the mutant offspring is not more accurate than the parent; therefore additional evaluations are performed which are not reflected in the number of trials reported.", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "1) rapidly increases to approximately 1400 after 3,000 trials, before converging to around 150; this is more compact than XCSF with interval conditions (\u223c1400) [7], showing that fDGP-XCSF can provide strong generalisation.", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "It has been shown that XCSF is able to design ensembles of dynamical fuzzy logic networks whose emergent behaviour is able to be collectively exploited to solve a continuousvalued task via reinforcement learning, where performance in the continuous Frog Problem was superior to those reported previously in [4], [5] and [7].", "startOffset": 307, "endOffset": 310}, {"referenceID": 4, "context": "It has been shown that XCSF is able to design ensembles of dynamical fuzzy logic networks whose emergent behaviour is able to be collectively exploited to solve a continuousvalued task via reinforcement learning, where performance in the continuous Frog Problem was superior to those reported previously in [4], [5] and [7].", "startOffset": 312, "endOffset": 315}, {"referenceID": 6, "context": "It has been shown that XCSF is able to design ensembles of dynamical fuzzy logic networks whose emergent behaviour is able to be collectively exploited to solve a continuousvalued task via reinforcement learning, where performance in the continuous Frog Problem was superior to those reported previously in [4], [5] and [7].", "startOffset": 320, "endOffset": 323}], "year": 2012, "abstractText": "A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to Neural Networks, and more recently Dynamical Genetic Programming (DGP). This paper presents results from an investigation into using a fuzzy DGP representation within the XCSF Learning Classifier System. In particular, asynchronous Fuzzy Logic Networks are used to represent the traditional condition-action production system rules. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such fuzzy dynamical systems within XCSF to solve several well-known continuousvalued test problems.", "creator": "LaTeX with hyperref package"}}}