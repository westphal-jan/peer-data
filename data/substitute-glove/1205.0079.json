{"id": "1205.0079", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2012", "title": "Complexity Analysis of the Lasso Regularization Path", "abstract": "The semigroups point of the Lasso if take shown to be cross-correlation parameter, way it possible although \" allow \" and adopting formula_2 the entire heading. We processes 2004 this paper given show strategy, those circumstances once creation verge serious variables whole infinite was put number bringing approximated. We having veto if cautioned result to an (favorable) densities analysis: We show that long approximate path with high most O (1 / sqrt (ccp) ) linear includes can they be obtained, others we only while the obstacle is promise to most depends rest to instead characteristics epsilon - duality broad. We final ensure qualitative sampling with rather complicated algorithm leave parameters make arithmetic ovals.", "histories": [["v1", "Tue, 1 May 2012 03:37:13 GMT  (56kb)", "https://arxiv.org/abs/1205.0079v1", "Accepted to the 29th International Conference on Machine Learning (ICML 2012). To appear in the proceedings"], ["v2", "Sat, 19 May 2012 21:06:21 GMT  (56kb)", "http://arxiv.org/abs/1205.0079v2", "To appear in the proceedings of 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Accepted to the 29th International Conference on Machine Learning (ICML 2012). To appear in the proceedings", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["julien mairal", "bin yu 0001"], "accepted": true, "id": "1205.0079"}, "pdf": {"name": "1205.0079.pdf", "metadata": {"source": "META", "title": "Complexity Analysis of the Lasso Regularization Path", "authors": [], "emails": ["julien@stat.berkeley.edu", "binyu@stat.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 5.\n00 79\nv2 [\nst at\n.M L\n] 1\n9 M\nay 2\n01 2\n\u221a \u03b5) linear segments can always\nbe obtained, where every point on the path is guaranteed to be optimal up to a relative \u03b5-duality gap. We complete our theoretical analysis with a practical algorithm to compute these approximate paths."}, {"heading": "1. Introduction", "text": "Without a priori knowledge about data, it is often difficult to estimate a model or make predictions, either because the number of observations is too small, or the problem dimension too high. When a problem solution is known to be sparse, sparsity-inducing penalties have proven to be useful to improve both the quality of the prediction and its intepretability. In particular, the \u21131-norm has been used for that purpose in the Lasso formulation (Tibshirani, 1996).\nControlling the regularization often requires to tune a parameter. In a few cases, the regularization path\u2014 that is, the set of solutions for all values of the regularization parameter, can be shown to be piecewise linear (Rosset & Zhu, 2007). This property is exploited in homotopy methods, which consist of following the piecewise linear path by computing the direction of the current linear segment and the points where the direction changes (also known as kinks). Piecewise linearity of regularization paths was discovered by Markowitz (1952) for portfolio selection; it was similarly exploited\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nby Osborne et al. (2000) and Efron et al. (2004) for the Lasso, and by Hastie et al. (2004) for the support vector machine (SVM). As observed by Ga\u0308rtner et al. (2010), all of these examples are in fact particular instances of parametric quadratic programming formulations, for which path-following algorithms appear early in the optimization literature (Ritter, 1962).\nIn this paper, we study the number of linear segments of the Lasso regularization path. Even though experience with data suggests that this number is linear in the problem size (Rosset & Zhu, 2007), it is known that discrepancies can be observed between worst-case and empirical complexities. This is notably the case for the simplex algorithm (Dantzig, 1951), which performs empirically well for solving linear programs even though it suffers from exponential worst-case complexity (Klee & Minty, 1972). Similarly, by using geometrical tools originally developed to analyze the simplex algorithm, Ga\u0308rtner et al. (2010) have shown that the complexity of the SVM regularization path can be exponential. However, to the best of our knowledge, none of these results do apply to the Lasso regularization path, whose theoretical complexity remains unknown. The goal of our paper is to fill in this gap.\nOur first contribution is to show that in the worst-case the number of linear segments of the Lasso regularization path is exactly (3p+1)/2, where p is the number of variables (predictors). We remark that our proof is constructive and significantly different than the ones proposed by Klee & Minty (1972) for the simplex algorithm and by Ga\u0308rtner et al. (2010) for SVMs. Our approach does not rely on geometry but on an adversarial scheme. Given a Lasso problem with p variables, we show how to build a new problem with p+1 variables increasing the complexity of the path by a multiplicative factor. It results in explicit pathological examples that are surprisingly simple, unlike pathological examples for the simplex algorithm or SVMs.\nWorst-case complexity analyses are by nature pessimistic. Our second contribution on approximate regularization paths is more optimistic. In fact, we show\nthat an approximate path for the Lasso with at most O(1/ \u221a \u03b5) segments can always be obtained, where every point on the path is guaranteed to be optimal up to a relative \u03b5-duality gap. We follow in part the methodology of Giesen et al. (2010) and Jaggi (2011), who have presented weaker results but in a more general setting for parameterized convex optimization problems. Our analysis builds upon approximate optimality conditions, which we maintain along the path, leading to a practical approximate homotopy algorithm.\nThe paper is organized as follows: Section 2 presents some brief overview of the Lasso. Section 3 is devoted to our worst-case complexity analysis, and Section 4 to our results on approximate regularization paths."}, {"heading": "2. Background on the Lasso", "text": "In this section, we present the Lasso formulation of Tibshirani (1996) and well known facts, which we exploit later in our analysis. For self-containedness and clarity reasons we include simple proofs of these results. Let y be a vector in Rn and X = [x1, . . . ,xp] be a matrix in Rn\u00d7p. The Lasso is formulated as:\nmin w\u2208Rp\n1 2 \u2016y \u2212Xw\u201622 + \u03bb\u2016w\u20161, (1)\nwhere the \u21131-norm induces sparsity in the solution w and \u03bb>0 controls the amount of regularization. Under a few assumptions, which are detailed in the sequel, the solution of this problem is unique. We denote it by w\u22c6(\u03bb) and define the regularization path P as the set of all solutions for all positive values of \u03bb:1\nP , {w\u22c6(\u03bb) : \u03bb > 0}. The following lemma presents classical optimality and uniqueness conditions for the Lasso solution (see Fuchs, 2005), which are useful to characterize P : Lemma 1 (Optimality Conditions of the Lasso). A vector w\u22c6 in Rp is a solution of Eq. (1) if and only if for all j in {1, . . . , p},\nxj\u22a4(y \u2212Xw\u22c6) = \u03bb sign(w\u22c6j ) if w\u22c6j 6= 0, |xj\u22a4(y \u2212Xw\u22c6)| \u2264 \u03bb otherwise. (2)\nDefine J , {j \u2208 {1, . . . , p} : |xj\u22a4(y \u2212 Xw\u22c6)| = \u03bb}. Assuming the matrix XJ = [x\nj ]j\u2208J to be full rank, the solution is unique and we have\nw\u22c6J = (X \u22a4 J XJ) \u22121(X\u22a4J y \u2212 \u03bb\u03b7J), (3) where \u03b7 , sign(X\u22a4(y\u2212Xw\u22c6)) is in {\u22121; 0;+1}p, and the notation uJ for a vector u denotes the vector of size |J | recording the entries of u indexed by J .\n1For technicality reasons, we enforce \u03bb>0 even though the limit w\u22c6(0+) , lim\u03bb\u21920+ w \u22c6(\u03bb) may exist.\nProof. Eq. (2) can be obtained by considering subgradient optimality conditions. These can be written as 0 \u2208 {\u2212X\u22a4(y \u2212 Xw\u22c6) + \u03bbp : p \u2208 \u2202\u2016w\u22c6\u20161}, where \u2202\u2016w\u22c6\u20161 denotes the subdifferential of the \u21131norm at w\u22c6. A classical result (Borwein & Lewis, 2006) says that the subgradients p are the vectors in Rp such that for all j in {1, . . . , p}, pj = sign(w\u22c6j ) if w\u22c6j 6= 0, and |pj | \u2264 1 otherwise. This gives Eq. (2). The equalities in Eq. (2) define a linear system that has a unique solution given by (3) when XJ is full rank.\nLet us now show the uniqueness of the Lasso solution. Consider another solution w\u2032\u22c6 and choose a scalar \u03b8 in (0, 1). By convexity, w\u03b8\u22c6 , \u03b8w\u22c6 + (1 \u2212 \u03b8)w\u2032\u22c6 is also a solution. For all j /\u2208 J , we have |xj\u22a4(y \u2212Xw\u03b8\u22c6)| \u2264 \u03b8|xj\u22a4(y\u2212Xw\u22c6)|+(1\u2212 \u03b8)|xj\u22a4(y\u2212Xw\u2032\u22c6)| < \u03bb. Combining this inequality with the conditions (2), we necessarily have w\u03b8\u22c6\nJ\u2201 = w\u22c6 J\u2201 = 0,2 and the vector w\u03b8\u22c6J is\nalso a solution of the following reduced problem:\nmin w\u0303\u2208R|J|\n1 2 \u2016y\u2212XJw\u0303\u201622 + \u03bb\u2016w\u0303\u20161.\nWhen XJ is full rank, the Hessian X \u22a4 J XJ is positive definite and this reduced problem is strictly convex. Thus, it admits a unique solution w\u03b8\u22c6J = w \u22c6 J . It is then easy to conclude that w\u22c6 = w\u03b8\u22c6 = w\u2032\u22c6.\nWith the assumption that the matrixXJ is always fullrank, we can formally recall a well-known property of the Lasso (see Markowitz, 1952; Osborne et al., 2000; Efron et al., 2004) in the following lemma:\nLemma 2 (Piecewise Linearity of the Path). Assume that for any \u03bb > 0 and solution of Eq. (1) the matrix XJ defined in Lemma 1 is full-rank. Then, the regularization path {w\u22c6(\u03bb) : \u03bb > 0} is well defined, unique and continuous piecewise linear.\nProof. The existence/uniqueness of the regularization path was shown in Lemma 1.\nLet us define {\u03b7\u22c6(\u03bb) , sign(w\u22c6(\u03bb)) : \u03bb > 0} the set of sparsity patterns. Let us now consider \u03bb1 < \u03bb2 such that \u03b7\u22c6(\u03bb1) = \u03b7\n\u22c6(\u03bb2). For all \u03b8 \u2208 [0, 1], it is easy to see that the solution w\u03b8\u22c6 , \u03b8w\u22c6(\u03bb1) + (1 \u2212 \u03b8)w\u22c6(\u03bb2) satisfies the optimality conditions of Lemma 1 for \u03bb = \u03b8\u03bb1+(1\u2212\u03b8)\u03bb2, and that w\u22c6(\u03b8\u03bb1 + (1\u2212\u03b8)\u03bb2)= w\u03b8\u22c6. This shows that whenever two solutions w\u22c6(\u03bb1) and w\u22c6(\u03bb2) have the same signs for \u03bb1 6=\u03bb2, the regularization path between \u03bb1 and \u03bb2 is a linear segment. As an important consequence, the number of linear segments of the path is smaller than 3p, the number of possible sparsity patterns in {\u22121, 0, 1}p. The path P is therefore piecewise linear with a finite number of kinks.\n2J\u2201 denotes the complement of the set J in {1, . . . , p}.\nMoreover, since the function \u03bb \u2192 w\u22c6(\u03bb) is piecewise linear, it is piecewise continuous and has right and left limits for every \u03bb > 0. It is easy to show that these limits satisfy the optimality conditions of Eq. (2). By uniqueness of the Lasso solution, they are equal to w\u22c6(\u03bb) and the function is in fact continuous.\nAssuming again that XJ is always full rank, we can now present in Algorithm 1 the homotopy method (Osborne et al., 2000; Efron et al., 2004).\nAlgorithm 1 Homotopy Algorithm for the Lasso.\n1: Inputs: a vector y in Rn; a matrix X in Rn\u00d7p; 2: initialization: set \u03bb to \u2016X\u22a4y\u2016\u221e; we have\nw\u22c6(\u03bb) = 0 (trivial solution); 3: set J , {j0} such that |xj0\u22a4y| = \u03bb; 4: while \u03bb > 0 do 5: Set \u03b7 , sign(X\u22a4(y\u2212Xw\u22c6(\u03bb)); 6: compute the direction of the path:\n{ w\u22c6J(\u03bb) = (X \u22a4 J XJ ) \u22121(X\u22a4J y\u2212\u03bb\u03b7J) w\u22c6\nJ\u2201 (\u03bb) = 0.\n7: Find the smallest step \u03c4 > 0 such that: \u2022 there exists j \u2208 J\u2201 such that |xj\u22a4(y\u2212Xw\u22c6(\u03bb\u2212\u03c4))| = \u03bb\u2212\u03c4 ; add j to J ; \u2022 there exists j in J such that w\u22c6j (\u03bb) 6= 0 and w\u22c6j (\u03bb\u2212\u03c4) = 0; remove j from J ; 8: replace \u03bb by \u03bb\u2212 \u03c4 ; record the pair (\u03bb,w\u22c6(\u03bb)); 9: end while\n10: Return: sequence of recorded values (\u03bb,w\u22c6(\u03bb)).\nIt can be shown that this algorithmmaintains the optimality conditions of Lemma 1 when \u03bb decreases. Two assumptions have nevertheless to be made for the algorithm to be correct. First, (XTJXJ ) has to be invertible, which is a reasonable assumption commonly made when working with real data and when one is interested in sparse solutions. When (XTJXJ ) becomes ill-conditioned, which may typically occur for small values of \u03bb, the algorithm has to stop and the path is truncated. Second, one assumes in Step 7 of the algorithm that the value \u03c4 corresponds to a single event |xj\u22a4(y\u2212Xw\u22c6(\u03bb\u2212\u03c4))| = \u03bb\u2212\u03c4 for j in J\u2201 orw\u22c6j (\u03bb\u2212\u03c4) hits zero for j in J . In other words, variables enter or exit the path one at a time. Even though this assumption is reasonable most of the time, it can be problematic from a numerical point of view in rare cases. When the length of a linear segment of P is smaller than the numerical precision, the algorithm can fail. In contrast, our approximate homotopy algorithm presented in Section 4 is robust to this issue. In the next section, we present our worst-case complexity analysis of the regularization path, showing that Algorithm 1 can have exponential complexity."}, {"heading": "3. Worst-Case Complexity", "text": "We denote by {\u03b7\u22c6(\u03bb) , sign(w\u22c6(\u03bb)) : \u03bb > 0} the set of sparsity patterns in {\u22121, 0, 1}p encountered along the path P . We have seen in the proof of Lemma 2 that whenever \u03b7\u22c6(\u03bb1) = \u03b7\n\u22c6(\u03bb2), for \u03bb1, \u03bb2 > 0, then \u03b7 \u22c6(\u03bb) = \u03b7\u22c6(\u03bb1) for all \u03bb \u2208 [\u03bb1, \u03bb2], and thus the number of linear segments of P is upper-bounded by 3p. With an additional argument, we can further reduce this number, as stated in the following proposition:\nProposition 1 (Upper-bound Complexity). Let assume the same conditions as in Lemma 2. The number of linear segments in the regularization path of the Lasso is less than (3p + 1)/2.\nProof. We have already noticed that the number of linear segments of the path is at most 3p. Let us consider \u03b7 \u22c6(\u03bb1) 6=0 for \u03bb1>0. We now show that for all \u03bb2>0, we have \u03b7\u22c6(\u03bb2) 6=\u2212\u03b7\u22c6(\u03bb1), and therefore the number of different sparsity patterns on the path P is in fact less than or equal to (3p+1)/2.\nLet us assume that there exists \u03bb2 > 0 with \u03b7 \u22c6(\u03bb2) = \u2212\u03b7\u22c6(\u03bb1), and look for a contradiction. We define the set J \u2032 , {j \u2208 {1, . . . , p} : \u03b7\u22c6j (\u03bb1) 6= 0}, and consider the solution of the reduced problem for all \u03bb \u2265 0:\nw\u0303\u22c6(\u03bb) , argmin w\u0303\u2208R|J\u2032|\n1 2 \u2016y \u2212XJ\u2032w\u0303\u201622 + \u03bb\u2016w\u0303\u20161,\nwhich is well defined since the optimization problem is strictly convex (the conditions of Lemma 2 imply that XJ\u2032 is full rank). We remark that w\u0303\n\u22c6(\u03bb1) = w\u22c6J\u2032(\u03bb1), and w\u0303 \u22c6(\u03bb2) = w \u22c6 J\u2032(\u03bb2). Given the optimality conditions of Lemma 1, it is then easy to show that w\u0303\u22c6(0) = (X\u22a4J\u2032XJ\u2032) \u22121X\u22a4J\u2032y = \u03bb2 \u03bb1+\u03bb2 w\u0303\u22c6(\u03bb1) +\n\u03bb1 \u03bb1+\u03bb2 w\u0303\u22c6(\u03bb2). Since the signs of w\u0303 \u22c6(\u03bb1) and w\u0303 \u22c6(\u03bb2) are opposite to each other and non-zero, we have \u2016w\u0303\u22c6(0)\u20161 < \u2016w\u0303\u22c6(\u03bb1)\u20161. Independently, it is also easy to show that the function \u03bb \u2192 \u2016w\u0303\u22c6(\u03bb)\u20161 should be non-increasing, and we obtain a contradiction.\nIn the next proposition, we present our adversarial strategy to build a pathological regularization path. Given a Lasso problem with p variables and a path P , we design an additional variable along with an extra dimension, such that the number of kinks of the new path P\u0303 increases by a multiplicative factor compared to P . We call our strategy \u201cadversarial\u201d since it consists of iteratively designing \u201cpathological\u201d variables.\nProposition 2 (Adversarial Strategy). Let us consider y in Rn and X in Rn\u00d7p such that the conditions of Lemma 2 are satisfied and y is in the span of X. We denote by P the regularization path of the Lasso problem corresponding to (y,X), by k the\nnumber of linear segments of P, and by \u03bb1 > 0 the smallest value of the parameter \u03bb corresponding to a kink of P. We define the vector y\u0303 in Rn+1 and the matrix X\u0303 in R(n+1)\u00d7(p+1) as follows:\ny\u0303 ,\n[ y\nyn+1\n]\n, X\u0303 , [ X 2\u03b1y 0 \u03b1yn+1 ] ,\nwhere yn+1 6= 0 and 0 < \u03b1 < \u03bb1/(2y\u22a4y + y2n+1).\nThen, the regularization path P\u0303 of the Lasso problem associated to (y\u0303, X\u0303) exists and has 3k\u22121 linear segments. Moreover, let us consider {\u03b71 =0,\u03b72, . . . ,\u03b7k} the sequence of sparsity patterns in {\u22121, 0, 1}p of P (the signs of the solutions w\u22c6(\u03bb)), ordered from large to small values of \u03bb. The sequence of sparsity patterns in {\u22121, 0, 1}p+1 of the new path P\u0303 is the following:\n{ first k patterns\n\ufe37 \ufe38\ufe38 \ufe37 [ \u03b7 1\n0\n]\n,\n[ \u03b7 2\n0\n]\n, . . . ,\n[ \u03b7 k\n0\n]\n,\nmiddle k patterns \ufe37 \ufe38\ufe38 \ufe37 [ \u03b7 k\n1\n]\n,\n[ \u03b7 k\u22121\n1\n]\n, . . . , [ \u03b7 1=0 1 ] ,\n[ \u2212\u03b72 1 ] , [ \u2212\u03b73 1 ] , . . . , [ \u2212\u03b7k 1 ] \ufe38 \ufe37\ufe37 \ufe38\nlast k\u22121 patterns\n}\n. (4)\nLet us first make some remarks about this proposition: \u2022 According to Eq. (4) the sparsity patterns of the new path P\u0303 are related to those of P . More precisely, they have either the form [\u03b7i\u22a4, 0]\u22a4 or [\u00b1\u03b7i\u22a4, 1]\u22a4, where \u03b7i is a sparsity pattern in {\u22121, 0, 1}p of P .\n\u2022 The last column of X\u0303 involves a factor \u03b1 that controls its norm. With \u03b1 small enough, the (p+1)-th variable enters late the path P\u0303 . As shown in Eq. (4), the first k sparsity patterns of P\u0303 do not involve this variable and are exactly the same as those of P .\n\u2022 Let us give some intuition about the pathological behavior of the path P\u0303 . The first k kinks of P\u0303 are the same as those of P , and after these first k kinks we have y \u2248 Xw\u22c6(\u03bb). Then, the (p+1)-th variable enters the path and we heuristically have\nX\u0303\n[ w\u22c6(\u03bb)\n0\n]\n+\n[ 0\nyn+1\n] \u2248 y\u0303 \u2248 X\u0303 [ \u2212w\u22c6(\u03bb) 1/\u03b1 ] . (5)\nThe left side of Eq. (5) tells us that when the (p+1)th variable is inactive, the coefficients associated to the first p variables should be close to w\u22c6(\u03bb). At the same time, the right side of Eq. (5) tells us that when the (p+1)-th variable is active, these same p coefficients should be instead close to \u2212w\u22c6(\u03bb). According to Eq. (4), the signs of these p coefficients along the path switch from \u03b7k=sign(w\u22c6(\u03bb)) to \u2212\u03b7k by following the sequence \u03b7k,\u03b7k\u22121, . . . , (\u03b71 =0 =\u2212\u03b71),\u2212\u03b72, . . . ,\u2212\u03b7k, resulting in a path with 3k\u22121 linear segments. The proof below more rigorously describes this strategy:\nProof. Existence of the new regularization path: Let us rewrite the Lasso problem for (y\u0303, X\u0303).\nmin w\u0303\u2208Rp,w\u0303\u2208R\n1\n2\n\u2225 \u2225 \u2225 \u2225 y\u0303 \u2212 X\u0303 [ w\u0303 w\u0303 ]\u2225 \u2225 \u2225 \u2225 2\n2\n+ \u03bb \u2225 \u2225 \u2225 \u2225 [ w\u0303 w\u0303 ]\u2225 \u2225 \u2225 \u2225 1 ,\n= min w\u0303\u2208Rp,w\u0303\u2208R\n1 2 \u2016(1\u22122\u03b1w\u0303)y\u2212Xw\u0303\u201622+ 1 2 (yn+1\u2212\u03b1yn+1w\u0303)2\n+\u03bb\u2016w\u0303\u20161+\u03bb|w\u0303|. (6)\nLet (w\u0303\u22c6, w\u0303\u22c6) be a solution for a given \u03bb > 0. By fixing w\u0303 = w\u0303\u22c6 in Eq. (6) and optimizing with respect to w\u0303, we obtain an equivalent problem to (6):\nmin w\u0303\u2032\u2208Rp\n1 2 \u2016y\u2212Xw\u0303\u2032\u201622 +\n\u03bb\n|1\u2212 2\u03b1w\u0303\u22c6| \u2016w\u0303 \u2032\u20161,\nwith the change of variable w\u0303 = (1 \u2212 2\u03b1w\u0303\u22c6)w\u0303\u2032 and assuming 1\u2212 2\u03b1w\u0303\u22c6 6= 0. The solution of this problem is unique since it is a point of P and we therefore have\nw\u0303\u22c6 =\n{ (1\u2212 2\u03b1w\u0303\u22c6)w\u22c6 (\n\u03bb |1\u22122\u03b1w\u0303\u22c6|\n)\nif w\u0303\u22c6 6= 12\u03b1 0 otherwise .\n(7) Since the last column of X\u0303 is not in the span of the first p columns by construction of X\u0303, it is then easy to see that the conditions of Lemma 2 are necessarily satisfied and therefore (w\u0303\u22c6, w\u0303\u22c6) is in fact the unique solution of Eq. (6). Since this is true for all \u03bb > 0, the regularization path is well defined, and we denote from now on the above solutions by w\u0303\u22c6(\u03bb) and w\u0303\u22c6(\u03bb).\nMaximum number of linear segments: We now show that the number of linear segments of the path is upper-bounded by 3k\u22121. Eq. (7) shows that sign(w\u0303\u22c6(\u03bb)) has the form \u00b1\u03b7i, where \u03b7i in {\u22121, 0, 1}p is one of the k sparsity patterns from P , whereas we have three possibilities for sign(w\u0303\u22c6(\u03bb)), namely {\u22121, 0,+1}. Since one can not have two non-zero sparsity patterns that are opposite to each other on the same path, as shown in the proof of Proposition 1, the number of possible sparsity patterns reduces to 3k\u22121. Characterization of the first k linear segments: Let us consider \u03bb\u2265 \u03bb1 and show that w\u0303\u22c6(\u03bb) =w\u22c6(\u03bb) and w\u0303\u22c6(\u03bb)=0 by checking the optimality conditions of Lemma 1. The first p equalities/inequalities in Eq. (2) are easy to verify, the last one being also satisfied:\n|2\u03b1y\u22a4(y\u2212Xw\u22c6(\u03bb))+\u03b1y2n+1| \u2264 2\u03b1\u2016y\u201622 + \u03b1y2n+1 < \u03bb1,\nwhere the last inequality is obtained from the definition of \u03b1. Since this inequality is strict, this also ensures that there exists 0 < \u03bb\u20321 < \u03bb1 such that w\u0303\u22c6(\u03bb) = w\u22c6(\u03bb) and w\u0303\u22c6(\u03bb) = 0 for all \u03bb \u2265 \u03bb\u20321. We have therefore shown that the first k sparsity patterns of the regularization path are given in Eq. (4).\nCharacterization of the last 2k\u22121 segments: We mainly use here the form of Eq. (7) and a few continuity arguments to characterize the rest of the path. First, we remark that for all \u03b2 in [0, 1\u03b1 ), there exists a value for \u03bb > 0 such that w\u0303\u22c6(\u03bb) = \u03b2. This is true because: (i) \u03bb\u2192 w\u0303\u22c6(\u03bb) is continuous; (ii) w\u0303\u22c6(\u03bb1)= 0; (iii) w\u0303\u22c6(0+)= 1\u03b1 . Point (i) was shown in Lemma 2, point (ii) in the previous paragraph, and point (iii) is necessary to have the term (yn+1\u2212\u03b1yn+1w\u0303)2 in Eq. (6) go to 0 when \u03bb goes to 0+.\nWe now consider two values \u03bb\u20321, \u03bb \u2032 2 > 0 such that w\u0303\u22c6(\u03bb\u20321) = 0, w\u0303 \u22c6(\u03bb\u20322) = 1 2\u03b1 and w\u0303\n\u22c6(\u03bb) \u2208 (0, 12\u03b1 ) for all \u03bb \u2208 (\u03bb\u20321, \u03bb\u20322). On this open interval, we have that (1 \u2212 2\u03b1w\u0303\u22c6(\u03bb))> 0, and the continuous function \u03bb \u2192 \u03bb/|(1\u2212 2\u03b1w\u0303\u22c6(\u03bb))| ranges from \u03bb\u20321 to +\u221e. Combining this observation with Eq. (7), we obtain that all sparsity patterns of the form [\u03b7i\u22a4, 1]\u22a4 for i in {1, . . . , k} appear on the regularization path. With similar continuity arguments, it is easy to show that all sparsity patterns of the form [\u2212\u03b7i\u22a4, 1]\u22a4 for i in {1, . . . , k} appear on the path as well.\nWe had previously identified k of the sparsity patterns, and now have identified 2k\u22121 different ones. Since we have at most 3k\u22121 linear segments, the set of sparsity patterns on the path P\u0303 is entirely characterized. The fact that the sequence of sparsity patterns is the one given in Eq. (4) can easily be shown by reusing similar continuity arguments.\nWith this proposition in hand, we can now state the main result of this section:\nTheorem 1 (Worst-case Complexity). In the worst case, the regularization path of the Lasso has exactly (3p + 1)/2 linear segments.\nProof. We start with n = p = 1, and define y = [1], and X= [1], leading to a path with k = 2 segments. We then recursively apply Proposition 2, keeping n=p, choosing at iteration p + 1, yp+1 = 1, and a factor \u03b1=\u03b1p+1 satisfying the conditions of Proposition 2. Denoting by kp the number of linear segments at iteration p, we have that kp+1 =3kp\u22121, and it is easy to show that kp=(3\np+1)/2. According to Proposition 1, this is the longest possible regularization path. Note that this example has a particularly simple shape:\ny ,\n\n      1 1 1 ... 1\n\n      , X ,\n\n      \u03b11 2\u03b12 2\u03b13 . . . 2\u03b1p 0 \u03b12 2\u03b13 . . . 2\u03b1p 0 0 \u03b13 . . . 2\u03b1p ... ... ... . . . ...\n0 0 0 . . . \u03b1p\n\n      ."}, {"heading": "3.1. Numerical Simulations", "text": "We have implemented Algorithm 1 in Matlab, optimizing numerical precision regardless of computational efficiency, which has allowed us to check our theoretical results for small values of p. For instance, we obtain a path with (3p + 1)/2 = 88 574 linear segments for p = 11, and present such a pathological path in Figure 1. Note that when p gets larger, these examples quickly lead to precision issues where some kinks are very close to each other. Our implementation and our pathological examples will be made publicly available. In the next section, we present more optimistic results on approximate regularization paths."}, {"heading": "4. Approximate Homotopy", "text": "We now present another complexity analysis when exact solutions of Eq. (1) are not required. We follow in part the methodology of Giesen et al. (2010), later refined by Jaggi (2011), on approximate regularization paths of parameterized convex functions. Their results are quite general but, as we show later, we obtain stronger results with an analysis tailored to the Lasso.\nA natural tool to guarantee the quality of approximate solutions is the duality gap. Writing the Lagrangian of problem (1) and minimizing with respect to the primal variable w yields the following dual formulation of (1):\nmax \u03ba\u2208Rn \u22121 2 \u03ba \u22a4 \u03ba\u2212 \u03ba\u22a4y s.t. \u2016X\u22a4\u03ba\u2016\u221e \u2264 \u03bb, (8)\nwhere \u03ba in Rn is a dual variable. Let us denote by f\u03bb(w) the objective function of the primal problem (1) and by g\u03bb(\u03ba) the objective function of the dual (8). Given a pair of feasible primal and dual variables (w,\u03ba), the difference \u03b4\u03bb(w,\u03ba) , f\u03bb(w) \u2212 g\u03bb(\u03ba) is called a duality gap and provides an optimality guar-\nantee (see Borwein & Lewis, 2006):\n0 \u2264 f\u03bb(w) \u2212 f\u03bb(w\u22c6(\u03bb)) \u2264 \u03b4\u03bb(w,\u03ba). In plain words, it upper bounds the difference between the current value of the objective function f\u03bb(w) and the optimal value of the objective function f\u03bb(w\n\u22c6(\u03bb)). In this paper, we use a relative duality gap criterion to guarantee the quality of an approximate solution:3\nDefinition 1 (\u03b5-approximate Solution). let \u03b5 be in [0, 1]. A vector w in Rp is said to be an \u03b5approximate solution of problem (1) if there exists \u03ba in Rn such that \u2016X\u22a4\u03ba\u2016\u221e\u2264\u03bb and \u03b4\u03bb(w,\u03ba)\u2264\u03b5f\u03bb(w). Given a set P\u0303 , {w\u0303(\u03bb) \u2208 Rp : \u03bb > 0}, we say that P\u0303 is an \u03b5-approximate regularization path if any point w\u0303(\u03bb) of P\u0303 is an \u03b5-approximate solution for problem (1).\nOur goal is now to build \u03b5-approximate regularization paths and study their complexity. To that effect, we introduce approximate optimality conditions based on small perturbations of those given in Lemma 1:\nDefinition 2 (OPT\u03bb(\u03b51, \u03b52) Condition). Let \u03b51\u22650 and \u03b52\u2265\u2212\u03b51. A vector w in Rp satisfies the OPT\u03bb(\u03b51, \u03b52) condition if and only if for all 1\u2264j\u2264p, \u03bb(1\u2212\u03b52)\u2264xj\u22a4(y\u2212Xw) sign(wj)\u2264\u03bb(1+\u03b51) if wj 6=0,\n|xj\u22a4(y\u2212Xw)|\u2264\u03bb(1+\u03b51) otherwise. (9)\nNote that when \u03b51 = \u03b52 =0, this condition reduces to the exact optimality conditions of Lemma 1. Of interest for us is the relation between Definitions 1 and 2. Let us consider a vector w such that OPT\u03bb(\u03b51, \u03b52) is satisfied. Then, the vector \u03ba , 11+\u03b51 (Xw\u2212y) is feasible for the dual (8) and we can compute a duality gap:\n\u03b4\u03bb(w,\u03ba) = f\u03bb(w) \u2212 g\u03bb(\u03ba)\n= 1\n2 (1 + \u03b51)\n2 \u03ba \u22a4 \u03ba+ \u03bb\u2016w\u20161 +\n1 2 \u03ba \u22a4 \u03ba+ \u03ba\u22a4y\n= \u03b521 2 \u03ba \u22a4 \u03ba+ \u03bb\u2016w\u20161 + \u03ba\u22a4 ( y + (1 + \u03b51)\u03ba ) = \u03b521\n(1 + \u03b51)2 1 2 \u2016y \u2212Xw\u201622 + \u03bb\u2016w\u20161 + \u03ba\u22a4Xw.\nFrom Eq. (9), it is easy to show that \u03bb\u2016w\u20161+\u03ba\u22a4Xw \u2264 \u03b51+\u03b52 1+\u03b51 \u03bb\u2016w\u20161, and we can obtain the following bound:\n\u03b4\u03bb(w,\u03ba) \u2264 max ( \u03b521 (1 + \u03b51)2 , \u03b51 + \u03b52 1 + \u03b51 ) f\u03bb(w). (10)\nFrom this upper bound, we derive our first result:\n3Note that our criterion is not exactly the same as in Jaggi (2011). Whereas Jaggi (2011) consider a formulation where the \u21131-norm appears in a constraint, Eq. (1) involves an \u21131-penalty. Even though these formulations have the same regularization path, they involve slightly different objective functions, dual formulations, and duality gaps.\nProposition 3 (Approximate Analysis). Let y be in Rn and X in Rn\u00d7p such that the conditions of Lemma 2 are satisfied. Let \u03bb\u221e,\u2016X\u22a4y\u2016\u221e be the value of \u03bb corresponding to the start of the path, and \u03bb1 > 0 be the one corresponding to the last kink. For all \u03b5\u2208(0, 1), there exists an \u03b5-approximate regularization path with at most \u2308 log(\u03bb\u221e/\u03bb1)\u221a\n\u03b5\n\u2309\nlinear segments.\nProof. From Eq. (9), one can show by a simple calculation that an exact solution w\u22c6(\u03bb) for a given \u03bb satisfies OPT\u03bb(1\u2212\u03b53)(\u03b53/(1\u2212\u03b53),\u2212\u03b53/(1\u2212\u03b53)). According to Eq. (10), there exists a dual variable \u03ba such that \u03b4\u03bb(1\u2212\u03b53)(w\n\u22c6(\u03bb),\u03ba)\u2264 \u03b523. Thus, for any \u03bb\u2032 chosen in [\u03bb, \u03bb(1\u2212\u221a\u03b5)], the solutionw\u22c6(\u03bb) is an \u03b5-approximate solution for the parameter \u03bb\u2032. Between \u03bb\u221e and \u03bb1, we can obtain an \u03b5-approximate piecewise linear (in fact piecewise constant) regularization path by sampling solutions w\u22c6(\u03bb) for \u03bb in {\u03bb\u221e, \u03bb\u221e(1\u2212 \u221a \u03b5), . . . , \u03bb\u221e(1\u2212\u221a\n\u03b5)k, \u03bb1} with \u03bb\u221e(1\u2212 \u221a \u03b5)k+1 \u2264 \u03bb1. The number of segments of the corresponding approximate path is at most \u230a \u2212 log(\u03bb\u221e/\u03bb1) log(1\u2212\u221a\u03b5) \u230b +1\u2264 \u2308 log(\u03bb\u221e/\u03bb1)\u221a \u03b5 \u2309 .\nNote that the term \u03bb\u221e/\u03bb1 is possibly large, but it is controlled by a logarithmic function and can be considered as constant for finite precision machines. In other words, the complexity of the approximate path is upper-bounded by O(1/ \u221a \u03b5). In contrast, the analysis of Giesen et al. (2010) and Jaggi (2011) give us: \u2022 an approximate path with O(1/\u03b5) linear segments can be obtained with a weaker approximation guarantee than ours. Namely, a bound \u03b4\u2264 \u03b5 along the path, where \u03b4 is a duality gap, whereas we use relative duality gaps of the form \u03b4\u2264 \u03b5f\u03bb(w);4 Interestingly, this bound is proven to be optimal in the context of parameterized convex functions on the \u21131-ball. Our result show that such bound can be improved for the Lasso.\n\u2022 a methodology to obtain relative duality gaps along the path, which can easily provide complexity bounds for the full path of different problems, notably support vector machines, but not for the Lasso.\nProposition 3 is optimistic, but not practical since it requires sampling exact solutions of the path P . We introduce an approximate homotopy method in Algorithm 2 which does not require computing exact solutions and still enjoys a similar complexity. It exploits the piecewise linearity of the path, but uses a firstorder method (Beck & Teboulle, 2009; Fu, 1998) when the linear segments of the path are too short.\n4When there exists m,M>0 such that m<f\u03bb<M , the relative duality gap guarantee is similar (up to a constant) to the simple bound \u03b4 \u2264 \u03b5. However, we have for the Lasso that f\u03bb(w\n\u22c6(\u03bb)) \u2192 0 when \u03bb goes to 0+, as long as y is in the span of X. Note that as noticed in footnote 3, Jaggi (2011) uses a slightly different duality gap than ours.\nAlgorithm 2 Approximate Homotopy for the Lasso.\n1: Inputs: a vector y in Rn, a matrix X in Rn\u00d7p, the required precision \u03b5 \u2208 [0, 1]; \u03bb1 > 0; 2: initialization: set \u03bb to \u2016X\u22a4y\u2016\u221e; set w\u0303(\u03bb) = 0; 3: set \u03b8 = 1 + \u03b5/2\u2212\u221a\u03b5/2; 4: set J , {j0} such that |xj0\u22a4y| = \u03bb; 5: while \u03bb \u2265 \u03bb1 do 6: if (X\u22a4J XJ ) is not invertible then go to 12; 7: set \u03b7\u0303 , (1/\u03bb)X\u22a4(y\u2212Xw\u0303(\u03bb)); 8: compute the approximate direction of the path:\n{ w\u0303J(\u03bb) = (X \u22a4 J XJ ) \u22121(X\u22a4J y\u2212\u03bb\u03b7\u0303J) w\u0303J\u2201(\u03bb) = 0.\nFind the smallest step \u03c4 > 0 such that: \u2022 there exists j in J\u2201 such that |xj\u22a4(y\u2212Xw\u0303(\u03bb\u2212\u03c4))|=(\u03bb\u2212\u03c4)(1+ \u03b52 ); add j to J ; \u2022 there exists j in J such that w\u0303j(\u03bb) 6= 0 and w\u0303j(\u03bb\u2212\u03c4)=0; remove j from J ;\n9: if \u03c4 \u2265 \u03bb\u03b8\u221a\u03b5 then 10: replace \u03bb by \u03bb\u2212 \u03c4 ; 11: else 12: replace \u03bb by \u03bb(1 \u2212 \u03b8\u221a\u03b5); 13: use a first-order optimization method to find a solution w\u0303(\u03bb) satisfying OPT\u03bb(\u03b5/2, \u03b5/2); 14: set J = {j \u2208 {1, . . . , p} : w\u0303j(\u03bb) 6= 0}. 15: end if 16: record the pair (\u03bb, w\u0303(\u03bb)); 17: end while 18: Return: sequence of recorded values (\u03bb, w\u0303(\u03bb)).\nNote that when \u03b5 = 0, Algorithm 2 reduces to Algorithm 1. Our approach exploits the following ideas, which we formally prove in the sequel. Assume that w\u0303(\u03bb) satisfies OPT\u03bb(\u03b5/2, \u03b5/2). Then,\n\u2022 w\u0303(\u03bb) is an \u03b5-approximation for all \u03bb\u2032 in [\u03bb, \u03bb(1\u2212 \u03b8 \u221a \u03b5)]. This guarantees us that one can always make step sizes for \u03bb greater than or equal to \u03bb\u03b8 \u221a \u03b5;\n\u2022 the direction followed in Step 8 maintains OPT\u03bb(\u03b5/2, \u03b5/2), but when two kinks are too close to each other\u2014that is, \u03c4 < \u03bb\u03b8 \u221a \u03b5, we directly look for a solution for the parameter \u03bb\u2032 = \u03bb(1\u2212\u03b8\u221a\u03b5) that satisfies OPT\u03bb\u2032(\u03b5/2, \u03b5/2). Any first-order method can be used for that purpose, e.g., a proximal gradient method (Beck & Teboulle, 2009), using the current value w\u0303(\u03bb) as a warm start. Note also that when (X\u22a4J XJ ) is not invertible, the method uses first-order steps. The next proposition precisely describes the guarantees of our algorithm.\nProposition 4 (Analysis of Algorithm 2). Let y be in Rn and X in Rn\u00d7p. For all \u03bb1>0 and \u03b5\u2208 (0, 1), Algorithm 2 returns an \u03b5-approximate regularization path on [\u03bb\u221e, \u03bb1]. Moreover, it terminates in at most \u2308 log(\u03bb\u221e/\u03bb1)\n\u03b8 \u221a \u03b5\n\u2309\niterations, where \u03bb\u221e , \u2016X\u22a4y\u2016\u221e.\nProof. We first show that any solution on the path is an \u03b5-approximate solution. First, it is easy to check thatOPT\u03bb(\u03b5/2, \u03b5/2) is always satisfied at Step 6. This is either a consequence of Step 13, or because the direction w\u0303J (\u03bb \u2032) = (X\u22a4J XJ ) \u22121(X\u22a4J y\u2212\u03bb\u2032\u03b7\u0303J ) maintains OPT\u03bb\u2032(\u03b5/2, \u03b5/2) when \u03bb \u2032 varies between \u03bb and \u03bb\u2212\u03c4 . From Eq. (10), we obtain that w\u0303(\u03bb) is an \u03b5approximate solution whenever OPT\u03bb(\u03b5/2, \u03b5/2) is satisfied. Thus, we only need to check that w\u0303(\u03bb) is also an \u03b5-approximate solution for \u03bb\u2032 in [\u03bb, \u03bb(1 \u2212 \u03b8\u221a\u03b5)]: for \u03b53 \u2265 0, it is easy to check that OPT\u03bb(\u03b5/2, \u03b5/2) implies OPT\u03bb(1\u2212\u03b53)((\u03b5/2+\u03b53)/(1\u2212\u03b53), (\u03b5/2\u2212\u03b53)/(1\u2212\u03b53)). Setting \u03b53 = \u03b8 \u221a \u03b5 and using Eq. (10), it is possible to show that the desired condition is satisfied. Since the step size for \u03bb is always greater than \u03bb\u03b8 \u221a \u03b5, the maximum number of iterations is upper-bounded by \u230a \u2212 log(\u03bb\u221e/\u03bb1) log(1\u2212\u03b8\u221a\u03b5) \u230b + 1 \u2264 \u2308 log(\u03bb\u221e/\u03bb1) \u03b8 \u221a \u03b5 \u2309\nWe remark that the scalar \u03b8 is very close to 1 and therefore the complexity is similar to the one of Proposition 3, with a logarithmic function controlling the possibly large term \u03bb\u221e/\u03bb1. This algorithm is practical in different aspects: (i) it is almost as simple to implement as the homotopy method; (ii) it is robust to cases where two kinks are too close for the classical homotopy method to work; (iii) it provides optimality guarantees along the path; (iv) whenever possible, it explicitly exploits the piecewise linearity of the path. We next present experiments to verify our analysis."}, {"heading": "4.1. Numerical Simulations", "text": "We have implemented Algorithm 2 with a few modifications to the code used in Section 3.1. The inner solver is a coordinate descent algorithm (see Fu, 1998), with a stopping criterion based on Definition 2.\nWe consider 4 datasets. The first one dubbed SYNTH consists of a pure noise fitting scenario with no statistical meaning. The entries of the corresponding vector y and matrix X are i.i.d. draws from a standard normal distribution. The next dataset is called PATHOL and is a pathological example obtained from the analysis of Section 3. Finally, we consider two datasets based on real data, respectively dubbed MADELON5 and PCMAC6. For each dataset, we center and normalize the columns of X and the vector y, and choose the parameter \u03bb1 corresponding to the last kink of the true path.\nFor all datasets, we compute the full regularization path using Algorithm 1 and several \u03b5-approximate regularization paths using Algorithm 2. Note that\n5http://www.nipsfsc.ecs.soton.ac.uk/datasets/. 6http://featureselection.asu.edu/datasets.php.\nthe path of PCMAC was stopped around \u03bb \u2248 10\u22124 where the matrix X\u22a4J XJ became ill-conditioned and the Lasso solution dense. As a simple sanity check, we first experimentally verify the correctness of Propositions 3 and 4, by sampling solutions on the approximate path we obtain, computing duality gaps, and checking that the solutions are indeed \u03b5-approximate. We conclude that our experimental results match our theoretical analysis. We present the different path complexities in Table 1.\nInterestingly, the complexity of the pathological example significantly reduces when one is looking for an approximate solution. For example, for \u03b5=10\u22123, the complexity of the approximate path is less than 0.5% the one of the full path. This significantly contrasts with the pessimistic result obtained in Section 3. As expected, the two examples based on real data exhibit a path complexity of the same order of the problem size, which also significantly reduces when \u03b5 increases."}, {"heading": "5. Conclusion", "text": "We have presented new results on the regularization path and thus on homotopy methods for the Lasso. First, we have shown that the path has an exponential worst-case complexity, which, as far as we know, had never been formally proved before. Our second result is more optimistic, and shows that when an exact path is not required, only a relatively small number of points on the path need to be computed. Finally, we propose a practical approximate homotopy algorithm, which can provide such approximate paths at a desired precision."}, {"heading": "Acknowledgments", "text": "This paper was supported in part by NSF grants SES0835531, CCF-0939370, DMS-1107000, DMS-0907632, and by ARO-W911NF-11-1-0114."}], "references": [{"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sci.,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Convex analysis and nonlinear optimization: theory and examples", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": null, "citeRegEx": "Borwein and Lewis,? \\Q2006\\E", "shortCiteRegEx": "Borwein and Lewis", "year": 2006}, {"title": "Maximization of a linear function of variables subject to linear inequalities", "author": ["G.B. Dantzig"], "venue": "In Koopmans, T .C. (ed.), Activity Analysis of Production and Allocation, pp. 339\u2013347.Wiley,", "citeRegEx": "Dantzig,? \\Q1951\\E", "shortCiteRegEx": "Dantzig", "year": 1951}, {"title": "Penalized regressions: The bridge versus the Lasso", "author": ["W.J. Fu"], "venue": "J. Comput. Graph. Stat.,", "citeRegEx": "Fu,? \\Q1998\\E", "shortCiteRegEx": "Fu", "year": 1998}, {"title": "Recovery of exact sparse representations in the presence of bounded noise", "author": ["J.J. Fuchs"], "venue": "IEEE T. Inform. Theory.,", "citeRegEx": "Fuchs,? \\Q2005\\E", "shortCiteRegEx": "Fuchs", "year": 2005}, {"title": "An exponential lower bound on the complexity of regularization", "author": ["B. G\u00e4rtner", "M. Jaggi", "C. Maria"], "venue": "paths. preprint arXiv:0903.4817v2,", "citeRegEx": "G\u00e4rtner et al\\.,? \\Q2010\\E", "shortCiteRegEx": "G\u00e4rtner et al\\.", "year": 2010}, {"title": "Approximating parameterized convex optimization problems", "author": ["J. Giesen", "M. Jaggi", "S. Laue"], "venue": "In Algorithms - ESA, Lectures Notes Comp. Sci", "citeRegEx": "Giesen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Giesen et al\\.", "year": 2010}, {"title": "The entire regularization path for the support vector machine", "author": ["T. Hastie", "S. Rosset", "R. Tibshirani", "J. Zhu"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Hastie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2004}, {"title": "Sparse Convex Optimization Methods for Machine Learning", "author": ["M. Jaggi"], "venue": "PhD thesis, ETH Zu\u0308rich,", "citeRegEx": "Jaggi,? \\Q2011\\E", "shortCiteRegEx": "Jaggi", "year": 2011}, {"title": "How good is the simplex algorithm", "author": ["V. Klee", "G.J. Minty"], "venue": "In Shisha, O. (ed.), Inequalities,", "citeRegEx": "Klee and Minty,? \\Q1972\\E", "shortCiteRegEx": "Klee and Minty", "year": 1972}, {"title": "A new approach to variable selection in least squares problems", "author": ["M. Osborne", "B. Presnell", "B. Turlach"], "venue": "IMA J. Numer. Anal.,", "citeRegEx": "Osborne et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 2000}, {"title": "Ein verfahren zur l\u00f6sung parameterabh\u00e4ngiger, nichtlinearer maximum-probleme", "author": ["K. Ritter"], "venue": "Math. Method Oper. Res.,", "citeRegEx": "Ritter,? \\Q1962\\E", "shortCiteRegEx": "Ritter", "year": 1962}, {"title": "Piecewise linear regularized solution paths", "author": ["S. Rosset", "J. Zhu"], "venue": "Ann. Stat.,", "citeRegEx": "Rosset and Zhu,? \\Q2007\\E", "shortCiteRegEx": "Rosset and Zhu", "year": 2007}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["R. Tibshirani"], "venue": "J. Roy. Stat. Soc. B,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}], "referenceMentions": [{"referenceID": 13, "context": "In particular, the l1-norm has been used for that purpose in the Lasso formulation (Tibshirani, 1996).", "startOffset": 83, "endOffset": 101}, {"referenceID": 11, "context": "(2010), all of these examples are in fact particular instances of parametric quadratic programming formulations, for which path-following algorithms appear early in the optimization literature (Ritter, 1962).", "startOffset": 193, "endOffset": 207}, {"referenceID": 8, "context": "by Osborne et al. (2000) and Efron et al.", "startOffset": 3, "endOffset": 25}, {"referenceID": 8, "context": "by Osborne et al. (2000) and Efron et al. (2004) for the Lasso, and by Hastie et al.", "startOffset": 3, "endOffset": 49}, {"referenceID": 6, "context": "(2004) for the Lasso, and by Hastie et al. (2004) for the support vector machine (SVM).", "startOffset": 29, "endOffset": 50}, {"referenceID": 5, "context": "As observed by G\u00e4rtner et al. (2010), all of these examples are in fact particular instances of parametric quadratic programming formulations, for which path-following algorithms appear early in the optimization literature (Ritter, 1962).", "startOffset": 15, "endOffset": 37}, {"referenceID": 2, "context": "This is notably the case for the simplex algorithm (Dantzig, 1951), which performs empirically well for solving linear programs even though it suffers from exponential worst-case complexity (Klee & Minty, 1972).", "startOffset": 51, "endOffset": 66}, {"referenceID": 2, "context": "This is notably the case for the simplex algorithm (Dantzig, 1951), which performs empirically well for solving linear programs even though it suffers from exponential worst-case complexity (Klee & Minty, 1972). Similarly, by using geometrical tools originally developed to analyze the simplex algorithm, G\u00e4rtner et al. (2010) have shown that the complexity of the SVM regularization path can be exponential.", "startOffset": 52, "endOffset": 327}, {"referenceID": 5, "context": "We remark that our proof is constructive and significantly different than the ones proposed by Klee & Minty (1972) for the simplex algorithm and by G\u00e4rtner et al. (2010) for SVMs.", "startOffset": 148, "endOffset": 170}, {"referenceID": 6, "context": "We follow in part the methodology of Giesen et al. (2010) and Jaggi (2011), who have presented weaker results but in a more general setting for parameterized convex optimization problems.", "startOffset": 37, "endOffset": 58}, {"referenceID": 6, "context": "We follow in part the methodology of Giesen et al. (2010) and Jaggi (2011), who have presented weaker results but in a more general setting for parameterized convex optimization problems.", "startOffset": 37, "endOffset": 75}, {"referenceID": 13, "context": "In this section, we present the Lasso formulation of Tibshirani (1996) and well known facts, which we exploit later in our analysis.", "startOffset": 53, "endOffset": 71}, {"referenceID": 10, "context": "With the assumption that the matrixXJ is always fullrank, we can formally recall a well-known property of the Lasso (see Markowitz, 1952; Osborne et al., 2000; Efron et al., 2004) in the following lemma:", "startOffset": 116, "endOffset": 179}, {"referenceID": 10, "context": "Assuming again that XJ is always full rank, we can now present in Algorithm 1 the homotopy method (Osborne et al., 2000; Efron et al., 2004).", "startOffset": 98, "endOffset": 140}, {"referenceID": 6, "context": "We follow in part the methodology of Giesen et al. (2010), later refined by Jaggi (2011), on approximate regularization paths of parameterized convex functions.", "startOffset": 37, "endOffset": 58}, {"referenceID": 6, "context": "We follow in part the methodology of Giesen et al. (2010), later refined by Jaggi (2011), on approximate regularization paths of parameterized convex functions.", "startOffset": 37, "endOffset": 89}, {"referenceID": 8, "context": "Note that our criterion is not exactly the same as in Jaggi (2011). Whereas Jaggi (2011) consider a formulation where the l1-norm appears in a constraint, Eq.", "startOffset": 54, "endOffset": 67}, {"referenceID": 8, "context": "Note that our criterion is not exactly the same as in Jaggi (2011). Whereas Jaggi (2011) consider a formulation where the l1-norm appears in a constraint, Eq.", "startOffset": 54, "endOffset": 89}, {"referenceID": 6, "context": "In contrast, the analysis of Giesen et al. (2010) and Jaggi (2011) give us: \u2022 an approximate path with O(1/\u03b5) linear segments can be obtained with a weaker approximation guarantee than ours.", "startOffset": 29, "endOffset": 50}, {"referenceID": 6, "context": "In contrast, the analysis of Giesen et al. (2010) and Jaggi (2011) give us: \u2022 an approximate path with O(1/\u03b5) linear segments can be obtained with a weaker approximation guarantee than ours.", "startOffset": 29, "endOffset": 67}, {"referenceID": 3, "context": "It exploits the piecewise linearity of the path, but uses a firstorder method (Beck & Teboulle, 2009; Fu, 1998) when the linear segments of the path are too short.", "startOffset": 78, "endOffset": 111}, {"referenceID": 8, "context": "Note that as noticed in footnote 3, Jaggi (2011) uses a slightly different duality gap than ours.", "startOffset": 36, "endOffset": 49}], "year": 2012, "abstractText": "The regularization path of the Lasso can be shown to be piecewise linear, making it possible to \u201cfollow\u201d and explicitly compute the entire path. We analyze in this paper this popular strategy, and prove that its worst case complexity is exponential in the number of variables. We then oppose this pessimistic result to an (optimistic) approximate analysis: We show that an approximate path with at most O(1/ \u221a \u03b5) linear segments can always be obtained, where every point on the path is guaranteed to be optimal up to a relative \u03b5-duality gap. We complete our theoretical analysis with a practical algorithm to compute these approximate paths.", "creator": "LaTeX with hyperref package"}}}