{"id": "1205.0627", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2012", "title": "Rule-weighted and terminal-weighted context-free grammars have identical expressivity", "abstract": "Two formalisms, both group again relation - on lexicons, have he immediately proposed although into therefore, kind longer - uniform describe makes many combinatorial objects. The former, unlike turned Denise maison operatives, executive weights now page, them the of, that visions by Weinberg th\u00e9orie al place end perspective common formula_7 developing, associates matching to smoother. In way opening quotations, everything allowing perfect this modification of present Greibach Normal Form transformation infinitesimal, due did Blum already Koch, to show the ratio expressivities, was increases of once pain distributions, called these two formalisms.", "histories": [["v1", "Thu, 3 May 2012 06:49:59 GMT  (9kb)", "http://arxiv.org/abs/1205.0627v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yann ponty"], "accepted": false, "id": "1205.0627"}, "pdf": {"name": "1205.0627.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["YANN PONTY"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n20 5.\n06 27\nv1 [\ncs .C\nL ]\n3 M\nay 2"}, {"heading": "1. Introduction", "text": "The random generation of combinatorial objects is one of the natural applications of enumerative combinatorics. Following general principles outlined by Wilf [12], Flajolet et al [8] proposed a fully-automated algebraic approach for the extensive class of decomposable combinatorial objects, a large class of objects that includes context-free languages. This pioneering work was later completed by the introduction of Boltzmann samplers, an alternative family of random generation algorithms based on analytical properties of the underlying generating functions [7]. However, these works only addressed the uniform distribution, while many applications of random generation (e.g. in RNA bioinformatics [6]) require non-uniform distributions to be modeled.\nTo that purpose, Denise et al [4] introduced (terminal)-weighted grammars, a non-uniform framework where the terminal symbols (letters) are associated with a real positive value, inherited multiplicatively by words in the language. Such weights were then used, through a trivial renormalization, to induce a probability distribution on the finite set of words of a given length. Generic random generation algorithms were proposed [4] and implemented within a general random generation toolbox [10]. Analytic and numerical approaches were proposed for figuring out suitable set of weights that would mimic a given, observed, distribution [3]. Finally, a multidimensional rejection scheme was explored to sample words of a given composition, yielding efficient algorithms by generalizing the principles of Boltzmann sampling [2].\nMore recently, Weinberg et al [11] proposed an alternative definition for weighted grammars, associating positive real-values to rules instead of terminal letters. The\nKey words and phrases. Weighted Context-Free Grammars; Random Generation; Normal forms.\n1\nauthors proposed a random generation procedure based on formal grammar manipulations, followed by a call to an unranking algorithm due to Martinez and Molinero [9]. However, the relative expressivities, in term of the distribution induced by the respective weighting schemes, of the two formalisms were not compared.\nIn this short note, we establish the equivalence of the two formalisms with respect to their induced distributions. After this short introduction, we remind in Section 2 the definitions of terminal-weighted and rule-weighted grammars. Then we turn to an analysis of the relative expressivities of the two formalisms, and establish in Section 3 that any terminal-weighted grammar can be simulated by a rule-weighted grammar. Furthermore, we use a Greibach Normal Form transformation to prove, in Section 4, that any rule-weighted grammar can be transformed into a terminal-weighted grammar inducing the same probability distribution, from which one concludes on the equivalence of the two formalisms. We conclude in Section 5 with some closing remarks and perspectives."}, {"heading": "2. Definitions", "text": "A context-free grammar is a 4-tuple G = (\u03a3,N ,P ,S) where\n\u2022 \u03a3 is the alphabet, i.e. a finite set of terminal symbols, also called letters. \u2022 N is a finite set of non-terminal symbols. \u2022 P is the finite set of production rules of the formN \u2192 X , whereN \u2208 N is a non-terminal and X \u2208 {\u03a3 \u222a N}\u2217 is a sequence of letters and non-terminals. \u2022 S is the axiom of the grammar, i. e. the initial non-terminal.\nWe will denote by L(G)n the set of all words of length n generated by G. This set is generated by iteratively applying production rules to non-terminals until a word in \u03a3\u2217 is obtained.\nNote that the non-terminals on the right-hand side of a production rule can be independently derived. It follows that the derivation process, starting from the initial axiom and ending with a word w over the terminal alphabet, can be represented by a parse tree dw. This (ordered directed) tree associates production rules to each internal node and terminal letters to each leaf, such that the i-th child of a node labeled with N \u2192 x1. \u00b7 \u00b7 \u00b7 .xk is either a terminal letter xi \u2208 \u03a3 or a further derivation of xi \u2208 P , starting from a root node that derives the axiom S.\nAssumptions: Let us assume, for the sake of simplicity, that the grammars considered in the following are unambiguous, i.e. that any word in L(G)n has exactly one associated parse tree. Moreover, let us assume, without loss of generality, that the grammar is given using a binary variant of the Chomsky Normal Form (CNF), which partitions the non-terminals into four classes, restricting their production rules to:\n\u2022 Axiom: S \u2192 N , N \u2208 N , and/or S \u2192 \u03b5. \u2022 Unions: N \u2192 N \u2032 | N \u2032\u2032, such that N,N \u2032, N \u2032\u2032 \u2208 N/{S}. \u2022 Products: N \u2192 N \u2032.N \u2032\u2032, such that N,N \u2032, N \u2032\u2032 \u2208 N/{S}. \u2022 Terminals: N \u2192 t, t \u2208 \u03a3.\nFinally, we will postulate the absence of non-productive terminals, e.g. having rules of the form N \u2192 N.N \u2032.\n2.1. Terminal-Weighted Grammars. A non-uniform distribution can be postulated on the language generated by the grammar. To that purpose, two formalisms have been independently proposed, reminded here for the sake of completeness.\nDefinition 2.1 ((Terminal)-Weighted Grammar [4]). A terminal-weighted grammar G\u03c0 is a 5-tuple G\u03c0 = (\u03c0,\u03a3,N ,P ,S) where:\n\u2022 (\u03a3,N ,P ,S) defines a context-free grammar, \u2022 \u03c0 : \u03a3 \u2192 R+ is a terminal-weighting function that associates a non-null positive real-valued weight \u03c0t to each terminal symbol t.\nThe weight of a word w \u2208 L(G\u03c0) is then given by\n\u03c0(w) = \u220f\nt\u2208\u03a3\n\u03c0 |w|t t\nand extended into a probability distribution over L(G)n by\np\u03c0,n(w) = \u03c0(w) \u2211\nw\u2208L(G)n \u03c0(w)\n.\n2.2. Rule-Weighted Grammars.\nDefinition 2.2 ((Rule)-Weighted Grammar [11]). A rule-weighted grammar G\u03bb is a 5-tuple G\u03bb = (\u03bb,\u03a3,N ,P ,S) where:\n\u2022 (\u03a3,N ,P ,S) defines a context-free grammar, \u2022 \u03bb : P \u2192 R+ is a rule-weighting function that associates a positive nonnull real-valued1 weight \u03bbr to each derivation r \u2208 P , using the notation N \u2192y X to indicate the association of a weight \u03bbr = y to a rule r = (N \u2192 X).\nThe weight function \u03bb can then be extended multiplicatively over L(G\u03bb) through\n\u03bb(w) = \u220f\nr\u2208dw r=(N\u2192\u03bbrX)\n\u03bbr, \u2200w \u2208 L(G\u03bb)\nwhere dw is the (unique) parse tree of w in G\u03bb. This induces a probability distribution over L(G\u03bb)n such that\np\u03bb,n(w) = \u03bb(w) \u2211\nw\u2208L(G\u03bb)n \u03bb(w)\n."}, {"heading": "3. Any terminal-weighted distribution can be obtained using a rule-weighted grammar", "text": "Theorem 3.1. For any terminal-weighted grammar G\u03c0, there exists a rule-weighted grammar G\u03bb, L(G\u03c0) = L(G\u03bb), inducing an identical probability distribution.\nProof. We give a constructive proof of this theorem. For any grammar G\u03c0 = (\u03c0,\u03a3,N ,P ,S), let us consider the rule-weighted grammar defined by G\u03bb := (\u03bb,\u03a3,N ,P ,S), such that \u03bb(N \u2192 t) = \u03c0t and \u03bb(\u00b7) = 1 otherwise.\nClearly, the production rules and axioms of G\u03bb and G\u03c0 are identical, therefore one has L(G\u03bb) = L(G\u03c0) and, in particular,\nL(G\u03bb)n = L(G\u03c0)n, \u2200n \u2265 0.\n1More precisely, Weinberg et al restrict their formalism to rational weights, based on the rationale that real-numbers would lead to unstable computations. However their framework could easily be extended to any computable real numbers without loss of precision, e.g. by implementing a confidence intervals approach described in Denise and Zimmermann [5], therefore we consider a trivial extension of this formalism here.\nLet us now remark that any terminal letter t in a produced word w results from the application of a rule of the form N \u2192 t, and that the parse trees in G\u03c0 and G\u03bb of any word w \u2208 L(G\u03c0) = L(G\u03bb) are identical. It follows that the occurrences of the terminal letter t in w are in bijection with the occurrences of the N \u2192 t rule in its parse tree dw, and therefore\n\u03bb(w) = \u220f\nr=(N\u2192\u03bbr t)\u2208dw\n\u03bbr = \u220f\n(N\u2192\u03bbr t)\u2208dw\n\u03c0t = \u03c0(w), \u2200w \u2208 L(G\u03c0).\nSince L(G\u03bb)n = L(G\u03c0)n, then one has \u2211\nw\u2208L(G\u03bb)n\n\u03bb(w) = \u2211\nw\u2208L(G\u03c0)n\n\u03c0(w),\nand we conclude that, for any length n \u2265 0, one has\np\u03c0,n(w) = \u03bb(w) \u2211\nw\u2208L(G\u03bb)n \u03bb(w)\n= \u03c0(w) \u2211\nw\u2208L(G\u03c0)n \u03c0(w)\n= p\u03bb,n(w)\nwhich proves our claim."}, {"heading": "4. Any rule-weighted distribution can be obtained using a terminal-weighted grammar", "text": "Theorem 4.1. For any rule-weighted grammar G\u03bb, there exists a terminal-weighted grammar G\u03c0, L(G\u03bb) = L(G\u03c0), inducing an identical probability distribution.\nProof. Let us first remind the definition of the Greibach Normal Form (GNF), which requires each production rule to be of the form:\n\u2022 S \u2192 \u03b5, where S is the axiom, \u2022 N \u2192 t.X , where t \u2208 \u03a3 and X \u2208 {\u03a3 \u222aN/{S}}\u2217.\nBased on Lemma 4.2 proven below, we know that any rule-weighted grammar in Chomsky-Normal Form can be transformed into a GNF grammar that generates the same language and induces the same distribution. Let us then assume, without loss of generality, that the input grammar G\u03bb = (\u03bb,\u03a3,N ,P ,S) is in GNF.\nBy duplicating the vocabulary, one easily builds a terminal-weighted grammar that induces the same probability distribution as G\u03bb. Namely, let us define G\u03c0 = (\u03c0,\u03a3\u03c0,N ,P\u03c0,S) such that \u03a3\u03c0 := {tr}r\u2208P , \u03c0(tr) := \u03bb(r), and\nP\u03c0 := {N \u2192 tr.X | r = (N \u2192x t.X) \u2208 P} \u222a {S \u2192 \u03b5 | S \u2192x \u03b5 \u2208 P}.\nClearly, each terminal letter in a word produced by G\u03c0 can be unambiguously associated with a rule of G\u03bb, therefore the weight of any non-empty word is preserved. Furthermore, the generated languages of G\u03bb and G\u03c0 are identical, so the distribution is preserved. Finally, the weight of the empty word \u03b5, implicitly set to 1 in the new grammar, may generally differ from its original value \u03bb(S \u2192 \u03b5) in G\u03bb. However, \u03b5 is the only word of length 0, and therefore has probability 1 in both grammars. We conclude that the probability distribution induced by G\u03bb is the same as that of G\u03c0.\nLemma 4.2. For any rule-weighted grammar G\u03bb = (\u03bb,\u03a3,N ,P ,S), there exists a grammar H\u03bb\u2032 in Greibach Normal Form inducing the same distribution.\nProof. Again, we use a constructive proof, showing that the weight distribution can be preserved during the transformation of the grammar performed by the Blum and Koch normalisation algorithm [1]. Let us state the algorithm:\n(1) Renumber non-terminals in any order, starting with the Axiom S \u21d2 N1. (2) For k = 1 to |N |, consider the non-terminal Nk:\n(a) For each r = (Nk \u2192x Nj.X) \u2208 P , such that j < k and\nNj \u2192x1 X1, Nj \u2192x2 X2, \u00b7 \u00b7 \u00b7 , Nj \u2192xm Xm,\nreplace r in P as follows\nFormer rule(s) New rule(s)\nNk \u2192x Nj .X\nNk \u2192x\u00b7x1 X1.X Nk \u2192x\u00b7x2 X2.X ... Nk \u2192x\u00b7xm Xm.X.\n(b) Fix any left-recursive non-terminal Nk by replacing its rules as follows, using an alternative chain-rule construct:\nFormer rule(s) New rule(s)\nNk \u2192x1 Nk.X1 N \u2032k \u2192x1 X1.N \u2032 k\nN \u2032k \u2192x1 X1 ... ...\nNk \u2192xm Nk.Xm N \u2032k \u2192xm Xm.N \u2032 k\nN \u2032k \u2192xm Xm\nNk \u2192y1 Y1 N \u2032k \u2192y1 Y1.N \u2032 k\nN \u2032k \u2192y1 Y1 ... ...\nNk \u2192ym\u2032 Ym\u2032 N \u2032k \u2192ym\u2032 Ym.N \u2032 k\nN \u2032k \u2192ym\u2032 Ym\u2032\n(3) For k = |N | down to 1, consider the non-terminal Nk: (a) For each r = (Nk \u2192x Nj.X) \u2208 P , such that j > k and\nNj \u2192x1 X1, Nj \u2192x2 X2, \u00b7 \u00b7 \u00b7 , Nj \u2192xm Xm,\nreplace r in P as follows\nFormer rule(s) New rule(s)\nNk \u2192x Nj .X\nNk \u2192x\u00b7x1 X1.X Nk \u2192x\u00b7x2 X2.X ... Nk \u2192x\u00b7xm Xm.X.\nOne easily verifies that, after any iteration of step (2), the grammar no longer contains any rule Nj \u2192 Nl.X such that l \u2264 j \u2264 k. This holds for Nk which, after the full execution of step (2), does not depend from any non-terminal, and is therefore in GNF. Furthermore, one may assume that, anytime a non-terminal Nk is considered during step (3), every Nj such that k < j is in GNF. Consequently, the expansion of Nj only creates rules that are GNF-compliant, thus Nk is in GNF at the end of the iteration.\nLet us denote by H\u03bb\u2032 = (\u03bb\u2032,\u03a3,N \u2032,P \u2032,S) the rule-weighted grammar obtained at the end of the execution. One first remarks that both the expansions (Steps (2.a) and (3.a)) and the chain-rule reversal (Steps (2.b)) preserve the generated language,\nso that the language generated by a non-terminal in G\u03bb is also the language generated by its corresponding non-terminal in H\u03bb\u2032 . Furthermore, one can prove, by induction on the number of derivations required to generate a word, that the induced probability distribution is kept invariant by rules substitutions operated by the algorithm.\nTo that purpose, let us first extend the definition of a rule-weighting function to include a partial derivation instead of a single non-terminal. Namely, \u03bbX(w) will represent the weight of w, as derived from X \u2208 {P \u222a\u03a3}\u2217 and, in particular, one has \u03bbS \u2261 \u03bb. Let us now consider the rule-weighting functions \u03bb\u25ee and \u03bb\u25ed, respectively induced by the grammar before and after a modification:\n\u2022 Induction hypothesis: Any word w generated from any X{P \u222a\u03a3}\u2217 using d derivations, 1 \u2264 d < n, is such that \u03bb\u2032X(w) = \u03bb \u2032\u2032 X(w). \u2022 Rule expansion (Steps (2.a) and (3.a)): Consider a word w, generated using n derivations from some non-terminal N . Clearly, if N 6= Nk or if the first derivation used is N \u2192 X \u2032 6= Nj .X , then the rule used to generated w is not affected by the modification. The induction hypothesis applies and one trivially gets \u03bb\u25eeNk(w) = \u03bb \u25ed Nk (w).\nConsider the initial state of the grammar. When w results from a derivation N \u2192x Nj .X , then there exists a (unique) decomposition w = w\u2032.w\u2032\u2032, where w\u2032 is produced by the application of some rule Nj \u2192xi Xi, and w\u2032\u2032 is derived from X . The weight of w is then given by \u03bb\u25eeNk(w) = x \u00b7 xi \u00b7 \u03bb \u25ee\nXi (w\u2032) \u00b7 \u03bb\u25eeX(w \u2032\u2032). In the modified version of the grammar, w = w\u2032.w\u2032\u2032 unambiguous derives from an application of the new rule Nk \u2192x\u00b7xi Xi.X (w \u2032 \u2208 L(Xi) and w\u2032\u2032 \u2208 L(X)), with associated weight \u03bb\u25edk (w) = x \u00b7 xi \u00b7 \u03bb \u2032\u2032 k(w \u2032) \u00b7 \u03bb\u25edk (w \u2032\u2032). Since both w\u2032 and w\u2032\u2032 are generated using less than n derivations, then the induction hypothesis applies and one gets\n\u03bb\u25edNk(w) = x \u00b7 xi \u00b7 \u03bb \u25ed Xi (w\u2032) \u00b7 \u03bb\u25edX(w \u2032\u2032) = x \u00b7 xi \u00b7 \u03bb \u25ee Xi (w\u2032) \u00b7 \u03bb\u25eeX(w \u2032\u2032) = \u03bb\u25eeNk(w).\n\u2022 Chain-rule reversal (Steps (2.b)): Any word produced using the initial leftrecursive chain-rule can be uniquely decomposed as w = w\u2032.w\u2032\u20321 . \u00b7 \u00b7 \u00b7 .w \u2032\u2032 p ,\nwhere w\u2032 is generated from some Nk \u2192yq Yq, q \u2208 [1,m \u2032], and each w\u2032\u2032i is generated by some rule Nk \u2192xqi Nk.Xqi , qi \u2208 [1,m]. Its weight is therefore given by \u03bb\u25eeNk(w) = yq \u00b7 ( \u220fm i=1 xqi ) \u00b7 \u03bb \u25ee Yq (w\u2032) \u00b7 ( \u220fm i=1 \u03bb \u25ee Xqi (w\u2032\u2032i ) ) .\nAfter chain-rule reversal, the same decomposition w = w\u2032.w\u2032\u20321 . \u00b7 \u00b7 \u00b7 .w \u2032\u2032 p holds, but the sequence of derivation is now either Nk \u2192yq Yq (w = w \u2032), or\nNk \u2192yq Yq.N \u2032 k \u2192xq1 Yq.Xq1 .N \u2032 k\nYq.Xq1 . \u00b7 \u00b7 \u00b7 .Xqm\u22121 .N \u2032 k\n\u2192xqm Yq.Xq1 . \u00b7 \u00b7 \u00b7 .Xqm w \u2032.w\u2032\u20321 . \u00b7 \u00b7 \u00b7 .w \u2032\u2032 p .\nIn both cases, the induction hypothesis applies for each element of the decomposition, and the weight of w in the new decomposition is given by\n\u03bb\u25edNk(w) = yq \u00b7\n(\nm \u220f\ni=1\nxqi\n)\n\u00b7 \u03bb\u25edYq (w \u2032) \u00b7\n(\nm \u220f\ni=1\n\u03bb\u25edXqi (w\u2032\u2032i )\n)\n= yq \u00b7\n(\nm \u220f\ni=1\nxqi\n)\n\u00b7 \u03bb\u25eeYq (w \u2032) \u00b7\n(\nm \u220f\ni=1\n\u03bb\u25eeXqi (w\u2032\u2032i )\n)\n= \u03bb\u25eeNk(w).\nIt follows that the weight of any word is left unchanged by the substitutions performed in the algorithm. Since the generated language is also preserved, then such a preservation of the weights implies a preservation of the probabilities. We conclude that the returned grammar, in addition to being in GNF, also induces the same probability distribution as G\u03bb."}, {"heading": "5. Conclusion", "text": "Using a trivial modification of the Blum and Koch algorithm [1], we showed that weighting terminal or weighting rules have equal expressive power, i.e. that any distribution captured by the former formalism is also captured by the other and vice-versa.\nWhile both proofs are relatively trivial, going from rule-weighted grammars to terminal-weighted grammars turned out to be more involved than the alternative, leading to an increase of the number of rules. However, this observation might be deceptive, as the choice of the Greibach Normal Form as an intermediate form is only one out of possibly many alternatives, and one could devise more efficient grammar transforms capturing the same distributions. Moreover, it is noteworthy that, even if one chooses to use GNF grammars, there still seems to be a gap between the O(|P|4) size of the grammar returned by the Blum and Koch algorithm, and the minimum O(|P|2) increase observed for some infinite family of grammars, motivating the search for better GNF transformation algorithms."}], "references": [{"title": "Greibach normal form transformation revisited", "author": ["Norbert Blum", "Robert Koch"], "venue": "Information and Computation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Multi-dimensional Boltzmann sampling of languages, Proceedings of AOFA\u201910 (Vienna)", "author": ["O. Bodini", "Y. Ponty"], "venue": "Discrete Mathematics and Theoretical Computer Science Proceedings,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Controlled non uniform random generation of decomposable structures", "author": ["A. Denise", "Y. Ponty", "M. Termier"], "venue": "Theoretical Computer Science", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Random generation of words of context-free languagesaccording to the frequencies of letters, Mathematics and Computer Science: Algorithms,Trees, Combinatorics and probabilities (D", "author": ["A. Denise", "O. Roques", "M. Termier"], "venue": "Gardy and A. Mokkadem, eds.), Trends in Mathematics, Birkhau\u0308ser,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Uniform random generation of decomposable structures using floating-point arithmetic", "author": ["A. Denise", "P. Zimmermann"], "venue": "Theor. Comput. Sci", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "A statistical sampling algorithm for RNA secondary structure prediction", "author": ["Y. Ding", "E. Lawrence"], "venue": "Nucleic Acids Research", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Random sampling from Boltzmann principles, Automata, Languages, and Programming (P", "author": ["P. Duchon", "P. Flajolet", "G. Louchard", "G. Schaeffer"], "venue": "Widmayer et al., ed.), Lecture Notes in Computer Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Calculus for the random generation of labelled combinatorial structures", "author": ["P. Flajolet", "P. Zimmermann", "B. Van Cutsem"], "venue": "Theoretical Computer Science", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "A generic approach for the unranking of labeled combinatorial classes", "author": ["C. Martinez", "X. Molinero"], "venue": "Random Structures & Algorithms,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "GenRGenS: Software for generating random genomic sequences and structures, Bioinformatics", "author": ["Y. Ponty", "M. Termier", "A. Denise"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Non uniform generation of combinatorial objects, Tech. report", "author": ["F. Weinberg", "M.E. Nebel"], "venue": "Technical Report University of Kaiserslauter,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "A unified setting for sequencing, ranking, and selection algorithms for combinatorial objects", "author": ["H.S. Wilf"], "venue": "Advances in Mathematics", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1977}], "referenceMentions": [{"referenceID": 11, "context": "Following general principles outlined by Wilf [12], Flajolet et al [8] proposed a fully-automated algebraic approach for the extensive class of decomposable combinatorial objects, a large class of objects that includes context-free languages.", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "Following general principles outlined by Wilf [12], Flajolet et al [8] proposed a fully-automated algebraic approach for the extensive class of decomposable combinatorial objects, a large class of objects that includes context-free languages.", "startOffset": 67, "endOffset": 70}, {"referenceID": 6, "context": "This pioneering work was later completed by the introduction of Boltzmann samplers, an alternative family of random generation algorithms based on analytical properties of the underlying generating functions [7].", "startOffset": 208, "endOffset": 211}, {"referenceID": 5, "context": "in RNA bioinformatics [6]) require non-uniform distributions to be modeled.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "To that purpose, Denise et al [4] introduced (terminal)-weighted grammars, a non-uniform framework where the terminal symbols (letters) are associated with a real positive value, inherited multiplicatively by words in the language.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "Generic random generation algorithms were proposed [4] and implemented within a general random generation toolbox [10].", "startOffset": 51, "endOffset": 54}, {"referenceID": 9, "context": "Generic random generation algorithms were proposed [4] and implemented within a general random generation toolbox [10].", "startOffset": 114, "endOffset": 118}, {"referenceID": 2, "context": "Analytic and numerical approaches were proposed for figuring out suitable set of weights that would mimic a given, observed, distribution [3].", "startOffset": 138, "endOffset": 141}, {"referenceID": 1, "context": "Finally, a multidimensional rejection scheme was explored to sample words of a given composition, yielding efficient algorithms by generalizing the principles of Boltzmann sampling [2].", "startOffset": 181, "endOffset": 184}, {"referenceID": 10, "context": "More recently, Weinberg et al [11] proposed an alternative definition for weighted grammars, associating positive real-values to rules instead of terminal letters.", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "authors proposed a random generation procedure based on formal grammar manipulations, followed by a call to an unranking algorithm due to Martinez and Molinero [9].", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "1 ((Terminal)-Weighted Grammar [4]).", "startOffset": 31, "endOffset": 34}, {"referenceID": 10, "context": "2 ((Rule)-Weighted Grammar [11]).", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "by implementing a confidence intervals approach described in Denise and Zimmermann [5], therefore we consider a trivial extension of this formalism here.", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "Again, we use a constructive proof, showing that the weight distribution can be preserved during the transformation of the grammar performed by the Blum and Koch normalisation algorithm [1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 0, "context": "Using a trivial modification of the Blum and Koch algorithm [1], we showed that weighting terminal or weighting rules have equal expressive power, i.", "startOffset": 60, "endOffset": 63}], "year": 2013, "abstractText": "Two formalisms have recently been proposed to perform a nonuniform random generation of combinatorial objects based on context-free grammars. The former, introduced by Denise et al, associates weights with letters, while the latter, recently explored by Weinberg et al in the context of random generation, associates weights to transitions. In this short note, we use a trivial modification of the Greibach Normal Form transformation algorithm, due to Blum and Koch, to show the equivalent expressivities of these two formalisms.", "creator": "LaTeX with hyperref package"}}}