{"id": "1205.2602", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "The Entire Quantile Path of a Risk-Agnostic SVM Classifier", "abstract": "A quantile verb chuckwalla uses long decades: Classify] so + 1 if P (Y = 1 | X = b) & pbr; = w, and more - 0 usually, the puts transactions belyi \u03c9 microsoft {[ vs., 1 ]. It widely fact similar as Support Vector Machines (SVMs) day though limit are sinc classifiers a t = fifth / {. In this paper, n't it come by it polygon cost over gortyna SVMs certain without appropriately came would saved, 1998 where rate, over quantile probability end-product have any logo. We place more has honesty checksum however solve same completion SVM torlakian part all terms including pants and. This recent seven weaknesses: First, to can recover of except conditional larger P (Y = 1997 | X = 52) = does this / {[ 0, 8 ]. Second, too go build another risk - introvert SVM classifier several similar for under misclassification need does probably a codice_43. Preliminary parameter brain show similar factors beyond all implementation non-linear.", "histories": [["v1", "Wed, 9 May 2012 18:46:51 GMT  (356kb)", "http://arxiv.org/abs/1205.2602v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jin yu", "s v n vishwanatan", "jian zhang"], "accepted": false, "id": "1205.2602"}, "pdf": {"name": "1205.2602.pdf", "metadata": {"source": "CRF", "title": "The Entire Quantile Path of a Risk-Agnostic SVM Classifier", "authors": ["Jin Yu", "S.V.N. Vishwanathan", "Jian Zhang"], "emails": ["jin.yu@anu.edu.au", "jianzhan}@stat.purdue.edu"], "sections": [{"heading": null, "text": "A quantile binary classifier uses the rule: Classify x as +1 if P (Y = 1|X = x) \u2265 \u03c4 , and as \u22121 otherwise, for a fixed quantile parameter \u03c4 \u2208 [0, 1]. It has been shown that Support Vector Machines (SVMs) in the limit are quantile classifiers with \u03c4 = 12 . In this paper, we show that by using asymmetric cost of misclassification SVMs can be appropriately extended to recover, in the limit, the quantile binary classifier for any \u03c4 . We then present a principled algorithm to solve the extended SVM classifier for all values of \u03c4 simultaneously. This has two implications: First, one can recover the entire conditional distribution P (Y = 1|X = x) = \u03c4 for \u03c4 \u2208 [0, 1]. Second, we can build a risk-agnostic SVM classifier where the cost of misclassification need not be known apriori. Preliminary numerical experiments show the effectiveness of the proposed algorithm."}, {"heading": "1 Introduction", "text": "Support Vector Machines (SVMs) have emerged as a popular tool for binary classification. Given a set of n training instances xi and their corresponding labels yi \u2208 {\u00b11} the task of training a linear SVM classifier1 can be cast as a regularized risk minimization problem:\nminJ(w) := \u03bb 2 \u2016w\u20162 + 1 n n\u2211 i=1 l(w>xi, yi), (1)\nwhere l is the so-called hinge loss defined by\nl(w>x, y) := max(0, 1\u2212 yw>x). (2) 1For ease of exposition we will stick to linear SVMs, although all our results extend to the non-linear case where one maps the data into an RKHS H via the map x\u2192 \u03c6(x) and uses the kernel k(x,x\u2032) = \u3008\u03c6(x), \u03c6(x\u2032)\u3009H.\nIt is obvious that the hinge loss l(\u00b7) is non-negative and convex in w. The loss function measures the discrepancy between y and the prediction given by sign(w>x), while the L2 regularizer with regularization constant \u03bb > 0 controls the complexity of the solution w. It has been shown (Lin, 2002) that the minimizer of the hinge loss is exactly sign(\u03b7(x)\u22121/2), where \u03b7(x) = P (Y = 1|X = x) is the probability of Y = 1 conditioned on X = x. Thus the SVM solution should approach the Bayes rule as the sample size gets large with appropriately chosen function class.\nOne problem of the standard SVM is that even though we can use the resulting SVM classifier w\u0302 = argminw J(w) to classify a new observation x, the prediction w>x does not have probabilistic interpretation. More importantly, the SVM classification results cannot be directly applied to situations where the misclassification cost is asymmetric, i.e. when the cost of a false positive error is different from that of a false negative error. To address such a problem, several methods have been proposed to convert the SVM output into well-calibrated probabilistic scores, such as (Platt, 2000). However, such methods either rely on parametric assumption or lack theoretical justification about the transformed scores.\nWe instead aim to estimate the quantity sign(\u03b7(x)\u2212\u03c4), which we call the quantile classification rule with \u03c4 \u2208 [0, 1] as the quantile parameter. It can be shown that sign(\u03b7(x)\u2212\u03c4) is the minimizer of the asymmetric hinge loss, which assigns different costs to false positive and false negative errors. The SVM formulation with the asymmetric hinge loss can be defined as\nmin \u03bb\n2 \u2016w\u20162 + n\u2211 i=1 c\u03c4yi max(0, 1\u2212 yiw >xi). (3)\nHere, c\u03c4yi controls the two types of misclassification cost, and for reasons that will become apparent\nshortly, we set\nc\u03c4yi := { [2(1\u2212 \u03c4)]/n if yi = +1, (2\u03c4)/n if yi = \u22121.\n(4)\nfor \u03c4 \u2208 [0, 1]. Note that when \u03c4 = 12 , i.e., the misclassification costs are symmetric, we recover (1).\nThere are many natural applications where the cost of misclassification is not known in advance until the classifier is deployed. As an illustrative example consider the problem of spam detection (also see Figure 1). Given training emails the learning task is to distinguish between spam and non-spam emails. The tolerance of a user to spam is influenced by various factors. For instance, a busy professor might have a very low tolerance for spam. In other words, he/she might not mind losing a few genuine emails as long as all spam emails are kept out of his/her inbox. On the other hand, a not-so-busy graduate student might not mind a few spam emails as long as genuine emails are not lost in the junk mail folder. In such cases, the brute force approach of training a classifier for every user preference is both tedious and time consuming. Furthermore, one needs to train a new classifier for every user preference.\nIn this paper we present a principled algorithm to solve (3) for all values of \u03c4 simultaneously by utilizing the fact that the solution path is piecewise linear as a function of the quantile parameter \u03c4 \u2208 [0, 1]. In other words, once our classifier is trained, we can recover the solution for any \u03c4 efficiently. Consequently, our classifier is risk agnostic. Furthermore, we show that (3) is an instance of the quantile classification problem, with \u03c4 being the quantile parameter.\nThe rest of the paper is organized as follows: In Section 2 we establish the connection between the asymmetric cost SVM and the quantile classification rule. In Section 3 we formulate the dual of (3). In Section 4 we describe the proposed algorithm and its worst case time complexity analysis. We discuss some related work in Section 5. Numerical experiments are presented in Section 6, and we conclude the paper with an outlook and discussion in Section 7."}, {"heading": "2 Statistical Underpinnings", "text": "Let (X, Y ) be a pair of random variables with training instances X \u2208 X and labels Y \u2208 {\u00b11}. For any realization x of X, denote the conditional probability P (Y = 1|X = x) as \u03b7(x). Furthermore, let C+ (resp. C\u2212) denote the cost of misclassifying a x labeled as +1 (resp. \u22121). The cost sensitive classification risk of\na decision function g : X \u2192 {\u00b11} is defined as\nR(g) := C+P (Y = \u22121, g(X) = 1) (5) + C\u2212P (Y = 1, g(X) = \u22121).\nThe following lemma follows from elementary Bayes decision theory (see e.g., Section 2.2 of Duda et al., 2001).\nLemma 2.1 For any decision function g\nR(sign(\u03b7(x)\u2212 \u03c4)) \u2264 R(g),\nwhere \u03c4 = C+C++C\u2212 \u2208 [0, 1].\nLemma 2.1 says that when the misclassification cost is asymmetric, the classifier which leads to the minimum risk should take the form sign(\u03b7(x)\u2212 \u03c4) where \u03c4 only depends on the ratio of the misclassification costs C+/C\u2212. The following lemma, whose proof can be found in Appendix A, shows that sign(\u03b7(x)\u2212 \u03c4) is the minimizer of the asymmetric hinge loss.\nLemma 2.2 For any \u03c4 \u2208 [0, 1], the minimizer f\u2217 of EX,Y [ ((1 + Y )/2\u2212 \u03c4Y ) (1\u2212 Y f(X))+ ] (6)\ntakes the form f\u2217(x) = sign(\u03b7(x)\u2212 \u03c4).\nIn the infinite sample case, if we let \u03bb \u2192 0 as n \u2192 \u221e in (3), it is easy to see that the regularized risk converges to (6). Therefore, Lemma 2.2 implies that the estimator obtained by minimizing the regularized risk (3) is risk consistent.\nThe above observation has a number of consequences. First, it shows that the usual SVM with the symmetric hinge loss (2) estimates P (Y = 1|X = x) = 12 , a well known result (see e.g., Lin, 2002; Sollich, 2002). It also shows that the SVM with the asymmetric hinge loss (4) is essentially a quantile estimator. This result has been hinted many times (e.g. Grandvalet et al., 2006), but to the best of our knowledge, not proven rigorously."}, {"heading": "3 Dual Formulation", "text": "Similar to the case of standard SVM, we can rewrite (3) as a constrained optimization problem:\nmin \u03bb\n2 \u2016w\u20162 + n\u2211 i=1 c\u03c4yi\u03bei (7)\ns.t. \u03bei \u2265 0, \u03bei \u2265 1\u2212 yiw>xi, \u2200i\nallows us to derive its dual as:\nmin D(\u03b1) := 1 2\u03bb \u03b1>Q\u03b1\u2212\u03b1>1 (8)\ns.t. 0 \u2264 \u03b1i \u2264 c\u03c4yi , \u2200i.\nwhere 1 is a vector of all ones and Qij = yiyjx>i xj . Let \u03b1\u03c4 denote the optimal solution to (8) for a given \u03c4 . Then the primal solution w\u03c4 can be recovered via the primal-dual connection:\nw\u03c4 = 1 \u03bb n\u2211 i=1 \u03b1\u03c4i yixi. (9)\nThe dual problem is a quadratic problem with box constrains, which can be solved by various optimization techniques (see e.g., Byrd et al., 1995; More\u0301 and Toraldo, 1989)\nDefine \u2207iD(\u03b1\u03c4 ) as the ith element of the gradient:\n\u2207D(\u03b1\u03c4 ) = 1 \u03bb Q\u03b1\u03c4 \u2212 1; (10)\nand let L, M, andR index entries of the dual solution \u03b1\u03c4 such that\nL := {i : \u2207iD(\u03b1\u03c4 ) < 0} = {i : yiw\u03c4>xi < 1}, M := {i : \u2207iD(\u03b1\u03c4 ) = 0} = {i : yiw\u03c4>xi = 1}, (11) R := {i : \u2207iD(\u03b1\u03c4 ) > 0} = {i : yiw\u03c4>xi > 1},\nwhere the connection with the primal parameter w\u03c4 is made via (9). It is easy to find that L, M, and R in fact index the data xi which are in error, on the margin, and well-classified, respectively. Furthermore, it follows from the KKT conditions (see Appendix B) that the optimal dual solution \u03b1\u03c4 satisfies\n\u03b1\u03c4i =  c\u03c4yi if i \u2208 L, [0, c\u03c4yi ] if i \u2208M, 0 if i \u2208 R .\n(12)\nGiven index sets, I and J , let \u03b1\u03c4I be a vector of \u03b1\u03c4i with i \u2208 I and QI J a submatrix of Q taking entries Qij with i \u2208 I and j \u2208 J . Then, we can define L+1 := L\u2229{i : yi = +1} and L\u22121 := L\\L+1, and use (12) to\ndecompose \u03b1\u03c4 into [\u03b1\u03c4>L+ , \u03b1 \u03c4> L\u2212 , \u03b1 \u03c4> M , 0 >]>, where 0 is a vector of all zeros. Using (10), we can get a decomposed view of \u2207D(\u03b1\u03c4 ):\n[\u2207LD(\u03b1\u03c4 )>,\u2207MD(\u03b1\u03c4 )>,\u2207RD(\u03b1\u03c4 )>]> := (13)\n1 \u03bb  c\u03c4+1QLL+11+c\u03c4\u22121QLL\u221211+QLM\u03b1\u03c4Mc\u03c4+1QML+11+c\u03c4\u22121QML\u221211+QMM\u03b1\u03c4M c\u03c4+1QRL+11+c \u03c4 \u22121QRL\u221211+QRM\u03b1 \u03c4 M \u2212 1 Since \u2207MD(\u03b1\u03c4 ) = 0 by the definition (11) ofM, we can use (13) to get a closed form representation of \u03b1\u03c4M:\n\u03b1\u03c4M = Q \u22121 MM[\u03bb1\u2212 c \u03c4 +1QML+11\u2212 c\u03c4\u22121QML\u221211]. (14)\nNote that Q\u22121MM does not exist when QMM is rank deficient. Nevertheless, standard optimization techniques, such as the conjugate gradient method (Nocedal and Wright, 1999), should always recover \u03b1\u03c4M as a solution to a linear system.\nAn important observation from (4) is that the upper bound c\u03c4yi only changes linearly with \u03c4 : As we increase the quantile parameter \u03c4 to \u03c4 + , we have\nc\u03c4+ yi = c \u03c4 yi \u2212 yi \u2206c , where \u2206c := 2 n . (15)\nAssume an deviation from \u03c4 does not change the index sets defined in (11), then (14) still holds for \u03b1\u03c4+ M . Therefore, we can use (15) to expand it as:\n\u03b1\u03c4+ M = \u03b1 \u03c4 M +\u2206c \u2206\u03b1\u03c4M, where (16)\n\u2206\u03b1\u03c4M := Q \u22121 MM[QML+11\u2212QML\u221211].\nThe optimality condition (12) then allows us to recover \u03b1\u03c4+ from \u03b1\u03c4 via:\n\u03b1\u03c4+ L+1 \u03b1\u03c4+ L\u22121 \u03b1\u03c4+ M \u03b1\u03c4+ R\n = \n\u03b1\u03c4L+1 \u2212\u2206c \u03b1\u03c4L\u22121 +\u2206c\n\u03b1\u03c4M +\u2206c \u2206\u03b1\u03c4M\n0\n . (17)\nProposition 3.1 For the dual of the quantile classification problem (8), there exists a set of quantiles {\u03c4k}Kk=1, \u03c4k \u2208 [0, 1], such that we can find a solution path \u03b1\u03c4 that is continuous in \u03c4 , and linear in \u03c4,\u2200\u03c4 \u2208 (\u03c4k, \u03c4k+1).\nSee Appendix C for the proof of Proposition 3.1.\nProposition 3.1 shows that \u03b1\u03c4 is piecewise linear in \u03c4 . Using (13) and (17), we can see that the gradient \u2207D(\u03b1\u03c4 ) has the same property. In particular, \u2200 \u2208 (0, \u03c4k+1 \u2212 \u03c4k), we have \u2207MD(\u03b1(\u03c4k+ )) = 0 and\n\u2207ND(\u03b1(\u03c4k+ )) = \u2207ND(\u03b1\u03c4k)\n+ \u2206c\n\u03bb [QN L\u221211\u2212QN L+11+QN M \u2206\u03b1 \u03c4k M], (18)\nwhereM is the margin index set associated with \u03b1\u03c4k+ and N := L\u222aR is the complement set ofM."}, {"heading": "4 Finding the Dual Solution Path", "text": "It follows from Proposition 3.1 that if we can find a set of quantile parameters: K := {\u03c4k}Kk=1, that divide the interval [0, 1] into regions so that within these regions \u03b1\u03c4 changes linearly with \u03c4 , i.e., the index sets: L,M, and R remain fixed. Then we can quickly recover \u03b1\u03c4 for any value of \u03c4 from a \u03b1\u03c4k via (17). In what follows we present our algorithm (Algorithm 1) that is able to identify all \u03c4k, which we call kinks."}, {"heading": "4.1 The Algorithm", "text": "Our goal is to construct a sorted list of kinks {\u03c4k}Kk=1, at which one of the following events happens:\n1. Elements in N , i.e., not inM, move toM, 2. Elements inM move to L, 3. Elements inM move to R.\nTo this end, our algorithm starts with \u03c4 = 0, and then moves forward toward \u03c4 = 1 to identify all values of \u03c4 that alter the membership of an index.\nGiven a quantile parameter \u03c4k, its corresponding optimal dual solution \u03b1\u03c4k , and the associated index sets L,M, and R, we know from the definition (11) that \u2207iD(\u03b1\u03c4k) 6= 0, \u2200i \u2208 N . This means that Event 1 happens when an > 0 deviation from \u03c4k just turns a nonzero element of the gradient to zero, i.e., \u2207iD(\u03b1\u03c4k+ ) = 0, i \u2208 N . We immediately see from (18) that the deviation that leads to Event 1 is:\ntoM = min{ i : i > 0}i\u2208N , where (19)\ni := n\n2\n[ \u2212\u03bb\u2207iD(\u03b1\u03c4k )\nQiL\u221211\u2212QiL+11+QiM \u2206\u03b1 \u03c4k M\n] .\nWe know from the optimality condition (12) that an index i from M is just about to move into L (Event 2), when \u03b1(\u03c4k+ )i = c (\u03c4k+ ) yi . Expanding both sides of the last equation, using (15) and (16), shows that satisfies\n\u03b1\u03c4ki + 2 n \u2206\u03b1\u03c4ki = c \u03c4k yi \u2212 yi 2 n , i \u2208M . (20)\nHere care must be taken when \u03b1\u03c4ki = c \u03c4k yi , i \u2208 M, i.e., \u03b1\u03c4ki is on the boundary between L and M. In this case (20) can be reduced to \u2206\u03b1\u03c4ki = \u2212 yi; and if \u2206\u03b1\u03c4ki > \u2212yi, then an arbitrarily small > 0 will cause \u03b1 (\u03c4k+ ) i > c (\u03c4k+ ) yi , i.e., pushing the index i out toward L. Taking this boundary case into consideration, we determine a candidate using the following criteria:\ntoL = min{ i : i \u2265 0}i\u2208M, where (21)\ni :=  0 if \u03b1\u03c4ki = c \u03c4k yi & \u2206\u03b1 \u03c4k i > \u2212yi, +\u221e if \u03b1\u03c4ki = c\u03c4kyi & \u2206\u03b1 \u03c4k i \u2264 \u2212yi,\nn 2 (c \u03c4k yi \u2212 \u03b1 \u03c4k i )/(\u2206\u03b1 \u03c4k i + yi) otherwise.\nIf toL = 0, we treat \u03c4k as a kink, and update the index sets accordingly:\nM\u2190M\\{itoL}, L \u2190 L\u222a{itoL}, (22) where itoL = {i : i \u2208M, i = toL},\nsuch that the updated index sets coincide with the index sets of the optimal dual solution \u03b1\u03c4 ,\u2200\u03c4 \u2208 (\u03c4k, \u03c4k+1), \u03c4k+1 being the next kink.\nSimilarly, to detect Event 3, we find that satisfies \u03b1\u03c4k+ i = 0, \u2200i \u2208 M, and isolate \u03b1 \u03c4k i = 0 case for special treatment:\ntoR = min{ i : i \u2265 0}i\u2208M, where (23)\ni :=  0 if \u03b1\u03c4ki = 0 & \u2206\u03b1 \u03c4k i < 0, +\u221e if \u03b1\u03c4ki = 0 & \u2206\u03b1 \u03c4k i \u2265 0,\nn 2 (\u2212\u03b1 \u03c4k i )/(\u2206\u03b1 \u03c4k i ) otherwise.\nIn the case where toR = 0, we should recognize \u03c4k as a kink, and shift the corresponding index from M to R. See Algorithm 1 for detailed implementation."}, {"heading": "4.2 Complexity Analysis", "text": "The time complexity of Algorithm 1 is dominated by the calculation of \u2206\u03b1\u03c4M (16), which involves solving a linear system of size |M |. A standard solver such as the conjugate gradient method converges to the solution of such a linear system in at most O(r|M |2) time, r being the rank of QMM. Having computed \u2206\u03b1\u03c4M, the main cost of finding\ntoM (19) is the O(n| N |) cost of matrix-vector multiplication; and the\nAlgorithm 1 Dual Path Finding (DPF) 1: input data {(xi, yi)}ni=1 and regularizer \u03bb 2: output sorted list of kinks \u03c4k, corresponding dual solution \u03b1\u03c4k and index sets. 3: \u03b10 = argminD(\u03b1), s.t. 0 \u2264 \u03b1i \u2264 c0yi , \u2200i 4: construct L,M and R for \u03b10 via (11) 5: calculate \u2207D(\u03b10) and \u2206\u03b10M 6: \u03c4 \u2190 0 and K \u2190 {(0,\u03b10M,L,M)} 7: while \u03c4 < 1 do 8: \u2190 min{ toM(19), toL(21), toR(23)} 9: update \u2207D(\u03b1\u03c4 ) to \u2207D(\u03b1\u03c4+ ) via (18) 10: update \u03b1\u03c4 to \u03b1\u03c4+ via (17) 11: adjust index sets according to the event that triggers, e.g., if = toL, apply (22) 12: \u03c4 \u2190 \u03c4 + 13: calculate \u2206\u03b1\u03c4M 14: if = 0 then 15: overwrite the last element of K with (\u03c4,\u03b1\u03c4M,L,M) (cf. discussion in Sec. 4.1) 16: else 17: K \u2190 K\u222a{(\u03c4,\u03b1\u03c4M,L,M)} 18: end if 19: end while 20: return K\ntime required to find toL (21) and toR (23) is linear in |M |. Once we find all the kinks, we can recover \u03b1\u03c4 for any \u03c4 \u2208 [0, 1] via (17) in O(n) time by noting that \u03b1\u03c4M = \u03b1 \u03c4k M + \u2206c\n(\u03c4\u2212\u03c4k)\u2206\u03b1\u03c4kM, where \u2206\u03b1\u03c4kM = (\u03b1 \u03c4k+1 M \u2212 \u03b1 \u03c4k M)/\u2206c\n(\u03c4k+1\u2212\u03c4k) andM is associated with \u03b1\u03c4k .\nIn term of memory requirement, saving kink information at Step 17 of Algorithm 1 requires O(|M |) space for \u03b1\u03c4M; and after the initial O(n) cost of saving the entire L andM sets, we only need to keep track of the indices that move into or out of the two sets to recover them from their initial copy. We use the Q matrix in our calculation, e.g., (19). The Q matrix is usually dense; and caching it requires O(n2) space. This can be prohibitively expensive. However, noticing that Q = (Y X)(X>Y ), where Y is a diagonal matrix with labels yi on its diagonal andX = [x>1 , \u00b7 \u00b7 \u00b7 ,x>n ] \u2208 R n\u00d7d is a feature matrix, we can instead cache Y X, which is often very sparse. But, constructing Q from Y X at each step can be computationally expensive. Fortunately, since only the product of Q with a vector v is needed for our calculation, we can calculate it as Qv = Y X(X>Y v) to leverage fast sparse matrixvector product, and hence reduce the computational overhead. Although we do not have a formal bound on the size of | K |, our experiments show that it never exceeds O(n log n)."}, {"heading": "5 Related Work", "text": "Perhaps the closest in spirit to our paper is the work of Hastie et al. (2004), who studied the influence of the\nregularization constant \u03bb on the generalization performance of a binary SVM. They showed that solutions to a SVM training problem is a piecewise linear function of \u03bb. Based on this observation, they proposed an algorithm that finds SVM solutions for all values of \u03bb. The regularization constant controls the balance between the regularization term and the empirical risk in the objective function (1) to prevent a classifier from overfitting the training data. Therefore, it plays an important role in improving prediction accuracy on unseen data. The effect of \u03c4 on the behaviour of a SVM classifier is fundamentally different from that of \u03bb in a sense that \u03c4 determines the trade-off between the true positive rate (TPR) and the true negative rate (TNR) of a classifier by assigning asymmetric costs to false positive and false negative predictions. In applications where an appropriate balance between TPR and TNR is considered to be more important than prediction accuracy, e.g., in medical diagnosis, using a quantile classifier (3) with adjustable \u03c4 may be more desirable.\nAlthough SVM classifiers with built-in asymmetric misclassification costs have been applied to classification problems that are characterized by highly skewed training data and to problems arisen from medical diagnosis (Veropoulos et al., 1999; Morik et al., 1999; Grandvalet et al., 2006), no rigorous statistical properties were established. The misclassification cost is commonly chosen to reflect label proportions of training data or the ratio of false positive cost to false negative cost. From the optimization viewpoint training a SVM with asymmetric costs is very similar to the standard SVM training problem. Hence, optimization software such as SVMperf (Joachims, 2006) and LIBLINEAR (Hsieh et al., 2008) can be used for training. A common strategy to train a SVM classifier with multiple settings of asymmetric costs is to reassign costs, and retrain. Our DPF method exploits the piecewise linearity property of SVM dual solution, and finds the entire solution path in one shot. This allows us to quickly construct a classifier for any choice of misclassification costs in the post-training phrase.\nQuantile regression as an important statistical tool (Koenker and Bassett, 1978) has recently received attention from machine learning community. Takeuchi et al. (2006) showed that a quantile regression problem can be cast as a regularized risk minimization problem:\nmin \u03bb\n2 \u2016w\u20162 + n\u2211 i=1 max(\u03c4(yi \u2212 fi), (1\u2212 \u03c4)(fi \u2212 yi)),\nwhere \u03c4 \u2208 (0, 1) and fi = w>xi. This regression problem is very reminiscent of the quantile classification problem (3) we considered in this paper. In fact, by following the same principle as discussed in Section 4 we can extend our DPF method to find quantile regres-\nsion solutions for all choices of \u03c4 ."}, {"heading": "6 Experiments", "text": "We now evaluate the generalization performance of a quantile classifier for various values of \u03c4 , and compare the time complexity of our DPF method (Algorithm 1) with a state-of-the-art linear SVM classifier: LIBLINEAR version 1.32 (Hsieh et al., 2008). We used the LBFGSB quasi-Newton method of Byrd et al. (1995) to solve the dual problem at Step 3 of the Algorithm 1; and the conjugate gradient method was applied to find \u2206\u03b1\u03c4M at Step 13. We ran DPF without caching the Q matrix.\nOur experiments used three datasets: the UCI diabetes dataset (Asuncion and Newman, 2007), the spam dataset for task A of the ECML/PKDD 2006 discovery challenge,2 and 3 \u00d7 104 worm splice samples from a biological dataset provided by So\u0308ren Sonnenburg.3 Table 1 summarizes the datasets and our parameter settings. In all experiments the regularization parameter was chosen from the set 10{\u22126,\u22125,\u00b7\u00b7\u00b7 ,\u22121}, such that a SVM classifier with symmetric misclassification costs achieves the highest prediction accuracy on the validation set, while generalization performance is reported on the test set. We included a bias b in the decision function: sign f(x) := w>x + b by using the following strategy: xi \u2190 [x>i , 1]>,w \u2190 [w>, b]>.\nAll experiments were carried out on a Linux machine with dual 2.4GHz Intel Core 2 processors and 4GB of RAM. Our Python code is available for download from http://users.rsise.anu.edu. au/\u223cjinyu/Code/DPF.tar.gz.\nOur first set of experiments shows the influence of the quantile \u03c4 on the behaviour of a classifier. As \u03b1\u03c4 changes, the generalization performance of the quantile classifier in terms of TPR (a.k.a. sensitivity) and TNR (a.k.a. specificity) changes accordingly. Figure 2 shows that TPR decreases (but not necessarily strictly decreases) with \u03c4 , while TNR has an opposite trend. This is because increasing \u03c4 corresponds to increasing the false positive cost C+ (cf. Lemma 2.2; see also\n2The original evaluation set (http://www. ecmlpkdd2006.org/challenge.html) was equally divided into training, validation, and test set.\n3http://www.fml.tuebingen.mpg.de/raetsch/ projects/lsmkl\nFigure 1, right), which leads the classifier to recognize more and more instances as negative samples at an expense of a decreasing TPR. At \u03c4 = 0 (resp. \u03c4 = 1), the classifier simply resorts to labeling all the points as + (resp. \u2212). Therefore at these extreme points, the prediction accuracy depends on the proportion of the positive and negative samples in the dataset. For instance, on the splice dataset where 5.5% of the data is labeled as +, we obtain 5.5% accuracy at \u03c4 = 0. For intermediate values of \u03c4 the prediction accuracy depends on cleanliness of the dataset measured as the total percentage of the data which lies at the margin. For instance, on the spam dataset for \u03c4 = 0, around 0.28% of the training samples were at the margin. This number stabilized to about 0.32% for \u03c4 \u2208 (0.0, 0.9], leading to very stable classification accuracy, as can be seen from Figure 2.\nClearly, finding the solution for any value of \u03c4 \u2208 [0, 1] is more time consuming than finding the solution for a fixed \u03c4 . To investigate the excess time spent in this endeavor, we compare the time complexity of our DFP algorithm with one single run of LIBLINEAR, a state of the art linear SVM training algorithm which can handle asymmetric classification costs.4 Our second comparator is the LBFGSB algorithm, which can also be used to train a linear SVM for any fixed value of \u03c4 . The core functions of LIBLINEAR and LBFGSB are implemented in C++ and Fortran, respectively (We called these functions through their Python wrappers.), while our DPF algorithm is implemented in Python, which is inherently 2 to 5 times slower. Therefore, our CPU time comparison is in favor of LIBLINEAR and LBFGSB.\nRecall that our DFP algorithm invokes any linear SVM solver to find the initial solution,5 and then finds the solution path by constructing K. We compute the ratio of the CPU time spent on constructing K to the average time required by LIBLINEAR to find a solution for a given \u03c4 . The averaging is done by running LIBLINEAR (resp. LBFGSB) to compute the solution for \u03c4 = 0.1, 0.2, . . . , 0.9. As shown in Table 2 on the diabetes dataset DPF finds about 2\u00d7103 kinks, spending 2.8\u00d7103 (resp. 28) times of the average LIBLINEAR (resp. LBFGSB) running time. The running time of DPF increases to 3.6\u00d7 103 (resp. 69) times of that required by a typical run of LIBLINEAR (resp. LBFGSB) on the splice dataset where it finds over 2\u00d7 104 kinks. We found empirically that the number of kinks, | K |, increases with the size of training set, n, but is bounded by n log(n).\n4We called LIBLINEAR with input arguments \u2019-s 3 -B 1 -e 1e-3 -w1 (2-2\u03c4) -w-1 (2\u03c4) -c 1/(n\u03bb)\u2019.\n5Although in theory this is true, in practice we find that in the extreme case of \u03c4 = 0, LIBLINEAR\u2019s performance degrades dramatically. Therefore, we exclusively use LBFGSB as an initial solver\nIt is not surprising that DFP is computationally more expensive than a single run of LIBLINEAR and LBFGSB. But as can be seen in Table 2, after one run of DFP, we can recover the solution for any \u03c4 efficiently. For instance, on the spam dataset, this only requires 0.012 seconds, compared to 0.467 seconds (resp. 11.102 seconds) for a single run of LIBLINEAR (resp. LBFGSB)."}, {"heading": "7 Conclusions and Outlook", "text": "In this paper we first show that minimizing the asymmetric hinge loss will lead to a quantile classifier which is risk optimal for asymmetric misclassification costs. We then present an algorithm which finds the entire solution path of a quantile classifier in a principled way. Given the entire solution path, we can construct a classifier for any given asymmetric classification cost very efficiently. Admittedly, our numerical experiments are preliminary. Running conjugate gradient repeatedly to find \u03b1\u03c4M is the main bottleneck in our DFP algorithm. We are exploring decomposition methods, which can take advantage of warm starts to reduce the computational burden. Future work includes extension of our algorithm to quantile regression and to multi-class classification problems."}, {"heading": "Acknowledgements", "text": "NICTA is funded by the Australian Government\u2019s Backing Australia\u2019s Ability and the Centre of Excellence programs. This work is also supported by the IST Program of the European Community, under the FP7 Network of Excellence, ICT-216886-NOE."}, {"heading": "A Proof of Lemma 2.2", "text": "Proof Let Lx(f) be the risk conditioned on X = x:\nLx(f) = E [((1 + Y )/2\u2212 \u03c4Y )(1\u2212 Y f(X))+|X = x] = \u03c4(1\u2212 \u03b7(x))(1 + f(x))+ + (1\u2212 \u03c4)\u03b7(x)(1\u2212 f(x))+,\nthen we only need to show that f\u2217(x) minimizes Lx(f) for any fixed x.\nWe first show that if \u03b7(x) < \u03c4 then the minimizer f\u2217 satisfies f\u2217(x) = \u22121. Suppose not, that is, there exists x0 such that \u03b7(x0) < \u03c4 but f\u2217(x0) 6= \u22121. Let f\u0303(x) be the same as f\u2217(x) except that f\u0303(x0) = \u22121. Using the shorthand f\u2217(x0) = f\u2217, f\u0303(x0) = f\u0303 and \u03b7(x0) = \u03b7, we obtain\nLx0(f \u2217) = \u03c4(1\u2212 \u03b7)(1 + f\u2217)+ + (1\u2212 \u03c4)\u03b7(1\u2212 f\u2217)+ \u2265 (1\u2212 \u03c4)\u03b7[(1\u2212 f\u2217)+ + (1 + f\u2217)+] \u2265 2(1\u2212 \u03c4)\u03b7 = Lx0(f\u0303),\nwhere the last inequality comes from Jensen\u2019s inequality since (.)+ is a convex function. For the first inequality the bound is achieved only if f\u2217 \u2264 \u22121; and for the second inequality the bound is achieved only if f\u2217 \u2208 [\u22121, 1]. Thus when f\u2217 6= \u22121 it leads to a contradiction. A symmetric argument can be used to show that if \u03b7(x) > \u03c4 then f\u2217(x) = 1.\nB KKT Optimality Conditions\nThe Lagrangian of the constrained optimization problem (8) takes the form of\nL(\u03b1,\u03b2,\u03b3) := D(\u03b1)\u2212 n\u2211 i=1 \u03b2i\u03b1i + n\u2211 i=1 \u03b3i(\u03b1i \u2212 c\u03c4yi),\nwhere \u03b2i and \u03b3i are non-negative Lagrange multipliers. The KKT conditions (Nocedal and Wright, 1999) suggest that at optimum (\u03b1\u03c4 ,\u03b2\u2217,\u03b3\u2217) we have\n\u2207iL(\u03b1\u03c4 ,\u03b2\u2217,\u03b3\u2217) = \u2207iD(\u03b1\u03c4 )\u2212 \u03b2\u2217i + \u03b3\u2217i = 0, \u03b2\u2217i \u03b1 \u03c4 i = 0,\n\u03b3\u2217i (\u03b1 \u03c4 i \u2212 c\u03c4yi) = 0,\n0 \u2264 \u03b1\u03c4i \u2264 c\u03c4yi , \u03b2 \u2217 i \u2265 0, \u03b3\u2217i \u2265 0, \u2200i.\nSimple analysis reveals that the above KKT optimality conditions constrain \u03b1\u03c4 to take the form given in (12)."}, {"heading": "C Proof of Proposition 3.1", "text": "Proof Suppose the index sets (11) of \u03b1\u03c4 remain unchanged for all \u03c4 \u2208 (\u03c4k, \u03c4k+1). The linearity of \u03b1\u03c4 in (\u03c4k, \u03c4k+1) follows directly from (17). Let = \u03c4k+1 \u2212 \u03c4 , compute \u03b1\u03c4+ from \u03b1\u03c4 via (17), and let \u03c4k+1 be chosen in such a way that the membership of an index i changes at \u03b1\u03c4+ . This can only happen when \u03b1\u03c4+ i takes its boundary values: 0 or c \u03c4+ yi with \u2207iD(\u03b1\u03c4+ ) = 0, which means either an i \u2208 M is about to leave M, or an i /\u2208 M just moves into M, where M is the margin index set of \u03b1\u03c4 . We now show that \u03b1\u03c4+ is optimal. To show this, we only need to show \u03b1\u03c4+ i is optimal. By construction \u2207iD(\u03b1\u03c4+ ) = 0, and since \u03b1\u03c4+ i only takes 0 or c\u03c4+ yi , the KKT optimality conditions (Appendix B) can be easily satisfied with appropriate choices of \u03b2\u2217i and \u03b3\u2217i , implying that \u03b1 \u03c4+ i is optimal. Hence, \u03b1\n\u03c4+ is optimal. Therefore, we can set \u03b1\u03c4k+1 = \u03b1\u03c4+ , and use it as a starting point to construct subsequent dual solution path via (17). The dual solution path explored in this way is clearly continuous in \u03c4 ."}], "references": [{"title": "A limited memory algorithm for bound constrained optimization", "author": ["R. Byrd", "P. Lu", "J. Nocedal", "C. Zhu"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Byrd et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Byrd et al\\.", "year": 1995}, {"title": "Pattern Classification and Scene Analysis", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": null, "citeRegEx": "Duda et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Duda et al\\.", "year": 2001}, {"title": "The entire regularization path for the support vector machine", "author": ["T. Hastie", "S. Rosset", "R. Tibshirani", "J. Zhu"], "venue": "JMLR, 5:1391\u20131415,", "citeRegEx": "Hastie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2004}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "In Proc. ACM Conf. Knowledge Discovery and Data Mining (KDD). ACM,", "citeRegEx": "Joachims.,? \\Q2006\\E", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Support vector machines and the bayes rule in classification", "author": ["Y. Lin"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Lin.,? \\Q2002\\E", "shortCiteRegEx": "Lin.", "year": 2002}, {"title": "Algorithms for bound constrained quadratic programming problems", "author": ["J.J. Mor\u00e9", "G. Toraldo"], "venue": "Numerische Mathematik,", "citeRegEx": "Mor\u00e9 and Toraldo.,? \\Q1989\\E", "shortCiteRegEx": "Mor\u00e9 and Toraldo.", "year": 1989}, {"title": "Combining statistical learning with a knowledge-based approach - a case study in intensive care monitoring", "author": ["K. Morik", "P. Brockhausen", "T. Joachims"], "venue": "In Proc. Intl. Conf. Machine Learning,", "citeRegEx": "Morik et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Morik et al\\.", "year": 1999}, {"title": "Probabilities for SV machines", "author": ["J. Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "Platt.,? \\Q2000\\E", "shortCiteRegEx": "Platt.", "year": 2000}], "referenceMentions": [{"referenceID": 4, "context": "It has been shown (Lin, 2002) that the minimizer of the hinge loss is exactly sign(\u03b7(x)\u22121/2), where \u03b7(x) = P (Y = 1|X = x) is the probability of Y = 1 conditioned on X = x.", "startOffset": 18, "endOffset": 29}, {"referenceID": 7, "context": "To address such a problem, several methods have been proposed to convert the SVM output into well-calibrated probabilistic scores, such as (Platt, 2000).", "startOffset": 139, "endOffset": 152}, {"referenceID": 5, "context": "The dual problem is a quadratic problem with box constrains, which can be solved by various optimization techniques (see e.g., Byrd et al., 1995; Mor\u00e9 and Toraldo, 1989)", "startOffset": 116, "endOffset": 169}, {"referenceID": 2, "context": "Perhaps the closest in spirit to our paper is the work of Hastie et al. (2004), who studied the influence of the regularization constant \u03bb on the generalization performance of a binary SVM.", "startOffset": 58, "endOffset": 79}, {"referenceID": 6, "context": "Although SVM classifiers with built-in asymmetric misclassification costs have been applied to classification problems that are characterized by highly skewed training data and to problems arisen from medical diagnosis (Veropoulos et al., 1999; Morik et al., 1999; Grandvalet et al., 2006), no rigorous statistical properties were established.", "startOffset": 219, "endOffset": 289}, {"referenceID": 3, "context": "Hence, optimization software such as SVM (Joachims, 2006) and LIBLINEAR (Hsieh et al.", "startOffset": 41, "endOffset": 57}, {"referenceID": 0, "context": "We used the LBFGSB quasi-Newton method of Byrd et al. (1995) to solve the dual problem at Step 3 of the Algorithm 1; and the conjugate gradient method was applied to find \u2206\u03b1M at Step 13.", "startOffset": 42, "endOffset": 61}], "year": 2009, "abstractText": "A quantile binary classifier uses the rule: Classify x as +1 if P (Y = 1|X = x) \u2265 \u03c4 , and as \u22121 otherwise, for a fixed quantile parameter \u03c4 \u2208 [0, 1]. It has been shown that Support Vector Machines (SVMs) in the limit are quantile classifiers with \u03c4 = 1 2 . In this paper, we show that by using asymmetric cost of misclassification SVMs can be appropriately extended to recover, in the limit, the quantile binary classifier for any \u03c4 . We then present a principled algorithm to solve the extended SVM classifier for all values of \u03c4 simultaneously. This has two implications: First, one can recover the entire conditional distribution P (Y = 1|X = x) = \u03c4 for \u03c4 \u2208 [0, 1]. Second, we can build a risk-agnostic SVM classifier where the cost of misclassification need not be known apriori. Preliminary numerical experiments show the effectiveness of the proposed algorithm.", "creator": "TeX"}}}