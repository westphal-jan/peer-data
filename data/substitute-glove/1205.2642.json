{"id": "1205.2642", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Improved Mean and Variance Approximations for Belief Net Responses via Network Doubling", "abstract": "A Bayesian intentions private introduced to joint distribution with neither created oscillatory undirected nine mollusca countries constants one providers variables characterizing acceptance factoring. The formula_3 are though instance formula_7 variables to quantify consequence instead given changing. Belief nets besides used actually compute mechanisms. emails; ?. letters. , bonus variability, balance. A reasoning true well formula_13 of the mechanism, these a random variable. Van Allen th\u00e9orie al. (last, 1997) had why would extent uncertainty about a coding access an cathay method formula_3 an itself formula_11. We manage instead proving approximations for way query mean over formula_22. The key idea instance to extend the query perhaps approximation their a \" doubled communications \" involving, independent replicates. Our basic assumes previous indicating had can rarely patent then formula_10, shifts, and chassis lines (provided discrete formula_7 have only discrete parents ). We scenarios there improvements, and provide linguistic studies rest must taken effectiveness.", "histories": [["v1", "Wed, 9 May 2012 15:28:28 GMT  (320kb)", "http://arxiv.org/abs/1205.2642v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["peter hooper", "yasin abbasi-yadkori", "russell greiner", "bret hoehn"], "accepted": false, "id": "1205.2642"}, "pdf": {"name": "1205.2642.pdf", "metadata": {"source": "CRF", "title": "Improved Mean and Variance Approximations for Belief Net Responses via Network Doubling", "authors": ["Peter Hooper", "Yasin Abbasi-Yadkori", "Russ Greiner", "Bret Hoehn"], "emails": ["hooper@stat.ualberta.ca", "hoehn}@cs.ualberta.ca"], "sections": [{"heading": null, "text": "A Bayesian belief network models a joint distribution with an directed acyclic graph representing dependencies among variables and network parameters characterizing conditional distributions. The parameters are viewed as random variables to quantify uncertainty about their values. Belief nets are used to compute responses to queries; i.e., conditional probabilities of interest. A query is a function of the parameters, hence a random variable. Van Allen et al. (2001, 2008) showed how to quantify uncertainty about a query via a delta method approximation of its variance. We develop more accurate approximations for both query mean and variance. The key idea is to extend the query mean approximation to a \u201cdoubled network\u201d involving two independent replicates. Our method assumes complete data and can be applied to discrete, continuous, and hybrid networks (provided discrete variables have only discrete parents). We analyze several improvements, and provide empirical studies to demonstrate their effectiveness."}, {"heading": "1 INTRODUCTION", "text": "Consider a simple example. Suppose A represents presence/absence of a medical condition while B and Y are test results. Variables B and Y are conditionally independent given A, with A and B binary and Y continuous. The conditional independence assumption is represented by the directed acyclic graph structure in Figure 1(a). Let \u03b8a = P (A = a), \u03b8b|a = P (B = b |A = a), and let p(y |\u03b2a, \u03c3a) be the conditional density of Y given A = a, assumed normal with mean \u03b2a and variance \u03c32a. We want to estimate the probability that condition A is present given\nspecified results from the two tests B and Y . Let \u0398 represent all of the parameters. If \u0398 were known, we would use the formula:\nq(\u0398) = qa|b,y(\u0398) = \u03b8a\u03b8b|ap(y |\u03b2a, \u03c3a)\u2211\na1 \u03b8a1\u03b8b|a1p(y |\u03b2a1 , \u03c3a1)\n. (1)\nIn the Bayesian paradigm, uncertainty about \u0398 is quantified by modeling parameters as random variables. It follows that query probabilities such as (1) are also random. A query response is usually estimated by approximating its posterior mean. This approximation is similar to expression (1), but with \u03b8a and \u03b8b|a replaced by their posterior means and with the normal densities replaced by Student\u2019s t densities.\nOne may want more than just a point estimate. Van Allen et al. (2001, 2008) showed (for discrete networks) how one can approximate the variance and posterior distribution of a query. Their variance derivation employs the delta method; i.e., a first-order Taylor series expansion of the function q(\u0398) about the posterior mean of \u0398. They provide asymptotic theory and empirical experiments supporting this approach. They also showed how these approximations can be used to construct a Bayesian credible interval (error bars) for q(\u0398). Guo and Greiner (2005) applied this delta method approximation as part of a mean squared error (i.e., squared bias + variance) measure designed to estimate the quality of different belief net structures when seeking a best classifier. Lee et al. (2006) provide a technique for combining independent belief net classifiers that involves weighting their respective mean probability values by their inverse variances, and they show that this works well in practice.\nWe propose new approximations for the mean and variance based on a simple trick. Suppose (A1, B1, Y1) and (A2, B2, Y2) are replicates of the network variables, conditionally independent given \u0398. We represent the paired replicates as nodes in a \u201cdoubled network\u201d with the same structure; see Figure 1. The squared query q(\u0398)2 can be expressed as a query in this doubled net-\nFigure 1: A simple Bayesian net.\n1\nwork:\nP (A1 = A2 = a |B1 = B2 = b, Y1 = Y2 = y,\u0398) .\nThe method used to approximate the mean of q(\u0398) can be extended to the doubled network to approximate the mean of q(\u0398)2 and hence to approximate the variance. Unlike the delta method, our approach does not rely on approximate local linearity of q(\u0398). It does involve the addition of two incomplete observations to the data set when calculating the posterior mean of q(\u0398)2. In some situations, this addition results in under-estimation of the desired variance. This deficiency is largely eliminated by a simple adjustment. A similar adjustment substantially improves the usual query mean approximation.\nSection 2 reviews pertinent models and methods for belief networks. The network doubling technique is described in Section 3 for discrete, continuous, and hybrid networks. Proposed adjustments and numerical results are presented in Sections 4 and 5 for discrete networks. Corresponding work for continuous and hybrid networks is ongoing. Computational issues are discussed in Section 6. Contributions and plans for further work are summarized in Section 7."}, {"heading": "2 BACKGROUND", "text": ""}, {"heading": "2.1 NETWORK VARIABLES", "text": "We assume network structure is known. Let B denote a discrete network variable taking values b \u2208 DomB . Let Y denote a continuous network variable taking values y on the real line. Vectors of variables are denoted by boldface: A for discrete and X for continuous. Let \u0398 be a random vector comprising all unknown network parameters; i.e., \u0398 determines all conditional distributions of variables given their parents.\nWe assume that discrete variables have only discrete parents. Suppose pa(B) = A; i.e., the parents of B are the variables comprising the vectorA. The conditional probability that B = b given A = a is denoted\n\u03b8b|a = \u03b8B=b|A=a = P{B = b |A = a,\u0398}.\nVariables associated with values will be clear from context. We employ similar abbreviations for other parameters and hyperparameters. The \u03b8b|a parameters are often presented in conditional probability tables (CPtables) with rows indexed by a and columns by b; e.g., see Figure 1. Note that we use superscripts b1, b2 to list the distinct values in DomB . We use subscripts b1, b2 to denote arbitrary values in DomB , often related to replicated variables B1, B2.\nContinuous variables can have both discrete and continuous parents. Suppose pa(Y ) = \u3008A,X\u3009 with X = \u3008X1, . . . , Xd\u3009. The conditional distribution of Y is\n(Y |A = a,X = x,\u0398) \u223c N ( (1,xT )\u03b2a, \u03c3 2 a ) ; (2)\ni.e., normally distributed, conditional mean related to x by a linear regression model with coefficients depending on a. Here xT is the transpose of the ddimensional column vector x while \u03b2a is an (d + 1)- dimensional column vector of regression coefficients (the first entry is the constant term)."}, {"heading": "2.2 PRIOR AND POSTERIOR", "text": "The network parameters represented by \u0398 consist of CPtable parameters \u03b8b|a, regression coefficient vectors \u03b2a, and variances \u03c32a. We assume the prior distribution for \u0398 has the following form; e.g., see Gelman et al. (2003).\n\u2022 CPtable rows follow Dirichlet distributions:\n\u03b8B|a := \u3008\u03b8b|a, b \u2208 DomB\u3009 \u223c Dir(\u03b1B|a),\nwhere \u03b1B|a := \u3008\u03b1b|a, b \u2208 DomB\u3009 .\n\u2022 The regression coefficients and variance together have a normal-(inverse chi-square) distribution:\n(\u03b2a |\u03c32a) \u223c Nd+1 ( \u00b5a, \u03c3 2 a(\u03bda\u03a8a) \u22121) , \u03c3\u22122a \u223c (\u03c42a\u03bda)\u22121\u03c72\u03bda .\nI.e., dropping subscripts for a moment, \u03b2 conditioned on \u03c32 is multivariate normal with mean\nvector \u00b5 and covariance matrix \u03c32(\u03bd\u03a8)\u22121; and \u03bd\u03c42/\u03c32 has a \u03c72\u03bd distribution with \u03bd > 0 (not necessarily an integer). Note that \u03c42/\u03c32 has mean 1 and variance 2/\u03bd.\n\u2022 Parameters are assumed to be statistically independent except where joint distributions are specified above. In particular, we assume global independence: the parameters determining the conditional distribution of one variable given its parents are independent of all other parameters.\nThe prior is conjugate: given a data set D consisting of n independent replicates of complete tuples of network variables, the prior hyperparameter values are updated as follows. Let nab and na be the number of tuples in D with (A, B) = (a, b) and A = a, respectively. Let (xi, yi) be the observations of (X, Y ) for the na tuples with A = a. Let Xa be the na\u00d7 (d+1) matrix with rows (1,xTi ). Let ya be the column vector with entries yi. In the five equations below, the prior hyperparameter values appear on the right-hand side and are identified with tildes (e.g., \u03b1\u0303).\n\u03b1b|a = \u03b1\u0303b|a + nab \u03bda = \u03bd\u0303a + na\n\u03bda\u03a8a = \u03bd\u0303a\u03a8\u0303a +XTaXa \u03bda\u03a8a\u00b5a = \u03bd\u0303a\u03a8\u0303a\u00b5\u0303a +X T aya\n\u03bda [ \u03c42a + \u00b5 T a\u03a8a\u00b5a ] = \u03bd\u0303a [ \u03c4\u03032a + \u00b5\u0303 T a\u03a8\u0303a\u00b5\u0303a ] + yTaya\nThe values \u2211 a,b \u03b1b|a and \u2211\na \u03bda are called the effective sample sizes for variables B and Y , respectively. Our adjustments developed in Section 4 are motivated by large m asymptotics, where m is proportional to the effective sample size for each of the variables; i.e.,\n\u03b1b|a = m\u03b10b|a and \u03bda = m\u03bd 0 a\nwith (\u03b10b|a, \u03bd 0 a,\u03a8a,\u00b5a, \u03c4 2 a) fixed. (3)\nLarge m asymptotics are similar to but not the same as large n asymptotics. As the sample size n increases, the posterior mean E{\u03b8b|a | D} = \u03b1b|a/\u03b1\u00b7|a varies and converges to some value. (Here and elsewhere, the dot subscript indicates summation: \u03b1\u00b7|a = \u2211 b \u03b1b|a .) Under assumption (3), the posterior mean remains fixed as m varies."}, {"heading": "2.3 APPROXIMATING A QUERY MEAN", "text": "Consider a query involving outcomes of hypothesis variables H given values for evidence variables E. It is convenient to represent the query in terms of a function w(H). E.g., suppose H = A, E = (B, Y ), e = (b, y), and\nq(\u0398) = P (A = a |B = b, Y = y,\u0398) = E{w(A) |B = b, Y = y,\u0398} ,\nwhere w(A) = 1 for A = a and w(A) = 0 otherwise.\nFor discrete networks, query responses q(\u0398) are usually estimated by q(\u0398\u0302), where \u0398\u0302 := E{\u0398 | D} is the posterior mean of the parameter vector. This plugin estimate usually differs slightly from the posterior query mean E{q(\u0398) | D}. Cooper and Herskovits (1992, expression 19) showed that the plug-in estimate equals E{q(\u0398) | D, e}; i.e., the posterior query mean given an augmented data set consisting of D and an additional partial observation of the evidence variables E = e. Cooper and Herskovits (1991) derived a formula for E{q(\u0398) | D, e} that is valid for discrete, continuous, and hybrid networks. This formula provides a useful approximation of the less tractable E{q(\u0398 | D}. The plug-in estimate is a special case of this formula for discrete networks. The formula is important for our network doubling technique, so is reviewed here.\nIn the integral expression below, Z represents all variables not included in (H,E); dh and dz refer to product measures allowing both integration for continuous variables (Lebesgue measure) and summation for discrete variables (counting measure). Some manipulation yields\nE{q(\u0398) | D, e} = E{w(H) |E = e,D} = E [E{w(H) |E = e,\u0398} |D ] (4)\n= \u222b \u222b w(h) \u222b p(h, e,z |\u03b8)p(\u03b8 | D)d\u03b8dhdz\u222b \u222b \u222b\np(h, e,z |\u03b8)p(\u03b8 | D)d\u03b8dhdz .\nNow p(h, e,z |\u03b8) factors as a product of conditional probabilities and densities, one for each variable in the network. Due to global independence, the integral \u222b p(h, e,z |\u03b8)p(\u03b8 | D)d\u03b8 factors into a product of integrals, one for each variable. The result is a product of probabilities and densities described in Section 2.4 below. It follows that E{q(\u0398 | D, e} can be calculated in essentially the same manner as the function q(\u0398), but with two modifications.\n\u2022 For discrete variables, parameters \u03b8b|a are replaced by their posterior means. If all network variables are discrete, then we have the plug-in estimate:\nE{q(\u0398) | D, e} = q(E{\u0398 | D}). (5)\n\u2022 For continuous variables, the normal densities are replaced by the St1(\u03b7, \u03c92, \u03bd) densities described below. Note that this is not the same as replacing \u03b2 and \u03c32 parameters with their posterior means."}, {"heading": "2.4 PREDICTIVE DISTRIBUTIONS", "text": "The predictive distribution of the network variables is obtained by integrating out their joint conditional dis-\ntribution given \u0398 with respect to the posterior distribution of \u0398. Global independence allows this integration to be carried out separately for each conditional distribution of a variable given its parents.\nThe predictive distribution for a discrete variable B is\n\u03c0b|a := P (B = b |A = a,D) = E{\u03b8b|a | D} = \u03b1b|a\n\u03b1\u00b7|a .\nThe predictive distribution for a continuous variable is a location-scale version of the Student\u2019s t distribution with \u03bd degrees of freedom. We need the multivariate form of this distribution in Section 3, so we define it here. Suppose\nT = \u03b7 + U\u22121/2(Z \u2212 \u03b7),\nwhere Z and U are independent, Z \u223c Np(\u03b7,\u2126), U \u223c (1/\u03bd)\u03c72\u03bd , and \u2126 is a nonsingular covariance matrix. It follows that T has the following density function (Johnson and Kotz, 1972, page 134):\n\u0393[(\u03bd + p)/2] /\u0393(\u03bd/2) (\u03bd\u03c0)p/2|\u2126|1/2 [ 1 + 1\u03bd (t\u2212 \u03b7)T\u2126 \u22121(t\u2212 \u03b7) ](\u03bd+p)/2 .\nWe refer to this as the Stp(\u03b7,\u2126, \u03bd) distribution. For p = 1, we write St1(\u03b7, \u03c92, \u03bd). Note that St1(0, 1, \u03bd) is Student\u2019s t distribution.\nWe claim that (Y |A = a,X = x,D) \u223c St1(\u03b7, \u03c92, \u03bd) with \u03bd = \u03bda, \u03b7 = (1,xT )\u00b5a, and\n\u03c92 = \u03c42a { (1,xT )(\u03bda\u03a8a)\u22121(1,xT )T + 1 } . (6)\nTo see this, let us suppress subscripts for a moment. Let Z1 \u223c N(0, 1) be independent of (\u03b2, \u03c3). Put Z2 := \u03c3\u22121(\u03b2 \u2212 \u00b5) \u223c Nm+1 ( 0, (\u03bd\u03a8)\u22121 ) . We then have\n(Y |a,x,D) \u223c (1,xT )\u03b2 + \u03c3Z1 \u223c \u03b7 + (\u03c3/\u03c4)\u03c4 { (1,xT )Z2 + Z1 } ."}, {"heading": "3 NETWORK DOUBLING", "text": "In Section 2.3 we noted that E{q(\u0398) | D} is usually approximated by the more tractable E{q(\u0398) | D, e}. Here we propose approximating Var{q(\u0398) | D} by Var{q(\u0398) | D, e, e}; i.e., the posterior variance given D and additional replicates E1 and E2 of the vector of evidence variables, both having the same value e. We develop a formula for this latter variance by imagining a doubled network; see Figure 1(b). These mean and variance approximations can be improved by adjustments described in Section 4.\nConsider two replicated tuples of network variables, conditionally independent and identically distributed given \u0398. Use these to replace each variable in the\noriginal network by a pair of variables; e.g., B is replaced by B\u2217 := (B1, B2) with possible values b\u2217 = (b1, b2) \u2208 DomB\u2217 = DomB \u00d7 DomB . If pa(B) = A, then pa(B\u2217) = A\u2217 := (A1,A2). Conditional distributions of doubled variables given parents are obtained by multiplying probabilities or densities for single variables.\nFor discrete variables, we have\nP (B\u2217 = b\u2217 |A\u2217 = a\u2217,\u0398) = \u03b8b1|a1\u03b8b2|a2 .\nE.g., if A = A, DomA = {a1, a2}, and DomB = {b1, b2}, then the CPtable for B\u2217 is the 4 \u00d7 4 array shown in Figure 1(b). More generally, if a CPtable in the original network involves dr \u00d7 dc parameters, then corresponding table in the doubled network has d2r \u00d7 d2c entries. Note that CPtable rows in the doubled network are not independent (local independence does not hold) and do not have Dirichlet distributions. Fortunately, these properties are not needed for the factorization described following (4).\nFor continuous variables, the conditional density of Y \u2217 = (Y1, Y2) given (A\u2217 = a\u2217,X\u2217 = x\u2217,\u0398) is the product of the densities for two normal distributions of the form (2) with subscript i = 1, 2 on a and x.\nPut H\u2217 = (H1,H2), w\u2217(H\u2217) = w(H1)w(H2), E\u2217 = (E1,E2), and e\u2217 = (e, e). Some manipulation using conditional independence yields\nq(\u0398)2 = E{w\u2217(H\u2217) |E\u2217 = e\u2217,\u0398} , q(\u0398) = E{w(H1) |E\u2217 = e\u2217,\u0398} .\nWe thus have\nVar{q(\u0398) | D, e, e} (7) = E{q(\u0398)2 | D, e, e} \u2212 [E{q(\u0398) | D, e, e}]2\n= E{w\u2217(H\u2217) | e\u2217,D} \u2212 [E{w(H1) | e\u2217,D}]2 .\nThe doubled network satisfies global independence assumptions, so we can follow the approach of Section 2.3 to evaluate the two expected values in (7). To accomplish this task, we need bivariate predictive distributions for the doubled network.\nFor discrete variables, the calculation follows from the means and covariances of a Dirichlet distribution. Let \u03b4b1b2 be the Kronecker delta function. We have\n\u03c0\u2217b\u2217|a\u2217 := P{B \u2217 = b\u2217 |A\u2217 = a\u2217,D}\n= E{\u03b8b1|a1\u03b8b2|a2 | D} = \u03c0b1|a1\u03c0b2|a2 + \u03b4a1a2 \u03c0b1|a1(\u03b4b1b2 \u2212 \u03c0b2|a1)\n\u03b1\u00b7|a1 + 1 .\nIf all network variables are discrete, then we have an identity corresponding to (5). Let \u0398\u2217 be the vector\nof all CPtable entries in the doubled network; e.g., \u03b8b1|a1\u03b8b2|a2 appears in row a\n\u2217 and column b\u2217 for the CPtable of B\u2217. We then have\nE{q\u2217(\u0398\u2217) | D, e, e} = q\u2217(E{\u0398\u2217 | D}) (8)\nwith the entries in E{\u0398\u2217 | D} given by the \u03c0\u2217b\u2217|a\u2217 values above. The two expected values in the variance approximation (7) are calculated by applying (8) twice: with q\u2217(\u0398\u2217) = q(\u0398)2 and with q\u2217(\u0398\u2217) = q(\u0398).\nFor continuous variables, we need the density for {(Y1, Y2) |a1,a2,x1,x2,D}. There are two cases to consider.\n\u2022 If a1 6= a2, then the parameters (\u03b2a1 , \u03c3 2 a1) and\n(\u03b2a2 , \u03c3 2 a2) are mutually independent. Consequently, the joint distribution factors as a product of two St1(\u03b7, \u03c92, \u03bd) densities; see expression (6).\n\u2022 If a1 = a2 ( = a, say), then the joint distribution is St2(\u03b7,\u2126, \u03bd) with \u03bd = \u03bda, \u03b7 =X2\u00b5a, and\n\u2126 = \u03c42a { X2(\u03bda\u03a8a)\u22121XT2 + I2 } ,\nwhere X2 is the 2 \u00d7 (1 + d) matrix whose rows are each (1,xTi ) and I2 is the 2 \u00d7 2 identity matrix. The derivation is similar to that following (6). Note that (\u03b2a, \u03c32a) is the same for both Y1 and Y2 in this case."}, {"heading": "4 ADJUSTMENTS", "text": "We now narrow our focus to discrete networks and consider the four mean and variance approximations in Table 1. The delta method approximation is\nv\u03021 = gTCg , (9)\nwhere g is the gradient vector of q(\u0398) and C is the covariance matrix of \u0398, both evaluated at E{\u0398 | D}. The second variance approximation v\u03022 is the doubling method introduced in Section 3. The simple adjustments (q\u03023, v\u03023) and more complex adjustments (q\u03024, v\u03024) are developed in this section.\nFor conciseness we suppress D in our expressions; i.e., we implicitly assume that expectations are conditioned on D. PutQ = q(\u0398) = P (H = h |E = e,\u0398) and R = P (E = e |\u0398). Note that R is an unconditional query, with hypothesis E = e and no evidence variables. Let \u00b5q, \u00b5r, \u03c3qq, \u03c3rr, and \u03c3qr denote the means, variances, and covariance for (Q,R). We extend this notation to higher moments; e.g., \u03c3qqr = E{(Q\u2212 \u00b5q)2(R\u2212 \u00b5r)}.\nWe use approximations for higher moments motivated by large m asymptotics; i.e., a sequence of posterior distributions of the form (3) with m \u2192 \u221e. One may\nverify that the distribution of \u221a m(Q\u2212 \u00b5q, R\u2212 \u00b5r) converges to bivariate normal by modifying the proof of Theorem 2 in Van Allen et al. (2008). Asymptotic normality implies that\n\u03c3qqrr \u2212 2\u03c32qr \u2212 \u03c3qq\u03c3rr \u2192 0 at rate m\u22125/2 (10)\nwhile \u03c3qrr and \u03c3qqr converge to zero at rate m\u22122. We considered approximating \u03c3qqr and \u03c3qrr by zero but found that more accurate approximations give better results. Asymptotic bivariate normality suggests\nE{R\u2212 \u00b5r |Q} \u2248 (Q\u2212 \u00b5q) \u03c3qr \u03c3qq\nand hence \u03c3qqr \u2248 \u03c3qqq\u03c3qr/\u03c3qq. Now \u03c3qqq = 0 for normal distributions; however, Van Allen et al. (2008) argue that query distributions are usually better approximated by beta distributions. Substituting the third moment of a beta distribution for \u03c3qqq, we obtain\n\u03c3qqr \u2248 2\u03c3qr\u03c3qq(1\u2212 2\u00b5q) \u00b5q(1\u2212 \u00b5q) + \u03c3qq . (11)\nSwitching the roles of Q and R gives\n\u03c3qrr \u2248 2\u03c3qr\u03c3rr(1\u2212 2\u00b5r) \u00b5r(1\u2212 \u00b5r) + \u03c3rr . (12)\nBefore proceeding, we observe that \u00b5r and \u03c3rr can be calculated exactly because R can be expressed as a sum of products of independent terms. For queries with this property, all approximations (except v\u03021) are exact; i.e., additional observations of evidence variables have no effect on the posterior mean or variance. E.g., given a discrete network with structure E \u2192 B \u2192 H, we have q(\u0398) = \u2211 b \u03b8h|b\u03b8b|e . Since parameters in each product are independent, it follows that q\u03022 = q\u03021 = \u00b5q and v\u03022 = \u03c3qq.\nWe begin with adjustments to improve q\u03021. Bayes rule and some manipulation yields\nq\u03021 = E(QR) E(R) = \u00b5q + \u03c3qr \u00b5r\n(13)\nq\u03022 = E(QR2) E(R2) = \u00b5q + 2\u00b5r\u03c3qr + \u03c3qrr \u00b52r + \u03c3rr .\nIf \u00b5r = 1, then set \u03c3\u0302qr = 0. Otherwise, substituting (12) for \u03c3qrr and solving yields \u03c3\u0302qr =\n(q\u03022 \u2212 q\u03021)\u00b5r(\u00b52r + \u03c3rr){\u00b5r(1\u2212 \u00b5r) + \u03c3rr} \u00b53r(1\u2212 \u00b5r) + \u00b5r(1\u2212 2\u00b5r)\u03c3rr \u2212 \u03c32rr . (14)\nThe formula for q\u03024 in Table 1 follows from (13). Now recall that, under condition (3), \u00b5r remains fixed while \u03c3rr \u2192 0 as m \u2192 \u221e. It follows that setting \u03c3rr = 0 in (14) will have negligible effect for large m. We thus obtain \u03c3\u0302qr \u2248 (q\u03022 \u2212 q\u03021)\u00b5r, leading to the simpler q\u03023 approximation.\nIn trying to improve v\u03022, we began with the idea of replacing q\u03022 with \u00b5q:\nE{(Q\u2212 \u00b5q)2 | e, e} = v\u03022 + (q\u03022 \u2212 \u00b5q)2 . (15)\nThis suggests an approximation v\u03022+4(q\u03022\u2212 q\u03021)2, which does help to reduce the under-estimation problem; however, a greater improvement is obtained by further analysis of (15):\nE{(Q\u2212 \u00b5q)2R2} E(R2) = \u00b52r\u03c3qq + 2\u00b5r\u03c3qqr + \u03c3qqrr \u00b52r + \u03c3rr . (16)\nWe approximate \u03c3qqrr using (10), \u03c3qqr by (11), \u03c3qr by (14), \u00b5q by q\u03024, and replace \u03c3qq by v\u03024. Rearranging terms yields the identity: v\u03024 =\n(\u00b52r + \u03c3rr){v\u03022 + (q\u03022 \u2212 q\u03024)2} \u2212 2\u03c3\u03022qr \u00b52r + \u03c3rr + 4\u00b5r\u03c3\u0302qr(1\u2212 2q\u03024)/{q\u03024(1\u2212 q\u03024) + v\u03024} . (17)\nNotice that v\u03024 appears in the denominator of (17). We initially set this value to v\u03022, then iteratively solve for v\u03024. The values converge in a few iterations.\nWe observe that replacing \u03c3rr by zero has negligible effect on (17) as m \u2192 \u221e. By also replacing q\u03024 by q\u03023 and \u03c3\u0302qr/\u00b5r by q\u03022 \u2212 q\u03021, we obtain a simpler identity:\nv\u03023 = v\u03022 + 2(q\u03022 \u2212 q\u03021)2\n1 + 4(q\u03022 \u2212 q\u03021)(1\u2212 2q\u03023)/{q\u03023(1\u2212 q\u03023) + v\u03023} . (18)\nWe again initialize by v\u03022, then iteratively solve for v\u03023. The approximations q\u03023 and v\u03023 may be preferred to q\u03024 and v\u03024 since \u00b5r and \u03c3rr are not required.\nRates of convergence are summarized in Proposition 1 below. The proof of this result follows easily from Van Allen et al. (2008) and the development above.\nProposition 1. Assume a discrete network satisfying (3) and let m\u2192\u221e. The query mean \u00b5q remains constant while the variance \u03c3qq approaches zero at rate m\u22121. The mean approximations have errors q\u0302j \u2212 \u00b5q approaching zero at rate m\u22121 for j = 1 and 2, and at the faster rate m\u22123/2 for j = 3 and 4. All four variance approximations have relative errors (v\u0302j\u2212\u03c3qq)/\u03c3qq approaching zero at rate m\u22121.\n/Users/peterhooper/Documents/Research/Doubling paper/R figures"}, {"heading": "5 NUMERICAL RESULTS", "text": "We evaluated accuracy of approximations q\u0302j and v\u0302j using highly accurate empirical estimates of \u00b5q and \u03c3qq. These estimates q\u03020 and v\u03020 were obtained by simulating k = 106 replicates of \u0398 from the posterior distribution, evaluating q(\u0398) for each replicate, then calculating the sample mean and sample variance. Computational costs preclude using empirical variance estimates in practice. When m is large, asymptotic normality of q(\u0398) implies that the distribution of v\u03020/\u03c3qq is approximately (1/k)\u03c72k with variance 2/k. Consequently v\u03020/\u03c3qq varies over the interval 1\u00b1 2 \u221a 2/k for roughly 95% of samples. Since our variance approximations have relative errors of order m\u22121, it follows that k should be of order at least m2 for v\u03020 to have substantially smaller relative error. When comparing approximate relative errors (v\u0302j \u2212 v\u03020)/v\u03020 with k = 106, variation in v\u03020 has a noticeable effect for m = 500; see Figure 3(f).\nOur examples differ with respect to network structure, posterior distribution, and query. All variables are binary. All posterior distributions satisfy BDe constraints (e.g., see Hooper 2008), so all variables have the same effective sample size m. Hyperparameters are thus determined by m and the poste-\n/Users/peterhooper/Documents/Research/Doubling paper/R figures\nrior means E{\u0398 | D}. Our examples are from three small networks, each with one vector E{\u0398 | D} and m \u2208 {20, 50, 100, 200, 500}:\n\u2022 Two na\u0308\u0131ve Bayes networks (NB-2 and NB-4 with 2 and 4 features plus the root variable); H = root,\nE = all children of H, e varies over all combinations (22 for NB-2, 24 for NB-4).\n\u2022 Diamond network with 4 variables \u2199\u2198 \u2198 \u2199, all 108\ndistinct queries with one hypothesis variable.\nApproximations for means are compared in Figure 2 and for variances in Figure 3. The errors and relative errors are multiplied by m in these figures to facilitate comparisons across a range of effective sample sizes. Boxplots for m = 20, 100, and 500 are shown. Plots for other values of m are similar. By Proposition 1, relative errors (v\u0302j\u2212\u03c3qq)/\u03c3qq should approach zero at rates cj/m, where cj depends implicitly on the network, E(\u0398 | D), and the query. This theory is supported by Figure 3 and additional plots (not shown) comparing the four methods for individual queries. Our results suggest that c3 \u2248 c4 while c1 and c2 tend to be further from zero. Relative errors can be interpreted in terms of variances or standard deviations. If (v\u0302j \u2212 \u03c3qq)/\u03c3qq = cj/m, then we have\nv\u0302j \u03c3qq = 1 + cj m and\n\u221a v\u0302j \u221a \u03c3qq = \u221a 1 + cj m \u2248 1 + cj 2m ."}, {"heading": "6 COMPUTATIONAL ISSUES", "text": "Inference in Bayesian networks is in general an NPcomplete problem (Cooper, 1990). For instance, the complexity of the Variable Elimination (VE) Algorithm is O(dr), where d is an upper bound on the number of values that a variable can take and r is an upper bound on the size of a factor generated by the VE Algorithm (Koller and Friedman, 2008). Network doubling uses essentially the same technique to calculate a variance as that used to evaluate a query, resulting in corresponding computational complexity. The doubled CPtables are larger (squared number of rows and columns), so the computational complexity of VE is increased to O(d2r). The delta method retains O(dr) complexity (Van Allen et al., 2008), so is typically faster in large networks; see Table 2 below.\nIn some cases, we can exploit the structure of the network or query to achieve a polynomial time inference algorithm. For poly-tree Bayesian networks (i.e. networks with at most one undirected path between any pair of nodes), there exist inference algorithms with linear time complexity. Reduced complexity is also available when the query can be expressed in terms of probabilities of hypothesis and evidence nodes conditioned on their Markov blanket; i.e., the parents, the children and the parents of the children. Once again, we have a polynomial time inference algorithm. These techniques translate directly to efficient algorithms for computing all of the variance approximations in Table 1.\nWe empirically compared timing results for the delta and network doubling methods using queries from three networks: 1000 from Na\u0308\u0131ve Bayes (5 variables) repeated 100 times, 108 from the Diamond network (4 variables) repeated 1000 times, and 100 from the Alarm network (37 variables, Beinlich et al. 1989). The Alarm queries were randomly generated so that queries had on average 3 hypothesis variables, 25 evidence variables, and 9 non-specified variables. The results in Table 2 corroborate the earlier claim that doubling is faster for simple queries and delta is faster for complex queries."}, {"heading": "7 CONCLUSION", "text": "We plan to extend the implementation of the network doubling and delta methods to continuous and hybrid networks. This should be easy for the doubling method when all continuous variables are evidence variables. We will then compare the accuracies of these methods.\nOur main contributions are summarized as follows.\n\u2022 Development of a network doubling method to approximate query variances. This technique exploits the Cooper and Herskovits formula (5) for approximating the query mean and is easily implemented for discrete networks. The technique is also applicable for continuous and hybrid networks, but implementation may be less straightforward.\n\u2022 Adjustments to improve accuracy, motivated by asymptotic theory for discrete networks. This theory also provides rates of convergence for the approximations.\n\u2022 Numerical comparisons of the network doubling and delta methods, showing superior accuracy of the former in simple networks.\nWe do not recommend that network doubling replace delta in all applications. If the effective sample size is large, then both approaches may provide adequate approximations and the choice between them may depend primarily on computational complexity. If the network is large, then delta may have an advantage. If the effective sample size is small or the network is not large, then doubling may be the better choice."}, {"heading": "Acknowledgements", "text": "We are grateful for helpful comments from the anonymous reviewers. We acknowledge support provided by NSERC, iCORE, and the Alberta Ingenuity Centre for Machine Learning."}], "references": [], "referenceMentions": [], "year": 2009, "abstractText": "A Bayesian belief network models a joint distribution with an directed acyclic graph representing dependencies among variables and network parameters characterizing conditional distributions. The parameters are viewed as random variables to quantify uncertainty about their values. Belief nets are used to compute responses to queries; i.e., conditional probabilities of interest. A query is a function of the parameters, hence a random variable. Van Allen et al. (2001, 2008) showed how to quantify uncertainty about a query via a delta method approximation of its variance. We develop more accurate approximations for both query mean and variance. The key idea is to extend the query mean approximation to a \u201cdoubled network\u201d involving two independent replicates. Our method assumes complete data and can be applied to discrete, continuous, and hybrid networks (provided discrete variables have only discrete parents). We analyze several improvements, and provide empirical studies to demonstrate their effectiveness.", "creator": "TeX"}}}