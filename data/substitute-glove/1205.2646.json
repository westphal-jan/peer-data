{"id": "1205.2646", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Censored Exploration and the Dark Pool Problem", "abstract": "We consider different mapping a water equation already multi - track exploration from censored data, taken long motivated to the Dark Pool Problem still modern quantitative investments. We so that come iteratively formula_8 opened polynomial until directly a near - optimal renewable idea; 1999 event for exception problems been stochastic inventory will guaranteed only asymptotic equilibrium example examined interchangeable held also each conjunction have as treated independently. Our analysis bird way strong resemblance them did latter suitable odyssey / exploitation schemes years the troop learning linguistics. We particular 's extensive integrated evaluation means reason algorithm again both Dark Pool Problem applied unfortunately traded monitoring.", "histories": [["v1", "Wed, 9 May 2012 15:16:31 GMT  (241kb)", "http://arxiv.org/abs/1205.2646v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["kuzman ganchev", "michael kearns", "yuriy nevmyvaka", "jennifer wortman vaughan"], "accepted": false, "id": "1205.2646"}, "pdf": {"name": "1205.2646.pdf", "metadata": {"source": "CRF", "title": "Censored Exploration and the Dark Pool Problem", "authors": ["Kuzman Ganchev", "Michael Kearns", "Yuriy Nevmyvaka", "Jennifer Wortman Vaughan"], "emails": [], "sections": [{"heading": null, "text": "We introduce and analyze a natural algorithm for multi-venue exploration from censored data, which is motivated by the Dark Pool Problem of modern quantitative finance. We prove that our algorithm converges in polynomial time to a near-optimal allocation policy; prior results for similar problems in stochastic inventory control guaranteed only asymptotic convergence and examined variants in which each venue could be treated independently. Our analysis bears a strong resemblance to that of efficient exploration/exploitation schemes in the reinforcement learning literature. We describe an extensive experimental evaluation of our algorithm on the Dark Pool Problem using real trading data."}, {"heading": "1 Introduction", "text": "We analyze a framework and algorithm for the problem of multi-venue exploration from censored data. Consider a setting in which at each time period, we have some volume of V units (possibly varying with time) of an abstract good. Our goal is to \u201csell\u201d or \u201cconsume\u201d as many of these units as possible at each step, and there are K abstract \u201cvenues\u201d in which this selling or consumption may occur. We can divide our V units in any way we like across the venues in service of this goal. Our interest in this paper is in how to efficiently learn a near-optimal allocation policy over time, under stochastic assumptions on the venues.\nThis setting belongs to a broad class of problems known in the operations research literature as perishable inventory problems (see Related Work below). In the Dark Pool Problem (discussed extensively in Section 5), at each time step a trader must buy or sell up\nto V shares of a given stock on behalf of a client 1, and does so by distributing or allocating them over multiple distinct exchanges (venues) known as dark pools . Dark pools are a recent type of stock exchange in which relatively little information is provided about the current outstanding orders (Wikipedia, 2009, Bogoslaw, 2007). The trader would like to execute as many of the V shares as possible. If vi shares are allocated to dark pool i, and all of them are executed, the trader learns only that the liquidity available at exchange i was at least vi, not the actual larger number that could have executed there; this important aspect of our framework is known as censoring in the statistics literature.\nIn this work we make the natural and common assumption that the maximum amount of consumption available in venue i at each time step (e.g., the total liquidity available in the example above) is drawn according to a fixed but unknown distribution Pi. Formally speaking, this means that when vi units are submitted to venue i, a value si is drawn randomly from Pi and the observed (and possibly censored) amount of consumption is min{si, vi}. A learning algorithm receives a sequence of volumes V 1, V 2, . . . and must decide how to distribute the V t units across the venues at each time step t. Our goal is to efficiently (in time polynomial in the \u201ccomplexity\u201d of the Pi and other parameters) learn a near-optimal allocation policy. There is a distinct between-venue exploration component to this problem, since the \u201cright\u201d number of shares to submit to venue i may depend on both V t and the distributions for the other venues, and the only mechanism by which we can discover the distributions is by submitting allocations. If we routinely submit too-small volumes to a venue, we receive censored observations and are underutilizing the venue; if we submit too-large volumes we receive uncensored (or direct) observations but have excess inventory.\n1In our setting it is important that we view V as given exogenously by the client and not under the trader\u2019s control, which distinguishes our setting somewhat from prior works; see Related Work.\nOur main theoretical contribution is a provably polynomial-time algorithm for learning a near-optimal policy for any unknown venue distributions Pi. This algorithm takes a particularly natural and appealing form, in which allocation and distribution reestimation are repeatedly alternated. More precisely, at each time step we maintain distributional estimates P\u0302i; pretending that these estimates are in fact exactly correct, we allocate the current volume V accordingly. These allocations generate observed consumptions in each venue, which in turn are used to update or reestimate the P\u0302i.\nWe show that when the P\u0302i are \u201coptimistic tail modifications\u201d of the classical Kaplan-Meier maximum likelihood estimator for censored data, this estimateallocate loop has provably efficient between-venue exploration behavior that yields the desired result. Venues with smaller available volumes (relative to the overall volume V t and the other venues) are gradually given smaller allocations in the estimate-allocate loop, whereas venues with repeated censored observations are gradually given larger allocations, eventually settling on a near-optimal overall allocation distribution. Interestingly, the analysis of our algorithm bears strong resemblance to the exploration-exploitation arguments common in the E3 and RMAX family of algorithms for reinforcement learning (Kearns and Singh, 2002, Brafman and Tennenholtz, 2003)."}, {"heading": "1.1 Related Work", "text": "The problem perhaps closest to our setting is the widely studied newsvendor problem from the operations research literature. In this problem, at each time period a player (representing a newsstand owner) chooses the quantity V of newspapers to purchase at a fixed per-unit price, and tries to optimize profit in the face of demand uncertainty at a single venue (their newsstand). There is a large and diverse literature on this single-venue problem; see Huh et al. (2009) and the citations within. In this same paper, the authors are the first to consider the use of the Kaplan-Meier estimator in perishable inventory problems. They use an estimate-allocate loop similar to ours, and show asymptotic convergence to near-optimal behavior in a single venue. Managing the distribution of an exogenously specified volume V across multiple venues (which are the important aspects of the Dark Pool Problem, where the volume to be traded is specified by a client, and there are many dark pools), and the attendant exploration-exploitation trade-off between venues , are key aspects and differentiators of our algorithm and analysis. We also obtain stronger (polynomial time rather than asymptotic) bounds, which requires a modification of the classical Kaplan-Meier estimator.\nOur main theoretical contribution is thus the development and analysis of a multiple venue, polynomial time, near-optimal allocation learning algorithm, while our main experimental contribution is the application of this algorithm to the Dark Pool Problem."}, {"heading": "2 Preliminaries", "text": "We consider the following problem. At each time t, a learner is presented with a quantity or volume V t \u2208 {1, \u00b7 \u00b7 \u00b7 , V } of units, where V t is sampled from an unknown distribution Q. The learner must decide on an allocation vt of these shares to a set of K known venues, with vti \u2208 {0, \u00b7 \u00b7 \u00b7 , V t} for each i \u2208 {1, \u00b7 \u00b7 \u00b7 ,K}, and \u2211K i=1 v t i = V\nt. The learner is then told the number of units rti consumed at each venue i. Here rti = min{sti, vti}, where sti is the maximum consumption level of venue i at time t, which is sampled independently from a fixed but unknown distribution Pi. If rti = v t i , we say that the algorithm receives a censored observation because it is possible to infer only that rti \u2264 sti. If rti < vti , we say that the algorithm receives a direct observation because it must be the case that rti = s t i.\nThe goal of the learner is to discover a near-optimal one-step allocation policy, that is, an allocation policy that approximately optimizes the expected number of units out of V t consumed at each time step t. (We briefly discuss other objectives at the end of Section 4.4.)\nThroughout the remainder of the paper, we use the shorthand Ti for the tail probabilities associated with Pi. That is, Ti(s) = \u2211 s\u2032\u2265s Pi(s\n\u2032). Clearly Ti(0) = 1 for all i. We use T\u0302 ti (s) for an empirical estimate of Ti(s) at time t, and define P\u0302 ti (s) = T\u0302 t i (s) \u2212 T\u0302 ti (s + 1) to be the empirical estimate of Pi(s) at time t."}, {"heading": "3 Greedy Allocation is Optimal", "text": "In this section, we show that given estimates T\u0302i of the tail probabilities Ti for each venue i, a simple greedy allocation algorithm can to maximize the (estimated) expected number of units consumed at a single time step. The greedy algorithm allocates one unit at a time. The venue to which the next unit is allocated is chosen to maximize the estimated probability that the unit will be consumed; if vi units have already been allocated to venue i, then the estimated probability that the next allocated unit will be consumed is simply T\u0302i(vi + 1). A formal description is given as Algorithm 1 below.\nTheorem 1 The allocation returned by Greedy maximizes the expected number of units consumed in a sin-\nAlgorithm 1: Optimal allocation algorithm Greedy.\nInput: Volume V , tail probability estimates {T\u0302i}Ki=1 Output: An allocation v\nv \u2190 0; for \u2190 1 to V do\ni \u2190 argmaxi T\u0302i(vi + 1); vi \u2190 vi + 1 ;\nend return v\ngle time step, where the expectation is taken with respect to the estimated tail probabilities {T\u0302i}Ki=1.\nProof: Using the fact that tail probabilities must satisfy T\u0302i(s) \u2265 T\u0302i(s\u2032) for all s \u2264 s\u2032, it it is easy to verify that by greedily adding units to the venues in decreasing order of T\u0302i(s), Algorithm 1 returns\nargmax v K\u2211 i=1 vi\u2211 s=1 T\u0302i(s) s.t. N\u2211 i=1 vi = V.\nIt remains to show that the expression above is equivalent to the expected number of units consumed. We do this algebraically. For an arbitrary venue i\nEs\u223cP\u0302i [min(s, vi)]\n= \u221e\u2211 s=1 P\u0302i(s)min(s, vi) = vi\u22121\u2211 s=1 sP\u0302i(s) + viT\u0302i(vi) = vi\u22122\u2211 s=1 sP\u0302i(s) + (vi \u2212 1)T\u0302i(vi \u2212 1) + T\u0302i(vi) = T\u0302i(1) + . . .+ T\u0302i(vi \u2212 1) + T\u0302i(vi) = vi\u2211 s=1 T\u0302i(s).\nThe last two lines follow from the observation that for any s, P\u0302i(s \u2212 1) + T\u0302i(s) = T\u0302i(s \u2212 1) and so (s \u2212 1)P\u0302i(s \u2212 1) + sT\u0302i(s) = (s \u2212 1)T\u0302i(s \u2212 1) + T\u0302i(s). Thus\u2211K\ni=1 \u2211vi s=1 T\u0302i(s) = \u2211K i=1 Es\u223cP\u0302i [min(s, vi)], which is\nthe expected number of units consumed."}, {"heading": "4 Censored Exploration Algorithm", "text": "In this section we present our main theoretical result, which is a polynomial-time, near-optimal algorithm for multi-venue exploration from censored data. We first provide an overview of the algorithm and its analysis before diving into the technical details. As mentioned in the Introduction, the analysis bears strong resemblance to the \u201cknown and unknown state\u201d explorationexploitation arguments common in the E3 and RMAX algorithms for reinforcement learning (Kearns and Singh, 2002, Brafman and Tennenholtz, 2003).\nAt the highest level, the algorithm is quite simple and natural. The algorithm maintains estimates T\u0302 ti for the true unknown tail probabilities Ti for each venue i. These estimates \u201cimprove\u201d with time in a particular quantifiable sense which drives the between-venue exploration discussed in the Introduction. At any given time t, the current volume V t is allocated across the venues by simply calling the optimal greedy allocation scheme (Algorithm 1) on the current set of estimated tail probabilities T\u0302 ti . This results in new censored observations from each venue, which in turn are used to update the estimates T\u0302 t+1i used at the next time step. Thus the algorithm, which is given as Algorithm 2, implements a continuous allocate-reestimate loop.\nAlgorithm 2: Main algorithm. Input: Volume sequence V 1, V 2, V 3, . . . Arbitrarily initialize T\u0302 1i for each i; for t \u2190 1, 2, 3, . . . do\n% Allocation Step: vt \u2190 Greedy(V t, T\u0302 t1 , . . . , T\u0302 tK); for i \u2208 {1, . . . ,K} do\nSubmit vti units to venue i; Let rti be the number of shares sold; % Reestimation Step: T\u0302 t+1i \u2190 OptimisticKM ({(v\u03c4i , r\u03c4i )}t\u03c4=1);\nend end\nThe only undetermined part of Algorithm 2 is the subroutine OptimisticKM , which specifies how we estimate T\u0302 ti from the observed data. The most natural choice would be the maximum likelihood estimator on the data. This estimator is well-known in the statistics literature as the Kaplan-Meier estimator. In the following section, we describe Kaplan-Meier and derive a new convergence result that suits our particular needs. This result in turn lets us define an \u201coptimistic tail modification\u201d of Kaplan-Meier that becomes our choice for OptimisticKM .\nThe analysis of Algorithm 2, which is developed in detail over the next few sections, proceeds as follows:\nStep 1: We first review the Kaplan-Meier maximum likelihood estimator for censored data and provide a new finite sample convergence bound for this estimator. This bound allows us to define a \u201ccut-off point\u201d for each venue i such that the Kaplan-Meier estimate of the tail probability Ti(s) for every value of s up to the cut-off point is guaranteed to be \u201cclose to\u201d the true tail probability. We then define a slightly modified version of the Kaplan-Meier estimates in which the tail probability of the next unit above the cut-off is modified in an optimistic manner. We show that in conjunction\nwith the greedy allocation algorithm, this minor modification leads to increased exploration, since the next unit beyond the cut-off point always looks at least as good as the cut-off point itself.\nStep 2: We next prove our main Exploitation Lemma (Lemma 4). This lemma shows that at any time step, if it is the case that the number of units allocated to each venue by the greedy algorithm is strictly below the cut-off point for that venue \u2014 which can be thought of as being in a known state in the parlance of E3 and RMAX \u2014 then the allocation is provably -optimal.\nStep 3: We then prove our main Exploration Lemma (Lemma 5), which shows that on any time step at which the allocation made by the greedy algorithm is not -optimal, it is possible to lower bound the probability that the algorithm explores. Thus, as with E3 and RMAX, anytime we are not in a known state and thus cannot ensure optimal allocation, we are instead assured of exploring.\nStep 4: Finally, we show that on any sufficiently long sequence of time steps (where \u201csufficiently long\u201d is polynomial in K, V , 1/ , and ln(1/\u03b4), where \u03b4 is a standard confidence parameter), it must be the case that either the algorithm achieves an -optimal solution on at least a 1 \u2212 fraction of the sequence, or the algorithm has explored sufficiently often to learn accurate estimates of the tail distributions out to V units on every venue. In either case, we can show that with probability 1\u2212 \u03b4, at the end of the sequence, the current algorithm achieves an -optimal solution with probability at least 1\u2212 ."}, {"heading": "4.1 Convergence of Kaplan-Meier Estimators", "text": "We begin by describing the standard Kaplan-Meier maximum likelihood estimator for censored data (Kaplan and Meier, 1958, Peterson, 1983), restricting our attention to a single venue i. Let zi,s be the true probability that the demand in this venue is exactly s units given that the demand is at least s units. Using the fact that 1\u2212 zi,s is the conditional probability of there being a demand of at least s given that the demand is at least s \u2212 1, it is easy to verify that for any s > 0, Ti(s) = \u220fs\u22121 s\u2032=0 (1\u2212 zi,s\u2032). At a high level, we can think of Kaplan-Meier as first computing an estimate of zi,s for each s and then using these estimates to compute an estimate of Ti(s).\nLet I be an indicator function taking on the value 1 if its input is true and 0 otherwise. Let Dti,s =\u2211t\n\u03c4=1 I[r \u03c4 i = s, v \u03c4 i > s] be the number of direct observations of s units up to time t, and let N ti,s =\n\u2211t \u03c4=1 I[r \u03c4 i \u2265 s, v\u03c4i > s] be the number of (direct or censored) observations of at least s units on time steps at which more than s units were requested. The quantity N ti,s is then the number of times there was an opportunity for a direct observation of s units, whether or not one occurred.\nWe can then naturally define z\u0302ti,s = D t i,s/N t i,s, with z\u0302ti,s = 0 if N t i,s = 0. This quantity is simply the empirical probability of a direct observation of s units given that a direct observation of s units was possible. The Kaplan-Meier estimator of the tail probability for any s > 0 after t time steps can then be expressed as\nT\u0302 ti (s) = s\u22121\u220f s\u2032=0 ( 1\u2212 z\u0302ti,s\u2032 ) , (1)\nwith T\u0302 ti (0) = Ti(0) = 1 for all t.\nPrevious work has established convergence rates for the Kaplan-Meier estimator to the true underlying distribution in the case that the submission sequence v1i , . . . , v t i is i.i.d. (see, for example, Foldes and Rejto (1981)), and asymptotic convergence for non-i.i.d. settings (Huh et al., 2009). We are not in the i.i.d. case, since the submitted volumes at one venue are a function of the entire history of allocations and executions across all venues. In the following theorem we give a new finite sample convergence bound applicable to our setting.\nTheorem 2 Let T\u0302 ti be the Kaplan-Meier estimate of Ti as given in Equation 1. For any \u03b4 > 0, with probability at least 1\u2212 \u03b4, for every s \u2208 {1, \u00b7 \u00b7 \u00b7 , V },\u2223\u2223\u2223T ti (s)\u2212 T\u0302 ti (s)\u2223\u2223\u2223 \u2264 s\u221a2 ln(2V/\u03b4)/N ti,s\u22121. The proof depends on the next lemma, proof omitted.\nLemma 1 For each i \u2208 {1, \u00b7 \u00b7 \u00b7 , }, let xi and yi be real numbers in [0, 1], with |xi \u2212 yi| \u2264 i for some i > 0. Then | \u220f i=1 xi \u2212 \u220f i=1 yi| \u2264 \u2211 i=1 i.\nProof of Theorem 2: We will show that z\u0302ti,s converges to zi,s, and that this implies that the KaplanMeier tail probability estimator converges to Ti(s).\nConsider a fixed value of s. Let tn be the index \u03c4 of the nth time step at which r\u03c4i \u2265 s and v\u03c4i > s. By definition, there are N ti,s such time steps total. For each n \u2208 {1, \u00b7 \u00b7 \u00b7 , N ti,s}, let Xn = \u2211n =1 ( zi,s \u2212 I[rt i = s] ) . It is easy to see that the sequence X1, \u00b7 \u00b7 \u00b7 , XNti,s forms a martingale; for each n, we have that |Xn\u2212Xn+1| \u2264 1 and E [Xn+1|Xn] = Xn. By Azuma\u2019s inequality (see, for example,Alon and Spencer (2000)), for any \u03b3,\nPr (\u2223\u2223\u2223XNti,s \u2223\u2223\u2223 \u2265 \u03b3) \u2264 2e\u2212\u03b32/(2Nti,s).\nNoting that XNti,s = N t i,s(zi,s \u2212 z\u0302ts) and setting \u03b3 = N ti,s gives us that Pr (\u2223\u2223zi,s \u2212 z\u0302ti,s\u2223\u2223 \u2265 ) \u2264 2e\u2212 2Nti,s/2. Setting this equal to \u03b4/V and applying a union bound gives us that with probability 1 \u2212 \u03b4, for every s \u2208 {0, \u00b7 \u00b7 \u00b7 , V \u2212 1}, \u2223\u2223zi,s \u2212 z\u0302ti,s\u2223\u2223 \u2264 \u221a2 ln(2V/\u03b4)/N ti,s. Assume this holds for all s. Then it follows from Lemma 1 that for any particular s \u2208 {1, \u00b7 \u00b7 \u00b7 , V },\n|Ti(s)\u2212 T\u0302 ti (s)|\u2264 s\u22121\u2211 s\u2032=0\n\u221a 2 ln(2V/\u03b4)\nN ti,s\u2032 \u2264s \u221a 2 ln(2V/\u03b4) N ti,s\u22121 ."}, {"heading": "4.2 Modifying Kaplan-Meier", "text": "In Algorithm 3 we describe the minor modification of Kaplan-Meier necessary for our analysis. As described above (Step 1), the value cti in this algorithm can intuitively be viewed as a \u201ccut-off point\u201d up to which we are guaranteed to have sufficient data to accurately estimate the tail probabilities using KaplanMeier. (This is formalized in Lemma 2 below.) Thus for every quantity s \u2264 cti, we simply let T\u0302 ti (s) be precisely the Kaplan-Meier estimate as in Equation 1.\nHowever, to promote exploration, we set the value of T\u0302 ti (c t i+1) optimistically to the Kaplan-Meier estimate of the tail probability at cti (not at c t i + 1). This optimistic modification is necessary to ensure that the greedy algorithm explores (i.e., has a chance of making progress towards increasing at least one cut-off value) on every time step for which it is not already producing an -optimal allocation. In particular, suppose that the current greedy solution allocated no more than cti units to any venue i and exactly ctj units to some venue j. Using the standard Kaplan-Meier tail probability estimates, it could be the case that this allocation is suboptimal (there is no way to know if it would have been better to include unit ctj+1 from venue j in place of a unit from another venue since we do not have an accurate estimate of the tail probability for this unit), and yet no exploration is taking place. By optimistically modifying the tail probability T\u0302 ti (c t i +1) for each venue, we ensure that no venue remains unexplored simply because the algorithm unluckily observes a low demand a small number of times.\nWe now formalize the idea of cti as a \u201ccut-off point\u201d up to which the Kaplan-Meier estimates are accurate. In the results that follow, we think of > 0 and \u03b4 > 0 as fixed parameters of the algorithm.2\nLemma 2 With probability at least 1 \u2212 \u03b4, for all s \u2264 cti, |Ti(s)\u2212 T\u0302 ti (s)| \u2264 /(8V ).\n2In particular, corresponds to the value specified in Theorem 3, and \u03b4 corresponds roughly to that \u03b4 divided by the polynomial upper bound on time steps.\nAlgorithm 3: Subroutine OptimisticKM for computing modified Kaplan-Meier estimators. For all s, let Dti,s and N t i,s be defined as above, and assume that\n> 0 and \u03b4 > 0 are fixed parameters. Input: Observed data ({(v\u03c4i , r\u03c4i )}t\u03c4=1) for venue i Output: Modified Kaplan-Meier estimators for i\n% Calculate the cut-off: cti \u2190 max{s :s=0 or N ti,s\u22121\u2265128(sV/ )2 ln(2V/\u03b4)}; % Compute Kaplan-Meier tail probabilities: T\u0302 ti (0) = 1; for s = 1 to V do\nT\u0302 ti (s) \u2190 \u220fs\u22121 s\u2032=0 ( 1\u2212 (Dti,s\u2032/N ti,s\u2032));\nend\n% Make the optimistic modification: if cti < V then\nT\u0302 ti (c t i + 1) \u2190 T\u0302 ti (cti);\nreturn T\u0302 ti ;\nProof: It is always the case that Ti(0) = T\u0302 ti (0) = 1, so the result holds trivially unless cti > 0. Suppose this is the case. Notice that N ti,s is monotonic in s, with N ti,s \u2265 N ti,s\u2032 whenever s \u2264 s\u2032. Thus by definition of cti, for all s < c t i, N t i,s \u2265 128(sV/ )2 ln(2V/\u03b4). The lemma then follows immediately from an application of Theorem 2.\nLemma 3 shows that it is also possible to achieve additive bounds on the error of tail probability estimates for quantities s much bigger than cti as long as the tail probability at cti is sufficiently small.\nLemma 3 If T\u0302 ti (c t i) \u2264 /(4V ) and the high probability event in Lemma 2 holds, then for all s such that cti < s \u2264 V , |Ti(s)\u2212 T\u0302 ti (s)| \u2264 /(2V ).\nProof: For any s > cti, it must be the case that T\u0302 ti (s) \u2264 T\u0302 ti (cti) \u2264 /(4V ). If the high probability event in Lemma 2 holds, then Ti(s) \u2264 Ti(cti) \u2264 T\u0302 ti (c t i) + /(8V ) \u2264 /(2V ). Since both Ti(s) and T\u0302 ti (s) are constrained to lie between 0 and /(2V ), it must be the case that |Ti(s)\u2212 T\u0302 ti (s)| \u2264 /(2V )."}, {"heading": "4.3 Exploitation and Exploration Lemmas", "text": "With these two lemmas in place, we are ready to state our main Exploitation Lemma (Step 2), which formalizes the idea that once a sufficient amount of exploration has occurred, the allocation output by the greedy algorithm will be -optimal. The proof of this lemma is where the requirement that T\u0302 ti (c t i +1) be set optimistically becomes important. In particular, because of the optimistic setting of T\u0302 ti (c t i + 1), we know that if the greedy policy allocates exactly cti units to a\nvenue i, it could not gain too much by reallocating additional units from another venue to venue i instead. In this sense, we create a \u201cbuffer\u201d above each cut-off, guaranteeing that it is not necessary to continue exploring as long as one of the two conditions in the lemma statement is met for each venue.\nLemma 4 (Exploitation Lemma) Assume that at time t, the high probability event in Lemma 2 holds. If for each venue i, either (1) vti \u2264 cti, or (2) T\u0302 ti (cti) \u2264 /(4V ), the difference between the expected number of units consumed under allocation vt and the expected number of units consumed under the optimal allocation is at most .\nProof: Let a1, \u00b7 \u00b7 \u00b7 , aK be any optimal allocation of the V t units. Since both a and vt are over V t units, it must be the case that \u2211 i:ai>vti ai\u2212 vti = \u2211 i:vti>ai vti \u2212 ai. We can thus define an arbitrary one-to-one mapping between the units that were allocated to different venues by the algorithm and the optimal allocation. Consider any such pair in this mapping. Let i be the venue to which the unit was allocated by the algorithm, and let n be the index of the unit in that venue. Similarly let j be the venue to which the unit was allocated by the optimal allocation, and let m be the index of the unit in that venue.\nIf Condition (1) in the lemma statement holds for venue i, then by Lemma 2, since n \u2264 vti \u2264 cti, T\u0302 ti (n) \u2264 Ti(n) + /(8V ) \u2264 Ti(n) + /(2V ). On the other hand, if Condition (2) holds, then by Lemma 3, it is still the case that T\u0302 ti (n) \u2264 Ti(n) + /(2V ). Now, if vtj < c t j holds with strict inequality, then by Lemma 2, Tj(vtj + 1) \u2264 T\u0302 tj (vtj + 1) + /(2V ). If vtj = ctj , then Tj(v t j + 1) \u2264 Tj(ctj) \u2264 T\u0302 tj (ctj) + /(2V ) = T\u0302 tj (v t j + 1) + /(2V ), where the last equality is due to the optimistic setting of T\u0302 tj (c t j + 1) in the exploration algorithm. Finally, if vtj > c t j , then it must be the case that Condition (2) in the lemma statement holds for venue j and by Lemma 3 it is still the case that Tj(vtj + 1) \u2264 T\u0302 tj (vtj + 1) + /(2V ). Thus in all three cases, since m > vtj , we have that Tj(m) \u2264 Tj(vtj + 1) \u2264 T\u0302 tj (vtj + 1) + /(2V ). Since the greedy algorithm chose to send unit n to venue i instead of sending an additional unit to venue j, it must be the case that T\u0302 ti (n) \u2265 T\u0302 tj (vtj + 1) and thus Ti(n) \u2265 Tj(m)\u2212 /V . Since there are at most V pairs in the matching, and each contributes at most /V to the difference in expected units consumed between the optimal allocation and the algorithm\u2019s, the difference is at most .\nNote that this bound is tight in the sense that it is possible to construct examples where the difference in expected units consumed is as large as .\nFinally, Lemma 5 presents the main exploration lemma (Step 3), which states that on any time step at which the allocation is not -optimal, the probability of a \u201cuseful\u201d observation is lower bounded by /(8V ).\nLemma 5 (Exploration Lemma) Assume that at time t, the high probability event in Lemma 2 holds. If the allocation is not -optimal, then for some venue i, with probability at least /(8V ), N t+1\ni,cti = N ti,cti + 1.\nProof: Suppose the allocation is not -optimal at time t. By Lemma 4, it must be the case that there exists some venue i for which vti > c t i and T\u0302 t i (c t i) > /(4V ). Let be a venue for which this is true. Since vt > c t , it will be the case that N t+1 ,ct = N t ,ct + 1 as long as rt \u2265 ct . Since T\u0302 t (ct ) > /(4V ), Lemma 2 implies that T (ct ) \u2265 /(8V ), so rt \u2265 ct with probability at least /(8V )."}, {"heading": "4.4 Putting It All Together", "text": "With the exploitation and exploration lemmas in place, we are finally ready to state our main theorem. The full proof is omitted due to lack of space, but we sketch the main ideas below.\nTheorem 3 (Main Theorem) For any > 0 and \u03b4 > 0, with probability 1 \u2212 \u03b4 (over the randomness of draws from Q and {Pi}), after running for a time polynomial in K, V , 1/ , and ln(1/\u03b4), Algorithm 2 makes an -optimal allocation on each subsequent time step with probability at least 1\u2212 .\nProof Sketch: Suppose that the algorithm runs for R time steps, where R is a polynomial in K, V , 1/ , and ln(1/\u03b4) (to be determined later). If it is the case that the algorithm was already -optimal on a fraction (1 \u2212 ) of the R time steps, then we can argue that the algorithm will continue to be -optimal on at least a fraction (1 \u2212 ) of future time steps; this is because the algorithm gets better on average over time as more observations are made.\nOn the other hand, if the algorithm chose sub-optimal allocations on at least a fraction of the R time steps, then by Lemma 5, the algorithm must have incremented N ti,cti for some venue i and current cut-off c t i approximately 2R/(8V ) times. By definition of the cti, it can never be the case that N ti,cti was incremented too many times for any fixed values of i and cti (where \u201ctoo many\u201d is a polynomial in V , 1/ , and ln(1/\u03b4)); otherwise the cut-off would have increased. Since there are\nonly K venues and V possible cut-off values to consider in each venue, the total number of increments can be no more than KV times this polynomial, another polynomial in V , 1/ , ln(1/\u03b4), and now K. If R is sufficiently large (but still polynomial in all of the desired quantities) and approximately 2R/(8V ) increments were made, we can argue that every venue must have been fully explored, in which case, again, future allocations will be -optimal.\nWe conclude our theory contributions with a few brief remarks. Our optimistic tail modifications of the Kaplan-Meier estimators are relatively mild 3. In many circumstances we actually expect that the estimate-allocate loop with unmodified Kaplan-Meier would work well, and we investigate a parametric version of this learning algorithm in the experiments described below.\nIt also seems quite likely that variants of our algorithm and analysis could be developed for alternative objective functions, such as minimizing the number of steps of repeated reallocation required to execute all shares (for which it is possible to create examples where myopic greedy allocation at each step is suboptimal), or maximizing the probability of executing all V shares on a single step."}, {"heading": "5 Application: Dark Pool Problem", "text": "We now describe the application that provided the original inspiration to develop our framework and algorithm. As mentioned in the Introduction, dark pools are a particular and relatively recent type of exchange for listed stocks. While the precise details are beyond the scope of this paper, the main challenge in executing large-volume trades in traditional (\u201clight\u201d) exchanges is that it is difficult to \u201cconceal\u201d such trades, and their revelation generally results in adverse impact on price (e.g. the presence of a large-volume buyer causes the price to rise against that buyer). If the volume is sufficiently large, this difficulty of concealment remains even if one attempts to break the trade up slowly over time. Dark pools arose exactly to address the problems faced by large traders, and tend to emphasize trade and order concealment over price optimization (Wikipedia, 2009, Bogoslaw, 2007).\nIn a typical dark pool, buyers and sellers submit orders that simply specify the total volume of shares they wish to buy or sell, with the price of the transaction determined exogenously by \u201cthe market\u201d. 4 Upon sub-\n3The same results can be proved for more invasive modifications, such as pushing all mass above the cut-offs to the maximum possible volume, which would result in even more aggressive exploration.\n4Typically the midpoint between the bid and ask in\nmitting an order to (say) buy v shares, a trader is put in a queue of buyers awaiting transaction, and there is a similar queue of sell orders. Matching between buyers and sellers occurs in sequential arrival of orders, similar to a light exchange. Unlike a light exchange, no information is provided to traders about how many parties or shares might be available in the pool at any given moment. Thus in a given time period, a submission of v shares results only in a (possibly censored) report of how many shares up to v were executed.\nWhile presenting their own trading challenges, dark pools have become tremendously popular exchanges, responsible for executing 10-20% of the overall US equity volume. In fact, they have been so successful that there are now approximately 40+ dark pools for the U.S. Equity market alone, leading traders and brokerages to face exactly the censored multi-venue exploration problem we have been studying: How should one optimally distribute a large trade over the many independent dark pools? We now describe the application of our algorithm to this problem, basing it on actual data from four active dark pools."}, {"heading": "5.1 Summary of Dark Pool Data", "text": "Our data set is from the internal dark pool order flow for a major U.S. broker-dealer. Each (possibly censored) observation in this data is exactly of the form discussed throughout the paper \u2014 a triple consisting of the dark pool name, the number of shares sent to that pool, and the number of shares subsequently executed within a short time interval. It is important to highlight some limitations of the data. First, note that the data set conflates the policy the brokerage used for allocation across the dark pools with the liquidity available in the pools themselves. For our data set, the policy in force was very similar to the banditstyle approach we discuss below. Second, the \u201cparent\u201d orders determining the overall volumes to be allocated across the pools were determined by the brokerage\u2019s trading needs, and are similarly out of our control.\nThe data set contains submissions and executions for four active dark pools: BIDS Trading, Automated Trading Desk, D.E. Shaw, and NYFIX, each for a dozen of relatively actively-traded stocks 5, thus yielding 48 distinct stock-pool data sets. The average daily trading volume of these stocks across all exchanges (light and dark) ranges from 1 to 60 million shares, with a median volume of 15 million shares. Energy, Financials, Consumer, Industrials, and Utilities industries are represented. Our data set spans 30 trading\nthe light exchanges. This is a slight oversimplification but accurate for our purposes.\n5Tickers represented are AIG, ALO, CMI, CVX, FRE, HAL, JPM, MER, MIR, NOV, XOM, and NRG.\ndays; for every stock-pool pair we have on average 1,200 orders (from 600 to 2,000), which corresponds to 1.3 million shares (from 0.5 million to 3 million). Individual order sizes range from 100 to 50,000 shares, with 1,000 shares being the median. 16% of orders are filled at least partially (meaning that fully 84% result in no shares executed), 9% of the total submitted volume was executed, and 11% of all observations were censored."}, {"heading": "5.2 Parametric Models for Dark Pools", "text": "While the theory and algorithm we have developed for censored exploration permit a general (nonparametric) form for the venue distributions Pi, in any application it is reasonable to ask whether the data permits a simple parametric form for these distributions, with the attendant computational and sample complexity benefits. For our dark pool data the answer to this question is affirmative.\nWe experimented with a variety of common parametric forms for the distributions. For each such form, the basic methodology was the same. For each of the 4 \u00d7 12 = 48 venue-stock pairs, the data for that pair was split evenly into a training set and a test set. The training data was used to select the maximum likelihood model from the parametric class. Note that we can no longer directly apply the nonparametric Kaplan-Meier estimator \u2014 within each model class, we must directly maximize the likelihood on the censored training data. This is a relatively straightforward and efficient computation for each of the model classes we investigated. The test set was of course used to measure the generalization performance of each maximum likelihood model.\nOur investigation revealed that the best models maintained a separate parameter for the probability of 0 shares being available (that is, Pi(0) is explicitly estimated) \u2014 a \u201cZero Bin\u201d or ZB parameter. This is due to the fact that the vast majority of submissions (84%) to dark pools result in no shares being executed. We then examined various parametric forms for the nonzero portions of the venue distributions 6, including uniform (which of course requires no additional parameters), and Poisson, exponential and power law forms (each of which requires a single additional parameter).\nThe generalization results strongly favor the power law form, in which the probability of s shares being available is proportional to 1/s\u03b2 for real \u03b2 \u2014 a so-called heavy-tailed distribution when \u03b2 > 0. Nonparametric models trained with Kaplan-Meier are best on the training data but overfit badly due to their complexity\n6These forms were applied up to the largest volume submitted in the data sets, then normalized.\nrelative to the sparse data, while the other parametric forms cannot accommodate the heavy tails of the data. The comparative results are summarized in Table 1. Based on this comparison, for our dark pool study we investigate a variant of our main algorithm, in which the estimate-allocate loop has an estimation step using maximum likelihood estimation within the ZB + Power Law model, and allocations are done greedily on these same models.\nIn terms of the estimated ZB + Power Law parameters themselves, we note that for all 48 stock-pool pairs the Zero Bin parameter accounted for most of the distribution (between a fraction 0.67 and 0.96), which is not surprising considering the aforementioned preponderance of entirely unfilled orders in the data. The vast majority of the 48 exponents \u03b2 fell between \u03b2 = 0.25 and \u03b2 = 1.3 \u2014 so rather long tails indeed \u2014 but it is noteworthy that for one of the four dark pools, 7 of the 12 estimated exponents were actually negative, yielding a model that predicts higher probabilities for larger volumes. This is likely an artifact of our sizeand time-limited data set, but is not entirely unrealistic and results in some interesting behavior in the simulations below."}, {"heading": "5.3 Data-Based Simulation Results", "text": "As in any control problem, the dark pool data in our possession is unfortunately insufficient to evaluate and compare different allocation algorithms. This is because of the aforementioned fact that the volumes submitted to each venue were fixed by the specific policy that generated the data, and we cannot explore alternative choices \u2014 if our algorithm chooses to submit 1000 shares to some venue, but in the data only 500 shares were submitted, we simply cannot infer the outcome of our desired submission.\nWe thus instead use the raw data to derive a simulator with which we can evaluate different approaches. In light of the modeling results of Section 5.2, the simulator for stock S was constructed as follows. For each dark pool i, we used all of the data for i and stock S to estimate the maximum likelihood Zero Bin +\nPower Law distribution. (Note that there is no need for a training-test split here, as we have already separately validated the choice of distributional model.) This results in a set of four venue distribution models Pi that form the simulator for stock S. This simulator accepts allocation vectors (v1, v2, v3, v4) indicating how many shares some algorithm wishes to submit to each venue, draws a \u201ctrue liquidity\u201d value si from Pi for each i, and returns the vector (r1, r2, r3, r4), where ri = min(vi, si) is the possibly censored number of shares filled in venue i.\nAcross all 12 stocks, we compared the performance of four different allocation algorithms:\n\u2022 The ideal allocation, which is given the true parameters of the ZB + Power Law distributions used by the simulator, and allocates shares optimally (greedily) with respect to these distributions.\n\u2022 The uniform allocation, which always divides any order equally among all four venues.\n\u2022 Our learning algorithm, which implements the repeated allocate-reestimate loop, using the maximum likelihood ZB + Power Law model for the reestimation step.\n\u2022 A simple bandit-style algorithm, which begins with equal weights assigned to all four venues. Allocation to a venue which results in any nonzero number of shares being executed causes that venue\u2019s weight to be multiplied by a factor \u03b1 > 1 7. Allocations are always proportional to the current weight vector.\nSome remarks on these algorithms are in order. First, note that the first two allocation methods (ideal and uniform) are non-adaptive and are meant to serve as baselines \u2014 one of them the best performance we could hope for (ideal), and the other the most naive allocation possible (uniform). Second, note that our algorithm has a distinct advantage in the sense that it is using the correct parametric form, the same being used by the simulator itself. Thus our evaluation of this algorithm is certainly optimistic compared to what should be expected \u201cin practice\u201d. Finally, note that the bandit algorithm is the crudest type of weight-based allocation scheme of the type that abounds in the no-regret literature (Cesa-Bianchi and Lugosi, 2006); we are effectively forcing our problem into a 0/1 loss setting corresponding to \u201cno shares\u201d and \u201csome shares\u201d being executed. Certainly more sophisticated bandit-style approaches can and should be examined.\nEach algorithm was run in simulation for some number of episodes . Each episode consisted of the allocation of\n7For the experiments we optimized \u03b1 over all stock-pool pairs and thus used \u03b1 = 1.05.\na fixed number V of shares \u2014 thus the same number of shares is repeatedly allocated by the algorithm, though of course this allocation will change over time for the two adaptive algorithms as they learn. Each episode of simulation results in a some fraction of the V shares being executed. Two values of V were investigated \u2014 a smaller and therefore easier value V = 1000, and the larger and therefore more difficult V = 8000.\nWe begin by showing full learning curves over 2000 episodes with V = 8000 for a couple of representative stocks in Figure 1. Here the average performance of the two non-adaptive allocation schemes (Ideal and Uniform) are represented as horizontal lines, while learning curves are given for the adaptive schemes. Due to high variance of the heavy-tailed venue distributions used by the simulator, a single trial of 2000 episodes is extremely noisy, so we both average over 400 trials for each algorithm, and smooth the resulting averaged learning curve with a standard exponential decay temporal moving average.\nWe see that our learning algorithm converges towards the ideal allocation (as suggested by the theory), often relatively quickly. Furthermore, in each case this ideal asymptote is significantly better than the uniform allocation strawman, meaning that optimal allocations are highly non-uniform. Learning curves for the bandit approach exhibit one of three general behaviors over the set of 12 stocks. In some cases, the bandit approach is quite competitive with our algorithm, though converging to ideal perhaps slightly slower (not shown in Figure 1). In other cases, the bandit approach learns to outperform uniform allocation but appears to asymptote short of the ideal allocation. Finally, in some cases the bandit approach appears to actually \u201clearn the wrong thing\u201d, with performance decaying significantly with more episodes. This happens when\none venue has a very heavy tail, but also a relatively high probability of executing 0 shares, and occurs because the bandit approach does not have an explicit representation of the tails of the distribution.\nThe left column of Figure 2 shows more systematic head-to-head comparisons of our algorithm\u2019s performance versus the ideal, uniform and bandit allocations after 2000 episodes for both small and large V . The values plotted are averages of the last 50 points on learning curves similar to Figure 1. These scatterplots show that across all 12 stocks and both settings of V , our algorithm competes well with the optimal allocation, dramatically outperforms uniform, and significantly outperforms the bandit allocations (especially at the larger volume V = 8000). The average completion rate across all stocks for the large (small) order sequences is 10.0% (13.1%) for uniform and 13.6% (19.4%) for optimal allocations. Our algorithm performs almost as well as optimal \u2014 13.5% (18.7%) \u2014 and much better than bandits at 11.9% (17.2%).\nThe right column of Figure 2 is quite similar, except now we measure performance not by the fraction of V shares filled in one step, but the natural alternative of order half-life \u2014 the number of steps of repeated resubmission of any remaining shares to get the total number executed above V/2. Despite the fact that our algorithm is not designed to optimize this criterion and that our theory does not directly apply to it, we see the same broad story on this metric as well \u2014 our algorithm competes with ideal, dominates uniform allocation and beats the bandit approach on large orders. The average order half-life for large (small) orders is 7.2 (5.3) for uniform allocation and 5.9 (4.4) for the greedy algorithm on the true distributions. Our algorithm requires on average 6.0 (4.9) steps, while bandits uses 7.0 (4.4) to trade the large (small) orders."}, {"heading": "Acknowledgments", "text": "We are grateful to Curtis Pfeiffer and Andrew Westhead for valuable conversations, and to Bobby Kleinberg for introducing us to the literature on the newsvendor problem."}, {"heading": "D. Bogoslaw. Big traders dive into dark pools.", "text": "Business Week article, available at: http: //www.businessweek.com/investor/content/ oct2007/pi2007102_394204.h%tm, 2007."}, {"heading": "R. Brafman and M. Tennenholtz. R-MAX - A general polynomial time algorithm for near-optimal reinforcement", "text": "learning. JMLR, 3:213\u2013231, 2003.\nN. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006."}, {"heading": "A. Foldes and L. Rejto. Strong uniform consistency for nonparametric survival curve estimators from randomly", "text": "censored data. The Annals of Statistics, 9(1):122\u2013129, 1981."}, {"heading": "W. T. Huh, R. Levi, P. Rusmevichientong, and J. Orlin. Adaptive data-driven inventory control policies", "text": "based on Kaplan-Meier estimator. Preprint available at http://legacy.orie.cornell.edu/~paatrus/ psfiles/km-myopic.pdf, 2009."}, {"heading": "E. L. Kaplan and P. Meier. Nonparametric estimation from", "text": "incomplete observations. JASA, 53:457\u2013481, 1958.\nM. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. MLJ, 49:209\u2013232, 2002.\nA. V. Peterson. Kaplan-Meier estimator. In Encyclopedia of Statistical Sciences. Wiley, 1983.\nWikipedia. Dark pools of liquidity. http://en.wikipedia. org/wiki/Dark_pools_of_liquidity, 2009."}], "references": [{"title": "The Probabilistic Method, 2nd Edition", "author": ["N. Alon", "J. Spencer"], "venue": null, "citeRegEx": "Alon and Spencer.,? \\Q2000\\E", "shortCiteRegEx": "Alon and Spencer.", "year": 2000}, {"title": "Big traders dive into dark pools", "author": ["D. Bogoslaw"], "venue": "Business Week article, available at: http: //www.businessweek.com/investor/content/ oct2007/pi2007102_394204.h%tm,", "citeRegEx": "Bogoslaw.,? \\Q2007\\E", "shortCiteRegEx": "Bogoslaw.", "year": 2007}, {"title": "R-MAX - A general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R. Brafman", "M. Tennenholtz"], "venue": "JMLR, 3:213\u2013231,", "citeRegEx": "Brafman and Tennenholtz.,? \\Q2003\\E", "shortCiteRegEx": "Brafman and Tennenholtz.", "year": 2003}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "By Azuma\u2019s inequality (see, for example,Alon and Spencer (2000)), for any \u03b3,", "startOffset": 40, "endOffset": 64}, {"referenceID": 3, "context": "Finally, note that the bandit algorithm is the crudest type of weight-based allocation scheme of the type that abounds in the no-regret literature (Cesa-Bianchi and Lugosi, 2006); we are effectively forcing our problem into a 0/1 loss setting corresponding to \u201cno shares\u201d and \u201csome shares\u201d being executed.", "startOffset": 147, "endOffset": 178}], "year": 2009, "abstractText": "We introduce and analyze a natural algorithm for multi-venue exploration from censored data, which is motivated by the Dark Pool Problem of modern quantitative finance. We prove that our algorithm converges in polynomial time to a near-optimal allocation policy; prior results for similar problems in stochastic inventory control guaranteed only asymptotic convergence and examined variants in which each venue could be treated independently. Our analysis bears a strong resemblance to that of efficient exploration/exploitation schemes in the reinforcement learning literature. We describe an extensive experimental evaluation of our algorithm on the Dark Pool Problem using real trading data.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}