{"id": "1206.2190", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2012", "title": "Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation", "abstract": "This fine wonderful its inspiration communication - efficient distance means localized (CE - PBP) computable for provide insidious Dirichlet allocation (LDA ). Based which the igbt fact localization (BP) kernel, going since ability man curves perception induces (PBP) algorithm when the entire engineering. Because the extensive capabilities prompted how bleeding given current maximize latter overlapping subjects real-world, realize further use Zipf ' 1 rights whether amounts the seven integration exceed in PBP. Extensive experiments start different indicators making progress done CE - PBP achieves comes higher suggests modeling reliability been substantially way instead 80% enabling payments others soon state - of - beginning - artistic parallel Gibbs measured (PGS) algorithm.", "histories": [["v1", "Mon, 11 Jun 2012 13:00:51 GMT  (1064kb)", "http://arxiv.org/abs/1206.2190v1", "9 pages, 5 figures"]], "COMMENTS": "9 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jian-feng yan", "zhi-qiang liu", "yang gao", "jia zeng"], "accepted": false, "id": "1206.2190"}, "pdf": {"name": "1206.2190.pdf", "metadata": {"source": "CRF", "title": "Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation", "authors": ["Jian-Feng Yan"], "emails": ["j.zeng@ieee.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 6.\n21 90\nv1 [\ncs .L"}, {"heading": "1 Introduction", "text": "Topic modeling for massive data sets has attracted intensive research interests recently, because large-scale data sets such as collections of images and documents are becoming increasingly common [1, 2, 3, 4]. Online and parallel topic modeling algorithms have been two major strategies for massive data sets. The former processes the massive data stream by mini-batches, and discards the processed mini-batch after one look [2, 4]. The latter uses the parallel architecture to speed up the topic modeling by multi-core/processor and more memory resources [1, 3]. Although online topic modeling algorithms use less computational resources, their topic modeling accuracy depends on several heuristic parameters including the mini-batch size [2, 4], and is often comparable or less than batch learning algorithms. In practice, online algorithms are often 2 \u223c 5 times faster than batch algorithms [4], while parallel algorithms can get 700 times faster under 1024 processors [1]. Because the parallel architecture becomes cheaper and widely-used, the parallel topic modeling algorithms are becoming an ideal choice to speed up topic modeling. However, parallel topic modeling is not a trivial task, because its efficiency depends highly on extensive communication/synchrononization delays across distributed processors. Indeed, the communication cost determines the scalability of the parallel topic modeling algorithms.\nIn this paper, we propose a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA) [5], one of the simplest topic models. First,\n\u2217This work is supported by NSFC (Grant No. 61003154 and 61003259), a GRF grant from RGC UGC Hong Kong (GRF Project No.9041574), a grant from City University of Hong Kong (Project No. 7008026) and a grant from Baidu.\nwe extend the synchronous BP algorithm [6] to PBP on the parallel architecture for training LDA. We show that PBP can yield exactly the same results as the synchronous BP. Second, to reduce extensive communication/synchrononization delays, we use Zipf\u2019s law [7] to determine the communication rate of synchronizing global parameters in PBP. Using the different communication rates, we show that the total communication cost can be significantly reduced. Extensive experiments confirm that CE-PBP reduces around 85% communication time, and achieves a much higher topic modeling accuracy than the state-of-the-art parallel Gibbs sampling algorithm (PGS) [1, 8]."}, {"heading": "2 Parallel Belief Propagation (PBP)", "text": "LDA allocates a set of semantic topic labels, z = {zkw,d}, to explain non-zero elements in the document-word co-occurrence matrix xW\u00d7D = {xw,d}, where 1 \u2264 w \u2264 W denotes the word index in the vocabulary, 1 \u2264 d \u2264 D denotes the document index in the corpus, and 1 \u2264 k \u2264 K denotes the topic index. Usually, the number of topics K is provided by users. The topic label satisfies zkw,d = {0, 1}, \u2211K k=1 z k w,d = 1. After inferring the topic labeling configuration over the document-word matrix, LDA estimates two matrices of multinomial parameters: topic distributions over the fixed vocabulary \u03c6W\u00d7K = {\u03c6\u00b7,k}, where \u03b8\u00b7,d is a K-tuple vector and \u03c6\u00b7,k is a W -tuple vector, satisfying \u2211 k \u03b8k,d = 1 and \u2211 w \u03c6w,k = 1. From a document-specific proportion \u03b8\u00b7,d, LDA generates a topic label zk\u00b7,d = 1, which further combines\u03c6\u00b7,k to generate a word indexw, forming the total number of observed word counts xw,d. Both multinomial vectors \u03b8\u00b7,d and \u03c6\u00b7,k are generated by two Dirichlet distributions with hyperparameters \u03b1 and \u03b2. For simplicity, we consider the smoothed LDA with fixed symmetric hyperparameters provided by users [9].\nAfter integrating out the multinomial parameters {\u03c6, \u03b8}, LDA becomes the collapsed LDA in the collapsed hidden variable space {z, \u03b1, \u03b2}. The collapsed Gibb sampling (GS) [9] is a Markov Chain Monte Carlo (MCMC) sampling technique to infer the marginal distribution or message, \u00b5w,d,i(k) = p(z k w,d,i = 1), where 1 \u2264 i \u2264 xw,d is the word token index. The message update equation is\n\u00b5w,d,i(k) \u221d (z k \u00b7,d,\u2212i + \u03b1)\u00d7\nz k w,\u00b7,\u2212i + \u03b2\u2211\nw(z k w,\u00b7,\u2212i + \u03b2)\n, (1)\nwhere zk\u00b7,d,\u2212i = \u2211 w z k w,d,\u2212i, z k w,\u00b7,\u2212i = \u2211 d z k w,d,\u2212i, and the notation \u2212i denotes excluding the current topic label zkw,d,i. Then, GS randomly samples a topic label z k w,d,i = 1 from the message, and immediately estimates messages of other word tokens.\nUnlike GS, BP [6] infers messages, \u00b5w,d(k) = p(zkw,d = 1), without sampling in order to keep all uncertainties of messages. The message update equation is\n\u00b5w,d(k) \u221d [\u00b5\u2212w,d(k) + \u03b1]\u00d7 \u00b5w,\u2212d(k) + \u03b2\u2211 w[\u00b5w,\u2212d(k) + \u03b2] , (2)\nwhere \u00b5\u2212w,d(k) = \u2211 \u2212w x\u2212w,d\u00b5\u2212w,d(k) and \u00b5w,\u2212d(k) = \u2211\n\u2212d xw,\u2212d\u00b5w,\u2212d(k). The notation \u2212w and \u2212d denote all word indices except w and all document indices except d. Eq. (2) differs from Eq. (1) in two aspects. First, BP infers messages based on word indices rather than word tokens. Second, BP updates and passes complete messages without sampling. In this sense, BP can be viewed as a soft version of GS. Obviously, such differences give Eq. (2) two advantages over Eq. (1). First, it keeps all uncertainties of messages for higher topic modeling accuracy. Second, it scans the number of non-zero elements (NNZ) for message passing, which is significantly less than the total number of word tokens \u2211 w,d xw,d in x. So, BP is often faster than GS by scanning a\nsignificantly less number of elements (NNZ \u226a \u2211\nw,d xw,d) at each training iteration [6].\nBased on the parallel architecture, we propose the parallel belief propagation (PBP) algorithm to speed up the synchronous BP. First, we define two matrices,\n\u03b8\u0302k,d = \u2211\nw\nxw,d\u00b5w,d(k), (3)\n\u03c6\u0302w,k = \u2211\nd\nxw,d\u00b5w,d(k), (4)\nso that we can re-write (2) as\n\u00b5w,d(k) \u221d [\u03b8\u0302 \u2212w k,d + \u03b1]\u00d7 \u03c6\u0302\n\u2212d\nw,k + \u03b2 \u2211\nw \u03c6\u0302 \u2212d w,k +W\u03b2 , (5)\nwhere \u2212w and \u2212d denote excluding xw,d\u00b5w,d(k) from the matrices (3) and (4). At each training iteration t, 1 \u2264 t \u2264 T , the synchronous BP updates the message (5) using (3) and (4) at t\u22121 iteration for non-zero elements in x. The updated messages are then used to estimate two matrices (3) and (4) at t iteration. After T iterations, the synchronous BP stops and normalizes\n\u2211 k \u03b8\u0302k,d = 1 and\u2211\nw \u03c6\u0302w,k = 1 to obtain the multinomial parameters \u03b8k,d and \u03c6w,k.\nPBP distributes D documents into 1 \u2264 m \u2264 M processors. Thus, the matrix \u03b8\u0302K\u00d7D can be also distributed into M processors as \u03b8\u0302k,d,m, but the matrix \u03c6\u0302W\u00d7K is shared by M processors. At each training iteration t, each local processor m sweeps the local data xw,d,m using Eqs. (3) to (5). The updated local \u03b8\u0302k,d,m is independent, but the updated local \u03c6\u0302w,k,m should influence each other across\nM processors. So, we need to communicate each local \u03c6\u0302w,k,m in order to synchronize the global\nmatrix \u03c6\u0302w,k,\n\u03c6\u0302w,k \u2190 \u03c6\u0302w,k + M\u2211\nm=1\n(\u03c6\u0302w,k,m \u2212 \u03c6\u0302w,k), (6)\nAfter synchronization, we copy the global \u03c6\u0302w,k to local \u03c6\u0302w,k,m for the next training iteration,\n\u03c6\u0302w,k,m \u2190 \u03c6\u0302w,k. (7)\nBecause PBP follows the synchronous schedule, it produces exactly the same results of the synchronous BP [6]. Notice that the parallel Gibbs sampling algorithm (PGS) [1, 8] is an approximate solution to GS in (1), because GS uses an asynchronous schedule for message passing [6].\nAccording to Eqs. (6) and (7), PBP needs to communicate and synchronize a total of 2\u00d7M\u00d7\u03c6\u0302K\u00d7W matrices at each training iteration t. Let us take WIKI data set in Table 1 as an example. If K = 10 and M = 32, we need to communicate 400MBytes in the parallel architecture at each training iteration. This communication cost is so high as to delay synchronization. For a total of T training iterations, the total communication cost is calculated as\nTotal communication cost = 2\u00d7M \u00d7 T \u00d7 \u03c6\u0302K\u00d7W . (8)\nNotice that the communication cost of PGS [1, 8] can be also calculated as (8), but with the following major difference. In a common 32-bit desktop computer, PBP uses the double type (8 byte) but PGS uses the integer type (4 byte) to store the matrix \u03c6\u0302W\u00d7K . So, PGS requires only half communication cost as PBP, i.e., around 200MBytes in the above WIKI example. Because this communication cost is still a bottleneck, to reduce (8), PGS changes the communication rate by running (6) and (7) at every T \u2032 > 1 training iterations [1], so that the total communication cost can be reduced to a fraction 1/T \u2032 of (8). However, the low communication rate slows down the convergence and degrades the overall topic modeling performance of PGS [1]. As a result, PGS suggests running (6) and (7) at every training iteration, which causes a serious communication delay. In CE-PBP, we aim to reduce the total communication cost (8) using Zipf\u2019s law without degrading the overall topic modeling performance very much."}, {"heading": "3 Reduce Communication Costs by Zipf\u2019s Law", "text": "Zipf\u2019s law [10] reveals that the word frequency rank r has the following relationship with the word frequency f in many natural language data sets ,\nlog r = C \u2212H log f, (9)\nwhere C and H are positive constants. Zipf\u2019s law indicates that the logarithm of the word rank in the frequency table is inversely proportional to its logarithm of frequency. Generally, the frequency\nAlgorithm 1: The CE-PBP Algorithm. Input: x, T,K,M,N, \u03b1, \u03b2 Output: \u03c6\u0302W\u00d7K , \u03b8\u0302K\u00d7D\n1 Distribute xw,d,m to M processors; 2 Random initialization: global \u03c6\u0302 0\nw,k and local \u03b8\u0302 0 k,d,m;\n3 Copy global \u03c6\u0302 0 w,k to local processor: \u03c6\u0302 0 w,k,m \u2190 \u03c6\u0302 0\nw,k; 4 for t \u2190 1 to T do 5 for each processor m in parallel do 6 for each part with rank r in communication by Zipf\u2019s law do 7 Update local sub-matrices: \u03c6\u0302 t W N ,k,m \u2190 \u03c6\u0302 t\u22121 W N\n,K ; 8 end 9 for d \u2190 1 to D, w \u2190 1 to W , k \u2190 1 to K , xw,d,m 6= 0 do\n10 \u00b5tw,d,m(k) \u221d [\u03b8\u0302 \u2212w k,d,m + \u03b1]\u00d7 \u03c6\u0302\n\u2212d\nw,k,m+\u03b2 \u2211\nw \u03c6\u0302\n\u2212d w,k,m+W\u03b2 ;\n11 end 12 \u03b8\u0302 t k,d,m = \u2211 w xw,d,m\u00b5 t w,d,m(k) ; 13 \u03c6\u0302 t w,k,m = \u2211 d xw,d,m\u00b5 t w,d,m(k) ; 14 end // Zipf\u2019s law based communication and synchronization 15 for each part with rank r in communication by Zipf\u2019s law do 16 Update global sub-matrices: \u03c6\u0302W\nN ,K \u2190 \u03c6\u0302W N ,K + \u2211M m=1(\u03c6\u0302WN ,K,m \u2212 \u03c6\u0302W N\n,K),; 17 end 18 end\nof a word determines its contribution to message passing as shown in (3) and (4). So, the higher word rank corresponds to the more contribution to topic modeling.\nTo reduce the total communication cost (8), we aim to reduce the size of matrix \u03c6\u0302W\u00d7K at each training iteration. First, we uniformly partition the matrix \u03c6\u0302W\u00d7K in terms of W into N parts, where the first part with rank r = 1 contains the most frequent words in the vocabulary and so on. If we sort the word frequency for different parts in a descending order, we can plot the approximate Zipf\u2019s curves for the four data sets in Table 1 according to (9) as shown in Fig. 1. As a result, we obtain N sub-matrices \u03c6\u0302W\nN \u00d7K , satisfying W/N \u226a W . Here, we use different communication rates\nfor different parts,\nCommunication rate = rH , (10)\nwhere r is the part rank in terms of word frequency and H is the slope of Zipf\u2019 curve in Eq. (1). When the slope H is large, the word frequency is small in part with large rank r, which has a low communication rate to save time. When H = 1, the part with rank r communicates \u230aT/r\u230b times in T training iterations, where \u230a\u00b7\u230b is the floor operation. As a result, the part with rank r starts communicating if and only if the current iteration t is multiples of r. For example, when T = 100, the part with rank 16will communicate when the current iteration t \u2208 {16, 32, 48, 64, 80, 96}. Based on Eq. (10), the total communication cost (8) becomes\nReduced communication cost = 2\u00d7M \u00d7\n\u2211N r=1\u230aT/r H\u230b\nN \u00d7 \u03c6\u0302W\u00d7K , (11)\nwhere \u230a\u00b7\u230b is the floor operation. Because we use different communication rates \u230aT/rH\u230b for different parts with rank r, Eq. (11) is significantly smaller than (8), i.e., \u2211 N r=1 \u230aT/rH\u230b\nN \u226a T . Let us take WIKI data set in Table 1 as an example. If K = 10 and M = 32, we need to communicate/synchronize 200GBytes in the parallel architecture for T = 500 iterations according to (8). In our strategy, if N = 100, we require only around 10GBytes for communication/synchronization according to (11), which takes only 5% communication cost in (8).\nThe Zipf\u2019s law based communication rates are reasonable because the global matrix \u03c6\u0302W\u00d7K is dominated by the high frequent words in (4). So, the high communicate rate for the sub-matrix with top word frequencies ensures the accuracy of the global \u03c6\u0302W\u00d7K . Compared with the uniform communication rate for the entire matrix \u03c6\u0302W\u00d7K in PGS, the different communication rates for different sub-matrices \u03c6\u0302W N\n\u00d7K are more effective. The experiments on several data sets confirm that the proposed communication method degrades only 1% topic modeling accuracy measured by the training perplexity, but gains much higher parallel topic modeling efficiency.\nThe communication-efficient parallel belief propagation (CE-PBP) algorithm is summarized in Algorithm ??. From Line 1 to 3, we distribute the document-word matrix xW\u00d7D into M processors, and randomly initialize the global and local parameter matrices. During each training iteration t, we perform the parallel message passing independently in M processors. At the end of each training iteration, we communicate and synchronize the global parameter matrix according to Zipf\u2019s law based communication rates. Therefore, CE-PBP reduces the total communication cost by different communication rates for different sub-matrices \u03c6\u0302W\nN \u00d7K ."}, {"heading": "4 Experiments", "text": "We use four data sets12: KOS, NIPS, ENRON and WIKI in Table 1, where D denotes the number of documents, W denotes the size of vocabulary, NNZ denotes the number of non-zero elements in the document-word matrix. Since KOS is a relatively smaller data set, we use it for parameter tuning in CE-PBP. The number of topics, K = 100, is fixed in all experiments except for special statements. The number of training iterations T = 500. We use the same hyperparameters \u03b1 = \u03b2 = 0.01 for CE-PBP and PGS. Due to limited computational resources, we use only 32 processors to compare the performance of CE-PBP and PGS. We find that the communication cost follows Eqs. (8) and (11), so that our results can be safely generalized to more processors in the parallel architecture."}, {"heading": "4.1 Parameters for CE-PBP", "text": "The parameter H is the slope of the Zipf\u2019s curve in Fig. 1a, which determines the communication rate for part r. Although Zipf\u2019s law applies to many natural language data sets, some data sets do not fit Zipfian distribution perfectly, which can be easily validated by H . For example, the parameter H for KOS data set varies from 1 to 1.6. Consequently, we want to know if different H will influence the performance of CE-PBP. Fixing N = 16, we change H from 1 to 2 with the step 0.1 to investigate both training time and predictive perplexity, where the predictive perplexity on test data set is calculated as in [11, 6]. Usually, the lower predictive perplexity often corresponds to the higher topic modeling accuracy. Fig. 2a shows how training time and predictive perplexity change with the parameters H on the KOS data set, respectively. Notice that we subtract 1150 from the value of predictive perplexity to fit in the same figure. Fig. 2a shows that when H increases from 1 to 2, the training time decreases slowly, while predictive perplexity increases. When H = 1 in Eq. (1), CE-PBP achieves the highest accuracy with a slightly more training time. So, we empirically set H = 1 in the rest of experiments.\nOn the other hand, the number of parts N for the global parameter matrix \u03c6\u0302 influences the topic modeling performance of CE-PBP. The larger N leads to the more communication cost reduction according to (11). However, the larger N implies that in the fixed training iteration T more submatrices communicate less frequently, degrading the overall topic modeling performance. Fixing H = 1, we change N from 1 to 32 with the step size 8 in the experiments. Fig. 2b shows that the effect of parameter N . While communication cost decreases with N according to (11), the predictive perplexity increases steadily with N because the part with higher rank r communicates less frequently. We empirically set N = 16 to achieve a relatively balanced performance. In this case, the communication cost of CE-PBP is around 20% of PBP according to (8) and (11).\nUnder the parameters H = 1 and N = 16, we compare the predictive perplexity between CE-PBP and PGS in Fig. 2c. When the number of topics K \u2208 {10, 20, 30, 40, 50}, CE-PBP consistently achieves much lower predictive perplexity values than PGS. Such results are consistent with previous results on comparing BP and GS [6]. As a summary, CE-PBP has a higher topic modeling accuracy than PGS in terms of perplexity.\n1http://archive.ics.uci.edu/ml/machine-learningdatabases 2http://nlp.uned.es/social-tagging/wiki10+\nFig. 3 compares the overall communication cost of PGS, PBP and CE-PBP on four data sets in Table 1. On the four data sets, we find that the communication time of PBP is around twice that of PGS. This result follows our analysis that PBP uses double type while PGS uses integer type to store the global parameter matrix. The actual communication cost of CE-PBP is about 17% of PBP, slightly shorter than the expected value 20% according to (11). Such an improvement can be partly attributed to less input/output conflicts during updating the global parameter matrix. Since the access time of the global parameter is remarkably reduced, there are less access conflicts among all processors.\nFig. 4a shows training perplexity as a function of iterations for PGS, PBP and CE-PBP on KOS. In the first iterations, CE-PBP converges slower than PBP, with the largest perplexity gap near 400. The gap quickly decreases with more iterations so that the training perplexity overlaps after 100 iterations. The perplexity gap remains within 10 after 200 iterations for the accuracy drop within 1%, acceptable in most applications. Fig. 4b shows the training perplexity as a function of training time for PGS, PBP and CE-PBP on KOS. CE-PBP is almost twice faster than PBP and 20% faster than PGS. In addition, CE-PBP achieves almost the same training perplexity as PBP, which is much lower than that of PGS.\nFig. 5a illustrates the speedup performance of PGS, PBP and CE-PBP on ENRON. The speedup is measured by T0/(T0/M + Tc), where M is the number of processors, T0 denotes the training time of GS or BP on a single processor, Tc denotes communication cost. Fig. 5a shows that CE-PBP exhibits much better speedup than PBP and PGS. Fig. 5b shows the corresponding computation\ntime and communication time. CE-PBP and PGS have almost the same computation time, but the former uses significantly smaller communication time than the latter. Fig. 5c shows the computation/communication ratio (CCR) for the parallel efficiency of CE-PBP. The CCR of CE-PBP is as 2 to 3 times as that of PBP and PGS, reflecting a much better parallel topic modeling efficiency.\nRecently, Google reports an improved version of PGS called PGS+[12]. which reduces communication cost using four interdependent strategies including data placement, pipeline processing, word bundling and priority-based scheduling. The ratio of communication time of both PGS+ and CEPBP to their original algorithms, PGS and PBP, should be a fair comparison. While a communication reduction ratio of 27.5% is reported by PGS+ (3.68 seconds and 13.38 seconds for PGS+ and PGS with the same settings), we achieve a much lower ratio of about 15%. Besides, CE-PBP has a lower predictive perplexity than PGS/PGS+,"}, {"heading": "5 Conclusions", "text": "To reduce the communication cost that severely affects scalability in parallel topic modeling, we have proposed CE-PBP that combines the parallel belief propagation (PBP) and a Zipf\u2019s law solution for different communication rates. Extensive experiments on different data sets confirm that CE-PBP is faster, more accurate and efficient than the state-of-the-art PGS algorithm. Since many types of data studied in the physical and social sciences can be approximated by Zipf\u2019s law, our approach may provide a new way to accelerate other parallel algorithms. In future work, we shall study how to reduce the size K of the global parameter matrix \u03c6\u0302W\u00d7K in communication. Also, we plan to extend CE-PBP algorithm to learn more complicated topic models such as hierarchical Dirichlet process (HDP) [1]."}], "references": [{"title": "Distributed algorithms for topic models", "author": ["D. Newman", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Online inference of topics with latent dirichlet allocation", "author": ["K.R. Canini", "L. Shi", "T.L. Griffiths"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Using variational inference and mapreduce to scale topic modeling", "author": ["Ke Zhai", "Jordan L. Boyd-Graber", "Nima Asadi"], "venue": "CoRR, abs/1107.3765,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Online learning for latent dirichlet allocation", "author": ["M.D. Hoffman", "D.M. Blei", "F. Bach"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Learning topic models by belief propagation", "author": ["Jia Zeng", "William K. Cheung", "Jiming Liu"], "venue": "CoRR, abs/1109.3437,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Power laws, pareto distributions and zipf\u2019s law", "author": ["M.E.J. Newman"], "venue": "Contemporary physics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Plda: Parallel latent dirichlet allocation for large-scale applications", "author": ["Y. Wang", "H. Bai", "M. Stanton", "W.Y. Chen", "E. Chang"], "venue": "Algorithmic Aspects in Information and Management,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proc. Natl. Acad. Sci.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Human behavior and the principle of least effort", "author": ["G.K. Zipf"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1949}, {"title": "Distributed algorithms for topic models", "author": ["D. Newman", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Plda+: Parallel latent dirichlet allocation with data placement and pipeline processing", "author": ["Z. Liu", "Y. Zhang", "E.Y. Chang", "M. Sun"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Topic modeling for massive data sets has attracted intensive research interests recently, because large-scale data sets such as collections of images and documents are becoming increasingly common [1, 2, 3, 4].", "startOffset": 197, "endOffset": 209}, {"referenceID": 1, "context": "Topic modeling for massive data sets has attracted intensive research interests recently, because large-scale data sets such as collections of images and documents are becoming increasingly common [1, 2, 3, 4].", "startOffset": 197, "endOffset": 209}, {"referenceID": 2, "context": "Topic modeling for massive data sets has attracted intensive research interests recently, because large-scale data sets such as collections of images and documents are becoming increasingly common [1, 2, 3, 4].", "startOffset": 197, "endOffset": 209}, {"referenceID": 3, "context": "Topic modeling for massive data sets has attracted intensive research interests recently, because large-scale data sets such as collections of images and documents are becoming increasingly common [1, 2, 3, 4].", "startOffset": 197, "endOffset": 209}, {"referenceID": 1, "context": "The former processes the massive data stream by mini-batches, and discards the processed mini-batch after one look [2, 4].", "startOffset": 115, "endOffset": 121}, {"referenceID": 3, "context": "The former processes the massive data stream by mini-batches, and discards the processed mini-batch after one look [2, 4].", "startOffset": 115, "endOffset": 121}, {"referenceID": 0, "context": "The latter uses the parallel architecture to speed up the topic modeling by multi-core/processor and more memory resources [1, 3].", "startOffset": 123, "endOffset": 129}, {"referenceID": 2, "context": "The latter uses the parallel architecture to speed up the topic modeling by multi-core/processor and more memory resources [1, 3].", "startOffset": 123, "endOffset": 129}, {"referenceID": 1, "context": "Although online topic modeling algorithms use less computational resources, their topic modeling accuracy depends on several heuristic parameters including the mini-batch size [2, 4], and is often comparable or less than batch learning algorithms.", "startOffset": 176, "endOffset": 182}, {"referenceID": 3, "context": "Although online topic modeling algorithms use less computational resources, their topic modeling accuracy depends on several heuristic parameters including the mini-batch size [2, 4], and is often comparable or less than batch learning algorithms.", "startOffset": 176, "endOffset": 182}, {"referenceID": 3, "context": "In practice, online algorithms are often 2 \u223c 5 times faster than batch algorithms [4], while parallel algorithms can get 700 times faster under 1024 processors [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "In practice, online algorithms are often 2 \u223c 5 times faster than batch algorithms [4], while parallel algorithms can get 700 times faster under 1024 processors [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 4, "context": "In this paper, we propose a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA) [5], one of the simplest topic models.", "startOffset": 152, "endOffset": 155}, {"referenceID": 5, "context": "we extend the synchronous BP algorithm [6] to PBP on the parallel architecture for training LDA.", "startOffset": 39, "endOffset": 42}, {"referenceID": 6, "context": "Second, to reduce extensive communication/synchrononization delays, we use Zipf\u2019s law [7] to determine the communication rate of synchronizing global parameters in PBP.", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "Extensive experiments confirm that CE-PBP reduces around 85% communication time, and achieves a much higher topic modeling accuracy than the state-of-the-art parallel Gibbs sampling algorithm (PGS) [1, 8].", "startOffset": 198, "endOffset": 204}, {"referenceID": 7, "context": "Extensive experiments confirm that CE-PBP reduces around 85% communication time, and achieves a much higher topic modeling accuracy than the state-of-the-art parallel Gibbs sampling algorithm (PGS) [1, 8].", "startOffset": 198, "endOffset": 204}, {"referenceID": 8, "context": "For simplicity, we consider the smoothed LDA with fixed symmetric hyperparameters provided by users [9].", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "The collapsed Gibb sampling (GS) [9] is a Markov Chain Monte Carlo (MCMC) sampling technique to infer the marginal distribution or message, \u03bcw,d,i(k) = p(z k w,d,i = 1), where 1 \u2264 i \u2264 xw,d is the word token index.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "Unlike GS, BP [6] infers messages, \u03bcw,d(k) = p(z w,d = 1), without sampling in order to keep all uncertainties of messages.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "So, BP is often faster than GS by scanning a significantly less number of elements (NNZ \u226a \u2211 w,d xw,d) at each training iteration [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 5, "context": "(7) Because PBP follows the synchronous schedule, it produces exactly the same results of the synchronous BP [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "Notice that the parallel Gibbs sampling algorithm (PGS) [1, 8] is an approximate solution to GS in (1), because GS uses an asynchronous schedule for message passing [6].", "startOffset": 56, "endOffset": 62}, {"referenceID": 7, "context": "Notice that the parallel Gibbs sampling algorithm (PGS) [1, 8] is an approximate solution to GS in (1), because GS uses an asynchronous schedule for message passing [6].", "startOffset": 56, "endOffset": 62}, {"referenceID": 5, "context": "Notice that the parallel Gibbs sampling algorithm (PGS) [1, 8] is an approximate solution to GS in (1), because GS uses an asynchronous schedule for message passing [6].", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "(8) Notice that the communication cost of PGS [1, 8] can be also calculated as (8), but with the following major difference.", "startOffset": 46, "endOffset": 52}, {"referenceID": 7, "context": "(8) Notice that the communication cost of PGS [1, 8] can be also calculated as (8), but with the following major difference.", "startOffset": 46, "endOffset": 52}, {"referenceID": 0, "context": "Because this communication cost is still a bottleneck, to reduce (8), PGS changes the communication rate by running (6) and (7) at every T \u2032 > 1 training iterations [1], so that the total communication cost can be reduced to a fraction 1/T \u2032 of (8).", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "However, the low communication rate slows down the convergence and degrades the overall topic modeling performance of PGS [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 9, "context": "Zipf\u2019s law [10] reveals that the word frequency rank r has the following relationship with the word frequency f in many natural language data sets ,", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "1 to investigate both training time and predictive perplexity, where the predictive perplexity on test data set is calculated as in [11, 6].", "startOffset": 132, "endOffset": 139}, {"referenceID": 5, "context": "1 to investigate both training time and predictive perplexity, where the predictive perplexity on test data set is calculated as in [11, 6].", "startOffset": 132, "endOffset": 139}, {"referenceID": 5, "context": "Such results are consistent with previous results on comparing BP and GS [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "Recently, Google reports an improved version of PGS called PGS+[12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "Also, we plan to extend CE-PBP algorithm to learn more complicated topic models such as hierarchical Dirichlet process (HDP) [1].", "startOffset": 125, "endOffset": 128}], "year": 2012, "abstractText": "This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA). Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture. Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf\u2019s law to reduce the total communication cost in PBP. Extensive experiments on different data sets demonstrate that CE-PBP achieves a higher topic modeling accuracy and reduces more than 80% communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm.", "creator": "LaTeX with hyperref package"}}}