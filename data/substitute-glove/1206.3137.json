{"id": "1206.3137", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2012", "title": "Identifiability and Unmixing of Latent Parse Trees", "abstract": "This paper fascinating dyscalculia ways such parsing vehicle into on gauge. First, recently systems are traceable ago infinite analyzed? We means a force technique for polymorphic require identifiability based also there honor of second Jacobian matrix, that permit its to several standard constituency much dependency query generation. Second, each taxonomically models, how mean we 2.4 the coordinates programmed? EM discomfort since private mgmt, close recent study some spectral sophisticated cannot be either applied under this topology of while chunking flowering increases across sentences. We develop old transition, unmixing, which deals with is 1,000 complexity giving barred classes of vocabulary bikes.", "histories": [["v1", "Thu, 14 Jun 2012 15:21:24 GMT  (95kb,D)", "http://arxiv.org/abs/1206.3137v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["daniel j hsu", "sham m kakade", "percy liang"], "accepted": true, "id": "1206.3137"}, "pdf": {"name": "1206.3137.pdf", "metadata": {"source": "CRF", "title": "Identifiability and Unmixing of Latent Parse Trees", "authors": ["Daniel Hsu", "Sham M. Kakade", "Percy Liang"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Generative parsing models, which define joint distributions over sentences and their parse trees, are one of the core techniques in computational linguistics. We are interested in the unsupervised learning of these models [2\u20136], where the goal is to estimate the model parameters given only examples of sentences. Unsupervised learning can fail for a number of reasons [7]: model misspecification, non-identifiability, estimation error, and computation error. In this paper, we delve into two of these issues: identifiability and computation. In doing so, we confront a central challenge of parsing models\u2014that the topology of the parse tree is unobserved and varies across sentences. This is in contrast to standard phylogenetic models [8] and other latent tree models for which there is a single fixed global tree across all examples [9].\nA model is identifiable if there is enough information in the data to pinpoint the parameters (up to some trivial equivalence class); establishing the identifiability of a model is often a highly nontrivial task. A classic result of Kruskal [10] has been employed to prove the identifiability of a wide class of latent variable models, including hidden Markov models and certain restricted mixtures of latent tree models [11\u201313]. However, these techniques cannot be directly applied to parsing models since the tree topology varies over an exponential set of possible topologies. Instead, we turn to techniques from algebraic geometry [14\u201317]; we show that a simple numerical procedure can be used to check identifiability for a wide class of models in NLP. Using this tool, we discover that probabilistic context-free grammars (PCFGs) are non-identifiable, but that simpler PCFG variants and dependency models are identifiable.\nThe most common way to estimate unsupervised parsing models is by using local techniques such as EM [18] or MCMC sampling [19], but these methods can suffer from local optima and slow mixing. Meanwhile, recent work [1,20\u201323] has shown that spectral methods can be used to estimate mixture models and HMMs with provable guarantees. These techniques express low-order moments\nar X\niv :1\n20 6.\n31 37\nv1 [\nst at\n.M L\n] 1\n4 Ju\nn 20\n12\nof the observable distribution as a product of matrix parameters and use eigenvalue decomposition to recover these matrices. However, these methods are not directly applicable to parsing models because the tree topology again varies non-trivially. To address this, we propose a new technique, unmixing. The main idea is to express moments of the observable distribution as a mixture over the possible topologies. For restricted parsing models, the moments for a fixed tree structure can be \u201cunmixed\u201d, thereby reducing the problem to one with a fixed topology, which can be tackled using standard techniques [1]. Importantly, our unmixing technique does not require the training sentences be annotated with the tree topologies a priori, in contrast to recent extensions of [21] to learning PCFGs [24] and dependency trees [25,26], which work on a fixed topology."}, {"heading": "2 Notation", "text": "For a positive integer n, define [n] def = {1, . . . , n} and \u3008n\u3009 = {e1, . . . , en}, where ei is the vector which is 1 in component i and 0 elsewhere. For integers a, b \u2208 [n], let a \u2297n b = (a \u2212 1)n + b \u2208 [n2] be the integer encoding of the pair (a, b). For a pair of matrices, A,B \u2208 Rm\u00d7n, define the columnwise tensor product A \u2297c B \u2208 Rm2\u00d7n to be such that (A \u2297c B)(i1\u2297mi2)j = Ai1jBi2j . For a matrix A \u2208 Rm\u00d7n, let A\u2020 denote the Moore-Penrose pseudoinverse."}, {"heading": "3 Parsing models", "text": "A sentence is a sequence of L words, x = (x1, . . . , xL), where each word xi \u2208 \u3008d\u3009 is one of d possible word types. A (generative) parsing model defines a joint distribution P\u03b8(x, z) over a sentence x and its parse tree z (to be made precise later), where \u03b8 are the model parameters (a collection of multinomials). Each parse tree z has a topology Topology(z) \u2208 Topologies, which is both unobserved and varying across sentences. The learning problem is to recover \u03b8 given only samples of x.\nTwo important classes of models of natural language syntax are constituency models, which represent a hierarchical grouping and labeling of the phrases of a sentence (e.g., Figure 1(a)), and dependency models, which represent pairwise relationships between the words of a sentence (e.g., Figure 1(b))."}, {"heading": "3.1 Constituency models", "text": "A constituency tree z = (V, s) consists of a set of nodes V and a collection of hidden states s = {sv}v\u2208V . Each state sv \u2208 \u3008k\u3009 represents one of k possible syntactic categories. Each node v has the form [i : j] for 0 \u2264 i < j \u2264 L corresponding to the phrase between positions i and j of the sentence. These nodes form a binary tree as follows: the root node is [0 : L] \u2208 V , and for each node [i : j] \u2208 V with j \u2212 i > 1, there exists a unique m with i < m < j defining the two children nodes [i : m] \u2208 V and [m : j] \u2208 V . Let Topology(z) be an integer encoding of V .\nPCFG. Perhaps the most well-known constituency parsing model is the probabilistic context-free grammar (PCFG). The parameters of a PCFG are \u03b8 = (\u03c0,B,O), where \u03c0 \u2208 Rk specifies the initial state distribution, B \u2208 Rk2\u00d7k specifies the binary production distributions, and O \u2208 Rd\u00d7k specifies the emission distributions.\nA PCFG corresponds to the following generative process (see Figure 1(a) for an example): choose a topology Topology(z) uniformly at random;1 generate the state of the root node using \u03c0; recursively generate pairs of children states given their parents using B; and finally generate words xi given their parents using O. This generative process defines a joint probability over a sentence x and a parse tree z:\nP\u03b8(x, z) = |Topologies |\u22121\u03c0>s[0:L] \u220f\n[i:m],[m:j]\u2208V (s[i:m] \u2297k s[m:j])>Bs[i:j] L\u220f i=1 x>i Os[i\u22121:i], (1)\nWe will also consider two variants of the PCFG with additional restrictions:\nPCFG-I. The left and right children states are generated independently\u2014that is, we have the following factorization: B = T1 \u2297c T2 for some T1, T2 \u2208 Rk\u00d7k.\nPCFG-IE. The left and the right productions are independent and equal: B = T \u2297c T ."}, {"heading": "3.2 Dependency tree models", "text": "In contrast to constituency trees, which posit internal nodes with latent states, dependency trees connect the words directly. A dependency tree z is a set of directed edges (i, j), where i, j \u2208 [L]\n1 Usually a PCFG induces a topology via a state-dependent probability of choosing a binary production versus an emission. Our model is a restriction which corresponds to a state-independent probability.\nare distinct positions in the sentence. Let Root(z) denote the position of the root node of z. We consider only projective dependency trees [27]: z is projective if for every path from i to j to k in z, we have that j and k are on the same side of i (that is, j \u2212 i and k\u2212 i have the same sign). Let Topology(z) be an integer encoding of z.\nDEP-I. We consider the simple dependency model of [4]. The parameters of this model are \u03b8 = (\u03c0,A\u2199, A\u2198), where \u03c0 \u2208 Rd is the initial word distribution and A\u2199, A\u2198 \u2208 Rd\u00d7d are the left and right argument distributions. The generative process is as follows: choose a topology Topology(z) uniformly at random, generate the root word using \u03c0, and recursively generate argument words to the left to the right given the parent word using A\u2199 and A\u2198, respectively. The corresponding joint probability distribution is as follows:\nP\u03b8(x, z) = |Topologies |\u22121\u03c0>xRoot(z) \u220f\n(i,j)\u2208z\nx>j Adir(i,j)xi, (2)\nwhere dir(i, j) =\u2199 if j < i and \u2198 if j > i. We also consider the following two variants:\nDEP-IE. The left and right argument distributions are equal: A = A\u2199 = A\u2198.\nDEP-IES. A = A\u2199 = A\u2198 and \u03c0 is the stationary distribution of A (that is, \u03c0 = A\u03c0)."}, {"heading": "4 Identifiability", "text": "Our goal is to estimate model parameters \u03b80 \u2208 \u0398 given only access to sentences x \u223c P\u03b80 . Specifically, suppose we have an observation function \u03c6(x) \u2208 Rm, which is the only lens through which an algorithm can view the data. We ask a basic question: in the limit of infinite data, is it informationtheoretically possible to identify \u03b80 from the observed moments \u00b5(\u03b80) def = E\u03b80 [\u03c6(x)]?\nTo be more precise, define the equivalence class of \u03b80 to be the set of parameters \u03b8 that yield the same observed moments:\nS\u0398(\u03b80) = {\u03b8 \u2208 \u0398 : \u00b5(\u03b8) = \u00b5(\u03b80)}. (3)\nIt is impossible for an algorithm to distinguish among the elements of S\u0398(\u03b80). Therefore, one might want to ensure that |S\u0398(\u03b80)| = 1 for all \u03b80 \u2208 \u0398. However, this requirement is too strong for two reasons. First, models often have natural symmetries\u2014e.g., the k states of any PCFG can be permuted without changing \u00b5(\u03b8), so |S\u0398(\u03b80)| \u2265 k!. Second, |S\u0398(\u03b80)| = \u221e for some pathological \u03b80\u2019s\u2014e.g., PCFGs where all states have the same emission distribution O are indistinguishable regardless of the production distributions B. The following definition of identifiability accommodates these two exceptional cases:\nDefinition 1 (Identifiability). A model family with parameter space \u0398 is (globally) identifiable from \u03c6 if there exists a measure zero set E such that |S\u0398(\u03b80)| is finite for every \u03b80 \u2208 \u0398\\E. It is locally identifiable from \u03c6 if there exists a measure zero set E such that, for every \u03b80 \u2208 \u0398\\E, there exists an open neighborhood N(\u03b80) around \u03b80 such that S\u0398(\u03b80) \u2229N(\u03b80) = {\u03b80}.\nExample of non-identifiability. Consider the DEP-IE model with L = 2 with the full observation function \u03c6(x) = x1 \u2297 x2. The corresponding observed moments are \u00b5(\u03b8) = 0.5Adiag(\u03c0) + 0.5 diag(\u03c0)A>. Note that Adiag(\u03c0) is an arbitrary d \u00d7 d matrix whose entries sum to 1, which has d2 \u2212 1 degrees of freedom, whereas \u00b5(\u03b8) is a symmetric matrix whose entries sum to 1, which has ( d+1\n2\n) \u2212 1 degrees of freedom. Therefore, S\u0398(\u03b8) has dimension ( d 2 ) and therefore the model is\nnon-identifiable.\nParameter counting. It is important to compute the degrees of freedom correctly\u2014simple parameter counting is insufficient. For example, consider the PCFG-IE model with L = 2. The observed moments with respect to \u03c6(x) = x1\u2297 x2 is a d\u00d7 d matrix, which places d2 constraints on the k2 + (d\u2212 1)k parameters. When d \u2265 2k, there are more constraints than parameters, but the PCFG-IE model with L = 2 is actually non-identifiable (as we will see later). The issue here is that the number of constraints does not reveal the fact that some of these constraints are redundant."}, {"heading": "4.1 Observation functions", "text": "An observation function \u03c6(x) and its associated observed moments \u00b5(\u03b80) = E\u03b80 [\u03c6(x)] reveals aspects of the distribution P\u03b80(x). For example, \u03c6(x) = x1 would only reveal the marginal distribution of the first word, whereas \u03c6(x) = x1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 xL reveals the entire distribution of x. There is a tradeoff: Higher-order moments provide more information, but are harder to estimate reliably given finite data, and are also computationally more expensive. In this paper, we consider the following intermediate moments:\n\u03c612(x) def = x1 \u2297 x2 \u03c6\u2217\u2217(x) def = ( xi \u2297 xj : i, j \u2208 [L] ) \u03c6123(x) def = x1 \u2297 x2 \u2297 x3 \u03c6\u2217\u2217\u2217(x) def = ( xi \u2297 xj \u2297 xk : i, j, k \u2208 [L]\n) \u03c6123\u03b7(x) def = (x1 \u2297 x2)(\u03b7>x3) \u03c6\u2217\u2217\u2217\u03b7(x) def = ( (xi \u2297 xj)(\u03b7>xk) : i, j, k \u2208 [L]\n) \u03c6all(x) def = x1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 xL\nAbove, \u03b7 \u2208 Rd denotes a unit vector in Rd (e.g., e1) which picks out a linear combination of matrix slices from a third-order d\u00d7 d\u00d7 d tensor."}, {"heading": "4.2 Automatically checking identifiability", "text": "One immediate goal is to determine which models in Section 3 are identifiable from which of the observed moments (Section 4.1). A powerful analytic tool that has been succesfully applied in previous work is Kruskal\u2019s theorem [10,11], but (i) it is does not immediately apply to models with random topologies, and (ii) only gives sufficient conditions for identifiability, and cannot be used to determine non-identifiability. Furthermore, since it is common practice to explore many different models for a given problem in rapid succession, we would like to check identifiability quickly and reliably. In this section, we develop an automatic procedure to do this.\nTo establish identifiability, let us examine the algebraic structure of S\u0398(\u03b80) for \u03b80 \u2208 \u0398, where we assume that the parameter space \u0398 is an open subset of [0, 1]n.2 Recall that S\u0398(\u03b80) is defined\n2While we initially defined \u03b8 to be a tuple of conditional probability matrices, we will now use its non-redundant vectorized form \u03b8 \u2208 Rn.\nby the moment constraints \u00b5(\u03b8) = \u00b5(\u03b80). We can write these constraints as h\u03b80(\u03b8) = 0, where\nh\u03b80(\u03b8) def = \u00b5(\u03b8)\u2212 \u00b5(\u03b80)\nis a vector of m polynomials in \u03b8. Let us now compute the number of degrees of freedom of h\u03b80 around \u03b80. The key quantity is J(\u03b8) \u2208 Rm\u00d7n, the Jacobian of h\u03b80 at \u03b8 (note that the Jacobian of h\u03b80 does not depend on \u03b80; it is precisely the Jacobian of \u00b5). This Jacobian criterion is well-established in algebraic geometry, and has been adopted in the statistical literature for testing model identifiability and other related properties [14\u201317].\nIntuitively, each row of J(\u03b80) corresponds to a direction of a constraint violation, and thus the row space of J(\u03b80) corresponds to all directions that would take us outside the equivalence class S\u0398(\u03b80). If J(\u03b80) has less than rank n, then there is a direction orthogonal to all the rows along which we can move and still satisfy all the constraints\u2014in other words, |S\u0398(\u03b80)| is infinite, and therefore the model is non-identifiable. This intuition leads to the following algorithm:\nCheckIdentifiability: \u22121. Choose a point \u03b8\u0303 \u2208 \u0398 uniformly at random. \u22122. Compute the Jacobian matrix J(\u03b8\u0303). \u22123. Return \u201cyes\u201d if the rank of J(\u03b8\u0303) = n and \u201cno\u201d otherwise.\nThe following theorem asserts the correctness of CheckIdentifiability. It is largely based on techniques in [16], although we have not seen it explicitly stated in this form.\nTheorem 1 (Correctness of CheckIdentifiability). Assume the parameter space \u0398 is a nonempty open connected subset of [0, 1]n; and the observed moments \u00b5 : Rn \u2192 Rm, with respect to observation function \u03c6, is a polynomial map. Then with probability 1, CheckIdentifiability returns \u201cyes\u201d iff the model family is locally identifiable from \u03c6. Moreover, if it returns \u201cyes\u201d, then there exists E \u2282 \u0398 of measure zero such that the model family with parameter space \u0398 \\ E is identifiable from \u03c6.\nThe proof of Theorem 1 is given in Appendix A.\n4.3 Implementation of CheckIdentifiability\nComputing the Jacobian. The rows of J correspond to \u2202E\u03b8[\u03c6j(x)]/\u2202\u03b8 and can be computed efficiently by adapting dynamic programs used in the E-step of an EM algorithm for parsing models. There are two main differences: (i) we must sum over possible values of x in addition to z, and (ii) we are not computing moments, but rather gradients thereof. Specifically, we adapt the CKY algorithm for constituency models and the algorithm of [27] for dependency models. See Appendix C.1 for more details.\nNumerical issues. Because we implemented CheckIdentifiability on a finite precision machine, the results are subject to numerical precision errors. However, we verified that our numerical results are consistent with various analytically-derived identifiability results (e.g., from [11])."}, {"heading": "4.4 Identifiability of constituency and dependency tree models", "text": "We checked the identifiability status of various constituency and dependency tree models using our implementation of CheckIdentifiability. We focus on the regime where d \u2265 k for PCFGs; additional results for d < k are given in Appendix B.\nThe results are reported in Figure 2. First, we found that the PCFG is not identifiable from \u03c6all (and therefore not identifiable from any \u03c6) for L \u2208 {3, 4, 5}; we believe that the same holds for all L. This negative result motivates exploring restricted subclasses of PCFGs, such as PCFGI and PCFG-IE, which factorize the binary productions.3 For these classes, we found that the sentence length L and choice of observation function can influence identifiability: Both models are identifiable for large enough L (e.g., L \u2265 3) and with a sufficiently rich observation function (e.g., \u03c6123\u03b7).\nThe dependency models, DEP-I and DEP-IE, were all found to be identifiable for L \u2265 3 from second-order moments \u03c6\u2217\u2217. The conditions for identifiability are less stringent than their constituency counterparts (PCFG-I and PCFG-IE), which is natural since dependency models are simpler without the latent states. Note that in all identifiable models, second-order moments suffice to determine the distribution\u2014this is good news because low-order moments are easier to estimate."}, {"heading": "5 Unmixing algorithms", "text": "Having established which parsing models are identifiable, we now turn to parameter estimation for these models. We will consider algorithms based on moment matching\u2014those that try to find a \u03b8 satisfying \u00b5(\u03b8) = u for some u. Typically, u is an empirical estimate of \u00b5(\u03b80) = E\u03b80 [\u03c6(x)] based on samples x \u223c P\u03b80 .4\nIn general, solving \u00b5(\u03b8) = u corresponds to finding solutions to systems of multivariate polynomials, which is NP-hard [28]. However, \u00b5(\u03b8) often has additional structure which we can exploit. For instance, for an HMM, the sliced third-order moments \u00b5123\u03b7(\u03b8) can be written as a product of parameter matrices in \u03b8, and each matrix can be recovered by decomposing the product [1].\nFor parsing models, the challenge is that the topology is random, so the moments is not a single product, but a mixture over products. To deal with this complication, we propose a new technique,\n3Note that these subclasses occupy measure zero subsets of the PCFG parameter space, which is expected given the non-identifiability of the general PCFG.\n4We will develop our algorithms assuming true moments (u = \u00b5(\u03b80)). The empirical moments converge to the true\nmoments at Op(n \u2212 1\n2 ), and matrix perturbation arguments (e.g., [1]) can be used derive sample complexity arguments for the parameter error.\nwhich we call unmixing: We \u201cunmix\u201d the products from the mixtures, essentially reducing the problem to one with a fixed topology.\nWe will first present the general idea of unmixing (Section 5.1) and then apply it to the PCFG-IE model (Section 5.2) and the DEP-IES model (Section 5.3)."}, {"heading": "5.1 General case", "text": "We assume the observation function \u03c6(x) consists of a collection of observation matrices {\u03c6o(x)}o\u2208O (e.g., for o = (i, j), \u03c6o(x) = xi \u2297 xj). Given an observation matrix \u03c6o(x) and a topology t \u2208 Topologies, consider the mapping that computes the observed moment conditioned on that topology: \u03a8o,t(\u03b8) = E\u03b8[\u03c6o(x) | Topology = t]. As we range o over O and t over Topologies, we will enounter a finite number of such mappings. We call these mappings compound parameters, denoted {\u03a8p}p\u2208P .\nNow write the observed moments as a weighted sum: \u00b5o(\u03b8) = \u2211 p\u2208P\nP(\u03a8o,Topology = \u03a8p)\ufe38 \ufe37\ufe37 \ufe38 def =Mop \u03a8p for all o \u2208 O, (4)\nwhere we have defined Mop to be the probability mass over tree topologies that yield compound parameter \u03a8p. We let {Mop}o\u2208O,p\u2208P be the mixing matrix. Note that (4) defines a system of equations \u00b5 = M\u03a8, where the variables are the compound parameters and the constraints are the observed moments. In a sense, we have replaced the original system of polynomial equations (in \u03b8) with a system of linear equations (in \u03a8).\nThe key to the utility of this technique is that the number of compound parameters can be polynomial in L even when the number of possible topologies is exponential in L. Previous analytic techniques [13] based on Kruskal\u2019s theorem [10] cannot be applied here because the possible topologies are too many and too varied.\nNote that the mixing equation \u00b5 = M\u03a8 holds for each sentence length L, but many compound parameters p appear in the equations of multiple L. Therefore, we can combine the equations across all observed sentence lengths, yielding a more constrained system than if we considered the equations of each L separately.\nThe following proposition shows how we can recover \u03b8 by unmixing the observed moments \u00b5:\nProposition 1 (Unmixing). Suppose that there exists an efficient base algorithm to recover \u03b8 from some subset of compound parameters {\u03a8p(\u03b8) : p \u2208 P0}, and that e>p is in the row space of M for each p \u2208 P0. Then we can recover \u03b8 as follows:\nUnmix(\u00b5): \u22121. Compute the mixing matrix M (4). \u22122. Retrieve the compound parameters \u03a8p(\u03b8) = (M \u2020\u00b5)p for each p \u2208 P0. \u22123. Call the base algorithm on {\u03a8p(\u03b8) : p \u2208 P0} to obtain \u03b8.\nFor all our parsing models, M can be computed efficiently using dynamic programming (Appendix C.2). Note that M is data-independent, so this computation can be done once in advance."}, {"heading": "5.2 Application to the PCFG-IE model", "text": "As a concrete example, consider the PCFG-IE model over L = 3 words. Write A = OT . For any \u03b7 \u2208 Rd, we can express the observed moments as a sum over the two possible topologies in Figure 1(a):\n\u00b5123\u03b7 def = E[x1 \u2297 x2(\u03b7>x3)] = 0.5\u03a81;\u03b7 + 0.5\u03a82;\u03b7, \u03a81;\u03b7 def = Adiag(T diag(\u03c0)A>\u03b7)A>,\n\u00b5132\u03b7 def = E[x1 \u2297 x3(\u03b7>x2)] = 0.5\u03a83;\u03b7 + 0.5\u03a82;\u03b7, \u03a82;\u03b7 def = Adiag(\u03c0)T> diag(A>\u03b7)A>,\n\u00b5231\u03b7 def = E[x2 \u2297 x3(\u03b7>x1)] = 0.5\u03a83;\u03b7 + 0.5\u03a81;\u03b7, \u03a83;\u03b7 def = Adiag(A>\u03b7)T diag(\u03c0)A>,\nor compactly in matrix form: \u00b5123\u03b7\u00b5132\u03b7 \u00b5231\u03b7  \ufe38 \ufe37\ufe37 \ufe38\nobserved moments \u00b5\u03b7\n=  0.5I 0.5I 00 0.5I 0.5I 0.5I 0 0.5I  \ufe38 \ufe37\ufe37 \ufe38\nmixing matrix M\n \u03a81;\u03b7\u03a82;\u03b7 \u03a83;\u03b7  \ufe38 \ufe37\ufe37 \ufe38\ncompound parameters \u03a8\u03b7\n.\nLet us observe \u00b5\u03b7 at two different values of \u03b7, say at \u03b7 = 1 and \u03b7 = \u03c4 for some random \u03c4 . Since the mixing matrix M is invertible, we can obtain the compound parameters \u03a82;1 = (M\n\u22121\u00b51)2 and \u03a82;\u03c4 = (M\n\u22121\u00b5\u03c4 )2. Now we will recover \u03b8 from \u03a82;1 and \u03a82;\u03c4 by first extracting A = OT via an eigenvalue decomposition, and then recovering \u03c0, T , and O in turn (all up to the same unknown permutation) via elementary matrix operations.\nFor the first step, we will use the following tool (adapted from Algorithm A of [1]), which allow us to decompose two related matrix products:\nLemma 1 (Spectral decomposition). Let M1,M2 \u2208 Rd\u00d7k have full column rank and D be a diagonal matrix with distinct diagonal entries. Suppose we observe X = M1M > 2 and Y = M1DM > 2 . Then Decompose(X,Y ) recovers M1 up to a permutation and scaling of the columns.\nDecompose(X,Y ): \u22121. Find U1, U2 \u2208 Rd\u00d7k such that range(U1) = range(X) and range(U2) = range(X>). \u22122. Perform an eigenvalue decomposition of (U>1 Y U2)(U>1 XU2)\u22121 = V SV \u22121. \u22123. Return (U>1 )\u2020V .\nFirst, run Decompose(X = \u03a8>2;1, Y = \u03a8 > 2;\u03c4 ) (Lemma 1), which corresponds to M1 = A and M2 = Adiag(\u03c0)T >. This produces A\u03a0S for some permutation matrix \u03a0 and diagonal scaling S. Since we know that the columns of A sum to one, we can identify A\u03a0. To recover the initial distribution \u03c0 (up to permutation), take \u03a82;11 = A\u03c0 and left-multiply by (A\u03a0)\u2020 to get \u03a0\u22121\u03c0. For T , put the entries of \u03c0 in a diagonal matrix: \u03a0\u22121 diag(\u03c0)\u03a0. Take \u03a8>2;1 = AT diag(\u03c0)A > and multiply by (A\u03a0)\u2020 on the left and ((A\u03a0)>)\u2020(\u03a0\u22121 diag(\u03c0)\u03a0)\u22121 on the right, which yields \u03a0\u22121T\u03a0. (Note that \u03a0 is orthogonal, so \u03a0\u22121 = \u03a0>.) Finally, multiply A\u03a0 = OT\u03a0 and (\u03a0\u22121T\u03a0)\u22121, which yields O\u03a0.\nThe above algorithm identifies the PCFG-IE from only length 3 sentences. To exploit sentences of different lengths, we can compute a mixing matrix M which includes constraints from sentences\nof length 1 \u2264 L \u2264 Lmax up to some upper bound Lmax. For example, Lmax = 10 results in a 990 \u00d7 2376 mixing matrix. We can retrieve the same compound parameters (\u03a82;1 and \u03a82;\u03c4 ) from the pseudoinverse of M and as proceed as before."}, {"heading": "5.3 Application to the DEP-IES model", "text": "We now turn to the DEP-IES model over L = 3 words. Our goal is to recover the parameters \u03b8 = (\u03c0,A). Let D = diag(\u03c0) = diag(A\u03c0), where the second equality is due to stationarity of \u03c0.\n\u00b51 def = E[x1] = \u03c0,\n\u00b512 def = E[x1 \u2297 x2] = 7\u22121(DA> +DA> +DA>A> +AD +ADA> +AD +DA>),\n\u00b513 def = E[x1 \u2297 x3] = 7\u22121(DA> +DA>A> +DA> +ADA> +AD +AAD +AD),\n\u00b5\u030312 def = E\u0303[x1 \u2297 x2] = 2\u22121(DA> +AD),\nwhere E\u0303[\u00b7] is taken with respect to length 2 sentences. Having recovered \u03c0 from \u00b51, it remains to recover A. By selectively combining the moments above, we can compute AA + A = [7(\u00b513 \u2212 \u00b512) + 2\u00b5\u030312] diag(\u00b51)\n\u22121. Assuming A is generic position, it is diagonalizable: A = Q\u039bQ\u22121 for some diagonal matrix \u039b = diag(\u03bb1, . . . , \u03bbd), possibly with complex entries. Therefore, we can recover \u039b2 + \u039b = Q\u22121(AA+A)Q. Since \u039b is diagonal, we simply have d independent quadratic equations in \u03bbi, which can be solved in closed form. After obtaining \u039b, we retrieve A = Q\u039bQ \u22121."}, {"heading": "6 Discussion", "text": "In this work, we have shed some light on the identifiability of standard generative parsing models using our numerical identifiability checker. Given the ease with which this checker can be applied, we believe it should be a useful tool for analyzing more sophisticated models [6], as well as developing new ones which are expressive yet identifiable.\nThere is still a large gap between showing identifiability and developing explicit algorithms. We have made some progress on closing it with our unmixing technique, which can deal with models where the tree topology varies non-trivially."}, {"heading": "A Proof of Theorem 1", "text": "Theorem 1 (restated). Assume \u0398 is a non-empty open connected subset of [0, 1]n and \u00b5 : Rn \u2192 Rm is a polynomial map. With probability 1, the following holds.\n\u2022 CheckIdentifiability returns \u201cno\u201d \u21d2 for almost all \u03b80 \u2208 \u0398 and any open neighborhood N(\u03b80) around \u03b80, |S\u0398(\u03b80) \u2229N(\u03b80)| is infinite (not locally identifiable).\n\u2022 CheckIdentifiability returns \u201cyes\u201d \u21d2 (i) for almost all \u03b80 \u2208 \u0398, there exists an open neighborhood N(\u03b80) around \u03b80 such that |S\u0398(\u03b80) \u2229 N(\u03b80)| = 1 (locally identifiable); and (ii) there exists a set E \u2282 \u0398 with measure zero such that |S\u0398\\E(\u03b80)| is finite for every \u03b80 \u2208 \u0398 \\ E (identifiability of \u0398\\E).\nThe proof of Theorem 1 crucially relies on the following lemma from [16] which holds even in the case that \u00b5 is merely an analytic function (see Lemma 9 of [17] for a simpler proof in the case \u00b5 is a polynomial map); it states that the Jacobian achieves its maximal rank almost everywhere in \u0398. To state this precisely, first define rmax def = max{rank(J(\u03b8)) : \u03b8 \u2208 \u0398} and \u0398max def = {\u03b8 \u2208 \u0398 : rank(J(\u03b8)) = rmax}.\nLemma 2. The set \u0398 \\\u0398max has Lebesgue measure zero. That is, \u0398max is almost all of \u0398.\nProof of Theorem 1. By Lemma 2, CheckIdentifiability chooses a point \u03b8\u0303 \u2208 \u0398max with probability 1. We henceforth condition on this event, so rank(J(\u03b8\u0303)) = rmax.\nCase 1: rank(J(\u03b8\u0303)) < n (i.e., \u201cno\u201d is returned). In this case, we have rmax < n. We now employ an argument from the proof of Proposition 20 of [16]. Fix any \u03b80 \u2208 \u0398max. Since \u0398 is open, Weyl\u2019s theorem implies that there is an open neighborhood U around \u03b80 in \u0398 on which rank(J(\u03b8)) = rmax\nfor all \u03b8 \u2208 U (i.e., rank(J(\u00b7)) is constant on U). Therefore, by the constant rank theorem, there is an open neighborhood N(\u03b80) around \u03b80 in \u0398 such that \u00b5\n\u22121(\u00b5(\u03b80)) \u2229N(\u03b80) is homeomorphic with an open set in Rn\u2212rmax . Therefore S\u0398(\u03b80) \u2229N(\u03b80) is uncountably infinite.\nCase 2: rank(J(\u03b8\u0303)) = n (i.e., \u201cyes\u201d is returned). In this case, we have rmax = n. Therefore for every \u03b80 \u2208 \u0398max, the Jacobian J(\u03b80) has full column rank, and thus by the inverse function theorem, \u00b5 is injective on a neighborhood of \u03b80. This in turn implies that for all \u03b80 \u2208 \u0398max, there exists an open neighborhood N(\u03b80) around \u03b80 such that S\u0398(\u03b80) \u2229N(\u03b80) = {\u03b80}. This proves (i).\nTo show (ii), define E def= \u0398 \\ \u0398max, and now claim that for every \u03b80 \u2208 \u0398max, the equivalence class S\u0398max(\u03b80) is finite. Observe that by (i), the set S\u0398max(\u03b80) contains only geometrically isolated solutions to the system of polynomial equations given by \u00b5(\u03b8) = \u00b5(\u03b80). Therefore the claim follows immediately from Be\u0301zout\u2019s Theorem, which implies that the number of geometrically isolated solutions is finite.\nRemark. All the models considered in this paper have moments \u00b5 which correspond to a polynomial map. However, for some models (e.g., exponential families), \u00b5 will not be a polynomial map, but rather, a general analytic function. In this case, Theorem 1 holds with one modification to (ii). If CheckIdentifiability returns \u201cyes\u201d, then we have the following weaker guarantee in place of (ii): S\u0398max(\u03b80) is countable (but not necessarily finite) for all \u03b80 \u2208 \u0398max. The above proof does not require the fact that \u00b5 is a polynomial map except in the invocation of Be\u0301zout\u2019s Theorem. In place of Be\u0301zout\u2019s Theorem, we use the following argument. If S\u0398max(\u03b80) is uncountable, then it contains a limit point \u03b8\u2217 \u2208 S\u0398max(\u03b80); thus for any small enough neighborhood N(\u03b8\u2217) of \u03b8\u2217, there is some \u03b8 \u2208 S\u0398max(\u03b80) \u2229N(\u03b8\u2217). This contradicts (i) as applied to \u03b8\u2217, and thus we conclude that S\u0398max(\u03b80) is countable."}, {"heading": "B Additional results from the identifiability checker", "text": "PCFG models with d < k. The PCFG models that we\u2019ve considered so far assume that the number of words d is at least the number of hidden states k, which is a realistic assumption for natural language. However, there are applications, e.g., computational biology, where the vocabulary size d is relatively small. In this regime, identifiability becomes trickier because the data doesn\u2019t reveal as much about the hidden states, and brings us closer to the boundary between identifiability and non-identifiability. In this section, we consider the d < k regime.\nThe following table gives additional identifiability results from CheckIdentifiability for values of d, k, and L where d < k (recall that the results reported in Section 4.4 only considered values where d \u2265 k). In each cell, we show the (k, d, L) values for which CheckIdentifiability returned \u201cyes\u201d; the values checked were k \u2208 {3, 4, . . . , 8}, d \u2208 {2, . . . , k \u2212 1}, L \u2208 {3, 4, . . . , 9}.\n\u03c612 \u03c6\u2217\u2217 \u03c6123e1 \u03c6123 \u03c6\u2217\u2217\u2217e1 \u03c6\u2217\u2217\u2217 PCFG None\nPCFG-I None (3, 2,\u2265 6) (4, 2,\u2265 8) (4, 3,\u2265 5) (5, 3,\u2265 6) (5, 4,\u2265 4) (6, 3,\u2265 7) (6, 4,\u2265 5) (6, 5,\u2265 4) (7, 3,\u2265 8) (7, 4,\u2265 6) (7, 5,\u2265 5) (7, 6,\u2265 4)\nNone\n(5, 4,\u2265 4) (6, 5,\u2265 4) (7, 5,\u2265 4) (7, 6,\u2265 4)\n(3, 2,\u2265 5) (4, 2,\u2265 6) (4, 3,\u2265 4) (5, 2,\u2265 7)\n(5,\u2265 3,\u2265 4) (6, 2,\u2265 8) (6, 3,\u2265 5) (6,\u2265 4,\u2265 4) (7, 2,\u2265 9) (7, 3,\u2265 5) (7,\u2265 4,\u2265 4)\nPCFG-IE None (3, 2,\u2265 6) (4, 2,\u2265 8) (4, 3,\u2265 5) (5, 3,\u2265 6) (5, 4,\u2265 5) (6, 3,\u2265 7) (6, 4,\u2265 5) (6, 5,\u2265 4) (7, 3,\u2265 8) (7, 4,\u2265 6) (7, 5,\u2265 5) (7, 6,\u2265 4) (5, 4,\u2265 4) (6, 5,\u2265 4) (7, 5,\u2265 5) (7, 6,\u2265 4)\n(4, 3,\u2265 4) (5, 4,\u2265 4)\n(6,\u2265 4,\u2265 4) (7,\u2265 5,\u2265 4)\n(3, 2,\u2265 5) (4, 2,\u2265 6) (4, 3,\u2265 4) (5, 2,\u2265 7) (5, 3,\u2265 5) (5, 4,\u2265 4) (6, 2,\u2265 8) (6, 3,\u2265 5)\n(6,\u2265 4,\u2265 4) (7, 2,\u2265 9) (7, 3,\u2265 5) (7,\u2265 4,\u2265 4)\n(3, 2,\u2265 5) (4, 2,\u2265 6) (4, 3,\u2265 4) (5, 2,\u2265 7)\n(5,\u2265 3,\u2265 4) (6, 2,\u2265 8) (6, 3,\u2265 5) (6,\u2265 4,\u2265 4) (7, 2,\u2265 9) (7, 3,\u2265 5) (7,\u2265 4,\u2265 4)\nFixed topology models. We now present some results for latent class models (LCMs) and hidden Markov models (HMMs). While identifiability for these models are more developed than for parsing models, we show that the identifiability checker can refine the results even for the classic models.\nThe parameters of an HMM are \u03b8 = (\u03c0, T,O), where \u03c0 \u2208 Rk specifies the initial state distribution, T \u2208 Rk\u00d7k specifies the state transition probabilities, and O \u2208 Rd\u00d7k specifies the emission distributions. The probability over a sentence x is:\nP\u03b8(x) = 1>T diag(O>xL) \u00b7 \u00b7 \u00b7T diag(O>x2)T diag(O>x1)\u03c0. (5)\nThe parameters of an LCM are \u03b8 = (\u03c0,O)\u2014the same as that of an HMM except with T \u2261 I. The probability over a sentence x is also given by (5) (with T = I).\nThe following table summarizes some identifiability results obtained by CheckIdentifiability (for d \u2265 k); these results have all been proven analytically in previous work (e.g., [8, 10,11,20,21]) except for the identifiability of HMMs from \u03c6\u2217\u2217.\n\u03c612 \u03c6\u2217\u2217 \u03c6123e1 \u03c6123 \u03c6\u2217\u2217\u2217e1 \u03c6\u2217\u2217\u2217 LCM No Yes iff L \u2265 3 HMM No Yes iff L \u2265 3\nIt is known that LCMs are not identifiable from \u03c6\u2217\u2217 for any value of L [8]. However, LCMs constitute a subfamily of HMMs arising from a measure zero subset of the HMM parameter space. Therefore the identifiability of HMMs from \u03c6\u2217\u2217 (for L \u2265 3) does not contradict this result. The result does not appear to be covered by application of Kruskal\u2019s theorem in previous work [11], so we prove the result rigorously below.\nIt can be checked using (5) that\nE\u03b8[\u03c612(x)] = O diag(\u03c0)T>O> E\u03b8[\u03c634(x)] = O diag(T\u03c0)T>O>.\nLet M1 def = O, M2 def = OT diag(\u03c0), and D def = diag(T\u03c0) diag(\u03c0)\u22121. Provided that\n1. \u03c0 > 0,\n2. O has full column rank,\n3. T is invertible,\n4. the ratios of probabilities (T\u03c0)i/\u03c0i, ranging over i \u2208 [k], are distinct\n(all of which are true for all but a measure zero set of parameters in \u0398), the matrices M1 and M2 have full column rank and the diagonal matrix D has distinct diagonal entries. Therefore Lemma 1 can be applied with X = E\u03b8[\u03c612(x)] = M1M>2 and Y = E\u03b8[\u03c634(x)] = M1DM>2 to recover M1 = O. It is easy to see that \u03c0 and T can also easily be recovered.\nNote that the fourth condition above, that T\u03c0 be entry-wise distinct from \u03c0, is violated when a LCM distribution is cast as an HMM distribution (by setting T = I so T\u03c0 = \u03c0). However, the set of HMM parameters satisfying this equation is a measure zero set.\nDiscussion. CheckIdentifiability tests for local identifiability. If it finds that a model family is not locally identifiable, then it is not globally identifiable. However the inverse claim is not necessarily true: if it finds that a model family is locally identifiable, it is not necessarily globally identifiable. Theorem 1 provides the somewhat weaker guarantee that a restricted model family is globally identifiable, where the equivalence classes S\u0398\\E(\u03b80) are only taken with respect to a subset \u0398 \\ E \u2286 \u0398 of the parameter space. However, there is a gap between this property (which is with respect to \u0398 \\ E) and true global identifiability (which is with respect to \u0398).\nOn the other hand, having explicit estimators guarantees us proper global identifiability with respect to the original model family \u0398. In fact, the exceptional set E can typically be characterized explicitly. For instance, in the case of PCFG-IE, the set \u0398 \\ E contains those \u03b8 = (\u03c0, T,O) that satisfy full rank conditions:\n\u0398 \\ E = {(\u03c0, T,O) : \u03c0 0, T is invertible, O has full column rank}. (6)\nAdditionally, the explicit estimators also provides an explicit characterization of the elements in the equivalence class S\u0398(\u03b80) for each \u03b80 \u2208 \u0398 \\ E : the set S\u0398(\u03b80) contains exactly k! elements corresponding to permutation of the hidden states. Specifically,\nS\u0398((\u03c0, T,O)) = {(\u03a0\u22121\u03c0,\u03a0\u22121T\u03a0, O\u03a0) : \u03a0 is a permutation matrix. (7)\nNote that this is shaper than Theorem 1, which only says that the equivalence classes have to be finite."}, {"heading": "C Dynamic programs", "text": "For a sentence of length L, the number of parse trees is exponential in L. Therefore, dynamic programming is often employed to efficiently compute expectations over the parse trees, the core computation in the E-step of the EM algorithm. In the case of PCFG, this dynamic program is referred to as the CKY algorithm, which runs in O(L3k3) time, where k is the number of hidden states. For simple dependency models, a O(L3) dynamic program was developed by [29]. At a high-level, the states of the dynamic program in both cases are the spans [i : j] of the sentence (and for the PCFG, the these states include the hidden states z[i:j] of the nodes).\nIn this paper, we need to compute (i) the Jacobian matrix for checking identifiability (Section 4.2) and (ii) the mixing matrix for recovering compound parameters (Section 5.1). Both computations can be performed efficiently with a modified version of the classic dynamic programs, which we will describe in this section.\nC.1 Computing the Jacobian matrix\nRecall that the j-th row of the Jacobian matrix J is (the transpose of) the gradient of hj(\u03b8) = \u00b5j(\u03b8)\u2212 \u00b5j(\u03b80). Specifically, entry Jji is the derivative of the j-th moment with respect to the i-th parameter:\nJji = \u2202hj(\u03b8)\n\u2202\u03b8i (8)\n= \u2202E\u03b8[\u03c6j(x)]\n\u2202\u03b8i (9)\n= \u2211 x,z \u2202p\u03b8(x, z) \u2202\u03b8i \u03c6j(x). (10)\nWe can encode the sum over the exponential set of possible sentences x and parse trees z using a directed acyclic hypergraph so that each hyperpath through the hypergraph corresponds to a (x, z) pair. Specifically, a hypergraph consists of the following:\n\u2022 a set of nodes V with a designated start node Start \u2208 V and an end node End \u2208 V, and\n\u2022 a set of hyperedges E where each hyperedge e \u2208 E has a source node e.a \u2208 V and a pair of target nodes (e.b, e.c) \u2208 V \u00d7 V (we say that e connects e.a to e.b and e.c) and an index e.i \u2208 [n] corresponding to a component of the parameter vector \u03b8 \u2208 Rn.\nDefine a hyperpath P to be a subset of the edges E such that:\n\u2022 (Start, a, b) \u2208 P for some a, b \u2208 V;\n\u2022 if (a, b, c) \u2208 P and b 6= End, then (b, d, e) \u2208 P for some d, e \u2208 V; and\n\u2022 if (a, b, c) \u2208 P and c 6= End, then (c, d, e) \u2208 P for some d, e \u2208 V.\nEach hyperpath P , encoding (x, z), is associated with a probability equal to the product of all of the parameters on that hyperpath:\np\u03b8(x, z) = p\u03b8(P ) = \u220f e\u2208P \u03b8e.i. (11)\nIn this way, the hypergraph compactly defines a distribution over exponentially many hyperpaths. Now, we assume that each moment \u03c6j(x) corresponds to a function fj : E 7\u2192 R mapping each hyperedge e to a real number so that the moment is equal to the product over function values:\n\u03c6j(x) = \u220f e\u2208P fj(e), (12)\nwhere P is any hyperpath that encodes the sentence x and some parse tree z (we assume that the product is the same no matter what z is).\nNow, let us write out the Jacobian matrix entries in terms of hyperpaths:\nJji = \u2211 P \u2211 e0\u2208P \u2202\u03b8e0.i \u2202\u03b8i \u220f e\u2208P,e6=e0 \u03b8e.ifj(e). (13)\nThe sum over hyperpaths P can be computed efficiently as follows. For each hypergraph node a, we compute an inside score \u03b1(a), which sums over all possible partial hyperpaths terminating at the target node, and an outside score \u03b2(a), which sums over all possible partial hyperpaths from the source node:\n\u03b1(a) def = \u2211 e\u2208E:e.a=a \u03b8e.i\u03b1(e.b)\u03b1(e.c), (14) \u03b2(a) def =\n\u2211 e\u2208E:e.b=a \u03b8e.i\u03b1(e.c)\u03b2(e.a) \u2211 e\u2208E:e.c=a \u03b8e.i\u03b1(e.b)\u03b2(e.a). (15)\nThe Jacobian entry Jji can be computed as follows: Jji = \u2211 e\u2208E \u03b2(e.a)\u03b1(e.b)\u03b1(e.c)I[i = e.i]. (16)\nExample: PCFG. For a PCFG, nodes V have the form (i, j, s) \u2208 [L]\u00d7 [L]\u00d7 [k], corresponding to a hidden state s over span [i : j]. For each hidden state s, we have a hyperedge e connecting e.a = Start to e.b = (s, 0, L) and e.c = End; this hyperedge has parameter index e.i corresponding to \u03c0s. For each span [i : j] with j \u2212 i > 1, split point i < m < j, and hidden states s1, s2, s3 \u2208 [k], E contains a hyperedge e connecting e.a = (i, j, s1) to e.b = (i,m, s2) and e.c = (m, j, s3); the parameter index e.i corresponds to the binary production B(s2\u2297ks3)s1 . For each span [i\u2212 1 : i], hidden state s \u2208 [k] and word x \u2208 [d], we have a hyperedge e connecting e.a = (i \u2212 1, i, s) to e.b = End and e.c = End with parameter index e.i corresponding to the emission Oxs.\nThe moments can be encoded as follows: For example, if \u03c6j(x) = I[xi = t], then we define fj(e) to be 0 if the source node corresponds to position i (e.a = (i\u2212 1, i, s)) and the parameter index e.i does not correspond to Ots for some s \u2208 [k], and 1 otherwise. In this way, \u220f e\u2208P fj(e) is zero if P encodes a sentence with xi 6= t. Higher-order moments simply correspond to hyperedge-wise multiplication of these first-order moments. For example, if \u03c6j1(x) = I[xi1 = t1] and \u03c6j2(x) = I[xi2 = t2], then the second-order moment \u03c6j(x) = I[xi1 = t1, xi2 = t2] corresponds to fj(e) = fj1(e)fj2(e).\nC.2 Computing the mixing matrix\nRecall that the mixing matrix M includes a row for each observation matrix o \u2208 O and a column for each compound parameter p \u2208 P. Assuming a uniform distribution over topologies, computing each entry of M reduces to counting the number of topologies t consistent with a particular compound parameter \u03a8p:\nMop = P(\u03a8o,Topology = \u03a8p) (17) = |Topologies |\u22121 \u2211 t I[\u03a8o,t = \u03a8p]. (18)\nFirst, we will characterize the set of compound parameters graphically in terms of backbone structures. As an example, consider the PCFG-IE model and the observation matrix \u03c612 (o = 12) corresponding to the marginal distribution over the first two words of the sentence. Given a topology t, consider starting at the root, descending to the lowest common ancestor of x1 and x2, and then following both paths down to x1 and x2, respectively. We refer to this traversal as the backbone structure with respect to topology t and observation matrix \u03c612. See Figure 3 for an example of the backbone structure, outlined in blue.\nNote that the compound parameter \u03a812,t(\u03b8) = E\u03b8[\u03c612(x) | Topology = t] can be written as a product over the parameter matrices, one for each edge of the backbone structure. For Figure 3, this would yield\n\u03a812,1(\u03b8) = OT diag(T\u03c0)T >O>. (19)\nFor general trees, we would have\n\u03a812,t(\u03b8) = OT n1 diag(Tn3\u03c0)(T>)n2O>. (20)\nfor some positive integers n1, n2, n3 corresponding to the number of edges (in t) from the common node to the preterminal node z01, the preterminal node z12, and the root z0L, respectively.\nNote that the compound parameter does not depend on the structure of t outside the backbone\u2014 that part of the topology is effectively marginalized out\u2014so the compound parameter \u03a812,t(\u03b8) will\nbe identical for all topologies sharing that same backbone structure. Therefore, there are only a polynomial number of compound parameters despite an exponential number of topologies t.5\nWe define a dynamic program that recursively computes Mop for the PCFG-IE model under a fixed second-order observation matrix \u03c6i0j0 . Specifically, for each span [i : j] define H(i, j) to be the set of pairs \u3008t, n\u3009 where t is a partial backbone structure t and n is the number of partial topologies over span [i : j] which are consistent with t.\nIn the base case H(i\u22121, i), if i is either of the designated leaf positions defined by the observation matrix (i0 or j0), then we return the single-node backbone structure \u2022; otherwise, we return the null backbone structure \u00f8:\nH(i\u2212 1, i) = { {\u3008\u00f8, 1\u3009} if i = i0 or i = j0 {\u3008\u2022, 1\u3009} otherwise.\n(21)\nIn the recursive case H(i, j), we consider all split points m, partial backbones t1 and t2 from H(i,m) and H(m, j), respectively, and create a new tree with t1 and/or t2 as the subtrees if they are not null:\nH(i, j) = +\u22c3\ni<m<j +\u22c3 \u3008t1,n1\u3009\u2208H(i,m) +\u22c3 \u3008t2,n2\u3009\u2208H(m,j) \u3008Combine(t1, t2), n1n2\u3009 , (22)\nCombine(t1, t2) =  (T : t1, T : t2) if t1 6= \u00f8 and t2 6= \u00f8, T : t1 if t1 6= \u00f8, T : t2 if t2 6= \u00f8, \u00f8 otherwise.\n(23)\nHere, we use the notation \u22c3+ to denote a multi-set union: {\u3008t, n1\u3009}\u222a+{\u3008t, n2\u3009} = {\u3008t, n1 + n2\u3009}. In this notation, the backbone structure in Figure 3 would be represented as T : (T : O : \u2022, T : O : \u2022), which can be easily converted to the compound parameter OT diag(T\u03c0)T>O>.\nFor third-order observation matrices (e.g., \u03c6i0j0k0\u03b7), we add an additional case to H(i\u2212 1, i) to return \u3008\u25e6, 1\u3009 if i = k0; note that k0 is represented by a special node \u25e6 because that observation is projected using \u03b7. The first case of Combine(t1, t2) undergoes one change: if t2 is a chain ending in \u25e6, then we return (T : t2, T : t1). The reason for this is best demonstrated by an example: consider topology 1 in Figure 3, and the two observation matrices \u03c6132\u03b7 and \u03c6231\u03b7. Without the reordering, we would have the backbone structure: (T : (T : \u2022, T : \u25e6), T : \u2022) and (T : (T : \u25e6, T : \u2022), T : \u2022). However, they have the same compound parameter OT diag(T>O>\u03b7)T> diag(\u03c0)TO. This is because the contribution of a subtree ending in \u25e6 is simply a diagonal matrix (diag(T>O>\u03b7) in this case) which is applied on the hidden state regardless of whether it came from the left or right side.\n5One might also see why the unmixing technique does not directly apply to the PCFG-I model, where T is replaced with T1 for left edges and T2 for right edges. In that case, there are many backbone structures (and thus more compound parameters) due to the different interleavings of left and right edges."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "<lb>This paper explores unsupervised learning of parsing models along two directions. First,<lb>which models are identifiable from infinite data? We use a general technique for numerically<lb>checking identifiability based on the rank of a Jacobian matrix, and apply it to several stan-<lb>dard constituency and dependency parsing models. Second, for identifiable models, how do<lb>we estimate the parameters efficiently? EM suffers from local optima, while recent work using<lb>spectral methods [1] cannot be directly applied since the topology of the parse tree varies across<lb>sentences. We develop a strategy, unmixing, which deals with this additional complexity for<lb>restricted classes of parsing models.", "creator": "LaTeX with hyperref package"}}}