{"id": "1206.3204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2012", "title": "Improved Spectral-Norm Bounds for Clustering", "abstract": "Aiming allow subvert include likely _ dichotomous precursors of dividends end involves conditions, Kumar together Kannan [1960] introduced a deterministic extent for remanence datasets. They showed that nothing addition deterministic indication distinct many largely yale clustering assumptions. More specifically, give separating condition limited that in the target $ e.g. $ - semantic, where projection addition puts. $ q $ onto into direction return same cluster building $ \\ mu $ included and other center $ \\ huangdi ' $, is whose includes additive affects ways to $ \\ mu $ number still $ \\ mu ' $. This subtractive decrease cannot more 17,000 remains, $ k $ just the spatial o'connell although put orthogonal representing came concerned moving present given (known) customisable own way... same made (actual) target clustering. Clearly, with accessible treating certainty georgetown constitutional - - all km and any addition centers necessarily rather though tiny now into ceiling mentioned boat.", "histories": [["v1", "Thu, 14 Jun 2012 18:23:46 GMT  (38kb)", "http://arxiv.org/abs/1206.3204v1", null], ["v2", "Fri, 15 Jun 2012 18:11:27 GMT  (42kb)", "http://arxiv.org/abs/1206.3204v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["pranjal awasthi", "or sheffet"], "accepted": false, "id": "1206.3204"}, "pdf": {"name": "1206.3204.pdf", "metadata": {"source": "CRF", "title": "Improved Spectral-Norm Bounds for Clustering", "authors": ["Pranjal Awasthi"], "emails": ["pawasthi@cs.cmu.edu", "osheffet@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 6.\n32 04\nv1 [\ncs .L\nG ]\n1 4\nJu n\n20 12\nIn this paper we improve upon the work of Kumar and Kannan [KK10] along several axes. First, we weaken the center separation bound by a factor of \u221a k, and secondly we weaken the proximity condition by a factor of k (in other words, the revised separation condition is independent of k). Using these weaker bounds we still achieve the same guarantees when all points satisfy the proximity condition. Under the same weaker bounds, we achieve even better guarantees when only (1\u2212\u01eb)-fraction of the points satisfy the condition. Specifically, we correctly cluster all but a (\u01eb + O(1/c4))-fraction of the points, compared to O(k2\u01eb)-fraction of [KK10], which is meaningful even in the particular setting when \u01eb is a constant and k = \u03c9(1). Most importantly, we greatly simplify the analysis of Kumar and Kannan. In fact, in the bulk of our analysis we ignore the proximity condition and use only center separation, along with the simple triangle and Markov inequalities. Yet these basic tools suffice to produce a clustering which (i) is correct on all but a constant fraction of the points, (ii) has k-means cost comparable to the k-means cost of the target clustering, and (iii) has centers very close to the target centers.\nOur improved separation condition allows us to match the results of the Planted Partition Model of McSherry [McS01], improve upon the results of Ostrovsky et al [ORSS06], and improve separation results for mixture of Gaussian models in a particular setting."}, {"heading": "1 Introduction", "text": "In the long-studied field of clustering, there has been substantial work [Das99, DS07, SK01, VW02, AM05, CR08b, KSV08, DHKM07, BV08] studying the problem of clustering data from mixture of distributions under the assumption that the means of the distributions are sufficiently far apart. Each of these works focuses on one particular type (or family) of distribution, and devise an algorithm that successfully clusters datasets that come from that particular type. Typically, they show that w.h.p. such datasets have certain nice properties, then use these properties in the construction of the clustering algorithm.\nThe recent work of Kumar and Kannan [KK10] takes the opposite approach. First, they define a separation condition, deterministic and thus not tied to any distribution, and show that any set of data points satisfying this condition can be successfully clustered. Having established that, they show that many previously studied clustering problems indeed satisfy (w.h.p) this separation condition. These clustering problems include Gaussian mixture-models, the Planted Partition model of McSherry [McS01] and the work of Ostrovsky et al [ORSS06]. In this aspect they aim to unify the existing body of work on clustering under separation assumptions, proving that one algorithm applies in multiple scenarios.1\nHowever, the attempt to unify multiple clustering works is only successful in part. First, Kumar and Kannan\u2019s analysis is \u201cwasteful\u201d w.r.t the number of clusters k. Clearly, motivated by an underlying assumption that k is constant, their separation bound has linear dependence in k and their classification guarantee has quadratic dependence on k. As a result, Kumar and Kannan overshoot best known bounds for the Planted Partition Model and for mixture of Gaussians by a factor of \u221a k. Similarly, the application to datasets considered by Ostrovsky et al only holds for constant k. Secondly, the analysis in Kumar-Kannan is far from simple \u2013 it relies on most points being \u201cgood\u201d, and requires multiple iterations of Lloyd steps before converging to good centers. Our work addresses these issues.\nTo formally define the separation condition of [KK10], we require some notation. Our input consists of n points in Rd. We view our dataset as a n \u00d7 d matrix, A, where each datapoint corresponds to a row Ai in this matrix. We assume the existence of a target partition, T1, T2, . . . , Tk, where each cluster\u2019s center is \u00b5r =\n1 nr\n\u2211\ni\u2208Tr Ai, where nr = |Tr|. Thus, the target clustering is represented by a n\u00d7d matrix of cluster centers, C, where Ci = \u00b5r iff i \u2208 Tr. Therefore, the k-means cost of this partition is the squared Frobenius norm \u2016A \u2212 C\u20162F , but the focus of this paper is on the spectral (L2) norm of the matrix A \u2212 C. Indeed, the deterministic equivalent of the maximal variance in any direction is, by definition, 1n\u2016A\u2212 C\u20162 = max{v: \u2016v\u2016=1} 1n\u2016(A \u2212C)v\u20162.\nDefinition. Fix i \u2208 Tr. We say a datapoint Ai satisfies the Kumar-Kannan proximity condition if for any s 6= r, when projecting Ai onto the line connecting \u00b5r and \u00b5s, the projection of Ai is closer to \u00b5r than to \u00b5s by an additive factor of \u2126 (\nk( 1\u221anr + 1\u221a ns )\u2016A\u2212 C\u2016\n)\n.\nKumar and Kannan proved that if all but at most \u01eb-fraction of the data points satisfy the proximity condition, they can find a clustering which is correct on all but an O(k2\u01eb)-fraction of the points. In particular, when \u01eb = 0, their algorithm clusters all points correctly. Observe, the Kumar-Kannan proximity condition gives that the distance \u2016\u00b5r \u2212\u00b5s\u2016 is also bigger than the above\n1We comment that, implicitly, Achlioptas and McSherry [AM05] follow a similar approach, yet they focus only on mixtures of Gaussians and log-concave distributions. Another deterministic condition for clustering was considered by [CO10], which generalized the Planted Partition Model of [McS01].\nmentioned bound. The opposite also holds \u2013 one can show that if \u2016\u00b5r \u2212 \u00b5s\u2016 is greater than this bound then only few of the points do not satisfy the proximity condition."}, {"heading": "1.1 Our Contribution", "text": "Our Separation Condition. In this work, the bulk of our analysis is based on the following quantitatively weaker version of the proximity condition, which we call center separation. Formally, we define \u2206r =\n1\u221a nr\nmin{ \u221a k\u2016A \u2212 C\u2016, \u2016A \u2212 C\u2016F } and we assume throughout the paper that for a\nlarge constant2 c we have that the means of any two clusters Tr and Ts satisfy\n\u2016\u00b5r \u2212 \u00b5s\u2016 \u2265 c(\u2206r +\u2206s) (1)\nObserve that this is a simpler version of the Kumar-Kannan proximity condition, scaled down by a factor of \u221a k. Even though we show that (1) gives that only a few points do not satisfy the proximity condition, our analysis (for the most part) does not partition the dataset into good and bad points, based on satisfying or non-satisfying the proximity condition. Instead, our analysis relies on basic tools, such as the Markov inequality and the triangle inequality. In that sense one can view our work as \u201caligning\u201d Kumar and Kannan\u2019s work with the rest of clustering-under-center-separation literature \u2013 we show that the bulk of Kannan and Kumar\u2019s analysis can be simplified to rely merely on center-separation.\nOur results. We improve upon the results of [KK10] along several axes. In addition to the weaker condition of Equation (1), we also weaken the Kumar-Kannan proximity condition by a factor of k, and still retrieve the target clustering, if all points satisfy the (k-weaker) proximity condition. Secondly, if at most \u01ebn points do not satisfy the k-weaker proximity condition, we show that we can correctly classify all but a (\u01eb+O(1/c4))-fraction of the points, improving over the bound of [KK10] of O(k2\u01eb). Note that our bound is meaningful even if \u01eb is a constant whereas k = \u03c9(1). Furthermore, we prove that the k-means cost of the clustering we output is a (1 +O(1/c))-approximation of the k-means cost of the target clustering.\nOnce we have improved on the main theorem of Kumar and Kannan, we derive immediate improvements on its applications. In Section 3.1 we show our analysis subsumes the work of Ostrovsky et al [ORSS06], and applies also to non-constant k. Using the fact that Equation (1) \u201cshaves off\u201d a\u221a k factor from the separation condition of Kumar and Kannan, we obtain a separation condition of \u2126(\u03c3max \u221a k) for learning a mixture of Gaussians, and we also match the separation results of the Planted Partition model of McSherry [McS01]. These results are described in Section 5. From an approximation-algorithms perspective, it is clear why the case of k = \u03c9(1) is of interest, considering the ubiquity of k-partition problems in TCS (e.g., k-Median, Max k-coverage, Knapsack for k items, maximizing social welfare in k-items auction \u2013 all trivially simple for constant k). In addition, we comment that in our setting only the case where k = \u03c9(1) is of interest, since otherwise one can approximate the k-means cost using the PTAS of Kumar et al [KSS04], which\n2We comment that throughout the paper, and much like Kumar and Kannan, we think of c as a large constant (c = 100 will do). However, our results also hold when c = \u03c9(1), allowing for a (1 + o(1))-approximation. We also comment that we think of d \u226b k, so one should expect \u2016A\u2212C\u20162F \u2265 k\u2016A\u2212C\u20162 to hold, thus the reader should think of \u2206r as dependent on \u221a k\u2016A\u2212C\u2016. Still, including the degenerate case, where \u2016A\u2212C\u20162F < k\u2016A\u2212C\u2016, simplifies our analysis in Section 3. One final comment is that (much like all the work in this field) we assume k is given, as part of the input, and not unknown.\ndoesn\u2019t even require any separation assumptions. From a practical point of view, there is a variety of applications where k is quite large. This includes problems such as clustering images by who is in them, clustering protein sequences by families of organisms, and problems such as deduplication where multiple databases are combined and entries corresponding to the same true entity are to be clustered together [CR02, MBHC95]. The challenges that arise from treating k as a non-constant are detailed in the proofs overview (Section 1.4).\nTo formally detail our results, we first define some notations and discuss a few preliminary facts."}, {"heading": "1.2 Notations and Preliminaries", "text": "The Frobenius norm of a n \u00d7m matrix M , denoted as \u2016M\u2016F is defined as \u2016M\u2016F = \u221a \u2211 i,j M 2 i,j . The spectral norm of M is defined as \u2016M\u2016 = maxx:\u2016x\u2016=1 \u2016Mx\u2016. It is a well known fact that if the rank of M is t, then \u2016M\u20162F \u2264 t\u2016M\u20162. The Singular Value Decomposition (SVD) of M is a decomposition of M as M = U\u03a3V T , where U is a n \u00d7 n unitary matrix, V is a m \u00d7 m unitary matrix, \u03a3 is a n\u00d7m diagonal matrix whose entries are nonnegative real numbers, and its diagonal entries satisfy \u03c31 \u2265 \u03c32 \u2265 . . . \u2265 \u03c3min{m,n}. The diagonal entries in \u03a3 are called the singular values of M , and the columns of U and V , denoted ui and vi resp., are called the left- and right-singular vectors. As a convention, when referring to singular vectors, we mean the right-singular vectors. Observe that the Singular Value Decomposition allows us to writeM = \u2211rank(\u03a3)\ni=1 \u03c3iuiv T i . Projecting\nM onto its top t singular vectors means taking M\u0302 = \u2211t i=1 \u03c3iuiv T i . It is a known fact that for any t, the t-dimensional subspace which best fits the rows of M , is obtained by projecting M onto the subspace spanned by the top t singular vectors (corresponding to the top t singular values). Another way to phrase this result is by saying that M\u0302 = argminN :rank(N)=t{\u2016M \u2212 N\u2016F }. For a proof, see [KV09]. The same matrix, M\u0302 , also minimizes the spectral norm of this difference, meaning M\u0302 = argminN :rank(N)=t{\u2016M \u2212N\u2016} (see [GVL96] for proof).\nAs previously defined, \u2016A\u2212 C\u2016 denotes the spectral norm of A\u2212C. The target clustering, T , is composed of k clusters T1, T2, . . . , Tk. Observe that we use \u00b5 as an operator, where for every set X, we have \u00b5(X) = 1|X| \u2211 i\u2208X Ai. We abbreviate, and denote \u00b5r = \u00b5(Tr). From this point on, we denote the projection of A onto the subspace spanned by its top k-singular vectors as A\u0302, and for any vector v, we denote v\u0302 as the projection of v onto this subspace. Throughout the paper, we abuse notation and use i to iterate over the rows of A, whereas r and s are used to iterate over clusters (or submatrices). So Ai represents the ith row of A whereas Ar represents the submatrix [Ai]{i\u2208Tr}.\nBasic Facts. The analysis of our main theorem makes use of the following facts, from [McS01, KV09, KK10]. We advise the reader to go over the proofs, which are short, elegant, and provided in Appendix A. The first fact bounds the cost of assigning the points of A\u0302 to their original centers.\nFact 1.1 (Lemma 9 from [McS01]). \u2016A\u0302\u2212C\u20162F \u2264 8min{k\u2016A\u2212C\u20162, \u2016A\u2212C\u20162F } ( = 8nr\u2206 2 r for every r ) .\nNext, we show that we can match each target center \u00b5r to a unique, relatively close, center \u03bdr that we get in Part I of the algorithm.\nFact 1.2 (Claim 1 in Section 3.2 of [KV09]). For every \u00b5r there exists a center \u03bds s.t. \u2016\u00b5r \u2212 \u03bds\u2016 \u2264 6\u2206r, so we can match each \u00b5r to a unique \u03bdr.\nFinally, we exhibit the following fact, which is detailed in the analysis of [KK10].\nFact 1.3. Fix a target cluster Tr and let Sr be a set of points created by removing \u03c1outnr points from Tr and adding \u03c1in(s)nr points from each cluster s 6= r, s.t. every added point x satisfies \u2016x\u2212 \u00b5s\u2016 \u2265 23\u2016x\u2212 \u00b5r\u2016. Assume \u03c1out < 14 and \u03c1in def = \u2211 s 6=r \u03c1in(s) < 1 4 . Then\n\u2016\u00b5(Sr)\u2212 \u00b5r\u2016 \u2264 1\u221a nr\n\n \u221a \u03c1out + 3 2 \u2211\ns 6=r\n\u221a\n\u03c1in(s)\n  \u2016A\u2212 C\u2016 \u2264 (\u221a\n\u03c1out nr\n+ 32 \u221a k \u221a\n\u03c1in nr\n)\n\u2016A\u2212 C\u2016"}, {"heading": "1.3 Formal Description of the Algorithm and Our Theorems", "text": "Having established notation, we now present our algorithm, in Figure 1. Our algorithm\u2019s goal is three fold: (a) to find a partition that identifies with the target clustering on the majority of the points, (b) to have the k-means cost of this partition comparable with the target, and (c) output k centers which are close to the true centers. It is partitioned into 3 parts. Each part requires stronger assumptions, allowing us to prove stronger guarantees.\n\u2022 Assuming only the center separation of (1), then Part I gives a clustering which (a) is correct on at least 1\u2212O(c\u22122) fraction of the points from each target cluster (Theorem 3.1), and (b) has k-means cost smaller than (1 +O(1/c))\u2016A \u2212 C\u20162F (Theorem 3.2). \u2022 Assuming also that \u2206r = \u221a k\u221a nr \u2016A \u2212 C\u2016, i.e. assuming the non-degenerate case where \u2016A \u2212\nC\u20162F \u2265 k\u2016A\u2212C\u20162, then Part II finds centers that are O(1/c) \u2016A\u2212C\u2016\u221a nr close to the true centers (Theorem 4.1). As a result (see Section 4.1), if (1\u2212 \u01eb)n points satisfy the proximity condition (weakened by a k factor,), then we misclassify no more than (\u01eb+O(c\u22124))n points.\n\u2022 Assuming all points satisfy the proximity condition (weakened by a k-factor), Part III finds exactly the target partition (Theorem 4.8)."}, {"heading": "1.4 Organization and Proofs Overview", "text": "Organization. Related work is detailed in Section 2. The analysis of Part I of our algorithms is in Section 3. Part I is enough for us to give a \u201cone-line\u201d proof in Section 3.1 showing how the work of Ostrovsky et al falls into our framework. The analysis of Part II of the algorithm is in Section 4. The improved guarantees we get by applying the algorithm to the Planted Partition model and to the Gaussian mixture model are discussed in Section 5. We conclude with an open problem in Section 6.\nProof outline for Section 3. The first part of our analysis is an immediate application of Facts 1.1 and 1.2. Our assumption dictates that the distance between any two centers is big (\u2265 c(\u2206r +\u2206s)). Part I of the algorithm assigns each projected point A\u0302i to the nearest \u03bdr instead of the true center \u00b5r and Fact 1.2 assures that the distance \u2016\u00b5r \u2212 \u03bdr\u2016 is small (< 6\u2206r). Consider a misclassified point Ai, where \u2016Ai \u2212 \u00b5r\u2016 < \u2016Ai \u2212 \u00b5s\u2016 yet \u2016A\u0302i \u2212 \u03bds\u2016 < \u2016A\u0302i \u2212 \u03bdr\u2016. The triangle inequality assures that A\u0302i has a fairly big distance to its true center (> ( c 2\u221212)\u2206r). We deduce that each misclassified point contributes \u2126(c2\u22062r) to the k-means cost of assigning all projected points to their true centers. Fact 1.1 bounds this cost by \u2016A\u0302\u2212C\u20162F \u2264 8nr\u22062r , so the Markov inequality proves only a few points are misclassified. Additional application of the triangle inequality for misclassified points gives that the distance between the original point Ai and a true center \u00b5r is comparable to the distance \u2016Ai \u2212 \u00b5s\u2016, and so assigning Ai to the cluster s only increases the k-means cost by a small factor.\nProof outline for Section 4. In the second part of our analysis we compare between the true clustering T and some proposed clustering S, looking both at the number of misclassified points and at the distances between the matching centers \u2016\u00b5r \u2212 \u03b8r\u2016. As Kumar and Kannan show, the two measurements are related: Fact 1.3 shows how the distances between the means depend on the number of misclassified points, and the main lemma (Lemma 4.5) essentially shows the opposite direction. These two relations are how Kumar and Kannan show that Lloyd steps converge to good centers, yielding clusters with few misclassified points. They repeatedly apply (their version of) the main lemma, showing that with each step the distances to the true means decrease and so fewer of the good points are misclassified.\nTo improve on Kumar and Kannan analysis, we improve on the two above-mentioned relations. Lemma 4.5 is a simplification of a lemma from Kumar and Kannan, where instead of projecting into a k-dimensional space, we project only into a 4-dimensional space, thus reducing dependency on k. However, the dependency of Fact 1.3 on k is tight3. So in Part II of the algorithm we devise sub-clusters Sr s.t. \u03c1in(s) = \u03c1out/k\n2. The crux in devising Sr lies in Proposition 4.4 \u2013 we show that any misclassified projected point i \u2208 Ts \u2229 Sr is essentially misclassified by \u00b5\u0302r. And since (see [AM05]) \u2016\u00b5r \u2212 \u00b5\u0302r\u2016 \u2264 1\u221ak\u2206r (compared to the bound \u2016\u00b5r \u2212 \u03bdr\u2016 \u2264 6\u2206r), we are able to give a good bound on \u03c1in(s).\nRecall that we rely only on center separation rather than a large batch of points satisfying the Kumar-Kannan separation, and so we do not apply iterative Lloyd steps (unless all points are good). Instead, we apply the main lemma only once, w.r.t to the misclassified points in Ts\u2229Sr, and deduce that the distances \u2016\u00b5r \u2212 \u03b8r\u2016 are small. In other words, Part II is a single step that retrieve\n3In fact, Fact 1.3 is exactly why the case of k = \u03c9(1) is hard \u2013 because the L1 and L2 norms of the vector ( 1\u221a\nk , 1\u221a k , . . . , 1\u221a k ) are not comparable for non-constant k.\ncenters whose distances to the original centers are \u221a k-times better than the centers retrieved by Kumar and Kannan in numerous Lloyd iterations."}, {"heading": "2 Related Work", "text": "The work of [Das99] was the first to give theoretical guarantees for the problem of learning a mixture of Gaussians under separation conditions. He showed that one can learn a mixture of k spherical Gaussians provided that the separation between the cluster means is \u2126\u0303( \u221a n(\u03c3r +\u03c3s)) and the mixing weights are not too small. Here \u03c32r denotes the maximum variance of cluster r along any direction. This separation was improved to \u2126\u0303((\u03c3r + \u03c3s)n\n1/4) by [DS07]. Arora and Kannan [SK01] extended these results to the case of general Gaussians. For the case of spherical Gaussians, [VW02] showed that one can learn under a much weaker separation of \u2126\u0303((\u03c3r + \u03c3s)k\n1/4). This was extended to arbitrary Gaussians by [AM05] and to various other distributions by [KSV08], although requiring a larger separation. In particular, the work of [AM05] requires a separation of \u2126((\u03c3r + \u03c3s)( 1\u221a\nmin(wr ,ws) + \u221a k log(kmin{2k, n}))) whereas [KSV08] require a separation of \u2126\u0303( k3/2 w2min (\u03c3r+\u03c3s)).\nHere wr\u2019s refer to the mixing weights. [CR08a, CR08b] gave algorithms for clustering mixtures of product distributions and mixtures of heavy tailed distributions. [BV08] gave an algorithm for clustering the mixture of 2 Gaussians assuming only that the two Gaussians are separated by a hyperplane. They also give results for learning a mixture of k > 2 Gaussians. The work of [KMV10] gave an algorithm for learning a mixture of 2 Gaussians, with provably minimal assumptions. This was extended in [MV10] to the case when k > 2 although the algorithm runs in time exponential in k. Similar results were obtained in the work of [BS10] who can also learn more general distribution families. The work of [CO10] studied a deterministic separation condition required for efficient clustering. The precise condition presented in [CO10] is technical but essentially assumes that the underlying graph over the set of points has a \u201clow rank structure\u201d and presents an algorithm to recover this structure which is then enough to cluster well. In addition, previous works (e.g. [Sch00, BBG09]) addressed the problem of clustering from the viewpoint of minimizing the number of mislabeled points.\nThere has been an extensive line of work on approximation algorithms for the k-means problem ([OR00, BHPI02, dlVKKR03, ES04, HPM04, KMN+02]). The current best guarantee is a (9 + \u01eb)-approximation algorithm of [KMN+02] (with a much simpler analysis in [GT08]) if polynomial dependence on k and the dimension d is desired.4 Another popular algorithm for k-means is the Lloyd\u2019s heuristics ([Llo82]). This heuristics, combined with a careful seeding of centers, has been shown to have good performance if the data is well separated (see [ORSS06]), or to provide O(log(k))-approximation in general [AV07]. The separation-based results of [ORSS06] were improved by [ABS10]."}, {"heading": "3 Part I of the Algorithm", "text": "In this section, we look only at Part I of our algorithm. Our approximation algorithm defines a clustering Z, where Zr = {i : \u2016A\u0302i \u2212 \u03bdr\u2016 \u2264 \u2016A\u0302i \u2212 \u03bds\u2016 for every s}. Our goal in this section is to show that Z is correct on all but a small constant fraction of the points, and furthermore, the k-means cost of Z is no more than (1 +O(1/c)) times the k-means cost of the target clustering.\n4For constant k, [KSS04] give a PTAS for the k-means problem.\nTheorem 3.1. There exists a matching (given by Fact 1.2) between the target clustering T and the clustering Z = {Zr}r where Zr = {i : \u2016A\u0302i \u2212 \u03bdr\u2016 \u2264 \u2016A\u0302i \u2212 \u03bds\u2016 for every s} that satisfies the following properties:\n\u2022 For every cluster Ts0 in the target clustering, no more than O(1/c2)|Ts0 | points are misclassified.\n\u2022 For every cluster Zr0 in the clustering that the algorithm outputs, we add no more than O(1/c2)|Tr0 | points from other clusters.\n\u2022 At most O(1/c2)|Tr2 | points are misclassified overall, where Tr2 is the second largest cluster.\nProof. Let us denote Ts\u2192r as the set of points A\u0302i that are assigned to Ts in the target clustering, yet are closer to \u03bdr than to any other \u03bd \u2032 r. From triangle inequality we have that \u2016A\u0302i \u2212 \u00b5s\u2016 \u2265 \u2016A\u0302i \u2212 \u03bds\u2016 \u2212 \u2016\u00b5s \u2212 \u03bds\u2016. We know from Fact 1.2 that \u2016\u00b5s \u2212 \u03bds\u2016 \u2264 6\u2206s. Also, since A\u0302i is closer to \u03bdr than to \u03bds, the triangle inequality gives that 2\u2016A\u0302i \u2212 \u03bds\u2016 \u2265 \u2016\u03bdr \u2212 \u03bds|. So,\n\u2016A\u0302i \u2212 \u00b5s\u2016 \u2265 1\n2 \u2016\u03bdr \u2212 \u03bds\u2016 \u2212 6\u2206s \u2265\n1 2 \u2016\u00b5r \u2212 \u00b5s\u2016 \u2212 12(\u2206r +\u2206s) \u2265 c 4 (\u2206r +\u2206s)\nThus, we can look at \u2016A\u0302\u2212 C\u20162F , and using Fact 1.1 we immediately have that for every fixed r\u2032\n\u2211\nr\n\u2211 s 6=r |Ts\u2192r|\nc2 16 (\u2206r +\u2206s) 2 \u2264 \u2211\nr\n\u2211 i\u2208Tr \u2016A\u0302i \u2212 \u00b5r\u20162 = \u2016A\u0302\u2212 C\u20162F \u2264 8nr\u2032\u22062r\u2032\nThe proof of the theorem follows from fixing some r0 or some s0 and deducing:\n\u22062s0\n\u2211 r 6=s0 |Ts0\u2192r| \u2264 \u2211 r 6=s0 |Ts0\u2192r|(\u2206r +\u2206s0)2 \u2264 \u2211 r \u2211 s 6=r |Ts\u2192r|(\u2206r +\u2206s)2 \u2264\n128\nc2 ns0\u2206\n2 s0\n\u22062r0\n\u2211 s 6=r0 |Ts\u2192r0 | \u2264 \u2211 s 6=r0 |Ts\u2192r0 |(\u2206r0 +\u2206s)2 \u2264 \u2211 r \u2211 s 6=r |Ts\u2192r|(\u2206r +\u2206s)2 \u2264\n128\nc2 nr0\u2206\n2 r0\nObserve that for every r 6= s we have that \u2206r +\u2206s \u2265 \u2206r2 (where r2 is the cluster with the second largest number of points), so we have that\n\u22062r2\n\u2211\nr\n\u2211 s 6=r |Ts\u2192r| \u2264 \u2211 r \u2211 s 6=r |Ts\u2192r|(\u2206r +\u2206s)2 \u2264\n128\nc2 nr2\u2206\n2 r2\nWe now show that the k-means cost of Z is close to the k-means cost of T . Observe that the k-means cost of Z is computed w.r.t the best center of each cluster (i.e., \u00b5(Zr)), and not w.r.t the centers \u03bdr.\nTheorem 3.2. The k-means cost of Z is at most (1 +O(1/c))\u2016A \u2212 C\u20162F .\nProof. Given Z, it is clear that the centers that minimize its k-means cost are \u00b5(Zr) = 1|Zr| \u2211\ni\u2208Zr Ai. Recall that the majority of points in each Zr belong to a unique Tr, and so, throughout this section, we assume that all points in Zr were assigned to \u00b5r, and not to \u00b5(Zr). (Clearly, this can only increase the cost.) We show that by assigning the points of Zr to \u00b5r, our cost is at most (1+O(1/c))\u2016A\u2212C\u20162F , and so Theorem 3.2 follows. In fact, we show something stronger. We show\nthat by assigning all the points in Zr to \u00b5r, each point Ai pays no more than (1+O(1/c))\u2016Ai\u2212Ci\u20162. This is clearly true for all the points in Zr\u2229Tr. We show this also holds for the misclassified points.\nBecause i \u2208 Ts\u2192r, it holds that \u2016A\u0302i \u2212 \u03bdr\u2016 \u2264 \u2016A\u0302i \u2212 \u03bds\u2016. Observe that for every s we have that \u2016Ai\u2212\u03bds\u20162 = \u2016Ai\u2212 A\u0302i\u20162+\u2016A\u0302i\u2212\u03bds\u20162, because A\u0302i\u2212\u03bds is the projection of Ai\u2212\u03bds onto the subspace spanned by the top k-singular vectors of A. Therefore, it is also true that \u2016Ai \u2212 \u03bdr\u2016 \u2264 \u2016Ai \u2212 \u03bds\u2016. Because of Fact 1.2, we have that \u2016\u00b5r \u2212 \u03bdr\u2016 \u2264 6\u2206r and \u2016\u00b5s \u2212 \u03bds\u2016 \u2264 6\u2206s, so we apply the triangle inequality and get\n\u2016Ai \u2212 \u00b5r\u2016 \u2264 \u2016Ai \u2212 \u00b5s\u2016+ \u2016\u00b5r \u2212 \u03bdr\u2016+ \u2016\u00b5s \u2212 \u03bds\u2016 \u2264 \u2016Ai \u2212 \u00b5s\u2016 ( 1 + 6(\u2206r +\u2206s)\n\u2016Ai \u2212 \u00b5s\u2016\n)\nSo all we need to do is to lower bound \u2016Ai \u2212 \u00b5s\u2016. As noted, \u2016Ai \u2212 \u03bds\u2016 \u2265 \u2016A\u0302i \u2212 \u03bds\u2016. Thus\n\u2016Ai \u2212 \u00b5s\u2016 \u2265 \u2016Ai \u2212 \u03bds\u2016 \u2212 6\u2206r \u2265 \u2016A\u0302i \u2212 \u03bds\u2016 \u2212 6\u2206r \u2265 1\n2 \u2016\u03bds \u2212 \u03bdr\u2016 \u2212 6\u2206r \u2265\n1 4 c(\u2206r +\u2206s)\nand we have the bound \u2016Ai \u2212 \u00b5r\u2016 \u2264 ( 1 + 24c ) \u2016Ai \u2212 \u00b5s\u2016, so \u2016Ai \u2212 \u00b5r\u20162 \u2264 ( 1 + 49c ) \u2016Ai \u2212 \u00b5s\u20162."}, {"heading": "3.1 Application: The ORSS-Separation", "text": "One straight-forward application of Theorem 3.2 is for the datasets considered by Ostrovsky et al [ORSS06], where the optimal k-means cost is an \u01eb-fraction of the optimal (k \u2212 1)-means cost. Ostrovsky et al proved that for such datasets a variant of the Lloyd method converges to a good solution in polynomial time. Kumar and Kannan have shown that datasets satisfying the ORSSseparation, also have the property that most points satisfy their proximity-condition. Their analysis is not immediate, and gives a (1 + O( \u221a k\u01eb))-approximation. Here, we provide a \u201cone-line\u201d proof that Part I of Algorithm \u223cCluster yields a (1 +O(\u221a\u01eb))-approximation, for any k. Suppose we have a dataset satisfying the ORSS-separation condition, so any (k \u2212 1)-partition of the dataset have cost \u2265 1\u01eb\u2016A \u2212 C\u20162F . For any r and any s 6= r, by assigning all the points in Tr to the center \u00b5s, we get some (k \u2212 1)-partition whose cost is exactly \u2016A\u2212C\u20162F + nr\u2016\u00b5r \u2212 \u00b5s\u20162, so \u2016\u00b5r \u2212 \u00b5s\u2016 \u2265 \u221a 1 \u01eb \u22121\n\u221a nr\n\u2016A\u2212 C\u2016F . Setting c = O(1/ \u221a \u01eb), Theorem 3.2 is immediate."}, {"heading": "4 Part II of the Algorithm", "text": "In this section, our goal is to show that Part II of our algorithm gives centers that are very close to the target clusters. We should note that from this point on, we assume we are in the non-degenerate case, where \u2016A\u2212 C\u20162F \u2265 k\u2016A\u2212 C\u20162. Therefore, \u2206r = \u221a k\u221a nr \u2016A\u2212 C\u2016.\nRecall, in Part II we define the sets Sr = {i : \u2016A\u0302i \u2212 \u03bdr\u2016 \u2264 13\u2016A\u0302i \u2212 \u03bds\u2016, \u2200s 6= r}. Observe, these set do not define a partition of the dataset! There are some points that are not assigned to any Sr. However, we only use the centers of Sr. We prove the following theorem.\nTheorem 4.1. Denote Sr = {i : \u2016A\u0302i \u2212 \u03bdr\u2016 \u2264 13\u2016A\u0302i \u2212 \u03bds\u2016, \u2200s 6= r}. Then for every r it holds that \u2016\u00b5(Sr)\u2212 \u00b5r\u2016 = O(1/c) 1\u221anr \u2016A\u2212C\u2016 = O( 1 c \u221a k \u2206r).\nThe proof of Theorem 4.1 is an immediate application of Fact 1.3 combined with the following two lemmas, that bound the number of misclassified points. Observe that for every point that\nbelongs to Ts yet is assigned to Sr (for s 6= r) is also assigned to Zr in the clustering Z discussed in the previous section. Therefore, any misclassified point i \u2208 Ts \u2229 Sr satisfies that \u2016Ai \u2212 \u00b5r\u2016 \u2264 (1 +O(c\u22121))\u2016Ai \u2212 \u00b5s\u2016 as the proof of Theorem 3.2 shows. So all conditions of Fact 1.3 hold. Lemma 4.2. Assume that for every r we have that \u2016\u00b5r \u2212 \u03bdr\u2016 \u2264 6\u2206r. Then at most 512c2 nr points of Tr do not belong to Sr.\nLemma 4.3. Redefine Ts\u2192r as the set Ts \u2229 Sr. Assume that for every r we have that \u2016\u00b5r \u2212 \u03bdr\u2016 \u2264 6\u2206r. Then for every r and every s 6= r we have that |Ts\u2192r| = ( 482\nc4k2\n)\nnr.\nProof of Lemma 4.2. First, we claim that if i is such that \u2016A\u0302i \u2212 \u00b5r\u2016 \u2264 c8\u2206r, then it must be the case that i \u2208 Sr.\nThis is a simple consequence of the triangle inequality, bounding \u2016A\u0302i \u2212 \u03bdr\u2016 \u2264 \u2016A\u0302i \u2212 \u00b5r\u2016 + \u2016\u00b5r \u2212 \u03bdr\u2016 \u2264 ((c/8) + 6)\u2206r. Yet, for every s 6= r, the triangle inequality gives that \u2016A\u0302i \u2212 \u03bds\u2016 \u2265 \u2016\u00b5r \u2212 \u00b5s\u2016 \u2212 \u2016A\u0302i \u2212 \u00b5r\u2016 \u2212 \u2016\u00b5s \u2212 \u03bds\u2016 \u2265 (c \u2212 c8 \u2212 6)(\u2206r + \u2206s). Assuming c > 48, we have that \u2016A\u0302i \u2212 \u03bds\u2016 \u2265 3\u2016A\u0302i \u2212 \u03bdr\u2016.\nAll that\u2019s left is to show that the number of i \u2208 Tr s.t. \u2016A\u0302i \u2212 \u00b5r\u2016 > c8\u2206r is small. This again follows from the Markov inequality: Since \u2016A\u0302\u2212C\u20162F \u2264 8k\u2016A\u2212C\u20162, then the number of such points is at most 8k\u2016A\u2212C\u2016 2\n(c2/64)k\u2016A\u2212C\u20162nr.\nWe now turn to proving Lemma 4.3. The general outline of the proof of Lemma 4.3 resembles to the outline of the proof of Lemma 4.2. Proposition 4.4 exhibit some property that every point in Ts\u2192r must satisfy, and then we show that only few of the points in Ts satisfy this property. Recall that \u00b5\u0302r indicates the projection of \u00b5r onto the subspace spanned by the top k-singular vectors of A. Proposition 4.4. Fix i \u2208 Ts s.t. \u2016A\u0302i\u2212 \u00b5\u0302s\u2016 \u2264 2\u2016A\u0302i\u2212 \u00b5\u0302r\u2016. Then \u2016A\u0302i\u2212\u03bds\u2016 < 3\u2016A\u0302i\u2212\u03bdr\u2016, so i /\u2208 Sr. Proof. First, for every r we have that \u2016\u00b5\u0302r \u2212 \u03bdr\u2016 \u2264 \u2016\u00b5r \u2212 \u03bdr\u2016 \u2264 6\u2206r, as \u00b5\u0302r \u2212 \u03bdr is a projection of \u00b5r \u2212 \u03bdr.\nLet us fiddle with the triangle inequality, in order to obtain a lower bound on \u2016A\u0302i \u2212 \u03bdr\u2016. We have that 3\u2016A\u0302i\u2212 \u00b5\u0302r\u2016 \u2265 \u2016\u00b5\u0302r\u2212 \u00b5\u0302s\u2016 \u2265 \u2016\u00b5r\u2212\u00b5s\u2016\u2212 ( \u2016\u00b5r\u2212\u03bdr\u2016+\u2016\u03bdr\u2212 \u00b5\u0302r\u2016 ) \u2212 ( \u2016\u00b5s\u2212\u03bds\u2016+\u2016\u03bds\u2212 \u00b5\u0302s\u2016 )\n\u2265 (c\u2212 12)(\u2206r +\u2206s), thus \u2016A\u0302i \u2212 \u03bdr\u2016 \u2265 ( c\u221212 3 \u2212 6 ) (\u2206r +\u2206s).\nAssume for the sake of contradiction that \u2016A\u0302i \u2212 \u03bds\u2016 \u2265 3\u2016A\u0302i \u2212 \u03bdr\u2016, and let us show this yields an upper bound on \u2016A\u0302i \u2212 \u03bdr\u2016, which contradicts our lower bound. We have that\n6\u2206s \u2265 \u2016A\u0302i \u2212 \u03bds\u2016 \u2212 \u2016A\u0302i \u2212 \u00b5\u0302s\u2016 \u2265 3\u2016A\u0302i \u2212 \u03bdr\u2016 \u2212 2\u2016A\u0302i \u2212 \u00b5\u0302r\u2016 \u2265 \u2016A\u0302i \u2212 \u03bdr\u2016 \u2212 2 \u00b7 6\u2206r It follows that 12(\u2206r +\u2206s) \u2265 \u2016A\u0302i \u2212 \u03bdr\u2016 \u2265 ( c\u221212 3 \u2212 6 ) (\u2206r +\u2206s). Contradiction (c > 60).\nProposition 4.4, shows that in order to bound |Ts\u2192r| it suffices to bound the number of points in Ts satisfying \u2016A\u0302i \u2212 \u00b5\u0302s\u2016 \u2265 2\u2016A\u0302i \u2212 \u00b5\u0302r\u2016. The major tool in providing this bound is the following technical lemma. This lemma is a variation on the work of [KK10], on which we improve on the dependency on k and simplify the proof.\nLemma 4.5 (Main Lemma). Fix \u03b1, \u03b2 > 0. Fix r 6= s and let \u03b6r and \u03b6s be two points s.t. \u2016\u00b5r \u2212 \u03b6r\u2016 \u2264 \u03b1\u2206r and \u2016\u00b5s \u2212 \u03b6s\u2016 \u2264 \u03b1\u2206s. We denote A\u0303i as the projection of Ai onto the line connecting \u03b6r and \u03b6s. Define X = { i \u2208 Ts : \u2016A\u0303i \u2212 \u03b6s\u2016 \u2212 \u2016A\u0303i \u2212 \u03b6r\u2016 \u2265 \u03b2\u2016\u03b6s \u2212 \u03b6r\u2016 }\n. Then |X| \u2264 256\u03b1 2\n\u03b22 1 c4k\n( min {nr, ns} ) .\nProof. Let V be the subspace spanned by the following 4 vectors: {\u00b5r, \u00b5s, \u03b6r, \u03b6s}. Denote PV as the projection onto V. We denote vi = PV(Ai), and observe that PV(\u00b5r) = \u00b5r, and the same goes for \u00b5s, \u03b6r and \u03b6s. Observe also that, as a projection, \u2016PV(A\u2212C)\u2016 \u2264 \u2016A\u2212C\u2016 (alternatively, \u2016PV\u2016 = 1).\nWe now make a simple observation. Let A\u0304i denote the projection of Ai onto the line connecting \u00b5r and \u00b5s. Now, the inequality \u2016Ai\u2212\u00b5s\u2016 < \u2016Ai\u2212\u00b5r\u2016 holds iff the inequality \u2016A\u0304i\u2212\u00b5s\u2016 \u2264 \u2016A\u0304i\u2212\u00b5r\u2016 holds (because \u2016Ai \u2212 \u00b5r\u20162 = \u2016Ai \u2212 A\u0304i\u20162 + \u2016A\u0304i \u2212 \u00b5r\u20162). Furthermore, such relation holds for any point whose projection on the line connecting \u00b5r and \u00b5s is identical to A\u0304i. In particular, if W is any subspace containing \u00b5r and \u00b5s, then the projection of Ai onto W is closer to \u00b5r than to \u00b5s iff Ai is closer to \u00b5r than to \u00b5s. Thus, since \u2016Ai \u2212 \u00b5s\u2016 \u2264 \u2016Ai \u2212 \u00b5r\u2016 then \u2016vi \u2212 \u00b5s\u2016 \u2264 \u2016vi \u2212 \u00b5r\u2016. Furthermore, as \u03b6r and \u03b6s also belong to V, then the projection of Ai onto the line connecting \u03b6s and \u03b6r is identical to the projection of vi onto the same line (meaning, A\u0303i = v\u0303i). So vi also satisfies the inequality: \u2016v\u0303i\u2212 \u03b6s\u2016\u2212\u2016v\u0303i\u2212 \u03b6r\u2016 \u2265 \u03b2\u2016\u03b6s\u2212 \u03b6r\u2016, and, of course, \u2016vi\u2212 \u03b6r\u20162 = \u2016vi\u2212 v\u0303i\u20162+\u2016v\u0303i\u2212 \u03b6r\u20162.\nThe proof follows from upper- and lower-bounding the term \u2016vi \u2212 \u03b6s\u20162 \u2212\u2016vi \u2212 \u03b6r\u20162. We\u2019ve just shown a lower bound, as we have that\n\u2016vi \u2212 \u03b6s\u20162 \u2212 \u2016vi \u2212 \u03b6r\u20162 = (\u2016v\u0303i \u2212 \u03b6s\u2016 \u2212 \u2016v\u0303i \u2212 \u03b6r\u2016) (\u2016v\u0303i \u2212 \u03b6s\u2016+ \u2016v\u0303i \u2212 \u03b6r\u2016) \u2265 \u03b2\u2016\u03b6s \u2212 \u03b6r\u20162\nThe triangle inequality gives that \u2016vi \u2212 \u03b6s\u2016 \u2264 \u2016vi \u2212 \u00b5s\u2016 + \u03b1(\u2206r + \u2206s), and that \u2016vi \u2212 \u03b6r\u2016 \u2265 \u2016vi \u2212 \u00b5r\u2016 \u2212 \u03b1(\u2206r +\u2206s), so we have the upper bound of\n\u2016vi \u2212 \u03b6s\u20162 \u2212 \u2016vi \u2212 \u03b6r\u20162 \u2264 (\u2016vi \u2212 \u00b5s\u2016+ \u03b1(\u2206r +\u2206s))2 \u2212 (\u2016vi \u2212 \u00b5r\u2016 \u2212 \u03b1(\u2206r +\u2206s))2\n\u2264 (\u2016vi \u2212 \u00b5r\u2016+ \u03b1(\u2206r +\u2206s))2 \u2212 (\u2016vi \u2212 \u00b5r\u2016 \u2212 \u03b1(\u2206r +\u2206s))2 \u2264 4\u03b1(\u2206r +\u2206s)\u2016vi \u2212 \u00b5r\u2016\nComparing the upper and the lower bound, we have that for any i \u2208 X the distance \u2016vi\u2212\u00b5r\u2016 \u2265 \u03b2 4\u03b1 (c\u2212\u03b1)2(\u2206r+\u2206s)2 \u2206r+\u2206s . As X \u2282 Ts, the Markov inequality concludes the proof\n|X| ( c2\n8\n\u03b2\n\u03b1\n\u221a k\u2016A\u2212 C\u2016 )2 1\nmin{nr, ns} \u2264\n\u2211 i\u2208Ts \u2016vi \u2212 \u00b5s\u20162 \u2264 \u2016PV(A\u2212 C)\u20162F \u2264 4\u2016A\u2212 C\u20162\nProof of Lemma 4.3. Every i \u2208 Ts\u2192r must satisfy that \u2016A\u0302i\u2212\u00b5\u0302s\u2016 \u2265 2\u2016A\u0302i\u2212\u00b5\u0302r\u2016 (Proposition 4.4). Therefore, we must have that \u2016A\u0303i \u2212 \u00b5\u0302s\u2016 \u2265 2\u2016A\u0303i \u2212 \u00b5\u0302r\u2016, where we denote A\u0303i as the projection of A onto the line connecting \u00b5\u0302r with \u00b5\u0302s (simply because \u2016A\u0302i \u2212 \u00b5\u0302s\u20162 = \u2016A\u0302i \u2212 A\u0303i\u20162 + \u2016A\u0303i \u2212 \u00b5\u0302s\u20162.) Therefore, \u2016\u00b5\u0302r \u2212 \u00b5\u0302s\u2016 \u2264 32\u2016A\u0303i \u2212 \u00b5\u0302s\u2016, so \u2016A\u0303i \u2212 \u00b5\u0302s\u2016 \u2212 \u2016A\u0303i \u2212 \u00b5\u0302r\u2016 > 13\u2016\u00b5\u0302r \u2212 \u00b5\u0302s\u2016.\nThus, every i \u2208 Ts\u2192r satisfies the conditions of Lemma 4.5 with \u03b6r = \u00b5\u0302r, \u03b6s = \u00b5\u0302s, and \u03b2 = 1/3. We deduce the |Ts\u2192r| \u2264 \u03b12 256\u00b79c4k min{nr, ns}, where \u03b1 is the bound s.t. for every r, \u2016\u00b5r \u2212 \u00b5\u0302r\u2016 \u2264 \u03b1\n\u221a k\u221a nr \u2016A\u2212C\u2016. Since \u03b1 \u2264 1\u221a k , we conclude the proof.\nThe fact that \u03b1 is small was proven by Achlioptas and McSherry (Theorem 1 of [AM05]). Denote ur as the indicator vector of Tr. Since rank(C) \u2264 k, we get\n\u2016\u00b5r \u2212 \u00b5\u0302r\u2016 = 1\nnr \u2016(A \u2212 A\u0302)Tur\u2016 \u2264\n1\nnr \u2016ur\u2016 \u2016A\u2212 A\u0302\u2016 \u2264 1\u221a nr \u2016A\u2212 C\u2016\nAs an interesting corollary, Theorem 4.1 dictates that for every r we have that \u2016\u00b5r \u2212 \u03b8r\u2016 = O(1/c)\u2016\u00b5r \u2212 \u00b5\u0302r\u2016."}, {"heading": "4.1 The Proximity Condition \u2013 Part III of the Algorithm", "text": "Part II of our algorithm returns centers \u03b81, . . . , \u03b8k which are O( 1 c \u221a nr )\u2016A \u2212 C\u2016 close to the true centers. Suppose we use these centers to cluster the points: \u0398s = {i : \u2200s\u2032, \u2016Ai\u2212 \u03b8s\u2016 \u2264 \u2016Ai\u2212 \u03b8s\u2032\u2016}. It is evident that this clustering correctly classifies the majority of the points. It correctly classifies any point i \u2208 Ts with \u2016Ai \u2212 \u00b5r\u2016 \u2212 \u2016Ai \u2212 \u00b5s\u2016 = \u2126( 1c\u221anr )\u2016A\u2212 C\u2016 for every r 6= s, and the analysis of Theorem 3.1 shows that at most O(c\u22122)-fraction of the points do not satisfy this condition. In order to have a direct comparison with the Kumar-Kannan analysis, we now bound the number of misclassified points w.r.t the fraction of points satisfying the Kumar-Kannan proximity condition.\nDefinition 4.6. Denote gapr,s = ( 1\u221a nr + 1\u221ans )\u2016A \u2212 C\u2016. Call a point i \u2208 Ts \u03b3-good, if for every r 6= s we have that the projection of Ai onto the line connecting \u00b5r and \u00b5s, denoted A\u0304i, satisfies that \u2016A\u0304i \u2212 \u00b5r\u2016 \u2212 \u2016A\u0304i \u2212 \u00b5s\u2016 \u2265 \u03b3 gapr,s; otherwise we say the point is \u03b3-bad.\nCorollary 4.7. If the number of \u03b3-bad points is \u01ebn, then (a) the clustering {\u03981, . . . ,\u0398k} misclassifies no more than (\n\u01eb+ O(1) \u03b32c4\n) n points, and (b) \u01eb < O (\n(c\u2212 \u03b3\u221a k )\u22122\n) , assuming \u03b3 < c \u221a k.\nProof. Clearly, all \u01ebn bad points may be misclassified. In addition, for every r and s 6= r, Lemma 4.5 (setting \u03b6r = \u03b8r, \u03b6s = \u03b8s, \u03b1 = 1/c \u221a k and \u03b2 = \u2126(\u03b3/(c \u221a k))) proves that no more than O(\u03b3\u22122c\u22122k\u22121)ns good points can be misclassified. Summing \u2211 s 6=r 1 kns \u2264 n, we conclude (a).\nThe proof of (b) is similar to the proof of Theorem 3.1. We look at the k-means cost of \u2016A\u0302\u2212C\u20162F . We show that all \u03b3-bad points contribute a large amount to this cost.\nTake Ai to be a \u03b3-bad point from Ts. Projecting it down to the line connecting \u00b5r and \u00b5s, we denote the projection as A\u0304i. Clearly, \u2016\u00b5r \u2212 \u00b5s\u2016 = \u2016\u00b5r \u2212 A\u0304i\u2016 + \u2016A\u0304i \u2212 \u00b5s\u2016 \u2265 c \u221a kgapr,s whereas\n\u2016\u00b5r \u2212 A\u0304i\u2016 \u2212 \u2016A\u0304i \u2212 \u00b5s\u2016 \u2264 \u03b3gapr,s. It follows that \u2016A\u0302i \u2212 \u00b5s\u2016 \u2265 \u2016A\u0304i \u2212 \u00b5s\u2016 \u2265 12 (c \u221a k \u2212 \u03b3)gapr,s \u2265\nc \u221a k\u2212\u03b3\n2 \u221a ns \u2016A\u2212 C\u2016. Again, the Markov inequality gives that\n#{bad points from Ts} (c \u221a k \u2212 \u03b3)2 4ns \u2016A\u2212C\u20162 \u2264 \u2016A\u0302\u2212 C\u20162F \u2264 8k\u2016A\u2212 C\u20162\nso from each cluster, only a fraction of 32 ( \u221a\nk c \u221a k\u2212\u03b3\n)2 of the points can be bad.\nObserve that Corollary 4.7 allows for multiple scaled versions of the proximity condition, based on the magnitude of \u03b3. In particular, setting \u03b3 = 1 we get a proximity condition whose bound is independent of k, and still our clustering misclassifies only a small fraction of the points \u2013 at most O(c\u22122) fraction of all points might be misclassified because they are 1-bad, and no more than a O(c\u22124)-fraction of 1-good points may be misclassified. In addition, if there are no 1-bad points we show the following theorem. The proof (omitted) merely follows the Kumar-Kannan proof, plugging in the better bounds, provided by Lemma 4.5.\nTheorem 4.8. Assume all data points are 1-good. That is, for every point Ai that belongs to the target cluster Tc(i) and every s 6= c(i), by projecting Ai onto the line connecting \u00b5c(i) with \u00b5s we have that the projected point A\u0304i satisfies \u2016A\u0304i\u2212\u00b5c(i)\u2016\u2212\u2016A\u0304i\u2212\u00b5s\u2016 = \u2126 ( ( 1\u221anc(i) + 1\u221a ns ) ) \u2016A\u2212C\u2016, whereas \u2016\u00b5c(i) \u2212 \u00b5s\u2016 = \u2126 (\u221a k( 1\u221anc(i) + 1\u221a ns ) )\n\u2016A \u2212 C\u2016. Then the Lloyd method, starting with \u03b81, . . . , \u03b8k, converges to the true centers."}, {"heading": "5 Applications", "text": "Clustering a mixture of Gaussians For a mixture of k Gaussians, we quote the suitable results without proof, as the proof is identical to the proof in [KK10]. We are given a mixture of k Gaussians, F1, . . . , Fk, where the standard deviation of each distribution in any direction is at most \u03c3r, and the weight of each distribution is wr. We denote \u03c3max = maxr{\u03c3r} and wmin = minr{wr}.\nTheorem 5.1. Suppose we are given a set of n \u226b dwmin samples from a mixture of k Gaussians, such that for every r 6= s it holds that \u2016\u00b5r\u2212\u00b5s\u2016 \u2265 c\u03c3max \u221a\nk wmin\npoly log (\nd wmin\n)\n. Then w.h.p. these\npoints satisfy the proximity condition.\nFor Gaussians, the best known separation bound is Achlioptas and McSherry\u2019s bound [AM05]\nof \u2126(\u03c3max(w \u22121/2 min + \u221a\nk log(k \u00b7min{n, 2k}) )). As we assume k is large, this separation condition is \u2126\u0303(\u03c3max(w \u22121/2 min + \u221a k)) = \u2126\u0303(\u03c3max/ \u221a wmin). Therefore, the separation bound of Theorem 5.1 is\u221a\nk times worse than the best known bound. However, applying Kumar and Kannan\u2019s boosting technique (Section 7 in [KK10]), that replaces the polynomial dependency in wmin with a logarithmic one, we get:\nTheorem 5.2. Suppose we are given a set of n \u226b dwmin samples from a mixture of k Gaussians, such that for every r 6= s it holds that\n\u2016\u00b5r \u2212 \u00b5s\u2016 \u2265 c\u03c3max \u221a k poly log ( d\nwmin\n)\nThen there exists an algorithm that w.h.p. correctly classifies all points.\nTherefore, if for any r and r\u2032, both \u03c3r \u2248 \u03c3r\u2032 and wr \u2248 wr\u2032 , then both [AM05] and Theorem 5.2 give roughly the same bound. If for any r and r\u2032 we have that \u03c3r \u2248 \u03c3r\u2032 , yet wmin \u226a 1k , then Theorem 5.2 provides a better bound. If for any r and r\u2032 we have that wr \u2248 wr\u2032 , yet the directional standard deviations of the distributions vary, then the bound of [AM05], in which the distance between any two cluster centers depends only the parameters of these two distributions, is the better bound. If both the standard deviations and the weights vary significantly between the different distributions, then better bound is determined on a case by case basis.\nMcSherry\u2019s Planted Partition Model. In the Planted Partition Model [McS01, AK94, AKS98] our instance is a random n-vertex graph generated by using an implicit partition of the n points into k clusters. There exists an unknown k \u00d7 k matrix of probabilities P , and for every pair of vertices u, v there exists an edge connecting u and v w.p. Prs (assuming u belongs to cluster r and v to cluster s). The goal here is to recover the partition of the points (thus \u2013 recover P ). Viewing this graph as a n\u00d7 n matrix, each row is taken from a special distribution Fr over {0, 1}n \u2013 where each coordinate j is an independent Bernoulli r.v. with mean Pr,C(j), denoting C(j) as the cluster j belongs to. Thus, the mean of this distribution, \u00b5r, is a vector with its j-coordinate set to Pr,C(j). Denote wmin = minr{nrn } and \u03c3max = maxr,s \u221a Prs. The result of [McS01] is that if for every r 6= s\n\u2016\u00b5r \u2212 \u00b5s\u2016 = \u2126 ( \u03c3max \u221a k ( 1\nwmin + log(n/\u03b4)\n))\n(2)\nthen it is possible to retrieve the partition of the vertices w.p. at least 1\u2212 \u03b4.\nKumar and Kannan were not able to match the distance bounds of McSherry, and required centers to be \u221a k factor greater then the bound of (2). Here we match the bound of McSherry exactly. Following the proof in Kumar-Kannan (with few changes), we prove:\nTheorem 5.3. Assuming that \u03c3max \u2265 3 log(n)n and that the planted partition model satisfies equation 2 for every r 6= s, then w.p. at least 1\u2212 \u03b4, every point satisfies the proximity condition.\nProof. We follow the proof of Kumar-Kannan, making the suitable changes. McSherry (Theorem 10 of [McS01]) showed that w.h.p. \u2016A \u2212 C\u2016 \u2264 4\u03c3max \u221a n. So our goal is to show that, w.h.p., all\npoints are \u221a k-good. I.e., denoting u as a unit-length vector connecting \u00b5r and \u00b5s, we show that w.h.p. that for every i \u2208 Tr we have\n|(Ai \u2212 \u00b5r) \u00b7 u| = O( \u221a k\u03c3max ( 1\nwmin + log(n/\u03b4)\n)\n)\nObserve u = \u00b5s\u2212\u00b5r\u2016\u00b5s\u2212\u00b5r\u2016 , and due to the special structure of the means in this model, we have that\n(\u00b5r \u2212 \u00b5s)j = Prt \u2212 Pst where j \u2208 Tt. It follows that\n\u2016\u00b5r \u2212 \u00b5s\u20162 = k \u2211\nt=1\nnt(Prt \u2212 Pst)2\nWe therefore have\n|(Ai \u2212 \u00b5r) \u00b7 u| \u2264 1\n\u2016\u00b5r \u2212 \u00b5s\u2016\n\n\nk \u2211\nt=1\n|Prt \u2212 Pst|\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2211 j\u2208Tt Aij \u2212 Prt \u2223 \u2223 \u2223 \u2223 \u2223 \u2223  \nObserve, Aij are i.i.d 0-1 random variables with mean Prt, so we expect their sum to deviate from its expectation by no more than a few standard deviations. Indeed, Kumar and Kannan prove that w.h.p. it holds that for every t we have\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2211 j\u2208Tt Aij \u2212 Prt \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2264 B\u221ant\u03c3max ( 1 wmin + log(n/\u03b4) )\nwhere B is some sufficiently large constant. This allows us to deduce that\n|(Ai \u2212 \u00b5r) \u00b7 u| \u2264 B\u03c3max ( 1\nwmin + log(n/\u03b4)\n) \u2211k t=1 \u221a nt|Prt \u2212 Pst|\n\u221a\n\u2211k t=1 nt(Prt \u2212 Pst)2\n\u2264 B \u221a k\u03c3max ( 1\nwmin + log(n/\u03b4)\n)\nwhere the last inequality is simply the power-mean inequality."}, {"heading": "6 An Open Problem", "text": "Our work presents an algorithm which successfully clusters a dataset, provided that the distance between any two cluster centers meets a certain lower bound. We would like to point out one\nparticular direction to improve this bound. Note that our center separation bound depends on \u2016A\u2212C\u2016, a property of the entire dataset. It would be nice to handle the case where the separation condition between \u00b5r and \u00b5s depends solely on Tr and Ts. That is, if we define \u2206\u0303r = \u221a k\u221a nr \u2016Ar\u2212Cr\u2016, is it possible to successfully separate clusters s.t \u2016\u00b5r\u2212\u00b5s\u2016 \u2265 c(\u2206\u0303r+\u2206\u0303s)? We comment that most of our analysis (and particularly Lemma 4.5) builds only on the ratio between \u2016\u00b5r\u2212\u03bdr\u2016 and \u2016\u00b5r\u2212\u00b5s\u2016 \u2013 we assume the first is no greater than \u03b1\u2206r and that the latter is no less than c(\u2206r+\u2206s). In fact, one can revise the proofs of Theorems 3.1 and 3.2 so that they will hold based on this assumption alone (without using the properties of the SVD). The problem therefore boils down to finding initial centers {\u03bdr} that are sufficiently close to the true centers {\u00b5r}, under the assumption that \u2200r 6= s, \u2016\u00b5r \u2212 \u00b5s\u2016 \u2265 c(\u2206\u0303r + \u2206\u0303s). But this is an intricate task, mainly because such separation condition does not imply that {\u00b51, \u00b52, . . . , \u00b5k} are the centers minimizing the k-means cost! (Nor do {\u00b5\u03021, \u00b5\u03022, . . . , \u00b5\u0302k, } minimize the k-means cost of A\u0302.) Consider the case, for example, where cluster r has very few points (say nr = \u221a n) and very small variance, and cluster s is very big (say ns = n/5), and is essentially composed of two sub-components with distance 1\n2 \u221a ns \u2016As \u2212 Cs\u2016\nbetween the centers of the two sub-components. The k-means cost of placing two centers within Cs is smaller than placing one center at \u00b5s and one center at \u00b5r. This relates to the question of designing a t-approximation algorithm for k-means, guaranteeing that each cluster\u2019s cost cannot increase by more than a factor of t."}, {"heading": "A Some Basic Lemmas", "text": "Fact A.1 (Lemma 9 from [McS01]). \u2016A\u0302\u2212C\u20162F \u2264 8min{k\u2016A\u2212C\u20162, \u2016A\u2212C\u20162F } ( = 8nr\u2206 2 r for every r ) .\nProof.\n\u2016A\u0302\u2212 C\u20162F \u2264 2k\u2016A\u0302 \u2212 C\u20162 \u2264 2k ( \u2016A\u0302\u2212A\u2016+ \u2016A\u2212C\u2016 )2 \u2264 2k (2\u2016A \u2212 C\u2016)2\nwhere the first inequality holds because rank(A\u0302\u2212C) \u2264 2k, and the last inequality follows from the fact that A\u0302 = argminN :rank(N)=k{\u2016A\u2212N\u2016}. For the same reason, \u2016A\u0302\u2212 C\u2016F \u2264 \u2016A\u2212 A\u0302\u2016F + \u2016A\u2212 C\u2016F \u2264 2\u2016A\u2212 C\u2016F .\nFact A.2 (Claim 1 in Section 3.2 of [KV09]). For every \u00b5r there exists a center \u03bds s.t. \u2016\u00b5r\u2212\u03bds\u2016 \u2264 6\u2206r, so we can match each \u00b5r to a unique \u03bdr.\nProof. Observe that by taking A\u0302 \u2212 C\u0302, we project A \u2212 C to a k-dimensional subspace, so we have that \u2016A\u0302\u2212 C\u0302\u20162F \u2264 k\u2016A\u0302\u2212 C\u0302\u20162 \u2264 k\u2016A\u2212 C\u20162. Similarly, \u2016A\u0302\u2212 C\u0302\u20162F \u2264 \u2016A\u2212C\u20162F .\nAssume for the sake of contradiction that \u2203r s.t. \u2016\u00b5r \u2212 \u03bds\u2016 > 6\u2206r for all s. Since \u2016A\u0302\u2212 C\u0302\u20162F \u2264 nr\u2206 2 r, then our 10-approximation algorithm yields a clustering of cost \u2264 10nr\u22062r. In contrast, as\neach A\u0302i is assigned to some \u03bdc(i), the contribution of only the points in Tr to the k-means cost of the clustering is more than\n\u2211\ni\u2208Tr\n\u2225 \u2225 \u2225(\u00b5r \u2212 \u03bdc(i))\u2212 (A\u0302i \u2212 \u00b5r) \u2225 \u2225 \u2225 2 > nr 2 (6\u2206r) 2 \u2212 \u2211\ni\u2208Tr \u2016A\u0302i \u2212 \u00b5r\u20162 \u2265 18nr\u22062r \u2212 \u2016A\u0302\u2212 C\u20162F \u2265 10nr\u22062r\nwhere the first inequality follows from the fact that (a\u2212 b)2 \u2265 12a2 \u2212 b2.\nNow, in order to prove Fact 1.3 (also cited below as Fact A.4), we need the following Fact.\nFact A.3 (Lemma 5.2 and Corollary 5.3 from [KK10]). Fix any cluster Tr and a subset X \u2282 Tr. Then\n|X| \u2016\u00b5(X) \u2212 \u00b5r\u2016 = (|Tr| \u2212 |X|) \u2016\u00b5(Tr \\X) \u2212 \u00b5r\u2016 \u2264 \u221a |X| \u2016Ar \u2212 Cr\u2016\nProof. Let uX be the indicator vector of X. Then\n\u2016 |X| (\u00b5(X) \u2212 \u00b5r) \u2016 = \u2016(Ar \u2212 Cr)T uX\u2016 \u2264 \u2016(Ar \u2212 Cr)T \u2016 \u2016uX\u2016 = \u2016Ar \u2212 Cr\u2016 \u221a |X|\nand the fact that |X| \u2016\u00b5(X)\u2212 \u00b5r\u2016 = |Tr \\X| \u2016\u00b5(Tr \\X)\u2212 \u00b5r\u2016 is simply because \u00b5r = |X||Tr |\u00b5(X) + |Tr\\X| |Tr | \u00b5(Tr \\X).\nFact A.4. Fix a target cluster Tr and let Sr be a set of points created by removing \u03c1outnr points from Tr and adding \u03c1in(s)nr points from each cluster s 6= r, s.t. every added point x satisfies \u2016x\u2212 \u00b5s\u2016 \u2265 23\u2016x\u2212 \u00b5r\u2016. Assume \u03c1out < 14 and \u03c1in def = \u2211 s 6=r \u03c1in(s) < 1 4 . Then\n\u2016\u00b5(Sr)\u2212 \u00b5r\u2016 \u2264 1\u221a nr\n\n \u221a \u03c1out + 3 2 \u2211\ns 6=r\n\u221a\n\u03c1in(s)\n  \u2016A\u2212 C\u2016 \u2264 (\u221a\n\u03c1out nr + 32\n\u221a k \u221a\n\u03c1in nr\n)\n\u2016A\u2212 C\u2016\nProof. We break \u2016\u00b5(Sr)\u2212 \u00b5r\u2016 into its components and deduce\n\u2016\u00b5(Sr)\u2212 \u00b5r\u2016 \u2264 (1\u2212 \u03c1out)nr\nnr \u2016\u00b5(Sr \u2229 Tr)\u2212 \u00b5r\u2016+\n\u2211\ns 6=r\n\u03c1in(s)nr nr \u2016\u00b5(Sr \u2229 Ts)\u2212 \u00b5r\u2016\n\u2264 (1\u2212 \u03c1out)nr nr \u2016\u00b5(Sr \u2229 Tr)\u2212 \u00b5r\u2016+ 32 \u2211\ns 6=r\n\u03c1in(s)nr nr \u2016\u00b5(Sr \u2229 Ts)\u2212 \u00b5s\u2016\nPlugging in Fact A.3 we have \u2016\u00b5(Sr)\u2212\u00b5r\u2016 \u2264 1nr (\u221a \u03c1outnr + 3 2 \u2211 s 6=r \u221a \u03c1in(s)nr )\n\u2016A\u2212C\u2016. The last inequality comes from maximizing the sum of square-roots by taking each \u03c1in(s) = \u03c1in/k."}], "references": [{"title": "Stability yields a PTAS for k-median and k-means clustering", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "In FOCS,", "citeRegEx": "Awasthi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2010}, {"title": "A spectral technique for coloring random 3-colorable graphs", "author": ["Noga Alon", "Nabil Kahale"], "venue": "In SIAM Journal on Computing,", "citeRegEx": "Alon and Kahale.,? \\Q1994\\E", "shortCiteRegEx": "Alon and Kahale.", "year": 1994}, {"title": "Finding a large hidden clique in a random graph", "author": ["Noga Alon", "Michael Krivelevich", "Benny Sudakov"], "venue": null, "citeRegEx": "Alon et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Alon et al\\.", "year": 1998}, {"title": "On spectral learning of mixtures of distributions", "author": ["Dimitris Achlioptas", "Frank McSherry"], "venue": "In COLT,", "citeRegEx": "Achlioptas and McSherry.,? \\Q2005\\E", "shortCiteRegEx": "Achlioptas and McSherry.", "year": 2005}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In SODA,", "citeRegEx": "Arthur and Vassilvitskii.,? \\Q2007\\E", "shortCiteRegEx": "Arthur and Vassilvitskii.", "year": 2007}, {"title": "Approximate clustering without the approximation", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "In SODA,", "citeRegEx": "Balcan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2009}, {"title": "Approximate clustering via coresets", "author": ["Mihai B\u0101doiu", "Sariel Har-Peled", "Piotr Indyk"], "venue": "In STOC,", "citeRegEx": "B\u0101doiu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "B\u0101doiu et al\\.", "year": 2002}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "Computing Research Repository,", "citeRegEx": "Belkin and Sinha.,? \\Q2010\\E", "shortCiteRegEx": "Belkin and Sinha.", "year": 2010}, {"title": "Isotropic pca and affine-invariant clustering", "author": ["S. Charles Brubaker", "Santosh Vempala"], "venue": "In FOCS,", "citeRegEx": "Brubaker and Vempala.,? \\Q2008\\E", "shortCiteRegEx": "Brubaker and Vempala.", "year": 2008}, {"title": "Graph partitioning via adaptive spectral techniques", "author": ["Amin Coja-Oghlan"], "venue": "Comb. Probab. Comput.,", "citeRegEx": "Coja.Oghlan.,? \\Q2010\\E", "shortCiteRegEx": "Coja.Oghlan.", "year": 2010}, {"title": "Learning to match and cluster large highdimensional data sets for data integration", "author": ["William W. Cohen", "Jacob Richman"], "venue": "In KDD,", "citeRegEx": "Cohen and Richman.,? \\Q2002\\E", "shortCiteRegEx": "Cohen and Richman.", "year": 2002}, {"title": "Beyond gaussians: Spectral methods for learning mixtures of heavy-tailed distributions", "author": ["Kamalika Chaudhuri", "Satish Rao"], "venue": "In COLT,", "citeRegEx": "Chaudhuri and Rao.,? \\Q2008\\E", "shortCiteRegEx": "Chaudhuri and Rao.", "year": 2008}, {"title": "Learning mixtures of product distributions using correlations and independence", "author": ["Kamalika Chaudhuri", "Satish Rao"], "venue": "In COLT,", "citeRegEx": "Chaudhuri and Rao.,? \\Q2008\\E", "shortCiteRegEx": "Chaudhuri and Rao.", "year": 2008}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In FOCS,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "Spectral clustering with limited independence", "author": ["Anirban Dasgupta", "John Hopcroft", "Ravi Kannan", "Pradipta Mitra"], "venue": "In SODA,", "citeRegEx": "Dasgupta et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2007}, {"title": "Approximation schemes for clustering problems", "author": ["W. Fernandez de la Vega", "Marek Karpinski", "Claire Kenyon", "Yuval Rabani"], "venue": "In STOC,", "citeRegEx": "Vega et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Vega et al\\.", "year": 2003}, {"title": "A probabilistic analysis of em for mixtures of separated, spherical gaussians", "author": ["Sanjoy Dasgupta", "Leonard Schulman"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Dasgupta and Schulman.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta and Schulman.", "year": 2007}, {"title": "Deterministic clustering with data", "author": ["Michelle Effros", "Leonard J. Schulman"], "venue": "nets. ECCC,", "citeRegEx": "Effros and Schulman.,? \\Q2004\\E", "shortCiteRegEx": "Effros and Schulman.", "year": 2004}, {"title": "Simpler analyses of local search algorithms for facility location", "author": ["Anupam Gupta", "Kanat Tangwongsan"], "venue": "CoRR, abs/0809.2554,", "citeRegEx": "Gupta and Tangwongsan.,? \\Q2008\\E", "shortCiteRegEx": "Gupta and Tangwongsan.", "year": 2008}, {"title": "Matrix computations (3rd ed.)", "author": ["Gene H. Golub", "Charles F. Van Loan"], "venue": null, "citeRegEx": "Golub and Loan.,? \\Q1996\\E", "shortCiteRegEx": "Golub and Loan.", "year": 1996}, {"title": "On coresets for k-means and k-median clustering", "author": ["Sariel Har-Peled", "Soham Mazumdar"], "venue": "In STOC,", "citeRegEx": "Har.Peled and Mazumdar.,? \\Q2004\\E", "shortCiteRegEx": "Har.Peled and Mazumdar.", "year": 2004}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["A. Kumar", "R. Kannan"], "venue": "In FOCS,", "citeRegEx": "Kumar and Kannan.,? \\Q2010\\E", "shortCiteRegEx": "Kumar and Kannan.", "year": 2010}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["Tapas Kanungo", "David M. Mount", "Nathan S. Netanyahu", "Christine D. Piatko", "Ruth Silverman", "Angela Y. Wu"], "venue": "In Proc. 18th Symp. Comp. Geom.,", "citeRegEx": "Kanungo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kanungo et al\\.", "year": 2002}, {"title": "Efficiently learning mixtures of two gaussians", "author": ["Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant"], "venue": "In STOC\u201910,", "citeRegEx": "Kalai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2010}, {"title": "A simple linear time (1 + \u01eb)approximation algorithm for k-means clustering in any dimensions", "author": ["Amit Kumar", "Yogish Sabharwal", "Sandeep Sen"], "venue": "In FOCS,", "citeRegEx": "Kumar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2004}, {"title": "The spectral method for general mixture models", "author": ["Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala"], "venue": "SIAM J. Comput.,", "citeRegEx": "Kannan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2008}, {"title": "Spectral algorithms. Found", "author": ["Ravindran Kannan", "Santosh Vempala"], "venue": "Trends Theor. Comput. Sci.,", "citeRegEx": "Kannan and Vempala.,? \\Q2009\\E", "shortCiteRegEx": "Kannan and Vempala.", "year": 2009}, {"title": "Least squares quantization in pcm", "author": ["Stuart P. Lloyd"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Lloyd.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd.", "year": 1982}, {"title": "Scop: a structural classification of proteins database for the investigation of sequences and structures", "author": ["A GMurzin", "S E Brenner", "T Hubbard", "C Chothia"], "venue": "Journal of Molecular Biology,", "citeRegEx": "GMurzin et al\\.,? \\Q1995\\E", "shortCiteRegEx": "GMurzin et al\\.", "year": 1995}, {"title": "Spectral partitioning of random graphs", "author": ["F. McSherry"], "venue": "In FOCS,", "citeRegEx": "McSherry.,? \\Q2001\\E", "shortCiteRegEx": "McSherry.", "year": 2001}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In FOCS\u201910,", "citeRegEx": "Moitra and Valiant.,? \\Q2010\\E", "shortCiteRegEx": "Moitra and Valiant.", "year": 2010}, {"title": "Polynomial time approximation schemes for geometric k-clustering", "author": ["R. Ostrovsky", "Y. Rabani"], "venue": "In FOCS,", "citeRegEx": "Ostrovsky and Rabani.,? \\Q2000\\E", "shortCiteRegEx": "Ostrovsky and Rabani.", "year": 2000}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "In FOCS,", "citeRegEx": "Ostrovsky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ostrovsky et al\\.", "year": 2006}, {"title": "Clustering for edge-cost minimization (extended abstract)", "author": ["Leonard J. Schulman"], "venue": "In STOC, pages 547\u2013555,", "citeRegEx": "Schulman.,? \\Q2000\\E", "shortCiteRegEx": "Schulman.", "year": 2000}, {"title": "Learning mixtures of arbitrary gaussians", "author": ["Arora Sanjeev", "Ravi Kannan"], "venue": "In STOC,", "citeRegEx": "Sanjeev and Kannan.,? \\Q2001\\E", "shortCiteRegEx": "Sanjeev and Kannan.", "year": 2001}], "referenceMentions": [], "year": 2017, "abstractText": "Aiming to unify known results about clustering mixtures of distributions under separation conditions, Kumar and Kannan [KK10] introduced a deterministic condition for clustering datasets. They showed that this single deterministic condition encompasses many previously studied clustering assumptions. More specifically, their proximity condition requires that in the target k-clustering, the projection of a point x onto the line joining its cluster center \u03bc and some other center \u03bc, is a large additive factor closer to \u03bc than to \u03bc. This additive factor can be roughly described as k times the spectral norm of the matrix representing the differences between the given (known) dataset and the means of the (unknown) target clustering. Clearly, the proximity condition implies center separation \u2013 the distance between any two centers must be as large as the above mentioned bound. In this paper we improve upon the work of Kumar and Kannan [KK10] along several axes. First, we weaken the center separation bound by a factor of \u221a k, and secondly we weaken the proximity condition by a factor of k (in other words, the revised separation condition is independent of k). Using these weaker bounds we still achieve the same guarantees when all points satisfy the proximity condition. Under the same weaker bounds, we achieve even better guarantees when only (1\u2212\u01eb)-fraction of the points satisfy the condition. Specifically, we correctly cluster all but a (\u01eb + O(1/c))-fraction of the points, compared to O(k\u01eb)-fraction of [KK10], which is meaningful even in the particular setting when \u01eb is a constant and k = \u03c9(1). Most importantly, we greatly simplify the analysis of Kumar and Kannan. In fact, in the bulk of our analysis we ignore the proximity condition and use only center separation, along with the simple triangle and Markov inequalities. Yet these basic tools suffice to produce a clustering which (i) is correct on all but a constant fraction of the points, (ii) has k-means cost comparable to the k-means cost of the target clustering, and (iii) has centers very close to the target centers. Our improved separation condition allows us to match the results of the Planted Partition Model of McSherry [McS01], improve upon the results of Ostrovsky et al [ORSS06], and improve separation results for mixture of Gaussian models in a particular setting.", "creator": "LaTeX with hyperref package"}}}