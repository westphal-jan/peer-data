{"id": "1206.3266", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Partitioned Linear Programming Approximations for MDPs", "abstract": "Approximate linear interface (ALP) a an designed action directly interaction large substantially Markov step phase (MDPs ). The capital idea fact into phase latter to geographical in useful value formula_11 and goes step of issues linear having smes their vertical late linear programming (LP ). This laid reduction that new ALP approximation. Comparing made the quality ALP formulation, think polymerized of integral module they this made according cut - two-dimensional dimensions. This similar require take solving still includes LP efficiently. In actual, early constraints of own LP can same satisfied in good integrated form without full exponential accelerating on all treewidth known ALP changing. We study latter rigorous and theoretical emphasizing result along proposed good. Moreover, simply opportunity its scale - up moreover saturday means MDP both more rather second ^ 100 states.", "histories": [["v1", "Wed, 13 Jun 2012 15:36:14 GMT  (426kb)", "http://arxiv.org/abs/1206.3266v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["branislav kveton", "milos hauskrecht"], "accepted": false, "id": "1206.3266"}, "pdf": {"name": "1206.3266.pdf", "metadata": {"source": "META", "title": "Partitioned Linear Programming Approximations for MDPs", "authors": ["Branislav Kveton", "Milos Hauskrecht"], "emails": ["branislav.kveton@intel.com", "milos@cs.pitt.edu"], "sections": [{"heading": null, "text": "Approximate linear programming (ALP) is an efficient approach to solving large factored Markov decision processes (MDPs). The main idea of the method is to approximate the optimal value function by a set of basis functions and optimize their weights by linear programming (LP). This paper proposes a new ALP approximation. Comparing to the standard ALP formulation, we decompose the constraint space into a set of low-dimensional spaces. This structure allows for solving the new LP efficiently. In particular, the constraints of the LP can be satisfied in a compact form without an exponential dependence on the treewidth of ALP constraints. We study both practical and theoretical aspects of the proposed approach. Moreover, we demonstrate its scale-up potential on an MDP with more than 2100 states."}, {"heading": "1 Introduction", "text": "Markov decision processes (MDPs) [19] are an established framework for solving sequential decision problems under uncertainty. Unfortunately, traditional methods for solving MDPs, such as value and policy iteration, are unsuitable for solving real-world problems. These problems are generally structured, and their state and action spaces are represented by state and action variables. The size of these problems is naturally exponential in the number of the variables, and so are their exact solutions. Approximate linear programming (ALP) [21] has emerged as a promising approach to solving these problems efficiently [6, 12, 15].\nThe main idea of this method is to approximate the optimal value function by a set of basis functions and optimize their weights by linear programming (LP). The optimization can be performed in a structured manner [10, 20]. The structure is a result of combining the structure of factored MDPs and linear value function approximations.\nThe complexity of computing exact ALP solutions [10, 20] is exponential in the treewidth of the dependency graph that\nrepresents the constraint space in ALP. Therefore, when the treewidth of an ALP is large, its exact solution is infeasible. This type of problems can be still solved approximately using Monte Carlo constraint sampling [7, 14]. This approach can be interpreted as an outer approximation to the feasible region of the ALP.\nIn this work, we propose inner approximations to the feasible region. In comparison to the standard ALP formulation, the constraint space is factored into a set of subspaces. This structure allows for solving the new LP more efficiently. In particular, its constraints can be satisfied in a compact form without an exponential dependence on the treewidth of the original constraint space. We investigate both practical and theoretical aspects of the approach. In addition, we demonstrate that the approach yields an exponential speedup over ALP.\nThe paper is organized as follows. First, we review factored MDPs [5] and linear value function approximations [2, 22]. Second, we discuss in detail existing work on approximate linear programming. Third, we propose a novel partitioned ALP formulation and study its properties. Finally, we evaluate the quality of the approximation on decision problems with more than 2100 states."}, {"heading": "2 Factored MDPs", "text": "Many real-world decision problems are naturally described in a factored form. Factored MDPs [5] allow for a compact representation of this structure.\nA factored MDP [5] is a 4-tuple M = (X,A, P,R), where X = {X1, . . . ,Xn} is a state space represented by a set of state variables, A = {a1, . . . , am} is a finite set of actions1, P (X\u2032 | X,A) is a transition function, which represents the dynamics of the MDP, andR is a reward function assigning immediate payoffs to state-action configurations. The state of the system is completely observed and given by a vector of value assignments x = (x1, . . . , xn).\n1For simplicity of exposition, we consider an MDP model with a single action variableA. Our ideas straightforwardly generalize to MDPs with factored action spaces [11].\nTransition model: The transition model is represented by a conditional probability distribution P (X\u2032 | X,A), where X andX\u2032 denote the state variables at two successive time steps. Since the full tabular representation of P (X\u2032 | X,A) is infeasible when the number of state variables is large, we assume that the distribution factors alongX\u2032 as:\nP (X\u2032 | X, a) =\nn\u220f\ni=1\nP (X \u2032i | Par(X \u2032 i), a) (1)\nand is described compactly by a dynamic Bayesian network (DBN) [8]. The network reflects independencies among the variables X and X\u2032 given an action a. One-step dynamics of every state variable is given by its conditional probability distribution P (X \u2032i | Par(X \u2032 i), a), where Par(X \u2032 i)\u2286X is the parent set of X \u2032i . The parent set is usually a small subset of state variables which simplifies the parameterization of the model.\nReward model: The reward model is factored similarly to the transition model. Specifically, the reward function:\nR(x, a) = \u2211\nj\nRj(xj , a) (2)\nis an additive function of local reward functions defined on the subsetsXj and A. These local functions are compactly represented by reward nodes Rj , which are conditioned on their parent sets Par(Rj) = Xj \u222a A.\nOptimal value function and policy: The quality of a policy \u03c0 is measured by the infinite horizon discounted reward E[ \u2211\u221e t=0 \u03b3\ntrt], where \u03b3 \u2208 [0, 1) is a discount factor and rt is the immediate reward at the time step t. In such a setting, there always exists an optimal policy \u03c0\u2217 which is stationary and deterministic [19]. The policy is greedy with respect to the optimal value function V \u2217, which is a fixed point of the Bellman equation [1]:\nV \u2217(x) = max a\n[ R(x, a) + \u03b3EP (x\u2032|x,a)[V \u2217(x\u2032)] ] . (3)\nSimilarly to the above equation, all expectation terms in the rest of the paper are written compactly as EP (x)[f(x)]."}, {"heading": "3 Solving factored MDPs", "text": "Markov decision processes can be solved by exact dynamic programming (DP) methods in polynomial time in the size of their state space [19]. Unfortunately, the space spaceX of factored MDPs is exponential in the number of state variables. Therefore, the DP methods are unsuitable for solving these problems. Since a factored representation of an MDP does not guarantee a structure in its solution [13], we resort to value function approximations.\nIn this work, we focus on the linear value function approximation [2, 22]:\nV w(x) = \u2211\ni\nwifi(x). (4)\nThe approximation restricts the form of the value function to the linear combination of basis functions fi(x), where w is a vector of optimized weights. The basis functions fi(x) are arbitrary functions, which are usually restricted to small subsets of state variablesXi [2, 13]. The functions play the same role as features in machine learning. They are usually provided by domain experts but can also be discovered automatically [18, 16]."}, {"heading": "4 Approximate linear programming", "text": "Various techniques for optimizing the linear value function approximation have been studied and analyzed [3]. We focus on approximate linear programming (ALP) [21], which restates this problem as a linear program:\nminimizew \u2211\ni\nwi\u03b1i (5)\nsubject to: \u2211\ni\nwiFi(x, a)\u2212R(x, a) \u2265 0\n\u2200 x \u2208 X, a \u2208 A;\nwherew denotes the variables in the LP, \u03b1i is a basis function relevance weight:\n\u03b1i = E\u03c8(x)[fi(x)] , (6)\n\u03c8(x) \u2265 0 is a state relevance density function that weights the quality of the approximation, and:\nFi(x, a) = fi(x)\u2212 \u03b3EP (x\u2032|x,a)[fi(x \u2032)] (7)\ndenotes the difference between the basis function fi(x) and its discounted backprojection. This linear program is feasible if the set of basis functions includes a constant function f0(x) \u2261 1. We assume that such a basis function is present.\nSince our basis functions fi(x) are often restricted to small subsets of state variables, expectation terms in the ALP formulation (5) can be computed efficiently [10]. For instance, the backprojection terms can be rewritten as:\nEP (x\u2032|x,a)[fi(x \u2032)] = EP (x\u2032\ni |x,a)[fi(x \u2032 i)] , (8)\nwhereX\u2032i is a lower dimensional state space corresponding to the basis function fi(x), and P (x\u2032i |x, a) is a distribution defined on this subspace. Similarly, state relevance weights \u03b1i can be computed efficiently if the state relevance density \u03c8(x) is structured."}, {"heading": "4.1 Solving ALP formulations", "text": "The major problem in solving ALP formulations efficiently is in satisfying their constraints. This problem is hard since the number of the constraints is exponential in the number of state variables. Fortunately, the constraints exhibit some structure. The structure is a result of combining linear value function approximations (Equation 4) with factored reward and transition models (Equations 1 and 2). Therefore, ALP\nconstraints can be satisfied in a structured form and without being enumerated exhaustively.\nBased on these observations, Guestrin et al. [10] proposed a variable elimination method [9] that rewrites the constraint space compactly. Schuurmans and Patrascu [20] solved the constraint satisfaction problem by the cutting plane method [4]. The approach iteratively searches for the most violated constraint:\nargmin x,a\n[ \u2211\ni\nw (t) i Fi(x, a)\u2212R(x, a)\n] (9)\nwith respect to the solutionw(t) of a relaxed ALP. The most violated constraint is added to the linear program, which is in turn resolved for a new vectorw(t+1). This procedure is iterated until no violated constraint is found. In such a case, the vector w(t) is an optimal solution to the ALP.\nThe space complexity of both constraint satisfaction methods [10, 20] is exponential in the treewidth of the constraint space. As a result, the methods are unsuitable for problems with a large treewidth. However, such problems can be still solved approximately. For instance, de Farias and Van Roy [7] proposed Monte Carlo approximations of the constraint space. Kveton and Hauskrecht [14] showed how to search for the most violated constraint (Equation 9) using Markov chain Monte Carlo (MCMC) sampling."}, {"heading": "4.2 Theoretical analysis", "text": "The quality of the ALP formulation has been studied by de Farias and Van Roy [6]. Based on their work, we conclude that ALP minimizes the L1-norm error \u2016V \u2217\u2212V w\u20161,\u03c8 . The following theorem draws a parallel between optimizing this objective and the max-norm error \u2016V \u2217 \u2212 V w\u2016\u221e.\nTheorem 1 (de Farias and Van Roy [6]). Let w\u0303 be a solution to the ALP formulation (5). Then the expected error of the value function V w\u0303 can be bounded as:\n\u2225\u2225\u2225V \u2217 \u2212 V w\u0303 \u2225\u2225\u2225 1,\u03c8 \u2264 2 1\u2212 \u03b3 min w \u2016V \u2217 \u2212 V w\u2016\u221e ,\nwhere \u2016\u00b7\u20161,\u03c8 is an L1-norm weighted by the state relevance density function \u03c8 and \u2016\u00b7\u2016\u221e is the max-norm.\nDe Farias and Van Roy [6] also proved a tighter version of Theorem 1, which reweights the error \u2016V \u2217 \u2212 V w\u2016\u221e."}, {"heading": "5 Partitioned ALP", "text": "In this section, we propose a novel approximate linear programming formulation. In comparison to the standard ALP (5), the proposed formulation has an additional structure in its constraint space. The structure allows for controlling the complexity of solving the new LP.\nThe LP solves a more restrictive problem than the standard ALP. As a result, the formulation can be viewed as an inner\napproximation to the feasible region of the ALP (Figure 1). This differentiates our work from existing ALP approximations [7, 14]. These approximations are based on constraint sampling. As a result, they approximate the feasible region of the ALP from outside."}, {"heading": "5.1 An illustrative example", "text": "First, let us consider an optimization problem:\nminimizew,h w1\u03b11 + w2\u03b12 + h (10)\nsubject to: w1F1(x1) + w2F2(x2) + h \u2265 0\n\u2200 x1 \u2208 X1, x2 \u2208 X2;\nwherew = (w1, w2) denotes the main optimized variables, and h is an auxiliary variable that guarantees the feasibility of the LP. This problem involves |X1 \u00d7X2| = |X1|\u00d7 |X2| constraints. If the number of the constraints is large, a suboptimal but feasible solution to the problem can be obtained by solving a new linear program:\nminimizew,h w1\u03b11 + w2\u03b12 + h (11)\nsubject to: h1 + h2 = h\nw1F1(x1) + h1 \u2265 0 \u2200 x1 \u2208 X1\nw2F2(x2) + h2 \u2265 0 \u2200 x2 \u2208 X2;\nwhere h1 and h2 are new auxiliary variables that guarantee the feasibility of the LP. Note that the new LP decomposes the original constraint w1F1(x1)+w2F2(x2)+h \u2265 0 into two smaller constraint spaces with |X1|+ |X2| constraints. Therefore, it is typically faster to solve the new LP than our original problem (10). In the next section, we show how to apply similar ideas in the context of ALP."}, {"heading": "5.2 Partitioned ALP formulation", "text": "Similarly to Section 5.1, we may decompose the constraint space in the ALP formulation (5). Formally, the partitioned\nALP (PALP) formulation with K constraint spaces is given by a linear program:\nminimizew \u2211\ni\nwi\u03b1i (12)\nsubject to: DMw(x, a) T \u2265 0 \u2200 x \u2208 X, a \u2208 A;\nwhere:\nMw(x, a) = (w1F1(x, a), . . . ,\u2212R1(x1, a), . . . ) (13)\nis a vector whose i-th element corresponds to the i-th term in the ALP constraint, and the partitioning matrix:\nD =   d1,1 d1,2 d1,3 \u00b7 \u00b7 \u00b7 d2,1 d2,2 d2,3 \u00b7 \u00b7 \u00b7 d3,1 d3,2 d3,3 \u00b7 \u00b7 \u00b7\n... ...\n... . . .\n  (14)\ndetermines how the ALP constraint decomposes into theK new constraint spaces. Specifically, the term dk,i measures the contribution of the i-th term in the ALP constraint to the k-th constraint space. Due to this interpretation, we assume that all terms dk,i are non-negative and that the partitioning matrixD is normalized such that the equality \u2211 k dk,i = 1 holds for all i. Under such assumptions, it is trivial to show that the satisfaction of the K constraintsDMw(x, a)T \u2265 0 leads to the satisfaction of a corresponding ALP constraint. The claim can be proved based on the identity:\n1DMw(x, a) T =\n\u2211\ni\nwiFi(x, a)\u2212R(x, a), (15)\nwhere 1 is a row vector of ones. It follows that every PALP solution is feasible in a corresponding ALP.\nSimilarly to ALP, the feasibility of the PALP formulation is guaranteed if the set of basis functions includes a constant function f0(x) \u2261 1. We assume that the function is present in allK constraint spaces. In each of them, we define a new weight wk0 , which reflects the contribution of this function. As a result of these changes, the PALP formulation slightly changes its form: minimizew \u2211\ni\nwi\u03b1i + w0 (16)\nsubject to: \u2211\nk\nwk0 = w0\nDMw(x, a) T + (1\u2212 \u03b3)(w10, . . . , w K 0 ) T \u2265 0\n\u2200 x \u2208 X, a \u2208 A.\nIn the rest of the paper, we use the above and original PALP formulations interchangeably."}, {"heading": "5.3 Partitioning matrix", "text": "The partitioning matrixD allows for trading off the quality and complexity of PALP solutions. To achieve high-quality and tractable approximations, the rows of the matrix should\nreflect tree decompositions of the cost network corresponding to ALP constraints (Figure 2). The width of the decompositions should be small since the complexity of satisfying a single constraint space is exponential in its treewidth [10].\nHow to generate the best PALP approximation within a certain complexity limit is an open question. In the experimental section, we build the matrixD based on a heuristic. The heuristic generates a constraint space for every expectation term Fk(x, a) in Equation 9. This constraint space consists of the termwkFk(x, a) and its cost network neighbors. The constraint space is not included in the matrixD if its terms constitute a subset of another constraint space.\nThis decomposition of our initial problem can be viewed as optimizing K smaller MDPs, which have overlapping state and action spaces, and share value functions. To clarify the construction of the matrixD, we demonstrate it on the cost network in Figure 2. The cost network involves 7 functions, out of which 5 have the form ofwkFk(x, a). Therefore, the corresponding matrixD has 5 rows and 7 columns:\nD =   0.33\u0304 0.33\u0304 0 0.25 0 0 0 0.33\u0304 0.33\u0304 0.25 0 0 0.5 0 0 0.33\u0304 0.25 0.25 0.33\u0304 0.5 0\n0.33\u0304 0 0.25 0.25 0.33\u0304 0 0 0 0 0.25 0.25 0.33\u0304 0 1\n  . (17)\nNon-zero entries dk,i in the matrix indicate that the i-th cost network term is present in the k-th constraint space."}, {"heading": "5.4 Solving PALP formulations", "text": "The PALP formulation (12) is similar to the ALP formulation (5). As a result, it can be solved in a similar fashion. In the experimental section, we implemented the cutting plane method for solving linear programs (Figure 3). In principle, any method for solving ALPs (Section 4.1) can be adapted to PALPs."}, {"heading": "5.5 Theoretical analysis", "text": "In this section, we discuss the quality of the PALP formulation (12). First, we prove that its solution is an upper bound on the optimal value function V \u2217. Proposition 1. Let w\u0303 be a solution to the PALP formulation (12). Then V w\u0303 \u2265 V \u2217.\nProof: Since w\u0303 is a solution to the PALP formulation (12), it is also a suboptimal solution to the ALP formulation (5). Therefore, the constraint V w\u0303\u2265T \u2217V w\u0303 is satisfied. Furthermore, note that the Bellman operator T \u2217 is both monotonic and contracting. Hence, the inequality V w\u0303\u2265T \u2217V w\u0303 yields the following sequence of inequalities:\nV w\u0303 \u2265 T \u2217V w\u0303 \u2265 T \u2217T \u2217V w\u0303 \u2265 \u00b7 \u00b7 \u00b7 \u2265 V \u2217.\nThis step concludes our proof.\nThe above result allows us to restate the objective E\u03c8[V w] in PALP. Proposition 2. The objective in the PALP formulation (12) can be rewritten as \u2016V \u2217 \u2212 V w\u20161,\u03c8 , where \u2016\u00b7\u20161,\u03c8 is an L1norm weighted by the state relevance density function \u03c8.\nProof: Follows from the fact that all solutions to the PALP formulation (12) satisfy the constraint V w\u2265V \u2217.\nBased on Proposition 2, we conclude that PALP optimizes the linear value function approximation with respect to the reweighted L1-norm error \u2016V \u2217 \u2212 V w\u20161,\u03c8 . The following theorem draws a parallel between optimizing this objective and the max-norm error \u2016V \u2217 \u2212 V w\u2016\u221e. Theorem 2. Let w\u0303 be a solution to the PALP formulation (12). Then the expected error of the value function V w\u0303 can be bounded as: \u2225\u2225\u2225V \u2217 \u2212 V w\u0303\n\u2225\u2225\u2225 1,\u03c8 \u2264 2 1\u2212 \u03b3 min w \u2016V \u2217 \u2212 V w\u2016\u221e + K\u03b4 1\u2212 \u03b3 ,\nwhere \u2016\u00b7\u20161,\u03c8 is an L1-norm weighted by the state relevance density function \u03c8, \u2016\u00b7\u2016\u221e is the max-norm, \u03b4 is a scalar that reflects how hard is to make an ALP solution feasible in our PALP formulation, andK denotes the number of constraint spaces in the PALP.\nProof: Our proof is similar to the proof of Theorem 2 by de Farias and Van Roy [6]. The vectors w\u0303, w, and w\u2217 denote an optimal solution to the PALP formulation, its suboptimal solution, and the vector that minimizes the max-norm error \u2016V \u2217 \u2212 V w\u2016\u221e, respectively. First, we bound the objective in the PALP as follows:\n\u2225\u2225\u2225V \u2217 \u2212 V w\u0303 \u2225\u2225\u2225 1,\u03c8 \u2264 \u2225\u2225V \u2217 \u2212 V w \u2225\u2225 1,\u03c8\n\u2264 \u2225\u2225V \u2217 \u2212 V w \u2225\u2225 \u221e .\nSecond, we bound the max-norm error of V w by the triangle inequality:\n\u2225\u2225V \u2217 \u2212 V w \u2225\u2225 \u221e \u2264 \u2225\u2225\u2225V \u2217 \u2212 V w\u0302 \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225V w\u0302 \u2212 V w \u2225\u2225\u2225 \u221e ,\nwhere w\u0302 is an arbitrary solution to the ALP formulation. In the rest of the proof, we bound the two terms on the righthand side of the inequality. The first term reflects how hard is to fit the linear value function approximation to the value function V \u2217. If the vector w\u0302 is set such that:\nw\u0302 = w\u2217 + 1 + \u03b3\n1\u2212 \u03b3\n\u2225\u2225\u2225V \u2217 \u2212 V w \u2217 \u2225\u2225\u2225 \u221e i0,\nwhere i0=(1, 0, . . . , 0) is an indicator of the constant basis function f0(x) \u2261 1, the following inequality:\n\u2225\u2225\u2225V \u2217 \u2212 V w\u0302 \u2225\u2225\u2225 \u221e \u2264 2 1\u2212 \u03b3 \u2225\u2225\u2225V \u2217 \u2212 V w \u2217 \u2225\u2225\u2225 \u221e\nholds [6]. The second term reflects how hard it to make the ALP solution w\u0302 feasible in the PALP. If the vectorw is set such that:\nw = w\u0302 + K\u03b4\n1\u2212 \u03b3 i0,\nwhere i0=(1, 0, . . . , 0) is an indicator of the constant basis function f0(x)\u22611, \u03b4 = \u2212minx,amin(DMw\u0302(x, a)T), and the function min(DM\nw\u0302 (x, a)T) computes the minimum of\nthe vectorDM w\u0302 (x, a)T, we can guarantee the feasibility of w. The proof is based on the observation that all constraints in the feasible PALP formulation (16) are satisfied when the weights wk0 are set such that:\nwk0 = 1\nK w\u03020 +\n\u03b4\n1\u2212 \u03b3 .\nBased on this setting, the max-norm error between V w\u0302 and V w is bounded as:\n\u2225\u2225\u2225V w\u0302 \u2212 V w \u2225\u2225\u2225 \u221e \u2264 K\u03b4 1\u2212 \u03b3 .\nThis step concludes our proof.\nThe above result can be interpreted as follows. PALP yields a close approximation V w\u0303 to the optimal value function V \u2217 if the function V \u2217 lies in the span of basis functions and the penalty \u03b4 for partitioning the ALP constraint space is small. Unfortunately, we do not have a good bound for the penalty term \u03b4. The value of \u03b4 can be as bad as \u2016w\u0302\u20161+Rmax, where Rmax denotes the maximum immediate reward in an MDP. Hence, the bound in Theorem 2 is not very tight in practice. Nevertheless, it provides valuable insights into two sources of errors for PALP approximations."}, {"heading": "6 Experiments", "text": "The objective of the experimental section is to demonstrate the quality and scale-up potential of PALP approximations.\nThe approximations are studied with respect to ALP, which is a state-of-the-art approach to solving large-scale factored MDPs. Our experiments are performed on various forms of the network administration problem [10]. This is a standard benchmark for testing the scalability of MDP algorithms."}, {"heading": "6.1 Experimental setup", "text": "The network administration problem involves a network of randomly crashing computers. When a computer crashes, it increases the probability of its network neighbors crashing. The objective is to reboot crashed computers to restore their functionality and prevent further spreading of their failures into the network. Examples of three network topologies are shown in Figure 4. Each network consists of one server and several workstations. The difference between the two types of the computers is in the reward for keeping them running.\nThe immediate reward for keeping a workstation running is 1. The reward for keeping the server running is 2.\nThe network administration problem is a challenging MDP due to the size of its state space. Specifically, since the state of the network is a product of individual computer states, it is exponential in the number of computers. Therefore, only small instances of the problem can be solved exactly. In the rest of the section, we focus on large-scale problems and try to solve them through linear value function approximations (Equation 4). In all experiments, we define a basis function fi(x)=xi for every computer Xi. Furthermore, in the ring and ring-of-rings topologies (Figures 4a and 4b), we assign a pairwise basis function fi\u2192j(x) = xixj to every network connection Xi\u2192Xj .\nOur linear value function approximations are optimized using ALP and PALP formulations. The cutting plane method is employed to solve these LPs exactly and efficiently (Figure 3). In addition, we experiment with ALP formulations, which are solved approximately by Monte Carlo constraint sampling [7]. The number of sampled constraints is 100n, where n is the number of state variablesX. Therefore, it is proportional to the size of solved problems. To demonstrate the non-triviality of learned policies, we also report results of a heuristic for solving our problem. The heuristic places the administrator at the server so the computer is protected from crashing."}, {"heading": "6.2 Experimental results", "text": "Our main experimental results are summarized in Figure 5. Based on these results, we conclude that PALP policies are almost as good as ALP policies. Specifically, note that the rewards of the policies are within 95 percent of our baseline in all experiments. Unfortunately, these good results cannot be explained by Theorem 2 because our bound is too loose. To explain our results, we tried to investigate the similarity of basis function weightsw obtained by ALP and PALP. As illustrated in Figure 6, the magnitudes of the weights can be very different. However, the weights exhibit similar trends. In turn, value function approximations corresponding to the weights must have similar shapes, and their greedy policies\nare similar as a result.\nFigure 5 also suggests that PALP policies can be computed significantly faster than ALP policies. This speedup results from working with sparse decompositions (Figure 7) of the original constraint space rather than the space itself. Moreover, note that the treewidth of the n\u00d7 n network administration problem (Figure 4c) is n. Therefore, the complexity of learning ALP policies for this problem is naturally exponential in n. On the other hand, the complexity of learning PALP policies is polynomial in n. This claim follows from the observation that the number of PALP constraint spaces is n2 and their treewidth is not dependent on n. As a result, PALP on the grid network provides an exponential speedup over ALP. This result can be verified by the analysis of the computation time trends in Figure 5.\nFinally, Figure 5 illustrates that PALP policies are superior to ALP policies, which are obtained by ALP with randomly sampled constraints. In most cases, the PALP policies yield significantly higher rewards than the average sampled ALP approximation. For all larger network administration problems, the policies are as good or better than the best of these\napproximations. At the same time, the computation time of the PALP policies is shorter or comparable to the computation time of the sampled approximations."}, {"heading": "7 Conclusions", "text": "Development of scalable algorithms for solving real-world MDPs is a challenging task. In this work, we investigated a novel approach to approximate linear programming. Comparing to the standard ALP formulation, we decompose the constraint space into a set of low-dimensional spaces. This structure allows for solving the new LP more efficiently. In particular, its constraints can be satisfied in a compact form without an exponential dependence on the treewidth of the original constraint space. Our experiments demonstrate the superiority of the new approach when compared to existing exact and approximate solutions to ALP.\nResults of this paper can be extended in several ways. First, we have not addressed the topic of learning good partitioning matricesD. This topic is in many aspects similar to the problem of efficient inference in Bayesian networks. In this context, Meila [17] proposed using a mixture of trees to approximate an arbitrary joint probability distribution defined by a Bayesian network. Second, the bound in Theorem 2 is definitely loose in practice. How to make this bound tight is an interesting open question. Finally, PALP and its benefits should be studied on a more realistic problem than the one presented in the experimental section."}, {"heading": "Acknowledgment", "text": "We thank anonymous reviewers for helpful comments that led to the improvement of this paper. We also thank Carlos Guestrin for encouragement and positioning this paper in a broader context."}], "references": [{"title": "Dynamic Programming", "author": ["Richard Bellman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1957}, {"title": "Polynomial approximation \u2013 a new computational technique in dynamic programming: Allocation processes", "author": ["Richard Bellman", "Robert Kalaba", "Bella Kotkin"], "venue": "Mathematics of Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1963}, {"title": "Neuro-Dynamic Programming", "author": ["Dimitri Bertsekas", "John Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Introduction to Linear Optimization", "author": ["Dimitris Bertsimas", "John Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Exploiting structure in policy construction", "author": ["Craig Boutilier", "Richard Dearden", "Mois\u00e9s Goldszmidt"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "The linear programming approach to approximate dynamic programming", "author": ["Daniela Pucci de Farias", "Benjamin Van Roy"], "venue": "Operations Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "On constraint sampling for the linear programming approach to approximate dynamic programming", "author": ["Daniela Pucci de Farias", "Benjamin Van Roy"], "venue": "Mathematics of Operations Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A model for reasoning about persistence and causation", "author": ["Thomas Dean", "Keiji Kanazawa"], "venue": "Computational Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Bucket elimination: A unifying framework for probabilistic inference", "author": ["Rina Dechter"], "venue": "In Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Maxnorm projections for factored MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Ronald Parr"], "venue": "In Proceedings of the 17th International Joint Conference on Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Multiagent planning with factored MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Ronald Parr"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Efficient solution algorithms for factored MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Ronald Parr", "Shobha Venkataraman"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Computing factored value functions for policies in structured MDPs", "author": ["Daphne Koller", "Ronald Parr"], "venue": "In Proceedings of the 16th International Joint Conference on Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "An MCMC approach to solving hybrid factored MDPs", "author": ["Branislav Kveton", "Milos Hauskrecht"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Solving factored MDPs with hybrid state and action variables", "author": ["Branislav Kveton", "Milos Hauskrecht", "Carlos Guestrin"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Samuel meets Amarel: Automating value function approximation using global state space analysis", "author": ["Sridhar Mahadevan"], "venue": "In Proceedings of the 20th National Conference on Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Learning with Mixtures of Trees", "author": ["Marina Meila"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Greedy linear valueapproximation for factored Markov decision processes", "author": ["Relu Patrascu", "Pascal Poupart", "Dale Schuurmans", "Craig Boutilier", "Carlos Guestrin"], "venue": "In Proceedings of the 18th National Conference on Artificial Intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["Martin Puterman"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Direct valueapproximation for factored MDPs", "author": ["Dale Schuurmans", "Relu Patrascu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Generalized polynomial approximations in Markovian decision processes", "author": ["Paul Schweitzer", "Abraham Seidmann"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1985}, {"title": "Planning Under Uncertainty in Complex Structured Environments", "author": ["Benjamin Van Roy"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}], "referenceMentions": [{"referenceID": 18, "context": "Markov decision processes (MDPs) [19] are an established framework for solving sequential decision problems under uncertainty.", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "Approximate linear programming (ALP) [21] has emerged as a promising approach to solving these problems efficiently [6, 12, 15].", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "Approximate linear programming (ALP) [21] has emerged as a promising approach to solving these problems efficiently [6, 12, 15].", "startOffset": 116, "endOffset": 127}, {"referenceID": 11, "context": "Approximate linear programming (ALP) [21] has emerged as a promising approach to solving these problems efficiently [6, 12, 15].", "startOffset": 116, "endOffset": 127}, {"referenceID": 14, "context": "Approximate linear programming (ALP) [21] has emerged as a promising approach to solving these problems efficiently [6, 12, 15].", "startOffset": 116, "endOffset": 127}, {"referenceID": 9, "context": "The optimization can be performed in a structured manner [10, 20].", "startOffset": 57, "endOffset": 65}, {"referenceID": 19, "context": "The optimization can be performed in a structured manner [10, 20].", "startOffset": 57, "endOffset": 65}, {"referenceID": 9, "context": "The complexity of computing exact ALP solutions [10, 20] is exponential in the treewidth of the dependency graph that represents the constraint space in ALP.", "startOffset": 48, "endOffset": 56}, {"referenceID": 19, "context": "The complexity of computing exact ALP solutions [10, 20] is exponential in the treewidth of the dependency graph that represents the constraint space in ALP.", "startOffset": 48, "endOffset": 56}, {"referenceID": 6, "context": "This type of problems can be still solved approximately using Monte Carlo constraint sampling [7, 14].", "startOffset": 94, "endOffset": 101}, {"referenceID": 13, "context": "This type of problems can be still solved approximately using Monte Carlo constraint sampling [7, 14].", "startOffset": 94, "endOffset": 101}, {"referenceID": 4, "context": "First, we review factored MDPs [5] and linear value function approximations [2, 22].", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "First, we review factored MDPs [5] and linear value function approximations [2, 22].", "startOffset": 76, "endOffset": 83}, {"referenceID": 21, "context": "First, we review factored MDPs [5] and linear value function approximations [2, 22].", "startOffset": 76, "endOffset": 83}, {"referenceID": 4, "context": "Factored MDPs [5] allow for a compact representation of this structure.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "A factored MDP [5] is a 4-tuple M = (X,A, P,R), where X = {X1, .", "startOffset": 15, "endOffset": 18}, {"referenceID": 10, "context": "Our ideas straightforwardly generalize to MDPs with factored action spaces [11].", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "and is described compactly by a dynamic Bayesian network (DBN) [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 18, "context": "In such a setting, there always exists an optimal policy \u03c0 which is stationary and deterministic [19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "The policy is greedy with respect to the optimal value function V , which is a fixed point of the Bellman equation [1]:", "startOffset": 115, "endOffset": 118}, {"referenceID": 18, "context": "Markov decision processes can be solved by exact dynamic programming (DP) methods in polynomial time in the size of their state space [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "Since a factored representation of an MDP does not guarantee a structure in its solution [13], we resort to value function approximations.", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "In this work, we focus on the linear value function approximation [2, 22]:", "startOffset": 66, "endOffset": 73}, {"referenceID": 21, "context": "In this work, we focus on the linear value function approximation [2, 22]:", "startOffset": 66, "endOffset": 73}, {"referenceID": 1, "context": "The basis functions fi(x) are arbitrary functions, which are usually restricted to small subsets of state variablesXi [2, 13].", "startOffset": 118, "endOffset": 125}, {"referenceID": 12, "context": "The basis functions fi(x) are arbitrary functions, which are usually restricted to small subsets of state variablesXi [2, 13].", "startOffset": 118, "endOffset": 125}, {"referenceID": 17, "context": "They are usually provided by domain experts but can also be discovered automatically [18, 16].", "startOffset": 85, "endOffset": 93}, {"referenceID": 15, "context": "They are usually provided by domain experts but can also be discovered automatically [18, 16].", "startOffset": 85, "endOffset": 93}, {"referenceID": 2, "context": "Various techniques for optimizing the linear value function approximation have been studied and analyzed [3].", "startOffset": 105, "endOffset": 108}, {"referenceID": 20, "context": "We focus on approximate linear programming (ALP) [21], which restates this problem as a linear program:", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "Since our basis functions fi(x) are often restricted to small subsets of state variables, expectation terms in the ALP formulation (5) can be computed efficiently [10].", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "[10] proposed a variable elimination method [9] that rewrites the constraint space compactly.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] proposed a variable elimination method [9] that rewrites the constraint space compactly.", "startOffset": 44, "endOffset": 47}, {"referenceID": 19, "context": "Schuurmans and Patrascu [20] solved the constraint satisfaction problem by the cutting plane method [4].", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": "Schuurmans and Patrascu [20] solved the constraint satisfaction problem by the cutting plane method [4].", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "The space complexity of both constraint satisfaction methods [10, 20] is exponential in the treewidth of the constraint space.", "startOffset": 61, "endOffset": 69}, {"referenceID": 19, "context": "The space complexity of both constraint satisfaction methods [10, 20] is exponential in the treewidth of the constraint space.", "startOffset": 61, "endOffset": 69}, {"referenceID": 6, "context": "For instance, de Farias and Van Roy [7] proposed Monte Carlo approximations of the constraint space.", "startOffset": 36, "endOffset": 39}, {"referenceID": 13, "context": "Kveton and Hauskrecht [14] showed how to search for the most violated constraint (Equation 9) using Markov chain Monte Carlo (MCMC) sampling.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "The quality of the ALP formulation has been studied by de Farias and Van Roy [6].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "Theorem 1 (de Farias and Van Roy [6]).", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "De Farias and Van Roy [6] also proved a tighter version of Theorem 1, which reweights the error \u2016V \u2217 \u2212 V w\u2016\u221e.", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "This differentiates our work from existing ALP approximations [7, 14].", "startOffset": 62, "endOffset": 69}, {"referenceID": 13, "context": "This differentiates our work from existing ALP approximations [7, 14].", "startOffset": 62, "endOffset": 69}, {"referenceID": 9, "context": "The width of the decompositions should be small since the complexity of satisfying a single constraint space is exponential in its treewidth [10].", "startOffset": 141, "endOffset": 145}, {"referenceID": 5, "context": "Proof: Our proof is similar to the proof of Theorem 2 by de Farias and Van Roy [6].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "holds [6].", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "Our experiments are performed on various forms of the network administration problem [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "In addition, we experiment with ALP formulations, which are solved approximately by Monte Carlo constraint sampling [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 16, "context": "In this context, Meila [17] proposed using a mixture of trees to approximate an arbitrary joint probability distribution defined by a Bayesian network.", "startOffset": 23, "endOffset": 27}], "year": 2008, "abstractText": "Approximate linear programming (ALP) is an efficient approach to solving large factored Markov decision processes (MDPs). The main idea of the method is to approximate the optimal value function by a set of basis functions and optimize their weights by linear programming (LP). This paper proposes a new ALP approximation. Comparing to the standard ALP formulation, we decompose the constraint space into a set of low-dimensional spaces. This structure allows for solving the new LP efficiently. In particular, the constraints of the LP can be satisfied in a compact form without an exponential dependence on the treewidth of ALP constraints. We study both practical and theoretical aspects of the proposed approach. Moreover, we demonstrate its scale-up potential on an MDP with more than 2 states.", "creator": "dvips(k) 5.96 Copyright 2007 Radical Eye Software"}}}