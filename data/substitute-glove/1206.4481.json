{"id": "1206.4481", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2012", "title": "Parsimonious Mahalanobis Kernel for the Classification of High Dimensional Data", "abstract": "The classification of as dimensional contents with ms-dos used is considered began same article. Exploit - ij the emptiness property although high arrays spaces, a filesystem based month while Mahalanobis distance is which. The arithmetic of the Mahalanobis distance basic instead inversion latter a bernoulli formula_3. In continues four-dimensional viewing, for 1,800 covariance equation being suffering - accommodates and which transposition has unstable. not. Using close simpleminded comparison simplified, these for High Dimensional Discriminant Analysis model, the specific activation while throttle subspaces are 8,000 an usually nor addition while main inverse means the class context univariate eigenvalues explicit and stable, leading should the necessarily of which parsimonious Mahalanobis kernel. A SVM based requires form which there selecting the hyperparameters of set hit-and-miss Mahalanobis debugger the optimizing although think - kind extends - margin bound. Experimental final saturday other students transformations material sets mtv any since proposed compatibility gives adequate for classifying well dimensional computerized, providing too defines conceptualizations which the definition Gaussian non-uniform.", "histories": [["v1", "Wed, 20 Jun 2012 12:49:48 GMT  (71kb,D)", "https://arxiv.org/abs/1206.4481v1", null], ["v2", "Mon, 10 Sep 2012 13:01:47 GMT  (71kb,D)", "http://arxiv.org/abs/1206.4481v2", null]], "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["m fauvel", "a villa", "j chanussot", "j a benediktsson"], "accepted": false, "id": "1206.4481"}, "pdf": {"name": "1206.4481.pdf", "metadata": {"source": "CRF", "title": "Parsimonious Mahalanobis Kernel for the Classification of High Dimensional Data", "authors": ["M. Fauvel", "A. Villa", "J. Chanussot", "J.A. Benediktsson"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "High Dimensional (HD) data sets are commonly available for fully or partially automatic processing: For a relatively low number of samples, n, a huge number of variables, d, is simultaneously accessible. For instance, in hyperspectral imagery, hundreds of spectral wavelengths are recorded for a given pixel; in gene expression analysis, the measure of expression level of thousands of genes is typical; in customer recommendation systems for web services, to each potential client a high number of variables is associated (his past choices, his personal information ...) [1, 2, 3]. For each sample, it is possible to have either numerical or alphabetical variables which can be sparse or with a different signal to noise ratio.\nIn terms of processing, such data may need to be either classified, clustered, filtered or inversed in a supervised or unsupervised way. Although many algorithms exist in the literature for small or moderate dimensions (from Bayesian methods to Machine Learning techniques) most of them are not well suited to HD data. Actually, HD data pose critical theoretical and practical problems that need to be addressed specifically [2].\nIndeed, HD spaces exhibit non intuitive geometrical and statistical properties when compared to lower dimensional spaces. Most of them do not behave in a similar way as in three dimensional Euclidean spaces (Table 1 summarizes the main properties of HD spaces) [4]. For instance, samples following a uniform law will have a tendency to have a high concentration in the corners [5]. The same property holds for normally distributed data: samples tend to have a high concentration in the tails [6], making density estimation a difficult task. This problem can be related to the number of parameters t to be estimated to fit a Gaussian\nar X\niv :1\n20 6.\n44 81\nv2 [\ncs .N\nA ]\n1 0\ndistribution which grows quadratically with the space dimensionality, t = d(d + 3)/2 (5150 for d = 100). Because of this, conventional generative methods are not suitable for analyzing this type of data.\nUnfortunately, discriminative methods also suffer if the dimensionality is high, due to the \u201cconcentration of measure phenomenon\u201d [2]. In HD spaces, samples tend to be equally distant from each other [7]. Hence, it is clear that Nearest Neighbors methods will definitively fail to process such data. Moreover, the Euclidean distance will not be appropriate to assess the similarity between two samples. In fact, it is has been shown that every Minkowski norm (\u2016x\u2016m = (\u2211d i=1 |xi|m\n)1/m, m = 1, 2 . . . ) is affected by this phenomenon [8]. Therefore, every method based on the distance between samples [9] (SVM with Gaussian kernel, neural network, Nearest Neighbors, Locally Linear Embedding. . . ) are potentially affected by this phenomenon [10, 11].\nAn additional property, for which the consequences are more practical than theoretical, is the \u201cempty space phenomenon\u201d [12]: In HD spaces, the available samples usually fill a very small part of the space. Therefore, most of the space is empty. Note that if originally the empty space phenomenon was considered as a problem, it will be seen in the following that it is actually the basis of several useful statistical models.\nToday, the phrasing \u201ccurse of dimensionality\u201d, originally from R. Bellman [12], refers to the aforementioned problems of HD data and reflects how processing HD data is difficult. However, as D. Donoho has noticed [2], there is also a \u201cBlessing of dimensionality\u201d: For instance in classification, the class separability is improved when the dimensionality of the data increases. Consider for example a comparison between hyperspectral (hundreds of spectral wavelengths) and multispectral (tens of spectral wavelengths) remote sensing images[13]. The former contains much more information, and enables a more accurate distinction of the land cover classes. However, if conventional methods are used, the additional information contained in hyperspectral images will not lead to an increase of the classification accuracy [5]. Hence, using conventional methods, classification accuracies remains low.\nSeveral methods have been proposed in the literature to deal with HD data for the purpose of classification. A highly used strategy is Dimension Reduction (DR). DR aims at reducing the dimensionality of data by mapping them onto another space of a lower dimension, without discarding any, or as less as possible, of the meaningful information. Recent overviews of DR can be found in [14, 15, 16]. Two main approaches can be defined. 1) Unsupervised DR: The algorithms are applied directly on the data without exploiting any prior information, and project the data into a lower dimensional space, according to some criterion (data variance maximization for PCA, independence for ICA . . . ). 2) Supervised DR: Training samples are available and are exploited to find a lower dimensional subspace where the class separability is improved. Fisher Discriminant Analysis (FDA) is surely one of the most famous supervised DR method. However, FDA maximizes the ratio of the \u201cbetween classes\u201d scatter matrix, Sb, and the \u201cwithin classes\u201d scatter matrix, Sw. The optimal solution is given by the eigenvector corresponding to the first eigenvalues of S\u22121w Sb. In HD, S\u22121w is in general ill-conditioned which limits the effectiveness of the method. Other popular DR methods such as Laplacian eigenmaps, Isomap or Locally Linear Embedding [15, Chapter 4 and 5] may be also limited by the dimensionality since they are based on the Euclidean distance between the samples. One last drawback of DR methods is the risk of losing relevant information. In general, DR methods act globally, which can be a problem for classification purpose: Different classes may be mapped onto the same subspace, even if the global discrimination criteria is maximized.\nAn alternative strategy to DR has been recently proposed, i.e., the subspace models [17]. These models assume that each class is located in a specific subspace and consider the original space without DR for the processing. For instance, the Probabilistic Principal Component Analysis (PPCA) [18] assumes that the classes are normally distributed in a lower dimensional subspace and are linearly embedded in the original subspace with additive white noise. Such models exploit the empty space property of HD data without discarding any dimension of the data [19, 20]. A general subspace model that encompasses several other models is the High Dimensional Discriminant Analysis (HDDA) model, proposed by Bouveyron et al. [21, 22].\nConversely, kernel based methods do not reduce the dimensionality but rather work with the full HD data [23]. These discriminative methods are known to be more robust to size of the dimensionality\nthan conventional generative methods. However, local kernel methods are sensitive to the size of the dimensionality [24]. A kernel method is said to be local if the decision function value for a new sample depends on the neighbors of that sample in the training set. Since in HD data the neighborhood of a sample is mostly empty, such local methods are negatively impacted by the dimension. For instance, SVM with Gaussian kernel\nkg(x, z) = exp\n( \u2212 \u2016x\u2212 z\u2016 2\n2\u03c32\n) (1)\nis such a local kernel method. In this paper, it is proposed to use subspace models to construct a kernel adapted to high dimensional data. The chosen approach for including subspace models in a kernel function is to consider the Mahalanobis distance, d\u03a3c , between two samples for a given class, c, with covariance matrix, \u03a3c:\nd\u03a3c(x, z) = \u221a (x\u2212 z)t\u03a3\u22121c (x\u2212 z).\nPrevious works on the Mahalanobis kernel [25, 26, 27, 28] were limited by the effect of dimensionality on the matrix inversion. In [25], the covariance matrix was computed on the whole training set. The associated implicit model is that the classes share the same covariance matrix, which is not true in practice. Diagonal and full covariance matrices were investigated in [26] for the purpose of classification and in [27] for the purpose of regression. However, in a similar way, the covariance matrix was computed for all the training samples. Computing the covariance matrix for the Mahalanobis distance with all the training samples is equivalent to project the data on all the principal components, scale the variance to one, and then applying the Euclidean distance. By doing so, classes could overlap more than in the original input space and the discrimination between them would be decreased.\nIn this work, the HDDA model is used for the definition of a class specific covariance matrix adapted for HD data. The specific signal and noise subspaces are estimated for each considered class, ensuring a parsimonious characterization of the classes. Following the HDDA model it is then possible to derive an explicit formulation of the inverse of the covariance matrix, without any regularization or dimension reduction. The parsimonious Mahalanobis kernel is constructed by substituting the Euclidean distance with the Mahalanobis distance computed using the HDDA model. It is proposed in this work to define several hyperparameters in the kernel to control the influence of the signal and noise subspaces in the classification process. These hyperparameters are optimized during the training process by the minimization of the so-called radius margin bound of the SVM classifier. Compared to the previous works on the Mahalanobis kernel for HD data, the proposed method allows the use of a more complex model, a separate covariance matrix per class, with higher efficiency in terms of accuracy. The remainder of the paper is organized as follows. The subspace model and the proposed kernel are discussed in Section 2. The problem of selecting the hyperparameters for classification with SVM is addressed in Section 3. The Section 4 details the estimation of the size of the signal subspace. Results on simulated and real high dimensional data are reported in Section 5. Conclusions and perspectives conclude the paper."}, {"heading": "2 Regularized Mahalanobis Kernel", "text": ""}, {"heading": "2.1 Review of HDDA model", "text": "The most general HDDA sub-model is used in this work1, i.e., each class has his own specific subspace. Here, we will review the HDDA model but restricted to the problem of the covariance matrix inversion. However HDDA was originally proposed for classification or clustering with Gaussian mixture model. Interested readers can find a detailed presentation of HDDA in [21, 22].\nIn subspace models, it is assumed that the data from each class are clustered in the vector space. This cluster does not need to have an elliptic shape but it is generally assumed that the data follow a Gaussian distribution. The covariance matrix of the class c can be written through its eigenvalue decomposition:\n\u03a3c = Qc\u039bcQ t c\nwhere \u039bc is the diagonal matrix of eigenvalues \u03bbci, i \u2208 {1, . . . , d}, of \u03a3c and Qc is the matrix that contains the corresponding eigenvectors qci. The HDDA model assumes the pc first eigenvalues are different and the remaining d\u2212 pc eigenvalues are identical. The model is similar to PPCA, but more general in the sense that additional sub-models can be defined. In particular, the intrinsic dimension pc are not constrained in HDDA whereas there are assumed to be equal for each class in PPCA. Under the HDDA framework, the covariance matrix has the following expression:\n\u03a3c = pc\u2211 i=1 \u03bbciqciq t ci + bc d\u2211 i=pc+1 qciq t ci\nwhere the last d\u2212 pc eigenvalue are equal to bc. The inverse can be computed explicitly by\n\u03a3\u22121c = pc\u2211 i=1 1 \u03bbci qciq t ci\ufe38 \ufe37\ufe37 \ufe38\nAc\n+ 1\nbc d\u2211 i=pc+1\nqciq t ci\ufe38 \ufe37\ufe37 \ufe38\nA\u0304c\n.\nThis statistical model can be understood equivalently by a geometrical assumption: For each class, the data belong to a cluster that lives in a lower dimensional space Ac, namely the signal subspace. The original input space can be decomposed as Rd = Ac \u2295 A\u0304c (by construction A\u0304 is the noise subspace which contains only white noise). Figure 1 gives an illustration of that in R3. Using I = \u2211d i=1 qciq t ci, I being the identity matrix, the inverse can be finally written as\n\u03a3\u22121c = pc\u2211 i=1 ( 1 \u03bbci \u2212 1 bc ) qciq t ci + 1 bc I. (2)\nStandard likelihood maximization shows that the parameters (\u03bbci,qci)i=1,...,pc and bc can be computed from the sample covariance matrix [21]:\n\u03a3\u0302c = 1\nnc nc\u2211 i=1 ( xi \u2212 x\u0304c )( xi \u2212 x\u0304c )t where x\u0304c is the sample mean for class c and nc the number of samples of the class. \u03bbci is estimated by the i first eigenvalue of \u03a3\u0302c, qci by the corresponding eigenvector and b\u0302c = ( trace(\u03a3\u0302c)\u2212 \u2211p\u0302c i=1 \u03bb\u0302ci ) /(d\u2212 p\u0302c) (the estimation of the dimension pc of the subspace is discussed later). The last d \u2212 pc eigenvalues and their corresponding eigenvectors are not needed for the computation of the inverse in (2).\n1Refers to [aijbiQidi] in [21, 22].\nThe major advantage of such a model is that it reduces drastically the number of parameters to estimate for computing the inverse matrix. Indeed, with the full covariance matrix, d(d + 3)/2 parameters are to be estimated. With the HDDA model, only d(pc + 1) + 1\u2212 pc(pc \u2212 1)/2 parameters are to be estimated. For instance, if d = 100 and pc = 10, 5150 parameters are needed for the full covariance and only 1056 for the HDDA model. Furthermore, the stability is improved since the smallest eigenvalues of the covariance matrix and their corresponding eigenvectors, which are difficult to compute accurately, are not used in (2). Finally, using the HDDA model, the square Mahalanobis distance for class c is approximated by\nd2 \u03a3\u0302c (x, z) = p\u0302c\u2211 i=1 ( 1 \u03bb\u0302ci \u2212 1 b\u0302c ) \u2016q\u0302tci(x\u2212 z)\u20162 + \u2016x\u2212 z\u20162 b\u0302c . (3)\nThis approach relies on the analysis of the empirical covariance matrix, as with PCA. But instead of keeping only significant eigenvalues, (3) considers all the original space, without discarding any dimension. This has two main theoretical advantages over the conventional PCA:\n1. Two samples may be close in the signal subspace but far apart in the original space, which is a problem for classification tasks. It can be handled by considering the noise subspace together with the signal subspace. Consider for instance z, z\u2032 and x in Figure 1. In A, z\u2032 seems closer to x than z, while it is not as it can be seen by adding A\u0304 in the distance computation.\n2. An accurate estimation of the signal subspace size p\u0302c is necessary if PCA is applied: The worst scenario being p\u0302c << pc, i.e., relevant eigenvectors are discarded. By considering both the signal and the noise subspaces, the method becomes less sensitive to p\u0302c. Even in the worst case scenario, the eigenvectors are still considered."}, {"heading": "2.2 Mahalanobis Kernel", "text": "The regularized Mahalanobis kernel for class c is constructed by substituting (3) to the Euclidean distance in the Gaussian kernel (1) and switching eigenvalues (\u03bb\u0302ci, b\u0302c) to hyperparameters (\u03c32ci, \u03c3 2 cp\u0302c+1\n) that are optimized during the training step:\nkm(x, z|c) =\nexp ( \u2212 1\n2 ( p\u0302c\u2211 i=1 \u2016q\u0302tci(x\u2212 z)\u20162 \u03c32ci + \u2016x\u2212 z\u20162 \u03c32cp\u0302c+1 )) (4)\nwhere \u03c3ci, i \u2208 {1, . . . , p\u0302c + 1} are the hyperparameters of the kernel. As described in Section 3, these parameters are tuned during the training step. The hyperparameters have been introduced for the following reason. It is known that the principal directions are not optimal for classification since they do not maximize any discrimination criterion. However, they still span a subspace where there are variations in the data of the considered class. The hyperparameters \u03c3ci allow to control which directions are more relevant (or discriminative) for the classification process: The feature space is modified during the training process to ensure a better discrimination between samples.\nIt is interesting to note that the regularized Mahalanobis kernel can be expressed as the product of Gaussian kernels:\nkm(x, z|c) = kg(x, z)\u00d7 p\u0302c\u220f i=1 kg(q\u0302 t cix, q\u0302 t ciz). (5)\nThe feature space induced by the kernel and the influence of the hyperparameters is analyzed in the next section."}, {"heading": "2.3 Geometry of the induced feature space", "text": "Working with a kernel function is equivalent to work with samples mapped onto a feature space H, where the dot product is equivalent to the kernel evaluation in the input space [23, 29]:\nk(x, z) = \u3008\u03c6(x), \u03c6(z)\u3009H,\n\u03c6 being the feature map. Under some weak conditions, the projected samples in the feature space live on a Riemannian manifold [30, 31]. The metric tensor is\ngij(x) = \u22022k(x, z)\n\u2202xi\u2202zj \u2223\u2223\u2223\u2223\u2223 z=x\n(6)\nwhich is, for the Gaussian kernel, gij(x) = \u03c3\u22122\u03b4ij with \u03b4ij = 1 if i = j and 0 otherwise. This metric stretches or compresses the Euclidean distance between x and z by a factor \u03c3\u22122. Each variable is assumed equally relevant for the given task, e.g., classification or regression.\nFor the kernel (4) the metric tensor is:\ngij(x|c) = p\u0302c\u2211 l=1 qcliqclj \u03c32cl + \u03b4ij \u03c32cp\u0302c+1 (7)\nwith qcli the ith element of qcl. The distance between two samples is stretched (if \u03c32cl \u2265 1) or compressed (if \u03c32cl \u2264 1) along the p\u0302c first principal components of class c (first term of the right part of the equation) and along the original components (last term of the equation). In other words, each principal component is weighted according to its relevance for the processing.\nThe analysis of the metric tensor exhibits the nature of the proposed kernel for a given class: It is a mixture of a Gaussian kernel on the original variables and a Gaussian kernel on the p\u0302c first principal components of the considered class. The hyperparameters \u03c3cl are tuned during the training process. This allows the optimization of the weight of each kernel. If \u03c32cl = +\u221e, \u2200l \u2208 {1, . . . , p\u0302c}, (4) reduces to the conventional Gaussian kernel. On the contrary, if \u03c32cp\u0302c+1 = +\u221e, (4) reduces to the Gaussian kernel on the p\u0302c first principal components. Figure 2 shows the kernel values for different values of the hyperparameters. Note that opposite to the Gaussian kernel, the kernel in (4) is not isotropic.\nThe following section reviews the basics of SVM classifier and presents how the hyperparameters are computed."}, {"heading": "3 L2-SVM and Radius margin bound optimization", "text": "Support vector machines (SVM) is a standard classification kernel methods [32]. It has shown to performs very well on several data sets from moderate dimension to high dimensional data [33, 34]. In the following section, the main results are presented but interested readers could see references [32, 35, 36] for further mathematical details about the SVM framework."}, {"heading": "3.1 L2-Support Vector Machines", "text": "The L2-SVM is considered in this work rather than the conventional L1-SVM [35]: With L2-SVM it is possible to tune the hyperparameters automatically by optimizing the so called radius-margin bound [37]. The L2-SVM solves the conventional L1-SVM optimization problem with a quadratic penalization of errors [35]. Given a training set S = {(x1, y1), . . . , (xn, yn)}, (xi, yi) \u2208 Rd \u00d7 {\u22121; 1}, the parameters (\u03b1i) n i=1 and b of the decision function f ,\nf(z) = n\u2211 i=1 \u03b1ik(xi, z) + b,\nare found by solving the convex optimization problem:\nmax \u03b1 g(\u03b1) = n\u2211 i=1 \u03b1i \u2212 1 2 n\u2211 i,j=1 \u03b1i\u03b1jyiyj k\u0303(xi,xj)\nsubject to 0 \u2264 \u03b1i and n\u2211 i=1 \u03b1iyi = 0\n(8)\nwhere k\u0303(xi,xj) = k(xi,xj) + C\u22121\u03b4ij with k the kernel function and C a positive hyperparameter that is used to penalize the training errors.\nAn estimate of the generalization errors is given by an upper bound on the number of errors of the leave-one-out procedure, the radius-margin bound T [36]:\nT (p) := R2M2. (9)\nR2 is the radius of the smallest hypersphere that contains all \u03c6(xi), M2 is the margin of the classifier, it is given by the optimal objective function of (8), and p are the hyperparameters. In our setting\np = [\u03c321, . . . , \u03c3 2 p\u0302c+1 , C]. R2 is obtained by the optimal objective function of the following constraint optimization problem [36]:\nmax \u03b2\ng\u2032(\u03b2) = n\u2211 i=1 \u03b2ik\u0303(xi,xi)\u2212 n\u2211 i,j=1 \u03b2i\u03b2j k\u0303(xi,xj)\nsubject to 0 \u2264 \u03b2i and n\u2211 i=1 \u03b2i = 1.\n(10)\nSince both R2 andM2 depend on p, it is possible to optimize T to set the hyperparameters. Chapelle et al. [37], followed later by S. S. Keerthi [38], have proposed an algorithm based on gradient optimization method. It is discussed in the following section.\nWith the proposed kernel, if two classes i and j are considered, the classifier for \u201ci vs j\u201d is not the same as the classifier for \u201cj vs i\u201d since the kernel function is specific to the classes i and j, respectively. Indeed, for a multiclass problem, the \u201cone vs one\u201d approach must not be used and the \u201cone vs all\u201d approach should be preferred [39]."}, {"heading": "3.2 Radius-margin bound Optimization", "text": "Computing the gradient of T requires the computation of the gradient of the following expressions2\nM2 = 2 n\u2211 i=1 \u03b1\u0303i \u2212 n\u2211 i,j=1 \u03b1\u0303i\u03b1\u0303jyiyj k\u0303(xi,xj) (11)\nand of\nR2 = n\u2211 i=1 \u03b2\u0303ik\u0303(xi,xi)\u2212 n\u2211 i,j=1 \u03b2\u0303i\u03b2\u0303j k\u0303(xi,xj) (12)\nwhere (\u03b1\u0303i)ni=1 and (\u03b2\u0303i) n i=1 are the optimal parameters of (8) and (10). The gradient of (11) depends on \u03b1\u0303i, which depends on p (similar comments hold for (12)). Chapelle et al. have proven that sinceM2 and R2 are computed via an optimization problem, the gradients of \u03b1\u0303i and \u03b2\u0303i do not enter into account in the computation of their gradients [37]. Hence, the gradient of (10) can be written as:\n\u2207T = [ \u2202T \u2202C , \u2202T \u2202\u03c321 , . . . , \u2202T \u2202\u03c32p\u0302c+1 ]t (13)\nwith\n\u2202T \u2202C = \u2202R2 \u2202C M2 +R2\u2202M 2 \u2202C (14)\nand\n\u2202T \u2202\u03c32` = \u2202R2 \u2202\u03c32` M2 +R2\u2202M 2 \u2202\u03c32` (15)\nfor ` \u2208 {1, . . . , p\u0302c + 1}. The derivatives of R2 are\n\u2202R2\n\u2202C =\n1\nC2 n\u2211 i=1 \u03b2\u0303i(\u03b2\u0303i \u2212 1) (16)\n\u2202R2 \u2202\u03c32` = \u2212 n\u2211 i,j=1 \u03b2\u0303i\u03b2\u0303j \u2202k\u0303(xi,xj) \u2202\u03c32` (17)\n2For simplicity, the parameter c of the kernel function is omitted, i.e., k\u0303(x, z|c) is written as k\u0303(x, z).\nwith\n\u2202k\u0303(xi,xj)\n\u2202\u03c32` = \u2016q\u0302tc`(xi \u2212 xj)\u20162 \u03c34` k\u0303(xi,xj) if ` \u2208 {1, . . . , p\u0302c} \u2016xi \u2212 xj\u20162\n\u03c34` k\u0303(xi,xj) if ` = p\u0302c + 1.\n(18)\nThe derivatives ofM2 are\n\u2202M2\n\u2202C =\n1\nC2 n\u2211 i=1 \u03b1\u0303i (19)\n\u2202M2\n\u2202\u03c32` = \u2212 n\u2211 i,j=1 \u03b1\u0303i\u03b1\u0303jyiyj \u2202k\u0303(xi,xj) \u2202\u03c32` . (20)\nOnce the derivatives have been computed, the optimization of T is done through a conventional gradient descent, following the framework in [37]. At each iteration t, the set of hyperparameters are updated as with a step proportional to the negative of the gradient of T :\npt+1 = pt \u2212 \u03b3\u2207T\nwhere \u03b3 \u2265 0 is a step size parameter. For implementation details, see [40]."}, {"heading": "4 Estimation of p\u0302c", "text": "The size of the signal subspace was estimated by the scree test of Cattell [41] using the same methodology as in [21]. The test consists in comparing the difference, \u2206i, between two consecutive eigenvalues \u03bbi and \u03bbi+1, \u2206i = \u03bbi \u2212 \u03bbi+1. When the differences \u2206i are below a user-defined threshold s for all i, i.e., \u2206j < s,\u2200j \u2208 {i, . . . , d \u2212 1}, p\u0302c is estimated as p\u0302c = i. In general the threshold is a percentage of the highest difference. Figure 3 shows an example on a simulated data set (see Section 5.1 for a description of the data). The correct value in that case is p = 10 but its estimate is p\u0302 = 14."}, {"heading": "5 Experimental results", "text": "Classification results are presented in this section. Regarding the multiclass strategy, the results must be considered as individual binary classification problems: No fusion rules were applied. For instance, in Table 4, the results for the class \u201cAsphalt\u201d should be read as \u201cAsphalt vs all\u201d. The reason for the use of this approach is the better interpretation of the results which is obtained because the results are not biased by the multiclass fusion strategy."}, {"heading": "5.1 Classification of simulated data following HDDA model", "text": "In this section, the proposed kernel, namely the HDDA-Mahalanobis Kernel (HDDA-MK), is used with the SVM in classification and evaluated on simulated data. The performances in terms of classification accuracy have been compared to a SVM with a conventional Gaussian kernel on the original data and on the data projected on the first principal axis of the considered classes, called the PCA-Mahalanobis kernel. The main difference between the HDDA-MK and the PCA-Mahalanobis kernel is that the PCAMahalanobis kernel discards the noise subspace while the HDDA-MK also exploits the noise subspace in order to improve the class discrimination. As previously stated, Gaussian kernel and PCA-Mahalanobis kernel correspond to extreme cases of HDDA-MK: \u03c32p\u0302 = +\u221e, \u2200p \u2208 {1, . . . , p\u0302Nc} or \u03c32p\u0302+1 = +\u221e.\nSimulated data were constructed using a linear mixture model [42]:\nx = Nc\u2211 i=1 \u03b1isi + b (21)\nwhere Nc is the number of classes, y = j such as \u03b1j = maxi \u03b1i, b \u223c N (0, \u03b52I) and si follows the HDDA model. The mean values of si were extracted from a spectral library provided with the ENVI software used in hyperspectral imagery [43]. The number of spectral variables d was set to 413 and pc was set to 10 for each class. The noise variance \u03b52 was adjusted to get a SNR = 1. Three experiments were run for a different number of classes, i.e., 2, 3 and 4 classes, respectively. Figure 4 presents two simulated spectra and their linear mixture. The number of training samples was 1000 and the number of testing samples was 1500. The experiment was repeated 50 times for each configuration. The hyperparameters were estimated using the radius margin bound, for each classifier. Since no difference in terms of classification accuracies were observed, we only report the results of the \u201c1 vs all\u201d classifier."}, {"heading": "5.1.1 Estimation of p\u0302c", "text": "With simulated data, it is possible to assess how the size of the intrinsic signal subspace is estimated. Figure 5 presents the boxplots of the estimations. After several tries, the threshold for the scree test was\nfixed to 10%. Fixing the threshold to a too high value would lead to underestimate p while a too low value would lead to drastically overestimate it.\nFrom the figure, the scree test overestimates the parameter p, for each configuration. The variance of the estimation is larger when the number of classes is increased while the bias of the estimation decreases. However, the error in estimating p is not too important with regards to the original size of the data (d = 413). Furthermore, in previous work [44], another criterion (the BIC [45]) was used to estimate the correct dimension of the subspace where the data live. The BIC criterion showed poor results when the number of training samples for single class nc was close to the dimension of the data (d \u2248 nc). From the experiments, the scree test is more robust in such a situation."}, {"heading": "5.1.2 Classification accuracies", "text": "The percentages of correct classification are reported in Figure 6. For the three experiments, the proposed kernel leads to the best results in terms of accuracies. Although p\u0302c was overestimated, it did not penalize the performances of the algorithm in terms of classification accuracies. For Nc = 2, the second best result is provided by the PCA-Mahalanobis kernel, while for Nc = 3 or 4 it is provided by the Gaussian kernel applied on the original data. For instance, for Nc = 4, the mean value of correct classification is 92.2% for the HDDA-Mahalanobis kernel, 91.3% for the conventional Gaussian kernel and only 76.3% for the PCA-Mahalanobis. The results confirm the poor generalization capability of the Mahalanobis kernel when dealing with high dimensional spaces. Although the conventional Gaussian kernel is less sensitive to the problem, the proposed kernel gives a significant improvement of the classification accuracy."}, {"heading": "5.2 Classification of Madelon data", "text": "Madelon data set is a simulated data set used for the NIPS Feature Selection Challenge3. It has 5 useful features, 15 redundant features and 480 random probes for a total of 500 features (d = 500). It is composed of two classes and the number of training samples is 2000 and the number of testing sample is 600. The threshold for the scree test has been set to 20%. The proposed kernel has been compared to the same kernels as in the previous section.\nThe classification results in terms of accuracies are reported in Table 2. The conventional Gaussian kernel performs badly on that data set with a global precision of 69,7% (random classifier would achieve 50%). The best accuracy is obtained for the proposed kernel with an average accuracy of 83.9%. The size p\u0302c of the signal subspace for the two classes was p\u03021 = 4 and p\u03022 = 4, respectively. The results obtained with the PCA-Mahalanobis kernel are worse than those obtained with the HDDA-Mahalanobis kernel. Thus, it confirms the pertinence to use both the signal and the noise subspace from the HDDA model in the classification."}, {"heading": "5.3 Classification of Arcene data set", "text": "Arcene data set is data set used for the NIPS Feature Selection Challenge. It has 7000 real variables, 3000 random probes for a total of 10000 features (d = 10000). It is composed of two classes. The number of available training samples is 100 and the number of test samples is 100. Therefore, the number of training samples is very small in comparison with the number of variables. About 50% of the data are non zero.\nThe classification accuracies are reported in Table 3. The selected threshold for the scree test has been set to 0.5%. The results obtained for other values of the threshold are also reported for comparison. The Gaussian kernel achieves a global accuracy of 80%. It performs quite well in terms of classification accuracies related to the dimension of the data. The PCA-Mahalanobis kernel performs worst whatever the threshold value. For the HDDA-Mahalanobis kernel, for the highest value of s the results are equal to those obtain with the Gaussian kernel. Then a slight increased of the accuracy is observed for s = 0.005. When the size of the signal subspace is too large (s = 0.0001 and (p\u03021, p\u03022) = (22, 23)), too many hyperparameters have to be estimated and thus the classification accuracy becomes low.\n3http://www.nipsfsc.ecs.soton.ac.uk/"}, {"heading": "5.4 Classification of real hyperspectral data", "text": "The data set considered in this experiment is the University Area of Pavia, Italy, acquired with the ROSIS03 sensor. The image has 103 spectral variables, i.e., each pixel is represented by a vector with 103 features (d=103) [46]. Nine classes have been defined by photo-interpretation as seen in first column of Table 4. Here, the threshold was set to 0.01%, because of a very high value of the first principal component (mainly due to the albedo).\nClassification results are reported in Table 4. The proposed kernel leads to an increase of the accuracy, compared to the conventional Gaussian kernel. However, for this data set, the PCA-Mahalanobis and HDDA-Mahalanobis kernel perform equally well in terms of accuracies, except for the classes meadow and bare soil.\nTo assess the influence of p\u0302c on the classification accuracies, the class meadow has been classified for a range of values of p\u0302c. The results are reported in the Figure 7 for the PCA-Mahalanobis and HDDAMahalanobis kernels. From the figure, the optimal p\u0302c is 11 which is close to the value selected with the scree test (p\u0302c = 10). The cumulative variance is 99.72% for p\u0302c = 10, 99.75% for p\u0302c = 11 and 99.77% for p\u0302c = 12. The proposed kernel is slightly influenced by the choice of p\u0302c for that class, compared to PCA-Mahalanobis kernel. In particular, the proposed kernel is robust if p\u0302c is underestimated. However, if p\u0302c is heavily overestimated, too many parameters would need to be estimated and it could degrade the training process."}, {"heading": "5.5 Analysis of the processing time", "text": "To assess the computational load of the proposed method, the processing time was computed for four data sets. The two data sets from the NIPS Feature Selection Challenge were used and well as the two first classes (Asphalt and Method) of the hyperspectral data set. The results are reported in Table 5. The program runs under Matlab on a two cores 2.67GHz laptop.\nFor the Arcene data set, the Gaussian kernel shows the lowest computational time. The computation of the first eigenvalues/eigenvectors is demanding since the data have 10000 features. It requires about 14 seconds. The optimization of the hyperparameters is fast since a few number of training samples are available. For the Madelon data sets, the computation of the firs eigenvalues/eigenvectors is fast, about 1.3 seconds while the optimization of the kernel hyperparameters is more demanding. For that data set, the HDDA-Mahalanobis is the slowest. Regarding the University data set, for the first two classes, the estimation of the first eigenvalues/eigenvectors is very fast, about 0.3 second. However, the estimation of the kernel hyperparameters is more demanding than with the two others data sets. For the Asphalt problem, the HDDA-Mahalanobis performs the fastest, while it is the slowest for the Meadow problem.\nFrom the above results, it is difficult to point out a clear winner in terms of processing time. From a computational viewpoint, the optimization of the Gaussian kernel is less demanding than PCA- or HDDA-Mahalanobis kernels. However, since the optimization problem is a gradient descent on a nonconvex function, a local minimum might be found sooner with one method and make it faster than the others. Nevertheless, the HDDA-Mahalanobis is usually more demanding in terms of processing time. Computational complexity of PCA- and HDDA-Mahalanobis can be assumed to be comparable in terms of processing time."}, {"heading": "6 Conclusions", "text": "In this paper, a novel kernel adapted to high dimensional data has been proposed. The parsimonious Mahalanobis kernel is based on the emptiness property of HD spaces. For each class, the original input space is split into a signal subspace and a noise subspace. Using this assumption, the inversion of the covariance matrix in the Mahalanobis kernel can be accurately computed. The proposed kernel was tested in a SVM framework for the purpose of classification. Experimental results on four data sets demonstrate the potential of the proposed kernel. In each case, the classification accuracy increased compared to the conventional Gaussian kernel and for three cases the proposed kernel showed superior results to simply map the data on the first PCA axes. Consequently, for HD data the HDDA-Mahalanobis kernel should be prefered.\nRegarding the computational load, the HDDA-Mahalanobis kernel is more demanding during the training process than the Gaussian kernel since more hyperparameters have to be estimated. Besides that, the HDDA-Mahalanobis kernel is efficient when the dimension of the input space is high. Hence, for moderate or small dimensions, conventional kernels should be preferred.\nIn the article, only classification is investigated. But other processing could also have been considered, e.g., regression [27]. The critical point is to be able to tune the hyperparameter efficiently, which it is feasible for regression. However, the actual optimization of the hyperparameters is demanding in terms of computations and it is sensitive to local minima. Therefore, a different strategy must be studied.\nPerspectives of this work concern the development of new kernels using the HDDA-model. For instance,\nit is possible to define a new dot product for the conventional polynomial kernel: k(x, z|c) = ( xt\u03a3\u22121c z + 1 )r . (22)\nFurthermore, a mixture of kernels using the HDDA could be used. From (5), the HDDA-Mahalanobis kernel can be considered as a product of several kernels. In the future, we will investigate the summation of kernels.\nFinally, a free-parameter alternative to the scree test for estimation of the intrinsic dimension must be addressed. For instance, an maximum likelihood estimator for HDDA exits and must be investigated [47]."}, {"heading": "Acknowledgment", "text": "The authors would like to thank the IAPR - TC7 for providing the data and Prof. Paolo Gamba and Prof. Fabio Dell\u2019Acqua of the University of Pavia, Italy, for providing reference data."}], "references": [{"title": "Clustering high-dimensional data: A survey on subspace clustering, pattern-based clustering, and correlation clustering", "author": ["H.-P. Kriegel", "P. Kr\u00f6ger", "A. Zimek"], "venue": "ACM Trans. Knowl. Discov. Data, vol. 3, Mar. 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "High-dimensional data analysis: the curses and blessing of dimensionality", "author": ["D.L. Donoho"], "venue": "AMS Mathematical challenges of the 21st century, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "A new approach to mixed pixel classification of hyperspectral imagery based on extended morphological profiles", "author": ["A. Plaza", "P. Martinez", "R. Perez", "J. Plaza"], "venue": "Pattern Recognition, vol. 37, no. 6, pp. 1097 \u2013 1116, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "A course in the geometry of n-dimensions", "author": ["M.G. Kendall"], "venue": "New York: Dover Publication,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1961}, {"title": "Supervised classification in high dimensional space: geometrical, statistical and asymptotical properties of multivariate data", "author": ["L. Jimenez", "D.A. Landgrebe"], "venue": "IEEE Trans. Syst., Man, Cybern. B, vol. 28, pp. 39\u201354, Feb. 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Searching for the embedded manifolds in high dimensional data, problems and unsolved questions", "author": ["J. Herault", "A. Guerin-Dugue", "P. Villemain"], "venue": "European Symposium on Artificial Neural Networks, pp. 1\u201312, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "On the surprising behavior of distance metrics in high dimensional space", "author": ["C. Aggarwal", "A. Hinneburg", "D. Keim"], "venue": "Database Theory \u2014 ICDT 2001, vol. 1973 of Lecture Notes in Computer Science, pp. 420\u2013434, Springer Berlin / Heidelberg, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "The concentration of fractional distances", "author": ["D. Francois", "V. Wertz", "M. Verleysen"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 19, pp. 873\u2013886, July 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Distance-preserving projection of high-dimensional data for nonlinear dimensionality reduction", "author": ["L. Yang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 26, pp. 1243 \u20131246, Sept. 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "On the effects of dimensionality on data analysis with neural networks", "author": ["M. Verleysen", "D. Francois", "G. Simon", "V. Wertz"], "venue": "Artificial Neural Nets Problem Solving Methods, vol. 2687 of Lecture Notes in Computer Science, pp. 1044\u20131044, Springer Berlin / Heidelberg, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning high dimensional data", "author": ["M. Verleysen"], "venue": "V. Piuri eds, IOS Press, Amsterdam (The Netherlands),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Adaptive control processes - A guided tour", "author": ["R.E. Bellman"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1961}, {"title": "Signal Theory Methods in Multispectral Remote Sensing", "author": ["D.A. Landgrebe"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Dimension reduction: A guided tour", "author": ["C.J.C. Burges"], "venue": "Foundations and Trends in Machine Learning, vol. 2, no. 4, pp. 275\u2013365, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1157\u20131182, Mar. 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Subspace clustering for high dimensional data: a review", "author": ["L. Parsons", "E. Haque", "H. Liu"], "venue": "SIGKDD Explor. Newsl., vol. 6, pp. 90\u2013105, June 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Probabilistic principal component analysis", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology), vol. 61, no. 3, pp. 611\u2013622, 1999.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Mixtures of probabilistic principal component analyzers", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Neural Computation, vol. 11, no. 2, pp. 443\u2013482, 1999.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Mixtures of factor analyzers with common factor loadings: Applications to the clustering and visualization of high-dimensional data", "author": ["J. Baek", "G.J. McLachlan", "L.K. Flack"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 1298 \u20131309, July 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "High-Dimensional Discriminant Analysis", "author": ["C. Bouveyron", "S. Girard", "C. Schmid"], "venue": "Communication in Statistics- Theory and Methods / Communications in Statistics Theory and Methods, vol. 36, p. 2607 \u2013 2623, Jan. 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "High-Dimensional Data Clustering", "author": ["C. Bouveyron", "S. Girard", "C. Schmid"], "venue": "Computational Statistics and Data Analysis, vol. 52, no. 1, pp. 502\u2013519, 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel methods in machine learning", "author": ["T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Annals of Statistics, vol. 36, no. 3, pp. 1171\u20131220, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "The curse of highly variable functions for local kernel machines", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "Advances in Neural Information Processing Systems 18 (NIPS\u201905) (Y. Weiss, B. Sch\u00f6lkopf, and J. Platt, eds.), pp. 107\u2013114, MIT Press, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Hyperspectral image classification with Mahalanobis relevance vector machines", "author": ["G. Camps-Valls", "A. Rodrigo-Gonzalez", "J. Muoz-Mari", "L. Gomez-Chova", "J. Calpe-Maravilla"], "venue": "Geoscience and Remote Sensing Symposium, 2007. IGARSS 2007. IEEE International, pp. 3802\u20133805, July 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Training of support vector machines with Mahalanobis kernels", "author": ["S. Abe"], "venue": "Artificial Neural Networks: Formal Models and Their Applications - ICANN 2005, Lecture Notes in Computer Science, pp. 571\u2013 576, Springer Berlin / Heidelberg, 2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Support vector regression using mahalanobis kernels", "author": ["Y. Kamada", "S. Abe"], "venue": "Artificial Neural Networks in Pattern Recognition (F. Schwenker and S. Marinai, eds.), vol. 4087 of Lecture Notes in Computer Science, pp. 144\u2013152, Springer Berlin / Heidelberg, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Weighted mahalanobis distance kernels for support vector machines", "author": ["D. Wang", "D. Yeung", "E. Tsang"], "venue": "Neural Networks, IEEE Transactions on, vol. 18, pp. 1453 \u20131462, sept. 2007. 16", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"], "venue": "Pattern Recognition, vol. 41, no. 1, pp. 176 \u2013 190, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Geometry and Invariance in Kernel Based Methods In Advances in Kernel Methods - Support Vector Learning", "author": ["B. Scholkopf", "C. Burges", "A. Smola"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "A geometrical method to improve performance of the vector machine", "author": ["P. Williams", "S. Li", "J. Feng", "S. Wu"], "venue": "IEEE Trans. Neural Netw., vol. 18, pp. 942\u2013947, May 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "A tutorial on support vector machines for pattern recognition", "author": ["C.J. Burges"], "venue": "Data Mining and Knowledge Discovery, vol. 2, pp. 121\u2013167, 1998.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Svm-based feature extraction for face recognition", "author": ["S.-K. Kim", "Y.J. Park", "K.-A. Toh", "S. Lee"], "venue": "Pattern Recognition, vol. 43, no. 8, pp. 2871 \u2013 2881, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "A spatial-spectral kernel-based approach for the classification of remote-sensing images", "author": ["M. Fauvel", "J. Chanussot", "J.A. Benediktsson"], "venue": "Pattern Recogn., vol. 45, pp. 381\u2013392, Jan. 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "An Introduction to Support Vector Machines and Other Kernelbased Learning Methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2000}, {"title": "The Nature of Statistical Learning Theory, Second Edition", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1999}, {"title": "Choosing multiple parameters for support vector machines", "author": ["O. Chapelle", "V. Vapnik", "O. Bousquet", "S. Mukherjee"], "venue": "Machine Learning, vol. 46, pp. 131\u2013159, 2002.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient tuning of svm hyperparameters using radius/margin bound and iterative algorithms", "author": ["S.S. Keerthi"], "venue": "IEEE Trans. Neural Netw., vol. 13, pp. 1225 \u2013 1229, Sept. 2002.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2002}, {"title": "A comparison of methods for multiclass support vector machines", "author": ["H.C.-W.", "L.C.-J."], "venue": "IEEE Trans. Neural Netw., vol. 13, pp. 415 \u2013425, Mar. 2002.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2002}, {"title": "Selection of kernel parameters.", "author": ["O. Chapelle"], "venue": "http://olivier.chapelle.cc/ams/,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "The Scree Test For The Number Of Factors", "author": ["R.B. Cattell"], "venue": "Multivariate Behavioral Research, vol. 1, no. 2, pp. 245\u2013276, 1966.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1966}, {"title": "Spectral unmixing", "author": ["N. Keshava", "J. Mustard"], "venue": "Signal Processing Magazine, IEEE, vol. 19, pp. 44 \u201357, Jan. 2002.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2002}, {"title": "Mahalanobis kernel for the classification of hyperspectral images", "author": ["M. Fauvel", "A. Villa", "J. Chanussot", "J. Benediktsson"], "venue": "Geoscience and Remote Sensing Symposium (IGARSS), 2010 IEEE International, pp. 3724 \u20133727, July 2010.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The annals of Statistics, vol. 6, no. 2, pp. 461\u2013464, 1978.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1978}, {"title": "Statistical pattern recognition in remote sensing", "author": ["C.H. Chen", "P.-G.P. Ho"], "venue": "Pattern Recognition, vol. 41, no. 9, pp. 2731 \u2013 2741, 2008.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Intrinsic dimension estimation by maximum likelihood in isotropic probabilistic pca", "author": ["C. Bouveyron", "G. Celeux", "S. Girard"], "venue": "Pattern Recognition Letters, vol. 32, no. 14, pp. 1706 \u2013 1713, 2011. 17", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": ") [1, 2, 3].", "startOffset": 2, "endOffset": 11}, {"referenceID": 1, "context": ") [1, 2, 3].", "startOffset": 2, "endOffset": 11}, {"referenceID": 2, "context": ") [1, 2, 3].", "startOffset": 2, "endOffset": 11}, {"referenceID": 1, "context": "Actually, HD data pose critical theoretical and practical problems that need to be addressed specifically [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 3, "context": "Most of them do not behave in a similar way as in three dimensional Euclidean spaces (Table 1 summarizes the main properties of HD spaces) [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "For instance, samples following a uniform law will have a tendency to have a high concentration in the corners [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "The same property holds for normally distributed data: samples tend to have a high concentration in the tails [6], making density estimation a difficult task.", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "Unfortunately, discriminative methods also suffer if the dimensionality is high, due to the \u201cconcentration of measure phenomenon\u201d [2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 6, "context": "In HD spaces, samples tend to be equally distant from each other [7].", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": ") is affected by this phenomenon [8].", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "Therefore, every method based on the distance between samples [9] (SVM with Gaussian kernel, neural network, Nearest Neighbors, Locally Linear Embedding.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": ") are potentially affected by this phenomenon [10, 11].", "startOffset": 46, "endOffset": 54}, {"referenceID": 10, "context": ") are potentially affected by this phenomenon [10, 11].", "startOffset": 46, "endOffset": 54}, {"referenceID": 11, "context": "An additional property, for which the consequences are more practical than theoretical, is the \u201cempty space phenomenon\u201d [12]: In HD spaces, the available samples usually fill a very small part of the space.", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "Bellman [12], refers to the aforementioned problems of HD data and reflects how processing HD data is difficult.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "Donoho has noticed [2], there is also a \u201cBlessing of dimensionality\u201d: For instance in classification, the class separability is improved when the dimensionality of the data increases.", "startOffset": 19, "endOffset": 22}, {"referenceID": 12, "context": "Consider for example a comparison between hyperspectral (hundreds of spectral wavelengths) and multispectral (tens of spectral wavelengths) remote sensing images[13].", "startOffset": 161, "endOffset": 165}, {"referenceID": 4, "context": "However, if conventional methods are used, the additional information contained in hyperspectral images will not lead to an increase of the classification accuracy [5].", "startOffset": 164, "endOffset": 167}, {"referenceID": 13, "context": "Recent overviews of DR can be found in [14, 15, 16].", "startOffset": 39, "endOffset": 51}, {"referenceID": 14, "context": "Recent overviews of DR can be found in [14, 15, 16].", "startOffset": 39, "endOffset": 51}, {"referenceID": 15, "context": ", the subspace models [17].", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "For instance, the Probabilistic Principal Component Analysis (PPCA) [18] assumes that the classes are normally distributed in a lower dimensional subspace and are linearly embedded in the original subspace with additive white noise.", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "Such models exploit the empty space property of HD data without discarding any dimension of the data [19, 20].", "startOffset": 101, "endOffset": 109}, {"referenceID": 18, "context": "Such models exploit the empty space property of HD data without discarding any dimension of the data [19, 20].", "startOffset": 101, "endOffset": 109}, {"referenceID": 19, "context": "[21, 22].", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[21, 22].", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "Conversely, kernel based methods do not reduce the dimensionality but rather work with the full HD data [23].", "startOffset": 104, "endOffset": 108}, {"referenceID": 22, "context": "However, local kernel methods are sensitive to the size of the dimensionality [24].", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "Previous works on the Mahalanobis kernel [25, 26, 27, 28] were limited by the effect of dimensionality on the matrix inversion.", "startOffset": 41, "endOffset": 57}, {"referenceID": 24, "context": "Previous works on the Mahalanobis kernel [25, 26, 27, 28] were limited by the effect of dimensionality on the matrix inversion.", "startOffset": 41, "endOffset": 57}, {"referenceID": 25, "context": "Previous works on the Mahalanobis kernel [25, 26, 27, 28] were limited by the effect of dimensionality on the matrix inversion.", "startOffset": 41, "endOffset": 57}, {"referenceID": 26, "context": "Previous works on the Mahalanobis kernel [25, 26, 27, 28] were limited by the effect of dimensionality on the matrix inversion.", "startOffset": 41, "endOffset": 57}, {"referenceID": 23, "context": "In [25], the covariance matrix was computed on the whole training set.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Diagonal and full covariance matrices were investigated in [26] for the purpose of classification and in [27] for the purpose of regression.", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "Diagonal and full covariance matrices were investigated in [26] for the purpose of classification and in [27] for the purpose of regression.", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "Interested readers can find a detailed presentation of HDDA in [21, 22].", "startOffset": 63, "endOffset": 71}, {"referenceID": 20, "context": "Interested readers can find a detailed presentation of HDDA in [21, 22].", "startOffset": 63, "endOffset": 71}, {"referenceID": 19, "context": ",pc and bc can be computed from the sample covariance matrix [21]:", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "Refers to [aijbiQidi] in [21, 22].", "startOffset": 25, "endOffset": 33}, {"referenceID": 20, "context": "Refers to [aijbiQidi] in [21, 22].", "startOffset": 25, "endOffset": 33}, {"referenceID": 21, "context": "3 Geometry of the induced feature space Working with a kernel function is equivalent to work with samples mapped onto a feature space H, where the dot product is equivalent to the kernel evaluation in the input space [23, 29]: k(x, z) = \u3008\u03c6(x), \u03c6(z)\u3009H, \u03c6 being the feature map.", "startOffset": 217, "endOffset": 225}, {"referenceID": 27, "context": "3 Geometry of the induced feature space Working with a kernel function is equivalent to work with samples mapped onto a feature space H, where the dot product is equivalent to the kernel evaluation in the input space [23, 29]: k(x, z) = \u3008\u03c6(x), \u03c6(z)\u3009H, \u03c6 being the feature map.", "startOffset": 217, "endOffset": 225}, {"referenceID": 28, "context": "Under some weak conditions, the projected samples in the feature space live on a Riemannian manifold [30, 31].", "startOffset": 101, "endOffset": 109}, {"referenceID": 29, "context": "Under some weak conditions, the projected samples in the feature space live on a Riemannian manifold [30, 31].", "startOffset": 101, "endOffset": 109}, {"referenceID": 30, "context": "Support vector machines (SVM) is a standard classification kernel methods [32].", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "It has shown to performs very well on several data sets from moderate dimension to high dimensional data [33, 34].", "startOffset": 105, "endOffset": 113}, {"referenceID": 32, "context": "It has shown to performs very well on several data sets from moderate dimension to high dimensional data [33, 34].", "startOffset": 105, "endOffset": 113}, {"referenceID": 30, "context": "In the following section, the main results are presented but interested readers could see references [32, 35, 36] for further mathematical details about the SVM framework.", "startOffset": 101, "endOffset": 113}, {"referenceID": 33, "context": "In the following section, the main results are presented but interested readers could see references [32, 35, 36] for further mathematical details about the SVM framework.", "startOffset": 101, "endOffset": 113}, {"referenceID": 34, "context": "In the following section, the main results are presented but interested readers could see references [32, 35, 36] for further mathematical details about the SVM framework.", "startOffset": 101, "endOffset": 113}, {"referenceID": 33, "context": "1 L2-Support Vector Machines The L2-SVM is considered in this work rather than the conventional L1-SVM [35]: With L2-SVM it is possible to tune the hyperparameters automatically by optimizing the so called radius-margin bound [37].", "startOffset": 103, "endOffset": 107}, {"referenceID": 35, "context": "1 L2-Support Vector Machines The L2-SVM is considered in this work rather than the conventional L1-SVM [35]: With L2-SVM it is possible to tune the hyperparameters automatically by optimizing the so called radius-margin bound [37].", "startOffset": 226, "endOffset": 230}, {"referenceID": 33, "context": "The L2-SVM solves the conventional L1-SVM optimization problem with a quadratic penalization of errors [35].", "startOffset": 103, "endOffset": 107}, {"referenceID": 34, "context": "An estimate of the generalization errors is given by an upper bound on the number of errors of the leave-one-out procedure, the radius-margin bound T [36]: T (p) := RM.", "startOffset": 150, "endOffset": 154}, {"referenceID": 34, "context": "R2 is obtained by the optimal objective function of the following constraint optimization problem [36]:", "startOffset": 98, "endOffset": 102}, {"referenceID": 35, "context": "[37], followed later by S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "Keerthi [38], have proposed an algorithm based on gradient optimization method.", "startOffset": 8, "endOffset": 12}, {"referenceID": 37, "context": "Indeed, for a multiclass problem, the \u201cone vs one\u201d approach must not be used and the \u201cone vs all\u201d approach should be preferred [39].", "startOffset": 127, "endOffset": 131}, {"referenceID": 35, "context": "have proven that sinceM2 and R2 are computed via an optimization problem, the gradients of \u03b1\u0303i and \u03b2\u0303i do not enter into account in the computation of their gradients [37].", "startOffset": 167, "endOffset": 171}, {"referenceID": 35, "context": "Once the derivatives have been computed, the optimization of T is done through a conventional gradient descent, following the framework in [37].", "startOffset": 139, "endOffset": 143}, {"referenceID": 38, "context": "For implementation details, see [40].", "startOffset": 32, "endOffset": 36}, {"referenceID": 39, "context": "4 Estimation of p\u0302c The size of the signal subspace was estimated by the scree test of Cattell [41] using the same methodology as in [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 19, "context": "4 Estimation of p\u0302c The size of the signal subspace was estimated by the scree test of Cattell [41] using the same methodology as in [21].", "startOffset": 133, "endOffset": 137}, {"referenceID": 40, "context": "Simulated data were constructed using a linear mixture model [42]:", "startOffset": 61, "endOffset": 65}, {"referenceID": 41, "context": "Furthermore, in previous work [44], another criterion (the BIC [45]) was used to estimate the correct dimension of the subspace where the data live.", "startOffset": 30, "endOffset": 34}, {"referenceID": 42, "context": "Furthermore, in previous work [44], another criterion (the BIC [45]) was used to estimate the correct dimension of the subspace where the data live.", "startOffset": 63, "endOffset": 67}, {"referenceID": 43, "context": ", each pixel is represented by a vector with 103 features (d=103) [46].", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": ", regression [27].", "startOffset": 13, "endOffset": 17}, {"referenceID": 44, "context": "For instance, an maximum likelihood estimator for HDDA exits and must be investigated [47].", "startOffset": 86, "endOffset": 90}], "year": 2012, "abstractText": "The classification of high dimensional data with kernel methods is considered in this article. Exploiting the emptiness property of high dimensional spaces, a kernel based on the Mahalanobis distance is proposed. The computation of the Mahalanobis distance requires the inversion of a covariance matrix. In high dimensional spaces, the estimated covariance matrix is ill-conditioned and its inversion is unstable or impossible. Using a parsimonious statistical model, namely the High Dimensional Discriminant Analysis model, the specific signal and noise subspaces are estimated for each considered class making the inverse of the class specific covariance matrix explicit and stable, leading to the definition of a parsimonious Mahalanobis kernel. A SVM based framework is used for selecting the hyperparameters of the parsimonious Mahalanobis kernel by optimizing the so-called radius-margin bound. Experimental results on three high dimensional data sets show that the proposed kernel is suitable for classifying high dimensional data, providing better classification accuracies than the conventional Gaussian kernel.", "creator": "LaTeX with hyperref package"}}}