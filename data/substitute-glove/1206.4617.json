{"id": "1206.4617", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Continuous Inverse Optimal Control with Locally Optimal Examples", "abstract": "Inverse determining control, the based as orthogonal appropriate course, is with problem of recovers similar unknown ,000 integral started after Markov action work from expert crowds has made optimal dealing. We implement a simulation formula_3 depending control algorithm called scales madly some task pulsation, and now accommodation for and, continuous domains, sometimes functionality entire full stance comes impractical. By using a organized correlation result held bribes function, things method must once drop on assumption that entire pro- are globally optimal, requiring turn including optimality. This allows it instead learn during generally that are unsuitable first prior mathematical.", "histories": [["v1", "Mon, 18 Jun 2012 15:02:28 GMT  (1793kb)", "http://arxiv.org/abs/1206.4617v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["sergey levine", "vladlen koltun"], "accepted": true, "id": "1206.4617"}, "pdf": {"name": "1206.4617.pdf", "metadata": {"source": "META", "title": "Continuous Inverse Optimal Control with Locally Optimal Examples", "authors": ["Sergey Levine", "Vladlen Koltun"], "emails": ["svlevine@stanford.edu", "vladlen@stanford.edu"], "sections": [{"heading": "1. Introduction", "text": "Algorithms for inverse optimal control (IOC), also known as inverse reinforcement learning (IRL), recover an unknown reward function in a Markov decision process (MDP) from expert demonstrations of the corresponding policy. This reward function can be used to perform apprenticeship learning, generalize the expert\u2019s behavior to new situations, or infer the expert\u2019s goals (Ng & Russell, 2000). Performing IOC in continuous, high-dimensional domains is challenging, because IOC algorithms are usually much more computationally demanding than the corresponding \u201cforward\u201d control methods. In this paper, we present an IOC algorithm that efficiently handles deterministic MDPs with large, continuous state and action spaces by considering only the shape of the learned reward function in the neighborhood of the expert\u2019s demonstrations.\nSince our method only considers the shape of the reward function around the expert\u2019s examples, it does\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nnot integrate global information about the reward along alternative paths. This is analogous to trajectory optimization methods, which solve the forward control problem by finding a local optimum. However, while the lack of global optimality is a disadvantage for solving the forward problem, it can actually be advantageous in IOC. This is because it removes the assumption that the expert demonstrations are globally optimal, thus allowing our algorithm to use examples that only exhibit local optimality. For complex tasks, human experts might find it easier to provide such locally optimal examples. For instance, a skilled driver might execute every turn perfectly, but still take a globally suboptimal route to the destination.\nOur algorithm optimizes the approximate likelihood of the expert trajectories under a parameterized reward. The approximation assumes that the expert\u2019s trajectory lies near a peak of this likelihood, and the resulting optimization finds a reward function under which this peak is most prominent. Since this approach only considers the shape of the reward around the examples, it does not require the examples to be globally optimal, and remains efficient even in high dimensions. We present two variants of our algorithm that learn the reward either as a linear combination of the provided features, as is common in prior work, or as a nonlinear function of the features, as in a number of recent methods (Ratliff et al., 2009; Levine et al., 2010; 2011)."}, {"heading": "2. Related Work", "text": "Most prior IOC methods solve the entire forward control problem in the inner loop of an iterative procedure (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart, 2010). Such methods often use an arbitrary, possibly approximate forward solver, but this solver must be used numerous times during the learning process, making reward learning significantly more costly than the forward problem. Dvijotham and Todorov avoid repeated calls to a forward solver by directly learning a value function (Dvijotham & Todorov, 2010). However, this requires value function bases to impose\nstructure on the solution, instead of the more common reward bases. Good value function bases are difficult to construct and are not portable across domains. By only considering the reward around the expert\u2019s trajectories, our method removes the need to repeatedly solve a difficult forward problem, without losing the ability to utilize informative reward features.\nMore efficient IOC algorithms have been proposed for the special case of linear dynamics and quadratic rewards (LQR) (Boyd et al., 1994; Ziebart, 2010). However, unlike in the forward case, LQR approaches are difficult to generalize to arbitrary inverse problems, because learning a quadratic reward matrix around an example path does not readily generalize to other states in a non-LQR task. Because of this, such methods have only been applied to tasks that conform to the LQR assumptions (Ziebart, 2010). Our method also uses a quadratic expansion of the reward function, but instead of learning the values of a quadratic reward matrix directly, it learns a general parameterized reward using its Hessian and gradient. As we show in Section 5, a particularly efficient variant of our algorithm can be derived when the dynamics are linearized, and this derivation can in fact follow from standard LQR assumptions. However, this approximation is not required, and the general form of our algorithm does not assume linearized dynamics.\nMost previous methods also assume that the expert demonstrations are globally optimal or near-optimal. Although this makes the examples more informative insofar as the learning algorithm can extract relevant global information, it also makes such methods unsuitable for learning from examples that are only locally optimal. As shown in the evaluation, our method can learn rewards even from locally optimal examples."}, {"heading": "3. Background", "text": "We address deterministic, fixed-horizon control tasks with continuous states x = (x1, . . . ,xT )\nT, continuous actions u = (u1, . . . ,uT )\nT, and discrete time. Such tasks are characterized by a dynamics function F , which we define as\nF(xt\u22121,ut) = xt,\nas well as a reward function r(xt,ut). Given the initial state x0, the optimal actions are given by\nu = arg max u \u2211 t r(xt,ut).\nIOC aims to find a reward function r under which the optimal actions match the expert\u2019s demonstrations,\ngiven by D = {(x(1)0 ,u(1)), . . . , (x (n) 0 ,u (n))}. The algorithm might also be presented with reward features f : (xt,ut)\u2192 R that can be used to represent the unknown reward r. Unfortunately, real demonstrations are rarely perfectly optimal, so we require a model for the expert\u2019s behavior that can explain suboptimality or \u201cnoise.\u201d We employ the maximum entropy IRL (MaxEnt) model (Ziebart et al., 2008), which is closely related to linearly-solvable MDPs (Dvijotham & Todorov, 2010). Under this model, the probability of the actions u is proportional to the exponential of the rewards encountered along the trajectory:1\nP (u|x0) = 1\nZ exp (\u2211 t r(xt,ut) ) , (1)\nwhere Z is the partition function. Under this model, the expert follows a stochastic policy that becomes more deterministic when the stakes are high, and more random when all choices have similar value. In prior work, the log likelihood derived from Equation 1 was maximized directly. However, computing the partition function Z requires finding the complete policy under the current reward, using a variant of value iteration (Ziebart, 2010). In high dimensional spaces, this becomes intractable, since this computation scales exponentially with the dimensionality of the state space.\nIn the following sections, we present an approximation to Equation 1 that admits efficient learning in high dimensional, continuous domains. In addition to breaking the exponential dependence on dimensionality, this approximation removes the requirement that the example trajectories be globally optimal, and only requires approximate local optimality. An example of a locally optimal but globally suboptimal trajectory is shown in Figure 1: although another path has a higher total reward, any local perturbation of the trajectory decreases the reward total.\n1In the case of multiple example trajectories, their probabilities are simply multiplied together to obtain P (D)."}, {"heading": "4. IOC with Locally Optimal Examples", "text": "To evaluate Equation 1 without computing the partition function Z, we apply the Laplace approximation, which locally models the distribution as a Gaussian (Tierney & Kadane, 1986). Note that this is not equivalent to modeling the reward function itself as a Gaussian, since Equation 1 uses the sum of the rewards along a path. In the context of IOC, this corresponds to assuming that the expert performs a local optimization when choosing the actions u, rather than global planning. This assumption is strictly less restrictive than the assumption of global optimality.\nUsing r(u) to denote the sum of rewards along path (x0,u), we can write Equation 1 as\nP (u|x0) = er(u) [\u222b er(u\u0303)du\u0303 ]\u22121 .\nWe approximate this probability using a second order Taylor expansion of r around u:\nr(u\u0303) \u2248 r(u) + (u\u0303\u2212 u)T \u2202r \u2202u + 1 2 (u\u0303\u2212 u)T \u2202\n2r\n\u2202u2 (u\u0303\u2212 u).\nDenoting the gradient \u2202r\u2202u as g and the Hessian \u22022r \u2202u2 as H, the approximation to Equation 1 is given by P (u|x0) \u2248 er(u) [\u222b er(u)+(u\u0303\u2212u) Tg+ 12 (u\u0303\u2212u) TH(u\u0303\u2212u)du\u0303\n]\u22121 = [\u222b e\u2212 1 2g TH\u22121g+ 12 (H(u\u0303\u2212u)+g) TH\u22121(H(u\u0303\u2212u)+g)du\u0303\n]\u22121 = e 1 2g TH\u22121g |\u2212H| 1 2 (2\u03c0)\u2212 du 2 ,\nfrom which we obtain the approximate log likelihood\nL = 1 2 gTH\u22121g + 1 2 log |\u2212H| \u2212 du 2 log 2\u03c0. (2)\nIntuitively, this likelihood indicates that reward functions under which the example paths have small gradients and large negative Hessians are more likely. The magnitude of the gradient corresponds to how close the example is to a local peak in the (total) reward landscape, while the Hessian describes how steep this peak is. For a given parameterization of the reward, we can learn the most likely parameters by maximizing Equation 2. In the next section, we discuss how this objective and its gradients can be computed efficiently."}, {"heading": "5. Efficient Likelihood Optimization", "text": "We can optimize Equation 2 directly with any optimization method. The computation is dominated by the linear system H\u22121g, so the cost is cubic in the\npath length T and the action dimensionality. We will describe two approximate algorithms that evaluate the likelihood in time linear in T by linearizing the dynamics. This greatly speeds up the method on longer examples, though it should be noted that modern linear solvers are well optimized for symmetric matrices such as H, making it quite feasible to evaluate the likelihood without linearization for moderate length paths.\nTo derive the approximate linear-time solution to H\u22121g, we first express g and H in terms of the derivatives of r with respect to x and u individually:\ng = \u2202r\n\u2202u\ufe38\ufe37\ufe37\ufe38 g\u0303\n+ \u2202x\n\u2202u\nT \ufe38 \ufe37\ufe37 \ufe38 J \u2202r \u2202x\ufe38\ufe37\ufe37\ufe38 g\u0302 ,\nH = \u22022r\n\u2202u2\ufe38\ufe37\ufe37\ufe38 H\u0303\n+ \u2202x\n\u2202u\nT \ufe38 \ufe37\ufe37 \ufe38 J \u22022r \u2202x2\ufe38\ufe37\ufe37\ufe38 H\u0302 \u2202x \u2202u\ufe38\ufe37\ufe37\ufe38 JT + \u22022x \u2202u2\ufe38\ufe37\ufe37\ufe38 H\u0306 \u2202r \u2202x\ufe38\ufe37\ufe37\ufe38 g\u0302 .\nSince r(xt,ut) depends only on the state and action at time t, H\u0302 and H\u0303 are block diagonal, with T blocks. To build the Jacobian J, we differentiate the dynamics:\n\u2202F \u2202ut (xt\u22121,ut) = \u2202xt \u2202ut = Bt,\n\u2202F \u2202xt\u22121 (xt\u22121,ut) = \u2202xt \u2202xt\u22121 = At.\nFuture actions do not influence past states, so J is block upper triangular. Using the Markov property, we can express the nonzero blocks recursively:\nJt1,t2 = \u2202xt1 \u2202ut2\nT\n=\n{ BTt1 , t1 = t2\nJt1,t1\u22121A T t1 , t1 > t2\nWe can now write g and H almost entirely in terms of matrices that are block diagonal or block triangular. Unfortunately, the final second order term H\u0306 does not exhibit such convenient structure. In particular, the Hessian of the last state xT with respect to the actions u can be arbitrarily dense. We will therefore disregard this term. Since H\u0306 is zero only when the dynamics are linear, this corresponds to linearizing the dynamics."}, {"heading": "5.1. Direct Likelihood Evaluation", "text": "We first describe an approach for directly evaluating the likelihood under the assumption that H\u0306 is zero. We first exploit the structure of J to evaluate Jg\u0302 in time linear in T , which is essential for computing g. This requires a simple recursion from t = T to 1:\n[Jg\u0302]t \u2190 BTt (g\u0302t + z(t)g ) z(t+1)g \u2190 ATt (g\u0302t + z(t)g ), (3)\nwhere zg accumulates the product of g\u0302 with the offdiagonal elements of J. The linear system h = H\u22121g\ncan be solved with a stylistically similar recursion. We first use the assumption that H\u0306 is zero to factor H:\nH = (H\u0303P + JH\u0302)JT,\nwhere PJT = I. The nonzero blocks of P are\nPt,t = B \u2020 t Pt,t\u22121 = \u2212B \u2020 tAt,\nand B\u2020t is a pseudoinverse of the potentially nonsquare matrix Bt. This linear system is solved in two passes: an upward pass to solve (H\u0303P + JH\u0302)h\u0304 = g, and a downward pass to solve JTh = h\u0304. Each pass is a block generalization of forward or back substitution and, like the recursion in Equation 3, can exploit the structure of J to run in time linear in T . However, the upward pass must also handle the off-diagonal entries of P and the potentially nonsquare blocks of J, which are not invertible. We therefore only construct a partial solution on the upward pass, with each h\u0304t expressed in terms of h\u0304t\u22121. The final values are reconstructed on the downward pass, together with the solution h. The complete algorithm computes h = H\u22121g and the determinant |\u2212H| in time linear in T , and is included in Appendix A of the supplement.\nFor a given parameterization of the reward, we determine the most likely parameters by maximizing the likelihood with gradient-based optimization (LBFGS in our implementation). This requires the gradient of Equation 2. For a reward parameter \u03b8, the gradient is\n\u2202L \u2202\u03b8 = hT \u2202g \u2202\u03b8 \u2212 1 2 hT \u2202H \u2202\u03b8 h + 1 2 tr\n( H\u22121 \u2202H\n\u2202\u03b8\n) .\nAs before, the gradients of g and H can be expressed in terms of the derivatives of r at each time step, which allows us to rewrite the gradient as\n\u2202L \u2202\u03b8 = \u2211 ti \u2202g\u0303ti \u2202\u03b8 hti + \u2211 ti \u2202g\u0302ti \u2202\u03b8 [JTh]ti+ (4)\n1\n2 \u2211 tij \u2202H\u0303tij \u2202\u03b8 ( [H\u22121]ttij \u2212 htihtj ) +\n1\n2 \u2211 tij \u2202H\u0302tij \u2202\u03b8 ( [JTH\u22121J]ttij \u2212 [JTh]ti[JTh]tj ) +\n1\n2 \u2211 ti \u2202g\u0302ti \u2202\u03b8 \u2211 t1t2jk ( [H\u22121]t1t2jk \u2212 ht1jht2k ) H\u0306t1t2jkti,\nwhere [H\u22121]ttij denotes the ij th entry in the block tt. The last sum vanishes if H\u0306 is zero, so all quantities can be computed in time linear in T . The diagonal blocks of H\u22121 and JTH\u22121J can be computed while solving for h = H\u22121g, as shown in Appendix A of the supplement. To find the gradients for any reward parameterization, we can compute the gradients of g\u0302, g\u0303, H\u0302, and H\u0303, and then apply the above equation."}, {"heading": "5.2. LQR-Based Likelihood Evaluation", "text": "While the approximate likelihood in Equation 2 makes no assumptions about the dynamics of the MDP, the algorithm in Section 5.1 linearizes the dynamics around the examples. This matches the assumptions of the commonly studied linear-quadratic regulator (LQR) setting, and suggests an alternative derivation of the algorithm as IOC in a linear-quadratic system, with linearized dynamics given by At and Bt, quadratic reward matrices given by the diagonal blocks of H\u0302 and H\u0303, and linear reward vectors given by g\u0302 and g\u0303. A complete derivation of the resulting algorithm is presented in Appendix B of the supplement, and is similar to the MaxEnt LQR IOC algorithm described by Ziebart (Ziebart, 2010), with an addition recursion to compute the derivatives of the parameterized reward Hessians. Since the gradients are computed recursively, this method lacks the convenient form provided by Equation 4, but may be easier to implement."}, {"heading": "6. Algorithms for IOC with Locally Optimal Examples", "text": "We can use the objective in Equation 2 to learn reward functions with a variety of representations. We present one variant that learns the reward as a linear combination of features, and a second variant that uses a Gaussian process to learn nonlinear reward functions."}, {"heading": "6.1. Learning Linear Reward Functions", "text": "In the linear variant, the algorithm is provided with features f that depend on the state xt and action ut. The reward is given by r(xt,ut) = \u03b8\nTf(xt,ut), and the weights \u03b8 are learned. Letting g\u0303(k), g\u0302(k), H\u0303(k), and H\u0302(k) denote the gradients and Hessians of each feature with respect to actions and states, the full gradients and Hessians are sums of these quantities, weighted by \u03b8k \u2013 e.g., g\u0303 = \u2211 k g\u0303\n(k)\u03b8k. The gradient of g\u0303 with respect to \u03b8k is simply g\u0303\n(k), and the gradients of the other matrices are given analogously. The likelihood gradient is then obtained from Equation 4.\nWhen evaluating Equation 2, the log determinant of the negative Hessian is undefined when the determinant is not positive. This corresponds to the example path lying in a valley rather than on a peak of the energy landscape. A high-probability reward function will avoid such cases, but it is nontrivial to find an initial point for which the objective can be evaluated. We therefore add a dummy regularizer feature that ensures that the negative Hessian has a positive determinant. This feature has a gradient that is uniformly zero, and a Hessian equal to the negative identity.\nThe initial weight \u03b8r on this feature must be set such that the negative Hessians of all example paths are positive definite. We can find a suitable weight simply by doubling \u03b8r until this requirement is met. During the optimization, we must drive \u03b8r to zero in order to solve the original problem. In this way, \u03b8r has the role of a relaxation, allowing the algorithm to explore the parameter space without requiring the Hessian to always be negative definite. Unfortunately, driving \u03b8r to zero too quickly can create numerical instability, as the Hessians become ill-conditioned or singular. Rather than simply penalizing the regularizing weight, we found that we can maintain numerical stability and still obtain a solution with \u03b8r = 0 by using the Augmented Lagrangian method (Birgin & Mart\u0301\u0131nez, 2009). This method solves a sequence of maximization problems that are augmented by a penalty term of the form\n\u03c6(j)(\u03b8r) = \u2212 1\n2 \u00b5(j)\u03b82r + \u03bb (j)\u03b8r,\nwhere \u00b5(j) is a penalty weight, and \u03bb(j) is an estimate of the Lagrange multiplier for the constraint \u03b8r = 0. After each optimization, \u00b5(j+1) is increased by a factor of 10 if \u03b8r has not decreased, and \u03bb (j+1) is set to\n\u03bb(j+1) \u2190 \u03bb(j) \u2212 \u00b5(j)\u03b8r.\nThis approach allows \u03b8r to decrease gradually with each optimization without using large penalty terms."}, {"heading": "6.2. Learning Nonlinear Reward Functions", "text": "In the nonlinear variant of our algorithm, we represent the reward function as a Gaussian process (GP) that maps from feature values to rewards, as proposed by Levine et al. (Levine et al., 2011). The inputs of the Gaussian process are a set of inducing feature points F = [f1 . . . fn]T, and the noiseless outputs y at these points are learned. The location of the inducing points can be chosen in a variety of ways, but we follow Levine et al. and choose the points that lie on the example paths, which concentrates the learning on the regions where the examples are most informative. In addition to the outputs y, we also learn the hyperparameters \u03bb and \u03b2 that describe the GP kernel function, given by\nk(f i, f j) = \u03b2 exp ( \u22121\n2 \u2211 k \u03bbk [ (f ik \u2212 f j k) 2 + 1i 6=j\u03c3 2 ]) .\nThis kernel is a variant of the radial basis function kernel, regularized by input noise \u03c32 (since the outputs are noiseless). The GP covariance is then defined as Kij = k(f i, f j), producing the following GP likelihood:\nlogP (y, \u03bb, \u03b2|F)=\u22121 2 yTK\u22121y\u22121 2 log |K|+logP (\u03bb, \u03b2|F).\nThe last term in the likelihood is the prior on the hyperparameters. This prior encourages the feature weights \u03bb to be sparse, and prevents degeneracies that occur as y \u2192 0. The latter is accomplished with a prior that encodes the belief that no two inducing points are deterministically dependent, as captured by their partial correlation:\nlogP (\u03bb, \u03b2|F) = \u22121 2\ntr ( K\u22122 ) \u2212 \u2211 k log (\u03bbk + 1)\nThis prior is discussed in more detail in previous work (Levine et al., 2011). The reward at a feature point f(xt,ut) is given by the GP posterior mean, and can be augmented with a set of linear features f`:\nr(xt,ut) = kt\u03b1+ \u03b8 Tf`(xt,ut),\nwhere \u03b1 = K\u22121y, and kt is a row vector corresponding to the covariance between f(xt,ut) and each inducing point f i, given by kti = k(f(xt,ut), f\ni). The exact log likelihood, before the proposed approximation, is obtained by using the GP likelihood as a prior on the IOC likelihood in Equation 1:\nlogP (u|x0) = \u2211 t r(xt,ut)\u2212 logZ + logP (y, \u03bb, \u03b2|F).\nOnly the IOC likelihood is altered by the proposed approximation. The gradient and Hessian of the reward with respect to the states are\ng\u0302t = \u2202kt \u2202xt \u03b1+ g\u0302 (`) t and H\u0302t = \u22022kt \u2202x2t \u03b1+ H\u0302 (`) t ,\nand the kernel derivatives are given by\n\u2202kt \u2202xt = \u2211 k \u2202kt \u2202f tk g\u0302 (k) t\n\u22022kt \u2202x2t = \u2211 k \u2202kt \u2202fk H\u0302 (k) t + \u2211 k1,k2 \u22022kt \u2202f tk1\u2202f t k2 g\u0302 (k1) t g\u0302 (k2) t .\nThe feature derivatives g\u0302(k) and H\u0302(k) are defined in the previous section, and g\u0303 and H\u0303 are given analogously. Using these quantities, the likelihood can be computed as described in Section 5. The likelihood gradients are derived in Appendix C of the supplement.\nThis algorithm can learn more expressive rewards in domains where a linear reward basis is not known, but with the usual bias and variance tradeoff that comes with increased model complexity. As shown in our evaluation, the linear method requires fewer examples when a linear basis is available, while the nonlinear variant can work with much less expressive features."}, {"heading": "7. Evaluation", "text": "We evaluate our method on simulated robot arm control, planar navigation, and simulated driving. In the robot arm task, the expert sets continuous torques on each joint of an n-link planar robot arm. The reward depends on the position of the end-effector. Each link has an angle and a velocity, producing a state space with 2n dimensions. By changing the number of links, we can vary the dimensionality of the task. An example of a 4-link arm is shown in Figure 2. The complexity of this task makes it difficult to compare with prior work, so we also include a simple planar navigation task, in which the expert takes continuous steps on a plane, as shown in Figure 3. Finally, we use humancreated examples on a simulated driving task, which shows how our method can learn complex policies from human demonstrations on a more realistic domain.\nThe reward function in the robot arm and navigation tasks has a Gaussian peak in the center, surrounded by four pits. The reward also penalizes each action with the square of its magnitude. The IOC algorithms are provided with a grid of 25 evenly spaced Gaussian features and the squared action magnitude. In the nonlinear test in Section 7.2, the Cartesian coordinates of the arm end-effector are provided instead of the grid.\nWe compare the linear and nonlinear variants of our method with the MaxEnt IRL and OptV algorithms (Ziebart et al., 2008; Dvijotham & Todorov, 2010). We present results for the linear time algorithm in Section 5.1, though we found that both the LQR variant and the direct, non-linearized approach produced similar results. MaxEnt used a grid discretization for both states and actions, while OptV used discretized actions and adapted the value function features as described by Dvijotham and Todorov. Since OptV cannot learn action-dependent rewards, it was provided with the true weight for the action penalty term.\nTo evaluate a learned reward, we first compute the optimal paths with respect to this reward from 32 random initial states that are not part of the training set. We also find the paths that begin in the same initial states but are optimal with respect to the true reward. In both cases, we compute evaluation paths that are\nglobally optimal, by first solving a discretization of the task with value iteration, and then finetuning the paths with continuous optimization. Once the evaluation paths are computed for both reward functions, we obtain a reward loss by subtracting the true reward along the learned reward\u2019s path from the true reward along the true optimal path. This loss is low when the learned reward induces the same policy as the true one, and high when the learned reward causes costly mistakes. Since the reward loss is measured entirely on globally optimal paths, it captures how well each algorithm learns the true, global reward, regardless of whether the examples are locally or globally optimal."}, {"heading": "7.1. Locally Optimal Examples", "text": "To test how well each method handles locally optimal examples, we ran the navigation task with increasing numbers of examples that were either globally or locally optimal. As discussed above, globally optimal examples were obtained with discretization, while locally optimal examples were computed by optimizing the actions from a random initialization. Each test was repeated eight times with random initial states for each example.\nThe results in Figure 4 show that both variants of our algorithm converge to the correct policy. The linear variant requires fewer examples, since the features form a good linear basis for the true reward. MaxEnt assumes global optimality and does not converge to the correct policy when the examples are only locally optimal. It also suffers from discretization error. OptV\nhas difficulty generalizing the reward to unseen parts of the state space, because the value function features do not impose meaningful structure on the reward."}, {"heading": "7.2. Linear and Nonlinear Rewards", "text": "On the robot arm task, we evaluated each method with both the Gaussian grid features, and simple features that only provide the position of the end effector, and therefore do not form a linear basis for the true reward. The examples were globally optimal. The number of links was set to 2, resulting in a 4-dimensional state space. Only the nonlinear variant of our algorithm could successfully learn the reward from the simple features, as shown in Figure 5. Even with the grid features, which do form a linear basis for the reward, MaxEnt suffered greater discretization error due to the complex dynamics of this task, while OptV could not meaningfully generalize the reward due to the increased dimensionality of the task."}, {"heading": "7.3. High Dimensional Tasks", "text": "To evaluate the effect of dimensionality, we increased the number of robot arm links. As shown in Figure 6, the processing time of our methods scaled gracefully with the dimensionality of the task, while the quality of the reward did not deteriorate appreciably. The processing time of OptV increased exponentially due to the action space discretization. The MaxEnt discretization was intractable with more than two links, and is therefore not shown."}, {"heading": "7.4. Human Demonstrations", "text": "We evaluate how our method handles human demonstrations on a simulated driving task. Although driving policies have been learned by prior IOC methods (Abbeel & Ng, 2004; Levine et al., 2011), their discrete formulation required a discrete simulator where the agent makes simple decisions, such as choosing which lane to switch to. In constrast, our driving simulator is a fully continuous second order dynamical system. The actions correspond directly to the gas, breaks, and steering of the simulated car, and the state space includes position, orientation, and linear and angular velocities. Because of this, prior methods that rely on discretization cannot tractably handle this domain.\nWe used our nonlinear method to learn from sixteen 13-second examples of an aggressive driver that cuts off other cars, an evasive driver that drives fast but keeps plenty of clearance, and a tailgater who follows closely behind the other cars. The features are speed, deviation from lane centers, and Gaussians covering the front, back, and sides of the other cars on the road. Since there is no ground truth reward for these tasks, we cannot use the reward loss metric. We follow prior work and quantify how much the learned policy resembles the demonstration by using task-relevant statistics (Abbeel & Ng, 2004). We measure the average speed of sample paths for the learned reward and the amount of time they spend within two car-lengths behind and in front of other cars, and compare these statistics with those from an unobserved holdout set of user demonstrations that start in the same initial states. The results in Table 1 show that the statistics of the learned policies are similar to the holdout demonstrations.\nPlots of the learned rewards are shown in Figure 7. Videos of the optimal paths for the learned rewards can be downloaded from the project website, along with the supplementary appendices and source code: http://graphics.stanford.edu/projects/cioc."}, {"heading": "8. Discussion and Future Work", "text": "We presented an IOC algorithm designed for continuous, high dimensional domains. Our method remains efficient in high dimensional domains by using a local approximation to the reward function likelihood. This approximation also removes the global optimality requirement for the expert\u2019s demonstrations, allowing the method to learn the reward from examples that are only locally optimal. Local optimality can be easier to demonstrate than global optimality, particularly in high dimensional domains. As shown in our evaluation, prior methods do not converge to the underlying reward function when the examples are only locally optimal, regardless of how many examples are provided.\nSince our algorithm relies on the derivatives of the reward features to learn the reward function, we require the features to be differentiable with respect to the states and actions. These derivatives must be precomputed only once, so it is quite practical to use finite differences when analytic derivatives are unavailable, but features that exhibit discontinuities are still poorly suited for our method. Our current formulation also only considers deterministic, fixed-horizon control problems, and an extension to stochastic or infinitehorizon cases is an interesting avenue for future work.\nIn addition, although our method handles examples that lack global optimality, it does not make use of global optimality when it is present: the examples are always assumed to be only locally optimal. Prior methods that exploit global optimality can infer more information about the reward function from each example when the examples are indeed globally optimal.\nAn exciting avenue for future work is to apply this approach to other high dimensional, continuous problems that have previously been inaccessible for inverse optimal control methods. One challenge with such applications is to generalize and impose meaningful structure in high dimensional tasks without requiring detailed features or numerous examples. While the nonlinear variant of our algorithm takes one step in this direction, it still relies on features to generalize the reward to unseen regions of the state space. A more sophisticated way to construct meaningful, generalizable features would allow IOC to be easily applied to complex, high dimensional tasks."}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their constructive comments. Sergey Levine was supported by NSF Graduate Research Fellowship DGE-0645962."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Proceedings of ICML,", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "Practical augmented lagrangian methods", "author": ["Birgin", "Ernesto G", "Mart\u0301\u0131nez", "Jos\u00e9 Mario"], "venue": "In Encyclopedia of Optimization,", "citeRegEx": "Birgin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Birgin et al\\.", "year": 2009}, {"title": "Linear Matrix Inequalities in System and Control Theory", "author": ["S. Boyd", "L. El Ghaoui", "E. Feron", "V. Balakrishnan"], "venue": null, "citeRegEx": "Boyd et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 1994}, {"title": "Inverse optimal control with linearly-solvable MDPs", "author": ["Dvijotham", "Krishnamurthy", "Todorov", "Emanuel"], "venue": "In Proceedings of ICML,", "citeRegEx": "Dvijotham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dvijotham et al\\.", "year": 2010}, {"title": "Feature construction for inverse reinforcement learning", "author": ["Levine", "Sergey", "Popovi\u0107", "Zoran", "Koltun", "Vladlen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Levine et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2010}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["Levine", "Sergey", "Popovi\u0107", "Zoran", "Koltun", "Vladlen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Levine et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2011}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Ng", "Andrew Y", "Russell", "Stuart J"], "venue": "In Proceedings of ICML,", "citeRegEx": "Ng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2000}, {"title": "Maximum margin planning", "author": ["Ratliff", "Nathan", "Bagnell", "J. Andrew", "Zinkevich", "Martin A"], "venue": "In Proceedings of ICML,", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["Ratliff", "Nathan", "Silver", "David", "Bagnell", "J. Andrew"], "venue": "Autonomous Robots,", "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "Accurate approximations for posterior moments and marginal densities", "author": ["Tierney", "Luke", "Kadane", "Joseph B"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Tierney et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Tierney et al\\.", "year": 1986}, {"title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy", "author": ["Ziebart", "Brian D"], "venue": "PhD thesis,", "citeRegEx": "Ziebart and D.,? \\Q2010\\E", "shortCiteRegEx": "Ziebart and D.", "year": 2010}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Ziebart", "Brian D", "Maas", "Andrew", "Bagnell", "J. Andrew", "Dey", "Anind K"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 8, "context": "We present two variants of our algorithm that learn the reward either as a linear combination of the provided features, as is common in prior work, or as a nonlinear function of the features, as in a number of recent methods (Ratliff et al., 2009; Levine et al., 2010; 2011).", "startOffset": 225, "endOffset": 274}, {"referenceID": 4, "context": "We present two variants of our algorithm that learn the reward either as a linear combination of the provided features, as is common in prior work, or as a nonlinear function of the features, as in a number of recent methods (Ratliff et al., 2009; Levine et al., 2010; 2011).", "startOffset": 225, "endOffset": 274}, {"referenceID": 7, "context": "Most prior IOC methods solve the entire forward control problem in the inner loop of an iterative procedure (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart, 2010).", "startOffset": 108, "endOffset": 164}, {"referenceID": 2, "context": "More efficient IOC algorithms have been proposed for the special case of linear dynamics and quadratic rewards (LQR) (Boyd et al., 1994; Ziebart, 2010).", "startOffset": 117, "endOffset": 151}, {"referenceID": 11, "context": "\u201d We employ the maximum entropy IRL (MaxEnt) model (Ziebart et al., 2008), which is closely related to linearly-solvable MDPs (Dvijotham & Todorov, 2010).", "startOffset": 51, "endOffset": 73}, {"referenceID": 5, "context": "(Levine et al., 2011).", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "This prior is discussed in more detail in previous work (Levine et al., 2011).", "startOffset": 56, "endOffset": 77}, {"referenceID": 11, "context": "We compare the linear and nonlinear variants of our method with the MaxEnt IRL and OptV algorithms (Ziebart et al., 2008; Dvijotham & Todorov, 2010).", "startOffset": 99, "endOffset": 148}, {"referenceID": 5, "context": "Although driving policies have been learned by prior IOC methods (Abbeel & Ng, 2004; Levine et al., 2011), their discrete formulation required a discrete simulator where the agent makes simple decisions, such as choosing which lane to switch to.", "startOffset": 65, "endOffset": 105}], "year": 2012, "abstractText": "Inverse optimal control, also known as inverse reinforcement learning, is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy. We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality, and is suitable for large, continuous domains where even computing a full policy is impractical. By using a local approximation of the reward function, our method can also drop the assumption that the demonstrations are globally optimal, requiring only local optimality. This allows it to learn from examples that are unsuitable for prior methods.", "creator": "LaTeX with hyperref package"}}}