{"id": "1206.4674", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Comparison-Based Learning with Rank Nets", "abstract": "We consider the it of search rest anticipated, where goes user is nominated with followed candidate objects without explain which though key to her intended aim. We university optics specific included finding the target, that requirements basic of assume relationships but not actual visibility between decorative. We devise a new approach based by assumed nets, and performances that for least distributions it given diagonal increase constant, it discovers when target came also number according illustrate left them the entropy years for target distribution example, result, between only achievable. We expansion tend positive to 's instance of bonfires upanishads, and attribute far strategy, prior science into same reproducible.", "histories": [["v1", "Mon, 18 Jun 2012 15:36:16 GMT  (229kb)", "http://arxiv.org/abs/1206.4674v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["amin karbasi", "stratis ioannidis", "laurent massouli\u00e9"], "accepted": true, "id": "1206.4674"}, "pdf": {"name": "1206.4674.pdf", "metadata": {"source": "META", "title": "Comparison-Based Learning with Rank Nets", "authors": ["Amin Karbasi", "Stratis Ioannidis"], "emails": ["amin.karbasi@epfl.ch", "stratis.ioannidis@technicolor.com", "laurent.massoulie@technicolor.com"], "sections": [{"heading": "1. Introduction", "text": "In search through comparisons, a user locates a target object in a database as follows. At each step, the database presents two objects to the user, who then selects among the pair the object closest to the target that she has in mind. This process continues until, based on the user\u2019s answers, the database can uniquely identify the target she has in mind.\nThis kind of interactive navigation, also known as exploratory search, has numerous real-life applications (Marchionini, 2006; Ruthven, 2008), such as navigation in a database of pictures of people photographed in an uncontrolled environment (Tschopp et al., 2011). Automated methods may fail to extract meaningful features from such photos. Even if this were possible, in many practical cases, images with similar\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nlow-level descriptors may have very different semantic content, and thus be perceived differently by users (Smeulders et al., 2000; Lew et al., 2006). On the other hand, a human can easily sort images of people w.r.t. their similarity to a given person, and her answers can be used to rank images in the database in terms of this similarity.\nFormally, the human user\u2019s feedback can be modelled as a \u201ccomparison oracle\u201d (Goyal et al., 2008). Assuming a database N endowed with a distance metric d, capturing the \u201cdistance\u201d or \u201cdissimilarity\u201d between different objects, a comparison oracle answers questions of the kind: \u201cBetween two objects x and y in N , which one is closest to t under the metric d?\u201d.\nIn this paper, we study algorithms for identifying an unknown target with as few queries to such an oracle as possible. Most importantly, the algorithms we consider do not rely on a priori knowledge of the distance between objects: they cannot access an embedding of N in a metric space, nor can they compute the distance between two objects. Decisions on which queries to submit to the oracle depend only on (a) ranking relationships between objects, which can indeed be obtained through a comparison oracle and (b) the prior distribution \u00b5 from which the target is sampled.\nAs discussed in Section 3.3, content search through comparisons can be framed as an active learning problem. A well-known active learning algorithm is the Generalized Binary Search (GBS) or splitting algorithm (Dasgupta, 2005). Using GBS to submit queries to the oracle locates the target in OPT \u00b7 ( Hmax(\u00b5)+1 ) queries, where Hmax(\u00b5) = maxx\u2208supp(\u00b5) log 1 \u00b5(x) and OPT is the number of queries submitted by an optimal algorithm. In practice, GBS performs very well in terms of query complexity, suggesting that this bound can be tightened. However, the computational complexity of GBS is \u0398(n3) for n = |N |, which makes it intractable for most large databases.\nRecently, Karbasi et al. (2011) proposed an algorithm that determines the target in O ( c3H(\u00b5)Hmax(\u00b5) ) queries, whose computational complexity is O(1) per query. Here, H(\u00b5) is the entropy of the prior \u00b5 and c, defined formally in Section 3.2, is the doubling constant of the prior \u00b5. It captures the dimension of the database, as determined by the underlying distance d (Clarkson, 2006). Karbasi et al. also show that OPT = \u2126(cH(\u00b5)), indicating that their algorithm is within a c2Hmax(\u00b5) factor from the optimal.\nWe make the following contributions: First, we propose a new adaptive algorithm, RankNetSearch, locating the target with O(c6H(\u00b5)) queries to the oracle, in expectation. Our algorithm therefore improves on GBS and Karbasi et al. by removing the term Hmax\u2014 which can be quite large in practice\u2014at a cost of a higher exponent in the dependence on the constant c. Its computational complexity is O ( n(logn+ c6) log c ) per query, which is manageable compared to GBS; moreover, this cost can be reduced to O(1) by precomputing an additional data structure.\nSecond, we extend RankNetSearch to the case of a faulty oracle that lies with a probability \u01eb > 0, and show that it locates the target w.h.p. at an expected query cost O( \u2211\nx\u2208N \u00b5(x) log 1 \u00b5(x) log log(\u00b5(x))\nand, thereby, close to H(\u00b5).\nThird, we evaluate RankNetSearch and prior art algorithms over several datasets. We observe that RankNetSearch establishes a desirable trade-off between query and computational complexity.\nThe remainder of this paper is organized as follows. We overview related work in Section 2, and discuss definitions and preliminaries in Section 3. Our algorithm and the analysis of its complexity are presented in Section 4, and its robustness to noise in Section 5. Section 6 includes our numerical evaluations."}, {"heading": "2. Related Work", "text": "Search through comparisons was first introduced by Goyal et al. (2008), and further explored by Lifshitz and Zhang (2009) and Tschopp et al. (2011). The above works study the problem in terms of worst-case (prior-free) bounds, so our work departs in introducing a prior \u00b5 and studying query complexity in expectation. All these works introduce a disorder constant, that plays the same role as the quantity c in our setup. Lifshitz and Zhang also employ hierarchical data structures similar to the rank-nets we study here. Our upper bound coincides with theirs when \u00b5 is uniform over N and can thus be seen as an extension to the more general Bayesian setting under prior \u00b5.\nCover trees based on nets have been extensively studied in the context of nearest neighbour search (Clarkson, 1999; Beygelzimer et al., 2006). These works too focus on worst-case bounds and, crucially, assume full access to the underlying distance metric d. Our approach thus differs in both of these respects. In earlier work, Fredman (1976) and others have considered decision trees for determining a complete ordering of objects rather than just the first one in the list.\nTo the best of our knowledge, the work closest to our is (Karbasi et al., 2011), which was the first to study search through comparisons in a Bayesian setting. Our work improves their bound by the factor of Hmax and establishes the connection to active learning and GBS."}, {"heading": "3. Definitions and Preliminaries", "text": ""}, {"heading": "3.1. Search through Comparisons", "text": "Consider a large finite set of objects N of size n = |N |, endowed with a distance metric d, capturing the \u201cdissimilarity\u201d between objects. A user selects a target t \u2208 N from a prior distribution \u00b5; our goal will be to design an interactive algorithm that queries the user with the purpose of discovering t.\nComparison Oracle. Though we assume that the metric d exists, our view of distances is constrained to only observing order relationships. More precisely, we only have access to information that can be obtained through a comparison oracle (Goyal et al., 2008). Given object z, a comparison oracleOz receives as a query an ordered pair (x, y) \u2208 N 2 and answers the question \u201cis z strictly closer to x than to y?\u201d, i.e.,\nOz(x, y) =\n{\n+1 if d(x, z) < d(y, z), \u22121 if d(x, z) \u2265 d(y, z)\n(1)\nNote that a tie d(x, z) = d(y, z) is revealed by two calls Oz(x, y) and Oz(y, x). Our algorithm for determining the unknown target t can submit queries to a comparison oracle Ot\u2014namely, the user. We thus assume, effectively, that the user can order objects w.r.t. their distance from t, but does not need to disclose (or even know) the exact values of these distances. We will first assume that the oracle always gives correct answers; in Section 5, we relax this assumption by considering a faulty oracle that lies with probability \u01eb < 0.5.\nPrior Knowledge and Performance Metrics. The algorithms we study rely only on a priori knowledge of (a) the distribution \u00b5 and (b) the values of the mapping Oz : N\n2 \u2192 {\u22121,+1}, for every z \u2208 N . This is in line with our assumption that, although the distance metric d exists, it cannot be directly observed.\nOur focus is on adaptive algorithms, whose decision on which query in N 2 to submit next are determined by the oracle\u2019s previous answers.\nThe prior \u00b5 can be estimated empirically as the frequency with which objects have been targets in the past. The order relationships can be computed off-line by submitting \u0398(n2 logn) queries to a comparison oracle, and requiring \u0398(n2) space: for each possible target z \u2208 N , objects in N can be sorted w.r.t. their distance from z with \u0398(n logn) queries to Oz . We store the result of this sorting in (a) a linked list, whose elements are sets of objects at equal distance from z, and (b) a hash-map, that associates every element y with its rank in the sorted list. Note that Oz(x, y) can thus be retrieved in O(1) time by comparing the relative ranks of x and y with respect to their distance from z.\nWe measure the performance of an algorithm through two metrics. The first is the query complexity, determined by the expected number of queries the algorithm needs to submit to the oracle to determine the target. The second is the computational complexity, determined by the time-complexity of determining the query to submit to the oracle at each step."}, {"heading": "3.2. A Lower Bound", "text": "Recall that the entropy of \u00b5 is defined as H(\u00b5) = \u2211\nx\u2208supp(\u00b5) \u00b5(x) log 1 \u00b5(x) where supp(\u00b5) is the support of \u00b5. Given an object x \u2208 N , let Bx(r) = {y \u2208 N : d(x, y) \u2264 r} be the closed ball of radius r \u2265 0 around x. Given a set A \u2286 N let \u00b5(A) = \u2211\nx\u2208A \u00b5(x). The doubling constant1 c(\u00b5) of a distribution \u00b5 is the minimum c > 0 for which \u00b5(Bx(2R)) \u2264 c \u00b7 \u00b5(Bx(R)), for any x \u2208 supp(\u00b5) and any R \u2265 0.\nThe doubling constant has a natural connection to the underlying dimension of the dataset (Clarkson, 2006; Karbasi et al., 2011), as determined by the distance d. Both the entropy and the doubling constant are also inherently connected to content search through comparisons. Karbasi et al. show that any adaptive mechanism for locating a target t must submit at least \u2126 ( c(\u00b5)H(\u00b5) )\nqueries to the oracle Ot, in expectation. Moreover, they provide an algorithm for determining the target in O ( c3H(\u00b5)Hmax(\u00b5) )\nqueries, where Hmax(\u00b5) = maxx\u2208supp(\u00b5) log 1 \u00b5(x) ."}, {"heading": "3.3. Active Learning", "text": "Search through comparisons can be seen as a special case of active learning (Dasgupta, 2005; Nowak, 2012). In active learning, a hypothesis space H is a set of bi-\n1c relates to the doubling dimension \u03b4 through c = 2\u03b4.\nnary valued functions defined over a finite setQ, called the query space. Each hypothesis h \u2208 H generates a label from {\u22121,+1} for every query q \u2208 Q. A target hypothesis h\u2217 is sampled from H according to some prior \u00b5; asking a query q amounts to revealing the value of h\u2217(q), thereby restricting the possible candidate hypotheses. The goal is to determine h\u2217 in an adaptive fashion, by asking as few queries as possible.\nIn our setting, the hypothesis space H is the set N , and the query space Q is the set of ordered pairs N 2. The target hypothesis sampled from \u00b5 is the unknown target t. Each hypothesis/object z \u2208 N is uniquely2 identified by the mapping Oz : N 2 \u2192 {\u22121,+1}, which we have assumed to be a priori known.\nGeneralized Binary Search A well-known algorithm for determining the true hypothesis in the general active-learning setting is the so-called generalized binary search (GBS) or splitting algorithm (Dasgupta, 2005; Nowak, 2012). Define the version space V \u2286 H to be the set of possible hypotheses that are consistent with the query answers observed so far. At each step, GBS selects the query q \u2208 Q that minimizes | \u2211\nh\u2208V \u00b5(h)h(q)|. Put differently, GBS selects the query that separates the current version space into two sets of roughly equal probability mass; this leads, in expectation, to the largest reduction in the mass of the version space as possible, so GBS can be seen as a greedy query selection policy.\nA bound on the query complexity of GBS originally obtained by Dasgupta (2005) and recently tightened (w.r.t. constants) by Golovin and Krause (2010) is given by the following theorem:\nTheorem 1. GBS makes at most OPT \u00b7 ( Hmax(\u00b5)+1 ) queries in expectation to identify hypothesis h\u2217 \u2208 N , were OPT is the minimum expected number of queries made by any adaptive policy.\nGBS in Search through Comparisons. In our setting, the version space V comprises all possible objects in z \u2208 N that are consistent with oracle answers given so far. In other words, z \u2208 V iff Oz(x, y) = Ot(x, y) for all queries (x, y) submitted to the oracle. Selecting the next query therefore amounts to finding the pair (x, y) \u2208 N 2 that minimizes\nf(x, y) = \u2223 \u2223 \u2211 z\u2208V \u00b5(z)Oz(x, y) \u2223 \u2223. (2)\nAs the simulations in Section 6 show, the query complexity of GBS is excellent in practice. This suggests\n2Note that, for any two objects/hypotheses z, z\u2032 \u2208 N , there exists at least one query in N 2 that differentiates them, namely (z\u2032, z).\nAlgorithm 1 RankNetSearch(Ot) Input: Oracle Ot Output: Target t 1: Let E \u2190 N ; select arbitrary x \u2208 E 2: repeat 3: ( R, {By(ry)}y\u2208R )\u2190RankNet(x,E) 4: Find y\u2217, the object in R closest to t, using Ot. 5: Let E \u2190 By\u2217(ry\u2217) and x\u2190 y\n\u2217; 6: until E is a singleton 7: return y\nAlgorithm 2 RankNet(x,E)\nInput: Root object x, Ball E = Bx(R) Output: \u03c1-rank net R, Voronoi balls {By(ry)}y\u2208R 1: \u03c1 \u2190 1 2: repeat 3: \u03c1 \u2190 \u03c1/2; construct a \u03c1-net R of E 4: \u2200 y \u2208 R, construct ball By(ry) 5: Let I \u2190 {y \u2208 E : |By(ry)| > 1} 6: until I = \u2205 or maxy\u2208I \u00b5(By(ry)) \u2264 0.5\u00b5(E) 7: return (R,{By(ry)}y\u2208R)\nthat the bound of Theorem 1 could be improved in the specific context of search through comparisons.\nNevertheless, the computational complexity of GBS is \u0398(n2|V |) operations per query, as it requires minimizing f(x, y) over all pairs in N 2. For large sets N , this can be truly prohibitive. This motivates us to propose a new algorithm, RankNetSearch, whose computational complexity is almost linear and its query complexity is within a O(c5(\u00b5)) factor from the optimal."}, {"heading": "4. An Efficient Adaptive Algorithm", "text": "Our algorithm is inspired by \u01eb-nets, a structure introduced by Clarkson (1999; 2006) in the context of Nearest Neighbor Search (NNS). The main challenge that we face is that, contrary to standard NNS, we have no access to the underlying distance metric. In addition, the query complexity bounds on \u01eb-nets are worst-case (i.e., prior-free); our construction takes the prior \u00b5 into account to provide bounds in expectation."}, {"heading": "4.1. Rank Nets", "text": "To address the above issues, we introduce the notion of rank nets, which will play the role of \u01eb-nets in our setting. For some x \u2208 N , consider the ball E = Bx(R) \u2286 N . For any y \u2208 E, we define\ndy(\u03c1,E) = inf{r : \u00b5(By(r)) \u2265 \u03c1\u00b5(E)} (3)\nto be the radius of the smallest ball around y that\nmaintains a mass above \u03c1\u00b5(E). Using this definition3, we define a \u03c1-rank net as follows.\nDefinition 1. For some \u03c1 < 1, a \u03c1-rank net of E = Bx(r) \u2286 N is a maximal4 set of objects R \u2282 E such that for any two distinct y, y\u2032 \u2208 R\nd(y, y\u2032) > min{dy(\u03c1,E), dy\u2032(\u03c1,E)}. (4)\nFor any y \u2208 R, consider the Voronoi cell Vy = {z \u2208 E : d(y, z) \u2264 d(y\u2032, z), \u2200y\u2032 \u2208 R, y\u2032 6= y}. We also define the radius ry of the Voronoi cell Vy as ry = inf{r : Vy \u2286 By(r)}. Critically, a rank net and the Voronoi tessellation it defines can both be computed using only ordering information:\nLemma 1. A \u03c1-rank net R of E can be constructed in O(|E|(log |E| + |R|)) steps, and the balls By(ry) \u2282 E circumscribing the Voronoi cells around R can be constructed in O(|E||R|) steps using only (a) \u00b5 and (b) the mappings Oz : N 2 \u2192 {\u22121,+1} for every z \u2208 E.\nThe proof is in Appendix A. Armed with this result, we turn our attention to how the selection of \u03c1 affects the size of the net as well as the mass of the Voronoi balls around it. Our next lemma, whose proof is in Appendix B, bounds |R|.\nLemma 2. The size of the net R is at most c3/\u03c1.\nFinally, our last lemma determines the mass of the Voronoi balls in the net.\nLemma 3. If ry > 0 then \u00b5(By(ry)) \u2264 c3\u03c1\u00b5(E).\nThe proof is in Appendix C. Note that Lemma 3 does not bound the mass of Voronoi balls of radius zero."}, {"heading": "4.2. Rank Net Data Structure and Algorithm", "text": "Rank nets can be used to identify a target t using a comparison oracle Ot as described in Algorithm 1. Initially, a net R covering N is constructed; nodes y \u2208 R are compared w.r.t. their distance from t, and the closest to the target is determined, say y\u2217. Note that this requires submitting |R| \u2212 1 queries to the oracle. The version space V (the set of possible hypotheses) is thus the Voronoi cell Vy\u2217 , and is a subset of the ball By\u2217(ry\u2217). The algorithm then proceeds by limiting the search to By\u2217(ry\u2217) and repeating the above process. Note that, at all times, the version space is included in the current ball to be covered by a net. The process terminates when this ball becomes a singleton which, by construction, must contain the target.\n3Whenever \u03c1 and E are unambiguous, we simply write dy rather than dy(\u03c1,E).\n4I.e., a set to which no more objects can be added.\nA question in the above setup is how to select \u03c1: by Lemma 3, small values lead to a sharp decrease in the mass of Voronoi balls from one level to the next, hence reaching the target with fewer iterations. On the other hand, by Lemma 2, small values also imply larger nets, leading to more queries to the oracle per iteration. We select \u03c1 in an iterative fashion, as indicated in the pseudocode of Algorithm 2: we repeatedly halve \u03c1 until all non-singleton Voronoi balls By(ry) of the resulting net have a mass bounded by 0.5\u00b5(E). This selection leads to the following bounds on the corresponding query and computational complexity of RankNetSearch:\nTheorem 2. RankNetSearch locates the target by making 4c6(1 +H(\u00b5)) queries to a comparison oracle, in expectation. The cost of determining which query to submit next is O ( n(logn+ c6) log c ) .\nIn light of the \u2126(cH(\u00b5)) lower bound on query complexity by Karbasi et al. (2011), RankNetSearch is within a O(c5) factor of the optimal algorithm, and is thus order-optimal for constant c. Moreover, the computational complexity per query is O (\nn(log n+ c6), in contrast to the cubic cost of GBS. As shown in Section 6, in practice, this leads to drastic reductions in the computational costs compared to GBS.\nThe computational complexity can be further reduced to O(1) through amortization. In particular, it is easy to see that the possible paths followed by RankNetSearch define a hierarchy, whereby every object serves as a parent to the rank net of its Voronoi ball. This tree can be pre-constructed, and search reduces descending this tree; we elaborate on this in Section 6."}, {"heading": "5. Noisy Comparison Oracle", "text": "In a noisy setting the search must be robust against erroneous answers. Specifically, assume that for any query Ot(x, y), the noisy oracle returns the wrong answer with probability bounded by \u01eb, for some \u01eb < 1/2, independently of previous answers. In this context, a problem with RankNetSearch arises in line 4 of Algorithm 1: it is not clear how to identify the object closest to the target among elements in a net. We resolve this by introducing repetitions at each iteration. Specifically, at the \u2113-th step of the search, \u2113 \u2265 1, and rank-net size m, we define a repetition factor\nk\u03b4(\u2113,m) := 2 log\n( (\u2113+ 1/\u03b4)2\u2308log2(m)\u2309 )\n(1\u2212 \u01eb)2 (5)\nfor some design parameter \u03b4 \u2208 (0, 1). The modified algorithm then proceeds down the hierarchy, starting at the top level for \u2113 = 1. The basic step at step \u2113 with a netR proceeds as follows. A tournament is organized\namong elements of R, who are initially paired. Pairs of competing members are compared k\u03b4(\u2113, |R|) times. The \u201cplayer\u201d from a given pair winning the largest number of games moves to the next stage, where it will be paired again with another winner of the first round, and so forth until only one player is left. Note that the number of repetitions k\u03b4(\u2113,m) increases only logarithmically with the level \u2113.\nTo find the closest object to target t with the noiseless oracle, clearly we need to make O(|R|) number of queries. The proposed algorithm achieves the same goal with high probability by making at most a factor 2k\u03b4(\u2113, |R|) more comparisons. In this context we have the following\nTheorem 3. For a comparison oracle with error probability \u01eb, the algorithm with repetitions (5) outputs the correct target with probability at least 1 \u2212 \u03b4 in O( 1\n( 1 2 \u2212\u01eb)2\n\u2211\nx\u2208N \u00b5(x) log 1 \u00b5(x) log( 1 \u03b4+log 1 \u00b5(x) )) queries,\nwith constants depending on c.\nThe proof is given in Appendix E. For uniform distribution \u00b5(x) \u2261 1/n, for all x \u2208 N , this yields an extra log log(n) factor in addition to the term of order H(\u00b5) = log(n) which, by the lower bound by Karbasi et al., is optimal."}, {"heading": "6. Numerical Evaluation", "text": "We evaluate RankNetSearch over six publicly available datasets: iris, abalone, ad, faces, swiss roll, and netflix. We subsampled the latter two, taking 1000 randomly selected data points from swiss roll, and the 1000 most rated movies in netflix. We map these datasets to Rd (categorical variables are mapped to binary values in the standard fashion) for d as shown in Fig. 1(a). For netflix, movies were mapped to 50- dimensional vectors by obtaining a low rank approximation of the user/movie rating matrix through SVD. For all experiments, the distance metric d is the \u21132 distance and the prior \u00b5 is power-law with \u03b1 = 0.4.\nWe evaluated the performance of two versions of RankNetSearch: one as described by Algo. 1, and another one (T-RankNetSearch) in which the hierarchy of rank nets is precomputed and stored as a tree. Both propose exactly the same queries to the oracle, so have the same query complexity; however, TRankNetSearch has only O(1) computational cost per query. The sizes of the trees precomputed by T-RankNetSearch for each dataset are shown in Fig. 1(a).\nWe compare these algorithms to (a) the policy proposed by Karbasi et al. (2011), denoted by Memo-\nryless, and (b) two heuristics based on GBS (the \u0398(n3) computational cost of GBS per query makes it intractable over the datasets we consider). The first heuristic, termed F-GBS for fast GBS, selects like GBS the query that minimizes (2); however, it does so by restricting the queries to pairs of objects in the current version space V . This reduces the computational cost per query to \u0398(|V |3), rather than \u0398(n2|V |). The second heuristic, termed S-GBS for sparse GBS, exploits rank nets as follows. First, we costruct the rank-net hierarchy over the dataset, as in T-RankNetSeach. Then, we minimize (2) restricted only on pairs of objects that appear in the same net. Intuitively, S-GBS assumes that an equitable partition of the objects exists among such pairs.\nQuery vs. Computational Complexity. The query complexity of different algorithms, expressed as average number of queries per search, is shown in Fig. 1(b). Although there are no known guarantees for either F-GBS nor S-GBS both algorithms are excellent in terms of query complexity across all datasets, finding the target within about 10 queries, in expectation. As GBS should perform as well as\nthese algorithms, these suggest that it should also have low query cost. The query complexity of RankNetSearch is between 2 to 10 times higher; the impact is greater for high-dimensional datasets, as expected through the dependence of the rank net size on the c doubling constant. Finally, Memoryless performs worse compared to all other algorithms. As shown in Fig. 1, the above ordering is fully reversed w.r.t. computational costs, measured as the aggregate number of operations performed per search. Differences from one algorithm to the next range between 50 to 100 orders of magnitude. F-GBS requires close to 109 operations in expectation for some datasets; in contrast, RankNetSearch ranges between 100 and 1000 operations and, in conclusion, presents an excellent trade-off between query and computational complexity.\nScalability and Robustness. To study how the above algorithms scale with the dataset size, we also evaluate them on a synthetic dataset comprising objects placed uniformly at random at R3. The query and computational complexity of the five algorithms is shown in Fig. 2(a) and (b).\nWe observe the same discrepancies betwen algorithms we noted in Fig. 1. The linear growth in terms of logn implies a linear relationship between both measures of complexity w.r.t. the entropy H(\u00b5) for all methods (we ommit the relevant figure for lack of space). In Fig. 2(b), we plot the query complexity of the robust RankNetSearch algorithm outlined in Section 5. For all simulations, the target success rate was set to 0.9, but the actual success rates we observed were considerably higher, close to 0.99. We observe that, even for high error rates \u01eb, the query complexity remains low. Moreover, the high success rates that we observe, combined with the independence of the cost on n, suggest that we can further reduce the number of queries to lower values than the ones required by (5)."}, {"heading": "7. Conclusion", "text": "We presented RankNetSearch, an algorithm that strikes an excellent balance between query and computational costs. Further improving this trade-off, in particular for more general kinds of noise, is an interesting future direction for this line of work. Throughout, we assumed that human inference of proximity is accurately captured by a metric space structure. An interesting research direction is assessing the validity of this assumption through user trials."}, {"heading": "A. Proof of Lemma 1", "text": "Using the ordered list containing the sets of equidistant objects described in Section 3.1, for any z \u2208 N , we can partitionN into equivalence classesAz1, A z 2, . . . , A z k such that for any two objects y, y\u2032 \u2208 N , y \u2208 Azi and y\u2032 \u2208 Azj with i < j if and only if d(y, z) < d(y \u2032, z).\nTo construct R, it suffices to show that (4) can be verified for any z, z\u2032 \u2208 E using only the above partition and \u00b5. If so, a \u03c1-rank net can be constructed in a greedy fashion as a maximal set whose points verify (4). This can be obtained by adding sequentially an arbitrary object to the net and excluding from future selections any nodes that violate (4) w.r.t. this newly added object. Indeed, for all y \u2208 E, By(dy) = \u22c3\u2113\nj=1 A y j , where \u2113 = inf{i : \u2211i j=1 \u00b5(A y j ) \u2265 \u03c1\u00b5(E)}. The statement thus follows as (4) is equivalent to y\u2032 /\u2208 By(dy) \u2228 y /\u2208 By\u2032(dy\u2032). To construct the Voronoi balls By(ry) \u2286 E, y \u2208 R, we initialize each such ball to contain its center y. For each z \u2208 E \\R, let jmin be the smallest j such that R \u2229 Azj 6= \u2205; the object z is then added to the ball By(ry) of every y \u2208 N \u2229Azjmin .\nFor each y, By(dy) can constructed in O(log |E|) time via binary search on the ordered list of equidistant objects. Constructing the rank net in a greedy fashion requires determining which objects violate (4) w.r.t. a newly added object on the net, which may take O(|E|) time. Hence, the overall complexity of constructing R is O(|E|(|R| + log |E|)). Finally, the construction of the Voronoi balls requires O(|R|) steps per object in E to assign each object to a ball."}, {"heading": "B. Proof of Lemma 2", "text": "Note first that, for all distinct y, y\u2032 \u2208 R, the balls B(y, dy(\u03c1,E)/4) \u2229B(y\u2032, dy\u2032(\u03c1,E)/4) = \u2205. To see this, assume w.l.o.g. that dy \u2265 dy\u2032 which implies that d(y, y\u2032) \u2265 dy \u2212 dy\u2032 . This is due to the fact that \u00b5(B(y, dy\u2032)) \u2265 \u03c1\u00b5(E), and hence, by (3), dy can be at most dy\u2032 + d(y, y \u2032). In case dy or dy\u2032 is zero, clearly\nd(y, y\u2032) > dy/2 > dy/4+dy\u2032/4. If 0 < dy\u2032 < dy/2, then d(y, y\u2032) > dy/2 \u2265 dy/4 + dy\u2032/4.. If dy\u2032 \u2265 dy/2 > 0, then d(y, y\u2032) \u2265 dy\u2032 \u2265 dy/2 > dy/4 + dy\u2032/4.. Hence, in all cases d(y, y\u2032) > dy/4 + dy\u2032/4 and as a result B(y, dy/4) \u2229B(y\u2032, dy\u2032/4) = \u2205.\nTo prove Lemma 2, observe that dy \u2264 2R for all y \u2208 R since \u00b5(y, d) \u2265 \u00b5(E) > \u03c1\u00b5(E) for d \u2264 2R. Therefore, dy/4 \u2264 R/2 and thus B(y, dy/4) \u2286 B(x, 2R). Hence, by the definition of c(\u00b5), \u2211\ny\u2208R \u00b5(B(y, dy/4)) \u2264 \u00b5(B(x, 2R)) \u2264 c\u00b5(E). More-\nover, \u2211 y\u2208R \u00b5(B(y, dy/4)) \u2265 c \u22122 \u2211 y\u2208R \u00b5(B(y, dy)) \u2265 c\u22122\u03c1\u00b5(E)|R|. Therefore, |R| \u2264 c3/\u03c1."}, {"heading": "C. Proof of Lemma 3", "text": "Observe first that, for all z \u2208 E, there exists a y \u2208 R such that z \u2208 B(y, dy(\u03c1,E)). To see this, assume otherwise. Then for any y \u2208 R, d(z, y) > dy(\u03c1,E) \u2265 min{dy(\u03c1,E), dz(\u03c1,E)} and we can add z to R, which contradicts its maximality.\nTo prove Lemma 3, we consider the following two cases. Suppose first 0 < ry \u2264 dy. By (3), for any r\u0303 < dy, we have \u00b5(B(y, r\u0303) < \u03c1\u00b5(E). In particular, \u00b5(B(y, dy/2) < \u03c1\u00b5(E). By the definition of c, \u00b5(B(y, ry) \u2264 \u00b5(B(y, dy) \u2264 c\u03c1\u00b5(E). For the second case, suppose that ry > dy. Let z \u2208 Vy is the point for which d(y, z) = ry. By the above observation, we know that there exists a y\u2032 \u2208 R such that d(z, y\u2032) \u2264 dy\u2032 . As ry > dy, y 6= y\u2032. On the other hand, d(z, y\u2032) \u2265 d(z, y) since z \u2208 Vy. Using the triangle inequality, we get d(y, y\u2032) \u2264 d(y, z) + d(y\u2032, z) \u2264 2d(y\u2032, z) \u2264 2dy\u2032 . We know that B(y, ry) \u2286 B(y\u2032, d(y, y\u2032) + ry). Since ry = d(y, z) \u2264 dy\u2032 we can say B(y, ry) \u2286 B(y\u2032, 3dy\u2032). Finally, by the definition of c, we have \u00b5(B(y, ry)) \u2264 \u00b5(B(y\u2032, 3dy\u2032)) \u2264 c 2\u00b5(B(y\u2032, dy\u2032)) \u2264 c 3\u03c1\u00b5(E)."}, {"heading": "D. Proof of Theorem 2", "text": "Note first that, by induction, it can be shown that the version space is a subset of E; correctness is implied by this fact and the termination condition. To bound the number of queries, we first show that the process RankNet constructs a net with small cardinality.\nLemma 4. RankNet terminates at \u03c1 > 14c3 .\nProof. To see that the while loop terminates, observe that, by Lemma 3, for small enough \u03c1 < minz\u2208E \u00b5(z)/(c\n3\u00b5(E)), all Voronoi balls By(ry) of the \u03c1-rank net R will be singletons, so I will indeed be empty. Suppose thus that the loop terminates at some \u03c1 = \u03c1\u2217. Since it did not terminate at \u03c1 = 2\u03c1\u2217, there exists a ball By(ry) of the 2\u03c1-rank net R such that ry > 0 and \u00b5(By(ry)) > 0.5\u00b5(E). By Lemma 3, \u00b5(By(ry)) \u2264 c32\u03c1\u00b5(E), and the lemma follows. Hence, from Lemmas 2 and 4, we get that the rank nets returned by RankNet have cardinality at most\n4c6. On the other hand, by construction, a net covering a ball Byry consists of either singletons or balls with mass less than 0.5\u00b5(By(ry)). As a result, at each iteration, moving to the next object either halves the mass of the version space or leads to a leaf, and the search terminates. As at any point the version space has a mass greater than \u00b5(t), the search will terminate after traversing most \u2308log2(1/\u00b5(t))\u2309 iterations. Since, at each level, the number of accesses to the oracle are R \u2212 1 \u2264 4c3, the total query cost for finding target t is at most 4c6\u2308log2(1/\u00b5(t)\u2309, and the query complexity statement follows. Finally, from Lemmas 1 and 4, the computational complexity of each RankNet call is at most O ( n(logn+ c3) log c ) ."}, {"heading": "E. Proof of Theorem 3", "text": "We first show the following auxiliary result.\nLemma 5. Given a target t and a noisy oracle with error probability bounded by \u01eb, the tournament among elements of the net R with repetitions k\u03b4(\u2113, |R|) returns the element in the set R that is closest to target t with probability at least 1\u2212 (\u2113 + 1/\u03b4)\u22122.\nProof. We assume for simplicity that there are no ties, i.e. there is a unique point in R that is closest to t. The case with ties can be deduced similarly. We first bound the probability p(k) that upon repeating k times queries Ot(x, y), among x and y the one that wins the majority of comparisons is not the closest to t. Because of the bound \u01eb on the error probability, one has p(k) \u2264 Pr(Bin(k, \u01eb) \u2265 k/2), where Bin(\u00b7, \u00b7) denotes the Binomial distribution. Azuma-Hoeffding inequality ensures that the right-hand side of the above is no larger than exp(\u2212k(1/2 \u2212 \u01eb)2/2). Upon replacing the number of repetitions k by the expression (5), one finds that p(k\u03b4(\u2113, |R|)) \u2264 (\u2113+1/\u03b4)\u22122/\u2308log2(|R|)\u2309. Consider now the games to be played by the element within R that is closest to t. There are at most \u2308log2(|R|)\u2309 such games. By the union bound, the probability that the closest element loses on any one of these games is no less than (\u2113+ 1/\u03b4)\u22122, as announced. By the union bound and the previous Lemma we have conditionally on any target t \u2208 N that Pr(success|T = t) \u2265 1 \u2212 \u2211\n\u2113\u22651(\u2113 + 1/\u03b4) \u22122).\nThe latter sum is readily bounded by \u03b4. The number of comparisons given that the target is T = t is at most \u2211\u2308log 2 (1/\u00b5(t))\u2309\n\u2113=1 2|R\u2113|k\u03b4(\u2113, |R\u2113|) =\nO (\n1 ( 1 2 \u2212\u01eb)2 log 1\u00b5(t) log( 1 \u03b4 + log 1 \u00b5(t) )\n)\n, where the O-\nterm depends only on the doubling constant c. The bound on the expected number of queries follows by averaging over t \u2208 N ."}], "references": [{"title": "Cover trees for nearest neighbor", "author": ["A. Beygelzimer", "S. Kakade", "J. Langford"], "venue": "In ICML,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2006}, {"title": "Nearest-neighbor searching and metric space dimensions. In Nearest-Neighbor Methods for Learning and Vision", "author": ["K.L. Clarkson"], "venue": null, "citeRegEx": "Clarkson,? \\Q2006\\E", "shortCiteRegEx": "Clarkson", "year": 2006}, {"title": "Nearest neighbor queries in metric spaces", "author": ["K.L. Clarkson"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "Clarkson,? \\Q1999\\E", "shortCiteRegEx": "Clarkson", "year": 1999}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": null, "citeRegEx": "Dasgupta,? \\Q2005\\E", "shortCiteRegEx": "Dasgupta", "year": 2005}, {"title": "How good is the information theory bound in sorting", "author": ["M.L. Fredman"], "venue": "In Theoretical Computer Science,", "citeRegEx": "Fredman,? \\Q1976\\E", "shortCiteRegEx": "Fredman", "year": 1976}, {"title": "Adaptive submodularity: A new approach to active learning and stochastic optimization", "author": ["D. Golovin", "A. Krause"], "venue": "In COLT,", "citeRegEx": "Golovin and Krause,? \\Q2010\\E", "shortCiteRegEx": "Golovin and Krause", "year": 2010}, {"title": "Disorder inequality: a combinatorial approach to nearest neighbor search", "author": ["N. Goyal", "Y. Lifshits", "H. Schutze"], "venue": "In WSDM,", "citeRegEx": "Goyal et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goyal et al\\.", "year": 2008}, {"title": "Content search through comparisons", "author": ["A. Karbasi", "S. Ioannidis", "L. Massouli\u00e9"], "venue": "In ICALP,", "citeRegEx": "Karbasi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Karbasi et al\\.", "year": 2011}, {"title": "Contentbased multimedia information retrieval: State of the art and challenges", "author": ["M.S. Lew", "N. Sebe", "C. Djeraba", "R. Jain"], "venue": "ACM Trans. Multimedia Comput. Commun. Appl.,", "citeRegEx": "Lew et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lew et al\\.", "year": 2006}, {"title": "Combinatorial algorithms for nearest neighbors, near-duplicates and small-world design", "author": ["Y. Lifshits", "S. Zhang"], "venue": "In SODA,", "citeRegEx": "Lifshits and Zhang,? \\Q2009\\E", "shortCiteRegEx": "Lifshits and Zhang", "year": 2009}], "referenceMentions": [{"referenceID": 8, "context": "low-level descriptors may have very different semantic content, and thus be perceived differently by users (Smeulders et al., 2000; Lew et al., 2006).", "startOffset": 107, "endOffset": 149}, {"referenceID": 6, "context": "Formally, the human user\u2019s feedback can be modelled as a \u201ccomparison oracle\u201d (Goyal et al., 2008).", "startOffset": 77, "endOffset": 97}, {"referenceID": 3, "context": "A well-known active learning algorithm is the Generalized Binary Search (GBS) or splitting algorithm (Dasgupta, 2005).", "startOffset": 101, "endOffset": 117}, {"referenceID": 7, "context": "Recently, Karbasi et al. (2011) proposed an algorithm that determines the target in O (", "startOffset": 10, "endOffset": 32}, {"referenceID": 1, "context": "It captures the dimension of the database, as determined by the underlying distance d (Clarkson, 2006).", "startOffset": 86, "endOffset": 102}, {"referenceID": 2, "context": "Cover trees based on nets have been extensively studied in the context of nearest neighbour search (Clarkson, 1999; Beygelzimer et al., 2006).", "startOffset": 99, "endOffset": 141}, {"referenceID": 0, "context": "Cover trees based on nets have been extensively studied in the context of nearest neighbour search (Clarkson, 1999; Beygelzimer et al., 2006).", "startOffset": 99, "endOffset": 141}, {"referenceID": 2, "context": "Search through comparisons was first introduced by Goyal et al. (2008), and further explored by Lifshitz and Zhang (2009) and Tschopp et al.", "startOffset": 51, "endOffset": 71}, {"referenceID": 2, "context": "Search through comparisons was first introduced by Goyal et al. (2008), and further explored by Lifshitz and Zhang (2009) and Tschopp et al.", "startOffset": 51, "endOffset": 122}, {"referenceID": 2, "context": "Search through comparisons was first introduced by Goyal et al. (2008), and further explored by Lifshitz and Zhang (2009) and Tschopp et al. (2011). The above works study the problem in terms of worst-case (prior-free) bounds, so our work departs in introducing a prior \u03bc and studying query complexity in expectation.", "startOffset": 51, "endOffset": 148}, {"referenceID": 0, "context": "Cover trees based on nets have been extensively studied in the context of nearest neighbour search (Clarkson, 1999; Beygelzimer et al., 2006). These works too focus on worst-case bounds and, crucially, assume full access to the underlying distance metric d. Our approach thus differs in both of these respects. In earlier work, Fredman (1976) and others have considered decision trees for determining a complete ordering of objects rather than just the first one in the list.", "startOffset": 116, "endOffset": 343}, {"referenceID": 7, "context": "To the best of our knowledge, the work closest to our is (Karbasi et al., 2011), which was the first to study search through comparisons in a Bayesian setting.", "startOffset": 57, "endOffset": 79}, {"referenceID": 6, "context": "More precisely, we only have access to information that can be obtained through a comparison oracle (Goyal et al., 2008).", "startOffset": 100, "endOffset": 120}, {"referenceID": 1, "context": "The doubling constant has a natural connection to the underlying dimension of the dataset (Clarkson, 2006; Karbasi et al., 2011), as determined by the distance d.", "startOffset": 90, "endOffset": 128}, {"referenceID": 7, "context": "The doubling constant has a natural connection to the underlying dimension of the dataset (Clarkson, 2006; Karbasi et al., 2011), as determined by the distance d.", "startOffset": 90, "endOffset": 128}, {"referenceID": 3, "context": "Search through comparisons can be seen as a special case of active learning (Dasgupta, 2005; Nowak, 2012).", "startOffset": 76, "endOffset": 105}, {"referenceID": 3, "context": "Generalized Binary Search A well-known algorithm for determining the true hypothesis in the general active-learning setting is the so-called generalized binary search (GBS) or splitting algorithm (Dasgupta, 2005; Nowak, 2012).", "startOffset": 196, "endOffset": 225}, {"referenceID": 3, "context": "A bound on the query complexity of GBS originally obtained by Dasgupta (2005) and recently tightened (w.", "startOffset": 62, "endOffset": 78}, {"referenceID": 3, "context": "A bound on the query complexity of GBS originally obtained by Dasgupta (2005) and recently tightened (w.r.t. constants) by Golovin and Krause (2010) is given by the following theorem:", "startOffset": 62, "endOffset": 149}, {"referenceID": 7, "context": "In light of the \u03a9(cH(\u03bc)) lower bound on query complexity by Karbasi et al. (2011), RankNetSearch is within a O(c) factor of the optimal algorithm, and is thus order-optimal for constant c.", "startOffset": 60, "endOffset": 82}, {"referenceID": 7, "context": "We compare these algorithms to (a) the policy proposed by Karbasi et al. (2011), denoted by Memo-", "startOffset": 58, "endOffset": 80}], "year": 2012, "abstractText": "We consider the problem of search through comparisons, where a user is presented with two candidate objects and reveals which is closer to her intended target. We study adaptive strategies for finding the target, that require knowledge of rank relationships but not actual distances between objects. We propose a new strategy based on rank nets, and show that for target distributions with a bounded doubling constant, it finds the target in a number of comparisons close to the entropy of the target distribution and, hence, of the optimum. We extend these results to the case of noisy oracles, and compare this strategy to prior art over multiple datasets.", "creator": "LaTeX with hyperref package"}}}