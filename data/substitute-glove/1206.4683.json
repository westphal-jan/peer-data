{"id": "1206.4683", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Marginalized Denoising Autoencoders for Domain Adaptation", "abstract": "Stacked jackline autoencoders (SDAs) once appeared successfully used whether else called representations took second-level adaptation. Recently, know how gaining record skill following range benchmark tasks while sentiment analysis across elements handwritten alternatively. SDAs learn productivity data representations by plan, recovering referred features however releases that are substantially unwittingly with panic. In this distributed, else pave marginalized SDA (mSDA) that document other yet regardless many SDAs: high computational minimum and widespread fact industry-specific to california - simulations etc.. In although set SDAs, our shift while mSDA vocalizing signals that thus reason not requiring recursive stochastic descent other other gameplay calculations to learn formula_4? 1998 idea, being are infinitesimal ago climbed - derived. Consequently, mSDA, which keep one adopted new just 34 directly by MATLAB ^ {TM }, reduced megahertz taking SDAs as two orders it temblor. Furthermore, only canonical learnt taken mSDA only as option not the combination SDAs, optimum almost number life-span last 0.1 tasks.", "histories": [["v1", "Mon, 18 Jun 2012 15:40:50 GMT  (514kb)", "http://arxiv.org/abs/1206.4683v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["minmin chen", "zhixiang eddie xu", "kilian q weinberger", "fei sha"], "accepted": true, "id": "1206.4683"}, "pdf": {"name": "1206.4683.pdf", "metadata": {"source": "META", "title": "Marginalized Denoising Autoencoders for Domain Adaptation", "authors": ["Minmin Chen", "Zhixiang (Eddie) Xu", "Kilian Q. Weinberger", "Fei Sha"], "emails": ["MC15@CSE.WUSTL.EDU", "XUZX@CSE.WUSTL.EDU", "KILIAN@WUSTL.EDU", "FEISHA@USC.EDU"], "sections": [{"heading": "1. Introduction", "text": "Domain adaptation (Ben-David et al., 2009; Huang et al., 2007; Weinberger et al., 2009; Xue et al., 2008) aims to generalize a classifier that is trained on a source domain, for which typically plenty of training data is available, to a target domain, for which data is scarce. Cross-domain generalization is important in many application areas of machine learning, where such an imbalance of training data may oc-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ncur. Examples are computational biology (Liu et al., 2008), natural language processing (Daume III, 2007; McClosky et al., 2006) and computer vision (Saenko et al., 2010).\nData in the source and the target are often distributed differently. This presents a major obstacle in adapting predictive models. Recent work has investigated several techniques for alleviating the difference: instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al., 2011b) and learning joint target and source feature representations (Blitzer et al., 2006; Glorot et al., 2011; Xue et al., 2008).\nRecently, Glorot et al. (2011) proposed a new approach that falls into the third category. The authors propose to learn robust feature representations with stacked denoising autoencoders (SDA) (Vincent et al., 2008). Denoising autoencoders are one-layer neural networks that are optimized to reconstruct input data from partial and random corruption. These denoisers can be stacked into deep learning architectures. The outputs of their intermediate layers are then used as input features for SVMs (Lee et al., 2009). Glorot et al. (2011) demonstrate that using SDA-learned features in conjunction with linear SVM classifiers yields record performance on the benchmark tasks of sentiment analysis across different product domains (Blitzer et al., 2006).\nDespite their remarkable and promising results, SDAs are limited by their high computational cost. They are significantly slower to train than competing algorithms (Blitzer et al., 2006; Chen et al., 2011a; Xue et al., 2008), primarily because of their reliance on iterative and numerical optimization to learn model parameters. The challenge is further compounded by the dimensionality of the input data and the need for computationally intensive model selection procedures to tune hyperparameters. Consequently, even a highly optimized implementation (Bergstra et al., 2010) may require hours (even days) of training time.\nIn this paper, we address this challenge with a variant of\nSDA. The proposed method, which we refer to as marginalized Stacked Denoising Autoencoder (mSDA), adopts the greedy layer-by-layer training of SDAs. However, a crucial difference is that we use linear denoisers as the basic building blocks. The key observation is that, in this setting, the random feature corruption can be marginalized out. Conceptually, this is equivalent to training the models with an infinitely large number of corrupted input data. Fitting models on such a scale would be impossible for the conventional SDAs, which often rely on stochastic gradient descent, and need to sweep through all the training data.\nOur contributions are summarized as follows: i) we contribute to deep learning by demonstrating that linear denoisers can be used as building blocks for learning feature representations. ii) we show that linearity can significantly simplify parameter estimation \u2014 our approach results in closed-form solutions for the optimal parameters. iii) we evaluate our approach rigorously on established domain adaptation benchmark data sets and compare with several competing state-of-the-art algorithms. We show that the classification performance of mSDA matches that of SDA across our benchmark data sets, while achieving tremendous speedups during training time (reducing training from up to 2 days for SDA to a few minutes with mSDA)."}, {"heading": "2. Notation and Background", "text": "We follow the setup of Glorot et al. (2011) and focus on the problem of domain adaptation throughout this paper. We assume that our data originates from two domains, source S and target T . From the source domain S, we sample data DS = {x1, . . . ,xns} \u2282 Rd with known labels LS = {y1, . . . , yns}, whereas from the target domain we are only able to sample data without labels DT = {xns+1, . . . xn} \u2282 Rd. We do not assume that both domains use identical features and we pad all input vectors with zeros to make both domains be of equal dimensionality d. Our goal is to learn a classifier h\u2208H with the help of the labeled set DS and the unlabeled set DT , to accurately predict the labels of data from the target domain T . In practice (and as we show in section 5) it is straightforward to also extend this framework to multiple target domains.\nStacked Denoising Autoencoder. Various forms of autoencoders have been developed in the deep learning literature (Rumelhart et al., 1986; Baldi & Hornik, 1989; Kavukcuoglu et al., 2009; Lee et al., 2009; Vincent et al., 2008; Rifai et al., 2011). In its simplest form, an autoencoder has two components, an encoder h(\u00b7) maps an input x \u2208 Rd to some hidden representation h(x) \u2208 Rdh , and a decoder g(\u00b7) maps this hidden representation back to a reconstructed version of x, such that g(h(x))\u2248x. The parameters of the autoencoders are learned to minimize the reconstruction error, measured by some loss `(x, g(h(x))).\nChoices for the loss include squared error or KullbackLeibler divergence when the feature values are in [0, 1].\nDenoising Autoencoders (DAs) incorporate a slight modification to this setup and corrupt the inputs before mapping them into the hidden representation. They are trained to reconstruct (or denoise) the original input x from its corrupted version x\u0303 by minimizing `(x, g(h(x\u0303))). Typical choices of corruption include additive isotropic Gaussian noise or binary masking noise. In this work, as in Vincent et al. (2008), we use the latter and set a fraction of the features of each input to zero. This is a natural choice for bag-of-word representations of texts, where typical classspecific words can be missing due to the writing style of the author or differences between train and test domains.\nThe stacked denoising autoencoder (SDA) of Vincent et al. (2008) stacks several DAs together to create higher-level representations, by feeding the hidden representation of the tth DA as input into the (t + 1)th DA. The training is performed greedily, layer by layer.\nFeature Generation. Many researchers have seen autoencoders as a powerful tool for automatic discovery and extraction of nonlinear features. For example, Lee et al. (2009) demonstrate that the hidden representations computed by either all or partial layers of a convolutional neural network (CNN) make excellent features for classification with SVMs. The pre-processing with a CNN improves the generalization by increasing robustness against noise and label-invariant transformations.\nGlorot et al. (2011) successfully apply SDAs to extract features for domain adaptation in document sentiment analysis. The authors train an SDA to reconstruct the input vectors (ignoring the labels) on the union of the source and target data. A classifier (e.g. a linear SVM) trained on the resulting feature representation h(x) transfers significantly better from source to target than one trained on x directly. Similar to CNNs, SDAs also combine correlated input dimensions, as they reconstruct removed feature values from uncorrupted features. It is shown that SDAs are able to disentangle hidden factors which explain the variations in the input data, and automatically group features in accordance with their relatedness to these factors (Glorot et al., 2011). This helps transfer across domains as these generic concepts are invariant to domain-specific vocabularies.\nAs an intuitive example, imagine that we classify product reviews according to their sentiments. The source data consists of book reviews, the target of kitchen appliances. A classifier trained on the original source never encounters the bigram \u201cenergy efficient\u201d during training and therefore assigns zero weight to it. In the learned SDA representation, the bigram \u201cenergy efficient\u201d would tend to reconstruct, and be reconstructed by, co-occurring features, typ-\nically of similar sentiment (e.g. \u201cgood\u201d or \u201clove\u201d). Hence, the source-trained classifier can assign weights even to features that never occur in its original domain representation, which are \u201cre-constructed\u201d by the SDA.\nAlthough SDAs generate excellent features for domain adaptation, they have several drawbacks: 1. Training with (stochastic) gradient descent is slow and hard to parallelize (although a dense-matrix GPU implementation exists (Bergstra et al., 2010) and an implementation based on reconstruction sampling exists (Dauphin Y., 2011) for sparse inputs); 2. There are several hyper-parameters (learning rate, number of epochs, noise ratio, mini-batch size and network structure), which need to be set by cross validation \u2014 this is particularly expensive as each individual run can take several hours; 3. The optimization is inherently non-convex and dependent on its initialization."}, {"heading": "3. SDA with Marginalized Corruption", "text": "In this section we introduce a modified version of SDA, which preserves its strong feature learning capabilities, and alleviates the concerns mentioned above through speedups of several orders of magnitudes, fewer meta-parameters, faster model-selection and layer-wise convexity."}, {"heading": "3.1. Single-layer Denoiser", "text": "The basic building block of our framework is a one-layer denoising autoencoder. We take the inputs x1, . . . ,xn from D=DS\u222aDT and corrupt them by random feature removal \u2014 each feature is set to 0 with probability p\u22650. Let us denote the corrupted version of xi as x\u0303i. As opposed to the two-level encoder and decoder in SDA, we reconstruct the corrupted inputs with a single mapping W : Rd\u2192Rd, that minimizes the squared reconstruction loss\n1\n2n n\u2211 i=1 \u2016xi \u2212Wx\u0303i\u20162. (1)\nTo simplify notation, we assume that a constant feature is added to the input, xi = [xi; 1], and an appropriate bias is incorporated within the mapping W = [W,b]. The constant feature is never corrupted.\nThe solution to (1) depends on which features of each input are randomly corrupted. To lower the variance, we perform multiple passes over the training set, each time with different corruption. We solve for the W that minimizes the overall squared loss\nLsq(W) = 1\n2mn m\u2211 j=1 n\u2211 i=1 \u2016xi \u2212Wx\u0303i,j\u20162, (2)\nwhere x\u0303i,j represents the jth corrupted version of the original input xi.\nAlgorithm 1 mDA in MATLABTM. function [W,h]=mDA(X,p); X=[X;ones(1,size(X,2))]; d=size(X,1); q=[ones(d-1,1).*(1-p); 1]; S=X*X\u2019; Q=S.*(q*q\u2019); Q(1:d+1:end)=q.*diag(S); P=S.*repmat(q\u2019,d,1); W=P(1:end-1,:)/(Q+1e-5*eye(d)); h=tanh(W*X);\nLet us define the design matrix X= [x1, . . . ,xn] \u2208Rd\u00d7n and its m-times repeated version as X= [X, . . . ,X]. Further, we denote the corrupted version of X as X\u0303. With this notation, the loss in eq. (1) reduces to\nLsq(W)= 1 2nm tr [( X\u2212WX\u0303 )> ( X\u2212WX\u0303 )] . (3)\nThe solution to (3) can be expressed as the well-known closed-form solution for ordinary least squares (Bishop, 2006):\nW = PQ\u22121 with Q = X\u0303X\u0303> and P = XX\u0303>. (4)\n(In practice this can be computed as a system of linear equations, without the costly matrix inversion.)"}, {"heading": "3.2. Marginalized Denoising Autoencoder", "text": "The largerm is, the more corruptions we average over. Ideally we would like m \u2192 \u221e, effectively using infinitely many copies of noisy data to compute the denoising transformation W.\nBy the weak law of large numbers, the matrices P and Q, as defined in (3), converge to their expected values as m becomes very large. If we are interested in the limit case, where m\u2192\u221e, we can derive the expectations of Q and P, and express the corresponding mapping W as\nW = E[P]E[Q]\u22121. (5)\nIn the remainder of this section, we compute the expectations of these two matrices. For now, let us focus on\nE[Q] = n\u2211 i=1 E [ x\u0303ix\u0303 > i ] . (6)\nAn off-diagonal entry in the matrix x\u0303ix\u0303>i is uncorrupted if the two features \u03b1 and \u03b2 both \u201csurvived\u201d the corruption, which happens with probability (1 \u2212 p)2. For the diagonal entries, this holds with probability 1 \u2212 p. Let us define a vector q = [1 \u2212 p, . . . , 1 \u2212 p, 1]> \u2208 Rd+1, where q\u03b1 represents the probability of a feature \u03b1 \u201csurviving\u201d the corruption. As the constant feature is never corrupted, we\nhave qd+1=1. If we further define the scatter matrix of the original uncorrupted input as S = XX>, we can express the expectation of the matrix Q as\nE[Q]\u03b1,\u03b2 = { S\u03b1\u03b2q\u03b1q\u03b2 if \u03b1 6= \u03b2 S\u03b1\u03b2q\u03b1 if \u03b1 = \u03b2 . (7)\nSimilarly, we obtain the expectation of P in closed-form as E[P]\u03b1\u03b2 = S\u03b1\u03b2q\u03b2 .\nWith the help of these expected matrices, we can compute the reconstructive mapping W directly in closed-form without ever explicitly constructing a single corrupted input x\u0303i. We refer to this algorithm as marginalized Denoising Autoencoder (mDA). Algorithm 1 shows a 10-line MATLABTM implementation. The mDA has several advantages over traditional denoisers: 1. It requires only a single sweep through the data to compute the matrices E[Q], E[P]; 2. Training is convex and a globally optimal solution is guaranteed; 3. The optimization is performed in non-iterative closed-form."}, {"heading": "3.3. Nonlinear feature generation and stacking", "text": "Arguably two of the key contributors to the success of the SDA are its nonlinearity and the stacking of multiple layers of denoising autoencoders to create a \u201cdeep\u201d learning architecture. Our framework has the same capabilities.\nIn SDAs, the nonlinearity is injected through the nonlinear encoder function h(\u00b7), which is learned together with the reconstruction weights W. Such an approach makes the training procedure highly non-convex and requires iterative procedures to learn the model parameters. To preserve the closed-form solution from the linear mapping in section 3.2 we insert nonlinearity into our learned representation after the weights W are computed. A nonlinear squashing-function is applied on the output of each mDA. Several choices are possible, including sigmoid, hyperbolic tangent, tanh(), or the rectifier function (Nair & Hinton, 2010). Throughout this work, we use the tanh() function.\nInspired by the layer-wise stacking of SDA, we stack several mDA layers by feeding the output of the (t\u22121)th mDA (after the squashing function) as the input into the tth mDA. Let us denote the output of the tth mDA as ht and the original input as h0 = x. The training is performed greedily layer by layer: each map Wt is learned (in closed-form) to reconstruct the previous mDA output ht\u22121 from all possible corruptions and the output of the tth layer becomes ht = tanh(Wtht\u22121). In our experiments, we found that even without the nonlinear squashing function, stacking still improves the performance. However, the nonlinearity improves over the linear stacking significantly. We refer to the stacked denoising algorithm as marginalized Stacked Denoising Autoencoder (mSDA). Algorithm 2 shows a 8- lines MATLABTM implementation of mSDA.\nAlgorithm 2 mSDA in MATLABTM. function [Ws,hs]=mSDA(X,p,l); [d,n]=size(X); Ws=zeros(d,d+1,l); hs=zeros(d,n,l+1); hs(:,:,1)=X; for t=1:l [Ws(:,:,t), hs(:,:,t+1)]=mDA(hs(:,:,t),p);\nend;"}, {"heading": "3.4. mSDA for Domain Adaptation", "text": "We apply mSDA to domain adaptation by first learning features in an unsupervised fashion on the union of the source and target data sets. One observation reported in (Glorot et al., 2011) is that if multiple domains are available, sharing the unsupervised pre-training of SDA across all domains is beneficial compared to pre-training on the source and target only. We observe a similar trend with our approach. The results reported in section 5 are based on features learned on data from all available domains. Once a mSDA is trained, the output of all layers, after squashing, tanh(Wtht\u22121), combined with the original features h0, are concatenated and form the new representation. All inputs are transformed into the new feature space. A linear Support Vector Machine (SVM) (Chang & Lin, 2011) is then trained on the transformed source inputs and tested on the target domain. There are two meta-parameters in mSDA: the corruption probability p and the number of layers l. In our experiments, both are set with 5-fold cross validation on the labeled data from the source domain. As the mSDA training is almost instantaneous, this grid search is almost entirely dominated by the SVM training time."}, {"heading": "4. Extension for High Dimensional Data", "text": "Many data sets (e.g. bag-of-words text documents) are naturally high dimensional. As the dimensionality increases, hill-climbing approaches used in SDAs can become prohibitively expensive. In practice, a work-around is to truncate the input data to the r dmost common features (Glorot et al., 2011). Unfortunately, this prevents SDAs from utilizing important information found in rarer features. (As we show in section 5, including these rarer features leads to significantly better results.) High dimensionality also poses a challenge to mSDA, as the system of linear equations in (5) of complexity O(d3) becomes too costly. In this section we describe how to approximate this calculation with a simple division into dr sub-problems of O(r 3).\nWe combine the concept of \u201cpivot features\u201d from Blitzer et al. (2006) and the use of most-frequent features from Glorot et al. (2011). Instead of learning a single mapping W \u2208 Rd\u00d7(d+1) to reconstruct all corrupted features, we learn multiple mappings but only reconstruct the r d\nmost frequent features (here, r = 5000). For an input xi we denote the shortened r-dimensional vector of only the r most-frequent features as zi \u2208Rr. We perform this reconstruction with S random non-overlapping sub-sets of input features. Without loss of generality, we assume that the feature-dimensions in the input space are in random order\nand divide-up the input vectors as xi = [ x1i > , . . . ,xSi > ]>\n. For each one of these sub-spaces we learn an independent mapping Ws which minimizes\nLs(Ws) = 1\n2n n\u2211 i=1 S\u2211 s=1 \u2016zi \u2212Wsx\u0303si\u20162. (8)\nEach mapping Ws can be solved in closed-form as in (5), following the method described in section 3.2. We define the output of the first layer in the resulting mSDA as the average of all reconstructions,\nh1 = tanh\n( 1\nS S\u2211 s=1 Wsxs\n) . (9)\nOnce the first layer, of dimension r d, is built, we can stack multiple layers on top of it using the regular mSDA as described in section 3.3 and Algorithm 2. It is worth pointing out that, although features might be separated in different sub-sets within the first layer, they can still be combined in subsequent layers of the mSDA."}, {"heading": "5. Results", "text": "We evaluate mSDA on the Amazon reviews benchmark data sets (Blitzer et al., 2006) together with several other algorithms for representation learning and domain adaptation. The dataset contains more than 340, 000 reviews from 25 different types of products from Amazon.com. For simplicity (and comparability), we follow the convention of (Chen et al., 2011b; Glorot et al., 2011) and only consider the binary classification problem whether a review is positive\n(higher than 3 stars) or negative (3 stars or lower). As mSDA and SDA focus on feature learning, we use the raw bag-of-words (bow) unigram/bigram features as their input. To be fair to other algorithms that we compare to, we also pre-process with tf-idf (Salton & Buckley, 1988) and use the transformed feature vectors as their input if that leads to better results. Finally, we remove five domains which contain less than 1, 000 reviews.\nDifferent domains in the complete set vary substantially in terms of number of instances and class distribution. Some domains (books and music) have hundreds of thousands of reviews, while others (food and outdoor) have only a few hundred. There are a total of 380 possible transfer tasks (e.g. Apparel \u2192 Baby). The proportion of negative examples in different domains also differs greatly. To counter the effect of class- and size-imbalance, a more controlled smaller dataset was created by Blitzer et al. (2006), which contains reviews of four types of products: books, DVDs, electronics, and kitchen appliances. Here, each domain consists of 2, 000 labeled inputs and approximately 4, 000 unlabeled ones (varying slightly between domains) and the two classes are exactly balanced. Almost all prior work provides results only on this smaller set with its more manageable twelve transfer tasks. We focus most of our comparative analysis on this smaller set but also provide results on the entire data for completeness.\nMethods. As baseline, we train a linear SVM on the raw bag-of-words representation of the labeled source and test it on target. We also include the results of the same setup with dense features obtained by projecting the entire data set (labeled and unlabeled source+target) onto a lowdimensional sub-space with PCA (we refer to this setting as PCA). Besides these two baselines, we evaluate the efficacy of a linear SVM trained on features learned by mSDA and two alternative feature learning algorithms, Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and\n1-layer1 SDA (Glorot et al., 2011). Finally, we also compare against CODA (Chen et al., 2011b), a state-of-the-art domain adaptation algorithm which is based on sampleand feature-selection, applied to tf-idf features. For CODA, SDA and SCL we use implementations provided by the authors. All hyper-parameters are set by 5-fold cross validation on the source training set2.\nMetrics. Following Glorot et al. (2011), we evaluate our results with the transfer error e(S, T ) and the in-domain error e(T, T ). The transfer error e(S, T ) denotes the classification error of a classifier trained on the labeled source data and tested on the unlabeled target data. The in-domain error e(T, T ) denotes the classification error of a classifier that is trained on the labeled target data and tested on the unlabeled target data. Similar to Glorot et al. (2011) we measure the performance of a domain adaptation algorithm in terms of the transfer loss, defined as e(S, T )\u2212eb(T, T ), where eb(T, T ) defines the in-domain error of the baseline. In other words, the transfer loss measures how much higher the error of an adapted classifier is in comparison to a linear SVM that is trained on actual labeled target bow data.\nThe various domain-adaptation tasks vary substantially in difficulty, which is why we do not average the transfer losses (which would be dominated by a few most difficult tasks). Instead, we average the transfer ratio, e(S, T )/eb(T, T ), the ratio of the transfer error over the in-domain error. As with the transfer loss, a lower transfer ratio implies better domain adaptation.\n1We were only able to obtain the 1-layer implementation from the authors. Anecdotally, multiple-layer SDA only leads to small improvements on this benchmark set but increases the training time drastically.\n2We keep the default values of some of the parameters in SCL, e.g. the number of stop-words removed and stemming parameters \u2014 as they were already tuned for this benchmark set by the authors.\nFor timing purposes, we ignore the time of the SVM training and only report the mSDA or SDA training time. As both algorithms are unsupervised, we do not re-train for different transfer tasks within a benchmark set \u2014 instead we learn one representation on the union of all domains. CODA (Chen et al., 2011a) does not take advantage of data besides source and target and we report the average training time per transfer task.3 All experiments were conducted on an off-the-shelf desktop with dual 6-core Intel i7 CPUs clocked at 2.66Ghz."}, {"heading": "5.1. Comparison with Related Work", "text": "In the first set of experiments, we use the setting from (Glorot et al., 2011) on the small Amazon benchmark set. The input data is reduced to only the 5, 000 most frequent terms of unigrams and bigrams as features.\nComparison per task. Figure 1 presents a detailed comparison of the transfer loss across the twelve domain adaptation tasks using the various methods mentioned. A linear SVM trained on the features generated by SDA and mSDA clearly outperform all the other methods. For several tasks, the transfer loss goes to negative \u2014 in other words, a SVM trained on the transformed source data has higher accuracy than one trained on the original target data. This is a strong indication that the learned new representation bridges the gap between domains. It is worth pointing out that in ten out of the twelve tasks mSDA achieves a lower transfer-loss than SDA.\nTiming. Figure 2 (left) depicts the transfer ratio as a function of training time required for different algorithms, averaged over 12 tasks. The time is plotted in log scale. We can make three observations: 1. SDA outperforms all other related work in terms of transfer-ratio, but is also the slow-\n3In CODA, the feature splitting and classifier training are inseparable and we necessarily include both in our timing.\nest to train (more than 5 hours of training time). 2. SCL and PCA are relatively fast, but their features cannot compete in terms of transfer performance. 3. The training time of mSDA is two orders of magnitude faster that of SDAs (180\u00d7 speedup), with comparable transfer ratio. Training one layer of mDA on all 27, 677 documents from the small set requires less than 25 seconds. A 5-layer mSDA requires less than 2 minutes to train, and the resulting feature transformation achieves slightly better transfer ratio than SDAs.\nLarge scale results. To demonstrate the capabilities of mSDA to scale to large data sets, we also evaluate it on the complete set with n = 340, 000 reviews from 20 domains and a total of 380 domain adaptation tasks (see right plot in Figure 2). We compare mSDA to SDA (1-layer). The large set is more heterogenous in terms of the number of domains, domain size and class distribution than the small set and both the transfer error and transfer ratio are averaged across 380 tasks. Nonetheless, a similar trend can be observed. The transfer ratio reported in Figure 2 (right) corresponds to averaged transfer errors of (baseline) 13.93%, ( one-layer SDA) 10.50%, (mSDA, l=1) 11.50%, (mSDA, l = 3) 10.47%, (mSDA, l = 5) 10.33%. With only one layer, mSDA performs a little worse than SDA but reduces the training time from over two days to about five minutes (700\u00d7 speedup). With three layers, mSDA matches the transfer-error and transfer-ratio of SDA and still only requires 14 minutes of training time (230\u00d7 speedup)."}, {"heading": "5.2. Further Analysis", "text": "In addition to comparison with prior work, we also analyze various other aspects of mSDA.\nLow-frequency features. Prior work often limits the input data to the most frequent features (Glorot et al., 2011). We use the modification from section 4 to scale mSDA (5-layers) up to high dimensions and include less-frequent uni-grams and bi-grams in the input (small Amazon set). In the case of SDA we make the first layer a dimensionality reducing transformation from d dimensions to 5000.\nThe left plot in Figure 3 shows the performance of mSDA and SDA as the input dimensionality increases (words are picked in decreasing order of their frequency). The transfer ratio is computed relative to the baseline with d=5000 feature. Clearly, both algorithms benefit from having more features up to 30, 000. mSDA matches the transfer-ratio of SDA consistently and, as the dimensionality increases, gains even higher speed-up. With 30, 000 input features, SDA requires over one day and mSDA only 3 minutes (458\u00d7 speedup).\nTransfer distance. Ben-David et al. (2007) suggest the Proxy-A-distance (PAD) as a measure of how different two domains are from each other. The metric is defined as 2(1 \u2212 2 ), where is the generalization error of a classifier (a linear SVM in our case) trained on the binary classification problem to distinguish inputs between the two domains. The right plot in Figure 3 shows the PAD before and after mSDA is applied. Surprisingly, the distance increases in the new representation \u2014 i.e. distinguishing between two domains becomes easier with the mSDA features. We explain this effect through the fact that mSDA is unsupervised and learns a generally better representation for the input data. This helps both tasks, distinguishing between domains and sentiment analysis (e.g. in the electronicdomain mSDA might interpolate the feature \u201cdvd player\u201d from \u201cblue ray\u201d, both are not particularly relevant for sentiment analysis but might help distinguish the review from the book domain.). Glorot et al. (2011) observe a similar effect with the representations learned with SDA."}, {"heading": "5.3. General Trends", "text": "In summary, we observe a few general trends across all experiments: 1. With one layer, mSDA is up to three orders of magnitudes faster but slightly less expressive than the original SDA. This can be attributed to the fact that mSDA has no hidden layer. 2. There is a clear trend that additional \u201cdeep\u201d layers improve the results significantly (here, up to five layers). With additional layers, the mSDA fea-\ntures reach (and surpass) the accuracy of 1-layer SDA and still obtain a several hundred-fold speedup. 3. The mSDA features help diverse classification tasks, domain classification and sentiment analysis, and can be trained very efficiently on high-dimensional data."}, {"heading": "6. Discussion and Conclusion", "text": "Although mSDA first and foremost marginalizes out the corruption in SDA training, the two algorithms differ in several profound ways: First, the mDA layers do not have hidden nodes \u2014 this allows a closed-form solution with substantial speed-ups but might entail limitations that still need to be investigated. Second, mSDA only has two free meta-parameters, controlling the amount of noise as well as the number of layers to be stacked, which greatly simplifies the model selection. Finally, leveraging on the analytic tractability of linear regression, the parameters of an mDA are trained to optimally denoise all possible corrupted training inputs \u2014 arguably \u201cinfinitely many\u201d. This is practically infeasible for SDAs.\nWe hope that our work on mSDA will inspire future research on efficient training of SDA, beyond domain adaptation, and impact a variety of research problems. The fast training time, the capability to scale to large and high-dimensional data and implementation simplicity make mSDA a promising method with appeal to a large audience within and beyond machine learning."}, {"heading": "Acknowledgements", "text": "KQW, MC, ZX were supported by NSF IIS-1149882 and NIH U01 1U01NS073457-01. FS was supported by NSF IIS-0957742, DARPA CSSG N10AP20019 and D11AP00278."}], "references": [{"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural networks,", "citeRegEx": "Baldi and Hornik,? \\Q1989\\E", "shortCiteRegEx": "Baldi and Hornik", "year": 1989}, {"title": "Analysis of representations for domain adaptation", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "F. Pereira"], "venue": null, "citeRegEx": "Ben.David et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "A Theory of Learning from Different Domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "Wortman", "Jenn"], "venue": "Machine Learning,", "citeRegEx": "Ben.David et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2009}, {"title": "Theano: a CPU and GPU Math Expression Compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In SciPy,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Pattern Recognition and Machine Learning", "author": ["Bishop", "Christopher"], "venue": null, "citeRegEx": "Bishop and Christopher.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and Christopher.", "year": 2006}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "In Proceedings of the 2006 Conference on EMNLP, pp", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Transactions on IST,", "citeRegEx": "Chang and Lin,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin", "year": 2011}, {"title": "Co-Training for Domain Adaptation", "author": ["M. Chen", "K.Q. Weinberger", "J.C. Blitzer"], "venue": "In NIPS,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Automatic Feature Decomposition for Single View Co-training", "author": ["M. Chen", "K.Q. Weinberger", "Y. Chen"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Frustratingly Easy Domain Adaptation", "author": ["III H. Daume"], "venue": "In ACL,", "citeRegEx": "Daume,? \\Q2007\\E", "shortCiteRegEx": "Daume", "year": 2007}, {"title": "Large-Scale Learning of Embeddings with Reconstruction Sampling", "author": ["Y. Dauphin", "X. Glorot", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Dauphin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2011}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Correcting Sample Selection Bias by Unlabeled Data", "author": ["J. Huang", "A.J. Smola", "A. Gretton", "K.M. Borgwardt", "B. Scholkopf"], "venue": "In NIPS", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "IEEE Conference on, pp", "author": ["K. Kavukcuoglu", "M.A. Ranzato", "R. Fergus", "Le-Cun", "Y. Learning invariant features through topographic filter maps. In CVPR"], "venue": "1605\u20131612. IEEE, 2009.", "citeRegEx": "Kavukcuoglu et al\\.,? 2009", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2009}, {"title": "Unsupervised Feature Learning for Audio Classification using Convolutional Deep Belief Networks", "author": ["Lee", "Honglak", "Largman", "Yan", "Pham", "Peter", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Evigan: a hidden variable model for integrating gene evidence for eukaryotic gene", "author": ["Liu", "Qian", "Mackey", "Aaron", "Roos", "David", "Pereira", "Fernando"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2008}, {"title": "Domain Adaptation with Multiple Sources", "author": ["T. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "In NIPS", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Reranking and selftraining for parser adaptation", "author": ["D. McClosky", "E. Charniak", "M. Johnson"], "venue": "In ACL, pp", "citeRegEx": "McClosky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hintont", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "Computer Vision\u2013ECCV", "citeRegEx": "Saenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saenko et al\\.", "year": 2010}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Information processing & management,", "citeRegEx": "Salton and Buckley,? \\Q1988\\E", "shortCiteRegEx": "Salton and Buckley", "year": 1988}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "In ICML,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Feature hashing for large scale multitask learning", "author": ["K.Q. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "In ICML", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Topic-bridged PLSA for cross-domain text classication", "author": ["G. Xue", "W. Dai", "Q. Yang", "Y. Yu"], "venue": "In SIGIR,", "citeRegEx": "Xue et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "Domain adaptation (Ben-David et al., 2009; Huang et al., 2007; Weinberger et al., 2009; Xue et al., 2008) aims to generalize a classifier that is trained on a source domain, for which typically plenty of training data is available, to a target domain, for which data is scarce.", "startOffset": 18, "endOffset": 105}, {"referenceID": 12, "context": "Domain adaptation (Ben-David et al., 2009; Huang et al., 2007; Weinberger et al., 2009; Xue et al., 2008) aims to generalize a classifier that is trained on a source domain, for which typically plenty of training data is available, to a target domain, for which data is scarce.", "startOffset": 18, "endOffset": 105}, {"referenceID": 24, "context": "Domain adaptation (Ben-David et al., 2009; Huang et al., 2007; Weinberger et al., 2009; Xue et al., 2008) aims to generalize a classifier that is trained on a source domain, for which typically plenty of training data is available, to a target domain, for which data is scarce.", "startOffset": 18, "endOffset": 105}, {"referenceID": 25, "context": "Domain adaptation (Ben-David et al., 2009; Huang et al., 2007; Weinberger et al., 2009; Xue et al., 2008) aims to generalize a classifier that is trained on a source domain, for which typically plenty of training data is available, to a target domain, for which data is scarce.", "startOffset": 18, "endOffset": 105}, {"referenceID": 15, "context": "Examples are computational biology (Liu et al., 2008), natural language processing (Daume III, 2007; McClosky et al.", "startOffset": 35, "endOffset": 53}, {"referenceID": 17, "context": ", 2008), natural language processing (Daume III, 2007; McClosky et al., 2006) and computer vision (Saenko et al.", "startOffset": 37, "endOffset": 77}, {"referenceID": 21, "context": ", 2006) and computer vision (Saenko et al., 2010).", "startOffset": 28, "endOffset": 49}, {"referenceID": 12, "context": "Recent work has investigated several techniques for alleviating the difference: instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al.", "startOffset": 101, "endOffset": 143}, {"referenceID": 16, "context": "Recent work has investigated several techniques for alleviating the difference: instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al.", "startOffset": 101, "endOffset": 143}, {"referenceID": 5, "context": ", 2011b) and learning joint target and source feature representations (Blitzer et al., 2006; Glorot et al., 2011; Xue et al., 2008).", "startOffset": 70, "endOffset": 131}, {"referenceID": 11, "context": ", 2011b) and learning joint target and source feature representations (Blitzer et al., 2006; Glorot et al., 2011; Xue et al., 2008).", "startOffset": 70, "endOffset": 131}, {"referenceID": 25, "context": ", 2011b) and learning joint target and source feature representations (Blitzer et al., 2006; Glorot et al., 2011; Xue et al., 2008).", "startOffset": 70, "endOffset": 131}, {"referenceID": 23, "context": "The authors propose to learn robust feature representations with stacked denoising autoencoders (SDA) (Vincent et al., 2008).", "startOffset": 102, "endOffset": 124}, {"referenceID": 14, "context": "The outputs of their intermediate layers are then used as input features for SVMs (Lee et al., 2009).", "startOffset": 82, "endOffset": 100}, {"referenceID": 5, "context": "(2011) demonstrate that using SDA-learned features in conjunction with linear SVM classifiers yields record performance on the benchmark tasks of sentiment analysis across different product domains (Blitzer et al., 2006).", "startOffset": 198, "endOffset": 220}, {"referenceID": 10, "context": "Recently, Glorot et al. (2011) proposed a new approach that falls into the third category.", "startOffset": 10, "endOffset": 31}, {"referenceID": 10, "context": "Recently, Glorot et al. (2011) proposed a new approach that falls into the third category. The authors propose to learn robust feature representations with stacked denoising autoencoders (SDA) (Vincent et al., 2008). Denoising autoencoders are one-layer neural networks that are optimized to reconstruct input data from partial and random corruption. These denoisers can be stacked into deep learning architectures. The outputs of their intermediate layers are then used as input features for SVMs (Lee et al., 2009). Glorot et al. (2011) demonstrate that using SDA-learned features in conjunction with linear SVM classifiers yields record performance on the benchmark tasks of sentiment analysis across different product domains (Blitzer et al.", "startOffset": 10, "endOffset": 539}, {"referenceID": 5, "context": "They are significantly slower to train than competing algorithms (Blitzer et al., 2006; Chen et al., 2011a; Xue et al., 2008), primarily because of their reliance on iterative and numerical optimization to learn model parameters.", "startOffset": 65, "endOffset": 125}, {"referenceID": 25, "context": "They are significantly slower to train than competing algorithms (Blitzer et al., 2006; Chen et al., 2011a; Xue et al., 2008), primarily because of their reliance on iterative and numerical optimization to learn model parameters.", "startOffset": 65, "endOffset": 125}, {"referenceID": 3, "context": "Consequently, even a highly optimized implementation (Bergstra et al., 2010) may require hours (even days) of training time.", "startOffset": 53, "endOffset": 76}, {"referenceID": 11, "context": "We follow the setup of Glorot et al. (2011) and focus on the problem of domain adaptation throughout this paper.", "startOffset": 23, "endOffset": 44}, {"referenceID": 20, "context": "Various forms of autoencoders have been developed in the deep learning literature (Rumelhart et al., 1986; Baldi & Hornik, 1989; Kavukcuoglu et al., 2009; Lee et al., 2009; Vincent et al., 2008; Rifai et al., 2011).", "startOffset": 82, "endOffset": 214}, {"referenceID": 13, "context": "Various forms of autoencoders have been developed in the deep learning literature (Rumelhart et al., 1986; Baldi & Hornik, 1989; Kavukcuoglu et al., 2009; Lee et al., 2009; Vincent et al., 2008; Rifai et al., 2011).", "startOffset": 82, "endOffset": 214}, {"referenceID": 14, "context": "Various forms of autoencoders have been developed in the deep learning literature (Rumelhart et al., 1986; Baldi & Hornik, 1989; Kavukcuoglu et al., 2009; Lee et al., 2009; Vincent et al., 2008; Rifai et al., 2011).", "startOffset": 82, "endOffset": 214}, {"referenceID": 23, "context": "Various forms of autoencoders have been developed in the deep learning literature (Rumelhart et al., 1986; Baldi & Hornik, 1989; Kavukcuoglu et al., 2009; Lee et al., 2009; Vincent et al., 2008; Rifai et al., 2011).", "startOffset": 82, "endOffset": 214}, {"referenceID": 19, "context": "Various forms of autoencoders have been developed in the deep learning literature (Rumelhart et al., 1986; Baldi & Hornik, 1989; Kavukcuoglu et al., 2009; Lee et al., 2009; Vincent et al., 2008; Rifai et al., 2011).", "startOffset": 82, "endOffset": 214}, {"referenceID": 23, "context": "In this work, as in Vincent et al. (2008), we use the latter and set a fraction of the features of each input to zero.", "startOffset": 20, "endOffset": 42}, {"referenceID": 23, "context": "The stacked denoising autoencoder (SDA) of Vincent et al. (2008) stacks several DAs together to create higher-level representations, by feeding the hidden representation of the t DA as input into the (t + 1) DA.", "startOffset": 43, "endOffset": 65}, {"referenceID": 14, "context": "For example, Lee et al. (2009) demonstrate that the hidden representations computed by either all or partial layers of a convolutional neural network (CNN) make excellent features for classification with SVMs.", "startOffset": 13, "endOffset": 31}, {"referenceID": 11, "context": "It is shown that SDAs are able to disentangle hidden factors which explain the variations in the input data, and automatically group features in accordance with their relatedness to these factors (Glorot et al., 2011).", "startOffset": 196, "endOffset": 217}, {"referenceID": 3, "context": "Training with (stochastic) gradient descent is slow and hard to parallelize (although a dense-matrix GPU implementation exists (Bergstra et al., 2010) and an implementation based on reconstruction sampling exists (Dauphin Y.", "startOffset": 127, "endOffset": 150}, {"referenceID": 11, "context": "One observation reported in (Glorot et al., 2011) is that if multiple domains are available, sharing the unsupervised pre-training of SDA across all domains is beneficial compared to pre-training on the source and target only.", "startOffset": 28, "endOffset": 49}, {"referenceID": 11, "context": "In practice, a work-around is to truncate the input data to the r dmost common features (Glorot et al., 2011).", "startOffset": 88, "endOffset": 109}, {"referenceID": 5, "context": "We combine the concept of \u201cpivot features\u201d from Blitzer et al. (2006) and the use of most-frequent features from Glorot et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 5, "context": "We combine the concept of \u201cpivot features\u201d from Blitzer et al. (2006) and the use of most-frequent features from Glorot et al. (2011). Instead of learning a single mapping W \u2208 Rd\u00d7(d+1) to reconstruct all corrupted features, we learn multiple mappings but only reconstruct the r d", "startOffset": 48, "endOffset": 134}, {"referenceID": 5, "context": "We evaluate mSDA on the Amazon reviews benchmark data sets (Blitzer et al., 2006) together with several other algorithms for representation learning and domain adaptation.", "startOffset": 59, "endOffset": 81}, {"referenceID": 11, "context": "For simplicity (and comparability), we follow the convention of (Chen et al., 2011b; Glorot et al., 2011) and only consider the binary classification problem whether a review is positive (higher than 3 stars) or negative (3 stars or lower).", "startOffset": 64, "endOffset": 105}, {"referenceID": 5, "context": "To counter the effect of class- and size-imbalance, a more controlled smaller dataset was created by Blitzer et al. (2006), which contains reviews of four types of products: books, DVDs, electronics, and kitchen appliances.", "startOffset": 101, "endOffset": 123}, {"referenceID": 5, "context": "Besides these two baselines, we evaluate the efficacy of a linear SVM trained on features learned by mSDA and two alternative feature learning algorithms, Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and", "startOffset": 196, "endOffset": 218}, {"referenceID": 11, "context": "1-layer1 SDA (Glorot et al., 2011).", "startOffset": 13, "endOffset": 34}, {"referenceID": 11, "context": "Following Glorot et al. (2011), we evaluate our results with the transfer error e(S, T ) and the in-domain error e(T, T ).", "startOffset": 10, "endOffset": 31}, {"referenceID": 11, "context": "Following Glorot et al. (2011), we evaluate our results with the transfer error e(S, T ) and the in-domain error e(T, T ). The transfer error e(S, T ) denotes the classification error of a classifier trained on the labeled source data and tested on the unlabeled target data. The in-domain error e(T, T ) denotes the classification error of a classifier that is trained on the labeled target data and tested on the unlabeled target data. Similar to Glorot et al. (2011) we measure the performance of a domain adaptation algorithm in terms of the transfer loss, defined as e(S, T )\u2212eb(T, T ), where eb(T, T ) defines the in-domain error of the baseline.", "startOffset": 10, "endOffset": 470}, {"referenceID": 11, "context": "In the first set of experiments, we use the setting from (Glorot et al., 2011) on the small Amazon benchmark set.", "startOffset": 57, "endOffset": 78}, {"referenceID": 11, "context": "Prior work often limits the input data to the most frequent features (Glorot et al., 2011).", "startOffset": 69, "endOffset": 90}, {"referenceID": 1, "context": "Ben-David et al. (2007) suggest the Proxy-A-distance (PAD) as a measure of how different two domains are from each other.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Ben-David et al. (2007) suggest the Proxy-A-distance (PAD) as a measure of how different two domains are from each other. The metric is defined as 2(1 \u2212 2 ), where is the generalization error of a classifier (a linear SVM in our case) trained on the binary classification problem to distinguish inputs between the two domains. The right plot in Figure 3 shows the PAD before and after mSDA is applied. Surprisingly, the distance increases in the new representation \u2014 i.e. distinguishing between two domains becomes easier with the mSDA features. We explain this effect through the fact that mSDA is unsupervised and learns a generally better representation for the input data. This helps both tasks, distinguishing between domains and sentiment analysis (e.g. in the electronicdomain mSDA might interpolate the feature \u201cdvd player\u201d from \u201cblue ray\u201d, both are not particularly relevant for sentiment analysis but might help distinguish the review from the book domain.). Glorot et al. (2011) observe a similar effect with the representations learned with SDA.", "startOffset": 0, "endOffset": 990}], "year": 2012, "abstractText": "Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters \u2014 in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.", "creator": "LaTeX with hyperref package"}}}