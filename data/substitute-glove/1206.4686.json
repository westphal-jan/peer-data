{"id": "1206.4686", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Discriminative Probabilistic Prototype Learning", "abstract": "In might documents we propose a easy yet powerful specific bringing learning relating in establishing learning scenarios where besides original input uwsp is described before way rest of vectors while and associated outputs further be probably by lean labels indicating, brought often, competing nonlinear. We should leading uses datapoint as full potatoes its trigonometric over only nominal set most feature initialization where each variability shown we fact each vector and to communities to main exact indigenously different. We apply gave probabilistic model that parameterizes there patented specific also means of hidden variables up therefore every it be aided without newer understanding provides on likelihood maximization. More importantly, fact the model input and when prototype simple can if mind same data though a discriminative instead. We show that our experimental can no especially a full probabilistic generalization of learning vector quantization (LVQ ). We apply our required put well experience entire attached classification, invisalign optical differs bringing all ' \u00a5 work generation analogical, turned their superior technical of kind therapeutic compared the. standard prototype - publishing qualifications consider and either competitive benchmark applying.", "histories": [["v1", "Mon, 18 Jun 2012 15:42:34 GMT  (350kb)", "http://arxiv.org/abs/1206.4686v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["edwin v bonilla", "antonio robles-kelly"], "accepted": true, "id": "1206.4686"}, "pdf": {"name": "1206.4686.pdf", "metadata": {"source": "META", "title": "Discriminative Probabilistic Prototype Learning", "authors": ["Edwin V. Bonilla", "Antonio Robles-Kelly"], "emails": ["edwin.bonilla@nicta.com.au", "Antonio.Robles-Kelly@nicta.com.au"], "sections": [{"heading": "1. Introduction", "text": "A fundamental problem in machine learning is that of coming up with useful characterizations of the input so that we can achieve better generalization ca-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\npabilities with our learning algorithms. We refer to this problem as that of learning representations. This has been a long standing goal in machine learning and has been addressed throughout the years from different perspectives. In fact, one of the simplest and oldest attempts to tackle this problem was given by Rosenblatt (1962) with the Perceptron algorithm for classification problems. In such an approach it was suggested that we can have non-linear mappings of the inputs so that the obtained representation allows us to discriminate between the classes with a simple linear function. However, the mapping (or \u201cfeatures\u201d) should have been engineered beforehand instead of being learned from the available data. Neural networks and their back-propagation algorithm (Rumelhart et al., 1986) became popular because they offered an automatic way of learning flexible representations by introducing the so-called hidden layers and hidden units into multilayer Perceptron architectures. Kernel-based algorithms and, in particular, support vector machines (SVMs, Scholkopf & Smola, 2001) offered a clever alternative to neural nets, circumventing the problem of learning representations by using kernels to map the input into feature spaces where the patterns are likely to be linearly separable. However, SVMs are inherently non-probabilistic and unsuitable for applications where one requires uncertainty measures around their predictions.\nIn this paper we propose a simple yet powerful approach to learning representations for classification problems where an original input datapoint is described by a set of feature vectors and its associated output may be given by soft labels indicating, for example, class probabilities, degrees of membership or noisy labels. Our approach to this problem is to represent an input datapoint as a K-dimensional vector, where each component is a mixture of probabilities over its corresponding set of feature vectors. Each probability indicates how likely a feature vector is to belong to one-out-of-K unknown prototype patterns.\nWe propose a probabilistic model that parameterizes these prototype patterns in terms of hidden variables and therefore it can be trained with conventional approaches based on likelihood maximization. More importantly, both the model parameters and the prototype patterns can be learned from data in a discriminative way. To our knowledge, previous approaches have not addressed the problem of discriminative prototype learning within a consistent multi-class probabilistic framework (see section 5 for details)."}, {"heading": "2. Problem Setting", "text": "In this paper we are interested in multi-class classification problems for which an input point is characterized by a set of feature vectors S = {x(1), . . . ,x(M)}, where each feature vector may describe, for example, some local characteristics of the input point. Additionally, we consider the general case where the outputs may be given by soft labels indicating, for example, class probabilities, degrees of membership or noisy class labels. Hence, our goal is to build a probabilistic classifier based on given training data, which is comprised by the tuples D = {(S(n), P\u0303(n)), n = 1, . . . , N}, where S(n) is the set of feature vectors in the nth tuple. In general, the number of feature vectors is different for each input and therefore we shall describe it by Mn. Similarly, P\u0303(n) \u2208 [0, 1]C , corresponds to the C-dimensional vector of soft labels (e.g. empirical probabilities) associated with the C output classes of the nth training instance. Obviously, these probabilities are constrained by \u2211C j=1 P\u0303 (y n = j) = 1, where yn denotes the latent class assignment of datapoint n.\nA common approach to prototype-based learning describes an input by a histogram of words from a vocabulary of size K. This histogram is commonly known as the bag-of-words representation. In order to specify the vocabulary it is customary to use clustering methods such asK-means or generative models such as Gaussian Mixtures, which are often used as a disjoint step before training a specific classifier. The model for extracting such representations is given by:\nfk(x) = { 1 iff \u2016\u00b5k \u2212 x\u2016 < \u2016\u00b5j \u2212 x\u2016 \u2200j 6= k 0 otherwise\n(1)\nz (n) k = \u2211 x\u2208S(n) \u03c0kfk(x), (2)\nwhere z is a K-dimensional vector to be used as the input representation for a specific classifier; {\u00b5k}Kk=1 are usually referred to as the centers; and \u03c0k is set to 1/Mn. We will refer to each fk(x) as a prototype function as it performs the encoding of each D-dimensional vector x into its corresponding (binary)K-dimensional\nrepresentation. It is important to realize that this method is a winner-takes-all approach where each feature vector x is assigned to only one centre. This is the main motivation for our method where we will relax this assumption and propose a fully discriminative probabilistic model for learning such representations.\nOur model assumes a bag-of-words representation as given by equation (2). However, here we consider that the prototypes are probabilities and are given by:\nfk(x) = exp (\u2212\u03b2\u2016\u00b5k \u2212 x\u20162)\u2211K j=1 exp (\u2212\u03b2\u2016\u00b5j \u2212 x\u20162) , (3)\nwhere \u03b2 is a rate parameter (or inverse temperature). Note that when \u03b2 \u2192 \u221e Equation (3) becomes equivalent to the hard limit in Equation (1). Therefore, fk(x) is the probability of feature vector x belonging to \u201ccluster\u201d k and zk is a mixture of these probabilities.\nIn addition to defining how to map the set of input vectors into parameterized probabilistic prototype representations, our method requires the definition of a discriminative probabilistic classifier. In principle, this could be any classifier that focuses on defining the conditional probability:\n\u03c3i(z(X); \u0398) def = p(y = i|z(X),\u0398) (4)\ndirectly in terms of our prototype representation z. Here we have used X def = {x(j)}Mj=1 and made explicit the dependency of z on its corresponding feature vectors. In the sequel, for simplicity in the notation, we will drop this dependency . Note that our model is a (conditional) directed probabilistic model. This contrasts with other approaches such as latent-variable CRFs which are undirected graphical models. As we shall see later, we will focus on a softmax classifier due to the simplicity and efficacy of this parametric model. However, it is clear that we can also incorporate nonparametric classifiers. We expect such approaches to be more effective than their parametric counterparts and we postpone their study to future work."}, {"heading": "3. Parameter Learning", "text": "In this section we are interested in learning the parameters of our discriminative probabilistic prototype framework. These parameters are: the rate parameter \u03b2, the vocabulary or centers {\u00b5k}Kk=1 and the parameters of the discriminative classifier \u0398. This could be effected using a number of optimization methods including simulated annealing and Markov Chain Monte Carlo. Here we propose direct gradient-based optimization of the data log-likelihood."}, {"heading": "3.1. Direct Likelihood Maximization with Gradient-Based Methods", "text": "Assuming iid data, the log-likelihood of the model parameters given the data can be expressed as:\nL(\u0398, {\u00b5k}kk=1, \u03b2) = N\u2211\nn=1\nLn(\u0398, {\u00b5k}Kk=1, \u03b2) (5)\n= N\u2211 n=1 P\u0303 (yn) logP ( yn|zn(\u0398, {\u00b5k}Kk=1, \u03b2) ) , (6)\nwhere P\u0303 (yn) refers to the soft labels associated with input n, e.g. the empirical class probabilities. In order to optimize the data log-likelihood we use a QuasiNewton method (BFGS) to which we provide the following gradient information:\n\u2207\u00b5`L = N\u2211\nn=1\n\u2207\u00b5`Ln \u2202L \u2202\u03b2 = N\u2211 n=1 \u2202Ln \u2202\u03b2 (7)\nand\n\u2207\u00b5`Ln = (G(n,`))Tg(n), (8)\nwhere:\ng(n) def = \u2207znLn G(n,`) def = \u2207\u00b5`zn (9)\ng (n) k =\n\u2202Ln \u2202znk G (n,`) k,d = \u2202znk \u2202\u00b5`d\n(10)\nfor k, ` = 1, . . .K and d = 1, . . . D. The advantage of the above formulation is that the partial derivatives wrt the cluster centers can be computed in closedform. Note that although these updates are straightforward to derive, we specify all the details here for completeness and for their later use when showing the relation between our method and LVQ in Section 3.3. The derivatives are given as follows:\n\u2202Ln \u2202\u00b5`d = K\u2211 k=1 \u2202Ln \u2202znk \u2202znk \u2202\u00b5`d . (11)\nMoreover, by using equations (2) and (3) we have that:\n\u2207\u00b5`znk = \u2211 x\u2208Sn \u2207\u00b5`fk(x) (12)\n=  2\u03b2 \u2211 x\u2208Sn fk(x)(1\u2212 fk(x))(x\u2212 \u00b5`), iff k = ` 2\u03b2 \u2211 x\u2208Sn fk(x)f`(x)(\u00b5 ` \u2212 x), otherwise.\n(13)\nA similar approach can be followed to compute the partial derivatives wrt \u03b2. Hence we have that:\n\u2202Ln\n\u2202\u03b2 = K\u2211 k=1 \u2202Ln \u2202znk \u2202znk \u2202\u03b2\n(14)\n\u2202znk \u2202\u03b2 = \u2211 x\u2208Sn fk(x) ( K\u2211 `=1 \u2016\u00b5` \u2212 x\u20162f`(x)\u2212 \u2016\u00b5k \u2212 x\u20162 ) .\n(15)"}, {"heading": "3.2. Discriminative Parametric Model: Softmax Classifier", "text": "For the case of a parametric model such as the softmax classifier:\nP (y = i|z,\u0398) = exp((\u03b8 i)T z)\u2211C\nj=1 exp((\u03b8 j)T z)\ndef = \u03c3i(z; \u0398) (16)\nthe local log-likelihood terms can be written as:\nLn = C\u2211\nj=1\nP\u0303 (yn = j) log \u03c3j(zn; \u0398). (17)\nThe corresponding derivatives wrt the prototype representations are given by:\n\u2207znLn = C\u2211\nj=1\n( P\u0303 (yn = j)\u2212 \u03c3j(zn; \u0398) ) \u03b8j . (18)\nFinally, we also need the gradient information wrt the model parameters (\u0398):\n\u2207\u03b8iL = n\u2211\nn=1\n( P\u0303 (yn = i)\u2212 \u03c3i(zn; \u0398) ) zn. (19)\nEquations (5) and (19) can be modified so as to include a regularization term \u2212\u03bb tr (\u0398T\u0398), where \u03bb is a regularization parameter; tr (\u00b7) is the trace operator; and \u0398 is the matrix of weights with columns given by each \u03b8i, i = 1 . . . , C. It is well known that the solution for the weights in this case corresponds to the MAP solution when considering a Gaussian prior over the weights \u03b8i \u223c N (\u03b8i|0, \u03bbI). We can have a similar prior or regularization for \u03b2.\nWe note here that optimization of the objective function in Equation (5) wrt to all the parameters is a non-convex problem. However, as we shall see in section 4, we use coordinate ascent and optimization of the model parameters \u0398, given all others fixed, is a convex problem."}, {"heading": "3.3. Relation to LVQ", "text": "Learning Vector Quantization (LVQ, Kohonen, 1990) is a prototype-based learning algorithm that uses the class label information to adapt the prototypes (i.e. cluster centers). The idea is that the prototypes should move towards the training examples in their corresponding class and away from training examples with different labels. As in the k-means algorithm, the assignment of datapoints to prototypes corresponds to the rule in Equation (1). If we were going to update the prototypes in our model with gradient ascent, this update would become:\n\u00b5 (new) ` = \u00b5 (old) ` + \u03b7\u2207\u00b5`L, (20)\nwhere \u03b7 is the learning rate.\nBy expanding Equation (11), substituting Equations (12) and (18), and assuming hard labels, i.e. P\u0303 (yn) is 1 for only one label and zero for all the others, we should get:\n\u00b5` (new) = \u00b5`+\n\u03b7 \u2211 x\u2208Sn K\u2211 k=1 \u03b8ynk \u2212 C\u2211 j=1 \u03b8jk\u03c3 j(z;\u03b8)  fk(x)f`(x)(\u00b5` \u2212 x) + \u2211 x\u2208Sn \u03b8yn` \u2212 C\u2211 j=1 \u03b8j`\u03c3 j(z;\u03b8)  f`(x)(\u00b5` \u2212 x)  .\n(21)\nBy taking the hard limit for the assignment of the samples to a single prototype (and assuming that all samples corresponding to a single data-point are assigned to the same cluster): f`(x) = 1 and, consequently, fk(x) = 0 for k 6= ` we have:\n\u00b5` (new) = \u00b5`+\u03b7 \u2211 x\u2208Sn \u03b8yn` \u2212 C\u2211 j=1 \u03b8j`\u03c3 j(z;\u03b8)  (\u00b5`\u2212x). (22) If |Sn| = 1 then the factor premultiplying (\u00b5` \u2212 x) can be absorbed into the learning rate and, therefore, we obtain the LVQ update. In our model, this factor is interpreted as the difference between the parameter corresponding to the prototype ` for the label of the current datapoint , i.e (\u03b8y n\n` ) and the average (wrt the posterior probabilities) of the parameters of the other classes. As a result, we can view the gradient-ascent updates for our method as a relaxed version of LVQ."}, {"heading": "4. Experiments and Results", "text": "In this section we present results on synthetic data, shape classification, hyperspectral image classification\nand people\u2019s work class categorization. For all our experiments, we employ our prototype learning approach where we iterate the learning of the prototype parameters {\u00b5`}K`=1, \u03b2 and the model parameters \u0398 via coordinate ascent on the model likelihood. We have initialized the cluster centers making use of k-means and varied the size of the vocabulary, i.e. K, according to the experimental vehicle. To illustrate the behavior of our algorithm and to show its performance with respect to competitive benchmarks, the first three problems studied (synthetic data, shape classification, hyperspectral image classification) consider the common case of hard labels and our last experiment on people\u2019s work class categorization investigates the use of class probabilities as soft labels."}, {"heading": "4.1. Illustration on Synthetic Data", "text": "These experiments are based on a set of 20 datapoints each comprised by a set of Mn \u2208 {1, . . . , 20} feature vectors in a 2-D space. Here we compare the baseline model yielded by k-means clustering (with K = 2) against our discriminative model. Figure 1 shows the original datapoints in the two-dimensional space along with the cluster centers (top). We see that the cluster centers learned by our discriminative approach (Panel b) are quite different even though the clusters obtained by k-means (Panel a) are very close to those used to generate the data. At the bottom panel we show the prototype representation given by both methods and we observe that the representation learned by our method is much more discriminative as it takes into consideration the class labels."}, {"heading": "4.2. MPEG DataSet", "text": "Our first real dataset is given by the MPEG-7 CE-1 Part B database (Latecki et al., 2000), which we will refer to as the mpeg-7 dataset. This contains 1400 binary shapes organized in 70 classes, each comprised of 20 images. We have sampled 1 in every 10 pixels on the shape contours and we have built a fully connected graph whose edge-weights are given by the Euclidean distances between each pair of pixel locations. These weights are then normalized to be in the interval [0, 1]. The feature vectors are given by the frequency histograms of these distances for every node. In our experiments, we have used 10 bins for the frequency histogram computation and set K = 200.\nFor purposes of shape categorization, we compare our method to three alternatives. The first one is a prototype-based baseline akin to the bag of words approaches in computer vision (Fei-Fei & Perona, 2005). Our baseline recovers prototypes via k-means cluster-\ning. The shapes under study are then described by the prototype-based representation which we then employ as input to a classifier. The classifier used for this baseline is also a softmax classifier as in our method. In other words, the only difference between this baseline method and our approach (probabilistic prototype) is how these prototypes are learned.\nThe other two alternatives are specifically designed for purposes of shape classification. These are the shape and skeletal contexts in Belongie et al. (2002) and Xie et al. (2008), respectively. Once the shape and skeletal contexts are at hand, we train one-versus-all SVM classifiers whose parameters have been selected through ten-fold cross validation. For all our shape categorization experiments, we have divided the graphs in the dataset into a training and a testing set. The training set comprises 50% randomly selected graphs, i.e. 700 and the other 50% of the data was used for testing. For these partitions we have effected five trials. The categorization results are shown in Table 1. The table shows the mean percentage of correctly classified shapes and the corresponding variance. Despite\nthe basic strategy taken for the construction of our graphs, which contrasts with the specialized nature of the skeletal and shape contexts, our probabilistic prototype method outperforms the alternatives."}, {"heading": "4.3. SPECTRAL Dataset", "text": "This application considers hyperspectral imagery from real-world scenes that include various types of materials. We have annotated these images at a pixel-level considering 10 different classes (C=10): tree trunk, light poles, shadow on grass, grass, road, white line on road, shadow on road, leaves, sky, and white regions on sky. We considered 24 different images from which we have extracted 1, 746, 708 data points with their corresponding labels. In order to characterize an input data point with a set of vectors (Sn) we have considered neighborhood information according to 7\u00d77 windows. Hence, for each data point we have 49 feature vectors. We have subsampled the data so as to include 1000 training instances and 16643 test instances across 10 different partitions.\nNote that our method is effectively learning the prototypes that can be used to represent the spectra for the objects in the scene as a linear combination of the spectral signatures of its material constituents. In geosciences and process control this is known as unmixing (Bergman, 2006). Current unmixing methods assume availability of the end-member spectra, which usually involves a cumbersome labeling of the end member data, effected through expert intervention.\nFor comparison we use the standard prototype approach (based on k-means for learning the centers), and the InfoLoss (Lazebnik & Raginsky, 2009) method. InfoLoss adapts the prototypes based on the optimization of an information-theoretical criterion. For this method, we have used an in-house implementation whose parameter settings have been set as described in Lazebnik & Raginsky (2009). For the standard prototype approach and InfoLoss, as in our method, we have used a softmax classifier so as to allow a direct representation-quality comparison via the classification rates for our approach and those yielded by the alternatives. In Table 2, we show the accuracy of the methods evaluated along with two standard errors. As before, probabilistic prototype refers to our method and standard prototype refers to the representation computed via the direct application of k-means for learning the centers. From the table, we can conclude that our method outperforms the alternatives by delivering a mean categorization rate of 81.25%."}, {"heading": "4.4. Test Likelihoods on MPEG-7 and SPECTRAL", "text": "Now, we turn our attention to the evaluation of the methods from a probabilistic point of view by reporting the likelihood of the models on the test data. In Figure 2(a), we show the test data log-likelihood for our probabilistic prototype approach and the baseline. The bar corresponds to the mean across the five trials and the segments account for two standard errors. Note that the log-likelihood for our approach is significantly greater than the one yielded by the standard approach. This is since the performance measures above account for the correctness of the labels yielded by the\nclassifier devoid of their probability of error. Thus, our method not only delivers better performance than the alternatives, but also provides better probability estimates. In Figure 2(b) we show similar results for the spectral dataset. As with the mpeg-7 dataset, the test data log-likelihood for the spectral dataset delivered by our approach is greater than the one yielded by the alternatives and shows less variability."}, {"heading": "4.5. ADULT Dataset", "text": "Finally, we applied our method to the adult dataset from the UCI machine learning repository (Frank & Asuncion, 2010). This dataset has been extracted from census-based data and the original task was to predict whether a person makes over $50K a year. It contains continuous and discrete input variables including education, age, work class (with eight different categories), etc. In order to use this dataset as a test benchmark for our method, we have grouped people\u2019s records according to the values of the discrete variables except native country and work class and we have selected the latter variable as our target labels. Hence, we have an 8-dimensional output variable. We represented an input instance by the set of vectors that belong to the same group (i.e. with the same values for all the discrete features excluding native country). Similarly, we have computed soft labels (P\u0303) by calculating for each group the proportion of records that have the same value for the corresponding label. Finally, we have sub-sampled the data to have 4146 different instances and considered 50% for training and the other 50% for testing. The regularization parameters for our model and the baseline have been set-up via cross-validation.\nThe (average) KL-divergence between the true (empirical) class distribution and the predictive distribution is 1.12 bits and if the soft labels were thresholded to provide a hard class assignment the accuracy of the model would be 77%. One interesting application of our model on this dataset is the automatic \u201cdiscovery\u201d of the latent population prototypes and how these relate to the class labels under consideration. Due to space limitations, we have not included these results here and postpone their analysis to future work.\nTable 2. Classification results for the spectral dataset. The mean classification accuracy is shown along with (\u00b1) two standard errors when using 100 data-points per class for training and the rest for testing. Our probabilistic prototype method outperforms the baseline methods.\nProbabilistic Prototype\nStandard Prototype\nInfoLoss (Lazebnik & Raginsky, 2009)\n81.25 \u00b1 0.72 % 75.17 \u00b1 0.49 % 72.16 \u00b1 0.82 %\nStd Prototype Our Method \u22123.5\n\u22123\n\u22122.5\n\u22122\n\u22121.5\n\u22121\nM e a n T\ne s t\nL o g 2\nL ik\ne lih\no o d\nStd Prototype Our Method Infoloss\n\u22121.1\n\u22121\n\u22120.9\n\u22120.8\n\u22120.7\n\u22120.6\nM e a n T\ne s t\nL o g 2\nL ik\ne lih\no o d\n(a) (b)\nFigure 2. (a) The test data log likelihood on the mpeg-7 dataset, with K = 200 and stratified sampling for training using 50% of the data and the rest for testing. The mean values are reported along with two standard errors across 5 replications of the experiment. (b) The test data log likelihood on the spectral dataset when using 1000 data-points for training and K = 50."}, {"heading": "5. Related Work", "text": "Generative models have been proposed as probabilistic generalizations of ad-hoc learning methods mostly for unsupervised learning scenarios (see e.g. Bishop et al., 1998). The work by Seo & Obermayer (2002) is related to ours in their attempt to generalize LVQ but their probabilistic model is inherently generative (see their equation 4). Their objective function is based on likelihood ratios and does not arise naturally from their original model.\nNeural networks and, more recently, deep belief networks (DBNs) have proved popular as methods for learning latent representations with the goal of tackling difficult problems in AI (see e.g. Hinton & Salakhutdinov, 2006; Bengio & LeCun, 2007). In particular, our method can be seen as a generalization of radial basis function (RBF) networks that considers multiple probabilistic observations; applies a pooling operator over the set of vectors belonging to the same instance; and optimizes the parameters via a coordinate ascent mechanism. There is also an interesting relation between our proposed learning algorithm and how deep architectures are currently trained as we ini-\ntialize our method with k-means, which can been seen as a pre-training stage, which is followed by a finetuning stage in DBNs.\nIn the computer vision community, summarization into a codebook has been used by a number of approaches in the literature. For example, Gemert et al. (2008) replace histograms with kernel density estimation (KDE) in the construction of codebooks and use the extracted features in conjunction with SVMs (nonprobabilistic approach) for classification. It is interesting to mention the different between InfoLoss (Lazebnik & Raginsky, 2009) and our approach. The methods are similar in spirit but their difference is analogous to the difference between filter and wrapper methods in feature selection. While InfoLoss focuses on a filter metric (an information-theoretical objective function), our method directly includes the specific classifier in the learning process (wrapper). On a similar vein, Boureau et al. (2010) propose the learning of mid-level features for recognition in computer vision. Their work focuses on (multiple) binary classifiers and does not provide consistent probabilistic outputs across all classes. It is well-known in the machine\nlearning theoretical literature that simple approaches to combining these classifiers such as one-against-all are inconsistent in that given optimal binary classifiers this \u201creduction\u201d may not yield optimal multi-class classifiers (see e.g. Beygelzimer et al., 2009).\nFinally, recent work by Bergamo et al. (2011) has proposed an approach to learning compact representations for novel category recognition. Their focus is on learning compact descriptions that can be used, for example, in image retrieval."}, {"heading": "6. Conclusions", "text": "We have presented a probabilistic model for the discriminative learning of latent representations, which corresponds to a relaxed version of the popular approach of prototype-based classification. From the application viewpoint, our method can be viewed as a discriminative technique that can be used for unmixing in geosciences and remote sensing (Bergman, 2006). It can also be applied to other problems, such as population modeling where labels and proportions of these can be associated to groups of instances.\nOur method requires, in general, a model for a probabilistic discriminative classifier and for purposes of illustrating the utility of our approach we have used a softmax classifier. However, our approach can, in principle, be used with any discriminative and probabilistic classifier including non-parametric methods. In the future we aim to explore the combination of non-parametric methods with our prototype learning approach and also investigate better optimization algorithms for parameter learning, e.g. based on mean field approximations."}, {"heading": "Acknowledgments", "text": "We thank Sarah Namin and Lars Petersson for providing us with the spectral dataset. NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program."}], "references": [{"title": "Shape matching and object recognition using shape contexts", "author": ["S. Belongie", "J. Malik", "J. Puzicha"], "venue": "IEEE TPAMI,", "citeRegEx": "Belongie et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Belongie et al\\.", "year": 2002}, {"title": "Scaling learning algorithms towards AI. In Large-Scale Kernel Machines", "author": ["Bengio", "Yoshua", "LeCun", "Yann"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Learning a compact code for novelcategory recognition", "author": ["Andrew. Picodes"], "venue": "In NIPS", "citeRegEx": "Picodes,? \\Q2011\\E", "shortCiteRegEx": "Picodes", "year": 2011}, {"title": "Some unmixing problems and algorithms in spectroscopy and hyperspectral imaging", "author": ["M. Bergman"], "venue": "In Applied Imagery and Pattern Recognition Workshop,", "citeRegEx": "Bergman,? \\Q2006\\E", "shortCiteRegEx": "Bergman", "year": 2006}, {"title": "Error-correcting tournaments", "author": ["Beygelzimer", "Alina", "Langford", "John", "Ravikumar", "Pradeep"], "venue": "In International conference on Algorithmic Learning Theory,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "GTM: The generative topographic mapping", "author": ["Bishop", "Christopher M", "Svens\u00e9n", "Markus", "Williams", "Christopher K. I"], "venue": "Neural Computation,", "citeRegEx": "Bishop et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bishop et al\\.", "year": 1998}, {"title": "Learning mid-level features for recognition", "author": ["Boureau", "Y-Lan", "Bach", "Francis", "LeCun", "Yann", "Ponce", "Jean"], "venue": "In CVPR,", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "In CVPR,", "citeRegEx": "Fei.Fei and Perona,? \\Q2005\\E", "shortCiteRegEx": "Fei.Fei and Perona", "year": 2005}, {"title": "Kernel codebooks for scene categorization", "author": ["Gemert", "Jan C", "Geusebroek", "Jan-Mark", "Veenman", "Cor J", "Smeulders", "Arnold W"], "venue": "In ECCV,", "citeRegEx": "Gemert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gemert et al\\.", "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improved versions of learning vector quantization", "author": ["Kohonen", "Teuvo"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Kohonen and Teuvo.,? \\Q1990\\E", "shortCiteRegEx": "Kohonen and Teuvo.", "year": 1990}, {"title": "Shape descriptors for non-rigid shapes with a single closed contour", "author": ["L.J. Latecki", "R. Lakamper", "U. Eckhardt"], "venue": "In CVPR,", "citeRegEx": "Latecki et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Latecki et al\\.", "year": 2000}, {"title": "Supervised learning of quantizer codebooks by information loss minimization", "author": ["Lazebnik", "Svetlana", "Raginsky", "Maxim"], "venue": "IEEE TPAMI, pp", "citeRegEx": "Lazebnik et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lazebnik et al\\.", "year": 2009}, {"title": "Principles of neurodynamics; perceptrons and the theory of brain mechanisms", "author": ["Rosenblatt", "Frank"], "venue": "Spartan Books,", "citeRegEx": "Rosenblatt and Frank.,? \\Q1962\\E", "shortCiteRegEx": "Rosenblatt and Frank.", "year": 1962}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Scholkopf", "Bernhard", "Smola", "Alexander J"], "venue": null, "citeRegEx": "Scholkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Scholkopf et al\\.", "year": 2001}, {"title": "Soft learning vector quantization", "author": ["Seo", "Sambu", "Obermayer", "Klaus"], "venue": "Neural Computation,", "citeRegEx": "Seo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2002}, {"title": "Shape matching and modeling using skeletal context", "author": ["J. Xie", "P. Heng", "M. Shah"], "venue": "Pattern Recognition,", "citeRegEx": "Xie et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": "Neural networks and their back-propagation algorithm (Rumelhart et al., 1986) became popular because they offered an automatic way of learning flexible representations by introducing the so-called hidden layers and hidden units into multilayer Perceptron architectures.", "startOffset": 53, "endOffset": 77}, {"referenceID": 11, "context": "Our first real dataset is given by the MPEG-7 CE-1 Part B database (Latecki et al., 2000), which we will refer to as the mpeg-7 dataset.", "startOffset": 67, "endOffset": 89}, {"referenceID": 0, "context": "These are the shape and skeletal contexts in Belongie et al. (2002) and Xie et al.", "startOffset": 45, "endOffset": 68}, {"referenceID": 0, "context": "These are the shape and skeletal contexts in Belongie et al. (2002) and Xie et al. (2008), respectively.", "startOffset": 45, "endOffset": 90}, {"referenceID": 0, "context": "Probabilistic Prototype Standard Prototype Shape Context (Belongie et al., 2002) Skeletal Context (Xie et al.", "startOffset": 57, "endOffset": 80}, {"referenceID": 17, "context": ", 2002) Skeletal Context (Xie et al., 2008) 85.", "startOffset": 25, "endOffset": 43}, {"referenceID": 3, "context": "In geosciences and process control this is known as unmixing (Bergman, 2006).", "startOffset": 61, "endOffset": 76}, {"referenceID": 5, "context": "Bishop et al., 1998). The work by Seo & Obermayer (2002) is related to ours in their attempt to generalize LVQ but their probabilistic model is inherently generative (see their equation 4).", "startOffset": 0, "endOffset": 57}, {"referenceID": 7, "context": "For example, Gemert et al. (2008) replace histograms with kernel density estimation (KDE) in the construction of codebooks and use the extracted features in conjunction with SVMs (nonprobabilistic approach) for classification.", "startOffset": 13, "endOffset": 34}, {"referenceID": 6, "context": "On a similar vein, Boureau et al. (2010) propose the learning of mid-level features for recognition in computer vision.", "startOffset": 19, "endOffset": 41}, {"referenceID": 3, "context": "From the application viewpoint, our method can be viewed as a discriminative technique that can be used for unmixing in geosciences and remote sensing (Bergman, 2006).", "startOffset": 151, "endOffset": 166}], "year": 2012, "abstractText": "In this paper we propose a simple yet powerful method for learning representations in supervised learning scenarios where an input datapoint is described by a set of feature vectors and its associated output may be given by soft labels indicating, for example, class probabilities. We represent an input datapoint as a K-dimensional vector, where each component is a mixture of probabilities over its corresponding set of feature vectors. Each probability indicates how likely a feature vector is to belong to one-out-of-K unknown prototype patterns. We propose a probabilistic model that parameterizes these prototype patterns in terms of hidden variables and therefore it can be trained with conventional approaches based on likelihood maximization. More importantly, both the model parameters and the prototype patterns can be learned from data in a discriminative way. We show that our model can be seen as a probabilistic generalization of learning vector quantization (LVQ). We apply our method to the problems of shape classification, hyperspectral imaging classification and people\u2019s work class categorization, showing the superior performance of our method compared to the standard prototype-based classification approach and other competitive benchmarks.", "creator": "LaTeX with hyperref package"}}}