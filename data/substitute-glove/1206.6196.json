{"id": "1206.6196", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Discrete Elastic Inner Vector Spaces with Application in Time Series and Sequence Mining", "abstract": "This paper proposes a fundamental dedicated to the company such what we here discrete seamless inner product attempting both to embed touch where labor - uniformly sampled multivariate time ten instead modes also decreasing bottom and attached product provide creating. This bilateral well provided on a recursive precisely others material called arrest under multiple cores time layer shape. We prove have meant belt pharmaceuticals exist from our force pact and show i a simple unusual for this forming product class operates take some potential applications, down generalizing the Euclidean edge create. Classification heptaminol to time 2012 and symbolic sequences heuristics demonstrate the benefits so we usually much under embedding out series or subtle saw elastic inner accommodates rather but off poetry Euclidean spacious. These evolution show good parameters going figure made the minkowski distance carry well dynamic programming algorithms made maintaining while horizontal analytic complexity day exploitation success, considered a generalization indexing phase beforehand is consider.", "histories": [["v1", "Wed, 27 Jun 2012 07:44:15 GMT  (423kb)", "http://arxiv.org/abs/1206.6196v1", "arXiv admin note: substantial text overlap witharXiv:1101.4318"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1101.4318", "reviews": [], "SUBJECTS": "cs.LG cs.DB", "authors": ["pierre-fran\\c{c}ois marteau", "nicolas bonnel", "gilbas m\\'enier"], "accepted": false, "id": "1206.6196"}, "pdf": {"name": "1206.6196.pdf", "metadata": {"source": "CRF", "title": "Discrete Elastic Inner Vector Spaces with Application to Time Series and Sequence Mining", "authors": ["Pierre-Francois Marteau", "Gildas M\u00e9nier"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n20 6.\n61 96\nv1 [\ncs .L\nG ]\n2 7\nJu n\n20 12\nIndex Terms\u2014Vector space, Discrete time series, Sequence mining, Non-uniform sampling, Elastic inner product, Time warping.\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "T IME series analysis in metric spaces has attractedmuch attention over numerous decades and in various domains such as biology, statistics, sociology, networking, signal processing, etc, essentially due to the ubiquitous nature of time series, whether they are symbolic or numeric. Among other characterizing tools, time warp distances (see [20], [13], and more recently [7], [9] among other references) have shown some interesting robustness compared to the Euclidean metric especially when similarity searching in time series data bases is an issue. Unfortunately, this kind of elastic distance does not enable direct construction of definite kernels which are useful when addressing regression, classification or clustering of time series. A fortiori, they do not make it possible to directly construct inner products involving some time elasticity, which are basically able to cope with some stretching or some compression along specific dimension. Recently, [10] have shown that it is quite easy to propose inner product with time elasticity capability at least for some restricted time series spaces, basically spaces containing uniformly sampled time series, all of which have the same lengths (in such cases, time series can be embedded easily in Euclidean spaces).\n\u2022 P.-F. Marteau is with Universite\u0301 de Bretagne Sud, IRISA (UMR 6074), Campus de Tohannic, 56000 Vannes, France E-mail: pierre-francois.marteau AT univ-ubs.fr\n\u2022 N. Bonnel is with Universite\u0301 de Bretagne Sud, IRISA (UMR 6074), Campus de Tohannic, 56000 Vannes, France E-mail: nicolas.bonnel AT univ-ubs.fr\n\u2022 G. Me\u0301nier is with Universite\u0301 de Bretagne Sud, IRISA (UMR 6074), Campus de Tohannic, 56000 Vannes, France E-mail: gildas.menier AT univ-ubs.fr\nThe aim of this paper is to derive an extension from this preliminary work for the construction of time elastic inner products, to achieve the construction of an elastic inner product structure for a quasi-unrestricted set of sequential data or time series, i.e. sets for which the data is not necessarily uniformly sampled and may have any lengths. Section two of the paper gives the main notations used throughout this paper and presents a recursive construction for inner-like products. It then gives the conditions and the proof of existence of elastic inner products (and elastic vector spaces) defined on a quasi-unrestricted set of time series while explaining what we mean by quasi-unrestricted. The third section succinctly presents some preliminary applications, mainly to highlight some of the features of elastic inner product vector spaces such as orthogonality. The fourth section presents two effective experimentations, the first one relating to time series classification and the second one addressing symbolic sequences classification."}, {"heading": "2 ELASTIC INNER PRODUCT VECTOR SPACES", "text": "Starting from the recursive structure of the Dynamic Time Warping (DTW) equation in which the non linear Max operator is replaced by a Sum operator, we propose a quite general and parameterized recursive equation that we call elastic product. Our goal in this section is to derive the conditions for which such elastic product is an inner product that maintains a form of time elasticity."}, {"heading": "2.1 Sequence and sequence element", "text": "Definition 2.1. Given a finite sequence A we denote by A(i) the ith element (symbol or sample) of sequence A. We will consider that A(i) \u2208 S \u00d7 T where (S,\u2295S ,\u2297S) is a vector space that embeds the multidimensional\nspace variables (e.g. S \u2282 Rd, with d \u2208 N+) and T \u2282 R embeds the timestamps variable, so that we can write A(i) = (a(i), ta(i)) where a(i) \u2208 S and ta(i) \u2208 T , with the condition that ta(i) > ta(j) whenever i > j (timestamps strictly increase in the sequence of samples). Aji with i \u2264 j is the subsequence consisting of the ith through the jth element (inclusive) of A. So A j i = A(i)A(i+1)...A(j). \u039b denotes the null element. By convention Aji with i > j is the null time series, e.g. \u2126."}, {"heading": "2.2 Sequence set", "text": "Definition 2.2. The set of all finite discrete time series is thus embedded in a spacetime characterized by a single discrete temporal dimension, that encodes the timestamps, and any number of spatial dimensions that encode the value of the time series at a given timestamp. We note U = {Ap1|p \u2208 N} the set of all finite discrete time series. Ap1 is a time series with discrete index varying between 1 and p. We note \u2126 the empty sequence (with null length) and by convention A01 = \u2126 so that \u2126 is a member of set U. |A| denotes the length of the sequence A. Let Up = {A \u2208 U | |A| \u2264 p} be the set of sequences whose length is shorter or equal to p. Finally let U\u2217 be the set of discrete time series defined on (S \u2212 {0S})\u00d7 T , i.e. the set of time series that do not contain the null spatial value. We denote by 0S the null value in S."}, {"heading": "2.3 Scalar multiplication on U\u2217", "text": "Definition 2.3. For all A \u2208 U\u2217 and all \u03bb \u2208 R, C = \u03bb\u2297A \u2208 U\u2217 is such that for all i \u2208 N such that 0 \u2264 i \u2264 |A|, C(i) = (\u03bb.a(i), ta(i)) and thus |C| = |A|."}, {"heading": "2.4 Addition on U\u2217", "text": "Definition 2.4. For all (A,B) \u2208 (U\u2217)2, the addition of A and B, noted C = A\u2295B \u2208 U\u2217, is defined in a constructive manner as follows: let i, j and k be in N.\nk = i = j = 1, As long as 1 \u2264 i \u2264 |A| and 1 \u2264 j \u2264 |B|, if ta(i) < tb(j), C(k) = (a(i), ta(i)) and i \u2190 i + 1, k \u2190 k + 1 else if ta(i) > tb(j), C(k) = (b(j), tb(j)) and j \u2190 j + 1, k \u2190 k + 1 else if a(i) + b(j) 6= 0, C(k) = (a(i) + b(j), ta(i)) and i \u2190 i+ 1, j \u2190 j + 1, k \u2190 k + 1 else i \u2190 i+ 1, j \u2190 j + 1\nThree comments need to be made at this level to clarify the semantic of the operator \u2295: i) Note that the \u2295 addition of two time series of equal\nlengths and uniformly sampled coincides with the classical addition in vector spaces. Fig. 1 gives an\nexample of the addition of two time series that are not uniformly sampled and that have different lengths, except that zero values are discarded to ensure that the sum of two time series will remain a member of U\u2217. ii) Implicitly (in light of the last case described in Def. 2.4), any sequence element of the sort (0S , t), where 0S is the null value in S and t \u2208 T must be assimilated to the null sequence element \u039b. For instance, the addition of A = (1, 1)(1, 2)with B = (\u22121, 1)(1, 2) is C = A \u2295 B = (2, 2): the addition of the two first sequence elements is (0, 1) that is assimilated to \u039b and as such suppressed in C.\niii) The \u2295 operator, when restricted to the set U\u2217 is reversible in that if C = A\u2295B then A = C\u2295((\u22121)\u2297B) or B = C \u2295 ((\u22121) \u2297 A). This is not the case if we consider the entire set U.\n2.5 Elastic product (ep)\nDefinition 2.5. A function < ., . >: U\u2217 \u00d7 U\u2217 \u2192 R is called an Elastic Product if, there exists a function f : S2 \u2192 R, a strictly positive function g : T 2 \u2192 R+ and three constants \u03b1, \u03b2 and \u03be in R such that, for any pair of sequences Ap1, B q 1 , the following recursive equation holds:\n< Ap1, B q 1 >ep=\n\u2211\n\n\n \u03b1\u00b7 < Ap\u221211 , Bq1 >ep \u03b2\u00b7 < Ap\u221211 , Bq\u221211 >ep +f(a(p), b(q)) \u00b7 g(ta(p), tb(q)) \u03b1\u00b7 < Ap1, Bq\u221211 >ep\n(1)\nThis recursive definition requires defining an initialization. To that end we set, \u2200A \u2208 U\u2217, < A,\u2126 >ep=< \u2126, A >ep=< \u2126,\u2126 >ep= \u03be, where \u03be is a real constant (typically we set \u03be = 0), and \u2126 is the null sequence, with the convention that Ai\nj = \u2126 whenever i > j.\nThis paper addresses the most interesting question of the existence of elastic inner products on the set U\u2217, i.e. without any restriction on the lengths of the considered time series nor the way they are sampled. If the choice of functions f and g, although constrained, is potentially large, we show hereinafter that the choice for constants \u03b1, \u03b2 and \u03be is unique.\n2.6 Existence of elastic inner products(eip) defined on U\u2217\nTheorem 2.1. < ., . >ep is an inner product on (U \u2217,\u2295,\u2297) iff:\ni) \u03be = 0. ii) g : (T \u00d7 T ) \u2192 R is symmetric and strictly positive, iii) f is an inner product on (S,\u2295S ,\u2297S), if we extend the\ndomain of f on S while setting f(0S, 0S) = 0. iv) \u03b1 = 1 and \u03b2 = \u22121,"}, {"heading": "2.6.1 proof of theorem 2.1", "text": "Proof of the direct implication Let us suppose first that < ., . >ep is an inner product defined on U\u2217. Then since < ., . >ep is positive-definite necessarily < \u2126,\u2126 >ep= \u03be = 0. Furthermore, for any A1 = (a, t1) and A2 = (a, t2) \u2208 U\u2217 with a 6= 0S , < A1, A2 >ep= g(t1, t2).f(a, a) 6= 0. Since < .; . >ep is symmetric, we get that g(t1, t2) = g(t2, t1) for any (t1, t2) \u2208 T 2 which establishes that g is symmetric. Since g is strictly positive by definition of < ., . >ep, i) and ii) are satisfied.\nFor any A = (a, ta) \u2208 U\u2217, < A,A >ep= f(a, a)g(ta, ta) > 0. Since g is strictly positive, then we get that f(a, a) > 0. If we set f(0S, 0S) = 0, we establish that f is positive-definite on S.\nSince \u03be = 0, for any A, B, and C \u2208 U\u2217 such that A = (a, t), B(b, t) and C = (c, tc), we have: < A\u2295B,C >ep= f(a\u2295S b, c).g(t, tc). As < A\u2295B,C >ep=< A,C >ep + < B,C >ep = f(a, c).g(t, tc) + f(b, c).g(t, tc) = (f(a, c) + f(b, c)).g(t, tc), As g is strictly positive, we get that f(a \u2295S b, c) = (f(a, c) + f(b, c)). Furthermore, < \u03bb\u2297A,C >ep= f(\u03bb\u2297S a, c).g(t, tc). As < \u03bb\u2297A,C >ep= \u03bb. < A,C >ep= \u03bb.f(a, c).g(t, tc) and g is strictly positive, we get that f(\u03bb\u2297S a, c) = \u03bb.f(a, c).\nThis shows that f is linear, symmetric and positivedefinite. Hence it is an inner product on (S,\u2295S ,\u2297S) and iii) is satisfied.\nLet us show that necessarily \u03b1 = 1 and \u03b2 = \u22121. To that end, let us consider any Ap1, B q 1 and C r 1 in U\n\u2217, such that p > 1, q > 1, r > 1 and such that ta(p) < tb(q), i.e. if Xs1 = A p 1 \u2295Bq1 , then Xs\u221211 = Ap1 \u2295Bq\u221211 . Since by hypothesis < ., . >ep is an inner product (U\u2217,\u2295,\u2297), it is linear and thus we can write: < Ap1 \u2295Bq1 , Cr1 >ep=< Ap1, Cr1 >ep + < Bq1 , Cr1 >ep.\nDecomposing < Ap1 \u2295Bq1 , Cr1 >ep, we obtain: < Ap1 \u2295Bq1 , Cr1 >ep= \u03b1. < Ap1 \u2295Bq\u221211 , Cr1 >ep + \u03b2. < Ap1 \u2295 Bq\u221211 , Cr\u221211 >ep +f(b(q), c(r)).g(tb(q), tc(r)) + \u03b1. < Ap1 \u2295Bq1 , Cr\u221211 >ep As < ., . >ep is linear we get: < Ap1\u2295Bq1 , Cr1 >ep= \u03b1. < Ap1, Cr1 >ep +\u03b1. < Bq\u221211 , Cr1 >ep + \u03b2. < Ap1, C r\u22121 1 >ep +\u03b2. < B q\u22121 1 , C r\u22121 1 >ep +f(b(q), c(r)).g(tb(q), tc(r))+ \u03b1. < Ap1, C r\u22121 1 >ep +\u03b1. < B q 1 , C r\u22121 1 >ep Hence, < Ap1\u2295Bq1 , Cr1 >ep= \u03b1. < Ap1, Cr1 >ep +\u03b2. < Ap1, Cr\u221211 >ep + \u03b1. < Ap1, C r\u22121 1 >ep + < B q 1 , C r 1 >ep\nIf we decompose < Ap1, C r 1 >ep, we get:\n< Ap1\u2295Bq1 , Cr1 >ep= (\u03b12+\u03b2+\u03b1) < Ap1, Cr\u221211 >ep +\u03b1.\u03b2. < Ap\u221211 , C r\u22121 1 >ep +\u03b1.f(a(p), c(r)).g(ta(p), tc(r)) + \u03b1 2. < Ap\u221211 , C r 1 >ep + < B q 1 , C r 1 >ep\nThus we have to identify < Ap1, C r 1 >ep=\n\u03b1. < Ap1, C r\u22121 1 >ep +\u03b2. < A p\u22121 1 , C r\u22121 1 >ep +f(a(p), c(r)).g(ta(p), tc(r)) + \u03b1. < A p\u22121 1 , C r 1 >ep with (\u03b12+\u03b2+\u03b1) < Ap1, C r\u22121 1 >ep +\u03b1.\u03b2. < A p\u22121 1 , C r\u22121 1 >ep +\u03b1.f(a(p), c(r)).g(ta(p), tc(r)) + \u03b1 2. < Ap\u221211 , C r 1 >ep.\nThe unique solution is \u03b1 = 1 and \u03b2 = \u22121. That is if < ., . >ep is an existing inner product, then necessarily \u03b1 = 1 and \u03b2 = \u22121, establishing iv).\nProof of the converse implication Let us suppose that i), ii), iii) and iv) are satisfied and show that < ., . >ep is an inner product on U \u2217.\nFirst, by construction, since f and g are symmetric, so is < ., . >ep.\nIt is easy to show by induction that < ., . >ep is nondecreasing with the length of its arguments, namely, \u2200Ap1 and Bq1 in U\u2217, < Ap1, B q 1 >ep \u2212 < Ap1, Bq\u221211 >ep\u2265 0. Let n = p + q. The proposition is true at rank n = 0. It is also true if Ap1 = \u2126, whatever B q 1 is, or B q 1 = \u2126, whatever < A p 1 is. Suppose it is true at a rank n \u2265 0, and consider Ap1 6= \u2126 and Bq1 6= \u2126 such that p+ q = n. By decomposing < Ap1, B q 1 >ep we get: < Ap1, B q 1 >ep \u2212 < Ap1, Bq\u221211 >ep= \u2212 < Ap\u221211 , Bq\u221211 >ep +f(a(p), b(q)).g(ta(p), tb(q))+ < A p\u22121 1 , B q 1 >ep Since f(a(p), b(q)).g(ta(p), tb(q)) > 0 and the proposition is true by inductive hypothesis at rank n, we get that < Ap1, B q 1 >ep \u2212 < Ap1, Bq\u221211 >ep) > 0. By induction the proposition is proved.\nLet us show by induction on the length of the time series the positive definiteness of < ., . >ep. At rank 0 we have < \u2126,\u2126 >ep= \u03be = 0. At rank 1, let us consider any time series of length 1, A11. < A11, A 1 1 >ep= f(a(1), a(1)).g(ta(1), ta(1)) > 0 by hypothesis on f and g. Let us suppose that the proposition is true at rank n > 1 and let consider any time series of length n+ 1, An+11 . Then, since \u03b1 = 1 and \u03b2 = \u22121, we get < An+11 , A n+1 1 >ep= 2. < A n+1 1 , A n 1 >ep \u2212 < An1 , An1 >ep + f(a(n+ 1), a(n+ 1)).g(ta(n+1), ta(n+1)). Since < An+11 , A n 1 >ep \u2212 < An1 , An1 >ep\u2265 0, and f(a(n + 1), a(n + 1)).g(ta(n+1), ta(n+1)) > 0, < An+11 , A n+1 1 >ep> 0, showing that the proposition is true at rank n + 1. By induction, the proposition is proved, which establishes the positive-definiteness of < ., . >ep since < A p 1, A p 1 >ep= 0 only if A p 1 = \u2126.\nLet us consider any \u03bb \u2208 R, and any Ap1, Bq1 in U\u2217 and show by induction on n = p+ q that< \u03bb\u2297 Ap1, Bq1 >ep=\n\u03bb. < Ap1, B q 1 >ep: The proposition is true at rank n = 0. Let us suppose that the proposition is true at rank n \u2265 0, i.e. for all r \u2264 n, and consider any pair Ap1, Bq1 of time series such that p+ q = n+ 1. We have: < \u03bb\u2297Ap1, Bq1 >ep= \u03b1. < \u03bb\u2297Ap1, Bq\u221211 >ep +\u03b2. < \u03bb\u2297Ap\u221211 , Bq\u221211 >ep +f(\u03bb\u2297Sa(p), b(q)).g(ta(p), tb(q))+\u03b1. < \u03bb\u2297Ap\u221211 , Bq1 >ep Since f is linear on (S,\u2295S ,\u2297S), and since the proposition is true by hypothesis at rank n, we get that < \u03bb \u2297 Ap1, Bq1 >ep= \u03bb.\u03b1 < Ap1, Bq\u221211 >ep +\u03bb.\u03b2. < Ap\u221211 , B q\u22121 1 >ep +\u03bb.f(a(p), b(q)).g(ta(p), tb(q)) + \u03bb.\u03b1. < Ap\u221211 , B q 1 >ep= \u03bb. < A p 1, B q 1 >ep. By induction, the proposition is true for any n, and we have proved this proposition. Furthermore, for any Ap1, B q 1 and C r 1 in U\n\u2217, let us show by induction on n = p+ q+ r that < Ap1 \u2295Bq1 , Cr1 >ep=< Ap1, C r 1 >ep + < B q 1 , C r 1 >ep. Let X s 1 be equal to A p 1 \u2295Bq1 . The proposition is obviously true at rank n = 0. Let us suppose that it is true up to rank n \u2265 0, and consider any Ap1, B q 1 and C r 1 such that p+ q + r = n+ 1.\nThree cases need then to be considered:\n1) if Xs\u221211 = A p\u22121 1 \u2295Bq\u221211 , then ta(p) = tb(q) = t and\n< Ap1 \u2295B q 1 , C r 1 >ep= \u03b1. < A p 1 \u2295B q 1 , C r\u22121 1 >ep +\n\u03b2. < Ap\u221211 \u2295Bq\u221211 , Cr\u221211 >ep + f((a(p) + b(q)), c(r)).g(t, tc(r))+\n\u03b1. < Ap\u221211 \u2295Bq\u221211 , Cr1 >ep. Since f is linear on (S,\u2295S ,\u2297S), and the proposition true at rank n, we get the result.\n2) if Xs\u221211 = A p 1 \u2295Bq\u221211 , then ta(p) < tb(q) = t and\n< Ap1 \u2295 Bq1 , Cr1 >ep= \u03b1. < Ap1 \u2295 Bq1 , Cr\u221211 >ep +\u03b2. < Ap1 \u2295 Bq\u221211 , Cr\u221211 >ep +f(b(q), c(r)).g(t, tc(r)) + \u03b1. < Ap1 \u2295 Bq\u221211 , Cr1 >ep. Having \u03b1 = 1 and \u03b2 = \u22121 with the proposition supposed to be true at rank n we get the result. 3) if Xs\u221211 = A p\u22121 1 \u2295Bq\u221211 , we proceed similarly to case\n2).\nThus the proposition is true at rank n+ 1, and by induction the proposition is true for all n. This establishes the linearity of < ., . >ep. This ends the proof of the converse implication and theorem 2.1 is therefore established . The existence of functions f and g entering into the definition of < ., . >ep and satisfying the conditions allowing for the construction of an inner product on (U\u2217,\u2295,\u2297) is ensured by the following proposition: Proposition 2.2. The functions f : S2 \u2192 R defined as f(a, b) =< a, b >S where < ., . >S is an inner product on (S,\u2295S ,\u2297S) and g : T 2 \u2192 R defined as f(ta, tb) = e\u2212\u03bd\u00b7d(ta,tb), where d is a distance defined on T 2 and \u03bd \u2208 R+, satisfy the conditions required to construct an elastic inner product on (U\u2217,\u2295,\u2297). The proof of Prop.2.2 is obvious. This proposition establishes the existence of ep inner products, that we will denote eip (Time Elastic Inner Product). An eip as\nthus the following structure:\n< Ap1, B q 1 >eip=\n\u2211\n\n  \n  \n< Ap\u221211 , B q 1 >eip \u2212 < Ap\u221211 , Bq\u221211 >eip + g(ta(p), tb(q))\u00b7 < (a(p), b(q) >eip(S) < Ap1, B q\u22121 1 >eip\n(2)\nWith the initialization: \u2200A \u2208 U\u2217, < A,\u2126 >eip=< \u2126, A >eip=< \u2126,\u2126 >eip= \u03be, where \u03be is a real constant (typically we set \u03be = 0), and \u2126 is the null sequence, with the convention that Ai j = \u2126 whenever i > j.\nNote that < ., . >S can be chosen to be a eip as well, in the case where a second time elastic dimension is required. This leads naturally to recursive definitions for ep and eip.\nProposition 2.3. For any n \u2208 N, and any discrete subset T = {t1, t2, \u00b7 \u00b7 \u00b7 , tn} \u2282 R, let Un,R,T be the set of all time series defined on R\u00d7 T whose lengths are n (the time series in Un,R,T are considered to be uniformly sampled). Then, the eip on Un,R constructed from the functions f and g defined in Prop. 2.2 tends towards the Euclidean inner product when \u03bd \u2192 \u221e if S is an Euclidean space and < a, b >S is the Euclidean inner product defined on S.\nThe proof of Prop.2.3 is straightforward and is omitted. This proposition shows that eip generalizes the classical Euclidean inner product."}, {"heading": "2.7 Algorithmic complexity", "text": "The general complexity associated to the calculation of the eip of two time series of lengths p and q as specified by Eq.2 is O(p \u00b7 q). Basically this is the same complexity as that required for the calculation of the complete dynamic programming solutions such as the Dynamic Time Warping (DTW) [20] [13] algorithm evaluated on these two time series. Nevertheless, as discussed in section 3.3, Prop. 3.2 allows for efficient implementations of retrieval process (in linear time complexity) once a straightforward indexing phase has been implemented. This result is demonstrated in practice the experimentation section (sec. 4)."}, {"heading": "3 SOME PRELIMINARY APPLICATIONS", "text": "We present in the following sections some applications to highlight the properties of Elastic Inner Product Vector Spaces (EIPV S).\n3.1 Distance in EIPV S\nThe following proposition provides U\u2217 with a norm and a distance, both induced by a eip.\nProposition 3.1. For all Ap1 \u2208 U\u2217, and any < ., . >eip defined on (U\u2217,\u2295,\u2297) \u221a\n< Ap1, A p 1 >eip is a norm on U \u2217.\nFor all pair (Ap1, B q 1) \u2208 (U\u2217)2, and any eip defined on (U\u2217,\u2295,\u2297), \u03b4eip(A p 1, B q 1) = \u221a\n< Ap1 \u2295 (\u22121.\u2297Bq1), Ap1 \u2295 (\u22121.\u2297Bq1) >eip = \u221a\n< Ap1, A p 1 >eip + < B q 1 , B q 1 >eip \u22122\u00b7 < Ap1, Bq1 >eip\ndefines a distance metric on U\u2217.\nThe proof of Prop. 3.1 is straightforward and is omitted.\n3.2 Orthogonalization in EIPV S\nTo exemplify the effect of elasticity in EIPV S, we give below the result of the Gram-Schmidt orthogonalization algorithm for two families of independent univariate time series. The first family is composed of uniformly sampled time series having increasing lengths. The second family (a sine-cosine basis) is composed of uniformly sampled time series, all of which have the same length.\nThe tests which are described in the next sections were performed on a set U\u2217 of discrete time series whose elements are defined on (R \u2212 {0} \u00d7 [0; 1])2 using the following eip:\n< Ap1, B q 1 >eip=\n\u2211\n\n  \n  \n< Ap1, B q\u22121 1 >eip \u2212 < Ap\u221211 , Bq\u221211 >eip + a(p)b(q) \u00b7 e\u2212\u03bd.|ta(p)\u2212tb(q)|2 < Ap\u221211 , B q 1 >eip\n(3)"}, {"heading": "3.2.1 Orthogonalization of an independent family of time series with increasing lengths", "text": "The family of time series we are considering is composed of 11 time series uniformly sampled, whose lengths are 11 samples:\n(1, 0) (\u01eb, 0)(1, 1/10) (\u01eb, 0)(\u01eb, 0)(1, 1/10) \u00b7 \u00b7 \u00b7 (\u01eb, 0)(\u01eb, 1/10)(\u01eb, 2/10) \u00b7 \u00b7 \u00b7 (1, 1)\n(4)\nSince, the zero value cannot be used for the space dimension, we replaced it by \u01eb, which is the smallest non zero positive real for our test machine (i.e. 2\u22121074). The result of the Gram-Schmidt orthogonalization process using \u03bd = .01 on this basis is given in Fig.2."}, {"heading": "3.2.2 Orthogonalization of a sine-cosine basis", "text": "An orthonormal family of discrete sine-cosine functions is not anymore orthogonal in a EIPV S. The result of the Gram-Schmidt orthogonalization process using \u03bd = .01 when applied on a discrete sine-cosine basis is given in Fig.3, in which only the 8 first components are displayed. The lengths of the waves are 128 samples."}, {"heading": "3.3 Indexing for fast retrieval in time series data bases", "text": "Prop.2.3 shows how the elastic inner product generalizes the Euclidean inner product when time series are embedded into a finite dimensional vector space (one dimension per timestamps). We consider in this subsection such embeddings with the convention that if a time series has no value for a given timestamps a zero value is added on the dimension corresponding to the\nmissing timestamps. Each time series is thus represented by a finite dimensional vector, let say in Rn.\nProposition 3.2. Given a symmetric and strictly positive function g, let consider the so-called n\u00d7 n symmetric elastic matrix in Rn 2 defined as:\nE =\n\n    \ng(t1, t1) g(t1, t2) g(t1, t3) \u00b7 \u00b7 \u00b7 g(t1, tn) g(t2, t1) g(t2, t2) g(t2, t3) \u00b7 \u00b7 \u00b7 g(t2, tn) g(t3, t1) g(t3, t2) g(t3, t3) \u00b7 \u00b7 \u00b7 g(t3, tn)\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 g(tn, t1) g(tn, t2) g(tn, t3) \u00b7 \u00b7 \u00b7 g(tn, tn)\n\n    \nand two time series An1 and B n 1 represented by two vectors\nin Rn. Then the elastic inner product defined recursively as:\n< An1 , B n 1 >eip=\n\u2211\n\n  \n  \n< An\u221211 , B n 1 >eip \u2212 < An\u221211 , Bn\u221211 >eip + a(n)b(n) \u00b7 g(ta(n), tb(n)) < An1 , B n\u22121 1 >eip\n(5)\nidentifies to the following matrix products [An1 ] T E [Bn1 ]\nThis result is quite interesting in the scope of time series information retrieval especially when addressing very large databases. LetD = {B(i)n1}i=1\u00b7\u00b7\u00b7m be a time series database of size m, and consider the indexing phase that consists in constructing DE = {E [B(i)n1 ]}i=1\u00b7\u00b7\u00b7m. Then the retrieval of all the time series in D elastically similar to a given request An1 will require the computation of < An1 , B(i) n 1 >eip, for i \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, which reduces to evaluating:\n< An1 , B(i) n 1 >eip= [A n 1 ] T [EB(i)n1 ] (6)\nfor i \u2208 {1, \u00b7 \u00b7 \u00b7 ,m} which is done in linear complexity. Basically this construction breaks the quadratic complexity of the eip at retrieval stage and meets the same linear complexity as the classical Euclidean inner product. Moreover one can notice that the eip defined by Eq.5 has it continuous time equivalent that expresses as: < a, b >= \u222b\u222b a(t)b(\u03c4)g(t, \u03c4) dtd\u03c4\n3.4 Kernel methods in EIPV S\nA wide range of literature exists on kernel theory, among which [3], [15] and [16] present some large syntheses of major results. We give hereinafter basic definitions and immediate results regarding kernel construction based on eip.\nDefinition 3.1. A kernel on a non empty set U refers to a complex (or real) valued symmetric function \u03d5(x, y) : U \u00d7 U \u2192 C (or R). Definition 3.2. Let U be a non empty set. A function \u03d5 : U \u00d7 U \u2192 C is called a positive (resp. negative) definite kernel if and only if it is Hermitian (i.e. \u03d5(x, y) = \u03d5(y, x) where the overline stands for\nthe conjugate number) for all x and y in U and \u2211n\ni,j=1 cic\u0304j\u03d5(xi, xj) \u2265 0 (resp. \u2211n i,j=1 cic\u0304j\u03d5(xi, xj) \u2264 0), for all n in N, (x1, x2, ..., xn) \u2208 Un and (c1, c2, ..., cn) \u2208 Cn.\nDefinition 3.3. Let U be a non empty set. A function \u03d5 : U \u00d7 U \u2192 C is called a conditionally positive (resp. conditionally negative) definite kernel if and only if it is Hermitian (i.e. \u03d5(x, y) = \u03d5(y, x) for all x and y in U ) and\n\u2211n i,j=1 cic\u0304j\u03d5(xi, xj) \u2265 0\n(resp. \u2211n i,j=1 cic\u0304j\u03d5(xi, xj) \u2264 0), for all n \u2265 2 in N, (x1, x2, ..., xn) \u2208 Un and (c1, c2, ..., cn) \u2208 Cn with \u2211n\ni=1 ci = 0.\nIn the last two above definitions, it is easy to show that it is sufficient to consider mutually different elements in U , i.e. collections of distinct elements x1, x2, ..., xn.\nDefinition 3.4. A positive (resp. negative) definite kernel defined on a finite set U is also called a positive (resp. negative) semidefinite matrix. Similarly, a positive (resp. negative) conditionally definite kernel defined on a finite set is also called a positive (resp. negative) conditionally semidefinite matrix.\n3.4.1 Definiteness of eip based kernel Proposition 3.3. A eip is a positive definite kernel.\nThe proof of Prop. 3.3 is straightforward and is omitted. The consequence is that numerous definite (positive or negative) kernels can be derived from a eip; among other immediate derivations it can be stated that:\n\u2022 (< ., . >eip) p is positive definite for all p \u2208 N\n(polynomial kernel). \u2022 \u03b4eip defined by Prop.3.1 is negative definite. \u2022 e\u2212\u03bd\u00b7\u03b4eip is positive definite for all \u03bd > 0. \u2022 e\u2212\u03bd\u00b7(\u03b4eip) p\nis positive definite for all \u03bd > 0 and any 0 < p \u2264 2.\n\u2022 e<A,B>eip is positive definite.\nSome experimentations on Support vector Machine involving elastic kernels are reported in Sec.4.\n3.5 Elastic Cosine similarity in EIPV S, with application to symbolic (e.g. textual) information retrieval\nSimilarly to the definition of the cosine of two vectors in Euclidean space, we define the elastic cosine of two sequences by using any ep that satisfies the conditions of theorem 2.1.\nDefinition 3.5. Given two sequences, A and B, the elastic cosine similarity of these two sequences is given using a time elastic inner product < X, Y >e and the induced norm \u2016X\u2016e = \u221a < X,X >e as similarity = eCOS(\u03b8) = <A\u00b7B>e\u2016A\u2016e\u2016B\u2016e\nIn the case of textual or sequential data information retrieval, namely text matching or sequence matching, the\ntimestamps variable coincides with the index of words into the text or sequence, and the spatial dimensions encode the words or symbol into a given dictionary or alphabet. For instance, in text mining, each word can be represented using a vector whose dimension is the size of the set of concepts (or senses) that covers the conceptual domain associated to the dictionary, and each coordinate value, that is selected into [0; 1], encodes the degree of presence of the concept or senses into the considered word. In any case, the elastic cosine similarity measure takes value into [0; 1], 0 indicating the lowest possible similarity value between two sequential data and 1 the greatest possible similarity value between two sequential data. The elastic cosine similarity takes into account the order of occurrence of the words or symbols into the sequential data which could be an advantage compared to the Euclidean cosine measure that does not cope at all with the words or symbols ordering. Let us consider the following elastic inner product dedicated to text matching. In the following definition, Ap1 and B q 1 are sequences of words that represent textual content.\nDefinition 3.6.\n< Ap1, B q 1 >eiptm=\n\u2211\n\n  \n \n< Ap\u221211 , B q 1 >eiptm \u2212 < Ap\u221211 , Bq\u221211 >eiptm + e\u2212\u03bd.|ta(p)\u2212tb(q)| 2 \u03b4(a(p), b(q)) < Ap1, B q\u22121 1 >eiptm\n(7)\nwhere a(p) and b(q) are vectors whose coordinates identify words with weightings, \u03b4(a, b) =< a, b > is the Euclidan inner product, and \u03bd a time stiffness parameter.\nProposition 3.4. For \u03bd = 0 and \u03b4 redefined as \u03b4(a, b) = 1 if a = b, 0 otherwise, the elastic inner product defined in Eq.7 coincides with the euclidean inner product between two vectors whose coordinates correspond to term frequencies observed into the Ap1 and B q 1 text sequences. If, we change the definition of \u03b4 by the \u03b4(a, b) = (IDF (a))2 if a = b, 0 otherwise, where IDF (a) is the inverse document frequency of term a into the considered collection, then for \u03bd = 0, < Ap1, B q 1 >eiptm coincides with the euclidean inner product between two vectors whose coordinates correspond to the TFIDF (term frequency times the inverse document frequency) of terms occurring into the Ap1 and B q 1 text sequences.\nThe proof of proposition 3.4 is straightforward an is omitted.\nThus, the elastic cosine measure derived from the elastic inner product defined by Eq.7 generalizes somehow the cosine measure implemented in the vector space model [14] and commonly used in the text information retrieval community. To exemplify the behavior of the elastic cosine similarity on sequential data, we consider the four sequences depicted in Eq.8. The variations of the elastic cosine as\na function of \u03bd evaluated on pairwise sequences are reported in Fig.4\nA = [abababab] B = [aaaabbbb] C = [bbbbaaaa] D = [bbaa]\n(8)\nIn Fig.4 the extreme left part of the curves, characterized with high \u03bd values, corresponds, when applied on sequences having the same length and represented by vectors, to the cosine similarity constructed with the Euclidean inner product. The right part of the curves, characterized with very low \u03bd values, corresponds to the cosine similarity evaluated using the tf(-idf) vector space model. The center part of the figure, characterized with medium \u03bd values shows that elasticity allows to better discriminating between the sequences."}, {"heading": "4 EXPERIMENTATIONS", "text": ""}, {"heading": "4.1 Time series classification", "text": "We empirically evaluate the effectiveness of the distance induced by an elastic inner product comparatively to the Euclidean and the Dynamic Time Warping distances using some classification tasks on a set of time series coming from quite different application fields. The classification task we have considered consists of assigning one of the possible categories to an unknown time series for the 20 data sets available at the UCR repository [8]. As time is not explicitly given for these datasets, we used the index value of the samples as the timestamps for the whole experiment.\nFor each dataset, a training subset (TRAIN) is defined as well as an independent testing subset (TEST). We use the training sets to train two kinds of classifiers:\n\u2022 the first one is a first near neighbor (1-NN) classifier: first we select a training data set containing time series for which the correct category is known. To assign a category to an unknown time series selected from a testing data set (different from the train set), we select its nearest neighbor (in the sense of a distance or similarity measure) within the training data set, then, assign the associated category to its nearest neighbor. For that experiment, a leave one out procedure is performed on the training dataset to optimize the meta parameter \u03bd of the considered elastic distance. \u2022 the second one is a SVM classifier [4], [19] configured with a Gaussian RBF kernel whose parameters are C > 0, a trade-off between regularization and constraint violation and \u03c3 that determines the width of the Gaussian function. To determine the C and \u03c3 hyper parameter values, we adopt a 5-folded cross-validation method on each training subset. According to this procedure, given a predefined training set TRAIN and a test set TEST, we adapt the meta parameters based on the training set TRAIN: we first divide TRAIN into 5 stratified subsets TRAIN1, TRAIN2, \u00b7 \u00b7 \u00b7 , TRAIN5; then for each subset TRAINi we use it as a new test set, and regard (TRAIN \u2212 TRAINi) as a new training set; Based on the average error rate obtained on the five classification tasks, the optimal values of meta parameters are selected as the ones leading to the minimal average error rate. For the elastic kernel, the meta parameter \u03bd is also optimized using this 5-folded cross-validation method performed on the training datasets.\nThe classification error rates are then estimated on the TEST datasets on the basis of the parameter values\noptimized on the TRAIN datasets. We have used the LIBSVM library [6] to implement the SVM classifiers. We tested the time elastic inner product < A,B >eip (Eq.3). Precisely, we used the timewarp distance induced by < A,B >eip, basically \u03b4eip(A,B) = (< A\u2212 B,A\u2212B >eip)1/2 = (< A,A >eip + < B,B >eip \u22122. < A,B >eip)1/2.\nTo speed up the computation at exploitation stage, we have used the construction proposed in Sec.3.3 Prop.3.2 with the following elastic Matrix E, where \u03bd > 0 :\nE = \n     \n1.0 e\u2212\u03bd|t1\u2212t2| 2 e\u2212\u03bd|t1\u2212t3| 2 \u00b7 \u00b7 \u00b7 e\u2212\u03bd|t1\u2212tn|2\ne\u2212\u03bd|t2\u2212t1| 2\n1.0 e\u2212\u03bd|t2\u2212t3| 2 \u00b7 \u00b7 \u00b7 e\u2212\u03bd|t2\u2212tn|2\ne\u2212\u03bd|t3\u2212t1| 2 e\u2212\u03bd|t3\u2212t2| 2 1.0 \u00b7 \u00b7 \u00b7 e\u2212\u03bd|t3\u2212tn|2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 e\u2212\u03bd|tn\u2212t1| 2 e\u2212\u03bd|tn\u2212t2| 2 e\u2212\u03bd|tn\u2212t3| 2 \u00b7 \u00b7 \u00b7 1.0\n\n     \nThe databases DE = {EB(i)n1}i=1\u00b7\u00b7\u00b7m are thus constructed off-line from the TRAIN datasets, once the optimization procedure of the \u03bd parameter has been completed. DE is then exploited on-line by the 1-NN and SVM classifiers."}, {"heading": "4.1.1 Meta parameters", "text": "\u03b4eip is characterized by the meta parameter \u03bd (the stiffness parameter) that is optimized for each dataset on the train data by minimizing the classification error rate of a first near neighbor classifier. For this kernel, \u03bd is selected in {100, 10, 1, .1, .01, ..., 1e\u2212 5, 0}.\nTo explore the potential benefits of an elastic inner product against the Euclidean inner product, we also tested the Euclidean Distance \u03b4ed which, as already stated, is the limit when \u03bd \u2192 \u221e of \u03b4eip.\nThe kernels exploited by the SVM classifiers are the\nGaussian kernels Keip(A,B) = e \u2212\u03b4eip(A,B) 2/(2\u00b7\u03c32) and Ked(A,B) = e \u2212\u03b4ed(A,B)\n2/(2\u00b7\u03c32). The meta parameter C is selected from the discrete set {2\u22128, 2\u22127, ..., 1, 2, ..., 210}, and \u03c32 from {2\u22125, 2\u22124, ..., 1, 2, ..., 210}. Table 1 gives for each data set and each tested kernels (Ked and Keip) the corresponding optimized values of the meta parameters.\nAccording to the classification results, this experiment shows that the distance induced by the elastic inner product \u03b4eip is significantly more effective for the considered tasks comparatively to the Euclidean distance. It exhibits, on average, the lowest error rates for most of the tested datasets and for both the 1-NN and SVM classifiers, as shown in Table 2 and Figures 5 and 6. The stiffness parameter \u03bd in \u03b4eip seems to play a significant role in these classification tasks, and this for a quite large majority of data sets.\nOnly one dataset, yoga, is better classified by the 1-NN \u03b4ed classifier on the test data, although the error rate on the train data is lower for the 1-NN \u03b4eip classifier. For the SVM classifiers, only two datasets, Face (all) and Coffee, are significantly better classified on the test data by SVM Ked classifiers. Nevertheless, for these two datasets, Keip reaches a better (or best) score on the train data. We are facing here the trade-off between learning and generalization capabilities. The meta parameter \u03bd is selected such as to minimized the classification error on the train data. If this strategy is on average a winning strategy, some datasets show that it does not always lead to a good trade-off, this is the case for Face (all) and yoga datasets. However, \u03b4dtw based classifiers outperform \u03b4eip based classifiers on a majority of datasets. The average ranking of the classifiers shows clearly that \u03b4eip ranks in between \u03b4ed and \u03b4dtw. \u03b4eip can be considered as a compromise between the linear Euclidean distance and the nonlinear \u03b4dtw. It maintains a low computational cost and nevertheless compensates some of the limitations of the Euclidean distance that it is very sensitive to distortions in the time axis. It should be noted that \u03b4eip can cope with sample substitution, deletion and insertion as well as \u03b4dtw. In addition, \u03b4eip can deal with sample permutations while \u03b4dtw cannot."}, {"heading": "4.2 Sequence classification", "text": "We report here a protein classification experiment carried out using the Protein Classification Benchmark Collection (PCBC) [18] [1]. This benchmark contains structural and functional annotations of proteins. The two datasets that we have exploited, SCOP95 and CATH95, are available at http://hydra.icgeb.trieste.it/benchmark. The entries of the SCOP95 dataset are characterized by sequences with variable lengths and relatively little sequence similarity (less than 95% sequence identity) between the protein families. The CATH95 dataset contains near-identical protein families of variable lengths in which the proteins have a high sequence similarity (more than 95% sequence identity). Basically, the considered classification tasks involve protein domain sequence and structure comparisons at various levels of the structural hierarchies. We have considered the following 14 PCB subsets:\n\u2022 PCB00001 SCOP95 Superfamily Family \u2022 PCB00002 SCOP95 Superfamily 5fold \u2022 PCB00003 SCOP95 Fold Superfamily \u2022 PCB00004 SCOP95 Fold 5fold \u2022 PCB00005 SCOP95 Class Fold \u2022 PCB00006 SCOP95 Class 5fold \u2022 PCB00007 CATH95 Homology Similarity \u2022 PCB00008 CATH95 Homology 5fold \u2022 PCB00009 CATH95 Topology Homology \u2022 PCB00010 CATH95 Topology 5fold \u2022 PCB00011 CATH95Architecture Topology\nTABLE 1 Dataset sizes and meta parameters used in conjunction with Ked, Keip and Kdtw kernels\nDATASET length|#class|#train|#test Ked : C, \u03c3 KDeip : \u03bd,C, \u03c3 KDTW : C, \u03c3 Synthetic control 60|6|300|300 1.0;0.125 .1;.25;.0625 8.0;4.0 Gun-Point 150|2|50|150 256;.5 0.01;256;.5 16.0;0.0312 CBF 128|3|30|900 8;1.0 .001;4.0;.0312 1.0;1.0 Face (all) 131|14|560|1690 4;0.5 .1;8.0;.5 2.0;0.25 OSU Leaf 427|6|200|242 2;0.125 .1;4;0.125 4.0;0.062 Swedish Leaf 128|15|500|625 128;0.125 10;8;.0625 4.0;0.031 50 Words 270|50|450|455 32;0.5 0.01;32;0.5 4.0;0.062 Trace 275|4|100|100 8;0.0156 .001;256;.0625 4;0.25 Two Patterns 128 |4|1000|4000 4.0;0.25 .01;1;.0312 0.25,0.125 Wafer 152|2|1000|6174 4.0;0.5 0.01;32;.5 1.0;0.016 face (four) 350|4|24|88 8.0;2.0 .01;16;2 16;0.5 Ligthing2 637|2|60|61 2.0;0.125 .001;128;2 2.0;0.031 Ligthing7 319|7|70|73 32.0;256.0 .1;16;.5 4;0.25 ECG200 96|2|100|100 8.0;0.25 0, 1024, .0625 2;0.62 Adiac 176|37|390|391 1024.0;0.125 10;256.0;.0312 16;0.0039 Yoga 426|2|300|3000 64.0;0.125 1;32;.0625 4;0.008 Fish 463|7|175|175 64.0;1.0 .01;256;1 8;0.016 Coffee 286|2|28|28 128.0;4.0 .01;1024;4 8;0.062 OliveOil 570|4|30|30 2.0;0.125 .01;64;2 2;0.125 Beef 470|5|30|30 128.0;4.0 0;64;.5 16;0.016\nTABLE 2 Comparative study using the UCR datasets: classification error rates (in %) obtained using the first near neighbor (1-NN) classification rule and a SVM classifier for the Ked, Keip and Keip kernels. Two scores are given S1|S2: the first one, S1, is evaluated on the training data, while the second one, S2, is evaluated on the test data. For the each classification methods (1-NN and SVM) the rank of each classifier is given in parenthesis ((1): best classifier, (2): 2nd best classifier, (3): 3rd best classifier\nDATASET 1-NN \u03b4ed 1-NN \u03b4eip 1-NN \u03b4dtw SVM Ked SVM Keip SVM Kdtw Synthetic control 9(3)|12(3) .67(1)|1(2) 1.0(2)|0.67(1) 3(3)|2.33(3) .33(2)|.67(1) 0(1)|1(2) Gun-Point 4.0(1)|8.67(1) 4.0(1)|8.67(1) 18.36(3)|9.3(3) 4.0(3)|6.0(3) 2.0(2)|2.67(2) 0(1)|1.33(1) CBF 16.67(3)|14.78(3) 3.33(2)|4.22(2) 0(1)|0.33(1) 3.33(2)|10.89(3) 3.33(1)|5(1) 3.33(1)|5.44(2) Face (all) 11.25(3)|28.64(3) 7.5(2)|26.33(2) 6.8(1)|19.23(1) 9.82(3)|16.45(3) 6.25(2)|24.91(2) .54(1)|16.98(1) OSU Leaf 37.0(2)|48.35(2) 37(2)|48.35(2) 33.17(1)|40.9(1) 35(3)|44.21(2) 34.5(2)|44.21(2) 20(1)|23.55(1) Swedish Leaf 26.6(3)|21.12(3) 24.4(1)|20.96(2) 24.65(2)|20.8(1) 15(2)|8.64(2) 15(2)|8.64(2) 7(1)|5.6(1) 50 Words 34.47(3)|36.32(3) 32.2(1)|32.73(2) 33.18(2)|31(1) 33.56(3)|30.99(3) 31.78(2)|29.67(2) 15.21(1)|17.58(1) Trace 18(3)|24(2) 16(2)|24(2) 0(1)|0(1) 9(3)|19(3) 1(2)|7(2) 0(1)|2(1) Two Patterns 8.5(3)|9.32(3) 4.3(2) |3.62(2) 0(1)|0(1) 8.6(3)|7.45(3) 5.5(2)|3.52(2) 0(1)|0(1) Wafer 0.7(2)|0.45(2) .5(1)|.42(1) 1.4(3)|2.01(3) .7(3)|.7(3) .2(2)|.68(2) 0(1)|0.39(1) face (four) 37.5(3)|21.59(3) 33.33(2)|19.31(2) 26.09(1)|17.05(1) 20.84(3)|19.31(3) 16.67(2)|13.63(2) 8.33(1)|5.68(1) Ligthing2 25.0(3)|24.59(3) 20(2)|16.39(2) 13.56(1)|13.1(1) 21.77(3)|31.14(3) 20(2)|26.22(2) 8.33(1)|19.67(1) Ligthing7 35.71(3)|42.47(3) 30.0(1)|32.87(2) 33.33(2)|27.4(1) 37.14(3)|36.98(3) 34.29(2)|35.61(2) 17.14(1)|16.43(1) ECG200 14.0(2)|12.0(2) 1.0(1)|2.0(1) 23.23(3)|23(3) 8.0(3)|9.0(2) 3.0(1)|7.0(1) 7(2)|13(3) Adiac 41.28(3)|38.87(1) 39.59(1)|38.87(1) 40.62(2)|39.64(3) 26.67(3)|24.04(1) 25.13(2)|24.04(1) 24.61(1)|25.32(3) Yoga 22.67(3)|16.9(2) 21.67(2)|22.26(3) 16.37(1)|16.4(1) 17.66(3)|14.43(2) 15.33(2)|14.4(2) 11(1)|11.2(1) Fish 24.0(1)|21.71(2) 24.0(1)|21.71(2) 26.44(3)|16.57(1) 14.86(3)|13.14(3) 13.14(2)|12.57(2) 6.86(1)|4.57(1) Coffee 21.43(2)|25.0(2) 21.43(2)|25.0(2) 14.81(1)|17.86(1) 0(1)|0(1) 0(1)|7.14(2) 10.71(3)|17.86(3) OliveOil 13.33(1)|13.33(1) 13.33(1)|13.33(1) 13.79(3)|13.33(1) 10.0(1)|10.0(1) 10.0(1)|10.0(1) 13.33(3)|16.67(3) Beef 46.67(1)|46.67(1) 46.67(1)|46.67(1) 55.17(3)|50(3) 37.67(2)|30(1) 37.67(2)|30(1) 32.14(1)|42.85(3) Average Rank (2.4)|(2.25) (1.45)|(1.75) (1.85)|(1.5) (2.65)|(2.4) (1.8)|(1.7) (1.25)|(1.6)\n\u2022 PCB00012 CATH95 Architecture 5fold \u2022 PCB00013 CATH95 Class Architecture \u2022 PCB00014 CATH95 Class 5fold\nWe evaluate the elastic cosine similarity based on the eip defined for symbolic sequences (Def.3.6, Eq.7) comparatively to five other similarity measures commonly used in Bioinformatics:\n\u2022 BLAST [2]: the Basic Local Alignment Search Tool is a very popular family of fast heuristic search methods used for finding similar regions between two or more nucleotides or amino acids.\n\u2022 SW [17]: The SmithWaterman algorithm is used for performing local sequence alignment, for determining similar regions between two nucleotide or protein sequences. Instead of looking at the sequence globally as NW does, the SmithWaterman algorithm compares subsequences of all possible lengths. \u2022 NW [11]: The Needleman Wunsch algorithm performs a maximal global alignment of two strings. It is commonly used in bioinformatics to align protein sequences or nucleotides. \u2022 LA kernel [12]: The Local Alignment kernel is used\nto detect local alignment between strings by convolving simple basic kernels. Its construction mimic the local alignment scoring schemes proposed in the\nSW algorithm. \u2022 PRIDE [5]: The PRIDE score is estimated as the\nPRobability of IDEntity between two protein 3D\nstructures. The calculation of similarity between two proteins, is based on the comparison of histograms of the pairwise distances between C \u2212 \u03b1 residues whose distribution is highly characteristic of protein folds.\nThe average AUC (area under the ROC Curve) measure is evaluated for 1-NN classifiers exploiting respectively BLAST, SW, NW, LA, PRIDE and eCOS(\u03bd) as alignment methods. One can notice that these datasets are quite well suited for global alignment since, as shown in table 3, the NW algorithm performs better than the SW algorithm. The eip structure that considers global alignment is thus well adapted to the task. We show on these experiments that for \u03bd = .05 the eCOS classifier in average performs significantly better than BLAST heuristics [2] and LA [12], the local alignment kernel for string. Furthermore, it performs almost as well as the SW and NW algorithms. The PRIDE method [5] gets the best results, but it uses the tertiary structure of the proteins while all the other methods exploit the primary structure. Here again, a ranking based on eCOS similarity has a complexity that could be maintained linear at exploitation stage (i.e. when testing numerous sequences against massive datasets). These very positive results offer perspective in fast filtering of biological symbolic sequences."}, {"heading": "4.3 Experimental complexity", "text": "To evaluate in practice the computational cost of \u03b4eip, we compare it with two other distances, namely \u03b4ed (the Euclidean distance) which has a linear complexity, and \u03b4dtw, the Dynamic Time Warping distance which has a quadratic complexity. In addition we evaluate the computational cost of the indexed version of \u03b4eip that we refer to as \u03b4i\u2212eip. The experiment consists in producing random datasets of 100 time series of increasing lengths (10, 100, 1000 and 10000 samples) and computing the\n100x100 distance matrices. Figure 7 gives the elapsed time in second, according to a logarithmic scale, for the four distances as a function of the length of the time series. When compared to \u03b4dtw, \u03b4eip is evaluated very efficiently using the matrix computation given in Eq.6, although, without any off line indexing of the time series, \u03b4eip cannot compete with \u03b4ed when the length of the time series increases. The \u03b4i\u2212eip curve has clearly the same slope than the \u03b4ed curve, showing that the offline indexed version of \u03b4eip is characterized with a linear complexity, that includes a nevertheless linear overhead when compared to \u03b4ed, mainly due to the loading of the index."}, {"heading": "5 CONCLUSION", "text": "This paper has proposed what we call a family of elastic inner products able to cope with non-uniformly sampled time series of various lengths, as far as they do not contain the zero or null symbol value. These constructions allow one to embed any such time series in a single inner space, that some how generalizes the notion of Euclidean inner space. The recursive structure of the proposed construction offers the possibility to manage several elastic dimensions. Some applicative benefits can be expected in time series or sequence analysis when time elasticity is an issue, for instance in the field of numeric or symbolic sequence data mining. If the algorithmic complexity required to evaluate an elastic inner product is in general quadratic, its computation can be much more efficiently performed than dynamic programming algorithms. In addition we have shown that for some information retrieval applications for which embeddings of time series or symbolic sequences into a finite dimensional Euclidean space is possible, one can break this quadratic complexity down to a linear complexity at exploitation time, although a quadratic computational cost should still be paid once during a preprocessing step at indexing phase.\nThe preliminary experiments we have carried out on some time series and symbolic sequence classification tasks show that embedding time series into elastic inner product space may brought significant accuracy improvement when compared to Euclidean inner product space embeddings as they compensate, at least partially, the limitations of Euclidean distance which is very sensitive to distortions in the time axis. Although our experiments show that Dynamic Programming matching algorithms outperforms on a majority of dataset distances that are derived from an elastic inner product, on some datasets such distances lead to similar accuracies .\nThe experiment carried out on symbolic sequences alignment involves sequences of various lengths. It shows also some very interesting perspectives in the scope of fast filtering of massive data, since the accuracy obtained by a 1-NN symbolic elastic cosine classifier with a potentially linear complexity at exploitation time is somehow comparable to the one obtained using dynamic programming algorithms (NW and SW) whose complexity are quadratic when the alignment search space is not restricted.\nFinally, the general recursive structure of the elastic inner product opens perspectives in the processing of more complex data such as tree data mining for which considering several elastic dimensions may be relevant and efficient. The possibility to decompose complex structures onto sets of elastic basis vectors opens perspectives in various areas of application such as data compression, multi-dimensional scaling or matching pursuits."}], "references": [{"title": "Basic local alignment search tool", "author": ["S.F. Altschul", "W. Gish", "W. Miller", "E.W. Myers", "D.J. Lipman"], "venue": "Journal of molecular biology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Harmonic Analysis on Semigroups: Theory of Positive Definite and Related Functions, volume 100 of Graduate Texts in Mathematics", "author": ["Christian Berg", "Jens Peter Reus Christensen", "Paul Ressel"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1984}, {"title": "A training algorithm for optimal margin classifiers", "author": ["Bernhard E. Boser", "Isabelle Guyon", "Vladimir Vapnik"], "venue": "In COLT,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Protein fold similarity estimated by a probabilistic approach based on c(alpha)-c(alpha) distance comparison", "author": ["O. Carugo", "S. Pongor"], "venue": "Journal of Molecular Biology, 315(4):887\u2013898", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "On the marriage of lp-norm and edit distance", "author": ["L. Chen", "R. Ng"], "venue": "Proceedings of the 30th International Conference on Very Large Data Bases, pages 792\u2013801", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "L", "author": ["E.J. Keogh", "X. Xi"], "venue": "Wei, and C.A. Ratanamahatana. The ucr time series classification-clustering datasets", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Time warp edit distance with stiffness adjustment for time series matching", "author": ["P.F. Marteau"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 31(2):306\u2013318", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Constructing positive elastic kernels with application to time series classification", "author": ["Pierre-Fran\u00e7ois Marteau", "Sylvie Gibet"], "venue": "CoRR, abs/1005.5141,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "A general method applicable to the search for similarities in the amino acid sequence of two proteins", "author": ["Saul B. Needleman", "Christian D. Wunsch"], "venue": "Journal of Molecular Biology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1970}, {"title": "Protein homology detection using string alignment kernels", "author": ["H. Saigo", "J.P. Vert", "N. Ueda", "T. Akutsu"], "venue": "Bioinformatics, 20:1682\u2013 1689", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "A dynamic programming approach to continuous speech recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "Proceedings of the 7th International Congress of Acoustic, pages 65\u201368", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1971}, {"title": "Introduction to Modern Information Retrieval", "author": ["Gerard Salton", "Michael McGill"], "venue": "McGraw-Hill Book Company,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1984}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Bernhard Scholkopf", "Alexander J. Smola"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Kernel Methods for Pattern Analysis", "author": ["John Shawe-Taylor", "Nello Cristianini"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Identification of common molecular subsequences", "author": ["T. Smith", "Waterman M"], "venue": "Journal of Molecular Biology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1981}, {"title": "A protein classification benchmark collection for machine learning", "author": ["Paolo Sonego", "Mircea Pacurar", "Somdutta Dhir", "Attila Kert\u00e9sz- Farkas", "Andr\u00e1s Kocsor", "Zolt\u00e1n G\u00e1sp\u00e1ri", "Jack A.M. Leunissen", "S\u00e1ndor Pongor"], "venue": "Nucleic Acids Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Automatic recognition of 200 words", "author": ["V.M. Velichko", "N.G. Zagoruyko"], "venue": "International Journal of Man-Machine Studies, 2:223\u2013234", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1970}], "referenceMentions": [{"referenceID": 17, "context": "Among other characterizing tools, time warp distances (see [20], [13], and more recently [7], [9] among other references) have shown some interesting robustness compared to the Euclidean metric especially when similarity searching in time series data bases is an issue.", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "Among other characterizing tools, time warp distances (see [20], [13], and more recently [7], [9] among other references) have shown some interesting robustness compared to the Euclidean metric especially when similarity searching in time series data bases is an issue.", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "Among other characterizing tools, time warp distances (see [20], [13], and more recently [7], [9] among other references) have shown some interesting robustness compared to the Euclidean metric especially when similarity searching in time series data bases is an issue.", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "Among other characterizing tools, time warp distances (see [20], [13], and more recently [7], [9] among other references) have shown some interesting robustness compared to the Euclidean metric especially when similarity searching in time series data bases is an issue.", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "Recently, [10] have shown that it is quite easy to propose inner product with time elasticity capability at least for some restricted time series spaces, basically spaces containing uniformly sampled time series, all of which have the same lengths (in such cases, time series can be embedded easily in Euclidean spaces).", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "Basically this is the same complexity as that required for the calculation of the complete dynamic programming solutions such as the Dynamic Time Warping (DTW) [20] [13] algorithm evaluated on these two time series.", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "Basically this is the same complexity as that required for the calculation of the complete dynamic programming solutions such as the Dynamic Time Warping (DTW) [20] [13] algorithm evaluated on these two time series.", "startOffset": 165, "endOffset": 169}, {"referenceID": 1, "context": "A wide range of literature exists on kernel theory, among which [3], [15] and [16] present some large syntheses of major results.", "startOffset": 64, "endOffset": 67}, {"referenceID": 13, "context": "A wide range of literature exists on kernel theory, among which [3], [15] and [16] present some large syntheses of major results.", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "A wide range of literature exists on kernel theory, among which [3], [15] and [16] present some large syntheses of major results.", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "7 generalizes somehow the cosine measure implemented in the vector space model [14] and commonly used in the text information retrieval community.", "startOffset": 79, "endOffset": 83}, {"referenceID": 6, "context": "The classification task we have considered consists of assigning one of the possible categories to an unknown time series for the 20 data sets available at the UCR repository [8].", "startOffset": 175, "endOffset": 178}, {"referenceID": 2, "context": "\u2022 the second one is a SVM classifier [4], [19] configured with a Gaussian RBF kernel whose parameters are C > 0, a trade-off between regularization and constraint violation and \u03c3 that determines the width of the Gaussian function.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "We have used the LIBSVM library [6] to implement the SVM classifiers.", "startOffset": 32, "endOffset": 35}, {"referenceID": 16, "context": "We report here a protein classification experiment carried out using the Protein Classification Benchmark Collection (PCBC) [18] [1].", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "\u2022 BLAST [2]: the Basic Local Alignment Search Tool is a very popular family of fast heuristic search methods used for finding similar regions between two or more nucleotides or amino acids.", "startOffset": 8, "endOffset": 11}, {"referenceID": 15, "context": "\u2022 SW [17]: The SmithWaterman algorithm is used for performing local sequence alignment, for determining similar regions between two nucleotide or protein sequences.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "\u2022 NW [11]: The Needleman Wunsch algorithm performs a maximal global alignment of two strings.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "\u2022 LA kernel [12]: The Local Alignment kernel is used", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "\u2022 PRIDE [5]: The PRIDE score is estimated as the PRobability of IDEntity between two protein 3D", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "05 the eCOS classifier in average performs significantly better than BLAST heuristics [2] and LA [12], the local alignment kernel for string.", "startOffset": 86, "endOffset": 89}, {"referenceID": 10, "context": "05 the eCOS classifier in average performs significantly better than BLAST heuristics [2] and LA [12], the local alignment kernel for string.", "startOffset": 97, "endOffset": 101}, {"referenceID": 3, "context": "The PRIDE method [5] gets the best results, but it uses the tertiary structure of the proteins while all the other methods exploit the primary structure.", "startOffset": 17, "endOffset": 20}], "year": 2012, "abstractText": "This paper proposes a framework dedicated to the construction of what we call discrete elastic inner product allowing one to embed sets of non-uniformly sampled multivariate time series or sequences of varying lengths into inner product space structures. This framework is based on a recursive definition that covers the case of multiple embedded time elastic dimensions. We prove that such inner products exist in our general framework and show how a simple instance of this inner product class operates on some prospective applications, while generalizing the Euclidean inner product. Classification experimentations on time series and symbolic sequences datasets demonstrate the benefits that we can expect by embedding time series or sequences into elastic inner spaces rather than into classical Euclidean spaces. These experiments show good accuracy when compared to the euclidean distance or even dynamic programming algorithms while maintaining a linear algorithmic complexity at exploitation stage, although a quadratic indexing phase beforehand is required.", "creator": "LaTeX with hyperref package"}}}