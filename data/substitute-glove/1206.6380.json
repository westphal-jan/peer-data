{"id": "1206.6380", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring", "abstract": "In both cover simply address the february reasons: Can 've approximately sample from a Bayesian posterior allows if anything are any allowed to touch a variety pickup - fresh of users - spare it every counting get larger? . An approximated based followed over Langevin formula_15 two stochastic gradients (SGLD) initially was reform it things of, well its mixing rate a difficult. By leveraging the Bayesian Central Limit Theorem, we extend however SGLD approximated 'd that at raised mixture expectations there will actual down is rarely approximation similar their nodes, while made slow mixing income but will expressive when activity than SGLD way a pre - bulb matrix. As a dvd, the proposed sequential is marked such Fisher scored (own hamiltonian capillary) out and because now sophisticated rijndael brought shake - after.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (470kb)", "http://arxiv.org/abs/1206.6380v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.CO stat.ML", "authors": ["sungjin ahn", "anoop korattikara balan", "max welling"], "accepted": true, "id": "1206.6380"}, "pdf": {"name": "1206.6380.pdf", "metadata": {"source": "META", "title": "Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring", "authors": ["Sungjin Ahn", "Anoop Korattikara", "Max Welling"], "emails": ["SUNGJIA@ICS.UCI.EDU", "AKORATTI@ICS.UCI.EDU", "WELLING@ICS.UCI.EDU"], "sections": [{"heading": "1. Motivation", "text": "When a dataset has a billion data-cases (as is not uncommon these days) MCMC algorithms will not even have generated a single (burn-in) sample when a clever learning algorithm based on stochastic gradients may already be making fairly good predictions. In fact, the intriguing results of Bottou and Bousquet (2008) seem to indicate that in terms of \u201cnumber of bits learned per unit of computation\u201d, an algorithm as simple as stochastic gradient descent is almost optimally efficient. We therefore argue that for Bayesian methods to remain useful in an age when the datasets grow at an exponential rate, they need to embrace the ideas of the stochastic optimization literature.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nA first attempt in this direction was proposed by Welling and Teh (2011) where the authors show that (uncorrected) Langevin dynamics with stochastic gradients (SGLD) will sample from the correct posterior distribution when the stepsizes are annealed to zero at a certain rate. While SGLD succeeds in (asymptotically) generating samples from the posterior at O(n) computational cost with (n N) it\u2019s mixing rate is unnecessarily slow. This can be traced back to its lack of a proper pre-conditioner: SGLD takes large steps in directions of small variance and reversely, small steps in directions of large variance which hinders convergence of the Markov chain. Our work builds on top of Welling and Teh (2011). We leverage the \u201cBayesian Central Limit Theorem\u201d which states that when N is large (and under certain conditions) the posterior will be well approximated by a normal distribution. Our algorithm is designed so that for large stepsizes (and thus at high mixing rates) it will sample from this approximate normal distribution, while at smaller stepsizes (and thus at slower mixing rates) it will generate samples from an increasingly accurate (non-Gaussian) approximation of the posterior. Our main claim is therefore that we can trade-in a usually small bias in our estimate of the posterior distribution against a potentially very large computational gain, which could in turn be used to draw more samples and reduce sampling variance.\nFrom an optimization perspective one may view this algorithm as a Fisher scoring method based on stochastic gradients (see e.g. (Schraudolph et al., 2007)) but in such a way that the randomness introduced in the subsampling process is used to sample from the posterior distribution when we arrive at its mode. Hence, it is an efficient optimization algorithm that smoothly turns into a sampler when the correct (statistical) scale of precision is reached."}, {"heading": "2. Preliminaries", "text": "We will start with some notation, definitions and preliminaries. We have a large dataset XN consisting of N i.i.d. data-points {x1...xN} and we use a family of distributions parametrized by \u03b8 \u2208 RD to model the distribution of the xi\u2019s. We choose a prior distribution p(\u03b8) and are interested in obtaining samples from the posterior distribution, p(\u03b8|XN ) \u221d p(XN |\u03b8)p(\u03b8).\nAs is common in Bayesian asymptotic theory, we will also make use of some frequentist concepts in the development of our method. We assume that the true data generating distribution is in our family of models and denote the true parameter which generated the dataset XN by \u03b80. We denote the score or the gradient of the log likelihood w.r.t. data-point xi by gi(\u03b8) = g(\u03b8;xi) = \u2207\u03b8 log p(\u03b8;xi). We denote the sum of scores of a batch of n data-points Xr = {xr1 ...xrn} by Gn(\u03b8;Xr) = \u2211n i=1 g(\u03b8;xri) and the average by gn(\u03b8;Xr) = 1nGn(\u03b8;Xr). Sometimes we will drop the argument Xr and instead simply write Gn(\u03b8) and gn(\u03b8) for convenience.\nThe covariance of the gradients is called the Fisher information defined as I(\u03b8) = Ex[g(\u03b8;x)g(\u03b8;x)T ], where Ex denotes expectation w.r.t the distribution p(x; \u03b8) and we have used the fact that Ex[g(\u03b8;x)] = 0. It can also be shown that I(\u03b8) = \u2212Ex[H(\u03b8;x)], where H is the Hessian of the log likelihood.\nSince we are dealing with a dataset with samples only from p(x; \u03b80) we will henceforth be interested only in I(\u03b80) which we will denote by I1. It is easy to see that the Fisher information of n data-points, In = nI1. The empirical covariance of the scores computed from a batch of n data-points is called the empirical Fisher information, V (\u03b8;Xr) = 1 n\u22121 \u2211n i=1 (gri(\u03b8)\u2212 gn(\u03b8)) (gri(\u03b8)\u2212 gn(\u03b8)) T (Scott, 2002). Also, it can be shown that V (\u03b80) is a consistent estimator of I1 = I(\u03b80).\nWe now introduce an important result in Bayesian asymptotic theory. AsN becomes large, the posterior distribution becomes concentrated in a small neighbourhood around \u03b80 and becomes asymptotically Gaussian. This is formalized by the Bernstein-von Mises theorem, a.k.a the Bayesian Central Limit Theorem, (Le Cam, 1986), which states that under suitable regularity conditions, p(\u03b8| {x1...xN}) approximately equals N (\u03b80, I\u22121N ) as N becomes very large."}, {"heading": "3. Stochastic Gradient Fisher Scoring", "text": "We are now ready to derive our Stochastic Gradient Fisher Scoring (SGFS) algorithm. The starting point in the derivation of our method is the Stochastic Gradient Langevin Dynamics (SGLD) algorithm (Welling & Teh, 2011) which we describe in section 3.1. SGLD can sample accurately\nfrom the posterior but suffers from a low mixing rate. In section 3.2, we show that it is easy to construct a Markov chain that can sample from a normal approximation of the posterior at any mixing rate. We will then combine these methods to develop our Stochastic Gradient Fisher Scoring (SGFS) algorithm in section 3.3."}, {"heading": "3.1. Stochastic Gradient Langevin Dynamics", "text": "The SGLD algorithm has the following update equation:\n\u03b8t+1 \u2190 \u03b8t + C\n2\n{ \u2207 log p(\u03b8t) +Ngn(\u03b8t;Xtn) } + \u03bd\nwhere \u03bd \u223c N (0, C) (1)\nHere is the step size, C is called the preconditioning matrix (Girolami & Calderhead, 2010) and \u03bd is a random variable representing injected Gaussian noise. The gradient of the log likelihoodGN (\u03b8;XN ) over the whole dataset is approximated by scaling the mean gradient gn(\u03b8t;X t n) computed from a mini-batchXtn = {xt1 ...xtn} of size n N . Welling & Teh (2011) showed that Eqn. (1) generates samples from the posterior distribution if the step size is annealed to zero at a certain rate. As the step size goes to zero, the discretization error in the Langevin equation disappears and we do not need to conduct expensive MetropolisHasting(MH) accept/reject tests that use the whole dataset. Thus, this algorithm requires only O(n) computations to generate each sample, unlike traditional MCMC algorithms which require O(N) computations per sample.\nHowever, since the step sizes are reduced to zero, the mixing rate is reduced as well, and a large number of iterations are required to obtain a good coverage of the parameter space. One way to make SGLD work at higher step sizes is to introduce MH accept/reject steps to correct for the higher discretization error, but our initial attempts using only a mini-batch instead of the whole dataset were unsuccessful."}, {"heading": "3.2. Sampling from the Approximate Posterior", "text": "Since it is not clear how to use Eqn. (1) at high step sizes, we will move away from Langevin dynamics and explore a different approach. As mentioned in section 2, the posterior distribution can be shown to approach a normal distribution,N (\u03b80, I\u22121N ), as the size of the dataset becomes very large. It is easy to construct a Markov chain which will sample from this approximation of the posterior at any step size. We will now show that the following update equation achieves this:\n\u03b8t+1 \u2190 \u03b8t + C\n2 {\u2212IN (\u03b8t \u2212 \u03b80)}+ \u03c9\nwhere \u03c9 \u223c N (0, C \u2212 2\n4 CINC) (2)\nThe update is an affine transformation of \u03b8t plus injected independent Gaussian noise, \u03c9. Thus if \u03b8t has a Gaussian distributionN (\u00b5t,\u03a3t), \u03b8t+1 will also have a Gaussian distribution, which we will denote as N (\u00b5t+1,\u03a3t+1). These distributions are related by:\n\u00b5t+1 = (I \u2212 C\n2 IN )\u00b5t +\nC 2 IN\u03b80\n\u03a3t+1 = (I \u2212 C\n2 IN )\u03a3t(I \u2212\nC 2 IN ) T + C \u2212 2 4 CINC\n(3)\nIf we choose C to be symmetric, it is easy to see that the approximate posterior distribution, N (\u03b80, I\u22121N ), is an invariant distribution of this Markov chain. Since Eqn. (2) is not a Langevin equation, it samples from the approximate posterior at large step-size and does not require any MH accept/reject steps. The only requirement is that C should be symmetric and should be chosen so that the covariance matrix of the injected noise in Eqn. (2) is positive-definite."}, {"heading": "3.3. Stochastic Gradient Fisher Scoring", "text": "In practical problems both sampling accuracy and mixing rate are important, and the extreme regimes dictated by both the above methods are very limiting. If the posterior is close to Gaussian (as is usually the case), we would like to take advantage of the high mixing rate. However, if we need to capture a highly non-Gaussian posterior, we should be able to trade-off mixing rate for sampling accuracy. One could also think about doing this in an \u201canytime\u201d fashion where if the posterior is somewhat close to Gaussian, we can start by sampling from a Gaussian approximation at high mixing rates, but slow down the mixing rate to capture the non-Gaussian structure if more computation becomes available. In other words, one should have the freedom to manage the right trade off between sampling accuracy and mixing rate depending on the problem at hand.\nWith this goal in mind, we combine the above methods to develop our Stochastic Gradient Fisher Scoring (SGFS) algorithm. We accomplish this using a Markov chain with the following update equation:\n\u03b8t+1 \u2190 \u03b8t + C\n2\n{ \u2207 log p(\u03b8t) +Ngn(\u03b8t;Xtn) } + \u03c4\nwhere \u03c4 \u223c N (0, Q) (4)\nWhen the step size is small, we want to choose Q = C so that it behaves like the Markov chain in Eqn (1). Now we will see how to choose Q so that when the step size is large and the posterior is approximately Gaussian, our algorithm behaves like the Markov chain in Eqn. (2). First, note that if n is large enough for the central limit theorem to hold, we have:\ngn(\u03b8t;X t n) \u223c N ( Ex[g(\u03b8t;x)], 1\nn Cov [g(\u03b8t;x)]\n) (5)\nHere Cov [g(\u03b8t;x)] is the covariance of the scores at \u03b8t. Using NCov [g(\u03b8t;x)] \u2248 IN and NEx[g(\u03b8t;x)] \u2248 GN (\u03b8t;XN ), we have:\n\u2207 log p(\u03b8t) +Ngn(\u03b8t;Xtn) \u2248 \u2207 log p(\u03b8t) +GN (\u03b8t;XN ) + \u03c6\nwhere \u03c6 \u223c N (\n0, NIN n\n) (6)\nNow, \u2207 log p(\u03b8t) + GN (\u03b8t;XN ) = \u2207 log p(\u03b8t|XN ), the gradient of the log posterior. If we assume that the posterior is close to its Bernstein-von Mises approximation, we have \u2207 log p(\u03b8t|XN ) = \u2212IN (\u03b8t \u2212 \u03b80). Using this in Eqn. (6) and then substituting in Eqn. (4), we have:\n\u03b8t+1 \u2190 \u03b8t + C\n2 {\u2212IN (\u03b8t \u2212 \u03b80)}+ \u03c8 + \u03c4 (7)\nwhere, \u03c8 \u223c N ( 0, 2\n4\nN n CINC\n) and \u03c4 \u223c N (0, Q)\nComparing Eqn. (7) and Eqn. (2), we see that at high step sizes, we need:\nQ+ 2\n4\nN n CINC = C \u2212\n2\n4 CINC \u21d2\nQ = C \u2212 2\n4\nN + n\nn CINC (8)\nThus, we should choose Q such that:\nQ = { C for small C \u2212 2\n4 \u03b3CINC for large\nwhere we have defined \u03b3 = N+nn . Since dominates 2 when is small, we can choose Q = C \u2212 2\n4 \u03b3CINC for both the cases above. With this, our update equation becomes:\n\u03b8t+1 \u2190 \u03b8t + C\n2\n{ \u2207 log p(\u03b8t) +Ngn(\u03b8t;Xtn) } + \u03c4\nwhere \u03c4 \u223c N ( 0, C \u2212 2\n4 \u03b3CINC\n) (9)\nNow, we have to choose C so that the covariance matrix of the injected noise in Eqn. (9) is positive-definite. One way to enforce this, is by setting:\nC \u2212 2\n4 \u03b3CINC = CBC \u21d2 C = 4 [ \u03b3IN + 4B]\u22121\n(10)\nwhere B is any symmetric positive-definite matrix. Plugging in this choice of C in Eqn. 9, we get:\n\u03b8t+1 \u2190 \u03b8t + 2 [ \u03b3IN + 4B ]\u22121 \u00d7\n{\u2207 log p(\u03b8t) +Ngn(\u03b8t;Xt) + \u03b7} where \u03b7 \u223c N ( 0, 4B ) (11)\nHowever, the above method considers IN to be a known constant. In practice, we use NI\u03021,t as an estimate of IN , where I\u03021,t is an online average of the empirical covariance of gradients (empirical Fisher information) computed at each \u03b8t.\nI\u03021,t = (1\u2212 \u03bat)I\u03021,t\u22121 + \u03batV (\u03b8t;Xtn) (12)\nwhere \u03bat = 1/t. In the supplementary material we prove that this online average converges to I1 plus O(1/N) corrections if we assume that the samples are actually drawn from the posterior:\nTheorem 1. Consider a sampling algorithm which generates a sample \u03b8t from the posterior distribution of the model parameters p(\u03b8|XN ) in each iteration t. In each iteration, we draw a random minibatch of size n, Xtn = {xt1 ...xtn}, and compute the empirical covariance of the scores V (\u03b8t;Xtn) = 1\nn\u22121 \u2211n i=1 {g(\u03b8t;xti)\u2212 gn(\u03b8t)} {g(\u03b8t;xti)\u2212 gn(\u03b8t)}\nT . Let VT be the average of V (\u03b8t) across T iterations. For large N , as T \u2192 \u221e, VT converges to the Fisher information I(\u03b80) plus O( 1N ) corrections, i.e.\nlim T\u2192\u221e\n[ VT , 1\nT T\u2211 t=1 V (\u03b8t;X t n)\n] = I(\u03b80) +O( 1\nN ) (13)\nNote that this is not a proof of convergence of the Markov chain to the correct distribution. Rather, assuming that the samples are from the posterior, it shows that the online average of the covariance of the gradients converges to the Fisher information (as desired). Thus, it strengthens our confidence that if the samples are almost from the posterior, the learned pre-conditioner converges to something sensible. What we do know is that if we anneal the stepsizes according to a certain polynomial schedule, and we keep the pre-conditioner fixed, then SGFS is a version of SGLD which was shown to converge to the correct equilibrium distribution (Welling & Teh, 2011). We believe the adaptation of the Fisher information through an online average is slow enough for the resulting Markov chain to still be valid, but a proof is currently lacking. The theory of adaptive MCMC (Andrieu & Thoms, 2009) or two time scale stochastic approximations (Borkar, 1997) might hold the key to such a proof which we leave for future work. Putting it all together, we arrive at algorithm 1 below.\nThe general method still has a free symmetric positivedefinite matrix, B, which may be chosen according to our convenience. Examine the limit \u2192 0. In this case our method becomes SGLD with preconditioning matrix B\u22121 and step size .\nIf the posterior is Gaussian, as is usually the case whenN is large, the proposed SGFS algorithm will sample correctly for arbitrary choice of B even when the step size is large.\nAlgorithm 1: Stochastic Gradient Fisher Scoring (SGFS) Input: n, B, {\u03bat}t=1:T Output: {\u03b8t}t=1:T\n1: Initialize \u03b81, I\u03021,0 2: \u03b3 \u2190 n+Nn 3: for t = 1 : T do 4: Choose random minibatch Xtn = {xt1 ...xtn} 5: gn(\u03b8t)\u2190 1n \u2211n i=1 gti(\u03b8t) 6: V (\u03b8t)\u2190 1\nn\u22121 \u2211n i=1 {gti(\u03b8t)\u2212 gn(\u03b8t)} {gti(\u03b8t)\u2212 gn(\u03b8t)} T\n7: I\u03021,t \u2190 (1\u2212 \u03bat)I\u03021,t\u22121 + \u03batV (\u03b8t) 8: Draw \u03b7 \u223c N [0, 4B ] 9: \u03b8t+1 \u2190 \u03b8t+\n2 ( \u03b3NI\u03021,t + 4B )\u22121 {\u2207 log p(\u03b8t) +Ngn(\u03b8t) + \u03b7}\n10: end for\nHowever, for some models the conditions of the Bernsteinvon Mises theorem are violated and the posterior may not be well approximated by a Gaussian. This is the case for e.g. neural networks and discriminative RBMs, where the identifiability condition of the parameters do not hold. In this case, we have to choose a small to achieve accurate sampling (see section 5). These two extremes can be combined in a single \u201canytime\u201d algorithm by slowly annealing the stepsize. For a non-adaptive version of our algorithm (i.e. where we would stop changing I\u03021) after a fixed number of iterations) this would according to the results from Welling and Teh (2011) lead to a valid Markov chain for posterior sampling.\nWe recommend choosing B \u221d IN . With this choice, our method is highly reminiscent of \u201cFisher scoring\u201d which is why we named it \u201cStochastic Gradient Fisher Scoring\u201d (SGFS). In fact we can think of the proposed updates as a stochastic version of Fisher scoring based on small minibatches of gradients. But remarkably, the proposed algorithm is not only much faster than Fisher scoring (because it only requires small minibatches to compute an update), it also samples approximately from the posterior distribution. So the knife cuts on both sides: SGFS is a faster optimization algorithm but also doesn\u2019t overfit due to the fact that it switches to sampling when the right statistical scale of precision is reached."}, {"heading": "4. Computational Efficiency", "text": "Clearly, the main computational benefit relative to standard MCMC algorithms comes from the fact that we use stochastic minibatches instead of the entire dataset at every iteration. However, for a model with a large number of parameters another source of significant computational effort is the computation of the D \u00d7D matrix \u03b3NI\u03021,t + 4B and\nmultiplying its inverse with the mean gradient resulting in a total computational complexity of O(D3) per iteration. In the case n < D the computational complexity per iteration can be brought down to O(nD2) by using the ShermanMorrison-Woodbury equation. A more numerically stable alternative is to update Cholesky factors (Seeger, 2004).\nIn case even this is infeasible one can factor the Fisher information into k independent blocks of variables of, say size d, in which case we have brought down the complexity toO(kd3). The extreme case of this is when we treat every parameter as independent which boils down to replacing the Fisher information by a diagonal matrix with the variances of the individual parameters populating the diagonal. While for a large stepsize this algorithm will not sample from the correct Gaussian approximation, it will still sample correctly from the posterior for very small stepsizes. In fact, it is expected to do this more efficiently than SGLD which does not rescale its stepsizes at all. We have used the full covariance algorithm (SGFS-f) and the diagonal covariance algorithm (SGFS-d) in the experiments section."}, {"heading": "5. Experiments", "text": "Below we report experimental results where we test SGFSf, SGFS-d, SGLD, SGD and HMC on three different models: logistic regression, neural networks and discriminative RBMs. The experiments share the following practice in common. Stepsizes for SGD and SGLD are always selected through cross-validation for at least five settings. The minibatch size n is set to either 300 or 500, but the results are not sensitive to the precise value as long as it is large enough for the central limit theorem to hold (typically, n > 100 is recommended). Also, we used \u03bat = 1t ."}, {"heading": "5.1. Logistic Regression", "text": "A logistic regression model (LR) was trained on the MNIST dataset for binary classification of two digits 7 and 9 using a total of 10,000 data-items. We used a 50 dimensional random projection of the original features and ran SGFS with \u03bb = 1. We used B = \u03b3IN and tested the algorithm for a number of \u03b1 values (where \u03b1 = 2\u221a ). We ran\nthe algorithm for 3,000 burn-in iterations and then collected 100,000 samples. We compare the algorithm to Hamiltonian Monte Carlo sampling (Neal, 1993) and to SGLD (Welling & Teh, 2011). For HMC, the \u201cleapfrogstep\u201d size was adapted during burn-in so that the acceptance ratio was around 0.8. For SGLD we also used a range of fixed stepsizes.\nIn figure 1 we show 2-d marginal distributions of SGFS compared to the ground truth from a long HMC run where we used \u03b1 = 0 for SGFS. From this we conclude that even for the largest possible stepsize the fit for SGFS-f is al-\nmost perfect while SGFS-d underestimates the variance in this case (note however that for smaller stepsizes (larger \u03b1) SGFS-d becomes very similar to SGLD and is thus guaranteed to sample correctly albeit with a low mixing rate).\nNext, we studied the inverse autocorrelation time per unit computation (ATUC)1 averaged over the 51 parameters and compared this with the relative error after a fixed amount of computation time. The relative error is computed as follows: first we compute the mean and covariance of the parameter samples up to time t : \u03b8t = 1t \u2211t t\u2032=1 \u03b8t\u2032 and\nCt = 1t \u2211t t\u2032=1(\u03b8t\u2032 \u2212 \u03b8t)(\u03b8t\u2032 \u2212 \u03b8t)T . We do the same for the long HMC run which we indicate with \u03b8\u221e and C\u221e. Finally we compute\nE1t = \u2211 i |\u03b8ti \u2212 \u03b8\u221ei |\u2211 i |\u03b8\u221ei | , E2t = \u2211 ij |Ctij \u2212 C\u221eij |\u2211 ij |C\u221eij | (14)\nIn Figure 2 we plot the \u201cError at time T\u201d for two values of T (T=100, T=3000) as a function of the inverse ATUC, which is a measure of the mixing rate. Top plots show the results for the mean and bottom plots for the covariance. Each point denoted by a cross is obtained\n1ATUC = Autocorrelation Time \u00d7 Time per Sample. Autocorrelation time is defined as 1 + 2 \u2211\u221e s=1 \u03c1(s) with \u03c1(s) the autocorrelation at lag s Neal (1993).\nfrom a different setting of parameters that control the mixing rate: \u03b1 = [0, 1, 2, 3, 4, 5, 6] for SGFS, stepsizes = [1e\u22123, 5e\u22124, 1e\u22124, 5e\u22125, 1e\u22125, 5e\u22126, 1e\u22126] for SGLD, and number of leapfrog steps s = [50, 40, 30, 20, 10, 1] for HMC. The circle is the result for the fastest mixing chain.\nFor SGFS and SGLD, if the slope of the curve is negative (downward trend) then the corresponding algorithm was still in the phase of reducing error by reducing sampling variance at time T. However, when the curve bends upwards and develops a positive slope the algorithm has reached its error floor corresponding to the approximation bias. The situation is different for HMC, (which has no bias) but where the bending occurs because the number of leapfrog steps has become so large that it is turning back on itself. HMC is not faring well because it is computationally expensive to run (which hurts both its mixing rate and error at time T). We also observe that in the allowed running time SGFS-f has not reached its error floor (both for the mean and the covariance). SGFS-d is reaching its error floor only for the covariance (which is consistent with Figure 1 bottom) but still fares well in terms of the mean. Finally, for SGLD we clearly see that in order to obtain a high mixing rate (low ATUC) it has to pay the price of a large bias. These plots clearly illustrate the advantage of SGFS over both HMC as well as SGLD."}, {"heading": "5.2. SGFS on Neural Networks", "text": "We also applied our methods to a 3 layer neural network (NN) with logistic activation functions. Below we describe classification results for two datasets."}, {"heading": "5.2.1. HERITAGE HEALTH PRIZE (HHP)", "text": "The goal of this competition is to predict how many days between [0 \u2212 15] a person will stay in a hospital given his/her past three years of hospitalization records2. We used the same features as the team market makers that won the first milestone prize. Integrating the first and second year data, we obtained 147,473 data-items with 139 feature dimensions and then used a randomly selected 70% for training and the remainder for testing. NNs with 30 hidden units were used because more hidden units did not noticeably improve the results. Although we used \u03b1 = 6 for SGFS-d, there was no significant difference for values in the range 3 \u2264 \u03b1 \u2264 6. However, \u03b1 < 3 did not work for this dataset due to the fact that many features had values 0.\nFor SGD, we used stepsizes from a polynomial annealing schedule a(b + t)\u2212\u03b4 . Because the training error decreased slowly in a valid range \u03b4 = [0.5, 1], we used \u03b4 = 3, a = 1014, b = 2.2\u00d7 105 instead which was found optimal through cross-validation. (This setting reduced the stepsize from 10\u22122 to 10\u22126 during 1e+7 iterations). For SGLD, a = 1, b = 104, and \u03b4 = 1 reducing the step size from 10\u22124 to 10\u22126 was used. Figure 3 (left) shows the classification errors averaged over the posterior samples for two regularizer values, \u03bb = 0 and the best regularizer value \u03bb found through cross-validation. First, we clearly see that SGD severely overfits without a regularizer while SGLD and SGFS prevent it because they average predictions over samples from a posterior mode. Furthermore, we see that when the best regularizer is used, SGFS (marginally) outperforms both SGD and SGLD. The result from SGFS-d submitted to the actual competition leaderboard gave us an error of 0.4635 which is comparable to 0.4632 obtained by the milestone winner with a fine-tuned Gradient Boosting Machine.\n2http://www.heritagehealthprize.com"}, {"heading": "5.2.2. CHARACTER RECOGNITION", "text": "We also tested our methods on the MNIST dataset for 10 digit classification which has 60,000 training instances and 10,000 test instances. In order to test with SGFS-f, we used inputs from 20 dimensional random projections and 30 hidden units so that the number of parameters equals 940. Moreover, we increased the mini-batch size to 2,000 to reduce the time required to reach a good approximation of the 940\u00d7 940 covariance matrix. The classification error averaged over the samples is shown in Figure 3 (right). Here, we used a small regularization parameter of \u03bb = 0.001 for all methods as overfitting was not an issue. For SGFS, \u03b1 = 2 is used while for both SGD and SGLD the stepsizes were annealed from 10\u22123 to 10\u22127 using a = 1, b = 1000, and \u03b3 = 1."}, {"heading": "5.3. Discriminative Restricted Boltzmann Machine (DRBM)", "text": "We trained a DRBM (Larochelle & Bengio, 2008) on the KDD99 dataset which consists of 4,898,430 datapoints with 40 features, belonging to a total of 23 classes. We first tested the classification performance by training the DRBM using SGLD, SGFS-f, SGFS-d and SGD. For this experiment the dataset was divided into a 90% training set, 5% validation and 5% test set. We used 41 hidden units giving us a total of 2647 parameters in the model. We used\n\u03bb = 10 and B = \u03b3IN . We tried 6 different (\u03b1, ) combinations for SGFS-f and SGFS-d and tried 18 annealing schedules for SGD and SGLD, and used the validation set to pick the best one. The best results were obtained with an \u03b1 value of 8.95 for SGFS-f and SGFS-d, and [a = 0.1, b = 100000, \u03b4 = 0.9] for SGD and SGLD. We ran all algorithms for 100,000 iterations. Although we experimented with different burn-in iterations, the algorithms were insensitive to this choice. The final error rates are given in table 1 from which we conclude that the samplers based on stochastic gradients can act as effective optimizers whereas HMC on the full dataset becomes completely impractical because it has to compute 11.7 billion gradients per iteration which takes around 7.5 minutes per sample (4408587 datapoints \u00d7 2647 parameters).\nTo compare the quality of the samples drawn after burn-in, we created a 10% subset of the original dataset. This time we picked only the 6 most populous classes. We tested all algorithms with 41, 10 and 5 hidden units, but since the posterior is highly multi-modal, the different algorithms ended up sampling from different modes. In an attempt to get a meaningful comparison, we therefore reduced the number of hidden units to 2. This improved the situation to some degree, but did not entirely get rid of the multi-modal and non-Gaussian structure of the posterior. We compare results of SGFS-f/SGLD with 30 independent HMC runs, each providing 4000 samples for a total of 120,000 samples. Since HMC was very slow (even on the reduced set) we initialized at a mode and used the Fisher information at the mode as a pre-conditioner. We used 1 leapfrog step and tuned the step-size to get an acceptance rate of 0.8. We ran SGFS-f with \u03b1 = [2, 3, 4, 5, 10] and SGLD with fixed step sizes of [5e-4, 1e-4, 5e-5, 1e-5, 5e-6]. Both algorithms were initialized at the same mode and ran for 1 million iterations. We looked at the marginal distribu-\ntions of the top 25 pairs of variables which had the highest correlation coefficient. In Figure 4 (top-left and bottomleft) we show a set of parameters where both SGFS-f and SGLD obtained an accurate estimate of the marginal posterior. In 4 (top-right and bottom-right) we show an example where SGLD failed. The thin solid red lines correspond to HMC runs computed from various subsets of the samples, whereas the thick solid red line is computed using the all samples from all HMC runs. We have shown marginal posterior estimates of the SGFS-f/SGLD algorithms with a thick dashed blue ellipse. After inspection, it seemed that the posterior structure was highly non-Gaussian with regions where the probability very sharply decreased. SGLD regularly stepped into these regions and then got catapulted away due to the large gradients there. SGFS-f presumably avoided those regions by adapting to the local covariance structure. We found that in this region even the HMC runs are not consistent with one another. Note that the SGFS-f contours seem to agree with the HMC contours as much as the HMC contours agree with the results of its own subsets, in both the easy and the hard case.\nFinally, we plot the error after 6790 seconds of computation versus the mixing rate. Figure 5-left shows the results for the mean and the right for the covariance (for an explanation of the various quantities see discussion in section 5.1). We note again that SGLD incurs a significantly larger approximation bias at the same mixing rate as SGFS-f."}, {"heading": "6. Conclusions", "text": "We have introduced a novel method, \u201cStochastic Gradient Fisher Scoring\u201d (SGFS) for approximate Bayesian learning. The main idea is to use stochastic gradients in the Langevin equation and leverage the central limit theorem to estimate the noise induced by the subsampling process. This subsampling noise is combined with artificially injected noise and multiplied by the estimated inverse Fisher information matrix to approximately sample from the posterior. This leads to the following desirable properties.\n\u2022 Unlike regular MCMC methods, SGFS is fast because it uses only stochastic gradients based on small mini-batches to draw samples. \u2022 Unlike stochastic gradient descent, SGFS samples (approximately) from the posterior distribution. \u2022 Unlike SGLD, SGFS samples from a Gaussian approximation of the posterior distribution (that is correct for N \u2192\u221e) for large stepsizes. \u2022 By annealing the stepsize, SGFS becomes an anytime method capturing more non-Gaussian structure with smaller stepsizes but at the cost of slower mixing. \u2022 During its burn-in phase, SGFS is an efficient optimizer because like Fisher scoring and Gauss-Newton methods, it is based on the natural gradient.\nFor an appropriate annealing schedule, SGFS thus goes through three distinct phases: 1) during burn-in we use a large stepsize and the method is similar to a stochastic gradient version of Fisher scoring, 2) when the stepsize is still large, but when we have reached the mode of the distribution, SGFS samples from the asymptotic Gaussian approximation of the posterior, and 3) when the stepsize is further annealed, SGFS will behave like SGLD with a pre-conditioning matrix and generate increasingly accurate samples from the true posterior."}, {"heading": "Acknowledgements", "text": "This material is based upon work supported by the National Science Foundation under Grant No. 0447903, 0914783, 0928427."}], "references": [{"title": "A tutorial on adaptive mcmc", "author": ["C. Andrieu", "J. Thoms"], "venue": "Statistics and Computing,", "citeRegEx": "Andrieu and Thoms,? \\Q2009\\E", "shortCiteRegEx": "Andrieu and Thoms", "year": 2009}, {"title": "Stochastic approximation with two time scales", "author": ["V.S. Borkar"], "venue": "Systems and Control Letters,", "citeRegEx": "Borkar,? \\Q1997\\E", "shortCiteRegEx": "Borkar", "year": 1997}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bottou and Bousquet,? \\Q2008\\E", "shortCiteRegEx": "Bottou and Bousquet", "year": 2008}, {"title": "Riemann manifold langevin and hamiltonian monte carlo", "author": ["M. Girolami", "B. Calderhead"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Girolami and Calderhead,? \\Q2010\\E", "shortCiteRegEx": "Girolami and Calderhead", "year": 2010}, {"title": "Classification using discriminative Restricted Boltzmann Machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "In Proceedings of the 25 International Conference on Machine learning,", "citeRegEx": "Larochelle and Bengio,? \\Q2008\\E", "shortCiteRegEx": "Larochelle and Bengio", "year": 2008}, {"title": "Asymptotic methods in statistical decision theory", "author": ["L.M. Le Cam"], "venue": null, "citeRegEx": "Cam,? \\Q1986\\E", "shortCiteRegEx": "Cam", "year": 1986}, {"title": "Probabilistic inference using markov chain monte carlo methods", "author": ["R.M. Neal"], "venue": "Technical Report CRG-TR-93-1,", "citeRegEx": "Neal,? \\Q1993\\E", "shortCiteRegEx": "Neal", "year": 1993}, {"title": "A stochastic quasiNewton method for online convex optimization", "author": ["N.N. Schraudolph", "J. Yu", "S. G\u00fcnter"], "venue": "Proc. 11 Intl. Conf. Artificial Intelligence and Statistics (AIstats),", "citeRegEx": "Schraudolph et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Schraudolph et al\\.", "year": 2007}, {"title": "Maximum likelihood estimation using the empirical fisher information matrix", "author": ["W.A. Scott"], "venue": "Journal of Statistical Computation and Simulation,", "citeRegEx": "Scott,? \\Q2002\\E", "shortCiteRegEx": "Scott", "year": 2002}, {"title": "Low rank updates for the cholesky decomposition", "author": ["M. Seeger"], "venue": "Technical report, University of California Berkeley,", "citeRegEx": "Seeger,? \\Q2004\\E", "shortCiteRegEx": "Seeger", "year": 2004}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML),", "citeRegEx": "Welling and Teh,? \\Q2011\\E", "shortCiteRegEx": "Welling and Teh", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "In fact, the intriguing results of Bottou and Bousquet (2008) seem to indicate that in terms of \u201cnumber of bits learned per unit of computation\u201d, an algorithm as simple as stochastic gradient descent is almost optimally efficient.", "startOffset": 35, "endOffset": 62}, {"referenceID": 10, "context": "A first attempt in this direction was proposed by Welling and Teh (2011) where the authors show that (uncorrected) Langevin dynamics with stochastic gradients (SGLD) will sample from the correct posterior distribution when the stepsizes are annealed to zero at a certain rate.", "startOffset": 50, "endOffset": 73}, {"referenceID": 10, "context": "A first attempt in this direction was proposed by Welling and Teh (2011) where the authors show that (uncorrected) Langevin dynamics with stochastic gradients (SGLD) will sample from the correct posterior distribution when the stepsizes are annealed to zero at a certain rate. While SGLD succeeds in (asymptotically) generating samples from the posterior at O(n) computational cost with (n N) it\u2019s mixing rate is unnecessarily slow. This can be traced back to its lack of a proper pre-conditioner: SGLD takes large steps in directions of small variance and reversely, small steps in directions of large variance which hinders convergence of the Markov chain. Our work builds on top of Welling and Teh (2011). We leverage the \u201cBayesian Central Limit Theorem\u201d which states that when N is large (and under certain conditions) the posterior will be well approximated by a normal distribution.", "startOffset": 50, "endOffset": 708}, {"referenceID": 7, "context": "(Schraudolph et al., 2007)) but in such a way that the randomness introduced in the subsampling process is used to sample from the posterior distribution when we arrive at its mode.", "startOffset": 0, "endOffset": 26}, {"referenceID": 8, "context": "(Scott, 2002).", "startOffset": 0, "endOffset": 13}, {"referenceID": 1, "context": "The theory of adaptive MCMC (Andrieu & Thoms, 2009) or two time scale stochastic approximations (Borkar, 1997) might hold the key to such a proof which we leave for future work.", "startOffset": 96, "endOffset": 110}, {"referenceID": 10, "context": "where we would stop changing \u00ce1) after a fixed number of iterations) this would according to the results from Welling and Teh (2011) lead to a valid Markov chain for posterior sampling.", "startOffset": 110, "endOffset": 133}, {"referenceID": 9, "context": "A more numerically stable alternative is to update Cholesky factors (Seeger, 2004).", "startOffset": 68, "endOffset": 82}, {"referenceID": 6, "context": "We compare the algorithm to Hamiltonian Monte Carlo sampling (Neal, 1993) and to SGLD (Welling & Teh, 2011).", "startOffset": 61, "endOffset": 73}, {"referenceID": 6, "context": "Autocorrelation time is defined as 1 + 2 \u2211\u221e s=1 \u03c1(s) with \u03c1(s) the autocorrelation at lag s Neal (1993).", "startOffset": 92, "endOffset": 104}], "year": 2012, "abstractText": "In this paper we address the following question: \u201cCan we approximately sample from a Bayesian posterior distribution if we are only allowed to touch a small mini-batch of data-items for every sample we generate?\u201d. An algorithm based on the Langevin equation with stochastic gradients (SGLD) was previously proposed to solve this, but its mixing rate was slow. By leveraging the Bayesian Central Limit Theorem, we extend the SGLD algorithm so that at high mixing rates it will sample from a normal approximation of the posterior, while for slow mixing rates it will mimic the behavior of SGLD with a pre-conditioner matrix. As a bonus, the proposed algorithm is reminiscent of Fisher scoring (with stochastic gradients) and as such an efficient optimizer during burn-in.", "creator": "LaTeX with hyperref package"}}}