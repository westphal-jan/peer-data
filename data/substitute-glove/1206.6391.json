{"id": "1206.6391", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Gaussian Process Quantile Regression using Expectation Propagation", "abstract": "Direct eldership predictive initiating estimating taken merely quantile of a expressed variable as when consequently of input variables. We present turn new framework did proposed surjective probit taken second Gaussian involves model gives working, minimising the continue solidly thanks function. The aims receive in learning there anything rephrased tractable so for slower up the ways say 3,000 within Expectation Propagation iteratively. We describe how time time novel to other translocation regression methods and whereby there instance opened. synthetic and real data complete. The method being because to given bidding by education country the collection methods whilst allowing full through leverage latter well full Gaussian establish probabilistic formulated.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (496kb)", "http://arxiv.org/abs/1206.6391v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "stat.ME cs.LG stat.AP", "authors": ["alexis boukouvalas", "remi louis barillec", "dan cornford"], "accepted": true, "id": "1206.6391"}, "pdf": {"name": "1206.6391.pdf", "metadata": {"source": "META", "title": "Direct Gaussian Process Quantile Regression  using Expectation Propagation", "authors": ["Alexis Boukouvalas", "Remi Barillec", "Dan Cornford"], "emails": ["boukouva@aston.ac.uk", "r.barillec@aston.ac.uk", "d.cornford@aston.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Quantile regression has been applied in a variety of domains and for different purposes (Yu et al., 2003). Applications include medical reference charts, survival analysis, economics and the detection of heteroscedasticity. Quantile regression allows a comprehensive analysis of the relationships between a response, y, and input variables, x. In traditional regression analysis there is often an implicit assumption that any uncertainty in the learned model is a result of incomplete knowledge of an underlying deterministic function due to incomplete, noisy observations. However, quantile regression is most relevant when the response is likely to be subject to variability or intrinsic randomness, such as might occur in population or meta-studies, regression modelling where not all relevant inputs are\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\navailable or considered, or when modelling the output of a stochastic simulation.\nTwo main approaches to quantile regression have been described in the literature: the Estimating Equation (EE) approach and inverting a Cumulative Distribution Function (CDF). The EE approach is based on directly modelling the quantile function, learning the parameters by minimising an appropriate loss function. The CDF approach is based on estimation of the CDF of the response and inverting this to obtain the desired quantiles. This differentiation is akin to the difference between discriminatory (EE) and generative (CDF) models in classification.\nIn the quantile regression setting, the generative case corresponds to estimating the full conditional CDF F (y|x) and then inverting it to obtain specific quantiles. This model-based approach allows for a natural Bayesian formulation. Taddy & Kottas (2010) provide a clear overview of options for quantile regression, and propose a model based on a non-parametric Dirichlet process prior to construct a flexible joint model for the response and inputs, conditioning this on inputs to obtain the required conditional response distribution. Chen & Muller (2012) develop a nonparametric CDF approach based on computing indicator functions which are smoothed using a kernel spectral decomposition to form the full conditional density model, and thus determine the quantile functions. A related approach in the field of Geostatistics is known as indicator cokriging (Pardoiguzquiza & Dowd, 2005) where a Gaussian process is used to estimate a discretised approximation to the CDF. All of these methods require carefully designed Markov Chain Monte Carlo (MCMC) inference methods, and require significant computational effort, making their application to problems with many inputs infeasible. Alternatively for a Gaussian posterior model the inversion of the CDF can be done analytically to retrieve quantile functions as demonstrated in Quadrianto et al. (2009).\nThe major advantage of the CDF approach is that an appropriate likelihood function is defined and the joint estimation of the quantiles means order violations (quantile crossings) are not possible by construction. However in scenarios where the interest is in the specification of one, or a small set of quantiles, intuitively it seems unnecessary to attempt to describe the entire conditional distribution. The direct EE approach may also be more appropriate in application domains such as real-time systems where inference time is critical as it allows for faster computation using simpler models than the CDF approach.\nThe EE approach (Koenker, 2005) can be seen as akin to directly constructing a decision boundary to separate the classes. In the quantile regression case a loss function is minimised to obtain the quantiles of interest directly. Various EE approaches to quantile regression exist. The frequentist interpretation minimises an empirical risk function, related to the tilted loss function (Figure 1) given by\nL\u03c4 (y \u2212 y\u2217) = { \u03c4 (y \u2212 y\u2217) if y \u2265 y\u2217 \u2212(1\u2212 \u03c4)(y \u2212 y\u2217) if y < y\u2217\nwhich has been shown to consistently estimate the \u03c4 \u2019th quantile. The tilted loss is also known as the pinball loss (Takeuchi et al., 2006).\nMany approaches use linear in parameter, or spline models (Koenker, 2005). These approaches are consistent and produce classical estimates for the quantile functions, which can also incorporate the use of simplex methods or post processing to ensure no order violations for multiple quantiles (Koenker, 2005). Many papers have also attempted to provide a \u2018Bayesian\u2019 version of the EE approach to quantile regression. These exploit the association between the tilted loss function and the Asymmetric Laplace Distribution (ALD), as explained in more detail in Section 2.\nYu & Moyeed (2001) develop a \u2018Bayesian\u2019 linear model for quantile regression assuming an ALD likelihood, but care must be taken in the interpretation of the posterior distribution in the typical case that one does not believe the errors on the response actually follow an ALD. The use of the ALD remains common (Yue & Rue, 2011; Lum & Gelfand, 2012), and while the mode of the solution can be shown to be consistent\nwith the true quantile, the uncertainty on the quantiles has no clear interpretation. In Lum & Gelfand (2012) a conditional Gaussian representation of the ALD is used to incorporate spatially dependent errors. Inference in this model is accomplished via MCMC. Their approach is quite similar to what is proposed in this paper in that spatial dependence is modelled via a stochastic process. Yue & Rue (2011) propose a similar model where a Gauss-Markov random field model is used to address spatial (input) dependency and both iterated nested Laplace approximations and MCMC are used for inference.\nOur focus in this paper is on quantile regression where the conditional quantile functions are of interest. Our contribution consists of presenting a novel method for quantile regression which uses approximate inference methods to improve efficiency. We place a Gaussian Process (GP) prior on the quantile regression function similarly to Lum & Gelfand (2012) and directly minimise the expected tilted loss using an Expectation Propagation (EP) approach (Minka, 2001). Further we clarify the justification of the EE approach which has been used by a variety of authors and show that although not truly a Bayesian approach, a decision theoretic grounding is possible.\nThe paper is structured as follows. In Section 2 we present our approach. A simulation study on both synthetic and real data is presented in Section 3. A discussion of the results and future extensions is given in Section 4."}, {"heading": "2. GP Quantile Regression using EP", "text": "Our model, which we term QGP-EP, places a GP prior on the space of quantile regression functions1. The training of the model proceeds by directly minimising the expected loss or maximising an equivalent utility function. The latter is found to correspond to the ALD which has been widely used in direct quantile estimation. The expected utility turns out to not be analytically tractable and we employ the EP algorithm to perform the integration. A high level description of the algorithm is given in Algorithm 1. We conclude by discussing how hyper-parameter estimation and prediction are accomplished.\nMinimising the expected tilted loss\nargmin y\n\u222b L(y\u2217, y)p(y\u2217|x\u2217,D) dy\u2217 (1)\nleads to the \u03c4 \u2019th quantile of p(y\u2217|x\u2217,D). This is re1Code is available at http://wiki.aston.ac.uk/ AlexisBoukouvalas.\nferred to as the expected quantile risk in Takeuchi et al. (2006). For more details on the optimality conditions for different loss functions see Berger (1985). If we take the exponent of the negative of the tilted loss and normalise, we have the ALD (Yu & Zhang, 2005). The density function is:\nL(t|\u00b5, \u03c3, \u03c4) = \u03c4(1\u2212 \u03c4) \u03c3 exp\n[ \u2212 t\u2212 \u00b5\n\u03c3 (\u03c4 \u2212 I(t \u2264 \u00b5))\n] .\n(2) The parameter \u03c4 \u2208 [0, 1] controls the skewness of the distribution. For \u03c4 = 0.5 we retrieve the Laplace distribution. The mean \u00b5 can take any real value and the standard deviation has to be positive \u03c3 > 0. The indicator function I(t \u2264 \u00b5) is 1 if the condition is true, 0 otherwise. We can therefore define a utility:\nU\u03c4 (y,q) = Z exp[\u2212L\u03c4 (y,q)]. (3)\nwhere q is the predicted value of the \u03c4 quantile, y the observations and Z the normalisation. If we take the common assumption that the utilities are independent for each observation, we have:\nU\u03c4 (y,q) = Z exp [ \u2212\nN\u2211 i=1 L\u03c4 (yi, qi)\n] . (4)\nLastly we place a GP prior on the quantile regression function:\np(q) = GP(q|0,K) (5)\nFor brevity, we have omitted the conditioning on the inputs X. We propose to train the model by directly maximising the expected utility, also known as the gain or reward:\narg max \u03b8 \u222b q U\u03c4 (y,q)p(q) dq (6)\nwhere \u03b8 = {\u03c3, \u03b8K}, that is the ALD scale parameter \u03c3, and the GP kernel hyper-parameters \u03b8K . This integral is not analytically tractable. However because of the independence assumption of the utility, we can employ a message-passing algorithm that locally approximates each site given the effect, known as the context, of all other sites. EP is such an algorithm and is discussed in the next section. The maximisation of the expected utility with respect to \u03b8 is also done numerically."}, {"heading": "2.1. Expectation Propagation", "text": "In EP, the posterior is approximated using an exponential-family distribution (Minka, 2001). This is usually a Gaussian. A local approximation is made where each factor is approximated separately in an iterative algorithm until convergence. Our motivation\nfor using EP stems from the computational burden of sampling methods which would preclude the use of the method in time critical application domains or where numerous model training evaluations are required such as in experimental design. Also note that simple approximations that use the Hessian to obtain an approximate Gaussian posterior centred on the mode are not applicable as the Laplace distribution is not differentiable at the mode (Seeger, 2008).\nThe algorithm proceeds in two steps: first compute the expected utility (EP step), then maximise it (Section 2.2). This is performed repeatedly until convergence. Prediction of the quantile is done using a plug-in value for the parameters \u03b8 - see Section 2.2.\nAs the utility factorises, we approximate each factor with a local Gaussian approximation. The expected utility \u222b\nq\nU\u03c4 (y,q)p(q) dq = \u222b p(q) N\u220f i=1 \u03c0i\nis approximated with the factorised Gaussian\u222b p(q) \u220fN i=1 \u03c0\u0303i where we have introduced a shorthand notation for each factor of the utility (Equation (4)). The exact utility is \u03c0i = U\u03c4 (yi, qi) and the approximate utility is Gaussian\n\u03c0\u0303i = Z\u0303iN ( qi|\u00b5\u0303i, \u03c3\u03032i ) where Z\u0303i is the normalisation, \u00b5\u0303i the mean and \u03c3\u0303 2 i the variance.\nThe other quantity we will need before describing the algorithm is the context, also known as the cavity field. It is the product of all factors except the ith, q\\i = p(q) \u220fN j 6=i \u03c0\u0303j , and encapsulates the effect of all factors except for site i.\nWe also define the projection operator where Proj[p(x)] = p\u0303(x) matches the moments (mean and variance) of the Gaussian p\u0303(x) to p(x). A high level description of the algorithm is given in Algorithm 1. The projection step is the only problem-dependent step in the EP algorithm (Section 3.6 of Rasmussen & Williams (2006)). We need to find the un-normalised Gaussian marginal which best approximates the product of the cavity distribution and the exact (local) utility:\nZ\u0302iN ( qi|\u00b5\u0302i, \u03c3\u03022i ) = Proj [ \u03c0iq \\i ]\n(7)\nBecause it is un-normalised, we match the zero-th moment in addition to the mean and variance. All three expectations have been computed analytically and are given in Appendix A. The expressions for the new approximate \u03c0\u0303i are given in Section 3.6 of Rasmussen & Williams (2006).\nAlgorithm 1 QGP-EP Training Algorithm.\nInput: Training data D = {xi, yi}N , size N repeat\nInitialize all EP sites Z\u0303, \u00b5\u0303i, \u03c3\u0303 2 i . for i = 1 to N do Calculate context q\\i. Calculate projection to un-normalised Gaussian \u03c0\u0303iq \\i = Proj [ \u03c0iq \\i].\nCalculate new approximation \u03c0\u0303i by dividing by the context. end for Maximise expected utility with respect to parameters \u03b8 (Section 2.2).\nuntil Convergence."}, {"heading": "2.2. Hyperparameter learning and prediction", "text": "Hyperparameter learning and QGP-EP prediction proceed in a similar fashion to ordinary GP regression. Values for the hyperparameters are obtained via the maximisation of the expected utility (Eq. (6)). EP provides a direct estimate of the expected utility:\nZEP = N\u220f i=1 Z\u0303i \u222b N (q|0,K)N ( qi|\u00b5\u0303i, \u03c3\u03032i ) dqi\n= (2\u03c0)D/2|K + \u03a3\u0303|\u22121/2 exp [ \u22121\n2 \u00b5\u0303T (K + \u03a3\u0303)\u22121\u00b5\u0303 ] N\u220f i=1 Z\u0303i\nwith \u03a3\u0303 the diagonal matrix of \u03c3\u03032i for all sites and \u00b5\u0303 the vector of all \u00b5\u0303i. As in ordinary GP regression, in practice we minimise the negative log of the expected utility. The predictive mean and variance at a new point x\u2217 for the quantile q is:\nE[q\u2217|D, x\u2217] = kT\u2217K\u22121\u00b5 = kT\u2217 (K + \u03a3\u0303)\u22121\u00b5\u0303 V ar[q\u2217|D, x\u2217] = kT\u2217\u2217 \u2212 kT\u2217 (K + \u03a3\u0303)\u22121k\u2217 .\nwhere D is the training data, k\u2217\u2217 and k\u2217 the test-test and test-train set covariances respectively. We note here that the prediction is on the latent variables for the quantile q and not for the noisy observations y. In our framework, it would be meaningless to discuss the latter as we have not defined a likelihood for y."}, {"heading": "3. Experimental Evaluation", "text": ""}, {"heading": "3.1. Synthetic Data", "text": "To illustrate the method and provide some validation in a context where the true quantiles of p(y|x) are known, we look at a stochastic processes with heteroscedastic (i.e. input-dependent) variance mentioned in Quadrianto et al. (2009). The process is of\nthe form: p(y|x) = \u00b5(x) + \u03c3(x) \u03be\nwhere \u00b5(x) is the mean component and \u03be is a Chisquared, X 2, noise process scaled by some inputdependent factor \u03c3(x). Realisations from the process are observed at randomly sampled inputs x. Table 1 summarises the setup.\nWe fit, independently, one QGP-EP to the data for each of the following quantiles: \u03c4 \u2208 {0.1, 0.25, 0.5, 0.75, 0.9}. The quality of the fit is sensitive to the realisation, thus the experiment is repeated 30 times to assess robustness. Figure 2 shows the regression curves (solid lines) obtained for a given realisation sample (dots). The true quantiles are shown as dashed lines.\nFigure 3 (a) shows the absolute error between the true and estimated quantile for 30 different realisations of the underlying stochastic process, sorted by increasing mean error for each quantile. The estimated quantile curves provide a reasonable fit to the true quantile. Lower quantiles are better estimated due to the skewness of the stochastic process. It is worth noting that\nthe method remains robust with respect to outlying data points (Figure 2 top third).\nThe same heteroscedastic scenario has been utilised in Quadrianto et al. (2009) allowing us to compare the performance of the QGP method to the methods examined therein. Our method performed, on average, on par with the Gaussian posterior method of Quadrianto et al. (2009) and the Quantile SVM (QSVM) method of Takeuchi et al. (2006) although it is hard to compare as the results are given for a single experimental realisation only (see Table 1 in Quadrianto et al. (2009)).\nFor comparison with a well established quantile regression method, Figure 3 (b) shows the same information as Figure 3 (a) for spline quantile regression (Koenker, 2005). The order of the splines was set to 5 (orders between 3 and 15 were considered, the used values seemed to give the best results). With a strongly skewed underlying process, the higher quantiles are typically more difficult to estimate due to the limited number of data points emanating from the tail of the distribution. Samples with particularly poor spread can even lead to numerical issues in the EP algorithm."}, {"heading": "3.2. Benchmark Data", "text": "In this section we compare the QGP method to QSVM on a set of benchmarks data sets. We do not compare against the Gaussian posterior method of Quadrianto et al. (2009) since that method relies on a Gaussian error model unlike the QGP and the QSVM methods. As in Takeuchi et al. (2006), we perform 10-fold cross validation on the data sets and transform the data to have zero mean, unit variance. The datasets used are the caution dataset which has 2 regressors and 100 points, the ftcollinssnow set which has 93 points and 1 regressor and the motorcycle set which has 1 regressor and 133 points. We utilised a zero mean GP with a squared exponential kernel with independent lengths scales for each input dimension.\nThe average pinball loss is shown in Table 2. The pinball losses are similar for the two models although we note that the deviations are typically much higher for the QGP model. We would caution on the interpretation of the standard deviations however. We implemented the unconditional quantile model as in Takeuchi et al. (2006) and the standard deviation values we obtained were higher than those shown in the paper (although the mean values were very close). Differences in the partitioning of the datasets for cross validation may give rise to the larger deviations. The cross validation was done using completely random partitions in our experiments. Just examining the\nmean values, the performances of the two models are similar although in some cases the QSVM is better (\u03c4 = 0.9 ftcollinssnow) while in others the QGP is better (\u03c4 = 0.9 caution). We discuss the main benefits of QGP in Section 4."}, {"heading": "3.3. English Longitudinal Study of Ageing", "text": "In this section we apply the QGP-EP to the English Longitudinal Study of Ageing (ELSA) dataset. ELSA2 is a multi-purpose large study which follows individuals aged 50 years or older (Banks et al., 2006). Factors include clinical, physical, financial and general wellbeing. One of the primary interests in examining ageing populations is the effect of the different factors on Quality of Life (QoL). There exist various measures to estimate the latter and we have selected to use the CASP-19 measure following (Blane et al., 2008), which is a compound measure of several health and socioeconomical indicators.\nWe investigate the effect of lung function, obesity, blood pressure and age on the CASP-19 QoL measure. This analysis was done for the mean response in Blane et al. (2008) using structural equation modelling and our aim is to investigate whether their conclusions extend to the response quantiles as well. Our analysis is done cross-sectionally on the second ELSA wave dataset as in Blane et al. (2008). Therein it was found that lung function and obesity, but not blood pressure, were directly associated with QoL.\nWe examine the effect of these factors on the 25th,\n2The data is freely available through the Economic and Social Data service http://www.esds.ac.uk/aandp/ access/access.asp.\n50th and 75th quantiles. These quantiles were selected to reflect worse than typical, typical and better than typical QoL outcomes. We utilise the Automatic Relevance Determination (ARD) method to estimate the effect of each factor on the QoL output. In the ARD approach we use a separate length-scale parameter in the kernel for each input. The input domains are linearly rescaled to equal ranges ensuring the length-scale parameters can be interpreted as importance measures. The intuition is that the length scales tell us how far along a particular axis one needs to move for the values to be uncorrelated ((Rasmussen & Williams, 2006) Section 5.1). To implement the ARD approach we utilise a zero mean GP with a squared exponential kernel.\nA 1500 point training set is used to train a QGP-EP for each quantile. This is accomplished by generating 1000 random designs and selecting the design which maximises the minimum distance between any 2 training inputs. In this fashion, we achieve a reasonable coverage of the input space whilst including a variety of inter-point distances in the training set to help identify the length scale parameters. The rest of the Wave 2 ELSA data set (3364 points) is utilised for validation.\nAs the true or sample quantiles are not available for this dataset, we follow the approach of Chen & Muller (2012) to assess the quality of the model predictions Q\u0302i(\u03b1) for a quantile \u03b1. For a given test set {Xi, Yi}Ni=1, the expectation of the indicator function Ii(\u03b1) = I{Yi \u2264 Q\u0302i(\u03b1)} is E{Ii(\u03b1)|Xi} = \u03b1. We therefore utilise the mean of the indicator function, I(\u03b1) = 1N \u2211N i=1 I{Yi \u2264 Q\u0302i(\u03b1)}, which should be close to the true quantile, as a diagnostic. The I(\u03b1) mea-\nsure is calculated for each quantile and is shown in Figure 4. We see that for all three quantiles, the QGP-EP achieves the optimal value. For comparison we have included the measure achieved by a linear-Gaussian model which is linear in the inputs with i.i.d Gaussian noise and is estimated using ordinary least squares on the same training data as the QGP-EP. The quantile estimates are obtained by inverting the Gaussian CDF. For the 25th quantile, both models achieve the optimal measure whilst for the median and 75th quantile the QGP-EP achieves a better score.\nThe length-scales for each input are shown in Table 3. The inputs consist of a measurement of lung function, Body Mass Index (BMI), higher values of which are indicative of obesity, diastolic blood pressure and age3.\n3The variables are referred to as htfvc, bmi, diaval and dhager respectively in the ELSA dataset.\nBMI is ranked highly and diastolic blood pressure low for all quantiles in agreement with the findings of Blane et al. (2008) on the mean QoL. Lung function is found to be most critical for the 25th quantile whereas it is found less relevant for the other quantiles. Age is ranked highly for the median and 75th quantile but less so for the 25th quantile. We therefore conclude that in terms of predicting quality of life as measured by the CASP-19 measure, the findings of Blane et al. (2008) on the mean hold for all quantiles considered in terms of BMI and diastolic blood pressure. The former is found to be a good predictor of QoL whilst the latter is not and consideration may be given to omitting this variable from future measurement and analysis. On the other hand, lung function seems to be a good predictor for low QoL. We hypothesize that as lung function deteriorates past a threshold, QoL seems to be drastically affected."}, {"heading": "Q25 Q50 Q75", "text": ""}, {"heading": "4. Discussion and Future work", "text": "We have presented a framework for quantile regression. The framework relies on the maximisation of the expected ALD utility under a GP prior and uses EP to compute the intractable integrals. The method has been validated on a non-trivial synthetic example of a strongly skewed heteroscedastic process. The method performs well, providing a good fit to the true underlying quantile function. As one would expect, the performance of the method is linked quite strongly to the identifiability of a specific quantile, leading to better estimation of the lower quantiles for a positively skewed process.\nWhen comparing the method against the QSVM of Takeuchi et al. (2006) in Section 3.2, the performance of the algorithms was similar. As the QSVM was shown to outperform several other methods in Takeuchi et al. (2006), we believe the QGP offers state of the art performance as well. However, the main benefit of the QGP lies in the ability to leverage the full probabilistic GP framework for quantile estimation in a computationally efficient framework. For instance, the QGP framework could be extended to handle very large data sets by using well known sparse approximations (Quinonero-Candela & Rasmussen, 2005). The\nmodeller can also easily incorporate prior information by setting different mean and covariance functions for the QGP. Another example of the benefits of the GP framework is to perform variable selection by using ARD as was demonstrated in the ELSA study in Section 3.3. Finally unlike fully Bayesian approaches where the entire conditional CDF is estimated, the QGP method, like other direct quantile estimation methods, can be applied to higher dimensional problems and does not require computationally expensive MCMC methods for parameter estimation.\nThe second main contribution of this paper is to offer an alternative perspective on the approach of Yu & Moyeed (2001) and subsequent follow up papers (e.g. (Yue & Rue, 2011; Lum & Gelfand, 2012)). The use of the asymmetric Laplace as a likelihood is only informally justified in these papers. We have described how this approach can be interpreted as a minimisation of the expected tilted loss and can therefore be well grounded in decision theoretic terms.\nThere are limitations to quantile estimation using QGP-EP, some inherent to the method, some to quantile estimation itself. Estimating quantiles in the tails of a distribution can be problematic if the distribution is very skewed, due to the smaller number of informative data points. In such cases, the EP algorithm can become numerically unstable. Fractional EP (Minka, 2004) may be used to ease these problems as Seeger (2008) has noted, by reducing the impact of individual EP updates. It is also possible to set priors on the GP parameters to enforce smoother regression functions for those less identifiable quantiles, in accordance with one\u2019s beliefs. In the QGP-EP framework, these would appear as regularisation terms in the maximisation of the log expected utility.\nOne possible criticism of QGP-EP (and several other quantile regression methods) is the lack of information about the uncertainty in the quantile. While variance estimates are easily obtained for Bayesian regression models of the mean, these are not easily interpreted for quantile regression models. The difficulty lies in the fact that we estimate quantiles to avoid having to specify a particular shape for the underlying distribution of the response. This is more akin to likelihood free methods and makes a full Bayesian treatment, yielding a posterior distribution, impossible. Alternative approaches have been considered to try and estimate the quantile variance, typically using resampling methods such as the bootstrap (Koenker, 2005).\nIn this work, QGP-EP only allows us to estimate a single quantile function at a time. While several quantiles can easily be learned independently, there is no\nguarantee that these will respect order constraints, although a stochastic ordering was established in Lum & Gelfand (2012). While in the presence of sufficient training data the problem is relatively minor, further work is needed to address order constraint violation for smaller datasets. In particular, this could be done within a framework allowing several quantiles to be jointly estimated by introducing an order constraint via a step function in EP."}, {"heading": "Acknowledgments", "text": "The analogy with classification problems was first suggested by Neil Lawrence. We thank the anonymous reviewer for suggesting how to extend the approach to multiple quantiles. Thanks to Ian Nabney for useful discussions on this work. This work was funded as part of the Managing Uncertainty in Complex Models project (EPSRC grant D048893/1)."}, {"heading": "A. EP Updates", "text": "We provide the expressions for the normalisation, mean and variance of the Projection operator of the ALD and cavity field. The latter is a normal distribution with mean \u03b2 and variance v . The normalisation constant is Z\u0302i = ZALD 2 [ KA erfc ( \u2212yi\u2212hA\u221a\n2v\n) +KB erfc ( yi\u2212hB\u221a\n2v )] where ZALD = \u03c4(1\u2212\u03c4) \u03c3 , hA = \u03b2 + \u03c4v \u03c3 ,\nKA = exp [ v 2\u03c32 \u03c4 2 + \u03b2\u2212yi\u03c3 \u03c4 ] . The corresponding hB and KB are obtained by replacing \u03c4 with \u03c4 \u2212 1 in hA and KA respectively. The complementary error function is defined as erfc(x) = 2\u221a\n\u03c0 \u222b +\u221e x e\u2212t 2\ndt. The mean is E[qi] = 1 Z\u0302i 1\u221a 2\u03c0v ZALD (KAIA +KBIB). The second order moment is E[qi 2] = 1\nZ\u0302i\n1\u221a 2\u03c0v\nZALD ( KAI 2 A +KBI 2 B ) ,\nwhere IA = \u2212v exp ( \u2212 (yi\u2212hA) 2\n2v\n) +hAMerfc ( hA\u2212yi\u221a\n2v\n) ,\nIB = v exp ( \u2212 (yi\u2212hB) 2\n2v\n) + hBMerfc ( yi\u2212hB\u221a\n2v\n) ,\nI2A = M ( h2A + v ) erfc ( hA\u2212yi\u221a\n2v\n) \u2212 v(hA + yi)eA,\nI2B = M ( h2B + v ) ( erf ( hB\u2212yi\u221a\n2v\n) + 1 ) + v(hB + yi)e B ,\nand M = \u221a\n\u03c0v 2 , e\nA = e\u2212 (hA\u2212yi)\n2\n2v , eB = e\u2212 (hB\u2212yi)\n2\n2v ."}], "references": [{"title": "Retirement, health and relationships of the older population in England: The 2004 English Longitudinal Study of Ageing", "author": ["J. Banks", "E. Breeze", "C. Lessof", "J. Nazroo"], "venue": "Technical report, Institute of Fiscal Studies,", "citeRegEx": "Banks et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Banks et al\\.", "year": 2006}, {"title": "Statistical Decision Theory and Bayesian Analysis", "author": ["Berger", "J O"], "venue": null, "citeRegEx": "Berger and O.,? \\Q1985\\E", "shortCiteRegEx": "Berger and O.", "year": 1985}, {"title": "Quality of life, health and physiological status and change at older ages", "author": ["D. Blane", "G. Netuveli", "S.M. Montgomery"], "venue": "Social Science and Medicine,", "citeRegEx": "Blane et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blane et al\\.", "year": 2008}, {"title": "Conditional quantile analysis when covariates are functions, with application to growth data", "author": ["K. Chen", "Muller", "H.-G"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Quantile Regression", "author": ["R. Koenker"], "venue": null, "citeRegEx": "Koenker,? \\Q2005\\E", "shortCiteRegEx": "Koenker", "year": 2005}, {"title": "Spatial quantile multiple regression using the asymmetric Laplace process", "author": ["K. Lum", "A. Gelfand"], "venue": "Bayesian Analysis,", "citeRegEx": "Lum and Gelfand,? \\Q2012\\E", "shortCiteRegEx": "Lum and Gelfand", "year": 2012}, {"title": "Expectation propagation for approximate bayesian inference", "author": ["T. Minka"], "venue": "Proceedings UAI, pp", "citeRegEx": "Minka,? \\Q2001\\E", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Multiple indicator cokriging with application to optimal sampling for environmental monitoring", "author": ["E. Pardoiguzquiza", "P. Dowd"], "venue": "Computers & Geosciences,", "citeRegEx": "Pardoiguzquiza and Dowd,? \\Q2005\\E", "shortCiteRegEx": "Pardoiguzquiza and Dowd", "year": 2005}, {"title": "Kernel conditional quantile estimation via reduction revisited", "author": ["N. Quadrianto", "K. Kersting", "M.D. Reid", "T.S. Caetano", "W.L. Buntine"], "venue": "In 2009 Ninth IEEE International Conference on Data Mining,", "citeRegEx": "Quadrianto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quadrianto et al\\.", "year": 2009}, {"title": "A unifying view of sparse approximate gaussian process regression", "author": ["J. Quinonero-Candela", "C.E. Rasmussen"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Quinonero.Candela and Rasmussen,? \\Q2005\\E", "shortCiteRegEx": "Quinonero.Candela and Rasmussen", "year": 2005}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Bayesian Inference and Optimal Design for the Sparse Linear Model", "author": ["M.W. Seeger"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Seeger,? \\Q2008\\E", "shortCiteRegEx": "Seeger", "year": 2008}, {"title": "A Bayesian Nonparametric Approach to Inference for Quantile Regression", "author": ["M.A. Taddy", "A. Kottas"], "venue": "Journal of Business and Economic Statistics,", "citeRegEx": "Taddy and Kottas,? \\Q2010\\E", "shortCiteRegEx": "Taddy and Kottas", "year": 2010}, {"title": "Bayesian quantile regression", "author": ["K. Yu", "R.A. Moyeed"], "venue": "Statistics & Probability Letters,", "citeRegEx": "Yu and Moyeed,? \\Q2001\\E", "shortCiteRegEx": "Yu and Moyeed", "year": 2001}, {"title": "A three-parameter asymmetric laplace distribution and its extension", "author": ["K. Yu", "J. Zhang"], "venue": "Communications in Statistics - Theory and Methods,", "citeRegEx": "Yu and Zhang,? \\Q2005\\E", "shortCiteRegEx": "Yu and Zhang", "year": 2005}, {"title": "Quantile regression: applications and current research areas", "author": ["K. Yu", "Z. Lu", "J. Stander"], "venue": "The Statistician,", "citeRegEx": "Yu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2003}, {"title": "Bayesian inference for additive mixed quantile regression models", "author": ["Y.R. Yue", "H. Rue"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "Yue and Rue,? \\Q2011\\E", "shortCiteRegEx": "Yue and Rue", "year": 2011}], "referenceMentions": [{"referenceID": 15, "context": "Quantile regression has been applied in a variety of domains and for different purposes (Yu et al., 2003).", "startOffset": 88, "endOffset": 105}, {"referenceID": 8, "context": "Alternatively for a Gaussian posterior model the inversion of the CDF can be done analytically to retrieve quantile functions as demonstrated in Quadrianto et al. (2009).", "startOffset": 145, "endOffset": 170}, {"referenceID": 4, "context": "The EE approach (Koenker, 2005) can be seen as akin to directly constructing a decision boundary to separate the classes.", "startOffset": 16, "endOffset": 31}, {"referenceID": 4, "context": "Many approaches use linear in parameter, or spline models (Koenker, 2005).", "startOffset": 58, "endOffset": 73}, {"referenceID": 4, "context": "These approaches are consistent and produce classical estimates for the quantile functions, which can also incorporate the use of simplex methods or post processing to ensure no order violations for multiple quantiles (Koenker, 2005).", "startOffset": 218, "endOffset": 233}, {"referenceID": 6, "context": "We place a Gaussian Process (GP) prior on the quantile regression function similarly to Lum & Gelfand (2012) and directly minimise the expected tilted loss using an Expectation Propagation (EP) approach (Minka, 2001).", "startOffset": 203, "endOffset": 216}, {"referenceID": 6, "context": "In EP, the posterior is approximated using an exponential-family distribution (Minka, 2001).", "startOffset": 78, "endOffset": 91}, {"referenceID": 11, "context": "Also note that simple approximations that use the Hessian to obtain an approximate Gaussian posterior centred on the mode are not applicable as the Laplace distribution is not differentiable at the mode (Seeger, 2008).", "startOffset": 203, "endOffset": 217}, {"referenceID": 8, "context": "input-dependent) variance mentioned in Quadrianto et al. (2009). The process is of the form: p(y|x) = \u03bc(x) + \u03c3(x) \u03be", "startOffset": 39, "endOffset": 64}, {"referenceID": 8, "context": "The same heteroscedastic scenario has been utilised in Quadrianto et al. (2009) allowing us to compare the performance of the QGP method to the methods examined therein.", "startOffset": 55, "endOffset": 80}, {"referenceID": 8, "context": "The same heteroscedastic scenario has been utilised in Quadrianto et al. (2009) allowing us to compare the performance of the QGP method to the methods examined therein. Our method performed, on average, on par with the Gaussian posterior method of Quadrianto et al. (2009) and the Quantile SVM (QSVM) method of Takeuchi et al.", "startOffset": 55, "endOffset": 274}, {"referenceID": 8, "context": "The same heteroscedastic scenario has been utilised in Quadrianto et al. (2009) allowing us to compare the performance of the QGP method to the methods examined therein. Our method performed, on average, on par with the Gaussian posterior method of Quadrianto et al. (2009) and the Quantile SVM (QSVM) method of Takeuchi et al. (2006) although it is hard to compare as the results are given for a single experimental realisation only (see Table 1 in Quadrianto et al.", "startOffset": 55, "endOffset": 335}, {"referenceID": 8, "context": "The same heteroscedastic scenario has been utilised in Quadrianto et al. (2009) allowing us to compare the performance of the QGP method to the methods examined therein. Our method performed, on average, on par with the Gaussian posterior method of Quadrianto et al. (2009) and the Quantile SVM (QSVM) method of Takeuchi et al. (2006) although it is hard to compare as the results are given for a single experimental realisation only (see Table 1 in Quadrianto et al. (2009)).", "startOffset": 55, "endOffset": 475}, {"referenceID": 4, "context": "For comparison with a well established quantile regression method, Figure 3 (b) shows the same information as Figure 3 (a) for spline quantile regression (Koenker, 2005).", "startOffset": 154, "endOffset": 169}, {"referenceID": 8, "context": "We do not compare against the Gaussian posterior method of Quadrianto et al. (2009) since that method relies on a Gaussian error model unlike the QGP and the QSVM methods.", "startOffset": 59, "endOffset": 84}, {"referenceID": 8, "context": "We do not compare against the Gaussian posterior method of Quadrianto et al. (2009) since that method relies on a Gaussian error model unlike the QGP and the QSVM methods. As in Takeuchi et al. (2006), we perform 10-fold cross validation on the data sets and transform the data to have zero mean, unit variance.", "startOffset": 59, "endOffset": 201}, {"referenceID": 0, "context": "ELSA is a multi-purpose large study which follows individuals aged 50 years or older (Banks et al., 2006).", "startOffset": 85, "endOffset": 105}, {"referenceID": 2, "context": "There exist various measures to estimate the latter and we have selected to use the CASP-19 measure following (Blane et al., 2008), which is a compound measure of several health and socioeconomical indicators.", "startOffset": 110, "endOffset": 130}, {"referenceID": 2, "context": "This analysis was done for the mean response in Blane et al. (2008) using structural equation modelling and our aim is to investigate whether their conclusions extend to the response quantiles as well.", "startOffset": 48, "endOffset": 68}, {"referenceID": 2, "context": "This analysis was done for the mean response in Blane et al. (2008) using structural equation modelling and our aim is to investigate whether their conclusions extend to the response quantiles as well. Our analysis is done cross-sectionally on the second ELSA wave dataset as in Blane et al. (2008). Therein it was found that lung function and obesity, but not blood pressure, were directly associated with QoL.", "startOffset": 48, "endOffset": 299}, {"referenceID": 2, "context": "BMI is ranked highly and diastolic blood pressure low for all quantiles in agreement with the findings of Blane et al. (2008) on the mean QoL.", "startOffset": 106, "endOffset": 126}, {"referenceID": 2, "context": "BMI is ranked highly and diastolic blood pressure low for all quantiles in agreement with the findings of Blane et al. (2008) on the mean QoL. Lung function is found to be most critical for the 25th quantile whereas it is found less relevant for the other quantiles. Age is ranked highly for the median and 75th quantile but less so for the 25th quantile. We therefore conclude that in terms of predicting quality of life as measured by the CASP-19 measure, the findings of Blane et al. (2008) on the mean hold for all quantiles considered in terms of BMI and diastolic blood pressure.", "startOffset": 106, "endOffset": 494}, {"referenceID": 6, "context": "Fractional EP (Minka, 2004) may be used to ease these problems as Seeger (2008) has noted, by reducing the impact of individual EP updates.", "startOffset": 15, "endOffset": 80}, {"referenceID": 4, "context": "Alternative approaches have been considered to try and estimate the quantile variance, typically using resampling methods such as the bootstrap (Koenker, 2005).", "startOffset": 144, "endOffset": 159}], "year": 2012, "abstractText": "Direct quantile regression involves estimating a given quantile of a response variable as a function of input variables. We present a new framework for direct quantile regression where a Gaussian process model is learned, minimising the expected tilted loss function. The integration required in learning is not analytically tractable so to speed up the learning we employ the Expectation Propagation algorithm. We describe how this work relates to other quantile regression methods and apply the method on both synthetic and real data sets. The method is shown to be competitive with state of the art methods whilst allowing for the leverage of the full Gaussian process probabilistic framework.", "creator": "LaTeX with hyperref package"}}}