{"id": "1206.6451", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "The Greedy Miser: Learning under Test-time Budgets", "abstract": "As machine studies algorithms waiting applications where industrial settings, there is increased interest in effective rest 8080 - time part similar. The cpu - time active own the put place now itself algorithm making the abrasion to its the known. The most even actual expanded when with clips set for diverse. In not that, definitely policies was differential, to Greedy Miser, might updates as feature artificial estimated december teaching to contrary overcome since cpu - return on lab. The deterministic is a straightforward line latter stage - thinking contour several is equally essential this regression or multi - class evaluated. Compared for prior work, rest however due more million - aggressive and blackish instead larger precise sets.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (956kb)", "http://arxiv.org/abs/1206.6451v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["zhixiang eddie xu", "kilian q weinberger", "olivier chapelle"], "accepted": true, "id": "1206.6451"}, "pdf": {"name": "1206.6451.pdf", "metadata": {"source": "META", "title": "The Greedy Miser: Learning under Test-time Budgets", "authors": ["Zhixiang (Eddie) Xu", "Kilian Q. Weinberger", "Olivier Chapelle"], "emails": ["XUZX@CSE.WUSTL.EDU", "KILIAN@WUSTL.EDU", "OLIVIER@CHAPELLE.CC"], "sections": [{"heading": "1. Introduction", "text": "The past decade has witnessed how the field of machine learning has established itself as a necessary component in several multi-billion-dollar industries. The applications range from web-search engines (Zheng et al., 2008), over product recommendation (Fleck et al., 1996), to email and web spam filtering (Weinberger et al., 2009). The realworld industrial setting introduces an interesting new problem to machine learning research: computational resources must be budgeted and costs must be strictly accounted for during test-time. Imagine an algorithm that is executed 10 million times per day. If a new feature improves the accuracy by 3%, but also increases the running time by 1s per execution, that would require the project manager to purchase 58 days of additional cpu time per day.\nAt its core, this problem is an inherent tradeoff between accuracy and test-time computation. The test-time compu-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ntation consists of two components: 1. the actual running time of the algorithm; 2. the time required for feature extraction.\nIn this paper, we propose a novel algorithm that makes this trade-off explicit and considers the feature extraction cost during training in order to minimize cpu usage during testtime. We first state the (non-continuous) global objective which explicitly trades off feature cost and accuracy, and relax it into a continuous loss function. Subsequently, we derive an update rule that shows the resulting loss lends itself naturally to greedy optimization with stage-wise regression (Friedman, 2001).\nWhile algorithms such as (Viola & Jones, 2002) directly attack the problem of fast evaluation for visual object detection, in most machine learning application domains, such as web-search ranking or email-spam filtering, analysis and algorithms for on-demand feature-cost amortization are still in their early stages.\nDifferent from previous approaches (Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012), our algorithm does not build cascades of classifiers. Instead, the cost/accuracy tradeoff is pushed into the training and selection of the weak classifiers. The resulting learning algorithm is much simpler than any prior work, as it is a variant of regular stage-wise regression, and yet leads to superior test-time performance. We evaluate our algorithm\u2019s efficacy on two real world data sets from very different application domains: scene recognition in images and ranking of web-search documents. Its accuracy matches that of the unconstrained baseline (with unlimited resources) while achieving an order of magnitude reduction of test-time cost. Because of its simplicity, high accuracy and drastic test-time cost-reduction we believe our approach to be of strong practical value for a wide range of problems."}, {"heading": "2. Related Work", "text": "Previous work on cost-sensitive learning appears in the context of many different applications. Most prominently, Viola & Jones (2002) greedily train a cascade of weak classifiers with Adaboost (Schapire, 1999) for visual object recognition. Cambazoglu et al. (2010) propose a cascade framework explicitly for web-search ranking. They learn a set of additive weak classifiers using gradient boosting, and remove data points during test-time using proximity scores. Although their algorithm requires almost no extra training cost, the improvement is typically limited. Lefakis & Fleuret (2010) and Dundar & Bi (2007) learn a softcascade, which re-weights inputs based on their probability of passing all stages. Different from our method, they employ a global probabilistic model, do not explicitly incorporate feature extraction costs and are restricted to binary classification problems. Saberian & Vasconcelos (2010) also learn classifier cascades. In contrast to prior work, they learn all cascades levels simultaneously in a greedy fashion. Unlike our approach, all of these algorithms focus on learning of cascades and none explicitly focus on individual feature costs.\nTo consider the feature cost, Gao & Koller (2011) published an algorithm to dynamically extract features during test-time. Raykar et al. (2010) learn classifier cascades, but they group features by their costs and restrict classifiers at each stage to only use a small subset. Pujara et al. (2011) suggest the use of sampling to derive a cascade of classifiers with increasing cost for email spam filtering. Most recently, Chen et al. (2012) introduce Cronus, which explicitly considers the feature extraction cost during training and constructs a cascade to encourage removal of unpromising data points early-on. At each stage, they optimize the coefficients of the weak classifiers to minimize the classification error and trees/features extraction costs. We pursue a very different (orthogonal) approach and do not optimize the cascade stages globally. Instead, we strictly incorporate the feature cost into the weak learners. Moreover, as our algorithm is a variant of stage-wise regression, it can operate naturally in both regression and multi-class classification scenarios. (Simultaneous with this publication, Grubb & Bagnell (2012) also proposed a complementary approach to incorporate feature cost into gradient boosting.)"}, {"heading": "3. Notation and Setup", "text": "Our training data consist of n input vectors {x1, . . . ,xn}\u2208 Rd with corresponding labels {y1, . . . , yn} \u2208 Y drawn from an unknown distributionD. Labels can be continuous (regression) or categorial (binary or multi-class classification). We assume that each feature \u03b1 has an acquisition cost c\u03b1 > 0 during its initial retrieval. Once a feature has been acquired its subsequent retrieval is free (or set to a small\nconstant).\nFurther, we are provided an arbitrary continuous loss function ` and aim to learn a linear predictorH\u03b2(x) = \u03b2>h(x) to minimize the loss function,\nmin \u03b2 `(\u03b2), (1)\nwithin some test-time cost budget, which will be defined in the following section. One example for ` is the squaredloss\n`sq(\u03b2) = 1\n2n n\u2211 i=1 (H\u03b2(xi)\u2212 yi)2 , (2)\nbut other losses, for example the multi-class logloss (Hastie et al., 2009), are equally suitable. The mapping x \u2192 h(x) is a non-linear transformation of the input data that allows the linear classifier to produce nonlinear decision boundaries in the original input space. Typically, the mapping h can be performed implicitly through the kernel-trick (Scho\u0308lkopf, 2001) or explicitly through, for example, the boosting-trick (Friedman, 2001; Rosset et al., 2004; Chapelle et al., 2010). In this paper we use the latter approach with limited-depth regression trees (Breiman, 1984). More precisely, h(xi) = [h1(xi), . . . , hT (xi)]>, ht \u2208 H where H is the set of all possible regression trees of some limited depth b (e.g. b = 4) and T = |H|. The resulting feature space is extremely high dimensional and the weight-vector \u03b2 is always kept to be correspondingly sparse. Because regression trees are negation closed (i.e. for each h \u2208 H we also have\u2212h \u2208 H) we assume throughout this paper w.l.o.g. that \u03b2 \u2265 0. Finally, we define a binary matrix F \u2208 {0, 1}d\u00d7T in which an entry F\u03b1t = 1 if and only if the regression tree ht \u2208H splits on feature \u03b1 somewhere within its tree."}, {"heading": "4. Method", "text": "In this section, we formalize the optimization problem of test-time computational cost, and then intuitively state our algorithm. We follow the setup introduced in (Chen et al., 2012), formalizing the test-time computational cost of evaluating the classifier H for a given weight-vector \u03b2.\nTest-time computational cost. There are two factors that contribute to this cost: The function evaluation cost of all trees ht with \u03b2t > 0 and the feature extraction cost for all features that are used in these trees. Let e > 0 be the cost to evaluate one tree ht if all features were previously extracted. With this notation, both costs can be expressed in a single function as\nc(\u03b2) = e\u2016\u03b2\u20160 + d\u2211\n\u03b1=1\nc\u03b1 \u2225\u2225\u2225\u2225\u2225 T\u2211 t=1 F\u03b1t\u03b2t \u2225\u2225\u2225\u2225\u2225 0 , (3)\nwhere the l0-norm for scalars is defined as \u2016a\u20160 \u2192 {0, 1} with \u2016a\u20160 = 1 if and only if a 6= 0. The first term captures the function-evaluation costs and the second term captures the feature costs of all used features. If we combine (1) with (3) we obtain our overall optimization problem\nmin \u03b2 `(\u03b2), subject to: c(\u03b2) \u2264 B, (4)\nwhere B\u2265 0 denotes some pre-defined budget that cannot be exceeded during test-time.\nAlgorithm. In the remainder of this paper we derive an algorithm to approximately minimize (4). For better clarity, we first give an intuitive overview of the resulting method in this paragraph. Our algorithm is based on stage-wise regression, which learns an additive classifier H\u03b2(x) =\u2211m t=1 \u03b2tht(x) that aims to minimize the loss function (4). 1 During iteration t, the greedy Classification and Regression Tree (CART) algorithm (Breiman, 1984) is used to generate a new tree ht, which is added to the classifier H\u03b2.\nSpecifically, CART generates a limited-depth regression tree ht \u2208 H by greedily minimizing an impurity function, g : H\u2192R+0 . Typical choices for g are the squared loss (2) or the label entropy (Hastie et al., 2009). CART minimizes the impurity function g by recursively splitting the data set on a single feature per tree-node. We propose an impurity function which on the one hand approximates the negative gradient of ` with the squared-loss, such that adding the resulting tree ht minimizes `, and on the other hand penalizes the initial extraction of features by their cost c\u03b1. To capture this initial extraction cost, we define an auxiliary variable \u03c6\u03b1 \u2208 {0, 1} indicating if feature \u03b1 has already been extracted (\u03c6\u03b1 = 0) in previous trees, or not (\u03c6\u03b1 = 1). We update the vector \u03c6 after generating each tree, setting the corresponding entry for used features \u03b1 to \u03c6\u03b1 := 0. Our impurity function in iteration t becomes\ng(ht)= 1\n2 \u2211 i ( \u2212 \u2202` \u2202H(xi) \u2212ht(xi) )2 +\u03bb d\u2211 \u03b1=1 \u03c6\u03b1c\u03b1F\u03b1t,\n(5) where \u03bb trades off the loss with the cost.\nTo combine the trees ht into a final classifier H\u03b2, our algorithm follows the steps of regular stage-wise regression with a fixed step-size \u03b7 > 0. As our algorithm is based on a greedy optimiser, and is stingy with respect to featureextraction, we refer to it as the Greedy Miser (short miser). Algorithm (1) shows a pseudo-code implementation.\n1Here, w.l.o.g. the trees inH are conveniently re-ordered such that exactly the first m trees have non-zero weight \u03b2t.\nAlgorithm 1 Greedy Miser in pseudo-code Require: D = {(xi, yi)}ni=1, step-size \u03b7, iterations m H = 0 for t = 1 to m do ht \u2190 Use CART to greedily minimize (5). H \u2190 H + \u03b7ht. For each feature \u03b1 used in ht, set \u03c6\u03b1 \u2190 0.\nend for Return H"}, {"heading": "5. Algorithm Derivation", "text": "In this section, we derive a connection between (4) and our miser algorithm by showing that miser approximately solves a relaxed version of the optimization problem."}, {"heading": "5.1. Relaxation", "text": "The optimization as stated in eq. (4) is non-continuous, because of the l0-norm in the cost term\u2014and hard to optimize. We start by introducing minor relaxations to both terms in (3) to make it better behaved.\nAssumptions. Our optimization algorithm (for details see section 5.2) performs coordinate descent and \u2014 starting from \u03b2=0 \u2014 increments one dimension of \u03b2 by \u03b7>0 in each iteration. Because of the extremely high dimensionality (which is dictated by the number of all possible regression trees that can be represented within the accuracy of the computer) and the comparably tiny number of iterations (\u2264 5000) it is reasonable to assume that one dimension is never incremented twice. In other words, the weight vector \u03b2 is extremely sparse and (up to re-scaling by 1\u03b7 ) binary: 1 \u03b7\u03b2 \u2208 {0, 1}T .\nTree-evaluation cost. The l0-norm is often relaxed into the convex and continuous l1-norm. In our scenario, this is particularly attractive, because if 1\u03b7\u03b2 is binary, then the re-scaled l1 norm is identical to the l0 norm\u2014and the relaxation is exact. We use this approach for the first term:\ne\u2016\u03b2\u20160 \u2212\u2192 e\n\u03b7 \u2016\u03b2\u20161. (6)\nFeature cost. In the case of the feature cost, the l1 norm is not a good approximation of the original l0-norm, because features are re-used many times, in different trees. Using the l1-norm would imply that features that are used more often would be penalized more than features that are only used once. This contradicts our assumption that features become free after their initial construction.\nWe therefore define a new function q, which is a re-scaled\nand amputated version of the `1-norm:\nq(x) = { |x\u03b7 | for |x| \u2208 [0, \u03b7) 1 for |x| \u2208 [\u03b7,\u221e). (7)\nThis penalty function q behaves like the regular `1 norm when |x| is small, but is capped to a constant when x \u2265 \u03b7. With this definition, our relaxation of the feature-cost term becomes:\nd\u2211 \u03b1=1 c\u03b1 \u2225\u2225\u2225\u2225\u2225 T\u2211 t=1 F\u03b1t\u03b2t \u2225\u2225\u2225\u2225\u2225 0 \u2212\u2192 d\u2211 \u03b1=1 c\u03b1q ( T\u2211 t=1 F\u03b1t\u03b2t ) . (8)\nSimilar to the previous case, if 1\u03b7\u03b2 is binary, this relaxation is exact. This holds because in (8) all arguments of q are non-negative multiples of \u03b7 (as F\u03b1t \u2208 {0, 1} and \u03b2t \u2208 {0, \u03b7}) and it is easy to see from the definition of q that for all k = 0, 1, . . . , we have q(k\u03b7) = \u2016k\u03b7\u20160.\nContinuous cost-term. To simplify the optimization, we split the budget into two terms B = Bt + Bf\u2014the treeevaluation budget and the feature extraction budget\u2014and re-write (4) with the two penalties (6) and (8) as two individual constraints. If we use the Lagrangian formulation, with Lagrange multiplier \u03bb (up to re-scaling), for the feature cost constraint and the explicit constraint formulation for the tree-evaluation cost, we obtain our final optimization problem:\nmin \u03b2 `(\u03b2) + \u03bb d\u2211 \u03b1=1 c\u03b1q (\u2211 t F\u03b1t\u03b2t ) (9)\ns.t. 1\n\u03b7 \u2016\u03b2\u20161 \u2264 Bt e ."}, {"heading": "5.2. Optimization", "text": "In this section we describe how miser, our adaptation of stage-wise regression (Friedman, 2001), finds a (local) solution to the optimization problem in (9).\nSolution path. We follow the approach from Rosset et al. (2004) and find a solution path for (9) for evenly spaced tree-evaluation budgets, ranging from B\u2032t = 0 to B \u2032 t =Bt. Along the path we iteratively increment B\u2032t by \u03b7. We repeatedly solve the intermediate optimization problem by warm-starting (9) with the previous solution and allowing the weight vector to change by \u03b7,\nmin \u03b4\u22650\nL(\u03b2+\u03b4)\ufe37 \ufe38\ufe38 \ufe37 `(\u03b2 + \u03b4) + \u03bb\nd\u2211 \u03b1=1 c\u03b1q (\u2211 t F\u03b1t(\u03b2t + \u03b4t) ) , (10)\ns.t. \u2016\u03b4\u20161 \u2264 \u03b7.\nEach iteration, we update the weight vector \u03b2 := \u03b2 + \u03b4.\nTaylor approximation. The Taylor expansion of L is defined as\nL(\u03b2 + \u03b4) = L(\u03b2) + \u3008\u2207L(\u03b2), \u03b4\u3009+O(\u03b42). (11) If \u03b7 is sufficiently small2, and because |\u03b4| \u2264 \u03b7, we can use the dominating linear term in (11) to approximate the optimization in (10) as\nmin \u03b4\u22650 \u3008\u2207L(\u03b2), \u03b4\u3009, s.t. \u2016\u03b4\u20161 \u2264 \u03b7. (12)\nCoordinate descent. The optimization (12) can be reduced to identifying the direction of steepest descent. Let \u2207L(\u03b2)t denote the gradient w.r.t. the tth dimension, and let us define\nt\u2217 = argmin t \u2207L(\u03b2)t, (13)\nto be the gradient dimension of steepest descent. Because H is negation closed, we have \u2207L(\u03b2)t\u2217 = \u2212\u2016\u2207L(\u03b2)\u2016\u221e. (If \u2207L(\u03b2)t\u2217 = 0 we are done, so we focus on the case when it is <0.) With Ho\u0308lder\u2019s inequality we can derive the following lower bound of the inner product in (12),\n\u3008\u2207L(\u03b2), \u03b4\u3009 \u2265 \u2212|\u3008\u2207L(\u03b2), \u03b4\u3009| \u2265 \u2212\u2016\u2207L(\u03b2)\u2016\u221e\u2016\u03b4\u20161 \u2265 \u03b7\u2207L(\u03b2)t\u2217 . (14)\nWe can now construct a vector \u03b4\u2217 for which (14) holds as equality, which implies that it must be the optimal solution to (12). This is the case if we set \u03b4\u2217t\u2217 = \u03b7 and \u03b4 \u2217 6=t\u2217 = 0. Consequently, we can find the solution path with steepest coordinate descent under step-size \u03b7.\nGradient derivation. The gradient \u2207L(\u03b2)t consists of two parts, the gradient of the loss ` and the gradient of the feature-cost term. For the latter, we need the gradient of q ( \u2211 t F\u03b1t\u03b2t), which, according to its definition in (7), is\nnot well-defined if \u2211 t F\u03b1t\u03b2t = \u03b7. As our optimization algorithm can only increase \u03b2t, we derive this gradient from the right, yielding\n\u2207q (\u2211\nt\nF\u03b1t\u03b2t\n) = { 1 \u03b7F\u03b1t | \u2211 t F\u03b1t\u03b2t| < \u03b7\n0 |\u2211t F\u03b1t\u03b2t| \u2265 \u03b7. (15) Note that the condition |\u2211t F\u03b1t\u03b2t|<\u03b7 is true if and only if feature \u03b1 is not used in any trees with \u03b2t>0. Let us define \u03c6\u03b1 = {0, 1} with \u03c6\u03b1 = 1 iff feature | \u2211 t F\u03b1t\u03b2t|<\u03b7. We can then express the gradient of L (with a slight abuse of notation) as\n\u2207L(\u03b2)t := \u2202` \u2202\u03b2t + \u03bb \u03b7 d\u2211 \u03b1=1 c\u03b1\u03c6\u03b1F\u03b1t. (16)\n2Please note that we see this as a true approximation, and do not expect \u03b7 to be infinitesimally small\u2014which would cause the number of steps (and therefore trees) to become too large for practical use.\nApplying the chain rule, we can decompose the first term in (16), \u2202`\u2202\u03b2t , into two parts: the derivatives w.r.t. the current prediction H\u03b2(xi), and the partial derivatives of H\u03b2(xi) w.r.t. \u03b2t. This results in\n\u2207L(\u03b2)t= n\u2211 i=1\n\u2202`\n\u2202H\u03b2(xi)\n\u2202H\u03b2(xi) \u2202\u03b2t + \u03bb \u03b7 d\u2211 \u03b1=1 c\u03b1\u03c6\u03b1F\u03b1t.\n(17)\nAsH\u03b2(xi)=\u03b2>h(xi) is linear, we have \u2202H\u03b2(xi) \u2202\u03b2t\n=ht(xi). If we define ri = \u2212 \u2202`\u2202H\u03b2(xi) , which we can easily compute for every xi, we can re-phrase (17) as\n\u2207L(\u03b2)t= n\u2211 i=1 \u2212riht(xi) + \u03bb \u03b7 d\u2211 \u03b1=1 c\u03b1\u03c6\u03b1F\u03b1t. (18)\nThe Greedy Miser. For simplicity, we restrict H to only normalized regression-trees (i.e. \u2211 i h 2 t (xi) = 1), which\nallows us to add two constant terms 12 \u2211 i h 2 t (xi) and r 2 i to (18) without affecting the outcome of the minimization in (13), as both are independent of t. This completes the binomial equation and we obtain a quadratic form:\nht=argmin ht\u2208H\n1\n2 n\u2211 i (ri \u2212 ht(xi))2+\u03bb\u2032 d\u2211 \u03b1=1 c\u03b1\u03c6\u03b1F\u03b1t,\n(19)\nwith \u03bb\u2032 = \u03bb\u03b7 . Note that (19) is exactly what miser minimizes in (5), which concludes our derivation.\nMeta-parameters. The meta-parameters of miser are surprisingly intuitive. The maximum number of iterations, m, is tightly linked to the tree-evaluation budget Bt. The optimal solution of (12) must satisfy the equality \u2016\u03b4\u2217\u20161 = \u03b7 (unless \u2207L= 0, in which case a local minimum has been reached and the algorithm would terminate). As \u2016\u03b2\u20161 is exactly increased by \u03b7 in each iteration, it can be expressed in terms of the number of iterationsm of the algorithm, and we obtain 1\u03b7\u2016\u03b2\u20161 = m. Consequently, in order to satisfy the l1 constraint in (9), we must limit to the number of iterations to m \u2264 Bte . The parameter \u03bb\u2032 corresponds directly to the feature-budget Bf . The algorithm is not particularly sensitive to the exact step-size \u03b7, and throughout this paper we set it to \u03b7=0.1."}, {"heading": "6. Results", "text": "We conduct experiments on two benchmark tasks from very different domains: the Yahoo Learning to Rank Challenge data set (Chapelle & Chang, 2011) and the scene recognition data set from Lazebnik et al. (2006).\nYahoo Learning to Rank. The Yahoo data set contains document/query pairs with label values from {0, 1, 2, 3, 4},\nwhere 0 means the document is irrelevant to the query, and 4 means highly relevant. In total, it has 473134, 71083, 165660, training, validation, and testing pairs. As this is a regression task, we use the squared-loss as our loss function `. Although the data set is representative for a web-search ranking training data set, in a real world test setting, there are many more irrelevant data points. Usually, for each query, only a few documents are relevant, and the other hundreds of thousands are completely irrelevant. Therefore, we follow the convention of Chen et al. (2012) and replicate each irrelevant data point (label value is 0) 10 times.\nEach feature in the data set has an acquisition cost. The feature costs are discrete values in the set {1, 5, 10, 20, 50, 100, 150}. The unit of these costs is approximately the time to evaluate a feature. The cheapest features (cost value is 1) are those that can be acquired by looking up a table (such as the statistics of a given document), whereas the most expensive ones (such as BM25FSD described in Broder et al. (2010)), typically involve term proximity scoring.\nTo evaluate the performance on this task, we follow the typical convention and use Normalized Discounted Cumulative Gain (NDCG@5) (Ja\u0308rvelin & Keka\u0308la\u0308inen, 2002), as it places stronger emphasis on retrieving relevant documents within a large set of irrelevant documents.\nLoss/cost trade-off. Figure 1 (left) shows the traces (dashed lines) of the NDCG@5/cost generated by repeatedly adding trees to the predictor until 3000 trees in total \u2014 essentially depicting the results under increasing treeevaluation budgets Bt. The different traces are obtained under varying values of the feature-cost trade-off parameter \u03bb. The baseline, stage-wise regression (Friedman, 2001), is equivalent to miser with \u03bb = 0 and is essentially building trees without any cost consideration. The red circles indicate the iteration with the highest NDCG@5 value on the validation data set. The graph shows, that under increased \u03bb (the solid red line), the NDCG@5 ranking accuracy of miser drops very gradually, while the test-time cost is reduced drastically (compared to \u03bb=0).\nComparison with prior work. In addition to stage-wise regression, we also compare against Stage-wise regression feature subsets, Early Exit (Cambazoglu et al., 2010) and Cronus (Chen et al., 2012). Stage-wise regression feature subsets is a natural extension to stage-wise regression. We group all features according to the feature cost, and gradually use more expensive feature groups. The curve is generated by only using features whose cost\u2264 1, 20, 100, 200. Early Exit, proposed by Cambazoglu et al. (2010), trains trees identical to stage-wise regression\u2014however, it reduces the average test-time cost by removing unpromising documents early-on during test-time. Among all methods\nof early-exit the authors suggested, we plot the best performing one (Early Exit Using Proximity Threshold). We introduce an early exit every 10 trees (300 in total), and at the ith early-exit, we remove all test-inputs that have a score of at least (300\u2212i)s299 lower than the fifth best input (where s is a parameter regulating the pruning aggressiveness). The overall improvement over stage-wise regression is limited because the cost is dominated by the feature acquisition, rather than tree computation. It is worth pointing out that the cascade-based approaches of Early-Exits and Cronus are actually complementary to miser and future work should combined them.\nSince Cronus does not scale to the full data set, we use the subset of the Yahoo data from Chen et al. (2012) of 141397, 146769, 184968, training, validation and testing points respectively. In comparison to Cronus, which requires O(mn) memory, miser requires no significant operational memory besides the data and scales easily to millions of data points. Figure 1 (right) depicts the tradeoff curves, of miser and competing algorithms, between the test-time cost and generalization error. We generate the curves by varying the feature-cost trade-off \u03bb (or the pruning parameter s for Early-Exits). For each setting we choose the iteration that has the best validation NDCG@5 score. The graph shows that all algorithms manage to match the unconstrained cost-results of stage-wise regression. However, the trade-off curve of miser stays consistently above that of Cronus and Early Exits, leading to better ranking accuracy at lower test-time cost. In fact, miser can almost match the ranking accuracy of stage-wise regression with 1/10 of the cost, whereas Cronus reduces the cost only to 1/4 and Early-Exits to 1/2.\nFeature extraction. To investigate what effect the featurecost trade-off parameter \u03bb has on the classifier\u2019s feature choices, Figure 2 visualizes what type of features are extracted by miser as \u03bb increases. For this visualization, we group features by cost and show what fraction of features in each group are extracted. The legend in the right indicates the cost of a feature group and the number of features that fall into it (in the parentheses). We plot the feature fraction at the best performing iteration based on the validation set. With \u03bb=0, miser does not consider the feature cost when building trees, and thus extracts a variety of expensive features. As \u03bb increases, it extracts fewer expensive features and re-uses more cheap features (c\u03b1 = 1). It is interesting to point out that across all different miser settings, a few\nexpensive features (cost\u2265150) are always extracted within early iterations. This highlights a great advantage of miser over some other cascade algorithms (Raykar et al., 2010), which learn cascades with pre-assigned feature costs and cannot extract good but expensive features until the very end.\nScene Recognition. The Scene-15 data set (Lazebnik et al., 2006) is from a very different data domain. It contains 4485 images from 15 scene classes and the task is to classify images according to scene. Figure 4 shows one example image for each scene category. We follow the procedure used by Lazebnik et al. (2006); Li et al. (2010), randomly sampling 100 images from each class, resulting in 1500 training images. From the remaining 2985 images, we randomly sample 20 images from each class as validation, and leave the rest 2685 for test.\nWe use a diverse set of visual descriptors varying in computation time and accuracy: GIST, spatial HOG, Local Binary Pattern, self-similarity, texton histogram, geometric texton, geometric texton, geometric color, and Object Bank (Li et al., 2010). The authors from Object Bank apply 177 object detectors to each image, where each object detector works independently of each other. We treat each object detector as an independent descriptor and end up with a total of 184 different visual descriptors.\nWe split the training data 30/70 and use the smaller subset to construct a kernel and train 15 one-vs-all SVMs for each descriptor. We use the predictions of these SVMs on the larger subset as the features of miser (totaling d=184\u00d715= 2760 features.) As loss function `, we use the multi-class log-loss (Hastie et al., 2009) and maintain 15 tree-ensemble classifiers H1, . . . ,H15, one for each class. During each\niteration, we construct 15 regression trees (depth 3) and update all classifiers. For a given image, each classifier\u2019s (normalized) output represents the probability of this data point belonging to one class.\nWe compute the feature-extraction-cost as the cpu-time required for the computation for the visual descriptor, the kernel construction and the SVM evaluation. Each visual descriptor is used by 15 one-vs-all features. The moment any one of these features is used, we set the feature extraction cost of all other features that are based on the same visual descriptor to only the SVM evaluation time (e.g. if the first HOG-based feature is used, the cost of all other HOGbased features is reduced to the time required to evaluate the SVM). Figure 3 summarizes the results on the Scene-15 data set. As baseline we use stage-wise regression (Friedman, 2001) and an SVM with the averaged kernel of all descriptors. We also apply stage-wise regression with Early Exits. As this is multi-class classification instead of regression we introduce an early exit every 10 trees (300 in total), and we remove test-inputs whose maximum classlikelihood is greater than a threshold s. We generate the curve of early exit by gradually increasing the value for s. The last baseline is original vision features with `1 regularization, and we notice that its accuracy never exceeds 0.74, and therefore we do not plot it. The miser curve is generated by varying loss/feature-cost trade-off \u03bb. For each setting we choose the iteration that has the best validation accuracy, and all results are obtained by averaging over 10 randomly generated training/testing splits.\nBoth, multiple-kernel SVM and stage-wise regression achieve high accuracy, but their need to extract all features significantly increases their cost. Early Exit has only limited improvement due to the inability to select a few expensive but important features in early iterations. As before, miser champions the cost/accuracy trade-off and its accuracy drops gently with increasing \u03bb.\nAll experiments (on both data sets) were conducted on a desktop with dual 6-core Intel i7 cpus with 2.66GHz. The training time for miser requires comparable amount of time\nas stage-wise regression (about 80 minutes for the full Yahoo data set and 12 minutes for Scene-15.)"}, {"heading": "7. Conclusion", "text": "Accounting for the operational cost of machine learning algorithms is a crucial problem that appears throughout current and potential applications of machine learning. We believe that understanding and controlling this trade-off will become a fundamental part of machine-learning research in the near future. This paper introduces a natural extension to stage-wise regression (Friedman, 2001), which incorporates feature cost during training. The resulting algorithm, the Greedy Miser, is simple to implement, naturally scales to large data sets and outperforms previously most costeffective classifiers.\nFuture work includes combining our approach with Early Exits (Cambazoglu et al., 2010) or cascade based learning methods such as (Chen et al., 2012)."}, {"heading": "8. Acknowledgements", "text": "KQW and ZX would like to thank NIH for their support through grant U01 1U01NS073457-01."}], "references": [{"title": "Classification and regression trees", "author": ["L. Breiman"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Breiman,? \\Q1984\\E", "shortCiteRegEx": "Breiman", "year": 1984}, {"title": "Exploiting site-level information to improve web search", "author": ["A. Broder", "E. Gabrilovich", "V. Josifovski", "G. Mavromatis", "D. Metzler", "J. Wang"], "venue": "In CIKM", "citeRegEx": "Broder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Broder et al\\.", "year": 2010}, {"title": "Early exit optimizations for additive machine learned ranking systems", "author": ["B.B. Cambazoglu", "H. Zaragoza", "O. Chapelle", "J. Chen", "C. Liao", "Z. Zheng", "J. Degenhardt"], "venue": "In ICDM,", "citeRegEx": "Cambazoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cambazoglu et al\\.", "year": 2010}, {"title": "Yahoo! learning to rank challenge overview", "author": ["O. Chapelle", "Y. Chang"], "venue": "In JMLR Workshop and Conference Proceedings,", "citeRegEx": "Chapelle and Chang,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Chang", "year": 2011}, {"title": "Boosted multi-task learning", "author": ["O. Chapelle", "P. Shivaswamy", "S. Vadrevu", "K. Weinberger", "Y. Zhang", "B. Tseng"], "venue": "Machine Learning, pp", "citeRegEx": "Chapelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2010}, {"title": "Classifier cascade for minimizing feature evaluation cost", "author": ["Chen", "Minmin", "Xu", "Zhixiang", "Weinberger", "Kilian Q", "Chapelle", "Olivier"], "venue": "In AISTATS", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Joint optimization of cascaded classifiers for computer aided detection", "author": ["M.M. Dundar", "J. Bi"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Dundar and Bi,? \\Q2007\\E", "shortCiteRegEx": "Dundar and Bi", "year": 2007}, {"title": "Finding naked people", "author": ["M. Fleck", "D. Forsyth", "C. Bregler"], "venue": "Computer Vision\u2014ECCV\u201996, pp", "citeRegEx": "Fleck et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Fleck et al\\.", "year": 1996}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics, pp. 1189\u20131232,", "citeRegEx": "Friedman,? \\Q2001\\E", "shortCiteRegEx": "Friedman", "year": 2001}, {"title": "Active classification based on value of classifier", "author": ["Gao", "Tianshi", "Koller", "Daphne"], "venue": "K.Q. (eds.),", "citeRegEx": "Gao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2011}, {"title": "Speedboost: Anytime prediction with uniform near-optimality", "author": ["Grubb", "Alex", "Bagnell", "Drew"], "venue": "In AISTATS", "citeRegEx": "Grubb et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Grubb et al\\.", "year": 2012}, {"title": "The elements of statistical learning", "author": ["Hastie", "Trevor", "Tibshirani", "Robert", "Friedman", "JH (Jerome H"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Cumulated gain-based evaluation of IR techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen", "year": 2002}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Lazebnik et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lazebnik et al\\.", "year": 2006}, {"title": "Object bank: A highlevel image representation for scene classification and semantic feature sparsification", "author": ["L.J. Li", "H. Su", "E.P. Xing", "L. Fei-Fei"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Using classifier cascades for scalable e-mail classification", "author": ["J. Pujara", "H. Daum\u00e9 III", "L. Getoor"], "venue": "CEAS", "citeRegEx": "Pujara et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pujara et al\\.", "year": 2011}, {"title": "Designing efficient cascaded classifiers: tradeoff between accuracy and cost", "author": ["V.C. Raykar", "B. Krishnapuram", "S. Yu"], "venue": "In SIGKDD", "citeRegEx": "Raykar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raykar et al\\.", "year": 2010}, {"title": "Boosting as a regularized path to a maximum margin classifier", "author": ["S. Rosset", "J. Zhu", "T. Hastie"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Rosset et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosset et al\\.", "year": 2004}, {"title": "Boosting Classifier Cascades", "author": ["Saberian", "Mohammad", "Vasconcelos", "Nuno"], "venue": "NIPS 23,", "citeRegEx": "Saberian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saberian et al\\.", "year": 2010}, {"title": "A brief introduction to boosting", "author": ["R.E. Schapire"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Schapire,? \\Q1999\\E", "shortCiteRegEx": "Schapire", "year": 1999}, {"title": "The kernel trick for distances", "author": ["B. Sch\u00f6lkopf"], "venue": "NIPS, pp", "citeRegEx": "Sch\u00f6lkopf,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf", "year": 2001}, {"title": "Robust real-time object detection", "author": ["P. Viola", "M. Jones"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Viola and Jones,? \\Q2002\\E", "shortCiteRegEx": "Viola and Jones", "year": 2002}, {"title": "Feature hashing for large scale multitask learning", "author": ["K.Q. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "ICMl", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 7, "context": ", 2008), over product recommendation (Fleck et al., 1996), to email and web spam filtering (Weinberger et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 22, "context": ", 1996), to email and web spam filtering (Weinberger et al., 2009).", "startOffset": 41, "endOffset": 66}, {"referenceID": 8, "context": "Subsequently, we derive an update rule that shows the resulting loss lends itself naturally to greedy optimization with stage-wise regression (Friedman, 2001).", "startOffset": 142, "endOffset": 158}, {"referenceID": 15, "context": "Different from previous approaches (Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012), our algorithm does not build cascades of classifiers.", "startOffset": 35, "endOffset": 130}, {"referenceID": 5, "context": "Different from previous approaches (Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012), our algorithm does not build cascades of classifiers.", "startOffset": 35, "endOffset": 130}, {"referenceID": 19, "context": "Most prominently, Viola & Jones (2002) greedily train a cascade of weak classifiers with Adaboost (Schapire, 1999) for visual object recognition.", "startOffset": 98, "endOffset": 114}, {"referenceID": 2, "context": "Cambazoglu et al. (2010) propose a cascade framework explicitly for web-search ranking.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "Cambazoglu et al. (2010) propose a cascade framework explicitly for web-search ranking. They learn a set of additive weak classifiers using gradient boosting, and remove data points during test-time using proximity scores. Although their algorithm requires almost no extra training cost, the improvement is typically limited. Lefakis & Fleuret (2010) and Dundar & Bi (2007) learn a softcascade, which re-weights inputs based on their probability of passing all stages.", "startOffset": 0, "endOffset": 351}, {"referenceID": 2, "context": "Cambazoglu et al. (2010) propose a cascade framework explicitly for web-search ranking. They learn a set of additive weak classifiers using gradient boosting, and remove data points during test-time using proximity scores. Although their algorithm requires almost no extra training cost, the improvement is typically limited. Lefakis & Fleuret (2010) and Dundar & Bi (2007) learn a softcascade, which re-weights inputs based on their probability of passing all stages.", "startOffset": 0, "endOffset": 374}, {"referenceID": 2, "context": "Cambazoglu et al. (2010) propose a cascade framework explicitly for web-search ranking. They learn a set of additive weak classifiers using gradient boosting, and remove data points during test-time using proximity scores. Although their algorithm requires almost no extra training cost, the improvement is typically limited. Lefakis & Fleuret (2010) and Dundar & Bi (2007) learn a softcascade, which re-weights inputs based on their probability of passing all stages. Different from our method, they employ a global probabilistic model, do not explicitly incorporate feature extraction costs and are restricted to binary classification problems. Saberian & Vasconcelos (2010) also learn classifier cascades.", "startOffset": 0, "endOffset": 677}, {"referenceID": 14, "context": "Raykar et al. (2010) learn classifier cascades, but they group features by their costs and restrict classifiers at each stage to only use a small subset.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "Pujara et al. (2011) suggest the use of sampling to derive a cascade of classifiers with increasing cost for email spam filtering.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "Most recently, Chen et al. (2012) introduce Cronus, which explicitly considers the feature extraction cost during training and constructs a cascade to encourage removal of unpromising data points early-on.", "startOffset": 15, "endOffset": 34}, {"referenceID": 5, "context": "Most recently, Chen et al. (2012) introduce Cronus, which explicitly considers the feature extraction cost during training and constructs a cascade to encourage removal of unpromising data points early-on. At each stage, they optimize the coefficients of the weak classifiers to minimize the classification error and trees/features extraction costs. We pursue a very different (orthogonal) approach and do not optimize the cascade stages globally. Instead, we strictly incorporate the feature cost into the weak learners. Moreover, as our algorithm is a variant of stage-wise regression, it can operate naturally in both regression and multi-class classification scenarios. (Simultaneous with this publication, Grubb & Bagnell (2012) also proposed a complementary approach to incorporate feature cost into gradient boosting.", "startOffset": 15, "endOffset": 734}, {"referenceID": 11, "context": "but other losses, for example the multi-class logloss (Hastie et al., 2009), are equally suitable.", "startOffset": 54, "endOffset": 75}, {"referenceID": 20, "context": "Typically, the mapping h can be performed implicitly through the kernel-trick (Sch\u00f6lkopf, 2001) or explicitly through, for example, the boosting-trick (Friedman, 2001; Rosset et al.", "startOffset": 78, "endOffset": 95}, {"referenceID": 8, "context": "Typically, the mapping h can be performed implicitly through the kernel-trick (Sch\u00f6lkopf, 2001) or explicitly through, for example, the boosting-trick (Friedman, 2001; Rosset et al., 2004; Chapelle et al., 2010).", "startOffset": 151, "endOffset": 211}, {"referenceID": 17, "context": "Typically, the mapping h can be performed implicitly through the kernel-trick (Sch\u00f6lkopf, 2001) or explicitly through, for example, the boosting-trick (Friedman, 2001; Rosset et al., 2004; Chapelle et al., 2010).", "startOffset": 151, "endOffset": 211}, {"referenceID": 4, "context": "Typically, the mapping h can be performed implicitly through the kernel-trick (Sch\u00f6lkopf, 2001) or explicitly through, for example, the boosting-trick (Friedman, 2001; Rosset et al., 2004; Chapelle et al., 2010).", "startOffset": 151, "endOffset": 211}, {"referenceID": 0, "context": "In this paper we use the latter approach with limited-depth regression trees (Breiman, 1984).", "startOffset": 77, "endOffset": 92}, {"referenceID": 5, "context": "We follow the setup introduced in (Chen et al., 2012), formalizing the test-time computational cost of evaluating the classifier H for a given weight-vector \u03b2.", "startOffset": 34, "endOffset": 53}, {"referenceID": 0, "context": "1 During iteration t, the greedy Classification and Regression Tree (CART) algorithm (Breiman, 1984) is used to generate a new tree ht, which is added to the classifier H\u03b2.", "startOffset": 85, "endOffset": 100}, {"referenceID": 11, "context": "Typical choices for g are the squared loss (2) or the label entropy (Hastie et al., 2009).", "startOffset": 68, "endOffset": 89}, {"referenceID": 8, "context": "In this section we describe how miser, our adaptation of stage-wise regression (Friedman, 2001), finds a (local) solution to the optimization problem in (9).", "startOffset": 79, "endOffset": 95}, {"referenceID": 17, "context": "We follow the approach from Rosset et al. (2004) and find a solution path for (9) for evenly spaced tree-evaluation budgets, ranging from B\u2032 t = 0 to B \u2032 t =Bt.", "startOffset": 28, "endOffset": 49}, {"referenceID": 13, "context": "We conduct experiments on two benchmark tasks from very different domains: the Yahoo Learning to Rank Challenge data set (Chapelle & Chang, 2011) and the scene recognition data set from Lazebnik et al. (2006).", "startOffset": 186, "endOffset": 209}, {"referenceID": 5, "context": "Therefore, we follow the convention of Chen et al. (2012) and replicate each irrelevant data point (label value is 0) 10 times.", "startOffset": 39, "endOffset": 58}, {"referenceID": 1, "context": "The cheapest features (cost value is 1) are those that can be acquired by looking up a table (such as the statistics of a given document), whereas the most expensive ones (such as BM25FSD described in Broder et al. (2010)), typically involve term proximity scoring.", "startOffset": 201, "endOffset": 222}, {"referenceID": 8, "context": "The baseline, stage-wise regression (Friedman, 2001), is equivalent to miser with \u03bb = 0 and is essentially building trees without any cost consideration.", "startOffset": 36, "endOffset": 52}, {"referenceID": 2, "context": "In addition to stage-wise regression, we also compare against Stage-wise regression feature subsets, Early Exit (Cambazoglu et al., 2010) and Cronus (Chen et al.", "startOffset": 112, "endOffset": 137}, {"referenceID": 5, "context": ", 2010) and Cronus (Chen et al., 2012).", "startOffset": 19, "endOffset": 38}, {"referenceID": 2, "context": "In addition to stage-wise regression, we also compare against Stage-wise regression feature subsets, Early Exit (Cambazoglu et al., 2010) and Cronus (Chen et al., 2012). Stage-wise regression feature subsets is a natural extension to stage-wise regression. We group all features according to the feature cost, and gradually use more expensive feature groups. The curve is generated by only using features whose cost\u2264 1, 20, 100, 200. Early Exit, proposed by Cambazoglu et al. (2010), trains trees identical to stage-wise regression\u2014however, it reduces the average test-time cost by removing unpromising documents early-on during test-time.", "startOffset": 113, "endOffset": 483}, {"referenceID": 8, "context": "Stage\u2212wise regression (Friedman, 2001) Stage\u2212wise regression feature subsets Early exit (Cambazoglu et.", "startOffset": 22, "endOffset": 38}, {"referenceID": 5, "context": "Since Cronus does not scale to the full data set, we use the subset of the Yahoo data from Chen et al. (2012) of 141397, 146769, 184968, training, validation and testing points respectively.", "startOffset": 91, "endOffset": 110}, {"referenceID": 16, "context": "This highlights a great advantage of miser over some other cascade algorithms (Raykar et al., 2010), which learn cascades with pre-assigned feature costs and cannot extract good but expensive features until the very end.", "startOffset": 78, "endOffset": 99}, {"referenceID": 13, "context": "The Scene-15 data set (Lazebnik et al., 2006) is from a very different data domain.", "startOffset": 22, "endOffset": 45}, {"referenceID": 13, "context": "The Scene-15 data set (Lazebnik et al., 2006) is from a very different data domain. It contains 4485 images from 15 scene classes and the task is to classify images according to scene. Figure 4 shows one example image for each scene category. We follow the procedure used by Lazebnik et al. (2006); Li et al.", "startOffset": 23, "endOffset": 298}, {"referenceID": 13, "context": "The Scene-15 data set (Lazebnik et al., 2006) is from a very different data domain. It contains 4485 images from 15 scene classes and the task is to classify images according to scene. Figure 4 shows one example image for each scene category. We follow the procedure used by Lazebnik et al. (2006); Li et al. (2010), randomly sampling 100 images from each class, resulting in 1500 training images.", "startOffset": 23, "endOffset": 316}, {"referenceID": 14, "context": "We use a diverse set of visual descriptors varying in computation time and accuracy: GIST, spatial HOG, Local Binary Pattern, self-similarity, texton histogram, geometric texton, geometric texton, geometric color, and Object Bank (Li et al., 2010).", "startOffset": 230, "endOffset": 247}, {"referenceID": 11, "context": ") As loss function `, we use the multi-class log-loss (Hastie et al., 2009) and maintain 15 tree-ensemble classifiers H, .", "startOffset": 54, "endOffset": 75}, {"referenceID": 8, "context": "As baseline we use stage-wise regression (Friedman, 2001) and an SVM with the averaged kernel of all descriptors.", "startOffset": 41, "endOffset": 57}, {"referenceID": 8, "context": "This paper introduces a natural extension to stage-wise regression (Friedman, 2001), which incorporates feature cost during training.", "startOffset": 67, "endOffset": 83}, {"referenceID": 2, "context": "Future work includes combining our approach with Early Exits (Cambazoglu et al., 2010) or cascade based learning methods such as (Chen et al.", "startOffset": 61, "endOffset": 86}, {"referenceID": 5, "context": ", 2010) or cascade based learning methods such as (Chen et al., 2012).", "startOffset": 50, "endOffset": 69}], "year": 2012, "abstractText": "As machine learning algorithms enter applications in industrial settings, there is increased interest in controlling their cpu-time during testing. The cpu-time consists of the running time of the algorithm and the extraction time of the features. The latter can vary drastically when the feature set is diverse. In this paper, we propose an algorithm, the Greedy Miser, that incorporates the feature extraction cost during training to explicitly minimize the cpu-time during testing. The algorithm is a straightforward extension of stagewise regression and is equally suitable for regression or multi-class classification. Compared to prior work, it is significantly more cost-effective and scales to larger data sets.", "creator": "LaTeX with hyperref package"}}}