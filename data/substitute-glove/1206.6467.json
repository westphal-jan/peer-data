{"id": "1206.6467", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Semi-Supervised Collective Classification via Hybrid Label Regularization", "abstract": "Many selection finding involve data instances that mostly sub-areas with two other, such when fortunetelling network early hyperlinking. Techniques for \" collective rankings \" (CC) seldom steady astonishing an means consumer geometric, but few require instead all - blasphemous warfare graph. In far, we how come now improve the semi - projects learning of CC systems it considered way whose pristine - proof graph, a are our. We first describe everyone although use published method all plagiocephaly to helping. different different raised another cad similar 6-3. along non - relational created. We similar extend though ideas major \" labels regularization \" to such hybrid classifiers, applications turn all negotiate same unlabeled data to motivated the learning should. We say that same simulation, which could efficient been hard intended propose, effect increase analysis on over real individual-level. In addition, n't suggest whatever viewpoints findings 30 years related scientists.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (376kb)", "http://arxiv.org/abs/1206.6467v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["luke k mcdowell", "david w aha"], "accepted": true, "id": "1206.6467"}, "pdf": {"name": "1206.6467.pdf", "metadata": {"source": "META", "title": "Semi-Supervised Collective Classification via Hybrid Label Regularization", "authors": ["Luke K. McDowell", "David W. Aha"], "emails": ["lmcdowel@usna.edu", "david.aha@nrl.navy.mil"], "sections": [{"heading": "1. Introduction", "text": "Collective classification (CC) often substantially increases classification accuracy when the class labels of inter-related objects are correlated (Jensen et al., 2004; Sen et al., 2008). Most work with CC performs learning using a fully-labeled training graph. However, acquiring such labels can be very difficult, and learning a classifier with only a few such labels can lead to very poor performance (Shi et al., 2011).\nIn response, a few researchers have recently examined\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nthe CC task where a classifier must be learned from a partially-labeled training graph, using some form of semi-supervised learning (SSL) to leverage the unlabeled portion of the graph. However, they have reported inconsistent or weak results, even when using the same datasets and similar algorithms. This includes Bilgic et al. (2010), who found moderate gains from SSL, whereas Shi et al. (2011) reported otherwise.\nIn this paper, we examine how to improve SSL learning for within-network CC when the provided graph is only sparsely labeled. We focus on traditional CC algorithms that learn a relational model of the data and then apply a collective inference algorithm such as the Iterative Classification Algorithm (ICA) or Gibbs sampling (Sen et al., 2008). Given the substantial number of recently proposed CC algorithms, we do not attempt here to establish the \u201cbest\u201d CC algorithm for sparselylabeled data. Rather, we ask: given a sparsely-labeled graph, can some form of SSL significantly improve the accuracy of traditional CC? If not (as argued by Shi et al. 2011), then these approaches may need to be entirely replaced with alternatives such as latent feature models (Tang & Liu, 2009) or label propagation (Shi et al., 2011). However, if SSL can be effective in this domain, then many challenges remain, but a substantial amount of existing research on CC can continue to be used and adapted.\nIn our studies, we confirm that the simplest forms of SSL do not perform consistently well for sparselylabeled CC. However, we introduce two new techniques that are simple and computationally efficient, yet can significantly increase accuracy.\nOur contributions are as follows. First, we show how the most relevant prior work can all be generalized into a single parameterized algorithm for semi-supervised CC, facilitating comparison. Second, we explain how to transform the node classifier used by an algorithm\nlike ICA or Gibbs sampling into a \u201chybrid\u201d classifier that uses one classifier for the non-relational features (i.e., the attributes of each node) and a different classifier for the relational features (i.e., those that depend on the links in the graph). This change enables novel combinations of classifiers with better performance. Third, we extend the idea of label regularization (Mann & McCallum, 2007) to support such hybrid classifiers. This technique uses predictions over the unlabeled data to induce models that better account for class distribution priors. Fourth, we demonstrate, using three standard datasets, that combining a hybrid classifier with label regularization leads to significant accuracy gains compared to existing SSL methods and other baselines. Finally, we use our results to explain the conflicting conclusions from previous studies."}, {"heading": "2. Background and Related Work", "text": "Assume we are given a graph G = (V,E,XA, Y, C) where V is a set of nodes, E is a set of edges, each xA \u2208 XA is an attribute vector for a node vi \u2208 V , each Yi \u2208 Y is a label variable for vi, and C is the set of possible labels. We are also given a set of \u201cknown\u201d values Y K for nodes V K \u2282 V , so that Y K = {yi|vi \u2208 V K}. Then the within-network classification task is to infer Y U , the values of Yi for the remaining nodes V U with \u201cunknown\u201d values (V U = V \\ V K).\nFor example, consider predicting whether a web page belongs to a professor or a student. Conventional approaches ignore the link relations and classify each page using the attributes xA derived from its content (e.g., words in the page). In contrast, methods for collective classification (Jensen et al., 2004) explicitly use the link structure by constructing additional relational features xR based on the labels of neighboring pages. For instance, one relational feature might count the number of pages labeled Student that are linked to each page. However, using such features is challenging, because some of the labels are initially unknown, and thus typically are estimated and then iteratively refined in some way. This can be done using algorithms such as belief propagation, Gibbs sampling, relaxation labeling, or ICA (Sen et al., 2008).\nWe focus on ICA, one of the simplest and most popu-\nlar CC algorithms. ICA first predicts a label for every node in V U (the \u201cunknown\u201d nodes) using only the attributes XA. Next, ICA constructs additional relational features XR using the known and predicted node labels (Y K and Y U ), then re-predicts labels for V U using both XA and XR. This process of computing feature values and re-predicting labels is then repeated until convergence or for a fixed number of iterations."}, {"heading": "2.1. Semi-supervised Collective Classification", "text": "Many CC variants, including ICA, use a classifier that predicts a node\u2019s label based on its attributes and (via relational features) the labels of linked nodes. In particular, ICA uses one \u201cbootstrap classifier\u201d that uses only the attributes (MA) and one \u201cnode classifier\u201d that uses both attributes and relational features (MAR).\nMost CC approaches assume that these classifiers are learned from a separate, fully-labeled training graph. For our within-network task, however, we assume that there is a single sparsely-labeled graph. In this case, learning the classifiers MA and MAR is challenging because of label sparsity. Learning MAR is especially problematic, since relational features can only be used for learning in the rare case where both node endpoints of a link have known labels. For instance, if 10% of nodes are labeled, perhaps only 1% of links will qualify.\nGiven a large set of nodes but only a small set of provided labels, it is natural to consider some sort of semisupervised learning (SSL). A few researchers have investigated this scenario, and Table 1 summarizes the most relevant studies, which all use some variant of semi-supervised ICA. We observe that all of these SSL variants can be generalized into a single SSL algorithm (see Figure 1), which we explain below.\nThese variants first learn an attribute-only classifier MA from the known labels, then predict labels for the unknown nodes V U with MA (steps 1-2). The known and predicted labels are then used to compute relational feature values (step 4). The variants then differ in how they use these values and in how many steps each variant takes. The simplest approach, taken by Shi et al. (2011), is to learn the classifier MAR using all of the labels, attributes, and relational feature values (step 5). Step 6 then uses MA and MAR to predict new\nlabels (by executing ICA), and step 7 returns the set of predicted labels (n = 1). Bilgic et al. (2010) use the same approach, except that during step 5, they perform learning using only the known nodes V K . This approach is still semi-supervised because the relational feature values for V K (as computed in step 4) are influenced by the predicted labels for the unknown nodes V U . We call this latter variant Known-OnePass, since it uses only the known nodes\u2019 labels for the actual learning and does one step of relational learning, and naturally call the former variant All-OnePass.\nAlternatively, more complex variants perform a form of EM where the algorithm repeatedly estimates new labels given the current models (i.e., step 6 is the Estep) and then maximizes the probability of a new model given the current label estimates (i.e., steps 4- 5 are the M-step). Based on the learning choice in step 5, this yields the variants All-EM and KnownEM, which are approximately the approaches of Lu & Getoor (2003) and Xiang & Neville (2008), respectively. Lu & Getoor repeat based on a convergence condition, while Xiang & Neville use a fixed number of iterations (e.g., n = 10). Xiang & Neville also perform inference in step 6 (and learning) with a \u201csoft\u201d variant of ICA that uses probability estimates of the predicted labels, rather than choosing the most likely label for each node in V U (the \u201chard\u201d-labeling approach used by the other variants). We use hard labeling; future work should compare these alternatives.\nThis prior work leaves three key problems unaddressed. First, there are conflicting results for whether semi-supervised ICA improves accuracy, with reports of no improvement (Shi et al., 2011), moderate improvement (Bilgic et al., 2010), or mixed results (Xiang & Neville, 2008). Lu & Getoor (2003) report substantial gains, but only consider graphs with at least 20% of the nodes labeled. Second, there is almost no comparison between the four SSL variants (or even discus-\nsion of the choices involved). One exception is Bilgic (2010), which finds Known-OnePass to be superior to All-OnePass on two datasets, but does not investigate why. In contrast, we have generalized these algorithms in Figure 1, and study their relative performance in Section 5. Such comparisons aid understanding of the choices involved, and also establish the best baseline for future studies.\nFinally, we find that none of the variants perform consistently well. Sections 3 and 4 describe the two key steps that we propose for improving their performance."}, {"heading": "2.2. Other Approaches to Semi-supervised CC", "text": "Two additional relevant studies are Taskar et al. (2001) and Chu et al. (2006). However, these methods cannot handle cyclic graphs or have time complexity at least quadratic in the number of nodes (N), and thus do not scale to large, realistic graphs. The methods we use are only linear in N (assuming realistic link densities), even with our improvements in Sections 3 and 4.\nOthers have explored how to perform within-network CC without needing to learn an explicit model for linkbased features (the challenge for ICA discussed in Section 2.1). For instance, Tang & Liu (2009) use the links to create latent features that enable node classification without collective inference. Shi et al. (2011) propose label propagation based on derived latent links.\nA few authors have proposed \u201crelational-only\u201d methods that perform no learning but use some label propagation or random walk to classify the nodes (e.g., Macskassy & Provost 2007). These variants can increase accuracy, but only for graphs that match their assumptions and have enough known labels.\nAs noted in Section 1, we do not attempt to compare against all of these methods but do use that of Macskassy & Provost, a common CC baseline."}, {"heading": "3. Hybrid Classifiers for CC", "text": "Prior work with CC using ICA or Gibbs sampling has usually used a single node classifier (often logistic regression or Naive Bayes) to predict a label y for each node based on the node\u2019s attributes (xA) and relational features (xR). Instead, we propose to use two distinct classifiers that make separate predictions based on xA and xR. If we assume that xA and xR are conditionally independent given the class label y, we can then compute the combined prediction\np(y|x) = p(y|xA, xR) = p(xA|y)p(xR|y)p(y)\np(xA, xR)\n=\np(y|xA)p(xA) p(y) p(y|xR)p(xR) p(y) p(y)\np(xA, xR)\n= \u03b1 p(y|xA)p(y|xR)\np(y) (1)\nwhere \u03b1 is a normalizing constant independent of y.\nUsing such a \u201chybrid\u201d classifier has two main advantages. First, this method allows us to choose different types of classifiers for the attributes vs. the relational features. For instance, most prior work (e.g., Sen et al. 2008) has found that logistic regression (LR) performs best overall for CC. However, McDowell et al. (2009) found that \u201cmultiset\u201d relational features usually performed best, but are incompatible with the vectorbased representation of LR. Combining LR with attributes plus Naive Bayes (NB) with multiset relational features yields a new LR+NB classifier that resolves this representational conflict and may increase accuracy. Second, hybrid classifiers may increase accuracy, even when the two classifiers are the same type (e.g., LR+LR), as we show and explain in Section 5.\nCombining two different classifiers to make a single prediction is a special case of an ensemble. A few papers have considered how to apply some type of ensemble for CC. For instance, Preisach & SchmidtThieme (2008) use an ensemble to combine predictions based on different link types, while Eldardiry & Neville (2011) use an ensemble after each step of collective inference. These studies combine multiple classifiers using voting, stacking, or averaging, rather than via a probabilistic rule like Equation 1. Also, they do not use the unlabeled data for SSL; instead, they use fullylabeled training data or relational-only algorithms.\nThe work most closely related to ours is Lu & Getoor (2003), which uses an ensemble of two LR classifiers, combined similarly to Equation 1. They informally state that this approach outperformed a single LR classifier with SSL, but did not explain why or report comparisons. In contrast, our paper is the first to specifically demonstrate that a hybrid LR+LR classifier can often improve accuracy dramatically, and Section 5 explains why. In addition, this paper is the first to propose the LR+NB combination, and we show that it can perform particularly well for some datasets."}, {"heading": "4. Adding Label Regularization", "text": "Label regularization (Mann & McCallum, 2007) is designed to make SSL more robust by encouraging a learned LR classifier (or other exponential family model) to produce probability estimates on the unlabeled data so that the resultant class distribution\nresembles an expected distribution. More specifically, let p\u0303(y) be the expected label distribution, which can be computed from the training data. Let p\u0302\u03b8(y) be the empirical distribution, which is computed over the unlabeled part of the training data, given the current parameter settings \u03b8, as follows:\np\u0302\u03b8(y) = 1 |XU | \u2211 x\u2208XU p\u03b8(y|x).\nMann & McCallum then augment the traditional LR objective function with an additional term \u03bb\u2206(p\u0303, p\u0302\u03b8) based on the KL-divergence between p\u0303 and p\u0302\u03b8:\n\u2206(p\u0303, p\u0302\u03b8) = \u2211 yj\u2208Y p\u0303(yj) log p\u0303(yj) p\u0302\u03b8(yj) .\nThey (and we) set the tuning parameter \u03bb = 10\u00d7|V K |. The new term \u03bb\u2206(p\u0303, p\u0302\u03b8) penalizes parameter settings where there is a large difference between the expected distribution and empirical distribution. Thus, it uses the unlabeled data to help choose more plausible parameter values and avoid degenerate cases (e.g., where most nodes are assigned to the same class label).\nLabel regularization could be directly applied to CC classifiers based on non-hybrid LR (though to our knowledge this has not been done previously). However, this would not exploit the advantages we later demonstrate for hybrid classifiers. Given a hybrid LR+LR classifier, we could apply label regularization separately to each classifier. However, this would not ensure that the combined predictions given by Equation 1 resemble the desired label distribution, nor would this work for hybrid combinations not based on LR, such as LR+NB. Instead, we use the following strategy to adapt label regularization to hybrid classifiers. First, we learn p(y|xR) (the relational classifier) ignoring the attributes and label regularization. Then, we treat p(y|xR) as fixed and define \u03b2y = \u03b1p(y|xR)p(y) , which allows us to simplify Equation 1 to\np(y|x) = p(y|xA, xR) = \u03b2yp(y|xA).\nNext, we assume that the attribute-based classifier is a standard multinomial logistic regression model, so\np\u03b8(y|xA) = exp(xTA\u03b8y)\u2211\ny\u2032\u2208C exp(x T A\u03b8y\u2032)\nwhere \u03b8y is a parameter vector for class y. Combining the previous two equations and normalizing yields\np\u03b8(y|x) = 1\nZ \u03b2yexp(x\nT A\u03b8y)\nwhere, since \u2211 y p\u03b8(y|x) = 1, Z = \u2211 y\u2032 \u03b2y\u2032exp(x T A\u03b8y\u2032).\nGradient methods can now be used to optimize the loglikelihood of the training data, with the term \u2206(p\u0303, p\u0302\u03b8) in the objective providing label regularization. Thus, we now compute the gradient of this term. For each node vi, let xA be the values of vi\u2019s attributes, and xk be the k\nth such attribute value. Then the gradient with respect to \u03b8y,k (the parameter associated with the kth attribute for class y) is\n\u2202\u2206\n\u2202\u03b8y,k =\n\u2202\n\u2202\u03b8y,k \u2211 y\u2032\u2208C p\u0303(y\u2032) log p\u0303(y\u2032) p\u0302\u03b8(y\u2032)\n= \u2212 \u2211 y\u2032\u2208C p\u0303(y\u2032) p\u0302\u03b8(y\u2032) \u2202 \u2202\u03b8y,k p\u0302\u03b8(y \u2032) = \u22121 |XU | \u2211 x\u2208XU \u2211 y\u2032\u2208C p\u0303(y\u2032) p\u0302\u03b8(y\u2032) \u2202 \u2202\u03b8y,k p\u03b8(y \u2032|x)\n= \u22121 |XU | \u2211 x\u2208XU [ p\u0303(y) p\u0302\u03b8(y) xk(Z\u03b2ye xTA\u03b8y \u2212 (\u03b2yex T A\u03b8y )2) Z2\n+ \u2211\ny\u2032\u2208C\\y\np\u0303(y\u2032)\np\u0302\u03b8(y\u2032)\n0\u2212 (xk\u03b2yex T A\u03b8y )\u03b2y\u2032e xTA\u03b8y\u2032\nZ2\n]\n= \u2212 1 |XU | \u2211 x\u2208XU [ p\u0303(y) p\u0302\u03b8(y) xkp\u03b8(y|x)(1\u2212 p\u03b8(y|x))\n+ \u2211\ny\u2032\u2208C\\y\np\u0303(y\u2032)\np\u0302\u03b8(y\u2032) (\u2212xk)p\u03b8(y|x)p\u03b8(y\u2032|x)\n]\n= \u2211 x\u2208XU xkp\u03b8(y|x) |XU | [ \u2211 y\u2032\u2208C p\u0303(y\u2032) p\u0302\u03b8(y\u2032) p\u03b8(y \u2032|x)\u2212 p\u0303(y) p\u0302\u03b8(y) ] .\nThis approach can work for any hybrid classifier that includes LR in the combination, including LR+LR and LR+NB. In contrast, recent work (Mann & McCallum, 2010) extended label regularization to support conditional random fields, but did not consider the kind of hybrid classifiers that we examine here."}, {"heading": "5. Experimental Study", "text": ""}, {"heading": "5.1. Datasets and Features", "text": "Prior studies used at most two non-synthetic datasets with semi-supervised ICA; we used the union of their datasets (see Tables 1 & 2). We removed all nodes with no links, but we did not (like some others did) use only the largest connected component of the graphs.\nCora (see Sen et al. 2008) is a collection of machine learning papers. Citeseer (see Sen et al.) is a collection of research papers drawn from the Citeseer collection. For both datasets, the attributes represent the presence or absence of particular words, and citations provide links between the documents. We ignored link\ndirection, as with Bilgic et al. (2010). They also report substantially higher accuracies using principal component analysis (PCA) to reduce the dimensionality of the attributes, so to provide a stronger baseline we mimic their setup and use the 100 top attribute features after applying PCA to the entire graph.\nGene (see Jensen et al. 2004) describes the yeast genome at the protein level; links represent protein interactions. We mimic Xiang & Neville (2008) and predict protein localization using four attributes: Phenotype, Class, Essential, and Chromosome. We binarized these attributes, yielding 54 binary attributes.\nFor relational features, LR classifiers used \u201cproportion\u201d features, which compute the fraction of a node\u2019s neighbors that have label y, as done by Bilgic et al. (2010). NB classifiers instead used \u201cmultiset\u201d features which record the distribution of labels in each node\u2019s neighborhood, then use conditional independence assumptions to update the estimated probabilities based on each such label. Previous work found such features to perform best for NB (McDowell et al., 2009)."}, {"heading": "5.2. Node Classifier and Regularization", "text": "Prior work has usually found LR to be superior to NB for CC (Sen et al., 2008; Bilgic et al., 2010) and therefore we always use LR for (at least) the attribute-based classification. For the node classifier MAR, we evaluate five options: LR is a single (non-hybrid) classifier that uses logistic regression. LR+LR is a hybrid classifier that uses two LR classifiers, while LR+LR+Reg adds label regularization. LR+NB is a hybrid classifier that uses LR for the attributes and NB for the relational features, and LR+NB+Reg adds label regularization.\nFor standard regularization (separate from label regularization), we used a Gaussian prior with LR parameters, with variance \u03c32 chosen as described below. For NB, we used a Dirichlet prior on each feature with \u03b1 chosen as described below (McDowell et al., 2009).\nOf the four studies listed in Table 1, none specify how regularization parameters (if any) were chosen. For sparsely-labeled data, we observed that these choices can have a large impact on accuracy. To ensure fair comparisons, we used five-fold cross-validation on the labeled data (learning also had access to the unlabeled data), selecting the value that maximized accuracy on\nthe held-out labeled data. For LR+LR classifiers, we normalized all features, which allowed us to use a single value of \u03c32 for both classifiers. For LR+NB, we first found \u03c32 for LR, then estimated \u03b1 for NB."}, {"heading": "5.3. Learning Algorithms", "text": "We evaluate four variants of semi-supervised ICA (see Section 2): All-EM, All-OnePass, Known-EM, and Known-OnePass. For each, step 6 of the algorithm executes ICA with 10 iterations, and the EMvariants use n = 10 iterations of the main loop.\nWe compare against three baselines. The first, NoSSL, is like Known-OnePass in applying ICA one time using both attributes and relational features, but No-SSL learns the node classifier without using any unlabeled data (i.e., with no SSL). The second, AttrOnly, predicts the unknown labels only once, using an attribute-only classifier that ignores unlabeled data while learning. The third, Relat-Only, is a standard relational-only baseline (the wvRN+RL classifier of Macskassy & Provost 2007) that repeatedly estimates labels based on the labels of all linked neighbors. These three baselines never use label regularization."}, {"heading": "5.4. Evaluation Procedure", "text": "We report accuracy averaged over 15 trials. For each trial, we randomly selected some fraction of the nodes (the \u201clabel density\u201d) to be \u201cknown\u201d nodes V K . The remaining nodes V U have unknown labels and form the test set part of graph G. We focus on the sparselylabeled case where the density is less than 10%.\nTo assess significance, we use paired t-tests with a 5% significance level. However, the test sets are not disjoint across the 15 trials, and thus a traditional paired t-test may yield false conclusions. To compensate for this effect, we use the recently described methodology of Wang et al. (2011), which was shown to reduce false positives to the expected level. This makes our results more conservative compared to uncorrected t-tests."}, {"heading": "5.5. Results", "text": "We first consider the best learning algorithm (AllEM, as shown later) with various classifiers, then compare different learning algorithms with the best node classifier (LR+NB+Reg). Finally, we use our findings to explain the results of previous studies.\nResult 1: Using a hybrid classifier and label regularization each increases accuracy, and combining the two techniques yields the best overall results. Figure 2 shows the average accu-\nracy, for All-EM, as label density is varied from 1% to 50%. Below, we discuss results only for the sparse case (density less than 10%). Each line represents a different node classifier, and symbols indicate (some of the) significant differences (see caption).\nThe hybrid LR+LR almost always outperforms LR, with especially large gains for Cora. Likewise, changing the classifier to LR+NB almost always yields some additional gain, even when the label density is as high as 9%. Furthermore, adding label regularization (with LR+LR+Reg or LR+NB+Reg) always outperforms LR+LR, and always matches or exceeds the accuracy of LR+NB. The gains from label regularization are sometimes substantial, especially when the label density is low, but remain large even at a density of 9% for Gene. For instance, for Citeseer when the density is 1%, label regularization improves LR+LR by 13.1% and LR+NB by 9.6% (both significantly).1\nOverall, LR+NB+Reg performs best and its accuracy is never lower than the alternatives. It significantly outperforms LR, and often substantially outperforms the other classifiers, especially vs. those without label regularization and/or when the label density is lower.\nWithout label regularization, some learning runs converged on degenerate distributions, with one class disproportionately represented. Adding label regularization substantially reduced this problem, as intended. We also found that, compared to LR, LR+LR learned much more reasonable weights \u03b8 for the relational features (e.g., that match our knowledge of the actual link-based correlations). When there are few labeled nodes, the LR optimization routine may find a model where the attributes alone explain the data well, with small weights for the relational features. Placing these features in a separate model (as with LR+LR) ensures that they will be more heavily used, increasing accuracy since both attributes and relational features are informative for these datasets.\nResult 2: All-EM outperforms the baselines and usually the other SSL variants. Table 3 compares the four SSL algorithms and the three baseline algorithms, using LR+NB+Reg. All-EM is the most consistent, and is always one of the best algorithms (with one exception for Gene). In many cases, AllEM\u2019s gains are significant vs. the other methods.\nAll-OnePass and Known-OnePass outperform\n1Other results (not shown) confirm that LR+LR+Reg (or LR+NB+Reg) also outperformed a (non-hybrid) LR with label regularization added. For example, for density 1%-9%, LR+NB+Reg had (usually significant) gains of 1.1- 7.7% (Cora), 0.3-2.6% (Citeseer), and 3.4-12.3% (Gene).\nNo-SSL by varying degrees, but Known-EM is much worse, never matching the best accuracy and often under-performing No-SSL. For this data, the repeated EM iterations seem to be a poor choice when the actual learning is performed over only the small number of \u201cknown\u201d nodes; Known-OnePass performs better.\nResult 3: Without hybrid classification and label regularization, accuracy decreases substantially. Table 4 compares the same SSL algorithms shown in Table 3, but now using the simpler LR classifier. This setting is closest to that of Bilgic et al. (2010) and Shi et al. (2011). To save space, we show only Cora; trends are similar with Citeseer and Gene. Compared to Table 3\u2019s results with LR+NB+Reg, performance with LR usually drops substantially (as already partly seen in Figure 2). The All variants suffer the biggest drops, especially All-EM, because without a good hybrid classifier and label regularization, learning may be based on estimated labels with a degenerate distribution. The Known variants are more insulated from this effect because poor label estimates only affect the relational feature values (not the labels directly seen by the learning algorithm).\nDiscussion: Table 4 showed that using LR changes the relative performance of the SSL algorithms compared to when using LR+NB+Reg. These results help us explain the previously discussed conflicting findings related to Table 1. First, with a simple LR classifier, Known-OnePass outperforms All-OnePass (and provides reasonable gains over No-SSL) \u2013 consistent with Bilgic et al. (2010). Second, All-OnePass behaves similarly to No-SSL \u2013 the same poor behavior that led Shi et al. (2011) to reject semi-supervised ICA. Note that both of these conclusions change when a better classifier like LR+NB+Reg is used (see Table 3). Third, Known-EM behaves reasonably well but not the best; this may explain why Xiang & Neville found gains for this algorithm only in some cases. Finally,\nLu & Getoor reported strong results with All-EM and a classifier like LR+LR, tested for label densities of at least 20%. For lower densities, our results (see Figure 2) show that this combination is not as strong.\nOverall, our results show that, while the simplest forms of semi-supervised ICA do not perform well, using a hybrid classifier with label regularization enables the most sophisticated learning algorithm (All-EM) to work well, leading to significant accuracy gains. To quantify the overall impact of our changes, the bottom row of Table 3 shows results with a natural benchmark: the basic LR algorithm with Known-OnePass. This SSL algorithm worked best with LR and mimics the setup of Bilgic et al. (2010), who reported spending considerable effort to improve their performance. Comparing vs. the top row of Table 3 (LR+NB+Reg with All-EM), we find consistent and mostly significant gains, ranging from 4.3-26.8% for Cora, 0.9-12.7% for Citeseer, and 2.7-3.4% (with one loss of -0.8%) for Gene. Thus, we find consistent gains for our new methods compared to this benchmark method."}, {"heading": "6. Conclusion", "text": "We have generalized the algorithms of multiple previous studies of semi-supervised ICA, explained their performance trends, and demonstrated that a hybrid classifier with label regularization can significantly increase accuracy compared to alternative approaches. For our data, the LR+NB combination performed best, though this will not hold for every dataset. However, our hybrid approach enables any combination of probabilistic classifiers to be selected based on the data characteristics. Moreover, our extension of label regularization can also be used to increase accuracy, as long as one of the classifiers is in the exponential family (like LR). Such hybrid classifiers with label regularization may be useful in many other tasks, including across-network CC or non-relational classification.\nOur results need to be confirmed with additional datasets, and we will explore alternatives to ICA such as Gibbs sampling and soft-labeling methods, and the use of other hybrid combination rules such as stacking. Moreover, the best algorithms should be compared against other methods discussed in Section 2.2. However, our results already show that there is more potential for semi-supervised learning based on ICA than was suggested by earlier studies, and we have presented two techniques that improve its performance."}, {"heading": "Acknowledgments", "text": "Thanks to Alexandra Olteanu, Li Pu, Majid Yazdani, and the anonymous referees for comments that helped to improve this work. Portions of this analysis used Proximity, an open-source software environment from the Univ. of Massachusetts, Amherst. This work was supported in part by NSF award number 1116439."}], "references": [{"title": "Cost-Sensitive Information Acquisition in Structured Domains", "author": ["M. Bilgic"], "venue": "PhD thesis, Department of Computer Science, University of Maryland at College Park,", "citeRegEx": "Bilgic,? \\Q2010\\E", "shortCiteRegEx": "Bilgic", "year": 2010}, {"title": "Active learning for networked data", "author": ["M. Bilgic", "L. Mihalkova", "L. Getoor"], "venue": "In Proc. of ICML,", "citeRegEx": "Bilgic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bilgic et al\\.", "year": 2010}, {"title": "Relational learning with gaussian processes", "author": ["W. Chu", "V. Sindhwani", "Z. Ghahramani", "S.S. Keerthi"], "venue": "In NIPS, pp", "citeRegEx": "Chu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2006}, {"title": "Across-model collective ensemble classification", "author": ["H. Eldardiry", "J. Neville"], "venue": "In Proc. of AAAI,", "citeRegEx": "Eldardiry and Neville,? \\Q2011\\E", "shortCiteRegEx": "Eldardiry and Neville", "year": 2011}, {"title": "Why collective inference improves relational classification", "author": ["D. Jensen", "J. Neville", "B. Gallagher"], "venue": "In Proc. of KDD, pp", "citeRegEx": "Jensen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jensen et al\\.", "year": 2004}, {"title": "Link-based classification using labeled and unlabeled data", "author": ["Q. Lu", "L. Getoor"], "venue": "In ICML Workshop on the Continuum from Labeled to Unlabeled data,", "citeRegEx": "Lu and Getoor,? \\Q2003\\E", "shortCiteRegEx": "Lu and Getoor", "year": 2003}, {"title": "Classification in networked data: A toolkit and a univariate case study", "author": ["S. Macskassy", "F. Provost"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Macskassy and Provost,? \\Q2007\\E", "shortCiteRegEx": "Macskassy and Provost", "year": 2007}, {"title": "Generalized expectation criteria for semi-supervised learning with weakly labeled data", "author": ["G. Mann", "A. McCallum"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mann and McCallum,? \\Q2010\\E", "shortCiteRegEx": "Mann and McCallum", "year": 2010}, {"title": "Simple, robust, scalable semi-supervised learning via expectation regularization", "author": ["G.S. Mann", "A. McCallum"], "venue": "In Proc. of ICML, pp", "citeRegEx": "Mann and McCallum,? \\Q2007\\E", "shortCiteRegEx": "Mann and McCallum", "year": 2007}, {"title": "Cautious collective classification", "author": ["L.K. McDowell", "K.M. Gupta", "D.W. Aha"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "McDowell et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McDowell et al\\.", "year": 2009}, {"title": "Ensembles of relational classifiers", "author": ["C. Preisach", "L. Schmidt-Thieme"], "venue": "Knowledge and Information Systems,", "citeRegEx": "Preisach and Schmidt.Thieme,? \\Q2008\\E", "shortCiteRegEx": "Preisach and Schmidt.Thieme", "year": 2008}, {"title": "Collective classification in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi-Rad"], "venue": "AI Magazine,", "citeRegEx": "Sen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sen et al\\.", "year": 2008}, {"title": "Collective prediction with latent graphs", "author": ["X. Shi", "Y. Li", "P.S. Yu"], "venue": "In Proc. of CIKM, pp", "citeRegEx": "Shi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2011}, {"title": "Relational learning via latent social dimensions", "author": ["L. Tang", "H. Liu"], "venue": "In Proc. of KDD, pp", "citeRegEx": "Tang and Liu,? \\Q2009\\E", "shortCiteRegEx": "Tang and Liu", "year": 2009}, {"title": "Probabilistic classification and clustering in relational data", "author": ["B. Taskar", "E. Segal", "D. Koller"], "venue": "In Proc. of IJCAI, pp", "citeRegEx": "Taskar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2001}, {"title": "Correcting bias in statistical tests for network classifier evaluation", "author": ["T. Wang", "J. Neville", "B. Gallagher", "T. Eliassi-Rad"], "venue": "In Proc. of ECML,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Pseudolikelihood EM for withinnetwork relational learning", "author": ["R. Xiang", "J. Neville"], "venue": "In Proc. of ICDM,", "citeRegEx": "Xiang and Neville,? \\Q2008\\E", "shortCiteRegEx": "Xiang and Neville", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "Collective classification (CC) often substantially increases classification accuracy when the class labels of inter-related objects are correlated (Jensen et al., 2004; Sen et al., 2008).", "startOffset": 147, "endOffset": 186}, {"referenceID": 11, "context": "Collective classification (CC) often substantially increases classification accuracy when the class labels of inter-related objects are correlated (Jensen et al., 2004; Sen et al., 2008).", "startOffset": 147, "endOffset": 186}, {"referenceID": 12, "context": "However, acquiring such labels can be very difficult, and learning a classifier with only a few such labels can lead to very poor performance (Shi et al., 2011).", "startOffset": 142, "endOffset": 160}, {"referenceID": 0, "context": "This includes Bilgic et al. (2010), who found moderate gains from SSL, whereas Shi et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 0, "context": "This includes Bilgic et al. (2010), who found moderate gains from SSL, whereas Shi et al. (2011) reported otherwise.", "startOffset": 14, "endOffset": 97}, {"referenceID": 11, "context": "We focus on traditional CC algorithms that learn a relational model of the data and then apply a collective inference algorithm such as the Iterative Classification Algorithm (ICA) or Gibbs sampling (Sen et al., 2008).", "startOffset": 199, "endOffset": 217}, {"referenceID": 12, "context": "2011), then these approaches may need to be entirely replaced with alternatives such as latent feature models (Tang & Liu, 2009) or label propagation (Shi et al., 2011).", "startOffset": 150, "endOffset": 168}, {"referenceID": 10, "context": "per Figure 1 Data used with SSL-ICA Shi et al. (2011) Log.", "startOffset": 36, "endOffset": 54}, {"referenceID": 0, "context": "regression (LR) Hard All-OnePass Citeseer Bilgic et al. (2010) Log.", "startOffset": 42, "endOffset": 63}, {"referenceID": 0, "context": "regression (LR) Hard All-OnePass Citeseer Bilgic et al. (2010) Log. regression (LR) Hard Known-OnePass Cora, Citeseer Lu & Getoor (2003) Log.", "startOffset": 42, "endOffset": 137}, {"referenceID": 0, "context": "regression (LR) Hard All-OnePass Citeseer Bilgic et al. (2010) Log. regression (LR) Hard Known-OnePass Cora, Citeseer Lu & Getoor (2003) Log. regression (LR) Hard All-EM Cora, Citeseer Xiang & Neville (2008) RPT Soft Known-EM Gene, synthetic", "startOffset": 42, "endOffset": 208}, {"referenceID": 4, "context": "In contrast, methods for collective classification (Jensen et al., 2004) explicitly use the link structure by constructing additional relational features xR based on the labels of neighboring pages.", "startOffset": 51, "endOffset": 72}, {"referenceID": 11, "context": "This can be done using algorithms such as belief propagation, Gibbs sampling, relaxation labeling, or ICA (Sen et al., 2008).", "startOffset": 106, "endOffset": 124}, {"referenceID": 12, "context": "The simplest approach, taken by Shi et al. (2011), is to learn the classifier MAR using all of the labels, attributes, and relational feature values (step 5).", "startOffset": 32, "endOffset": 50}, {"referenceID": 0, "context": "Bilgic et al. (2010) use the same approach, except that during step 5, they perform learning using only the known nodes V K .", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "First, there are conflicting results for whether semi-supervised ICA improves accuracy, with reports of no improvement (Shi et al., 2011), moderate improvement (Bilgic et al.", "startOffset": 119, "endOffset": 137}, {"referenceID": 1, "context": ", 2011), moderate improvement (Bilgic et al., 2010), or mixed results (Xiang & Neville, 2008).", "startOffset": 30, "endOffset": 51}, {"referenceID": 0, "context": ", 2011), moderate improvement (Bilgic et al., 2010), or mixed results (Xiang & Neville, 2008). Lu & Getoor (2003) report substantial gains, but only consider graphs with at least 20% of the nodes labeled.", "startOffset": 31, "endOffset": 114}, {"referenceID": 0, "context": ", 2011), moderate improvement (Bilgic et al., 2010), or mixed results (Xiang & Neville, 2008). Lu & Getoor (2003) report substantial gains, but only consider graphs with at least 20% of the nodes labeled. Second, there is almost no comparison between the four SSL variants (or even discussion of the choices involved). One exception is Bilgic (2010), which finds Known-OnePass to be superior to All-OnePass on two datasets, but does not investigate why.", "startOffset": 31, "endOffset": 350}, {"referenceID": 13, "context": "Two additional relevant studies are Taskar et al. (2001) and Chu et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 2, "context": "(2001) and Chu et al. (2006). However, these methods cannot handle cyclic graphs or have time complexity at least quadratic in the number of nodes (N), and thus do not scale to large, realistic graphs.", "startOffset": 11, "endOffset": 29}, {"referenceID": 12, "context": "Shi et al. (2011) propose label propagation based on derived latent links.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "However, McDowell et al. (2009) found that \u201cmultiset\u201d relational features usually performed best, but are incompatible with the vectorbased representation of LR.", "startOffset": 9, "endOffset": 32}, {"referenceID": 0, "context": "direction, as with Bilgic et al. (2010). They also report substantially higher accuracies using principal component analysis (PCA) to reduce the dimensionality of the attributes, so to provide a stronger baseline we mimic their setup and use the 100 top attribute features after applying PCA to the entire graph.", "startOffset": 19, "endOffset": 40}, {"referenceID": 4, "context": "Gene (see Jensen et al. 2004) describes the yeast genome at the protein level; links represent protein interactions. We mimic Xiang & Neville (2008) and predict protein localization using four attributes: Phenotype, Class, Essential, and Chromosome.", "startOffset": 10, "endOffset": 149}, {"referenceID": 9, "context": "Previous work found such features to perform best for NB (McDowell et al., 2009).", "startOffset": 57, "endOffset": 80}, {"referenceID": 0, "context": "For relational features, LR classifiers used \u201cproportion\u201d features, which compute the fraction of a node\u2019s neighbors that have label y, as done by Bilgic et al. (2010). NB classifiers instead used \u201cmultiset\u201d features which record the distribution of labels in each node\u2019s neighborhood, then use conditional independence assumptions to update the estimated probabilities based on each such label.", "startOffset": 147, "endOffset": 168}, {"referenceID": 11, "context": "Prior work has usually found LR to be superior to NB for CC (Sen et al., 2008; Bilgic et al., 2010) and therefore we always use LR for (at least) the attribute-based classification.", "startOffset": 60, "endOffset": 99}, {"referenceID": 1, "context": "Prior work has usually found LR to be superior to NB for CC (Sen et al., 2008; Bilgic et al., 2010) and therefore we always use LR for (at least) the attribute-based classification.", "startOffset": 60, "endOffset": 99}, {"referenceID": 9, "context": "For NB, we used a Dirichlet prior on each feature with \u03b1 chosen as described below (McDowell et al., 2009).", "startOffset": 83, "endOffset": 106}, {"referenceID": 15, "context": "To compensate for this effect, we use the recently described methodology of Wang et al. (2011), which was shown to reduce false positives to the expected level.", "startOffset": 76, "endOffset": 95}, {"referenceID": 0, "context": "This setting is closest to that of Bilgic et al. (2010) and Shi et al.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": "This setting is closest to that of Bilgic et al. (2010) and Shi et al. (2011). To save space, we show only Cora; trends are similar with Citeseer and Gene.", "startOffset": 35, "endOffset": 78}, {"referenceID": 0, "context": "First, with a simple LR classifier, Known-OnePass outperforms All-OnePass (and provides reasonable gains over No-SSL) \u2013 consistent with Bilgic et al. (2010). Second, All-OnePass behaves similarly to No-SSL \u2013 the same poor behavior that led Shi et al.", "startOffset": 136, "endOffset": 157}, {"referenceID": 0, "context": "First, with a simple LR classifier, Known-OnePass outperforms All-OnePass (and provides reasonable gains over No-SSL) \u2013 consistent with Bilgic et al. (2010). Second, All-OnePass behaves similarly to No-SSL \u2013 the same poor behavior that led Shi et al. (2011) to reject semi-supervised ICA.", "startOffset": 136, "endOffset": 258}, {"referenceID": 0, "context": "This SSL algorithm worked best with LR and mimics the setup of Bilgic et al. (2010), who reported spending considerable effort to improve their performance.", "startOffset": 63, "endOffset": 84}], "year": 2012, "abstractText": "Many classification problems involve data instances that are interlinked with each other, such as webpages connected by hyperlinks. Techniques for collective classification (CC) often increase accuracy for such data graphs, but usually require a fully-labeled training graph. In contrast, we examine how to improve the semi-supervised learning of CC models when given only a sparsely-labeled graph, a common situation. We first describe how to use novel combinations of classifiers to exploit the different characteristics of the relational features vs. the non-relational features. We also extend the ideas of label regularization to such hybrid classifiers, enabling them to leverage the unlabeled data to bias the learning process. We find that these techniques, which are efficient and easy to implement, significantly increase accuracy on three real datasets. In addition, our results explain conflicting findings from prior related studies.", "creator": "LaTeX with hyperref package"}}}