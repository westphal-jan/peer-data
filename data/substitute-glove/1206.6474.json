{"id": "1206.6474", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Estimation of Simultaneously Sparse and Low Rank Matrices", "abstract": "The page explains a penalized formula_22 estimation proceedings concerted with sharing over all sparse when term - rank every still same probably. Such structures inevitably recently place changes an social devices there protein symbiotic where perception graphs have vwf multiplicative which are block - narrow in this appropriate basis. We step another convex tone penalty one focus $ \\ ell_1 $ - formula_6 while determining norm rather. We authorisation an ebay extra-sensory which indicates very the previous natural infect according to it nature of once potential formula_10. We bound fallacy precise in the link definitive problem. We also fully proximal descent strategies did solve entire optimization difference efficiently such findings creative. unapproved for real report sets.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (632kb)", "http://arxiv.org/abs/1206.6474v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.DS cs.LG cs.NA stat.ML", "authors": ["pierre-andr\u00e9 savalle", "emile richard", "nicolas vayatis"], "accepted": true, "id": "1206.6474"}, "pdf": {"name": "1206.6474.pdf", "metadata": {"source": "META", "title": "Estimation of Simultaneously Sparse and Low Rank Matrices", "authors": ["Emile Richard", "Pierre-Andr\u00e9 Savalle"], "emails": ["emile.richard@cmla.ens-cachan.fr", "pierre-andre.savalle@ecp.fr", "nicolas.vayatis@cmla.ens-cachan.fr"], "sections": [{"heading": "1. Introduction", "text": "Matrix estimation is at the center of many modern applications and theoretical advances in the field of high dimensional statistics. The key element which differentiates this problem from standard high dimensional vector estimation lies in the structural assumptions which are formulated in this context. Indeed, the notion of sparsity assumption has been transposed into the concept of low-rank matrices and opened the way to numerous achievements (see for instance (Srebro, 2004; Cai et al., 2008)). In this paper, we argue that being low-rank is not only an equivalent of sparsity for matrices but that being low-rank and sparse\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ncan actually be seen as two orthogonal concepts. The underlying structure we have in mind is that of a block diagonal matrix. This situation occurs for instance in covariance matrix estimation in the case of groups of highly correlated variables or when denoising/clustering social graphs.\nEfficient procedures developed in the context of sparse model estimation mostly rely on the use of `1-norm regularization (Tibshirani, 1996). Natural extensions include cases where subsets of related variables are known to be active simultaneously (Yuan & Lin, 2006). These methods are readily adapted to matrix valued data and have been applied to covariance estimation (El Karoui, 2009; Bien & Tibshirani, 2010) and graphical model structure learning (Banerjee et al., 2007; Friedman et al., 2008). In the low-rank matrix completion problem, the standard relaxation approach leads to the use of the trace norm as the main regularizer within the optimization procedures (Srebro et al., 2005; Koltchinskii et al., 2011) and their resolution can either be obtained in closed form (loss measured in terms of Frobenius norm) or through iterative proximal solutions (Combettes & Pesquet, 2011; Beck & Teboulle, 2009) (for general classes of losses). However, solutions of low-rank estimation problems are in general not sparse at all, while denoising and variable selection on matrix-valued data are blind to the global structure of the matrix and process each variable independently.\nIn this paper, we study the benefits of using the sum of `1 and trace-norms as regularizer. This sum of penalties on the same object allows to benefit from the virtues of both of them, in the same way as the elastic-net (Zou & Hastie, 2005) combines the sparsityinducing property of the `1 norm with the smoothness of the quadratic regularizer. Trace norm and `1 penalties have already been combined in a different context.\nIn Robust PCA (Candes et al., 2009) and related literature, the signal S is assumed to have an additive decomposition S = X + Y where X is sparse and Y low-rank. Note that S is not in general sparse nor lowrank and that this decomposition is subject to identifiability issues, as analyzed, e.g., in (Chandrasekaran et al., 2011). The decomposition is recovered by using `1-norm regularization over X and trace norm regularization over Y . This technique has been successfully applied to background substraction in image sequences, to graph clustering (Jalali et al., 2011) and covariance estimation (Luo, 2011).\nHere, we consider the different situation where the matrix S is sparse and low-rank at the same time. We demonstrate the applicability of our mixed penalty on different problems. We develop proximal methods to solve these convex optimization problems and we provide numerical evidence as well as theoretical arguments which illustrate the trade-off which can be achieved with the suggested method.\nThe remainder of the paper is organized as follows. In Section 2, we present the setup and motivations. Sections 3 and 4 are devoted to theoretical results on the interplay between sparse and low-rank effects. Section 5 presents algorithms used for resolution of the optimization problem and Section 6 is devoted to numerical experiments. The last Section explores related topics."}, {"heading": "2. Setup and motivations", "text": ""}, {"heading": "2.1. Problem formulation and notations", "text": "We first set some notations. For a matrix S = (Si,j)i,j , we set the following matrix norms: \u2016S\u20161 = \u2211 i,j |Si,j |\nand \u2016S\u2016\u2217 = \u2211rank(S) i=1 \u03c3i, where \u03c3i are the singular values of S and rank(S) is the rank of S. We consider the following setup. Let A \u2208 Rn\u00d7n be a fixed matrix and ` a loss function over matrices. We introduce the following optimization problem:\narg min S\u2208S\n{`(S,A) + \u03b3\u2016S\u20161 + \u03c4\u2016S\u2016\u2217}\nfor some convex admissible set S \u2282 Rn\u00d7n and nonnegative regularization parameters \u03b3, \u03c4 .\nIn the sequel, the projection of a matrix Z onto S is denoted by PS(Z). The matrix (M)+ is the componentwise positive part of the matrix M, and sgn(M) is the sign matrix associated to M with the convention sgn(0) = 0. The component wise product of matrices is denoted by \u25e6. The class S+n of matrices is the convex cone of positive semidefinite matrices in Rn\u00d7n. The sparsity index of M is ||M ||0 =\n|{Mi,j 6= 0}| and the Frobenius norm of a matrix M is defined by \u2016M\u20162F = \u2211 i,jM 2 i,j . In Section 3, we shall also use \u2016M\u2016op = supx : \u2016x\u20162=1 \u2016Mx\u20162 and \u2016M\u2016\u221e = max |Mi,j |."}, {"heading": "2.2. Main examples", "text": "The underlying assumption in this work is that the unknown matrix to be recovered has a block-diagonal structure. We now describe the main modeling choices through the following motivating examples:\n\u2022 Covariance matrix estimation - the matrix A represents a noisy estimate of the true covariance matrix obtained for instance with very few observations; the search space is S = S+n the class of positive semidefinite matrices; for the loss, we consider the squared norm `(S,A) = \u2016S \u2212A\u20162F .\n\u2022 Graph denoising - the matrix A is the adjacency matrix of a noisy graph with both irrelevant and missing edges; the search space is all of S = Rn\u00d7n and the coefficients of a candidate matrix estimate S are interpreted as signed scores for adding/removing edges from the original matrix A; again, we use `(S,A) = \u2016S \u2212A\u20162F .\n\u2022 Link prediction - the matrix A is the adjacency matrix of a partially observed graph: entries are 0 for both not-existing and undiscovered links. The search space is unrestricted as before and the matrix S contains the scores for link prediction; the ideal loss function is the empirical average of the zero-one loss for each coefficient\n`E(S,A) = 1 |E| \u2211\n(i,j)\u2208E\n1{(Ai,j \u2212 1/2) \u00b7 Si,j \u2264 0} ,\nwhere E is the set of edges in A. However, as in classification theory, practical algorithms should use a convex surrogate (e.g., the hinge loss)."}, {"heading": "3. Oracle inequality", "text": "The next result shows how matrix recovery is governed by the trade-off between the rank and the sparsity index of the unknown target matrix, or by their convex surrogates: the trace norm and the `1-norm. Proposition 1. Let S0 \u2208 Rn\u00d7n and A = S0 + with \u2208 Rn\u00d7n having i.i.d. entries with zero mean. Assume for some \u03b1 \u2208 [0; 1] that \u03c4 \u2265 2\u03b1\u2016 \u2016op and \u03b3 \u2265 2(1\u2212 \u03b1)\u2016 \u2016\u221e. Let\nL(S) = \u2016S \u2212A\u20162F + \u03c4\u2016S\u2016\u2217 + \u03b3\u2016S\u20161 ,\nand S\u0302 = arg minS\u2208S L(S) . Then\n\u2016S\u0302 \u2212 S0\u20162F \u2264 inf S\u2208S\n{ \u2016S \u2212 S0\u20162F + 2\u03c4\u2016S\u2016\u2217 + 2\u03b3\u2016S\u20161 } and\n\u2016S\u0302 \u2212 S0\u20162F \u2264min {\n2\u03c4\u2016S0\u2016\u2217 + 2\u03b3\u2016S0\u20161,( \u03c4 \u221a\nrank(S0)\n\u221a 2 + 1 2 + \u03b3 \u221a \u2016S0\u20160 )2} .\nThe techniques used in the proof (see the Appendix) are very similar to those introduced in (Koltchinskii et al., 2011). Note that the upper bound interpolates between the results known for trace-norm penalization and Lasso. In fact, for \u03b1 = 0, \u03c4 can be set to zero, and we get a sharp bound for Lasso, while the tracenorm regression bounds of (Koltchinskii et al., 2011) are obtained for \u03b1 = 1."}, {"heading": "4. Generalization error in link prediction", "text": "We dwell for a moment on the task of link prediction in order to illustrate how rank and sparsity constraints can help in this setting. Given a subset E of observed edges from a graph adjacency matrix A \u2208 {0, 1}n\u00d7n, we set out to predict unobserved links by finding a sparse rank r predictor S \u2208 Rn\u00d7n with small zero-one loss\n`(S,A) = 1\nn2 \u2211 (i,j)\u2208{1,...,n}2 1{(Ai,j \u2212 1/2) \u00b7 Si,j \u2264 0}\nby minimizing the empirical zero-one loss\n`E(S,A) = 1 |E| \u2211\n(i,j)\u2208E\n1{(Ai,j \u2212 1/2) \u00b7 Si,j \u2264 0} .\nThe objective of a generalization bound is to relate `(S,A) with `E(S,A). In the case of the sole rank constraint, (Srebro, 2004) remarked that all low-rank matrices with the same sign pattern are equivalent in terms of loss and applied a standard argument for generalization in classes of finite cardinality. In the work of Srebro, a beautiful argument is used to upper bound the number of distinct sign configurations for predictors of rank r\nslr(n, r) = |{sgn(S) |S \u2208 Rn\u00d7n, rank(S) \u2264 r}|\nleading to the following generalization performance: for \u03b4 > 0, A \u2208 {0, 1}n\u00d7n and with probability 1\u2212\u03b4 over choosing a subset E of entries in {1, . . . , n}2 uniformly\namong all subsets of |E| entries, we have for any matrix S of rank at most r and \u2206(n, r) = ( 8en r )2nr `(S,A) < `E(S,A) + \u221a log \u2206(n, r)\u2212 log \u03b4\n2|E| . (1)\nWe consider the class of sparse rank r predictors\nM(n, r, s) = {UV T |U, V \u2208 Rn\u00d7r, ||U ||0 + ||V ||0 \u2264 s}\nand let ssplr(n, r, s) be the number of sign configurations for the set M(n, r, s). By upper bounding the number of sign configurations for a fixed sparsity pattern in (U, V ) using an argument similar to (Srebro, 2004), a union bound gives\nssplr(n, r, s) \u2264 \u0393(n, r, s) = ( 16en2\ns\n)s( 2nr\ns\n) .\nUsing the same notations as previously, we deduce from this result the following generalization bound: with probability 1\u2212 \u03b4 and for all S \u2208M(n, r, s),\n`(S,A) < `E(S,A) +\n\u221a log \u0393(n, r, s)\u2212 log \u03b4\n2|E| . (2)\nIn general, bound (2) is tighter than (1) for sufficiently large values of n as shown in the next proposition. The two bounds coincide when s = 2nr, that is, when (U, V ) is dense and there is no sparsity constraint.\nProposition 2. For rn = n\u03b2 with \u03b2 \u2208]0, 1] and sn = n\u03b1 with \u03b1 \u2264 2\u03b2,\n\u2206(n, rn)\n\u0393(n, rn, sn) = \u2126\n([ 8en(\u03b2n\u2212 \u03b1)\n(\u03b2n)2\n]2n2\u03b2) ,\nwhich diverges when n goes to infinity.\nProof. The result follows from the application of Stirling\u2019s formula.\nBy considering a predictor class of lower complexity than low-rank matrices, we can thus achieve better generalization performances."}, {"heading": "5. Algorithms", "text": "We now present how to solve the optimization problem with mixed penalties presented in Section 2. We consider a loss function `(S,A) convex and differentiable in S, and assume that its gradient is Lipschitz with constant L and can be efficiently computed. This is, in particular, the case for the squared Frobenius norm previously mentioned and for other classical choices such as the hinge loss."}, {"heading": "5.1. Proximal operators", "text": "We encode the presence of a constraint set S using the indicator function 1S(S) that is zero when S \u2208 S and +\u221e otherwise, leading to\nS\u0302 = arg min S\u2208Rn\u00d7n\n{`(S,A) + \u03b3||S||\u2217 + \u03c4 ||S||1 + 1S(S)} .\nThis formulation involves a sum of a convex differentiable loss and of convex non differentiable regularizers which renders the problem non trivial. A string of algorithms have been developed for the case where the optimal solution is easy to compute when each regularizer is considered in isolation. Formally, this corresponds to cases where the proximal operator defined for a convex regularizer R : Rn\u00d7n \u2192 R at a point Z by\nproxR(Z) = arg min S\u2208Rn\u00d7n\n1 2 ||S \u2212 Z||2F +R(S) .\nis easy to compute for each regularizer taken separately. See (Combettes & Pesquet, 2011) for a broad overview of proximal methods.\nThe proximal operator of the indicator function is simply the projection onto S, which justifies the alternate denomination of generalized projection operator for proxR. The proximal operator for the trace norm is given by the shrinkage operation as follows (Beck & Teboulle, 2009). If Z = U diag(\u03c31, \u00b7 \u00b7 \u00b7 , \u03c3n)V T is the singular value decomposition of Z,\nSHR\u03c4 (Z) := prox\u03c4 ||.||\u2217(Z) = U diag((\u03c3i \u2212 \u03c4)+)iV T .\nSimilarly, the proximal operator for the `1-norm is the soft thresholding operator\nST\u03b3(Z) := prox\u03b3||.||1 = sgn(Z) \u25e6 (|Z| \u2212 \u03b3)+ ."}, {"heading": "5.2. Generalized Forward-Backward splitting", "text": "The family of Forward-Backward splitting methods are iterative algorithms applicable when there is only one non differentiable regularizer. These methods alternate a gradient step and and a proximal step, leading to updates of the form\nSk+1 = prox\u03b8R(Sk \u2212 \u03b8 gradS `(S,A)) .\nIn particular, this corresponds to projected gradient descent when R is the indicator function of a convex set. On the other hand, Douglas-Rachford splitting tackles the case of q \u2265 2 terms but does not benefits from differentiability. A generalization of these two setups has been recently proposed in (Raguet\net al., 2011) under the name of Generalized ForwardBackward, which we specialize to our problem in Algorithm 1. The proximal operators are applied in parallel, and the resulting (Z1, Z2, Z3) is projected onto the constraint that Z1 = Z2 = Z3 which is given by the mean. The auxiliary variable Z3 can be simply dropped when S = Rn\u00d7n. The algorithm converges under very mild conditions when the step size \u03b8 is smaller than 2L .\nAlgorithm 1 Generalized Forward-Backward\nInitialize S,Z1, Z2, Z3 = A, q = 3 repeat\nCompute G = \u2207S`(S,A). Compute Z1 = proxq\u03b8\u03c4 ||.||\u2217(2S \u2212 Z1 \u2212 \u03b8G) Compute Z2 = proxq\u03b8\u03b3||.||1(2S \u2212 Z2 \u2212 \u03b8G) Compute Z3 = PS(2S \u2212 Z3 \u2212 \u03b8G) Set S = 1q \u2211q k=1 Zk\nuntil convergence return S"}, {"heading": "5.3. Incremental Proximal Descent", "text": "Although Algorithm 1 performs well in practice, the O(n2) memory footprint with a large leading constant due to the parallel updates can be a drawback in some cases. As a consequence, we mention a matching serial algorithm (Algorithm 2) introduced in (Bertsekas, 2011) that has a flavor similar to multi-pass stochastic gradient descent. We present here a version where updates are performed according to a cyclic order, although random selection of the order of the updates is also possible.\nAlgorithm 2 Incremental Proximal Descent\nInitialize S = A repeat\nSet S = S \u2212 \u03b8\u2207S`(S,A) Set S = prox\u03b8\u03c4 ||.||\u2217(S) Set S = prox\u03b8\u03b3||.||1(S) Set S = PS(S)\nuntil convergence return S"}, {"heading": "5.4. PSD constraint", "text": "For any positive semidefinite matrix, we have ||Z||\u2217 = Tr(Z). The simple form of the trace norm allows to take into account the positive semidefinite constraint at no additional cost, as the shrinkage operation and the projection onto the convex cone of positive semidefinite matrices can be combined into a single operation.\nLemma 1. For \u03c4 \u2265 0 and S \u2208 Rn\u00d7n,\nprox\u03c4 ||.||\u2217+1S+n (S) = arg min\nZ 0\n1 2 ||Z \u2212 S||2F + \u03c4 ||Z||\u2217\n= PS+n (S \u2212 \u03c4In) ."}, {"heading": "6. Numerical experiments", "text": "We present numerical experiments to highlight the benefits of our method. For efficiency reasons, we use the serial proximal descent algorithm (Algorithm 2)."}, {"heading": "6.1. Synthetic data", "text": "Covariance matrix estimation. We draw N vectors xi \u223c N (0,\u03a3) for a block diagonal covariance matrix \u03a3 \u2208 Rn\u00d7n. We use r blocks of random sizes and of the form vv> where the entries of v are drawn i.i.d. from the uniform distribution on [\u22121, 1]. Finally, we add gaussian noise N (0, \u03c32) on each entry. In our experiments r = 5, N = 20, n = 100, \u03c3 = 0.6. We apply our method (SPLR), as well as trace norm regularization (LR) and `1 norm regularization (SP) to the empirical covariance matrix, and report average results over ten runs. Figure 1 shows the RMSE normalized by the norm of \u03a3 for different values of \u03c4 and \u03b3. Note that the effect of the mixed penalty is visible as the minimum RMSE is reached inside the (\u03c4, \u03b3) region. We perform, on the same data, separate cross-validations on (\u03c4, \u03b3) for SPLR, on \u03c4 for LR and on \u03b3 for SP. We show in Figure 2 the supports recovered by each algorithm, the output matrix of LR being thresholded in absolute value. The support recovery demonstrates how our approach discovers the underlying patterns despite the noise and the small number of observations."}, {"heading": "6.2. Real data sets", "text": "Protein Interactions. We use data from (Hu et al., 2009), in which protein interactions in Escherichia coli bacteria are scored by strength in [0, 2]. The data is, by nature, sparse. In addition to this, it is often suggested that interactions between two proteins are governed by a small set of factors, such as surface accessible amino acid side chains (Bock & Gough, 2001), which motivates the estimation of a low-rank representation. Representing the data as a weighted graph, we filter to retain only the 10% of all 4394 proteins that exhibit the most interactions as measured by weighted degree. We corrupt 10% of entries of the adjacency matrix selected uniformly at random by uniform noise in [0, \u03b7]. Parameters are selected by cross-validation and algorithms are evaluated using mean RMSE between estimated and original adjacency matrices over 25 runs. RMSE scores are shown in Table 1 and show the empirical superiority of our approach (SPLR).\nSocial Networks. We have performed experiments with the Facebook100 data set analyzed by (Traud et al., 2011). The data set comprises all friendship relations between students affiliated to a specific university, for a selection of one hundred universities. We select a\nsingle university with 41554 users and filter as in the previous case to keep only the 10% users with highest degrees. In this case, entries are corrupted by impulse noise: a fixed fraction \u03c3 of randomly chosen edges are flipped, thus introducing noisy friendship relations and masking some existing relations. The task is to discover the noisy relations and recover masked relations. We compare our method to standard baselines in link prediction (Liben-Nowell & Kleinberg, 2007). Nearest Neighbors (NN) relies on the number of common friends between each pair of users, which is given by A2 when A is the noisy graph adjacency matrix. Katz\u2019s coefficient connects a pair of nodes according to a score based on the number of paths connecting them, emphasizing short paths. Results are reported in Table 2 using the area under the ROC curve (AUC). SPLR outperforms LR but also NN and Katz which do not directly seek a low-rank representation."}, {"heading": "7. Discussion", "text": "Other loss functions. The methods presented in this paper can be seamlessly extended to non-square matrices, which can arise, for instance, from adjacency matrices of bipartite graphs. Our work also applies to a wide range of other losses. A useful example that links our work to the matrix completion framework is when linear measurements of the target matrix or graph are available, or can be predicted as in (Richard et al., 2010). In this case, the loss can be defined in the feature space. Due to the low-rank assumption, our method does not directly apply to the estimation of precision matrices often used for gaussian graphical model structure learning (Friedman et al., 2008), and the applications of conditional independence structures generated by low-rank and possibly sparse models is to be discussed. Note that the trace norm constraint is vacuous for some special classes of positive semi-definite matrices. For instance, it is not useful for estimating a correlation matrix as, in this case, the trace is always equal to the dimension.\nMatrix factorizations. A related and popular task is finding low-rank factorizations of matrices of the form\nUV T (see, e.g., (Srebro et al., 2005; Srebro, 2004)), thus jointly optimizing in U, V \u2208 Rn\u00d7r loss functions of the form `((U, V ), A) = ||UV T \u2212A||2F for some target maximum rank r. This implicitly encodes the lowrank constraint which leads to efficient optimization schemes, and allows for interpretability as estimated (U, V ) pairs can be considered as latent factors. Nonnegative Matrix Factorization (NMF) (Lee et al., 1999) imposes non negativity constraints on the coefficients of U and V to enhance interpretability by allowing only for additive effects and tends to produce sparse factor matrices U, V , although this a rather indirect effect. There is no strong guarantee on the sparsity achieved by NMF nor is it easy to set the target sparsity and different methods for sparse NMF have been proposed in (Hoyer, 2004; Kim & Park, 2008). Sparse matrix factorizations have also been proposed without the positivity constraint. Most work along this line is motivated by extending the classical PCA and finding sparse directions that maximize the variance of the projection. Most methods give up orthogonality between the components and can thus be seen as sparse matrix factorization techniques. SPCA proposed in (Zou et al., 2004) penalizes the `1 norm of the principal components and can be reduced to solving independent elastic-nets. A different formulation using SDP programming is introduced in (D\u2019Aspremont et al., 2007) with good empirical results. In spite of good empirical performances, all these methods based on matrix factorization suffer from a significant drawback. Although formulations are usually convex in U or V, they are not in general jointly convex and optimization procedures can get stuck in local minima.\nRegularization parameters. We showed how to empirically select using cross-validation the hyper parameters \u03c4 and \u03b3 for a specific application. From a theoretical point of view, Proposition 1 provides us with performance guarantees when the regularization parameters are large enough. We know from random matrix theory that the operator norm of a random gaussian matrix concentrates around \u221a n which enforces a stringent constraint on \u03c4 for \u03c4 \u2265 2\u03b1|| ||op to hold with high probability. Similarly, the \u221e-norm \u2016 \u2016\u221e can be bounded by \u2016 \u2016op or using the multivariate Tchebycheff inequality of (Olkin & Pratt, 1958) which implies that the condition \u03b3 \u2265 2(1 \u2212 \u03b1)\u2016 \u2016\u221e is satisfied with probability 1 \u2212 \u03b4 when \u03b3 = \u2126 ( (1\u2212 \u03b1) 2n\u03c3\u03b4 ) . In practice, \u03b3 should not exceed the order of magnitude of the entries of the matrix, as this leads to a trivial zero solution. Asymptotically, to keep the sparsity regularization parameter \u03b3 of the order of magnitude of elements of the observation matrix A, the free parameter \u03b1 must be chosen so that 1\u2212\u03b1n \u223cn 1n . This gives\nthe same asymptotic behavior in O( \u221a n) for the lower bound on \u03c4 as in matrix completion.\nOptimization. Other optimization techniques can be considered for future work. A trace norm constraint alone can be taken into account without projection or relaxation into a penalized form by casting the problem as a SDP as proposed in (Jaggi, 2011). The special form of this SDP can be leveraged to use the efficient resolution technique from (Hazan, 2008). This method applies to a differentiable objective whose curvature determines the performances. Extending these methods with projection onto the `1 ball or a sparsityinducing penalty could lead to interesting developments."}, {"heading": "Appendix- Sketch of proof for Prop. 1", "text": "For any S in S and by optimality of S\u0302,\n\u2212 2\u3008S\u0302 \u2212 S, S0\u3009 \u2264 \u22122\u3008S\u0302 \u2212 S, S0\u3009+ L(S)\u2212 L(S\u0302)\n\u2264 2\u03b1\u2016S\u0302 \u2212 S\u2016\u2217\u2016 \u2016op + 2(1\u2212 \u03b1)\u2016S\u0302 \u2212 S\u20161\u2016 \u2016\u221e + \u03c4(\u2016S\u2016\u2217 \u2212 \u2016S\u0302\u2016\u2217) + \u03b3(\u2016S\u20161 \u2212 \u2016S\u0302\u20161) + \u2016S\u20162F \u2212 \u2016S\u0302\u20162F\nfor any \u03b1 \u2208 [0; 1]. The assumptions on \u03c4, \u03b3 and triangular inequality lead to the first bound.\nLet r = rank(S), k = \u2016S\u20160, S = \u2211r j=1 \u03c3jujv > j the SVD of S, S = \u0398 \u25e6 |S|, where \u0398 = sgn(S), and \u0398\u22a5 \u2208 {0, 1}n\u00d7n the complementary sparsity pattern. We use PS\u22a51 (resp. PS\u22a52 ) to denote the projection operator onto the orthogonal of the left (resp. right) singular space of S. We also note PS(X) = X \u2212 PS\u22a51 XPS\u22a52 such that X = PS(X) + PS\u22a51 XS\u22a52 .\nAny element V of the subgradient of the convex function S 7\u2192 \u03c4\u2016S\u2016\u2217 + \u03b3\u2016S\u20161 can be decomposed as\nV = \u03c4 ( r\u2211 j=1 ujv > j + PS1\u22a5W\u2217PS2\u22a5 ) + \u03b3 ( \u0398 +W1 \u25e6\u0398\u22a5 ) for W1,W\u2217 with \u2016W\u2217\u2016op \u2264 1, \u2016W1\u2016\u221e \u2264 1, which can be chosen such that\n\u3008V, S\u0302 \u2212 S\u3009 = \u03c4\u3008 r\u2211 j=1 ujv > j , S\u0302 \u2212 S\u3009+ \u03c4\u2016PS1\u22a5S\u0302PS2\u22a5\u2016\u2217\n+\u03b3\u3008\u0398, S\u0302 \u2212 S\u3009+ \u03b3\u2016\u0398\u22a5 \u25e6 S\u0302\u20161 .\nBy monotonicity of the subdifferential and optimality conditions,\n2\u3008S\u0302 \u2212 S0, S\u0302 \u2212 S\u3009\n\u2264 2\u3008 , S\u0302 \u2212 S\u3009 \u2212 \u03c4\u3008 r\u2211 j=1 ujv > j , S\u0302 \u2212 S\u3009\n\u2212 \u03c4\u2016PS\u22a51 S\u0302PS\u22a52 \u2016\u2217 \u2212 \u03b3\u3008\u0398, S\u0302 \u2212 S\u3009 \u2212 \u03b3\u2016\u0398 \u22a5 \u25e6 S\u0302\u20161 .\nDecompose\n= \u03b1 ( PS( ) + PS\u22a51 PS\u22a52 ) + (1\u2212 \u03b1) ( |\u0398| \u25e6 + \u0398\u22a5 \u25e6 ) .\nUsing results on dual norms, we have\n|\u3008M1,M2\u3009| \u2264 ||M1||\u2217||M2||op |\u3008M1,M2\u3009| \u2264 ||M1||1||M2||\u221e\nfor all M1,M2 \u2208 Rn\u00d7n and hence,\n\u3008 , S\u0302 \u2212 S\u3009 \u2264 \u03b1\u2016PS( )\u2016F \u2016PS1(S\u0302 \u2212 S)PS2\u2016F + \u03b1\u2016PS\u22a51 PS\u22a52 \u2016op\u2016PS\u22a51 S\u0302PS\u22a52 \u2016\u2217\n+ (1\u2212 \u03b1)\u2016\u0398 \u25e6 \u2016F \u2016\u0398 \u25e6 ( S\u0302 \u2212 S ) \u2016F\n+ (1\u2212 \u03b1)\u2016\u0398\u22a5 \u25e6 \u2016\u221e\u2016\u0398\u22a5 \u25e6 S\u0302\u20161 .\nUsing\n\u2016PS( )\u2016F \u2264 \u221a 2 r\u2016 \u2016op, \u2016\u0398 \u25e6 \u2016F \u2264 \u221a k\u2016 \u2016\u221e\nleads for \u03c4 \u2265 2\u03b1 \u2016 \u2016op and \u03b3 \u2265 2(1\u2212 \u03b1) \u2016 \u2016\u221e to\n\u2016S\u0302 \u2212 S0\u20162F + \u2016S\u0302 \u2212 S\u20162F \u2264 \u2016S \u2212 S0\u20162F + ( \u03c4 \u221a r( \u221a 2 + 1) + 2\u03b3 \u221a k ) \u2016S\u0302 \u2212 S\u2016F . Using \u03b2x\u2212 x2 \u2264 ( \u03b2 2 )2 , we obtain\n\u2016S\u0302 \u2212 S0\u20162F \u2264 \u2016S \u2212 S0\u20162F + 1\n4\n(\u221a r\u03c4( \u221a 2 + 1) + 2 \u221a k\u03b3 )2 and setting S = S0 gives the result."}], "references": [{"title": "Model selection through sparse maximum likelihood estimation", "author": ["O. Banerjee", "L. El Ghaoui", "A. d\u2019Aspremont"], "venue": "Machine Learning Research", "citeRegEx": "Banerjee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2007}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal of Imaging Sciences,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Incremental gradient, subgradient, and proximal methods for convex optimization: a survey", "author": ["D.P. Bertsekas"], "venue": "Optimization for Machine Learning, pp", "citeRegEx": "Bertsekas,? \\Q2011\\E", "shortCiteRegEx": "Bertsekas", "year": 2011}, {"title": "Sparse estimation of a covariance matrix", "author": ["J. Bien", "R. Tibshirani"], "venue": null, "citeRegEx": "Bien and Tibshirani,? \\Q2010\\E", "shortCiteRegEx": "Bien and Tibshirani", "year": 2010}, {"title": "Predicting protein\u2013 protein interactions from primary", "author": ["J.R. Bock", "D.A. Gough"], "venue": "structure. Bioinformatics,", "citeRegEx": "Bock and Gough,? \\Q2001\\E", "shortCiteRegEx": "Bock and Gough", "year": 2001}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.F. Cai", "E.J. Candes", "Z. Shen"], "venue": "Arxiv preprint Arxiv:0810.3286,", "citeRegEx": "Cai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2008}, {"title": "Robust principal component analysis", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "Arxiv preprint ArXiv:0912.3599,", "citeRegEx": "Candes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2009}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky"], "venue": "SIAM J. Opt.,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2011}, {"title": "Proximal splitting methods in signal processing. Fixed-Point Algorithms for Inverse Problems", "author": ["P.L. Combettes", "J.C. Pesquet"], "venue": "Science and Engineering,", "citeRegEx": "Combettes and Pesquet,? \\Q2011\\E", "shortCiteRegEx": "Combettes and Pesquet", "year": 2011}, {"title": "A direct formulation for sparse pca using semidefinite programming", "author": ["A. D\u2019Aspremont", "L. El Ghaoui", "M.I. Jordan", "G.R.G. Lanckriet"], "venue": "SIAM review,", "citeRegEx": "D.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "D.Aspremont et al\\.", "year": 2007}, {"title": "Operator norm consistent estimation of large-dimensional sparse covariance matrices", "author": ["N. El Karoui"], "venue": "Annals of Statistics,", "citeRegEx": "Karoui,? \\Q2009\\E", "shortCiteRegEx": "Karoui", "year": 2009}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Sparse approximate solutions to semidefinite programs", "author": ["E. Hazan"], "venue": "In Proceedings of the 8th Latin American conference on Theoretical informatics,", "citeRegEx": "Hazan,? \\Q2008\\E", "shortCiteRegEx": "Hazan", "year": 2008}, {"title": "Non-negative Matrix Factorization with Sparseness Constraints", "author": ["P.O. Hoyer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoyer,? \\Q2004\\E", "shortCiteRegEx": "Hoyer", "year": 2004}, {"title": "Global functional atlas of escherichia coli encompassing previously uncharacterized proteins", "author": ["P. Hu", "S.C. Janga", "M. Babu", "J.J. D\u0131\u0301az-Mej\u0301\u0131a", "G. Butland", "W. Yang", "O. Pogoutse", "X. Guo", "S. Phanse", "P Wong"], "venue": "PLoS biology,", "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "Convex optimization without projection steps", "author": ["M. Jaggi"], "venue": "Arxiv preprint arXiv:1108.1170,", "citeRegEx": "Jaggi,? \\Q2011\\E", "shortCiteRegEx": "Jaggi", "year": 2011}, {"title": "Clustering partially observed graphs via convex optimization", "author": ["A. Jalali", "Y. Chen", "S. Sanghavi", "H. Xu"], "venue": "ICML \u201911,", "citeRegEx": "Jalali et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jalali et al\\.", "year": 2011}, {"title": "Sparse nonnegative matrix factorization for clustering", "author": ["J. Kim", "H. Park"], "venue": "Technical report, Georgia Institute of Technology,", "citeRegEx": "Kim and Park,? \\Q2008\\E", "shortCiteRegEx": "Kim and Park", "year": 2008}, {"title": "Nuclear norm penalization and optimal rates for noisy matrix completion", "author": ["V. Koltchinskii", "K. Lounici", "A. Tsybakov"], "venue": "Annals of Statistics,", "citeRegEx": "Koltchinskii et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Koltchinskii et al\\.", "year": 2011}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "Seung", "H.S"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1999}, {"title": "The link-prediction problem for social networks", "author": ["D. Liben-Nowell", "J. Kleinberg"], "venue": "Journal of the American society for information science and technology,", "citeRegEx": "Liben.Nowell and Kleinberg,? \\Q2007\\E", "shortCiteRegEx": "Liben.Nowell and Kleinberg", "year": 2007}, {"title": "High dimensional low rank and sparse covariance matrix estimation via convex minimization", "author": ["X. Luo"], "venue": "Arxiv preprint arXiv:1111.1133,", "citeRegEx": "Luo,? \\Q2011\\E", "shortCiteRegEx": "Luo", "year": 2011}, {"title": "A multivariate tchebycheff inequality", "author": ["I. Olkin", "J.W. Pratt"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Olkin and Pratt,? \\Q1958\\E", "shortCiteRegEx": "Olkin and Pratt", "year": 1958}, {"title": "Generalized forward-backward splitting", "author": ["H. Raguet", "J. Fadili", "G. Peyr\u00e9"], "venue": "Arxiv preprint arXiv:1108.4404,", "citeRegEx": "Raguet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Raguet et al\\.", "year": 2011}, {"title": "Link discovery using graph feature tracking", "author": ["E. Richard", "N. Baskiotis", "Evgeniou", "Th", "N. Vayatis"], "venue": "Proceedings of Neural Information Processing Systems (NIPS),", "citeRegEx": "Richard et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Richard et al\\.", "year": 2010}, {"title": "Learning with matrix factorizations", "author": ["N. Srebro"], "venue": "PhD thesis,", "citeRegEx": "Srebro,? \\Q2004\\E", "shortCiteRegEx": "Srebro", "year": 2004}, {"title": "Maximummargin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakkola"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Yuan and Lin,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin", "year": 2006}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Computational and Graphical Statistics, pp", "citeRegEx": "Zou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2004}, {"title": "Regularization and variable selection via the elastic net", "author": ["Zou", "Hui", "Hastie", "Trevor"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Zou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 25, "context": "Indeed, the notion of sparsity assumption has been transposed into the concept of low-rank matrices and opened the way to numerous achievements (see for instance (Srebro, 2004; Cai et al., 2008)).", "startOffset": 162, "endOffset": 194}, {"referenceID": 5, "context": "Indeed, the notion of sparsity assumption has been transposed into the concept of low-rank matrices and opened the way to numerous achievements (see for instance (Srebro, 2004; Cai et al., 2008)).", "startOffset": 162, "endOffset": 194}, {"referenceID": 27, "context": "Efficient procedures developed in the context of sparse model estimation mostly rely on the use of `1-norm regularization (Tibshirani, 1996).", "startOffset": 122, "endOffset": 140}, {"referenceID": 0, "context": "These methods are readily adapted to matrix valued data and have been applied to covariance estimation (El Karoui, 2009; Bien & Tibshirani, 2010) and graphical model structure learning (Banerjee et al., 2007; Friedman et al., 2008).", "startOffset": 185, "endOffset": 231}, {"referenceID": 11, "context": "These methods are readily adapted to matrix valued data and have been applied to covariance estimation (El Karoui, 2009; Bien & Tibshirani, 2010) and graphical model structure learning (Banerjee et al., 2007; Friedman et al., 2008).", "startOffset": 185, "endOffset": 231}, {"referenceID": 26, "context": "In the low-rank matrix completion problem, the standard relaxation approach leads to the use of the trace norm as the main regularizer within the optimization procedures (Srebro et al., 2005; Koltchinskii et al., 2011) and their resolution can either be obtained in closed form (loss measured in terms of Frobenius norm) or through iterative proximal solutions (Combettes & Pesquet, 2011; Beck & Teboulle, 2009) (for general classes of losses).", "startOffset": 170, "endOffset": 218}, {"referenceID": 18, "context": "In the low-rank matrix completion problem, the standard relaxation approach leads to the use of the trace norm as the main regularizer within the optimization procedures (Srebro et al., 2005; Koltchinskii et al., 2011) and their resolution can either be obtained in closed form (loss measured in terms of Frobenius norm) or through iterative proximal solutions (Combettes & Pesquet, 2011; Beck & Teboulle, 2009) (for general classes of losses).", "startOffset": 170, "endOffset": 218}, {"referenceID": 6, "context": "In Robust PCA (Candes et al., 2009) and related literature, the signal S is assumed to have an additive decomposition S = X + Y where X is sparse and Y low-rank.", "startOffset": 14, "endOffset": 35}, {"referenceID": 7, "context": ", in (Chandrasekaran et al., 2011).", "startOffset": 5, "endOffset": 34}, {"referenceID": 16, "context": "This technique has been successfully applied to background substraction in image sequences, to graph clustering (Jalali et al., 2011) and covariance estimation (Luo, 2011).", "startOffset": 112, "endOffset": 133}, {"referenceID": 21, "context": ", 2011) and covariance estimation (Luo, 2011).", "startOffset": 34, "endOffset": 45}, {"referenceID": 18, "context": "The techniques used in the proof (see the Appendix) are very similar to those introduced in (Koltchinskii et al., 2011).", "startOffset": 92, "endOffset": 119}, {"referenceID": 18, "context": "In fact, for \u03b1 = 0, \u03c4 can be set to zero, and we get a sharp bound for Lasso, while the tracenorm regression bounds of (Koltchinskii et al., 2011) are obtained for \u03b1 = 1.", "startOffset": 119, "endOffset": 146}, {"referenceID": 25, "context": "In the case of the sole rank constraint, (Srebro, 2004) remarked that all low-rank matrices with the same sign pattern are equivalent in terms of loss and applied a standard argument for generalization in classes of finite cardinality.", "startOffset": 41, "endOffset": 55}, {"referenceID": 25, "context": "By upper bounding the number of sign configurations for a fixed sparsity pattern in (U, V ) using an argument similar to (Srebro, 2004), a union bound gives", "startOffset": 121, "endOffset": 135}, {"referenceID": 23, "context": "A generalization of these two setups has been recently proposed in (Raguet et al., 2011) under the name of Generalized ForwardBackward, which we specialize to our problem in Algorithm 1.", "startOffset": 67, "endOffset": 88}, {"referenceID": 2, "context": "As a consequence, we mention a matching serial algorithm (Algorithm 2) introduced in (Bertsekas, 2011) that has a flavor similar to multi-pass stochastic gradient descent.", "startOffset": 85, "endOffset": 102}, {"referenceID": 14, "context": "We use data from (Hu et al., 2009), in which protein interactions in Escherichia coli bacteria are scored by strength in [0, 2].", "startOffset": 17, "endOffset": 34}, {"referenceID": 24, "context": "A useful example that links our work to the matrix completion framework is when linear measurements of the target matrix or graph are available, or can be predicted as in (Richard et al., 2010).", "startOffset": 171, "endOffset": 193}, {"referenceID": 11, "context": "Due to the low-rank assumption, our method does not directly apply to the estimation of precision matrices often used for gaussian graphical model structure learning (Friedman et al., 2008), and the applications of conditional independence structures generated by low-rank and possibly sparse models is to be discussed.", "startOffset": 166, "endOffset": 189}, {"referenceID": 26, "context": ", (Srebro et al., 2005; Srebro, 2004)), thus jointly optimizing in U, V \u2208 Rn\u00d7r loss functions of the form `((U, V ), A) = ||UV T \u2212A||F for some target maximum rank r.", "startOffset": 2, "endOffset": 37}, {"referenceID": 25, "context": ", (Srebro et al., 2005; Srebro, 2004)), thus jointly optimizing in U, V \u2208 Rn\u00d7r loss functions of the form `((U, V ), A) = ||UV T \u2212A||F for some target maximum rank r.", "startOffset": 2, "endOffset": 37}, {"referenceID": 19, "context": "Nonnegative Matrix Factorization (NMF) (Lee et al., 1999) imposes non negativity constraints on the coefficients of U and V to enhance interpretability by allowing only for additive effects and tends to produce sparse factor matrices U, V , although this a rather indirect effect.", "startOffset": 39, "endOffset": 57}, {"referenceID": 13, "context": "There is no strong guarantee on the sparsity achieved by NMF nor is it easy to set the target sparsity and different methods for sparse NMF have been proposed in (Hoyer, 2004; Kim & Park, 2008).", "startOffset": 162, "endOffset": 193}, {"referenceID": 29, "context": "SPCA proposed in (Zou et al., 2004) penalizes the `1 norm of the principal components and can be reduced to solving independent elastic-nets.", "startOffset": 17, "endOffset": 35}, {"referenceID": 9, "context": "A different formulation using SDP programming is introduced in (D\u2019Aspremont et al., 2007) with good empirical results.", "startOffset": 63, "endOffset": 89}, {"referenceID": 15, "context": "A trace norm constraint alone can be taken into account without projection or relaxation into a penalized form by casting the problem as a SDP as proposed in (Jaggi, 2011).", "startOffset": 158, "endOffset": 171}, {"referenceID": 12, "context": "The special form of this SDP can be leveraged to use the efficient resolution technique from (Hazan, 2008).", "startOffset": 93, "endOffset": 106}], "year": 2012, "abstractText": "The paper introduces a penalized matrix estimation procedure aiming at solutions which are sparse and low-rank at the same time. Such structures arise in the context of social networks or protein interactions where underlying graphs have adjacency matrices which are block-diagonal in the appropriate basis. We introduce a convex mixed penalty which involves `1-norm and trace norm simultaneously. We obtain an oracle inequality which indicates how the two effects interact according to the nature of the target matrix. We bound generalization error in the link prediction problem. We also develop proximal descent strategies to solve the optimization problem efficiently and evaluate performance on synthetic and real data sets.", "creator": "LaTeX with hyperref package"}}}