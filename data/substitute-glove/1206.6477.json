{"id": "1206.6477", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Discovering Support and Affiliated Features from Very High Dimensions", "abstract": "In example books, a novel aspects definition in show to regardless identifies groups called startlingly three correlated features from think high geometric. Specifically, need publicly tool correlation appropriate also constraints and just implementing an efficient tools show fair method using recently developed increased plane reform. The raise there has extend coding are with - thin. First, alone can identify the equilibrium muscularity with spatially feature finite to set current labels, subscript here except Support Features, which sense where importance improvements with prognosis debut five other community whose the painting feature selection methods yet by later cover. Second, subsequent by learning specific, the extent umbrella exterior another correlated contemporary associated out each needs feature, denoted as Affiliated Features, can also be where get consider needed cost. These affiliated features serve come recovery entire interpretation held the school assessing. Extensive empirical psychiatry on to synthetic where things highest dimensional real - well datasets verify under determine these efficiency of the adopted mechanical.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (543kb)", "http://arxiv.org/abs/1206.6477v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yiteng zhai", "mingkui tan", "ivor w tsang", "yew-soon ong"], "accepted": true, "id": "1206.6477"}, "pdf": {"name": "1206.6477.pdf", "metadata": {"source": "META", "title": "Discovering Support and Affiliated Features from Very High Dimensions", "authors": ["Yiteng Zhai", "Mingkui Tan", "Ivor W. Tsang", "Yew-Soon Ong"], "emails": ["YZHAI1@NTU.EDU.SG", "TANM0097@NTU.EDU.SG", "IVORTSANG@NTU.EDU.SG", "ASYSONG@NTU.EDU.SG"], "sections": [{"heading": "1. Introduction", "text": "Many real-world datasets in text and digital media domains are typically represented with very high dimensional features, bringing significant challenges in data mining. Learning performance is often degraded with inflating of dimensions, leading to the well-known notion of \u201ccurse of dimensionality\u201d. This problem becomes particularly critical when the number of informative features is relatively small, but involved with a vast variety of irrelevant features and redundant features (Yu & Liu, 2004).\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nTo address this issue, a plethora of feature selection methods have been developed in the recent decades. In general, these methods have been categorized as three core themes (Guyon, 2008): filter methods (Yu & Liu, 2003; Peng et al., 2005), wrapper methods (Guyon & Elisseeff, 2003; Zhu et al., 2007) and embedded methods (Yuan et al., 2011; Tan et al., 2010; Mao & Tsang, 2011). Specifically, filter methods select informative features based on their individual discriminative power or correlation criterion. The benefits of filter methods lie in their low computational requirements. The drawback however is that it may not identify the optimal feature subset suitable for the predictive model of interest. On the contrary, since wrapper methods, such as SVM-RFE (Guyon & Elisseeff, 2003), select the discriminative features solely based on the inductive learning rule, they typically exhibit higher predictive performance but at the expense of a lower computational efficiency on large scale and very high dimensional problems. Embedded methods refer to approaches that directly optimize some regularized risk function w.r.t. two sets of parameters: parameter of the learning machine, and parameter to control the feature sparsity (Guyon, 2008). As such they are usually more efficient than wrapper methods.\nTo date, it is worth noting that most methods in all three categories generally assume a good feature subset has strong prediction ability pertaining to the output labels; meanwhile the selected features should also maintain low correlations. In other words, correlated features are deemed as redundant, and this redundancy should be minimized (Hall, 1999; Guyon, 2008; Zhao et al., 2011; 2012). Though elim-\ninating redundant features has been widely used in practice and regarded as the guiding principle behind the development of feature selection methods, it may not always hold since these correlated features can be useful and informative for the tasks on hand. As also discussed in (O\u2019Sullivan et al., 2000; Caruana & de Sa, 2003; Xu et al., 2012), such feature redundancy has the benefits of bringing about stable generalization performances. Here, we present an illustrating example using Figure 1. Despite a perfect prediction by the classifier on the male and female face images based on the optimal features identified (denoted here as support features), these uncorrelated features which are depicted by the white pixels in the middle of Figure 1, can be observed to be sparsely spread across the entire image, thus giving little cue to assist the human user in the interpretation of the features. The white pixels shown in the right column of Figure 1, denoting the highly correlated features, on the other hand, are much more informative. To be precise, it is easy for the human user to spot the core group features in the regions of the mustache and beard, which can be helpful to the users in grasping a better understanding on the critical objects in the images so that further analysis can be made. On the contrary, existing feature selection methods usually eliminate these correlated features and treat them as pure redundancies. Further, the act of identifying these correlated features from high dimensions is typically very compute intensive. Taking this cue, in this paper, we introduce an efficient feature selection method that can identify the optimal feature subset to the output labels; while minimizing the correlation among the selected features. Each selected feature is defined as Support Feature; while the correlated features associate with this support feature are denoted as Affiliated Features, and are discovered during the feature selection process without any additional cost. The core contributions of this paper are listed as follows:\n1. We develop an efficient Correlation Redundancy Matching (CRM) algorithm, which accommodates the correlation constraints among features, to identify both discriminative features and correlated features. Similar to L1-SVM, the proposed method can be categorized as an embedded feature selection approach and attains global convergence. Theoretical analysis and empirical studies show that our method is scalable to very high dimensional problems. 2. For each support feature, the respective associated affiliated feature subset contains correlated features that are informative to the output labels and further, with these two types of features, a group structure of features can be established. These affiliated groups maintain redundancy for robust prediction, and help users understand the learning tasks for further analysis. 3. Empirical studies on both synthetic and real-world experiments verify the superior performance of the pro-\nposed method over the state-of-the-art feature selection methods in terms of both testing accuracies and redundancy rate (Zhao et al., 2012)."}, {"heading": "2. Preliminaries and Related Works", "text": "In this paper, we denote a data point by xi \u2208 Rn and the dataset by X = [x1, . . . ,xn] = [f \u20321, . . . , f \u2032 m] \u2032 \u2208 Rm\u00d7n, where fj represents a row vector corresponding to the jth feature of the data points in X. Each xi is associated with an output yi \u2208 {\u00b11}. We also define y as the vector of the label for the data. Symbols 0 and 1 are the column vectors with all zeros and all ones, respectively. For any vector f , we denote the mean and standard deviation of the entries in f as \u00b5f and \u03c3f , respectively. Additionally, we denote the element wise product between two matrices A and B by A B. Finally, we denote |S| to be the size of a set S. As previously stated, feature correlation is of particular interest in our present research. In the past decades, a large branch of feature selection approaches have targeted on reducing the redundancy among the selected features. The notion of feature redundancy is usually measured by means of feature correlation. The major motivation has been to find the optimal or minimized feature subset corresponding to the output labels. Thus, when a feature is selected, other features that are highly related to this feature is typically rejected so as to minimize feature redundancies."}, {"heading": "2.1. Feature Correlation Measures", "text": "To date, various criteria have been introduced for defining the correlation between features. For instance, a widely used correlation criterion is the Pearson\u2019s correlation coefficient (PCC), which measures the linear correlation between variables. In more details, given two feature vectors fj and fk, the metric \u03c1 in PCC can be defined as follows,\n\u03c1(fj , fk)= cov(fj , fk)\n\u03c3fj\u03c3fk =\n1 n (fj \u2212 \u00b5fj1 \u2032)(fk \u2212 \u00b5fk1 \u2032)\u2032\n\u03c3fj\u03c3fk . (1)\nNotice that \u03c1 is a symmetrical measure that ranges in [\u22121, 1]. If two variables are fully independent, \u03c1 = 0. On the other hand, when the two variables are completely correlated to each other, namely, one variable can exactly predict another variable, we have |\u03c1| = 1. Other metrics such as information gain IG(fj |fk) and symmetrical uncertainty SU(fj |fk) have also been discussed in (Yu & Liu, 2003)."}, {"heading": "2.2. Feature Redundancy Reduction", "text": "Based on these correlation measures, several methods have attempted to reduce the redundancy among the selected features. For instance, in Fast Correlation Based Filter (FCBF) (Yu & Liu, 2003), feature importance and feature correlation are assessed by means of SU measure. FCBF first selects a set of predominant features that are relevant to the output labels. Subsequently, the informative features to labels are kept while the correlated features are removed\nbased on some elegantly designed intuitive rules (Yu & Liu, 2003). Another notable redundancy reduction method is Minimum Redundancy Maximum Relevance (mRMR) (Peng et al., 2005), which selects the most correlated features to the labels such that they are mutually far apart from each other by maximizing the dependency between the joint distribution of the selected features and the output labels. Zhou et al. (2010) proposed Redundancy Constrained Feature Selection (RCFS), which first performs feature clustering by using some distance measures (e.g., 1 \u2212 |\u03c1(fj , fk)|). Hence the correlated features may be grouped into several clusters. After that, some features are then identified from each cluster. Next, a feature subset is further identified from the selected features in each cluster group using graph based feature selection criteria (Nie et al., 2008) that capture the global and local intrinsic structures of the data. This strategy however is heavily sensitive to the choice of graph Laplacian matrices used. For example, the Laplacian score is usually constructed using K nearest neighbor (KNN). In practice, on very high dimensional problems, KNNs can be very far away from each other in reality due to the effect of the curse of dimensionality. Besides, the high computational cost of feature clustering on high dimensional data and graph based methods (taking O(n2m)) make this approach less attractive on large scale data. Recently, Zhao et al. (2012) proposed a framework to unify different criteria for removing feature redundancies. Nevertheless, existing methods have remained to focus on reducing these redundancies. More importantly, to date the discovery of correlated yet informative features has been relatively unexplored."}, {"heading": "3. Group Discovery Machine", "text": "In this section, we present an efficient automatic feature grouping method, which identifies groups of discriminative yet correlated features. Similar to (Guyon, 2008), a vector \u03b4 = [\u03b41, . . . , \u03b4m]\u2032 \u2208 {0, 1}m is introduced to indicate whether the corresponding feature is selected (\u03b4j = 1) or not (\u03b4j = 0), such that the decision function is defined as: f(x) = w\u2032(x \u03b4), where w = [w1, \u00b7 \u00b7 \u00b7 , wm]\u2032 is the weight vector. To limit the number of selected features to be less than B, the `0 constraint \u2016\u03b4\u20160 \u2264 B is imposed for the purpose of feature selection (Nie et al., 2008)."}, {"heading": "3.1. Correlation Constraints", "text": "To control the correlation among the selected features, we explicitly introduce the following constraint on \u03b4 such that,\n\u03b4j\u03b4k = 0, |\u03c1(fj , fk)| \u2265 1\u2212 \u03c4, \u2200j 6= k, (2)\nwhich states that any selected feature pair should not be correlated so long as their coefficient defined in (1) does not exceeds (1\u2212 \u03c4 ) where \u03c4 \u2208 (0, 1). We also define \u2206 = {\u03b4| \u2211m\nj=1 \u03b4j \u2264 B; \u03b4j \u2208 {0, 1}, \u2200j = 1, \u00b7 \u00b7 \u00b7 ,m; \u03b4j\u03b4k =\n0, |\u03c1(fj , fk)| \u2265 1\u2212\u03c4,\u2200j 6= k} as the domain of \u03b4. Here, (2) defines O(m2) quadratic constraints with m integer variables, thus finding the solution \u03b4 \u2208 \u2206 involves combinatorial subset selection, resulting in high computational cost especially when the dimension m is high."}, {"heading": "3.2. Proposed Formulation", "text": "Here, we aim to find a large margin decision function f(x) for robust prediction, and seamlessly identify the informative yet uncorrelated feature subset that satisfies the constraint in (2). For the sake of simplicity, we use the square hinge loss in SVM, and arrive at the following problem:\nmin \u03b4\u2208\u2206 min w,\u03b3,\u03be\n1 2 \u2016w\u201622 \u2212 \u03b3 + C 2 n\u2211 i=1 \u03be2i (3)\ns.t. yiw \u2032(xi \u03b4) \u2265 \u03b3 \u2212 \u03bei, i = 1, \u00b7 \u00b7 \u00b7 , n,\nwhere \u03bei \u2265 0 is the slack variable, \u03b3/\u2016w\u2016 denotes the margin and C is a tradeoff parameter to regulate the function complexity \u2016w\u201622 and the training error (\u03bei\u2019s). Note, as discussed in Section 3.1, the optimization problem in (3) with the constraints defined in \u2206 is very challenging."}, {"heading": "3.3. Cutting Plane Algorithm", "text": "To tackle this, we first transform the inner minimization in (3) w.r.t. w, \u03b3, \u03bei into the dual of SVM, then (3) becomes a minimax saddle-point problem. Inspired by (Tan et al., 2010), by applying the minimax optimization theory, one can obtain a tight convex relaxation to (3), which is in the form of the following Quadratically Constrained Quadratic Programming (QCQP) problem:\nmin \u03b1\u2208A,\u03b8 \u03b8 : \u03b8 \u2265 g\u03b4(\u03b1), \u2200\u03b4 \u2208\u2206 or min\u03b1\u2208A max\u03b4\u2208\u2206 g\u03b4(\u03b1), (4)\nwhere g\u03b4(\u03b1) = 1 2 \u2225\u2225\u2211n i=1 \u03b1iyi(xi \u03b4) \u2225\u22252 + 12C\u03b1\u2032\u03b1, \u03b1 = [\u03b11, . . . , \u03b1n]\n\u2032 is the vector of dual variables, A = {\u03b1 \u2223\u2223\u2211n\ni=1 \u03b1i = 1, \u03b1i \u2265 0,\u2200i = 1, \u00b7 \u00b7 \u00b7 , n} is the domain of \u03b1, and \u03b8 is the upper bound of g\u03b4(\u00b7). Nevertheless, since there are as many as ( \u2211B i=0 ( m i ) ) quadratic constraints in (4), it remains computationally expensive to solve (4). Rather than solving the original problem with a large collection of constraints, the cutting plane strategy (Mutapcic & Boyd, 2009) can be employed to iteratively generate a set of active constraints and then solve this reduced optimization problem with the current constraint set. Since max\u03b4\u2208\u2206 g\u03b4(\u03b1) \u2265 g\u03b4t(\u03b1),\u2200\u03b4\nt \u2208 \u2206, with a reduced active constraint set C \u2282 \u2206, the lower bound approximation of (4) can be obtained by max\u03b4\u2208\u2206 g\u03b4(\u03b1) \u2265 maxt=1,...,T g\u03b4t(\u03b1) with T = |C|, where T is the maximum number of constraints that will be added. This leads to solving a reduced problem of (4) as follows,\nmin \u03b1\u2208A,\u03b8 \u03b8 : \u03b8 \u2265 g\u03b4t(\u03b1), \u2200 \u03b4 t \u2208 C. (5)\nThe details to solve (5) are outlined in Algorithm 1, where some notations will be explained later. Specifically, at each\nAlgorithm 1 Group Discovery Machine Input: Given a dataset (X,y), parameter B and \u03c4 . Output: S and Q are the index sets of support features and affiliated features, respectively. Set \u03b1 = 1/n,S = \u2205 andQ = \u2205. for t = 1 to T do\n1: Call \u03b4t = CRM(X,y, B, \u03c4,\u03b1,S,Q) to find the most violated \u03b4t and acquire S,Q 2: Set C = C \u222a {\u03b4t} 3: Solve (5) defined on C while updating \u03b1\nend for\nAlgorithm 2 CRM(X,y, B, \u03c4,\u03b1,S,Q) Input: Given a dataset (X,y), parameter B and \u03c4 , S and Q are the index sets of support features and affiliated features, respectively. Initialize an index set B = \u2205. Output: A zero-one vector \u03b4 \u2208 Rm, initialized as \u03b4 = 0m. 1: Compute c = \u2211n i=1 \u03b1iyixi, sort |cj | in the descending or-\nder, and record the feature ranking list as E . 2: G = \u2205 denotes a temporary affiliated feature set and k = 1. while |B| \u2264 B do\nPick the kth feature fz from X, where z = E(k) Set B = B \u222a {z} and G = G \u222a {z} while k < m do k = k + 1 Pick the current kth feature fh from X, where h = E(k) if (|\u03c1(fz, fh)| \u2265 1\u2212 \u03c4) then\nUpdate G = G \u222a {h} end if if (|cz| \u2212 \u221a 2\u03c4\u2016\u03b1\u2016 > |ch|) then\nbreak end if\nend while SetQ = Q\u222a {G} and G = \u2205.\nend while Update S = S \u222a {B} and \u03b4B = 1.\niteration of Algorithm 1, one needs to solve the worst case analysis (the same as finding the most violated constraint \u03b4t) of Problem (4), which shall be described in Section 3.4. Subsequently, the obtained \u03b4t would be appended into the active constraint set C. Finally, the problem w.r.t. a reduced active constraint set C can be solved by some efficient QCQP solvers (Tan et al., 2010). To summarize, the cutting plane algorithm generally converges to a robust optimal solution within tens of iterations with the exact worst case analysis and shows good performance in many real applications (Mutapcic & Boyd, 2009)."}, {"heading": "3.4. Correlation Redundancy Matching", "text": "In this subsection, we discuss the worst case analysis of problem (4) (i.e., equivalent to finding the most violated constraints), which plays the key role in cutting plane algorithms (Mutapcic & Boyd, 2009). In our setting, this translate to solving the following integer optimization problem:\nmax \u03b4\u2208\u2206\n\u2225\u2225\u2225 n\u2211 i=1 \u03b1iyi(xi \u03b4) \u2225\u2225\u22252. (6)\nIn general, solving this problem is NP hard. However, since 1 2\u2016 \u2211n i=1 \u03b1iyi(xi \u03b4)\u20162 = 1 2\u2016 \u2211n\ni=1(\u03b1iyixi) \u03b4\u20162 = 1 2 \u2211m j=1 c 2 j\u03b4j , where cj = \u2211n i=1 \u03b1iyixij = fj\u03b1\u0303, and \u03b1\u0303 = [\u03b11y1, . . . , \u03b1nyn] \u2032, this indicates that the informative features accord with the features with the highest |cj | \u2019s. In addition, based on this observation, the following proposition1 will further show that for a set of correlated features, if one of them is informative to the output labels, all of them can be deem as informative to the output labels as well.\nProposition 1. Given a nonzero column vector \u03b1\u0303, and any two feature vectors f1 and f2 that \u03c3f1 = \u03c3f2 = 1/ \u221a n and \u00b5f1 = \u00b5f2 = 0. Suppose |\u03c1(f1, f2)| \u2265 1 \u2212 \u03c4 , then | |f1\u03b1\u0303| \u2212 |f2\u03b1\u0303| | \u2264 \u221a 2\u03c4\u2016\u03b1\u0303\u2016, where \u03c4 \u2208 (0, 1).\nProof. With |\u03c1(f1, f2)| \u2265 1 \u2212 \u03c4 , using (1), we have |\u03c1(f1, f2)| = |f1f \u20322| \u2265 1 \u2212 \u03c4 , namely, f1f \u20322 \u2265 1 \u2212 \u03c4 (positive correlation) or f1f \u20322 \u2264 \u03c4 \u2212 1 (negative correlation). Suppose f1 and f2 are positive correlated, we have \u2016f1 \u2212 f2\u20162 = \u2016f1\u20162 + \u2016f2\u20162 \u2212 2f1f \u20322 = 2(1 \u2212 f1f \u20322) \u2264 2\u03c4 , as \u2016f1\u20162 = \u2016f2\u20162 = 1 when \u03c32f1 = \u03c3 2 f1\n= 1/n. Note that | \u2016f2\u2212 \u03b1\u0303\u2032\u20162\u2212\u2016f1\u2212 \u03b1\u0303\u2032\u20162 | = |2(f1\u2212 f2)\u03b1\u0303| \u2264 2\u2016\u03b1\u0303\u2016\u2016f1\u2212 f2\u2016 \u2264 2 \u221a 2\u03c4\u2016\u03b1\u0303\u2016. In other words, |f1\u03b1\u0303\u2212 f2\u03b1\u0303| \u2264 \u221a 2\u03c4\u2016\u03b1\u0303\u2016.\nHence, we have | |f1\u03b1\u0303| \u2212 |f2\u03b1\u0303| | \u2264 \u221a 2\u03c4\u2016\u03b1\u0303\u2016. In the case of negative correlation, we define a positive correlated vector f\u03022 = \u2212f2 and the proof follows the derivation of the positive correlation case, we complete the proof.\nThe above results state that if two feature vectors f1 and f2 are highly correlated, their distance (or correlation) to any exemplar vector \u03b1\u0303\u2032 will be very similar to one other. Then a natural question arises, considering the correlated features, which feature poses greater importance to the output labels for a given \u03b1\u0303? To address this question, we first offer the definitions of Support Features and Affiliated Features. In particular, support features refer to informative features with relatively low correlations. affiliated features, on the other hand, refer to the correlated features associated with each support feature.\nDefinition 1. Support and Affiliated features: Given any exemplar vector \u03b1\u0303 \u2208 Rn and a collection of feature vectors {fi}, where f \u2032i \u2208 Rn. The support feature is given by maxi |fi\u03b1\u0303| for the given \u03b1\u0303. The remaining correlated features in {fj} w.r.t. fi denote the affiliated features.\nFor the sake of conciseness, we let S be the index set of the support features and introduce a data structure Q = {Gi} to represent the hierarchical structure of features, where Gi denotes the index set of the affiliated features for the ith support feature. Notice that the support feature is correlated to itself, we have S \u2282 Q. By taking this scheme, we can keep all the correlated features rather than omitting them. Based on these definitions, once a support feature is\n1Here, only the linear correlation is considered.\nidentified (i.e., the feature with the largest cj), all relevant features that correlate with this support feature will form the corresponding affiliated feature group or cluster. Since the proposed method can discover the correlated feature groups, we name it Group Discovery Machine (GDM). Note that, alternatively, one could use a brute-force approach to search across all features and identify all features that are correlated to the support feature as the affiliated features to achieve the same goal. However, such a strategy can be computationally infeasible. Fortunately, we show in what follows a theorem to illustrate that in practice one can address this problem by scanning only a small subset of the features on very high dimensional problems.\nTheorem 1. Given a nonzero column vector \u03b1\u0303 and any two feature vectors fj and fk that \u00b5fj = \u00b5fk = 0 and \u03c3fj = \u03c3fk = 1/ \u221a n, if |\u03c1(fj , fk)| \u2265 1 \u2212 \u03c4 and fj is the support feature with score |cj | = |fj\u03b1\u0303|, then the score of the feature fk will satisfy |ck| = |fk\u03b1\u0303| \u2265 |cj | \u2212 \u221a 2\u03c4\u2016\u03b1\u0303\u2016.\nProof. From Proposition 1, we know that | |fj\u03b1\u0303| \u2212 |fk\u03b1\u0303| | \u2264 \u221a 2\u03c4\u2016\u03b1\u0303\u2016, if |\u03c1| > 1\u2212 \u03c4 . Since fj is the support feature,\nso |fj\u03b1\u0303| \u2265 |fk\u03b1\u0303|, we have |fk\u03b1\u0303| \u2265 |fj\u03b1\u0303| \u2212 \u221a 2\u03c4\u2016\u03b1\u0303\u2016. This completes the proof.\nThe above theorem says that if two features are highly correlated, their scores will be very close. In other words, for a given support feature fj , the feature with a score lower than |cj | \u2212 \u221a 2\u03c4\u2016\u03b1\u0303\u2016 shall not be considered as an affiliated feature to fj , at the correlation level of (1 \u2212 \u03c4 ). Based on this, the worst case analysis can be conducted in Algorithm 2, which is termed here as Correlation Redundancy Matching (CRM). The basic idea of CRM is that, for every iteration, we first find the support features with larger feature scores. Once the support features are identified, the corresponding affiliated features are then identified from the remaining unselected features. The whole procedure is repeated until a maximum of B numbers of support features (see (2)) is selected. Thus, the computational cost of this worst case analysis can be substantially reduced, which we will go through details in the upcoming subsection.\nProposition 2. With the Correlation Redundancy Matching algorithm, Problem (6) can be globally solved.\nProof. From Algorithm 2, once a support feature fz is identified, all corresponding correlated features of fz (features with scores (|ch| > |cz|\u2212 \u221a 2\u03c4\u2016\u03b1\u2016)) will be identified and stored inQ. This fact also implies that all the subsequently selected support features will not be correlated to any of the previously selected support feature. Finally, the top scoring features that satisfy the constraint in (2) then form the support features, hence max\u03b4,fj /\u2208XQ 1 2\u2016 \u2211n\ni=1 \u03b1iyi(xi \u03b4)\u20162 will be maximized. Inductively, it becomes possible to conclude that the proposed CRM algorithm can solve (6) globally and exactly. This completes the proof.\nThe following theorem indicates that the proposed algorithm can globally converge and exhibits the nonmonotonic property for feature selection.\nTheorem 2. Given that in each iteration of Algorithm 1, the reduced minimax subproblem (5) and the most active constraint selection (6) can be exactly and globally solved, Algorithm 1 stops after a finite number of iterations with a global solution of (4).\nThe proof can be adapted from (Tan et al., 2010)."}, {"heading": "3.5. Complexity Analysis", "text": "As finding the most violated \u03b4 can be obtained exactly via the Correlation Redundancy Matching algorithm, which firstly sorts the m features, followed by scanning of the B features and then computes the PCC w.r.t. the other features. Here, sorting takes O(m logm) and finding the support and affiliated features consumes O(Bmn). Therefore, with T iterations on hand, the overall time complexity for CRM is O(T (m logm+Bmn)). As there are at most TB selected features, the training time complexity to solve (5) isO(TBn). Note, by taking benefits from the cutting plane strategy, a relative small T is needed for convergence: as it is noted to converge well within 10 iterations in the experimental studies."}, {"heading": "4. Experiments", "text": "In this section, we conduct experiments to study the feature selection performances of several state-of-the-art methods, including: 1) ReliefF (Robnik-Sikonja & Kononenko, 2003), 2) mRMR2 (Peng et al., 2005), 3) FCBF3 (Yu & Liu, 2003), 4) RCFS (Zhou et al., 2010), 5) SVMRFE (Guyon & Elisseeff, 2003), 6) L1-SVM4 (Yuan et al., 2011), 7) FGM5 (Tan et al., 2010), and 8) our proposed GDM using only support features for prediction. The first four algorithms belong to filter methods. SVM-RFE is a wrapper method, while the last three are embedded methods. For fair comparisons, all methods except ReliefF, which is integrated in MATLAB R2011b, are implemented in C++ with MATLAB interface. Moreover, the parameters of these methods are configured as suggested by the respective authors. For ReliefF, we report the best results of with K \u2208 {1, 2, 3, . . . , 100} (KNN classifier). Further, C is configured to 1 for all methods except L1SVM, where C varies with different number of selected features. In the experimental study, we set our \u03c4 = 0.25 and consider 10, 20, . . . , 200 features for each method and report the corresponding resultant training time. Therefore, to facilitate a fair comparison, standard SVM classifier is used to judge the accuracy according to the number of se-\n2http://penglab.janelia.org/proj/mRMR. 3http://www.cs.man.ac.uk/\u02dcgbrown/fstoolbox. 4http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear. 5http://c2inet.sce.ntu.edu.sg/Mingkui/FGM.htm.\nlected features and their indexes. Further, all experiments are conducted on a PC with Intelr CoreTM i7 Processor and 24.0GB memory under Windows Serverr 2008. To evaluate the feature selection performances, three criteria, namely, 1) Classification Accuracy, 2) Training Time Complexity and 3) Redundancy Rate are considered. Following the consistent definition of (Zhao et al., 2012), and assuming F is the set of selected feature subset with size m\u0302, the redundancy rate can be measured by RED(F) = 1m\u0302(m\u0302\u22121) \u2211 fi,fj\u2208F,i>j |\u03c1(fi, fj)|. The metric assesses the averaged correlation among all selected feature pairs. Obviously, for the same accuracy level, a smaller redundancy rate is deem to be more meaningful."}, {"heading": "4.1. Evaluation on Synthetic Data", "text": "To illustrate the mechanisms of the proposed method, we first conduct a study on a synthetic dataset where the ground truth correlated features are known in advance. The data contains 2,048 observations and 10,000 features, and the predefined predictive features are categorized as 200 feature groups of different sizes, others then serve as noise. Moreover, each of the 30 out of 200 feature groups contains some inner highly correlated features. While in the remaining 170 groups, there is only one feature per group. The predictive ability of each group follows a normal distribution N(0, 1). Noticed that the way of constructing testing set is consistent with training. Our goal is to assess whether a method can correctly identify the relevant members of the ground truth feature set. Nevertheless, for this synthetic data, when the correlated features are truly redundant in the data, removing it would lead the classifier to achieve high accuracy performances. To this end, we manually remove the redundant features directly from the data, (i.e., based on the ground truth available) to solve (6). So that correlation\nconstraint on \u03c1(fi, fj) has been imposed. Then we name it as \u201cMethodA\u201d. The experimental results obtained from the synthetic dataset are presented in Figure 2, wherein dash line represented the \u201cMethodA\u201d. As expected, we observe from Figure 2(a) that the proposed method outperforms others in most cases; while filter methods such as ReliefF and mRMR achieve the worst prediction performances. Among the filter methods considered, RCFS achieve the highest accuracy since the use of feature clustering helped remove redundancy among features. However, since filter method such as RCFS does not take the classifier into consideration in the feature selection process, the classification accuracy is generally lower than the wrapper and embedded counterparts as expected. FGM outperforms L1SVM but is noted to be competitive to SVM-RFE when small portion of the features are selected. However, due to the non-convexity optimization formulation of SVM-RFE, which suffers from correlated features, it underperforms FGM when large amount features are considered. Moreover, as FGM imposes a tight convex approximation in the `0-model (Tan et al., 2010), it can be observed from Figure 2(a) that both FGM and GDM achieve competitive accuracy result when the number of selected features approaches the ground truth. Thus, it is possible to conclude that the potential of the proposed method can correctly identify the ground truth feature groups. The training time incurred by all the methods considered is also reported in Figure 2(b). It can be observed that FGM emerges as the fastest among all, while ReliefF incurs the highest training effort. Due to the high cost of feature clustering involving 10,000 features, RCFS also consumes significant training time. Though the proposed GDM incurs slightly more time than FGM, it is noted to be more efficient than the state-of-the-art L1-SVM. In addition, the results on metric Redundancy Rate are depicted in Figure 2(c). Overall, GDM achieves competitive low redundancy rate and superior accuracy performance on the synthetic problem considered."}, {"heading": "4.2. Evaluation on Real-world Data", "text": "To assess the practical performance of all feature selection methods, we include a variety of datasets, in both data scale and dimension, which consist of two digit recognition datasets: mnist7, usps6 and two other very high dimensional datasets. The first one is the challenge dataset\n6http://c2inet.sce.ntu.edu.sg/ivor/cvm.html.\nkdd20107 used in the educational data mining competition and the second is the spam webpage data webspam7. Detailed information about the datasets is listed in Table 1. For kdd2010, we use 10,000 points as the training set and maintain the original testing set. On the webspam data, we randomly select 80,000 points as the training set, while 70,000 for testing. To provide further evidence on the robustness performance of each method, we set 50% as the minimum for accuracy expectation and 1 hour as the maximum training time for all the experiments. Figure 3 summarizes the accuracy performance attained by the various methods. By appropriately identifying the support feature set, GDM obtains superb accuracy improvements on mnist, kdd2010, and webspam. On the other hand, SVM-RFE performs well only for small number of selected features while deteriorating hereafter as observed in Figure 3(b). When the correlated features are retained to form the affiliated feature groups, the performance of GDM-affiliated is noted to emerge as superior to all the other methods considered. In particular, the results on digit identification in Figure 6 highlights the significance of affiliated feature set, where a comprehensive explanation shall follows later in the discussion and conclusion section. Further, as shown in Figure 5, GDM achieves relatively low redundancy rate in most cases. Although, FCBF and SVM-RFE exhibits lower redundancy rate than GDM on the mnist dataset, the accuracies have been impeded by their high sensitivity to the noise features, as observed from Figure 3(a). Compared to embedded methods, GDM out-\n7http://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets. As to mnist dataset, class \u201c3\u201d and \u201c8\u201d are taken to form a binary classification problem.\nperforms over FGM and L1-SVM in terms of redundancy reduction, thus exhibiting the improved accuracy. This also implies that GDM can identify a good feature subset (Hall, 1999). Since the kdd2010 data is sparse, Figure 4 shows that L1-SVM takes advantage of the sparseness in the data to achieve the shortest training time observed. However, on the webspam data which has more than 8 million features, GDM is noted to learn faster than L1-SVM. Moreover, there exists the situation that other methods fail in handling the data. For example, RCFS is sensitive to the number of points, thus even on the small mnist dataset with only 11,982 points, the method fails to perform well. SVM-RFE could not maintain an average accuracy of 50% on kdd2010. Both ReliefF and SVM-RFE fail to converge on webspam under the 1 hour maximum training time budget. Moreover, since no filter methods can handle very high dimensional data of webspam and kdd2010, the contests only hold among L1-SVM, FGM and GDM."}, {"heading": "5. Discussion and Conclusion", "text": "In this paper, we have presented a comprehensive study on potential correlated features, leading to the concepts of support feature and affiliated feature. While, superior prediction performance is attained through support features, maintaining some feature redundancies as affiliated features, can be useful for enhanced interpretation of the learning tasks while improving prediction robustness. By taking advantage of the cutting plane strategy, the proposed GDM can handle very high dimensional problems in an efficient way. Notably, the affiliated features are constructed in the proposed method without any additional cost, since they are generated along with the support features.\nIn what follows, we conclude with further details on the interpretation of the proposed GDM algorithm along with the affiliated features attained in Figure 6. With respect to the digit identification result of usps8, the regions highlighted by the affiliated features (129 features) can be useful in assisting the human user in identifying a \u201c0\u201d or \u201c1\u201d from the extracted images of the original pictures. This is consistent to the observation discussed earlier in Figure 1, where the feature groups congregate in the regions of the beard, mustache and silhouette of the face to form the affiliated feature groups. Information with great significance are reserved for further processing. Referring to the affiliated features in Figure 6, despite the highest accuracy achieved by SVM-RFE (70 features) as shown in Figure 3(b), the pixels selected correspond only to the background of the image rather than the digits, other methods also cannot manifest the clear structure of entire digits well. To summarize, we have introduced the notion and significance of correlated features, namely the affiliated feature group in the present paper, and have showcased how it can benefits the task of feature selection in general. We aspire to explore novel constraints that are suitable for the discovery of new structures in high dimensional tasks.\n8Identified digital handwritten characters extracted from the images of \u201c0\u201d and \u201c1\u201d were gathered in the usps data."}], "references": [{"title": "Benefitting from the variables that variable selection discards", "author": ["R. Caruana", "V.R. de Sa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Caruana and Sa,? \\Q2003\\E", "shortCiteRegEx": "Caruana and Sa", "year": 2003}, {"title": "Practical Feature Selection: from Correlation to Causality", "author": ["I. Guyon"], "venue": null, "citeRegEx": "Guyon,? \\Q2008\\E", "shortCiteRegEx": "Guyon", "year": 2008}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Guyon and Elisseeff,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff", "year": 2003}, {"title": "Correlation-based Feature Selection for Machine Learning", "author": ["M.A. Hall"], "venue": "PhD thesis, The University of Waikato,", "citeRegEx": "Hall,? \\Q1999\\E", "shortCiteRegEx": "Hall", "year": 1999}, {"title": "Optimizing performance measures for feature selection", "author": ["Q. Mao", "I.W. Tsang"], "venue": "In ICDM,", "citeRegEx": "Mao and Tsang,? \\Q2011\\E", "shortCiteRegEx": "Mao and Tsang", "year": 2011}, {"title": "Cutting-set methods for robust convex optimization with pessimizing oracles", "author": ["A. Mutapcic", "S. Boyd"], "venue": "Optimization Methods & Software,", "citeRegEx": "Mutapcic and Boyd,? \\Q2009\\E", "shortCiteRegEx": "Mutapcic and Boyd", "year": 2009}, {"title": "Trace ratio criterion for feature selection", "author": ["F. Nie", "S. Xiang", "Y. Jia", "C. Zhang", "S. Yan"], "venue": "In AAAI,", "citeRegEx": "Nie et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2008}, {"title": "Featureboost: A meta learning algorithm that improves model robustness", "author": ["J. O\u2019Sullivan", "J. Langford", "R. Caruana", "A. Blum"], "venue": "In ICML,", "citeRegEx": "O.Sullivan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "O.Sullivan et al\\.", "year": 2000}, {"title": "Feature selection based on mutual information: Criteria of max-dependency, max-relevance, and min-redundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Theoretical and empirical analysis of relieff and rrelieff", "author": ["M. Robnik-Sikonja", "I. Kononenko"], "venue": "Mach. Learn.,", "citeRegEx": "Robnik.Sikonja and Kononenko,? \\Q2003\\E", "shortCiteRegEx": "Robnik.Sikonja and Kononenko", "year": 2003}, {"title": "Learning sparse SVM for feature selection on very high dimensional datasets", "author": ["M. Tan", "L. Wang", "I.W. Tsang"], "venue": "In ICML,", "citeRegEx": "Tan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2010}, {"title": "Feature selection for high-dimensional data: A fast correlation-based filter solution", "author": ["L. Yu", "H. Liu"], "venue": "In ICML,", "citeRegEx": "Yu and Liu,? \\Q2003\\E", "shortCiteRegEx": "Yu and Liu", "year": 2003}, {"title": "Redundancy based feature selection for microarray data", "author": ["L. Yu", "H. Liu"], "venue": "In SIGKDD,", "citeRegEx": "Yu and Liu,? \\Q2004\\E", "shortCiteRegEx": "Yu and Liu", "year": 2004}, {"title": "An improved glmnet for l1-regularized logistic regression and support vector machines", "author": ["Yuan", "G.-X", "Ho", "C.-H", "Lin", "C.-J"], "venue": "In SIGKDD,", "citeRegEx": "Yuan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2011}, {"title": "Advancing feature selection research", "author": ["Z. Zhao", "F. Morstatter", "S. Sharma", "S. Alelyani", "A. Anand", "H. Liu"], "venue": "Technical report,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}, {"title": "Feature selection with redundancy-constrained class separability", "author": ["L. Zhou", "L. Wang", "C. Shen"], "venue": "IEEE Trans. Neural Netw.,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}, {"title": "Markov blanket-embedded genetic algorithm for gene selection", "author": ["Z. Zhu", "Y.S. Ong", "M. Dash"], "venue": "Pattern Recognition,", "citeRegEx": "Zhu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": "In general, these methods have been categorized as three core themes (Guyon, 2008): filter methods (Yu & Liu, 2003; Peng et al.", "startOffset": 69, "endOffset": 82}, {"referenceID": 8, "context": "In general, these methods have been categorized as three core themes (Guyon, 2008): filter methods (Yu & Liu, 2003; Peng et al., 2005), wrapper methods (Guyon & Elisseeff, 2003; Zhu et al.", "startOffset": 99, "endOffset": 134}, {"referenceID": 16, "context": ", 2005), wrapper methods (Guyon & Elisseeff, 2003; Zhu et al., 2007) and embedded methods (Yuan et al.", "startOffset": 25, "endOffset": 68}, {"referenceID": 13, "context": ", 2007) and embedded methods (Yuan et al., 2011; Tan et al., 2010; Mao & Tsang, 2011).", "startOffset": 29, "endOffset": 85}, {"referenceID": 10, "context": ", 2007) and embedded methods (Yuan et al., 2011; Tan et al., 2010; Mao & Tsang, 2011).", "startOffset": 29, "endOffset": 85}, {"referenceID": 1, "context": "two sets of parameters: parameter of the learning machine, and parameter to control the feature sparsity (Guyon, 2008).", "startOffset": 105, "endOffset": 118}, {"referenceID": 3, "context": "In other words, correlated features are deemed as redundant, and this redundancy should be minimized (Hall, 1999; Guyon, 2008; Zhao et al., 2011; 2012).", "startOffset": 101, "endOffset": 151}, {"referenceID": 1, "context": "In other words, correlated features are deemed as redundant, and this redundancy should be minimized (Hall, 1999; Guyon, 2008; Zhao et al., 2011; 2012).", "startOffset": 101, "endOffset": 151}, {"referenceID": 14, "context": "In other words, correlated features are deemed as redundant, and this redundancy should be minimized (Hall, 1999; Guyon, 2008; Zhao et al., 2011; 2012).", "startOffset": 101, "endOffset": 151}, {"referenceID": 7, "context": "As also discussed in (O\u2019Sullivan et al., 2000; Caruana & de Sa, 2003; Xu et al., 2012), such feature redundancy has the benefits of bringing about stable generalization performances.", "startOffset": 21, "endOffset": 86}, {"referenceID": 8, "context": "Another notable redundancy reduction method is Minimum Redundancy Maximum Relevance (mRMR) (Peng et al., 2005), which selects the most correlated features to the labels such that they are mutually far apart from each other by maximizing the dependency between the joint distribution of the selected features and the output labels.", "startOffset": 91, "endOffset": 110}, {"referenceID": 6, "context": "Next, a feature subset is further identified from the selected features in each cluster group using graph based feature selection criteria (Nie et al., 2008) that capture the global and local intrinsic structures of the data.", "startOffset": 139, "endOffset": 157}, {"referenceID": 6, "context": "Next, a feature subset is further identified from the selected features in each cluster group using graph based feature selection criteria (Nie et al., 2008) that capture the global and local intrinsic structures of the data. This strategy however is heavily sensitive to the choice of graph Laplacian matrices used. For example, the Laplacian score is usually constructed using K nearest neighbor (KNN). In practice, on very high dimensional problems, KNNs can be very far away from each other in reality due to the effect of the curse of dimensionality. Besides, the high computational cost of feature clustering on high dimensional data and graph based methods (taking O(nm)) make this approach less attractive on large scale data. Recently, Zhao et al. (2012) proposed a framework to unify different criteria for removing feature redundancies.", "startOffset": 140, "endOffset": 764}, {"referenceID": 1, "context": "Similar to (Guyon, 2008), a vector \u03b4 = [\u03b41, .", "startOffset": 11, "endOffset": 24}, {"referenceID": 6, "context": "To limit the number of selected features to be less than B, the `0 constraint \u2016\u03b4\u20160 \u2264 B is imposed for the purpose of feature selection (Nie et al., 2008).", "startOffset": 135, "endOffset": 153}, {"referenceID": 10, "context": "Inspired by (Tan et al., 2010), by applying the minimax optimization theory, one can obtain a tight convex relaxation to (3), which is in the form of the following Quadratically Constrained Quadratic Programming (QCQP) problem:", "startOffset": 12, "endOffset": 30}, {"referenceID": 10, "context": "a reduced active constraint set C can be solved by some efficient QCQP solvers (Tan et al., 2010).", "startOffset": 79, "endOffset": 97}, {"referenceID": 10, "context": "The proof can be adapted from (Tan et al., 2010).", "startOffset": 30, "endOffset": 48}, {"referenceID": 8, "context": "In this section, we conduct experiments to study the feature selection performances of several state-of-the-art methods, including: 1) ReliefF (Robnik-Sikonja & Kononenko, 2003), 2) mRMR2 (Peng et al., 2005), 3) FCBF3 (Yu & Liu, 2003), 4) RCFS (Zhou et al.", "startOffset": 188, "endOffset": 207}, {"referenceID": 15, "context": ", 2005), 3) FCBF3 (Yu & Liu, 2003), 4) RCFS (Zhou et al., 2010), 5) SVMRFE (Guyon & Elisseeff, 2003), 6) L1-SVM4 (Yuan et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 13, "context": ", 2010), 5) SVMRFE (Guyon & Elisseeff, 2003), 6) L1-SVM4 (Yuan et al., 2011), 7) FGM5 (Tan et al.", "startOffset": 57, "endOffset": 76}, {"referenceID": 10, "context": ", 2011), 7) FGM5 (Tan et al., 2010), and 8) our proposed GDM using only support features for prediction.", "startOffset": 17, "endOffset": 35}, {"referenceID": 10, "context": "Moreover, as FGM imposes a tight convex approximation in the `0-model (Tan et al., 2010), it can be observed from Figure 2(a) that both FGM and GDM achieve competitive accuracy result when the number of selected features approaches the ground truth.", "startOffset": 70, "endOffset": 88}, {"referenceID": 3, "context": "This also implies that GDM can identify a good feature subset (Hall, 1999).", "startOffset": 62, "endOffset": 74}], "year": 2012, "abstractText": "In this paper, a novel learning paradigm is presented to automatically identify groups of informative and correlated features from very high dimensions. Specifically, we explicitly incorporate correlation measures as constraints and then propose an efficient embedded feature selection method using recently developed cutting plane strategy. The benefits of the proposed algorithm are two-folds. First, it can identify the optimal discriminative and uncorrelated feature subset to the output labels, denoted here as Support Features, which brings about significant improvements in prediction performance over other state of the art feature selection methods considered in the paper. Second, during the learning process, the underlying group structures of correlated features associated with each support feature, denoted as Affiliated Features, can also be discovered without any additional cost. These affiliated features serve to improve the interpretations on the learning tasks. Extensive empirical studies on both synthetic and very high dimensional real-world datasets verify the validity and efficiency of the proposed method.", "creator": "LaTeX with hyperref package"}}}