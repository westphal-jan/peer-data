{"id": "1212.3631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2012", "title": "Learning efficient sparse and low rank models", "abstract": "Parsimony, the sparsity once likely member, another often shown all successfully model usage before numerous launcher classes making normally equipment tasks. Traditionally, creating workflow interaction more on an iterative algorithm once demonstrable although principles function giving parsimony - promoting policy. The inherently graphs structure they data - dependent computational and loving-kindness an mechanistic formulation constitute a including specified saw many applications restrictions better - time performance which including large - cost data. Another consequence possibly seen those modeling modeling is the difficulty of their respect in goal-oriented different scenarios. In this work, we reforms instead move brought emphasis from held model decided place attempting algorithm, and develop a process - centric viewed fact sagacious modeling, only only of did one-dimensional fixed - variations pursuit solve any type prior payment, step-by-step linear. We show a principled way put uses irreplacable pursued process architectures for structured undistinguished both robust risk given models, derived some the iteration given ulceration descent front-end. These macros think to approximate the detailed judicious represent at from fraction of the complexity of since standard analytic methods. We also this taken not returning imperialist introduce leave naturally extend punctilious marketed to exaggeratedly settings. State - has - through - design results actually progress on several merely confusion year bright several device processing taking notably orders taken magnitude-5 time-line 2.9 even along exact optimization interfaces.", "histories": [["v1", "Fri, 14 Dec 2012 22:50:44 GMT  (2274kb,D)", "http://arxiv.org/abs/1212.3631v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pablo sprechmann", "alex m bronstein", "guillermo sapiro"], "accepted": false, "id": "1212.3631"}, "pdf": {"name": "1212.3631.pdf", "metadata": {"source": "CRF", "title": "Learning Efficient Sparse and Low Rank Models", "authors": ["P. Sprechmann", "\u2217A", "M. Bronstein", "G. Sapiro"], "emails": ["pablo.sprechmann@duke.edu,", "guillermo.sapiro@duke.edu.", "bron@eng.tau.ac.il."], "sections": [{"heading": null, "text": "Parsimony, including sparsity and low rank, has been shown to successfully model data in numerous machine learning and signal processing tasks. Traditionally, such modeling approaches rely on an iterative algorithm that minimizes an objective function with parsimony-promoting terms. The inherently sequential structure and data-dependent complexity and latency of iterative optimization constitute a major limitation in many applications requiring real-time performance or involving large-scale data. Another limitation encountered by these modeling techniques is the difficulty of their inclusion in discriminative learning scenarios. In this work, we propose to move the emphasis from the model to the pursuit algorithm, and develop a process-centric view of parsimonious modeling, in which a learned deterministic fixed-complexity pursuit process is used in lieu of iterative optimization. We show a principled way to construct learnable pursuit process architectures for structured sparse and robust low rank models, derived from the iteration of proximal descent algorithms. These architectures learn to approximate the exact parsimonious representation at a fraction of the complexity of the standard optimization methods. We also show that appropriate training regimes allow to naturally extend parsimonious models to discriminative settings. State-ofthe-art results are demonstrated on several challenging problems in image and audio processing with several orders of magnitude speedup compared to the exact optimization algorithms.\n\u2217P. Sprechmann and G. Sapiro are with the Department of Electrical and Computer Engineering, Duke University, Durham 27708, USA. Email: pablo.sprechmann@duke.edu, guillermo.sapiro@duke.edu. \u2020A. M. Bronsteind is with School of Electrical Engineering, Tel Aviv University, Tel Aviv 69978, Israel.Email: bron@eng.tau.ac.il. \u2021Work partially supported by NSF, ONR, NGA, DARPA, AFOSR, ARO, and BSF.\nar X\niv :1\n21 2.\n36 31\nv1 [\ncs .L\nG ]\n1 4\nD ec"}, {"heading": "1 Introduction", "text": "Parsimony, preferring a simple explanation to a more complex one, is probably one of the most intuitive principles widely adopted in the modeling of nature. The past two decades of research have shown the power of parsimonious representation in a vast variety of applications from diverse domains of science.\nOne of the simplest among parsimonious models is sparsity, asserting that the signal has many coefficients close or equal to zero when represented in some domain, usually referred to as dictionary. The pursuit of sparse representations was shown to be possible using tools from convex optimization, in particular, via `1 norm minimization [1, 2]. Works [3, 4], followed by many others, introduced efficient computational techniques for dictionary learning and adaptation. Sparse modeling is in the heart of modern approaches to image enhancement such as denoising, demosaicing, impainting, and super-resolution, to mention just a few.\nAs many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10]. Such models have been shown useful in the analysis of functional MRI and genetic data for example.\nIn the case of matrix-valued data, complexity is naturally measured by the rank, which also induces a notion of parsimony. A recent series of works have elucidated the beautiful relationship between sparsity and low rank representations, showing that rank minimization can be achieved through convex optimization [11, 12]. The combination of low-rank and sparse models paved the path to new robust alternatives of principal component analysis (RPCA) [13, 14] and nonnegative matrix factorization (RNMF) [15], and addressing challenging matrix completion problems [12]. RPCA was also found useful in important applications such as face recognition and modeling, background modeling, and audio source separation. Another relevant low rank modeling scheme is non-negative matrix factorization (NMF)[16], where the input vectors are represented as nonnegative linear combination of a non-negative under-complete dictionary. NMF has been particularly successful in applications such as object recognition and audio processing."}, {"heading": "1.1 From model-centric to process-centric parsimonious", "text": "modeling\nAll existing parsimonious modeling methods essentially follow the same pattern: First, an objective comprising a data fitting term and parsimony-promoting penalty terms is constructed; next, an iterative optimization algorithm is invoked to minimize the objective, pursuing either the parsimonious representation of the data in a given dictionary, or the dictionary itself. Despite its remarkable achievements, such a model-centric approach suffers from critical disadvantages and limitations.\nThe inherently sequential structure and the data-dependent complexity and latency of iterative optimization tools often constitute a major computational bottleneck. The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems. Despite the permanent progress reported in the literature, the state-of-the-art algorithms require hundreds or thousands of iterations to converge, making their use impractical in scenarios demanding real-time performance or involving large-scale data.\nRelying on the explicit solution of an optimization problem furthermore limits the applicability of parsimonious models in supervised learning scenarios, where the higher-level training objective would depend on the solution of the lower-level pursuit problem. The resulting bilevel optimization problems are notoriously difficult to solve in general; the non-differentiability of the lowerlevel parsimony-inducing objective makes the solution practically impossible [27]. This partially explains why sparse representations, that are so widely adopted for the construction of generative models, had such a modest success in the construction of discriminative models.\nIn this paper, we take several steps to depart from the model-centric ideology relying on an iterative solver by shifting the emphasis from the model to the pursuit process. Our approach departs from the observation that, despite being highly non-linear and hard to compute, the mapping between a data vector and its parsimonious representation resulting from the optimization procedure is deterministic. The curse of dimensionality precludes the approximation of this mapping on all possible, even modestly sized input vectors; however, since real data tend to have low intrinsic dimensionality, the mapping can be inferred explicitly on the support of the distribution of the input data.\nRecently, [28, 29] have proposed to trade off precision in the sparse representation for computational speed-up by learning non-linear regressors capable\nof producing good approximations of sparse codes in a fixed amount of time. However, the large number of degrees of freedom, for which a good initialization is difficult to provide, made this effort only modestly successful. In their inspiring recent paper, [30] showed that a particular network architecture can be derived from the iterative shrinkage-thresholding (ISTA) [17] and proximal coordinate descent (CoD) algorithms [19].\nThese works were among the first to bridge between the optimization based sparse models and the inherently process-centric neural networks, and in particular auto-encoder networks [31, 32], extensively explored by the deep learning community."}, {"heading": "1.2 Contributions", "text": "The main contribution of this paper is to propose a comprehensive framework for process-centric parsimonious modeling. The obtained encoders can be used to produce fast approximants or predictors of optimization based parsimonious models or as modelers in their own right, this is, pursuit processes that might not be minimizing any specific objective function. Specifically, this paper makes four main contributions:\nFirst, in Section 4, we propose a process-centric approach to parsimonious modeling. We begin by proposing a principled way to construct encoders capable of approximating an important family of parsimonious models (briefly described in Section 2), including general sparse coding paradigms (hierarchical and non-overlapping group sparsity), robust PCA, and NMF. By extending the original ideas in [30], we propose tailored pursuit architectures derived from first-order proximal descent algorithms, which are briefly presented in Section 3. Note that unlike the standard sparse coding setting, the exact first-order RPCA and RNMF algorithms cannot be used directly, as each iteration involves the singular value decomposition (SVD). As a remedy, we propose to use an algorithm inspired by the non-convex optimization techniques in [25].\nSecond, this new approach allows the encoders to be trained in an online manner, which makes the fast encoders no more restricted to work with a fixed distribution of input vectors known a priori (limitation existing, for example, in [30]), and removes the need to run the exact algorithms at training. The proposed approach can be used with a predefined dictionary or learn it in an online manner on the very same data vectors fed to it. While differently motivated, in this setting, our framework is related to the sparse autoencoders [32].\nThird, we show that abandoning the iterative minimization in favor of a\nlearned pursuit process allows to incorporate the parsimonious representation into higher-level optimization problems in a natural way. In particular, in Section 5 we show a very simple and efficient extension of the proposed RPCA framework to cases where the data undergo an unknown transformation that is sought for during the pursuit [33]. We also show the construction of discriminative parsimonious models.\nFinally, in Secion 6 we demonstrate our approaches on applications in image classification, face modeling, signal separation and denoising, and speaker identification, where our fast encoders perform similarly to or better than the iterative pursuit processes at a fraction of the complexity of the latter. Faster than real-time state-of-the-art results are achieved in several such applications. The present paper generalizes and gives a more rigorous treatment to results previously published by the authors in [34, 35]."}, {"heading": "2 Parsimonious models", "text": "Let X \u2208 Rm\u00d7n be a give data matrix. In this work, we concentrate our attention on the general parsimonious modeling problem that can be posed as the solution of the minimization problem\nmin 1\n2 \u2016X\u2212DZ\u20162F + \u03c8(Z) + \u03c6(D), (1)\noptimized over Z \u2208 Rq\u00d7n alone or jointly with D \u2208 Rm\u00d7q. Here Z \u2208 Rq\u00d7n is the representation (parsimonious code) of the data in the dictionary, and the penalty terms \u03c8(Z) and \u03c6(D) induce a certain structure of the code and the dictionary, respectively. When the minimization is performed over both the dictionary and the representation, it is non-convex.\nWe will explicitly distinguish between parsimonious coding or representation pursuit problems (representing data with a given model), and the harder parsimonious modeling problem (constructing a model describing given data, e.g., learning a dictionary). In the former, D is fixed and \u03c6(D) is constant. Most useful formulations use convex regularization \u03c8(Z) of the representation.\nIn many relevant applications, the entire data matrix X is not available a priori. The data samples {xt}t\u2208N, xt \u2208 Rm, arrive sequentially; the index t should be interpreted as time. Online parsimonious modeling aims at estimating and refining the model as the data come in [36]. The need for online schemes also arises when the available training data are simply too large to be handled\ntogether. When the regularized function \u03c8 is vector-wise separable,\n\u03c8(Z) = n\u2211 i=1 \u03c8(zi), (2)\nproblem (1) can solved in an online fashion using an alternating minimization scheme. As a new data vector xt is received, we first obtain its representation zt given the current model estimate, Dt\u22121. This is achieved by solving the representation pursuit problem\nzt = argmin z\n1 2 \u2016xt \u2212Dt\u22121z\u201622 + \u03c8(z). (3)\nThen, we update the model using the coefficients, {zj}j\u2264t, computed during the previous steps of the algorithm,\nDt = argmin D t\u2211 j=1 \u03b2j 1 2 \u2016xj \u2212Dzj\u201622 + \u03c6(D), (4)\nwhere \u03b2j \u2208 [0, 1] is a forgetting factor that can be added to rescale older information so that newer estimates have more weight. This can be efficiently solved without remembering all the past codes [36].\nIn what follows, we detail several important instances of (1), for both mod-\neling or pursuit, and how they can be cast as online learning problems."}, {"heading": "2.1 Structured sparsity", "text": "The underlying assumption of sparse models is that the input vectors can be reconstructed accurately as a linear combination of the dictionary atoms with a small number of non-zero coefficients. Sparse models are enforced by using sparsity-promoting regularizers \u03c8(Z). The simplest choice of such a regularizer\nis \u03c8(Z) = \u03bb \u2211n i=1 \u2016zi\u20161 (with \u03bb > 0), for which the pursuit problem can be split into n independent problems on the columns of Z,\nmin z\u2208Rq\n1 2 \u2016x\u2212Dz\u201622 + \u03bb\u2016z\u20161. (5)\nThis is the classical unstructured sparse coding problem, often referred to as Lasso [2] or basis pursuit [1].\nStructured sparse models further assume that the pattern of the non-zero coefficients of Z exhibits a specific structure known a priori. Let A \u2286 {1, . . . , q} denote groups of indices of atoms. Then, we define a group structure, G, as a\ncollection of groups of atoms, G = {A1, . . . , A|G|}. The regularizer corresponding to the group structure is defined as the column-wise sum\n\u03c8G(Z) = n\u2211 i=1 \u03c8G(zi) where \u03c8G(z) = |G|\u2211 r=1 \u03bbr\u2016zr\u20162, (6)\nand zr denotes the subvector of z corresponding to the group of atoms Ar. The regularizer function \u03c8 in the Lasso problem (5) arises from the special case of singleton groups G = {{1}, {2}, . . . , {q}} and setting \u03bbr = \u03bb. As such, the effect of \u03c8G on the groups of z is a natural generalization of the one obtained with unstructured sparse coding: it \u201cturns on\u201d and \u201coff\u201d atoms in groups according to the structure imposed by G. Several important structured sparsity settings can be cast as particular cases of (6): Group sparse coding, a generalization of the standard sparse coding to the cases in which the dictionary is sub-divided into groups [5], in this case G is a partition of {1, . . . , q}; Hierarchical sparse coding, assuming a hierarchical structure of the non-zero coefficients [9, 7, 8]. The groups in G form a hierarchy with respect to the inclusion relation (a tree structure), that is, if two groups overlap, then one is completely included in the other one; Overlapping group sparse coding, relaxing the hierarchy assumption so that groups of atoms Ar are allowed to overlap. This model was found to be successful in modeling gene expression and other genetic data. Note that in all the above cases, the structure is repeated across the columns of Z; consequently, the structured sparse coding problem (1) can be split into n independent problems operating on the columns of X and Z. One possible way of extending sparse models is by imposing structure on sub-matrices of Z. Collaborative sparse coding generalizes the concept of structured sparse coding to collections of input vectors by promoting given patterns of non-zero elements in the coefficient matrix [9, 37]."}, {"heading": "2.2 Low-rank models and robust PCA", "text": "Another significant manifestation of parsimony typical to many classes of data is low rank. The classical low rank model is principal component analysis (PCA), in which the data matrix X \u2208 Rm\u00d7n (each column of X is an m-dimensional data vector), is decomposed into X = L + E, where L is a low rank matrix and E is a perturbation matrix.\nPCA is known to produce very good results when the perturbation is small [38]. However, its performance is highly sensitive to the presence of samples not following the model; even a single outlier in the data matrix X can render\nthe estimation of the low rank component arbitrarily far from the true matrix L. [13, 14] proposed to robustify the model by adding a new term to the decomposition to account for the presence of outliers, X = L + O + E, where O is an outlier matrix with a sparse number of non-zero coefficients of arbitrarily large magnitude. In one of its formulations, the robust principal component analysis can be pursued by solving the convex program\nmin L,O\u2208Rm\u00d7n\n1 2 \u2016X\u2212 L\u2212O\u20162F + \u03bb\u2217\u2016L\u2016\u2217 + \u03bb\u2016O\u20161. (7)\nThe same way the `1 norm is the convex surrogate of the `0 norm (i.e., the convex norm closest to `0), the nuclear norm, denoted as || \u00b7 ||\u2217, is the convex surrogate of matrix rank. The parameter \u03bb\u2217 controls the tradeoff between the data fitting error and the rank of the approximation.\nIn [11] it was shown that the nuclear norm of a matrix of L can be reformu-\nlated as a penalty over all possible factorizations\n\u2016L\u2016\u2217 = minA,B 1 2 \u2016A\u20162F + 1 2 \u2016B\u20162F s.t. AB = L, (8)\nThe minimum is achieved through the SVD of L = U\u03a3VT : the minimizer of (8) is A = U\u03a3 1 2 and B = \u03a3 1 2 V. This factorization has been recently exploited in parallel processing across multiple processors to produce state-ofthe-art algorithms for matrix completion problems [25], as well as an alternative approach to robustifying PCA in [39].\nIn (7), neither the rank of L nor the level of sparsity in O are assumed known a priori. However, in many applications, it is a reasonable to have a rough upper bound of the rank, say rank(L) \u2264 q. Combining this with (8), it was proposed in [39] to reformulate (7) as\nmin D0,S,O\n1 2 \u2016X\u2212D0S\u2212O\u20162F + \u03bb\u2217 2 (\u2016D0\u20162F + \u2016S\u2016 2 F) + \u03bb \u2016O\u20161 , (9)\nwith D0 \u2208 Rm\u00d7q, S \u2208 Rq\u00d7n, and O \u2208 Rm\u00d7n. This new factorized formulation reduces the number of optimization variables and reveals much structure hidden in the problem. The low rank component can now be thought of as an undercomplete dictionary D0, with q atoms, multiplied by a matrix S containing in its columns the corresponding coefficients for each data vector in X. This interpretation allows to write problem (7) in the form of our general parsimonious models (1), with Z = (S; O), \u03c8(Z) = \u03bb\u22172 \u2016S\u2016 2 F + \u03bb \u2016O\u20161, D = (D0, Im\u00d7m), and \u03c6(D) = \u03bb\u22172 \u2016D0\u2016 2 F. Furthermore, unlike the nuclear norm, this new formulation of the rank-reducing regularization is differentiable and vector-wise separable\n(well suited for the online setting (3)). However, problem (9) is no longer convex. Fortunately, it can be shown that any stationary point of (9), {D0,S,O}, satisfying ||X \u2212 D0S \u2212 O||2 \u2264 \u03bb\u2217 is an globally optimal solution of (9) [40]. Thus, problem (9) can be solved using an alternating minimization, as in our online setting, without the risk of falling into a stationary point that is not globally optimal."}, {"heading": "2.3 Non-negative matrix factorization", "text": "Another popular low rank model is non-negative matrix factorization (NMF). Given a non-negative data matrix X \u2208 Rm\u00d7n, NMF aims at finding a factorization X \u2248 DZ into non-negative matrices D \u2208 Rm\u00d7q and Z \u2208 Rq\u00d7n, with q \u2264 n. The factorization is obtained by solving the highly non-convex problem\nmin D,Z\u22650\n\u2016X\u2212DZ\u20162F . (10)\nProblem (10) can be stated as particular instance of (1) by setting \u03c8 and \u03c6 to be the sum of element-wise indicator functions of the form\ni+(t) = { 0 : t \u2265 0 \u221e : t < 0. (11)\nThe non-negativity constrain has been shown to be crucial for learning a part representation of the data, making it particularly attractive in the problem of source separation. An extensive amount of work reported in the literature has been devoted to regularizing (10) in meaningful ways.\nSimilarly to PCA, NMF is sensitive to outliers in the data matrix. A robust variant can be obtained by adding a sparse outlier term to the decomposition, X \u2248 D0S+O, as done in the RPCA model [15]. Again here the problem can be cast as a particular instance of (1) defining Z = (S; O) and D = (D0, Im\u00d7m).\nNMF is by construction a low rank representation, since the number of atoms in D0 is normally chosen to be significantly smaller than the dimension of the data. This means that an upper bound of the rank of the approximation needs to be known beforehand. NMF is known to be very sensitive to this parameter, due to the natural compromise between richness of the model and over-fitting. In most practical settings, q is carefully chosen based on empirical evidence. In [35], we proposed a cure to this phenomenon by incorporating a rank-reducing term into (10), establishing in this way a link between NMF and the RPCA problem (9). Combined with the outlier term, we can formulate a robust lowrank NMF problem\nmin D0,S,O\u22650\n\u2016X\u2212D0S\u2212O\u20162F + \u03bb\u2217 \u2016D0S\u2016\u2217 + \u03bb \u2016O\u20161 . (12)\nThe resemblance of (12) to the RPCA problem tempts to apply the reasoning we used before to get rid of the nuclear norm, adding non-negativity constraints to (9),\nmin D0,S,O\u22650\n1 2 \u2016X\u2212D0S\u2212O\u20162F + \u03bb\u2217 2 (\u2016D0\u20162F + \u2016S\u2016 2 F) + \u03bb \u2016O\u20161 . (13)\nHowever, unlike the RPCA case, problems (12) and (13) are not equivalent as the minimum of (8) is not necessarily attained by non-negative factors. In fact, adding non-negativity constraints to (8) produces\n||D0S||\u2217 \u2264 1\n2 min A,B\u22650\n{ ||A||2F + ||B||2F s.t. AB = D0S } \u2264 1\n2 \u2016D0\u20162F +\n1 2 \u2016S\u20162F . (14)\nThus, the sum of the Frobenius norms of the non-negative matrices D and S gives an upper bound on the nuclear norm of their product. While not being fully equivalent to (12), the objective in problem (13) still achieves both robustness to outliers and rank regularization. With some abuse of terminology, we will refer to problem (13) as to RNMF. Again here, (13) is a particular instance of (1) well suited for the online setting."}, {"heading": "3 Proximal methods", "text": "Proximal splitting is a powerful optimization technique allowing to efficiently solve a variety of optimization problems, such as non-smooth convex programs. They have been adopted by the machine learning and signal processing communities for their simplicity, convergence guarantees, and the fact that they are well suited for tackling sparse and structured sparse coding problems that can be written as (1) (refer to [22] for recent reviews). In Section 4 we will use these algorithms to construct efficient learnable pursuit processes.\nProximal splitting methods are designed for solving optimization problems in which the cost function can be split into the sum of two terms, one convex and differentiable with an \u03b1-Lipschitz continuous gradient, and another convex extended real valued and possibly non-smooth. Clearly, pursuit problems (3) fall into this category: the convex quadratic data fitting term has a linear gradient DT(Dz\u2212 x) with the Lipschitz constant given by the squared spectral norm of the dictionary, \u03b1 = \u2016D\u20162, and the regularizer \u03c8 is typically convex and nonsmooth. The proximal splitting method with fixed constant step defines a series\ninput : Data x, dictionary D, weights \u03bb. output: Sparse code z. Define H = I\u2212 1\u03b1DTD, W = 1\u03b1DT, t = 1\u03b1\u03bb. Initialize z0 = 0 and b0 = Wx. for k = 1, 2, . . . until convergence do\nzk+1 = \u03c0t(b k)\nbk+1 = bk + H(zk+1 \u2212 zk) end\nAlgorithm 1: Iterative shrinkage-thresholding algorithm (ISTA).\nof iterates, {zk}k\u2208N,\nzk+1 = \u03c0\u03b1\u03c8(z k \u2212 1\n\u03b1 DT(Dzk \u2212 x)), (15)\nwhere\n\u03c0\u03b1\u03c8(z) = argmin u\u2208Rm\n||u\u2212 z||22 + \u03b1\u03c8(u) (16)\ndenotes the proximal operator of \u03c8. Fixed-step algorithms have been shown to have relatively slow sub-linear convergence, and many alternatives have been studied in the literature to improve the convergence rate [18, 20]. Accelerated versions of the fixed-step algorithm can be used to reach linear convergence rates (the best possible for the class of first order methods). The discussion of theses methods is beyond of the scope of this paper.\nProximal splitting methods become particularly interesting when the proximal operator of \u03c8 can be computed exactly and efficiently. Many important cases of structured sparsity fall into this category. For the simple unstructured sparsity models induced by regularizers of the form\n\u03c8(z) = \u2016\u03bb z\u20161 = m\u2211 i=1 \u03bbizi,\nwith denoting element-wise multiplication, the proximal operator reduces to the element-wise scalar soft-thresholding operator, (\u03c0\u03bb(z))i = \u03c4\u03bbi(zi), with \u03c4\u03bb(t) = sign(t) max{0, |t| \u2212 \u03bb}. In this case, the fixed step proximal splitting algorithm corresponds to the popular iterative shrinkage-thresholding algorithm (ISTA) [17, 18] summarized in Algorithm 1. Note that the matrices H and W in Algorithm 1 are derived from the linear gradient of the data term.\nIf z is furthermore constrained to be non-negative, as the sparse outlier in the RNMF problem, the soft thresholding is replaced by its one-sided counterpart \u03c4+\u03bb (t) = max{0, t \u2212 \u03bb}. The proximal operator of the indicator function i+(t)\ninput : Data x, dictionary D0, weights \u03bb, \u03bb\u2217. output: Approximation l, outlier o.\nDefine H = I\u2212 1\u03b1\n( DT0 D0 + \u03bb\u2217I D T 0\nD0 (1 + \u03bb\u2217)I\n) , W = 1\u03b1 ( DT0 I ) , and\nt = \u03bb\u03b1\n( 0\n1\n) .\nInitialize z0 = 0, b0 = Wx. for k = 1, 2, . . . until convergence do\nzk+1 = \u03c0t(b k)\nbk+1 = bk + H(zk+1 \u2212 zk) end\nSplit zk+1 = (s; o) and output l = D0s. Algorithm 2: Proximal descent algorithm for the online RPCA and RNMF problems with fixed dictionary D0. The distinction between the two models is obtained through the selection of the proximal operator: \u03c0t(b) = \u03c4t(b) (RPCA) and \u03c0t(b) = \u03c4 + t (b) (RNMF).\nimposing the non-negativity constraint is simply \u03c4+0 (t) = max{0, t}. The fixedstep proximal descent algorithm for the online RPCA and RNMF problems is summarized in Algorithm 2.\nTo generalize the proximal operator to the structured sparsity case, let us first consider an individual term in the sum (6), \u03c8r(z) = \u03bbr \u2016zr\u20162. Its proximal operator, henceforth denoted as \u03c0\u03bbr , can be computed as,\n(\u03c0\u03bbr (z))s =  zr \u2016zr\u20162 \u03c4+\u03bbr (\u2016zr\u20162) : s = r,\nzr : s 6= r, (17)\nwhere sub-indices r and s denote a sub-vector of the qr-dimensional vector specified by the group of atoms Ar.\nNote that \u03c0\u03bbr applies a group soft thresholding to the coefficients belonging to the r-th group and leaves the remaining ones unaffected. For a nonoverlapping collection of groups G, the proximal operator of \u03c8G is group-separable and can be computed independently for each group as\n\u03c0G\u03bb(z) = ((\u03c0\u03bb1(z))1; \u00b7 \u00b7 \u00b7 ; (\u03c0\u03bb|G|(z))|G|),\nwhere \u03bb = (\u03bb1, . . . , \u03bb|G|) T is the vector with the threshold parameters \u03bbr.\nIn general, when the groups of G overlap, there is no efficient way of computing the proximal operator of \u03c8G . An important exception to this is the\nhierarchical setting with tree-structured groups. Let us be given a tree hierarchy of groups G = G1 \u222a \u00b7 \u00b7 \u00b7 \u222a GL with each Gl aggregating rl non-overlapping groups corresponding to the l-th level of the tree. Then, the proximal operator of \u03c8G can be shown to be given by the composition of the proximal operators of \u03c8Gl in ascending order from the leaves to the root [8, 9], \u03c0 G = \u03c0G1\u03bb1 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c0 GL \u03bbL . Here \u03bbl \u2208 Rrl denotes the sets of weights corresponding to the constituent group of each level. A particular case of the tree-structured hierarchical sparse model is the two-level HiLasso model introduced to simultaneously promote sparsity at both group and coefficient level [9, 41]. Algorithm 1 is straightforward to generalize to the case of hierarchical sparsity by using the appropriate proximal operator.\nIt is worthwhile noting that the update in Algorithm 1 can be applied to a single element (or group in case of structured sparsity) at a time in a (block) coordinate manner. Several variants of coordinate descent (CoD) and blockcoordinate descent (BCoD) proximal methods have been proposed [42, 19]. Typically, one proceeds as in Algorithm 1, first applying the proximal operator y = \u03c0(bk). Next, the residual e = y \u2212 zk is evaluated, and the group is selected e.g. according to r = arg maxr \u2016er\u20162 (in case of unstructured sparsity, r = arg maxr |er|). Then, bk+1 is computed by applying H only to the selected subgroup of e, and zk+1 is computed by replacing the subgroup of zk with the corresponding subgroup of y."}, {"heading": "4 Learnable pursuit processes", "text": "The general parsimonious modeling problem (1) can be alternatively viewed as the minimization problem\nmin z:Rm\u2192Rq x:Rq\u2192Rm\n1 n n\u2211 i=1 L(xi, z,x), (18)\nwith L(x, z,x) = 12 \u2016(id\u2212 x \u25e6 z)(x)\u2016 2 F + \u03c8(z(x)) + \u03c6(x). The optimization is now performed over an encoder z = z(x) mapping the data vector x to the representation vector z, and a decoder x = x(z) performing the converse mapping. The encoder/decoder pair is sought to make the composition x \u25e6 z close to the identity map, under the regularity constraints promoted by the penalties \u03c8 and \u03c6.\nExisting parsimonious models restrict the decoder to the class of linear functions xD(z) = Dz parametrized by the dictionary D. For a fixed dictionary D,\nthe optimal encoder is given by\nz\u2217 = arg min z:Rm\u2192Rq L(x, z,xD), (19)\nwhich is nothing but the solution of the representation pursuit problem obtained through and iterative optimization algorithm, such as the proximal methods described in Section 3. This interpretation is possible since the solution of the pursuit problem implicitly defines a deterministic mapping that assigns to each input vector x \u2208 Rn a unique parsimonious code z \u2208 Rm. Naturally, this mapping cannot be stated explicitly.\nIn contrast, the process-centric approach proposed in this work, aims at formulating a modeling scheme were both the encoder and the decoder can be explicitly stated and efficiently computed. In our proposed framework, the encoders are constructed explicitly as parametric deterministic functions, z\u0398 : Rn \u2192 Rm with a set of parameters collectively denoted as \u0398, while the decoders are the exact same simple linear decoders, xD(z) = Dz, used in model-centric approaches (we relax this assumption in the following section). We denote by F the family of the parametric functions z\u0398. Naturally, two fundamental question arise: how to select the family F capable of defining good parsimonious models, and how to efficiently select the best parameters \u0398 given a specific family. We will refer to the first problem as to selecting the architecture of the pursuit process, while the second will be referred to as process learning. We start with the latter, deferring the architecture selection to Section 4.3."}, {"heading": "4.1 Process learning", "text": "With the process-centric perspective in mind, problem (18) can be stated as\nmin z\u0398\u2208F,D\u2208Rm\u00d7q\n1 n n\u2211 i=1 L(xi, z\u0398,xD), (20)\nwhere the family F imposes some desired characteristics on the encoder such as continuity and almost everywhere differentiability, and certain computational complexity. As in (1), this problem can be naturally solved using an alternating minimization scheme, sequentially minimizing for z\u0398 or D while leaving the other one fixed. Note that when the encoder z\u0398 is fixed, the problem in D remains essentially the same dictionary update problem and can be solved exactly as before. In what follows, we therefore concentrate on solving the process learning problem\nmin z\u0398\u2208F\n1 n n\u2211 i=1 L(xi, z\u0398), (21)\ngiven a fixed dictionary. To simplify notation, we henceforth omit xD from L whenever D is fixed.\nObserve that problem (21) attempts to find a process z\u0398 in the family F that minimizes the empirical risk over a finite set of training examples, as an approximation to the expected risk\n \u0302L(z\u0398) = 1\nn n\u2211 i=1 L(xi, z\u0398) \u2248 \u222b L(x, z\u0398)dP (x) = L(z\u0398)\nover the data distribution P . While the empirical risk measures the encoder performance over the training set, the expected risk measures the expected performance over new data samples following the same distribution, that is, the generalization capabilities of the model. When the family F is sufficiently restrictive, the statistical learning theory justifies minimizing the empirical risk instead of the expected risk[43]. We will come back to this issue in Section 4.2, where we address the accuracy of the proposed encoders in approximating L(z\u0398).\nWhen the functions belonging to F are almost everywhere differentiable with respect to the parameters \u0398, stochastic gradient descent (SGD) can be used to optimize (21), with almost sure convergence to a stationary point [44]. At each iteration, a random subset of the training data, {xi1 , . . . ,xir}, is selected and used to produce an estimate of the (sub)-gradient of the objective function. Specifically, in our case the parameters of z\u0398 are updated as\n\u0398\u2190 \u0398\u2212 \u00b51 r r\u2211 k=1 \u2202L(xik , z\u0398) \u2202\u0398 , (22)\nwhere \u00b5 is a decaying step, repeating the process until convergence. This requires the computation of the (sub)-gradients \u2202L/\u2202\u0398, which is achieved by a back-propagation procedure as detailed in the sequel. SGD algorithms scale well to big data applications where the limiting factor is the computational time rather than the number of available samples."}, {"heading": "4.2 Approximation accuracy", "text": "Following [44], we split the process training approximation error into three terms, = app + est + opt. The approximation error app = E{ L(z\u2217\u0398) \u2212 L(z\u2217)} measures how well the optimal unrestricted pursuit process z\u2217 given by (19) is approximated by the optimal pursuit process restricted to F , z\u2217\u0398 = arg minz\u0398\u2208F L(z\u0398). The estimation error est = E{ L(z\u0302\u2217\u0398)\u2212 L(z\u2217\u0398)} with z\u0302\u2217\u0398 = arg minz\u0398\u2208F  \u0302L(z\u0398) measures the cost of optimizing the empirical risk instead of the expected risk. Finally, the optimization error opt = E{ L(z\u0302\u0398)\u2212 L(z\u0302\u2217\u0398)}\nmeasures the effect of having z\u0302\u0398 that minimizes the empirical risk only approximately.\nThe estimation error vanishes asymptotically with the increase of the training set size n. The optimization error can be made negligible (at least, in the offline setting) by simply increasing the number of SGD iterations. Consequently, the quality of the learned pursuit process largely depends on the choice of the family F , which we will address in the next section."}, {"heading": "4.3 Process architecture", "text": "We extend the ideas introduced in [30] to derive families of trainable pursuit processes from proximal methods. Let us examine a generic fixed-step proximal descent algorithm described in Section 3. Each iteration can be described as a function receiving the current state (bin, zin) and producing the next state (bout, zout) by applying the non-linear transformation zout = \u03c0t(bin) (representing the proximal operator), and the linear transformation bout = bin + H(zout \u2212 zin) (representing the linear part of the gradient). This can be described by the function (bout, zout) = fH,t(bin, zin) parametrized by the matrix H describing the linear transformation, and the vector t describing the parameters of the proximal operator \u03c0t.\nA generic fixed-step proximal descent algorithm can therefore be expressed as a long concatenation of such iterations, z\u2217(x) = \u00b7 \u00b7 \u00b7\u25e6fH,t \u25e6\u00b7 \u00b7 \u00b7\u25e6fH,t(Wx,0), with the initialization (bin, zin) = (Wx,0). For example, Algorithms 1 and 2 follow this structure exactly, for the appropriate choice of the parameters. Blockcoordinate proximal descent algorithms operate very similarly except that the result of the application of f is substituted to a subset of the elements of the state vector, selected in a state-dependent manner at each iteration.\nFollowing [30], we consider the family of pursuit processes derived from truncated proximal descent algorithms with T iterations, FT = {zT,\u0398(x) = fH,t \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 fH,t(Wx,0)}. For convenience, we collected all the process parameters into a single pseudo-vector \u0398 = {W,H, t}. Also note that at the last iteration, only z of the state vector (b, z) is retained; with some abuse of notation, we still denote the last iteration by f . Finally, we denote by F\u221e the family of untruncated processes.\nA process zT,\u0398 \u2208 FT can be thought of as a feed-forward neural network with identical layers fH,t. Flow diagrams of processes derived from the proximal descent Algorithms 1 and 2 for the Lasso and RPCA/RNMF problems are depicted in Figure 1. Since the processes are almost everywhere C1 with\nrespect to the input x and the parameters \u0398, it is possible to calculate the sub-gradients required for the optimization algorithms. The computation of the sub-gradients of L(x, z\u0398(x)) with respect to \u0398 is carried out by an iterated application of the chain rule starting at the output and propagating backward into the network. The procedure, frequently referred to as back-propagation, is detailed in Algorithm 3, where following the standard notation from the neural network literature, the \u03b4 prefix denotes the gradient of L with respect to the variable following it, \u03b4\u2217 = \u2202L/\u2202\u2217."}, {"heading": "4.4 Approximation error vs. complexity trade-off", "text": "Since the objective minimized via proximal descent is convex, it is guaranteed that there exists some selection of the parameters \u0398\u2217 such that limT\u2192\u221e zT,\u0398\u2217 = z\u2217. In other words, the optimal process z\u2217 is contained in F\u221e. Furthermore, reformulating the non-asymptotic convergence analysis from [18] in our language, the following holds:\nTheorem 1 (Beck&Teboulle). For every D, there exists \u0398\u2217 and C > 0 such that for every z and T \u2265 1, L(x, zT,\u0398\u2217)\u2212 L(x, z\u2217) \u2264 C\n2T \u2016z\u2217(x)\u201622.\nThis result is worst-case, in the sense that it holds for every input vector x.\ninput : Sub-gradient \u03b4z = \u2202L(zT )\n\u2202z .\noutput: Sub-gradients of L with respect to the parameters, \u03b4H, \u03b4W, \u03b4t;\nand with respect to the input, \u03b4x.\nInitialize \u03b4tT = \u2202\u03c0(bT )\n\u2202t \u03b4z, \u03b4bT =\n\u2202\u03c0(bT )\n\u2202b \u03b4z, \u03b4HT = 0, and \u03b4zT = 0\nfor k = T \u2212 1, T \u2212 2, . . . , 1 do \u03b4Hk\u22121 = \u03b4Hk + \u03b4bk(zk+1 \u2212 zk)T\n\u03b4tk\u22121 = \u03b4tk + \u2202\u03c0(bk)\n\u2202t (HT\u03b4bk \u2212 \u03b4zk)\n\u03b4zk\u22121 = \u2212HT\u03b4bk \u03b4bk\u22121 = \u03b4bk + \u2202\u03c0(bk)\n\u2202t HT\u03b4bk +\n\u2202\u03c0(bk)\n\u2202b \u03b4zk\nend Output \u03b4H = \u03b4H0, \u03b4t = \u03b4t0, \u03b4W = \u03b4bkxT, and \u03b4x = WT\u03b4bk\nAlgorithm 3: Computation of the sub-gradients of L(x, zT,\u0398(x)) for a pursuit process zT,\u0398 \u2208 FT . The reader is warned not to confuse the T -th iteration index, zT , with the transpose, zT.\nAssuming bounded support of the input distribution and taking expectation with respect to it, we obtain the following:\nCorollary 1. There exist \u0398\u2217 and C > 0 such that for T \u2265 1,\napp = E{ L(zT,\u0398\u2217)\u2212 L(z\u2217)} \u2264 C\n2T .\nIn other words, the family FT of pursuit processes allows to set the approximation error to an arbitrarily small number.\nOne may wonder whether the iter we have undergone so far is of any value at all, given that the process learning approach is only capable of approximating the optimal pursuit process achieved via an iterative algorithm. This picture totally changes, however, when we consider the trade-off between the approximation error, app, and the computational complexity of the encoder, which is proportional to T . The bound in Theorem 1 is uniform for every input x, independently of the input distribution P , while in practice we would strive for a much faster decrease of the error on more probable inputs. This implies that the bound in Corollary 1 is by no means tight, and there might be other selections of \u0398 giving much lower approximation error at a fixed T . While pursuit processes optimal in the sense of the expected risk can be found using the process learning approach, there is no simple way for an iterative pursuit algorithm to take the input data distribution into account.\nAn illustration to the advantage of the trained pursuit process is shown in Figure 2 (left), in which the performance of RNMF encoders is compared to that of the corresponding proximal algorithms. As the example, we used the audio separation problem described in further details in sections 4.4 and 6. The figure shows the optimality gap  \u0302L(zT,\u0398) \u2212  \u0302L(z\u2217) as a function of T for the truncated proximal descent (\u0398 = \u03980 set as prescribed by Algorithm 2) and for the trained encoder (\u0398 = \u0398\u2217). This optimality gap can be thought of as an empirical approximation error, app, for the corresponding spaces FT . It takes about 70 iterations of the proximal method to reach the error obtained by a 7\u2212layer encoder. A further and stronger justification to the process-centric approach advocated in this paper is presented in the next section."}, {"heading": "5 Training regimes", "text": "As it was described in the previous section, parsimonious modeling can be interpreted as training an encoder/decoder pair (z,x) that minimizes the empirical loss  \u0302L in problem (18). We refer to this setting as unsupervised, as no information beside the data samples themselves is provided at training. An important characteristic of this training regime is that it can be performed online, combining online adaptation of \u0398 with online dictionary update as described in Section 2.\nIn many applications, one would like to train the parsimonious model comprising the encoder/decoder pair to optimally perform a specific task, e.g., source separation, object recognition, or classification. This setting can be still viewed as the solution of the modeling problem (18), with the modification of the training objective L to include the task-specific knowledge. However, for most objectives, given a fixed linear decoder xD(z) = Dz, the encoder of the form\nz(x) = arg min z\u2208Rq\n1 2 \u2016x\u2212Dz\u20162F + \u03c8(z) (23)\nis no more optimal, in the sense that it is not the solution of (18) with the fixed decoder. Moreover, the inclusion of an encoder of the form (23) into the modeling problem (18) gives rise to the hard bi-level optimization problem\nmin D\u2208Rm\u00d7q\n1 n n\u2211 i=1 L(xi, zi,xD) s.t. zi = arg min z\u2208Rq 1 2 \u2016xi \u2212Dz\u20162F + \u03c8(z). (24)\nTrainable pursuit processes provide an advantageous alternative in these cases. First, by being an explicit deterministic function, they remove the need to solve the bi-level optimization problem. Second, they have fixed complexity and latency controllable through the parameter T , and they are expected to achieve a better complexity-approximation error trade-off than the iterative pursuit processes. Third, trainable processes constitute a good compromise between the two extremes: the iterative pursuit having no elements of learning at all, and the general regression problems, fully relying on learning. The described trainable processes have the structure of the iterative pursuit built into the architecture on one hand, while leaving tunable degrees of freedom that can improve their modeling capabilities on the other. Furthermore, such processes come with a very good initialization of the parameters, which is non-trivial in the more general learning scenarios.\nIn the sequel, we exemplify various training regimes (i.e., different objectives in (18)) that can be used in combination with the architectures described in Section 4.3, leading to a comprehensive framework of process-centric parsimonious models."}, {"heading": "5.1 Supervised learning", "text": "In many applications, parsimonious models are used as a first order approximation to various classes of natural signals. An example is the music separation problem discussed in Section 4.4. While achieving excellent separation results,\nboth the RPCA and RNMF models are merely a crude representation of the reality: the music is never exactly low-rank, as well as the voice does not exactly admit the na\u0308\u0131ve unstructured sparse model. We propose to fill the gap between the parsimonious models and the more sophisticated (and, hence, prone to errors) domain-specific models, by incorporating learning. This can be done by designing the objective function L of (18) to incorporate the domain-specific information. Figure 2 (bottom) shows that encoders trained this way outperform not only their unsupervisedly trained counterparts, but also the exact RPCA/RNMF algorithm.\nSticking to the example of audio source separation by means of online robust PCA or NMF, let us be given a collection of n data vector, xi, representing the short-time spectrum of the mixtures, for which the clean ground-truth accompaniment l\u2217i and voice o \u2217 i spectra are given. We aim at finding an RPCA/RNMF encoder (s; o) = z\u0398(x) with the architecture depicted in Figure 1, and the linear decoder (l,o) = D(s; o) parametrized by D = (D0, Im\u00d7m), that minimize (18) with the objective\n \u0302L(z\u0398,D) = 1\nn n\u2211 i=1 \u2016D0s\u0398(xi)\u2212 l\u2217i \u201622 + \u2016o\u0398(xi)\u2212 o\u2217i \u201622\n+ \u03bb\u2217 2\n( \u2016D0\u201622 + \u2016s\u0398(xi)\u20162F ) + \u03bb\u2016o\u0398(xi)\u20161. (25)\nNote that the architecture and the initial parameters of the encoder are already a very good starting point, yet the supervised training allows it to better model the reality that is not fully captured by the RPCA/RNMF model (Figure 2, bottom).\nIn a broader perspective, the sound separation example can be viewed as a particular instance of supervised encoder/decoder training regimes, in which the loss L is expressed in terms of the composite output of x \u25e6 z, but is supervised by some yi different from the input data, e.g.,\n \u0302L(z,x) = 1\nn n\u2211 i=1 \u2016yi \u2212 x \u25e6 z(xi)\u201622 + \u03c8(z(xi)) + \u03c6(x). (26)\nCompare this to the unsupervised setting, where essentially yi = xi, and the performance of the encoder/decoder pair is measured by their ability to reconstruct the input.\nThe idea of [30] to train pursuit processes to approximate the output of iterative pursuit algorithms falls into this category. For each training data vector xi, let z \u2217 i = z \u2217(xi) be the output of the optimal encoder (19). Setting\nthe decoder to the identity map, x = id in (26) reduces the supervision to the encoder outputs,\n \u0302L(z) = 1\nn n\u2211 i=1 \u2016zi \u2212 z(xi)\u201622. (27)\nWhile producing good results in practice, under the limiting factor that the testing data is from the same class as the training data, this training regime requires supervision yet it is unable to improve the modeling capabilities of the encoder beyond those of the underlying parsimonious model (unlike the supervision of the decoder outputs in (26)). On the other hand, we show in Section 6 that similar performance can be achieved by using unsupervised training. We refer to this regime as Approximation."}, {"heading": "5.2 Discriminative learning", "text": "The supervised training setting is also useful to extend parsimonious models beyond the conventional generative scenario, in which the data can be approximately recovered from the representation, to the discriminative scenario, such as classification problems, where the representation is typically non-invertible and is intended to capture various invariant properties of the data.\nAs an illustration, we use the simultaneous speech denoising and speaker identification model from [45], in which the spectrogram of the input signal is decomposed into X \u2248 D0S + DO, where D0S capturing the noise is required to be low-rank, while the activation O representing the speech is required to be sparse. A collection of speaker-specific dictionaries, D1, . . . ,Dk, is trained offline, and the models are fit to previously unobserved data. The lowest fitting error is then used to assign the speaker identity. See Section 6.4.2 for experimental evaluation with real data.\nUsing the process-centric methodology, we construct k encoders (s\u0398j ,o\u0398j )(x),\nand k corresponding decoders (D0s,Djo) with the shared noise dictionary D0. The encoder/decoder pairs are trained by minimizing an empirical loss of the form\nL(x, l,\u03981, . . . ,\u0398k,D0, . . . ,Dk) = \u2016x\u2212D0s\u0398l(x)\u2212Dlo\u0398l(x)\u201622 (28) + \u2211 j 6=l max { 0, \u2212 \u2016x\u2212D0s\u0398j (x)\u2212Djo\u0398j (x)\u201622 } ,\naveraged on the training set containing examples of noisy speech, x, and the corresponding labels, l \u2208 {1, . . . , k} indicating which speaker is present in the sample. For each training sample, the loss function (28) promotes low fitting error for the encoder from the class coinciding with the ground-truth class, while\nfavoring high fitting error for the rest of the encoders. The hinge parameter counters excessive influence of the negatives.\nIn Section 6.4.2, we show empirical evidence that encoder/decoder pairs trained using a discriminative loss perform the classification task better than those trained to produce the best reconstruction."}, {"heading": "5.3 Data transformations", "text": "Parsimonious models rely on the assumption that the input data vectors are \u201caligned\u201d with respect to each other. This assumption might be violated in many applications. A representative example is face modeling via RPCA, where the low dimensional model only holds if the facial images are pixel-wise aligned [33]. Even small misalignments can break the structure in the data; the representation then quickly degrades as the rank of the low dimensional component increases and the matrix of outliers loses its sparsity. In [33], it was proposed to simultaneously align the input vectors and solve RPCA by including the transformation parameters into the optimization variables. This problem is highly non-convex, yet if a good initialization of the transformation parameters is available, a solution can be found by solving a sequence of convex optimization problems, each of them being comparable to (7).\nFollowing [33], we propose to incorporate the optimization over geometric transformations of the input data into our modeling framework. We assume that the data are known to be subject to a certain class of parametric transformations T = {g\u03b1 : Rm \u2192 Rm : \u03b1}. Given a data matrix X, we search for its best alignment together with finding the best model by solving\nmin z\u0398,xD \u03b11,...,\u03b1n\n1 n n\u2211 i=1 L(g\u03b1i(xi), z\u0398,xD), (29)\njointly optimizing over the encoder z\u0398, possibly the decoder xD, and the transformation parameters \u03b1i of each of the training vectors. This approach can be used with any of the training objectives described before.\nThe optimization problem (29) can be carried out as a simple extension of our general process training scheme. Transformation parameters \u03b1 can be treated on par with the encoder parameters \u0398; note that since the encoder functions z\u0398 \u2208 F are by construction almost everywhere differentiable with respect to their input, we can find the sub-gradients of the composition z\u0398(g\u03b1(x)) with respect to \u03b1 by simply applying the chain rule. This allows to solve (29) using SGD and the back-propagation in Algorithm 3.\nThe obtained encoders are conceptually very similar to the ones we had before and can still be trained in an online manner. In this particular setting, our framework presents a significant advantage over existing approaches based on iterative pursuit. In general, as the new data arrive and the model is refined or adapted, the alignment of the previously received input vectors has to be adjusted. In our settings, there is no need to recompute the alignment for the whole dataset from scratch, and it can be adjusted via online SGD.\nUnlike the untransformed model, the pursuit is no more given by an explicit function z\u0398, but requires the solution of a simple optimization problem in \u03b1, \u03b1\u2217 = arg min \u03b1 L(x, z\u0398 \u25e6 g\u03b1,xD). A new data vector x is then encoded as z = z\u0398(g\u03b1\u2217(x))."}, {"heading": "6 Experimental results", "text": "In what follows, we describe experiments conducted to assess the efficiency of the proposed framework. Fast encoders were implemented in Matlab with builtin GPU acceleration and executed on Intel Xeon E5620 CPU and NVIDIA Tesla C2070 GPU. When referring to a particular encoder, we specify its architecture and the training regime; for example \u201cCoD (Unsupervised)\u201d stands for the CoD network trained in the unsupervised regime. We denote by Approximation, Supervised, Unsupervised and Discriminative the corresponding training regime as defined in Section 5. Untrained denotes an untrained encoder, that is, with parameters set as in the corresponding fixed-step proximal descent algorithm; the performance of such an encoder coincides with that of a truncated proximal descent."}, {"heading": "6.1 Online sparse encoders", "text": "To evaluate the performance of the unstructured CoD (Unsupervised) encoders in the online learning regime, we used 30\u00d7 104 randomly located 8\u00d7 8 patches from three images from the Brodatz texture dataset [46]. The patches were ordered in three consecutive blocks of 104 patches from each image. Dictionary size was fixed to q = 64 atoms, and T = 4 layers were used in all CoD encoders. Unsupervised online learning was performed using the Lasso objective with \u03bb = 1 on overlapping windows of 1,000 vectors with a step of 100 vectors. Online trained encoders were compared to an online version of Lasso with the dictionary adapted on the same data. As reference, we trained offline a CoD (Approximation) encoder and another CoD (Unsupervised) encoder. Of-\nfline training was performed on a distinct set of 6,000 patches extracted from the same images.\nPerformance measured in terms of the Lasso objective is reported in Figure 3 (high error corresponds to low performance). Initially, the performance of the online trained CoD (Unsupervised) encoder is slightly inferior to the offline trained counterpart; however, the online version starts performing better after the network parameters and the dictionary adapt to the current class of data. The CoD (Approximation) encoder trained offline exhibits the lowest performance. This experiment shows that, while the drop in performance compared to the exact Lasso is relatively low, the computational complexity of the online CoD (Unsupervised) encoder is tremendously lower and fixed."}, {"heading": "6.2 Structured sparse encoders", "text": "The performance of the BCoD structured sparse architecture derived from Algorithm 1 combined with different training regimes is evaluated on a speaker\nidentification task reproduced from [47]. In this application the authors use hierarchical sparse coding to automatically detect the speakers in a given mixed signal. The dataset consists of recordings of five different radio speakers, two females and three males. 25% of the samples were used for training, and the rest for testing. Within the testing data, two sets of waveforms were created: one containing isolated speakers, and another containing all possible combinations of mixtures of two speakers. Signals were decomposed into a set of overlapping time frames of 512 samples with 75% overlap, such that the properties of the signal remain stable within each frame. An 80-dimensional feature vector is obtained for each audio frame as its short-time power spectrum envelope (refer to [47] for details). Five under-complete dictionaries with 50 atoms each were trained on the single speaker set minimizing the Lasso objective with \u03bb = 0.2 (one dictionary per speaker), and then combined into a single structured dictionary containing 250 atoms. Increasing the dictionary size exhibited negligible performance benefits. Speaker identification was performed by first encoding a test vector in the structured dictionary and measuring the `2 energy of each of the five groups. Energies were sum-pooled over 500 time samples selecting the labels of the highest two.\nTo assess the importance of the process architecture, a CoD (Approximation) and a BCoD (Approximation) encoders with T = 2 layers were trained offline to approximate the solution of the exact HiLasso, with \u03bb2 = 0.05 (regularizer parameter in the lower level of the hierarchy). A BCoD (Discriminative) encoder with the same settings was also trained in the supervised regime using the discriminative loss function (28) to promote or discourage the activation of groups corresponding to knowingly active or silent speakers respectively.\nTable 1 summarizes the obtained misclassification rates. In agreement with the experiments shown in Section 4.3, using an appropriate structured architecture instead of its unstructured counterpart with the same number of layers\nand the same dictionary increases performance by nearly a factor of two. The use of the discriminative objective further improves performance. Surprisingly, using encoders with only two layers cedes just about 1% of correct classification rate. The structured architecture showed a crucial roll in producing accurate structured sparse codes. Other comparative experiments substantiating this observation can be found in [34]."}, {"heading": "6.3 Robust PCA encoders with geometric optimization", "text": "In order to evaluate the performance of robust PCA encoders, we used a face dataset consisting of 800 66 \u00d7 48 images of a female face photographed over the timespan of 4.5 years, roughly pose- and scale-normalized and aligned.The images manifested a significant variety of hair and dressing styles, while keeping similar facial traits. Following [33], we use RPCA to decompose a collection of faces represented as columns of the data matrix X into L+O, with the low-rank term L = D0S approximating the face identity (atoms of D0 can be thought of as \u201ceigenfaces\u201d), while the outlier term O capturing the appearance variability.\nWe trained a five layer RPCA (Unsupervised) encoder on 600 images from the faces dataset. The dictionary was initialized using standard SVD and parameters were set to q = 50, \u03bb\u2217 = 0.1 and \u03bb = 10 \u22122. To evaluate the representation capabilities of RPCA encoders in the presence of geometric transformations, as a test set, we used the remaining 200 faces, as well as a collection of geometrically transformed images from the same test set. Sub-pixel planar translations were used for geometric transformations. The encoder was applied to the misaligned set, optimizing the unsupervised objective over the transformation parameters. For reference, the encoder was also applied to the transformed and the untransformed test sets without performing optimization. Examples of the obtained representations are visualized in Figure 4. Note the relatively larger magnitude and the bigger active set of the sparse outlier vector o produced for the misaligned faces, and how they are re-aligned when optimization over the transformation is allowed. Since the original data are only approximately aligned, performing optimal alignment during encoding frequently yields lower cost compared to the plain encoding of the original data."}, {"heading": "6.4 Robust NMF encoders", "text": ""}, {"heading": "6.4.1 Singing voice separation", "text": "We now evaluate the source separation problem (singing-voice/backgroundaccompaniment), described in Section 4.4. The separation performance was evaluated on the MIR-1K dataset [48], containing 1,000 Chinese karaoke clips performed by amateur singers. The experimental settings closely followed that of [48], to which the reader is referred for further details. As the evaluation criteria, we used the normalized source-to-distortion ratio (NSDR) from the BSS-EVAL metrics [49], averaged over the test set. Encoders with RNMF architecture composed by T = 20 layers and q = 25 were trained using different training regimes. We used \u03bb = \u221a 2n\u03c3 and \u03bb\u2217 = \u221a 2\u03c3 with \u03c3 = 0.3 set following [13]. Table 2 summarizes the obtained separation performance. While (Unsupervised) training regime makes fast RNMF encoders on par with the exact RNMF (at a fraction of the computational complexity and latency of the latter), significant improvement is achieved by using the (Supervised) regime, where the encoders are trained to approximate the ground-truth separation over a reduced training set. For further details refer to [35]."}, {"heading": "6.4.2 Robust speaker identification", "text": "The purpose of this experiment is to show the benefits of training parsimonious models in a discriminative fashion when used for classification tasks. As an example we use speaker identification in environments heavily contaminated by unstructured noise described in Section 5.2. We evaluated the classification capabilities of different low rank NMF architectures in combination with two supervised training regimes discussed in Section 5, one aimed to produce a good reconstruction of the speech signal and another one optimized to produce the best classification. The reconstructive approach is analogous to the one used in Section 6.4.1 for music/singing-voice separation, but using a low rank NMF for both noise and human speech. In all our examples we used T = 10 layers and q = 50. Parameters \u03bb and \u03bb\u2217 were chosen as in Section 6.4.1.\nAs speech dataset we used a subset of the GRID dataset [50] containing 10 distinct speakers; each speaker comprising 1,000 short clips. Three sets of 200 distinct clips each were used for training, validation, and testing. The GRID clips were artificially contaminated by six categories of noise recorded from different real environments (street, restaurant, car, exhibition, train, and airport) taken from the AURORA corpus [51]. The voice and the noise clips were mixed linearly with equal energy (0 dB SNR). In all experiments, the spectrogram of each mixture was computed using a window of size 512 and a step size of 128 samples (at 8 KHz sampling rate). For further details please refer to [45]. Table 3 summarizes the classification rates of the compared methods. While the results obtained using the low rank NMF (Supervised) are very good, they are significantly outperformed by the low rank NMF (Discriminative) encoders."}, {"heading": "7 Conclusions and future work", "text": "In this work we have developed a comprehensive framework for process-centric parsimonious modeling. By combining ideas from convex optimization with multi-layer neural networks, we have shown how to produce deterministic functions capable of faithfully approximating the optimization-based solution of parsimonious models at a fraction of the computational time. Furthermore, at almost the same computational cost, the framework includes different objective functions that allow the encoders to be trained in a discriminative fashion or solve challenging alignment problems. We conducted empirical experiments in different settings and real applications such as image modeling, robust face modeling, audio sources separation and robust speaker recognition. A simple unoptimized implementation already achieves often several order of magnitude speedups when compared to exact solvers.\nWhile we limited our attention to synthesis models, the proposed framework can be naturally extended to analysis cosparse models [52, 53], in which the signal is known to be sparse in a transformed domain. Specifically, given a \u201csensing\u201d matrix M \u2208 Rn\u00d7q and an analysis dictionary \u2126 \u2208 Rp\u00d7m, in an analysis counterpart of (21), one looks for a function f \u2208 F , where again F is a space of functions with certain desired properties, that minimizes\nmin f\u2208F\n1 2 N\u2211 i=1 \u2016xi \u2212Mf(xi)\u20162 + \u03bb \u2016\u2126f(xi)\u20161 . (30)\nThe space F can be set by truncating suitable iterative optimization algorithms such as the augmented Lagrangian methods of multipliers (ADMM) [21]."}], "references": [{"title": "Atomic decomposition by basis pursuit", "author": ["S. Chen", "D. Donoho", "M. Saunders"], "venue": "SIAM J. Scientific Computing, vol. 20, no. 1, pp. 33\u201361, 1999.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["R. Tibshirani"], "venue": "J. Royal Stat. Society: Series B, vol. 58, no. 1, pp. 267\u2013288, 1996.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B. Olshausen", "D.J. Field"], "venue": "Nature, vol. 381, no. 6583, pp. 607\u2013609, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "k-SVD: an algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Trans. Sig. Proc., vol. 54, no. 11, pp. 4311\u20134322, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. Royal Stat. Society, Series B, vol. 68, pp. 49\u201367, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Group lasso with overlap and graph lasso", "author": ["L. Jacob", "G. Obozinski", "J. Vert"], "venue": "ICML, 2009, pp. 433\u2013440.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "The composite absolute penalties family for grouped and hierarchical variable selection", "author": ["P. Zhao", "G. Rocha", "B. Yu"], "venue": "Annals of Statistics, vol. 37, no. 6A, p. 3468, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Proximal methods for hierarchical sparse coding", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2297\u20132334, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "C-hilasso: A collaborative hierarchical sparse modeling framework", "author": ["P. Sprechmann", "I. Ram\u0131\u0301rez", "G. Sapiro", "Y.C. Eldar"], "venue": "IEEE Trans. Signal Process., vol. 59, no. 9, pp. 4183\u20134198, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting statistical dependencies in sparse representations for signal recovery", "author": ["T. Peleg", "Y. Eldar", "M. Elad"], "venue": "IEEE Trans. Sig. Proc., vol. 60, no. 5, pp. 2286\u20132303, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Rank, trace-norm and max-norm", "author": ["N. Srebro", "A. Shraibman"], "venue": "Proc. COLT, pp. 599\u2013764, 2005. 31", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Exact matrix completion via convex optimization", "author": ["E. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational mathematics, vol. 9, no. 6, pp. 717\u2013 772, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust principal component analysis?", "author": ["E. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Robust PCA via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "IEEE Trans. Inf. Theory, vol. 58, no. 5, pp. 3047\u20133064, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust non-negative matrix factorization", "author": ["L. Zhang", "Z. Chen", "M. Zheng", "X. He"], "venue": "Frontiers of Electrical and Electronic Engineering in China, vol. 6, no. 2, pp. 192\u2013200, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning parts of objects by non-negative matrix factorization", "author": ["D. Lee", "H. Seung"], "venue": "Nature, vol. 401, no. 6755, pp. 788\u2013791, 1999.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on Pure and Applied Mathematics, vol. 57, no. 11, pp. 1413\u20131457, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Img. Sci., vol. 2, pp. 183\u2013202, March 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Coordinate descent optimization for `1 minimization with application to compressed sensing; a greedy algorithm", "author": ["Y. Li", "S. Osher"], "venue": "Inverse Problems and Imaging, vol. 3, pp. 487\u2013503, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "CORE. Catholic University of Louvain, Louvain-la-Neuve, Belgium, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonlinear programming", "author": ["D. Bertsekas"], "venue": "1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Convex optimization with sparsity-inducing norms", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Optimization for Machine Learning. MIT Press, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM J. on Opt., vol. 20, no. 4, pp. 1956\u2013 1982, 2010. 32", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1956}, {"title": "Accelerated low-rank visual recovery by random projection", "author": ["Y. Mu", "J. Dong", "X. Yuan", "S. Yan"], "venue": "CVPR, 2011, pp. 2609\u20132616.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallel stochastic gradient algorithms for large-scale matrix completion", "author": ["B. Recht", "C. R\u00e9"], "venue": "Optimization Online, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Projected gradient methods for non-negative matrix factorization", "author": ["C.-J. Lin"], "venue": "Neural Computation, vol. 19, pp. 2756\u20132779, 2007.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "An overview of bilevel optimization", "author": ["B. Colson", "P. Marcotte", "G. Savard"], "venue": "Annals of Operations Research, vol. 153, no. 1, pp. 235\u2013256, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "What is the best multi-stage architecture for object recognition?", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "in CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "arXiv:1010.3467, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "ICML, 2010, pp. 399\u2013406.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxe", "H. Lee", "A.Y. Ng"], "venue": "In NIPS, 2009, pp. 646\u2013654.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "RASL: Robust alignment by sparse and low-rank decomposition for linearly correlated images", "author": ["Y. Peng", "A. Ganesh", "J. Wright", "W. Xu", "Y. Ma"], "venue": "CVPR, 2010, pp. 763\u2013770.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning efficient structured sparse models", "author": ["P. Sprechmann", "A.M. Bronstein", "G. Sapiro"], "venue": "ICML, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time online singing voice separation from monaural recordings using robust low-rank modeling", "author": ["\u2014\u2014"], "venue": "ISMIR, 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Online dictionary learning for sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "ICML, 2009, pp. 689\u2013696.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Average case analysis of multichannel sparse recovery using convex relaxation", "author": ["Y.C. Eldar", "H. Rauhut"], "venue": "IEEE Trans. on Inf. Theory, vol. 56, no. 1, pp. 505\u2013519, 2010. 33", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust PCA as bilinear decomposition with outlier-sparsity regularization", "author": ["G. Mateos", "G.B. Giannakis"], "venue": "arXiv.org:1111.1788, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Unveiling network anomalies in large-scale networks via sparsity and low rank", "author": ["M. Mardani", "G. Mateos", "G.B. Giannakis"], "venue": "Proc. of 44th Asilomar Conf. on Signals, Systems, and Computers, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "A note on the group lasso and a sparse group lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "2010, preprint.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "J. Optim. Theory Appl., vol. 109, no. 3, pp. 475\u2013494, June 2001.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2001}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V. Vapnik", "A. Chervonenkis"], "venue": "Theory of Probability & Its Applications, vol. 16, no. 2, pp. 264\u2013280, 1971.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1971}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "COMPSTAT, August 2010, pp. 177\u2013187.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Learnable low rank sparse models for speech denoising", "author": ["P. Sprechmann", "A.M. Bronstein", "M.M. Bronstein", "G. Sapiro"], "venue": "arXiv.org:1221.1288, 2012.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Filtering for texture classification: a comparative study", "author": ["T. Randen", "J.H. Husoy"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 21, no. 4, pp. 291\u2013310, 1999.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1999}, {"title": "Collaborative sources identification in mixed signals via hierarchical sparse modeling", "author": ["P. Sprechmann", "I. Ramirez", "P. Cancela", "G. Sapiro"], "venue": "Proc. ICASSP, May 2011.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "On the improvement of singing voice separation for monaural recordings using the MIR-1K dataset", "author": ["C. Hsu", "J. Jang"], "venue": "IEEE Trans. on Audio, Speech, and Lang. Proc., vol. 18, no. 2, pp. 310\u2013319, 2010.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "R. Gribonval", "C. F\u00e9votte"], "venue": "IEEE Trans. on Audio, Speech, and Lang. Proc., vol. 14, no. 4, pp. 1462\u20131469, 2006. 34", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2006}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "J. of the Acoust. Society of America, vol. 120, p. 2421, 2006.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "The AURORA experimental framework for the performance evaluation of speech recognition systems under noisy conditions", "author": ["D. Pearce", "H.-G. Hirsch"], "venue": "INTERSPEECH, 2000, pp. 29\u201332.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2000}, {"title": "The cosparse analysis model and algorithms", "author": ["S. Nam", "M. Davies", "M. Elad", "R. Gribonval"], "venue": "Applied and Computational Harmonic Analysis, 2012.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust sparse analysis regularization", "author": ["S. Vaiter", "G. Peyr\u00e9", "C. Dossal", "J. Fadili"], "venue": "arXiv preprint arXiv:1109.6222, 2011. 35", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The pursuit of sparse representations was shown to be possible using tools from convex optimization, in particular, via `1 norm minimization [1, 2].", "startOffset": 141, "endOffset": 147}, {"referenceID": 1, "context": "The pursuit of sparse representations was shown to be possible using tools from convex optimization, in particular, via `1 norm minimization [1, 2].", "startOffset": 141, "endOffset": 147}, {"referenceID": 2, "context": "Works [3, 4], followed by many others, introduced efficient computational techniques for dictionary learning and adaptation.", "startOffset": 6, "endOffset": 12}, {"referenceID": 3, "context": "Works [3, 4], followed by many others, introduced efficient computational techniques for dictionary learning and adaptation.", "startOffset": 6, "endOffset": 12}, {"referenceID": 4, "context": "As many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10].", "startOffset": 268, "endOffset": 287}, {"referenceID": 5, "context": "As many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10].", "startOffset": 268, "endOffset": 287}, {"referenceID": 6, "context": "As many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10].", "startOffset": 268, "endOffset": 287}, {"referenceID": 7, "context": "As many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10].", "startOffset": 268, "endOffset": 287}, {"referenceID": 8, "context": "As many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10].", "startOffset": 268, "endOffset": 287}, {"referenceID": 9, "context": "As many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10].", "startOffset": 268, "endOffset": 287}, {"referenceID": 10, "context": "A recent series of works have elucidated the beautiful relationship between sparsity and low rank representations, showing that rank minimization can be achieved through convex optimization [11, 12].", "startOffset": 190, "endOffset": 198}, {"referenceID": 11, "context": "A recent series of works have elucidated the beautiful relationship between sparsity and low rank representations, showing that rank minimization can be achieved through convex optimization [11, 12].", "startOffset": 190, "endOffset": 198}, {"referenceID": 12, "context": "The combination of low-rank and sparse models paved the path to new robust alternatives of principal component analysis (RPCA) [13, 14] and nonnegative matrix factorization (RNMF) [15], and addressing challenging matrix completion problems [12].", "startOffset": 127, "endOffset": 135}, {"referenceID": 13, "context": "The combination of low-rank and sparse models paved the path to new robust alternatives of principal component analysis (RPCA) [13, 14] and nonnegative matrix factorization (RNMF) [15], and addressing challenging matrix completion problems [12].", "startOffset": 127, "endOffset": 135}, {"referenceID": 14, "context": "The combination of low-rank and sparse models paved the path to new robust alternatives of principal component analysis (RPCA) [13, 14] and nonnegative matrix factorization (RNMF) [15], and addressing challenging matrix completion problems [12].", "startOffset": 180, "endOffset": 184}, {"referenceID": 11, "context": "The combination of low-rank and sparse models paved the path to new robust alternatives of principal component analysis (RPCA) [13, 14] and nonnegative matrix factorization (RNMF) [15], and addressing challenging matrix completion problems [12].", "startOffset": 240, "endOffset": 244}, {"referenceID": 15, "context": "Another relevant low rank modeling scheme is non-negative matrix factorization (NMF)[16], where the input vectors are represented as nonnegative linear combination of a non-negative under-complete dictionary.", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 134, "endOffset": 158}, {"referenceID": 17, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 134, "endOffset": 158}, {"referenceID": 18, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 134, "endOffset": 158}, {"referenceID": 19, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 134, "endOffset": 158}, {"referenceID": 20, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 134, "endOffset": 158}, {"referenceID": 21, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 134, "endOffset": 158}, {"referenceID": 12, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 166, "endOffset": 182}, {"referenceID": 22, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 166, "endOffset": 182}, {"referenceID": 23, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 166, "endOffset": 182}, {"referenceID": 24, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 166, "endOffset": 182}, {"referenceID": 15, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 191, "endOffset": 199}, {"referenceID": 25, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 191, "endOffset": 199}, {"referenceID": 26, "context": "The resulting bilevel optimization problems are notoriously difficult to solve in general; the non-differentiability of the lowerlevel parsimony-inducing objective makes the solution practically impossible [27].", "startOffset": 206, "endOffset": 210}, {"referenceID": 27, "context": "Recently, [28, 29] have proposed to trade off precision in the sparse representation for computational speed-up by learning non-linear regressors capable", "startOffset": 10, "endOffset": 18}, {"referenceID": 28, "context": "Recently, [28, 29] have proposed to trade off precision in the sparse representation for computational speed-up by learning non-linear regressors capable", "startOffset": 10, "endOffset": 18}, {"referenceID": 29, "context": "In their inspiring recent paper, [30] showed that a particular network architecture can be derived from the iterative shrinkage-thresholding (ISTA) [17] and proximal coordinate descent (CoD) algorithms [19].", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "In their inspiring recent paper, [30] showed that a particular network architecture can be derived from the iterative shrinkage-thresholding (ISTA) [17] and proximal coordinate descent (CoD) algorithms [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "In their inspiring recent paper, [30] showed that a particular network architecture can be derived from the iterative shrinkage-thresholding (ISTA) [17] and proximal coordinate descent (CoD) algorithms [19].", "startOffset": 202, "endOffset": 206}, {"referenceID": 30, "context": "These works were among the first to bridge between the optimization based sparse models and the inherently process-centric neural networks, and in particular auto-encoder networks [31, 32], extensively explored by the deep learning community.", "startOffset": 180, "endOffset": 188}, {"referenceID": 31, "context": "These works were among the first to bridge between the optimization based sparse models and the inherently process-centric neural networks, and in particular auto-encoder networks [31, 32], extensively explored by the deep learning community.", "startOffset": 180, "endOffset": 188}, {"referenceID": 29, "context": "By extending the original ideas in [30], we propose tailored pursuit architectures derived from first-order proximal descent algorithms, which are briefly presented in Section 3.", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "As a remedy, we propose to use an algorithm inspired by the non-convex optimization techniques in [25].", "startOffset": 98, "endOffset": 102}, {"referenceID": 29, "context": "Second, this new approach allows the encoders to be trained in an online manner, which makes the fast encoders no more restricted to work with a fixed distribution of input vectors known a priori (limitation existing, for example, in [30]), and removes the need to run the exact algorithms at training.", "startOffset": 234, "endOffset": 238}, {"referenceID": 31, "context": "While differently motivated, in this setting, our framework is related to the sparse autoencoders [32].", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "In particular, in Section 5 we show a very simple and efficient extension of the proposed RPCA framework to cases where the data undergo an unknown transformation that is sought for during the pursuit [33].", "startOffset": 201, "endOffset": 205}, {"referenceID": 33, "context": "The present paper generalizes and gives a more rigorous treatment to results previously published by the authors in [34, 35].", "startOffset": 116, "endOffset": 124}, {"referenceID": 34, "context": "The present paper generalizes and gives a more rigorous treatment to results previously published by the authors in [34, 35].", "startOffset": 116, "endOffset": 124}, {"referenceID": 35, "context": "Online parsimonious modeling aims at estimating and refining the model as the data come in [36].", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "where \u03b2j \u2208 [0, 1] is a forgetting factor that can be added to rescale older information so that newer estimates have more weight.", "startOffset": 11, "endOffset": 17}, {"referenceID": 35, "context": "This can be efficiently solved without remembering all the past codes [36].", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "This is the classical unstructured sparse coding problem, often referred to as Lasso [2] or basis pursuit [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "This is the classical unstructured sparse coding problem, often referred to as Lasso [2] or basis pursuit [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "Several important structured sparsity settings can be cast as particular cases of (6): Group sparse coding, a generalization of the standard sparse coding to the cases in which the dictionary is sub-divided into groups [5], in this case G is a partition of {1, .", "startOffset": 219, "endOffset": 222}, {"referenceID": 8, "context": ", q}; Hierarchical sparse coding, assuming a hierarchical structure of the non-zero coefficients [9, 7, 8].", "startOffset": 97, "endOffset": 106}, {"referenceID": 6, "context": ", q}; Hierarchical sparse coding, assuming a hierarchical structure of the non-zero coefficients [9, 7, 8].", "startOffset": 97, "endOffset": 106}, {"referenceID": 7, "context": ", q}; Hierarchical sparse coding, assuming a hierarchical structure of the non-zero coefficients [9, 7, 8].", "startOffset": 97, "endOffset": 106}, {"referenceID": 8, "context": "Collaborative sparse coding generalizes the concept of structured sparse coding to collections of input vectors by promoting given patterns of non-zero elements in the coefficient matrix [9, 37].", "startOffset": 187, "endOffset": 194}, {"referenceID": 36, "context": "Collaborative sparse coding generalizes the concept of structured sparse coding to collections of input vectors by promoting given patterns of non-zero elements in the coefficient matrix [9, 37].", "startOffset": 187, "endOffset": 194}, {"referenceID": 12, "context": "[13, 14] proposed to robustify the model by adding a new term to the decomposition to account for the presence of outliers, X = L + O + E, where O is an outlier matrix with a sparse number of non-zero coefficients of arbitrarily large magnitude.", "startOffset": 0, "endOffset": 8}, {"referenceID": 13, "context": "[13, 14] proposed to robustify the model by adding a new term to the decomposition to account for the presence of outliers, X = L + O + E, where O is an outlier matrix with a sparse number of non-zero coefficients of arbitrarily large magnitude.", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "In [11] it was shown that the nuclear norm of a matrix of L can be reformulated as a penalty over all possible factorizations", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "This factorization has been recently exploited in parallel processing across multiple processors to produce state-ofthe-art algorithms for matrix completion problems [25], as well as an alternative approach to robustifying PCA in [39].", "startOffset": 166, "endOffset": 170}, {"referenceID": 37, "context": "This factorization has been recently exploited in parallel processing across multiple processors to produce state-ofthe-art algorithms for matrix completion problems [25], as well as an alternative approach to robustifying PCA in [39].", "startOffset": 230, "endOffset": 234}, {"referenceID": 37, "context": "Combining this with (8), it was proposed in [39] to reformulate (7) as", "startOffset": 44, "endOffset": 48}, {"referenceID": 38, "context": "Fortunately, it can be shown that any stationary point of (9), {D0,S,O}, satisfying ||X \u2212 D0S \u2212 O||2 \u2264 \u03bb\u2217 is an globally optimal solution of (9) [40].", "startOffset": 145, "endOffset": 149}, {"referenceID": 14, "context": "A robust variant can be obtained by adding a sparse outlier term to the decomposition, X \u2248 D0S+O, as done in the RPCA model [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 34, "context": "In [35], we proposed a cure to this phenomenon by incorporating a rank-reducing term into (10), establishing in this way a link between NMF and the RPCA problem (9).", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "They have been adopted by the machine learning and signal processing communities for their simplicity, convergence guarantees, and the fact that they are well suited for tackling sparse and structured sparse coding problems that can be written as (1) (refer to [22] for recent reviews).", "startOffset": 261, "endOffset": 265}, {"referenceID": 17, "context": "Fixed-step algorithms have been shown to have relatively slow sub-linear convergence, and many alternatives have been studied in the literature to improve the convergence rate [18, 20].", "startOffset": 176, "endOffset": 184}, {"referenceID": 19, "context": "Fixed-step algorithms have been shown to have relatively slow sub-linear convergence, and many alternatives have been studied in the literature to improve the convergence rate [18, 20].", "startOffset": 176, "endOffset": 184}, {"referenceID": 16, "context": "In this case, the fixed step proximal splitting algorithm corresponds to the popular iterative shrinkage-thresholding algorithm (ISTA) [17, 18] summarized in Algorithm 1.", "startOffset": 135, "endOffset": 143}, {"referenceID": 17, "context": "In this case, the fixed step proximal splitting algorithm corresponds to the popular iterative shrinkage-thresholding algorithm (ISTA) [17, 18] summarized in Algorithm 1.", "startOffset": 135, "endOffset": 143}, {"referenceID": 7, "context": "Then, the proximal operator of \u03c8G can be shown to be given by the composition of the proximal operators of \u03c8Gl in ascending order from the leaves to the root [8, 9], \u03c0 G = \u03c0G1 \u03bb1 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c0 GL \u03bbL .", "startOffset": 158, "endOffset": 164}, {"referenceID": 8, "context": "Then, the proximal operator of \u03c8G can be shown to be given by the composition of the proximal operators of \u03c8Gl in ascending order from the leaves to the root [8, 9], \u03c0 G = \u03c0G1 \u03bb1 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c0 GL \u03bbL .", "startOffset": 158, "endOffset": 164}, {"referenceID": 8, "context": "A particular case of the tree-structured hierarchical sparse model is the two-level HiLasso model introduced to simultaneously promote sparsity at both group and coefficient level [9, 41].", "startOffset": 180, "endOffset": 187}, {"referenceID": 39, "context": "A particular case of the tree-structured hierarchical sparse model is the two-level HiLasso model introduced to simultaneously promote sparsity at both group and coefficient level [9, 41].", "startOffset": 180, "endOffset": 187}, {"referenceID": 40, "context": "Several variants of coordinate descent (CoD) and blockcoordinate descent (BCoD) proximal methods have been proposed [42, 19].", "startOffset": 116, "endOffset": 124}, {"referenceID": 18, "context": "Several variants of coordinate descent (CoD) and blockcoordinate descent (BCoD) proximal methods have been proposed [42, 19].", "startOffset": 116, "endOffset": 124}, {"referenceID": 41, "context": "When the family F is sufficiently restrictive, the statistical learning theory justifies minimizing the empirical risk instead of the expected risk[43].", "startOffset": 147, "endOffset": 151}, {"referenceID": 42, "context": "When the functions belonging to F are almost everywhere differentiable with respect to the parameters \u0398, stochastic gradient descent (SGD) can be used to optimize (21), with almost sure convergence to a stationary point [44].", "startOffset": 220, "endOffset": 224}, {"referenceID": 42, "context": "Following [44], we split the process training approximation error into three terms, = app + est + opt.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "We extend the ideas introduced in [30] to derive families of trainable pursuit processes from proximal methods.", "startOffset": 34, "endOffset": 38}, {"referenceID": 29, "context": "Following [30], we consider the family of pursuit processes derived from truncated proximal descent algorithms with T iterations, FT = {zT,\u0398(x) = fH,t \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 fH,t(Wx,0)}.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "Furthermore, reformulating the non-asymptotic convergence analysis from [18] in our language, the following holds:", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "The idea of [30] to train pursuit processes to approximate the output of iterative pursuit algorithms falls into this category.", "startOffset": 12, "endOffset": 16}, {"referenceID": 43, "context": "As an illustration, we use the simultaneous speech denoising and speaker identification model from [45], in which the spectrogram of the input signal is decomposed into X \u2248 D0S + DO, where D0S capturing the noise is required to be low-rank, while the activation O representing the speech is required to be sparse.", "startOffset": 99, "endOffset": 103}, {"referenceID": 32, "context": "A representative example is face modeling via RPCA, where the low dimensional model only holds if the facial images are pixel-wise aligned [33].", "startOffset": 139, "endOffset": 143}, {"referenceID": 32, "context": "In [33], it was proposed to simultaneously align the input vectors and solve RPCA by including the transformation parameters into the optimization variables.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "Following [33], we propose to incorporate the optimization over geometric transformations of the input data into our modeling framework.", "startOffset": 10, "endOffset": 14}, {"referenceID": 44, "context": "To evaluate the performance of the unstructured CoD (Unsupervised) encoders in the online learning regime, we used 30\u00d7 10 randomly located 8\u00d7 8 patches from three images from the Brodatz texture dataset [46].", "startOffset": 203, "endOffset": 207}, {"referenceID": 45, "context": "identification task reproduced from [47].", "startOffset": 36, "endOffset": 40}, {"referenceID": 45, "context": "An 80-dimensional feature vector is obtained for each audio frame as its short-time power spectrum envelope (refer to [47] for details).", "startOffset": 118, "endOffset": 122}, {"referenceID": 33, "context": "Other comparative experiments substantiating this observation can be found in [34].", "startOffset": 78, "endOffset": 82}, {"referenceID": 32, "context": "Following [33], we use RPCA to decompose a collection of faces represented as columns of the data matrix X into L+O, with the low-rank term L = D0S approximating the face identity (atoms of D0 can be thought of as \u201ceigenfaces\u201d), while the outlier term O capturing the appearance variability.", "startOffset": 10, "endOffset": 14}, {"referenceID": 46, "context": "The separation performance was evaluated on the MIR-1K dataset [48], containing 1,000 Chinese karaoke clips performed by amateur singers.", "startOffset": 63, "endOffset": 67}, {"referenceID": 46, "context": "The experimental settings closely followed that of [48], to which the reader is referred for further details.", "startOffset": 51, "endOffset": 55}, {"referenceID": 47, "context": "As the evaluation criteria, we used the normalized source-to-distortion ratio (NSDR) from the BSS-EVAL metrics [49], averaged over the test set.", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "3 set following [13].", "startOffset": 16, "endOffset": 20}, {"referenceID": 34, "context": "For further details refer to [35].", "startOffset": 29, "endOffset": 33}, {"referenceID": 48, "context": "As speech dataset we used a subset of the GRID dataset [50] containing 10 distinct speakers; each speaker comprising 1,000 short clips.", "startOffset": 55, "endOffset": 59}, {"referenceID": 49, "context": "The GRID clips were artificially contaminated by six categories of noise recorded from different real environments (street, restaurant, car, exhibition, train, and airport) taken from the AURORA corpus [51].", "startOffset": 202, "endOffset": 206}, {"referenceID": 43, "context": "For further details please refer to [45].", "startOffset": 36, "endOffset": 40}, {"referenceID": 50, "context": "While we limited our attention to synthesis models, the proposed framework can be naturally extended to analysis cosparse models [52, 53], in which the signal is known to be sparse in a transformed domain.", "startOffset": 129, "endOffset": 137}, {"referenceID": 51, "context": "While we limited our attention to synthesis models, the proposed framework can be naturally extended to analysis cosparse models [52, 53], in which the signal is known to be sparse in a transformed domain.", "startOffset": 129, "endOffset": 137}, {"referenceID": 20, "context": "The space F can be set by truncating suitable iterative optimization algorithms such as the augmented Lagrangian methods of multipliers (ADMM) [21].", "startOffset": 143, "endOffset": 147}], "year": 2012, "abstractText": "Parsimony, including sparsity and low rank, has been shown to successfully model data in numerous machine learning and signal processing tasks. Traditionally, such modeling approaches rely on an iterative algorithm that minimizes an objective function with parsimony-promoting terms. The inherently sequential structure and data-dependent complexity and latency of iterative optimization constitute a major limitation in many applications requiring real-time performance or involving large-scale data. Another limitation encountered by these modeling techniques is the difficulty of their inclusion in discriminative learning scenarios. In this work, we propose to move the emphasis from the model to the pursuit algorithm, and develop a process-centric view of parsimonious modeling, in which a learned deterministic fixed-complexity pursuit process is used in lieu of iterative optimization. We show a principled way to construct learnable pursuit process architectures for structured sparse and robust low rank models, derived from the iteration of proximal descent algorithms. These architectures learn to approximate the exact parsimonious representation at a fraction of the complexity of the standard optimization methods. We also show that appropriate training regimes allow to naturally extend parsimonious models to discriminative settings. State-ofthe-art results are demonstrated on several challenging problems in image and audio processing with several orders of magnitude speedup compared to the exact optimization algorithms. \u2217P. Sprechmann and G. Sapiro are with the Department of Electrical and Computer Engineering, Duke University, Durham 27708, USA. Email: pablo.sprechmann@duke.edu, guillermo.sapiro@duke.edu. \u2020A. M. Bronsteind is with School of Electrical Engineering, Tel Aviv University, Tel Aviv 69978, Israel.Email: bron@eng.tau.ac.il. \u2021Work partially supported by NSF, ONR, NGA, DARPA, AFOSR, ARO, and BSF. 1 ar X iv :1 21 2. 36 31 v1 [ cs .L G ] 1 4 D ec 2 01 2", "creator": "LaTeX with hyperref package"}}}