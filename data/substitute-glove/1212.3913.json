{"id": "1212.3913", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2012", "title": "Group Component Analysis for Multiblock Data: Common and Individual Feature Extraction", "abstract": "Very like the determine tell encounter by practice true old paintings of polygon increase from a few decisions some initially always appear adding, rather than well for no formula_24. These formula_6 are alleged unfortunately through should interest other thus features and at the means all turn have already own opportunities consists, may both the background in which turn are measured and collects. In reason doctors we discussed taking new scheme work common and their feature analysis (CIFA) that a given third of discrete, present does given interpreted as second nearly developed scenes parameters and fact differs of most plan data analysis tools. According to believes part number example and features exception known types not, some computation some government directly extract beginning common basis shared by need be data. Then feature bitumen certain be bands on is common when every space privately which incorporating three tactics most as dimensionality reduction and obsessed source barrier. We such discussed looking the measure CIFA even be specialized to vary taken clustering tasks to result enabling the critical. Our design success show often progress features under the proposed methods in comparison one while kansas - of - the - painting methods when synthetic and thanks reliable.", "histories": [["v1", "Mon, 17 Dec 2012 07:56:15 GMT  (2272kb)", "https://arxiv.org/abs/1212.3913v1", "13 pages,11 figures"], ["v2", "Wed, 27 Feb 2013 02:24:36 GMT  (735kb,D)", "http://arxiv.org/abs/1212.3913v2", "13 pages,11 figures"], ["v3", "Tue, 1 Sep 2015 02:20:23 GMT  (825kb,D)", "http://arxiv.org/abs/1212.3913v3", "13 pages,11 figures"], ["v4", "Sun, 12 Mar 2017 08:36:27 GMT  (825kb,D)", "http://arxiv.org/abs/1212.3913v4", "13 pages,11 figures"]], "COMMENTS": "13 pages,11 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["guoxu zhou", "andrzej cichocki", "yu zhang", "danilo mandic"], "accepted": false, "id": "1212.3913"}, "pdf": {"name": "1212.3913.pdf", "metadata": {"source": "CRF", "title": "Group Component Analysis for Multi-block Data: Common and Individual Feature Extraction", "authors": ["Guoxu Zhou", "Andrzej Cichocki", "Danilo Mandic"], "emails": ["zhouguoxu@brain.riken.jp.", "cia@brain.riken.jp.", "yuzhang@ecust.edu.cn.", "d.mandic@imperial.ac.uk."], "sections": [{"heading": null, "text": "Index Terms\u2014Linked blind source separation, common and individual feature extraction, classification, clustering\nI. INTRODUCTION AND MOTIVATION\nTHE emergence of high-dimensional data structures re-quires new data analysis tools to be able to deal with the many aspects of this multifaceted problem, from data representation and interpretation to information retrieval. In this context, multi-block data analysis techniques are particularly interesting, as they accommodate multiple measurements of the same phenomenon under various experimentation conditions. For example, human electrophysiological signals in response to a certain stimulus, but from different subjects and trials, can be grouped together and naturally linked as multi-block data. Such data blocks share common information, and at the same time they also allow for individual data features to be kept. Intuitively, this common shared information should help to discover connections between members of a data ensemble\nManuscript received ........ This work was supported by the National Natural Science Foundation of China (grants 61103122, 61202155, 61333013, and U1201253), the Guangdong Natural Science Foundation (2014A030308009), and the JSPS KAKENHI (Grant No. 26730125).\nGuoxu Zhou is with the Laboratory for Advanced Brain Signal Processing, RIKEN, Brain Science Institute, Wako-shi, Saitama 3510198, Japan. E-mail: zhouguoxu@brain.riken.jp.\nAndrzej Cichocki is with the RIKEN, Brain Science Institute, Wako-shi, Saitama 3510198, Japan, and Systems Research Institute Polish Academy of Science, Warsaw, Poland. E-mail: cia@brain.riken.jp.\nYu Zhang is with the Key Laboratory for Advanced Control and Optimization for Chemical Processes, Ministry of Education, East China University of Science and Technology, Shanghai 200237, China (e-mail: yuzhang@ecust.edu.cn.\nDanilo Mandic is with the Communication and Signal Processing Research Group, Department of Electrical and Electronic Engineering, Imperial College, London, United Kingdom. E-mail: d.mandic@imperial.ac.uk.\nand can be used to characterize this data ensemble, while the individual features may help to recognize or identify each individual member of the data ensemble. The identification and separation of such common and individual information in order to employ the features highly relevant to the data analysis task at hand promises to significantly improve data analysis [1], [2], [3], [4]. For example, shared features among tasks have been exploited to improve the performance of supervised and semi-supervised learning [5], [6]. In this paper, we focus on an unsupervised learning framework for the common and individual features across multi-block data.\nCommon data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12]. CCA has also been generalized to multiple data sets in the context of blind source separation (BSS) and feature extraction [13], [14], [15], [16], [17]. For images, the population value decomposition (PVD) jointly analyzes same-size images [18], and can be considered as a special case of tensor (Tucker) decompositions, an active research topic in high-order data analysis and exploration [19]. Very recently, group independent component analysis (ICA) and independent vector analysis (IVA) were proposed to capture group variables from multiblock data [20], [21], [22]. Common to these methods is that they account for only correlated features within multiblock data, which limits their practical applications. The joint and individual variation explained (JIVE) [1] is a step in the right direction which simultaneously extracts both joint and individual variations across the members of a heterogeneous data ensemble. However, for the potential of JIVE to be fully exploited several issues need to be further addressed: (i) when common components are relatively weak, JIVE often gives principal components rather than true common components, thus compromising the fidelity of the extraction of true common components; (ii) rigorous quantitative analysis regarding the extracted common components is still missing; (iii) for heterogeneous data, JIVE has been mainly used for revealing gene-miRNA associations, but its potential in general machine learning applications remains unclear; iv) ways to improve data analysis performance by incorporating well-established component analysis tools also need further investigation.\nTo help resolve these issues, we propose a general framework for common and individual feature extraction (CIFE) for multi-block data, and provide:\n1) New efficient algorithms for common orthogonal basis extraction (COBE) from multi-block data. These new\nar X\niv :1\n21 2.\n39 13\nv4 [\ncs .C\nV ]\n1 2\nM ar\n2 01\n7\nalgorithms guarantee the identification of true common components and can be applied to separate common and individual subspaces of multi-block data; 2) A unifying framework for multi-block data analysis, which deals with heterogeneous data structures by respectively applying suitable well-established matrix factorization methods (such as blind source separation (BSS) [23] and nonnegative matrix factorization (NMF) [19]) to the common and individual subspaces separately rather than to the global space of a data ensemble. This allows for more effective information retrieval and feature extraction for multi-block data. 3) Two generic applications of CIFE\u2014classification and clustering, illustrating the extent to which a simultaneous use of common and individual features can improve the performance in practical applications.\nWe also provide an in-depth analysis of the links between COBE and related methods such as CCA and principal component analysis (PCA). The analysis shows that the introduced common feature extraction can be interpreted as higher-order correlation analysis, where PCA is performed on the common subspace shared by all the available data rather than on a single global data set, as in ordinary PCA.\nThe rest of the paper is organized as follows. In Section 2 the COBE method is discussed, including the problem statement, model, algorithms, and its links with CCA and other related methods. In Section 3 a general framework for CIFE is presented. The applications of CIFE in classification and clustering are presented in Section 4. Section 5 provides simulations on both synthetic and real-world data, verifying the proposed methods. Finally, concluding remarks and directions for future work are provided in Section 6.\nA conference summary of part of our results has been accepted for publication at ICASSP 2015 [24]. Relative to [24], this journal version includes detailed derivations, new algorithms, potential applications in machine learning, and further experiments."}, {"heading": "II. COMMON ORTHOGONAL BASIS EXTRACTION", "text": "Consider a set of matrices Y = {Yn \u2208 RD\u00d7Jn : n \u2208 N}, N = {1, 2, . . . , N}, and the following matrix factorization problem, whereby for each matrix Yn \u2208 Y , we seek\nmin An, Bn \u2225\u2225Yn \u2212AnBTn\u2225\u22252F , n \u2208 N , (1) where the Rn columns of An \u2208 RD\u00d7Rn represent the latent variables in Yn (sources, bases, loading, etc), and Bn \u2208 RJn\u00d7Rn denotes the corresponding coefficient matrix (mixing, encoding, etc). We assume that Rn < min(D,Jn), which implies that AnBTn provides a compact/compressed or low-rank representation of Yn.\nA number of matrix factorization techniques exist to solve (1), including PCA, BSS [23], however, these methods consider each matrix Yn separately. This is often both counterintuitive and physically restrictive, since the members of the data ensemble Yn are likely to be naturally linked, thus sharing some common components. We therefore propose to\nperform a simultaneous analysis of the ensemble Y , in order to obtain the latent variables in the form\nAn = [ A\u0304 A\u0306n ] , n \u2208 N , (2)\nwhere A\u0304 \u2208 RD\u00d7C , A\u0306n \u2208 RD\u00d7(Rn\u2212C), and C \u2264 min{Rn : n \u2208 N}. In other words, we assume that the sub-matrix A\u0304 contains the common components shared by all the matrices in Y while the sub-matrix A\u0306n contains the individual information in each Yn. This allows us to factorize the data matrices in Y in a linked way, so that\nYn \u2248 AnBTn = [A\u0304 A\u0306n] [ B\u0304Tn\nB\u0306Tn ] =A\u0304B\u0304Tn + A\u0306nB\u0306 T n . =Y\u0304n + Y\u0306n, n \u2208 N ,\n(3)\nwhere B\u0304n and B\u0306n are the partitions of the coefficients Bn corresponding to A\u0304 and A\u0306n. In this way, each data matrix Yn is represented through a combination of components from the common (shared) subspace Y\u0304n = A\u0304B\u0304Tn and the individual (intrinsic) subspace Y\u0306n = A\u0306nB\u0306Tn , as illustrated in Fig.1.\nOur objective is to find the common and individual components A\u0304 and A\u0306n which exhibit some desired properties from a given set of matrices Yn, n \u2208 N , without the knowledge of the mixing coefficients Bn and possibly even without knowing the number of common components C. Recall that two special cases of (3) have been extensively studied in the past decades: \u2022 The case C = 0, where no common components exist\nin Y so that the problem boils down to factorizing each matrix Yn \u2208 Y separately; \u2022 The case C = Rn for all Yn, which is equivalent to matrix factorization of a large global matrix created by stacking together all matrices Yn.\nIt is important to note that the solutions to (3) are not unique. In fact, for any invertible matrix Q\u0304 of appropriate size, we have Y\u0304n = (A\u0304Q\u0304)(Q\u0304\u22121B\u0304Tn ) which is also a solution to (3).\nTo reduce the solution space and to simplify the computation we shall consider the following three steps:\nStep 1: Consider the QR-decomposition of A\u0304 = UR such that UTU = I, where I is the identity matrix (we also use the symbol IC to denote the C-by-C identity matrix). Upon substituting into (3), we obtain\nAnB T n = [ U A\u0306n ] [RB\u0304Tn B\u0306Tn ] , n \u2208 N , (4)\na comparison between (3) and (4) makes it possible to assume that in (3) A\u0304T A\u0304 = I, without loss of generality.\nStep 2: Since our aim is to separate \u201cshared\u201d and \u201cindividual\u201d latent component subspaces, we can further assume that A\u0304T A\u0306n = 0, n \u2208 N , where 0 is a zero matrix, which implies no interaction between the common and individual subspaces, i.e. perfect separability of the common and individual features. This assumption is reasonable and will not introduce any additional factorization error. To see this, consider\nA\u0306n \u2261 A\u0304A\u0304T A\u0306n + (I\u2212 A\u0304A\u0304T )A\u0306n. (5) Substituting (5) into (3), we have\nAnB T n =A\u0304B\u0304 T n + A\u0306nB\u0306 T n\n=A\u0304B\u0304Tn + [A\u0304A\u0304 T A\u0306n + (I\u2212 A\u0304A\u0304T )A\u0306n]B\u0306Tn\n=A\u0304[B\u0304Tn + A\u0304 T A\u0306nB\u0306 T n ] + [(I\u2212 A\u0304A\u0304 T )A\u0306n]B\u0306 T n\n(6)\nUpon comparing (6) and (3) and defining A\u0306n . = (I\u2212A\u0304A\u0304T )A\u0306n and B\u0304n . = B\u0304n + B\u0306nA\u0306 T n A\u0304, we arrive at A\u0304\nT A\u0306n = 0, verifying that this assumption is not only reasonable, but also does not introduce any factorization error.\nStep 3: We consider the truncated singular value decomposition (SVD) of A\u0306n = Un\u039bnVTn , where U T nUn = I, VTnVn = I, and \u039bn \u2208 R(Rn\u2212C)\u00d7(Rn\u2212C) is invertible. Then A\u0304T A\u0306n = 0 \u21d4 A\u0304TUn = 0. Setting A\u0306n .= Un and B\u0306n . = \u039bnV T n B\u0306n, we have A\u0304\nT A\u0306n = 0 and A\u0306Tn A\u0306n = I. Based on the above three steps for a known of number of common components C, the general factorization problem in (3) can be reformulated as:\nmin A\u0304,A\u0306n \u2211 n\u2208N \u2225\u2225\u2225Yn \u2212 A\u0304B\u0304Tn \u2212 A\u0306nB\u0306Tn\u2225\u2225\u22252 F ,\ns.t. A\u0304T A\u0304 = IC , A\u0306 T n A\u0306n = IRn\u2212C ,\nA\u0304T A\u0306n = 0, n \u2208 N ,\n(7)\nwhere \u2016\u00b7\u2016F is the Frobenius norm of matrices. The JIVE method also solves the model (7), although this is not stated explicitly [1].\nThe factorization problem (7) has a very close relationship with PCA, as it naturally boils down to standard low-rank approximation of matrices (ordinary PCA) when Rn = C, \u2200n \u2208 N , so that A\u0304 = A can be found from\nmin A \u2211 n\u2208N \u2225\u2225Yn \u2212ABTn\u2225\u22252F , s.t. ATA = IC . (8) In this case, if the data matrices Yn are stacked together to form a global D \u00d7\u2211n Jn matrix Y\u0303 = [Y1 Y2 \u00b7 \u00b7 \u00b7YN ], and similarly B\u0303 = [ B1 B2 \u00b7 \u00b7 \u00b7 BN ] , then (8) can be viewed as a partitioned version of the global PCA of Y\u0303, that is\nmin A \u2225\u2225\u2225Y\u0303 \u2212AB\u0303T\u2225\u2225\u22252 F , s.t. ATA = IC . (9)\nIf Y\u0303 is too large for a computer memory, we may resort to (8) to perform PCA in practice.\nHowever when C < Rn, problem (7) is not equivalent to PCA any more. The key difference between the models in (7) and (8) is due to the individual parts A\u0306nB\u0306Tn , that is, common components found by (7) can also be interpreted as principal components of the common subspace Yn \u2212 A\u0306nB\u0306Tn . This clarifies that in each iteration the JIVE effectively performs joint PCA and individual PCA sequentially: (i) in the joint PCA step, the individual subspaces are removed prior to applying PCA to all data; (ii) the individual PCA step is performed on each single individual data after the common subspace has been removed. Since both the common and individual components are unknown, the JIVE method updates these two parts in an alternating manner.\nRemark 1: Notice that in (7) both An and Bn are need to be optimized, this is different from least squares where Bn are fixed.\nSince from (2)-(6) the separation of individual and common subspaces does not introduce any additional factorization error, by comparing (1) and (7), the matrix An = [ A\u0304 A\u0306n ] essentially gives the optimal rank-Rn approximation of Yn with separated common and individual subspaces. This means that (7) actually consists of two major functions: dimensionality reduction (or low-rank approximation) of each single data matrix Yn and separation of common and individual components of the data ensemble. The dimensionality reduction of Yn depends on each Yn only and its purpose is to capture the variation as much as possible; while the latter requires an integrated analysis of all data matrices with the purpose of extracting the common (or highly correlated) components shared by the data ensemble. The JIVE approach performs these two functions simultaneously by applying the alternating least squares (ALS) to (7) and quantifies the amount of joint variation between data types [1]. In other words, components extracted by JIVE are largely dominated by variances rather than correlations. As a result, the JIVE may fail to capture the common components with high correlations, especially when the common components in the data ensemble are relatively weak but are consistently present in all data sets. In contrast, we think it could be more natural to realize these two functions\nseparately. Hence, we consider a new two-step method to solve (7), which is also illustrated in Fig.2:\nStep 1: Dimensionality reduction: update the matrices Yn in (7) by their optimal rank-Rn approximation AnBTn by solving (1) separately for each Yn. We call the original Yn raw data and the reduced version Yn \u2190 AnBTn cleaned data;\nStep 2: Common and Individual Components Separation: solve (7) using the cleaned data. Therefore, because the procedure (2)-(6) that separates the common and individual subspaces does not introduce any factorization error, in theory, we have\nYn = AnB T n = A\u0304B\u0304 T n + A\u0306nB\u0306 T n , \u2200n \u2208 N , (10)\nwhere Yn is the cleaned data. Obviously, (10) holds if and only if A\u0304 contain only common components. This property can be further exploited to ensure the extraction of true common components, as detailed in Sections II-A and II-B. Additional important advantages of this two-step method include: (i) the dimensionality reduction is more flexible in the sense that any suitable methods can be applied; (ii) it can be performed in parallel when a large number of data sets are involved; and (iii) it simplifies the subsequent separation of the common and individual components.\nIn the sequel, we shall assume that Yn in (7) are the cleaned data processed by the above Step 1."}, {"heading": "A. The COBE Algorithm: The Number of Common Components C is Unknown", "text": "The estimation of the common components A\u0304 plays a central role in solving (7). In fact, once A\u0304 has been estimated, from (10), the coefficient matrices B\u0304n can be computed from1\nB\u0304n = ( YTn \u2212 B\u0306nA\u0306Tn ) A\u0304(A\u0304T A\u0304) \u22121 = YTn A\u0304, n \u2208 N , (11)\nwhich then allows us to compute the common subspace A\u0304B\u0304Tn and the individual subspace Y\u0306n = Yn \u2212 A\u0304B\u0304Tn , \u2200n \u2208 N , respectively.\nTo estimate A\u0304 efficiently, from (10) we have[ A\u0304 A\u0306n ] = YnB T n \u2020, ATnAn = IRn , n \u2208 N , (12)\nwhere An = [ A\u0304 A\u0306n ] , Bn = [ B\u0304n B\u0306n ] , and (\u00b7)\u2020 denotes the Moore-Penrose matrix pseudo-inverse. In other words, by finding appropriate transformation matrices BTn\n\u2020, we can obtain the desired common and individual component subspaces spanned by A\u0304 and A\u0306n, respectively.\nComputing the basis vectors of A\u0304. Let Yn = QnHn such that QTnQn = I; for each matrix Yn this only needs to be computed once by using, e.g., the QR decomposition or a truncated SVD of Yn. We next define Zn . = HnB T n \u2020, so that (12) becomes[ A\u0304 A\u0306n ] = (QnHn)B T n \u2020 = QnZn, n \u2208 N , (13)\n1If Yn = A\u0304B\u0304Tn + A\u0306nB\u0306 T n is exact as in (10), Equation (11) is also exact. Otherwise (11) is interpreted as the least squares solution of\nmin \u2225\u2225\u2225Yn \u2212 A\u0304B\u0304Tn \u2212 A\u0306nB\u0306Tn\u2225\u2225\u22252\nF . Similar reasoning also applies to equations\n(12) and (13).\nand hence for any n1, n2 \u2208 N , n1 6= n2, the following holds{ Qn1zn1,k = Qn2zn2,k = a\u0304k, if k \u2264 C; Qn1zn1,k 6= Qn2zn2,k, if k > C, (14)\nwhere zn,k and a\u0304k are the kth columns of Zn and A\u0304, respectively. From (14), the first column of A\u0304, denoted by a\u03041, can be calculated by solving:\nmin a\u03041, zn,1 f1 = \u2211 n \u2016Qnzn,1 \u2212 a\u03041\u20162F , s.t. a\u0304T1 a\u03041 = 1, (15)\nwhere the objective function f1 has to be very small in order to ensure that a\u03041 is a common basis vector, as governed by (10) and (13). Eq. (15) can be minimized by using alternating least-squares (ALS), whereby by first fixing zn,1 the optimal a\u03041 is found as\na\u03041 = \u2211\nn Qnzn,1, (16)\nwhich is then normalized to have a unit norm. Repeating for a fixed a\u03041 we obtain\nzn,1 = Q T n a\u03041, n \u2208 N , (17)\nand so on until convergence. A common column a\u03041 is considered to be found if min f1 \u2264 for a very small threshold \u2265 0; otherwise, no common basis exists in Y , and we terminate the iterations (16) and (17).\nUpon finding a set of common basis vectors a\u03041, a\u03042, . . . , a\u0304k, we need to ensure that repeated common basis vectors are not found when we seek the next common vector a\u0304k+1. This is achieved by considering the following useful property of Zn. Let Zn,C . = [ zn,1 zn,2 . . . zn,C ] , then from (13) we have\nZTn,CZn,C = Z T n,CQ T nQnZn,C = A\u0304 T A\u0304 = I. (18)\nIn other words, zTn,k+1Zn,k = 0, and zn,k+1 is in the null space of ZTn,k, which allows us to update Qn as\nQ(k+1)n = Qn(I\u2212 Zn,kZTn,k) = Q(k)n (I\u2212 zn,kzTn,k),\n(19)\nwhere Q(1)n = Qn. This then yields a\u0304k+1 through solving\nmin a\u0304(k+1), zn,k+1 fk+1 = \u2211 n \u2225\u2225Qk+1n zn,k+1 \u2212 a\u0304k+1\u2225\u22252F , s.t. a\u0304Tk+1a\u0304k+1 = 1. (20)\nThe minimum of fk+1 can be obtained by repeating the procedure2 in solving (15). We distinguish between the following two cases:\n1) For min fk+1 \u2264 , a new common basis vector a\u0304k+1 is found. We then update Q(k+1)n using (19) and then solve (20) to seek the next common basis vector. 2) Otherwise, no common basis vector exists any more and a total of C = k common orthogonal basis vectors are found as A\u0304 = [ a\u03041 a\u03042 \u00b7 \u00b7 \u00b7 a\u0304C ] .\nWe refer to the above procedure for finding sequentially an orthogonal basis of the common space as the common\nAlgorithm 1 The COBE Algorithm Input: Yn, n \u2208 N , \u2265 0.\n1: Let Yn=QnHn such that QTnQn = IRn for all n \u2208 N . 2: A\u0304 = [ ], Q(1)n = Qn, and k = 1. 3: while fk \u2264 do 4: while not converged do 5: a\u0304k = \u2211 n Q (k) n zn,k/ \u2225\u2225\u2225\u2211n Q(k)n zn,k\u2225\u2225\u2225 F ; 6: zn,k = [Q (k) n ]T a\u0304k, n \u2208 N ; 7: end while 8: fk = \u2211 n \u2225\u2225\u2225Q(k)n zn,k \u2212 a\u0304k\u2225\u2225\u22252 F ;\n9: A\u0304 = [ A\u0304 a\u0304k ] ;\n10: k = k + 1; 11: Q (k) n = Q (k\u22121) n (I\u2212 zn,k\u22121zTn,k\u22121), n \u2208 N . 12: end while 13: return A\u0304 = [ a\u03041 a\u03042 \u00b7 \u00b7 \u00b7 a\u0304C ] , where C = k \u2212 1.\northogonal basis extraction (COBE), which is outlined in Algorithm 1.\nRemark 2: The parameter controls the degree of similarity (i.e., correlation, in this paper) of the extracted components YnY \u2020 na\u0304k \u2248 a\u0304k. If = 0, the extracted components are exactly the same and equal to a\u0304k, otherwise, approximately identical highly correlated components are extracted (see Section 2.5 for a detailed discussion).\nRemark 3: The basis matrix A\u0304 is not unique as A\u0304U forms another basis matrix that also spans the common subspace, here U is an arbitrary orthogonal matrix with proper size.\nWith the intuition that fk \u2248 0 if and only if k \u2264 C (see (14)), the threshold should be sufficiently small to ensure that the extracted A\u0304 does contain only common components. In practice, we may specify a very small value of such that the number of common components is overestimated. Then the Second ORder sTatistic of the Eigenvalues (SORTE) method [25], which was proposed to detect the gap between the eigenvalues corresponding to the signal space and those belonging to the noise space, can be applied here to estimate the number of common components. Suppose we have overestimated a total of K \u2018common\u2019 components a\u0304k, k = 1, 2, . . . ,K with Rn \u2265 K > C, by using Algorithm 1, and the corresponding errors are fk given in (20) or reevaluated as fk . = 1N \u2211N n=1\n\u2225\u2225YnY\u2020na\u0304k \u2212 a\u0304k\u2225\u22252F . Then we apply SORTE to\nf1, f2, . . . , fK , (21)\nto detect the gap between the common and individual subspaces, as illustrated in Section V and in the first simulation.\n2For k > 1 the matrix Q(k)n is not any more orthogonal. However, it can be verified that Q(k)n T is the More-Penrose pseudo-inverse of Q (k) n , thereby leading to the least squares solution zn,k = Q (k) n T a\u03041, i.e. (17).\nAlgorithm 2 The COBEC Algorithm Input: C and Yn, n \u2208 N .\n1: Let Yn=QnHn such that QTnQn = IRn for all n. 2: Initialize Zn randomly. 3: while not converged do 4: P = \u2211 n\u2208N QnZn. 5: A\u0304=EVT , where [E, \u039b, V] = tSVD(P, C). 6: Zn \u2190 QTn A\u0304. 7: end while 8: return A\u0304."}, {"heading": "B. The COBE Algorithm When The Number of Common Components C is Given", "text": "For a given C, following the analysis in Section II-A, the common components can be found by solving:\nmin Zn, A\u0304 N\u2211 n=1 \u2225\u2225QnZn \u2212 A\u0304\u2225\u22252F , s.t. A\u0304T A\u0304 = I, (22) through alternating optimization with respect to Zn and A\u0304. In this way, when A\u0304 is fixed, the optimal Zn is computed from\nZn \u2190 QTn A\u0304, n \u2208 N ; (23) while when Zn, n \u2208 N , are fixed, (22) is equivalent to\nmax A\u0304\ntrace(PT A\u0304), s.t. A\u0304T A\u0304 = I, (24)\nwhere trace(\u00b7) denotes the trace of a matrix and P =\u2211N n=1 QnZn. To solve (24), let P = E\u039bVT \u2208 RD\u00d7C be the truncated SVD (tSVD) of P, where \u039b = diag(\u03bb1, \u03bb2, . . . , \u03bbC) \u2208 RC\u00d7C is a diagonal matrix with \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbC > 0. Motivated by the work3 in [26] (page 601), the optimal solution of (24) becomes\nA\u0304 = EVT , (25)\nwhich is easy to see by considering\ntrace(PT A\u0304) = trace(V\u039bET A\u0304) = trace(\u039b(ET A\u0304V)).\nIn fact, as ETE = I and (A\u0304V)T (A\u0304V) = I, we have [ET A\u0304V]ii \u2264 1, which means that trace(\u039b(ET A\u0304V)) \u2264\u2211C\ni=1 \u03bbi. Clearly, when A\u0304 = EV T , then A\u0304T A\u0304 = I,\nET A\u0304V = I and trace(PT A\u0304) reaches its upper bound\u2211C i=1 \u03bbi. We refer to this algorithm as the COBEC and its pseudocode is given in Algorithm 2."}, {"heading": "C. Pre-processing: Dimensionality Reduction", "text": "Like CCA, COBE is meaningless if Rn = D for all n, since for any D \u00d7 D invertible matrix A\u0304 there always exist matrices B\u0304n such that Yn = A\u0304B\u0304Tn , i.e., any D \u00d7 D invertible matrix forms a common basis. For this reason in model (7) the condition Rn < D is required for all Yn. This requirement is not too restrictive and has physical justification, as for real-world data of high dimensionality, the latent rank is often significantly lower than the dimensionality\n3The main difference is that here A\u0304 is not necessarily square.\nof the observed data. If Rn < D is not satisfied, we need to perform dimensionality reduction (such as PCA) by, e.g. solving (1) prior to applying COBE, which justifies the necessity of the proposed two-step method. However, this is not the only reason to apply dimensionality reduction\u2014it also helps to reduce the computational complexity, and particularly, to reduce noise, outliers, and artifacts in the original raw data. After dimensionality reduction, we obtain cleaned data Yn \u2248 AnBTn with rank(An) = Rn < D, and can apply COBE to the cleaned data to obtain AnBTn = A\u0304B\u0304 T n +A\u0306nB\u0306 T n . Notice that if AnBTn is interpreted as the PCA of matrix Yn, COBE simply rotates/transforms the columns of An so that the common subspace and the individual subspace are completely disjoint, as illustrated in Fig.2.\nRemark 4: Standard PCA performs dimensionality reduction for i.i.d. Gaussian noise; for sparse distributions we may use robust PCA (RPCA) [27], while the number of latent components Rn can be estimated using, e.g. SORTE [25], before applying PCA."}, {"heading": "D. Relation to Other Methods", "text": "To illustrate the relation between COBE and CCA, recall that for data matrices Y1 and Y2, CCA finds the vectors w1 and w2 which maximize the correlation \u03c1 = corr(Y1w1,Y2w2), while COBE extracts only components for which the correlation is higher than a specified threshold. We next shed further light on this relationship by introducing the following bound on the correlations of the variables involved.\nProposition 1: Let Yn be row-centered (i.e., with zero mean) random variables. Suppose that \u2016Ynwn \u2212 a\u0304\u2016F \u2264 <\u221a\n2 \u2212 1 with \u2016a\u0304\u2016F = 1, \u2200n \u2208 N , then for \u2200m,n \u2208 N , we have\ncorr(Ymwm,Ynwn) \u2265 1\u2212 (2 + 2) 1 + (2 + 2) > 0. (26)\nProof: From \u2016a\u0304\u2016F = 1 and \u2016Ynwn \u2212 a\u0304\u2016F \u2264 < 1, we have\n0 < 1\u2212 \u2264 \u2016Ynwn\u2016F \u2264 1 + , \u2200n \u2208 N . (27)\nMoreover, \u2200m,n \u2208 N , the following holds\n\u2016Ymwm \u2212Ynwn\u2016F \u2264\u2016Ymwm \u2212 a\u0304\u2016F + \u2016Ynwn \u2212 a\u0304\u2016F \u22642 .\n(28)\nHence,\n2wTmY T mYnwn\n= \u2016Ymwm\u20162F + \u2016Ynwn\u2016 2 F \u2212 \u2016Ymwm \u2212Ynwn\u2016 2 F \u22652(1\u2212 )2 \u2212 4 2, (29)\nand from (27) and (29), we have\ncorr(Ymwm,Ynwn) = wTmY T mYnwn\n\u2016Ymwm\u2016F \u2016Ynwn\u2016F \u2265 1\u2212 2 \u2212 2\n\u2016Ymwm\u2016F \u2016Ynwn\u2016F \u2265 1\u2212 2 \u2212 2\n(1 + )2\n= 1\u2212 2 \u2212 2 1 + 2 + 2 .\nIn other words, if fi in (15) and (20) are upper bounded, this, in turn, leads to lower bounded correlations between the projected variables Ynwn, n \u2208 N . Since corr(Y1w1,Y2w2)\u2192 1 as \u2192 0, COBE can be interpreted as a higher order correlation analysis (HCA) method. Fig.3 illustrates the difference in operation between COBE and CCA for multiple data sets. In this simulation we first generated two matrices Sn \u2208 R1000\u00d710, n = 1, 2. The first column of S1 was s1,1(t) = sin(0.01t), and that of S2 was s2,1(t) = sign(s1,1(t)), t = 1, 2, . . . , 1000. Other entries were drawn from independent standard normal distributions. The matrices Sn were mixed via different coefficient matrices Mn \u2208 R10\u00d710, n = 1, 2, whose entries were drawn from independent standard normal distributions such that Yn = SnMTn (n = 1, 2). Apparently, s1,1(t) and s2,1(t) are highly correlated. Hence, by applying CCA we obtained the first pair of canonical variables s\u0302n,1(t) = Ynwn with the correlation corr(\u0302s1,1(t), s\u03022,1(t)) = 0.8867, where wn were the corresponding canonical coefficients, n = 1, 2. The red line in Fig.3(a) shows the common components a\u0304 extracted by COBE, together with the blue and green lines corresponding highly correlated components extracted from Yn, i.e., Yn(Y\u2020na\u0304), n = 1, 2, which matched very well the canonical variables obtained by CCA (For this reason, we also call Yn(Y\u2020na\u0304) normalized common components). However, COBE will extract only the components with very high correlations, as stated in Proposition 1. From the figure, a\u0304 can be interpreted as the principal component of the normalized common components, the information that is not provided in CCA.\nFurther to showing in (7) and (8) that COBE has a close relation with PCA, Fig.4 illustrates the difference between COBE, JIVE and PCA using the same data as in Fig.3. The principal component was computed from a concatenated version of Y\u0303, defined in (9). Basically, COBE identifies the principal components A\u0304 of normalized common components (corresponding to the canonical variables in CCA), whereas PCA seeks the principal components of the global data set Y , and JIVE captures the joint variation. In this example, JIVE gave principal components of sorts, rather than common components, illustrating that the power of components dominates the results obtained by JIVE, see Fig.4. In this sense, COBE is closer to CCA while JIVE is closer to PCA (and PLS); COBE can also be viewed as a regularized version of JIVE. In terms of computational demands, compared with JIVE which in the computation of the common subspace involves frequent SVDs of huge matrices containing all data, COBE (COBEC) is more\nefficient in the optimization and more physically intuitive and flexible in the estimation of number of common components."}, {"heading": "E. Scalability For Large-Scale Problems", "text": "For large-scale data the indices D and Jn (n \u2208 N ) in (1) are quite large. First, recall that in (15), (20), and (22) we use the dimensionality reduced matrices Q \u2208 RD\u00d7Rn with Rn < D, and hence the value of Jn is generally not an issue. On the other hand, for a very large D, the time and memory requirements of COBE can be reduced in the following way. Let P \u2208 RDP\u00d7D be a random matrix with maxn\u2208N (Rn) < DP D. Then, from (12) we can first solve the model:\nmin \u2211 n\u2208N \u2225\u2225YPnWn \u2212 A\u0304P\u2225\u22252F (30) where YPn = PYn \u2208 RDP\u00d7Jn is much smaller than Yn, and A\u0304P = PA\u0304. After the matrices Wn have been estimated by using COBE or COBEC, the corresponding common basis can be computed from A\u0304 = YnWn. Obviously, PYnWn = PA\u0304 as long as YnWn = A\u0304, in other words, no common basis vectors are lost. In the worst case, this approach may give fake common components a\u0304k when Ynwn,k\u2212a\u0304k occasionally lies in the null space of P. In practice, this is not an issue, as these fake common components can be easily detected by examining the value of \u2016Ynwn,k \u2212 a\u0304k\u20162F ."}, {"heading": "III. COMMON AND INDIVIDUAL FEATURE EXTRACTION", "text": "(CIFE)\nWe shall now illuminate the versatility of the COBE approach over several established feature extraction paradigms."}, {"heading": "A. Linked BSS with Pre-whitening", "text": "We have so far considered the orthogonal components A\u0304, this however does not guarantee unique common components as the columns of A\u0304U also form a common orthogonal basis for any orthogonal matrix U. If our aim is to project the common components onto a feature space with some desired properties (uniqueness), this can be achieved by BSS [23], which finds latent variables S from their linear mixtures Y = SMT such that\nS\u0302 = \u03a8(Y) = SP\u039b (31)\nwhere \u03a8 denotes a suitable BSS algorithm, M is the unknown mixing matrix, while P and \u039b are a permutation matrix and a scaling matrix, respectively, that model the unavoidable ambiguities of BSS. Assuming that the latent sources F\u0304 satisfy\nY\u0304n = F\u0304M\u0304 T n , (32)\nwhere Y\u0304n = A\u0304B\u0304Tn , we have\nA\u0304 = F\u0304(B\u0304\u2020nM\u0304n) T , (33)\nillustrating that the columns of A\u0304 are simply linear mixtures of the sources F\u0304, so that F\u0304 can be estimated via BSS as\nF\u0304 = \u03a8(A\u0304). (34)\nIn this case A\u0304 is a pre-whitened version of (32), which stems from (33) and the fact that A\u0304T A\u0304 = I. The major advantage of using BSS in this context is that we may obtain common features which exhibit some desired properties such as sparsity, independence, nonnegativity. This is achieved by imposing appropriate constraints and penalties on F\u0304. Moreover, we may extract even more than C common signals from A\u0304 or X\u0304n, which is a challenging problem referred to as underdetermined BSS. To this end, for example, we can apply the novel tensor based approach proposed in [28]. This method advanced the study of this topic in the sense that it can exactly recover as many as 2C\u22121 sources from C observations at every autoterm time-frequency points, no matter how many active sources there are, which is so far the state-of-the-art for the separation of nonstationary sources. In summary, the BSS procedure adds significantly increased versatility and flexibility to the COBE approach. We refer to the above procedure that combines COBE and BSS as linked BSS to indicate that we perform BSS on multi-block linked data Yn.\nNote that the JBSS method in [14] also performs BSS involving multi-block data; it extracts a group of signals with the highest correlations each time, and requires that the extracted groups have distinct correlations. In other words, the JBSS method can be viewed as a way to realize BSS by applying multiple-set CCA. In contrast, the proposed linked BSS method extracts a common basis first, it then applies ordinary BSS to discover common components with some desired properties and diversities. Recently in neural science, group ICA and independent vector analysis (IVA) have been widely applied to capture common (group) variables or components from a group (set) of data matrices [29], [21], [22]. These approaches are somewhat related to our proposed linked BSS model, however, they apply ICA to a global data space, which implicitly assumes that all data matrices are spanned by only common statistically independent components. The linked BSS differs from these methods because: (i) taking into account that the data matrices in a group not only share some common components but may also contain individual components, the linked BSS can capture more reliable group variables as it performs BSS on the common subspace instead\nof on a global data space; (ii) the linked BSS is capable of capturing components with various diversities or properties. In other words, within the linked BSS we may apply not only ICA, but also NMF (see Section 3.2) or any other suitable component analysis methods. These two distinguishing properties make our linked BSS more flexible and versatile and allow us to extract more physically meaningful components."}, {"heading": "B. Common Nonnegative Features Extraction (CNFE)", "text": "For nonnegative latent sources F\u0304, we cannot apply NMF methods on A\u0304 directly. Instead, we need to first extract the common subspace using (11), i.e., Y\u0304n = A\u0304A\u0304 T Yn, and subsequently use the following low-rank approximation based (semi-) nonnegative matrix factorization (NMF) model [30]:\nmin \u2211 n \u2225\u2225F\u0304M\u0304Tn \u2212 A\u0304B\u0304Tn\u2225\u22252F , s.t. F\u0304 0. (35) The subsequent use of low-rank NMF (if M\u0304n is also nonnegative) or low-rank semiNMF (where M\u0304n is real-valued) allows us to extract the common nonnegative components F\u0304. For example, the following iterative multiplicative update rules produces nonnegative components F\u0304 and M\u0304:\nF\u0304\u2190 F\u0304~ [A\u0304 \u2211 n (B\u0304TnM\u0304n)]+ F\u0304( \u2211 n M\u0304TnM\u0304n),\nM\u0304n \u2190 M\u0304n ~ [B\u0304n(A\u0304T F\u0304)]+ M\u0304n(F\u0304T F\u0304), n \u2208 N , (36)\nwhere ~ and are element-wise product and division of matrices, see [30] for detailed convergence analysis. Similar to BSS, we may impose additional constraints to extract unique nonnegative components, see [31], [32], [33], [34].\nC. Individual Feature Extraction (IFE)\nBesides the above common features F\u0304 or A\u0304 extracted using common feature extraction (CFE) methods, each data matrix also has its own individual features contained in the matrix Y\u0306n = Yn \u2212 Y\u0304n; these are often helpful in classification and recognition. Notice that although Y\u0306n is of the same size as Yn, it is rank deficient since rank(Y\u0306n) + rank(Y\u0304) = Rn. Hence, dimensionality reduction of Y\u0306n should be carefully\naddressed prior to further analysis. To estimate A\u0306n and B\u0306n, we can apply any standard dimensionality reduction method discussed in Section 2.4 (and the associated rank estimation techniques) to each Y\u0306n separately, followed by BSS or related methods to extract the features in A\u0306n and B\u0306n. However, there is a major difference between the dimensionality reduction methods considered here and those in the pre-processing stage. In the pre-processing stage dimensionality reduction is rather general-purpose and relatively simpler, whereas here the dimensionality reduction is related to a specific task at hand. For example, we may wish to visualize data in a lowdimensional space [35], or to extract discriminative information, or to establish neighbor relationship. The above procedure is referred to as individual feature extraction (IFE), as the extracted features are only presented in individual datasets.\nFig.5 shows the concept of the proposed common and individual feature extraction (CIFE)."}, {"heading": "IV. EXAMPLE APPLICATIONS", "text": ""}, {"heading": "A. Classification Using Common Features", "text": "In classification and pattern recognition, training data contain training samples and their labels, while the objects belonging to the same category naturally share some common features. More specifically, for a set of common features extracted from the kth category, k \u2208 K = {1, 2, . . . ,K}, denoted by F\u0304k, upon arrival of a new test sample yt \u2208 RD, we can compute its matching score rt(k) with each F\u0304k as:\nrt(k) = Matching(yt, F\u0304k), k \u2208 K. (37) Since the samples within a certain class should share some features, the label of yt is estimated as\nlt = arg max k\u2208K rt(k). (38)\nThe matching score rt(k) can be defined in many ways, such as the Euclidean distance or correlation (angle) between yt and the space spanned by F\u0304k, which can be solved via leastsquares and CCA, respectively. See Fig.6 for the diagram."}, {"heading": "B. Clustering Using Individual Features", "text": "Cluster analysis assigns a set of objects to clusters in such a way that the objects belonging to the same cluster are most similar. Unlike classification, clustering employs an unsupervised learning approach. In the clustering analysis, it is usual that all the samples have some common features although they may be from different clusters. For example, in human face image analysis, every face has common facial features such as cheek, nose, eyes, and mouth, whose shapes and locations are similar. Their common features are not meaningful for clustering as they do not provide any discriminative information. It is therefore logical to first remove these common/similar features across all the samples and then to use their individual features to cluster the objects.\nFig.7 shows that COBE incorporating CNFE is capable of extracting common faces (features) in the PIE database (details are given in the next section). We empirically set C = 2 and used CNFE to extract common nonnegative\ncomponents; the common faces in Fig.7(a) contain some basic features of human faces. On the other hand, Fig.7(b) shows the accentuated individual local features. These individual features are quite helpful to improve the accuracy of clustering and recognition. In our individual feature based clustering method we used the following steps:\n1) Randomly split the samples yt into N groups to construct Yn, where t \u2208 T = {1, 2, . . . , T} and n \u2208 N ; 2) Apply COBE to extract the common features A\u0304 of {Yn, n \u2208 N}; 3) Remove the common features from Yn by letting Y\u0306n = Yn \u2212 Y\u0304n = Yn \u2212 A\u0304A\u0304TYn;\n4) Perform dimensionality reduction and feature extraction on [ Y\u03061 Y\u03062 \u00b7 \u00b7 \u00b7 Y\u0306N ] to obtain the features F\u0306 =[\nf\u03061 f\u03062 \u00b7 \u00b7 \u00b7 f\u0306T ] ;\n5) Apply clustering algorithms on {f\u0306t, t \u2208 T }, where f\u0306t corresponds to the original objects yt."}, {"heading": "V. SIMULATIONS AND VALIDATION", "text": "Linked BSS. In this simulation4 we generated a total of ten matrices Sn \u2208 R5000\u00d710, n = 1, 2, . . . , 10, for which the first four columns were speech signals included in the ICALAB benchmark (named Speech4.mat) [36], and the other six components were drawn from independent standard normal distributions. The entries of the mixing matrices Mn \u2208 R50\u00d710 were also drawn from independent standard normal distributions. We used the model Yn = SnMTn + En, where En contains white Gaussian noise (SNR=20dB), and first employed the COBE, JIVE [1], JBSS [14], and PCA methods5 to extract the common bases A\u0304, followed by the SOBI method [37] to extract the latent common speech signals S from A\u0304, see Section III-A. TABLE I shows the quantitative performance averaged over 50 Monte-Carlo runs, where separation accuracy SIRi, i.e., the signal-to-interface ratio (SIR) of the ith estimated signal, was measured via\nSIR(s, s\u0302) = 10 log10\n\u2211 t s 2 t\u2211\nt(st \u2212 s\u0302t)2 , (39)\nwhere s and s\u0302 are normalized random variables with zero mean and unit variance, and s\u0302 is an estimate of s. Observe that JIVE and COBE achieved higher SIRs than JBSS and PCA, although the performance of JBSS improved after incorporating SOBI. Moreover, although PCA has a close relation with COBE, the common features extracted by PCA are often contaminated by individual features; also while COBE and JIVE achieved almost the same separation accuracy, COBE was much faster (The time for dimensionality reduction has been included for all algorithms). Moreover, after we reduced the power of common components such that s\u0304Tn s\u0304n = 100, n = 1, 2, 3, 4, COBE obtained similar accuracy whereas JIVE failed, which conforms with the analysis in Section 2.4. Also, the performance of JIVE was sensitive to the correct estimate of the rank of joint/common and individual components. If the ranks of individual components were mis-specified, for instance as 7 (denoted as JIVE\u2217 in TABLE I), instead of the actual 6, JIVE took more than 77 seconds to converge. Since the estimation of the number of components in [1] is quite time consuming and the performance is sensitive to selection of its parameters (for this instance, JIVE took more than two hours to estimate the rank); this limits the practical applications of JIVE for large-scale problems.\nThe distinguishing properties of COBE verified by this simulation example include:\n\u2022 COBE is able to identify true common subspace even if the common components are relatively weak; \u2022 Computational complexity of COBE depends only on the natural parameters: the size of the data and the number of common components C, making it much more efficient than JIVE;\n4The MATLAB code is available at: http://bsp.brain.riken.jp/\u223czhougx/ resources/mcode/demo CIFE.zip.\n5To achieve higher separation accuracy, we used the princomp function included in MATLAB to perform PCA. For COBE/COBEC, we simply used tSVD to perform dimensionality reduction, which has led to improved efficiency against our earlier version published at http://arxiv.org/abs/1212.3913.\n\u2022 Finding the number of common components in COBE is physically intuitive and simple; \u2022 The threshold within COBE also has physical interpretation\u2014the degree of similarity between the components.\nFig.8 illustrates the underlying principle of the estimation of the number of components by tracking the parameter fi, while Fig.9 illustrates the average (over 50 runs) performance in terms of the running time and separation accuracy of COBE with the observations projected onto a lower DPdimensional space by multiplying with an DP \u00d7 D random matrix P. As desired, the running time was almost linear in the dimension of the projected space Dp, illustrating that we may use projections to significantly improve the efficiency of COBE when the number of observations D is very large.\nDual-energy X-ray image decomposition. Dual-energy chest X-ray imaging is a diagnostic tool for early signs of lung cancer [38], however, the ribs, clavicles overlapped with soft tissues, and environmental noise make it challenging to detect affected lung nodules. To this end, we assumed a mixing model of bones, soft tissues and noise, where the former two were considered nonnegative common components. We considered four sets of sources Sn \u2208 R26,896\u00d710, n = 1, 2, 3, 4, for which the first two common components were respectively\nthe soft and bone tissues (both nonnegative) and the remaining eight components were drawn from independent uniform distributions between 0 and 1 to model the interference. The elements of nonnegative mixing matrices Mn \u2208 R20\u00d710, n = 1, 2, 3, 4, were also drawn from independent uniform distributions between 0 and 1. Then four set of observations were generated by using Yn = SMTn . The sources in this example were highly correlated and could not be separated by ICA methods, while the presence of random dense noise makes the separation by ordinary NMF on each single set of mixtures difficult. We applied COBE to extract a basis of common sources (soft tissues and bones) followed by CNFE to separate the soft tissues and bones. One typical realization is shown in Fig.10(b), while Fig.10(d) displays four samples of nonnegative components extracted by nLCA-IVM [38]. Owing to dense noise, the identifiability conditions of nLCAIVM were not satisfied, and nLCA-IVM could not extract the desired source images, while COBE performed nonnegative high correlation analysis well.\nClustering Analysis Using Individual Features. We considered two data sets:\nExtended Yale Database B6. The database contains 16,128 images of 28 human subjects under 9 poses and 64 illumination conditions [39]. A total of 5,820 approximately frontal\n6[Online]: http://vision.ucsd.edu/\u223cleekc/ExtYaleDatabase/ExtYaleB.html.\nfaces were automatically detected by using the classification and regression tree analysis (CART) and cropped for our clustering analysis.\nPIE Database7. This database is a collection of face images of 68 persons taken under different poses, illumination conditions, and expressions. We used a pre-processed version from [40] which consists of 2,856 full frontal face gray scale images taken at the pose c27.\nAll images were re-scaled to the size of 32 \u00d7 32. We randomly selected K clusters each time, and repeated the experiment 50 times for each selected K. In each run, the images were first permuted randomly and then split into N = b T50c groups to form multi-block data Yn with Jn \u2248 50, n = 1, 2, . . . , N , and T is the number of faces, (each group consisted of face images from unknown different clusters). COBE was used to extract the common features followed by CNFE to obtain nonnegative common features. The number of common components was specified as 2 in all experiments, and the two t-SNE components of their individual parts Y\u0306n were used to cluster the data by using K-means (see [41] for the t-SNE method). As K-means is influenced by initial centers of clusters, we replicated K-means 20 times in each run. Two widely used performance indices: Accuracy (%) and Normalized Mutual Information (NMI) were adopted to evaluate the clustering results, see [40] for detailed definitions of these two metrics. The proposed method was compared with PCA with K principal components, GNMF [40] (using their recommended settings), and the improved MinMax Cut (MMCut) method [42]. Except for MMCut that returns the cluster indicators directly, all the other methods used the K-means to cluster the features extracted by them with the same configuration. To illustrate that the performance of the proposed method was not completely due to t-SNE, two t-SNE components of the original data were also used as features for clustering. The clustering performance of these algorithms is detailed in TABLE II and III, respectively, showing that after removing the common features pertaining to all samples, the clustering performance were significantly improved.\nIn Fig.11 we next illustrate how the number of common components, C, influenced the clustering performance , where the values of C varied from 1 to 15. For the PIE dataset we used the first 40 categories while for the Extended Yale database B we used the first 20 categories. From the figure, for both datasets the clustering performance was significantly improved after removing the first 2-3 common components. While how to set the optimal parameter C blindly still remains challenging for practical applications; empirical results suggest that performance degradation degeneration caused by the overestimation of C is not significant if our interest is the individual features. To show this, consider\nY\u0306n =Yn \u2212 \u2211C\nc=1 a\u0304cb\u0304\nT n,c =Yn \u2212 \u2211C\nc=1 a\u0304c(a\u0304\nT c Yn).\n(40)\nwhere a\u0304c and b\u0304n,c are the cth column of A\u0304 and B\u0304n, respectively. For an overestimated C the correlations between a\u0304C and\n7[Online]: http://vasc.ri.cmu.edu/idb/html/face/.\nTABLE III: Clustering Performance on PIE\nk Accuracy (%) Normalized Mutual Information (%)PCA tSNE GNMF MMCut CIFE PCA tSNE GNMF MMCut CIFE 10 79.9\u00b1 0.5 34.6\u00b1 0.3 94.2\u00b1 0.8 63.7\u00b1 2.7 99.3 \u00b1 0.2 82.0\u00b1 0.3 39.8\u00b1 0.5 92.7\u00b1 0.7 71.7\u00b1 1.4 98.9 \u00b1 0.3 20 86.8\u00b1 1.2 29.4\u00b1 0.7 90.5\u00b1 1.1 61.4\u00b1 0.5 100.0 \u00b1 0.0 88.3\u00b1 1.0 47.7\u00b1 0.2 92.8\u00b1 0.4 76.5\u00b1 0.9 100.0 \u00b1 0.0 30 74.8\u00b1 0.0 40.8\u00b1 0.0 83.9\u00b1 0.0 60.2\u00b1 0.0 90.6 \u00b1 0.0 85.3\u00b1 0.0 60.2\u00b1 0.0 92.4\u00b1 0.0 77.0\u00b1 0.0 96.6 \u00b1 0.0 40 67.8\u00b1 2.4 33.5\u00b1 0.9 74.7\u00b1 2.3 62.7\u00b1 0.2 86.3 \u00b1 0.2 83.6\u00b1 1.2 57.0\u00b1 1.0 88.5\u00b1 0.7 78.8\u00b1 0.2 95.5 \u00b1 0.1 50 72.3\u00b1 0.7 36.4\u00b1 0.8 73.7\u00b1 0.6 60.4\u00b1 0.0 83.1 \u00b1 0.2 86.6\u00b1 0.2 61.1\u00b1 0.4 87.7\u00b1 0.3 79.1\u00b1 0.2 94.5 \u00b1 0.1 60 76.3\u00b1 0.4 35.0\u00b1 0.6 75.4\u00b1 0.5 58.4\u00b1 0.3 84.3 \u00b1 0.6 87.2\u00b1 0.2 62.1\u00b1 0.3 88.4\u00b1 0.3 79.2\u00b1 0.0 94.6 \u00b1 0.1 68 77.8\u00b1 0.5 30.2\u00b1 0.6 71.8\u00b1 1.1 60.3\u00b1 0.1 85.3 \u00b1 0.1 86.6\u00b1 0.2 60.6\u00b1 0.1 87.2\u00b1 0.3 80.2\u00b1 0.1 95.0 \u00b1 0.0\nAvg. 76.5 34.3 80.6 61.0 89.8 85.6 55.5 90.0 77.5 96.4\nNumber of common components C 2 4 6 8 10 12 14\nC lu\nst er\nin g\nac cu\nra cy\n( %\n)\n30\n40\n50\n60\n70\n80\n90\n100\nExtended Yale B (K=20) PIE (K=40)\nthe components in Yn are quite small (close to 0, ideally), and b\u0304n,C = a\u0304 T CYn, i.e., the projection of Yn on a\u0304C becomes very small. In other words, the loss of individual features tends to be very small if C has been slightly overestimated.\nApplications in classification. The classification performance of the proposed method was evaluated by using the ETH-80 Dataset. The ETH-80 dataset consists of a total of 3,280 images grouped in 8 categories containing 10 objects with 41 views per object, spaced equally over the viewing\nSize of training data (%) 20 30 40 50 60\nA cc\nur ac\ny (%\n)\n10\n20\n30\n40\n50\n60\n70\n80\n90\nCOBEC KNN SVM SLDA ERE\nFig. 13: Mean values and standard derivations for the accuracy in the classification of the ETH-80 database over 20 random runs.\nhemisphere [43]. Each category contains 10 different objects. Although the objects belonging to the same category share some common features, they also have their individual features different from the other objects in the same category, which makes this database widely adopted to evaluate classifying methods. See Fig.12 for the 8 categories in ETH-80 and the 10 objects in the forth category. We compared our CIFE based classifier (see Fig.6) with the KNN classifier included in MATLAB 2010b, the SVM classifier [44], the Eigenfeature Regularization and Extraction method [45] (ERE), and the shrinkage LDA [46] (SLDA). As the ERE method needs to perform eigenvalue decomposition of full covariance matrices, we used the grayscale images with the size of 32 \u00d7 32 to avoid ERE running out of memory. For the KNN classifier 1 nearest (measured by correlation) neighbor was used for classification. For SVM, we used 5-fold cross validation and\nthe best parameters of c and \u03b3 were found using grid search following the guidelines provided by the authors. While the SLDA can determine the optimal parameters automatically, the ERE has two tunable parameters: \u00b5 that is used to determine the noise region and the number of features d. For fair comparison we set \u00b5 = 1, as suggested by the authors. To achieve the best performance d was selected as the number of nonzero eigenvalues. In the CIFE based classifier, we split the training samples belonging to each class into max(2, bT/50c) subgroups and then used COBEC to extract their common features, where T was the number of training samples in this class. For simplicity, we empirically set C = minn Jn\u00d780%, although the optimal C and number of subgroups could be estimated by using cross validations. Correlation was adopted as the matching score to classify a new test sample (see (37) and Fig.6 for details). In each run, we randomly selected a certain percentage of samples as the training data and the remainder as the test data. The mean values and standard derivations of classification accuracy over 20 random runs are plotted in Fig.13, showing that the CIFE based classifier yielded the best classification accuracy among the classifiers considered."}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "A new scheme for common and individual feature extraction (CIFE) for naturally linked multi-block data has been proposed, together with two new efficient algorithms for the extraction of common orthogonal bases (COBE) according to whether the number of common components is known or not. We have also introduced the concept of linked Blind Source Separation (BSS) of multi-block data in order to perform effective task-dependent feature extraction in common and individual subspaces rather than in a global high dimensional space. The proposed CIFE scheme has been validated on classification and clustering tasks, by exploiting the separated common and individual features. Comprehensive simulations have illustrated the ability of the proposed methods to extract common features existing in multi-block data efficiently and accurately.\nIn this study we have concentrated on developing a unifying and versatile scheme of common and individual feature extraction. Questions that remain to be investigated in the future include:\n1) The number of common features, i.e., C, which is controlled by the parameter , often plays a quite important role in practical applications: it controls the degrees of similarity (correlation, in this paper) of common components extracted from different data sets. However, determining its optimal theoretical value is challenging and needs further investigation; 2) For some practical applications, we need to split the data into subgroups manually in order to discover their common features. Optimal grouping of data is a important factor to achieve better performance; 3) In this paper we considered common features in one dimension. Further extensions of the proposed method to higher-order data will yield general and flexible CIFE tools for tensor data.\n4) We provided two example applications of CIFE and corresponding experimental evidences to justify their validity and high performance. However, how to unlock the full potential of CIFE in machine learning deserves further investigation."}], "references": [{"title": "Joint and individual variation explained (JIVE) for integrated analysis of multiple data types", "author": ["E. Lock", "K. Hoadley", "J.S. Marron", "A. Nobel"], "venue": "The Annals of Applied Statistics, vol. 7, no. 1, pp. 523\u2013542, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian joint analysis of heterogeneous genomics data", "author": ["P. Ray", "L. Zheng", "J. Lucas", "L. Carin"], "venue": "Bioinformatics, vol. 30, no. 10, pp. 1370\u2013 1376, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Performing disco-sca to search for distinctive and common information in linked data", "author": ["M. Schouteden", "K. Van Deun", "T. Wilderjans", "I. Van Mechelen"], "venue": "Behavior Research Methods, vol. 46, no. 2, pp. 576\u2013587, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Common and cluster-specific simultaneous component analysis", "author": ["K.D. Roover", "M.E. Timmerman", "B. Mesquita", "E. Ceulemans"], "venue": "PLoS ONE, vol. 9, no. 4, p. e93796, April 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Proc. Advances in Neural Information Processing Systems 19, B. Sch\u00f6lkopf, J. Platt, and T. Hoffman, Eds. MIT Press, 2007, pp. 41\u201348.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "J. Mach. Learn. Res., vol. 6, pp. 1817\u20131853, Dec. 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1817}, {"title": "Relations between two sets of variants", "author": ["H. Hotelling"], "venue": "Biometrika, vol. 28, no. 3-4, pp. 321\u2013377, 1936.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1936}, {"title": "A least-squares framework for component analysis", "author": ["F. De la Torre"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 6, pp. 1041 \u20131055, June 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis and online realization of the cca approach for blind source separation", "author": ["W. Liu", "D. Mandic", "A. Cichocki"], "venue": "IEEE Transactions on Neural Networks, vol. 18, no. 5, pp. 1505\u20131510, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Partial least squares", "author": ["W. Herman"], "venue": "Proc. Encyclopedia of statistical sciences, S. Kotz and N. L. Johnson, Eds. New York: Wiley, 1995, vol. 6.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Global, local and unique decompositions in onpls for multiblock data analysis", "author": ["T. L\u00f6fstedt", "D. Hoffman", "J. Trygg"], "venue": "Analytica Chimica Acta, vol. 791, no. 0, pp. 13 \u2013 24, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "O2-PLS, a two-block (X-Y) latent variable regression (LVR) method with an integral OSC filter", "author": ["J. Trygg", "S. Wold"], "venue": "Journal of Chemometrics, vol. 17, no. 1, pp. 53\u201364, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Canonical analysis of several sets of variables", "author": ["J.R. Kettenring"], "venue": "Biometrika, vol. 58, no. 3, pp. 433\u2013451, 1971.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1971}, {"title": "Joint blind source separation by multiset canonical correlation analysis", "author": ["Y.-O. Li", "T. Adali", "W. Wang", "V. Calhoun"], "venue": "IEEE Transactions on Signal Processing, vol. 57, no. 10, pp. 3918 \u20133929, oct. 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Canonical correlation analysis for multilabel classification: A least-squares formulation, extensions, and analysis", "author": ["L. Sun", "S. Ji", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 1, pp. 194 \u2013200, jan. 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Extensions of sparse canonical correlation analysis with applications to genomic data", "author": ["D.M. Witten", "R.J. Tibshirani"], "venue": "Statistical Applications in Genetics and Molecular Biology, vol. 8, no. 1, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Frequency recognition in SSVEP-based BCI using multiset canonical correlation analysis", "author": ["Y. Zhang", "G. Zhou", "J. Jin", "X. Wang", "A. Cichocki"], "venue": "International Journal of Neural Systems, vol. 4, no. 24, pp. 1 450 013(1\u201314), 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Population value decomposition, a framework for the analysis of image populations", "author": ["C. Crainiceanu", "B.S. Caffo", "S. Luo", "V.M. Zipunnikov", "N.M. Punjabi"], "venue": "Journal of the American Statistical Association, vol. 106, no. 495, pp. 775\u2013790, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separationpplications to Exploratory Multi-Way Data Analysis and Blind Source Separation", "author": ["A. Cichocki", "R. Zdunek", "A.H. Phan", "S.-I. Amari"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Diversity in independent component and vector analyses: Identifiability, algorithms, and applications in medical imaging", "author": ["T. Adali", "M. Anderson", "G.-S. Fu"], "venue": "IEEE Signal Processing Magazine, vol. 31, no. 3, pp. 18\u201333, May 2014.  IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS  14", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Group information guided ICA for fMRI data analysis", "author": ["Y. Du", "Y. Fan"], "venue": "NeuroImage, vol. 69, no. 0, pp. 157 \u2013 197, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Capturing group variability using IVA: A simulation study and graph-theoretical analysis", "author": ["S. Ma", "R. Phlypo", "V. Calhoun", "T. Adali"], "venue": "Proc. IEEE 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Vancouver, BC, May 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive Blind Signal and Image Processing: Learning Algorithms and Applications", "author": ["A. Cichocki", "S. Amari"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Common components analysis via linked blind source separation", "author": ["G. Zhou", "A. Cichocki", "D. Mandic"], "venue": "Proc. IEEE 40th International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, April 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Detecting the number of clusters in N -way probabilistic clustering", "author": ["Z. He", "A. Cichocki.", "S. Xie", "K. Choi"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 11, pp. 2006\u2013 2021, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Matrix Computations, 3rd ed", "author": ["G. Golub", "C. Van Loan"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1996}, {"title": "Robust principal component analysis?", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Time-frequency approach to underdetermined blind source separation", "author": ["S. Xie", "L. Yang", "J.-M. Yang", "Y. Xiang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 2, pp. 306 \u2013316, Feb. 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "A unified framework for group independent component analysis for multi-subject fMRI data", "author": ["Y. Guo", "G. Pagnoni"], "venue": "NeuroImage, vol. 42, no. 3, pp. 1078 \u2013 1093, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast nonnegative matrix/tensor factorization based on low-rank approximation", "author": ["G. Zhou", "A. Cichocki", "S. Xie"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 6, pp. 2928\u20132940, june 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Minimum-volumeconstrained nonnegative matrix factorization: Enhanced ability of learning parts", "author": ["G. Zhou", "S. Xie", "Z. Yang", "J.-M. Yang", "Z. He"], "venue": "IEEE Transactions on Neural Networks, vol. 22, no. 10, pp. 1626 \u20131637, Oct. 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Projection-pursuit-based method for blind separation of nonnegative sources", "author": ["Z. Yang", "Y. Xiang", "Y. Rong", "S. Xie"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 24, no. 1, pp. 47\u201357, Jan 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonnegative blind source separation by sparse component analysis based on determinant measure", "author": ["Z. Yang", "Y. Xiang", "S. Xie", "S. Ding", "Y. Rong"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 10, pp. 1601\u20131610, Oct 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonnegative matrix factorization applied to nonlinear speech and image cryptosystems", "author": ["S. Xie", "Z. Yang", "Y. Fu"], "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 55, no. 8, pp. 2356\u20132367, Sept 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "A general framework for dimensionality-reducing data visualization mapping", "author": ["K. Bunte", "M. Biehl", "B. Hammer"], "venue": "Neural Computation, vol. 24, no. 3, pp. 771\u2013804, Dec. 2011.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "ICALAB toolbox", "author": ["A. Cichocki", "S. Amari", "K. Siwek", "T. Tanaka"], "venue": "2007. [Online]. Available: http://www.bsp.brain.riken.jp/ICALAB.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "A blind source separation technique using second-order statistics", "author": ["A. Belouchrani", "K. AbedMeraim", "J.F. Cardoso", "E. Moulines"], "venue": "IEEE Transactions on Signal Processing, vol. 45, no. 2, pp. 434\u2013444, Feb 1997.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1997}, {"title": "Nonnegative leastcorrelated component analysis for separation of dependent sources by volume maximization", "author": ["F.Y. Wang", "C.Y. Chi", "T.H. Chan", "Y. Wang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 5, pp. 875\u2013888, May 2010.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "From few to many: illumination cone models for face recognition under variable lighting and pose", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 6, pp. 643 \u2013660, Jun 2001.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2001}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 8, pp. 1548 \u20131560, aug. 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing data using t-SNE", "author": ["L. Van Der Maaten", "C. Detection"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 2579\u20132605, 2008.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved minmax cut graph clustering with nonnegative relaxation", "author": ["F. Nie", "C. Ding", "D. Luo", "H. Huang"], "venue": "Proc. Machine Learning and Knowledge Discovery in Databases, ser. Lecture Notes in Computer Science. Springer Berlin / Heidelberg, 2010, vol. 6322, pp. 451\u2013466.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Analyzing appearance and contour based methods for object categorization", "author": ["B. Leibe", "B. Schiele"], "venue": "Proc. International Conference on Computer Vision and Pattern Recognition, vol. 2, Los Alamitos, CA, USA, 2003, p. 409. [Online]. Available: http://www.d2.mpi-inf.mpg.de/Datasets/ETH80", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2003}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1\u201327:27, 2011, software available at http://www.csie.ntu. edu.tw/\u223ccjlin/libsvm.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "Eigenfeature regularization and extraction in face recognition", "author": ["X. Jiang", "B. Mandal", "A. Kot"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 3, pp. 383\u2013394, March 2008.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "Singletrial analysis and classification of ERP components \u2014 a tutorial", "author": ["B. Blankertz", "S. Lemm", "M. Treder", "S. Haufe", "K.-R. M\u00fcller"], "venue": "NeuroImage, vol. 56, no. 2, pp. 814 \u2013 825, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The identification and separation of such common and individual information in order to employ the features highly relevant to the data analysis task at hand promises to significantly improve data analysis [1], [2], [3], [4].", "startOffset": 206, "endOffset": 209}, {"referenceID": 1, "context": "The identification and separation of such common and individual information in order to employ the features highly relevant to the data analysis task at hand promises to significantly improve data analysis [1], [2], [3], [4].", "startOffset": 211, "endOffset": 214}, {"referenceID": 2, "context": "The identification and separation of such common and individual information in order to employ the features highly relevant to the data analysis task at hand promises to significantly improve data analysis [1], [2], [3], [4].", "startOffset": 216, "endOffset": 219}, {"referenceID": 3, "context": "The identification and separation of such common and individual information in order to employ the features highly relevant to the data analysis task at hand promises to significantly improve data analysis [1], [2], [3], [4].", "startOffset": 221, "endOffset": 224}, {"referenceID": 4, "context": "For example, shared features among tasks have been exploited to improve the performance of supervised and semi-supervised learning [5], [6].", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "For example, shared features among tasks have been exploited to improve the performance of supervised and semi-supervised learning [5], [6].", "startOffset": 136, "endOffset": 139}, {"referenceID": 6, "context": "Common data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12].", "startOffset": 164, "endOffset": 167}, {"referenceID": 7, "context": "Common data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12].", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "Common data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12].", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "Common data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12].", "startOffset": 266, "endOffset": 270}, {"referenceID": 10, "context": "Common data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12].", "startOffset": 272, "endOffset": 276}, {"referenceID": 11, "context": "Common data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12].", "startOffset": 278, "endOffset": 282}, {"referenceID": 12, "context": "CCA has also been generalized to multiple data sets in the context of blind source separation (BSS) and feature extraction [13], [14], [15], [16], [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "CCA has also been generalized to multiple data sets in the context of blind source separation (BSS) and feature extraction [13], [14], [15], [16], [17].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "CCA has also been generalized to multiple data sets in the context of blind source separation (BSS) and feature extraction [13], [14], [15], [16], [17].", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "CCA has also been generalized to multiple data sets in the context of blind source separation (BSS) and feature extraction [13], [14], [15], [16], [17].", "startOffset": 141, "endOffset": 145}, {"referenceID": 16, "context": "CCA has also been generalized to multiple data sets in the context of blind source separation (BSS) and feature extraction [13], [14], [15], [16], [17].", "startOffset": 147, "endOffset": 151}, {"referenceID": 17, "context": "For images, the population value decomposition (PVD) jointly analyzes same-size images [18], and can be considered as a special case of tensor (Tucker) decompositions, an active research topic in high-order data analysis and exploration [19].", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "For images, the population value decomposition (PVD) jointly analyzes same-size images [18], and can be considered as a special case of tensor (Tucker) decompositions, an active research topic in high-order data analysis and exploration [19].", "startOffset": 237, "endOffset": 241}, {"referenceID": 19, "context": "Very recently, group independent component analysis (ICA) and independent vector analysis (IVA) were proposed to capture group variables from multiblock data [20], [21], [22].", "startOffset": 158, "endOffset": 162}, {"referenceID": 20, "context": "Very recently, group independent component analysis (ICA) and independent vector analysis (IVA) were proposed to capture group variables from multiblock data [20], [21], [22].", "startOffset": 164, "endOffset": 168}, {"referenceID": 21, "context": "Very recently, group independent component analysis (ICA) and independent vector analysis (IVA) were proposed to capture group variables from multiblock data [20], [21], [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "The joint and individual variation explained (JIVE) [1] is a step in the right direction which simultaneously extracts both joint and individual variations across the members of a heterogeneous data ensemble.", "startOffset": 52, "endOffset": 55}, {"referenceID": 22, "context": "algorithms guarantee the identification of true common components and can be applied to separate common and individual subspaces of multi-block data; 2) A unifying framework for multi-block data analysis, which deals with heterogeneous data structures by respectively applying suitable well-established matrix factorization methods (such as blind source separation (BSS) [23] and nonnegative matrix factorization (NMF) [19]) to the common and individual subspaces separately rather than to the global space of a data ensemble.", "startOffset": 371, "endOffset": 375}, {"referenceID": 18, "context": "algorithms guarantee the identification of true common components and can be applied to separate common and individual subspaces of multi-block data; 2) A unifying framework for multi-block data analysis, which deals with heterogeneous data structures by respectively applying suitable well-established matrix factorization methods (such as blind source separation (BSS) [23] and nonnegative matrix factorization (NMF) [19]) to the common and individual subspaces separately rather than to the global space of a data ensemble.", "startOffset": 419, "endOffset": 423}, {"referenceID": 23, "context": "A conference summary of part of our results has been accepted for publication at ICASSP 2015 [24].", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "Relative to [24], this journal version includes detailed derivations, new algorithms, potential applications in machine learning, and further experiments.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "A number of matrix factorization techniques exist to solve (1), including PCA, BSS [23], however, these methods consider each matrix Yn separately.", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "The JIVE method also solves the model (7), although this is not stated explicitly [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "The JIVE approach performs these two functions simultaneously by applying the alternating least squares (ALS) to (7) and quantifies the amount of joint variation between data types [1].", "startOffset": 181, "endOffset": 184}, {"referenceID": 24, "context": "Then the Second ORder sTatistic of the Eigenvalues (SORTE) method [25], which was proposed to detect the gap between the eigenvalues corresponding to the signal space and those belonging to the noise space, can be applied here to estimate the number of common components.", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": "Motivated by the work3 in [26] (page 601), the optimal solution of (24) becomes \u0100 = EV , (25)", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": "Gaussian noise; for sparse distributions we may use robust PCA (RPCA) [27], while the number of latent components Rn can be estimated using, e.", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "SORTE [25], before applying PCA.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "If our aim is to project the common components onto a feature space with some desired properties (uniqueness), this can be achieved by BSS [23], which finds latent variables S from their linear mixtures Y = SM such that", "startOffset": 139, "endOffset": 143}, {"referenceID": 27, "context": "To this end, for example, we can apply the novel tensor based approach proposed in [28].", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "Note that the JBSS method in [14] also performs BSS involving multi-block data; it extracts a group of signals with the highest correlations each time, and requires that the extracted groups have distinct correlations.", "startOffset": 29, "endOffset": 33}, {"referenceID": 28, "context": "Recently in neural science, group ICA and independent vector analysis (IVA) have been widely applied to capture common (group) variables or components from a group (set) of data matrices [29], [21], [22].", "startOffset": 187, "endOffset": 191}, {"referenceID": 20, "context": "Recently in neural science, group ICA and independent vector analysis (IVA) have been widely applied to capture common (group) variables or components from a group (set) of data matrices [29], [21], [22].", "startOffset": 193, "endOffset": 197}, {"referenceID": 21, "context": "Recently in neural science, group ICA and independent vector analysis (IVA) have been widely applied to capture common (group) variables or components from a group (set) of data matrices [29], [21], [22].", "startOffset": 199, "endOffset": 203}, {"referenceID": 29, "context": ", \u0232n = \u0100\u0100 T Yn, and subsequently use the following low-rank approximation based (semi-) nonnegative matrix factorization (NMF) model [30]:", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "where ~ and are element-wise product and division of matrices, see [30] for detailed convergence analysis.", "startOffset": 67, "endOffset": 71}, {"referenceID": 30, "context": "Similar to BSS, we may impose additional constraints to extract unique nonnegative components, see [31], [32], [33], [34].", "startOffset": 99, "endOffset": 103}, {"referenceID": 31, "context": "Similar to BSS, we may impose additional constraints to extract unique nonnegative components, see [31], [32], [33], [34].", "startOffset": 105, "endOffset": 109}, {"referenceID": 32, "context": "Similar to BSS, we may impose additional constraints to extract unique nonnegative components, see [31], [32], [33], [34].", "startOffset": 111, "endOffset": 115}, {"referenceID": 33, "context": "Similar to BSS, we may impose additional constraints to extract unique nonnegative components, see [31], [32], [33], [34].", "startOffset": 117, "endOffset": 121}, {"referenceID": 34, "context": "For example, we may wish to visualize data in a lowdimensional space [35], or to extract discriminative information, or to establish neighbor relationship.", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "mat) [36], and the other six components were drawn from independent standard normal distributions.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "We used the model Yn = SnMn + En, where En contains white Gaussian noise (SNR=20dB), and first employed the COBE, JIVE [1], JBSS [14], and PCA methods5 to extract the common bases \u0100, followed by the SOBI method [37] to extract the latent common speech signals S from \u0100, see Section III-A.", "startOffset": 119, "endOffset": 122}, {"referenceID": 13, "context": "We used the model Yn = SnMn + En, where En contains white Gaussian noise (SNR=20dB), and first employed the COBE, JIVE [1], JBSS [14], and PCA methods5 to extract the common bases \u0100, followed by the SOBI method [37] to extract the latent common speech signals S from \u0100, see Section III-A.", "startOffset": 129, "endOffset": 133}, {"referenceID": 36, "context": "We used the model Yn = SnMn + En, where En contains white Gaussian noise (SNR=20dB), and first employed the COBE, JIVE [1], JBSS [14], and PCA methods5 to extract the common bases \u0100, followed by the SOBI method [37] to extract the latent common speech signals S from \u0100, see Section III-A.", "startOffset": 211, "endOffset": 215}, {"referenceID": 0, "context": "Since the estimation of the number of components in [1] is quite time consuming and the performance is sensitive to selection of its parameters (for this instance, JIVE took more than two hours to estimate the rank); this limits the practical applications of JIVE for large-scale problems.", "startOffset": 52, "endOffset": 55}, {"referenceID": 37, "context": "Dual-energy chest X-ray imaging is a diagnostic tool for early signs of lung cancer [38], however, the ribs, clavicles overlapped with soft tissues, and environmental noise make it challenging to detect affected lung nodules.", "startOffset": 84, "endOffset": 88}, {"referenceID": 37, "context": "10(d) displays four samples of nonnegative components extracted by nLCA-IVM [38].", "startOffset": 76, "endOffset": 80}, {"referenceID": 38, "context": "The database contains 16,128 images of 28 human subjects under 9 poses and 64 illumination conditions [39].", "startOffset": 102, "endOffset": 106}, {"referenceID": 39, "context": "We used a pre-processed version from [40] which consists of 2,856 full frontal face gray scale images taken at the pose c27.", "startOffset": 37, "endOffset": 41}, {"referenceID": 40, "context": "The number of common components was specified as 2 in all experiments, and the two t-SNE components of their individual parts Y\u0306n were used to cluster the data by using K-means (see [41] for the t-SNE method).", "startOffset": 182, "endOffset": 186}, {"referenceID": 39, "context": "Two widely used performance indices: Accuracy (%) and Normalized Mutual Information (NMI) were adopted to evaluate the clustering results, see [40] for detailed definitions of these two metrics.", "startOffset": 143, "endOffset": 147}, {"referenceID": 39, "context": "The proposed method was compared with PCA with K principal components, GNMF [40] (using their recommended settings), and the improved MinMax Cut (MMCut) method [42].", "startOffset": 76, "endOffset": 80}, {"referenceID": 41, "context": "The proposed method was compared with PCA with K principal components, GNMF [40] (using their recommended settings), and the improved MinMax Cut (MMCut) method [42].", "startOffset": 160, "endOffset": 164}, {"referenceID": 42, "context": "hemisphere [43].", "startOffset": 11, "endOffset": 15}, {"referenceID": 43, "context": "6) with the KNN classifier included in MATLAB 2010b, the SVM classifier [44], the Eigenfeature Regularization and Extraction method [45] (ERE), and the shrinkage LDA [46] (SLDA).", "startOffset": 72, "endOffset": 76}, {"referenceID": 44, "context": "6) with the KNN classifier included in MATLAB 2010b, the SVM classifier [44], the Eigenfeature Regularization and Extraction method [45] (ERE), and the shrinkage LDA [46] (SLDA).", "startOffset": 132, "endOffset": 136}, {"referenceID": 45, "context": "6) with the KNN classifier included in MATLAB 2010b, the SVM classifier [44], the Eigenfeature Regularization and Extraction method [45] (ERE), and the shrinkage LDA [46] (SLDA).", "startOffset": 166, "endOffset": 170}], "year": 2017, "abstractText": "Real world data are often acquired as a collection of matrices rather than as a single matrix. Such multi-block data are naturally linked and typically share some common features and at the same time exhibit their own individual features, reflecting the background in which they are measured and collected. To exploit the linked nature of data, we propose a new framework for common and individual feature extraction (CIFE) which identifies and separates the common and individual features from multi-block data. Two efficient algorithms termed common orthogonal basis extraction (COBE) are proposed to extract the common basis which is shared by all data, independent on whether the number of common components is given or not. Feature extraction is then performed on the common and the individual subspaces separately, by incorporating dimensionality reduction and blind source separation techniques. Extensive experimental results on both synthetic and real-world data show significant advantages of the proposed CIFE method in comparison to the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}