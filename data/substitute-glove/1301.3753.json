{"id": "1301.3753", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Switched linear encoding with rectified linear autoencoders", "abstract": "Several brought results in components studies have established consent various between autoencoders - - - occurs neural website models that attempt however microbes their utilized - - - turn people object-oriented models like sparse localization and K - same. This boxes explored then measuring an autoencoder model all is constructed own fixable linear imputations wednesday for hidden consisting. Our molecular builds end past concluded to extent consolidating the world its sparse configuration coding suv. We must also clever precedent with the understand, these splicing coupe other knowledge this intuition them sized, artificial datasets followed same multiples.", "histories": [["v1", "Wed, 16 Jan 2013 17:04:10 GMT  (8405kb,D)", "https://arxiv.org/abs/1301.3753v1", null], ["v2", "Sat, 19 Jan 2013 19:38:36 GMT  (8405kb,D)", "http://arxiv.org/abs/1301.3753v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["leif johnson", "craig corcoran"], "accepted": false, "id": "1301.3753"}, "pdf": {"name": "1301.3753.pdf", "metadata": {"source": "CRF", "title": "Switched linear coding with rectified linear autoencoders", "authors": ["Leif Johnson", "Craig Corcoran"], "emails": ["leif@cs.utexas.edu", "ccor@cs.utexas.edu"], "sections": [{"heading": null, "text": "Several recent results in machine learning have established formal connections between autoencoders\u2014artificial neural network models that attempt to reproduce their inputs\u2014and other coding models like sparse coding and K-means. This paper explores in depth an autoencoder model that is constructed using rectified linear activations on its hidden units. Our analysis builds on recent results to further unify the world of sparse linear coding models. We provide an intuitive interpretation of the behavior of these coding models and demonstrate this intuition using small, artificial datasets with known distributions."}, {"heading": "1 Introduction", "text": "Large quantities of natural data are now commonplace in the digital world\u2014images, videos, and sounds are all relatively cheap and easy to obtain. In comparison, labels for these datasets (e.g., answers to questions like, \u201cIs this an image of a cow or a horse?\u201d) remain relatively expensive and difficult to assemble. In addition, even when labeled data are available for a given task, there are often only a few bits of information in the labels, while the unlabeled data can easily contain millions of bits. Finally, most collections of data from the natural world also seem to be distributed non-uniformly in the space of all possible inputs, suggesting that there is some underlying structure inherent in a particular dataset, independent of labels or task. Furthermore, many unsupervised learning approaches assume that natural data even lie on a relatively continuous low-dimensional manifold of the available space, which provides further structure that can be captured in a model. For these reasons, much recent work in machine learning has focused on unsupervised modeling of large, easy to obtain datasets.\nAn unsupervised model attempts to create a compact representation of the structure of a dataset. Some models are designed to learn a set of \u201cbasis functions\u201d that can be combined linearly to create the observed data. Once such a set has been learned, this representation is often useful in other tasks like classification or recognition (but see [4] for interesting analysis). In addition to learning useful representations, many models that have resulted from this line of work have also been shown to have interesting theoretical ties with neuroscience and information theory (e.g., [12]).\nIn this paper, we first synthesize recent results from unsupervised machine learning, with a particular focus on neural networks and sparse coding. We then explore in detail the coding behavior of one important model in this class: autoencoders constructed with rectified linear hidden units. After describing the intuitive behavior of these models, we examine their behavior on artificial and natural datasets.\nar X\niv :1\n30 1.\n37 53\nv2 [\ncs .L\nG ]\n1 9\nJa n"}, {"heading": "2 Background", "text": "One way to take advantage of a readily available unlabeled datasetO is to learn features that encode the data well. Such features should then presumably be useful for describing the data effectively, regardless of the particular task. Formally, we wish to find a \u201cdictionary\u201d or \u201ccodebook\u201d matrix D \u2208 Rn\u00d7k whose columns can be combined or \u201cdecoded\u201d linearly to represent elements from the dataset. There are many ways to do this: for example, one could use random vectors or samples fromO, or define some cost function over the space of dictionaries and optimize to find a good one. Once D is known, then for any input x, we would also like to compute the coefficients h \u2208 Rk that give the best linear approximation x\u0302 = Dh to x. This problem is often expressed as an optimization of a squared error loss\nh = argmin u\n\u2016Du\u2212 x\u201622 +R(u)\nwhere R is some regularizer. If R = 0, for example, then this is linear regression; when R = \u2016 \u00b7 \u20161, the encoding problem is known as sparse coding or lasso regression [14]. Sparse coding yields stateof-the art results on many tasks for a wide variety of dictionary learning methods [2], but it requires a full optimization procedure to compute h, which might require more or less computation depending on the input. To address this complexity, Gregor and LeCun [6] tried approximating this coding procedure by training a neural network (which has a bounded complexity) on a dataset labeled with its sparse codes. For the remainder of the paper, we consider a similar problem\u2014learning an efficient code for a set of data\u2014but focus on simpler autoencoder neural network models to avoid solving the complete sparse coding optimization problem."}, {"heading": "2.1 Autoencoders", "text": "Autoencoders [8, 15, 10] are a versatile family of neural network models that can be used to learn a dictionary from a set of unlabeled data, and to compute an encoding h for an input x. This learning takes place simultaneously, since changing the dictionary changes the optimal encodings. An autoencoder attempts to reconstruct its input after passing activation forward through one or more nonlinear hidden layers. The general loss function for a one-layer autoencoder can be written as\n`(O) = 1 2 |O| \u2211 x\u2208O \u2016\u03c2 (D\u03c3 (Wx+ b))\u2212 x\u201622 +R(O, D,W,b)\nwhere \u03c2(\u00b7) and \u03c3(\u00b7) are activation functions for the output and hidden neurons, respectively, b \u2208 Rk is a vector of hidden unit activation biases, and W \u2208 Rk\u00d7n is a matrix of weights for the hidden units. By setting \u03c2(z) = z, this family of models clearly belongs to the class of coding models, where h = \u03c3(Wx + b). This encoding scheme is easy to compute, but it also assumes that the optimal coefficients can be represented as a linear transform of the input, passed through some nonlinearity.\nCommonly, the encoding and decoding weights are \u201ctied\u201d so that W = DT , which forces the dictionary to take on values that are useful for both tasks. Seen another way, tied weights provide an implicit orthonormality constraint on the dictionary [9]. Tying the weights also reduces by half the number of parameters that the model needs to tune."}, {"heading": "2.2 Rectified linear autoencoders", "text": "Traditionally, neural networks are constructed with sigmoid activations like \u03c3(z) = (1 + ez)\u22121. The rectified linear activation function [z]+ = max(0, z) has been shown to improve training and performance in multilayer networks by addressing several shortcomings of sigmoid activation functions [11, 5]. In particular, the rectified linear activation function produces a true 0 (not just a small value) for negative input and then increases linearly. Unlike the sigmoid, which has a derivative that vanishes for large input, the derivative of the rectified linear activation is either 0 for negative inputs, or 1 for positive inputs This \u201cswitching\u201d behavior is inspired by the firing rate pattern seen in some biological neurons, which are inactive for sub-threshold inputs and then increase firing rate as input magnitude increases. The true 0 output of the rectified linear activation function effectively deactivates neurons that are not tuned for a specific input, isolating a smaller active model for that input [5]. This also has the effect of combining exponentially many linear coding schemes into one codebook [11].\nTo learn a dictionary from data, Glorot et al. [5] proposed the single-layer rectified linear autoencoder with tied weights by setting W = DT , \u03c2(z) = z and \u03c3(z) = [z]+ in the general autoencoder loss, giving\n`(O) = 1 2 |O| \u2211 x\u2208O \u2225\u2225\u2225D [DTx+ b] + \u2212 x \u2225\u2225\u22252 2 +R(O, D,b).\nHidden units in rectified linear autoencoders behave similarly to the \u201ctriangle K-means\u201d encoding proposed by Coates et al. [1], which produces a coefficient vector h for input x such that hi = [ \u00b5\u2212 \u2016x\u2212 ci\u2016 ]+, where \u00b5 = 1 k \u2211k j=1 \u2016x\u2212 cj\u2016 is the mean distance to all cluster centroids. Like a rectified linear model, triangle K-means produces true 0 activations for many (approximately half) of the clusters, while coding the rest linearly. An equivalent [4] variation is the \u201csoft thresholding\u201d scheme [2], where hi = [ dTi x\u2212 \u03bb ] +\nfor some parameter \u03bb. This coding scheme is equivalent to a rectified linear autoencoder with a shared bias \u03bb on all hidden units. Rectified linear autoencoders generalize all of the models in this \u201cswitched linear\u201d class and provide a unified form for their loss functions."}, {"heading": "2.3 Whitening", "text": "The switched linear family of models uses linear codes for some regions of input space and produces zeros in others. These models perform best when applied to whitened datasets. In fact, Le et al. [9] showed that sparse autoencoders, sparse coding, and ICA all compute the same family of loss functions, but only if the input data are approximately white. Empirically, Coates et al. [1, 2] observed that for several different single-layer coding models, switched-linear coding schemes perform best when combined with pre-whitened data.\nWhitening seems to be important in biological systems as well: Although the human eye does not receive white input, neurons in retina and LGN appear to whiten local patches of natural images [13, 3] before further processing in the visual cortex."}, {"heading": "3 Switched linear codes", "text": "Many researchers (e.g., [7, 11, 5]) have described how the rectified linear activation function separates its inputs into two natural groups of outputs: those with 0 values, and those with positive values. We belabor this observation here to make a further point about the behavior of rectified linear autoencoders below.\nConsider a data point x and each encoding feature (row) wj of DT geometrically as vectors in Rn, shown in Figure 1. In the context of a linear classifier, the weight vector w is often described as defining the normal to a hyperplane Hw that divides the dataset into two subsets. A linear encoder, on the other hand, is attempting to determine which hidden units describe its input, so in this context it is the vector x that defines the normal to a hyperplaneHx passing through the origin. Each feature wj in the autoencoder corresponds to a bias term bj ; this bias can be seen as translating Hx in the direction away from x through a distance of bj . After this translation, if wj lies on the opposite side of Hx;wj ,bj from x, then the hidden unit hj will be set to 0 by a rectified activation function. 1 In other words, rectification selectively deactivates features wj in the loss for x whenever feature wj points sufficiently far away from x.2 After hidden unit hj is deactivated, decoding entry wj no longer has any effect on the output. We can thus define\n\u03c8x = {j : wjx+ bj > 0}\nas the set of features that have nonzero activations in the hidden layer of the autoencoder for input x. Then\nh\u03c8 = [h\u03c81 , . . . , h\u03c8r ] T\nis the vector of hidden activations for just the features in \u03c8, D\u03c8 is the matrix of corresponding columns from D, and b\u03c8 are the corresponding bias terms. This gives a new loss function\n`\u03c8(O) = 1 2 |O| \u2211 x\u2208O \u2225\u2225D\u03c8 (DT\u03c8x+ b\u03c8)\u2212 x\u2225\u222522 +R(O, D,b) that is computed only over the active features \u03c8x for each input x.\nWith b = 0 and a sparse regularizer R = \u2016h\u20161, the loss `\u03c8 reduces to that proposed by Le et al. [9] for ICA with a reconstruction cost. For whitened input, then, the rectified linear autoencoder behaves not only like a mixture of exponentially many linear models [11], but rather like a mixture of exponentially many ICA models\u2014one ICA model is selected for each input x, but all features are selected from, and must coexist in, one large feature set. Importantly, this whitening requirement only holds for the region of data delimited by the set of active features, potentially simplifying the whitening transform.\nMirrored by the geometric interpretation, from the perspective of a data point, nonzero bias terms can be thought of as translating the data to a new origin along the axis of that feature. Seen from the perspective of a feature, the bias terms can help move the active region for that feature to an area of space where data are available to describe\u2014potentially ignoring other regions of space that contain other portions of the data. From either viewpoint, the bias parameters in the model can\n1Analytically, zj = wjx+ bj < 0, so hj = max(0, zj) = 0. 2Where \"sufficiently\" is determined by the bias for that feature.\nthus help compensate for non-centered data. Figure 2 shows typical behavior for a 1\u00d7 complete dictionary of rectified linear units when trained on an artificial 3D gaussian dataset. In this case, each feature plane uses its associated bias to translate away from the origin, together providing a sort of \u201cbounding box\u201d for the data. Though the axes of the box are not necessarily orthogonal, the features define a new basis that is often aligned with or near the principal axes of the data."}, {"heading": "3.1 Effective coding region", "text": "Visualizing feature hyperplanes is actually instructive for many types of activation functions. Figure 3, for instance, shows the feature planes for a 2\u00d7 overcomplete tied-weights autoencoder with sigmoid hidden units, trained on an artificial 3D gaussian dataset.3 Several of the features learned by this model partition the data along the principal axis of variance, while the remaining features capture some of the variance along minor axes of the dataset. In comparison, a 2\u00d7 overcomplete rectified linear autoencoder trained on a 3D gaussian dataset (Figure 4) creates pairs of negated feature vectors, in effect providing a full-wave rectified linear output along each axis. Together, these\n3The feature planes in this case are aligned with the logistic activation value 0.5.\npairs of planes split the data into orthants, which produces a coding scheme that is 50 % sparse because only half of the features are active for any given region of the input space.\nClearly these two activation functions have different ways of modeling the input data, but why do these particular patterns of organization appear? In our view, the unbounded linear output of the rectified linear autoencoder units addresses a subtle but important issue inherent in coding with sigmoid activation functions: scale. Consider, for example, a two-dimensional dataset consisting of points along a one-dimensional manifold\nL = { [x, ]T : 0 < x < S } where is gaussian noise. Regardless of the activation function, this network requires just one hidden unit to represent points in L: we can set the bias to 0 and weights to [1, 0]T to align the feature of the hidden unit with the linear function that describes the data. Then points in the dataset are simply coded by their distance from the origin, as given by the output of the activation function. For a sigmoid activation function, however, as S increases, the activation function saturates, limiting the power of the hidden units to discriminate between small changes in x near S. This saturation limits the scale at which sigmoid units can describe data effectively, but it also prevents numerical instability by limiting the range of the output.\nRegardless of the activation function, then, each feature vector can be seen as normal to a hyperplane in input space. The activation function encodes data along the axis of the feature as a function of the distance to the hyperplane for that feature. Sigmoid activation functions saturate for points far from the hyperplane, whereas linear activations solve this \u201cvanishing signal\u201d problem by coding all x values with a linear output response. Rectified linear activations make a further distinction by coding only one half of the input space. However, the outputs of a linear activation are unbounded, so networks with inputs at large scales are prone to numerical instability during training."}, {"heading": "4 Behavior on complex data", "text": "So far, the visualizations tools that we have used are based on simple gaussian distributions of data. Many interesting datasets, however, are emphatically not gaussian\u2014indeed, one of the primary reasons that ICA is so effective is that it explicitly searches for a non-gaussian model of the data! In this section we present several visualizations for these models using more complex datasets: first we examine mixtures of gaussians, and then we use the MNIST digits dataset as a foray into a more natural set of data.\nWe first tested the behavior of rectified linear autoencoders on a small, 2D mixture of gaussians dataset. (See Figure 5.) After training, even massively overcomplete dictionaries tended to display the feature pairing behavior described above, particularly when combined with a sparse regularizer, suggesting that these networks are capable of identifying the number of features required to code the data effectively. Unfortunately, while it provides simple visualization, working in 2D does not necessarily generalize to higher dimensions, as our intuition in low-dimensional spaces can quickly lead us astray in larger spaces."}, {"heading": "4.1 MNIST digits", "text": "The MNIST dataset consists of 60000 28\u00d7 28 grayscale images of the handwritten digits 0 through 9. Figure 7 shows the PCA \u201ceigendigits\u201d computed using this dataset; these digits point in the directions of highest variance for this dataset overall, and are often interpreted as a coding of the digit using a Fourier basis. Figure 8 shows the features with the highest bias values computed by a singlelayer rectified linear autoencoder on the same dataset. For visualization purposes, each feature w on the left of each column is paired with its maximally negative feature v = argminu w\nTu from the dictionary. Many of the primary features in the dictionary are negative images of each other, indicating that the \u201cpairing\u201d of feature planes observed for low-dimensional datasets also occurs with higher-dimensional data.\nFinally, Figure 9 shows features learned by a two-layer rectified linear autoencoder with untied weights. This model was trained with just 64 first-layer hidden units, and 1024 second-layer units. As observed in the low-dimensional gaussian data, the features from the first layer appear to model the principal axes of the data, providing a bounding box of 64 dimensions that encodes the data for the next layer. The features from the second layer resemble a more traditional sparse code, consisting in this case of small segments of pen strokes. It is important to note that the training dataset was not\nwhitened beforehand, and no regularization was used to train the network\u2014the autoencoder learned these features automatically, using only a squared error reconstruction cost."}, {"heading": "5 Conclusion", "text": "This paper has synthesized many recent developments in encoding and neural networks, with a focus on rectified linear activation functions and whitening. It also presented an intuitive interpretation for the behavior of these encodings through simple visualizations. Sample features learned from both artificial and natural datasets provided examples of learning behavior when these algorithms are applied to different types of data.\nThere are many more paths to explore in this area of machine learning research. Whitening has appeared in many places throughout this paper and seems to be a very important component of linear autoencoders and coding systems in general. However, this connection seems poorly understood, and would benefit from further exploration, particularly with respect to the idea of whitening local regions of the input space."}], "references": [{"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In AISTATS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Efficient coding of natural scenes in the lateral geniculate nucleus: experimental test of a computational theory", "author": ["Y. Dan", "J.J. Atick", "R.C. Reid"], "venue": "The Journal of Neuroscience,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Recklessly approximate sparse coding", "author": ["M. Denil", "N. de Freitas"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Learning fast approximations of sparse coding", "author": ["K Gregor", "Y LeCun"], "venue": "In Proc. International Conference on Machine learning (ICML\u201910),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Permitted and forbidden sets in symmetric threshold-linear networks", "author": ["R.H.R. Hahnloser", "H.S. Seung", "J.J. Slotine"], "venue": "Neural Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Autoencoders, minimum description length, and Helmholtz free energy", "author": ["G.E. Hinton", "R.S. Zemel"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "ICA with reconstruction cost for efficient overcomplete feature learning", "author": ["Q.V. Le", "A. Karpenko", "J. Ngiam", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Efficient online learning of a non-negative sparse autoencoder", "author": ["A. Lemme", "R.F. Reinhart", "J.J. Steil"], "venue": "In Proceedings of the ESANN,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In 27th International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Natural image statistics and neural representation", "author": ["E.P. Simoncelli", "B.A. Olshausen"], "venue": "Annual review of neuroscience,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "Once such a set has been learned, this representation is often useful in other tasks like classification or recognition (but see [4] for interesting analysis).", "startOffset": 129, "endOffset": 132}, {"referenceID": 11, "context": ", [12]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "If R = 0, for example, then this is linear regression; when R = \u2016 \u00b7 \u20161, the encoding problem is known as sparse coding or lasso regression [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 1, "context": "Sparse coding yields stateof-the art results on many tasks for a wide variety of dictionary learning methods [2], but it requires a full optimization procedure to compute h, which might require more or less computation depending on the input.", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "To address this complexity, Gregor and LeCun [6] tried approximating this coding procedure by training a neural network (which has a bounded complexity) on a dataset labeled with its sparse codes.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "Autoencoders [8, 15, 10] are a versatile family of neural network models that can be used to learn a dictionary from a set of unlabeled data, and to compute an encoding h for an input x.", "startOffset": 13, "endOffset": 24}, {"referenceID": 14, "context": "Autoencoders [8, 15, 10] are a versatile family of neural network models that can be used to learn a dictionary from a set of unlabeled data, and to compute an encoding h for an input x.", "startOffset": 13, "endOffset": 24}, {"referenceID": 9, "context": "Autoencoders [8, 15, 10] are a versatile family of neural network models that can be used to learn a dictionary from a set of unlabeled data, and to compute an encoding h for an input x.", "startOffset": 13, "endOffset": 24}, {"referenceID": 8, "context": "Seen another way, tied weights provide an implicit orthonormality constraint on the dictionary [9].", "startOffset": 95, "endOffset": 98}, {"referenceID": 10, "context": "The rectified linear activation function [z]+ = max(0, z) has been shown to improve training and performance in multilayer networks by addressing several shortcomings of sigmoid activation functions [11, 5].", "startOffset": 199, "endOffset": 206}, {"referenceID": 4, "context": "The rectified linear activation function [z]+ = max(0, z) has been shown to improve training and performance in multilayer networks by addressing several shortcomings of sigmoid activation functions [11, 5].", "startOffset": 199, "endOffset": 206}, {"referenceID": 4, "context": "The true 0 output of the rectified linear activation function effectively deactivates neurons that are not tuned for a specific input, isolating a smaller active model for that input [5].", "startOffset": 183, "endOffset": 186}, {"referenceID": 10, "context": "This also has the effect of combining exponentially many linear coding schemes into one codebook [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 4, "context": "[5] proposed the single-layer rectified linear autoencoder with tied weights by setting W = D , \u03c2(z) = z and \u03c3(z) = [z]+ in the general autoencoder loss, giving", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], which produces a coefficient vector h for input x such that hi = [ \u03bc\u2212 \u2016x\u2212 ci\u2016 ]+, where \u03bc = 1 k \u2211k j=1 \u2016x\u2212 cj\u2016 is the mean distance to all cluster centroids.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "An equivalent [4] variation is the \u201csoft thresholding\u201d scheme [2], where hi = [ di x\u2212 \u03bb ]", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "An equivalent [4] variation is the \u201csoft thresholding\u201d scheme [2], where hi = [ di x\u2212 \u03bb ]", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "[9] showed that sparse autoencoders, sparse coding, and ICA all compute the same family of loss functions, but only if the input data are approximately white.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1, 2] observed that for several different single-layer coding models, switched-linear coding schemes perform best when combined with pre-whitened data.", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[1, 2] observed that for several different single-layer coding models, switched-linear coding schemes perform best when combined with pre-whitened data.", "startOffset": 0, "endOffset": 6}, {"referenceID": 12, "context": "Whitening seems to be important in biological systems as well: Although the human eye does not receive white input, neurons in retina and LGN appear to whiten local patches of natural images [13, 3] before further processing in the visual cortex.", "startOffset": 191, "endOffset": 198}, {"referenceID": 2, "context": "Whitening seems to be important in biological systems as well: Although the human eye does not receive white input, neurons in retina and LGN appear to whiten local patches of natural images [13, 3] before further processing in the visual cortex.", "startOffset": 191, "endOffset": 198}, {"referenceID": 6, "context": ", [7, 11, 5]) have described how the rectified linear activation function separates its inputs into two natural groups of outputs: those with 0 values, and those with positive values.", "startOffset": 2, "endOffset": 12}, {"referenceID": 10, "context": ", [7, 11, 5]) have described how the rectified linear activation function separates its inputs into two natural groups of outputs: those with 0 values, and those with positive values.", "startOffset": 2, "endOffset": 12}, {"referenceID": 4, "context": ", [7, 11, 5]) have described how the rectified linear activation function separates its inputs into two natural groups of outputs: those with 0 values, and those with positive values.", "startOffset": 2, "endOffset": 12}, {"referenceID": 8, "context": "[9] for ICA with a reconstruction cost.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "For whitened input, then, the rectified linear autoencoder behaves not only like a mixture of exponentially many linear models [11], but rather like a mixture of exponentially many ICA models\u2014one ICA model is selected for each input x, but all features are selected from, and must coexist in, one large feature set.", "startOffset": 127, "endOffset": 131}, {"referenceID": 0, "context": "Regardless of the activation function, this network requires just one hidden unit to represent points in L: we can set the bias to 0 and weights to [1, 0] to align the feature of the hidden unit with the linear function that describes the data.", "startOffset": 148, "endOffset": 154}], "year": 2013, "abstractText": null, "creator": "LaTeX with hyperref package"}}}