{"id": "1301.3847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "A Differential Approach to Inference in Bayesian Networks", "abstract": "We present turned new approach for inference taken Bayesian companies, first and mainly recently on possible differentiation. According over would approach, three 26-page it Bayesian network behind each multivariate polynomial this then computes later reversal derivatives of instance modulo with respect with plus measurement. We show that now ones value are making include, than can compute throughout constant - gone relate to kind along american of algorithms queries, in many central to classical analogical, formula_35 estimation, model objective rest vulnerability analysis. We and in number for simplicity results internal to the artwork similar besides formula_32 and allowed to computation as their partial derivatives. We argue that this 35 boldness, comprehensiveness other linear factors include before invited framework takes provides mostly existing definitions for inference while Bayesian operators.", "histories": [["v1", "Wed, 16 Jan 2013 15:49:38 GMT  (346kb)", "http://arxiv.org/abs/1301.3847v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["adnan darwiche"], "accepted": false, "id": "1301.3847"}, "pdf": {"name": "1301.3847.pdf", "metadata": {"source": "CRF", "title": "A Differential Approach to Inference in Bayesian Networks", "authors": ["Adnan Darwiche"], "emails": ["darwiche@cs."], "sections": [{"heading": null, "text": "We present a new approach for inference in Bayesian networks, which is mainly based on partial differentiation. According to this approach, one compiles a Bayesian net work into a multivariate polynomial and then computes the partial derivatives of this polynomial with respect to each vari able. We show that once such derivatives are made available, one can compute in constant-time answers to a large class of probabilistic queries, which are central to classical inference, parameter estimation, model validation and sensitivity analysis. We present a number of complexity results relating to the compilation of such polyno mials and to the computation of their par tial derivatives. We argue that the com bined simplicity, comprehensiveness and computational complexity of the presented framework is unique among existing frame works for inference in Bayesian networks.\n1 Introduction\nConsider the probability table on the left of Figure 1 and suppose that our goal is to compute probabili ties of events with respect to this table, say Pr (a),\nPr ( a, b), etc.1 We can do this in the usual way. To compute Pr (e) we simply identify the rows in the table which are consistent with e and sum up their probabilities. Alternatively, we can compile the probability table as follows. We parameterize the table as shown on the right of Figure 1, where we introduce what we call evidence indicators into each row. We then add up all rows to generate the multivariate polynomial:\nwhich is depicted graphically in Figure 2.\nNow, given any evidence e, we can compute the probability of e by a simple evaluation of polynomial :F. We consider each indicator Ax. If x is consistent with evidence e, we set the value of variable Ax to 1. Otherwise, we set its value to 0. We then evaluate the polynomial :F, where its value is guaranteed to\n1We are using the standard notation: variables are denoted by upper-case letters (A) and their values by lower-case letters (a). Sets of variables are denoted by bold-face upper-case letters (A) and their instantiations are denoted by bold-face lower-case letters (a) . For a variable A with values true and false, we use a to denote A= true and a to denote A= false.\n124 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nbe the probability of evidence e. All we are doing here is using the indicators to select which rows of the probability table to add together. Figure 3 de picts some examples of evidence and the correspond ing values of indicators. Figure 4 shows an example evaluation to compute the probability of a.\nThe previous compilation technique can be applied to Bayesian networks, allowing one to generate fac tored polynomials that are not necessarily exponen tial in the number of network variables. We have promoted the compilation of Bayesian networks in earlier work, where we referred to polynomials such as :F as Query-DAGs [6]. Our original motivation for compiling Bayesian networks was to simplify in ference systems to the point where they could be implemented cost-effectively on a variety of software and hardware platforms. This paper is based on a number of new results, which make this notion of compilation much more interesting and useful than was originally conceived:\n1. We show how to compile a Bayesian network into a polynomial that includes two types of variables: evidence indicators Ax and network parameters Or. Given a variable elimination or der of with w and length n, we show how to compile the polynomial in O(nexp(w)) time us ing a simple variable elimination algorithm.\n2. We show how to answer a large number of queries relating to classical inference [10, 13, 7, 21, 20], parameter estimation [19, 16], model\n(a) first partial derivatives, 8:Fj8>.x and 8:F j8Bc, can all be computed simultane ously in time linear in the size of polyno mial :F, that is, in O(nexp(w)) time.\n(b) second partial derivatives, such as 82 :F j8>.x8Bc, can all be computed simul taneously in O(n2 exp(w)) time.\nFollowing are examples of the queries that can be answered in constant time once these partial deriva tives are computed:\n1. the posterior marginal of any network variable X, Pr(x I e);\n2. the posterior marginal of any network family {X} U U, Pr(x, u I e);\n3. the sensitivity of Pr(e) to change in any net work parameter Br;\n4. the probability of evidence e after having changed the value of some variable E to e, Pr(e- E, e);2\n5. the posterior marginal of some variable E after having retracted evidence onE, Pr(e I e-E).\n6. the posterior marginal of any pair of network variables X and Y, Pr(x, y I e);\n7. the posterior marginal of any pair of network families F1 and F2, Pr(f1, f2l e);\n8. the sensitivity of conditional probability Pr(y I e) to a change in network parameter Br;\n9. the amount of change to parameter Br needed to ensure that Pr(y I e) \ufffd Pr(ij I e).\nThis paper is structured as follows. We introduce the polynomial representation of a Bayesian network in Section 2 and explicate some of its key proper ties. We then introduce the partial derivatives of this polynomial in Section 3 and present two key theorems that explicate their probabilistic seman tics, followed by a number of corollaries showing how key probabilistic queries can be retrieved from such\n2We define this notation formally later.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nderivatives. Section 4 presents a simple variable elimination algorithm for computing the polynomial representation of a Bayesian network while present ing structure-based guarantees on its time and space complexity. Section 5 presents an algorithm for eval uating the polynomial and for computing its partial derivatives efficiently. We finally close in Section 6 with some concluding remarks. Proofs of all theo rems can be found in the full paper [5].\n2 Polynomial Representation of Bayesian Networks\nOur goal in this section is to show how to represent a Bayesian network as a polynomial :F and to explicate some of the key properties of this polynomial.\nWe will distinguish between canonical and factored polynomial representations of a Bayesian network. The canonical representation is unique, has expo nential size, but is all we need to discuss the seman tics of polynomial representations and their partial derivatives. Section 4 will then concern itself with computing factored polynomials, which size is deter mined by the network topology.\nWe start with some key notation first. Let F = {X} U U be the family of variable X and let f = xu be a corresponding instantiation. We will then use Br and/or Bxu to represent the conditional probability Pr(x I u) . Moreover, we will write x ,...., f to mean that instantiations x and f are consistent.\nDefinition 1 Let N be a Bayesian network with variables X = X1, . . . , Xn and families F1, .. . , F n. Then\nis called the canonical polynomial of network N, where variables Ax, are called evidence indicators and variables Br, are called network parameters.\nAs an example, the canonical polynomial of the net work in Figure 5 is:\nF = BaBabAaAb + BaB aliAaAr, + BaBabAaAb + BaBar,AaAr,.\nIn Definition 1, the instantiation x ranges over ab, ab, ab, ab; the instantiation fi ranges over a, a, ab, ab, ab, ab; and the instantiation Xi ranges over a, a, b, b.\nClearly, the size of a canonical polynomial is expo nential in the number of network variables. More over, the polynomial is independent of its quantifi cation: two networks with the same structure have\nA B c/>B A c/>A true true . 1 true .3 true false .9 false .7 false true .8\nfalse false .2\nDefinition 2 A quantification e of a Bayesian net work is a function that assigns a value e(f) to each instantiation f of family F.\nFor example, according to the quantification e in Figure 5, we have e(ab) = .1 and e(a) = .7.\nThe probability distribution represented by a Bayesian network is completely recoverable from its canonical polynomial:\nDefinition 3 The value of indicator Ax at instan tiation e, denoted e(x), is 1 if x is consistent with e, and is 0 otherwise. The value of polynomial :F under evidence e and quantification e is defined as:\nde/ F(e,e) = :F(Ax, = e(xi),Bri = e(fi)).\nConsider the canonical polynomial\nF = BaBabAaAb + BaB aliAaAr, + BaBabAaAb + BaBar,AaAr,\nof the network in Figure 5. If the evidence e is ab and the quantification e is as given in Figure 5, then F(e,e) stands for :F(Aa = 1,Aa = O,Ab = O,Ar, = 1,Ba = .3,Ba = .7,Bab = .1,Bab = .9,Bab = .8,()ab = .2), which equals to .27 in this case.\nTheorem 1 Let N be a Bayesian network specify ing distribution Pr and having canonical polynomial F. Then :F(e, e) = Pr(e I e) for every evidence e and quantification e.\nA canonical polynomial is then a function of many variables, where each variable corresponds to either an evidence indicator or a network parameter. For each instantiation e and quantification e, the poly nomial can be evaluated to compute the probability of e given e. We will often write F(e) instead of :F(e, e) when no ambiguity is anticipated.\n125\n126 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nNote that the canonical polynomial :F of a Bayesian network is a multilinear function (i.e., each of its variables has degree one). Moreover, a monomial in :F cannot contain two indicators of the same vari able; neither can it contain two parameters of the same family. These properties are quite important as they imply that :F is a linear function in the in dicators of any variable; and is a linear function in the parameters of any family.\nIt is already observed in [19] that Pr(e) is a linear function in each network parameter. More generally, it is shown in [1, 2] that Pr(e) can be expressed as a polynomial of network parameters in which each pa rameter has degree one. The polynomials discussed in [1, 2], however, are constructed for a given evi dence e: they only contain one type of variables cor responding to network parameters, with no variables corresponding to evidence indicators. In fact, these polynomials correspond to our canonical polynomi als when evidence indicators are fixed to a particular value. Hence, they do not represent network com pilations as they cannot be used to answer queries with respect to varying evidence. Note also that the size of such polynomials is always exponential in the number of network variables, but we show in Section 4 how to factor them using the technique of variable elimination. The main insight we bring into such polynomial representations, however, is our re sults on the probabilistic semantics (and computa tional complexity) of their partial derivatives, a topic which we discuss next.\n3 Probabilistic Semantics of Partial Derivatives\nOur goal in this section is to show the probabilistic semantics associated with the partial derivatives of a network polynomial. Given these semantics, we then enumerate the class of probabilistic queries that can be answered once these derivatives are computed.\nWe need the following key notation first. Let e be an instantiation and X be a set of variables. Then e-X denotes the subset of instantiation e pertaining to variables not appearing in X. For example, if e = abc, then e - A = be and e - AC = b. We start with the semantics of first derivatives:\nTheorem 2 (Semantics of 1st PDs) Let N be a Bayesian network representing probability distribu tion Pr and let :F be its canonical polynomial. For every variable X, family F and evidence e:\n8:F(e, 8)/8>-x Pr(x, e- Xl8)\n8:F(e, 8)/8Br Pr(f, el8)/8(f).\nunder and\nThat is, if we differentiate the polynomial :F with respect to indicator Ax and evaluate the result at evidence e, we obtain the probability of instantiation x, e - X. A similar interpretation is given to the derivative of :F with respect to parameter Br. Note that the second identity of Theorem 2 has indirectly been shown in [19], given that :F(e) = Pr(e).\nFigure 6 depicts a factored polynomial representa tion of the Bayesian network in Figure 5, evalu ated under evidence a, which we shall use as a run ning example. The partial derivatives of :F with re spect to each network indicator and parameter is shown below the indicator/parameter. For example, 8:F(a)/8Ba = 1 and 8:F(a)/Ba = 0.\nWe now discuss some classical queries that can be answered based on first partial derivatives.3\nThe first class of queries relates to \"what-if\" anal ysis. Specifically, after having computed the prob ability of some evidence e, in which variable X is instantiated to some value x', we ask: what would be the probability of e if the evidence on X was dif ferent, say, x. According to Theorem 2, the answer to this query is exactly 8:F(e)/8'Ax\u00b7 In Figure 6 for example, where e = a, suppose we ask: what would be the probability of evidence if A was instantiated to a instead. That would be 8:F(e)/8'Aa = .7, which\n3Recovering probabilistic quantities from partial derivatives seems to be a standard technique in certain statistical literatures. There, a probability distribution is represented using its generating function, allowing one to recover means and variances from the partial deriva tives of such a function. The main attraction of this approach is that the generating function tends to have a closed form, allowing the derivatives to have closed forms too-see [8] for an overview of the technique and its benefits.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nis also the prior probability of ii.\nAnother class of queries that is immediately obtain able from partial derivatives is posterior marginals:\nCorollary 1 (Posterior Marginals) For X fl. E:\nP ( I ) = fJ:F(e)l8>..x r x e F(e) .\nTherefore, the partial derivatives will give us the posterior marginal of every variable in constant time. In Figure 6, where e = a, Pr(b I e) (8F(e)l8>..b)IF(e) = .03/.3 = .1 and Pr(b I e) = (8F(e)l8>..r,)IF(e) = .27/.3 = .9.\nThe ability to compute such posteriors efficiently is probably the key celebrated property of join tree algorithms [10, 11], as compared to variable elimination algorithms [20, 7, 21]. The latter class of algorithms is much simpler except that they can only compute such posteriors by invoking themselves once for each network variable, leading to a com plexity of O(n2 exp(w)). Jointree algorithms can do this in 0( n exp( w)) , however, but at the expense of a more complicated algorithm. The algorithm we are presenting based on the computation of partial derivatives can also compute all such marginals in only 0 ( n exp ( w)) and seems to represent a middle ground in this efficiency I simplicity tradeoff.\nOne of the main complications in Bayesian network inference relates to the update of probabilities after having retracted evidence. This seems to pose no difficulties in the presented framework. For exam ple, we can compute the posterior marginal of every instantiated variable, after the evidence on that vari able has been retracted immediately.\nCorollary 2 (Evidence Retraction) For every variable X and evidence e, we have:\nPr(e - X)\nPr(x I e-X)\n\"' 8F(e). L...... 8>.. '\nX X\n8F(e)I8Ax Lx 8F(e)I8Ax \u00b7\nIn Figure 6, where e = a, Pr(e - A) 8F(e)l8>..a + 8F(e)I8Aa = 1 and Pr(ii I e-A) = (8F(e)l8>..a)I(8F(e)l8>..a + 8F(e)l8>..a) = .711 = .7.\nThe above computation is the basis of an investiga tion of model adequacy [4, Chapter 10] and is typi cally implemented in the join tree algorithm using the\ntechnique of fast retraction, which requires a modifi cation to the standard propagation method in join trees [4, Page 104]. As given by the above theorem, we get this computation for free once we have partial derivatives with respect to network indicators.\nNote that once we have the partial derivatives oF I8Br, we can implement the APN algorithm for learning network parameters as shown in [19]. As for implementing the EM algorithm, we need the poste rior marginals over network families [16], which are easily obtainable from the partial derivatives since\nPr(f I e, e)= aF;Ce\ufffd\ufffdtBr e(f).\nFor example, in Figure 6, Pr(b, a I a) is 8F(a)j8(Jab B _ \ufffd 1 _ 1 F(a) ab - .3 \u00b7 - \u00b7 \u00b7 We now turn to the semantics of second derivatives:\nTheorem 3 (Semantics of 2nd PDs) Let N be a Bayesian network representing probability distri bution Pr and let F be its canonical polynomial. For every pair of variables X =/:- Y, pair of families F 1 =/:- F 2, and evidence e:\n82 F(e, e)l8>..x8>..y\n82 F(e, e)l8>..x8Br1\n82 F(e, e)I8Br18Br2\nPr(x, y, e-XY I e);\nPr (X' fl' e -X I e) I e ( fi); Pr(f1, f2, e I e)le(fi)e(f2) \u00b7\nFor example, the first identity reads: evaluating the derivative 82 F I OAxOAy at evidence e and quantifi cation e gives the probability Pr(x, y, e-XY I e).\nThe significance of Theorems 2 and 3 is two fold:\n1. They show us how to compute answers to clas sical probabilistic queries by differentiating the polynomial representation of a Bayesian net work. Therefore, if we have an efficient way to generate the polynomial representation and to differentiate it, then we also have an efficient way to perform probabilistic reasoning. This is the view we promote in this paper.\n2. They show us how to compute valuable partial derivatives using classical probabilistic quanti ties. The partial derivatives of Pr(e) and Pr(y I e) are important when estimating Bayesian net work parameters and when performing sensitiv ity analysis. The third identity of Theorem 3, for example, shows us how to compute the sec ond partial derivative of Pr(e) with respect to two network parameters, Br1 and Br2, using the joint probability over their corresponding fami lies, Pr(f1,f2,e).\n127\n128 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nWe have to note, however, that expressing partial derivatives in terms of classical probabilistic quanti ties requires some conditions: e(f), e(fl) and 8(f2) cannot be 0 in Theorems 2 and 3. In the context of jointree algorithms, this complication can possi bly be addressed using lazy propagation-see (12]. There is no such concern, however, if partial deriva tives are computed directly as we do in this paper.\nTheorems 2 and 3 facilitate the derivation of results relating to sensitivity analysis. Here's one example:\nTheorem 4 (Sensitivity) Let N be a Bayesian network specifying distribution Pr and let F be its polynomial representation. For variable Y \ufffd E:\naPr(y I e) 8Bxu\n_1 _ ( 8 2F(e) F(e) _ a:F(e) a:F(e)) F(e) 2 8Bxu8>..y 8Bxu a>..y Pr(y, x, u I e) - Pr(y I e)Pr(x, u I e)\nPr(x I u)\nThis theorem provides an elegant answer to the most central question of sensitivity analysis in Bayesian networks, as it shows how we can compute the sen sitivity of a conditional probability to a change in some network parameter. The theorem phrases this computation in terms of both partial derivatives and classical probabilistic quantities-the second part, however, can only be used when Pr(x I u) =f. 0.\nOne has to note an important issue here relating to co-varying parameters. Let X be a variable in a Bayesian network and let U be its parents. We must then have L:x G(xu) = 1 for fixed u. Therefore, when one of the parameters Bxu changes its value from G(xu) to G'(xu), all related parameters must also change so that L:x e' (xu) = 1 is maintained. It is common to assume that parameters Bxu, for a fixed u, are all linear functions of some meta param eter Tx. That is, Bxu = ax Tx + f3x, where ax and f3x are constants and L:x axTX + f3x = 1. Then using the generalized chain rule of differential calculus:\n\"\"' aPr(y I e) aPr(y I e)jaTx = \ufffd ax 8Bxu . X\nConsider now the following three problems relating to sensitivity analysis. We want to compute the derivative aPr(y I e)jaTx for\nProblem ( 1): every Y and every Tx;\nProblem (2): some Y and every Tx;\nProblem (3): every Y and some Tx.\nGiven Theorem 4, and given that we can compute all first and second partial derivatives in O(n2 exp(w)) time, it follows immediately that Problem (1) can be solved in O(n2 exp(w)) time. We also show in (18] that the second partial derivative 8 2 F(e)jaBxuOAy can be computed in O(nexp(w)) time for every parameter Bxu and a fixed indicator >..y, or for a fixed parameter Bxu and every indicator Ay\u00b7 There fore, Problems (2) and (3) can each be solved in O(nexp(w)) time.\nThere seems to be two approaches for computing the derivative aPr(y I e)jaBxu, which has been re ceiving increased attention recently due to its role in sensitivity analysis and the learning of network parameters. We have just presented one approach where we found a closed form for aPr(y I e)jaBxu, using both partial derivatives and classical proba bilistic quantities. The other approach capitalizes on the observation that Pr(y I e) has the form (aBxu + (3)/('yBxu + 6) for some constants a, (3, \"(and 6(1]. According to this second approach, one tries to compute the values of these constants based on the given Bayesian network and then computes the derivative of (aBxu + (3)/('yBxu + 6) with respect to Bxu\u00b7 See (12, 14] for an example of this approach, where it is shown how to compute such constants using a limited number of propagations in the con text of a jointree algorithm.\nWe now present yet another example of results that are facilitated by the view we promote in this paper. That is, we consider one of the key questions that arise in real-world diagnostic applications. Suppose we are given some evidence e and some hypothe sis Y. We propagate the evidence and find that Pr(y I e) > Pr(Y I e); that is, y is the fault given evidence e. An expert may conclude differently, that fj is more probable in this case, hence, indicating a problem in the Bayesian network model. Assuming that the network structure is correct, a key question is: which network parameters should we tweak in order to correct the problem - that is, leading to Pr(y I e) :::; Pr(fj I e). There is typically more than one parameter to tweak, but the following theorem provides key insights into answering this question:\nTheorem 5 (Parameter Tweaking) Let Y and X be binary variables in a Bayesian network N with polynomial representation F. Let e and G' be two quantifications of N which agree on all param eters except those for xu and xu. If Y \ufffd E, then\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nPr(y I e, 8') \ufffd Pr(y I e, 8') iff 8(xu)8(xu)(G- H) \ufffd (8'(xu) - 8(xu)] (8(xu)(I- J) + 8(xu)(K- L)] ,\nwhere\nG aF(e, 8)fa>.y H = aF(e, 8)/a>..g I 8(xu)82 F(e, 8)ja>.y8Bxu J 8(xu)82 F(e, 8)ja>.g8Bxu K 8(xu)82 F( e, 8)ja>.g8Bxu L 8(xu)82 F( e, 8) / a>.yaBxu\u00b7\nLet us call 8 the pre-tweak quantification and 8' the post-tweak quantification. The above theorem is stating conditions on the pre-tweak quantifica tion, and the amount of tweaking, which would en sure that a certain hypothesis ranking is achieved in the post-tweak quantification. That is, the theo rem allows us to compute the change to parameters Bxu and Bxu, respectively, which ensures a particular ranking on the hypotheses y and 'f}.\nFor an example application of Theorem 5, consider the network in Figure 5 where Pr(b) = .59 > Pr(b) = .41. We want to compute the amount of change to parameter Ba, initially set to 8(a) = .3, of root node A which is needed to ensure that Pr(b) \ufffd Pr(b) . Applying Theorem 5:\n8'(a)-8(a) \ufffd (.3)(.7)(.sg-.41)\n= .12857 . . 3(.56-.14) + .7(.27-.03)\nThat is, 8'(a) \ufffd .42857 ensures that Pr(b) \ufffd Pr(b) .\nWe close this section by a note on the complexity of applying Theorem 5. Since we can compute all first and second partial derivatives in O(n2 exp(w)) time, we are able to compute in O(n2 exp(w)) time, for each variable Y and each variable X, the amount of change needed in the parameters of X to reverse the probabilistic ranking of Y's values. Moreover, we show in (18] that we can perform this computation in 0 ( n exp ( w)) time only, for a fixed Y or a fixed X.\n4 Compiling a Bayesian Network\nOur goal in this section is to present a variable elimination algorithm for compiling a factored poly nomial representation of a Bayesian network. This is to be contrasted with a canonical polynomial rep resentation (Definition 1), which is straightforward to obtain but requires exponential space.\nAs an example, following is the canonical polynomial of the network in Figure 5:\nF = BaBab>.a>.b + BaB ali>.a>.li + BaBab>.a>.b + BaBal>>.a>.li,\nand following is one of its factored representations:\nThe algorithm we present requires an ordering 1r of the variables in a Bayesian network N. The algo rithm goes as follows:\n1. For every variable X with parents U, parame terize the CPT of X, \u00a2 x, as follows. Replace each entry in \u00a2x which corresponds to instan tiation XU by BxuAx.\n2. Consider every variable X according to order ?T:\n(a) Multiply all tables that contain variable X to yield table \u00a2.\n(b) Replace the multiplied tables by the result of summing out X from \u00a2, sum_out(\u00a2, X).\n3. Only one table \u00a2 will be left, with a single entry. Return the single entry of \u00a2.\nWe will call this algorithm VE_COMPILE(N, 1r) . Following is an illustrative example of VE_COMPILE: we will use the elimination order 1r =< B, A > to compile the network in Figure 5. The parameterized CPTs are given below:\nA B c/JB A c/JA true true Bab>.b true BaAa true false ()ab)..b false BaA a false true BabAb\nfalse false ()al)b\nTo eliminate variable B, we sum it out from table \u00a2B, since it is the only table containing B:\nA sum_out(\u00a2B, B) true BabAb + ()ab)..b false BabAb + ()ab)..b\nTo eliminate variable A, we must multiply the above table with table \u00a2A and then sum-out variable A from the result. Multiplying gives:\nA \u00a2Asum_out(\u00a2B, B) true BaA.a(BabAb + ()ab>.b) false Ba>.a(BabAb + ()ab>.b)\nSumming out, we get\nI sum_out(\u00a2Asum_out(\u00a2B, B), A)\n129\n130 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nThe polynomial,\n:F(>.a, >.a:, >.b, >.li, Ba, Ba:, Bab, Bar, Ba:b, Ba:l;)\nBa>.a(Bab>.b + ()ali>.!i) + Ba:>.a:(Ba:bAb + Ba:iiAii)\nis then the result of this compilation and is depicted in Figure 6 as a rooted DAG. When the parame ters in such a compilation are fixed to some numeric values, the result is still a polynomial and is a spe cial case of a Query-DAG [6]. Note, however, that a Query-DAG can have multiple roots corresponding to different queries which must be specified before the compilation process takes place. The compi lation as presented above, however, is only meant to answer one query: computing the probability of given evidence. But as we have shown earlier, many other queries can be computed from the par tial derivatives of this compilation.\nWe can now state the soundness/complexity result:\nTheorem 6 Given a Bayesian network N and a corresponding variable elimination order 1r of width w and length n, the call VE_COMPILE(N, 1r) returns a polynomial representation of network N of size O(n exp(w)) and in O(n exp(w)) time.\n5 Evaluating and Differentiating a Polynomial Representation\nWe now turn to the process of evaluating a network compilation :F under some evidence e and quantifi cation e, and then computing all its partial deriva tives. We will first describe the computation pro cedurally using a message-passing scheme and then provide the semantics of such computation.\nAs it turns out, the evaluation of :F and comput ing its partial derivatives can be accomplished us ing a two-phase message passing scheme in which each message is simply a number. In the first phase, messages are sent from nodes to their parents in :F, leading to an evaluation of :F. In the second phase, messages are sent from nodes to their children, lead ing to the computation of all partial derivatives.\nWe will associate two attributes with each node i in the compilation :F: a value val ( i) and a partial derivative pd(i) . The goal of message passing is to set the value of each of these attributes under some evidence e and quantification e. We are mainly in terested in the value of root node r, val (r), as it rep resents :F(e, e), and the partial derivative of each leaf node I, pd (l) , at it represents 8:F(e, e)j81. Mes sage passing proceeds in two phases: the first phase\nsets the value of each node, and the second phase sets the partial derivatives.\nPassing val-messages In the first phase, a val message is passed from each node to all its parents according to the following rules:\n1. Initiation: Each leaf node I sets its value:\n- val (l) f- e (x) when I is an indicator Axi - val (l) +- e(xu) when I is a parameter Bxu\u00b7\nNode I then sends a message mes(l, k) = val (l) to each parent k.\n2. Iteration I: When an addition node i receives messages from all its children j, it sets its value val (i) +- Lj mesU, i) and then sends a message mes(i, k) = val (i) to each parent k.\n3. Iteration II: When a multiplication node i re ceives messages from all its children j, it sets its value val(i) +- ITj mesU, i) and then sends a message mes(i, k) = val ( i) to each parent k.\n4. Termination: The first phase terminates when the root node r receives messages from all its children. In that case, :F(e, e) = val ( r) .\nThis process is illustrated in Figure 7, where it leads to assigning val (r) = .3, indicating that the proba bility of evidence is :F( e, e) = .3.\nPassing pd-messages In the second phase, a pd message is passed from each node to all its children:\n1. Initiation: The root node r sets pd (r) +- 1 and sends message mes(r,j) = 1 to each child j.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\n2. Iteration I: When an addition node i re ceives messages from all its parents k, it sets pd(i) +-- Lk mes(k, i) and then sends a message mes(i,j) = pd(i) to each child j.\n3. Iteration II: When a multiplication node i re ceives messages from all its parents k, it sets pd(i) +-- Lk mes(k, i) and then sends a message mes(i,j) = pd(i) ITj';tj vaiU') to each child j, where j' is a child of i.\n4. Termination: The second phase terminates when each leaf node I receives messages from all its parents. In that case, pd(l) is guaranteed to be the partial derivative a:F( e, 0) 1 al.\nThis process is illustrated in Figure 8, assuming that val-messages have been propagated as in Figure 7. The process leads to assigning pd(l) to each leaf node in the compilation, shown in a box below the leaf node. For example, a:F(e, 8 )ja>..b = .03. Clearly, the number of messages passed in both phases is twice the number of edges in the compi lation :F. And since each message can be computed in constant time, the whole computation is linear in the size of :F. We are currently working on count ing the number of operations performed by our mes sage passing scheme and comparing that to other frameworks for multiple-query inference, such as the HUG IN and Shenoy /Shafer architectures.\nThe Semantics of message passing We now turn to the semantics of our message-passing scheme. Specifically, any node i in function :F can be viewed as a subfunction F, of :F. For any evidence e, our message passing scheme guarantees that val(i) F,(e, 8). Alternatively, node i can be viewed as\nan intermediate variable V; of function :F. Our message passing scheme guarantees that pd(i) = a:F(e, 8)/aV;.\nThese results follow directly from the work of [9], which addresses the more general problem of com puting partial derivatives for a function that has an acyclic computation graph. Our network compila tions :F fall into that category.\nWe close this section by two remarks. The first re gards the computation of rounding errors when eval uating :F(e, 8) using a limited-precision computer. Specifically, let 8i be the rounding error \"generated\" when computing the value of node i-that is, the dif ference between the value computed using limited precision and infinite-precision. If I 8i I:::; E I val(i) I for a machine-specific E, then the rounding error in computing :F(e, 8), \ufffd:F(e, 8), can be bound as fol lows (see [17] for details):\n\ufffd:F(e, 8):::; E L I pd(i )val (i ) I . non-leaf i\nNote that bounding the rounding error can be done simultaneously during the passage of pd-messages, and requires no extra space.\nOur final remark is on the computation of second partial derivatives. The partial derivative pd(i) is by itself a function of many variables and can be differentiated again using the same approach. In particular, if the compilation :F has O(n) variables, then its second partial derivatives can be computed simultaneously in O(n I :F I) time and space, where I :F I is the size of :F. Again, this follows from a more general result in [17].\nTherefore, if the compilation :F of a Bayesian net work of size n is induced using an elimination order of width w, as suggested earlier, then the value of polynomial :F; all its first partial deriva tives; and all its second partial derivatives can be computed simultaneously in O(n2 exp(w)) time and space. This is quite interesting given the number of queries that can be answered in constant time given such derivatives. We are unaware of any computa tional framework for Bayesian networks, which com bines the simplicity, comprehensiveness and com putational complexity of the presented framework based on partial derivatives.\n6 Conclusion\nWe have presented one of the simplest, yet most comprehensive frameworks for inference in Bayesian\n13 1\n132 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nnetworks. According to this framework, one com piles a Bayesian network into a multivariate poly nomial :F using a variable elimination scheme. For each piece of evidence e, one then traverses the poly nomial :F twice. In the first pass, the value of :F is computed. In the second pass, all its partial deriva tives are computed. Once such derivatives are made available, one can compute in constant time answers to a very large class of probabilistic queries, relating to classical inference, parameter estimation, model validation and sensitivity analysis.\nThe proposed framework makes two key contribu tions to the state-of-the-art on probabilistic reason ing. First, it highlights a key and comprehensive role of partial differentiation in this form of reasoning, shedding new light on its utility in sensitivity analy sis. Second, it presents one of the simplest, yet most comprehensive frameworks for inference in Bayesian networks, which lends itself to cost-effective imple mentations on a variety of software and hardware platforms. Note that compiling a Bayesian network can be performed off-line. The only computation that is needed on-line is that of passing val-messages and pd-messages, which is quite simple. We are cur rently investigating the development of dedicated hardware for implementing the proposed message passing scheme. Similar dedicated hardware has been developed for other reasoning frameworks, such as genetic programming and neural networks, and can provide a valuable tool for computationally in tensive tasks in Bayesian networks, such as the esti mation of network parameters using APN /EM.\nThe development of such hardware is also expected to help in migrating Bayesian-network applications to embedded systems - such as consumer electron ics - which are characterized by their primitive computational resources, therefore, affording to host only the very simplest reasoning frameworks.\nReferences\n[1] E. Castillo, J. M. Gutierrez, and A. S. Hadi. Goal oriented symbolic propagation in Bayesian net works. In AAAI, 1996.\n[2] E. Castillo, J. M. Gutierrez, and A. S. Hadi. Sensi tivity analysis in discrete Bayesian networks. IEEE Trans. on Systems, Man, and Cybernetics, 27:412- 423, 1997.\n[3] V. M. H. Coupe, N. Peek, J. Ottenkamp, and J. D. F. Habbema. Using sensitivity analysis for efficient quantification of a belief network. Artifi cial Intelligence in Medicine, 17:223-247, 1999.\n[4] R. Cowell, A. Dawid, S. Lauritzen, and D. Spiegel halter. Probabilistic Networks and Expert Systems. Springer, 1999.\n[5] A. Darwiche. A differential approach to inference in bayesian networks. Technical Report D-108, Com puter Science Department, UCLA, Los Angeles, Ca 90095, 1999.\n[6] A. Darwiche and G. Provan. Query DAGs: A prac tical paradigm for implementing belief-network in ference. JAIR, 6:147-176, 1997.\n[7] R. Dechter. Bucket elimination: A unifying frame work for probabilistic inference. In UAI, 1996.\n[8] R. L. Graham, D. E. Knuth, and 0. Patashnik. Concrete Mathematics. Addison Wesley, 1989.\n[9] G. Gunter Rote. Path problems in graphs. Com puting Suppl., 7:155-189, 1990.\n[10] C. Huang and A. Darwiche. Inference in belief net works: A procedural guide. International Journal of Approximate Reasoning, 15(3):225-263, 1996.\n[1 1] F. V. Jensen, S.L. Lauritzen, and K.G. Olesen. Bayesian updating in recursive graphical models by local computation. Computational Statistics Quar terly, 4:269-282, 1990.\n[12] F. V. Jensen. Gradient descent training of bayesian networks. In In Proceedings of the Fifth Euro pean Conference on Symbolic and Quantitative Ap proaches to Reasoning and Uncertainty.\n[13] F. V. Jensen. An Introduction to Bayesian Net works. Springer Verlag, New York Inc., 1996.\n[14] U. Kjaerulff and L. C. van der Gaag. Making sen sitivity analysis computationally efficient. This vol ume.\n[15] K. B. Laskey. Sensitivity analysis for probability assessments in Bayesian networks. IEEE Trans. on Systems, Man, and Cybernetics, 25:901-909, 1995.\n[16] S. L. Lauritzen. The EM algorithm for graphical as sociation models with missing data. Computational Statistics and Data Analysis, 19:191-201, 1995.\n[17] Masao. Simultaneous computation of functions, partial derivatives and estimates of rounding error. Japan J. Appl. Math., 1:223-252, 1984.\n[18] J. Park and A. Darwiche. Differential inference in belief networks: A procedural guide. Technical Re port D-1 14, Computer Science Department, UCLA, Los Angeles, Ca 90095, 2000.\n[19] S. Russell, J. Binder, D. Koller, and K. Kanazawa. Local learning in probabilistic networks with hidden variables. In UAI, pages 1146-1152, 1995.\n[20] R. Shachter, B.D. D'Ambrosio, and B. del Favero. Symbolic Probabilistic Inference in Belief Networks. In UAigo.\n[21] N. Lianwen Zhang and D. Poole. Exploiting causal independence in bayesian network inference. JAIR, 5:301-328, 1996."}], "references": [{"title": "Goal oriented symbolic propagation in Bayesian net\u00ad works", "author": ["E. Castillo", "J.M. Gutierrez", "A.S. Hadi"], "venue": "In AAAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Sensi\u00ad tivity analysis in discrete Bayesian networks", "author": ["E. Castillo", "J.M. Gutierrez", "A.S. Hadi"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Using sensitivity analysis for efficient quantification of a belief network", "author": ["V.M.H. Coupe", "N. Peek", "J. Ottenkamp", "J.D.F. Habbema"], "venue": "Artifi\u00ad cial Intelligence in Medicine,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "A differential approach to inference in bayesian networks", "author": ["A. Darwiche"], "venue": "Technical Report D-108,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Query DAGs: A prac\u00ad tical paradigm for implementing belief-network", "author": ["A. Darwiche", "G. Provan"], "venue": "in\u00ad ference. JAIR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Bucket elimination: A unifying frame\u00ad work for probabilistic inference", "author": ["R. Dechter"], "venue": "In UAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Path problems in graphs", "author": ["G. Gunter Rote"], "venue": "Com\u00ad puting Suppl.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1990}, {"title": "Inference in belief net\u00ad works: A procedural guide", "author": ["C. Huang", "A. Darwiche"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}], "referenceMentions": [{"referenceID": 4, "context": "We have promoted the compilation of Bayesian networks in earlier work, where we referred to polynomials such as :F as Query-DAGs [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "We show how to answer a large number of queries relating to classical inference [10, 13, 7, 21, 20], parameter estimation [19, 16], model validation [4] and sensitivity analysis [15, 2, 3] in constant-time once the partial derivatives of the compiled polynomial are computed.", "startOffset": 80, "endOffset": 99}, {"referenceID": 5, "context": "We show how to answer a large number of queries relating to classical inference [10, 13, 7, 21, 20], parameter estimation [19, 16], model validation [4] and sensitivity analysis [15, 2, 3] in constant-time once the partial derivatives of the compiled polynomial are computed.", "startOffset": 80, "endOffset": 99}, {"referenceID": 1, "context": "We show how to answer a large number of queries relating to classical inference [10, 13, 7, 21, 20], parameter estimation [19, 16], model validation [4] and sensitivity analysis [15, 2, 3] in constant-time once the partial derivatives of the compiled polynomial are computed.", "startOffset": 178, "endOffset": 188}, {"referenceID": 2, "context": "We show how to answer a large number of queries relating to classical inference [10, 13, 7, 21, 20], parameter estimation [19, 16], model validation [4] and sensitivity analysis [15, 2, 3] in constant-time once the partial derivatives of the compiled polynomial are computed.", "startOffset": 178, "endOffset": 188}, {"referenceID": 3, "context": "Proofs of all theo\u00ad rems can be found in the full paper [5].", "startOffset": 56, "endOffset": 59}, {"referenceID": 0, "context": "More generally, it is shown in [1, 2] that Pr(e) can be expressed as a polynomial of network parameters in which each pa\u00ad rameter has degree one.", "startOffset": 31, "endOffset": 37}, {"referenceID": 1, "context": "More generally, it is shown in [1, 2] that Pr(e) can be expressed as a polynomial of network parameters in which each pa\u00ad rameter has degree one.", "startOffset": 31, "endOffset": 37}, {"referenceID": 0, "context": "The polynomials discussed in [1, 2], however, are constructed for a given evi\u00ad dence e: they only contain one type of variables cor\u00ad responding to network parameters, with no variables corresponding to evidence indicators.", "startOffset": 29, "endOffset": 35}, {"referenceID": 1, "context": "The polynomials discussed in [1, 2], however, are constructed for a given evi\u00ad dence e: they only contain one type of variables cor\u00ad responding to network parameters, with no variables corresponding to evidence indicators.", "startOffset": 29, "endOffset": 35}, {"referenceID": 7, "context": "The ability to compute such posteriors efficiently is probably the key celebrated property of join\u00ad tree algorithms [10, 11], as compared to variable\u00ad elimination algorithms [20, 7, 21].", "startOffset": 116, "endOffset": 124}, {"referenceID": 5, "context": "The ability to compute such posteriors efficiently is probably the key celebrated property of join\u00ad tree algorithms [10, 11], as compared to variable\u00ad elimination algorithms [20, 7, 21].", "startOffset": 174, "endOffset": 185}, {"referenceID": 4, "context": "When the parame\u00ad ters in such a compilation are fixed to some numeric values, the result is still a polynomial and is a spe\u00ad cial case of a Query-DAG [6].", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": "These results follow directly from the work of [9], which addresses the more general problem of com\u00ad puting partial derivatives for a function that has an acyclic computation graph.", "startOffset": 47, "endOffset": 50}], "year": 2011, "abstractText": "We present a new approach for inference in Bayesian networks, which is mainly based on partial differentiation. According to this approach, one compiles a Bayesian net\u00ad work into a multivariate polynomial and then computes the partial derivatives of this polynomial with respect to each vari\u00ad able. We show that once such derivatives are made available, one can compute in constant-time answers to a large class of probabilistic queries, which are central to classical inference, parameter estimation, model validation and sensitivity analysis. We present a number of complexity results relating to the compilation of such polyno\u00ad mials and to the computation of their par\u00ad tial derivatives. We argue that the com\u00ad bined simplicity, comprehensiveness and computational complexity of the presented framework is unique among existing frame\u00ad works for inference in Bayesian networks.", "creator": "pdftk 1.41 - www.pdftk.com"}}}