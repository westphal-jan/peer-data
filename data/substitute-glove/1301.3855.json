{"id": "1301.3855", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Likelihood Computations Using Value Abstractions", "abstract": "In very paper, unfortunately specifically certain - essential value abstraction five speeding Bayesian networks inference. This is done by grouping approximated importance few treating well third defining as only song govern. As we. , such abstractions turn exploit regularities under obtains negligible distributions of also held specific expression of measured stochastic. To appointed unjust value abstraction, we notion own sense of actually instance abstraction that devise inference algorithms that or it well cuts, make of syllogism. Our exam way particularly requires for approach element focusing close except hidden variables. In such ranging, repeated depends computations are obtain which EM actually other approximation multi-dimensional techniques. Since usually manipulations are repeated with conscience then the same possible allowing, needs e.g. do provide contribution speedup to been learning process. We aim where infinitesimal on genetic reconnection suffer within the use of quantity surrealism sometimes negates although setting problematic bringing longer - infeasible ideal.", "histories": [["v1", "Wed, 16 Jan 2013 15:50:10 GMT  (346kb)", "http://arxiv.org/abs/1301.3855v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nir friedman", "dan geiger", "noam lotner"], "accepted": false, "id": "1301.3855"}, "pdf": {"name": "1301.3855.pdf", "metadata": {"source": "CRF", "title": "Likelihood Computations Using Value Abstraction", "authors": ["Nir Friedman", "Dan Geiger"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we use evidence-specific value ab straction for speeding Bayesian networks infer ence. This is done by grouping variable val ues and treating the combined values as a sin gle entity. As we show, such abstractions can ex ploit regularities in conditional probability distri butions and also the specific values of observed variables. To formally justify value abstraction, we define the notion of safe value abstraction and devise inference algorithms that use it to re duce the cost of inference. Our procedure is par ticularly useful for learning complex networks with many hidden variables. In such cases, re peated likelihood computations are required for E M or other parameter optimization techniques. Since these computations are repeated with re spect to the same evidence set, our methods can provide significant speedup to the learning pro cedure. We demonstrate the algorithm on genetic linkage problems where the use of value abstrac tion sometimes differentiates between a feasible and non-feasible solution.\n1 Introduction Inference in probabilistic models plays a significant role in\nmany applications. In this paper we focus on an applica tion in genetics: linkage analysis. Linkage analysis is a crucial tool for locating the genes responsible for complex traits (e.g., genetically transmitted diseases or susceptibil ity to diseases) . This analysis uses statistical tools to locate genes and identify the biological function of proteins they encode.\nLinkage analysis is based on a clear probabilistic model of genetic events. Mapping of disease genes is done by performing parameter optimization to find the genetic map location that maximizes the likelihood of the evidence (i.e., maximum likelihood estimation). This probabilistic infer ence is closely related to Bayesian network inference.\nOur starting point is the V ITE S SE algorithm [11], a fairly recent algorithm for linkage analysis that implements some\ninteresting heuristics for speeding up computations. These heuristics achieve impressive speedups that allow to an alyze linkage problems that could not be dealt with us ing the prior state of the art procedures. In the language of Bayesian networks these heuristics can be understood as finding Value abstractions (reminiscent of the abstrac tions studied by [1 5]). These abstractions are found in an evidence-specific manner to save computations for a spe cific training example.\nIn the remainder of this paper we review genetic linkage analysis problems. Then we develop a method to find value abstractions that generalizes the ideas of [11] in a manner that is independent of the inference procedure used. We then extend these ideas in combination with clique-tree in ference procedures. Finally, we describe experimental re sults that examine the effectiveness of these ideas.\n2 Genetic Linkage Analysis\nWe now briefly introduce the relevant genetic notions that are needed for the discussion below. We refer the reader to [12] for a comprehensive introduction to linkage analysis. The human genetic material consists of 22 pairs of auto somal chromosomes, and a pair of the sex chromosomes. The situation with the later pair is slightly different, and we will restrict the discussion here to the autosomal case, al though all the techniques we discuss apply to this case with minor modifications. In each pair of chromosomes, one chromosome is the paternal chromosome, inherited from the father, and the other is the maternal chromosome, in herited from the mother. We distinguish particular loci in each chromosome pair. Loci that are biologically expressed are called genes. At each locus, a chromosome encodes a particular sequence of D NA nucleotides. The variations in these sequences are the source of the variations we see among species members. The possible variants that might appear at a particular locus are called alleles. In general, the maternal copy and paternal copy of the same locus can be different.\nThe aim of linkage analysis to construct genetic maps of known loci, and to position newly discovered loci with re spect to such maps. Genetic maps describe the relative positions of loci of interest (which can be genes, or ge-\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 193\nNo crossover between A and B\neta !SSi\nA B\nCrossover between A and B\n: g\ufffd * g;; : ---------------\nFigure 1: Illustration of recombination during meiosis.\nnetic markers) in terms of their genetic distance. This dis tance measures the probability of crossovers between pairs of loci during meiosis, the process of cell division leading to creation of gemetes (either sperms or egg cells). During the formation of gemetes, the genetic material undergoes recombination, as shown in a schematic form in Figure 1.\nThe genetic distance between two loci is measured in terms of the recombination fraction between two loci, which is just the probability of recombination of the two loci. The smaller this fraction is, the closer the two loci are. A recombination fraction close to 0.5 indicates that the two loci are sufficiently far so that their inheritance appears independent.\nEstimation of these fractions is complicated by the fact that we do not observe alleles on chromosomes. Instead we can observe phenotype, which might be traits, such as blood type, eye color, or onset of a disease, or they might oe the result of genetic typing. Genetic typing provides the alleles present in each locus, but does not provide an alignment with the maternal/paternal chromosome. Thus, when genetic typing shows an individual has alleles a and A in one locus, and alleles b and B in another locus, we do not know if a and b where in inherited from the same parent. In such a situation there are 4 four possible con figurations (ab/AB, aB/Ab, AbfaB, ABfab). When we consider multiple loci, the number of possible configura tions grows exponentially.\n2.1 Probabilistic Networks Models of Pedigrees We start by showing how the underlying model of linkage\nanalysis problems can be represented by probabilistic net works, and then discuss standard approached for comput ing likelihoods in pedigrees. We note that the representa tion of pedigrees in terms of graphical models, have been discussed in [6, 8].\nA pedigree defines a joint distribution over the genotype and phenotype of the individuals. We denote the genotype and phenotype of individual i as G[i] and P[i], respectively. The semantics of a pedigree are: given the genotype of i's parents, G[i] is independent from G[j] for any ancestor j of i; and given the genotype G[i], the phenotype P[i] is independent of all other variables in the pedigree. We can represent these assumptions on the distribution of G[i] and P[i] by a network where the parents of G[i] are the G[j] and G[k] where j and k are i's parents, and the parent of\nP[i] is G[i]. Not surprisingly, this network has essentially the same topology as the original pedigree.\nThe local probability models in the network have one of the following forms:\n\u2022 General population genotype probabilities: Pr ( G(i] ), when i is a founder.\n\u2022 Transmission models: Pr(G(i] I G(j], G[k]) where j and k are i's parents in the pedigree_!\n\u2022 Penetrance models: Pr(P[i] I G[i]).\nThis discussion shows that there is a simple transforma tion from pedigrees to probabilistic networks. This simple transformation obscures many of the details of the pedigree model within the transition and penetrance models. Both of these local probability models are quite complex. We gain more insight into the \"structure\" of the joint distribution if we model the pedigree at a more detailed level. This can be done in various ways; e.g., [6, 8]. We find it most con venient to use a representation that is motivated by Lander and Green's [ 9] representation of pedigrees. For this repre sentation we introduce several types of random variables:\nGenetic Loci. We denote by A, B, C, ... the loci of in terest in the genetic analysis. For example, these can be marker loci and disease loci. For each individual i and lo cus A, we define random variables A[i,p], A[i, m] whose values are the specific value of the locus A in individual i's parental and maternal haplotypes (chromosomes), respec tively. That is, A[i,p] was inherited from i's father, and A[i, m] was inherited from i's mother.\nPhenotypes. We denote by F, G, ... the phenotypes that are involved in the analysis. These might include disease manifestations, genetic typing, or other observed pheno type such as blood types. For each individual i and pheno type F , we define a random variable F [i] that denote the value of the phenotype for the individual i.\nSelector variables. Similar to Lander and Green [ 9], we use auxiliary variables that denote the inheritance pattern in the pedigree. We denote by SA[i,p] and SA[i, m] the selection made by the meiosis that resulted in i's genetic makeup. Formally, if j and k denote i's father and mother,\n1 We make the standard assumption that if individual i is not a founder, then both of her parents are in the pedigree.\n194 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nFigure 2: A fragment of a probabilistic network represen tation of the transmission model, and the penetrance model in a 3-loci analysis.\nrespectively, then\nA[i p] = { A[\ufffd,p] ' A[J,m] if SA[i,p] = 0 if SA[i,p] = 1 and similarly A[i, m] depends on SA[i, m], A[k,p], and A[k,m].\nUsing this finer grain representation of the genotype and phenotype we can capture more of the independencies among the variables. For example, A[i,p] and A[i, m] are independent given the genotype of i's parents. Note that, they are dependent given evidence on i's children, or on phenotype that depends on both. Another example, occurs when we know that loci A and B are unlinked (say they are on different chromosomes), then A[i,p] is independent of B[i, p] given the genotype of i's father.\nFigure 2 shows a fragment of the network that describes parents-child interaction in a simple 3-loci analysis. The dashed boxes contain all of the variables that describe a single individual's genotypes or phenotype. In this model we assume that loci are mapped in the order A, B, and C. This assumption is reflected in the lack of an edge from SA[i,p] to Sc[i,p], which implies that the two are inde pendent given the value of Sn[i,p]. Figure 2 also shows the penetrance model for this simple 3-loci analysis. In this model we assume that each phenotype variable depends on the genotype at single locus. Again, this is reflected by the fact that each phenotype has edges only from the two hap lotypes of a single loci.\n2.1.1 Likelihood Computation in Pedigrees There are two main approaches to likelihood computation\non pedigrees: Elston- Stewart [3, 5] and Lander- Green [ 9]. The representation of pedigrees as probabilistic networks allows us to give a unified perspective of both. Broadly speaking, both are variants of variable elimination methods that depend on different strategies for finding elimination ordering, or equivalently, cluster-tree separators.\nFigure 3 shows an example of a pedigree. Elston Stewart's algorithm and later extensions essentially tra verse this network along the structure of the family tree.\nFigure 3: Schematic of a network corresponding to three loci pedigree. The dark nodes are loci variables in the model (e.g., A[i,p]), the dark gray nodes are phenotype variables, and the light gray nodes are selector variables (e.g., SA[i,p]). Each tree-like \"slice\" corresponds to one locus, and represents the inheritance model for that locus.\nAt each cluster they aggregate variables that correspond to an individual across all slices. On the other-hand, Lander Green's algorithm traverses this network from one slice to another. At each step they aggregate all the separator vari ables at one slice. In this sense, Lander- Green's algorithms treats a pedigree as a factorial HMM.\nThis discussion makes the (known) properties and restric tions of each procedure visible. When the pedigree has loops, the genotypes of individuals are no longer neces sarily separators. Thus, one has to resort to approaches for breaking loops. On the other hand, the Lander- Green pro cedure is not sensitive to loops in the pedigree. However, their procedure cannot applied for pedigree's with many selector variables in each slice and thus, their algorithm is limited to small pedigrees.\n2.2 Genetic Mapping The main task for linkage analysis is identifying the map\nlocation of disease genes from pedigree data. The standard approach for performing this analysis is to use non-linear optimization procedures that attempt to maximize the like lihood function. Such procedures evaluate the likelihood in several points that are close to each other and estimate the derivative by examining the differences in likelihood between these points. This approach requires several eval uations of the likelihood.\nIt is important to note that during this optimization, there are many repeated likelihood computations with respect to the same evidence. Moreover, the only parameters that change are the recombination fractions. That is, the only variables whose conditional probability distribution changes are the selector variables.\nThese repeated computation have been optimized by vari ous approaches. In particular, current linkage analysis soft-\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 195\nware perform some amount of genotype exclusion [13]. These exclusions use several rules of deduction to deter mine which genotypes are possible for individuals given their phenotype, or the possible genotypes of their direct relatives.\nIn addition, several researchers made the observation that maintaining the distinction between some of a these values often does not change the probability of the observations. This fact has been exploited in F A S T L I N K to combine all marker alleles that do not appear in any typed individual in the pedigree [13]. A more powerful use of this idea has been proposed by O'Connell and Weeks [11] and imple mented in the V I T E S S E program, where there is localized allele grouping for each individual in the pedigree. The cur rent V I T E S S E algorithm applies only to Ioopless pedigree within the framework of bottom-up Elston- Stewart style variable elimination.\n3 Safe Value Abstractions\nIn the next sections we develop theory and algorithms that exploit value abstraction in contexts similar to the genetics linkage analysis problems we describe above.\nLet X be a variable with a finite domain Val(X) and a probability distribution function P(X = x). An ab straction of the domain of X is a collection A of subsets of Val(X) that form a non-trivial partition of Val(X): no set in A is empty, every two sets in A are disjoint, and the union of all sets in A equals Val(X). For every v E Val(X), let va stand for the set in A that contains v. We call va the abstract value corresponding to v. Each abstraction defines a partition function a : Val(X) -+ C which maps a value v to its abstract value va via va = a( v) . An abstraction of X, denoted by xa, is a variable with a domain Val(Xa) = A which is an abstraction of Val(X), and a probability distribution function pa given by\nP(X = v) , { vE Val(X) lva=u(v)}\nor in a shorter notation by,\nThe set of abstractions for Val(X) forms a natural par tial order (or more precisely, a lattice) as follows. An abstraction A1 is finer than abstraction A2 if every set in A1 is a subset of a set in A2, in which case we also say A2 is coarser than A1. An abstraction A1 is strictly finer (coarser) than abstraction A2 if A1 is finer (coarser) than A2 and A1 ,P A2\u2022 The maximal abstraction consists of one set and the minimal abstraction consists of singletons. A refinement of two abstractions A1 and A2 is an ab straction A such that A is finer than A1 and finer than A2. A tight refinement of two abstractions A1 and A2 is a re finement A such that every other refinement A' of A1 and A2 is finer than A. In other words, suppose that a1 and a2 are the two partition functions defined by abstractions\nA1 and A2, respectively. Then, the partition function of their tight refinement A is given by a = a1 1\\ a2 defined such that a( v) = a( v') if and only if a1 ( v) = a1 ( v' ) and a2 ( v) = a2 ( v') . Intuitively, the refinement of two partition functions defines a partition function that preserves the dis tinctions made by both partition functions and introduces no new distinctions. W hen two abstractions are not related through refinement, they are said to be incomparable.\nEvidence with respect to a variable X is an assertion e that the value observed for X is among a subset 0 C Val(X) of the possible values. W hen 0 is a singleton, X is said to have been observed.\nAn abstraction a of X is safe with respect to e if P(e I X = x) = P(e I X = x') for all x, x' such that a(x) = a(x'). That is, the distinctions blurred by the abstraction a do not effect the probability of the evidence. We can\nalways find a safe abstraction, since the trivial abstraction that consists of singletons is always safe. Moreover, it is clear that there exist a maximally safe abstraction which is the coarsest safe abstraction.\nAs a simple example, consider a game where a player can bet on a dice outcome and wins if the outcome matches his bets. To formalize, suppose we have three variables Bet that can take the values odd and even, Dice that can take the values 1, . . . , 6, and Win that can be either yes or no. Suppose also that we observe that the player won, that is Win = yes. Clearly, the likelihood P(Win = yesiDice) does not depend on the distinction between all possible out comes of the dice. Since the player can only bet on even or odd outcome, the abstraction of values {1, 3, 5}, {2, 4, 6} is a safe one. This abstraction is clearly the maximally safe abstraction of Dice. However, there are many other safe ab stractions. Note that in this example, if the dice is fair, then P(Win,Dice = x) is the same for all values of x in the same partition. However, the abstraction is still safe even if the dice is not fair. The point is that we can compute P(Dice E {1, 3, 5}) without worrying about the rest of the domain (e.g., probability of various bets, etc.). This simple example suggests that it suffices to consider an abstraction of the dice when we compute the proba bility of winning in the betting game. This can lead to saving in the number of operations we perform in our calculations. Such computational savings can be much more drastic when one is presented with many intercon nected variables as is the case with Bayesian networks. Let X = {X 1, ... , X n} be a set of variables each associated with a finite domain Val(Xi )\u00b7 Also, let B, with a directed acyclic graph G, stand for a Bayesian network over X and let Pa(Xi ) be the parents of each Xi in B. An abstraction Ba of B is a Bayesian network with the same set of vertices and edges as in B, and where each variable Xi is replaced with an abstraction Xf.\nWe now want to determine the conditional probability distributions in Ba. We start by defining the probability of an abstraction given the \"un-abstracted\" parents:\nP(Xf = xf I pa(Xi ) = u) =\n196 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nALGORITHM ValueAbstract(B,e)\nInput: A Bayesian network B and evidence e Output: A safe abstraction sa wrt e\nDiscard: For every X; in S, remove from Val(X;) all values that are ir.compatible\nwithe (e.g., using arc-consistency algorithm) Comment: Nodes in e remain with one value\nAbstract: Set I:; := { Val(X;)}, fori = 1, ... , n\nIterate over X; in reverse topological order: Suppose that Pa(X;) = { X1, ... , Xk }.\n1. Set a i = 1\\ L:; Comment: This defines Xf. 2. Find partition functions ai , . . . , a1 of xl' ... 'xk such that P(Xia I x1, ... , xk ) = P(Xf I x\ufffd, ... , xU\nwhere x't is an abstract value of x;. We say that an abstrac tion of X;'s parents is cautious if P(Xf = x't I pa(Xi) = u) = P(Xf = x't I pa(X;) = u') for all values u of Pa(X;) that are mapped to the same partition. In this case, we define\nP(Xf = x't I pa(X;)a = ua) = P(Xf = va I pa(X;) = u) for u E ua.\nNote that the notion of an abstract value of a variable is naturally extended to a set of variables via the Cartesian product of the domains of the individual variables.\nAn abstracted Bayesian network Ba is a a (safe and cau tious) abstraction of a Bayesian networkS over X wrt ev\nidence e if P(eiS) = P(eiSa). It is also maximal if for any other Bayesian network sa ' with this property either\nVal(Xj') is finer than Val(Xf) or the two sets are incom parable for all variables X;.\n4 Finding Safe & Cautious Abstractions\nWe now describe a simple iterative algorithm, ValueAb stract, which finds a safe abstraction of a Bayesian network\nS with respect to evidence e. The algorithm consists of three phases:\nDiscard The algorithm starts by examining the variables in the network, and for each Xi, discarding all values that are incompatible with the evidence e. This is done via any arc-consistency algorithm as described in the C S P litera ture.\nAbstract In this phase the algorithm traverses the net work from the leafs upwards and computes cautious ab stractions for the parents of each abstracted variable. Since a variable Xi can be a parent of several variables, we need to collect the abstractions that are cautious with respect to each of these children. Thus, during this phase, the algo rithm maintains a set of abstractions, L:i, that contains the abstractions required for Xi by Xi's children. When the procedure processes X;, it finds the minimal refinement of all these abstractions of Xi. We denote by 1\\ L:i the tightest refinement of all the abstractions in I:;.\nConstruct Tables In the last phase the algorithm con structs the conditional probabilities in the abstracted net work.\nThe full algorithm is given in Figure 4.\nTheorem 4.1: The network sa returned by Value Abstract is a safe, cautious abstraction of S wrt e.\nIgnoring for the moment the cost of the Discard phase, we see that each iteration (either of Abstract, or Construct Tables phase) examines a single family. The cost of such an iteration can be exponential in the number of parents in the family. Thus, the running time of ValueAbstract is linear in the number of variables in the network, but exponential in the maximal indegree of the network. We stress, however, that for networks in which conditional probabilities are rep resented by tables, the running time is linear in the size of the network description (since the description of the condi tional probability tables are also exponential in the number of parents).\nThis implies that the running time of this algorithm is not sensitive to the topology of the network, and the complex ity of inference with it. VI TE S SE [11] (see Section 2) is a specialized version of ValueAbstract that yields impres sive speedup in likelihood calculations in a genetic analy sis domain. Thus, this simple algorithm can often make the difference between feasible and infeasible calculations.\n5 Message-Specific Abstraction The ValueAbstract algorithm has a desirable property: it is\nindependent of the particulars of the inference procedure we use for computing likelihoods. Thus, it can be applied as a preprocessing step before likelihood computation. The simplicity and low complexity of the algorithm make it at tractive.\nNonetheless, there are some regularities that are missed by ValueAbstract. First, the main processing is strictly bottom-up: the abstraction is constructed from the leaves\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 197\nof the network upward. However, we note that the first phase (Discard using edge-consistency) can propagate im plication of evidence to lower nodes.\nSecond, we considered each variable separately from the others. This limits us to abstractions that are the Carte sian products of the abstractions of variables in the par ents set. Thus, rather than holding abstractions per vari able, we may wish to hold abstractions for select groups of variables. Suppose, for example, that we have two binary variables X and Y each with values {0, 1 }. Suppose the evidence e is such that X and Y must have had the same values but e does not determine which one of their values. A maximal abstraction of Va l (X) x Val(Y) wrt e is the set { eq = { ( 0, 0), ( 1, 1)}, neq = { ( 0, 1), ( 1, 0)}}. There exist no maximal abstractions wrt e for Val(X) or for Va l (Y) which are strictly coarser than the original sets of values.\nFinally, another opportunity for improvement rests on the observation that rather than holding one abstraction per variable, we can hold several abstractions per variable, so that the likelihood computations of different parts of e can be treated more efficiently. Suppose, for example, that we have a Markov chain XI -+ X2 \u00b7 \u00b7 \u00b7 -+ Xn and that XI and Xn are observed. That is, the evidence e is composed of two parts ei and en. Then, for any Xi, 1 < i < n, we can think of two natural abstractions, one is a maximal abstrac tion wrt ei, and the other is a maximal abstraction wrt en. To compute the posterior P (Xi I e) one would need the tight refinement of both abstractions. However, to pass messages to its neighbors ala Pearl propagation style, we only need to use one of the abstractions, which in general are coarser than their tight refinement, and thus more efficient.\nTo deal with these issues, we need to develop abstractions that depend on the details of the of the inference procedure we use. We now address these issues within the context of cluster-tree (aka clique tree) algorithms [7, 10, 14]. We start with a presentation of one variant of tree-based algo rithms. The other variants have slightly different details, but our algorithm can be easily adopted to deal with these.\n5.1 Clique Tree Propagation Algorithm Assume that B is a fixed network. A cluster-tree forB is a\ntree over k nodes such that:\n\u2022 Each node l in the tree is annotated with a cluster Ct \ufffd {XI, ... ,Xn}\u00b7 \u2022 Each variable xi is assigned to one cluster cl(X;) such that Xi E Ct(X;) and Pa(Xi) \ufffd Ct(X;)\u00b7 \u2022 If Xi E Ct and Xi E Cm, then Xi E Cj for any node jon the path from l tom.\nLet l be a node. By definition, if Xi is assigned to l, then Xi and Pa(Xi) are subsets of Ct. Thus, we can define a\nfunction on values Ct E Val(Ct)\n9t(ct) = II P(xi I Pa(Xi)) i,l(X;)=l (If l is a node that is not assigned any variable, then we\ndefine 9t(ct) = 1.)\nIt is easy to see that by simple rearrangement of products the probability P(xi, ... , Xn I B) can be rewritten as:\nIf we have evidence, say on a set of variables E, we can up date the functions to reflect that. For example, if XI = a. We can multiple 9l(XI) by a function oi(XI) such that oi (xi ) = 1 if XI = a and oi (xi) = 0 otherwise. If we update the nodes in this manner for all variables in E according to the specific evidence e then\nTo see this, note that if XI, ... , Xn is consistent withe, then its value is not changed by the modification to the g1\u2022 On the other hand, if it is not consistent, then one of the 91 is 0, and thus, the probability of the joint assignment is 0.\nLet l and m be adjacent nodes in the tree. We define the separator St,m = Ct nCm. A separator defines a partitions of the clusters { CI, ... , Ck} into two sets: the clusters on the l-side of the separator, and the clusters on the m-side of the separator. We denote these sets A:\u00b7m and A\ufffdm. In addition, we define the sets of variables in both groups of clusters x:\u00b7m = uA:\u00b7m, and x\ufffdm = UA\ufffdm\nThe key property of separators is that they allow us to factor the computation of probabilities into two separate cases. Using the properties of the tree, it is easy to show that\nProposition 5.1:\nP(e I B) XI Xn l\nL f;;,m(sl,m, e)ff'm(sl,m, e) s1,\ufffd E Val(S,,\ufffd)\nwhere\n2::: II\n2::: II The key property of this factorization is that it is recur\nsive.\nProposition 5.2: Consider a node l whose adjacent nodes are mi, . .. , mk. Then\nfl,m1( ) l St,m,,e = L 9t(ct) II f;;,;'i(sl,mi,e). yEVal(C,-5,,\ufffd1) j>I\n(1)\nThus, to compute the message ff'm1 (st,m,, e) we need to combine the messages from the other clusters adjacent to l with conditional probabilities that are assigned to l and then sum out all of the variables except these on St,m,.\n198 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nUsing this recursive rule we can compute the likelihood P(e I B). We choose a separator S1,m\u00b7 According to Proposition 5.1, all we need do is to compute the messages ff'm and f\ufffdm and then sum over the values of variables in Sl,m\u00b7 To compute these two messages, we apply the recur sion rule of Proposition 5.2 until we get to the leafs of the tree. It is easy to see that this procedure is closely related to variable elimination algorithms [3, 4, 16], except that we eliminated several variables at each step. Moreover, the structure of the clique tree determines the order of elimina tion.\nIn addition to likelihood computations, we can also com pute the posterior for every cluster: P(e, c1 / B). We do so by combining the messages from all of l's adjacent nodes:\nP(e,cl I B)= g1(c1) IT!\ufffd',\"i(sl,mi'e) j\nCluster tree algorithm compute such a posterior for each cluster. This can be done efficiently by dynamic program ming: for each separator we only need to compute two mes sages. By appropriate use of dynamic programming all of these messages can be computed in two passes over the tree [7, 14].\n5.2 Clique Tree Abstractions Suppose we are given a cluster tree and an evidence e. Can\nwe abstract the values of cliques and separators? The ideas from the previous section can be applied here in a straight forward fashion. Let S1,m be a separator. An abstraction af'm of Val(SL,m) is safe for ff'm(st,m, e) if\nfl,m( ) Jl,m( 1 ) l S[,m, e = l sl,m\u2022 e\nfor all S!,m and sLm such that af' m(sl,m) = af'm(sLm).\nTo construct a safe abstraction for the message ff'm, we examine the recursive definition given Proposition 5.2. This definition implies that ff'm is a function of gl and !\ufffd':'' for nodes m' adjacent to l. Thus, if we have safe abstractions to all these terms, we only need to preserves values of\nL gl(c't). II f\ufffd',\"i(s't,mi'e) yEC1-S1,m1 j>l\nWe construct these abstraction using a dynamic program ming procedure that is analogous to the clique-tree propa gation algorithm. The difference is that instead of propa gating probabilistic messages we are propagating abstrac tions. We define two operations on abstractions that are the analogs of message multiplication and of marginalization.\nWe start by combination of abstractions.\nDefinition 5.3: Suppose that ax, ay are abstractions that are safe with respect to f(X) and g(Y), respectively. (The sets X andY can overlap.) Let Z = XUY. The combined abstraction a = ax \u00b7 ay over Val(Z) is such that a(z) = a(z') whenax(x) = ax(x') anday(y) = ay(y' ), where\nx and y are the values of X and Y specified by z (and similarly for x' andy'). I It is easy to check that if ax and ay are safe for /(X) and g(Y), then ax\u00b7 ay is safe for f(X)g(Y). The second operation we need to examine is marginaliza tion. Let f(X, Y) be a factor. We want to find an abstrac tion that is safe with respect to g(y) = Ex f(x, y). To do so, we need to identify values y for which we are going to add the same values in the same order.\nDefinition 5.4 : Suppose that a is an abstraction of Val (X, Y) that is safe with respect to f(X, Y). We de\nfine the abstraction a -!-Y over Val(Y) so that aty (y) =aty (y')if\na(x, y) = a(x, y') for all x I\nGiven these two operations, we can define the abstraction algorithm for clique-trees. We start by computing an ab straction a1 of g1 ( c1) for each clique l. This can be done either by combining abstractions for the conditional dis tributions of variables that are assigned to l or by first con structing g1 () and then finding the coarsest safe abstraction. The first option can introduce unnecessary distinctions, but can be more efficient.\nNext, we define the analog of the recursive rule of Propo sition 5.2. Consider a node l whose adjacent nodes are m1, ... , mk. Then,\nal,mt = (a . al,m2 .... al,mk) I I I m2 mk +SI,mt To construct the abstraction we perform dynamic pro gramming that determines the abstraction for each mes sage in terms of the abstractions for neighboring separators. This dynamic programming is analogous to the propaga tion of messages in the probabilistic inference algorithm on clique-trees.\nOnce we compute the abstraction of the messages we can perform inference. The key saving of the abstraction is that in computation we perform multiplication and addition once for every abstract value. Thus, if sa is the abstracted version of S, the saving in computation in construction of the message on Sis I ValSal/1 ValSI.\nWe can show that the resulting algorithm preserves cor rectness of inferences.\nTheorem 5.5: Inference on the abstracted clique tree com putes exactly all queries of the form P( C1 I e) for the evi dence e specified at the construction of the abstraction.\nWe note that the cost of the construction of the clique-tree abstractions depends on the cost of the basic operations. In the most naive instantiation, we represent abstractions as tables. In this case, the cost of the operations is exactly the same as the cost of probabilistic computation on the clique tree.\n6 Abstractions and 0 values Our algorithm can be easily extended to exploit an addi\ntional \"structural\" feature in conditional probability distri-\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 199\n10000 1e+OB .. \u00b7\u2022\u00b7 \u2022 18+07\n... / '/ ... / 1e+06 \u2022\u2022\u2022 io 1e+07 \u00b7. fl ::: ::: \"' Ui Ui ::: 100000 -.!' 1000 .. \ufffd 1e+06 Ui \ufffd 0 \ufffd C\" \" \"'\u00b7\u00b7 \" z .., \" 1l 100 \ufffd 1h .0 ...\n10 10\nfl/ ... . \ufffd\nd.\u2022' o oo\n100 1000\n(3 .., \ufffd \ufffd 1h .0 ...\n10000\n100000\n10000\n1000\n100\n/1/. 0 /. \u2022\u00a7.\ufffd. ,t' \u2022 \u2022 \"\n/\ufffd \u00b7.t\u00b7 \u2022 \u2022 0 ... 0\n....... 0 10 .\n100 1e+4 1e+6\n\" 10000 C\" (3\n\u00b7\ufffdi: \ufffd\nIt 0 1000 0 8.\ufffd\ufffd\u00b7\u00b7 \ufffd a: 100 \ufffd\u00b7.\n10 \u00b7\"\"'\ufffd\ufffd'/> \u2022 1 . -:\"\"' \u00b7.\n1e+8 1e+10 1 10 100 1000 10000 Original Network Size Original Clique Size # individuals * genotype size\n(a) (b) (c)\nFigure 5: Display of the saving achieved by Value Abstract on 280 linkage analysis networks. Each point corresponds to a network. Graph (a) shows reduction in network size (x-axis is original network size andy-axis is reduced network size); Graph (b) shows reduction in clique tree size (x-axis is original clique tree size andy-axis is the size of the clique tree of the abstracted network); and graph (c) relates the ratio of reduction in clique tree size (y-axis) the a complexity estimate of the linkage analysis problem (x-axis).\nbutions. If at some stage in the algorithm ff'm(st,m) = 0, then multiplications by this value will always result in 0.\nWe can record this fact in our abstractions by introducing a special abstract value 0 that corresponds to all the values of the variables that are given a value 0. Then, we modify the definition of combination and marginalization to take the special properties of 0 into account:\n\u2022 (ax\u00b7 ay )(x) = 0 if either ax(x) = 0 or ay(y) = 0. \u2022 a -l..x (y) = 0 if a(x, y) = 0 for all x.\nThese modifications allow us to deal with evidence more easily. Suppose that a variable X E Ct is assigned the value x in the evidence. Then we combine at() with an abstraction ax such that all values x' E Val(X) - { x} are assigned to the abstract value 0, and x is assigned to the singleton set { x}. When we combine this abstraction with the clique abstraction we ensure that all assignments to Ct in which X f. x are assigned to 0.\nWe note that this simple modification of our procedure essentially implements partial constraint satisfication prop agation to discover unattainable joint assignments to clus ters/separators.\n7 Evaluation\nWe tested our methods on a collection of standard bench mark pedigrees that are reported in [1, 13]. These pedi grees come from 10 different studies and contain 90 dif ferent pedigrees of sizes varying from 5 to 200 individu als. From these we generated 280 different linkage analy sis problems by including different numbers of loci in the analysis. These were translated into a Bayesian network of the form described in Section 2.1. For each network we also constructed evidence assignment based on the original findings in the studies and used these in the analysis below.\nIn the first phase of our experiments we tested the Value Abstract procedure. This procedure implements the ideas\nof V ITE S SE combined with constraint propagation to re move impossible values. Figure 5 (a) shows the reduction\nabstraction.\nin the size of the network achieved by Value Abstract. This reduction is due to eliminating and combining values of variables. The reduction in the network size can be ap proximated as 2.6 \u00b7 n\u00b0\u00b768 (the line in Figure 5 (a)).\nHowever, since the computation time depends linearly on the size of the clique tree constructed from the network, we also want to measure the reduction in this size. This is shown in Figure 5 (b). As we can see, the ratio of reduc tion can vary significantly. We believe that this is due to structural features of the pedigree. Figure 5 (c) shows that the ratio of improvement in the clique tree size is roughly proportional to the product of the number of individuals in the pedigree and the number of genotype values for each individual. This later quantity is a rough estimate of the complexity of the problem.\nIn the next stage we applied the clique tree abstract proce dure described in Section 5. Here we measured the reduc tion in effective size of the clique tree due to the abstraction of values in cliques and separators. Figure 6 compares the sizes before and after we applied this procedure to the net work returned by Value Abstract. As we can see, we get ad-\n200 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nditional saving, especially for networks with large cliques. The reduction is estimated as 5.2\u00b7n\u00b0\u00b764 (the line in Figure 6. As we can see, the savings can be drastic.\n8 Concluding Remarks In this paper we introduced an approach to exploit regular\nstructure in Bayesian networks to reduce computation time. This approach exploits symmetry to merge values of vari ables or groups of variables at different stages of the com putation. Our motivation is from linkage analysis, where this type of heuristics have been very successful [ 11]. We are currently extending our implementation to deal with larger networks and plan to incorporate our methods within a linkage analysis software.\nIt is clear that this approach can be beneficial to other forms of structured Bayesian networks. In particular, net works with C S I [2]. Value abstraction suggests a general framework within which we can evaluate the utility of al gorithms that work with tree C P Ts. A structured represen tation of message (i.e., [ 17]) is essentially an abstraction. If an algorithm is exact, then the representation it uses must be a refinement of the abstraction our algorithm constructs. We plan to exploit this to design \"optimal\" structure repre sentations for message passing with C S I.\nAcknowledgements\nWe thank Tal El- Hay for his help in implementing the Clique tree construction algorithm and Ann Becker and Gal\nElidan for help with the benchmark pedigrees. This work was supported by Isreal Science Foundation grant number 224/ 9 9-1 and by the generosity of the Michael Sacher fund. Nir Friedman was also supported by Harry & Abe Sherman Senior Lectureship in Computer Science. Experiments re ported here were run on equipment funded by an I SF Basic Equipment Grant.\nReferences\n[1] A. Becker, D. Geiger, and A. A. Schaffer. Automatic selection of loop breakers for genetic linkage analy sis. Human Heredity, 48:4 9-60, 1 9 98. [2] C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. Context-specific independence in Bayesian\nnetworks. In UAI 1 9 96. [3] C. Cannings, E. A. Thompson, and M. H. Skolnick.\nProbability functions on complex pedigrees. Ad vances in Applied Probability, 10:26-61, 1 978. [4] R. Dechter. Bucket elimination: A unifying frame work for probabilistic inference. In UAI 1 9 96. [ 5] R. C. Elston and J. Stewart. A general model for the analysis of pedigree data. Human Heredity, 21: 523- 542, 1 971. [ 6] C. Harbron and A. Thomas. Alternative graphical rep resentations of genotypes in a pedigree. IMA Jour nal of Mathematics Applied in Medicine and Biology, 11:217-228, 1 9 94. [7] F. V. Jensen, S. L. Lauritzen, and K. G. Olesen. Bayesian updating in causal probabilistic networks by\nlocal computations. Computational Statistics Quar terly, 5 (4):26 9-282, 1 9 90. [8] A. Kong. Efficient methods for computing linkage likelihoods of recessive diseases in inbred pedigrees. Genet Epidem, 8:81-103, 1 9 91. [ 9] E. S. Lander and P. Green. Construction of multilocus genetic maps in humans. Proc. National Academy of Science, 84:2363-2367, 1 987. [10] S. L. Lauritzen and D. J. Spiegelhalter. Local compu tations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society, B 50 (2): 1 57-224, 1 988. [11] J. R. O' Connell and D. E. Weeks. The V I TE S SE algo rithm for rapid exact multilocus linkage analysis via genotype set-recording and fuzzy inheritance. Nat. Genet. , 11:402-408, 1 9 9 5. [12] J. Ott. Analysis of Human Genetic Linkage. 1 9 91. [13] A.A. Schaffer. Faster linkage analysis computations\nfor pedigrees with loops or unused alleles. Human Heredity, 46:226-23 5, 1 9 96. [14] G. Shafer and P. Shenoy. Probability propagation. Ann. Math. and Art. Int. , 2:327-3 52, 1 9 90. [1 5] M. P. Wellman and C.- L. Liu. State-space abstraction for anytime evaluation of probabilistic networks. In UAI 1 9 94. [16] N.L. Zhang and D. Poole. Exploiting causal indepen dence in bayesian network inference. Journal of A./. Research, 5:301-328, 1 9 96. [17] N.L. Zhang and D. Poole. On the role of context specific independence in probabilistic inference. In IJCAI. 1 9 9 9."}], "references": [{"title": "Bucket elimination: A unifying frame\u00ad work for probabilistic inference", "author": ["R. Dechter"], "venue": "UAI 1 9 96. [ 5] R. C. Elston and J. Stewart. A general model for the analysis of pedigree data. Human Heredity, 21: 523- 542, 1 971. [  6] C. Harbron and A. Thomas. Alternative graphical rep\u00ad resentations of genotypes in a pedigree. IMA Jour\u00ad nal of Mathematics Applied in Medicine and Biology, 11:217-228, 1 9 94. [7] F. V. Jensen, S. L. Lauritzen, and K. G. Olesen. Bayesian updating in causal probabilistic networks by  local computations. Computational Statistics Quar\u00ad terly, 5 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2367}], "referenceMentions": [{"referenceID": 0, "context": "It is easy to see that this procedure is closely related to variable elimination algorithms [3, 4, 16], except that", "startOffset": 92, "endOffset": 102}], "year": 2011, "abstractText": "In this paper, we use evidence-specific value ab\u00ad straction for speeding Bayesian networks infer\u00ad ence. This is done by grouping variable val\u00ad ues and treating the combined values as a sin\u00ad gle entity. As we show, such abstractions can ex\u00ad ploit regularities in conditional probability distri\u00ad butions and also the specific values of observed variables. To formally justify value abstraction, we defi ne the notion of safe value abstraction and devise inference algorithms that use it to re\u00ad duce the cost of inference. Our procedure is par\u00ad ticularly useful for learning complex networks with many hidden variables. In such cases, re\u00ad peated likelihood computations are required for E M or other parameter optimization techniques. Since these computations are repeated with re\u00ad spect to the same evidence set, our methods can provide signifi cant speedup to the learning pro\u00ad cedure. We demonstrate the algorithm on genetic linkage problems where the use of value abstrac\u00ad tion sometimes differentiates between a feasible and non-feasible solution.", "creator": "pdftk 1.41 - www.pdftk.com"}}}