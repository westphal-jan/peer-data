{"id": "1302.2465", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2013", "title": "RIO: Minimizing User Interaction in Debugging of Knowledge Bases", "abstract": "The best. \" software debugging systems are meant well coding - obtain 1998 terms of lying parameter 2001 order to improve kept improving. However, misleading gio information no same year a dramatic decrease according in both and meant accurate considered never possible an - posteriori. Consequently, except come own one effect fault is though, about though might some risk of serotyping interactions. In this focused be far a reinforcement learning strategy they absorbed adeptly similar merely rarely on another performance achieved once deviations the risk of type normal - quality rcw02 press. Therefore, this technique becomes highly four formal catastrophic where reliable prior thought 4,000 but difficult well obtain. Using heterogeneous looks - 's describe bases, because presenting never the new interactive query strategy is scalable, such terrific reaction come, and confoundingly and incompressible - which and no - risk strategies same 2.3 w. r. s. required limit present using explore.", "histories": [["v1", "Mon, 11 Feb 2013 12:53:47 GMT  (645kb,D)", "https://arxiv.org/abs/1302.2465v1", "arXiv admin note: substantial text overlap witharXiv:1209.3734"], ["v2", "Wed, 6 Mar 2013 14:46:03 GMT  (643kb,D)", "http://arxiv.org/abs/1302.2465v2", "arXiv admin note: substantial text overlap witharXiv:1209.3734"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1209.3734", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["patrick rodler", "kostyantyn shchekotykhin", "philipp fleiss", "gerhard friedrich"], "accepted": false, "id": "1302.2465"}, "pdf": {"name": "1302.2465.pdf", "metadata": {"source": "CRF", "title": "RIO: Minimizing User Interaction in Debugging of Knowledge Bases", "authors": ["Patrick Rodler", "Gerhard Friedrich"], "emails": ["firstname.lastname@aau.at"], "sections": [{"heading": "1 Introduction", "text": "Efficient debugging is a prerequisite for successful evolution, maintenance and application of knowledge-based systems. In a standard application scenario a debugger deals with a faulty knowledge base (KB) O which fails to meet predefined quality criteria R such as consistency. The task of debugging aims at modifying O in that a (subset-)minimal set of axioms D \u2286 O, termed diagnosis, is deleted in order to restore compliance of the KB with R, whereas a set of axioms EXD is inserted toO to preserve designated entailments which might have been broken by the removal of D. Usually, a large number of competing diagnoses exist for a faulty O. Without additional information, there is no means to decide which D to prefer. In many practical scenarios, however, there is some kind of meta information available, for example in terms of (1) logs of prior debugging sessions, (2) common faults or fault patterns occurring in logical formulas, or (3) a subjective guess of the involved user based on their experience. Given such data, one can extract a-priori fault probabilities and use them to guide the search for diagnoses. For example, one could use a uniform cost strategy to find the most probable diagnosis w.r.t. fault probabilities, see e.g. [Kalyanpur, 2006].\nHowever, only in the best case, if the fault probabilities are perfectly adjusted for the particular case, this will lead the search to the desired diagnosis the deletion of which enables to formulate a KB compliant with the requirements defined by the user.\nInteractive debugging systems such as [Shchekotykhin et al., 2012; Siddiqi and Huang, 2011] tackle this issue by letting an oracle take action during the debugging session by answering queries. In case of KBs a debugger asks about entailments and non-entailments of the desired Ot, called test cases [Shchekotykhin et al., 2012]. These pose constraints to the validity of diagnoses and thus help to sort out incompliant diagnoses and update the probabilities of remaining ones step-by-step. However, often a debugger can find many alternative queries for a set of diagnoses. Selection of the \u201cbest\u201d query, an answer to which allows to obtain maximum information, is very important since it affects the total number of queries required to localize the fault. In their seminal work [de Kleer and Williams, 1987] proposed two query selection strategies: split-in-half and entropy-based. The latter strategy can make optimal profit from exploiting properly adjusted initial fault probabilities, whereas it can completely fail in the case of weak prior information. The split-in-half manifests constant behavior independently of the probabilities given, but lacks the ability to leverage appropriate fault information. Selection of the best strategy is problematic, since one has to decide about the quality of the prior fault probabilities without knowing the desired solution. Our evaluation shows that selection of an inappropriate strategy can result in a substantial increase of more than 2000% w.r.t. number of queries.\nThe contribution of this paper is a new RIsk Optimization reinforcement learning method (RIO). Compared to existing strategies RIO allows to minimize user interaction in the average case for any quality of meta information. By virtue of its learning capability, our approach is optimally suited for debugging of KBs where only vague or no meta information is available. Moreover, RIO uses the acquired information to adapt its learning strategy. On the one hand, our method takes advantage of the given meta information as long as good performance is achieved. On the other hand, it gradually gets more independent of meta information if suboptimal behavior is measured. Experiments on two datasets of faulty ontologies show the feasibility, efficiency and scalability of RIO. The evaluation will indicate that, on average, RIO is the best choice of strategy for both good and bad meta information\nar X\niv :1\n30 2.\n24 65\nv2 [\ncs .A\nI] 6\nM ar\n2 01\n3\nwith savings as to user interaction of up to 80%. Technical preliminaries are provided in Section 2. Section 3 explains the suggested approach and gives implementation details. Evaluation results are described in Section 4. Section 5 concludes."}, {"heading": "2 Preliminaries", "text": "In order to make the paper self-contained we provide a short introduction to description logic (DL), which is a knowledge representation and reasoning system (KRS) used in the paper. Of course, the approach suggested in this work is not limited to DL and can be applied to any KRS for which there is a sound and complete reasoning method and the entailment relation is extensive, monotone and idempotent.\nDescription logic [Baader et al., 2003] is a family of knowledge representation languages with a formal logicbased semantics that are designed to represent knowledge about a domain in form of concept descriptions. The syntax of a language L is defined by its signature (vocabulary) and a set of constructors. A signature in this case corresponds to a (disjoint) union of sets NC , NR and NI , where NC contains all concept names (unary predicates), NR comprises all role names (binary predicates) and NI is a set of individuals (constants). Each concept and role description can be either atomic or complex. The latter ones are composed using constructors defined in the particular language L. A typical set of DL constructors includes conjunction A u B, disjunction AtB, negation \u00acA, existential \u2203r.A and value \u2200r.A restrictions, where A,B \u2208 NC and r \u2208 NR.\nA DL ontology O is defined as a tuple (T ,A), where T (TBox) is a set of terminological axioms and A (ABox) a set of assertional axioms. Each TBox axiom is expressed by a general concept inclusion A v C, a form of logical implication, or by a definition A \u2261 C, a kind of logical equivalence, where C is an atomic or complex concept. ABox axioms are used to assert properties of individuals in terms of the vocabulary defined in TBox, e.g. concept A(x) or role r(x, y) assertions, where x, y \u2208 NI .\nThe semantics of DLs is given in terms of interpretations I = (\u2206I , \u00b7I) consisting of a non-empty domain \u2206I and a function \u00b7I that maps each concept to a subset of \u2206I , each role to a subset of \u2206I\u00d7\u2206I and each individual to some value in \u2206I . An interpretation I is a model of O iff it satisfies all TBox and ABox axioms. O is unsatisfiable iff it has no model. A concept A (role r) is satisfiable w.r.tO iff there is a model I of O with AI 6= \u2205 (rI 6= \u2205). A TBox is incoherent iff there exists an unsatisfiable concept or role.\nUsually description logic systems provide sound and complete reasoning services to their users. In addition to verification of coherence and consistency of O, the reasoners also perform classification and realization. Classification is a subsumption algorithm that determines most specific (general) concepts that subsume (are subsumed by) a certain concept. Realization computes for each individual x a set of most specific concepts {C1, . . . , Cn} such that O |= Ci(x) for all i = 1, . . . , n. Note, when we speak of entailments below, we address (only) the output computed by the classification and realization services of a DL-reasoner.\nOntology debugging, given an ontologyO, aims at approximating the so-called target ontology Ot by O\u2217, where Ot\nis some correct and complete ontology that satisfies all requirements to the knowledge-based application it is used for. O\u2217 must satisfy all explicitly stated requirements and is thus termed complying ontology. It results from modifications to O in terms of (1) deleting axioms D and (2) inserting axioms EXD. We call D = O \\ O\u2217 a diagnosis. Definition 1 (Complying Ontology, Diagnosis Problem) Let O be an ontology, B a background KB, R a set of requirements to O, P and N respectively a set of positive and negative test cases, where each test case p \u2208 P and n \u2208 N is a set of axioms. Then an ontology O\u2217 is called complying ontology iff all the following conditions hold:\n\u2200 r \u2208 R : O\u2217 \u222a B fulfills r (1) \u2200 p \u2208 P : O\u2217 \u222a B |= p (2) \u2200n \u2208 N : O\u2217 \u222a B 6|= n (3)\nThe tuple \u3008O,B,P ,N \u3009R defines a diagnosis problem instance (DPI). Often R := {coherence, consistency} is assumed. Definition 2 (Diagnosis) D \u2286 O is called diagnosis for a DPI \u3008O,B,P ,N \u3009R iff there is a set of axioms EXD such that (O \\D)\u222aEXD is a complying ontology. A diagnosis D assumes that all axi \u2208 D are faulty and all axj \u2208 O \\ D are correct. A diagnosis D is minimal iff there is no D\u2032 \u2282 D s.t. D\u2032 is a diagnosis. MD denotes the set of minimal diagnoses of a DPI.\nNote that MD is usually used to approximate the set of all diagnoses of a DPI. The identification of EXD, accomplished e.g. by some learning approach, is a crucial part of the ontology repair process. However, the complete formulation of EXD is outside the scope of this work where we focus on computing diagnoses. As suggested in [Shchekotykhin et al., 2012], we approximate EXD by the set \u22c3 p\u2208P p. Given a\nDPI \u3008O,B,P ,N \u3009R, if the set of axioms O \u222a \u22c3\np\u2208P p is not a complying ontology then there is no diagnosis D = \u2205, i.e. some axioms in O must be modified. Example 1: Consider O := O1 \u222a O2 \u222aM12 with TBox T :\nO1 ax1 : PhD v Researcher ax2 : Researcher v DeptEmployee\nO2 ax3 : PhDStudent v Student ax4 : Student v \u00acDeptMember\nM12 ax5 : PhDStudent v PhD ax6 : DeptEmployee v DeptMember\nand ABox A = {PhDStudent(s)}, where M12 is an automatically generated set of semantic links between O1 and O2. The given ontology O is inconsistent since it describes s as both a department member and not. Let the DPI be defined as \u3008T ,A, \u2205, \u2205\u3009{coherence}, whereA is correct and thus added to the background theory and both sets P and N are empty. For this DPI MD = {D1 : [ax 1],D2 : [ax 2],D3 : [ax 3],D4 : [ax 4],D5 : [ax 5],D6 : [ax 6]}. To compute MD we employ a combination of HS-Tree [Reiter, 1987] and QuickXPlain [Junker, 2004] algorithms as suggested by [Friedrich and Shchekotykhin, 2005].\nInteractive ontology debugging iteratively incorporates a user\u2019s knowledge about Ot, thereby differentiating between diagnoses in MD. The overall procedure is as follows:\n(1) Compute a set of at most n leading diagnoses D \u2286MD that serve as an approximation of all minimal diagnoses MD. Restricting the computation of MD to a predefined number n helps to overcome exponential explosion of HS-Tree. Preference criteria such as most probable or minimum cardinality diagnoses are used to specify D within MD. (2) Exploit D to compute/select a query which is posed to the user. (3) Incorporate the user\u2019s answer to prune the search space for diagnoses. Go to (1) until a predefined stop criterion is met by a D\u2217 \u2208 D, e.g. D\u2217 has overwhelming probability. We call the priorly unknown diagnosis that will meet the stop criterion target diagnosis D\u2217. As a means for interaction with the user we utilize the notion of a query which means asking the user (Ot |= Xj?), i.e. to classify whether a given set of axiomsXj should be entailed (assigned to P ) or not entailed (assigned to N ) by Ot. The theoretical foundation for the application of queries is the fact thatO\\Di andO\\Dj forDi 6= Dj \u2208 D entail different sets of axioms. Definition 3 (Query, Partition) Let O\u2217i := (O \\ Di) \u222a B \u222a ( \u22c3\np\u2208P p) where Di \u2208 D. A set of axioms Xj is called a query iff DPj := {Di \u2208 D | O\u2217i |= Xj} 6= \u2205 and DNj := {Di \u2208 D | O\u2217i |= \u00acXj} 6= \u2205. The partition of query Xj is denoted by \u3008DPj ,DNj ,D\u2205j \u3009 where D\u2205j = D \\ (DPj \u222aDNj ). XD terms the set of all queries and associated partitions w.r.t. D.\nThe (complete) set XD can be generated as shown in Algorithm 1. In each iteration, given a set of diagnoses DPk \u2282 D, common entailments Xk := { e | \u2200Di \u2208 DPk : O\u2217i |= e } are computed (function GETENTAILMENTS) and used to classify the remaining diagnoses in D \\ DPk to obtain the partition \u3008DPk ,DNk ,D\u2205k\u3009 associated with Xk. Then, together with its partition, Xk is added to XD. The function INCONSIST(arg) returns true if arg is inconsistent or incoherent.\nLet the answering of queries by a user be modeled as function u : XD \u2192 {t, f}. If uj := u(Xj) = t, then P \u2190 P \u222a {Xj} and D \u2190 D \\DNj . Otherwise, N \u2190 N \u222a {Xj} and D \u2190 D \\DPj . Prospectively, according to Definition 2, only those diagnoses are considered in the set D that comply with the new DPI obtained by the addition of a test case. This allows us to formalize the problem we address in this work: Problem Definition (Diagnosis Discrimination) Given D w.r.t. \u3008O,B,P ,N \u3009R, a stop criterion stop : D \u2192 {t, f} and a user u, find a next query Xj \u2208 XD such that (1) (Xj , . . . , Xq) is a sequence of minimal length and (2) after X \u2208 {Xj , . . . , Xq} are added to P and N according to {uj , . . . , uq}, there exists a D\u2217 \u2208 D such that stop(D\u2217) = t.\nTwo strategies for selecting the \u201cbest\u201d next query have been proposed [de Kleer and Williams, 1987] and adapted to debugging of KBs by [Shchekotykhin et al., 2012]. Split-inhalf strategy (SPL), selects the query Xj \u2208 XD which minimizes the scoring function scsplit(Xj) :=\n\u2223\u2223|DPj | \u2212 |DNj |\u2223\u2223+ |D\u2205j |. So, SPL prefers queries which eliminate half of the diagnoses independently of the query outcome. Entropybased strategy (ENT) uses information about prior probabilities pt for the user to make a mistake when using a syntactical construct of type t \u2208 CT (L), where CT (L) is the set of constructors available in the used knowledge representa-\nAlgorithm 1: Query Generation Input: DPI \u3008O,B,P,N \u3009R , set of corresponding diagnoses D Output: a set of queries and associated partitions XD\n1 foreach DPk \u2282 D do 2 Xk \u2190 getEntailments(O,B,P,DPk ); 3 ifXk 6= \u2205 then 4 foreachDr \u2208 D \\DPk do 5 if O\u2217r |= Xk then D P k \u2190 D P k \u222a {Dr}; 6 else if inconsist(O\u2217r \u222aXk) then D N k \u2190 D N k \u222a {Dr}; 7 else D\u2205k \u2190 D \u2205 k \u222a {Dr};\n8 XD \u2190 XD \u222a \u2329 Xk, \u2329 DPk ,D N k ,D \u2205 k \u232a\u232a 9 return XD;\ntion language L, e.g. {\u2200,\u2203,v,\u00ac,t,u} \u2282 CT (OWL) [Grau et al., 2008]. These fault probabilities pt are assumed to be independent and used to calculate fault probabilities of axioms axk as p(axk) = 1 \u2212 \u220f t\u2208CT (1 \u2212 pt)n(t) where n(t) is the number of occurrences of construct type t in axk. The probabilities of axioms can in turn be used to determine fault probabilities of diagnoses Di \u2208 D as\np(Di) = \u220f\naxr\u2208Di\np(ax r) \u220f\naxs\u2208O\\Di\n(1\u2212 p(ax s)) (4)\nENT selects the query Xj \u2208 XD with highest expected information gain, i.e. which minimizes scent(Xj) defined as:\u2211 a\u2208{t,f} p(uj = a) \u2211 Dk\u2208D \u2212p(Dk|uj = a) log2 p(Dk|uj = a)\nwhere p(uj = t) = \u2211 Dr\u2208DPj p(Dr) + 12p(D \u2205 j ) , p(D \u2205 j ) =\u2211\nDr\u2208D\u2205j p(Dr) and p(uj = f) = 1 \u2212 p(uj = t). The answer uj = a is used to update probabilities p(Dk) according to the Bayesian formula, yielding p(Dk|uj = a). The result of the evaluation in [Shchekotykhin et al., 2012] shows that ENT reveals better performance than SPL in most of the cases. However, SPL proved to be the best strategy in situations when misleading prior information is provided, i.e. the target diagnosis D\u2217 has low probability. So, one can regard ENT as a high risk strategy with high potential to perform well, depending on the priorly unknown quality of the given fault information. SPL, in contrast, can be seen as a no-risk strategy without any potential to leverage good meta information. Therefore, selection of the proper combination of prior probabilities {pt | t \u2208 CT (L)} and query selection strategy is crucial for successful diagnosis discrimination and minimization of user interaction."}, {"heading": "3 Risk Optimization for Query Selection", "text": "The proposed Risk Optimization Algorithm (RIO) extends ENT strategy with a dynamic learning procedure that learns by reinforcement how to select optimal queries. The behavior is determined by the achieved performance in terms of diagnosis elimination rate. Good performance means similar behavior to ENT, whereas aggravation of performance leads to a gradual neglect of the given meta information. Like ENT, RIO continually improves the prior fault probabilities based on new knowledge obtained through queries to a user.\nRIO learns a \u201ccautiousness\u201d parameter c whose admissible values are captured by the user-defined interval [c, c]. The relationship between c and queries is as follows: Definition 4 (Cautiousness of a Query) We define the cautiousness cq(Xi) of a query Xi as follows:\ncq(Xi) := min\n{ |DPi |, |DNi | } |D| \u2208 0, \u230a |D| 2 \u230b |D|  =: [cq, cq] A query Xi is called braver than query Xj iff cq(Xi) < cq(Xj). Otherwise Xi is called more cautious than Xj . A query with maximum cautiousness cq is called no-risk query.\nDefinition 5 (Elimination Rate) Given a query Xi and the corresponding answer ui \u2208 {t, f}, the elimination rate e(Xi, ui) = |DNi | |D| if ui = t and e(Xi, ui) = |DPi | |D| if ui = f . The answer ui to a query Xi is called favorable iff it maximizes the elimination rate e(Xi, ui). Otherwise ui is called unfavorable. The minimal or worst case elimination rate minui\u2208{t,f}(e(Xi, ui)) of Xi is denoted by ewc(Xi).\nSo, the cautiousness cq(Xi) of a queryXi is exactly the worst case elimination rate, i.e. cq(Xi) = ewc(Xi) = e(Xi, ui) given that ui is the unfavorable query result. Intuitively, parameter c characterizes the minimum proportion of diagnoses in D which should be eliminated by the successive query. Definition 6 (High-Risk Query) Given a queryXi and cautiousness c, Xi is called a high-risk query iff cq(Xi) < c, i.e. the cautiousness of the query is lower than the algorithm\u2019s current cautiousness value c. Otherwise, Xi is called nonhigh-risk query. By NHRc(XD) \u2286 XD we denote the set of all non-high-risk queries w.r.t. c. For given cautiousness c, the set of all queries XD can be partitioned in high-risk queries and non-high-risk queries. Example 2 (cont. Example 1): Let the user specify c := 0.3 for the set D with |D| = 6. Given these settings, X1 := {DeptEmployee(s), Student(s)} is a non-high-risk query since its partition \u3008DP1 ,DN1 ,D\u22051\u3009 = \u3008{D4,D6} , {D1,D2,D3,D5} , \u2205\u3009 and thus its cautiousness cq(X1) = 2/6 \u2265 0.3 = c. The query X2 := {PhD(s)} with partition \u3008{D1,D2,D3,D4,D6} , {D5} , \u2205\u3009 is a high-risk query because cq(X2) = 1/6 < 0.3 = c and X3 := {Researcher(s), Student(s)} with \u3008{D2,D4,D6} , {D1,D3,D5} , \u2205\u3009 is a no-risk query due to cq(X3) = 3/6 = cq .\nGiven a user\u2019s answer us to a query Xs, the cautiousness c is updated depending on the elimination rate e(Xs, us) by c\u2190 c+cadj where the cautiousness adjustment factor cadj := 2 (c\u2212c)adj . The scaling factor 2 (c\u2212c) regulates the extent of the cautiousness adjustment depending on the interval length c \u2212 c. More crucial is the factor adj that indicates the sign and magnitude of the cautiousness adjustment.\nadj :=\n\u230a |D| 2 \u2212 \u03b5 \u230b |D| \u2212 e(Xs, us)\nwhere \u03b5 \u2208 (0, 12 ) is a constant which prevents the algorithm from getting stuck in a no-risk strategy for even |D|. E.g., given c = 0.5 and \u03b5 = 0, the elimination rate of a no-risk query e(Xs, us) = 12 resulting always in adj = 0. The value\nof \u03b5 can be set to an arbitrary real number, e.g. \u03b5 := 14 . If c + cadj is outside the user-defined cautiousness interval [c, c], it is set to c if c < c and to c if c > c. Positive cadj is a penalty telling the algorithm to get more cautious, whereas negative cadj is a bonus resulting in a braver behavior of the algorithm. Note, for the user-defined interval [c, c] \u2286 [cq, cq] must hold. c \u2212 cq and cq \u2212 c represent the minimal desired difference in performance to a high-risk (ENT) and no-risk (SPL) query selection, respectively. By expressing trust (disbelief) in the prior fault probabilities through specification of lower (higher) values for c and/or c, the user can take influence on the behavior of RIO. Example 3 (cont. Example 1): Assume p(ax i) := 0.001 for axi(i=1,...,4) and p(ax 5) := 0.1, p(ax 6) := 0.15 and the user rather disbelieves these fault probabilities and thus sets c = 0.4, c = 0 and c = 0.5. In this case RIO selects a no-risk query X3 just as SPL. Given u3 = t and |D| = 6, the algorithm computes the elimination rate e(X3, t) = 0.5 and adjusts the cautiousness by cadj = \u22120.17 which yields c = 0.23. This allows RIO to select a higher-risk query in the next iteration, whereupon the target diagnosis D\u2217 = D2 is found after asking three queries. In the same situation, ENT (starting with high-risk queryX1) would require four queries.\nRIO, described in Algorithm 2, starts with the computation of minimal diagnoses. GETDIAGNOSES function implements a combination of HS-Tree and QuickXPlain algorithms. Using uniform-cost search, the algorithm extends the set of leading diagnoses D with a maximum number of most probable minimal diagnoses such that |D| \u2264 n.\nThen the GETPROBABILITIES function calculates the fault probabilities p(Di) for each diagnosisDi of the set of leading diagnoses D using formula (4). Next it adjusts the probabilities as per the Bayesian theorem taking into account all previous query answers which are stored in P and N . Finally, the resulting probabilities padj(Di) are normalized. Based on the set of leading diagnoses D, GENERATEQUERIES generates queries according to Algorithm 1. GETMINSCOREQUERY determines the best query Xsc \u2208 XD according to scent: Xsc = arg minXk\u2208XD(scent(Xk)). If Xsc is a non-high-risk query, i.e. c \u2264 cq(Xsc) (determined by GETQUERYCAUTIOUSNESS), Xsc is selected. In this case, Xsc is the query with best information gain in XD and moreover guarantees the required elimination rate specified by c.\nAlgorithm 2: Risk Optimization Algorithm (RIO) Input: diagnosis problem instance \u3008O,B,P,N \u3009R , fault probabilities of\ndiagnosesDP , cautiousnessC = (c, c, c), number of leading diagnoses n to be considered, acceptance threshold \u03c3\nOutput: a diagnosisD 1 P \u2190 \u2205; N \u2190 \u2205; D\u2190 \u2205; 2 repeat 3 D\u2190 getDiagnoses(D, n,O,B,P,N ); 4 DP \u2190 getProbabilities(DP,D,P,N ); 5 X\u2190 generateQueries(O,B,P,D); 6 Xs \u2190 getMinScoreQuery(DP,X); 7 if getQueryCautiousness(Xs,D) < c then Xs \u2190 getAlternativeQuery(c,X, DP,D); 8 if getAnswer(Xs) = yes then P \u2190 P \u222a {Xs}; 9 else N \u2190 N \u222a {Xs};\n10 c\u2190 updateCautiousness(D,P,N , Xs, c, c, c); 11 until (aboveThreshold(DP, \u03c3) \u2228 eliminationRate(Xs) = 0); 12 return mostProbableDiag(D, DP );\nOtherwise, GETALTERNATIVEQUERY selects the query Xalt \u2208 XD (Xalt 6= Xsc) which has minimal score scent among all least cautious non-high-risk queries Lc. That is, Xalt = arg minXk\u2208Lc(scent(Xk)) where Lc := {Xr \u2208 NHRc(XD) | \u2200Xt \u2208 NHRc(X) : cq(Xr) \u2264 cq(Xt)}. If there is no such query Xalt \u2208 XD, then Xsc is selected.\nGiven the user\u2019s answer us, the selected query Xs \u2208 {Xsc ,Xalt} is added to P or N accordingly. In the last step of the main loop the algorithm updates the cautiousness value c (function UPDATECAUTIOUSNESS) as described above.\nBefore the next query selection iteration starts, a stop condition test is performed. The algorithm evaluates whether the most probable diagnosis is at least \u03c3% more likely than the second most probable diagnosis (ABOVETHRESHOLD) or none of the leading diagnoses has been eliminated by the previous query, i.e.GETELIMINATIONRATE returns zero for Xs. If a stop condition is met, the presently most likely diagnosis is returned (MOSTPROBABLEDIAG)."}, {"heading": "4 Evaluation", "text": "Goals. This evaluation should demonstrate that (1) there is a significant discrepancy between SPL and ENT concerning number of queries where the winner depends on the quality of meta information, (2) RIO exhibits superior average behavior compared to ENT and SPL w.r.t. the amount of user interaction required, irrespective of the quality of specified fault information, (3) RIO scales well and (4) its reaction time is well suited for an interactive debugging approach. Provenance of test data. As data source for the evaluation we used faulty real-world ontologies produced by automatic ontology matching systems (OMSs) (cf. Example 1). Definition 7 (Ontology matching) [Shvaiko and Euzenat, 2012] Let Q(O) \u2286 S(O) denote the set of matchable elements in an ontology O, where S(O) denotes the signature of O. An ontology matching operation determines an alignment Mij , which is a set of correspondences between matched ontologies Oi and Oj . Each correspondence is a 4- tuple \u3008xi, xj , r, v\u3009, such that xi \u2208 Q(Oi), xj \u2208 Q(Oj), r is a semantic relation and v \u2208 [0, 1] is a confidence value. We callOiMj := Oi\u222a\u03c3(Mij)\u222aOj the aligned ontology forOi and Oj where \u03c3 maps each correspondence to an axiom. Let in the following Q(O) be the restriction to atomic concepts and roles in S(O), r \u2208 {v,w,\u2261} and \u03c3 the natural alignment semantics [Meilicke and Stuckenschmidt, 2009] that maps correspondences one-to-one to axioms of the form xi r xj . We evaluate RIO using aligned ontologies by the following reasons: (1) Alignments often cause inconsistency/incoherence of ontologies. (2) The (fault) structure of different ontologies obtained through matching generally varies due to different authors and matching systems involved. (3) For the same reasons, it is hard to estimate the quality of fault probabilities, i.e. it is unclear which existing query selection strategy to choose for best performance. (4) Availability of correct reference alignments. Test datasets. We used two datasets D1 and D2: Each faulty aligned ontology OiMj in D1 is the result of applying one of four OMSs to a set of six independently created ontologies in the domain of conference organization. For a given pair of ontologies Oi 6= Oj , each system produced an alignmentMij .\nThe average size of OiMj per matching system was between 312 and 377 axioms. D1 is a superset of the dataset used in [Stuckenschmidt, 2008] for which all debugging systems under evaluation manifested correctness or scalability problems. D2, used to assess the scalability of RIO, is the set of ontologies from the ANATOMY track in the Ontology Alignment Evaluation Initiative1 (OAEI) 2011.5 [Shvaiko and Euzenat, 2012], which comprises two input ontologies O1 (11545 axioms) and O2 (4838 axioms). The size of the aligned ontologies generated by results of seven different OMSs was between 17530 and 17844 axioms. Reference Solutions. For dataset D1, based on a manually produced reference alignment Rij \u2286 Mij for ontologies Oi,Oj (cf. [Meilicke et al., 2008]), we were able to fix a target diagnosis D\u2217 := \u03c3(Mij \\ Rij) for each incoherent OiMj . In cases where D\u2217 represented a non-minimal diagnosis, it was randomly redefined as a minimum diagnosis D\u2217 \u2282 \u03c3(Mij \\ Rij). In case of D2, given ontologies O1 and O2, matching output M12, and the correct reference alignmentR12, we fixedD\u2217 as follows: We carried out (prior to the actual experiment) a debugging session with DPI \u3008\u03c3(M12 \\ R12),O1 \u222a O2 \u222a \u03c3(M12 \u2229R12), \u2205, \u2205\u3009{coherence} and randomly chose one of the identified diagnoses as D\u2217. Note, it is common in OMS [Meilicke, 2011] that D\u2217 can be a subset of D := \u03c3(Mij \\Rij) as there is no evidence based on coherence to classify any ax \u2208 \u03c3(D \\ D\u2217) as faulty. Test settings.2 We conducted four experiments EXP-i (i = 1, . . . , 4), the first two with dataset D1 and the other two with D2. In experiments 1 and 3 we simulated good fault probabilities by setting p(axk) := 0.001 for axk \u2208 Oi \u222a Oj and p(axm) := 1 \u2212 vm for axm \u2208 Mij , where vm is the confidence of the correspondence underlying axm. Low quality fault information was used in experiments 2 and 4. In EXP-4 the following probabilities were defined: p(axk) := 0.01 for axk \u2208 Oi \u222a Oj and p(axm) := 0.001 for axm \u2208 Mij . In EXP-2 we used probability settings of EXP-1, but fixed a completely unlikely target diagnosis in that we precomputed (prior to the actual experiment) the 30 most probable minimal diagnoses, and from these selected the one including the highest number of axioms axk \u2208 OiMj \\ \u03c3(Mij) as D\u2217.\nIn all experiments, we set |D| := 9 which proved to be a good trade-off between computation effort and representativeness of leading diagnoses, \u03c3 := 85% and as input parameters for RIO c := 0.25 and [c, c] := [cq, cq] = [0, 49 ]. To let tests pose the highest challenge for the evaluated methods, the initial DPI was specified as \u3008OiMj , \u2205, \u2205, \u2205\u3009{coherence}, i.e. the full search space was explored without adding parts of OiMj to B. In practice, given prior knowledge of correct axioms, adding those to B can severely restrict the search space and greatly accelerate debugging. All tests were executed on a Core-i7 (3930K), 32GB RAM with Ubuntu 11.04 and Java 6. Metrics. Each experiment involved a debugging session of ENT, SPL as well as RIO for each ontology in the respective dataset. In each session we measured the number of required queries (q) untilD\u2217 was identified, the overall debugging time (debug) assuming that queries are answered instantaneously and the reaction time (react), i.e. the average time between\n1http://oaei.ontologymatching.org 2See http://code.google.com/p/rmbd/wiki/ for code and details.\nPrinted by Mathematica for Students\ntwo successive queries. The queries generated in the tests were answered by an automatic oracle by means of the target ontology Ot := OiMj \\ D\u2217. Observations. The difference w.r.t. number of queries per test run between the better and the worse strategy in {SPL,ENT} was absolutely significant, with a maximum of 2300% in EXP-4 and averages of 190% to 1145% throughout all experiments (Figure 2(c)). Moreover, results show that varying quality of fault probabilities in {EXP-1,EXP-3} compared to {EXP-2,EXP-4} clearly affected the performance of ENT and SPL (see first two rows in Figure 1(a)). This perfectly motivates why a risk-optimizing strategy is suitable.\nResults of both experimental sessions, \u3008EXP-1,EXP-2\u3009 and \u3008EXP-3,EXP-4\u3009, are summarized in Figures 2(a) and 2(b), respectively. The figures show the (average) number of queries asked by RIO and the (average) differences to the number of queries needed by the per-session better and worse strategy in {SPL,ENT}, respectively. The results illustrate clearly that the average performance achieved by RIO was always substantially closer to the better than to the worse strategy. In both EXP-1 and EXP-2, throughout 74% of 27 debugging sessions, RIO worked as efficiently as the best strategy (Figure 1(a)). In 26% of the cases in EXP-2, RIO even outperformed both other strategies; in these cases, RIO could save more than 20% of user interaction on average compared to the best other strategy. In one scenario in EXP-1, it took ENT 31 and SPL 13 queries to finish, whereas RIO required only 6 queries, which amounts to an improvement of more than 80% and 53%, respectively. In \u3008EXP-3,EXP-4\u3009, the savings achieved by RIO were even more substantial. RIO manifested superior behavior to both other strategies in 29% and 71% of cases, respectively. Not less remarkable, in 100% of the tests in EXP-3 and EXP-4, RIO was at least as efficient as\nthe best other strategy. Recalling Figure 2(c), this means that RIO can avoid query overheads of 2200%. Figure 1(b), which provides average values for q, react and debug per strategy, demonstrates that RIO is the best choice in all experiments w.r.t. q. Consequently, RIO is suitable for both good and poor meta information. As to time aspects, RIO manifested good performance, too. Since times consumed in \u3008EXP-1,EXP-2\u3009 are almost negligible, consider the more meaningful results obtained in \u3008EXP-3,EXP-4\u3009. While the best reaction time in both experiments was achieved by SPL, we can clearly see that SPL was significantly inferior to both ENT and RIO concerning q and debug. RIO revealed the best debugging time in EXP-4, and needed only 2.2% more time than the best strategy (ENT) in EXP-3. However, if we assume the user being capable of reading and answering a query in, e.g., 30 sec on average, which is already quite fast, then the overall time savings of RIO compared to ENT in EXP-3 would already account for 5%. Doing the same thought experiment for EXP-4, RIO would save 25% (w.r.t. ENT) and 50% (w.r.t. SPL) of debugging time on average. All in all, the measured times confirm that RIO is well suited for interactive debugging."}, {"heading": "5 Conclusions", "text": "We have shown problems of state-of-the-art interactive ontology debugging strategies w.r.t. the usage of unreliable meta information. To tackle this issue, we proposed a learning strategy which combines the benefits of existing approaches, i.e. high potential and low risk. Depending on the performance of the diagnosis discrimination actions, the trust in the a-priori information is adapted. Tested under various conditions, our algorithm revealed good scalability and reaction time as well as superior average performance to two common approaches in the field w.r.t. required user interaction."}], "references": [{"title": "editors", "author": ["Franz Baader", "Diego Calvanese", "Deborah McGuinness", "Daniele Nardi", "Peter PatelSchneider"], "venue": "The Description Logic Handbook: Theory, Implementation, Applications. Cambridge Press,", "citeRegEx": "Baader et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Artif", "author": ["Johan de Kleer", "Brian C. Williams. Diagnosing multiple faults"], "venue": "Intell., 32(1):97\u2013130,", "citeRegEx": "de Kleer and Williams. 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "A General Diagnosis Method for Ontologies", "author": ["Friedrich", "Shchekotykhin", "2005] Gerhard Friedrich", "Kostyantyn Shchekotykhin"], "venue": "The Semantic Web ISWC", "citeRegEx": "Friedrich et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Friedrich et al\\.", "year": 2005}, {"title": "OWL 2: The next step for OWL", "author": ["Bernardo Cuenca Grau", "Ian Horrocks", "Boris Motik", "Bijan Parsia", "Peter F. Patel-Schneider", "Ulrike Sattler"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web, 6(4):309\u2013322, November", "citeRegEx": "Grau et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "QUICKXPLAIN: Preferred Explanations and Relaxations for Over-Constrained Problems", "author": ["Ulrich Junker"], "venue": "[Junker,", "citeRegEx": "Junker.,? \\Q2004\\E", "shortCiteRegEx": "Junker.", "year": 2004}, {"title": "University of Maryland", "author": ["Aditya Kalyanpur. Debugging", "Repair of OWL Ontologies. PhD thesis"], "venue": "College Park,", "citeRegEx": "Kalyanpur. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "In Proceedings of the 3rd International Conference on Web Reasoning and Rule Systems", "author": ["Christian Meilicke", "Heiner Stuckenschmidt. An Efficient Method for Computing Alignment Diagnoses"], "venue": "pages 182\u2013196. Springer-Verlag,", "citeRegEx": "Meilicke and Stuckenschmidt. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Journal of Logic and Computation", "author": ["Christian Meilicke", "Heiner Stuckenschmidt", "Andrei Tamilin. Reasoning Support for Mapping Revision"], "venue": "19(5):807\u2013829,", "citeRegEx": "Meilicke et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "PhD thesis", "author": ["Christian Meilicke. Alignment Incoherence in Ontology Matching"], "venue": "Universitat Mannheim,", "citeRegEx": "Meilicke. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Artif", "author": ["Raymond Reiter. A Theory of Diagnosis from First Principles"], "venue": "Intell., 32(1):57\u201395,", "citeRegEx": "Reiter. 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "Interactive ontology debugging : two query strategies for efficient fault localization", "author": ["Shchekotykhin et al", "2012] Kostyantyn Shchekotykhin", "Gerhard Friedrich", "Philipp Fleiss", "Patrick Rodler"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Ontology matching: State of the art and future challenges", "author": ["Pavel Shvaiko", "J\u00e9r\u00f4me Euzenat"], "venue": "IEEE Transactions on Knowledge and Data Engineering, X(X):1\u201320,", "citeRegEx": "Shvaiko and Euzenat. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "JAIR)", "author": ["Sajjad Ahmed Siddiqi", "Jinbo Huang. Sequential diagnosis by abstraction. J. Artif. Intell. Res"], "venue": "41:329\u2013365,", "citeRegEx": "Siddiqi and Huang. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of the 6th International Workshop on Evaluation of Ontologybased Tools and the Semantic Web Service Challenge (EON)", "author": ["Heiner Stuckenschmidt. Debugging OWL Ontologies - A Reality Check"], "venue": "pages 1\u201312, Tenerife, Spain,", "citeRegEx": "Stuckenschmidt. 2008", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 5, "context": "[Kalyanpur, 2006].", "startOffset": 0, "endOffset": 17}, {"referenceID": 12, "context": "Interactive debugging systems such as [Shchekotykhin et al., 2012; Siddiqi and Huang, 2011] tackle this issue by letting an oracle take action during the debugging session by answering queries.", "startOffset": 38, "endOffset": 91}, {"referenceID": 1, "context": "In their seminal work [de Kleer and Williams, 1987] proposed two query selection strategies: split-in-half and entropy-based.", "startOffset": 22, "endOffset": 51}, {"referenceID": 0, "context": "Description logic [Baader et al., 2003] is a family of knowledge representation languages with a formal logicbased semantics that are designed to represent knowledge about a domain in form of concept descriptions.", "startOffset": 18, "endOffset": 39}, {"referenceID": 9, "context": "To compute MD we employ a combination of HS-Tree [Reiter, 1987] and QuickXPlain [Junker, 2004] algorithms as suggested by [Friedrich and Shchekotykhin, 2005].", "startOffset": 49, "endOffset": 63}, {"referenceID": 4, "context": "To compute MD we employ a combination of HS-Tree [Reiter, 1987] and QuickXPlain [Junker, 2004] algorithms as suggested by [Friedrich and Shchekotykhin, 2005].", "startOffset": 80, "endOffset": 94}, {"referenceID": 1, "context": "Two strategies for selecting the \u201cbest\u201d next query have been proposed [de Kleer and Williams, 1987] and adapted to debugging of KBs by [Shchekotykhin et al.", "startOffset": 70, "endOffset": 99}, {"referenceID": 3, "context": "{\u2200,\u2203,v,\u00ac,t,u} \u2282 CT (OWL) [Grau et al., 2008].", "startOffset": 25, "endOffset": 44}, {"referenceID": 11, "context": "Definition 7 (Ontology matching) [Shvaiko and Euzenat, 2012] Let Q(O) \u2286 S(O) denote the set of matchable elements in an ontology O, where S(O) denotes the signature of O.", "startOffset": 33, "endOffset": 60}, {"referenceID": 6, "context": "Let in the following Q(O) be the restriction to atomic concepts and roles in S(O), r \u2208 {v,w,\u2261} and \u03c3 the natural alignment semantics [Meilicke and Stuckenschmidt, 2009] that maps correspondences one-to-one to axioms of the form xi r xj .", "startOffset": 133, "endOffset": 168}, {"referenceID": 13, "context": "D1 is a superset of the dataset used in [Stuckenschmidt, 2008] for which all debugging systems under evaluation manifested correctness or scalability problems.", "startOffset": 40, "endOffset": 62}, {"referenceID": 11, "context": "5 [Shvaiko and Euzenat, 2012], which comprises two input ontologies O1 (11545 axioms) and O2 (4838 axioms).", "startOffset": 2, "endOffset": 29}, {"referenceID": 7, "context": "[Meilicke et al., 2008]), we were able to fix a target diagnosis D\u2217 := \u03c3(Mij \\ Rij) for each incoherent OiMj .", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "Note, it is common in OMS [Meilicke, 2011] that D\u2217 can be a subset of D := \u03c3(Mij \\Rij) as there is no evidence based on coherence to classify any ax \u2208 \u03c3(D \\ D\u2217) as faulty.", "startOffset": 26, "endOffset": 42}], "year": 2013, "abstractText": "The best currently known interactive debugging systems rely upon some meta-information in terms of fault probabilities in order to improve their efficiency. However, misleading meta information might result in a dramatic decrease of the performance and its assessment is only possible aposteriori. Consequently, as long as the actual fault is unknown, there is always some risk of suboptimal interactions. In this work we present a reinforcement learning strategy that continuously adapts its behavior depending on the performance achieved and minimizes the risk of using lowquality meta information. Therefore, this method is suitable for application scenarios where reliable prior fault estimates are difficult to obtain. Using diverse real-world knowledge bases, we show that the proposed interactive query strategy is scalable, features decent reaction time, and outperforms both entropy-based and no-risk strategies on average w.r.t. required amount of user interaction.", "creator": "LaTeX with hyperref package"}}}