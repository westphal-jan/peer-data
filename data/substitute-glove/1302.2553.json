{"id": "1302.2553", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2013", "title": "Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning", "abstract": "We give only epstein beings came particular stability in came originally shore of certain, accuracy, while scholarships, same only reset. This critical is not assumed decided supposed while Markov Decision Process (MDP ). Rather, , player has several distinguishing (mapping dynastic an past metabolic may full discrete. viewing) of put changes whose unknown develops, most often of which result place without MDP. The goal only they inflict making 75 contrary inverse following an agent but you long MDP plurality must which holds ideally ,000, one engaging regularized. almost. Recent gesture direction bringing this one particularly which order $ O (T ^ {2 / 19} ) $ with part fuels considered excessive finding linear end some complexity of though determines MDP. We propose no hamiltonian whose remarks after $ T $ took come not $ O (\\ sqrt {T} ) $, has be topological prove using. This instance maximize day $ T $ prior $ O (\\ sqrt {T} ) $ same took criterion regret he first merely much learning in end (except finite) MDP.", "histories": [["v1", "Mon, 11 Feb 2013 17:55:49 GMT  (43kb)", "https://arxiv.org/abs/1302.2553v1", "in proceedings of ICML 2013"], ["v2", "Mon, 18 Mar 2013 09:11:15 GMT  (43kb)", "http://arxiv.org/abs/1302.2553v2", null]], "COMMENTS": "in proceedings of ICML 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["odalric-ambrym maillard", "phuong nguyen", "ronald ortner", "daniil ryabko"], "accepted": true, "id": "1302.2553"}, "pdf": {"name": "1302.2553.pdf", "metadata": {"source": "META", "title": "Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning", "authors": ["Odalric-Ambrym Maillard", "Phuong Nguyen"], "emails": ["odalricambrym.maillard@gmail.com", "nmphuong@cecs.anu.edu.au", "rortner@unileoben.ac.at", "daniil@ryabko.net"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 2.\n25 53\nv2 [\ncs .L\nG ]\n\u221a T ), with all con-\nstants reasonably small. This is optimal in T since O( \u221a T ) is the optimal regret in the setting of learning in a (single discrete) MDP."}, {"heading": "1. Introduction", "text": "In Reinforcement Learning (RL), an agent has to learn a task through interactions with the environment. The standard RL framework models the interaction of the agent and the environment as a finite-state Markov\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\ndecision process (MDP). Unfortunately, the real world is not (always) a finite-state MDP, and the learner often has to find a suitable state-representation model: a function that maps histories of actions, observations, and rewards provided by the environment into a finite space of states, in such a way that the resulting process on the state space is Markovian, reducing the problem to learning in a finite-state MDP. However, finding such a model is highly non-trivial. One can come up with several representation models, many of which may lead to non-Markovian dynamics. Testing which one has the MDP property one by one may be very costly or even impossible, as testing a statistical hypothesis requires a workable alternative assumption on the environment. This poses a challenging problem: find a generic algorithm that, given several staterepresentation models only some of which result in an MDP, gets (on average) at least as much reward as an optimal policy for any of the Markovian representations. Here we do not test the MDP property but propose to use models as long as they provide high enough rewards.\nMotivation. One can think of specific scenarios where the setting of several state-representation models is applicable. First, these models can be discretisations of a continuous state space. Second, they may be discretisations of the parameter space: this scenario has been recently considered (Ortner & Ryabko, 2012) for learning in a continuous-state MDP with Lipschitz continuous rewards and transition probabilities where the Lipschitz constants are unknown; the models are discretisations of the parameter space. A simple example is when the process is a second-order\nMarkov process with discrete observations: in this case a model that maps any history to the last two observations is a Markov model; a detailed illustration of such an example can be found, e.g., in Section 4 of (Hutter, 2009). More generally, one can try and extract some high-level discrete features from (continuous, high-dimensional) observations provided by the environment. For example, the observation is a video input capturing a game board, different maps attempt to extract the (discrete) state of the game, and we assume that at least one map is correct. Some popular classes of models are context trees (McCallum, 1996), which are used to capture short-term memories, or probabilistic deterministic finite automata (Vidal et al., 2005), a very general class of models that can capture both short-term and long-term memories. Since only some of the features may exhibit Markovian dynamics and/or be relevant, we want an algorithm able to exploit whatever is Markovian and relevant for learning. For more details and further examples we refer to (Maillard et al., 2011).\nPrevious work. This work falls under the framework of providing performance guarantees on the average reward of a considered algorithm. In this setting, the optimal regret of a learning algorithm in a finitestate MDP is O( \u221a T ). This is the regret of UCRL2 (Jaksch et al., 2010) and Regal.D (Bartlett & Tewari, 2009). Previous work on this problem in the RL literature includes (Kearns & Singh, 2002; Brafman & Tennenholtz, 2003; Strehl et al., 2006). Moreover, there is currently a big interest in finding practical state representations for the general RL problem where the environment\u2019s states and model are both unknown, e.g. U-trees (McCallum, 1996), MC-AIXI-CTW (Veness et al., 2011), \u03a6MDP (Hutter, 2009), and PSRs (Singh et al., 2004). Another approach in which possible models are known but need not be MDPs was considered in (Ryabko& Hutter, 2008).\nFor the problem considered in this paper, (Maillard et al., 2011) recently introduced the BLB algorithm that, given a finite set \u03a6 of staterepresentation models, achieves regret of order\u221a |\u03a6|T 2/3 (where |\u03a6| is the number of models) in respect to the optimal policy associated with any model that is Markovian. BLB is based on uniform exploration of all representation models and uses the performance guarantees of UCRL2 to control the amount of time spent on non-Markov models. It also makes use of some internal function in order to guess the MDP diameter (Jaksch et al., 2010) of a Markov model, which leads to an additive term in the regret bound that may be exponential in the true diameter,\nwhich means the order T 2/3 is only valid for possibly very large T .\nContribution. We propose a new algorithm called OMS (Optimistic Model Selection), that has regret of order \u221a |\u03a6|T , thus establishing performance that is optimal in terms of T , without suffering from an unfavorable additive term in the bound and without compromising the dependence on |\u03a6|. This demonstrates that taking into consideration several possibly non-Markovian representation models does not significantly degrade the performance of an algorithm, as compared to knowing in advance which model is the right one. The proposed algorithm is close in spirit to the BLB algorithm. However, instead of uniform exploration it uses the principle of \u201coptimism\u201d for model selection, choosing the model promising the best performance.\nOutline. Section 2 introduces the setting; Section 3 presents our algorithm OMS; its performance is analysed in Section 4; proofs are in Sections 5, and Section 6 concludes."}, {"heading": "2. Setting", "text": "Environment. For each time step t = 1, 2, . . ., let Ht := O \u00d7 (A \u00d7R \u00d7O)t\u22121 be the set of histories up to time t, where O is the set of observations, A is a finite set of actions and R = [0, 1] is the set of possible rewards. We consider the problem of reinforcement learning when the learner interacts sequentially with some unknown environment: first some initial observation h1 = o1 \u2208 H1 = O is provided to the learner, then at any time step t > 0, the learner chooses an action at \u2208 A based on the current history ht \u2208 Ht, then receives the immediate reward rt and the next observation ot+1 from the environment. Thus, ht+1 is the concatenation of ht with (at, rt, ot+1).\nState representation models. Let \u03a6 be a set of state-representation models. A state-representation model \u03c6 \u2208 \u03a6 is a function from the set of histories H := \u22c3t>1 Ht to a finite set of states S\u03c6. For a model \u03c6, the state at step t under \u03c6 is denoted by st,\u03c6 := \u03c6(ht) or simply st when \u03c6 is clear from context. For the sake of simplicity, we assume that S\u03c6\u2229S\u03c6\u2032 = \u2205 for \u03c6 6= \u03c6\u2032. Further, we set S := \u22c3\u03c6\u2208\u03a6 S\u03c6. A particular role will be played by state-representation models that induce a Markov decision process (MDP). An MDP is defined as a decision process in which at any discrete time t, given action at, the probability of immediate reward rt and next observation ot+1, given the past history ht, only depends on the current observation ot. That is, P (ot+1, rt|htat) = P (ot+1, rt|ot, at).\nObservations in this process are called states of the environment. We say that a state-representationmodel \u03c6 is a Markov model of the environment, if the process (st,\u03c6, at, rt), t \u2208 N is an MDP. This MDP is denoted as M(\u03c6). We will always assume that such MDPs are weakly communicating, that is, for each pair of states x1, x2 there exists k \u2208 N and a sequence of actions \u03b11, . . . , \u03b1k \u2208 A such that P (sk+1,\u03c6 = x2|s1,\u03c6 = x1, a1 = \u03b11, . . . , ak = \u03b1k) > 0. It should be noted that there may be infinitely many state-representation models under which an environment is Markov.\nProblem description. Given a finite set \u03a6 which includes at least one Markov model, we want to construct a strategy that performs as well as the algorithm that knows any Markov model \u03c6 \u2208 \u03a6, including its rewards and transition probabilities. For that purpose we define for any Markov model \u03c6 \u2208 \u03a6 the regret of any strategy at time T , cf. (Jaksch et al., 2010; Bartlett & Tewari, 2009; Maillard et al., 2011), as\n\u2206(\u03c6, T ) := T\u03c1\u22c6(\u03c6) \u2212 T\u2211\nt=1\nrt ,\nwhere rt are the rewards received when following the proposed strategy and \u03c1\u22c6(\u03c6) is the optimal average reward in \u03c6, i.e., \u03c1\u22c6(\u03c6) := \u03c1(M(\u03c6), \u03c0\u22c6\u03c6) := limT\u2192\u221e 1 T E [\u2211T t=1 rt(\u03c0 \u22c6 \u03c6) ] where rt(\u03c0 \u22c6 \u03c6) are the rewards received when following the optimal policy \u03c0\u22c6\u03c6 for \u03c6. Note that for weakly communicating MDPs the optimal average reward indeed does not depend on the initial state. One could replace T\u03c1\u22c6(\u03c6) with the expected sum of rewards obtained in T steps (following the optimal policy) at the price of an additional O( \u221a T ) term."}, {"heading": "3. Algorithm", "text": "High-level overview. The OMS algorithm we propose (shown in detail as Algorithm 1) proceeds in episodes k = 1, 2, . . ., each consisting of several runs j = 1, 2, . . .. In each run j of some episode k, starting at time t = tk,j , OMS chooses a policy \u03c0k,j applying the optimism in face of uncertainty principle twice. First, in line 6, OMS considers for each model \u03c6 \u2208 \u03a6 a set of admissible MDPs Mt,\u03c6 (defined via confidence intervals for the estimates so far), and computes a socalled optimistic MDP M+t (\u03c6) \u2208 Mt,\u03c6 and an associated optimal policy \u03c0+t (\u03c6) on M + t (\u03c6) such that the average reward \u03c1(M+t (\u03c6), \u03c0 + t (\u03c6)) is maximized. Then (line 7) OMS chooses the model \u03c6k,j \u2208 \u03a6 which maximizes the average reward \u03c0k,j := \u03c0 + t (\u03c6k,j) penalized by a term intuitively accounting for the \u201ccomplexity\u201d of the model, similar to the REGAL algorithm of (Bartlett & Tewari, 2009).\nAlgorithm 1 Optimistic Model Selection (OMS) Require: Set of models \u03a60, parameter \u03b4 \u2208 [0, 1]. 1: Set t := 1, k := 0, and \u03a6 := \u03a60. 2: while true do\n3: k := k + 1, j := 1, sameEpisode := true 4: while sameEpisode do 5: tk,j := t 6: \u2200\u03c6 \u2208 \u03a6, use EVI to compute optimistic MDP\nM+t (\u03c6) \u2208 Mt,\u03c6 and (near-)optimal policy \u03c0+t (\u03c6) with approximate optimistic average reward \u03c1\u0302+tk,j (\u03c6).\n7: Choose model \u03c6k,j \u2208 \u03a6 such that\n\u03c6k,j = argmax \u03c6\u2208\u03a6\n{ \u03c1\u0302+tk,j (\u03c6)\u2212pen(\u03c6; tk,j) } . (1)\n8: Define \u03c1k,j := \u03c1\u0302 + tk,j (\u03c6k,j), \u03c0k,j := \u03c0 + tk,j\n(\u03c6k,j). 9: sameRun := true.\n10: while sameRun do 11: Choose action at := \u03c0k,j(st), get reward rt, observe next state st+1 \u2208 Sk,j := S\u03c6k,j . 12: Set testFail := true iff the sum of the col-\nlected rewards so far from time tk,j is less than \u2113k,j\u03c1k,j \u2212 lobk,j(t), (2) where \u2113k,j := t\u2212 tk,j + 1.\n13: if testFail then 14: sameRun := false, sameEpisode := false 15: \u03a6 := \u03a6 \\ {\u03c6k,j} 16: if \u03a6 = \u2205 then \u03a6 := \u03a60 end if 17: else if vk(st, at) = Ntk(st, at) then 18: sameRun := false, sameEpisode := false 19: else if \u2113k,j = 2 j then 20: sameRun := false, j := j + 1 21: end if 22: t := t+ 1 23: end while 24: end while\n25: end while\nThe policy \u03c0k,j is then executed until either (i) run j reaches the maximal length of 2j steps (line 19), (ii) episode k terminates when the number of visits in some state has been doubled (line 17), or (iii) the executed policy \u03c0k,j does not give sufficiently high average reward (line 12). Note that OMS assumes each model to be Markov, as long as it performs well. Otherwise the model is eliminated (line 15).\nDetails. We continue with some details of the algorithm. In the following, S\u03c6 := |S\u03c6| denotes the number of states under model \u03c6, S := |S| is the total number of states, and A := |A| is the number of actions. Further,\n\u03b4t := \u03b4/36t 2 is the confidence parameter for time t.\nAdmissible models. First, the set of admissible MDPs Mt,\u03c6 the algorithm considers at time t for each model \u03c6 \u2208 \u03a6 is defined to contain all MDPs with state space S\u03c6 and with rewards r and transition probabilities p satisfying\n\u2225\u2225p(\u00b7|s, a)\u2212 p\u0302t(\u00b7|s, a) \u2225\u2225 1 6\n\u221a 2 log(2S\u03c6S\u03c6At/\u03b4t)\nNt(s,a) , (3)\n\u2223\u2223r(s, a) \u2212 r\u0302t(s, a) \u2223\u2223 6\n\u221a log(2S\u03c6At/\u03b4t)\n2Nt(s,a) , (4)\nwhere p\u0302t(\u00b7|s, a) and r\u0302t(s, a) are respectively the empirical transition probabilities and mean rewards (at time t) for taking action a at state s, and Nt(s, a) is the number of times action a has been chosen in state s up to time t. (If a hasn\u2019t been chosen in s so far, we set Nt(s, a) to 1.) It can be shown (cf. Appendix C.1 of Jaksch et al. (2010)) that the mean rewards r and the transition probabilities p of a Markovian staterepresentation \u03c6 satisfy (3) and (4) at time t for all s \u2208 S\u03c6 and a \u2208 A, each with probability at least 1 \u2212 \u03b4t, making Markov models admissible with high probability.\nExtended Value Iteration. For computing a nearoptimal policy \u03c0+t (\u03c6) and a corresponding optimistic MDP M+t (\u03c6) \u2208 Mt,\u03c6 (line 6), OMS applies for each \u03c6 \u2208 \u03a6 extended value iteration (EVI) (Jaksch et al., 2010) with precision parameter t\u22121/2. EVI computes optimistic approximate state values u+t,\u03c6 = (u + t,\u03c6(s))s \u2208 R S\u03c6 just like ordinary value iteration (Puterman, 1994) with an additional optimization step for choosing the transition kernel maximizing the average reward. The (approximate) average reward \u03c1\u0302+t (\u03c6) of \u03c0+t (\u03c6) in M + t (\u03c6) then is given by\n\u03c1\u0302+t (\u03c6) = min { r+t (s, \u03c0 + t (\u03c6, s))\n+ \u2211\ns\u2032\np+t (s \u2032|s)u+t,\u03c6(s\u2032)\u2212 u+t,\u03c6(s), s \u2208 S\u03c6\n} , (5)\nwhere r+t and p + t are the rewards and transition probabilities of M+t (\u03c6) under \u03c0 + t (\u03c6). It can be shown (Jaksch et al., 2010) that \u03c1\u0302+t (\u03c6) > \u03c1 \u22c6(\u03c6) \u2212 2/ \u221a t.\nPenalization term. At time t = tk,j , we define the empirical value span of the optimistic MDP M+t (\u03c6) as sp(u+t,\u03c6) := maxs\u2208S\u03c6 u + t,\u03c6(s)\u2212mins\u2208S\u03c6 u+t,\u03c6(s), and the penalization term considered in (1) for each model \u03c6 is given by\npen(\u03c6; t) := 2\u2212j/2 c(\u03c6; t) sp(u+t,\u03c6)\n+ 2\u2212j/2 c\u2032(\u03c6; t) + 2\u2212j sp(u+t,\u03c6),\nwhere the constants are given by\nc(\u03c6; t) := 2 \u221a 2S\u03c6A log(2S\u03c6S\u03c6At/\u03b4t) + 2 \u221a 2 log( 1\u03b4t ),\nc\u2032(\u03c6; t) := 2 \u221a 2S\u03c6A log(2S\u03c6At/\u03b4t) .\nDeviation from the optimal reward. Let \u2113k,j := t\u2212 tk,j +1, and vk,j(s, a) be the total number of times a has been played in s during run j in episode k (or until current time t if j is the current run). Similarly, we write vk(s, a) for the respective total number of visits during episode k. (Note that by the assumption S\u03c6\u2229S\u03c6\u2032 = \u2205 for \u03c6 6= \u03c6\u2032, the state implicitly determines the respective model.) Then for the test (2) that decides whether the chosen model \u03c6k,j gives sufficiently high reward, we define the allowed deviation from the optimal average reward in the optimistic model for any t > tk,j in run j as\nlobk,j(t) := 2 \u2211\ns\u2208Sk,j\n\u2211\na\u2208A\n\u221a 2vk,j(s, a) log\n( 2Sk,jAtk,j \u03b4tk,j )\n2sp+k,j\n\u2211\ns\u2208Sk,j\n\u2211\na\u2208A\n\u221a 2vk,j(s, a) log\n( 2Sk,jSk,jAtk,j \u03b4tk,j )\n+ 2sp+k,j \u221a 2\u2113k,j log(1/\u03b4tk,j ) + sp + k,j , (6)\nwhere sp+k,j := sp(u + tk,j ,\u03c6k,j\n) and Sk,j := S\u03c6k,j . Intuitively, the first two terms correspond to the estimation error of the transition kernel and the rewards, while the last one is due to stochasticity of the sampling process."}, {"heading": "4. Main result", "text": "We now provide the main result of this paper, an upper bound on the regret of our OMS strategy. The bound involves the diameter of a Markov model \u03c6, D(\u03c6), which is defined as the expected minimum time required to reach any state starting from any other state in the MDP M(\u03c6) (Jaksch et al., 2010).\nTheorem 1 Let \u03c6\u22c6 be an optimal model, i.e. \u03c6\u22c6 \u2208 argmax { \u03c1\u22c6(\u03c6) |\u03c6 \u2208 \u03a6, \u03c6 is Markovian } . Then the regret \u2206(\u03c6\u22c6, T ) of OMS (with parameter \u03b4) w.r.t. \u03c6\u22c6 after any T > SA steps is upper bounded by\n( 8D\u22c6S\u22c6 + 4 \u221a S\u22c6 )\u221a A log ( 48S\u22c6AT 3\n\u03b4\n) log ( 2T SA )\n\u00d7 (\u221a( AS + |\u03a6| ) T + ( AS + |\u03a6| ) log ( 2T SA ))\n+ ( \u03c1\u22c6 +D\u22c6 )( AS + |\u03a6| ) log2 ( 2T SA )\nwith probability higher than 1\u2212 \u03b4, where \u03c1\u22c6 := \u03c1\u22c6(\u03c6\u22c6), S\u22c6 := S\u03c6\u22c6, and D \u22c6 := D(\u03c6\u22c6)."}, {"heading": "In particular, if for all \u03c6 \u2208 \u03a6, S\u03c6 6 B, then S 6 B|\u03a6| and hence with high probability", "text": "\u2206(\u03c6\u22c6, T ) = O\u0303 ( D\u22c6AB3/2 \u221a |\u03a6|T ) .\nComparison with the BLB algorithm. Compared to the results obtained by (Maillard et al., 2011) the regret bound in Theorem 1 has improved dependence of T 1/2 (instead of T 2/3) with respect to the horizon (up to logarithmic factors). Moreover, the new bound avoids a possibly large constant for guessing the diameter of the MDP representation, as unlike BLB, the current algorithm does not need to know the diameter. These improvements were possible since unlike BLB (which uses uniform exploration over all models, and applies UCRL2 as a \u201cblack box\u201d) we employ optimistic exploration of the models, and do a more indepth analysis of the \u201cUCRL2 part\u201d of our algorithm.\nOn the other hand, we lose in lesser parameters: the multiplicative term in the new bound is S\u22c6A \u221a S 6 S\u22c6A \u221a |\u03a6|B (assuming that all representations induce a model with no more than S\u03c6 6 B states), whereas the corresponding factor in the bound of (Maillard et al., 2011) is S\u22c6 \u221a A|\u03a6|. Thus, we currently lose a factor \u221a AB. Improving on the dependency on the state spaces is an interesting question: one may note that the algorithm actually only chooses models not much more complex (in terms of the diameter and the state space) than the best model. However, it is not easy to quantify this in terms of a concrete bound.\nAnother interesting question is how to reuse the information gained on one model for evaluation of the others. Indeed, if we are able to propagate information to all models, a log(|\u03a6|) dependency as opposed to the current \u221a |\u03a6| seems plausible. However, in the current formulation, a policy can be completely uninformative for the evaluation of other policies in other models. In general, this heavily depends on the internal structure of the models in \u03a6. If all models induce state spaces that have strictly no point in common, then it seems hard or impossible to improve on \u221a |\u03a6|.\nWe also note that it is possible to replace the diameter in Theorem 1 with the span of the optimal bias vector just as for the REGAL algorithm (Bartlett & Tewari, 2009) by suitably modifying the OMS algorithm. However, unlike UCRL2 and OMS for which computation of optimistic model and respective (near-)optimal policy can be performed by EVI, this modified algorithm (as REGAL) relies on finding the solution to a constraint optimization problem, efficient computation of which is still an open problem.\n5. Regret analysis of the OMS strategy\nThe proof of Theorem 1 is divided into two parts. In Section 5.1, we first show that with high probability all Markovian state-representation models will collect sufficiently high reward according to the test in (2). This also means that the regret of any Markov model is not too large. This in turn is used in Section 5.2 to show that also the optimistic model employed by OMS (which is not necessarily Markov) does not lose too much with respect to an optimal policy in an arbitrary Markov model. In our proof we use analysis similar to (Jaksch et al., 2010) and (Bartlett & Tewari, 2009).\n5.1. Markov models pass the test in (2)\nAssume that \u03c6k,j \u2208 \u03a6 is a Markovmodel. We are going to show that \u03c6k,j will pass the test on the collected rewards in (2) of the algorithm at any step t w.h.p.\nInitial decomposition. First note that at time t when the test is performed, we have\u2211\ns\u2208Sk,j\n\u2211 a\u2208A vk,j(s, a) = \u2113k,j = t\u2212 tk,j + 1, so that\n\u2113k,j\u03c1k,j \u2212 t\u2211\n\u03c4=tk,j\nr\u03c4\n= \u2211\ns\u2208Sk,j\n\u2211\na\u2208A\nvk,j(s, a) ( \u03c1k,j \u2212 r\u0302tk,j :t(s, a) ) , (7)\nwhere r\u0302tk,j :t(s, a) is the empirical average reward collected for choosing a in s from time tk,j to the current time t in run j of episode k. Let r+k,j(s, a) be the optimistic rewards of the model M+tk,j (\u03c6k,j) under policy \u03c0k,j and P + k,j the respective optimistic transition matrix. Set vk,j := (vk,j(s, \u03c0k,j(s)))s \u2208 RSk,j , and let u+k,j := (u + tk,j ,\u03c6k,j\n(s))s \u2208 RSk,j be the state value vector given by EVI. By (5) and noting that vk,j(s, a) = 0 when a 6= \u03c0k,j(s) or s /\u2208 Sk,j , we get\n\u2113k,j\u03c1k,j \u2212 t\u2211\n\u03c4=tk,j\nr\u03c4 = \u2211\ns,a\nvk,j(s, a) ( \u03c1\u0302+k,j(\u03c6k,j)\u2212 r+k,j(s, a) )\n+ \u2211\ns,a\nvk,j(s, a) ( r+k,j(s, a)\u2212 r\u0302tk,j :t(s, a) )\n6 v\u22a4k,j ( P+k,j \u2212 I ) u+k,j\n+ \u2211\ns,a\nvk,j(s, a) ( r+k,j(s, a)\u2212 r\u0302tk,j :t(s, a) ) . (8)\nWe continue bounding each of the two terms on the right hand side of (8) separately.\nControl of the second term. Writing r(s, a) for the mean reward for choosing a in s (this is well-defined,\nsince we assume the model is Markov), we have\nr+k,j(s, a)\u2212 r\u0302tk,j :t(s, a) = ( r+k,j(s, a)\u2212 r\u0302tk,j (s, a) )\n+ ( r\u0302tk,j (s, a)\u2212 r(s, a) ) + ( r(s, a)\u2212 r\u0302tk,j :t(s, a) ) .\nThe terms of this decomposition are controlled. That is, using that M(\u03c6k,j) is an admissible model according to (4) with probability 1 \u2212 \u03b4tk,j (by applying the results of measure concentration in Appendix C.1 of (Jaksch et al., 2010) to the quantity r\u0302tk,j (s, a)), and the mere definition of r+k,j(s, a), and since Ntk(s, a) 6 Nt(s, a), we deduce that with probability higher than 1\u2212 \u03b4tk,j ,\n\u2211\ns,a\nvk,j(s, a) (( r+k,j(s, a)\u2212 r\u0302tk,j (s, a) )\n+ ( r\u0302tk,j (s, a)\u2212 r(s, a)\n))\n6 2 \u2211\ns,a\nvk,j(s, a)\u221a 2Ntk(s, a)\n\u221a log ( 2Sk,jAtk,j\n\u03b4tk,j\n)\n6 \u2211\ns,a\n\u221a 2vk,j(s, a) log ( 2Sk,jAtk,j\n\u03b4tk,j\n) . (9)\nOn the other hand, using again the results of measure concentration in Appendix C.1 of (Jaksch et al., 2010), and that vk,j(s, a) 6 Ntk(s, a) 6 tk,j , we deduce by a union bound over Sk,jAtk,j events that with probability higher than 1\u2212 \u03b4tk,j we get\n\u2211\ns,a\nvk,j(s, a) ( r(s, a)\u2212 r\u0302tk,j :t(s, a) )\n6 \u2211\ns,a\nvk,j(s, a)\u221a 2vk,j(s, a)\n\u221a log ( 2Sk,jAtk,j\n\u03b4tk,j\n)\n6 \u2211\ns,a\n\u221a 2vk,j(s, a) log ( 2Sk,jAtk,j\n\u03b4tk,j\n) . (10)\nControl of the first term. For the first term in (8), let us first notice that, since the rows of P+k,j sum to 1,( P+k,j \u2212 I ) u+k,j is invariant under a translation of the vector u+k,j . In particular, we can replace u + k,j with the quantity h+k,j , where\nh+k,j(s) := u + k,j(s)\u2212min { u+k,j(s) | s \u2208 Sk,j } .\nThen, we make use of the decomposition\nv\u22a4k,j ( P+k,j \u2212 I ) u+k,j = (11)\nv\u22a4k,j ( P+k,j \u2212Pk,j ) h+k,j + v \u22a4 k,j ( Pk,j \u2212 I ) h+k,j ,\nwhere Pk,j denotes the transition matrix corresponding to the MDP M(\u03c6k,j) under policy \u03c0k,j . Since\nboth matrices are close to the empirical transition matrix P\u0302tk,j at time tk,j , we can control the first term of this expression.\nFirst part of the first term. Indeed, since sp+k,j = \u2016h+k,j\u2016\u221e, we have for the first term in (11), using the decomposition p+k,j(\u00b7|s) \u2212 pk,j(\u00b7|s) = ( p+k,j(\u00b7|s) \u2212\np\u0302tk,j (\u00b7|s) ) + ( p\u0302tk,j (\u00b7|s)\u2212 pk,j(\u00b7|s) ) together with a concentration result and the definition of p+k,j , that with probability higher than 1\u2212 \u03b4tk,j v\u22a4k,j ( P+k,j \u2212Pk,j ) h+k,j (12)\n= \u2211\ns,a,s\u2032\nvk,j(s, a) ( p+k,j(s \u2032|s)\u2212 pk,j(s\u2032|s) ) h+k,j(s \u2032)\n6 \u2211\ns,a\nvk,j(s, a) \u2225\u2225p+k,j(\u00b7|s)\u2212 pk,j(\u00b7|s) \u2225\u2225 1 \u00b7 \u2225\u2225h+k,j \u2225\u2225 \u221e\n6 \u2211\ns,a\n2 vk,j(s, a)\n\u221a 2 log(2Sk,jSk,jAtk,j/\u03b4tk,j )\nNtk (s,a)\n\u2225\u2225h+k,j \u2225\u2225 \u221e\n6 2 sp+k,j\n\u2211\ns,a\n\u221a 2vk,j(s, a) log ( 2Sk,jSk,jAtk,j\n\u03b4tk,j\n) .\nSecond part of the first term. The second term of (11) can be rewritten using a martingale difference sequence. That is, let es \u2208 RSk,j be the unit vector with coordinates 0 for all s\u2032 6= s. Following (Jaksch et al., 2010) we set X\u03c4 := ( p(\u00b7|s\u03c4 , a\u03c4 )\u2212 e\u22a4s\u03c4+1 ) h+k,j and get\nv\u22a4k,j ( Pk,j \u2212 I ) h+k,j (13)\n=\nt\u2211\n\u03c4=tk,j\n( p(\u00b7|s\u03c4 , a\u03c4 )\u2212 e\u22a4s\u03c4 ) h+k,j\n= ( e\u22a4st+1 \u2212 e\u22a4stk,j + t\u2211\n\u03c4=tk,j\n( p(\u00b7|s\u03c4 , a\u03c4 )\u2212 e\u22a4s\u03c4+1 )) h+k,j\n=\nt\u2211\n\u03c4=tk,j\nX\u03c4 + h + k,j(st+1)\u2212 h+k,j(stk,j )\n=\nt\u2211\n\u03c4=tk,j\nX\u03c4 + u + k,j(st+1)\u2212 u+k,j(stk,j ) .\nNow the sequence {X\u03c4}tk,j6\u03c46t is a martingale difference sequence with\n|X\u03c4 | 6 \u2225\u2225p(\u00b7|s\u03c4 , a\u03c4 )\u2212 e\u22a4s\u03c4+1 \u2225\u2225 1 sp+k,j 6 2sp + k,j .\nThus, an application of Azuma-Hoeffding\u2019s inequality (cf. Lemma 10 and its application in Jaksch et al. (2010)) to (13) yields\nv\u22a4k,j ( Pk,j \u2212 I ) h+k,j\n6 2sp+k,j \u221a 2\u2113k,j log(1/\u03b4tk,j) + sp + k,j (14)\nwith probability higher than 1 \u2212 \u03b4tk,j . Together with (12) this concludes the control of the first term of (8).\nPutting all steps together. Combining (8), (9), (10), (11), (12), and (14), we deduce that at each time t of run j in episode k, any Markovian model \u03c6k,j passes the test in (2) with probability higher than 1\u2212 4\u03b4tk,j . Further, it passes all the tests in run j with probability higher than 1\u2212 4\u03b4tk,j2j ."}, {"heading": "5.2. Regret analysis", "text": "Next, let us consider a model \u03c6k,j \u2208 \u03a6, not necessarily Markovian, that has been chosen at time tk,j . Let t+1 be the time when one of the three stopping conditions in the algorithm (lines 12, 17, and 19) is met. Thus OMS employs the model \u03c6k,j between tk,j and t + 1, until a new model is chosen after the step t + 1. Noting that r\u03c4 \u2208 [0, 1] and that the total length of the run is (t + 1) \u2212 tk,j + 1 = \u2113k,j + 1 we can bound the regret \u2206k,j of run j in episode k by\n\u2206k,j := (\u2113k,j + 1)\u03c1 \u22c6 \u2212\nt+1\u2211\n\u03c4=tk,j\nr\u03c4\n6 \u2113k,j ( \u03c1\u22c6 \u2212 \u03c1k,j ) + \u03c1\u22c6 + \u2113k,j\u03c1k,j \u2212 t\u2211\n\u03c4=tk,j\nr\u03c4 .\nSince by assumption the test in (2) has been passed for all steps \u03c4 \u2208 [tk,j , t], we have\n\u2206k,j 6 \u2113k,j ( \u03c1\u22c6 \u2212 \u03c1k,j ) + \u03c1\u22c6 + lobk,j(t), (15)\nand we continue bounding the terms of lobk,j(t).\nStopping criterion based on the visit counter. Since \u2211\ns,a vk,j(s, a) = \u2113k,j 6 2 j, by Cauchy-Schwarz\ninequality \u2211\ns,a\n\u221a vk,j(s, a) 6 2 j/2 \u221a Sk,jA. Plugging\nthis into the definition (6) of lobk,j , we deduce from (15) that\n\u2206k,j 6 \u2113k,j ( \u03c1\u22c6 \u2212 \u03c1k,j ) + \u03c1\u22c6 (16)\n+sp+k,j + 2 j/2sp+k,jc(\u03c6k,j ; tk,j) + 2 j/2c\u2032(\u03c6k,j ; tk,j) .\nSelection procedure with penalization. Now, by definition of the algorithm, for any optimal Markov model \u03c6\u22c6 defined in the statement of Theorem 1, whenever M(\u03c6\u22c6) is admissible, i.e. M(\u03c6\u22c6) \u2208 Mtk,j ,\u03c6\u22c6 and was not eliminated during all runs before run j in episode k, we have \u03c1k,j \u2212 pen(\u03c6k,j ; tk,j) > \u03c1\u0302+k,j(\u03c6\u22c6)\u2212 pen(\u03c6\u22c6; tk,j) > \u03c1 \u22c6 \u2212 pen(\u03c6\u22c6; tk,j)\u2212 2t\u22121/2k,j , or equiva-\nlently\n\u03c1\u22c6 \u2212 \u03c1k,j 6 pen(\u03c6\u22c6; tk,j)\u2212 pen(\u03c6k,j ; tk,j) + 2t\u22121/2k,j 6 2\u2212j/2c(\u03c6\u22c6; tk,j) sp(u\n+ tk,j ,\u03c6\u22c6 )\n+2\u2212j/2c\u2032(\u03c6\u22c6; tk,j) + 2 \u2212jsp(u+tk,j ,\u03c6\u22c6) \u22122\u2212j/2c(\u03c6k,j ; tk,j) sp+k,j \u22122\u2212j/2c\u2032(\u03c6k,j ; tk,j)\u2212 2\u2212jsp+k,j + 2t \u22121/2 k,j . (17)\nNoting that \u2113k,j 6 2 j and recalling that when M(\u03c6\u22c6) is admissible, the span of the corresponding optimistic model is less than the diameter of the true model, i.e. sp(u+tk,j ,\u03c6\u22c6) 6 D \u22c6, see (Jaksch et al., 2010), and we obtain from (16), (17), and a union bound that\n\u2206k,j 6 \u03c1 \u22c6 +D\u22c6 + 2j/2D\u22c6c(\u03c6\u22c6; tk,j)\n+ 2j/2c\u2032(\u03c6\u22c6; tk,j) + 2 j+1t \u22121/2 k,j (18)\nwith probability higher than\n1\u2212 \u2211\nk\u2032,j\u2032;tk\u2032,j\u2032<tk,j\n4\u03b4tk\u2032,j\u2032 2 j\u2032 \u2212 2\u03b4tk,j . (19)\nThe sum in (19) comes from the event that \u03c6\u22c6 passes all tests (and is admissible) for all runs in all episodes previous to time tk,j , and 2\u03b4tk,j comes from the event that \u03c6\u22c6 is admissible at time tk,j . We conclude in the following by summing \u2206k,j over all runs and episodes.\nSumming over runs and episodes. Let Jk be the total number of runs in episode k, and let KT be the total number of episodes up to time T . Noting that c(\u03c6\u22c6; tk,j) 6 c(\u03c6 \u22c6;T ) and c\u2032(\u03c6\u22c6; tk,j) 6 c \u2032(\u03c6\u22c6;T ) as well as using that 2tk,j > 2 j (so that 2j+1t \u22121/2 kj 6\n2 \u221a 2 \u00b7 2j/2), summing (18) over all runs and episodes gives\n\u2206(\u03c6\u22c6, T ) =\nKT\u2211\nk=1\nJk\u2211\nj=1\n\u2206k,j 6 ( \u03c1\u22c6 +D\u22c6 ) KT\u2211\nk=1\nJk (20)\n+ ( D\u22c6c(\u03c6\u22c6;T ) + c\u2032(\u03c6\u22c6;T ) + 2 \u221a 2 ) KT\u2211\nk=1\nJk\u2211\nj=1\n2j/2,\nwith probability higher than 1 \u2212\u2211KTk=1 \u2211Jk j=1 4\u03b4tk,j2 j , where we used a union bound over all events considered in (19) for the control of all the \u2206k,j terms, avoiding redundant counts (such as the admissibility of \u03c6\u22c6 at time tk,j). Now, using the definition of \u03b4tk,j and the fact that 2tk,j > 2 j , we get that\n4\u03b4tk,j2 j =\n2j\u03b4\n9t2k,j 6\n2j\u03b4\n2tk,j(tk,j + 2j)\n= \u03b4 2tk,j \u2212 \u03b4 2(tk,j + 2j) 6\ntk,j+2 j \u22121\u2211\nt=tk,j\n\u03b4\n2t2 ,\nwhere the last inequality follows by a series-integral comparison, using that t 7\u2192 t\u22122 is a decreasing function. Thus, we deduce that the bound (20) is valid with probability at least 1\u2212\u2211\u221et=1 \u03b42t2 > 1\u2212\u03b4 for all T , and it remains to bound the double sum \u2211 k \u2211 j 2 j/2.\nFrom the number of runs... First note that by definition of the total number of episodes KT we must have\nT >\nKT\u2211\nk=1\nJk\u22121\u2211\nj=1\n2j =\nKT\u2211\nk=1\n( 2Jk \u2212 2 ) , (21)\nwhich implies also that we have the bound\nKT\u2211\nk=1\nJk\u2211\nj=1\n2j = 2\nKT\u2211\nk=1\n( 2Jk \u2212 2 ) + 2KT 6 2T + 2KT .\nFurther, by Jensen\u2019s inequality we get\nKT\u2211\nk=1\nJk\u22121\u2211\nj=1\n2j/2 6 \u221a\u2211KT\nk=1 Jk \u221a\u2211KT k=1 \u2211Jk j=1 2 j\n6 \u221a\u2211KT k=1 Jk \u221a 2T + 2KT . (22)\nNow, to bound the total number of runs \u2211KT\nk=1 Jk, using Jensen\u2019s inequality and (21), we deduce\nKT\u2211\nk=1\nJk =\nKT\u2211\nk=1\nlog2(2 Jk) 6 KT log2 ( 1 KT KT\u2211\nk=1\n2Jk )\n6 KT log2 ( T KT + 2 ) 6 KT log2 ( 2T KT ) , (23)\nand thus it remains to deal with KT .\n... to the number of episodes. First recall that an episode is terminated when either the number of visits in some state-action pair (s, a) has been doubled (line 17 of the algorithm) or when the test on the accumulated rewards has failed (line 12). We know that with probability at least 1\u2212 \u03b4 the optimal Markov model is not eliminated from \u03a6, while non-Markov models failing the test are deleted from \u03a6. Therefore, with probability 1 \u2212 \u03b4 the number of episodes terminated with a model failing the test is upper bounded by |\u03a6| \u2212 1. Next, let us consider the number of episodes which are ended since the number of visits in some stateaction pair (s, a) has been doubled. Let K(s, a) be the number of episodes which ended after the number of visits in (s, a) has been doubled, and let T (s, a) be the number of steps in these episodes. As it may happen that in an episode the number of visits is doubled in more than one state-action pair, we assume that K(s, a) and T (s, a) count only the episodes/steps where (s, a) is the first state-action pair\nfor which this happens. It is easy to see that K(s, a) 6 1 + log2 T (s, a) = log2 2T (s, a) for T (s, a) > 0. Then the bound \u2211 s\u2208S \u2211 a\u2208A log2 2T (s, a) on the total number of these episodes is maximal under the constraint\u2211 s\u2208S \u2211 a\u2208A T (s, a) = T when T (s, a) = T SA for all (s, a). This shows that the total number of episodes KT is upper bounded by\nKT 6 SA log2 ( 2T SA ) + |\u03a6| \u2212 1 (24)\nwith probability 1\u2212 \u03b4, provided that T > SA. Putting all steps together. Combining (20), (22) and (23) we get \u2206(\u03c6\u22c6, T ) 6( \u03c1\u22c6 + D\u22c6 ) KT log2 ( 2T KT ) + ( D\u22c6c(\u03c6\u22c6;T ) + c\u2032(\u03c6\u22c6;T ) +\n2 \u221a 2 )\u221a 2KT log2 ( 2T KT )( T +KT ) . Hence, by (24) and the definition of c, c\u2032, the regret of OMS is, with probability higher than 1\u2212 \u03b4, bounded by \u2206(\u03c6\u22c6, T ) 6 ( \u03c1\u22c6 +D\u22c6 )( SA+ |\u03a6| ) log22( 2T SA )\n+ ( 2D\u22c6 \u221a 2S\u22c6A log ( 2S \u22c6 24S\u22c6AT 3\n\u03b4\n) + 2D\u22c6 \u221a 2 log ( 24T 2\n\u03b4\n)\n+2 \u221a 2S\u22c6A log ( 48S\u22c6AT 3\n\u03b4\n) + 2 \u221a 2 )\n\u00d7 log2 ( 2T SA )(\u221a( SA+ |\u03a6| ) 2T+ ( SA+ |\u03a6| ) log2 ( 2T SA )) ,\nand we may conclude the proof with some minor simplifications."}, {"heading": "6. Outlook", "text": "The first natural question about the performance guarantees obtained is whether they are optimal. We know from the corresponding lower-bounds for learning MDPs (Jaksch et al., 2010) that the dependence on T we get for OMS is indeed optimal. Among other parameters, perhaps the most important one is the number of models |\u03a6|; here we conjecture that the\u221a |\u03a6| dependence we obtain is optimal, but this remains to be proven. Other parameters are the size of the action and state spaces for each model; here we lose with respect to the precursor BLB algorithm (see the remark after Theorem 1), and thus have room for improvement. It may be possible to obtain a better dependence for OMS at the expense of more sophisticated analysis. Note, however, that so far there are no known algorithms for learning even a single MDP that would have known optimal dependence on all these parameters. Another important direction for future research is infinite sets \u03a6 of models; perhaps, countably infinite sets is the natural first step, with separable \u2014 in a suitable sense \u2014 continuously-parametrized general classes of models being a foreseeable extension. A problem with\nthe latter formulation is that one would need to formalize the notion of a model being close to a Markovian model and quantify the resulting regret."}, {"heading": "Acknowledgments", "text": "This work was supported by the French National Research Agency (ANR-08-COSI-004 project EXPLORA), by the European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement 270327 (CompLACS) and 216886 (PASCAL2), the Nord-Pas-de-Calais Regional Council and FEDER through CPER 2007-2013, the Austrian Science Fund (FWF): J 3259-N13, and the Australian Research Council Discovery Project DP120100950, NICTA."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order O(T ) with an additive term constant yet exponential in some characteristics of the optimal MDP. We propose an algorithm whose regret after T time steps is O( \u221a T ), with all constants reasonably small. This is optimal in T since O( \u221a T ) is the optimal regret in the setting of learning in a (single discrete) MDP.", "creator": "LaTeX with hyperref package"}}}