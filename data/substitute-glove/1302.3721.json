{"id": "1302.3721", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2013", "title": "Thompson Sampling in Switching Environments with Bayesian Online Change Point Detection", "abstract": "Thompson Sampling already recently seen these but it rational in first Bernoulli Multi - Armed Bandit the [Kaufmann internationale al. , 1988 ]. This pkk something depends static matlab for the benefits. It exists describe adamant not newest into coming biggest of into trains instance. In so company wo preference now appropriate algorithms typically Thompson Sampling brought kind Switching Multi - Armed Bandit Problem. We propose a Thompson Sampling shaping prototype close close Bayesian would came specific came tackles but problem. We well algorithms would old related however number together constant switching spending: when machines infection all ties change (Global Switching ), easier usually operate for least stepping (Per - Arm Switching ), going making communication expected considered known with without it actually be variables leaving calculations. This goal allowed makes children large algorithms we ones reduction Change - Point Thompson Sampling (CTS ). We watching evolutionary doubt of the algorithm taken 4 artificial habitat, only 47 name some better decade moreover; friday ipod - work [Yahoo! , 1965] for foreign exchange tracking [Dukascopy, 1976 ], comparing them alone. primarily bandit approximations. In simply year signal CTS is the which effective.", "histories": [["v1", "Fri, 15 Feb 2013 10:48:57 GMT  (324kb,D)", "http://arxiv.org/abs/1302.3721v1", "A version will appear in the Sixteenth international conference on Artificial Intelligence and Statistics (AIStats 2013)"]], "COMMENTS": "A version will appear in the Sixteenth international conference on Artificial Intelligence and Statistics (AIStats 2013)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joseph mellor", "jonathan shapiro"], "accepted": false, "id": "1302.3721"}, "pdf": {"name": "1302.3721.pdf", "metadata": {"source": "CRF", "title": "Thompson Sampling in Switching Environments with Bayesian Online Change Point Detection", "authors": [], "emails": [], "sections": [{"heading": null, "text": "We show empirical results of the algorithm in 4 artificial environments, and 2 derived from real world data; news click-through[Yahoo!, 2011] and foreign exchange data[Dukascopy, 2012], comparing them to some other bandit algorithms. In real world data CTS is the most effective."}, {"heading": "1 Introduction", "text": "Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting [Kaufmann et al., 2012]. This bandit problem assumes stationary distributions for the rewards. It is often unrealistic to model the real world as a stationary distribution and algorithms such AdaptEvE have been proposed to solve bandit problems in this environment. In this paper we concern ourselves with a Switching Multi-Armed Bandit Problem.\nWe first review Thompson Sampling, before describing the non-stationary environment we are concerned with. We then propose a method to solve this problem and review the techniques we employ. We then test our algorithms on a variety of environments.\nar X\niv :1\n30 2.\n37 21\nv1 [\ncs .L\nG ]\n1 5"}, {"heading": "1.1 Thompson Sampling", "text": "Thompson Sampling is effectively a probability matching algorithm. The desired strategy is to pull an arm with the probability that the arm is the best arm. This probability can be written as\nP (ai = a \u2217) = \u222b \u03b8 I(ai = a \u2217|\u03b8)P (\u03b8|D)d\u03b8, (1)\nwhere \u03b8 is a model of our arms, a\u2217 is the optimal arm, ai is the i th arm and D is the history of past rewards. I(x) is the indicator function, which is 1 when x is true, and 0 otherwise. The strategy can thus be reduced to sampling from the model distribution P (\u03b8|D) and then picking the arm that is maximal given this model.\nThis paper considers multi-armed bandits with Bernoulli arms. Each arm, j, delivers a reward of 1 with probability \u03b8j and 0 otherwise. In the stationary case the parameter is \u03b8 = (\u03b81, . . . , \u03b8k) where k is the number of arms. The arms are assumed independent and so P (\u03b8|D) is a product of terms P (\u03b8j |Dj), where Dj are past rewards for are j. Further, since the arms are Bernoulli, for which the Beta distribution is a conjugate prior, we can write P (\u03b8) as a product of Beta distributions;\nP (\u03b8|D) = k\u220f j=1 P (\u03b8j |\u03b1j , \u03b2j , Dj) (2)\nwhere \u03b1j = #{reward = 1}+ \u03b10 and \u03b2j = #{reward = 0}+ \u03b20. We can sample from P (\u03b8|D) by sampling from all P (\u03b8j |\u03b1j , \u03b2j , Dj) and choosing the arm, j, with largest \u03b8j ."}, {"heading": "1.2 Model of Dynamic Environment", "text": "In this paper we assume that the environment changes over time. We assume abrupt switching defined by a hazard function, h(t), such that,\n\u03b8i(t) =\n{ \u03b8i(t\u2212 1) with probability h(t)\n\u03b8new \u223c U(0, 1) 1\u2212 h(t). (3)\nThe algorithms presented are designed with 2 such models in mind. The first model we will refer to as the Global Switching model. This model switches at a constant rate, when a change point happens all arms change their expected rewards. The second model will be referred to as Per-Arm Switching. In this model change points occur independently for each arm, such that when the expected reward switches for one arm, it is uncorrelated to when all other arms switch.\nExamples of this form of changing environment might include stock market data where stock prices can change their statistical nature very quickly subject to external events. Switching behaviour has been studied in Financial Markets [Preis et al., 2011], and multi-armed bandits have been applied to this field before [Sorensen, 2007]."}, {"heading": "1.3 Regret for Switching Environments", "text": "In a stationary multi-armed bandit the regret measure we use is taken to be the expected difference in reward between our strategy and that of a policy which always chose the arm with highest expected payoff, \u00b5\u2217. In a switching system the arm which is considered optimal changes. In this case a different form of regret is often used. Garivier and Moulines [2008] use the following definition of regret.\nR(T ) =< T\u2211 t=0 (\u00b5\u2217t \u2212 \u00b5it) >, (4)\nwhere \u00b5\u2217t is the highest expected payoff of an arm at time t and where < . > denotes expectation over possible sequences of {\u00b5it}. Unless otherwise stated our experiments will report a related quantity,\nRn(T ) =< T\u2211 t=0 I(\u00b5\u2217t 6= \u00b5it) >, (5)\nwhich is the expected number of times a suboptimal arm is pulled. This corresponds to the results Hartland et al. [2007] report."}, {"heading": "2 Switching Thompson Sampling", "text": "In order to perform Thompson Sampling we wish to sample from P (\u03b8|Dt\u22121), which is the probability of the arm model given the data so far. In a switching system the arms model \u03b8 is only dependent on the data since the last switching occurred, but we do not know when this happened. If we did we could just do the same Bayesian update as with the standard Bernoulli case to arrive at the distribution of our model. Since we do not know the runlength rt we can introduce it as a latent variable and marginalise it out. Taking Dt\u22121 as the history of rewards and arm pulls seen so far, we can write this as\nP (\u03b8|Dt\u22121) = \u2211 rt\nP (\u03b8|Dt\u22121, rt)\ufe38 \ufe37\ufe37 \ufe38 posterior of model given data\nprobability of runlength\ufe37 \ufe38\ufe38 \ufe37 P (rt|Dt\u22121) . (6)\nNow to sample from P (\u03b8|Dt\u22121) we just need to sample from the P (rt|Dt\u22121) (the runlength distribution) and then given that runlength, sample from P (\u03b8|Dt\u22121, rt) to arrive at our arm model \u03b8. We can select the arm to pull that in expectation maximises the reward given this model."}, {"heading": "3 Bayesian Online Change Detection", "text": "Fearnhead and Liu [2007] as well as Adams and MacKay [2007] have independently done work on calculating the online posterior of the runlength. They show exact inference on the runlength can be achieved by a simple message passing algorithm. Letting xt be the reward at time t so that Dt = xt \u222aDt\u22121. The inference procedure can be easily derived as follows.\nP (rt|xt\u22121, Dt\u22122) = P (rt, xt\u22121, Dt\u22122)\nP (xt\u22121, Dt\u22122) (7)\nThe numerator can then be expressed as\nP (rt, xt\u22121, Dt\u22122) = \u2211 rt\u22121 P (rt, rt\u22121, xt\u22121, Dt\u22122) (8)\n= \u2211 rt\u22121 P (rt, xt\u22121|rt\u22121, Dt\u22122)P (rt\u22121, Dt\u22122) (9) = \u2211 rt\u22121 switching rate\ufe37 \ufe38\ufe38 \ufe37 P (rt|rt\u22121) reward likelihood\ufe37 \ufe38\ufe38 \ufe37 P (xt\u22121|rt\u22121, Dt\u22122)P (rt\u22121, Dt\u22122). (10)\nThe derivation just applies the rules of probability up to and including equation 9. One assumption is made in equation 10, that the runlength is only dependent on the previous time steps runlength. This forms a simple message passing algorithm because rt can only take values depending on rt\u22121. In fact rt = rt\u22121 + 1 when switching does not occur and rt = 0 when it does. P (rt|rt\u22121) is defined by a hazard function h(t). For simplicity in our case this is a constant switching rate \u03b3.\nUnfortunately the exact inference has space and time requirements that grow linearly in time. The space requirements are linear because at each time step the support set of the posterior runlength distribution increases by one, which means we have to store information for an extra value of the runlength at every step. The update is also linear in time, as the message passing algorithm requires an update to each runlength in the support. Adams and MacKay suggest a simple thresholding technique to eliminate runlengths with small probability mass associated with them. As we can only know in expectation how much memory this algorithm will require, an alternative with hard guarantees on memory requirements is desirable. Fearnhead and Liu suggest a much more sophisticated particle filter resampling step to maintain a finite sample of the runlength distribution, which has the benefit that we can be certain on the upper limit of space the algorithm requires, this approach is the one taken in this paper."}, {"heading": "3.1 Particle Filters", "text": "A particle filter is a Monte-Carlo method for approximately estimating a sequential Bayesian model. Particles are used to represent points in the distribution to be estimated and are assigned weights that correspond to their approximate probabilities. The number of particles can grow at each time step and so occasionally some particles need to be thrown away. This leaves us to assign new weights to the remaining particles. This procedure is called resampling."}, {"heading": "3.1.1 Stratified Optimal Resampling", "text": "Fearnhead and Clifford [2003] originally proposed optimal resampling. We wish to reduce a discrete probability distribution with a support of N discrete points down to a stochastic distribution of M discrete points, where the set of M points is a subset of the original N . The original N points each have probability mass pi associated with them, and the procedure finds a reweighting of these probabilities, qi such that N \u2212M of the probabilities are 0. The idea is that we wish there to be no bias in the sampling procedure, which means that the expected value of qi should be the original probability mass pi. The algorithm is optimal in the sense that the expected squared difference between the original probabilities pi and the new weights qi are minimised.\nThis can be done by the following procedure;\n1. Find \u03ba such that M = \u2211N i=1 min(1, pi/\u03ba)\n2. Sample u from uniform distribution, U(0, \u03ba)\n3. Iterate through all pi\n(a) If pi > \u03ba Then qi = pi\n(b) Otherwise\ni. u = u\u2212 pi ii. If u < 0 Then qi = \u03ba and u = u+ \u03ba\niii. Otherwise qi = 0\nThe particles where pi = qi are kept with probability 1. The remaining particles are such that qi = \u03ba with probability pi/\u03ba and qi = 0 otherwise. Thus their expectation remains the same.\nThe worstcase time complexity of this algorithm is O(N logN), but it has an amortised cost of O(N) [Fearnhead and Clifford, 2003]."}, {"heading": "4 Proposed Inference Models", "text": "We have shown we can perform Thompson Sampling in a switching system by splitting the procedure into a stage that samples the runlength since a switch occurred and a stage that samples from the arm model given this runlength.\nThe Global Switching and Per-Arm Switching models are appealing due to their simplicity. Only one runlength distribution needs to be inferred for Global Switching, which does not depend on the number of arms the bandit has. The Per-Arm model can store the runlengths for each arm independent, and the space requirements grow linearly with respect to the number of arms. Other models with more complicated dependencies between arms can quickly become intractable."}, {"heading": "4.1 Global switching", "text": "In global switching there is a single change point process across all of the arms since when one arm switches distribution so do all other arms. This means that the data from every arm pull contributes to the posterior of the single runlength distribution. Effectively to sample from the posterior of the full bandit model, we first need to sample from the runlength distribution, this gives us an estimate of the runlength, which tells us how much data from the past our arms can use. Once the global runlength is sampled, we then proceed by sampling individually from the posterior distributions of the arms, given only the data since the last changepoint (determined by the runlength). The arm with the corresponding maximum sample is then pulled. We only need to store the posterior probabilities of the given runlengths and the hyperparameters for the arm posteriors associated with those runlengths. We will call the runlength distribution the Change Point model, and the set of hyperparameters associated with each runlength for a given action the Arm model. The Change Point model is an approximation of the runlength distribution storing a probability for at most N runlengths. Let wti be the probability of having a runlength of i at time t. In this paper the arm rewards are assumed to come from a Bernoulli distribution so the hyperparameters stored are the 2 parameters for the Beta distribution. Let \u03b1ti,j and \u03b2 t i,j be the hyperparameters for a runlength of i at time t for arm j. At any point in time t there is a set of runlengths Rt \u2282 N, |Rt| \u2264 N , where for every r \u2208 Rt there exists quantities wtr, \u03b1 t r,j and \u03b2 t r,j . When |Rt| = N then a resampling step is performed in order to reduce the number of runlengths stored. For ease of notation let {w}t be the set of runlength probabilities at time t and let {\u03b1}tj and {\u03b2}tj be the sets of hyperparameters for arm j at time t. Similarly let {\u03b1}t and {\u03b2}t be the set of all hyperparameters at time t. The algorithm is presented in pseudocode in figure 1.\nWe will refer to this algorithm as Global Change-Point Thompson Sampling (Global-CTS)."}, {"heading": "4.2 Per-arm switching", "text": "The difference in implementation with respect to global switching is that now there is a runlength distribution for each arm. That is, for each arm j we have a different set of runlength probabilities wti,j \u2208 {w}tj . In the per-arm switching model at a timestep t we update the Change Point model associated with the\nAlgorithm 1 Global Change-Point Thompson Sampling procedure Global-CTS(N, \u03b3, \u03b10 = 1, \u03b20 = 1)\nt\u2190 0 . Initialise time wt0 \u2190 1, and add to {w} t . Initialise runlength distribution For all arms j, \u03b1t0,j \u2190 \u03b10 . Initialise hyperparameters For all arms j, \u03b2t0,j \u2190 \u03b20 while Interacting do\na\u2190 SelectAction({w}t, {\u03b1}t, {\u03b2}t) r \u2190 PullArm(a) {w}t+1 \u2190 UpdateChangeModel({w}t, {\u03b1}ta, {\u03b2} t a, a, r), \u03b3) {\u03b1}t, {\u03b2}t \u2190 UpdateArmModels({\u03b1}t, {\u03b2}t, a, r) if \u2223\u2223{w}t+1\u2223\u2223 = N then ParticleResample({w}t+1, {\u03b1}t+1, {\u03b2}t+1) end if t\u2190 t+ 1\nend while end procedure\nprocedure UpdateChangeModel({w}t, {\u03b1}ta, {\u03b2} t a, a, r, \u03b3)\nif r = 1 then\nlikelihoodi \u2190 \u03b1ti,a\n\u03b1t i,a +\u03b2t i,a\n, For all i s.t. wti \u2208 {w} t\nelse\nlikelihoodi \u2190 \u03b2ti,a\n\u03b1t i,a +\u03b2t i,a\n, For all i s.t. wti \u2208 {w} t\nend if wt+1i+1 \u2190 (1\u2212 \u03b3) \u2217 likelihoodi \u2217 w t i , For all i s.t. w t i \u2208 {w} t\nwt+10 \u2190 \u2211 i \u03b3 \u2217 likelihoodi \u2217 w t i Normalise {w}t+1\nreturn{w}t+1 end procedure\nprocedure UpdateArmModels({\u03b1}t, {\u03b2}t, a, r) if r=1 then\n\u03b1t+1i+1,a \u2190 \u03b1 t i,a + 1, For all i s.t. \u03b1 t i,a \u2208 {\u03b1} t a\nelse \u03b2t+1i+1,a \u2190 \u03b2 t i,a + 1, For all i s.t. \u03b2 t i,a \u2208 {\u03b2} t a end if \u03b1t+10,j \u2190 \u03b10 , For all arms j . Set Prior for runlength 0 \u03b2t+10,j \u2190 \u03b20 , For all arms j return{\u03b1}t+1, {\u03b2}t+1\nend procedure procedure ParticleResample({w}t+1, {\u03b1}t+1, {\u03b2}t+1)\nFind set to discard d \u2208 D using Stratified Optimal Resampling on {w}t+1 Discard all wt+1d , \u03b1 t+1 d , \u03b2 t+1 d\nend procedure\nprocedure SelectAction({w}t, {\u03b1}t, {\u03b2}t) Pick i with probability wti for each arm j do\nsamplej \u2190 Beta(\u03b1ti,j , \u03b2 t i,j)\nend for returnmaxj samplej\nend procedure\narm that was pulled at t much like via the update equations sketched in 10. The Change Point models associated with arms not pulled at t are updated differently since the runlength for these arms is independent of the reward we received for the arm we actually pulled. The reward likelihood term disappears in the update equations for the runlength distribution of unpulled arms. This\nis shown in equation 13. Since we normalise the distribution at each step we can ignore the factor P (xt\u22121). This is shown as follows,\nP (rt, xt\u22121, Dt\u22122) = P (xt\u22121)P (rt, xt\u22121, Dt\u22122) (11) \u221d \u2211 rt\u22121 P (rt, rt\u22121, Dt\u22122) (12)\n\u221d \u2211 rt\u22121 P (rt|rt\u22121, Dt\u22122)P (rt\u22121, Dt\u22122). (13)\nWe will refer to this algorithm as Per-Arm Change-Point Thompson Sampling (PA-CTS)."}, {"heading": "5 Learning the Switching Rate", "text": "Both Wilson et al. [2010] and Turner et al. [2009] have proposed methods for learning the hazard function from the data. Wilson et al. method can learn a hazard function that is piecewise constant via a hierarchical generative model. Turner et al. can learn any parametric hazard rate via gradient descent, but from initial investigations appeared to not perform particularly well if the hazard rate is adapted at every time step. For the purposes of this paper, a constant switching rate was assumed which was learned using the approach of Wilson et al. For the simplest case where we consider a single constant switch rate, Wilson et al. model whether a change point occurred as a Bernoulli variable. The hyperparameters of this switching rate are those of a Beta distribution and can be thought of as the number of times the system has switched, at and the number of times it not switched bt. We now compute the joint distribution P (rt, at|xt\u22121, Dt\u22122) as oppose to the original distribution P (rt|xt\u22121, Dt\u22122). The message passing proceeds in a very similar fashion as before, except now the number of particles also grows quadratically rather than linearly. In the global switching model the algorithm now keeps track of sets of particles wtr,a, \u03b1 t r,i,a and \u03b2 t r,i,a associated with a runlenght r, learning rate hyperparam-\neter a, arm i and time t. The updates are as follows.\nw00,0 \u2190 1\nwt+1r+1,a \u2190 t\u2212 a+ 1 t+ 2 \u03b1tr,i,a \u03b1tr,i,a + \u03b2 t r,i,a wtr,aif reward = 1\nwt+1r+1,a \u2190 t\u2212 a+ 1 t+ 2 \u03b2tr,i,a \u03b1tr,i,a + \u03b2 t r,i,a wtr,aif reward = 0\nwt+1r,a+1 \u2190 a+ 1\nt+ 2 \u03b1tr,i,a \u03b1tr,i,a + \u03b2 t r,i,a wtr,aif reward = 1\nwt+1r,a+1 \u2190 a+ 1\nt+ 2 \u03b2tr,i,a \u03b1tr,i,a + \u03b2 t r,i,a wtr,aif reward = 0\nWe again use the resampling algorithm of Fearnhead to manage the space requirements of the algorithm.\nIn the Global Switching model there is only 1 runlength distribution, and so only 1 switching rate to learn, this leads naturally to an algorithm NonParametric Global Change-Point Thompson Sampling (NP Global-CTS). With Per-Arms there are many possibilities, there could be a single switching rate for each of the independent arms, or each arm could have a separate switching rate. In this paper we assume each arm has a separate switching rate and call this algorithm Non-Parametric Per-Arm Change-Point Thompson Sampling (NP PA-CTS)."}, {"heading": "6 Tracking Changes In The Best Arm", "text": "The algorithms presented so far attempt to track changes in all arms, irrespective of whether they are pulled. The distribution of \u03b8i for an arm not pulled will slowly become flatter and thus have higher variance. One of the stated assumptions of Adapt-EvE, was that it was only important to track whether the perceived best arm has changed distribution. We can modify the algorithms to better replicate this assumption. The model for the Per-Arm Switching is adapted most simply. Since both the Change Point models and the Arm models are independent for each arm, we can track the change of the best arm by only updating the Change Point and Arm models for the arm that was pulled. For the Global Switching model the change is not so clear since all arms share a Change Point model. In this paper we updated the shared Change Point model and only updated the Arm model for the arm pulled. The question then arises what should the hyperparameters for the unpulled arms be that are associated with the new runlength of zero. The approach taken here was to set them to the hyperparameters associated with a random other runlength in the distribution. We can apply the sample method from Wilson to infer the switching rate for\nthese architectures as well. The algorithms described in this section with be denoted by having a \u201c2\u201d appended to the algorithm name."}, {"heading": "7 Experiments", "text": "6 different non-stationary environments were used to evaluate our bandit model. 4 are based on purely synthetic data, and 2 use data collected from the real world. The parameters for all experiments shown were tuned based on the PASCAL challenge. Bold denotes best results in all tables."}, {"heading": "7.1 Global Switching Environment", "text": "We first compare the algorithms in an environment with a constant global switching rate. Global-CTS and NP Global-CTS were designed for this environment and so a-priori we would expect them to perform the best.\nThe first set of experiments were a single run of the algorithms working in an instance of this environment type with 2 arms. Figures 1 and 2 plot example heatmaps of the runlength distributions of some of the algorithms. At a particular time, the graphs show the runlength distribution. In the case of the PA-CTS and NP PA-CTS there are 2 plots for each algorithm, corresponding to the runlength distribution for each arm. The pay off of the 2 arms has been superimposed over the top of the plots so that it can be seen how the runlength distribution matches up with the changes in the environment From the heatmap figures we can see the change point prediction works when applied to a bandit problem. As expected the change point distribution looks to be more accurate for the Global-CTS and NP Global-CTS algorithms which use the Global Switching model, this is because each data point can contribute to the posterior runlength distribution. The PA-CTS also performs reasonably well even though the amount of data that has influence on each posterior is reduced. For the NP PA-CTS algorithm, learning the separate switching rates appears to significantly decrease the certainty for a particular runlength.\nAn experiment comparing the algorithms in this setting was performed. Each run was over a period of 106 time steps and the experiment was repeated 100 times. The results are displayed in table 1. All parameters were set as for the PASCAL challenge test run. The environment constant switching rate was 10\u22124. Global-CTS performs the best in the environment, which is not surprising since the environment fits the algorithms model. NP Global-CTS performs well in this too, which suggests that learning the hazard rate for this model may be feasible.\n0 2000 4000 6000 8000 10000 0\n500\n1000\n1500\n2000\n2500 3000 Ru nl en gt h\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 M ea n Pa yo ff\nGlobal-CTS Runlength Distribution\n0 2000 4000 6000 8000 10000Time 0\n500 1000 1500 2000 2500 3000 3500 Ru nl en gt h\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 M ea n Pa yo ff\nNon-Parametric Global-CTS Runlength Distribution\nFigure 1: Runlength Distribution for Global-CTS and NP Global-CTS in Global Switching Environment. The mean payoffs of the arms are super-imposed over the distribution."}, {"heading": "7.2 Per-Arm Switching Environment", "text": "The next environment was a switching system where the switching for each arm was independent of every other arm. PA-CTS and NP PA-CTS were designed with this situation in mind and again a-priori may be expected to perform better.\nAn experiment comparing the algorithms was performed with 106 iterations and then repeated 100 times. The results are shown in table 2. As expected the PA-CTS algorithm performs best in this environment. NP PA-CTS, the algorithm corresponding to PA-CTS that learns the hazard rate suffers much more regret, which would appear to indicate for the particular model the parameters are not being learned quickly enough. The algorithms designed for a Global Switching environment also perform reasonably in this sort of environment."}, {"heading": "7.3 Bernoulli Armed Bandit with Random Normal Walk", "text": "The PASCAL challenge environments were found to be periodic, which is not the sort of environment our algorithms were intended for. Another simulated environment was investigated. In this environment at time,t, each arm, i, was Bernoulli with probability of success \u03b8i(t). At each time step the success rate of the arm was allowed to drift as a truncated normal walk. That is the probability of success for an arm \u03b8i(t) \u2208 [0, 1] conditional on \u03b8i(t\u2212 1) \u2208 [0, 1] is,\nP (\u03b8i(t)|\u03b8i(t\u2212 1)) = e\n\u2212(\u03b8i(t\u22121)\u2212\u03b8i(t)) 2\n\u03c32\n\u221a 2\u03c0\u03c3 \u222b 1 0 e \u2212(\u03b8i(t\u22121)\u2212x)2 \u03c32\u221a 2\u03c0\u03c3 dx . (14)\nTable 3 shows an comparison of the algorithms. The experiment was run 100 times, where each run had a period of 106. The variance of the random walk was set to \u03c32 = 0.03.\nIn this sort of environment it appears that our algorithms perform better than the benchmark algorithms with NP PA-CTS2 achieving the smallest regret."}, {"heading": "7.4 PASCAL Challenge 2006", "text": "The PASCAL Exploration vs. Exploitation Challenge 2006 was a competition in a multi-armed bandit problem[Hussain et al., 2006]. The challenge revolved around website content optimisation, whereby the options available corresponded to different content to present to a user on a website. The challenge is a good general test for the algorithms presented in this paper as to perform well it was required for the bandit algorithms to be able to work in non-stationary environments. The challenge had 6 separate environments in which the algorithms needed to perform; Frequent Swap (FS), Long Gaussians (LG), Weekly Variation (WV), Daily Variation (DV), Weekly Close Variation (WCV) and Constant (C). These environments are artificially generated, where the dynamics of the expected payoffs resemble either periodic Gaussian, Sinusoidal or constant signals. Hartland et al. [2007] won this competition with the Adapt-EvE algorithm. The Adapt-EvE algorithms most prominent feature is its use of a change-point detection mechanism. Since the algorithms presented in this paper also use a change-point mechanism it is interesting to compare their performance. The challenge also provides an environment for which the algorithm was not directly designed for and so will hopefully indicate some robustness in their strategy. We were unable to implement a version of Adapt-EvE that replicated the performance reported, so here we are simply replicating the results published. To avoid an unfair comparison in other environments we did not run our own implementation of Adapt-EvE in those environments.\nTable 4 shows a comparison of the Change-Point Thompson Sampling algorithms (Global-CTS, PA-CTS, NP Global-CTS NP PA-CTS) against AdaptEvE Meta-Bandit and Meta-p-Bandit [Hartland et al., 2007]. The comparison also features the algorithm \u201cDiscountedUCB\u201d, which was submitted by Thomas\nJaksch to the same competition and performed comparably to Adapt-EvE. The code for this algorithm was available and so has been included for comparison in all other environments."}, {"heading": "7.5 Yahoo! Front Page Click Log Dataset", "text": "Yahoo! [2011] have produced a bandit algorithm dataset. The dataset provides information about the top story presented to a user on the front page of Yahoo!. Each entry in the dataset gives information about a single article presented, the time it was presented, contextual information about the user and whether the\nuser \u201cclicked-through\u201d to the article or not. The dataset was designed for the contextual bandit problem. Given context of a user the goal is to select an article to present to the user as to maximise the expected rate at which users click on the article to read more (click-through). The articles also change during the dataset, and so bandit algorithms designed specifically for this environment also need the ability to modify the number of arms they can select from. For the purposes of our experiments we do not concern ourselves with the contextual case, nor do we try to incorporate new articles as they arrive. Instead we ignore the context, and we only pick from a set number of articles. This reduces the problem to a conventional multi-armed bandit problem. To maximise the amount of data used, for each run we randomly selected the set of articles (in our case 5 articles) from a list of 100 permutations of possible articles which overlapped in time the most. The click-through rates were estimated from the data by taking the mean of an articles click-through rate every 1000 time ticks. The simulation then proceeded as described by Li et al. [2011], the results are presented in table 5. The regret for each run was normalised by the number of arm pulls, since this was different in each run of the simulation. Parameters were set as for the PASCAL challenge dataset."}, {"heading": "7.6 Foreign Exchange Rate Data", "text": "We constructed a final test environment from Foreign Exchange Rate data[Dukascopy, 2012]. Ask prices for 4 currency exchange rates (GBP-USD, USD-JPY, NZDCHF, EUR-CAD) at a resolution of 2 minutes spanning 7 years were used. This amounted to approximation 106 datapoints per exchange rate pair. The bandit problem using this data was set up as follows. Each exchange rate was thought of as a 2-armed bandit. It was imagined that the agent could make fictitious trades, and could either decide to buy a long call option (if they believe the rate will increase) and a short call option (if they believe the rate will go down). To turn this into a Bernoulli bandit problem, we ignore the scale of the change and provide a reward of 1 if the bandit predicted correctly the rate going up/down and 0 otherwise. When the rate remains the same, the agent receives a reward of 0 irrespective of their decision. For the purpose of the experiment we imagine the option length is 100 time ticks, so that the agent has to decide if the exchange rate will increase or decrease in 100 time ticks. Although this bandit scenario is not true to life, we believe that the underlying data should exhibit\nsome of the characteristics of a switching system for which the algorithms were designed [Preis et al., 2011]. We can not estimate a \u201ctrue\u201d average payoff at each timestep, and so can not measure the regret of these algorithms, instead we report the error. The results are shown in table 6."}, {"heading": "8 Conclusion", "text": "This paper has explored several algorithms using Thompson Sampling in conjunction with Change Point detection. We have shown that they perform well in the environments for which they are designed. Bandit scenarios based on real-world data such as the Yahoo! dataset and the Foreign Exchange also demonstrate their performance. They are shown not to perform as well as appropriately tuned competing algorithms in the PASCAL challenge. However our results suggest that a strategy that just tracks changes in the perceived best arm (Global-CTS2,PA-CTS2), similar to Adapt-EvE, works well. Since the model is extremely modular it is hoped that further assumptions can be incorporated into the model to improve performance. It is also worth noting that non-Bernoulli payoffs can just as easily be used, e.g. Normal distributed payoffs. A Bayesian approach also avoids difficulties that arise with handling false alarms in change point detection schemes. The algorithms have been derived from simple models and so are theoretically motivated, however steps still need to be taken to provide any theoretic justification for their performance."}], "references": [{"title": "Bayesian online changepoint detection", "author": ["Ryan Prescott Adams", "David J.C. MacKay"], "venue": null, "citeRegEx": "Adams and MacKay.,? \\Q2007\\E", "shortCiteRegEx": "Adams and MacKay.", "year": 2007}, {"title": "On-line inference for hidden Markov models via particle filters", "author": ["Paul Fearnhead", "Peter Clifford"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Fearnhead and Clifford.,? \\Q2003\\E", "shortCiteRegEx": "Fearnhead and Clifford.", "year": 2003}, {"title": "On-line inference for multiple change points problems", "author": ["Paul Fearnhead", "Zhen Liu"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Fearnhead and Liu.,? \\Q2007\\E", "shortCiteRegEx": "Fearnhead and Liu.", "year": 2007}, {"title": "On Upper-Confidence Bound Policies for NonStationary Bandit Problems", "author": ["A. Garivier", "E. Moulines"], "venue": "ArXiv e-prints,", "citeRegEx": "Garivier and Moulines.,? \\Q2008\\E", "shortCiteRegEx": "Garivier and Moulines.", "year": 2008}, {"title": "Change Point Detection and Meta-Bandits for Online Learning in Dynamic Environments", "author": ["C\u00e9dric Hartland", "Nicolas Baskiotis", "Sylvain Gelly", "Mich\u00e8le Sebag", "Olivier Teytaud"], "venue": "CAp, pages 237\u2013250,", "citeRegEx": "Hartland et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hartland et al\\.", "year": 2007}, {"title": "Exploration vs. exploitation pascal challenge", "author": ["Z. Hussain", "P. Auer", "N. Cesa-Bianchi", "L. Newnham", "J. Shawe-Taylor"], "venue": "http://pascallin.ecs.soton.ac.uk/Challenges/EEC/,", "citeRegEx": "Hussain et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hussain et al\\.", "year": 2006}, {"title": "Thompson sampling: An optimal finite time analysis", "author": ["Emilie Kaufmann", "Nathaniel Korda", "Rmi Munos"], "venue": "CoRR, abs/1205.4217,", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms", "author": ["Lihong Li", "Wei Chu", "John Langford", "Xuanhui Wang"], "venue": "WSDM, pages 297\u2013306", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Switching processes in financial markets", "author": ["T. Preis", "J.J. Schneider", "H.E. Stanley"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Preis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Preis et al\\.", "year": 2011}, {"title": "Learning by investing: Evidence from venture capital", "author": ["Morten Sorensen"], "venue": "SIFR Research Report Series 53,", "citeRegEx": "Sorensen.,? \\Q2007\\E", "shortCiteRegEx": "Sorensen.", "year": 2007}, {"title": "Adaptive sequential Bayesian change point detection", "author": ["Ryan Turner", "Yunus Saatci", "Carl Edward Rasmussen"], "venue": "In Advances in Neural Information Processing Systems (NIPS): Temporal Segmentation Workshop,", "citeRegEx": "Turner et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Turner et al\\.", "year": 2009}, {"title": "Bayesian online learning of the hazard rate in change-point problems", "author": ["Robert C. Wilson", "Matthew R. Nassar", "Joshua I. Gold"], "venue": "Neural Comput.,", "citeRegEx": "Wilson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting[Kaufmann et al., 2012].", "startOffset": 99, "endOffset": 122}, {"referenceID": 6, "context": "Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting [Kaufmann et al., 2012].", "startOffset": 100, "endOffset": 123}, {"referenceID": 8, "context": "Switching behaviour has been studied in Financial Markets [Preis et al., 2011], and multi-armed bandits have been applied to this field before [Sorensen, 2007].", "startOffset": 58, "endOffset": 78}, {"referenceID": 9, "context": ", 2011], and multi-armed bandits have been applied to this field before [Sorensen, 2007].", "startOffset": 72, "endOffset": 88}, {"referenceID": 3, "context": "Garivier and Moulines [2008] use the following definition of regret.", "startOffset": 0, "endOffset": 29}, {"referenceID": 4, "context": "This corresponds to the results Hartland et al. [2007] report.", "startOffset": 32, "endOffset": 55}, {"referenceID": 0, "context": "Fearnhead and Liu [2007] as well as Adams and MacKay [2007] have independently done work on calculating the online posterior of the runlength.", "startOffset": 36, "endOffset": 60}, {"referenceID": 1, "context": "The worstcase time complexity of this algorithm is O(N logN), but it has an amortised cost of O(N) [Fearnhead and Clifford, 2003].", "startOffset": 99, "endOffset": 129}, {"referenceID": 10, "context": "Both Wilson et al. [2010] and Turner et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 10, "context": "[2010] and Turner et al. [2009] have proposed methods for learning the hazard function from the data.", "startOffset": 11, "endOffset": 32}, {"referenceID": 5, "context": "Exploitation Challenge 2006 was a competition in a multi-armed bandit problem[Hussain et al., 2006].", "startOffset": 77, "endOffset": 99}, {"referenceID": 4, "context": "Table 4 shows a comparison of the Change-Point Thompson Sampling algorithms (Global-CTS, PA-CTS, NP Global-CTS NP PA-CTS) against AdaptEvE Meta-Bandit and Meta-p-Bandit [Hartland et al., 2007].", "startOffset": 169, "endOffset": 192}, {"referenceID": 4, "context": "Hartland et al. [2007] won this competition with the Adapt-EvE algorithm.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "The simulation then proceeded as described by Li et al. [2011], the results are presented in table 5.", "startOffset": 46, "endOffset": 63}, {"referenceID": 8, "context": "some of the characteristics of a switching system for which the algorithms were designed [Preis et al., 2011].", "startOffset": 89, "endOffset": 109}], "year": 2013, "abstractText": "Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumes stationary distributions for the rewards. It is often unrealistic to model the real world as a stationary distribution. In this paper we derive and evaluate algorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem. We propose a Thompson Sampling strategy equipped with a Bayesian change point mechanism to tackle this problem. We develop algorithms for a variety of cases with constant switching rate: when switching occurs all arms change (Global Switching), switching occurs independently for each arm (Per-Arm Switching), when the switching rate is known and when it must be inferred from data. This leads to a family of algorithms we collectively term Change-Point Thompson Sampling (CTS). We show empirical results of the algorithm in 4 artificial environments, and 2 derived from real world data; news click-through[Yahoo!, 2011] and foreign exchange data[Dukascopy, 2012], comparing them to some other bandit algorithms. In real world data CTS is the most effective.", "creator": "LaTeX with hyperref package"}}}