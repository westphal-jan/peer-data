{"id": "1302.7263", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2013", "title": "Online Similarity Prediction of Networked Data from Known and Unknown Graphs", "abstract": "We idea online similarity prediction suffering again networked identification. We preparation according relating no assess because on nearly usage means prediction problem, showing possible, given an arbitrary probabilistic for section prediction, we instead require direct solver taking identical calculated left \" nearly \" another same reason bus, all longtime mean. After cheerfully that this general production considered adaptable inapplicable, still target doing study up {\\ t\u00eb optimal} indicate prediction algorithms at programming data. We continued depends only to network structure is {\\ em that} failed the leg-spin. Here everyone notice taken Matrix Winnow \\ citep {w07} has a north - computed difficult promise, at. amount given 3.7 correction _ 38 place. This motivates anything initiatives two he efficient mechanisms of a Perceptron nonlinear three 's lower mistake guarantee enough with only obispo - logarithmic prediction out. Our expanding then turns now since way not entire messaging whose structures is reportedly {\\ drays unknown} supposed the learner. In this translation set, where taken separate framework is way incrementally revealed, n't certificates a mistake - roadway derivation from a variance likelihood time total match.", "histories": [["v1", "Thu, 28 Feb 2013 17:15:55 GMT  (2207kb,D)", "https://arxiv.org/abs/1302.7263v1", null], ["v2", "Fri, 1 Mar 2013 16:57:09 GMT  (2206kb,D)", "http://arxiv.org/abs/1302.7263v2", null], ["v3", "Fri, 15 Mar 2013 12:52:33 GMT  (1919kb,D)", "http://arxiv.org/abs/1302.7263v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["claudio gentile", "mark herbster", "stephen pasteris"], "accepted": false, "id": "1302.7263"}, "pdf": {"name": "1302.7263.pdf", "metadata": {"source": "CRF", "title": "Online Similarity Prediction of Networked Data from Known and Unknown Graphs", "authors": ["Claudio Gentile"], "emails": ["claudio.gentile@uninsubria.it", "m.herbster@cs.ucl.ac.uk", "s.pasteris@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The study of networked data has spurred a large amount of research efforts. Applications like spam detection, product recommendation, link analysis, community detection, are by now well-known tasks in Social Network analysis and E-Commerce. In all these tasks, networked data are typically viewed as graphs, where vertices carry some kind of relevant information (e.g., user features in a social network), and connecting edges reflect a form of semantic similarity between the data associated with the incident vertices. Such a similarity ranges from friendship among people in a social network to common user\u2019s reactions to online ads in a recommender system, from functional relationships among proteins in a protein-protein interaction network to connectivity patterns in\nar X\niv :1\n30 2.\n72 63\nv3 [\na communication network. Coarsely speaking, similarity prediction aims at inferring the existence of new pairwise relationships based on known ones. These pairwise constraints, which specify whether two objects belong to the same class or not, may arise directly from domain knowledge or be available with little human effort.\nThere is a wide range of possible means of capturing the structure of a graph in this learning context: through combinatorial and classical graph-theoretical methods (e.g., [19]); through spectral approachs (e.g. [3, 22]), using convex duality and resistive geometry (e.g., [24]), and even algebraic methods (e.g., [34]). In many of these approaches, the underlying assumption is that the graph structure is largely known in advance (a kind of \u201ctransductive\u201d learning setting), and serves as a way to bias the inference process, so as to implement the principle that \u201cconnected vertices tend to be similar.\u201d Yet, this setting is oftentimes unrealistic and/or infeasible. For instance, a large online social network with millions of vertices and tens of millions of edges hardly lends itself to be processed as a whole via a Laplacian-regularized optimization approach or, even if it does (thanks to the computationally powerful tools currently available), it need not be known ahead of time. As a striking example, if we are representing a security agency, and at each point in time we receive a \u201ctrace\u201d of communicating individuals, we still might want to predict whether a given pair in the trace belong to the same \u201cgang\u201d/community, even if the actual network of relationships is unknown to us. So, in this case, we are incrementally learning similarity patterns among individuals while, at the same time, exploring the network. Another important scenario of an unknown network structure is when the network itself grows with time, hence the prediction algorithms are expected to somehow adapt to its temporal evolution. Our results. We study online similarity prediction over graphs in two models. One in which the graph is known1 a priori to the learner, and one in which it is unknown. In both settings there is an undisclosed labeling of a graph so that each vertex is the member of one of K classes. Two vertices are similar if they are in the same class and dissimilar otherwise. The learner receives an online sequence of vertex pairs and similarity feedback. On the receipt of a pair the learner then predicts if the pair is similar. The true pair label, similar or dissimilar, is then received and the goal of the learner is to minimize mistaken predictions. Our aim in both settings is then to bound the number of prediction mistakes over an arbitrary (and adversarially generated) sequence of pairs.\nIn the model where the graph is known, we first show via reductions to online vertex classification methods on graphs (e.g., [23, 24, 25, 27, 29, 28, 11, 12, 13], and references therein), that a suitable adaptation of the Matrix Winnow algorithm [47] readily provides an almost optimal mistake bound. This adaptation amounts to sparsifying the underlying graph G via a random spanning tree, whose diameter is then shortened by a known rebalancing technique [29, 12]. Unfortunately, due to its computational burden (cubic time per round), the resulting algorithm does not provide a satisfactory answer to actual deployment on large networks. Therefore, we develop an analogous adaptation of a Matrix Perceptron algorithm that delivers a much more attractive answer (thanks to its poly-logarithmic time per round), though with an inferior online prediction performance guarantee.\nThe unknown model is identical to the known one, except that the learner does not initially receive the underlying graph G. Rather, G is incrementally revealed, as now when the learner receives a pair it also receives as side information an adversarially generated path within G con-\n1The reader should keep in mind that while the data at hand may not be natively graphical, it might still be convenient in practice to artificially generate a graph for similarity prediction, since the graph may encode side information that is otherwise unexploitable.\nnecting the vertices of the pair. Here, we observe that the machinery we used for the known graph case is inapplicable. Instead, we design and analyze an algorithm which may be interpreted as a matrix version of an adaptive p-norm Perceptron [20, 17] with the relatively efficient quadratic running time per round. Related work. This paper lies at the intersection between online learning on graphs and matrix/metric learning. Both fields include a substantial amount of work, so we can hardly do it justice here. Below we outline some of the main contributions in matrix/metric learning, with a special emphasis on those we believe are most related to this paper. Relevant papers in online class prediction on graphs will be recalled in Section 3.\nSimilarity prediction on graphs can be seen as a special case of matrix learning. Relevant works on this subject include [46, 47, 10, 31] \u2013 see also [21] for recent usage in the context of online cut prediction. In all these papers, special care is put into designing appropriate regularization terms driving the online optimization problem, the focus typically being on spectral sparseness. When operating on graph structures with Laplacian-based regularization, these algorithms achieve mistake bounds depending on functions of the cut-size of the labeled graph \u2013 see Section 4. Yet, in the absence of further efforts, their scaling properties make them inappropriate to practical usage in large networks. Metric learning is also relevant to this paper. Metric learning is a special case of matrix learning where the matrix is positive semi-definite. Relevant references include [45, 15, 39, 49, 9]. Some of these papers also contain generalization bound arguments. Yet, no specific concerns are cast on networked data frameworks. Related to our bidirectional reduction from class prediction to similarity prediction is the thread of papers on kernels on pairs (e.g., [2, 39, 35, 6]), where kernels over pairs of objects are constructed as a way to measure the \u201cdistance\u201d between the two referenced pairs. The idea is then to combine with any standard kernel algorithm. The so-called matrix completion task (specifically, the recent reference [32]) is also related to our work. In that paper, the authors introduce a matrix recovery method working in noisy environments, which incorporates both a low-rank and a Laplacian-regularization term. The problem of recovery of low-rank matrices has extensively been studied in the recent statistical literature (e.g., [7, 8, 18, 44, 40, 33], and references therein), the main concern being bounding the recovery error rate, but disregarding the computational aspects of the selected estimators. Moreover, the way they typically measure error rate is not easily comparable to online mistake bounds. Finally, the literature on semisupervised clustering/clustering with side information ([4, 16] \u2013 see also [43] for a recent reference on spectral approaches to clustering) is related to this paper, since the similarity feedback can be interpreted as a must-link/cannot-link feedback. Nonetheless, their formal statements are fairly different from ours.\nTo summarize, whereas we are motivationally close to [32], from a technical viewpoint, we are perhaps closer to [45, 46, 47, 10, 21, 31], as well as to the literature on online learning on graphs.\nBefore delving into the graph-based similarity problem, we start off by investigating the problem of similarity prediction in abstract terms, showing that similarity prediction reduces to classification, and vice versa. This will pave the way for all later results."}, {"heading": "2 Online class and similarity prediction", "text": "In this section we examine the correspondence in predictive performance (mistake bounds) between the classification and similarity prediction frameworks. Preliminaries. The set of all finite sequences from a set X is denoted X *. We use the Iverson bracket notation [predicate] = 1 if the predicate is true and [predicate] = 0 if false. In K-class\nprediction in the online mistake bound model, an example sequence (x1, y1), . . . , (xT , yT ) \u2208 (X \u00d7 Y)* is revealed incrementally, where X is a set of patterns and Y := {1, . . . ,K} is the set of K class labels. The goal on the t-th trial is to predict the class yt given the previous t\u22121 pattern/label pairs and xt. The overall aim of an algorithm is to minimize the number of its mistaken predictions. In similarity prediction, examples are pairs of patterns with \u201csimilarity\u201d labels i.e., ((x\u2032, x\u2032\u2032), y) \u2208 X 2 \u00d7 Ys with Ys = {0, 1}. We interpret y \u2208 Ys as similar if y = 0 and dissimilar if y = 1; we also introduce the convenient function sim(y\u2032, y\u2032\u2032) := 1 \u2212 [y\u2032 = y\u2032\u2032] which maps a pair of class labels y\u2032, y\u2032\u2032 \u2208 Y to a similarity label. A concept is a function f : X \u2192 Y that maps patterns to labels. An example sequence S is consistent with a concept f for classification if (x, y) \u2208 S implies y = f(x) and for similarity if ((x\u2032, x\u2032\u2032), y) \u2208 S implies y = sim(f(x\u2032), f(x\u2032\u2032)). We use MA(S) to denote the number of prediction mistakes of the online algorithm A on example sequence S. Given an algorithm A, we define the mistake bound with respect to a concept f as BA(f) := maxSMA(S), the maximum being over all sequences S consistent with f .\nTheorem 1. Given an online classification algorithm Ac one may construct a similarity algorithm As such that if S is any similarity sequence consistent with any concept f then\nMAs(S) \u2264 5BAc(f) log2K , (1)\nand given an online similarity algorithm As one may construct a classification algorithm Ac such that if S is any classification sequence consistent with any concept f then\nMAc(S) \u2264 BAs(f) +K . (2)\nThe direct implementation of the similarity algorithm As from the classification algorithm Ac is infeasible, as its running time is exponential in the mistake bound. In Appendix A.1 we prove a more general result (see Lemma 7) than in equation (1) which applies also to noisy sequences and to \u201corder-dependent\u201d bounds, as in the shifting-expert bounds in [26]. We also argue (Appendix A.1.1) that the \u201clogK\u201d term in (1) is necessary. Observe that equation (2) implies a lower bound for similarity prediction if we have a lower bound for the corresponding class prediction problem with only a weakening by an additive \u201c\u2212K\u201d term."}, {"heading": "3 Class and similarity prediction on graphs", "text": "We now introduce notation specific to the graph setting. Let then G = (V,E) be an undirected and connected graph with n = |V | vertices, V = {1, . . . , n}, and m = |E| edges. The assignment of K class labels to the vertices of a graph is denoted by a vector y = (y1, . . . , yn), where yi \u2208 {1, . . . ,K} denotes the label of the i-th vertex among the K possible labels. The vertex-labeled graph will often be denoted by the pairing (G,y). Associated with each pair (i, j) \u2208 V 2 of (not necessarily adjacent) vertices is a similarity label yi,j \u2208 {0, 1}, where yi,j = 1 if and only if yi 6= yj . As is typical of graph-based prediction problems (e.g., [23, 24, 25, 27, 29, 28, 11, 12, 13], and references therein), the graph structure plays the role of an inductive bias, where adjacent vertices tend to belong to the same class. The set of cut-edges in (G,y) is denoted as \u03a6G(y) := {(i, j) \u2208 E : yi,j = 1} (when nonambiguous, we abbreviate it to \u03a6G), and the associated cut-size as |\u03a6G(y)|. The set of cut-edges with respect to class label k is denoted as \u03a6Gk (y) := {(i, j) \u2208 E : k \u2208 {yi, yj}, yi,j = 1} (when nonambiguous, we abbreviate it to \u03a6Gk ). Notice that \u2211K k=1 |\u03a6Gk (y)| = 2|\u03a6G(y)|. We let \u03a8 be the m\u00d7n (oriented and transposed) incidence matrix of G. Specifically, if we let the edges in E be enumerated as (i1, j1), . . . , (im, jm), and fix arbitrarily an orientation for them (e.g., from the left\nendpoint to the right endpoint), then \u03a8 is the matrix that maps any vector v = (v1, . . . , vn) > \u2208 Rn to the vector \u03a8v \u2208 Rm, where [\u03a8v]` = vi` \u2212 vj` , ` = 1, . . . ,m. Moreover, since G is connected, the null space of \u03a8 is spanned by the constant vector 1 = (1, . . . , 1)>, that is, \u03a8v = 0 implies that v = c1, for some constant c. We denote by \u03a8+ the (n\u00d7m-dimensional) pseudoinverse of \u03a8. The graph Laplacian matrix may be defined as L := \u03a8>\u03a8, thus notice that L+ = \u03a8+(\u03a8+)>. If G is identified with a resistive network such that each edge is a unit resistor, then the effective resistance RGi,j between a pair of vertices (i, j) \u2208 V 2 can be defined as RGi,j = (ei \u2212 ej)>L+(ei \u2212 ej), where ei is the i-th vector in the canonical basis of Rn. When (i, j) \u2208 E then RGi,j also equals the probability that a spanning tree of G drawn uniformly at random (from the set of all spanning trees of G) includes (i, j) as one of its n\u2212 1 edges (e.g., [38]). The resistance diameter of G is max(i,j)\u2208V 2 RGi,j . It is known that the effective resistance defines a metric over the vertices of G. Moreover, when G is actually a tree, then RGi,j corresponds to the number of edges in the (unique) path from i to j. Hence, in this case, the resistance diameter of G coincides with its (geodesic) diameter."}, {"heading": "3.1 Class prediction on graphs", "text": "Roughly speaking, algorithms and bounds for sequential class prediction on graphs split between two types: Those which approximate the original graph with a tree or those that maintain the original graph. By approximating the graph with a tree, extremely efficient algorithms are obtained with strong optimality guarantees. By exploiting the full graph, algorithms are obtained which take advantage of the connectivity to achieve sharp bounds when the graph contains, e.g., dense clusters. Relevant literature on this subject includes [27, 25, 23, 28, 29, 11, 24, 12]. Known representatives of the first kind are upper bounds of the form O(|\u03a6T |(1 + log n|\u03a6T |)) [29] or of the form O(|\u03a6T | logDT ) [11], where T is some spanning tree of G, and DT is the (geodesic) diameter of T . In particular, if T is drawn uniformly at random, the above turn to bounds on the expected number of mistakes of the form O(E[|\u03a6T |] log n), where E[|\u03a6T |] is the resistance-weighted cut-size of G, E[|\u03a6T |] = \u2211 (i,j)\u2208\u03a6G(y)R G i,j , which can be far smaller than |\u03a6G| when G is well connected. Representatives of the second kind are bounds of the form O(\u03c1+ |\u03a6G|R\u03c1) [23, 24], where \u03c1 is the number of balls in a cover of the vertices of G such that R\u03c1 is the maximum over the resistance diameters of the balls in the cover. Since resistance diameter lower bounds geodesic diameter, this alternative approach leverages a different connectivity structure of the graph than the resistanceweighted cut-size.\nIn all of the above mentioned works, the bounds and algorithms are for the K = 2 class prediction case. In Appendix A.2 we argue for a simple reduction that will raise a variety of cut-based algorithms and bounds from the two-class to the K-class case. Specifically, a two-class mistake bound of the form M \u2264 c|\u03a6G(y)| \u2200y \u2208 {0, 1}n, for some c \u2265 0 easily turns into a Kclass mistake bound of the form M \u2264 2c|\u03a6G(y)| \u2200y \u2208 {1, . . . ,K}n, where K need not be known in advance to the algorithm. Therefore, bounds of the form O(E[|\u03a6T |] log n) also hold in the multiclass setting.\nOn the lower bound side, [11] contains an argument showing (for the K = 2 class case) that for any \u03c6 \u2265 0 a labeling y exists such that any algorithm will make at least \u03c6/2 mistakes while E[|\u03a6T |] < \u03c6. In short, \u2126(E[|\u03a6T |]) is also a lower bound on the number of mistakes in the class prediction problem on graphs. When combined with Theorem 1 in Section 2, the above results immediately yield upper and lower bounds for the similarity prediction problem over graphs.\nProposition 1. Let (G,y) be a labeled graph, and T be a random spanning tree of G. Then an algorithm exists for the similarity prediction problem on G whose expected number of mistakes\nE[M ] satisfies E[M ] = O(E[|\u03a6T (y)|] logK log n) . Moreover, for any \u03c6 \u2265 0 a K-class labeling y exists such that any similarity prediction algorithm on G will make at least \u03c6/2\u2212K mistakes while E[|\u03a6T |] < \u03c6.\nThe upper bound above refers to a computationally inefficient algorithm and, clearly enough, more direct version space arguments would lead to similar results. Section 4 contains a more efficient approach to similarity prediction over graphs.\nTo close this section, we observe that the upper bounds on class predictions of the form O(E[|\u03a6T |] log n) taken from [29, 12] are essentially relying on linearizing the graph G into a path graph, and then predicting optimally on it via an efficient Bayes classifier (aka Halving Algorithm, e.g., [36]). One might wonder whether a similar approach would directly apply to the similarity prediction problem. We now show that exact computation of the probabilities of the Bayes classifier for a path graph is #P-complete under similarity feedback.\nThe Ising distribution over graph labelings (y \u2208 {1, 2}n) is defined as p(y) \u221d 2\u2212\u03b2|\u03c6G(y)|. Given a set of vertices and associated labels, the marginal distribution at each vertex can be computed in linear time when the graph is a path ([42]). In [29] this simple fact was exploited to give an efficient class prediction algorithm by a particular linearization of a graph to a path graph. The equivalent problem in similarity prediction requires us to compute marginals given a set of pairwise constraints. The following theorem shows that computing the partition function (and hence the relevant marginals) of the Ising distribution on a path with pairwise label constraints is #P-complete.\nTheorem 2. Computing the partition function of the (ferromagnetic) Ising model on a path graph with pairwise constraints is #P-complete, where an Instance is an n-vertex path graph P , a set of pairs C \u2282 {1, . . . , n}2, and a natural number, \u03b2, presented in unary, and the desired Output is the value of the partition function, ZP (C, \u03b2) := \u2211 y\u2208{1,2}n:{(yi=yj)}(i,j)\u2208C 2 \u2212\u03b2|\u03c6P (y)| .\nThus computing the exact marginal probabilities on even a path graph will be infeasible (given the hardness of #P). As an alternative, in the following section we discuss the application of the Matrix Perceptron and Matrix Winnow algorithms to similarity prediction."}, {"heading": "3.2 Similarity prediction on graphs", "text": "In Algorithm 1 we give a simple application of the Matrix Winnow (superscript \u201cw\u201d) and Perceptron (superscript \u201cp\u201d) algorithms to similarity prediction on graphs. The key aspect of the construction (common to many methods in metric learning) is the creation of rank one matrices which correspond to similarity \u201cinstances\u201d (see (3)). We then use the standard analysis of the Perceptron [41] and Matrix Winnow [47] algorithms with appropriate thresholds to obtain Proposition 2. A key observation is that the squared Frobenius norm of the (un-normalized) instance matrices is bounded by the squared resistance diameter of the graph, and the squared Frobenius norm of the (unnormalized) \u201ccomparator\u201d matrix is bounded by the cut-size squared |\u03a6G|2. Proposition 2. Let (G,y) be a labeled graph and let \u03a8 be the (transposed) incidence matrix associated with the Laplacian of G. Then, if we run the Matrix Winnow and Perceptron algorithms with similarity instances constructed from \u03a8, we have the following mistake bounds:\nMw = O ( |\u03a6G| max\n(i,j)\u2208V 2 RGi,j log n\n) and Mp = O ( |\u03a6G|2 max\n(i,j)\u2208V 2 (RGi,j)\n2 ) .\nAlgorithm 1: Perceptron and Matrix Winnow algorithms on a graph\nInput: Graph G = (V,E), |V | = n, with Laplacian L = \u03a8>\u03a8, and R := max(i,j)\u2208V 2 RGi,j ; Parameters: Perceptron threshold \u03b8\u0302p = R2; Winnow threshold \u03b8\u0302w = \u03b7\ne\u03b7\u2212e\u2212\u03b7 1 R |\u03a6G| ,\nWinnow learning rate \u03b7 = 1.28; Initialization: W p0 = 0 \u2208 Rm\u00d7m; Ww0 = 1m I \u2208 R\nm\u00d7m; For t = 1, 2, . . . , T :\n\u2022 Get pair of vertices (it, jt) \u2208 V 2, and construct similarity instances,\nXpt = (\u03a8 +)>(eit \u2212 ejt)(eit \u2212 ejt)>\u03a8+; Xwt =\n(\u03a8+)>(eit \u2212 ejt)(eit \u2212 ejt)>\u03a8+\n(eit \u2212 ejt)>L+(eit \u2212 ejt) ; (3)\n\u2022 Predict: y\u0302pt = [tr((W p t\u22121) >Xpt ) > \u03b8\u0302 p]; y\u0302wt = [tr((W w t\u22121) >Xwt ) > \u03b8\u0302 w]; \u2022 Observe yt \u2208 {0, 1} and, if mistake (yt 6= y\u0302t), update\nW pt \u2190W p t\u22121 + (yt \u2212 y\u0302 p t )X p t ; logW w t \u2190 logWwt\u22121 + \u03b7 (yt \u2212 y\u0302wt )Xwt .\nA severe drawback of both these algorithms is that on a generic graph, initilization requires computing a pseudo-inverse (typically cubic time), and furthermore the update of Matrix Winnow requires a cubic-time computation of an eigendecomposition (to compute matrix exponentials) on each mistaken trial.2 In the following section, we focus on a construction based on a graph approximation for which we develop an efficient implementation of the Perceptron algorithm which will require only poly-logarithmic time per round."}, {"heading": "4 Efficient similarity prediction on graphs", "text": "Relying on the notation of Section 3, we turn to efficient similarity prediction on graphs. We present adaptations of Matrix Winnow and Matrix Perceptron to the case when the original graph G is sparsified through a linearized and rebalanced random spanning tree of G. This sparsification technique, called Binary Support Tree (BST) in [29], brings the twofold advantage of yielding improved mistake bounds and faster prediction algorithms. More specifically, the use of a BST replaces the (perhaps very large) resistance diameter term max(i,j)\u2208V 2 R G i,j in the mistake bounds of Proposition 2 by a logarithmic term, the other term in the mistake bound becoming (when dealing with the expected number of mistakes) only a logarithmic factor larger than the (often far smaller) sum of the resistance-weighted cut-sizes in a spanning tree. Moreover, when combined with the Perceptron algorithm, a BST allows us to develop a very fast implementation whose running time per round is poly-logarithmic in n, rather than cubic, as in Matrix Winnow-like algorithms.\nRecall that a uniformly random spanning tree of an unweighted graph can be sampled in expected time O(n lnn) for \u201cmost\u201d graphs [5]. Using the nice algorithm of [48], the expected time reduces to O(n) \u2014see also the work of [1]. However, all known techniques take expected time \u0398(n3) in certain pathological cases.\nIn a nutshell, a BST B of G is a full balanced binary tree whose leaves correspond to the vertices\n2 Additionally, there is a tuning issue related to Matrix Winnow, since the threshold \u03b8\u0302w depends on the (unknown) cut-size.\nin3 V . In order to construct B from G, we first extract a random spanning tree T of G, then we visit T through a depth-first visit, order its vertices according to this visit eliminating duplicates (thereby obtaining a path graph P ), and finally we build B on top of P . Since B has 2n\u22121 vertices, we extend the class labels from leaves to internal vertices by letting, for each internal vertex i of B, yi be equal to the class label of i\u2019s left child. Figure 1 illustrates the process. A simple adaptation of [29] (Section 6 therein) shows that for any class k = 1, . . . ,K we have |\u03a6Bk | \u2264 2 |\u03a6Tk | log2 n. With the above handy, we can prove the following bounds (see Appendix A.3 for further details).\nTheorem 3. Let (G,y) be a labeled graph, T be a random spanning tree of G, B be the corresponding BST, and \u03a8B be the (transposed) incidence matrix associated with B.\n1. If we run Matrix Winnow with similarity instances constructed from \u03a8B (see Algorithm 1) then the expected number of mistakes E[M ] on G satisfies E[M ] = O ( \u03d5 log3 n ) ,\n2. and if we run the Matrix Perceptron algorithm with similarity instances constructed from \u03a8B then E[M ] = O ( \u03d52 log4 n ) ,\nwhere we denote the resistance-weighted cut-size as \u03d5 = E[|\u03a6T |] = \u2211\n(i,j)\u2208\u03a6G R G i,j.\nThe bound for Matrix Winnow is optimal up to a log3 n factor \u2014 compare to the lower bound in Proposition 1. However, this tight bound is obtained at the cost of having an algorithm which is O(n3) per round, even when run on a tree. This is because matrix exponentials require storing and updating a full SVD of the algorithm\u2019s weight matrix at each round, thereby making this algorithm highly impractical when G is large. On the other hand, the Perceptron bound is significantly suboptimal (due to its dependence on the squared resistance-weighted cut-size), but it has the invaluable advantage of lending itself to a very efficient implementation: Whereas a naive implementation would lead to an O(n2) running time per round, we now show that a more involved implementation exists which takes only O(log2 n), yielding an exponential improvement in the per-round running time."}, {"heading": "4.1 Implementing Matrix Perceptron on BST", "text": "The algorithm operates on the BST B by maintaining a (2n \u2212 1) \u00d7 (2n \u2212 1) symmetric matrix F with integer entries initially set to zero. At time t, when receiving the pair of leaves (it, jt),\n3 We assume w.l.o.g. that n = |V | is a power of 2. Otherwise, we may add dummy \u201cleaves\u201d.\nthe algorithm constructs Pt, the (unique) path in B connecting it to jt. Then the prediction y\u0302it,jt \u2208 {0, 1} is computed as\ny\u0302it,jt =\n{ 1 if \u2211 `,`\u2032\u2208Pt F`,`\u2032 \u2265 4 log 2 n,\n0 otherwise . (4)\nUpon receiving label yit,jt , the algorithm updates F as follows. First of all, the algorithm is mistake driven, so an update takes place only if yit,jt 6= y\u0302it,jt . Let Nt be the set of neighbors of the vertices in Pt, and define St := Nt \\ (Pt \\{it, jt}). We recursively assign integer tags ft(`) to vertices ` \u2208 Nt as follows: 1. For all ` \u2208 Pt, if ` is the s-th vertex in Pt then we set ft(`) = s; 2. For all ` \u2208 Nt \\Pt, let n` be the (unique) neighbor of ` that is contained in Pt. Then we set ft(`) = ft(n`). We then update F on each pair (`, `\u2032) \u2208 S2t as\nF`,`\u2032 \u2190 F`,`\u2032 + (2yit,jt \u2212 1) (ft(`)\u2212 ft(`\u2032))2 . (5)\nFigure 2 illustrates the process. The following theorem is the main technical result of this section. Its involved proof is given in Appendix A.3.\nTheorem 4. Let B be a BST of a labeled graph (G,y) with |V | = n. Then the algorithm described by (4) and (5) is equivalent to Matrix Perceptron run with similarity instances constructed from \u03a8B. Moreover, the algorithm takes O(log2 n) per trial, and there exists an adaptive representation of F with an initialisation time of only O(n) (rather than O(n2))."}, {"heading": "5 The Unknown Graph Case", "text": "We now consider the case when the graph G = (V,E) is unknown to the learner beforehand. The graph structure is thus revealed incrementally as more and more pairs (it, jt) get produced by the adversary. A reasonable online protocol that includes progressive graph disclosure is the following. At the beginning of round t = 1 the learner knows nothing about G, but the number of vertices n\n\u2014 prior knowledge of n makes presentation easier, but could easily be removed from this setting. In the generic round t, the adversary presents to the learner both pair (it, jt) \u2208 V \u00d7 V and a path within G from it to jt. The learner is then compelled to predict whether or not the two vertices are similar. Notice that, although the presented path may have cut-edges, there might be alternative paths in G connecting the two vertices with no cut-edges. The learner need not see them. The adversary then reveals the similarity label yit,jt in G, and the next round begins. In this setting, the adversary has complete knowledge of G, and can decide to produce paths and place the cutedges in an adaptive fashion. Notice that, because of the incremental disclosure of G, no such constructions as \u03a8-based similarity instances and/or BST, as contained in Section 3.2 and Section 4, are immediately applicable.\nAs a simple warm-up, consider the case when G is a tree and the K class label sets of vertices (henceforth called clusters) correspond to connected components of G. Figure 3 (a) gives an example. Since the graph is a tree, the number of cut-edges equals K \u2212 1. We can associate with such a tree a linear-threshold function vector u = (u1, . . . , un\u22121)\n> \u2208 {0, 1}n\u22121, where ui is 1 if and only if the i-th edge is a cut-edge. The ordering of edges within u can be determined ex-post by first disclosure times. For instance, if in round t = 1 the adversary produces pair (6, 4) and path 6 \u2192 3 \u2192 1 \u2192 4 (Figure 3 (a)), then edge (6, 3) will be the first edge, (3, 1) will be the second, and (1, 4) will be the third. Then, if in round t = 2 the new pair is (3, 5) and the associated path is 3 \u2192 1 \u2192 5, the newly revealed edge (1, 5) will be the fourth edge within u. With this ordering in mind, the algorithm builds at time t the (n \u2212 1)-dimensional vector xt = (x1,t, . . . , xn\u22121,t)\n> \u2208 {0, 1}n\u22121 corresponding to the path disclosed at time t, where xi,t is 1 if and only if the i-th edge belongs to the path. Now, it is clear that yit,jt = 1 if u\n>xt \u2265 1, and yit,jt = 0 if u\n>xt = 0. Therefore, this turns out to be a sparse linear-threshold function learning problem, and a simple application of the standard Winnow algorithm [36] leads to an O(K log n) = O ( |\u03a6G| log n ) mistake bound obtained by an efficient (O(n) time per round) algorithm, independent of the structural properties of G, such as its diameter. One might wonder if an adaptation of the above procedure exists which applies to a general graph G by, say, extracting a spanning tree T out of G, and then applying the Winnow algorithm on T . Unfortunately, the answer is negative for at least two reasons. First, the above linearthreshold model heavily relies on the fact that clusters are connected, which need not be the case in our similarity problem. More critically, even if the clusters are connected in G, they need not be connected in T . Figure 3 (b)-(c) shows a typical example where Winnow applied to a spanning tree fails. Given this state of affairs, we are lead to consider a slightly different representation for pairs of vertices and paths. Yet, as before, this representation will suggest a linear separability condition, as well as the deployment of appropriate linear-threshold algorithms."}, {"heading": "5.1 Algorithm and analysis", "text": "Algorithm 2 contains the pseudocode of our algorithm. When interpreted as operating on vectors, the algorithm is simply an r-norm perceptron algorithm [20, 17] with nonzero threshold, and norm r = 2 log(n \u2212 1)2 = 4 log(n \u2212 1), being (n \u2212 1)2 the length of the vectors maintained throughout, and s the dual to norm r. At time t, the algorithm observes pair (it, jt) and path p(it\u2192jt), builds the instance vector xt \u2208 {\u22121, 0, 1}n\u22121 and the long vector vec(Xt) out of the rank-one matrix Xt = xtx > t , where vec(\u00b7) is the standard vectorization of a matrix that stacks its columns one underneath the other. In order to construct xt from p(it\u2192jt), the algorithm maintains a forest made up of the union of paths seen so far. If pair (it, jt) is already connected by a path p in the current forest, then xt is the instance vector associated with path p (as for the Winnow algorithm on\n3 4 7\n21\n6\n5\ne4 e6\ne1\ne2\ne5\ne3 3 4\n7\n21\n6\n5\n(a) (c)(b)\n3 4\n7\n21\n6\n5\na tree in the previous section, but taking edge orientations into account \u2013 see Figure 7 in Appendix A.4 for details). Otherwise, path p(it\u2192jt) is added to the forest and xt will be an instance vector associated with the new path p(it\u2192jt). In adding the new path to the forest, we need to make sure that no circuits are generated. In particular, as soon as a revealed edge in a path causes two subtrees to join, the algorithm merges the two subtrees and processes all remaining edges in that path in a sequential manner so as to avoid generating circuits. The algorithm will end up using a spanning tree T of G for building its instance vectors xt. This spanning tree is determined on the fly by the adversarial choices of pairs and paths, so it is not known to the algorithm ahead of time.4 But any later change to the spanning forest is designed so as to keep consistency with all previous vectors xt.\nThe decision threshold (r \u2212 1)||xt||4r = (r \u2212 1)||vec(Xt)||2r follows from a standard analysis of the r-norm perceptron algorithm with nonzero threshold (easily adapted from [20, 17]), as well as the update rule. In short, since the graph is initially unknown, the algorithm is pretending to learn vectors rather than (Laplacian-regularized) matrices, and relies on a regularization that takes advantage of the sparsity of such vectors. The analysis of Theorem 5 below rests on ancillary (and\n4 In fact, because the algorithm is deterministic, this spanning tree is fully determined by the adversary. We are currently exploring to what extent randomization is beneficial for an algorithm in this setting.\nclassical) properties of matroids on graphs. These are recalled in Appendix A.4, before the proof of the theorem.\nTheorem 5. With the notation introduced in this section, let Algorithm 2 be run on an arbitrary sequence of pairs (i1, j1), (i2, j2), . . . and associated sequence of paths p(i1\u2192j1), p(i2\u2192j2), . . . . Then we have the mistake bound M = O ( |\u03a6G|4 log n ) .\nRemark 1. As explained in the proof of Theorem 5, the separability condition (9) allows one to run any vector or matrix mirror descent linear-threshold algorithm. In particular, since matrix U therein is spectrally sparse (rank K << n), one could use unitarily invariant regularization methods, like (squared) trace norm-based online algorithms (e.g., [47, 10, 31]). For instance, Matrix Winnow (more generally, Matrix EG-like algorithms [46]) would get bounds which are linear in the cutsize but also (due to their unitary invariance) linear in ||xt||22. The latter can be as large as the diameter of T , which can easily be O(n) even if the diameter of G is much smaller. This makes these bounds significantly worse than Theorem 5 when the total cutsize |\u03a6G| is small compared to n (which is our underlying assumption throughout). Group norm regularizers can also be used. Yet, because Xt has rank one, when |\u03a6G| is small these regularizers do not lead to better bounds5 than Theorem 5. Moreover, it is worth mentioning that, among the standard mirror descent linear-threshold algorithms operating on vectors vec(\u00b7), our choice of the r-norm Perceptron is motivated by the fact this algorithm achieves a logarithmic bound in n with no prior knowledge of the actual cutsize |\u03a6G| (or an upper bound thereof) \u2013 see Section 3.2, and the discussion in [17] about tuning of parameters in r-norm Perceptron and Winnow/Weighted Majority-like algorithms.\nAs a final remark, our algorithm has an O(n2) running time per round, trivially due to the update rule operating on O(n2)-long vectors. The construction of instance vector xt out of path p(it\u2192jt) can indeed be implemented faster than \u0398(n\n2) by maintaining well-known data structures for disjoint sets (e.g., [14, Ch. 22])."}, {"heading": "A Proofs", "text": "This appendix contains all omitted proofs. Notation is as in the main text.\nA.1 Missing proofs from Section 2\nThe set of example sequences consistent with a concept f for class prediction is denoted by Sc(f) := ({(x, f(x))}x\u2208X )\u2217, and for similarity prediction by Ss(f) := ({((x\u2032, x\u2032\u2032), sim(f(x\u2032), f(x\u2032\u2032))}x\u2032,x\u2032\u2032\u2208X )\u2217. A prediction algorithm is a mapping A : (X \u00d7 Y)* \u2192 YX from example sequences to prediction functions. Thus if A is a prediction algorithm and S = (x1, y1), . . . , (xT , yT ) \u2208 (X \u00d7 Y)* is an example sequence, then the online prediction mistakes are\nMA(S) := T\u2211 t=1 [A((x1, y1), . . . , (xt\u22121, yt\u22121))(xt) 6= yt] .\nWe write S\u2032 \u2286 S to denote that S\u2032 is a subset of S as well to denote that S\u2032 is a subsequence of S. We now introduce a weaker notion of a mistake bound as defined with respect to specific sequences rather than to the alternate notion of a concept. The weakness of this definition allows the construction in the following lemma to apply to \u201cnoisy\u201d as well \u201cconsistent\u201d example sequences.\nDefinition 6. Given an algorithm A, we define the subsequential mistake bound with respect to an example sequence S as\nB\u25e6A(S) := max S\u2032\u2286S MA(S \u2032) .\nThus, a subsequential mistake bound is simply the \u201cworst-case\u201d mistake bound over all subsequences.\nLemma 7. Given an online classification algorithm A, there exists a similarity algorithm A\u2032 such that for every sequence S = (x1, y1), . . . , (x2T , y2T ) its mistakes on\nS\u2032 = ((x1, x2), sim(y1, y2)), . . . , ((x2T\u22121, x2T ), sim(y2T\u22121, y2T ))\nis bounded as MA\u2032(S \u2032)) \u2264 cB\u25e6A(S) log2K , (6)\nwith c < 5.\nProof. The proof of (6) works by a modification of the standard weighed majority algorithm [37] arguments. The key idea is that similarity reduces to classification if we received the actual class labels as feedback rather than just similar/dissimilar as feedback. Since we do not have the classlabels, we instead \u201challucinate\u201d all possible feedback histories and then combine these histories on each trial using a weighted majority vote. If we only keep track of histories generated when the weighted majority vote is mistaken, the bound is small. Our master voting algorithm A\u2032 follows.\n1. Initialisation: We initialize the parameter \u03b2 = 0.294. We create a pool containing example sequences (\u201challucinated histories\u201d) S := {s}, with initially the empty history s = \u3008\u3009 with weight ws := 1 .\n2. For t = 1, . . . , T do\n3. Receive: the pattern pair (x2t\u22121, x2t)\n4. Predict: similar if\u2211 s\u2208S ws[A(s)(x2t\u22121) = A(s)(x2t)] \u2265 \u2211 s\u2208S ws[A(s)(x2t\u22121) 6= A(s)(x2t)]\notherwise predict dissimilar.\n5. Receive: Similarity feedback sim(y2t\u22121, y2t) if prediction was correct go to 2.\n6. Two cases, first if this algorithm predicted similar when the pair was dissimilar then for each history s \u2208 S with a mistaken prediction create K \u00d7 (K \u2212 1) histories s1,2, . . . , sK,K\u22121 so that si,j is equal to s but has the two \u201cspeculated\u201d examples (x2t\u22121, i), (x2t, j) appended\nto it. Then set ws1,2 = . . . = wsK,K\u22121 := \u03b2 K(K\u22121)ws and remove s from S. Second (predicted dissimilar) as above but now we need to add only K new histories to the pool.\n7. Go to 2.\nObserve that there exists a history s\u2217 \u2208 S generated by no more B\u25e6A(S) \u201cmistakes\u201d since there is always at least one history in the pool S which is a subsequence of S. Thus\nws\u2217 \u2265 (\n\u03b2\nK(K \u2212 1)\n)B\u25e6A(S) .\nFurthermore, the total weight of the pool of histories W := \u2211\ns\u2208S ws is reduced to a fraction of\nits weight no larger than 1+\u03b22 whenever this master algorithm A \u2032 makes a mistake. Thus since\nW \u2265 ws\u2217 , we have ( 1 + \u03b2\n2\n)MA\u2032 (S\u2032) \u2265 (\n\u03b2\nK(K \u2212 1)\n)B\u25e6A(S) .\nSolving for MA\u2032(S \u2032) we have\nMA\u2032(S \u2032) \u2264 B\u25e6A(S)  log2 K(K\u22121)\u03b2 log2 2 1+\u03b2  . Substituting in \u03b2 = .294 allows us to obtain the upper bound of (6) with c \u2248 4.99.\nWe observe, that reduction of similarity to classification holds for a wide variety online mistake bounds. Thus, e.g., we do not require the input sequence to be consistent, i.e., in S we may have examples (x\u2032, y\u2032) and (x\u2032\u2032, y\u2032\u2032) such that x\u2032 = x\u2032\u2032 but y\u2032 6= y\u2032\u2032. The usual type of mistake bound is permutation invariant i.e., the bound is the \u201csame\u201d for all permutations of the input sequence; typical examples include the the Weighted Majority [37] and p-Norm Perceptron [20, 17] algorithms. Observe that if BpA(S) is a permutation invariant bound, then B \u25e6 A(S) \u2264 B p A(S), since every subsequence of S is the prefix of a permutation of S. However, our reduction also more broadly applies to such \u201corder-dependent\u201d bounds, as the shifting-expert bounds in [26].\nWe now show that classification reduces to similarity. This reduction is efficient and does not introduce a multiplicative constant but requires the stronger assumption of consistency not required by Lemma 7.\nLemma 8. Given an online similarity algorithm As there exists an online classification algorithm Ac such that for any concept f if S \u2208 Sc(f) then\nMAc(S) \u2264 max S\u2032\u2208Ss(f)\nMAs(S \u2032) +K . (7)\nProof. As a warm-up, pretend we know a set P \u2286 X such that |P | = K and for each i \u2208 {1, . . . ,K} there exists an x \u2208 P such that f(x) = i. Using P we create algorithm Ac as follows. We maintain a history (example sequence) h, which is initially empty. Then on every trial when we receive a pattern xt we predict y\u0302t \u2208 {f(x) : As(h)((x, xt)) = similar, x \u2208 P}, and if the set contains multiple elements or is empty then we predict arbitrarily. If Ac incurs a mistake, we add to our history h the K examples ((x, xt), sim(f(x), f(xt)))x\u2208P . Observe that if A\nc incurs a mistake then at least one example corresponding to a mistaken similarity prediction is added to h and necessarily h \u2208 Ss(f). Thus the mistakes of Ac are bounded by maxS\u2032\u2208Ss(f)MAs(S\u2032). Now, since we do not actually know a set P , we may modify our algorithm Ac so that although P is initially empty we predict as before, and if we make a mistake on xt because there does not exist an x \u2208 P such that f(x) = f(xt), we then add xt to P . We can only make K such mistakes, so we have the bound of (7).\nProof of Theorem 1: If S is a classification sequence consistent with a concept f and A is a classification algorithm, then B\u25e6A(S) \u2264 BA(f), and hence (6) implies (1). Then, since (7) is equivalent to (2), we are done. 2\nA.1.1 The logK term is necessary in Theorem 1\nIn our study of class prediction on graphs we observed (see Appendix A.2) that certain 2-class bounds may be converted to K-class bounds with no explicit dependence on K. Yet Theorem 1 introduces a factor of logK for similarity prediction. So a question that arises is this simply a byproduct of the above analysis or is the \u201clogK\u201d factor tight. In the following, we demonstrate it is tight by introducing the paired permutation problem, which may be \u201csolved\u201d in the classification setting with no more than O(K) mistakes. Conversely, we show that an adversary can force \u2126(K logK) mistakes in the similarity setting.\nWe introduce the following notation. Let z : {1, . . . ,K} \u2192 {1, . . . ,K} denote a permutation function, a member of the set ZK of all K! bijective functions from {1, . . . ,K} to {1, . . . ,K}. A paired permutation function is the mapping yz : {1, . . . ,K}2 \u2192 {1, . . . ,K}, with yz(x\u2032, x\u2032\u2032) := max(z(x\u2032), z(x\u2032\u2032)). So, for example, consider a 3-element permutation z(1) \u2192 2, z(2) \u2192 3, and z(3) \u2192 1. Then, e.g., yz(1, 2) \u2192 3 and yz(1, 1) \u2192 2. Thus, we define the set of the paired permutation problem example sequences for class prediction as PPc := \u222az\u2208ZKSc(yz), and for similarity as PPs := \u222az\u2208ZKSs(yz).\nTheorem 9. There exists a class prediction algorithm A such that for any S \u2208 PPc we have MA(S) = O(K). Furthermore, for any similarity prediction algorithm A\u2032, there exists an S\u2032 \u2208 PPs such that MA\u2032(S \u2032) = \u2126(K logK).\nProof of Theorem 9: First, we show that there exists a class prediction algorithm A such that for any S \u2208 PPc we have MA(S) = O(K). Consider the simpler problem for the concept class of permutations \u222az\u2208ZKSc(z). By simply predicting consistently with the past examples we cannot incur more than K \u2212 1 mistakes. The algorithm A0(s) (consistent predictor), predicts y on receipt of pattern xt if there exists some example (x, y) in its history s such that xt = x, otherwise it predicts a y \u2208 {1, . . . ,K} not in its history. Now, using A0 as a base algorithm, we can use the principle of the master algorithm of Lemma 7 to achieve O(K) mistakes for the paired permutation problem. Thus, when we receive a pair ((x\u2032, x\u2032\u2032), y) either yz(x \u2032) = y or yz(x \u2032\u2032) = y, hence on a mistake we may \u201challucinate\u201d these two possible continuations. Our class prediction example sequence is S = (x\u20321, x \u2032\u2032 1), y1), . . . , ((x \u2032 T , x \u2032\u2032 T ), yT ) \u2208 PP c, and the algorithm A follows.\n1. Initialisation: We initialize the parameter \u03b2 = 0.294. We create a pool containing example sequences (\u201challucinated histories\u201d) S := {s}, with initially the empty history s = \u3008\u3009 with weight ws := 1 .\n2. For t = 1, . . . , T do\n3. Receive: the pattern xt = (x \u2032 t, x \u2032\u2032 t )\n4. Predict: y\u0302t = argmax\nk\u2208{1,...,K} \u2211 s\u2208S ws[max(A0(s)(x \u2032 t), A0(s)(x \u2032\u2032 t )) = k] . (8)\n5. Receive: Class feedback yt \u2208 {1, . . . ,K}. If prediction was correct go to 2.\n6. For each history s \u2208 S with a mistaken prediction, create two histories s\u2032, s\u2032\u2032 so that s\u2032 (s\u2032\u2032) is equal to s but has the example (x\u2032t, y) (the example (x \u2032\u2032 t , y)) appended to it. Then set\nws\u2032 = ws\u2032\u2032 := \u03b2 2ws , and remove s from S.\n7. Go to 2.\nObserve that there exists a history s\u2217 \u2208 S generated by no more K\u22121 \u201cmistakes\u201d. This is because, by induction, there is always a consistent history (i.e., the empty history is initially consistent, and when the master algorithm A makes a mistake and a consistent history makes a mistake, then either the continuation (x\u2032t, y) or (x \u2032\u2032 t , y) is consistent). Finally, observe that once a consistent history contains K \u2212 1 examples, it can no longer make mistakes. Thus\nws\u2217 \u2265 ( \u03b2\n2\n)K\u22121 .\nFurthermore, the total weight of the pool of histories W := \u2211\ns\u2208S ws is reduced to a fraction of its\nweight no larger than 1+\u03b22 whenever this master algorithm A makes a mistake. Since W \u2265 ws\u2217 , we have (\n1 + \u03b2\n2\n)MA(S) \u2265 ( \u03b2\n2\n)K\u22121 .\nSolving for MA(S) we can write\nMA(S) \u2264 (K \u2212 1)\n( log2 2 \u03b2\nlog2 2\n1+\u03b2\n) .\nSubstituting in \u03b2 = 0.294 allows us to obtain the upper bound of MA(S) \u2264 4.1(K \u2212 1). The argument then follows as in Theorem 1.\nNow consider the similarity problem. If we receive an instance of the form (((x\u2032, x\u2032\u2032), (x\u2032\u2032, x\u2032\u2032)), y) (with x\u2032 6= x\u2032\u2032) then y = similar implies z(x\u2032) < z(x\u2032\u2032) and y = disimilar implies z(x\u2032) > z(x\u2032\u2032). Thus with each mistaken example we learn precisely a single \u2018<\u2019 comparison. It follows from standard lower bounds on comparison-based sorting algorithms (e.g., [14]) that an adversary can force \u2126(K logK) comparisons, and thus mistakes, for any \u201ccomparison\u201d-algorithm to learn an arbitrary permutation. 2\nAny problem associated with a set of example sequences S may be iterated into a set of r independent problems by a cross-product-like construction, so that if S1, . . . , Sr \u2208 S and if Si = (xi1, y i 1), . . . , (x i Ti , yiTi) then an r-iterated example sequence is\n((x11, 1), y 1 1), . . . , ((x 1 T1 , 1), y 1 T1), . . . , ((x i 1, i), y i 1), . . . , ((x r Tr , r), y r Tr) .\nWe have simply conjoined the r example sequences into a single example sequence with each pattern \u201cx\u201d paired with an integer indicating from which sequence it originated. Thus by r-iterating the paired permutation problem we trivially observe mistake bounds of O(rK) and \u2126(rK logK) for all r \u2208 N in the class and similarity setting, respectively, thereby implying that the multiplicative \u201clogK\u201d gap occurs for an infinite family of classification/similarity problems.\nA.2 Missing proofs from Section 3\nLifting 2-class prediction to K-class prediction on graphs\nSuppose we have an algorithm for the 2-class graph labeling problem with a mistake bound of the form M \u2264 c|\u03a6G(y)| for all y \u2208 {1, 2}n, with c \u2265 0, We show that this implies the existence in the K-class setting of an algorithm with a bound of M \u2264 2c|\u03a6G(y)| for all y \u2208 {1, . . . ,K}n, where K need not be known in advance to the algorithm.\nThe algorithm simply works by combining the predictions of \u201cone versus rest\u201d classifiers. We train one classifier per class, and introduce a new classifier as soon as that class first appears. On any given trial, the combination is straightforward: If there is only one classifier predicting with its own class then we go with that class, otherwise we just assume a mistake. Thus, on any given trial, we can only be mistaken if one of the current \u201cone-verse-rest\u201d classifiers makes a mistake. This implies that our mistake bound is the sum of the mistake bounds of all of the \u201cone-verse-rest\u201d classifiers. Because each such binary classifier has a mistake bound of the form M \u2264 c|\u03a6Gk (y)|, and\u2211K\nk=1 |\u03a6Gk (y)| = 2|\u03a6G(y)|, we have that the K-class classifier has a bound of the form 2c|\u03a6G(y)|.\nProof of Theorem 2: We show that computing the partition function for the Ising model on a general graph reduces to computing the partition function problem for the Ising Model on a path graph with pairwise constraints hence showing #P-completeness. The partition problem for the Ising Model on a graph is defined by,\nInstance : An n-vertex graphG, and a natural number, \u03b2, presented in unary notation.\nOutput: The value of the partition function ZG(\u03b2), ZG(\u03b2) := \u2211\ny\u2208{1,2}n 2\u2212\u03b2|\u03c6 G(y)| .\nThis problem was shown #P-complete in [30, Theorem 15]. The reduction to the partition problem on a path graph with constraints is as follows.\nWe are given a graph G = (VG, EG) with n = |VG|, and further assume each vertex is \u201clabeled\u201d uniquely from 1, . . . , n. We construct the following path graph with pairwise constraints (see Figure 4) for an illustration.\n1. Find a spanning tree T = (VT , ET ) of G, and let R = EG \u2212 ET .\n2. Perform a depth-first-visit of T . From the 2n\u2212 1 vertex visit sequence, create an isomorphic path graph P0 with 2n\u22121 vertices such that each vertex in P0 is labeled with the corresponding vertex label from the visit of T . Thus each edge of T is mapped to two edges in P0.\n3. We now proceed to create a path graph P = (VP , EP ) from P0, which also includes each edge in R twice. We initialize P as a \u201cduplicate\u201d of P0 including labels. For each edge (v \u2032 r, v \u2032\u2032 r ) \u2208 R\nwe then do the following:\n(a) Choose an arbitrary vertex v\u2032 \u2208 VP so that v\u2032 and v\u2032r have the same label; (b) Let v\u2032\u2032\u2032\u2032 be a neighbor of v\u2032 in P (i.e, (v\u2032, v\u2032\u2032\u2032\u2032) \u2208 EP ); (c) Add vertices v\u2032\u2032 and v\u2032\u2032\u2032 to P with the labels of v\u2032\u2032r and v \u2032 r, respectively;\n(d) Remove the edge (v\u2032, v\u2032\u2032\u2032\u2032) from P and add the edges (v\u2032, v\u2032\u2032), (v\u2032\u2032, v\u2032\u2032\u2032) and (v\u2032\u2032\u2032, v\u2032\u2032\u2032\u2032) to P .\n4. Finally create pairwise equality constraints between all vertices with the same label.\nThus observe for every edge in G there are two analogous edges in P , and furthermore if edge (v, w) 6\u2208 G then there is not an analogous edge in P . Hence ZG(2\u03b2) = ZP (C, \u03b2).\nProof sketch Proposition 2 We start off with the Matrix Perceptron bound. For brevity, we write Xt instead of X p t . Also, let \u3008A,B\u3009 be a shorthand for the inner product tr(ATB). We can write\n\u3008Xt, Xt\u3009 = tr((\u03a8+)T (eit \u2212 ejt)(eit \u2212 ejt)T\u03a8+(\u03a8+)T (eit \u2212 ejt)(eit \u2212 ejt)T\u03a8+) = tr((eit \u2212 ejt)T\u03a8+(\u03a8+)T (eit \u2212 ejt)(eit \u2212 ejt)T\u03a8+(\u03a8+)T (eit \u2212 ejt)) = ((eit \u2212 ejt)TL+(eit \u2212 ejt))2\n= (RGit,jt) 2 \u2264 R2 .\nMoreover, for any k \u2208 {1, . . . ,K}, define K vectors u1, . . . ,uK \u2208 Rn as follows.\nuk = (uk,1, . . . , uk,n) >, with uk,i = [k = yi] ,\nbeing yi the label of the i-th vertex of G. Now, if we let U := \u03a8 (\u2211K k=1 uku > k ) \u03a8>, we have\n\u3008U,Xt\u3009 = tr(UTXt)\n= K\u2211 k=1 tr(\u03a8uku T k \u03a8 T (\u03a8+)T (eit \u2212 ejt)(eit \u2212 ejt)T\u03a8+)\n= K\u2211 k=1 tr((eit \u2212 ejt)T\u03a8+\u03a8ukuTk \u03a8T (\u03a8+)T (eit \u2212 ejt))\n= K\u2211 k=1 ((eit \u2212 ejt)T\u03a8+\u03a8uk)2 .\nBy definition of pseudoinverse, \u03a8(\u03a8+\u03a8uk) = (\u03a8\u03a8 +\u03a8)uk = \u03a8uk for all k = 1, . . . ,K. Hence (recall Section 3), \u03a8+\u03a8uk = uk + c1 for some c \u2208 R. We therefore have that (eit \u2212 ejt)T\u03a8+\u03a8uk = uk,it \u2212 uk,jt , i.e.,\n\u3008U,Xt\u3009 = K\u2211 k=1 (uk,it \u2212 uk,jt)2 .\nNow, if yit,jt = 0 (i.e., yit = yjt) then for all k we have uk,it \u2212 uk,jt = 0, so that \u3008U,Xt\u3009 = 0. On the other hand, if yit,jt = 1 (i.e., yit 6= yjt) then there exist distinct a, b \u2208 {1, . . . ,K} such that |ua,it \u2212 ua,jt | = |ub,it \u2212 ub,jt | = 1, and for all other k 6= a, b we have uk,it \u2212 uk,jt = 0. So, in this case \u3008U,Xt\u3009 = 2.\nThis gives the linear separability condition of sequence (X1, yi1,j1), (X2, yi2,j2), . . . w.r.t. U . Finally, we bound \u3008U,U\u3009. Let \u03a6Ga,b := {(i, j) \u2208 E : yi = a, yj = b}. We have:\n\u3008U,U\u3009 = tr(UTU)\n= tr (( K\u2211 a=1 \u03a8uau T a\u03a8 T )( K\u2211 b=1 \u03a8ubu T b \u03a8 T ))\n= K\u2211 a=1 K\u2211 b=1 tr(\u03a8uau T a\u03a8 T\u03a8ubu T b \u03a8 T )\n= K\u2211 a=1 K\u2211 b=1 tr(uTb \u03a8 T\u03a8uau T a\u03a8 T\u03a8ua)\n= K\u2211 a=1 K\u2211 b=1 (uTb \u03a8 T\u03a8ua) 2\n= K\u2211 a=1 |\u03a6Ga |2 +\u2211 b6=a |\u03a6Ga,b|2  . So, noticing that \u2211 b : b 6=a |\u03a6Ga,b| = |\u03a6Ga | and hence that \u2211 b : b6=a |\u03a6Ga,b|2 \u2264 |\u03a6Ga |2, we conclude that\n\u3008U,U\u3009 \u2264 2|\u03a6G|2 .\nWith the above handy, the mistake bound on Mp easily follows from the standard analysis of the Perceptron algorithm with nonzero threshold.\nBy a similar token, the bound on Mw follows from the arguments in [47], after defining U to be a normalized version of the one we defined above for Matrix Perceptron, and noticing that Xwt in Algorithm 1 are positive semidefinite and normalized to trace 1.\nA.3 Missing proofs from Section 4\nThe following lemma relies on the equivalence between effective resistance RGi,j of an edge (i, j) and its probability of being included in a randomly drawn spanning tree.\nLemma 10. Let (G,y) be a labeled graph, and T be a spanning tree of G drawn uniformly at random. Then, for all k = 1, . . . ,K, we have:\n1. E[|\u03a6Tk |] = \u2211 (i,j)\u2208\u03a6Gk RGi,j, and\n2. E[|\u03a6Tk |2] \u2264 2( \u2211 (i,j)\u2208\u03a6Gk RGi,j) 2 .\nProof. Set s = |\u03a6Gk | and \u03a6Gk = {(i1, j1), (i2, j2), . . . , (is, js)}. Also, for ` = 1, . . . , s, let X` be the random variable which is 1 if (i`, j`) is an edge of T , and 0 otherwise. From E[X`] = RGi`,j` we immediately have 1). In order to prove 2), we rely on the negative correlation of variables X`, i.e., that E[X`X`\u2032 ] \u2264 E[X`]E[X`\u2032 ] for ` 6= `\u2032 (see, e.g., [38]). Then we can write\nE(|\u03a6Tk |2) = E ( s\u2211 `=1 X` )2 = E\n[ s\u2211 `=1 s\u2211 `\u2032=1 X`X`\u2032 ]\n= s\u2211 `=1 E[X`] + s\u2211 `=1 \u2211 `\u2032 6=` E[X`X`\u2032 ]\n\u2264 s\u2211 `=1 E[X`] + s\u2211 `=1 \u2211 `\u2032 6=` E[X`]E[X`\u2032 ] .\nNow, for any spanning tree T of G, if s \u2265 1 then it must be the case that |\u03a6Tk | \u2265 1, and hence\u2211s `=1 E[X`] = E[|\u03a6Tk |] \u2265 1 . Combined with the above we obtain:\nE[|\u03a6Tk |2] \u2264 ( s\u2211 `=1 E[X`] )2 + s\u2211 `=1 \u2211 `\u2032 6=` E[X`]E[X`\u2032 ] \u2264 2 ( s\u2211 `=1 E[X`] )2 = 2 ( s\u2211 `=1 RGi`,j` )2 ,\nas claimed.\nProof of Theorem 3 From Proposition 2 we have that if we execute Matrix Winnow on B = BT with similarity instances constructed from \u03a8B, then the number M of mistakes satisfies\nM = O ( |\u03a6B|DB log n ) ,\nwhere DB is the resistance diameter of B. Since B is a tree, its resistance diameter is equal to its diameter, which is O(log n). Moreover, |\u03a6Bk | = O(|\u03a6Tk | log n), for k = 1, . . . ,K, hence |\u03a6B| = O(|\u03a6T | log n). Plugging back, taking expectation over T , and using Lemma 10, 1) proves the Matrix Winnow bound. Similarly, if we run the Matrix Perceptron algorithm on B with similarity instances constructed from \u03a8B then\nM = O ( |\u03a6B|2D2B ) .\nProceeding as before, in combination with Lemma 10, 2), proves the Matrix Perceptron bound.\nProof of Theorem 4 First of all, the fact that the algorithm is O(log2 n) per round easily follows from the fact that, since B is a balanced binary tree, the sizes of sets Pt (prediction step in (4)) and St (update step in (5)) are both O(log n).\nAs for initialization time, a naive implementation would require O(n2) (we must build the zero matrix F ). We now outline a method of growing a data structure that stores a representation of F online for which the initialisation time is only O(n), while keeping the per round time to O(log2 n). For every vertex ` in B the algorithm maintains a subtree B` of B, initially set to {\u03c1}, being \u03c1 the root of B. At every vertex `\u2032 \u2208 B` is stored the value F`,`\u2032 . At the start of time t, the algorithm climbs B from it to \u03c1, in doing so storing the ordered list Lit of vertices in the path from \u03c1 to it. The same is done with jt. The set St is then computed. For all ` \u2208 St, the tree B` is then extended to include the vertices in Nt and the path from it (note that for each ` \u2208 St this takes only O(log n) time, since we have the list Lit). Whenever a new vertex `\u2032 is added to B`, the value F`,`\u2032 is set to zero. Hence, we initialize F \u201con demand\u201d, the only initialization step being the allocation of the BST, i.e., O(n) time.\nWe now continue by showing the equivalence of the sequence of predictions issued by (4) to those of the Matrix Perceptron algorithm with similarity instances constructed from \u03a8B.\nFor every ` \u2208 St define \u039bt(`) as the maximal subtree of B that contains ` and does not contain any nodes in Pt \\ {it, jt}.\nLemma 11. \u039bt(\u00b7) defined above enjoys the following properties (see Figure 5, left, for reference).\n1. For all `, \u039bt(`) is uniquely defined;\n2. Any subtree T of B that has no vertices from Pt \\ {it, jt} (and hence any of the trees \u039bt) contains at most one vertex from St;\n3. The subtrees {\u039bt(`) : ` \u2208 St} are pairwise disjoint;\n4. The set {\u039bt(`) : ` \u2208 St}\u222a (Pt \\ {it, jt}) covers B (so in particular {\u039bt(`) : ` \u2208 St} covers the set of leaves of B).\nProof. 1. Suppose we have subtrees T and T \u2032 with T 6= T \u2032 that both satisfy the conditions of \u039bt(`). Then w.l.o.g assume there exists a vertex ` \u2032 in T that is not in T \u2032. Since T and T \u2032\nare both connected and both contain `, the subgraph T \u222a T \u2032 of B is connected and is hence a subtree. Since neither T nor T \u2032 contains vertices in Pt \\ {it, jt}, T \u222a T \u2032 does not contain any such either. Hence, because T \u2032 is a strict subtree of T \u222a T \u2032, we have contradicted the maximality of T \u2032.\n2. Suppose T has distinct vertices `, `\u2032 \u2208 St. Since T is connected, it must contain the path in B from ` to `\u2032. This path goes from ` to the neighbor of ` that is in Pt \\ {it, jt}, then follows the path Pt \\ {it, jt} (in the right direction) until a neighbor of `\u2032 is reached. The path then terminates at `\u2032. Such a path contains at least one vertex in Pt \\ {it, jt}, contradicting the initial assumption about T .\n3. Assume the converse \u2013 that there exist distinct `, `\u2032 in St such that \u039bt(`) and \u039bt(`\u2032) share vertices. Then, since \u039bt(`) and \u039bt(`\n\u2032) are connected, \u039bt(`) \u222a \u039bt(`\u2032) must also be connected (and hence must be a subtree of B). Since \u039bt(`)\u222a\u039bt(`\u2032) shares no vertices with Pt\\{it, jt}, and contains both ` and `\u2032 (which are both in St), the statement in Item 2 above is contradicted.\n4. Assume that we have a ` \u2208 B \\ (Pt \\ {it, jt}). Then let P \u2032 be the path from ` to the (first vertex encountered in) the path Pt \\{it, jt}. Let `\u2032 be the second from last vertex in P \u2032. Then `\u2032 is a neighbor of a vertex in Pt, but is not in Pt \\ {it, jt}, so it must be in St. This implies that the path P \u2032\u2032 that goes from ` to `\u2032 contains no vertices in Pt \\ {it, jt} and is therefore (Item 1) a subtree of \u039bt(` \u2032). Hence, ` \u2208 \u039bt(`\u2032).\nLemma 12. Let L be the Laplacian matrix of B, and `, `\u2032 \u2208 St. Then for any pair of vertices \u03ba and \u03ba\u2032 of B with \u03ba \u2208 \u039bt(`) and \u03ba\u2032 \u2208 \u039bt(`\u2032) we have\n(e\u03ba \u2212 e\u03ba\u2032)TL+(eit \u2212 ejt) = ft(`\u2032)\u2212 ft(`) ,\nwhere ei is the i-th element in the canonical basis of R2n\u22121.\nProof. We first extend the tagging function ft to all vertices of B via the vector 6 f\u0303t as follows (note that, by Lemma 11, f\u0303t is well defined):\n1. For all ` \u2208 Pt \\ {it, jt}, set f\u0303t(`) = ft(`);\n2. For all `\u2032 \u2208 St and ` \u2208 \u039bt(`\u2032), set f\u0303t(`) = ft(`\u2032).\nClaim 1. Lf\u0303t = ejt \u2212 eit.\nProof of claim. For any vertex \u03ba of B \\ {it, jt} one of the following holds:\n1. If \u03ba \u2208 Pt, then \u03ba has a neighbor \u03ba1 with f\u0303t(\u03ba1) = f\u0303t(\u03ba)\u2212 1, one neighbour \u03ba2 with f\u0303t(\u03ba2) = f\u0303t(\u03ba) + 1, and (unless \u03ba is the root of B) one neighbour \u03ba3 with f\u0303t(\u03ba3) = f\u0303t(\u03ba). We therefore have that [Lf\u0303t]\u03ba = 3f\u0303t(\u03ba)\u2212 f\u0303t(\u03ba1)\u2212 f\u0303t(\u03ba2)\u2212 f\u0303t(\u03ba3) = 0.\n2. If \u03ba \u2208 Nt \\ Pt, then \u03ba has one neighbor \u03ba1 in Pt and we have f\u0303t(\u03ba1) = f\u0303t(\u03ba). Let T\u03ba be the subtree of B containing exactly vertex \u03ba and all neighbors of \u03ba bar \u03ba1. Since Pt is connected, it contains \u03ba1 and does not contain \u03ba, none of the other neighbors of \u03ba being in Pt. Hence T\u03ba is a subtree of B that contains \u03ba and no vertices from Pt \\ {it, jt}, and so by Lemma 11, item 1 it must be a subtree of \u039bt(\u03ba). Hence, by definition of f\u0303t, all vertices \u03ba2 in T\u03ba satisfy f\u0303t(\u03ba2) = f\u0303t(\u03ba). This implies that for all neighbors \u03ba3 of \u03ba we have f\u0303t(\u03ba3) = f\u0303t(\u03ba), which in turn gives [Lf\u0303t]k = 0.\n3. If \u03ba /\u2208 Nt then, by Lemma 11 item 4, let \u03ba be contained in \u039bt(`) for some ` \u2208 St. Let T\u03ba be the subtree of B containing exactly vertex \u03ba and all neighbors of \u03ba. Note that T\u03ba is a subtree of B that contains \u03ba and no vertices from Pt \\ {it, jt}. Since \u039bt(`) also contains \u03ba (hence \u039bt(`)\u222aT\u03ba is connected), we have that \u039bt(`)\u222aT\u03ba is a subtree of B that contains ` and no vertices from P \\ {it, jt}. By Lemma 11 item 1, this implies that \u039bt(`) \u222a T\u03ba is a subtree of (and hence equal to) \u039bt(`). Hence, by definition of f\u0303t, we have that f\u0303t is identical on T\u03ba. Thus all neighbors \u03ba1 of \u03ba satisfy f\u0303t(\u03ba1) = f\u0303t(\u03ba), implying again [Lf\u0303t]\u03ba = 0.\nSo in either case [Lf\u0303t]\u03ba = 0. Finally, let i\u2032t be the neighbor of it in B. We have [Lf\u0303t]it = f\u0303t(it)\u2212f\u0303t(i\u2032t) = 1\u22122 = \u22121. Similarly, we have [Lf\u0303t]jt = 1. Putting together, we have shown that Lf\u0303t = ejt \u2212 eit , thereby concluding the proof of Claim 1.\nNow, by definition of pseudoinverse,\nLf\u0303t = LL +Lf\u0303t = LL +(ejt \u2212 eit) .\nThis mplies that L(f\u0303t\u2212L+(ejt \u2212eit)) = 0. Therefore (see Section 3) there exists a constant c such that f\u0303t = L +(ejt \u2212 eit) + c1. From the definition of f\u0303 we can write\nft(` \u2032)\u2212 ft(`) = f\u0303t(\u03ba\u2032)\u2212 f\u0303t(\u03ba)\n= ([L+(ejt \u2212 eit)]\u03ba\u2032 \u2212 c)\u2212 ([L+(ejt \u2212 eit)]\u03ba \u2212 c) = (e\u03ba \u2212 e\u03ba\u2032)TL+(eit \u2212 ejt),\nas claimed.\n6 In our notation, we interchangeably view f\u0303 both as a tagging function from the 2n \u2212 1 vertices of B to the natural numbers and as a (2n\u2212 1)-dimensional vector.\nLemma 13. Let L be the Laplacian matrix of B, and \u03ba, \u03ba\u2032 be two vertices of B. Let P be the path from \u03ba to \u03ba\u2032 in B. Then for any t either |P \u2229 St| \u2264 1 or P \u2229 St = {`, `\u2032}, for two distinct vertices ` and `\u2032. No other cases are possible. Moreover,\n((e\u03ba \u2212 e\u03ba\u2032)TL+(eit \u2212 ejt))2 = { 0 if |P \u2229 St| \u2264 1 (ft(`)\u2212 ft(`\u2032))2 if P \u2229 St = {`, `\u2032} .\nProof. By Lemma 11 item 4, we have two possible cases only:\n1. There exists ` \u2208 St such that both \u03ba and \u03ba\u2032 are in \u039bt(`): In this case (since \u039bt(`) is connected) the path P lies in \u039bt(`). Since, by Lemma 11 item 2, no `\u2032 \u2208 St with `\u2032 6= ` can be in \u039bt(`), it is only ever possible that P contains at most one vertex ` (if any) of St.\n2. There exist two distinct nodes `, `\u2032 \u2208 St such that \u03ba \u2208 \u039b(`) and \u03ba\u2032 \u2208 \u039b(`\u2032). In this case, P corresponds to the following path: First go from \u03ba to ` (by Lemma 11 item 2, since this path lies in \u039b(`) the only vertex in St that lies in the section of the path is `); then go to the neighbor of ` that is in Pt \\ {it, jt}; then follow the path Pt \\ {it, jt} until you reach the neighbor of `\u2032 (this section of P contains no vertices in St); then go from `\u2032 to \u03ba (by Lemma 11 item 2, since this path lies in \u039b(`\u2032) the only vertex in St that lies in this section of the path is `\u2032). Thus, P \u2229 St = {`, `\u2032}.\nThe result then follows by applying Lemma 12 to the two cases above.\nFigure 5 illustrates the above lemmas by means of an example. To conclude the proof, let \u3008A,B\u3009 be a shorthand for tr(A>B). We see that from Algorithm 1,\nLemma 13, and the definition of F in (4) we can write\n\u3008Wt, Xt\u3009 = t\u22121\u2211\nt\u2032=1,t\u2032\u2208M (2yit\u2032 ,jt\u2032 \u2212 1)\u3008Xt\u2032 , Xt\u3009\n= t\u22121\u2211\nt\u2032=1,t\u2032\u2208M (2yit\u2032 ,jt\u2032 \u2212 1)((eit\u2032 \u2212 ejt\u2032 ) TL+(eit \u2212 ejt))2\n= \u2211\n(`,`\u2032)\u2208P2t\nF`,`\u2032 ,\nwhereM is the set of mistaken rounds, and the second-last equality follows from a similar argument as the one contained in the proof of Proposition 2. Threshold 2 log n in (4) is an upper bound on the radius squared \u3008Xt, Xt\u3009 of instance matrices (denoted by R2 in Algorithm 1). In fact, from the proof of Proposition 2,\nmax t \u3008Xt, Xt\u3009 \u2264 max (i,j)\u2208V 2 ((ei \u2212 ej)>L+(ei \u2212 ej))2 = max (i,j)\u2208V 2 (RGi,j) 2 ,\nwhich is upper bounded by the square of the diameter 2 logn of B.\nA.4 Ancillary results, and missing proofs from Section 5\nThis section contains the proof of Theorem 5, along with preparatory results."}, {"heading": "A digression on cuts and directed paths", "text": "Given7 a connected and unweighted graph G = (V,E), with n = |V | vertices and m = |E| edges, any partition of V into two subsets induces a cut over E. A cut is a cutset if it is induced by a two-connected component partition of V . Fix now any spanning tree T of G (this will be the one constructed by the algorithm at the end of the game, based on the paths produced by the adversary \u2013 see Section 5.1). In this context, the n \u2212 1 edges of T are often called branches and the remaining m \u2212 n + 1 edges are often called chords. Any branch of T cuts the tree into two components (it is therefore a cutset), and induces a two-connected component partition over V . Any such cutset is called a fundamental cutset of G (w.r.t. T ). Cuts are always subsets of E, hence they can naturally be represented as (binary) indicator vectors with m components. For reasons that will be clear momentarily, it is also convenient to assign each edge an orientation (tail vertex to head vertex) and each cut an inward/outward direction. In particular, it is customary to give a fundamental cut the orientation of its branch. As a consequence of orientations/directions, cuts are rather represented as m-dimensional vectors whose components have values in {\u22121, 0, 1}.\n7 The reader familiar with the theory of matroids will recognize what is recalled here as a well known example of a regular matroid on graphs. One can learn about them in standard textbooks/handbooks, e.g., [19, Ch.6].\nFigure 6 (a) gives an example. In this figure, all edges are directed from the low index vertex to the high index vertex. Branch e1 determines a cutset (more precisely, a fundamental cutset w.r.t. to the depicted spanning tree) separating vertices 2 and 7 from the remaining ones. Edge e6 isolates just vertex 6 from the rest (again, a fundamental cutset). The fundamental cut determined by branch e1 is represented by vector q = (1, 0, 0, 0, 0, 0,\u22121, 1, 0, 1, 0, 1). This is because if we interpret the orientation of branch e1 as outward to the cut, then e7 is inward, e8 is outward, as well as e10 and e12. The matrix in Figure 6 (b) contains as rows all fundamental cutsets. This is usually called the fundamental cutset matrix, often denoted by Q (recall that this matrix depends on spanning tree T \u2013 for readability, we drop this dependence from our notation). Matrix Q has rank n \u2212 1. Moreover, any cut (viewed as an m-dimensional vector) in the graph can be represented as a linear combination of fundamental cutset vectors with linear combination coefficients \u22121, +1, and 0. In essence, cuts are an (n \u2212 1)-dimensional vector space with fundamental cutsets (rows Qi of Q) as basis. It is important to observe that the vectors Qi involved in this representation are precisely those corresponding to the branches of T that are either moving inward (coefficient \u22121) or outward (coefficient +1). Hence the fewer are the branches of T cutting inward or outward, the sparser is this representation. Matrix Q has also further properties, like total unimodularity. This implies that any linear combination of their rows with coefficients in {\u22121, 0,+1} will result in a vector whose coefficients are again in {\u22121, 0,+1}.\nTo summarize, given a spanning tree T of G, a direction for G\u2019s edges, and the associated matrix Q, any cutset8 q in G can be represented as an m-dimensional vector q = Q> u, where u \u2208 {\u22121, 0,+1}n\u22121 has as many nonzero components as are the branches of T belonging to q. With this representation (induced by T ) in hand, we are essentially aimed at learning in a sequential fashion u\u2019s components.\nIn order to tie this up with our similarity problem, we view the edges belonging to a given cutset as the cut edges separating a (connected) cluster from the rest of the graph, and then associate with any given K-labeling of the vertices of G a sequence of K weight vectors uk, k = 1, . . . ,K, each one corresponding to one label. Since a given label can spread over multiple clusters (i.e., the vertices belonging to a given class label need not be a connected component of G), we first need to collect connected components belonging to the same cluster by summing the associated coefficient vectors. As an example, suppose in Figure 6 (a) we have 3 vertex labels corresponding to the three colors. The blue cluster contains vertices 4, 5, 6 and 7, the green one vertex 1, and the red one vertices 2 and 3. Now, whereas the blue and the green labels are connected, the red one is not. Hence we have u{4,5,6,7} = (0, 0,\u22121,\u22121,\u22121,\u22121)>, u{1} = (1, 1, 1, 1, 0, 0)>, and u{2,3} = (\u22121,\u22121, 0, 0, 1, 1)> is the sum of the two cutset coefficient vectors u{2} = (\u22121, 0, 0, 0, 1, 0)>, and u{3} = (0,\u22121, 0, 0, 0, 1)>. In general, our goal will then be to learn a sparse (and rank-K) matrix U = \u2211K k=1 uku > k , where uk corresponds to the k-th (connected or disconnected) class label. Consistent with the above, we represent the pair of vertices (it, jt) as an indicator vector encoding the unique path in T that connects the two vertices. This encoding takes edge orientation into account. For instance, the pair of vertices (6, 7) in Figure 6 (c) is connected in T by path p(6\u21927) = 6\u2192 3\u2192 1\u2192 2\u2192 7. According to the direction of traversed edges (edge e1 is traversed according to its orientation, edge e2 in the opposite direction, etc.), path p(6\u21927) is represented by vector p = (1,\u22121, 0, 0, 1,\u22121, 0, 0, 0, 0, 0, 0)> = ((p\u2032t)>|0>m\u2212n+1), hence Qp = p\u2032t = (1,\u22121, 0, 0, 1,\u22121)>.9\n8 Though we are only interested in cutsets here, this statement holds more generally for any cut of the graph. 9 Any other path connecting 6 to 7 in G would yield the same representation. For instance, going back to Figure 6 (a), consider path p\u2032(6\u21927) = 6 \u2192 4 \u2192 7, whose edges are not in T . This gives p\u2032 = (0, 0, 0, 0, 0, 0, 0, 0,\u22121, 1, 0, 0)>.\nIt is important to observe that computing Qp does not require full knowledge of matrix Q, since Qp only depends on T and the way its edges are traversed. With the above handy, we are ready to prove Theorem 5.\nProof of Theorem 5 For the constructed spanning tree10 T , let uk \u2208 {\u22121, 0, 1}n\u22121 be the vector of coefficients representing the k-th class label w.r.t. the fundamental cutset matrix Q associated with T , and set U = \u2211K k=1 uku > k . Also, let xt be the instance vector computed by the algorithm at time t. Observe that, by the way xt is constructed (see Figure 7 for an illustrative example) we have xt = Qpt = p \u2032 t for all t, being pt and p \u2032 t the path vectors alluded at above. For any given class k, we have that u>k xt = u > k p \u2032 t. Recall that vector uk contains +1 in each component corresponding to an\nYet, Qp\u2032 = Qp = (1,\u22121, 0, 0, 1,\u22121)>. This invariance holds in general: Given the pair (i, j), the quantity Qp is independent of p, if we let p vary over all paths in G departing from i and arriving at j. This common value is the (n \u2212 1)-dimensional vector containing the edges in the unique path in T joining i and j (taking traversal directions into account). Said differently, once we are given T , the quantity Qp only depends on i and j, not on the path chosen to connect them. This invariance easily follows from the fact that cuts are orthogonal to circuits, see, e.g., [19, Ch.6].\n10 If less than n \u2212 1 edges end up being revealed, the set of edges maintained by the algorithm cannot form a spanning tree of G. Hence T can be taken to be any spanning tree of G including all the revealed edges.\noutward branch of T , \u22121 in each component corresponding to an inward branch, and 0 otherwise. We distinguish four cases (see Figure 6 (c), for reference):\n1. it and jt are both in the k-th class. In this case, the path in T that connects it to jt must exit and enter the k-th class the same number of times (possibly zero). Since we only traverse branches, we have in the dot product u>k p \u2032 t an equal number of +1 terms (corresponding to\ndepartures from the k-th class) and \u22121 (corresponding to arrivals). Hence u>k p\u2032t = 0. Notice that this applies even when the k-th class is not connected.\n2. it is in the k-th class, but jt is not. In this case, the number of departures from the k-th class should exceed the number of arrivals by exactly one. Hence we must have u>k p \u2032 t = 1.\n3. it is not in the k-th class, but jt is. By symmetry (swapping it with jt), we have u > k p \u2032 t = \u22121.\n4. Neither it nor jt is in the k-th class. Again, we have an equal number of arrival/departures to/from the k-th class (possibly zero), hence u>k p \u2032 t = 0.\nWe are now in a position to state our linear separability condition. We can write\nvec(U)>vec(Xt) = tr(U >Xt) = tr( K\u2211 k=1 uku > k xtx > t ) = K\u2211 k=1 (u>k xt) 2 = K\u2211 k=1 (u>k p \u2032 t) 2,\nwhich is 2 if it and jt are in different classes (i.e., it and jt are dissimilar), and 0, otherwise (i.e., it and jt are similar). We have therefore obtained that the label yt associated with (it, jt) is delivered by the following linear-threshold function:\nyt = { 1 if vec(U)>Xt \u2265 1 0 otherwise .\n(9)\nBecause we can interchangeably view vec(\u00b7) as vectors or matrices, this opens up the possibility of running any linear-threshold learning algorithm (on either vectors or matrices). For r-norm Perceptrons with the selected norm r and decision threshold, we have a bound on the number M of mistakes of the form [20, 17]\nM = O ( ||vec(U)||21 ||vec(Xt)||2\u221e log n ) ,\nwhere\n||vec(U)||1 = ||vec( K\u2211 k=1 uku > k )||1 = || K\u2211 k=1 vec(uku > k )||1 \u2264 K\u2211 k=1 ||vec(uku>k )||1 = K\u2211 k=1 ||uk||21 ,\nand ||vec(Xt)||\u221e = ||vec(xtx>t )||\u221e = ||xt||2\u221e = 1 .\nMoreover, by the way vectors uk are constructed, we have ||uk||1 = |\u03a6Tk |. In turn, |\u03a6Tk | \u2264 |\u03a6Gk | holds independent of the connectedness of the k-th cluster. Putting together and upper bounding concludes the proof."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>We consider online similarity prediction problems over networked data. We begin by relat-<lb>ing this task to the more standard class prediction problem, showing that, given an arbitrary<lb>algorithm for class prediction, we can construct an algorithm for similarity prediction with<lb>\u201cnearly\u201d the same mistake bound, and vice versa. After noticing that this general construction<lb>is computationally infeasible, we target our study to feasible similarity prediction algorithms on<lb>networked data. We initially assume that the network structure is known to the learner. Here<lb>we observe that Matrix Winnow [47] has a near-optimal mistake guarantee, at the price of cubic<lb>prediction time per round. This motivates our effort for an efficient implementation of a Percep-<lb>tron algorithm with a weaker mistake guarantee but with only poly-logarithmic prediction time.<lb>Our focus then turns to the challenging case of networks whose structure is initially unknown<lb>to the learner. In this novel setting, where the network structure is only incrementally revealed,<lb>we obtain a mistake-bounded algorithm with a quadratic prediction time per round.", "creator": "LaTeX with hyperref package"}}}