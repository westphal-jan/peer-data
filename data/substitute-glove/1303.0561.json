{"id": "1303.0561", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2013", "title": "Top-down particle filtering for Bayesian decision trees", "abstract": "Decision tree learning not that best approach up classification with parameter however prototype creative and survey, however Bayesian interchangeable - - - an introduce his date produces both decision overgrown, different framework subjects with posterior nonlinear to data - - - them it their to produce competitive performance. Unlike style if tree mental predefined those ID3, C4. 0 included CART, which work by a leads - dropping acceptable, upgrade Bayesian estimation parts form algorithmic to the transverse marketing others evolving another complete patch (or created constitutes) iteratively via offices Monte Carlo renovations could from frame of. bright, website. g. , are Markov chain Monte Carlo (MCMC ). We years first 6-speed Monte Carlo (SMC) optimization that instead life in a top - out thoughtful, playfully the behavior up driving while history automate. We importance inferring however we useful delivers validation comparable well the sometimes popular MCMC variations, not operates with about no seek created temblor instead, and itself represents it should computation - coherence tradeoff.", "histories": [["v1", "Sun, 3 Mar 2013 20:36:44 GMT  (6100kb,D)", "https://arxiv.org/abs/1303.0561v1", null], ["v2", "Thu, 22 Aug 2013 23:10:00 GMT  (6099kb,D)", "http://arxiv.org/abs/1303.0561v2", "ICML 2013"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["balaji lakshminarayanan", "daniel m roy", "yee whye teh"], "accepted": true, "id": "1303.0561"}, "pdf": {"name": "1303.0561.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DANIEL M. ROY"], "emails": [], "sections": [{"heading": null, "text": "regression in machine learning and statistics, and Bayesian formulations\u2014 which introduce a prior distribution over decision trees, and formulate learning as posterior inference given data\u2014have been shown to produce competitive performance. Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, which work in a top-down manner, existing Bayesian algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection thereof) iteratively via local Monte Carlo modifications to the structure of the tree, e.g., using Markov chain Monte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that instead works in a top-down manner, mimicking the behavior and speed of classic algorithms. We demonstrate empirically that our approach delivers accuracy comparable to the most popular MCMC method, but operates more than an order of magnitude faster, and thus represents a better computation-accuracy tradeoff."}, {"heading": "1. Introduction", "text": "Decision tree learning algorithms are widely used across statistics and machine learning, and often deliver near state-of-the-art performance despite their simplicity. Decision trees represent predictive models from an input space, typically RD, to an output space of labels, and work by specifying a hierarchical partition of the input space into blocks. Within each block of the input space, a simple model predicts labels.\nThis version is identical in content to \u201cTop-down particle filtering for Bayesian decision trees\u201d in Proceedings of the 30th International Conference on Machine Learning (ICML 2013), and differs only in typographic layout.\nar X\niv :1\n30 3.\n05 61\nv2 [\nIn classical decision tree learning, a decision tree (or collection thereof) is learned in a greedy, top-down manner from the examples. Examples of classical approaches that learn single trees include ID3 (Quinlan, 1986), C4.5 (Quinlan, 1993) and CART (Breiman et al., 1984), while methods that learn combinations of decisions trees include boosted decision trees (Friedman, 2001), Random Forests (Breiman, 2001), and many others.\nBayesian decision tree methods, like those first proposed by Buntine (1992), Chipman et al. (1998), Denison et al. (1998), and Chipman and McCulloch (2000), and more recently revisited by Wu et al. (2007), Taddy et al. (2011) and Anagnostopoulos and Gramacy (2012), cast the problem of decision tree learning into the framework of Bayesian inference. In particular, Bayesian approaches start by placing a prior distribution on the decision tree itself. To complete the specification of the model, it is common to associate each leaf node with a parameter indexing a family of likelihoods, e.g., the means of Gaussians or Bernoullis. The labels are then assumed to be conditionally independent draws from their respective likelihoods. The Bayesian approach has a number of useful properties: e.g., the posterior distribution on the decision tree can be interpreted as reflecting residual uncertainty and can be used to produce point and interval estimates.\nOn the other hand, exact posterior computation is typically infeasible and so existing approaches use approximate methods such as Markov chain Monte Carlo (MCMC) in the batch setting. Roughly speaking, these algorithms iteratively improve a complete decision tree by making a long sequence of random, local modifications, each biased towards tree structures with higher posterior probability. These algorithms stand in marked contrast with classical decision tree learning algorithms like ID3 and C4.5, which rapidly build a decision tree for a data set in a top-down greedy fashion guided by heuristics. Given the success of these methods, one might ask whether they could be adapted to work in the Bayesian framework.\nIn this article, we present such an adaptation, proposing a sequential Monte Carlo (SMC) method for approximate inference in Bayesian decision trees that works by sampling a collection of trees in a top-down manner like ID3 and C4.5. Unlike classical methods, there is no pruning stage after the top-down learning stage to prevent over-fitting, as the prior combines with the likelihood to automatically cut short the growth of the trees, and resampling focuses attention on those trees that better fit the data. In the end, the algorithm produces a collection of sampled trees that approximate the posterior distribution. While both existing MCMC algorithms and our novel SMC algorithm produce approximations to the posterior that are exact in the limit, we show empirically that our algorithms run more than an order of magnitude faster than existing methods while delivering the same predictive performance.\nThe article is organized as follows: we begin by describing the Bayesian decision tree model precisely in Section 2, and then describe the SMC algorithm in detail in Section 3. Through a series of empirical tests, we demonstrate in Section 4 that this approach is fast and produces good approximations. We conclude in Section 5 with a discussion comparing this approach with existing ones in the Bayesian setting, and point towards future avenues."}, {"heading": "2. Model and notation", "text": "In this section, we present the decision tree model for the distribution of the labels Y = {yn}Nn=1 corresponding to input vectors X = {xn}Nn=1, xn \u2208 RD. The assumption is that the probabilistic mapping from input vectors to their labels is mediated by a latent decision tree T that serves to partition the input space into axis-aligned blocks. Each block is then associated with a parameter that determines the distribution of the labels of the input vectors falling in that block.\nA rooted, strictly binary tree T is a finite tree with a single root, denoted by the empty string , where each internal node p except the root has exactly two children, called the left child p0 and the right child p1. Denote the leaves of T (those nodes without children) by \u2202T. Each node of the tree p \u2208 T is associated with a block B(p) \u2282 RD of the input space as follows: At the root we have B( ) = RD, while each internal node p \u2208 T \\ \u2202T \u201ccuts\u201d its block into two halves, with \u03ba(p) \u2208 {1, . . . , D} denoting the dimension of the cut, and \u03c4(p) denoting the location of the cut, so that\nB(p0) = B(p) \u2229 {z \u2208 RD : z\u03ba(p) \u2264 \u03c4(p)} and B(p1) = B(p) \u2229 {z \u2208 RD : z\u03ba(p) > \u03c4(p)}. (1)\nWe call the tuple T = (T, \u03ba, \u03c4) the decision tree. (See Figure 1 for more intuition on the representation and notation of decision trees.) Note that the blocks associated with the leaves of the tree partition RD. It will be convenient to write N(p) for the set of data point indices n \u2208 {1, . . . , N} such that xn \u2208 B(p). For every subset A \u2286 {1, . . . , N}, let YA := {yn : n \u2208 A} and similarly for XA, so that XN(p) are the input vectors in block B(p) and YN(p) are their labels. Note that both B(p) and N(p) depend on T , although we have chosen to elide this dependence for notational simplicity.\nConditioned on the examples X, we assume that the joint density f(Y, T |X) of the labels Y and the latent decision tree T factorizes as follows:\nf(Y, T |X) = h(T |X) g(Y | T , X) = h(T |X) \u220fp\u2208\u2202T `(YN(p)|XN(p)) (2)\nwhere ` denotes a likelihood, defined below. In this paper, we focus on the case of categorical labels taking values in the set {1, . . . ,K}. It is natural to take ` to be the Dirichlet-Multinomial likelihood, corresponding to the data being conditionally i.i.d. draws from a multinomial distribution on {1, . . . ,K} with a Dirichlet prior. In particular,\n`(YN(p)|XN(p)) = \u0393(\u03b1)\n\u0393( \u03b1K ) K\n\u220fK k=1 \u0393(mpk + \u03b1 K )\n\u0393( \u2211K k=1mpk + \u03b1) , (3)\nwhere mpk denotes the number of labels yn = k among those n \u2208 N(p) and \u03b1 is the concentration parameter of the symmetric Dirichlet prior. Generalisations to other likelihood functions based on conjugate pairs of exponential families are straightforward.\nThe final piece of the model is the prior density h(T |X) over decision trees. In order to make straightforward comparisons with existing algorithms, we adopt the model proposed by Chipman et al. (1998). In this model, the prior distribution of the latent tree is defined conditionally on the given input vectors X (see Section 5 for a discussion of this dependence on X and its effect on the exchangeability of\nthe labels). Informally, the tree is grown starting at the root, and each new node either splits and grows two children (turning the node into an internal node) or stops (leaving it a leaf) stochastically.\nWe now describe the generative process more precisely in terms of a Markov chain capturing the construction of a decision tree in stages, beginning with the trivial tree T0 = { } containing only the root node. At each stage i, Ti is produced from Ti\u22121 by choosing one leaf in Ti\u22121 and either growing two children nodes or stopping the leaf. Once stopped, a leaf is ineligible for future growth. The identity of the chosen leaf is deterministic, while the choice to grow or stop is stochastic. The process proceeds until all leaves are stopped, and so each node is considered for expansion exactly once throughout the process. This will be seen to give rise to a finite sequence of decision trees Ti = (Ti, \u03bai, \u03c4i) once we define the associated cut functions \u03bai and \u03c4i. We will use this Markov chain in Section 3 as scaffolding for a sequential Monte Carlo algorithm. A similar approach was employed by Taddy et al. (2011) in the setting of online Bayesian decision trees. There are similarities also with the bottom-up SMC algorithms by Teh et al. (2008) and Bouchard-Co\u0302te\u0301 et al. (2012).\nWe next describe the rule for stopping or growing nodes, and the distribution of cuts. Let p be the node chosen at some stage of the generative process. If the input vectors XN(p) are all identical, then the node stops and becomes a leaf. (Chipman et al. chose this rule because no choice of cut to the block B(p) would result in both children containing at least one input vector.) Otherwise, let Dp be the set of dimensions along which XN(p) varies, and let I p d = [minn\u2208N(p) xnd,maxn\u2208N(p) xnd] be the range of the input vectors along dimension d \u2208 Dp. (See last subfigure of Figure 1.) Under the Chipman et al. model, the probability that node p is split is\n\u03b1s (1 + |p|)\u03b2s , \u03b1s \u2208 (0, 1), \u03b2s \u2208 [0,\u221e), (4)\nwhere |p| is the depth of the node, and \u03b1s and \u03b2s are parameters governing the shape of the resulting tree. For larger \u03b1s and smaller \u03b2s the typical trees are larger, while the deeper p is in the tree the less likely it will be cut. If p is cut, the dimension \u03ba(p) and then location \u03c4(p) of the cut are sampled uniformly from Dp and Ip\u03ba(p), respectively. Note that the choice for the support of the distribution over cut dimensions and locations are such that both children of p will, with probability one, contain at least one input vector. Finally, the choices of whether to grow or stop, as well the cut dimensions and locations, are conditionally independent across different subtrees.\nTo complete the generative model, we define T = T\u03b7, \u03ba = \u03ba\u03b7 and \u03c4 = \u03c4\u03b7, where \u03b7 is the first stage such that all nodes are stopped. We note that \u03b7 < 2N with probability one because each cut of a node p produces a non-trivial partition of the data in the block, and a node with one data point will be stopped instead of cut. The conditional density of the decision tree T = (T, \u03ba, \u03c4) can now be expressed as\nh(T, \u03ba, \u03c4 |X) = \u220f p\u2208\u2202T ( 1\u2212 \u03b1s (1 + |p|)\u03b2s )1(|Dp|>0) \u220f p\u2208T\\\u2202T \u03b1s (1 + |p|)\u03b2s 1 |Dp| 1 |Ip\u03ba(p)| . (5)\nNote that the prior distribution of T does not depend on the deterministic rule for choosing a leaf at each stage. However this choice will have an effect on the bias/variance of the corresponding SMC algorithm."}, {"heading": "3. Sequential Monte Carlo (SMC) for Bayesian decision trees", "text": "In this section we describe an SMC algorithm for approximating the posterior distribution over the decision tree (T, \u03ba, \u03c4) given the labeled training data (X,Y ). (We refer the reader to (Cappe\u0301 et al., 2007) for an excellent overview of SMC techniques.) The approach we will take is to perform particle filtering following the sequential description of the prior. In particular, at stage i, the particles approximate a modified posterior distribution where the prior on (T, \u03ba, \u03c4) is replaced by the distribution of (Ti, \u03bai, \u03c4i), i.e., the process truncated at stage i.\nLet Ei denote the set of unstopped leaves at stage i, all of which are eligible for expansion. An important freedom we have in our SMC algorithm is the choice of which candidate leaf (or set Ci \u2286 Ei of candidate leaves) to consider expanding. In order to avoid \u201cmultipath\u201d issues (Del Moral et al., 2006, \u00a73.5) which lead to high variance, we fix a deterministic rule for choosing Ci \u2286 Ei. (Multiple candidates are expanded or stopped in turn, independently.) This rule can be a function of (X,Y ) and the state of the current particle, as the correctness of resulting approximation is unaffected. We evaluate two choices in experiments: first, the rule Ci = Ei where we consider expanding all eligible nodes; and second, the rule where Ci contains a single node chosen in a breadth-first (i.e., oldest first) manner from Ei.\nWe may now define the sequence (PYi ) of target distributions. Recall the sequential process defined in Section 2. If the generative process for the decision tree has not completed by stage i, the process has generated (Ti, \u03bai, \u03c4i) along with Ei, capturing which leaves in Ti have been considered for expansion in previous stages already and which have not. Let Ti = (Ti, \u03bai, \u03c4i, Ei) be the variables generated on stage i, and write P for the prior distribution on the sequence (Ti). We construct the target distribution PYi as follows: Given Ti, we generate labels Y \u2032 with likelihood g(Y \u2032|Ti, X), i.e., as if (Ti, \u03bai, \u03c4i) were the complete decision tree. We then define PYi to be the conditional distribution of Ti given Y \u2032 = Y . That is, PYi is the posterior with a truncated prior.\nIn order to complete the description of our SMC method, we must define proposal kernels (Qi) that sample approximations for the ith stage given values for the (i \u2212 1)th stage. As with our choice of Ci, we have quite a bit of freedom. In particular, the proposals can depend on the training data (X,Y ). An obvious choice is to take Qi to be the conditional distribution of Ti given Ti\u22121 under the prior, i.e., setting Qi(Ti | Ti\u22121) = P(Ti | Ti\u22121). Informally, this choice would lead us to propose extensions to trees at each stage of the algorithm by sampling from the prior, so we will refer to this as the prior proposal kernel (aka the Bayesian bootstrap filter (Gordon et al., 1993)).\nWe consider two additional proposal kernels: The first,\nQi(Ti | Ti\u22121) = PYi (Ti | Ti\u22121), (6)\nis called the (one-step) optimal proposal kernel because it would be the optimal kernel assuming that the ith stage were the final stage. We return to discuss this kernel in Section 3.1. The second alternative, which we will refer to as the empirical proposal kernel, is a small modification to the prior proposal, differing only in the choice of the split point \u03c4 . Recall that, in the prior, \u03c4i(p) is chosen uniformly from the interval Ip\u03bai(p). This ignores the empirical distribution given by the input data XN(p) in the partition. We can account for this by first choosing, uniformly at random, a pair of adjacent data points along feature dimension \u03bai(p),\nand then sampling a cut \u03c4i(p) uniformly from the interval between these two data points.\nThe pseudocode for our proposed SMC algorithm is given in Algorithm 1 in Appendix A. Note that the SMC framework only requires us to compute the density of Ti under the target distribution up to a normalization constant. In fact, the SMC algorithm produces an estimate of the normalization constant, which, at the end of the algorithm, is equal to the marginal probability of the labels Y given X, with the latent decision tree T marginalized out. In general, the joint density of a Markov chain can be hard to compute, but because the set of nodes Ci considered at each stage is a deterministic function of Ti, the path (T0, T1, . . . , Ti\u22121) taken is a deterministic function of Ti. As a result, the joint density is simply a product of probabilities for each stage. The same property holds for the proposal kernels defined above because they use the same candidate set Ci, and have the same support as P. These properties justify the equations in Algorithm 1.\n3.1. The one-step optimal proposal kernel. In this section we revisit the definition of the one-step optimal proposal kernel. While the prior and empirical proposal kernels are relatively straightforward, the one-step optimal proposal kernel is defined in terms of an additional conditioning on the labels Y , which we now study in greater detail.\nRecall that the one-step optimal proposal kernel Qi is given by Qi(Ti | Ti\u22121) = PYi (Ti | Ti\u22121). To begin, we note that, conditionally on Ti\u22121 and Y , the subtrees rooted at each node p \u2208 Ci\u22121 are independent. This follows from the fact that the likelihood of Y given Ti factorizes over the leaves. Thus, the proposal\u2019s probability density is\nQi(Ti|Ti\u22121) = \u220f\np\u2208Ci\u22121\nQi(\u03c1i,p, \u03bai(p), \u03c4i(p)), (7)\nwhere Qi is the probability density of the cuts at node p under Qi, and \u03c1i,p denotes whether the node was split or not. On the event we split a node p \u2208 Ci\u22121, if we condition further on \u03bai(p) and \u03c1i,p, we note that the conditional likelihood of YN(p), when viewed as a function of the split \u03c4i(p), is piecewise constant, and in particular, only changes when the split crosses an example. It follows that we can sample from this proposal by first considering the discrete choice of an interval, and then sampling uniformly at random from within the interval, as with the empirical proposal. Some algebra shows that\nQi(\u03c1i,p = stop) \u221d (\n1\u2212 \u03b1s (1 + |p|)\u03b2s\n) `(YN(p)|XN(p)) ,\nand\nQi(\u03c1i,p = split, \u03bai(p), \u03c4i(p)) \u221d \u03b1s (1 + |p|)\u03b2s 1 |Dp| 1 |Ip\u03bai(p)| \u220f j=0,1 `(YN(pj)|XN(pj)).\n3.2. Computational complexity. Let Ud denote the number of unique values in dimension d, Np denote the number of training data points at node p and \u03b7(m) denote the number of nodes in particle m. For all the SMC algorithms, the space complexity is O(MN) + O(\u2211d Ud) + O(\u2211m \u03b7(m)). The time complexity is O(DN logN) + M\u2211pO(2DNp + Np) for prior and empirical proposals and\nM \u2211 p ( DO(Np logNp) +Np ) for the optimal proposal. The optimal proposal typically requires higher computational cost per particle, but fewer number of particles than the prior and empirical proposals."}, {"heading": "4. Experiments", "text": "In this section, we experimentally evaluate the design choices of the SMC algorithm (proposal, expansion strategy, number of particles and \u201cislands\u201d) on real world datasets. In addition, we compare the performance of SMC to the most popular MCMC method for Bayesian decision tree learning (Chipman et al., 1998), as well as CART, a popular (non-Bayesian) tree induction algorithm. We evaluate all the algorithms on the following datasets from the UCI ML repository (Asuncion and Newman, 2007):\n\u2022 MAGIC gamma telescope data 2004 (magic-04 ): N = 19020, D = 10, K = 2. \u2022 Pen-based recognition of handwritten digits (pen-digits): N = 10992, D = 16, K = 10.\nPrevious work has focused mainly on small datasets (e.g., the Wisconsin breast cancer database used by Chipman et al. (1998) has 683 data points). We chose the above datasets to illustrate the scalability of our approach. For the pen-digits dataset, we used the predefined training/test splits, while for the other datasets, we split the datasets randomly into a training set and a test set containing approximately 70% and 30% of the data points respectively.\nWe implemented our scripts in Python and applied similar software optimization techniques to SMC and MCMC scripts. Our experiments were run on a cluster with machines of similar processing power.\n4.1. Design choices in the SMC algorithm. In these set of experiments, we fix the hyperparameters to \u03b1 = 5.0, \u03b1s = 0.95, \u03b2s = 0.5 and compare the predictive performance of different configurations of the SMC algorithm for this fixed model. Under the prior, these values of \u03b1s, \u03b2s produce trees whose mean depth and number of nodes are 5.1 and 18.5, respectively. Given M particles, we use an effective sample size (ESS) threshold of M/10 and set the maximum number of stages to 5000 (although the algorithms never reached this number).\n4.1.1. Proposal choice and node expansion. We consider the SMC algorithm proposed in Section 3 under two proposals: optimal and prior. (The empirical proposal performed similar to the prior proposal and hence we do not report those results here.) We consider two strategies for choosing Ci, i.e., the list of nodes considered for expansion at stage i: (i) node-wise expansion, where a single node is considered for expansion per stage (i.e., Ci is a singleton chosen deterministically from eligible nodes Ei), and (ii) layer-wise expansion, where all nodes at a particular depth are considered for expansion simultaneously (i.e., Ci = Ei). For node-wise expansion, we evaluate two strategies for selecting the node deterministically from Ci: (i) breadth-first priority, where the oldest node is picked first, and (ii) marginal-likelihood based priority, where we expand the node with the lowest marginal likelihood. Both of these priority schemes performed similarly; hence we report only the results for breadth-first priority. We use multinomial resampling in\nThe scripts can be downloaded from the authors\u2019 webpages.\nour experiments. We also evaluated systematic resampling (Douc et al., 2005) but found that the performance was not significantly different.\nWe report the log predictive probability on test data as a function of runtime and of the number of particles (similar trends are observed for test accuracy; see Appendix B). The times reported do not account for prediction time. We average the numbers over 10 random initializations and report standard deviations. The results are shown in Figure 2. In summary, we observe the following:\n(1) node-wise expansion outperforms layer-wise expansion for prior proposal. The prior proposal does not account for likelihood; one could think of the resampling steps as \u2018correction steps\u2019 for the sub-optimal decisions sampled from the prior proposal. Because node-wise expansion can potentially resample at every stage, it can correct individual bad decisions immediately, whereas layer-wise expansion cannot. In particular, we have observed that layer-wise expansion tends to produce shallower trees compared to node-wise expansion, leading to poorer performance. This phenomenon can be explained as follows: as the depth of the node increases, the prior probability of stopping increases whereas the posterior probability of stopping might be quite low. In node-wise expansion, the resampling step can potentially retain the particles where the node has not been stopped. However, in layer-wise expansion, too many nodes might have stopped prematurely and the resampling step cannot \u2018correct\u2019 all these bad decisions easily (i.e., it would require many more particles to sample trees where all the nodes in a layer have not been stopped). Another interesting observation is that layer-wise expansion exhibits higher variance: this can be explained by the fact that layer-wise expansion samples a greater number of random variables (on average) than node-wise before resampling, and so suffers for the same reason that importance sampling can suffer from high variance. Note that both expansion strategies perform similarly for the optimal proposal due to the fact that the proposal accounts for the likelihood and resampling does not affect the results significantly. Due to its superior performance, we consider only node-wise expansion in the rest of the paper.\n(2) The plots on the right side of Figure 2 suggest that the optimal proposal requires fewer particles than the prior proposal (as expected). However, the per-stage cost of optimal proposal is much higher than the prior, leading to significant increase in the overall runtime (see Section 3.2 for a related discussion). Hence, the prior proposal offers a better predictive performance vs computation time tradeoff than the optimal proposal.\n(3) The performance of optimal proposal saturates very quickly and is near-optimal even when the number of particles is small (M = 10).\n4.1.2. Effect of irrelevant features. In the next experiment, we test the effect of irrelevant features on the performance of the various proposals. We use the madelon dataset for this experiment, in which the data points belong to one of 2 classes and lie in a 500-dimensional space, out of which only 20 dimensions are deemed relevant. The training dataset contains 2000 data points and the test dataset contains 600 data points. We use the validation dataset in the UCI ML repository as our test set because labels are not available for the test dataset.\nhttp://archive.ics.uci.edu/ml/datasets/Madelon\nThe setup is identical to the previous section. The results are shown in Figure 3. Here, the optimal proposal outperforms the prior proposal in both the columns, requiring fewer particles as well as outperforming the prior proposal for a given computational budget. While this dataset is atypical (only 4% of the features are relevant), it illustrates a potential vulnerability of the prior proposal to irrelevant features.\n4.1.3. Effect of the number of islands. Averaging the results of several independent particle filters (aka islands) is a way to reduce variance at the cost of bias, compared with running a single, larger filter. In the asymptotic regime, this would not make sense, but as we will see, performance is improved with multiple islands, suggesting we are not yet in the asymptotic regime. In this experiment, we evaluate the effect of the number of islands on the test performance of the prior proposal. We fix the total number of particles to 2000 and vary I, the number of islands (and hence, the number of particles per island). Note that all the islands operate on the entire dataset unlike bagging. Here, we present results only on the pen-digits dataset (see Appendix C for results on the magic-04 dataset). The results are shown in Figure 4. We observe that (i) the test performance drops sharply if we\nuse fewer than 100 particles per island and (ii) when M/I \u2265 100, the choices of I \u2208 [5, 100] outperform I = 1. Since the islands are independent, the computation across islands is \u2018embarrassingly parallelizable\u2019.\n4.2. SMC vs MCMC. In this experiment, we compare the SMC algorithms to the MCMC algorithm proposed by Chipman et al. (1998), which employs four types of Metropolis-Hastings proposals: grow (split a leaf node into child nodes), prune (prune a pair of leaf nodes belonging to the same parent), change (change the decision rule at a node) and swap (swap the decision rule of a parent with the decision rule of the child). In our experiments, we average the MCMC predictions over the trees from all previous iterations.\nThe experimental setup is identical to Section 4.1, except that we fix the number of islands, I = 5. We vary the number of particles for SMC and the number of iterations for MCMC and plot the log predictive probability and accuracy on the test data as a function of runtime. In Figure 5, we observe that SMC (prior, nodewise) is roughly two orders of magnitude faster than MCMC while achieving similar predictive performance on pen-digits and magic-04 datasets. Although the exact speedup factor depends on the dataset in general, we have observed that SMC (prior, node-wise) is at least an order of magnitude faster than MCMC. The SMC runtimes in Figure 5 are recorded by running the I islands in a serial fashion. As discussed in Section 4.1.3, one could parallelize the computation leading to an additional speedup by a factor of I. In the pen-digits dataset, the performance of prior proposal seems to drop as we increase M beyond 2000. However, the marginal likelihood on the training data increases with M (see Appendix D). We believe that the deteriorating performance is due to model misspecification (axisaligned decision trees are hardly the \u2018right\u2019 model for handwritten digits) rather than the inference algorithm itself: \u2018better\u2019 Bayesian inference in a misspecified model might lead to a poorer solution (see (Minka, 2000) for a related discussion).\nTo evaluate the sensitivity of the trends above to the hyper parameters \u03b1, \u03b1s, \u03b2s, we systematically varied the values of these hyper parameters and repeated the experiment. The results are qualitatively similar. See Appendix E for additional information.\n4.3. SMC vs other existing approaches. The goal of these experiments was to verify that our SMC approximation performed as well as the \u201cgold standard\u201d MCMC algorithms most commonly used in the Bayesian decision tree learning setting. Indeed, our results suggest that, for a fraction of the computational budget, we can achieve a comparable level of accuracy. In this final experiment, we reaffirm that the Bayesian algorithms are competitive in accuracy with the classic CART algorithm. (There are many other comparisons that one could pursue and other authors have already performed such comparisons. E.g., Taddy et al. (2011) demonstrated that their tree structured models yield similar performance as Gaussian processes and random forests.) We used the CART implementation provided by scikit-learn (Pedregosa et al., 2011) with two criteria: gini purity and information gain and set min samples leaf = 10 (minimum number of data points at a leaf node). In addition, we performed Laplacian smoothing on the probability estimates from CART using the same \u03b1 as for the Bayesian methods. Our Python implementation of SMC takes about 50-100x longer to achieve the same test accuracy\nWe fix I = 5 so that the minimum value of M (= 100) corresponds to M/I = 20 particles per island. Further improvements could be obtained by \u2018adapting\u2019 I to M as discussed in Section 4.1.3. Lower values (min samples leaf = 1, 5) tend to yield slightly higher test accuracies (comparable to SMC and MCMC) but much lower predictive probabilities.\nas the highly-optimized implementation of CART. For this reason, we plot CART accuracy as a horizontal bar. The accuracy and log predictive probability on test data are shown in Figure 5. The Bayesian decision tree frameworks achieve similar (or better) test accuracy to CART, and outperform CART significantly in terms of the predictive likelihood. SMC delivers the benefits of having an approximation to the posterior, but in a fraction of the time required by existing MCMC methods."}, {"heading": "5. Discussion and Future work", "text": "We have proposed a novel class of Bayesian inference algorithms for decision trees, based on the sequential Monte Carlo framework. The algorithms mimic classic top-down algorithms for learning decision trees, but use \u201clocal\u201d likelihoods along with resampling steps to guide tree growth. We have shown good computational and statistical performances, especially compared with a state-of-the-art MCMC inference algorithm. Our algorithms are easier to implement than their MCMC counterparts, whose efficient implementations require sophisticated book-keeping.\nWe have also explored various design choices leading to different SMC algorithms. We have found that expanding too many nodes simultaneously degraded performance, and more sophisticated ways of choosing nodes surprisingly did not\nimprove performance. Finally, while the one-step optimal proposal often required fewer particles to achieve a given accuracy, it was significantly more computationally intensive than the prior proposal, leading to a less efficient algorithm overall on datasets with few irrelevant input dimensions. As the number of irrelevant dimensions increased the balance tipped in favour of the optimal proposal. An interesting direction of exploration is to devise some way to interpolate between the prior and optimal proposals, getting the best of both worlds.\nThe model underlying this work assumes that the data is explained by a single tree. In contrast, many uses of decision trees, e.g., random forests, bagging, etc., can be interpreted as working within a model class where the data is explained by a collection of trees. Bayesian additive regression trees (BART) (Chipman et al., 2010) are such a model class. Prior work has considered MCMC techniques for posterior inference (Chipman et al., 2010). A significant but important extension of this work would be to tackle additive combinations of trees, potentially in a way that continues to mimic classic algorithms.\nFinally, in order to more closely match existing work in Bayesian decision trees, we have used a prior over decision trees that depends on the input data X. This has the undesirable side-effect of breaking exchangeability in the model, making it incoherent with respect to changing dataset sizes and to working with online data streams. One solution is to use an alternative prior for decision trees, e.g., based on the Mondrian process (Roy and Teh, 2009), whose projectivity would re-establish exchangeability while allowing for efficient posterior computations that depend on data."}, {"heading": "Acknowledgments", "text": "We would like to thank Charles Blundell, Arnaud Doucet, David Duvenaud, Jan Gasthaus, Hong Ge, Zoubin Ghahramani, and James Robert Lloyd for helpful discussions and feedback on drafts. DMR is supported by a Newton International Fellowship and Emmanuel College. BL and YWT gratefully acknowledge generous funding from the Gatsby Charitable Foundation."}, {"heading": "Appendix A. SMC algorithm", "text": "Algorithm 1 SMC for Bayesian decision tree learning\nInputs: Training data (X,Y ) Number of particles M Initialize: T (m) 0 = E (m) 0 = { }\n\u03c4 (m) 0 = \u03ba (m) 0 = \u2205 w (m) 0 = f(Y |T (m) 0 )\nW0 = \u2211 m w (m) 0\nfor i = 1 : MAX-STAGES do for m = 1 : M do\nSample T (m)i from Qi(\u00b7 | T (m) i\u22121 )\nwhere T (m)i := (T (m) i , \u03ba (m) i , \u03c4 (m) i , E (m) i )\nUpdate weights: (Here P,Qi denote their densities.)\nw (m) i =\nP(T (m)i ) g(Y | T (m) i , X)\nQi(T (m)i | T (m) i\u22121 )P(T (m) i\u22121 )\n(8)\n= w (m) i\u22121\nP(T (m)i | T (m) i\u22121 )\nQi(T (m)i | T (m) i\u22121 ) g(Y | T (m)i , X) g(Y | T (m)i\u22121 , X)\n(9)\nend for Compute normalization: Wi = \u2211 m w (m) i Normalize weights: (\u2200m) w\u0304(m)i = w (m) i /Wi\nif (\u2211\nm(w\u0304 (m) i )\n2 )\u22121\n< ESS-THRESHOLD then (\u2200m) Resample indices jm from \u2211 m\u2032 w\u0304 (m\u2032) i \u03b4m\u2032 (\u2200m) T (m)i \u2190 T (jm) i ; w (m) i \u2190Wi/M\nend if if (\u2200m)E(m)i = \u2205 then\nexit for loop end if\nend for return Estimated marginal probability Wi/M and\nweighted samples {w(m)i ,T (m) i , \u03ba (m) i , \u03c4 (m) i }Mm=1."}, {"heading": "Appendix B. Effect of SMC proposal and expansion strategy on test", "text": "accuracy\nThe results are shown in Figure 6.\nAppendix C. Effect of the number of islands: magic-04 dataset\nThe results are shown in Figure 7."}, {"heading": "Appendix D. Marginal likelihood", "text": "The log marginal likelihood of the training data for different proposals is shown in Figure 8. As the number of particles increases, the log marginal likelihood of prior and optimal proposals converge to the same value (as expected)."}, {"heading": "Appendix E. Sensitivity of results to choice of hyperparameters", "text": "In this experiment, we evaluate the sensitivity of the runtime vs predictive performance comparison between SMC (prior and optimal proposals), MCMC and CART to the choice of hyper parameters \u03b1 (Dirichlet concentration parameter) and \u03b1s, \u03b2s (tree priors). We consider only node-wise expansion since it consistently outperformed layer-wise expansion in our previous experiments. In the first variant, we fix \u03b1 = 5.0 (since we do not expect it to affect the timing results) and vary the hyper parameters from \u03b1s = 0.95, \u03b2s = 0.5 to \u03b1s = 0.8, \u03b2s = 0.2 (bold reflects changes) and also consider intermediate configurations \u03b1s = 0.95,\u03b2s = 0.2 and \u03b1s = 0.8, \u03b2s = 0.5. In the second variant, we fix \u03b1s = 0.95, \u03b2s = 0.5 and set \u03b1 = 1.0. Figures 9, 10, 11 and 12 display the results on pen-digits (top row), and magic-04 (bottom row). The left column plots test log p(y|x) vs runtime, while the right column plots test accuracy vs runtime. The blue circles and red squares represent optimal and prior proposals respectively. Comparing the results to Figure 5 (in main text), we observe that the trends are qualitatively similar to those observed for \u03b1 = 5.0, \u03b1s = 0.95, \u03b2s = 0.5 in Section 4.2 (in main text): (i) SMC consistently offers a better runtime vs predictive performance tradeoff than MCMC, (ii) the prior proposal offers a better runtime vs predictive performance tradeoff than the optimal proposal, (iii) \u03b1 = 1.0 leads to similar test accuracies as \u03b1 = 5.0 (the predictive probabilities are obviously not comparable)."}], "references": [{"title": "BART: Bayesian additive regression trees", "author": ["H.A. Chipman", "E.I. George", "R.E. McCulloch"], "venue": "Ann. Appl. Stat.,", "citeRegEx": "Chipman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chipman et al\\.", "year": 2010}, {"title": "Sequential Monte Carlo samplers", "author": ["P. Del Moral", "A. Doucet", "A. Jasra"], "venue": "J. R. Stat. Soc. Ser. B Stat. Methodol.,", "citeRegEx": "Moral et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moral et al\\.", "year": 2006}, {"title": "Comparison of resampling schemes for particle filtering", "author": ["R. Douc", "O. Capp\u00e9", "E. Moulines"], "venue": "In Image Sig. Proc. Anal.,", "citeRegEx": "Douc et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Douc et al\\.", "year": 2005}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Ann. Statist,", "citeRegEx": "Friedman.,? \\Q2001\\E", "shortCiteRegEx": "Friedman.", "year": 2001}, {"title": "Novel approach to nonlinear/nonGaussian Bayesian state estimation", "author": ["N.J. Gordon", "D.J. Salmond", "A.F.M. Smith"], "venue": "Radar Sig. Proc., IEE Proc. F,", "citeRegEx": "Gordon et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Gordon et al\\.", "year": 1993}, {"title": "Bayesian model averaging is not model combination", "author": ["T.P. Minka"], "venue": "MIT Media Lab note. http://research.microsoft.com/en-us/um/people/minka/ papers/bma.html,", "citeRegEx": "Minka.,? \\Q2000\\E", "shortCiteRegEx": "Minka.", "year": 2000}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine Learning,", "citeRegEx": "Quinlan.,? \\Q1986\\E", "shortCiteRegEx": "Quinlan.", "year": 1986}, {"title": "C4.5: programs for machine learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "Quinlan.,? \\Q1993\\E", "shortCiteRegEx": "Quinlan.", "year": 1993}, {"title": "The Mondrian process", "author": ["D.M. Roy", "Y.W. Teh"], "venue": "In Adv. Neural Information Proc. Systems,", "citeRegEx": "Roy and Teh.,? \\Q2009\\E", "shortCiteRegEx": "Roy and Teh.", "year": 2009}, {"title": "Dynamic trees for learning and design", "author": ["M.A. Taddy", "R.B. Gramacy", "N.G. Polson"], "venue": "J. Am. Stat. Assoc.,", "citeRegEx": "Taddy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taddy et al\\.", "year": 2011}, {"title": "Bayesian agglomerative clustering with coalescents", "author": ["Y.W. Teh", "H. Daum\u00e9 III", "D.M. Roy"], "venue": "In Adv. Neural Information Proc. Systems,", "citeRegEx": "Teh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2008}, {"title": "Bayesian CART: Prior specification and posterior simulation", "author": ["Y. Wu", "H. Tjelmeland", "M. West"], "venue": "J. Comput. Graph. Stat.,", "citeRegEx": "Wu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 6, "context": "Examples of classical approaches that learn single trees include ID3 (Quinlan, 1986), C4.", "startOffset": 69, "endOffset": 84}, {"referenceID": 7, "context": "5 (Quinlan, 1993) and CART (Breiman et al.", "startOffset": 2, "endOffset": 17}, {"referenceID": 3, "context": ", 1984), while methods that learn combinations of decisions trees include boosted decision trees (Friedman, 2001), Random Forests (Breiman, 2001), and many others.", "startOffset": 97, "endOffset": 113}, {"referenceID": 2, "context": ", 1984), while methods that learn combinations of decisions trees include boosted decision trees (Friedman, 2001), Random Forests (Breiman, 2001), and many others. Bayesian decision tree methods, like those first proposed by Buntine (1992), Chipman et al.", "startOffset": 98, "endOffset": 240}, {"referenceID": 0, "context": "Bayesian decision tree methods, like those first proposed by Buntine (1992), Chipman et al. (1998), Denison et al.", "startOffset": 77, "endOffset": 99}, {"referenceID": 0, "context": "Bayesian decision tree methods, like those first proposed by Buntine (1992), Chipman et al. (1998), Denison et al. (1998), and Chipman and McCulloch (2000), and more recently revisited by Wu et al.", "startOffset": 77, "endOffset": 122}, {"referenceID": 0, "context": "Bayesian decision tree methods, like those first proposed by Buntine (1992), Chipman et al. (1998), Denison et al. (1998), and Chipman and McCulloch (2000), and more recently revisited by Wu et al.", "startOffset": 77, "endOffset": 156}, {"referenceID": 0, "context": "Bayesian decision tree methods, like those first proposed by Buntine (1992), Chipman et al. (1998), Denison et al. (1998), and Chipman and McCulloch (2000), and more recently revisited by Wu et al. (2007), Taddy et al.", "startOffset": 77, "endOffset": 205}, {"referenceID": 0, "context": "Bayesian decision tree methods, like those first proposed by Buntine (1992), Chipman et al. (1998), Denison et al. (1998), and Chipman and McCulloch (2000), and more recently revisited by Wu et al. (2007), Taddy et al. (2011) and Anagnostopoulos and Gramacy (2012), cast the problem of decision tree learning into the framework of Bayesian inference.", "startOffset": 77, "endOffset": 226}, {"referenceID": 0, "context": "Bayesian decision tree methods, like those first proposed by Buntine (1992), Chipman et al. (1998), Denison et al. (1998), and Chipman and McCulloch (2000), and more recently revisited by Wu et al. (2007), Taddy et al. (2011) and Anagnostopoulos and Gramacy (2012), cast the problem of decision tree learning into the framework of Bayesian inference.", "startOffset": 77, "endOffset": 265}, {"referenceID": 0, "context": "In order to make straightforward comparisons with existing algorithms, we adopt the model proposed by Chipman et al. (1998). In this model, the prior distribution of the latent tree is defined conditionally on the given input vectors X (see Section 5 for a discussion of this dependence on X and its effect on the exchangeability of", "startOffset": 102, "endOffset": 124}, {"referenceID": 0, "context": "When defining the prior over decision trees given by Chipman et al. (1998), it will be necessary to refer to the \u201cextent\u201d of the data in a block.", "startOffset": 53, "endOffset": 75}, {"referenceID": 8, "context": "A similar approach was employed by Taddy et al. (2011) in the setting of online Bayesian decision trees.", "startOffset": 35, "endOffset": 55}, {"referenceID": 8, "context": "A similar approach was employed by Taddy et al. (2011) in the setting of online Bayesian decision trees. There are similarities also with the bottom-up SMC algorithms by Teh et al. (2008) and Bouchard-C\u00f4t\u00e9 et al.", "startOffset": 35, "endOffset": 188}, {"referenceID": 8, "context": "A similar approach was employed by Taddy et al. (2011) in the setting of online Bayesian decision trees. There are similarities also with the bottom-up SMC algorithms by Teh et al. (2008) and Bouchard-C\u00f4t\u00e9 et al. (2012). We next describe the rule for stopping or growing nodes, and the distribution of cuts.", "startOffset": 35, "endOffset": 220}, {"referenceID": 4, "context": "Informally, this choice would lead us to propose extensions to trees at each stage of the algorithm by sampling from the prior, so we will refer to this as the prior proposal kernel (aka the Bayesian bootstrap filter (Gordon et al., 1993)).", "startOffset": 217, "endOffset": 238}, {"referenceID": 0, "context": "In addition, we compare the performance of SMC to the most popular MCMC method for Bayesian decision tree learning (Chipman et al., 1998), as well as CART, a popular (non-Bayesian) tree induction algorithm. We evaluate all the algorithms on the following datasets from the UCI ML repository (Asuncion and Newman, 2007): \u2022 MAGIC gamma telescope data 2004 (magic-04 ): N = 19020, D = 10, K = 2. \u2022 Pen-based recognition of handwritten digits (pen-digits): N = 10992, D = 16, K = 10. Previous work has focused mainly on small datasets (e.g., the Wisconsin breast cancer database used by Chipman et al. (1998) has 683 data points).", "startOffset": 116, "endOffset": 605}, {"referenceID": 2, "context": "We also evaluated systematic resampling (Douc et al., 2005) but found that the performance was not significantly different.", "startOffset": 40, "endOffset": 59}, {"referenceID": 5, "context": "We believe that the deteriorating performance is due to model misspecification (axisaligned decision trees are hardly the \u2018right\u2019 model for handwritten digits) rather than the inference algorithm itself: \u2018better\u2019 Bayesian inference in a misspecified model might lead to a poorer solution (see (Minka, 2000) for a related discussion).", "startOffset": 293, "endOffset": 306}, {"referenceID": 0, "context": "In this experiment, we compare the SMC algorithms to the MCMC algorithm proposed by Chipman et al. (1998), which employs four types of Metropolis-Hastings proposals: grow (split a leaf node into child nodes), prune (prune a pair of leaf nodes belonging to the same parent), change (change the decision rule at a node) and swap (swap the decision rule of a parent with the decision rule of the child).", "startOffset": 84, "endOffset": 106}, {"referenceID": 9, "context": ", Taddy et al. (2011) demonstrated that their tree structured models yield similar performance as Gaussian processes and random forests.", "startOffset": 2, "endOffset": 22}, {"referenceID": 0, "context": "Bayesian additive regression trees (BART) (Chipman et al., 2010) are such a model class.", "startOffset": 42, "endOffset": 64}, {"referenceID": 0, "context": "Prior work has considered MCMC techniques for posterior inference (Chipman et al., 2010).", "startOffset": 66, "endOffset": 88}, {"referenceID": 8, "context": ", based on the Mondrian process (Roy and Teh, 2009), whose projectivity would re-establish exchangeability while allowing for efficient posterior computations that depend on data.", "startOffset": 32, "endOffset": 51}], "year": 2013, "abstractText": "Decision tree learning is a popular approach for classification and regression in machine learning and statistics, and Bayesian formulations\u2014 which introduce a prior distribution over decision trees, and formulate learning as posterior inference given data\u2014have been shown to produce competitive performance. Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, which work in a top-down manner, existing Bayesian algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection thereof) iteratively via local Monte Carlo modifications to the structure of the tree, e.g., using Markov chain Monte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that instead works in a top-down manner, mimicking the behavior and speed of classic algorithms. We demonstrate empirically that our approach delivers accuracy comparable to the most popular MCMC method, but operates more than an order of magnitude faster, and thus represents a better computation-accuracy tradeoff.", "creator": "LaTeX with hyperref package"}}}