{"id": "1303.2314", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2013", "title": "Mini-Batch Primal and Dual Methods for SVMs", "abstract": "We regarding in concerns of using mini - batches until covariance workflow means SVMs. We show believe a. trace, put impedance quirk by all reliable, controls itself parallelization speedup obtained for once doom finite subgradient french-canadian (SGD) however optimization competing coordinate trek (SCDA) simple and use done well complexity published standardised of models - batched SDCA. Our authorization six rest methods make expressed while terms of full earliest nonsmooth compulsion problem based taking the bind - loss.", "histories": [["v1", "Sun, 10 Mar 2013 12:00:59 GMT  (53kb)", "http://arxiv.org/abs/1303.2314v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["martin tak\u00e1c", "avleen singh bijral", "peter richt\u00e1rik", "nati srebro"], "accepted": true, "id": "1303.2314"}, "pdf": {"name": "1303.2314.pdf", "metadata": {"source": "META", "title": "Mini-Batch Primal and Dual Methods for SVMs", "authors": [], "emails": ["martin.taki@gmail.com", "abijral@ttic.edu", "peter.richtarik@ed.ac.uk", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 3.\n23 14\nv1 [\ncs .L\nG ]\n1 0"}, {"heading": "1. Introduction", "text": "Stochastic optimization approaches have been shown to have significant theoretical and empirical advantages in training linear Support Vector Machines (SVMs), as well as in many other learning applications, and are often the methods of choice in practice. Such methods use a single, randomly chosen, training example at each iteration. In the context of SVMs, approaches of this form include primal stochastic gradient descent (SGD) methods (e.g., Pegasos, Shalev-Shwartz et al. 2011, NORMA, Zhang 2004) and dual stochastic coordinate ascent (Hsieh et al., 2008).\nHowever, the inherent sequential nature of such approaches becomes a problematic limitation for parallel and distributed computations as the predictor must be updated after each training point is processed, providing very little opportunity for parallelization. A popular remedy is to use mini-batches. That is, to use several training points at each iteration, instead of just\none, calculating the update based on each point separately and aggregating the updates. The question is then whether basing each iteration on several points can indeed reduce the number of required iterations, and thus yield parallelization speedups.\nIn this paper, we consider using mini-batches with Pegasos (SGD on the primal objective) and with Stochastic Dual Coordinate Ascent (SDCA). We show that for both methods, the quantity that controls the speedup obtained using mini-batching/parallelization is the spectral norm of the data.\nIn Section 3 we provide the first analysis of minibatched Pegasos (with the original, non-smooth, SVM objective) that provably leads to parallelization speedups (Theorem 1). The idea of using mini-batches with Pegasos is not new, and is discussed already by Shalev-Shwartz et al. (2011), albeit without a theoretical justification. The original Pegasos theoretical analysis does not benefit from using mini-batches\u2014the same number of iterations is required even when large mini-batches are used, there is no speedup, and the serial runtime (overall number of operations, in this case data accesses) increases linearly with the minibatch size. In fact, no parallelization speedup can be guaranteed based only on a bound on the radius of the data, as in the original Pegasos analysis. Instead, we provide a refined analysis based on the spectral norm of the data.\nWe then move on to SDCA (Section 4). We show the situation is more involved, and a modification to the method is necessary. SDCA has been consistently shown to outperform Pegasos in practice (Hsieh et al., 2008; Shalev-Shwartz et al., 2011), and is also popular as it does not rely on setting a step-size as in Pegasos.\nIt is thus interesting and useful to obtain mini-batch variants of SDCA as well. We first show that a naive mini-batching approach for SDCA can fail, in particular when the mini-batch size is large relative to the spectral norm (Section 4.1). We then present a \u201csafe\u201d variant of mini-batched SDCA, which depends on the spectral norm, and an analysis for this safe variant that establishes the same spectral-norm-dependent parallelization speedups as for Pegasos (Section 4.2). Similar to a recent analysis of non-mini-batched SDCA by Shalev-Shwartz & Zhang (2012), we establish a guarantee on the duality gap, and thus also on the suboptimality of the primal SVM objective, when using mini-batched SDCA (Theorem 2). We then go on to describe a more aggressive, adaptive, method for mini-batched SDCA, which is based on the analysis of the \u201csafe\u201d approach, and which we show often outperforms it in practice (Section 4.3, with experiments in Section 5).\nFor simplicity of presentation we focus on the hinge loss, as in the SVM objective. However, all our results for both Pegasos and SDCA are valid for any Lipschitz continuous loss function.\nRelated Work. Several recent papers consider the use of mini-batches in stochastic gradient descent, as well as stochastic dual averaging and stochastic mirror descent, when minimizing a smooth loss function (Dekel et al., 2012; Agarwal & Duchi, 2011; Cotter et al., 2011). These papers establish parallelization speedups for smooth loss minimization with mini-batches, possibly with the aid of some \u201cacceleration\u201d techniques, and without relying on, or considering, the spectral norm of the data. However, these results do not apply to SVM training, where the objective to be minimized is the non-smooth hinge loss. In fact, the only data assumption in these papers is an assumption on the radius of the data, which is not enough for obtaining parallelization guarantees when the loss is non-smooth. Our contribution is thus orthogonal to these papers, showing that it is possible to obtain parallelization speedups even for non-smooth objectives, but only with a dependence on the spectral norm. We also analyze SDCA, which is a substantially different method from the methods analyzed in these papers. It is interesting to note that a bound of the spectral norm could perhaps indicate that it is easier to \u201csmooth\u201d the objective, and thus allow obtaining results similar to ours (i.e. on the suboptimality of the original non-smooth objective) by smoothing the objective and relying on mini-batched smooth SGD, where the spectral norm might control how well the smoothed loss captures the original loss. But we are\nnot aware of any analysis of this nature, nor whether such an analysis is possible.\nThere has been some recent work on mini-batched coordinate descent methods for \u21131-regularized problems (and, more generally, regularizes by a separable convex function), similar to the SVM dual. Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for \u21131-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a \u201csafe\u201d variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as \u201cnaive\u201d mini-batching (Section 4.1). More directly related is recent work of Richta\u0301rik & Taka\u0301c\u030c (2013; 2012) which provided a theoretical framework and analysis for a more general setting than SHOTGUN, that includes also the SVM dual as a special case. However, guarantees in this framework, as well as those of Bradley et al. (2011), are only on the dual suboptimality (in our terminology), and not on the more relevant primal suboptimality, i.e., the suboptimality of the original SVM problem we are interested in. Our theoretical analysis builds on that of Richta\u0301rik & Taka\u0301c\u030c (2012), combined with recent ideas of Shalev-Shwartz & Zhang (2012) for \u201cstandard\u201d (serial) SDCA, to obtain bounds on the duality gap and primal suboptimality."}, {"heading": "2. Support Vector Machines", "text": "We consider the optimization problem of training a linear1 Support Vector Machine (SVM) based on n labeled training examples {(xi, yi)} n i=1, where xi \u2208 R d and yi \u2208 \u00b11. We use X = [x1, . . . ,xn] \u2208 R d\u00d7n to denote the matrix of training examples. We assume the data is normalized such that maxi \u2016xi\u2016 \u2264 1, and thus suppress the dependence on maxi \u2016xi\u2016 in all results. Training a SVM corresponds to finding a linear predictor w \u2208 Rd with low \u21132-norm \u2016w\u2016 and small (empirical) average hinge loss L\u0302(w) := 1n \u2211n i=1 \u2113(yi \u3008w,xi\u3009), where \u2113(z) := [1 \u2212 z]+ = max{0, 1 \u2212 z}. This biobjective problem can be serialized as\nmin w\u2208Rd\n[\nP(w) := 1n\nn \u2211\ni=1\n\u2113(yi \u3008w,xi\u3009) + \u03bb 2 \u2016w\u2016\n2\n]\n, (1)\n1Since both Pegasos and SDCA can be kernelized, all methods discussed are implementable also with kernels, and all our results hold. However, the main advantage of SGD and SDCA is where the feature map is given explicitly, and so we focus our presentation on this setting.\nwhere \u03bb > 0 is a regularization trade-off parameter. It is also useful to consider the dual of (1):\nmax \u03b1\u2208Rn,0\u2264\u03b1i\u22641\n[\nD(\u03b1) := \u221212\u03bbn2\u03b1 \u22a4Q\u03b1+ 1n\nn \u2211\ni=1\n\u03b1i\n]\n, (2)\nwhere\nQ \u2208 Rn\u00d7n, Qi,j = yiyj \u3008xi,xj\u3009 , (3)\nis the Gram matrix of the (labeled) data. The (primal) optimum of (1) is given by w\u2217 = 1\u03bbn \u2211n i=1 \u03b1 \u2217 i yixi, where \u03b1\u2217 is the (dual) optimum of (2). It is thus natural to associate with each dual solution \u03b1 a primal solution (i.e., a linear predictor)\nw(\u03b1) := 1\u03bbn\nn \u2211\ni=1\n\u03b1iyixi. (4)\nWe will be discussing \u201cmini-batches\u201d of size b, represented by random subsets A \u2286 \u3008n\u3009 := {1, 2, . . . , n} of examples, drawn uniformly at random from all subsets of \u3008n\u3009 of cardinality b. Whenever we draw such a subset, we will for simplicity write A \u2208 Rand(b). For A \u2208 Rand(b) we use QA \u2208 R\nb\u00d7b to denote the random submatrix of Q corresponding to rows and columns indexed by A, vA \u2208 R\nb to denote a similar restriction of a vector v \u2208 Rn, and v[A] \u2208 R\nn for the \u201ccensored\u201d vector where entries inside A are as in v and entries outside A are zero. The average hinge loss on examples in A is denoted by\nL\u0302A(w) := 1 b\n\u2211\ni\u2208A\n\u2113(yi \u3008w,xi\u3009). (5)"}, {"heading": "3. Mini-Batches in Primal Stochastic Gradient Descent Methods", "text": "Algorithm 1 Pegasos with Mini-Batches\nInput: {(xi, yi)} n i=1, \u03bb > 0, b \u2208 \u3008n\u3009, T \u2265 1 Initialize: set w(1) = 0 \u2208 Rd for t = 1 to T do Choose random mini-batch At \u2208 Rand(b) \u03b7t = 1 \u03bbt , A + t = {i \u2208 At : yi\u3008w (t),xi\u3009 < 1}\nw(t+1) = (1\u2212 \u03b7t\u03bb)w (t) + \u03b7tb \u2211 i\u2208A+t yixi\nend for Output: w\u0304(T ) = 2T \u2211T t=\u230aT/2\u230b+1 w (t)\nPegasos is an SGD approach to solving (1), where at each iteration the iterate w(t) is updated based on an unbiased estimator of a sub-gradient of the objective P(w). Whereas in a \u201cpure\u201d stochastic setting, the subgradient is estimated based on only a single training\nexample, in our mini-batched variation (Algorithm 1) at each iteration we consider the partial objective:\nPt(w) := L\u0302At(w) + \u03bb 2 \u2016w\u2016 2 , (6)\nwhere At \u2208 Rand(b). We then calculate the subgradient of the partial objective Pt at w (t):\n\u2207 (t) := \u2207Pt(w (t)) (6) = \u2207L\u0302At(w (t)) + \u03bbw(t), (7)\nwhere\n\u2207L\u0302A(w) (5) = \u2212 1b \u2211\ni\u2208A\n\u03c7i(w)yixi (8)\nand \u03c7i(w) := 1 if yi \u3008w,xi\u3009 < 1 and 0 otherwise (indicator for not classifying example i correctly with a margin). The next iterate is obtained by setting w(t+1) = w(t) \u2212 \u03b7t\u2207 (t). We can now write\nw(t+1) (7)+(8) = (1\u2212\u03b7t\u03bb)w (t)+ \u03b7tb \u2211\ni\u2208At\n\u03c7i(w (t))yixi. (9)\nAnalysis of mini-batched Pegasos rests on bounding the norm of the subgradient estimates \u2207(t). An unconditional bound on this norm, used in the standard Pegasos analysis, follows from bounding\n\u2016\u2207L\u0302A(w)\u2016 (8) \u2264 1b \u2211\ni\u2208A\n\u2016\u03c7i(w)yixi\u2016 \u2264 1 b\n\u2211\ni\u2208A\n1 = 1.\nFrom (7) we then get \u2016\u2207(t)\u2016 \u2264 \u03bb\u2016w(t)\u2016+ 1; the standard Pegasos analysis follows. This bound relies only on the assumption maxi \u2016xi\u2016 \u2264 1, and is the tightest bound without further assumptions on the data.\nThe core novel observation here is that the expected (square) norm of \u2207L\u0302A can be bounded in terms of (an upper bound on) the spectral norm of the data:\n\u03c32 \u2265 1n \u2016X\u2016 2 = 1n\u2016\n\u2211\ni\nxix \u22a4 i \u2016 (3) = 1n \u2016Q\u2016 , (10)\nwhere \u2016\u00b7\u2016 denotes the spectral norm (largest singular value) of a matrix. In order to bound \u2207L\u0302A, we first perform the following calculation, introducing the key quantity \u03b2b, useful also in the analysis of SDCA. Lemma 1. For any v \u2208 Rn, Q\u0303 \u2208 Rn\u00d7n, A \u2208 Rand(b),\nE[v\u22a4[A]Q\u0303v[A]] = b n [(1\u2212 b\u22121 n\u22121 )\nn \u2211\ni=1\nQ\u0303iiv 2 i + b\u22121 n\u22121v \u22a4Q\u0303v].\nMoreover, if Q\u0303ii \u2264 1 for all i and 1 n\u2016Q\u0303\u2016 \u2264 \u03c3 2, then\nE[v\u22a4[A]Q\u0303v[A]] \u2264 b n\u03b2b \u2016v\u2016 2 , where\n\u03b2b := 1 + (b\u22121)(n\u03c32\u22121)\nn\u22121 . (11)\nProof.\nE[v\u22a4[A]Q\u0303v[A]] = E[ \u2211\ni\u2208A\nv2i Q\u0303ii + \u2211\ni,j\u2208A,i6=j\nvivjQ\u0303ij ]\n(\u2217) = bEi[v 2 i Q\u0303ii] + b(b\u2212 1)Ei,j [vivjQ\u0303ij ]\n= bn\n\u2211\ni\nQ\u0303iiv 2 i + b(b\u22121) n(n\u22121)v \u22a4(Q\u0303\u2212 diag(Q\u0303))v\n= bn [(1\u2212 b\u22121 n\u22121 )\n\u2211\ni\nQ\u0303iiv 2 i + b\u22121 n\u22121v \u22a4Q\u0303v],\nwhere in (\u2217) the expectations are over i, j chosen uniformly at random without replacement. Now using Q\u0303ii \u2264 1 and \u2016Q\u0303\u2016 \u2264 n\u03c3\n2, we can upper-bound the expectation as follows:\n\u2264 bn [(1\u2212 b\u22121 n\u22121 ) \u2016v\u2016 2 + b\u22121n\u22121n\u03c3 2 \u2016v\u20162] = bn\u03b2b \u2016v\u2016 2 .\nWe can now apply Lemma 1 to \u2207L\u0302A: Lemma 2. For any w \u2208 Rd and A \u2208 Rand(b) we have E[\u2016\u2207L\u0302A(w)\u2016 2] \u2264 \u03b2bb , where \u03b2b is as in Lemma 1.\nProof. If \u03c7 \u2208 Rn is the vector with entries \u03c7i(w), then\nE[\u2016\u2207L\u0302A(w)\u2016 2] (8) = E[\u2016 1b \u2211\ni\u2208A\n\u03c7iyixi\u2016 2]\n(3) = 1b2E[\u03c7 \u22a4 [A]Q\u03c7[A]]\n(Lem1)\n\u2264 1b2 b n\u03b2b \u2016\u03c7\u2016 2 \u2264 \u03b2bb .\nUsing the by-now standard analysis of SGD for strongly convex functions, we obtain the main result of this section:\nTheorem 1. After T iterations of Pegasos with minibatches (Algorithm 1), we have that for the averaged iterate w\u0304(T ) = 2T \u2211T t=\u230aT/2\u230b+1 w (t):\nE\n[ P(w\u0304(T )) ]\n\u2212 inf w\u2208Rd\nP(w) \u2264 \u03b2bb \u00b7 30 \u03bbT .\nProof. Unrolling (9) with \u03b7t = 1/(\u03bbt) yields\nw(t) = \u2212 1\u03bb(t\u22121)\nt\u22121 \u2211\n\u03c4=1\ng(\u03c4), (12)\nwhere g(\u03c4) := \u2207L\u0302A\u03c4 (w (\u03c4)). Using the inequality \u2016 \u2211t\u22121\n\u03c4=1 g (\u03c4)\u20162 \u2264 (t\u2212 1) \u2211t\u22121 \u03c4=1 \u2016g (\u03c4)\u20162, we now get\nE[\u2016w(t)\u20162] (12) \u2264\nt\u22121 \u2211\n\u03c4=1\nE[\u2016g(\u03c4)\u20162] \u03bb2(t\u22121)\n(Lem2)\n\u2264 \u03b2b\u03bb2b , (13)\nE[\u2016\u2207(t)\u20162] (7)+(Lem2) \u2264 2(\u03bb2E[\u2016w(t)\u20162] + \u03b2bb ) (13) \u2264 4\u03b2bb .\nThe performance guarantee is now given by the analysis of SGD with tail averaging (Theorem 5 of Rakhlin et al. 2012, with \u03b1 = 12 and G 2 = 4\u03b2bb ).\nParallelization speedup. When b = 1 we have \u03b2b = 1 (see (11)) and Theorem 1 agrees with the standard (serial) Pegasos analysis2 (Shalev-Shwartz et al., 2011). For larger mini-batches, the guarantee depends on the quantity \u03b2b, which in turn depends on the spectral norm \u03c32. Since 1n \u2264 \u03c3 2 \u2264 1, we have 1 \u2264 \u03b2b \u2264 b.\nThe worst-case situation is at a degenerate extreme, when all data points lie on a single line, and so \u03c32 = 1 and \u03b2b = b. In this case Lemma 2 degenerates to the worst-case bound of E[\u2016\u2207L\u0302A(w)\u2016\n2] \u2264 1, and in Theorem 1 we have \u03b2bb = 1, indicating that using larger mini-batches does not help at all, and the same number of iteration (i.e., the same parallel runtime, and b times as much serial runtime) is required.\nHowever, when \u03c32 < 1, and so \u03b2b < 1, we see a benefit in using mini-batches in Theorem 1, corresponding to a parallelization speedup of b\u03b2b . The best situation is when \u03c32 = 1n , and so \u03b2b = 1, which happens when all training points are orthogonal. In this case there is never any interaction between points in the minibatch, and using a mini-batch of size b is just as effective as making b single-example steps. When \u03b2b = 1 we indeed see that the speedup speedup is equal to the number of mini-batches, and that the behavior in terms of the number of data accesses (equivalently, serial runtime) bT , does not depend on b; that is, even with larger mini-batches, we require no more data accesses, and we gain linearly from being able to perform the accesses in parallel. The case \u03c32 = 1n is rather extreme, but even for intermediate values 1n < \u03c3\n2 < 1 we get speedup. In particular, as long as b \u2264 1\u03c32 , we have \u03b2b \u2264 2, and an essentially linear speedup. Roughly speaking, 1\u03c32 captures the number of examples in the mini-batch beyond which we start getting significant interactions between points."}, {"heading": "4. Mini-Batches in Dual Stochastic Coordinate Ascent Methods", "text": "An alternative stochastic method to Pegasos is Stochastic Dual Coordinate Ascent (SDCA, Hsieh et al. 2008), aimed to solve the dual problem (2). At each iteration we choose a single training example (xi, yi), uniformly at random, corresponding to a single dual variable (coordinate) \u03b1i = e \u22a4 i \u03b1. Subsequently, \u03b1i is updated so as to maximize the (dual) objective, keeping all other coordinates of \u03b1 unchanged and maintaining the box constraints. At\n2Except that we avoid the logarithmic factor by relying on tail averaging and a more modern SGD analysis.\niteration t, the update \u03b4 (t) i to \u03b1 (t) i is computed via\n\u03b4 (t) i := argmax\n0\u2264\u03b1 (t) i +\u03b4\u22641\nD(\u03b1(t) + \u03b4ei)\n(2) = argmax\n0\u2264\u03b1 (t) i +\u03b4\u22641\n(\u03bbn\u2212 (Qei) \u22a4\u03b1(t))\u03b4 \u2212 Qi,i 2 \u03b4 2\n= clip [\u2212\u03b1\n(t) i ,1\u2212\u03b1 (t) i ]\n\u03bbn\u2212(Qei) \u22a4 \u03b1\n(t)\nQi,i\n(3),(4) = clip\n[\u2212\u03b1 (t) i ,1\u2212\u03b1 (t) i ]\n\u03bbn(1\u2212yi\u3008w(\u03b1(t)),xi\u3009) \u2016xi\u20162 , (14)\nwhere clipI is projection onto the interval I. Variables \u03b1 (t) j for j 6= i are unchanged. Hence, a single iteration has the form \u03b1(t+1) = \u03b1(t) + \u03b4 (t) i ei. Similar to a Pegasos update, at each iteration a single, random, training point is considered, the \u201cresponse\u201d yi \u2329 w(\u03b1(t)),xi \u232a\nis calculated (this operation dominates the computational effort), and based on the response, a multiple of xi is added to the weight vector w (corresponding to changing \u03b1i). The two methods thus involve fairly similar operations at each iteration, with essentially identical computational costs. They differ in that in Pegasos, \u03b1i is changed according to some pre-determined step-size, while SDCA changes it optimally so as to maximize the dual objective (and maintain dual feasibility); there is no step-size parameter.\nSDCA was suggested and studied empirically by Hsieh et al. (2008), where empirical advantages over Pegasos were often observed. In terms of a theoretical analysis, by considering the dual problem (2) as an \u21131-regularized, box-constrained quadratic problem, it is possible to obtain guarantees on the dual suboptimality, D(\u03b1\u2217) \u2212 D(\u03b1(t)), after a finite number of SDCA iterations (Shalev-Shwartz & Tewari, 2011; Nesterov, 2012; Richta\u0301rik & Taka\u0301c\u030c, 2013). However, such guarantees do not directly imply guarantees on the primal suboptimality of w(\u03b1(t)). Recently, Shalev-Shwartz & Zhang (2012) bridged this gap, and provided guarantees on P(w(\u03b1(t))) \u2212 P(w\u2217) after a finite number of SDCA iterations. These guarantees serve as the starting point for our theoretical study."}, {"heading": "4.1. Naive Mini-Batching", "text": "A naive approach to parallelizing SDCA using minibatches is to compute \u03b4 (t) i in parallel, according to (14), for all i \u2208 At, all based on the current iterate \u03b1(t), and then update \u03b1 (t+1) i = \u03b1 (t) i + \u03b4 (t) i for i \u2208 At, and keep \u03b1 (t+1) j = \u03b1 (t) j for j 6\u2208 At. However, not only might this approach not reduce the number of required iterations, it might actually increase the number of required iterations. This is because the dual objective need not improve monotonically (as it does for \u201cpure\u201d SDCA), and even not converge.\nTo see this, consider an extreme situation with only two identical training examples: Q = [ 1 11 1 ], \u03bb = 1 n = 1 2 and mini-batch size b = 2 (i.e., in each iteration we use both examples). If we start with \u03b1(0) = 0 with D(\u03b1(0)) = 0 then \u03b4 (0) 1 = \u03b4 (0) 2 = 1 and following the naive approach we have \u03b1(1) = (1, 1)T with objective value D(\u03b1(1)) = 0. In the next iteration \u03b4 (1) 1 = \u03b4 (1) 2 = \u22121 which brings us back to \u03b1\n(2) = 0. So the algorithm will alternate between those two solutions with objective value D(\u03b1) = 0, while at the optimum D(\u03b1\u2217) = D((0.5, 0.5)\u22a4) = 0.25.\nThis is of course a simplistic toy example, but the same phenomenon will occur when a large number of training examples are identical or highly correlated. This can also be observed empirically in some of our experiments discussed later, e.g., in Figure 2.\nThe problem here is that since we update each \u03b1i independently to its optimal value as if all other coordinates were fixed, we are ignoring interactions between the updates. As we see in the extreme example above, two different i, j \u2208 At, might suggest essentially the same change to w(\u03b1(t)), but we would then perform this update twice, overshooting and yielding a new iterate which is actually worse then the previous one."}, {"heading": "4.2. Safe Mini-Batching", "text": "Properly accounting for the interactions between coordinates in the mini-batch would require jointly optimizing over all \u03b1i, i \u2208 At. This would be a very powerful update and no-doubt reduce the number of required iterations, but would require solving a boxconstrained quadratic program, with a quadratic term of the form \u03b4\u22a4AQA\u03b4A, \u03b4A \u2208 R\nb, at each iteration. This quadratic program cannot be distributed to different machines, each handling only a single data point.\nInstead, we propose a \u201csafe\u201d variant, where the term \u03b4\u22a4AQA\u03b4A is approximately bounded by the separable surrogate \u03b2 \u2016\u03b4A\u2016 2 , for some \u03b2 > 0 which we will discuss later. That is, the update is given by:\n\u03b4 (t) i := argmax\n0\u2264\u03b1 (t) i +\u03b4\u22641\n(\u03bbn\u2212 (Qei) \u22a4\u03b1(t))\u03b4 \u2212 \u03b22 \u03b4 2\n= clip [\u2212\u03b1\n(t) i ,1\u2212\u03b1 (t) i ]\n\u03bbn(1\u2212yi\u3008w(\u03b1(t)),xi\u3009) \u03b2 , (15)\nwith \u03b1 (t+1) i = \u03b1 (t) i + \u03b4 (t) i for i \u2208 At, and \u03b1 (t+1) j = \u03b1 (t) j for j 6\u2208 At. In essence, 1 \u03b2 serves as a step-size, where we are now careful not to take steps so big that they will accumulate together and overshoot the objective. If handling only a single point at each iteration, such a short-step approach is not necessary, we do not need a step-size, and we can take a \u201cfull step\u201d, setting \u03b1i\noptimally (\u03b2 = 1). But with the potential for interaction between coordinates updated in parallel, we must use a smaller step, depending on the potential for such interactions.\nWe will first rely on the bound (10), and establish that the choice \u03b2 = \u03b2b as in (11) provides for a safe step size. To do so, we consider the dual objective at \u03b1+\u03b4,\nD(\u03b1+\u03b4) = \u2212 (\u03b1 \u22a4Q\u03b1+2\u03b1\u22a4Q\u03b4+\u03b4\u22a4Q\u03b4)\n2\u03bbn2 +\nn \u2211\ni=1\n\u03b1i+\u03b4i n , (16)\nand the following separable approximation to it:\nH(\u03b4,\u03b1) := \u2212 (\u03b1 \u22a4Q\u03b1+2\u03b1\u22a4Q\u03b4+\u03b2b\u2016\u03b4\u2016 2) 2\u03bbn2 +\nn \u2211\ni=1\n\u03b1i+\u03b4i n ,\n(17)\nin which \u03b2b \u2016\u03b4\u2016 2 replaces \u03b4\u22a4Q\u03b4. Our update (15) with \u03b2 = \u03b2b can be written as \u03b4 = arg max \u03b4:0\u2264\u03b1+\u03b4\u22641 H(\u03b4,\u03b1) (we then use the coordinates \u03b4i for i \u2208 A and ignore the rest). We are essentially performing parallel coordinate ascent on the separable approximation H(\u03b4,\u03b1) instead of on D(\u03b1 + \u03b4). To understand this approximation, we note that H(0,\u03b1) = D(\u03b1), and show that H(\u03b4,\u03b1) provides an approximate expected lower bound on D(\u03b1+ \u03b4):\nLemma 3. For any \u03b1, \u03b4 \u2208 Rn and A \u2208 Rand(b),\nEA[D(\u03b1+ \u03b4[A])] \u2265 (1\u2212 b n )D(\u03b1) + b nH(\u03b4,\u03b1).\nProof. Examining (16) and (17), the terms that do not depend on \u03b4 are equal on both sides. For the linear term in \u03b4, we have that E[\u03b4[A]] = b n\u03b4, and again we have equality on both sides. For the quadratic term we use Lemma 1 which yields E[\u03b4\u22a4[A]Q\u03b4[A]] \u2264 b n\u03b2b \u2016\u03b4\u2016 2 , and after negation establishes the desired bound.\nInequalities of this general type are also studied in (Richta\u0301rik & Taka\u0301c\u030c, 2012) (see Sections 3 and 4). Based on the above lemma, we can modify the analysis of Shalev-Shwartz & Zhang (2012) to obtain (see complete proof in the appendix):\nTheorem 2. Consider the SDCA updates given by (15), with At \u2208 Rand(b), starting from \u03b1\n(0) = 0 and with \u03b2 = \u03b2b (given in eq. (11)). For any \u01eb > 0 and\nt0 \u2265 max{0, \u2308 n b log( 2\u03bbn \u03b2b )\u2309}, (18)\nT0 \u2265 t0 + \u03b2b b\n[\n4 \u03bb\u01eb \u2212 2 n \u03b2b\n]\n+ , (19)\nT \u2265 T0 +max{\u2308 n b \u2309, \u03b2b b 1 \u03bb\u01eb}, (20)\n\u03b1\u0304 := 1T\u2212T0\nT\u22121 \u2211\nt=T0\n\u03b1(t), (21)\nwe have\nE[P(w(\u03b1\u0304))]\u2212P(w\u2217) \u2264 E[P(w(\u03b1\u0304))\u2212D(\u03b1\u0304)] \u2264 \u01eb.\nThe number of iterations of mini-batched SDCA, sufficient to reach primal suboptimality \u01eb, is by Theorem 2 equal to\nO\u0303 (\nn b + \u03b2b b \u00b7 1 \u03bb\u01eb\n)\n. (22)\nWe observe the same speedup as in the case of minibatched Pegasos: factor of b\u03b2b , with an essentially linear speedup when b \u2264 1\u03c32 . It is interesting to note that the quantity \u03b2b only affects the second, \u01eb-dependent, term in (22). The \u201cfixed cost\u201d term, which essentially requires a full pass over the data, is not affected by \u03b2b, and is always scaled down by b."}, {"heading": "4.3. Aggressive Mini-Batching", "text": "Using \u03b2 = \u03b2\u03c3 is safe, but might be too safe/conservative. In particular, we used the spectral norm to bound \u03b4\u22a4Q\u03b4 \u2264 \u2016Q\u2016 \u2016\u03b4\u2016 2 in Lemma 3 (through Lemma 1), but this is a worst case bound over all possible vectors, and might be loose for the relevant vectors \u03b4. Relying on a worst-case bound might mean we are taking much smaller steps then we could be. Furthermore, the approach we presented thus far relies on knowing the spectral norm of the data, or at least a bound on the spectral norm (recall (10)), in order to set the step-size. Although it is possible to estimate this quantity by sampling, this can certainly be inconvenient.\nInstead, we suggest a more aggressive variant of minibatched SDCA which gradually adapts \u03b2 based on the actual values of \u2016\u03b4 (t) [At] \u20162 and \u03b4 (t) [At] Q\u03b4 (t) [At] . In Section 5 one can observe advantages of this aggressive strategy.\nIn this variant, at each iteration we calculate the ratio \u03b4\u0303 \u22a4 [A]Q\u03b4\u0303 \u22a4 [A]/\u2016\u03b4\u0303[A]\u2016 2, and nudge the step size towards it by updating it to a weighted geometric average of the previous step size and \u201coptimal\u201d step size based on the step \u03b4 considered. One complication is that due to the box constraints, not only the magnitude but also the direction of the step \u03b4 depends on the step-size \u03b2, leading to a circular situation. The approach we take is as follows: we maintain a \u201ccurrent step size\u201d \u03b2. At each iteration, we first calculate a tentative step \u03b4\u0303A, according to (15), with the current \u03b2. We then calculate \u03c1 = \u03b4\u0303 \u22a4 [A]Q\u03b4\u0303 \u22a4 [A]\n\u2016\u03b4\u0303[A]\u2016 2 , according to this step direction, and\nupdate \u03b2 to \u03b2\u03b3\u03c11\u2212\u03b3 for some pre-determined parameter 0 < \u03b3 < 1 that controls how quickly the step-size adapts. But, instead of using \u03b4\u0303 calculated with the previous \u03b2, we actually re-compute \u03b4A using the stepsize \u03c1. We note that this means the ratio \u03c1 does not\nAlgorithm 2 SDCA with Mini-Batches (aggressive)\nInput: {(xi, yi)} n i=1, \u03bb > 0, b \u2208 R d, T \u2265 1, \u03b3 = 0.95\nInitialize: set \u03b1(0) = 0, w(0) = 0, \u03b2(0) = \u03b2b for t = 0 to T do Choose At \u2208 Rand(b) For i \u2208 At, compute \u03b4\u0303i from (15) using \u03b2 = \u03b2 (t)\nSum \u03b6 := \u2211 i\u2208At \u03b4\u0303 2 i and \u2206\u0303 := \u2211 i\u2208At \u03b4\u0303iyixi\nCompute \u03c1 = clip[1,\u03b2b]\n(\n\u2016\u2206\u0303\u2016 2\n\u03b6\n)\nFor i \u2208 At, compute \u03b4i from (15) using \u03b2 = \u03c1. \u03b2(t+1) := (\u03b2(t))\u03b3\u03c11\u2212\u03b3 if D(\u03b1(t) + \u03b4[At]) > D(\u03b1 (t)) then\n\u03b1(t+1) = \u03b1(t) + \u03b4[At], w(t+1) = w(t) + 1\u03bbn \u2211 i\u2208At \u03b4iyixi\nelse\n\u03b1(t+1) = \u03b1(t), w(t+1) = w(t)\nend if\nend for\ncorrespond to the step \u03b4A actually taken, but rather to the tentative step \u03b4\u0303A. We could potentially continue iteratively updating \u03c1 according to \u03b4A and \u03b4A according to \u03c1, but we found that this does not improve performance significantly and is generally not worth the extra computational effort. This aggressive strategy is summarized in Algorithm 2. Note that we initialize \u03b2 = \u03b2b, and also constrain \u03b2 to remain in the range [1, \u03b2b], but we can use a very crude upper bound \u03c32 for calculating \u03b2b. Also, in our aggressive strategy, we refuse steps that do not actually increase the dual objective, corresponding to overly aggressive step sizes.\nCarrying out the aggressive strategy requires computing \u03b4\u0303 \u22a4\n[A]Q\u03b4\u0303[A] and the dual objective efficiently and in parallel. The main observation here is that:\n\u03b4\u0303 \u22a4\n[A]Q\u03b4\u0303[A] =\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2211\ni\u2208A\n\u03b4\u0303iyixi\n\u2225 \u2225 \u2225 \u2225 \u2225 2\n(23)\nand so the main operation to be performed is an aggregation of \u2211\ni\u2208A \u03b4\u0303iyixi, similar to the operation required in mini-batched Pegasos. As for the dual objective, it can be written as D(\u03b1) = \u2212\u2016w(\u03b1)\u2016\n2 \u2212 1n \u2016\u03b1\u20161\nand can thus be readily calculated if we maintain w(\u03b1), its norm, and \u2016\u03b1\u20161."}, {"heading": "5. Experiments", "text": "Figure 1 shows the required number of iterations (corresponding to the parallel runtime) required for achieving a primal suboptimality of 0.001 using Pegasos,\nnaive SDCA, safe SDCA and aggressive SDCA, on four benchmark datasets detailed in Table 1, using different mini-batch sizes. Also shown (on an independent scale; right axis) is the leading term \u03b2bb in our complexity results. The results confirm the advantage of SDCA over Pegasos, at least for b = 1, and that both Pegasos and SDCA enjoy nearly-linear speedups, at least for small batch sizes. Once the mini-batch size is such that \u03b2bb starts flattening out (corresponding to b \u2248 1\u03c32 , and so significant correlations inside each mini-batch), the safe variant of SDCA follows a similar behavior and does not allow for much parallelization speedup beyond this point, but at least does not deteriorate like the naive variant. Pegasos and the aggressive variant do continue showing speedups beyond b \u2248 1\u03c32 . The experiments clearly demonstrate the aggressive modification allows SDCA to continue enjoying roughly the same empirical speedups as Pegasos, even for large mini-batch sizes, maintaining an advantage throughout. It is interesting to note that the aggressive variant continues improving even past the point of failure of the naive variant, thus establishing that it is empirically important to adjust the step-size to achieve a balance between safety and progress.\nIn Figure 2 we demonstrate the evolution of solutions using the various methods for two specific data sets. Here we can again see the relative behaviour of the methods, as well as clearly see the failure of the naive approach, which past some point causes the objective to deteriorate and does not converge to the optimal solution."}, {"heading": "6. Conclusion", "text": "Contribution. Our contribution in this paper is twofold: (i) we identify the spectral norm of the data, and through it the quantity \u03b2b, as the important quantity controlling guarantees for minibatched/parallelized Pegasos (primal method) and\nSDCA (dual method). We provide the first analysis of mini-batched Pagasos, with the non-smooth hingeloss, that shows speedups, and we analyze for the first time mini-batched SDCA with guarantees expressed in terms of the primal problem (hence, our mini-batched SDCA is a primal-dual method); (ii) based on our analysis, we present novel variants of mini-batched SDCA which are necessary for achieving speedups similar to those of Pegasos, and thus open the door to effective mini-batching using the often-empirically-better SDCA.\nRelated work. Our safe SDCA mini-batching approach is similar to the parallel coordinate descent methods of Bradley et al. (2011) and Richta\u0301rik & Taka\u0301c\u030c (2012), but we provide an analysis in terms of the primal SVM objective, which is the more relevant object of interest. Furthermore, Bradley et al.\u2019s analysis does not use a step-size and is thus limited only to small enough mini-batches\u2014 if the spectral norm is unknown and too large a mini-batch is used, their method might not converge. Richta\u0301rik & Taka\u0301c\u030c\u2019s method does incorporate a fixed step-size, similar to our safe variant, but as we discuss this step-size might be too conservative for achieving the true potential of mini-batching.\nGenerality. We chose to focus on Pegasos and SDCA with regularized hinge-loss minimization, but all our results remain unchanged for any Lipschitz loss functions. Furthermore, Lemma 2 can also be used to es-\ntablish identical speedups for mini-batched SGD optimization of min\u2016w\u2016\u2264B L\u0302(w), as well as for direct stochastic approximation of the population objective (generalization error) minL(w). In considering the population objective, the sample size is essentially infinite, we sample with replacements (from the population), \u03c32 is a bound on the second moment of the data distribution, and \u03b2b = 1 + (b \u2212 1)\u03c3 2.\nExperiments. Our experiments confirm the empirical advantages of SDCA over Pegasos, previously observed without mini-batching. However, we also point out that in order to perform mini-batched SDCA effectively, a step-size is needed, detracting from one of the main advantages of SDCA over Pegasos. Furthermore, in the safe variant, this stepsize needs to be set according to the spectral norm (or bound on the spectral norm), with too small a setting for \u03b2 (i.e., too large steps) possibly leading to non-convergence, and too large a setting for \u03b2 yielding reduced speedups. In contrast, the Pegasos stepsize is independent of the spectral norm, and in a sense Pegasos adapts implicitly (see, e.g., its behavior compared to aggressive SDCA in the experiments). We do provide a more aggressive variant of SDCA, which does match Pegasos\u2019s speedups empirically, but this requires an explicit heuristic adaptation of the stepsize.\nParallel Implementation. In this paper we analyzed the iteration complexity, and behavior of the iterates, of mini-batched Pegasos and SDCA. Un-\nlike \u201cpure\u201d (b=1) Pegasos and SDCA, which are not amenable to parallelization, using mini-batches does provide opportunities for it. Of course, actually achieving good parallelization speedups on a specific architecture in practice requires an efficient parallel, possibly distributed, implementation of the iterations. In this regard, we point out that the core computation required for both Pegasos and SDCA is that of computing \u2211\ni\u2208A gi(\u3008w,xi\u3009)xi, where g is some scalar function. Parallelizing such computations efficiently in a distributed environment has been studied by e.g., Dekel et al. (2012); Hsu et al. (2011); their methods can be used here too. Alternatively, one could also consider asynchronous or delayed updates (Agarwal & Duchi, 2011; Niu et al., 2011)."}, {"heading": "A. Proof of Theorem 2", "text": "The proof of Theorem 2 follows mostly along the path of Shalev-Shwartz & Zhang (2012), crucially using Lemma 3, and with a few other required modifications detailed below.\nWe will prove the theorem for a general L-Lipschitz loss function \u2113(\u00b7). For consistency with Shalev-Shwartz & Zhang, we will also allow example-specific loss functions \u2113i, i = 1, 2, . . . , n, and only require each \u2113i be individually Lipschitz, and thus refer to the primal and dual problems (expressed slightly differently but equivalently):\nmin w\u2208Rd\n[\nP(w) := 1n\nn \u2211\ni=1\n\u2113i(\u3008w,xi\u3009) + \u03bb 2 \u2016w\u2016 2\n]\n, (P)\nmax \u03b1\u2208Rn\n[\nD(\u03b1) := \u2212 1n\nn \u2211\ni=1\n\u2113\u2217i (\u2212\u03b1i)\u2212 \u03bb 2\n\u2225 \u2225\n1 \u03bbnX \u22a4\u03b1 \u2225 \u2225\n2 2\n]\n, (D)\nwhere \u2113\u2217i (u) = maxz(zu \u2212 \u2113i(z)) is the Fenchel conjugate of \u2113i. In the above we dropped without loss of generality the labels yi since we can always substitute xi \u2190 yixi. For the hinge loss \u2113i(a) = [1 \u2212 a]+ we have \u2113\u2217i (\u2212a) = \u2212a for a \u2208 [0, 1] and \u2113 \u2217 i (\u2212a) = \u221e otherwise, thus encoding the box constraints. Recall also (from (4)) that w(\u03b1) = 1\u03bbn \u2211n i=1 \u03b1ixi and so \u2016w(\u03b1)\u2016 2 = 1\u03bb2n2\u03b1 \u22a4XX\u22a4\u03b1 = \u2225 \u2225 1 \u03bbnX \u22a4\u03b1 \u2225 \u2225 2 .\nThe separable approximation H(\u03b4,\u03b1) defined in (17) now has the more general form:\nH(\u03b4,\u03b1) := \u2212 1n\nn \u2211\ni=1\n\u2113\u2217i (\u2212(\u03b1i + \u03b4i))\u2212 \u03bb 2\n(\n\u2016w(\u03b1)\u2016 2 + \u03b2b 1 \u03bbn\nn \u2211\ni=1\n\u2016xi\u2016 2 \u03b42i + 2 ( 1 \u03bbn\u03b4 )\u22a4 Xw(\u03b1)\n)\n(24)\nand all the properties mentioned in Section 4, including Lemma 3, still hold.\nOur goal here is to get a bound on the duality gap, which we will denote by\nG(\u03b1) := P(w(\u03b1))\u2212D(\u03b1) = 1n\nn \u2211\ni=1\n[\u2113i(\u3008w(\u03b1),xi\u3009) + \u2113 \u2217 i (\u2212\u03b1i) +\u03b1i \u3008w(\u03b1),xi\u3009] . (25)\nThe analysis now rests on the following lemma, paralleling Lemma 1 of Shalev-Shwartz & Zhang (2012), which bounds the expected improvement in the dual objective after a single iteration in terms of the duality gap:\nLemma 4. For any t and any s \u2208 [0, 1] we have\nEAt [D(\u03b1 (t+1))]\u2212D(\u03b1(t)) \u2265 b\n(\ns nG(\u03b1 (t))\u2212 ( s n )2 \u03b2b 2\u03bbG\n(t) )\n, (26)\nwhere\nG(t) := 1n\nn \u2211\ni=1\n\u2016xi\u2016 2(\u03c7 (t) i \u2212\u03b1 (t) i ) 2 \u2264 G, (27)\nwith G = 4L for general L-Lipschitz loss, and G = 1 for the hinge loss, and \u2212\u03c7 (t) i \u2208 \u2113 \u2032 i( \u2329 w(\u03b1(t)),xi \u232a ).\nProof. The situation here is trickier then in the case b = 1 considered by Shalev-Shwartz & Zhang, and we will first bound the right hand side of (26) by H\u2212D and then use the fact that \u03b4(t) is a minimizer of H(\u00b7,\u03b1):\n\u2212 n\nb\n( E[D(\u03b1(t+1))]\u2212D(\u03b1(t)) ) = \u2212 n\nb\n(\nE[D(\u03b1(t) + \u03b4 (t) [At]\n)]\u2212D(\u03b1(t)) ) (Lemma 3) \u2264 \u2212H(\u03b4(t),\u03b1(t)) +D(\u03b1(t))\n= 1\nn\nn \u2211\ni=1\n(\n\u2113\u2217i (\u2212(\u03b1 (t) i + \u03b4 (t) i ))\u2212 \u2113 \u2217 i (\u2212\u03b1 (t) i ) ) + \u03bb\n2\n(\n\u03b2b\n\u2225 \u2225 \u2225 \u2225 1 \u03bbn \u03b4(t) \u2225 \u2225 \u2225 \u2225 2\nX\n+ 2\n(\n1 \u03bbn \u03b4(t) )\u22a4 Xw(\u03b1(t))\n)\n,\nwhere we denote \u2016u\u2016 2 X := \u2211n i=1 u 2 i \u2016xi\u2016 2. We will now use the optimally of \u03b4(t) to upper bound the above, noting that if we replace \u03b4(t) with any quantity, and in particular with s(\u03c7(t) \u2212\u03b1(t)), we can only decrease H(\u00b7,\u03b1(t)), and thus increase the right-hand-side above:\n\u2264 1n\nn \u2211\ni=1\n[\n\u2113\u2217i (\u2212(\u03b1 (t) i + s(\u03c7 (t) i \u2212\u03b1 (t) i )))\u2212 \u2113 \u2217 i (\u2212\u03b1 (t) i ) ]\n+ \u03bb2\n(\n\u03b2b\n\u2225 \u2225 \u2225\n1 \u03bbns(\u03c7\n(t) \u2212\u03b1(t)) \u2225 \u2225\n\u2225\n2 X + 2 ( 1 \u03bbns(\u03c7 (t) \u2212\u03b1(t)) )\u22a4 Xw(\u03b1(t))\n)\nNow from convexity we have \u2113\u2217i (\u2212(\u03b1 (t) i + s(\u03c7 (t) i \u2212\u03b1 (t) i ))) \u2264 s\u2113 \u2217 i (\u2212\u03c7 (t) i ) + (1\u2212 s)\u2113 \u2217 i (\u2212\u03b1 (t) i ), and so:\n\u2264 1n\nn \u2211\ni=1\n(\ns\u2113\u2217i (\u2212\u03c7 (t) i ) + s\u03c7 (t) i\n\u2329\nw(\u03b1(t)),xi\n\u232a \u2212 s\u2113\u2217i (\u2212\u03b1 (t) i ) )\n+ \u03bb2\n(\n\u03b2b\n\u2225 \u2225 \u2225\n1 \u03bbns(\u03c7\n(t) \u2212\u03b1(t)) \u2225 \u2225\n\u2225\n2 X + 2 ( 1 \u03bbns(\u2212\u03b1 (t)) )\u22a4 Xw(\u03b1(t))\n)\nand from conjugacy we have \u2113\u2217i (\u2212\u03c7 (t) i ) = \u2212\u03c7 (t) i \u2329 w(\u03b1(t)),xi \u232a \u2212 \u2113i( \u2329 w(\u03b1(t)),xi \u232a ), and so:\n\u2264 sn\nn \u2211\ni=1\n(\n\u2212\u03c7 (t) i\n\u2329\nw(\u03b1(t)),xi\n\u232a\n\u2212 \u2113i\n(\u2329\nw(\u03b1(t)),xi\n\u232a)\n+ \u03c7 (t) i\n\u2329\nw(\u03b1(t)),xi\n\u232a \u2212 \u2113\u2217i (\u2212\u03b1 (t) i ) )\n+ \u03bb2\n(\n\u03b2b\n\u2225 \u2225 \u2225\n1 \u03bbns(\u03c7\n(t) \u2212\u03b1(t)) \u2225 \u2225\n\u2225\n2 X + 2 ( 1 \u03bbns(\u2212\u03b1 (t)) )\u22a4 Xw(\u03b1(t))\n)\n\u2264 sn\nn \u2211\ni=1\n(\n\u2212\u2113i\n(\u2329\nw(\u03b1(t)),xi\n\u232a)\n\u2212 \u2113\u2217i (\u2212\u03b1 (t) i )\u2212\u03b1 (t) i\n\u2329\nw(\u03b1(t)),xi\n\u232a)\n+ \u03bb2\u03b2b\n\u2225 \u2225 \u2225\n1 \u03bbns(\u03c7\n(t) \u2212\u03b1(t)) \u2225 \u2225\n\u2225\n2\nX\n(25) = \u2212sG(\u03b1(t)) + 12\u03bb ( s n\n)2 (\n\u03b2b\n\u2225 \u2225 \u2225 (\u03c7(t) \u2212\u03b1(t)) \u2225 \u2225 \u2225 2\nX\n)\n.\nMultiplying both sides of the resulting inequality by \u2212bn we obtain (26). To get the bound on G (t), recall that \u2113(\u00b7) is L-Lipschitz, hence \u2212L \u2264 \u03c7 (t) i \u2264 L. Furthermore, \u03b1 (t) is dual feasible, hence \u2113\u2217i (\u2212\u03b1 (t) i ) < \u221e and so (\u2212\u03b1 (t) i ) is a (sub)derivative of \u2113i and so we also have \u2212L \u2264 \u03b1 (t) i \u2264 L and for each i, and (\u03c7 (t) i \u2212\u03b1 (t) i ) 2 \u2264 4L. For the hinge loss we have 0 \u2264 \u03c7 (t) i ,\u03b1 (t) i \u2264 1, and so (\u03c7 (t) i \u2212\u03b1 (t) i ) 2 \u2264 1.\nWe are now ready to prove the theorem.\nProof of Theorem 2. We will bound the change in the dual sub-optimality \u01eb (t) D := D(\u03b1 \u2217)\u2212D(\u03b1(t)):\nEAt [\u01eb (t+1) D ] = E[D(\u03b1 (t))\u2212D(\u03b1(t+1)) + \u01eb (t) D ]\n(Lemma 4)\n\u2264 \u2212b (\ns nG(\u03b1 (t))\u2212 ( s n )2 \u03b2b 2\u03bbG\n)\n+ \u01eb (t) D\n\u01eb (t) D \u2264G(\u03b1(t))\n\u2264 \u2212b sn\u01eb (t) D + b\n( s\nn\n)2 \u03b2b 2\u03bbG+ \u01eb (t) D = (1\u2212 b s n )\u01eb (t) D + b ( s\nn\n)2 \u03b2b 2\u03bbG. (28)\nUnrolling this recurrence, we have:\nE[\u01eb (t) D ] \u2264 (1 \u2212 b s n ) t\u01eb (0) D + b ( s n )2 \u03b2b 2\u03bb\nG t\u22121 \u2211\ni=0\n(1\u2212 b sn ) i \u2264 (1\u2212 b sn ) t\u01eb (0) D + ( s n ) \u03b2bG 2\u03bb .\nSetting s = 1 and\nt0 := [\u2308 n b log(2\u03bbn\u01eb (0) D /(G\u03b2b))\u2309]+ (29)\nyields:\nE[\u01eb (t0) D ] \u2264 (1\u2212 b n ) t0\u01eb (0) D +\ns\nn\n\u03b2bG\n2\u03bb \u2264 G\u03b2b 2\u03bbn\u01ebD \u01ebD + 1 n \u03b2bG 2\u03bb = \u03b2bG \u03bbn . (30)\nFollowing the proof of Shalev-Shwartz & Zhang we will now show by induction that\n\u2200t \u2265 t0 : E[\u01eb (t) D ] \u2264\n2\u03b2bG\n\u03bb(2n+ b(t\u2212 t0)) . (31)\nClearly, (30) implies that (31) holds for t = t0. Now, if it holds for some t \u2265 t0, we show that it also holds for t+ 1. Using s = 2n2n+b(t\u2212t0) in (28) we have:\nE[\u01eb (t+1) D ]\n(28)\n\u2264 (1 \u2212 b sn )E[\u01eb (t) D ] + b\n( s\nn )2 \u03b2b 2\u03bb G (31) \u2264 (1\u2212 b sn ) 2\u03b2bG \u03bb(2n+ b(t\u2212 t0)) + b ( s n )2 \u03b2b 2\u03bb G\n= (1\u2212 b 2\n2n+ b(t\u2212 t0) )\n2\u03b2bG\n\u03bb(2n+ b(t\u2212 t0)) + b\n(\n2\n2n+ b(t\u2212 t0)\n)2 \u03b2b 2\u03bb G\n= 2G\u03b2b\u03bb(2n+b(t\u2212t0)+b) (2n+b(t\u2212t0)+b)(2n+b(t\u2212t0)\u2212b) (2n+b(t\u2212t0))2 \u2264 2G\u03b2b \u03bb(2n+ b(t\u2212 t0) + b) , (32)\nwhere in the last inequality we used the arithmetic-geometric mean inequality. This establishes (31).\nNow, for the average \u03b1\u0304 defined in (21) we have:\nE[G(\u03b1\u0304)] = E\n[\nG\n(\nT\u22121 \u2211\nt=T0\n1 T\u2212T0 \u03b1(t)\n)]\n\u2264 1T\u2212T0E\n[\nT\u22121 \u2211\nt=T0\nG ( \u03b1(t) )\n]\nApplying Lemma 4 with s = nb(T\u2212T0) :\n\u2264 nb(T \u2212 T0)\nnb\n1\nT \u2212 T0\n( E[D(\u03b1(T ))]\u2212 E[D(\u03b1(T0))] ) + G\u03b2bn\n2nb(T \u2212 T0)\u03bb\n\u2264 ( D(\u03b1\u2217)\u2212 E[D(\u03b1(T0))] ) + G\u03b2b\n2b(T \u2212 T0)\u03bb\n(31)\n\u2264\n(\n2\u03b2bG\n\u03bb(2n+ b(T0 \u2212 t0))\n)\n+ G\u03b2b\n2b(T \u2212 T0)\u03bb\nand if T \u2265 \u2308nb \u2309+ T0 and T0 \u2265 t0:\n\u2264 \u03b2bG\nb\u03bb\n(\n2\n2nb + (T0 \u2212 t0) +\n1\n2(T \u2212 T0)\n)\n. (33)\nNow, we can ensure the above is at most \u01eb if we require:\nT0 \u2212 t0 \u2265 \u03b2b b\n(\n4G \u03bb\u01eb \u2212 2 n\n\u03b2b\n)\n(34)\nT \u2212 T0 \u2265 \u03b2b b G \u03bb\u01ebG . (35)\nCombining the requirements (29), (34) and (35) with T \u2265 \u2308nb \u2309+T0 and T0 \u2265 t0, and recalling that for the hinge loss G = 1 and with \u03b1(0) = 0 we have \u01eb (0) D = D(\u03b1 \u2217)\u2212D(0) \u2264 1\u22120 = 1 gives the requirements in Theorem 2."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J. Duchi"], "venue": "In NIPS,", "citeRegEx": "Agarwal and Duchi,? \\Q2011\\E", "shortCiteRegEx": "Agarwal and Duchi", "year": 2011}, {"title": "Parallel coordinate descent for l1-regularized loss minimization", "author": ["J.K. Bradley", "A. Kyrola", "D. Bickson", "C. Guestrin"], "venue": "In ICML,", "citeRegEx": "Bradley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 2011}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["A. Cotter", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "In NIPS,", "citeRegEx": "Cotter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2011}, {"title": "Optimal distributed online prediction using minibatches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "A dual coordinate descent method for large-scale linear svm", "author": ["Hsieh", "C-J", "Chang", "K-W", "Lin", "S.S. Keerthi", "S. Sundarajan"], "venue": "In ICML,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Nesterov", "Yu"], "venue": "SIAM J. Optimization,", "citeRegEx": "Nesterov and Yu.,? \\Q2012\\E", "shortCiteRegEx": "Nesterov and Yu.", "year": 2012}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. Re", "S. Wright"], "venue": "K.Q. (eds.),", "citeRegEx": "Niu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2011}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": null, "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": null, "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d,? \\Q2012\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d", "year": 2012}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d,? \\Q2013\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d", "year": 2013}, {"title": "Stochastic Methods for l1-regularized", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "Loss Minimization. JMLR,", "citeRegEx": "Shalev.Shwartz and Tewari,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz and Tewari", "year": 2011}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": null, "citeRegEx": "Shalev.Shwartz and Zhang,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang", "year": 2012}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming: Series A and B- Special Issue on Optimization and Machine Learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Solving large scale linear prediction using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "In ICML,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}], "referenceMentions": [{"referenceID": 4, "context": "2011, NORMA, Zhang 2004) and dual stochastic coordinate ascent (Hsieh et al., 2008).", "startOffset": 63, "endOffset": 83}, {"referenceID": 4, "context": "SDCA has been consistently shown to outperform Pegasos in practice (Hsieh et al., 2008; Shalev-Shwartz et al., 2011), and is also popular as it does not rely on setting a step-size as in Pegasos.", "startOffset": 67, "endOffset": 116}, {"referenceID": 12, "context": "SDCA has been consistently shown to outperform Pegasos in practice (Hsieh et al., 2008; Shalev-Shwartz et al., 2011), and is also popular as it does not rely on setting a step-size as in Pegasos.", "startOffset": 67, "endOffset": 116}, {"referenceID": 4, "context": "2011, NORMA, Zhang 2004) and dual stochastic coordinate ascent (Hsieh et al., 2008). However, the inherent sequential nature of such approaches becomes a problematic limitation for parallel and distributed computations as the predictor must be updated after each training point is processed, providing very little opportunity for parallelization. A popular remedy is to use mini-batches. That is, to use several training points at each iteration, instead of just one, calculating the update based on each point separately and aggregating the updates. The question is then whether basing each iteration on several points can indeed reduce the number of required iterations, and thus yield parallelization speedups. In this paper, we consider using mini-batches with Pegasos (SGD on the primal objective) and with Stochastic Dual Coordinate Ascent (SDCA). We show that for both methods, the quantity that controls the speedup obtained using mini-batching/parallelization is the spectral norm of the data. In Section 3 we provide the first analysis of minibatched Pegasos (with the original, non-smooth, SVM objective) that provably leads to parallelization speedups (Theorem 1). The idea of using mini-batches with Pegasos is not new, and is discussed already by Shalev-Shwartz et al. (2011), albeit without a theoretical justification.", "startOffset": 64, "endOffset": 1290}, {"referenceID": 13, "context": "Similar to a recent analysis of non-mini-batched SDCA by Shalev-Shwartz & Zhang (2012), we establish a guarantee on the duality gap, and thus also on the suboptimality of the primal SVM objective, when using mini-batched SDCA (Theorem 2).", "startOffset": 74, "endOffset": 87}, {"referenceID": 3, "context": "Several recent papers consider the use of mini-batches in stochastic gradient descent, as well as stochastic dual averaging and stochastic mirror descent, when minimizing a smooth loss function (Dekel et al., 2012; Agarwal & Duchi, 2011; Cotter et al., 2011).", "startOffset": 194, "endOffset": 258}, {"referenceID": 2, "context": "Several recent papers consider the use of mini-batches in stochastic gradient descent, as well as stochastic dual averaging and stochastic mirror descent, when minimizing a smooth loss function (Dekel et al., 2012; Agarwal & Duchi, 2011; Cotter et al., 2011).", "startOffset": 194, "endOffset": 258}, {"referenceID": 1, "context": "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data.", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a \u201csafe\u201d variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as \u201cnaive\u201d mini-batching (Section 4.", "startOffset": 0, "endOffset": 362}, {"referenceID": 1, "context": "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a \u201csafe\u201d variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as \u201cnaive\u201d mini-batching (Section 4.1). More directly related is recent work of Richt\u00e1rik & Tak\u00e1\u010d (2013; 2012) which provided a theoretical framework and analysis for a more general setting than SHOTGUN, that includes also the SVM dual as a special case. However, guarantees in this framework, as well as those of Bradley et al. (2011), are only on the dual suboptimality (in our terminology), and not on the more relevant primal suboptimality, i.", "startOffset": 0, "endOffset": 823}, {"referenceID": 1, "context": "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a \u201csafe\u201d variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as \u201cnaive\u201d mini-batching (Section 4.1). More directly related is recent work of Richt\u00e1rik & Tak\u00e1\u010d (2013; 2012) which provided a theoretical framework and analysis for a more general setting than SHOTGUN, that includes also the SVM dual as a special case. However, guarantees in this framework, as well as those of Bradley et al. (2011), are only on the dual suboptimality (in our terminology), and not on the more relevant primal suboptimality, i.e., the suboptimality of the original SVM problem we are interested in. Our theoretical analysis builds on that of Richt\u00e1rik & Tak\u00e1\u010d (2012), combined with recent ideas of Shalev-Shwartz & Zhang (2012) for \u201cstandard\u201d (serial) SDCA, to obtain bounds on the duality gap and primal suboptimality.", "startOffset": 0, "endOffset": 1074}, {"referenceID": 1, "context": "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a \u201csafe\u201d variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as \u201cnaive\u201d mini-batching (Section 4.1). More directly related is recent work of Richt\u00e1rik & Tak\u00e1\u010d (2013; 2012) which provided a theoretical framework and analysis for a more general setting than SHOTGUN, that includes also the SVM dual as a special case. However, guarantees in this framework, as well as those of Bradley et al. (2011), are only on the dual suboptimality (in our terminology), and not on the more relevant primal suboptimality, i.e., the suboptimality of the original SVM problem we are interested in. Our theoretical analysis builds on that of Richt\u00e1rik & Tak\u00e1\u010d (2012), combined with recent ideas of Shalev-Shwartz & Zhang (2012) for \u201cstandard\u201d (serial) SDCA, to obtain bounds on the duality gap and primal suboptimality.", "startOffset": 0, "endOffset": 1135}, {"referenceID": 12, "context": "When b = 1 we have \u03b2b = 1 (see (11)) and Theorem 1 agrees with the standard (serial) Pegasos analysis (Shalev-Shwartz et al., 2011).", "startOffset": 102, "endOffset": 131}, {"referenceID": 4, "context": "SDCA was suggested and studied empirically by Hsieh et al. (2008), where empirical advantages over Pegasos were often observed.", "startOffset": 46, "endOffset": 66}, {"referenceID": 4, "context": "SDCA was suggested and studied empirically by Hsieh et al. (2008), where empirical advantages over Pegasos were often observed. In terms of a theoretical analysis, by considering the dual problem (2) as an l1-regularized, box-constrained quadratic problem, it is possible to obtain guarantees on the dual suboptimality, D(\u03b1) \u2212 D(\u03b1), after a finite number of SDCA iterations (Shalev-Shwartz & Tewari, 2011; Nesterov, 2012; Richt\u00e1rik & Tak\u00e1\u010d, 2013). However, such guarantees do not directly imply guarantees on the primal suboptimality of w(\u03b1). Recently, Shalev-Shwartz & Zhang (2012) bridged this gap, and provided guarantees on P(w(\u03b1)) \u2212 P(w) after a finite number of SDCA iterations.", "startOffset": 46, "endOffset": 583}, {"referenceID": 13, "context": "Based on the above lemma, we can modify the analysis of Shalev-Shwartz & Zhang (2012) to obtain (see complete proof in the appendix): Theorem 2.", "startOffset": 73, "endOffset": 86}, {"referenceID": 12, "context": "cov is the forest covertype dataset of Shalev-Shwartz et al. (2011), astro-ph consists of abstracts of papers from physics also of Shalev-Shwartz et al.", "startOffset": 39, "endOffset": 68}, {"referenceID": 12, "context": "cov is the forest covertype dataset of Shalev-Shwartz et al. (2011), astro-ph consists of abstracts of papers from physics also of Shalev-Shwartz et al. (2011), rcv1 is from the Reuters collection and news20 is from the 20 news groups both obtained from libsvm collection (Libsvm).", "startOffset": 39, "endOffset": 160}, {"referenceID": 1, "context": "Our safe SDCA mini-batching approach is similar to the parallel coordinate descent methods of Bradley et al. (2011) and Richt\u00e1rik & Tak\u00e1\u010d (2012), but we provide an analysis in terms of the primal SVM objective, which is the more relevant object of interest.", "startOffset": 94, "endOffset": 116}, {"referenceID": 1, "context": "Our safe SDCA mini-batching approach is similar to the parallel coordinate descent methods of Bradley et al. (2011) and Richt\u00e1rik & Tak\u00e1\u010d (2012), but we provide an analysis in terms of the primal SVM objective, which is the more relevant object of interest.", "startOffset": 94, "endOffset": 145}], "year": 2013, "abstractText": "We address the issue of using mini-batches in stochastic optimization of SVMs. We show that the same quantity, the spectral norm of the data, controls the parallelization speedup obtained for both primal stochastic subgradient descent (SGD) and stochastic dual coordinate ascent (SCDA) methods and use it to derive novel variants of mini-batched SDCA. Our guarantees for both methods are expressed in terms of the original nonsmooth primal problem based on the hinge-loss.", "creator": "LaTeX with hyperref package"}}}