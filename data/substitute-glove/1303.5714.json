{"id": "1303.5714", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "A Bayesian Method for Constructing Bayesian Belief Networks from Databases", "abstract": "This it reality part Bayesian required for constructing Bayesian belief messaging started making database important cases. Potential provided use computer - recruited explanation testing, automated based discovery, and circuits construction beyond phenomenological assistant systems. Results one presented created put audit evaluation it that computational new constructing a impression exclusively from yet application similar deaths. We relate the theory rest this paper decided 1992 work, and me discuss moving possibility.", "histories": [["v1", "Wed, 20 Mar 2013 15:30:21 GMT  (359kb)", "http://arxiv.org/abs/1303.5714v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gregory f cooper", "edward h herskovits"], "accepted": false, "id": "1303.5714"}, "pdf": {"name": "1303.5714.pdf", "metadata": {"source": "CRF", "title": "A Bayesian Method for Constructing Bayesian Belief Networks from Databases", "authors": ["Gregory F. Cooper"], "emails": [], "sections": [{"heading": null, "text": "1 INTRODUCTION\nDecision making typically is replete with uncertainty. In general, it is important that computer systems that assist in decision making be capable of representing and reasoning with uncertainty. Probabilistic networks provide a precise and concise representation of probabilistic dependencies among variables. In the last few years, significant progress has been made in formalizing the theory of probabilistic networks [Pearl 1988]. Advances also have occurred in improving the efficiency of methods for inference on probabilistic networks [Hcnrion 1990], although for some complex networks additional improvements are still needed. The feasibility of using probabilistic networks in constructing diagnostic systems has been demonstrated in several domains [Agogino and Rcge 1987, Andreassen, Woldbyc, ct al. 1987, Beinlich, Suermondt, et al. 1989, Beckerman, Horvitz, ct al. 1989, Henrion and Cooley 1987, Holtzman 1989, Suermondt and Amylon 1989].\nAlthough substantial advances have been made in developing the theory and application of probabllisltc networks the actual construction of these networks often remains \ufffd difficult, time-consuming task. The task is time-consuming because typically it must be performed\nmanually by an expert or with an expert. Some important progress has been made in developing methods to improve the efficiency of knowledge acquisition from experts [Beckerman 1990]. These methods are likely to remain important in domains of small to moderate size in which there are readily available experts. Some domains, however, are large. In others, there are few, if any, available experts. Methods for assisting, or in some cases replacing, the manual expert-based methods of knowledge acquisition are needed.\nDatabases are becoming increasingly abundant in many areas, including science, engineering, and business. In each of these areas, there arc many potential opportunities for using probabilistic networks to provide assistance in decision making. By using databases to assist in constructing probabilistic networks, we may be able to decrease knowledge acquisition time significantly. Automatically generated networks could be used directly to provide decision-making assistance, or used as a starting point for modification by an expert. In the latter case,. the editing of a network may require substantially less time than de novo generation of the network by an expert.\nThe automated construction of probabilistic networks also can provide insight into the probabilistic dependencies that exist among the domain variables. One application IS the automated discovery of dependency relationships. The computer program searches for a probabilistic network structure that has a high posterior probability given the database, and outputs the structure and its probability. A related task is computer-assisted hypothesis testing: the user enters a hypothesized structure of the dependency relationships among a set of variables and the program calculates the probability of the structure given a database of cases on the variables. These applications have the potential to affect broad areas of scientific discovery and data evaluation.\nAs an example, consider the fictitious database of cases shown in Table 1. Suppose that xi is an experimental condition and x2 and x3 arc two experimental outcomes.\nA Bayesian Method for Constructing Bayesian Belief Networks from Databases 87\nGiven the database, what are the qualitative dependency relationships among the variables? For example, do x1 and x3 influence each other directly, or do they do so only through x2? What is the probability that x3 will be present if x1 is present? Clearly, there are no categorically correct answers to each of these questions. The answers depend on a number of factors, including the model that we use to represent the data, and our prior know ledge abcut the data in the database and the relationships among the variables. In this paper, we do not attempt to consider all such factors in their full generality. Rather, we specialize the general task by presenting one particular framework for constructing probabilistic networks from databases, as for example the database in Table 1, such that these networks can be used for probabilistic inference, as for instance in calculating P(x3 = present I x1 = present). In particular, we focus on using a Bayesian belief network as a model of probabilistic dependency. Our primary goal is to construct such a network (or networks), given a database and a set of explicit assumptions about our prior probabilistic knowledge of the domain, and then use that network (or networks) for inference.\nIn this section, we first briefly review some key concepts about Bayesian belief networks. Then, we present the primary theoretical developments of our work thus far in developing methods for learning the structure of Bayesian belief networks from databases; a more detailed discussion with proofs appears in [Cooper and Herskovits 199 1]. Finally, we discuss the empirical results of an algorithm that applies this theory to search for the most likely belief-network structure, given a database.\n2.1 BAYES IAN BELIEF NETWORKS\nA Bayesian belief-network structure B s is a directed acyclic graph in which nodes represent domain variables and arcs between nodes represent probabilistic dependencies [Cooper 1989, Horvitz, Breese, et al. 1988, Lauritzen and Spiegelhalter 1988, Neapolitan 1990, Pearl 1986, Pearl 1988, Shachter 1988]. A variable in a Bayesian belief network structure may be continuous [Shachter and Kenley 1989] or discrete. In this paper, we shall focus our discussion on discrete variables. Figure 1a shows an example of a belief-network structure, which we shall call B s1, containing three variables. The arc from x1 to x2 indicates that these two variables are probabilistically dependent. Similarly, the arc from x2 to x3 indicates a probabilistic dependency between these two variables. The absence of an arc from x1 to x3 implies that there is no direct probabilistic dependency between x1 and x3\u2022 In particular, the probability of each value of x3 is conditionally independent of the value of x1 given that the value of x2 is known. Figure 1 b shows an alternative structure that expresses different dependency relationships among the three variables. The representation of conditional dependencies and independencies is the essential function of belief networks. For a detailed discussion of the semantics of Bayesian belief networks, see [Pearl 1988].\nA Bayesian belief-network structure Bs is augmented by conditional probabilities, B p, to form a Bayesian belief network B. Thus, B = (B s. B p ). For brevity, we call B a\n88 Cooper and Herskovits\nbelief network. For each node1 in the network structure, there is a conditional probability function that relates this node to its immediate predecessors (parents). We shall use 11:; to denote the parent nodes of variable x,. If a node has no parents, then a prior probability function, P(x;), is specified. A set of probabilities is shown in Table 2 for the belief-network structure in Figure Ia. We shall use the term conditional probability to refer to a probability statement, such as P(x2 = present I x1 = present). We use the term conditional probability assignment to denote a numerical assignment to a conditional probability, as, for example, the assignment P(x2 = present I x1 =present) = 0.8. The network structure B s1 in Figure Ia and the probabilities BPI in Table 2 together define a belief network, which we denote as B1\u2022\nBelief networks can be used to represent the probabilities over any discrete sample space: the probability of any sample point in that space can be computed from the probabilities in the belief network. The key feature of belief networks is their explicit representation of the conditional independence among events. In particular, investigators have shown [Kiivcri, Speed, ct al. 1984, Pearl 1988, Shachter 1986] that the joint probability of any particular instantiation2 of all n variables in a belief net work can be calculated as\nn P(X 1, ... , Xn) = IT P(X; I Jr; ) ,\ni = 1 (I)\nSince there is a one-to-one correspondence between a node in B5 and a variable in Hp, we shall use the terms node and variable interchangeably.\n2 An instantiated variable is a variable with an assigned value. When we need to designate a particular value v;k of variable x;, we shall write x; = ViJc.\nwhere each X; represents an instantiated variable and Jr. ' represents an instantiation of the parents of X,.\nTherefore, the joint probability of any instantiation of all the variables in a belief network can be computed as the product of only n probabilities. In principle, we can rec?ver the complete joint-probability space from the behef-network representation by calculating the joint probab1ht1es that result from every possible instantiation of the n variables in the network. Thus, we can determine any probability of the form P(Z I Y), where Z and Y are sets of variables with known values (instantiated variables). For example, for our sample three-node belief networkS!> P(x3 =present I x1 =present)= 0. 75.\nLet us now consider the problem of finding the most probable belief-network structure, given a database. Once such a structure is found, we can derive numerical belief network probabilities from the database [Cooper and Herskovits 1991]. We can use the resulting belief network for probabilistic inference, such as calculating the value of P(x3 =present I x1 = present). In addition, the structure may lend insight into the dependency relationships among the vanables m the database, as, for example, possible causal relationships.\nTo be more specific, let D be a database of cases, Z be the set of variables represented by D, and Bs. and Bs be two\n. ' J behef-network structures containing exactly those variables that are in Z. In the next section, we develop a method for computing P(Bs, I D)/P(Bs1 I D). By computing such ratios for pairs of belief-network structures, we can rank order a set of structures by their posterior probabilities. To calculate the ratio of posterior probabilities, we shall calculate P(Bs., D) and P(Bs., D) ' J and use the following equivalence:\nP(Bs, D) P(Bs, I D) P(D) P(Bs, D) (2) = P(Bs1 I D) P(Bs1, D) P(Bs1, D)\nP(D)\n2.2 A FORMULA FOR COMPUTING P(Bs, D)\nLet B s represent an arbitrary belief-network structure containing just the variables in Z. In this section, we present a method for calculating P(B5, D). In doing so, we shall introduce five assumptions that render this calculation computationally tractable.\nAssumption 1. The process that generated the database is modeled as a belief network containing just the variables in Z, which arc discrete.\nA Bayesian Method for Constructing Bayesian Belief Networks from Databases 89\nAs this assumption states, we shall not consider continuous variables in this paper.\nA belief network (structure plus conditional probabilities) is sufficient to capture any probability distribution over the variables in Z [Pearl 1988]. A belief-network structure alone, containing just the variables in Z, can capture many-but not all-the qualitative independence relationships that might exist in an arbitrary probability distribution over Z [Pearl l 988]. Assumption 1, therefore, is justified to the extent that the relationships of independence and dependence, among variables in the underlying process that is hidden from us, can be represented by some belief network. In the remainder of this section, we shall write as though database D was generated by Monte Carlo sampling of a belief network with structure B s that is hidden from us. One of our primary goals will be to usc D to try to discover B 5\u2022 In this section, we assume that B s contains just the variables in Z. In [Cooper and Herskovits 1991], we allow Bs to contain variables beyond those in Z.\nThe application of Assumption 1 yields\nP(Bs, D) =\nL P(D I Bs , Bp ) j(Bp I Bs) P(Bs) dBp, (3) where B p is a vector whose values denote the conditional probability assignments associated with belief-network structure B 5, and f is the conditional probability density function over B p given B 5. The integral is over all possible value assignments to B P\u00b7 Thus, we are integrating over all possible belief networks that can have structure B 5. The integral in Equation 3 represents a multiple integral and the variables of integration are the conditional probabilities associated with structureB5.\nAssumption 2. Cases occur independently, given a belief network model.\nThis assumption is equivalent to assuming that the belief network that is generating the data is static, that is, it docs not change as cases are being generated. It follows from the conditional independence of cases expressed in Assumption 2 that Equation 3 may be rewritten as\nP(Bs, D) =\nP(Bs) f. [JJ1 P(Cj I Bs, Bp)] j(Bp I Bs) dBp, Bp\n(4)\nwhere m is the number of cases in D and Cj is the jth case in D.\nAssumption 3. Cases are complete, that is, there are no cases that have variables with missing values.\nIn [Cooper and Herskovits 1991], we relax this assumption. We now introduce additional notation to facilitate the application of Assumption 3. Let D ij denote the value assignment of variable i in case j. Thus, D21 = 0, since x2 = 0 ( i.e., x2 is absent) in case I in Table I. In B 5, for every variable X;, there is a set of parents Tr; (possibly the empty set). For each case in D, the variables in Tr; are each assigned a particular value; call such a case specific instantiation of all the variables in Tr; a ;r instantiation. Let 1/J; denote a list of the unique Tr instantiations for the parents of x; as seen in D. If x; has no parents, then we define 1/J; to be the list (0), where 0 represents the empty set of parents. Although the ordering of the elements of 1/J; is arbitrary, we shall use a list (vector), rather than a set, so that we can refer to members of 1/J; using an index. For example, consider variable x2 in Bsi> which has parent set n2 = {x1}. In this example, \u00a22 = ((x1 = 0), (x1 = !)), because there are cases in D where x1 has the value 0 and cases where it has the value I. Let 1/J;[j] be the jth clement of 1/J;. Thus, for example, \u00a22[1] is\nequal to (x1 = 0). Let cr(i, j) be an index function, such that the instantiation of Tr; in case j is the a(i, J)lh clement of 1/J;. Thus, for example, cr( 2, I) = 2, because in case I the parent set of variable x2 - namely {x1} - is instantiated as x1 = 1, which is the second clement of \u00a22\u2022 Therefore, I/Jz[a(2, l)l is equal to (x1 = 1). Let q, = I 1/J; I. Note that since there are m cases in D, q, :;:; m. Since, according to Assumption 3, cases arc complete, we can apply Equation I to represent the probability of a case as follows:\nn\nP(Cj I Bs, Bp) = IT P(x; = D;j I 1/J;[a(i,j)] , Bp) (5) i= 1 Substituting Equation 5 into Equation 4, we obtain\nP(Bs, D) =\n(6)\nx j(Bp I Bs ) dBp.\nFor a given i and j , lctf(P(x ; I 1/J;[jl,Bp)) denote our probability distribution over the possible values of\n90 Cooper and Herskovits\nP (xi I li'lj(j], B p ). That is, the density function f(P(x; I 1/J;[jl, Bp)) represents our belief about the values to assign the conditional probability function P (x; I 1/J;[j], Bp). For notational convenience, we shall leave the term B P implicit. We shall assume that our belief about the values to assign to a conditional probability function in a belief network is not influenced by our belief about the values to assign any other conditional probability function. More formally, we can express this assumption as follows:\nAssumption 4. For 1 $ i, i' $ n, 1 $j $ q;, 1 $}' $ q;\u00b7,\nif ij to i'j' then the distributionf(P(x; I 1/J;[j])) is marginally independent of the distributionflP(x;\u00b7l 1/J;\u00b7[J'] )) .\nAs an example, Assumption 4 implies that our belief about the assignment of a value to the conditional probability P (x3 = 0 I x2 = 0) is independent of our assignment of a value to the conditional probability P (x2 = 0 I x1 = 0), since these two probabilities are components of different conditional probability distributions. However, our belief about P (x2 = 0 I x1 = 0) must be dependent on our belief about P (x2 = 1 I x1 = 0), since they are members of the same conditional probability distribution; in particular, since x2 is a binary variable, P(x2 = 0 I x1 = 0) = 1 -P(xz = 1 I Xt = 0).\nAssumption 5. For 1 $ i $ n, 1 $ j $ qi, the distribution f(P(x; I 1/J;[j])) is uniform.\nThis assumption states that initially, before we see the data, we are indifferent regarding giving one assignment of values to a conditional probability function versus some other assignment. This probability density function is, however, just a special case of the Dirichlet distribution [deGroot 1970]. In [Cooper and Herskovits 1991] we generalize Assumption 5 by representing f(P (x; I 1/J;[jl)) with a Dirichlet distribution.\nAssumptions 4 and 5 permit us to define the joint density function f (Bp I Bs) using density functions of the form\nf(P(x; I cpJj])) which are uniform.\nWe now use Assumptions 1 through 5 in the following theorem, proven in [Cooper and Herskovits 199 1], which solves Equation 3 for P(Bs, D):\nTheorem Let Z be a set of n discrete variables, where a variable x; in Z has r; possible value assignments: (v;1, ... , v;7). Let D be a database of m cases, where ' each case contains a value assignment for each variable in Z. Let Bs denote a belief-network structure containing just the variables in Z. Each variable X; in Bs has\na set of parents n:;. Let cp;[jl denote the jth unique instantiation of n:; relative to D. Suppose there are q; such unique instantiations of n:;. Define aijk to be the number of cases in D in which variable x; i s\ninstantiated as v;k and n:; is instantiated as 1/J;[j].\nLetN;i = L:= 1 aiik\u00b7 If Assumptions 1 through 5 hold, then\nn qi (r; _ l )I r; P(Bs, D) = P(Bs) IT IT \u00b7 IT a;jk!\u00b7 (7) i=tj=l (N;j+ r;-1)! k=l 0\nEquation 7 allows us to calculate P (B 5, D) using knowledge of P (Bs) combined with simple enumeration over the cases in the database. For example, by applying Equation 7 to the structures in Figure 1 with the data in Table 1, we find that P (Bs1,D) = 8.9 1 x 10-11 and P(Bs2, D)= 8.9 1 x 10\u00b712, i f we assume uniform priors on P (B s). Note that these numbers are small because the probability of seeing exactly those data that are in D is small. Given the assumptions in this section, the data imply that B s1 is 10 times more likely than B sz\u00b7 This result is not surprising, because we used B 1 to generateD by the application of Monte Carlo sampling.\nConsider the time complexity of computing Equation 7. Let r = max;[r;] fori = 1 to n. Define Iss to be the time required to compute the prior probability of structure B s, P (Bs). In [Cooper and Herskovits 1991] we show that the time complexity of computing Equation 7 is O(m n2 r + Iss>\u00b7\nUsing Equation 7, we can calculate posterior probabilities of belief-network structures as\nP(Bs I D) = P(Bs, D) I, P(Bs, D) Bs\n(8)\nApplying Equation 8 to our previous example, we obtain P (Bs1 I D)= 0. 109, and P (B52 I D)= 0.0 1 1. The remaining probability mass of 0.88 is distributed among the other 23 possible three-node belief-network structures. When there are more than a few variables in the model, the complexity of computing Equation 8 is intractable, due to the large number of belief-network structures. Consider, however, the situation in which\n\ufffdBs e y P(Bs, D)\ufffd P(D), for some set Y of structures, where IY I is small. If Y can be efficiently located, then\nA Bayesian Method for Constructing Bayesian Belief Networks from Databases 91\nEquation 8 can be efficiently computed to a close approximation.\n2.3 FINDING THE MOST PROBABLE BELIEF-NETWORK STRUCTURE\nConsider the problem of determining the belief-network structure Bs that maximizes P(Bs I D). Knowing such a structure may lend insight into the causal relationships among the model variables, particularly if the structure has a high posterior probability. The structure also may be augmented with numerical probabilities, as we discuss in [Cooper and Herskovits 1991], and used to perform probabilistic inference.\nFor a given database D,P(Bs,D) oc P(BsiD), and therefore finding the Bs that maximizes P(Bs I D) is equivalent to finding the Bs that maximizes P(Bs, D). We can maximize P(B s, D) by applying exhaustively Equation 7 for every possible B s containing just the variables in z. As a function of the number of variables, the number of possible structures grows super-exponentially. Thus, an exhaustive enumeration of all network structures is not feasible in most domains. In particular, Robinson [Robinson 1976] has derived an efficiently computable recursive function for determining the number of possible belief-network structures that contain n nodes. For n = 2, the number of possible structures is 3; for n = 3, it is 25; for n = 5, it is 29,000; and, for n = 10, it is approximately 4.2 x 1018. Clearly, we need a method that is more efficient than is exhaustive enumeration for locating the B s that maximizes P(B s I D). In Section 2.3.1, we introduce additional assumptions that reduce the time complexity of enumeration. The complexity, however, remains exponential. Thus, in Section 2.3.2 we introduce and discuss a heuristic method that is polynomial time.\n2.3.1 An Exhaustive Search Procedure\nLet us assume that we can specify an ordering on all n variables, such that if x; precedes x1 in the ordering, then we do not allow structures in which there is an arc from xi to x;. In some domains, the time precedence of event variables could be used to establish such an ordering. Given an ordering as a constraint, there remain 2('2) = 2n(n\u00b71)!2 \"bJ b J\" f poss1 e e 1e -network structures. Let t(n) = An(n\u00b71)/Z F J . . f \"bJ I . 7 \" . or arge n, 1t 1s not eas1 e to app y Equauon for each of t(n) possible structures. Therefore, in addition to a node ordering, let us assume equal priors on B5. That is, initially, before we observe the data D, we believe that all structures arc equally likely. In that case, we obtain\nn q; r;\nP(B s, D) = c f1 f1 (r; - 1)! f1 aiJk! , (9) ; = 1 i = 1 (N;1 + r;- 1)! k = 1\nwhere c = 1/t(n) is our prior probability, P(B5), for each Bs. To maximize Equation 9, it is sufficient to find the parent set of each variable that maximizes the second inner product. Thus, we have\nmax[ P(Bs, D)] = Bs\nn \ufffd \ufffd\nrr [ rr (r;-1)! rr c max a ;Jk! ] , ; = 1 1'i J = 1 (N;1 + r; - 1)! .< = 1\n(10)\nwhere the maximization takes place over every possible set of parents :rr; of x; that is consistent with the ordering on the nodes. A generalization of Equation 10, which is discussed in [Cooper and Herskovits 1991], does not assume that P(Bs) is uniform. Although solving Equation 10 is no longer super-exponential in n, it remains exponential in n. Thus, further computational improvements are needed.\n2.3.2 A Heuristic Search Procedure\nWe propose here one polynomial-time heuristic method, among many possibilities, that attempts to find the Bs that maximizes (or nearly maximizes) P(Bs I D). We shall usc Equation 10 as our starting point, with the attendant assumptions that we have an ordering on the domain variables and that, a priori, all structures arc considered equally likely. We shall modify the maximization operation on the right of Equation 10 to use a greedy search method. In particular, we use an algorithm that begins by assuming that a node has no parents, and that then adds incrementally that parent whose addition most increases the probability of the resulting structure. When the addition of no single parent can increase the probability, we stop adding parents to the node. We shall use the following function:\ng(i, :rr;) q;\n= rr (r;- 1)! rr'' a\u00b7\u00b7 l yk. ' i=1 (NiJ+ r; - 1)! k=1\n(11)\nwhere the CX;jk arc computed rclati vc to :rr; being the parents of x; and relative to a database D which we leave implicit. Let u be the maximum number of parents allowed for any node. In [Cooper and Herskovits 1991] we show that g(i, :rr;) can be computed in O( m u r) time. We also shall use a function Pred(x;) that returns the set of nodes that precede xi in the node ordering. Figure 2 contains the heuristic search algorithm, which we call K2. The algorithm is named K2 because it evolved from a system\n92 Cooper and Herskovits\nnamed Kutat6 [Herskovits and Cooper 1990) that applies the same search heuristics to construct belief networks; Kutat6 uses entropy to score network structures .\nAs shown in [Cooper and Herskovits 1991], the time complexity of K2 is O(m u2 n2 r). This result assumes that the factorials we need in order to apply Equation 11 have been precomputed and stored in an array. We can further improve runtime speed by replacing g(i , rr;) and g(i , n ; u (z}) in K2 by /og(g(i , rr;)) and /og(g(i, n; u (z})), respectively. The logarithmic version of Equation 11 requires only addition and subtraction rather than multiplication and division. If the logarithmic version of Equation 11 is used in K2 then the logarithms of factorials should be precomputed and stored in an array.\n1. procedure K2; 2. (Input: A set of n nodes, an ordering on the\nnodes, an upper bound u on the number of parents a node may have, and a database D containing m cases.}\n(Output: For each node, a printout of the parents of the node.}\n3. for i := 1 to n do 4. 1li := 0; 5. Pold := g(i , rr;); (This function is computed using Equation 11.} 6. OKToProcecd :=true; 7. while OKToProcecd and lrr;l < u do 8. let z be the node in Prcd(x;)- n; that\nWe emphasize that K2 is just one of many possible methods for searching the space of belief networks to maximize the probability metric developed in Section 2.2. Accordingly, the metric developed in Section 2.2 is a more fundamental result than is the K2 algorithm. Nonetheless, K2 has proved valuable as an initial search\nmethod for obtaining some preliminary test results, which we shall describe in Section 3.\n3 PRELIMINARY RESULTS\nIn this section we describe an experiment in which we generated a database from a belief network by simulation, and then attempted to reconstruct the belief network from the database. In particular, we applied the K2 algorithm to a database of 10,000 cases generated from the ALARM belief network. Beinlich constructed the ALARM network as a research prototype to model potential anesthesia problems in the operating room [Beinlich, S ucrmondt, et a!. 1989). ALARM contains 46 arcs and 37 nodes, and each node has from two to five possible values. We generated cases using a Monte Carlo technique [Henrion 1988). Each case corresponds to a value assignment for each of the 37 variables. The Monte Carlo technique is an unbiased generator of cases, in the sense that the probability that a particular case is generated is equal to the probability of the case according to the belief network. We generated 10,000 such cases to create a database that we used as input to the K2 algorithm. We supplied K2 with an ordering on the 37 nodes that is consistent with the partial order of the nodes as specified by ALARM.\nFrom the 10,000 cases, the K2 algorithm constructed a network identical to ALARM, except that one arc was missing and one arc was added. A subsequent analysis revealed that the missing arc is not strongly supported by the 10,000 cases. The extra arc was added due to the greedy nature of the search algorithm. The total search time for the reconstruction was approximately 16 minutes and 38 seconds on a Macintosh II running LightSpccd Pascal version 2.0. We analyzed the performance of K2 when given the first 100, 200, 500, 1000, 2000 and 3000 cases from the same 10,000-case database. Using only 3,000 cases, K2 produced in about 5 minutes the same belief network as when it used the full 10,000 cases.\nAlthough preliminary, these results arc encouraging because they demonstrate that K2 can reconstruct a moderate size belief network rapidly from a set of cases using readily available computer hardware. We currently arc investigating the extent to which the performance of K2 is sensitive to the ordering of the nodes. We also are exploring methods that do not require an ordering.\n4 SUMMARY OF THE LEARNING METHOD AND RELATED WORK\nIn the preceding sections, we have described a Bayesian approach to learning the clepenclcncy relationships among a set of discrete variables. For notational simplicity, we\nA Bayesian Method for Constructing Bayesian Belief Networks from Databases 93\nshall call the approach BLN (.!iayesian learning of belief networks). BLN can represent arbitrary belief-network structures and arbitrary probability distributions on discrete variables. BLN calculates the probability of a structure of variable relationships given a database. The probability of multiple s tructures can be computed and displayed to the user. BLN also can use multiple structures in performing inference, as we discuss in [Cooper and Hcrskovits 1991]. When the number of domain variables is large, the combinatorics of enumerating all possible belief network structures becomes prohibitively expensive. Developing better methods for efficiently locating highly probable structures remains an open area of research. BLN is able to represent the prior probabilities of belief-network structures. For example, an expert could attach a high probability to the presence of an arc from node x to node y, indicating that according to current scientific belief-it is very likely that x directly influences y. More generally, a prior probability could be specified for the presence of a set of arcs. If prior probability distributions on such structures are not available to the computer, then uniform priors can be assumed.\nPreviously described methods for learning belief networks from databases arc non-Bayesian [Chow and Liu 1968, Fung and Crawford 1990, Geiger, Paz, et a!. 1990, Hcrskovits and Cooper 1990, Pearl and Verma 1991, Rcbane and Pearl 1987, Spines and Glymour 1990, Spines, Glymour, et a!. 1990, Srinivas, Russell, et a!. 1990, Verma and Pearl 1990, Wermuth and Lauritzen 1983]. With non-Bayesian methods, there is no principled way to attach prior probabilities to individual arcs or sets of arcs. In addition, all of these methods, except [Hcrskovits and Cooper 1990], rely on having threshold values (e.g., p values) for determining when conditional independence holds among variables. BLN docs not require the usc of such thresholds. Also, non-Bayesian belief network methods, as well as classical statistical methods, emphasize finding the single most likely structure, which they then may usc for inference. They do not, however, quantify the likelihood of that structure. If a single structure is used for inference, implicitly the probability of that structure is assumed to be 1.\nAcknowledgements\nWe thank Lyn Dupre and Clark Glyrnour for helpful comments on an earlier draft. This work was supported in part by the National Science Foundation under grant IRI-8703710 and by the U.S. Army Research Office under grant P-25514-EL. Computing resources were provided in part by the SUMEX-AlM resource under grant LM-05208 from the National Library of Medicine.\nReferences\nAgogino, A.M. and Rege, A., IDES: Influence diagram based expert system, Mathematical Modelling 8 (1987) 227-233.\nAndreassen, S., Woldbye, M., Falck, B. and Andersen, S.K., MUNIN - A causal probabilistic network for interpretation of elcctromyographic findings, In: Proceedings of the International Joint Conference on Artificial Intelligence, Milan, Italy (1987) 366-372.\nBcinlich, I.A., Suermondt, H.J., Chavez, R.M. and Cooper, G.F., The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks, In: Proceedings of the Conference on Artificial Intelligence in Medical Care, London (1989) 247-256.\nChow, C.K. and Liu, C.N., Approximating discrete probability distributions with dependence trees, IEEE Transactions on Information Theory 14 (1968) 462- 467.\nCooper, G.F., Current research directions in the development of expert systems based on belief networks, Applied Stochastic Models and Data Analysis 5 (1989) 39-52.\nCooper, G.F. and Hcrskovits, E.H., A Bayesian method for the induction of probabilistic networks from data, Report SMI-91-1, Section of Medical Informatics, University of Pittsburgh, 1991.\ndeGroot, M.H., Optimal Statistical Decisions (McGraw-Hill, New York, 1970).\nFung, R.M. and Crawford, S.L., Constructor: A system for the induction of probabilistic models, In: Proceedings of AAA!, Boston, Massachusetts (1990) 762-769.\nGeiger, D., Paz, A. and Pearl, J., Learning causal trees from dependence information, In: Proceedings of AAAI, Boston, Massachusetts (1990) 770-776.\nHcckcrman, D.E., Probabilistic Similarity Networks, Ph.D. dissertation, Medical Information Sciences, Stanford University (1990).\nHcckcrman, D.E., Horvitz, E.J. and Nathwani, B.N., Update on the Pathfinder project, In: Proceedings of the Symposium on Computer Applications in Medical Care (1989) 203-207.\nHcnrion, M., Propagating uncertainty in Bayesian networks by logic sampling. In: Lemmer J.F. and\n94 Cooper and Herskovits\nKana! L.N. (Eds.), Uncertainty in Artificial Intelligence 2 (North-Holland, Amsterdam, 1988) 149- 163.\nHenrion, M., An introduction to algorithms for inference in belief nets. In: Henrion M., Shachter R.D., Kana! L.N. and Lemmer J.F. (Eds.), Uncertainty in Artificial Intelligence 5 (North-Holland, Amsterdam, 1990) 129- 138.\nHenrion, M. and Cooley, D.R., An experimental comparison of knowledge engineering for expert systems and for decision analysis, In: Proceedings of AAAJ, Seattle ( 1987) 47 1-476.\nHerskovits, E.H. and Cooper, G.F., Kutat6: An entropy-driven system for the construction of probabilistic expert systems from databases, In: Proceedings of the Conference on Uncertainty in Artificial Intelligence, Cambridge, Massachusetts ( 1990) 54-62.\nHoltzman, S., Intelligent Decision Systems (Addison Wesley, Reading, MA, 1989).\nHorvitz, E.J., Breese, J.S. and Henrion, M., Decision theory in expert systems and artificial intelligence, International Journal of Approximate Reasoning 2 ( 1988) 247-302.\nKiiveri, H., Speed, T.P. and Carlin, J.B., Recursive causal models, Journal of the Australian Mathematical Society 36 ( 1984) 30-52.\nLauritzen, S .L. and Spiegelhaltcr, D.J ., Local computations with probabilities on graphical structures and their application to expert systems, Journal of the Royal Statistical Society (Series B) 50 ( 1988) 157-224.\nNeapolitan, R., Probabilistic Reasoning in Expert Systems (John Wiley & Sons, New York, 1990).\nPearl, J., Fusion, propagation and structuring in belief networks, Artificial Intelligence 29 ( 1986) 241-288.\nPearl, J., Probabilistic Reasoning in Intelligent Systems (Morgan Kaufmann, San Mateo, California, 1988).\nPearl, J. and Verma, T.S., A theory of inferred causality, In : Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning, Boston, MA ( 199 1) 44 1-452.\nRebane, G. and Pearl, J., The recovery of causal poly trees from statistical data, In: Proceedings of the\nWorkshop on Uncertainty in Artificial Intelligence, Seattle, Washington ( 1987) 222-228.\nRobinson, R.W., Counting unlabeled acyclic digraphs (Note : This paper also discusses counting labeled acyclic graphs.), In: Proceedings of the Fifth Australian Conference on Combinatorial Mathematics, Melbourne, Australia ( 1976) 28-43.\nShachter, R.D., Intelligent probabilistic inference. In: Kana! L.N. and Lemmer J.F. (Eds.), Uncertainty in Artificial Intelligence (North-Holland, Amsterdam, 1986) 371-382.\nShachter, R.D., Probabilistic inference and influence diagrams, Operations Research 36 ( 1988) 589-604.\nShachter, R.D. and Kenley, C.R., Gaussian influence diagrams, Management Science 35 ( 1989) 527-550.\nSpirtes, P. and Glymour, C., An algorithm for fast recovery of sparse causal graphs, Report CMU-LCL-904, Department of Philosophy, Carnegie-Mellon University, 1990.\nSpines, P., Glymour, C. and Scheines, R., Causal hypotheses, statistical inference, and automated model specification, Unpublished report, Department of Philosophy, Carnegie-Mellon University, 1990.\nSrinivas, S., Russell, S. and Agogino, A., Automated construction of sparse Bayesian networks for unstructured probabilistic models and domain information. In: Henrion M., Shachter R.D., Kana! L.N. and Lemmer J.F. (Eds.), Uncertainty in Artificial Intelligence 5 (North-Holland, Amsterdam, 1990) 295- 308.\nSuermondt, H.J. and Amylon, M.D., Probabilistic prediction of the outcome of bone-marrow transplantation, In: Proceedings of the Symposium on Computer Applications in Medical Care ( 1989) 208- 2 12.\nVerma, T.S. and Pearl, J., Equivalence and synthesis of causal models, In: Proceedings of the Conference on Uncertainty in Artificial Intelligence, Cam bridge, Massachusetts (1990) 220-227.\nWermuth, N. and Lauritzen, S., Graphical and recursive models for contingency tables, Biometrika 72 ( 1983) 537-552."}], "references": [{"title": "The ALARM monitoring system: A case study with two probabilistic inference techniques for belief", "author": ["I.A. Bcinlich", "H.J. Suermondt", "R.M. Chavez", "G.F. Cooper"], "venue": null, "citeRegEx": "Bcinlich et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Bcinlich et al\\.", "year": 1987}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C.K", "C.N. Liu"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "C.K. and Liu,? \\Q1989\\E", "shortCiteRegEx": "C.K. and Liu", "year": 1989}, {"title": "Current research directions in the development of expert systems based on belief networks, Applied Stochastic Models and Data Analysis", "author": ["G.F. Cooper"], "venue": null, "citeRegEx": "Cooper,? \\Q1968\\E", "shortCiteRegEx": "Cooper", "year": 1968}, {"title": "A Bayesian method for the induction of probabilistic networks from data, Report SMI-91-1, Section of Medical Informatics", "author": ["G.F", "E.H. Hcrskovits"], "venue": "University of Pittsburgh,", "citeRegEx": "G.F. and Hcrskovits,? \\Q1989\\E", "shortCiteRegEx": "G.F. and Hcrskovits", "year": 1989}, {"title": "Optimal Statistical Decisions (McGraw-Hill", "author": ["M.H. deGroot"], "venue": "New York,", "citeRegEx": "deGroot,? \\Q1970\\E", "shortCiteRegEx": "deGroot", "year": 1970}, {"title": "Learning causal trees from dependence information", "author": ["D. Geiger", "A. Paz", "J. Pearl"], "venue": null, "citeRegEx": "Geiger et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Geiger et al\\.", "year": 1990}, {"title": "Hcnrion, M., Propagating uncertainty in Bayesian networks by logic sampling", "author": ["J.F. Lemmer"], "venue": "Proceedings of the Symposium on Computer Applications in Medical Care", "citeRegEx": "Lemmer,? \\Q1989\\E", "shortCiteRegEx": "Lemmer", "year": 1989}, {"title": "Uncertainty in Artificial Intelligence 2 (North-Holland", "author": ["Cooper", "Herskovits Kana! L.N"], "venue": null, "citeRegEx": "Cooper and L.N.,? \\Q1988\\E", "shortCiteRegEx": "Cooper and L.N.", "year": 1988}, {"title": "Counting unlabeled acyclic digraphs", "author": ["R.W. Robinson"], "venue": "Workshop on Uncertainty in Artificial Intelligence,", "citeRegEx": "Robinson,? \\Q1987\\E", "shortCiteRegEx": "Robinson", "year": 1987}, {"title": "Probabilistic inference and influence", "author": ["L.N. Kana", "J.F. Lemmer"], "venue": "Uncertainty in Artificial Intelligence (North-Holland, Amsterdam,", "citeRegEx": "Kana. and Lemmer,? \\Q1986\\E", "shortCiteRegEx": "Kana. and Lemmer", "year": 1986}, {"title": "Gaussian influence diagrams", "author": ["R.D. Shachter", "C.R. Kenley"], "venue": "diagrams, Operations Research", "citeRegEx": "Shachter and Kenley,? \\Q1989\\E", "shortCiteRegEx": "Shachter and Kenley", "year": 1989}, {"title": "Automated construction of sparse Bayesian networks for unstructured probabilistic models and domain information", "author": ["S. Srinivas", "S. Russell", "A. Agogino"], "venue": null, "citeRegEx": "Srinivas et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 1990}, {"title": "Uncertainty in Artificial Intelligence 5 (North-Holland, Amsterdam, 1990) 295308. Suermondt, H.J. and Amylon, M.D., Probabilistic prediction of the outcome of bone-marrow", "author": ["L.N", "J.F. Lemmer"], "venue": null, "citeRegEx": "L.N. and Lemmer,? \\Q1990\\E", "shortCiteRegEx": "L.N. and Lemmer", "year": 1990}, {"title": "Equivalence and synthesis", "author": ["T.S. Verma", "J. Pearl"], "venue": "transplantation,", "citeRegEx": "Verma and Pearl,? \\Q1989\\E", "shortCiteRegEx": "Verma and Pearl", "year": 1989}, {"title": "Graphical and recursive models for contingency tables", "author": ["N. Wermuth", "S. Lauritzen"], "venue": "Proceedings of the Conference on Uncertainty in Artificial Intelligence, Cam bridge,", "citeRegEx": "Wermuth and Lauritzen,? \\Q1990\\E", "shortCiteRegEx": "Wermuth and Lauritzen", "year": 1990}], "referenceMentions": [{"referenceID": 10, "context": "A variable in a Bayesian belief\u00ad network structure may be continuous [Shachter and Kenley 1989] or discrete.", "startOffset": 69, "endOffset": 95}, {"referenceID": 4, "context": "This probability density function is, however, just a special case of the Dirichlet distribution [deGroot 1970].", "startOffset": 97, "endOffset": 111}], "year": 2011, "abstractText": "This paper presents a Bayesian method for constructing Bayesian belief networks from a database of cases. Potential applications include computer-assisted hypothesis testing, automated scientific discovery, and automated construction of probabilistic expert systems. Results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases. We relate the methods in this paper to previous work, and we discuss open problems.", "creator": "pdftk 1.41 - www.pdftk.com"}}}