{"id": "1303.5746", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Structuring Bodies of Evidence", "abstract": "In this editorial we today two interested of workflows their thus evidence, making allow bringing might reduce took complexity of with mission depending stages in several implementation for suggests aesthetics. The three essentially half rectangular followed spatial elements first while body \u2014 certain were come \u03b1. With particular structure nothing rarely chance to continue the complexity at the regression under the spirit responsibilities Bel, Pl, up Q. The been circular proposed here, into Hierarchical Trees, permits us could reduce during dimensions more the calculation of Bel, Pl, some Q, brought well as according been Dempster ' s the this such in hence to following brute - force convolution. Both these building do not benefits took popular of now the subsets significant still reference sh3.", "histories": [["v1", "Wed, 20 Mar 2013 15:33:02 GMT  (334kb)", "http://arxiv.org/abs/1303.5746v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["sandra sandri"], "accepted": false, "id": "1303.5746"}, "pdf": {"name": "1303.5746.pdf", "metadata": {"source": "CRF", "title": "Structuring Bodies of Evidence", "authors": ["Sandra A. Sandri", "Paul Sabatier-IRIT"], "emails": ["@irit"], "sections": [{"heading": null, "text": "1 INTRODUCTION\nEvidence Theory (Shafer 1976) is a well-known frame work for representing uncertainty in Knowledge-based systems. Its use in practical applications is however compromised by the important complexities involved in its manipulation. In particular, the method used to combine the evidence coming from independent sources, known as the Dempster's rule of evidence, may require a complexity of 22\" - 2n+t in the worst case, where n stands for the size of the reference do main. The algorithms proposed in the literature to reduce these complexities impose restrictions either on the pieces of evidence themselves, or on the ref erence domain of the variables modeling them. In the present paper, we are interested to show that some set theoretical properties underlying bodies of evidence, the set of pieces of evidence, can be used to produce algorithms that are efficient in the situations where the data cannot be restricted.\nThe text is divided as follows. Section 2 brings some basic notions on Evidence Theory and discusses some of the algorithms implemented within this framework.\nIn Section 3 we present two ways of structuring bodies of evidence : by partitioning the pieces of evidence on the cardinality relation, and by the use of Hierarchical Trees. In Section 4 we propose an algorithm to im plement the Dempster's rule using Hierarchical Trees, and in Section 5 we briefly discuss the use of Hierar chical Trees in the Local Propagation of Information. Section 6 brings the conclusion."}, {"heading": "2 BASIC NOTIONS IN EVIDENCE THEORY", "text": "In the framework of Evidence Theory, the information supplied by a source about the actual value of a vari able x is encoded in a body of evidence on n, where S1 stands for the set of all the possible values of x, called the frame of discernment of x. A body of evide!tce is characterized by a pair ( :F , m), where :F is a fam ily ot subsets of 11, ie :F c P(S1), and m (called \u2022.he mass assignement function) is a mapping of P(S1) to the unit interval, such that m(A) > 0 iff A E :F, and E{m(A)/A C S1} = 1. Each element A E :F is called a focal element, and m( A) represents the amount of evidence focused strictly in A itself, and not in any subset of A. A body of evidence (:F , m) on S1 can also be represented by means of any one of the three following set-functions on P(S1) :\nBei(A) = 2)m(B)/B c A, B f. 0} (1) PI( A)= 2:)m(B)/ B n A f. 0} (2)\nQ(A) = l:{m(B)fA c B} (3 ) where 0 represents the empty set. The belief func tion riel (also called a credibility function) gathers the piec.ol of information which support A. The plausibil ity function PI gathers the pieces of information whirh do not contradict A. The commonality function repte sents to what extent all the elements composing A are plausible (Dubois and Prade 1991). These measures\nare strongly inter-related ; for instance, the plausibil ity function PI can be calculated from the credibility function Bel using\nBel( A) = 1 - PI( A) (4)\nFast algorithms for the calculation of Bel, PI, and Q can be found in (Kennes 1990), (Kennes and Smets 1990a), (Kennes and Smets 1990b), and (Thoma 1991). These algorithms are based on Moebius trans forms, and are such that each element of P(fl)can be seen as an object that receives and sends information in the form of accumulated evidence. In the calcula tion of any of the belief functions, each element propa gates its accumulated evidence only once. Fig. 1 illus trates the calculation of Bel for all the focal elements in (F, m), on n = {a,b,c}. Dempster's rule of combination is the method used for pooling evidence in the framework of Evidence Theory (Shafer 1976). The combination of two given bodies of evidence (F1 , m1) and (F2 , m2) on !1, yields a third body (F3 , m3) on !1, where\n(5)\nThis result may be unnormalized, i.e. we might have m3(0) > 0. Originally, the application of Dempster's rule involves a normalization step, in which the mass assigned to each focal element of (F3 , m3) is divided by a constant K = 1- m3(0), representing the amount of conflict between the sources. The effective use of this normalization step is however quite controversial (see (Dubois and Prade 1988), (Smets 1988) for dis cussions over this point).\nThe use of ( 5) for the calculation of the Dempster's rule, here called the brute-force strategy, has a com plexity of I F 1 I * I F 2 I in terms of focal elements vis ited, which gets to 22n-2n+1 in the worst case. Drastic reductions on this complexity are however achieved in some restricted situations. Barnett (1981) treated the case in which the evidence is presented in the form of bodies of evidence having two focal elements : a sin gleton f E !1, and its complement in !1. Gordon and\nStructuring Bodies of Evidence 333\nShortliffe (1985), and Shafer and Logan (1987) treated the case in which all the possible evidence is hierarchi cal, in such a way as that all the possible focal elements can be arranged as nodes in a tree, where each father node represents the union of its son-nodes, and all the nodes having a common father are disjoint. Moreover, in this model, all the bodies of evidence consist of only two focal elements : one representing the reference do main !1, and another that is either a node of the tree, or the complement of one of its nodes. The contin uous case has been treated by Strat (1984), with the restriction that all the focal elements should be closed intervals. A general discussion on the complexity of Dempster's rule can be found in (Orponen 1990).\nAnother way of implementing Dempster rule, here called the Q-strategy, consists in calculating the com monality functions Q1 and Q2 for every set A E !1, and then using an important property of the commonality function Q, namely :\n(6)\nThe mass assigment function m3 can then be recovered by using (Smets 1988) :\nm(A)= L{(-1)1BIQ(AUB),BcA) (7) The exact number of operations performed on the Q strategy with the direct utilization of formulae ( 6) and (7) is 3n + 22n+ 1 - 2n+ 1. However, this value comes down to (n + 1)2n + n2n-1 - n, when Moe bius transforms are used (Kennes and Smets 1990a), and (Kennes 1990). The single inconvenience with this strategy is that it requires that he whole set P(O) be generated, being thus unapplicable when P(O) is not enumerable.\n3 PROPOSAL OF STRUCTURES\nWe see that efficient algorithms for the calculation of both belief measures and Dempster rule are achieved with restrictions on either the evidence, or on the frame of discernment. Set-theoretical properties un derlying the bodies of evidence can however be used to produce efficient algorithms without loss of expres sivity.\nOne of the simplest of such properties is that the focal elements contained in f E F are either f itself, or ele ments having cardinality smaller than f. Based on this prop\ufffdrty, we propose to structure a body of evidence with V(F), the partition induced on F, when the focal elements are classified by their cardinality. With this structure we are able to reduce the complexity of the calculation of the belief measures Bel, PI, and Q. An important characteristic of P(O) is that it forms a lattice with the relation C (in particular, it is this\n334 Sandri\nproperty that underlies the use of Moebius Transforms in Evidence Theory). Since F C P(fl), F forms an in complete lattice with C. We present here a structure, here, called Hierarchical Trees, that uses this property in order to reduce the complexity of the implementa tion of Dempster's rule, as well as of Bel, Pl, and Q. This structure establishes an hierarchy in a given body of evidence ; each node f in the tree (representing a focal element A) is connected to a father-node and to a set of sons-nodes, which are respectively greater or smaller than f in the sense of inclusion. Algorithms based on both of the structures proposed here do not require that the whole set P(fl) be generated, and can thus be employed when P(fl) is not enumerable.\n3.1 PARTITION V(F)\nLet (F , m)be a body of evidence on fl, and A and B be two of its focal elements contained in fl. We can structure F with the partition V(F) = {c;, 1 ::; i ::::1 n 1}, where A Cj B, iff I A 1=1 B I= i, ie A and B will belong to the same class if they have the same cardinality. The construction of V(F) is obvi ously linear with I F I, and can be done as the input is read.\nThe credibility function Bel on a set A E P(fl), I A I= j, in a given body of evidence (F , m), can be calcu lated from the partition V corresponding to F using :\nj-1 Bel ( A) = m(A) + I)m(B), BE c;,A:::) B} (8)\ni=l\nBel(A) is calculated by adding its own mass m (A) to the masses of its subsets that can be found lying in the classes below it in V. Each focal element thus vis its itself plus the elements with lower cardinality than itself in V. Thus if I c; I represents the number of ele ments of cardinality i present in partition V, the total cost of the algorithm is I F I + 2:::7=1 2::;;:,\ufffd I c; I Cj I\u00b7 The calculation of the plausibility function Pl(A) is similarly done by using formula ( 4), with the appli cation of (11) in the calculation of Bel( A). The to tal cost of calculating P l for all the elements of F is I F I + 2:::7=1 I;j,;; -\n1 I c; II Cj I\u00b7 The commonality function Q can be calculated using the formula :\nn\nQ(A) = m(A) + I:: {m(B), BE c;,A C B} (9) i=j+1\nleading to a cost of I F I + 2:::7=1 I:j=i+1 I c; I Cj I, when applied to all the elements in F.\n3.2 HIERARCHICAL TREES\nLet the set A (A) ={B IB E F,B:::) A,B =f. A}, be the set of ancestors of A in F, ie the set of proper supersets of A in F. Similarly, let D(A) = {B I B E F,A:::) B,B =f. A,B =f. 0}, be the set of de scendants of A in F, ie the set of all proper sub sets of A (except the empty set) in F. A Hierarchi cal Tree is a structure that relates the focal elements in F with their respective ancestors and descendants sets. Let J; be a node representing a set A; E P(fl). A Hierarchical Tree T = (N, E) on the body of ev idence (F , m). consists of a set of distinct nodes N = {!; I A; E F} and a set of edges E = {(f;,Jj) I f;,fi E N, f; =f. fi,A; :::) Aj, \ufffdA; :::) Ak :::) Aj} \u00b7 Set R = {!; I \ufffd, (!k, f;) E E} denotes the roots ofT, and Sons(!) = {!; I (!, J;) E E} denotes the set of sons of a node f in the Hierarchical Tree T. A Hierarchical Tree may in fact have several roots, thus constituting a forest. Note however that any forest can be trans formed into a tree with the addition of a dummy root node u, with m( u) = 0, where tt represents the union of the focal elements in F. Throughout this paper we suppose that R has a single element. To simplify the notation we make N = F, and f; =A;. Hierarchical Trees on ( F , m) can be seen as the struc tures that can be derived from the incomplete lat tice induced on F with the relation C when we ex tract edges in the lattice, in such a way that each node will have at most one father. Thus, from the same set of focal elements F several Hierarchical trees Tk = (Fk , m k) may be derived. They are equiv alent for our purposes, in the sense that if a focal elemmt A is a node f in Tk, then all the nodes in the 1 ath linking f to a root node belong to A( A), and all the nodes accessible from f belong to D(A). Moreover, the father of f in Tk refers to a focal ele ment with the smallest cardinality among the nodes in A ( A). Fig. 2 shows some Hierarchical trees derivable from F = P(fl),fl = {a,b,c,d}. Note that the trees described in both (Gordon and Shortliffe 1985), and (Shafer and Logan 1987) are particular cases of Hier archical Trees, in which all the nodes in a given level are disjoint focal elements.\n3.2.1 Hierarchical Trees Construction\nIn order to construct a Hierarchical Tree for a body of evidence ( F , m), we take partition V( F) and create to each focal element A with cardinality i, a node f;J in T. We start the process of identification of the father of 1;1 by examining the nodes on class c ;+l\u00b7 If there is no possible father on class c ;+1, then fiJ is compared with the focal elements on class c ;+2, and so succes sively. If no father can be found, then fiJ is set as a root node. Fig. 3 illustrates the application of this al gorithm for F = {abcd,abc,abd,ab,ac,bc,a,b,c,d}). (Fig 3-a shows partition V(F)).\nThe tree construction in Fig. 3 requires the exami nation of 13 nodes, and the complete tree on P(ll), with n = {a, b, c, d}, would require examining 22 nodes. The exact number of the nodes visited in order to construct the worst possible tree with I n I ele ments is (Sandri and Dugat 1991) : 2n + 2n-3- 2 + L\ufffd;} ( i \ufffd 2 ) 2n - i- 1 .\n3.2.2 Calculation of Belief Measures\nAn important characteristic of Hierarchical Trees is that a son-node always \"inherits\" the ancestors of its father- node. This property can be ad vantegeously used in the calculation of the Q function, by compar ing a Hierarchical Tree on F with the partition V(F), as it is seen in the algorithm presented below.\nStructuring Bodies of Evidence 335\nLet f be a node in the tree T, Q(f) be the common ality function of f, and Sons(!) and Father(!) re spectively be the immediate sons and the father of f in T. We compute Q recursively for all the nodes in tree T with the application of the following algorithm (initially f = r, and Q(Father(r)) = 0:\nIf node f (of cardinality i) has at least one son, it is compared to each element fii of its own class in V. Otherwise, it is only com pared to itself. If f n fiJ =f. 0 , we update the masses in V by transfering the mass on f;J to f n f;j' by making m(f n fij) <-- m(f n fij) + m(f;j ), and m(/;J) <-- 0, if f n f;J =f. fiJ. Q(f) is computed as Q(Father(f)) + m(f), and the algorithm is successively repeated for all the nodes in S ons(f).\nThe comparisons that f effectuates in its own level have the sole objective of transfering the masses of ele ments that might be ancestors of its sons, to subsets of f (note that if g E Sons(!), and/ :::J g, then(fnl) :::J g). Each node visits at least itself, and when it has sons, it visits also all of its class neighbours. The max imal number of nodes visited by this algorithm is 2n-l - n + \ufffd ( 2\ufffdl ) , that is closely bounded by 2n-l - n + 2,fo;-' (Sandri and Dugat 1991). For in stance, the cost of calculating Q for all F = P(O) with I n I= 5 is 961 with the usual algorithm, 3 86 us ing partition V, and 211 using a Hierarchical Tree : 74 for the tree construction and 13 7 for the Q algorithm itself (the approximation gives 141 instead of 13 7 for the Q algorithm).\nBel and PI can be calculated from the commonality function associated with the complement of the body of evidence, as exposed in (Dubois and Prade 1986). The complement of a body of evidence (F , m) is (\ufffdF , m), defined as VA c n,m(A) = m(A), so that \ufffdF = {A/A E F}. Function Q is defined as:\nQ(A) = 2:::{m(B) /A c B} (10)\nThe Bel function is then calculated using\nBel(A) = Q(A)- m(0) (11)\nFinally, function PI is calculated using (10), (11), and ( 4). In the worst case, the calculation of Bel and PI using Hierarchical Trees requires, besides the nodes visited in the calculation of Q, the visit of additional 2n - 1 nodes, 1 n 1 = n, due to the derivation of \ufffdr.\n336 Sandri\n4 DEMPSTER RULE OF\nCOMBINATION\nShafer (1987) comments that an exponential complex ity seems to be intrinsic to Dempster's rule.lndeed, the worst case complexity has to be calculated on IP(fl)!, since it represents the largest value that I :F I may take. However, using the Q-strategy, the complexity does not decrease when I :F 1<<1 'P(fl) I, i.e. we are al ways in a position of the worst case analysis no matter how the evidence is presented. This situation occurs because this strategy requires the generation of the whole set 'P(fl), and thus deals with more than just the sets :F 1, :F 2 and :F 3 involved in the process. On the other hand, efficient algorithms for the implemen tation of the brute-force strategy impose restrictions on the evidence. When we recall that bodies of evi dence benefit from set-theory properties, it seems nat ural that there should exist ways of calculating Demp ster's rule whose complexity in the mean case depends exclusively on I :F 1 I and I :F 2 I, and that restricts neither the evidence, nor the frame of discernment.\nWe propose here an algorithm for calculating Demp ster's rule using the brute-force strategy that takes advantage of the set-theoretical properties underlying two given bodies of evidence. We divide the process in two phases, the pre-processing phase, and the com bination phase in itself.\nPre-Processing Phase\nLet u1 and u2 respectively be the union of the focal elements of :F 1 and :F 2. It is obvious that the highest possible focal element of the resulting body of evidence (:F3 , m3) is U3 = u1 n u2. We can thus reduce any of the bodies :F k by comparing u3 to each of its focal elements Ii , and transporting the mass on Ii to Ii n u3, ie making m(f;i n u3) <- m(/;i n u3) + m(f;i) and m(f;i) <- 0, if Ii nu3 c::f Ii \u00b7 Note that :Fk remains of the same size in two cases: a) l;inu3 is not an element of :F k, and then Ii n u3 will be created as Ii will be eliminated (thus modifying :Fk but not its size), or b) /;j = Ii n U3, and then :F k is not modified. On the other hand, When lij # /;j n U3, and lij n U3 belongs already to :F k, the element Ii is simply eliminated, thus reducing the size of :F k. In the pre-processing phase we compare U3 only to the smallest body of evidence between :F1 and :F2 (the other body of evidence will be implicitly compared to u3 in the next phase). Thus, if :F1 is the smallest body of evidence, the cost of the pre-processing phase is 1 if u1 = u3, and I :F 1 I otherwise.\nCombination Phase\nLet us suppose that :F1 is the smallest body of evi dence, and T1 the Hierarchical Tree for :F 1 resulting\nfrom the construction algorithm as seen in Section 2. Let I be a node in T1, Father(!) the father-node of f in T1, and :F2(Father(f)) be the result of the in tersection of Father(!) with all the elements of :F 2. We will combine each node in T1 to the focal elements of :F 2 by applying the following algorithm, taking the root node as the initial value of I and :F2(Father(r)) as the set :F 2 itself :\nInitially :F2(!) = 0. We compare f to each focal element gin :F2(Father(f)). If [ng # 0, we update m2(! n g) in :F2(!) with m2(g), end m3(fng) with m1(f)\u2022m2(g). Then we successively apply the algorithm for the son nodes of I in T1 with :F 2(1) as input.\nFig. 3 illustrates the combination of node f =abc with :F2(Father(abc)) = {abcd,abc,abd,bcd,ab,bd,a,b}. Note that the set of elements :F2(Fa.ther(f)) exam ined by any node I in T1 is at most the power set 'P(Fa.ther(f)) (the root node examines at most 'P(fl)). The maximal cost of the combination phase is 2m + 2 * 3n - 3 * 2n - 1, with n =I u1 I and m =I U2 I (Sandri and Dugat 1991). This algorithm turns out to be more efficient the largest is I :F 1 I x I :F 2 1- For instance, two complete bodies of evidence on n,\nwith I 0 I= 5, will require visiting 496 nodes (74 for the tree construction, 1 for the pre-processing phase, and 421 for the combination phase), instead of 961 of the brute-force algorithm. This strategy is spe cially recommended if, after the pre-processing phase, the maximal cost of constructing a Hierarchical Tree and then of combining it, is found to be greater than I F1 I X I F2 I\u00b7 Nevertheless, experimental results show that this strategy is worse than the brute-force algorithm only when T1 is composed exclusively of a root node r and its immediate sons Sons( r). In this case, all the nodes in T1 visit all the focal elements in F 2, and thus the additional cost of the tree construc tion is not justified.\n5 LOCAL PROPAGATION OF\nINFORMATION\nKnowledge Bases are usually composed of individual ized pieces of data, each of which regarding a cluster of variables, that together model the system knowl edge about the world. When uncertainty is modeled within the framework of Evidence Theory, these pieces of data can be characterized by bodies of evidence on Ox, where X= {x1,x2, ... ,xn} denotes the set of all the variables in the Knowledge Base. Let G C X be a set of variables in X, Oc = 0,0,,0> .. . xa\u2022 = ox a' X Oxo2 X ... X Ox a\u2022, XGi E X. Each focal element of a body of evidence ( F G , me) on Oc, is then a set of g-uples on oxa' X Oxa2 X ... X o,o\u2022\u00b7 The exten sion and projection of a body of evidence (F , m )are done with the application of the usual set-theoretical extension and projection operations on the set F, and on the additive function m. (Fe l1 , me ll) and (Fe lH , me lH) respectively denote the exten sion and projection of (Fe , me) to a higher and lower dimensional space, H C G C J C X.\nThe belief function for a multi-dimensional variable S C X, taking into account all the evidence in the Knowledge Base, can be obtained by first calculating the overall belief function on ;t', and then marginaliz ing this overall belief function to S. The overall be lief function can be obtained by taking all the clusters of variables G C X on Oc present in the Knowledge Base, extending each of them to the highest possible frame of discernment Ox = Ox! X 0,2 X .. . x Oxn , and then applying Dempster rule on the resulting set of ex tended clusters. The cost of the computation of such a procedure is however prohibitive. An alternative is to use the Local Propagation of Information (Shafer and Shenoy 1986), (Shafer and Shenoy 1988a) and (Shafer and Shenoy, 1988b ) . This strategy requires that the Knowledge Base be partitioned in groups of pieces of knowledge, in such a way as that the variables involved in each group can be arranged as nodes in a Markov Tree (also called a Join Tree in recent literature). In this tree, the nodes are clusters of variables, and the\nStructuring Bodies of Evidence 337\nFigure 5: Markov Tree on X= {x1,x2, ... ,x9}\nedges are such that, if a variable is contained in two nodes, then it is contained in all the nodes along the path between these two nodes (see Fig. 5).\nThe process of Local Propagation of Information on a Markov Tree consists on propagating information from the leaf-nodes until the root node, by means of projec tion/ extension/ combination operations. To obtain the marginal on S C X from the overall belief func tion, it suffices to propagate information locally on a Markov Tree, setting as root one of the nodes contain ing S.\nWhen a body of evidence (Fe , me) on Oc is ex tended to a frame of discernment of higher dimension OJ, J C X, G C J, it undergoes only a minor change ment ; the focal elements change (a focal element f be comes f x OJ-G), but the inter-relationships between the focal elements remain the same. Thus there ex ists a Hierarchical Tree derivable from (Fe l1 , me 11) whose nodes and edges have a one-to-one correspon dance with those in the Hierarchical Tree T con structed for (Fe , me). This tree is in fact the tree that we obtain by changing the label on each node in T with its extension on OJ. However, in case of pr<r jection of a body of evidence (Fe , me) on Oe to a franw of discernment of lower dimension OH, not only the focal elements will change but also the structure may change to a simpler one, since many focal elements on 0\" may be projected on the same focal element on Olf. As a consequence, only changing the labels of the focal elements of T does not suffice to generate a Hinarchical Tree on (Fe lH , me lHJ- However, we obtain a Hierarchical Tree TH on (Fe l , me lH), di rectly from T if the projection operation on (Fe , me) is performed in the following manner. Let f be a node in Fa, Father(!) the father-node off, and J!H the projection of f on OH. We start the projection pr<r cess on the root node, towards the leaf-nodes. Ev erytime a node J!H is created in TH, we take as its father the node Father(J)!H that represents the pr<r jection of the father off on O H, i.e. we create an edge (Father(J)!H, J!H) in Tlf. At the end of the process we obtain a Hierarchical Tree TH, that represents the\n338 Sandri\nprojection ofT on QH. More details on the manipula tion of Hierarchical Trees in the Local Propagation of Information can be found in (Sandri Dugat 1991).\nThoma (1991) exposes the way Moebius Transforms can be efficiently employed in the application of the Q-strategy in the Local Propagation of Information. As in the case of a single variable, the choice between the brute-force or the Q-strategy for the combination of the information on a node G, depends on whether P(QG) is enumerable or not. If there exists a node G in the Markov Tree, such that P(QG) is not enumerable then Q-strategy can be used until the process reaches G ; afterwards only the brute-force strategy can be used in the remainder of the propagation process.\n6 CONCLUSION\nThe choice between the brute-force and the Q-strategy should be guided by the relation between I P(Q) I and I F1 I* I F2 I, when P(Q) can be enumerated. In the case where the use of the brute-force strategy is obliga tory, and I F 1 I * I F 2 I is rather large, structuring the bodies of evidence is the only possible way of reduc ing the computational load, without the imposition of restrictions on the data.\nIn this article we presented two ways of structuring a body of evidence ; by partitioning its focal elements by their cardinality, and by constructing Hierarchical Trees, that allows us to take into account the set inter relationships existing among the focal elements in a body of evidence. We also presented algorithms for calculating belief measures and the Dempster's rule of combination, that do not require the creation of the whole set of possible focal elements P(Q). These struc tures do not impose any restrictions on the data, and can be easily manipulated in the Local Propagation of Information.\nAcknowledgements\nThe author is mostly indebted to Vincent Dugat for the help on the calculation of the complexities involved in this work.\nReferences\nBarnett J. A. (1981) \"Computational Methods For a Mathematical Theory of Evidence\", Proc. 7th IJCAI, 868-875.\nDubois D. and Prade H. ( 1986) \"A Set-Theoretic View of Belief Functions\", Int. J. General Systems, 12, 193- 226.\nDubois D. and Prade H. (1988) \"Representation and Combination of Uncertainty with Belief Functions and Possibility Measures\", Computational Intelligence, 4, 244-264.\nDubois D. and Prade H. (1991) \"Fuzzy Rules in Knowledge-Based Systems\", An Introduction of Fuzzy Logic Applications in Intelligent Systems, (Yager R. R., Zadeh L. A., eds), Kluwer Academic Publishers,to appear.\nGordon J. and Shortliffe E.H. (1985) \"A Method for Managing Evidential reasoning\", Artificial Intel ligence,26, 323-358.\nKennes R. ( 1990) \"Computational Aspects of the Moe bius Transform of a Graph\", Technical Report IRIDIA N\" TR/IRIDIA/90-13, Universite Libre de Bruxelles, IRIDIA, Brussels.\nKennes R. and Smets P. (1990a) \"Fast Algorithms for Dempster-Shafer Theory\", Proc. 3rd IPMU, Paris, July 2-6, 99-101.\nKennes R. and Smets P. (1990a) \"Computational As pects of the Moebius Transform\", Proc. 6th Conf on Uncertainty in Artificial Intelligence, Cambridge, Mass., 344-351.\nOrponen P. (1990) \"Dempster's rule of Combination is #?-complete\", Artificial Intelligence,44, 245-253.\nSandri S. and Dugat V. (1991) \"Hierarchical Trees\", Tech. Report IRIT, Universite Paul Sabatier, Toulouse, to appear.\nShafer G. (1976) A Mathematical Theory of Evidence. Princeton University Press.\nShafer G. and Logan (1987) \"Implementing Demp ster's rule for Hierarchical Evidence\", Artificial Intel ligence,33, 271-298.\nShafer G. and Shenoy P. P. (1986) \"Propagating Be lief Functions with Local Computation\", IEEE Expert, 1(3), 43-51.\nShafer G. and Shenoy P. P. (1988a), Bayesian and Belief-Function Propagation, Working Paper N\u00b0 192, School of Business, The University of Kansas, Lawrence.\nShafer G. and Shenoy P. P. (1988b) Local Computa tion in Hypertrees. Working Paper N\u00b0 201, School of Business, The University of Kansas, Lawrence.\nSmets P. (1988) \"Belief Functions\", In Non Standard Logics for Automated Reasoning, (Smets P., Mamdani A., Dubois D. and Prade H., eds), Academic Press, 253-?86.\nStrat T. M. (1984), \"Continuous Belief Functions for Evidential Reasoning\", Proc. AAAI, 308-313.\nThoma (1991), \"Belief Function Computation\", In Conditional Logic in Expert Systems, (Goodman I. R., Gupta M. M., Nguyen H. T., Rogers G. S. eds), North Holland, 269-308."}], "references": [{"title": "Computational Methods For a Mathematical Theory of Evidence", "author": ["A. Barnett J"], "venue": "Proc. 7th IJCAI,", "citeRegEx": "J.,? \\Q1981\\E", "shortCiteRegEx": "J.", "year": 1981}, {"title": "Representation and Combination of Uncertainty with Belief Functions and Possibility Measures", "author": ["D. Dubois", "H. Prade"], "venue": "Computational Intelligence,", "citeRegEx": "193226", "shortCiteRegEx": "193226", "year": 1988}, {"title": "Fuzzy Rules in Knowledge-Based Systems", "author": ["D. Dubois", "H. Prade"], "venue": "An Introduction of Fuzzy Logic Applications in Intelligent Systems,", "citeRegEx": "Dubois and Prade,? \\Q1991\\E", "shortCiteRegEx": "Dubois and Prade", "year": 1991}, {"title": "A Method for Managing Evidential reasoning", "author": ["R. Yager R", "A. Zadeh L"], "venue": null, "citeRegEx": "R. and L.,? \\Q1985\\E", "shortCiteRegEx": "R. and L.", "year": 1985}, {"title": "1990a) \"Computational As\u00ad pects of the Moebius Transform", "author": ["R. Kennes", "P. Smets"], "venue": "Proc. 6th Conf on Uncertainty in Artificial Intelligence,", "citeRegEx": "Kennes and Smets,? \\Q1990\\E", "shortCiteRegEx": "Kennes and Smets", "year": 1990}, {"title": "Dempster's rule of Combination is #?-complete", "author": ["Cambridge", "Mass", "344-351. Orponen P"], "venue": "Artificial Intelligence,44,", "citeRegEx": "Cambridge et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Cambridge et al\\.", "year": 1990}, {"title": "A Mathematical Theory of Evidence", "author": ["G. Shafer"], "venue": null, "citeRegEx": "Shafer,? \\Q1976\\E", "shortCiteRegEx": "Shafer", "year": 1976}, {"title": "Implementing Demp\u00ad ster's rule for Hierarchical Evidence", "author": ["Shafer G", "Logan"], "venue": "Artificial Intel\u00ad", "citeRegEx": "G. and Logan,? \\Q1987\\E", "shortCiteRegEx": "G. and Logan", "year": 1987}, {"title": "Propagating Be\u00ad lief Functions with Local Computation", "author": ["G. Shafer", "P. Shenoy P"], "venue": "IEEE Expert,", "citeRegEx": "Shafer and P.,? \\Q1986\\E", "shortCiteRegEx": "Shafer and P.", "year": 1986}, {"title": "eds), Academic Press, 253-?86", "author": ["P. Smets", "A. Mamdani", "D. Dubois", "H. Prade"], "venue": "Strat T. M", "citeRegEx": "Smets et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Smets et al\\.", "year": 1984}], "referenceMentions": [{"referenceID": 6, "context": "Evidence Theory (Shafer 1976) is a well-known frame\u00ad work for representing uncertainty in Knowledge-based systems.", "startOffset": 16, "endOffset": 29}, {"referenceID": 2, "context": "The commonality function repte\u00ad sents to what extent all the elements composing A are plausible (Dubois and Prade 1991).", "startOffset": 96, "endOffset": 119}, {"referenceID": 6, "context": "Dempster's rule of combination is the method used for pooling evidence in the framework of Evidence Theory (Shafer 1976).", "startOffset": 107, "endOffset": 120}, {"referenceID": 6, "context": "Shortliffe (1985), and Shafer and Logan (1987) treated the case in which all the possible evidence is hierarchi\u00ad cal, in such a way as that all the possible focal elements can be arranged as nodes in a tree, where each father\u00ad node represents the union of its son-nodes, and all the nodes having a common father are disjoint.", "startOffset": 23, "endOffset": 47}, {"referenceID": 6, "context": "Shortliffe (1985), and Shafer and Logan (1987) treated the case in which all the possible evidence is hierarchi\u00ad cal, in such a way as that all the possible focal elements can be arranged as nodes in a tree, where each father\u00ad node represents the union of its son-nodes, and all the nodes having a common father are disjoint. Moreover, in this model, all the bodies of evidence consist of only two focal elements : one representing the reference do\u00ad main !1, and another that is either a node of the tree, or the complement of one of its nodes. The contin\u00ad uous case has been treated by Strat (1984), with the restriction that all the focal elements should be closed intervals.", "startOffset": 23, "endOffset": 600}], "year": 2011, "abstractText": "In this article we present two ways of struc\u00ad turing bodies of evidence, which allow us to reduce the complexity of the operations usu\u00ad ally performed in the framework of evidence theory. The first structure just partitions the focal elements in a body of evidence by their cardinality. With this structure we are able to reduce the complexity on the calculation of the belief functions Bel, PI, and Q. The other structure proposed here, the Hierarchi\u00ad cal Trees, permits us to reduce the complex\u00ad ity of the calculation of Bel, PI, and Q, as well as of the Dempster's rule of combination in relation to the brute-force algorithm. Both these structures do not require the generation of all the subsets of the reference domain.", "creator": "pdftk 1.41 - www.pdftk.com"}}}