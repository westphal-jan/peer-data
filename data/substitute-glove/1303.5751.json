{"id": "1303.5751", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Algorithms for Irrelevance-Based Partial MAPs", "abstract": "Irrelevance - development temporary MAPs been relevant constructs for discrete - active explanation using belief simultaneously. We look at other definitions those several requires MAPs, though truly important properties adding number highly over intricate mathematical having computing them enabling. We supposed for created these industrial in constructs rather standard MAP best - years exponential, those more to handle inexorably - according pending MAPs.", "histories": [["v1", "Wed, 20 Mar 2013 15:33:28 GMT  (315kb)", "http://arxiv.org/abs/1303.5751v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["solomon eyal shimony"], "accepted": false, "id": "1303.5751"}, "pdf": {"name": "1303.5751.pdf", "metadata": {"source": "CRF", "title": "Algorithms for Irrelevance-Based Partial MAPs", "authors": ["Solomon E. Shimony"], "emails": ["ses@cs.brown.edu"], "sections": [{"heading": null, "text": "1 INTRODUCTION\nProbabilistic explanation, finding causes for observed facts (or evidence), is an extremely important aspect of Artificial Intelligence in general, and probabilistic reasoning in particular. For example, [Charniak and Goldman, 1988], views the understanding of stories as finding high probability facts given the evidence as an explanation of the natural language input text. In automated medical dia\ufffdnosis (for example the work of [Cooper, 1984], and [Peng and Reggia, 1987]), one wants to find the disease or set of diseases of highest probability given the observed symptoms. In vision processing, recent research formulates the problem in terms of finding some set of objects that have the high est probability given the evidence (the image).\nThere is, however, no agreement on what should be maximized in finding a good explanation. In fact, Poole discussed six different schemes of probabilistic explanation in [Poole and Provan, 1990], and even these are not exhaustive. One of the schemes discussed was Maximum A-Posteriori models (MAP), which was presented in [Pearl, 1988] by the name of Maximum Probability Explanation (MPE). A MAP is a maxi mum probability (given the evidence) assignment to all the variables. We call such assignments complete MAPs, as opposed to partial MAPs which are max imum probability assignments to some of the vari ables. MAPs are useful for finding best globally con-\nsistent explanations, as argued by Pearl. In [Char niak and Shimony, 1990], we showed that MAPs are useful for explanation by demonstrating that MAP ex planations are equivalent to complete assignment cost based abduction. Cost based abduction is a variant of Hobbs' and Stickel's weighted abduction (see [Hobbs and Stickel, 1988]), which they used for natural lan guage story understanding.\nIn the rest of this section, we will present the essence of earlier papers: [Shimony and Charniak, 1990] (an algorithm for complete MAPs), and [Shimony, 1991] (definitions of irrelevance-based assignments). The fol lowing sections will deal with how we modify the al gorithm for complete MAPs to handle partial MAPs. We assume here, as well as in related papers, that the world knowledge is represented as a belief network (Bayesian network).\nIn [Shimony, 1991], we proposed a new, domain independent method of highest likelihood explanations called irrelevance-based (partial) MAPs. The idea is that the standard MAP solution, that of finding the most-probable complete model given the evidence, suf fers from the over specification problem (an instance of which appears in [Pearl, 1988], and [Shimony, 1991]). Our solution is a generalization of Pearl's idea of \"cir cumscribing explanations\". Pearl claimed that there is no need to consider the assignment to nodes which have no evidence coming in from below (evidential sup port).\nIn many cases the evidential support is insufficient as a criterion for deciding which nodes are irrele vant, as shown in [Shimony, 1991]. In that paper, we defined irrelevance-based assignments as assignments where every unassigned node is irrelevant. We then defined our notion of explanation, irrelevance-based MAP, as the irrelevance-based partial assignment of highest probability (given the evidence).\n1.1 IRREL EVANCE-BASED ASSIGNMENTS\nWe proceeded to give irrelevance a more formal footing, using statistical independence as a crite rion for irrelevance. There were two such definitions of irrelevance-based assignments: independence-based partial assignments, and a-independence based partial assignments; the second being a more general concept that was introduced because independence-based as signments were too restrictive and captured the intu itive meaning of irrelevance only in special cases.\nWe introduced the notion of independence given a (partial) assignment1 As. We use the notation In( a, biAs) to mean that a and b are independent given an assignment As, where a and bare either as signments (assignments are used interchangeably with sample-space events), or sets of nodes. Equivalently, we can say that P(aiAs) = P(alb,As). The latter constraint actually is a set of simple constraints, one for each possible assignments to the nodes of a and b. This is similar to Pearl's notation of I( a, S, b) stating that a and b are independent given S, where a, S, bare sets of variables. The difference is that the latter im plies the former, but not vice versa. That is because our notion only states that independence occurs given a particular assignment to S, whereas Pearl's notion states that independence occurs given any assignment to S. That is, I( a, S, b) stands for a set-wise larger set of constraints than In( a, biAs). An assignment can be seen as a set of pairs, where a pair ( v, V) means that node v is assigned the value V. The function nodes( As) evaluates to the set of nodes assigned (in this case S). We say that assignment A subsumes assignment B iff A s;; B. The evidence \u00a3 is assumed to be an assignment. We say that an assign ment As is evidentially supported by \u00a3 iff every node v E S is either an evidence node or there exists a path form v to some evidence node. Likewise, As is prop erly evidentially supported by \u00a3 iff every node v E S is either an evidence node or there exists a path form v to an evidence node that traverses only nodes in S.\nOur definition of irrelevance-based assignments relies on the directionality of belief networks, the \"cause and effect\" directionality. The potential causes of a node v are its parents, j ( v ), and we do not assign (i.e. are not interested in) variables that are irrelevant to the evi dence given the causes. We defined our first notion of an irrelevance-based partial assignment formally (and called it independence-based partial assignment):\nDefinition 1 An assignment As is an independence based assignment (IE assignment for short) iff for ev-\n1 The superscript over the assignment symbol denotes the set of variables assigned by A. If the assignment assigns values to all the nodes of S, we say that A is complete w.r.t. s.\nAlgorithms for Irrelevance-Based Partial MAPs 371\nery node v E S, A{\u2022} is independent of all its ancestors that are not inS, given ASi(v). 2\nIf v is independent of its unassigned parents, as in def inition 1, we say that the IB constraint holds at v. The idea behind this definition is that the unassigned nodes above each assigned node v should remain unas signed if they cannot affect v (and cannot be used to explain v). Nodes that are not above v are never used as an explanation of v anyway, as we stated implicitly earlier.\nDefinition 2 An IE assignment As is an indepen dence based MAP (IE-MAP) w.r.t. to evidence \u00a3 iff As is evidentially supported and subsumed by \u00a31 and there is no other IE assignment assignment eviden tially supported and subsumed by \u00a3 of greater probabil ity given the evidence.\nClearly, since As is subsumed by\u00a3, then P(\u00a31As) = 1, whenever P(As) f. 0. In fact, we are only interested in MAPs that are maximal w.r.t. subsumption, because they assign fewer variables and thus lead to \"simpler\" explanations. This distinction is immaterial if the dis tribution of the belief network is strictly positive, be cause then if assignment A subsumes B (with A f. B) then it also has a strictly greater probability.\nWe need to maximize P(Asl\u00a3), the posterior proba bility. Using Bayes rule, we can write:\n(1)\nSince the denominator P(\u00a31As) is 1 and P(\u00a3) is a constant for all the assignments we are comparing, it is sufficient to maximize the prior probability As, which is much easier to compute (see section 2).\nDefinition 2 handles the case where the assigned vari ables are ezactly statistically independent of the unas signed variables. There remained the problem that modifying the conditional probabilities very slightly would have a major effect on the solution. In fact, if the correlation factor between effects and poten tial causes is nearly 0, we would also want to con clude that these potential causes are irrelevant. In order to achieve that, we relax the exact indepen dence constraint by requiring that the equality hold only within a factor of a. That is (evaluating over all possible assignments for a set of variables), if the max imum conditional probability is within a factor of a of the minimum conditional probability, then we have a independence. Formally:\n2We use j ( v) to denote the set of immediate predeces sors of v. We omit the set-intersection operator between sets whenever unambiguous, thus S j ( v) is the intersection of S with the immediate predecessors of v.\n372 Shimony\nDefinition 3 We say that a is a-independent of b given As, where a, b and S are sets of variables (writ ten In0(a, biAs) for short}, iff\nminP(AaiAS I A b) 2': (1- a) maxP(AaiAS I A b) (2) A' A'\nWe expand the definition to include the case of a be ing a (possibly partial) assignment rather than a set of variables, by substituting a for Aa in the above defi nition. Likewise for the case of b being an assignment. This definition is parametric, i.e. a can vary between 0 and 1. We define a a-independent based assignment as an assignment where each node is a-independent of its unassigned ancestors given its assigned parents. Formally:\nDefinition 4 An assignment As is a-independence based iff for every v E s, In,(A{\u2022} I r+ (v)-SIAST'\u2022l).3\nThe case of a = 0 reduces to the independence-based assignment criterion. A a-independence based MAP is defined in the same way as independence-based MAPs, using I n0 in place of In.\n1.2 BEST-FIRST MAP ALGORITHM\nWe presented an algorithm for finding complete (rather than partial) MAP assignments to belief net works in [Shimony and Charniak, 1990]. The algo rithm finds MAP assignments in linear time for belief networks that are polytrees (when appropriate book keeping, not discussed here, is used). The algorithm is potentially exponential time in the general case, as the problem is provably NP-hard.\nAn agenda of states is kept (or assignments), sorted by current probability, which is a product of all con ditional probabilities seen in the current expansion. The operation of the algorithm is shown in the figure 1. An agenda item is complete iff all the variables are assigned. Expansion consists of selecting a fringe node (i.e. a node that has unassigned neighbors) and creat ing a new agenda item for each of the possible assign ments to neighboring nodes. The heuristic evaluation function for an agenda item, which is an assignment As to the set of nodes S, is the following product:\nH(As) = IT P(A{\u2022}IAT(\u2022l) (3) vEG(S)\nwhere G(S) = {v lv E S /\\ Vw E j (v), w E 5}, i.e. the product is over all assigned nodes which have all their parents assigned as well. Clearly the evaluation function is precise for complete assignments, as the product reduces to exactly the joint distribution of the network in that case. H is also optimistic, because if\n3We user+ to denote the non-reflexive transitive closure ofi. Thus i+ (v) is the set of ancestors ofv.\nsome nodes are not assigned, it essentially assumes that their probability is 1. Thus, the evaluation func tion H is heuristically admissible. The advantage of this best-first algorithm is that it can be easily modified to produce the next-best complete assignments in order of decreasing probability. This is done in the following manner (see figure 1): instead of ending with the first complete assignment, output it , and simply continue to loop (getting the next agenda item).\nIn the following sections, we discuss properties of independence-based and a-independence-based partial assignments that allow us to use essentially the same algorithm (with only local modifications) to compute them. We then present the modifications required to produce the IB-MAP algorithm. A formal specifica tion of the IB-MAP algorithm and a proof of its cor rectness follows. We conclude with suggestions of how to modify the algorithm to find a-independence based MAPs.\n2 IB-MAP ALGORITHM\nWe begin by informally introducing the changes re quired to convert the complete MAP algorithm to an IB-MAP algorithm. We then proceed to define the terms and the algorithm formally, and prove its cor rectness.\n2.1 ALGORITHM MODIFICATIONS\nThe algorithm modifications needed to compute the independence-based partial MAP are in checking whether an agenda item is complete, and in the ex pansion of an agenda item. Completeness checking in the modified algorithm is different in that an agenda item may be complete even if not all variables are as signed. Specifically, an agenda item is complete iff it is an independence-based (possibly partial) assignment. The other conditions for the agenda item being an IE MAP are guaranteed because the evidence nodes are assigned initially. Checking whether an assignment is independence-based is easy, due to the following theo rem (the locality theorem):\nTheorem 1 If As is a complete assignment to all the nodes of subset S of a belief network B, and for every node v E s, In(A{\u2022}, r (v)-SIAST(\u2022l), then As is an independence-based partial assignment to B.\nThe claim is essentially that if conditional indepen dence holds locally (i.e. with respect to just the im mediate predecessors, as opposed to all ancestors, as in the definition of independence-based partial assign ments), then it also holds globally. The theorem al lows us to test whether an assignment is independence based in time linear in the size of the network, and is thus an important theorem to use when we are con sidering the development of an algorithm to compute independence-based partial MAPs. The following the orem allows us to compute P(As) easily:\nTheorem 2 If In(v, r (v)- SIAST(v)) holds for every node v E S, then the probability of the assignment is:\nP(AS) = II P(A{v}IAST(v)) (4) vES\nTheorem 2 allows us to calculate P(As) in linear time for independence-based partial assignments, as the terms of the product are simply conditional prob abilities that can be read off from the conditional dis tribution array (or any other representation) of nodes given their parents.\nAnother modification is required because, when ex tending a node, we may want to leave some of the par ents unassigned, as we will show presently. Also, only nodes with unassigned parents are considered fringe nodes, since we do not need to assign nodes with no evidence nodes below them.\nTo take advantage of theorem 1, we precompute for each node v a set of all the cases where partial inde pendence occurs. We do that in the following man ner. The space defined by an assignment to a node v and some of its parents, (where the other parents are not assigned), defines a hypercube 1{ of possible value assignments. If the probability of v is the same given any assignment in fi, then 1{ is an independence based hypercube. Another way to look at this is: an independence-based hypercube is a sub-space of the conditional distribution array (of v given its parents) with equal conditional probability entries. Consider, for example the \"leaky\" OR node v of figure 2, where P(v = Tiui = T) = 0.9 for 1 :S: i::; 4 is independent of Uj, j '# i. This defines four 3-dimensional hypercubes of \"don't-care\" values. We also have the 1-dimensional hypercube where all Ui = F . When the algorithm ex pands v, it only assigns values to parents of which v is not independent (given the assignment to its other parents), i.e. it generates one agenda item for each of the above independence-based hypercubes.\nNaturally, since a belief net is not always a tree, some nodes may already be assigned. Consider, for example, figure 2. We are at the OR node v, with parents u1, u2, u3, u4, where v has the value T, and u1 has already been assigned F. We now have to expand all the states of the nodes ui, the parents of v. We would, however, like to generate as few new assignments as possible, while guaranteeing that the IB-MAP is still reachable.\nIn the complete MAP case, we add the following 8 assignments for the nodes ( u2, u3, u4):\n{( F, F, F), ( F, F, T), ( F, T, F), ( F, T, T), (T, F, F), (T, F, T), (T, T, F), (T, T, T)}\nThat is, all possible complete assignments to these three variables. When we need to find the partial\n374 Shimony\nMAP, however, only the following 4 assignments are added:\n{(T, U, U), (U, T, U, ), (U, U, T), (F, F, F)} where U stands for \"unassigned\" . If a hypercube is ruled out by a prior assignment to a parent node ( as is the case u1 = T here), it is ignored. Otherwise, the hypercubes are unified with the prior assignment, as in this case, the 3-dimensional hypercubes are reduced to 2-dimensional hypercubes by the prior assignment of u1 = F. All the other assignments are redundant, because they would assign values to variables that can not change the probability of v, and are subsumed by the 4 assignments listed above.\nFinally, to compute next-best partial assignments in decreasing order, we perform the same simple mod ification as for the complete MAP algorithm: simply continue to run, producing independence based partial assignments. A useful termination condition is now a probability threshold, i.e. stop producing assignments once the probability of an assignment is below some fraction of that of the first partial MAP produced.\n2.2 FORMAL DEFINITION OF THE ALGORITHM\nWe define the algorithm in terms of an input assign ment \u00a3, the evidence, and and output IB assignment. We shall define an expansion operator r, and a termi nation condition, and show that the algorithm termi nates with an IE-MAP.\nWe assume a total ordering 0 on the nodes, such that no node comes before its ( possibly indirect) descen dents. That is always possible, because belief net works are directed acyclic graphs ( DAGs). A fringe node w is minimal in an assignment if it is the first node w.r.t. the ordering 0 that has unassigned par ents. If w is a fringe node in an assignment, such that the independence-based assignment condition holds at w w.r.t. the assignment, then it is an independence based inactive (or just inactive, for short) fringe node. If the latter does not hold, then it is an active fringe node. If w is the first active node in the assignment, it is called a minimal active fringe node. Given an as signment and an ordering, the minimal active fringe node is unique. Unless otherwise specified, we shall assume that an implicit ordering 0 is present, and de fine the function index :nodes( B) --+ N, the index of a node w.r.t. 0.\nAn assignment A{w}uX to a node and a subset of its parents (X \ufffdi (w)) is called a hypercube based on w. If A{w}uX is complete w.r.t. wand X and P(A{w} lAX) is independent of the nodes i(w)- X, that is4:\n:lp 'fAV E AJ(w)-X P(A{w}IAX 1 AV) = p (5)\n4 Al(w)-X is the set of all complete assignments to the nodes i(w)- X\nthen A{w}uj(w) is an independence-based hypercube ( acronym IB hypercube), and p is the conditional probability of the hypercube.\nDefinition 5 An IB hypercube A{w}ux based on w is maximal if there does not ezist a different independence-based hypercube B{w}UY based on W that subsumes it (i.e. it is mazimal with respect to sub sumption).\nThe maximal IB hypercube based on w is not always unique. Note also that a maximal IB hypercube has the smallest set of nodes assigned. We currently as sume, for computation of hypercubes, that the distri bution is strictly positive.\nTheorem 3 If independence-based assignment A5 is subsumed by the evidence \u00a3, but is not evidentially supported w.r.t. \u00a3, then there ezists an independence based assignment AS' that subsumes As and is evi dentially supported w.r.t. \u00a3.\nProof: By construction: we show that we can drop all the nodes that have no evidence nodes below them from the assignment As. Since the belief network structure is a DAG, then so is any subgraph. Or der nodes of S that are not ancestors of some node in E ( nodes in E are considered to be ancestors here) in a list such that no node precedes its descendents. Now, proceed to eliminate nodes from the list ( and from the assignment), in order of the elements of S. As each node is eliminated, the assignment remains independence-based, as only nodes with no children are eliminated, and the independence-based assign ment criterion for each node depends only on ances tor nodes. We can thus eliminate the entire list, and remain with an assignment that is evidentially sup ported, is still subsumed by \u00a3, and is independence based. Q.E.D.\nTheorem 4 If As is an independence-based assign ment that is subsumed by \u00a3, then there exists an independence-based assignment As' that subsumes As and is properly evidentially supported w.r.t. \u00a3.\nProof: By construction: we show that we can delete from the assignment As, all the nodes that have no ev idence nodes below them, as well as all nodes for which no path to an evidence node ( that traverses only nodes in S) exists. Remove from the assignment As all nodes that are not ancestors of E as in the proof of theorem 3. Then, remove all the nodes T that have no path to a node in E that lies entirely inS, in a similar manner: sort the nodes of T into a list such that no node pre cedes its descendents. Removing the nodes of T will achieve a properly evidentially supported assignment, if we preserve the independence-based assignment con dition. But removing the nodes ofT in sequence will always preserve the criterion, because no node v is re moved if it has children in the resulting assignment (if\nit did, then the node 11 would not have been in T, as there would be a path from 11 to a node in E). Q.E.D. Theorems 3 and 4 are further support for our intuition for requiring that IB-MAPs be properly evidentially supported.\nLet Ap be the set of all possible (either partial or com plete) assi\u00a7nments. We define our expansion operator\nT : Ap U 2 \u2022 -> 2.A\u2022, as follows:\nDefinition 6 r(S) consists of ezactly the assignments As that obey the following conditions:\n\u2022 If S E Ap, then S subsumes As and there ez ists a fringe node w E S and a mazimal IB hy percube B{\"' }ux (based on w, where ezactly the nodes X <;j (w) are assigned), such that both the following conditions hold:\n1. S = nodes(S) U X 2. As = S U H{w}uX\n\u2022 If S E 2.A\u2022, then ezists an assignment As' E S that subsumes As, such that ezist a fringe node w E S' and a mazimal IB hypercube B{w}uX (based on w, where ezactly the nodes X <;j (w) are assigned), such that both the following condi tions hold:\n1. S = S' UX 2. As= As' U H{w}uX\nIn both cases, the hypercube should not contradict the assignment to variables that are already assigned.\nWe say that w is the fringe node expanded by r. Essen tially, applying the r operator is equivalent to taking all the IB-hypercubes at a certain node w, and creat ing a new assignment for each hypercube. The new assignment is a union of the old assignment and the hypercube.\nTheorem 5 If assignment As is r-reachable from \u00a3, then it is properly evidentially supported by \u00a3.\nProof: By induction on the number of applications of the tau operator. The theorem clearly holds for 0 ap plications, as the only assignment in that case is { \u00a3}, which is clearly properly evidentially supported. Now, assuming that the theorem holds for n applications of r, then another application ofT can only assign values to nodes that are in some IB-hypercube based on a node w already assigned. IB-hypercubes assign only direct parents of w, and w is either in E or there ex ists a path from w to E passing only through assigned nodes, by the induction hypothesis. Hence, there will always by a path from the nodes assigned in the n + 1 application of r to E. The theorem follows by induc tion. Q.E.D.\nAn assignment As is IB-terminated when each as signed node w E S either has no parents, or the IB\nAlgorithms for Irrelevance-Based Partial MAPs 375\ncondition holds at w. The latter is true iff the as signment for every wE S, A{w}uST(v) is subsumed by some IB hypercube based on w.\nTheorem 6 Every mazimal (w.r.t. subsumption) independence-based assignment As that is properly ev identially supported w.r.t. \u00a3 is r-reachable from \u00a3.\nProof outline: We show that there exists a sequence of assignments As\ufffd of sufficient length, such that each assignment subsumes As, As\ufffd E r(As\ufffd_, ) , and if 11k is the node expanded by r, then all the nodes 11;, i :-:::; k , are assigned exactly as in As. Thus, for some k, As\ufffd =As. The proof of this theorem shows that it is actually sufficient to expand only the minimal fringe node at each state, rather than all fringe nodes.\nUsing an agenda S (a set of states, or assign ments), evaluation function H, evidence \u00a3 (where E = nodes( \u00a3)) and expansion operator T, the algo rithm is defined formally as follows:\n1. SetS={\u00a3}, and i = minvEE index(v). 2. Set As to be a member of S of maximum H(As),\nand remove it from S. 3. If As, is IE-terminated, halt (As is an IB-MAP). 4. Set S = (S U r(As))- As, and go to step 2.\nThe evaluation function H is similar the one for the complete MAP algorithm. The only difference is that the (conditional) probability of a node 11 is included in the product if the IB condition holds at v, as well as when all its parents are assigned:\nH(As) = II P(A{\"}IAT(\")) (6) vEG(S)\nG(S) { v l11 E S Aln(A{v}, j(11)- SIAST(v))} U {11!11 E S !\\ Vw E j(11), wE S}\nH is obviously optimistic, and because of theorem 2, it is exact for IB assignments (the goal states). As the algorithm is implemented, H is actually computed before adding an assignment to the agenda, and the agenda is always kept sorted (e.g. using a heap). We now show that the algorithm is correct.\nTheorem 7 The IB-MAP algorithm terminates, and when it halts it does so with As being the most-probable IB assignment that is properly evidentially supported and subsumed by \u00a3.\nProof: The algorithm terminates, because the num ber of states added to the agenda in step 3 is finite, and since it always adds nodes to each assignment As, it will eventually assign all the nodes above E, in which case the IB condition is vacuously true. Nat urally, the runtime may be exponential. The assign ment found when the algorithm terminates is IB (that\n376 Shimony\nP(X) = 0.1\nP(A I XY) = P(A I XZ) = I\nP(A I other combo)= 0\nP(Z) = 0.1\nP(B I XZ) = P(B I YZ) = I\nP(B I other combo) = 0\nP(EI AB)= I\nP(E I other combo) = 0\nOrdering 0 =(E. A. B, Y, X, 7:)\nMinimal assignment: [E, A, B, X, Z}\nExpanded node:\nE\nA\nB\nAssignment (one possiblity)\n[E,A,B}\n[E, A, B, X, Y]\n[E. A, B, X, Y, Z]\nis the termination condition). It is properly eviden tially supported (from theorem 5) and the fact that all assignments generated are T accessible from E. The evaluation function admissible, and all possible max imal properly evidentially supported IB assignments are T-accessible. The theorem follows from the latter two properties, and from the correctness condition of heuristic search w.r.t. evaluation functions. Q.E.D.\nContinuing to run the algorithm after finding a first assignment will find next-best IB-assignments, in de creasing order of probability. Note that theorem 7 does not guarantee a mazimal (w.r.t. subsumption) IB-MAP. In fact, figure 3 shows a simple counterex ample, where all the nodes are binary, E is the evi dence node, and is known to be true. Given the set of agenda states shown, the non-maximal assignment, {E, A, B, X, Y, Z} is reached. The latter assignment is not maximal w.r.t. subsumption, because the assign ment {E, A, B, X, Z} subsumes it, and is both IB and properly evidentially supported.\nHowever, for positive distributions, subsumption also implies a higher probability, which guarantees that the IB-MAP found is indeed maximal. For other distribu tions, to find the maximal IB-MAPs, we need to com pare all IB-MAPs with equal probability, which is not hard in most cases. We are assured that the maximal IB-MAP will indeed appear if we continue to run the\nalgorithm, because of theorem 6.\n3 8-IB MAP ALGORITHM\nIn the case of 6-independence based MAPs, we use es sentially the same algorithm again, where we need to pre-compute 6-independence hypercubes, rather than independence hypercubes, as in the previous case. However, once that is done, we can again employ local checking:\nTheorem 8 If As is a complete assignment to all the nodes of subset S of a belief network B, and for every node v E S, In0(A{\u2022}, j (v) - SJAST(v)) (for 0 \ufffd 6 \ufffd 1}, then As is a 6-independence-based partial assignment to B.\nComputing the exact probability of a 6-independence based partial assignment seems to be hard (since we cannot use theorem 2, and would need to find poste rior probabilities of non-root nodes), but the following easily computable bound inequalities are always true:\nSince getting the exact probability is hard, but the upper and lower bounds above (denoted U(A)s and \u00a3(A)s respectively) are easily computable, a post processing step may be needed, to select the most probable assignment from a number of possible candi dates. During the first part of the algorithm, the as signments are sorted in the agenda according to U (A). We need to collect the set of assignments F such that for all assignments A not in F, U (A) is smaller than \u00a3(A)', for some A' in F. This assures us that the most-probable assignment is indeed in F. Hopefully, F is a small set (as indeed it will prove to be in almost all cases where one explanation clearly stands out). Then, we evaluate the exact probability of the assign ments in F in parallel by adding AND nodes for all of them and evaluating the diagram exactly once. Natu rally, ifF happens to contain only one assignment, we do not need to post-process the results.\n4 FUTURE WORK\nThe locality property of IB assignments and 6-IB as signments, i.e. the fact that local testing is sufficient to determine whether an assignment is an IB assignment, together with a quick way of computing its probabil ity, makes it possible, in principle, to use other types\nof algorithm. Future research will determine whether random simulation techniques will prove useful.\nAnother possibility is to reduce IB-MAP computation to complete MAP computation (on a different belief network). That will allow us to use any complete MAP algorithm, such as belief updating ([Pearl, 1988]), to be used. This may prove to be faster than our algo rithms for networks with a small maximal clique size (for using clustering), or a small cutset size (for using conditioning). The algorithm presented in [Santos Jr., 1991a] and [Santos Jr., 1991b] may also prove useful for our purposes, if it can be easily extended to work on multiple-valued nodes.\n5 SUMMARY\nWe introduced irrelevance-based partial MAPs in or der to solve the overspecification problem inherent in complete MAP explanation. We defined two methods for defining what nodes are irrelevant, one based on exact independence, the other based on approximate independence. We then discussed properties of the re sulting partial MAPs that allowed us to transform an existing best-first search for complete MAPs into one that computes irrelevance-based partial MAPs. The modifications required were minimal.\nFor independence-based assignments, we showed that we can check whether an assignment is independence based using only local information, (i.e. they are \"lo cally recognizable\" ). Computation of the probability of such an assignment is also easy, and can be done us ing only lSI conditional probability array entries. We showed how to adapt our best-first MAP algorithm to compute IB-MAPs, and proved the correctness of the resulting algorithm.\na-independence-based assignments were shown to be locally recognizable. Computing the exact probability is hard, but good, easily computable bounds are avail able. It is possible to compute the exact probabilities in parallel, using only one belief-network evaluation (with extra nodes).\nWe have implemented algorithms for finding IB-MAPs and for finding oiB-MAPs, and their running time seems roughly comparable to the running time of our complete MAP algorithm running on the same net works. That is not surprising, as the additional work required for expansion and for completion testing in small, and the number of states expanded is usually smaller than for complete MAPs.\nA cknowledgements\nThis work has been supported in part by the Na tional Science Foundation under grants IST 8416034 and IST 8515005 and Office of Naval Research under grant N00014-79-C-0529. The author is funded by a\nAlgorithms for Irrelevance-Based Partial MAPs 377\nCorinna Borden Keen Fellowship. Special thanks to Eugene Charniak for he! pful suggestions and for re viewing drafts of the paper.\nReferences\n[Charniak and Goldman, 1988] Eugene Charniak and Robert Goldman. A logic for semantic interpreta tion. In Proceedings of the A CL Conference, 1988.\n[Charniak and Shimony, 1990] Eugene Charniak and Solomon E. Shimony. Probabilistic semantics for cost-based abduction. In Proceedings of the 8th Na tional Conference on AI, August 1990.\n[Cooper, 1984] Gregory Floyd Cooper. NESTOR: A Computer-Based Medical Diagnosis Aid that Inte grates Causal and Probabilistic Know ledge. PhD thesis, Stanford University, 1984.\n[Hobbs and Stickel, 1988] Jerry R. Hobbs and Mark Stickel. Interpretation as abduction. In Proceedings of the 26th Conference of the ACL, 1988.\n(Pearl, 1988] J. Pearl. Probabilistic Reasoning in In telligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, CA, 1988.\n[Peng and Reggia, 1987] Y. Peng and J. A. Reggia. A probabilistic causal model for diagnostic problem solving (parts 1 and 2). In IEEE Transactions on Systems, Man and Cybernetics, pages 146-162 and 395-406, 1987.\n[Poole and Provan, 1990] David Poole and Gregory M. Provan. What is an optimal di agnosis? In Proceedings of the 6th Conference on Uncertainty in AI, pages 46-53, 1990.\n[Santos Jr., 1991a] Eugene Santos Jr. A linear con straint satisfaction approach to cost-based abduc tion. Technical report, Computer Science Depart ment, Brown University, 1991.\n[Santos Jr., 1991b] Eugene Santos Jr. On the gener ation of alternative explanations with implications. In Proceedings of the 7th Conference on Uncertainty in AI, 1991.\n(Shimony and Charniak, 1990] Solomon E. Shimony and Eugene Charniak. A new algorithm for finding map assignments to belief networks. In Proceedings of the 6th Conference on Uncertainty in AI, 1990.\n(Shimony, 1991] Solomon E. Shimony. Explanation, irrelevance and statistical independence. In AAAI Proceedings, 1991."}], "references": [{"title": "A logic for semantic interpreta\u00ad tion", "author": ["Charniak", "Goldman", "1988] Eugene Charniak", "Robert Goldman"], "venue": "In Proceedings of the A CL Conference,", "citeRegEx": "Charniak et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Charniak et al\\.", "year": 1988}, {"title": "Probabilistic semantics for cost-based abduction", "author": ["Charniak", "Shimony", "1990] Eugene Charniak", "Solomon E. Shimony"], "venue": "In Proceedings of the 8th Na\u00ad tional Conference on AI,", "citeRegEx": "Charniak et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Charniak et al\\.", "year": 1990}, {"title": "NESTOR: A Computer-Based Medical Diagnosis Aid that Inte\u00ad grates Causal and Probabilistic Know ledge", "author": ["Cooper", "1984] Gregory Floyd Cooper"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "Cooper and Cooper.,? \\Q1984\\E", "shortCiteRegEx": "Cooper and Cooper.", "year": 1984}, {"title": "Interpretation as abduction", "author": ["Hobbs", "Stickel", "1988] Jerry R. Hobbs", "Mark Stickel"], "venue": "In Proceedings of the 26th Conference of the ACL,", "citeRegEx": "Hobbs et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Hobbs et al\\.", "year": 1988}, {"title": "A probabilistic causal model for diagnostic problem solving (parts 1 and 2)", "author": ["Peng", "Reggia", "1987] Y. Peng", "J.A. Reggia"], "venue": "In IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "Peng et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Peng et al\\.", "year": 1987}, {"title": "What is an optimal di\u00ad agnosis", "author": ["Poole", "Gregory M. Provan"], "venue": "In Proceedings of the 6th Conference on Uncertainty in AI,", "citeRegEx": "Poole and Provan.,? \\Q1990\\E", "shortCiteRegEx": "Poole and Provan.", "year": 1990}, {"title": "A linear con\u00ad straint satisfaction approach to cost-based abduc\u00ad tion", "author": ["Santos Jr.", "1991a] Eugene Santos Jr."], "venue": "Technical report,", "citeRegEx": "Jr. and Jr.,? \\Q1991\\E", "shortCiteRegEx": "Jr. and Jr.", "year": 1991}, {"title": "On the gener\u00ad ation of alternative explanations with implications", "author": ["Santos Jr.", "1991b] Eugene Santos Jr."], "venue": "In Proceedings of the 7th Conference on Uncertainty in AI,", "citeRegEx": "Jr. and Jr.,? \\Q1991\\E", "shortCiteRegEx": "Jr. and Jr.", "year": 1991}, {"title": "A new algorithm for finding map assignments to belief networks", "author": ["Shimony", "Charniak", "1990] Solomon E. Shimony", "Eugene Charniak"], "venue": "In Proceedings of the 6th Conference on Uncertainty in AI,", "citeRegEx": "Shimony et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Shimony et al\\.", "year": 1990}], "referenceMentions": [{"referenceID": 5, "context": "In fact, Poole discussed six different schemes of probabilistic explanation in [Poole and Provan, 1990], and even these are not exhaustive.", "startOffset": 79, "endOffset": 103}], "year": 2011, "abstractText": "Irrelevance-based partial MAPs are useful constructs for domain-independent explana\u00ad tion using belief networks. We look at two definitions for such partial MAPs, and prove important properties that are useful in de\u00ad signing algorithms for computing them effec\u00ad tively. We make use of these properties in modifying our standard MAP best-first algo\u00ad rithm, so as to handle irrelevance-based par\u00ad tial MAPs.", "creator": "pdftk 1.41 - www.pdftk.com"}}}