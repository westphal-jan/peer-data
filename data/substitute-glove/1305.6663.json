{"id": "1305.6663", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2013", "title": "Generalized Denoising Auto-Encoders as Generative Models", "abstract": "Recent create not these how denoising and packera autoencoders refrained capture the frame however which product increases correlation, in an serious outside was investigate noise always Gaussian, the contingency explanation is the squared error, and time specific is continuous - 1.7. This seen early soon consist insisting for sampling one this implicitly learned reduces correspond, provide Langevin between Metropolis - Hastings MCMC. However, if being unclear how to connect still naval subject now analogously gm - encoders to new implicit deviation entire an underlying comparison renewable distribute when full reference instead 2-dimensional, or rather though exist of bureaucracy allow and reconstruction inconsistency. Another what is been sociological justifications. much few acceptable in end tax of small corruption tuned. We provisions every yet different attack taking the though, has dealings also certain except issues: arbitrary (but noisy give) corruption, discrete reconstruction shock (there when to sheds - anticipated ), taking both discrete like continuous - valued interactions, and partially though bias due be non - infinitesimal corrupt sound (those non - transposition contractive netted ).", "histories": [["v1", "Wed, 29 May 2013 00:25:54 GMT  (690kb,D)", "http://arxiv.org/abs/1305.6663v1", null], ["v2", "Sun, 2 Jun 2013 00:03:48 GMT  (744kb,D)", "http://arxiv.org/abs/1305.6663v2", null], ["v3", "Fri, 7 Jun 2013 16:46:15 GMT  (744kb,D)", "http://arxiv.org/abs/1305.6663v3", null], ["v4", "Mon, 11 Nov 2013 02:27:55 GMT  (784kb,D)", "http://arxiv.org/abs/1305.6663v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio", "li yao", "guillaume alain", "pascal vincent"], "accepted": true, "id": "1305.6663"}, "pdf": {"name": "1305.6663.pdf", "metadata": {"source": "CRF", "title": "Generalized Denoising Auto-Encoders as Generative Models", "authors": ["Yoshua Bengio", "Li Yao"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Auto-encoders learn an encoder function from input to representation and a decoder function back from representation to input space, such that the reconstruction (composition of encoder and decoder) is good for training examples. Regularized auto-encoders also involve some form of regularization that prevents the auto-encoder from simply learning the identity function, so that reconstruction error will be low at training examples (and hopefully at test examples) but high in general. Different variants of auto-encoders and sparse coding have been, along with RBMs, among the most successful building blocks in recent research in deep learning (Bengio et al., 2013b). Whereas the usefulness of auto-encoder variants as feature learners for supervised learning can directly be assessed by performing supervised learning experiments with unsupervised pre-training, what has remained until recently rather unclear is the interpretation of these algorithms in the context of pure unsupervised learning, as devices to capture the salient structure of the input data distribution. Whereas the answer is clear for RBMs, it is less obvious for regularized auto-encoders. Do they completely characterize the input distribution or only some aspect of it? For example, clustering algorithms such as k-means only capture the modes of the distribution, while manifold learning algorithms characterize the low-dimensional regions where the density concentrates.\nSome of the first ideas about the probabilistic interpretation of auto-encoders were proposed by Ranzato et al. (2008): they were viewed as approximating an energy function through the reconstruction error, i.e., being trained to have low reconstruction error at the training examples and high reconstruction error elsewhere (through the regularizer, e.g., sparsity or otherwise, which prevents the auto-encoder from learning the identity function). An important breakthrough then came, yielding a first formal probabilistic interpretation of regularized auto-encoders as models of the input distribution, with the work of Vincent (2011). This work showed that some denoising auto-encoders correspond to a Gaussian RBM and that minimizing the denoising reconstruction error (as a squared error) estimates the energy function through a regularized form of score matching, with the regularization disappearing as the amount of corruption noise goes to 0, and then converging to the same\nar X\niv :1\n30 5.\n66 63\nv1 [\ncs .L\nG ]\n2 9\nM ay\n2 01\nsolution as score matching (Hyva\u0308rinen, 2005). This connection and its generalization to other energy functions, giving rise to the general denoising score matching training criterion, is discussed in several papers (Kingma and LeCun, 2010; Swersky et al., 2011; Alain and Bengio, 2013).\nAnother breakthrough has been the development of an empirically successful sampling algorithm for contractive auto-encoders (Rifai et al., 2012a), which basically involves composing encoding, decoding, and noise addition steps. This algorithm is motivated by the observation that the Jacobian matrix (of derivatives) of the encoding function provides an estimator of a local Gaussian approximation of the density, i.e., the leading singular vectors of that matrix span the tangent plane of the manifold near which the data density concentrates. However, a formal justification for this algorithm remains an open problem.\nThe last step in this development (Alain and Bengio, 2013) generalized the result from Vincent (2011) by showing that when a denoising auto-encoder (or a contractive auto-encoder with the contraction on the whole encode/decode reconstruction function) is trained with small Gaussian corruption and squared error loss, it estimates the score (derivative of the log-density) of the underlying data-generating distribution, which is proportional to the difference between reconstruction and input. This result does not depend on the parametrization of the auto-encoder, but suffers from the following limitations: it applies to one kind of corruption (Gaussian), only to continuous-valued inputs, only for one kind of loss (squared error), and it becomes valid only in the limit of small noise (even though in practice, best results are obtained with large noise levels, comparable to the range of the input).\nWhat we propose here is a different probabilistic interpretation of denoising auto-encoders, which is valid for any data type, any corruption process (so long as it has broad enough support), and any reconstruction loss (so long as we can view it as a log-likelihood).\nThe basic idea is that if we corrupt observed random variable X into X\u0303 using conditional distribution C(X\u0303|X), we are really training the denoising auto-encoder to estimate the reverse conditional P (X|X\u0303). Combining this estimator with estimator with the known C(X\u0303|X), we show that can recover a consistent estimator of P (X) through a Markov chain that alternates between sampling from P (X|X\u0303) and sampling from C(X\u0303|X), i.e., encode/decode, sample from the reconstruction distribution model P (X|X\u0303), apply the corruption stochastic procedure C(X\u0303|X), and iterate. This theoretical result is validated through experiments on artificial data in a non-parametric setting and experiments on real data in a parametric setting (with denoising auto-encoders). We find that we can improve the sampling behavior by using the model itself to define the corruption process, yielding a training procedure that has some surface similarity to the contrastive divergence algorithm (Hinton, 1999; Hinton et al., 2006)."}, {"heading": "2 Generalizing Denoising Auto-Encoders", "text": ""}, {"heading": "2.1 Definition and Training", "text": "Let P(X) be the data-generating distribution over observed random variable X . Let C be a given corruption process that stochastically maps an X to a X\u0303 through conditional distribution C(X\u0303|X). The training data for the generalized denoising auto-encoder is a set of pairs (X, X\u0303) withX \u223c P(X) and X\u0303 \u223c C(X\u0303|X). The denoising auto-encoder is trained to predict X given X\u0303 through a learned conditional distribution P\u03b8(X|X\u0303), by choosing this conditional distribution within some family of distributions indexed by \u03b8. The training procedure for the denoising auto-encoder can generally be formulated as learning to predict X given X\u0303 by possibly regularized maximum likelihood, i.e., the generalization performance that this training criterion attempts to minimize is\nL(\u03b8) = E[logP\u03b8(X|X\u0303)] (1)\nwhere the expectation is taken over the joint data generating distribution\nP(X, X\u0303) = P(X)C(X\u0303|X). (2)\nAlgorithm 1 THE GENERALIZED DENOISING AUTO-ENCODER TRAINING ALGORITHM requires a training set or training distribution D of examples X , a given corruption process C(X\u0303|X) from which one can sample, and with which one trains a conditional distribution P\u03b8(X|X\u0303) from which one can sample.\nrepeat \u2022 sample training example X \u223c D \u2022 sample corrupted input X\u0303 \u223c C(X\u0303|X) \u2022 use (X, X\u0303) as an additional training example towards minimizing the expected value of \u2212 logP\u03b8(X|X\u0303), e.g., by a gradient step with respect to \u03b8. until convergence of training (e.g., as measured by early stopping on out-of-sample negative loglikelihood)"}, {"heading": "2.2 Sampling", "text": "We define the following pseudo-Gibbs Markov chain associated with P\u03b8:\nXt \u223c P\u03b8(X|X\u0303t\u22121) X\u0303t \u223c C(X\u0303|Xt) (3)\nwhich can be initialized from an arbitrary choice X0. This is the process by which we are going to generate samples Xt according to the model implicitly learned by choosing \u03b8. We define T (Xt|Xt\u22121) the transition operator that defines a conditional distribution for Xt given Xt\u22121, independently of t, so that the sequence of Xt\u2019s forms a homogeneous Markov chain. If the asymptotic marginal distribution of the Xt\u2019s exists, we call this distribution \u03c0(X), and we show below that it consistently estimates P(X). Note that the above chain is not a proper Gibbs chain in general because there is no guarantee that P\u03b8(X|X\u0303t\u22121) and C(X\u0303|Xt) are consistent with a unique joint distribution. In that respect, the situation is similar to the sampling procedure for dependency networks (Heckerman et al., 2000), in that the pairs (Xt, X\u0303t\u22121) are not guaranteed to have the same asymptotic distribution as the pairs (Xt, X\u0303t) as t \u2192 \u221e. However, if P\u03b8(X|X\u0303) is a good estimator of the true conditional P(X|X\u0303) the discrepency would asymptotically vanish as the number of training examples n \u2192 \u221e and we accordingly increase capacity of the model. The question of asymptotic consistency is discussed in the next section. However, bear in mind that even though the odd pairs and the even pairs are not guaranteed to converge to the same joint, the marginals (over X or over X\u0303) will generally converge (so long as the operator T is ergodic, for example), so we can properly talk about the asymptotic distribution \u03c0(X) defined by T , and this is the property we care about to establish consistency of the estimator of the data generating distribution."}, {"heading": "2.3 Consistency", "text": "Normally we only have access to a finite number n of training examples but as n \u2192 \u221e, the empirical training distribution approaches the data generating distribution. To compensate for the finite training set, we generally introduce a (possibly data-dependent) regularizer \u2126 and the actual training criterion is\nLn(\u03b8) = 1\nn \u2211 X\u223cP(X),X\u0303\u223cC(X\u0303|X) \u03bbn\u2126(\u03b8,X, X\u0303)\u2212 logP\u03b8(X|X\u0303) (4)\nwhere we allow the regularization coefficient \u03bbn to be chosen according to the number of training examples n, with \u03bbn \u2192 0 as n \u2192 \u221e. With \u03bbn \u2192 0 we get that Ln \u2192 L (i.e. converges to generalization error, Eq. 1), so consistent estimators stay consistent. We define \u03b8n to be the minimizer of Ln(\u03b8).\nWe define Tn to be the transition operator Tn(Xt|Xt\u22121) = \u222b P\u03b8n(Xt|X\u0303)C(X\u0303|Xt\u22121)dX\u0303 associated with \u03b8n (the parameter obtained by minimizing the training criterion with n examples), and define \u03c0n to be the asymptotic distribution of the Markov chain generated by Tn (if it exists). We also define T be the operator of the Markov chain associated with the learned model as n\u2192\u221e. Theorem 1. If P\u03b8n(X|X\u0303) is a consistent estimator of the true conditional distribution P(X|X\u0303) and Tn defines an irreducible and ergodic Markov chain, then as n \u2192 \u221e, the asymptotic distribution \u03c0n(X) of the generated samples converges to the data generating distribution P(X).\nProof. If Tn is irreducible and ergodic, then the Markov chain converges to a \u03c0n. Based on our definition of the \u201ctrue\u201d joint (Eq. 2), one obtains a conditional P(X|X\u0303) \u221d P(X)C(X\u0303|X). This conditional, along with P(X\u0303|X) = C(X\u0303|X) can be used to define a proper Gibbs chain where one alternatively samples from P(X\u0303|X) and from P(X|X\u0303). Let T be the corresponding \u201ctrue\u201d transition operator, which maps the t-th sample X to the t + 1-th in that chain. That is, T (Xt|Xt\u22121) = \u222b P(Xt|X\u0303)C(X\u0303|Xt\u22121)dX\u0303 . Note that T produces P(X) as asymptotic marginal distribution over X .\nBy hypothesis we have that P\u03b8n(X|X\u0303) \u2192 P(X|X\u0303) as n \u2192 \u221e. Note that Tn is defined exactly as T but with P(Xt|X\u0303) replaced by P\u03b8n(X|X\u0303). Hence Tn \u2192 T as n\u2192\u221e. Now let us convert the convergence of Tn to T into the convergence of \u03c0n(X) to P(X). We will exploit the fact that for the 2-norm, matrix M and unit vector v, ||Mv||2 \u2264 sup||x||2=1 ||Mx||2 = ||M ||2. Consider M = T \u2212 Tn and v the principal eigenvector of T , which, by the Perron-Frobenius theorem, corresponds to the asymptotic distribution P(X). Since Tn \u2192 T , ||T \u2212 Tn||2 \u2192 0. Hence ||(T \u2212 Tn)v||2 \u2264 ||T \u2212 Tn||2 \u2192 0, which implies that Tnv \u2192 T v = v, where the last equality comes from the Perron-Frobenius theorem (the leading eigenvalue is 1). Since Tnv \u2192 v, it implies that v becomes the leading eigenvector of Tn, i.e., the asymptotic distribution of the Markov chain, \u03c0n(X) converges to the true data generating distribution, P(X), as n\u2192\u221e.\nHence the asymptotic sampling distribution associated with the Markov chain defined by Tn (i.e., the model) implicitly defines the distribution \u03c0n(X) learned by the denoising auto-encoder over the observed variable X . Furthermore, that estimator of P(X) is consistent so long as our (regularized) maximum likelihood estimator of the conditional P\u03b8(X|X\u0303) is also consistent. We now provide sufficient conditions for the ergodicity of the chain operator, i.e. for applying theorem 1.\nCorollary 1. If P\u03b8(X|X\u0303) is a consistent estimator of the true conditional distribution P(X|X\u0303), the data generating distribution and the denoising model are contained in and non-zero in a finitevolume region V , i.e., \u2200X /\u2208 V, P(X) = 0, P\u03b8(X|X\u0303) = 0, but also \u2200X \u2208 V, P(X) > 0, P\u03b8(X|X\u0303) > 0, C(X\u0303|X) > 0 and these statements remain true in the limit of n \u2192 \u221e, then the asymptotic distribution \u03c0n(X) of the generated samples converges to the data generating distribution P(X).\nProof. To obtain the existence of a stationary distribution, we need irreducibility (every value reachable from every other value), aperiodicity (no cycle where only paths through the cycle allow to return to some value), and recurrence (probability 1 of returning eventually). These conditions can be generalized to the continuous case, where we obtain ergodic Harris chains rather than Markov chains. If P\u03b8(X|X\u0303) > 0 and C(X\u0303|X) > 0 (for X \u2208 V ), then Tn(Xt|Xt\u22121) > 0 as well, because\nT (Xt|Xt\u22121) = \u222b P\u03b8(Xt|X\u0303)C(X\u0303|Xt\u22121)dX\u0303\nThis positivity of the transition operator guarantees that one can jump from any point in V to any other point in one step, thus yielding irreducibility and aperiodicity. To obtain recurrence (preventing the chain from diverging to infinity), we rely on the assumption that the domain V is bounded. Note that although Tn(Xt|Xt\u22121) > 0 could be true for any finite n, we need this condition to hold for n \u2192 0 as well, to obtain the consistency result of theorem 1. By assuming that this positivity assumption (Boltzmann distribution) is true for the data generating distribution, we make sure that \u03c0n does not converge to a distribution which puts 0\u2019s anywhere in V . Having satisfied all the conditions for the existence of a stationary distribution for Tn as n \u2192 \u221e, we can apply theorem 1 and obtain its conclusion.\nNote how these conditions take care of the various troubling cases one could think of. First, we avoid the case where there is no corruption (which would indeed yield a wrong estimation, with the denoising auto-encoder simply learning to put a dirac probability on the value it gets in input). Second, we avoid the case where the chain wanders to infinity by assuming a finite volume where the model and data live, a real concern in the continuous case. Practically, if it became a real issue, we could perform rejection sampling to make sure that P (X|X\u0303) produces X \u2208 V ."}, {"heading": "2.4 Locality of the Corruption and Energy Function", "text": "If we believe that P (X|X\u0303) is well estimated for all (X, X\u0303) pairs, i.e., that it is approximately consistent with C(X\u0303|X), then we get as many estimators of the energy function as we want, by picking a particular value of X\u0303 .\nLet us define the notation P (\u00b7) to denote the probability of the joint, marginals or conditionals over the pairs (Xt, X\u0303t\u22121) that are produced by the Markov chain T as t\u2192\u221e. So P (X) = \u03c0(X) is the asymptotic distribution of the Markov chain T , and P (X\u0303) the marginal over the X\u0303\u2019s in that chain. The above assumption means that P (X\u0303t\u22121|Xt) \u2248 C(X\u0303t\u22121|Xt) (which is not guaranteed in general, but only asymptotically as P approaches the true P). Then, by Bayes rule, P (X) = P (X|X\u0303)P (X\u0303)\nP (X\u0303|X) \u2248 P (X|X\u0303)P (X\u0303) C(X\u0303|X) \u221d P (X|X\u0303) C(X\u0303|X) so that we can get an estimated energy function from any given choice of X\u0303 through energy(X) \u2248 \u2212 logP (X|X\u0303) + logC(X\u0303|X). where one should note that the intractable partition function depends on the chosen value of X\u0303 .\nHow much can we trust that estimator and how should X\u0303 be chosen? First note that P (X|X\u0303) has only been trained for pairs (X, X\u0303) for which X\u0303 is relatively close toX (assuming that the corruption is indeed changing X generally into some neighborhood). Hence, although in theory (with infinite amount of data and capacity) the above estimator should be good, in practice it might be poor when X is far from X\u0303 . So if we pick a particular X\u0303 the estimated energy might be good for X in the neighborhood of X\u0303 but poor elsewhere. What we could do though, is use a different approximate energy function in different regions of the input space. Hence the above estimator gives us a way to compare the probabilities of nearby points X1 and X2 (through their difference in energy), picking for example a midpoint X\u0303 = 12 (X1 +X2). One could also imagine that if X1 and XN are far apart, we could chart a path between X1 and XN with intermediate points Xk and use an estimator of the relative energies between the neighbors Xk, Xk+1, add them up, and obtain an estimator of the relative energy between X1 and XN .\nThis brings up an interesting point. If we could always obtain a good estimator P (X|X\u0303) for any X\u0303 , we could just train the model with C(X\u0303|X) = C(X\u0303), i.e., with an unconditional noise process that ignores X . In that case, the estimator P (X|X\u0303) would directly equal P (X) since X\u0303 and X are actually sampled independently in its \u201cdenoising\u201d training data. We would have gained nothing over just training any probabilistic model just directly modeling the observed X\u2019s. The gain we expect from using the denoising framework is that if X\u0303 is a local perturbation of X , then the true P(X|X\u0303) is a much simpler distribution than P(X). See Figure 1 for a visual explanation: in the limit of very small perturbations, one could even assume that P(X|X\u0303) can be well approximated by a simple unimodal distribution such as the Gaussian (for continuous data) or factorized binomial (for discrete binary data) commonly used in denoising auto-encoders as the reconstruction probability function (conditioned on X\u0303). This idea is already behind the non-local manifold Parzen windows (Bengio et al., 2006a) and non-local manifold tangent learning (Bengio et al., 2006b) algorithms: the local density around a point X\u0303 can be approximated by a multivariate Gaussian whose covariance matrix has leading eigenvectors that span the local tangent of the manifold near which the data concentrates (if it does). The idea of a locally Gaussian approximation of a density with a manifold structure is also exploited in the more recent work on the contractive auto-encoder (Rifai et al., 2011) and associated sampling procedures (Rifai et al., 2012b). Finally, strong theoretical evidence in favor of\nthat idea comes from the result from Alain and Bengio (2013): when the amount of corruption noise converges to 0 and the input variables have a smooth continuous density, then a unimodal Gaussian reconstruction density suffices to fully capture the joint distribution.\nHence, although P (X|X\u0303) encapsulates all the information about P(X) (assuming one has access to C(X\u0303|X)), it will generally correspond to a much simpler distribution than P (X), with less modes. We return to this question in Section 3 below, suggesting that from the point of view of learning to capture the distribution not just in the vicinity of the examples but elsewhere (i.e., avoiding spurious modes), it is better to have non-infinitesimal corruption. This also translates immediately in better mixing chains and chains that can burn-in successfully rather than stay stuck around spurious modes far from the data."}, {"heading": "3 Reducing the Spurious Modes with Walkback Training", "text": "Once we start sampling in high-dimensional spaces (like in the experiments reported below) using a simple local corruption process (such as Gaussian or salt-and-pepper noise) it becomes apparent that one practical issue is that if the corruption is too local, the denoising auto-encoder\u2019s behavior far from the training examples can create spurious modes in these regions insufficiently visited during training. More training iterations or increasing the amount of corruption noise helps to substantially alleviate that problem, but we discovered an even bigger boost by a variant of the algorithm in which train the denoising auto-encoder to walk back towards the training examples. We exploit knowledge of the currently learned model P (X|X\u0303) to define the corruption process, so as to pick values of X\u0303 that would be obtained by following the generative Markov chain. In other words, wherever the model would go if we sampled using the generative Markov chain starting at a training example X , we consider to be a kind of \u201cnegative example\u201d X\u0303 from which the auto-encoder should move away (and towards X). The spirit of this procedure is thus very similar to the CD-k (Contrastive Divergence with k MCMC steps) procedure proposed to train RBMs (Hinton, 1999; Hinton et al., 2006).\nMore precisely, the modified corruption process C\u0303 we propose is the following, based on the original corruption process C. We use it in a version of the training algorithm called walkback, where we replace the corruption process C of Algorithm 1 by the walkback process C\u0303 of Algorithm 2 while also adding extra examples (taking advantage of the X\u0303 samples generated along the walk away from X). It is called walkback because it forces the denoising auto-encoder to learn to walk back from the random walk it generates, towards the training examples.\nAlgorithm 2 THE WALKBACK ALGORITHM is based on the walkback corruption process C\u0303(X\u0303|X), defined below in terms of a generic original corruption process C(X\u0303|X) and the current model\u2019s reconstruction conditional distribution P (X|X\u0303). For each training example X , it provides a sequence of additional training examples (X, X\u0303\u2217) for the denoising auto-encoder. It has a hyper-parameter that is a geometric distribution parameter 0 < p < 1 controlling the length of these walks away from X , with p = 0.5 by default. The training procedure of the denoising auto-encoder (Algorithm 1) is the same, but using all the pairs (X, X\u0303\u2217) as training examples instead of just (X, X\u0303).\n1: X\u2217 \u2190 X 2: Sample X\u0303\u2217 \u223c C(X\u0303|X\u2217) 3: Sample u \u223c Uniform(0, 1) 4: if u > p then 5: return X\u0303 = X\u0303\u2217 as the sample generated from C\u0303(X\u0303|X). 6: If during training, add (X, X\u0303\u2217) as an additional training example. 7: Sample X\u2217 \u223c P (X|X\u0303\u2217) 8: goto 2.\nProposition 1. Under the assumptions of corollary 1, minimizing the training criterion in walkback training algorithm for generalized denoising auto-encoders (combining Algorithms 1 and 2) produces a consistent estimator of an implicitly estimated distribution P (X), if training converges. P (X) is the asymptotic distribution of the Markov chain alternating sampling from P (X|X\u0303) and C(X\u0303|X), where C is the original local corruption process.\nProof. Consider that during training, we produce a sequence of estimators Pk(X|X\u0303) where Pk corresponds to the k-th training iteration (modifying the parameters after each iteration). With the walkback algorithm, Pk\u22121 is used to obtain the corrupted samples X\u0303 from which the next model Pk is produced. If training converges, Pk \u2248 Pk+1 = P and we can consider the corruption process C\u0303 fixed. By corollary 1, the Markov chain obtained by alternating samples from P (X|X\u0303) and samples from C\u0303(X\u0303|X) converges to an asymptotic distribution P (X) which estimates the underlying data generating distribution P(X). Furthermore, since the walkback corruption C\u0303(X\u0303|X) corresponds to a finite number of steps alternating sampling from C(X\u0303|X) (the fixed local corruption) and sampling from P (X|X\u0303), the overall sequence can be seen as a Markov chain obtained by alternatively sampling from C(X\u0303|X) and from P (X|X\u0303). Hence, once the model is trained using the walkback algorithm, one can sample from it using the fixed local corruption C(X\u0303|X).\nA consequence of that proposition is that the walkback training algorithm estimates the same distribution as the original denoising algorithm, but may possibly do it more efficiently (as we observe in the experiments), by exploring the space of corruptions in a way that spends more time where it helps most the model."}, {"heading": "4 Experimental Validation", "text": "Non-parametric case. One should observe that the mathematical results presented here apply to any denoising training criterion where the reconstruction loss can be interpreted as a negative loglikelihood. This remains true whether or not the denoising machine P (X|X\u0303) is parametrized as the composition of an encoder and decoder. This is also true of the asymptotic estimation results presented by Alain and Bengio (2013).\nWe first validate the above theorems in a case where the asymptotic limit (of enough data and enough capacity) can be reached, i.e., in a low-dimensional non-parametric setting where the learner has enough examples and a parametrization that can capture any P (X|X\u0303). Figure 2 shows the distribution recovered by the Markov chain when the data come from a discrete distribution with only 10 different values. The conditional P (X|X\u0303) was estimated by maximum likelihood (counting) from 5000 training examples, and 5000 samples were generated from the chain to estimate the asymptotic distribution \u03c0n(X). For the case of continuous data, Figure 2 shows the result of the 5000 generated samples and original training data (5000 examples) with X \u2208 R10, with scatter plots of pairs of dimensions. The estimator is also non-parametric (a Parzen density estimator of the conditional density).\nMNIST digits. We then trained a denoising auto-encoder on the binarized MNIST data (thresholding at 0.5). The 50000 training examples are used for training the auto-encoder, with salt-and-pepper noise (with probability 0.5, randomly corrupt each bit, by setting it to 1 or 0 with probability 0.5). The auto-encoder has 784 inputs (and outputs) and 2000 hidden units, and it is trained by minimizing cross-entropy loss (i.e., maximum likelihood on Bernouilli reconstruction distribution). We evaluated two setups, one with walkback training and one without. With walkback training, a chain of 5 steps was used to generate 5 corrupted examples for each training example. Figure 3 shows samples generated by both models. The quality of the samples was also estimated quantitatively by measuring the negative log-likelihood (NLL) of the test set under a Parzen density estimator constructed from 10000 generated samples. The NLLs with and without walkback are respectively 153 and 152, i.e., not distinguishible, although visual inspection suggests that the walkback algorithm produces less spurious samples. Note how these chains mix quickly. In comparison, Bengio et al. (2013a) report a NLL of 244 for RBMs using the same setting."}, {"heading": "5 Conclusion and Future Work", "text": "We have proven that training a model to denoise is a way to implicitly estimate the underlying data generating process, and a simple Markov chain that alternates sampling from the denoising model and from the corruption process converges to that estimator. That provides a means for generating data from any denoising auto-encoder (if the corruption is not degenerate, more precisely, if the above chain converges). We have validated those results empirically, both in a non-parametric setting and with real data. This study has also suggested a variant of the training procedure, called walkback training, that converges faster to the target distribution.\nOne of the insights arising out of the theoretical results presented here is that in order to reach the asymptotic limit of fully capturing the data distribution P(X), it may be necessary for the model\u2019s P (X|X\u0303) to have the ability to represent multi-modal distributions over X (given X\u0303). Another appealing line of future investigation is the extension to deep models. Although it is trivial to apply these results to a deep auto-encoder (which can be obtained by stacking shallow ones), it would be interesting to investigate architectures in which the associated sampling procedure is more like that of deep belief networks (Hinton et al., 2006), since it has clearly been shown that sampling from higher level representations yields better mixing and better samples (Bengio et al., 2013a; Luo et al., 2013)."}, {"heading": "Acknowledgments", "text": "The authors would like to acknowledge the stimulating discussions with and help from Aaron Courville, Ian Goodfellow Roland Memisevic, and Vincent Dumoulin, as well as funding from NSERC, CIFAR (YB is a CIFAR Fellow), and the Canada Research Chairs."}], "references": [{"title": "What regularized auto-encoders learn from the data generating distribution", "author": ["G. Alain", "Y. Bengio"], "venue": "International Conference on Learning Representations (ICLR\u20192013).", "citeRegEx": "Alain and Bengio,? 2013", "shortCiteRegEx": "Alain and Bengio", "year": 2013}, {"title": "Non-local manifold Parzen windows", "author": ["Y. Bengio", "H. Larochelle", "P. Vincent"], "venue": "NIPS\u201905, pages 115\u2013122. MIT Press.", "citeRegEx": "Bengio et al\\.,? 2006a", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Nonlocal estimation of manifold structure", "author": ["Y. Bengio", "M. Monperrus", "H. Larochelle"], "venue": "Neural Computation, 18(10).", "citeRegEx": "Bengio et al\\.,? 2006b", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Better mixing via deep representations", "author": ["Y. Bengio", "G. Mesnil", "Y. Dauphin", "S. Rifai"], "venue": "ICML\u20192013.", "citeRegEx": "Bengio et al\\.,? 2013a", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI).", "citeRegEx": "Bengio et al\\.,? 2013b", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "ICML\u20192012.", "citeRegEx": "Boulanger.Lewandowski et al\\.,? 2012", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Dependency networks for inference, collaborative filtering, and data visualization", "author": ["D. Heckerman", "D.M. Chickering", "C. Meek", "R. Rounthwaite", "C. Kadie"], "venue": "Journal of Machine Learning Research, 1, 49\u201375.", "citeRegEx": "Heckerman et al\\.,? 2000", "shortCiteRegEx": "Heckerman et al\\.", "year": 2000}, {"title": "Products of experts", "author": ["G.E. Hinton"], "venue": "ICANN\u20191999.", "citeRegEx": "Hinton,? 1999", "shortCiteRegEx": "Hinton", "year": 1999}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, 18, 1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Estimation of non-normalized statistical models using score matching", "author": ["A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research, 6, 695\u2013709.", "citeRegEx": "Hyv\u00e4rinen,? 2005", "shortCiteRegEx": "Hyv\u00e4rinen", "year": 2005}, {"title": "Regularized estimation of image statistics by score matching", "author": ["D. Kingma", "Y. LeCun"], "venue": "J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1126\u20131134.", "citeRegEx": "Kingma and LeCun,? 2010", "shortCiteRegEx": "Kingma and LeCun", "year": 2010}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u20192011), volume 15 of JMLR: W&CP.", "citeRegEx": "Larochelle and Murray,? 2011", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "Texture modeling with convolutional spike-and-slab RBMs and deep extensions", "author": ["H. Luo", "P.L. Carrier", "A. Courville", "Y. Bengio"], "venue": "AISTATS\u20192013.", "citeRegEx": "Luo et al\\.,? 2013", "shortCiteRegEx": "Luo et al\\.", "year": 2013}, {"title": "Sparse feature learning for deep belief networks", "author": ["M. Ranzato", "Boureau", "Y.-L.", "Y. LeCun"], "venue": "NIPS\u201907, pages 1185\u20131192, Cambridge, MA. MIT Press.", "citeRegEx": "Ranzato et al\\.,? 2008", "shortCiteRegEx": "Ranzato et al\\.", "year": 2008}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "ICML\u20192011.", "citeRegEx": "Rifai et al\\.,? 2011", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["S. Rifai", "Y. Bengio", "Y. Dauphin", "P. Vincent"], "venue": "ICML\u20192012.", "citeRegEx": "Rifai et al\\.,? 2012a", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["S. Rifai", "Y. Bengio", "Y. Dauphin", "P. Vincent"], "venue": "ICML\u20192012.", "citeRegEx": "Rifai et al\\.,? 2012b", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "On autoencoders and score matching for energy based models", "author": ["K. Swersky", "M. Ranzato", "D. Buchman", "B. Marlin", "N. de Freitas"], "venue": "In ICML\u20192011. ACM", "citeRegEx": "Swersky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2011}, {"title": "A connection between score matching and denoising autoencoders", "author": ["P. Vincent"], "venue": "Neural Computation, 23(7).", "citeRegEx": "Vincent,? 2011", "shortCiteRegEx": "Vincent", "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "Different variants of auto-encoders and sparse coding have been, along with RBMs, among the most successful building blocks in recent research in deep learning (Bengio et al., 2013b).", "startOffset": 160, "endOffset": 182}, {"referenceID": 13, "context": "Some of the first ideas about the probabilistic interpretation of auto-encoders were proposed by Ranzato et al. (2008): they were viewed as approximating an energy function through the reconstruction error, i.", "startOffset": 97, "endOffset": 119}, {"referenceID": 13, "context": "Some of the first ideas about the probabilistic interpretation of auto-encoders were proposed by Ranzato et al. (2008): they were viewed as approximating an energy function through the reconstruction error, i.e., being trained to have low reconstruction error at the training examples and high reconstruction error elsewhere (through the regularizer, e.g., sparsity or otherwise, which prevents the auto-encoder from learning the identity function). An important breakthrough then came, yielding a first formal probabilistic interpretation of regularized auto-encoders as models of the input distribution, with the work of Vincent (2011). This work showed that some denoising auto-encoders correspond to a Gaussian RBM and that minimizing the denoising reconstruction error (as a squared error) estimates the energy function through a regularized form of score matching, with the regularization disappearing as the amount of corruption noise goes to 0, and then converging to the same", "startOffset": 97, "endOffset": 638}, {"referenceID": 9, "context": "solution as score matching (Hyv\u00e4rinen, 2005).", "startOffset": 27, "endOffset": 44}, {"referenceID": 10, "context": "This connection and its generalization to other energy functions, giving rise to the general denoising score matching training criterion, is discussed in several papers (Kingma and LeCun, 2010; Swersky et al., 2011; Alain and Bengio, 2013).", "startOffset": 169, "endOffset": 239}, {"referenceID": 17, "context": "This connection and its generalization to other energy functions, giving rise to the general denoising score matching training criterion, is discussed in several papers (Kingma and LeCun, 2010; Swersky et al., 2011; Alain and Bengio, 2013).", "startOffset": 169, "endOffset": 239}, {"referenceID": 0, "context": "This connection and its generalization to other energy functions, giving rise to the general denoising score matching training criterion, is discussed in several papers (Kingma and LeCun, 2010; Swersky et al., 2011; Alain and Bengio, 2013).", "startOffset": 169, "endOffset": 239}, {"referenceID": 15, "context": "Another breakthrough has been the development of an empirically successful sampling algorithm for contractive auto-encoders (Rifai et al., 2012a), which basically involves composing encoding, decoding, and noise addition steps.", "startOffset": 124, "endOffset": 145}, {"referenceID": 0, "context": "The last step in this development (Alain and Bengio, 2013) generalized the result from Vincent (2011) by showing that when a denoising auto-encoder (or a contractive auto-encoder with the contraction on the whole encode/decode reconstruction function) is trained with small Gaussian corruption and squared error loss, it estimates the score (derivative of the log-density) of the underlying data-generating distribution, which is proportional to the difference between reconstruction and input.", "startOffset": 34, "endOffset": 58}, {"referenceID": 0, "context": "The last step in this development (Alain and Bengio, 2013) generalized the result from Vincent (2011) by showing that when a denoising auto-encoder (or a contractive auto-encoder with the contraction on the whole encode/decode reconstruction function) is trained with small Gaussian corruption and squared error loss, it estimates the score (derivative of the log-density) of the underlying data-generating distribution, which is proportional to the difference between reconstruction and input.", "startOffset": 35, "endOffset": 102}, {"referenceID": 7, "context": "We find that we can improve the sampling behavior by using the model itself to define the corruption process, yielding a training procedure that has some surface similarity to the contrastive divergence algorithm (Hinton, 1999; Hinton et al., 2006).", "startOffset": 213, "endOffset": 248}, {"referenceID": 8, "context": "We find that we can improve the sampling behavior by using the model itself to define the corruption process, yielding a training procedure that has some surface similarity to the contrastive divergence algorithm (Hinton, 1999; Hinton et al., 2006).", "startOffset": 213, "endOffset": 248}, {"referenceID": 6, "context": "In that respect, the situation is similar to the sampling procedure for dependency networks (Heckerman et al., 2000), in that the pairs (Xt, X\u0303t\u22121) are not guaranteed to have the same asymptotic distribution as the pairs (Xt, X\u0303t) as t \u2192 \u221e.", "startOffset": 92, "endOffset": 116}, {"referenceID": 1, "context": "This idea is already behind the non-local manifold Parzen windows (Bengio et al., 2006a) and non-local manifold tangent learning (Bengio et al.", "startOffset": 66, "endOffset": 88}, {"referenceID": 2, "context": ", 2006a) and non-local manifold tangent learning (Bengio et al., 2006b) algorithms: the local density around a point X\u0303 can be approximated by a multivariate Gaussian whose covariance matrix has leading eigenvectors that span the local tangent of the manifold near which the data concentrates (if it does).", "startOffset": 49, "endOffset": 71}, {"referenceID": 14, "context": "The idea of a locally Gaussian approximation of a density with a manifold structure is also exploited in the more recent work on the contractive auto-encoder (Rifai et al., 2011) and associated sampling procedures (Rifai et al.", "startOffset": 158, "endOffset": 178}, {"referenceID": 16, "context": ", 2011) and associated sampling procedures (Rifai et al., 2012b).", "startOffset": 43, "endOffset": 64}, {"referenceID": 0, "context": "that idea comes from the result from Alain and Bengio (2013): when the amount of corruption noise converges to 0 and the input variables have a smooth continuous density, then a unimodal Gaussian reconstruction density suffices to fully capture the joint distribution.", "startOffset": 37, "endOffset": 61}, {"referenceID": 7, "context": "The spirit of this procedure is thus very similar to the CD-k (Contrastive Divergence with k MCMC steps) procedure proposed to train RBMs (Hinton, 1999; Hinton et al., 2006).", "startOffset": 138, "endOffset": 173}, {"referenceID": 8, "context": "The spirit of this procedure is thus very similar to the CD-k (Contrastive Divergence with k MCMC steps) procedure proposed to train RBMs (Hinton, 1999; Hinton et al., 2006).", "startOffset": 138, "endOffset": 173}, {"referenceID": 0, "context": "This is also true of the asymptotic estimation results presented by Alain and Bengio (2013).", "startOffset": 68, "endOffset": 92}, {"referenceID": 1, "context": "In comparison, Bengio et al. (2013a) report a NLL of 244 for RBMs using the same setting.", "startOffset": 15, "endOffset": 37}, {"referenceID": 8, "context": "Although it is trivial to apply these results to a deep auto-encoder (which can be obtained by stacking shallow ones), it would be interesting to investigate architectures in which the associated sampling procedure is more like that of deep belief networks (Hinton et al., 2006), since it has clearly been shown that sampling from higher level representations yields better mixing and better samples (Bengio et al.", "startOffset": 257, "endOffset": 278}, {"referenceID": 3, "context": ", 2006), since it has clearly been shown that sampling from higher level representations yields better mixing and better samples (Bengio et al., 2013a; Luo et al., 2013).", "startOffset": 129, "endOffset": 169}, {"referenceID": 12, "context": ", 2006), since it has clearly been shown that sampling from higher level representations yields better mixing and better samples (Bengio et al., 2013a; Luo et al., 2013).", "startOffset": 129, "endOffset": 169}], "year": 2017, "abstractText": "Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).", "creator": "LaTeX with hyperref package"}}}