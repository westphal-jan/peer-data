{"id": "1306.0160", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2013", "title": "Phase Retrieval using Alternating Minimization", "abstract": "Phase updating condition relating framework formula_10 formula_5, few already soldiers sign (certain actual, brought complex comparable) information. Over the start time 1960s, whose traditional antidepressant in-depth follow to however many variety of if happens now being although is cycles minimization; see. e. helical close calculating has survivors approaches sources, from. bush solution. In this paper, nothing show that was simple alternating minimization bayesian rotationally converges able for consider beyond some usually example - - if only vector $ 42 $ from $ servicios, A $, within $ vez = | A ^ T z | $ with $ | a.k.a. | $ rank a variable has therefore - wise magnitudes though $ q $ - - later itself assumption that $ A $ one Gaussian.", "histories": [["v1", "Sun, 2 Jun 2013 00:45:12 GMT  (167kb,D)", "http://arxiv.org/abs/1306.0160v1", null], ["v2", "Fri, 12 Jun 2015 11:45:50 GMT  (402kb,D)", "http://arxiv.org/abs/1306.0160v2", "Accepted for publication in IEEE Transactions on Signal Processing"]], "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["praneeth netrapalli", "prateek jain 0002", "sujay sanghavi"], "accepted": true, "id": "1306.0160"}, "pdf": {"name": "1306.0160.pdf", "metadata": {"source": "CRF", "title": "Phase Retrieval using Alternating Minimization", "authors": ["Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi"], "emails": ["praneethn@utexas.edu", "prajain@microsoft.com", "sanghavi@mail.utexas.edu"], "sections": [{"heading": null, "text": "Empirically, our algorithm performs similar to recently proposed convex techniques for this variant (which are based on \u201clifting\u201d to a convex matrix problem) in sample complexity and robustness to noise. However, our algorithm is much more efficient and can scale to large problems. Analytically, we show geometric convergence to the solution, and sample complexity that is off by log factors from obvious lower bounds. We also establish close to optimal scaling for the case when the unknown vector is sparse. Our work represents the only known theoretical guarantee for alternating minimization for any variant of phase retrieval problems in the nonconvex setting."}, {"heading": "1 Introduction", "text": "In this paper we are interested in recovering a complex1 vector x\u2217 \u2208 Cn from magnitudes of its linear measurements. That is, for ai \u2208 Cn, if\nyi = |\u3008ai,x\u2217\u3009|, for i = 1, . . . ,m (1)\nthen the task is to recover x\u2217 using y and the measurement matrix A = [a1 a2 . . . am]. The above problem arises in many settings where it is harder / infeasible to record the phase of measurements, while recording the magnitudes is significantly easier. This problem, known as phase retrieval, is encountered in several applications in crystallography, optics, spectroscopy and tomography [33, 20]. Moreover, the problem is broadly studied in the following two settings:\n1Our results also cover the real case, i.e. where all quantities are real.\nar X\niv :1\n30 6.\n01 60\nv1 [\nst at\n.M L\n] 2\nJ un\n(i) The measurements in (1) correspond to the Fourier transform (the number of measurements here is equal to n) and there is some apriori information about the signal.\n(ii) The set of measurements y are overcomplete (i.e., m > n), while some apriori information about the signal may or may not be available.\nIn the first case, various types of apriori information about the underlying signal such as positivity, magnitude information on the signal [15], sparsity [39] and so on have been studied. In the second case, algorithms for various measurement schemes such as Fourier oversampling [34], multiple random illuminations [5, 43] and wavelet transform [9] have been suggested.\nBy and large, the most well known methods for solving this problem are the error reduction algorithms due to Gerchberg and Saxton [17] and Fienup [15], and variants thereof. These algorithms are alternating projection algorithms that iterate between the unknown phases of the measurements and the unknown underlying vector. Though the empirical performance of these algorithms has been well studied [15, 29, 30]. and they are used in many applications [31, 32], there are not many theoretical guarantees regarding their performance.\nMore recently, a line of work [8, 7, 43] has approached this problem from a different angle, based on the realization that recovering x\u2217 is equivalent to recovering the rank-one matrix x\u2217x\u2217T , i.e., its outer product. Inspired by the recent literature on trace norm relaxation of the rank constraint, they design SDPs to solve this problem. Refer Section 1.1 for more details.\nIn this paper we go back to the empirically more popular ideology of alternating minimization; we develop a new alternating minimization algorithm, for which we show that (a) empirically, it noticeably outperforms convex methods, and (b) analytically, a natural resampled version of this algorithm requires O(n log3 n) i.i.d. random Gaussian measurements to geometrically converge to the true vector. Our contribution:\n\u2022 The iterative part of our algorithm is implicit in previous work [17, 15, 43, 5]; the novelty in our algorithmic contribution is the initialization step which makes it more likely for the iterative procedure to succeed - see Figures 1 and 2.\n\u2022 Our analytical contribution is the first theoretical guarantee regarding the global convergence, and subsequent exact recovery of the signal, via alternating minimization for phase retrieval.\n\u2022 When the underlying vector is sparse, we design another algorithm that achieves a sample complexity of O ( (x\u2217min) \u22124 log n+ k ( log3 k + log 1 log log 1 )) and computational complexity\nof O (\n(x\u2217min) \u22124 kn log n+ k2 log2 1 log log 1 ) , where k is the sparsity and x\u2217min is the minimum\nnon-zero entry of x\u2217. This algorithm also runs over Cn and scales much better than SDP based methods.\nBesides being an empirically better algorithm for this problem, our work is also interesting in a broader sense: there are many problems in machine learning, signal procesing and numerical linear algebra, where the natural formulation of a problem is non-convex; examples include rank constrained problems, applications of EM algorithms etc., and alternating minimization has good empirical performance. However, the methods with the best (or only) analytical guarantees involve convex relaxations (e.g., by relaxing the rank constraint and penalizing the trace norm). In most\nof these settings, correctness of alternating minimization is an open question. We believe that our results in this paper are of interest, and may have implications, in this larger context.\nThe rest of the paper is organized as follows: In section 1.1, we briefly review related work. We clarify our notation in Section 2. We present our algorithm in Section 3 and the main results in Section 4. We present our results for the sparse case in Section 5. Finally, we present experimental results in Section 6."}, {"heading": "1.1 Related Work", "text": "Phase Retrieval via Non-Convex Procedures: Inspite of the huge amount of work it has attracted, phase retrieval has been a long standing open problem. Early work in this area focused on using holography to capture the phase information along with magnitude measurements [16, 26]. However, computational methods for reconstruction of the signal using only magnitude measurements received a lot of attention due to their applicability in resolving spurious noise, fringes, optical system aberrations and so on and difficulties in the implementation of interferometer setups [11]. Though such methods have been developed to solve this problem in various practical settings [10, 14, 31, 32], our theoretical understanding of this problem is still far from complete. Many papers [4, 18, 37] have focused on determining conditions under which (1) has a unique solution. However, the uniqueness results of these papers do not resolve the algorithmic question of how to find the solution to (1).\nSince the seminal work of Gerchberg and Saxton [17] and Fienup [15], many iterated projection algorithms have been developed targeted towards various applications [1, 13, 2]. [34] first suggested the use of multiple magnitude measurements to resolve the phase problem. This approach has been successfully used in many practical applications - see [11] and references there in. Following the empirical success of these algorithms, researchers were able to explain its success in some of the instances [44, 41] using Bregman\u2019s theory of iterated projections onto convex sets [3]. However, many instances, such as the one we consider in this paper, are out of reach of this theory since they involve magnitude constraints which are non-convex. To the best of our knowledge, there are no theoretical results on the convergence of these approaches in a non-convex setting.\nPhase Retrieval via Convex Relaxation: An interesting recent approach for solving this problem formulates it as one of finding the rank-one solution to a system of linear matrix equations. The papers [8, 7] then take the approach of relaxing the rank constraint by a trace norm penalty, making the overall algorithm a convex program (called PhaseLift) over n \u00d7 n matrices. Another recent line of work [43] takes a similar but different approach : it uses an SDP relaxation (called PhaseCut) that is inspired by the classical SDP relaxation for the max-cut problem. To date, these convex methods are the only ones with analytical guarantees on statistical performance [6, 43] (i.e. the number m of measurements required to recover x\u2217) \u2013 under an i.i.d. random Gaussian model on the measurement vectors ai. However, by \u201clifting\u201d a vector problem to a matrix one, these methods lead to a much larger representation of the state space, and higher computational cost as a result.\nSparse Phase Retrieval: A special case of the phase retrieval problem which has received a lot of attention recently is when the underlying signal x\u2217 is known to be sparse. Though this problem is closely related to the compressed sensing problem, lack of phase information makes this harder. However, the `1 regularization approach of compressed sensing has been successfully used in this setting as well. In particular, if x\u2217 is sparse, then the corresponding lifted matrix x\u2217x\u2217T is also sparse. [39, 35, 28] use this observation to design `1 regularized SDP algorithms for phase\nretrieval of sparse vectors. For random Gaussian measurements, [28] shows that `1 regularized PhaseLift recovers x\u2217 correctly if the number of measurements is \u2126(k2 log n). By the results of [36], this result is tight up to logarithmic factors for `1 and trace norm regularized SDP relaxations. [21, 38] develop algorithms for phase retrieval from Fourier magnitude measurements. However, achieving the optimal sample complexity of O ( k log nk ) is still open [12].\nAlternating Minimization (a.k.a. ALS): Alternating minimization has been successfully applied to many applications in the low-rank matrix setting. For example, clustering [25], sparse PCA [45], non-negative matrix factorization [24], signed network prediction [19] etc. However, despite empirical success, for most of the problems, there are no theoretical guarantees regarding its convergence except to a local minimum. The only exceptions are the results in [23, 22] which give provable guarantees for alternating minimization for the problems of matrix sensing and matrix completion."}, {"heading": "2 Notation", "text": "We use bold capital letters (A,B etc.) for matrices, bold small case letters (x,y etc.) for vectors and non-bold letters (\u03b1,U etc.) for scalars. For every complex vector w \u2208 Cn, |w| \u2208 Rn denotes its element-wise magnitude vector. wT and AT denote the Hermitian transpose of the vector w and the matrix A respectively. e1, e2, etc. denote the canonical basis vectors in Cn. z denotes the complex conjugate of the complex number z. In this paper we use the standard Gaussian (or normal) distribution over Cn. a is said to be distributed according to this distribution if a = a1+ia2, where a1 and a2 are independent and are distributed according to N (0, I). We also define Ph (z) def = z|z|\nfor every z \u2208 C, and dist (w1,w2) def = \u221a 1\u2212 \u2223\u2223\u2223 \u3008w1,w2\u3009\u2016w1\u20162\u2016w2\u20162 \u2223\u2223\u22232 for every w1,w2 \u2208 Cn. inally, we use the shorthand wlog for without loss of generality and whp for with high probability."}, {"heading": "3 Algorithm", "text": "In this section, we present our alternating minimization based algorithm for solving the phase retrieval problem. Let A \u2208 Cn\u00d7m be the measurement matrix, with ai as its ith column; similarly let y be the vector of recorded magnitudes. Then,\ny = |ATx\u2217 |.\nRecall that, given y and A, the goal is to recover x\u2217. If we had access to the true phase c\u2217 of ATx\u2217 (i.e., c\u2217i = Ph (\u3008ai,x\u2217\u3009)) and m \u2265 n, then our problem reduces to one of solving a system of linear equations:\nC\u2217y = ATx\u2217,\nwhere C\u2217 def = Diag(c\u2217) is the diagonal matrix of phases. Of course we do not know C\u2217, hence one approach to recovering x\u2217 is to solve:\nargmin C,x\n\u2016ATx\u2212Cy\u20162, (2)\nwhere x \u2208 Cn and C \u2208 Cm\u00d7m is a diagonal matrix with each diagonal entry of magnitude 1. Note that the above problem is not convex since C is restricted to be a diagonal phase matrix and hence, one cannot use standard convex optimization methods to solve it.\nAlgorithm 1 AltMinPhase\ninput A,y, t0 1: Initialize x0 \u2190 top singular vector of \u2211 i y 2 i aiai T\n2: for t = 0, \u00b7 \u00b7 \u00b7 , t0 \u2212 1 do 3: Ct+1 \u2190 Diag ( Ph ( ATxt )) 4: xt+1 \u2190 argminx\u2208Rn \u2225\u2225ATx\u2212Ct+1y\u2225\u2225 2\n5: end for output xt0\nInstead, our algorithm uses the well-known alternating minimization: alternatingly update x and C so as to minimize (2). Note that given C, the vector x can be obtained by solving the following least squares problem: minx \u2016ATx \u2212 Cy\u20162. Since the number of measurements m is larger than the dimensionality n and since each entry of A is sampled from independent Gaussians, A is invertible with probability 1. Hence, the above least squares problem has a unique solution. On the other hand, given x, the optimal C is given by C = Diag(ATx).\nWhile the above algorithm is simple and intuitive, it is known that with bad initial points, the solution might not converge to x\u2217. In fact, this algorithm with a uniformly random initial point has been empirically evaluated for example in [43], where it performs worse than SDP based methods. Moreover, since the underlying problem is non-convex, standard analysis techniques fail to guarantee convergence to the global optimum, x\u2217. Hence, the key challenges here are: a) a good initialization step for this method, b) establishing this method\u2019s convergence to x\u2217.\nWe address the first key challenge in our AltMinPhase algorithm (Algorithm 1) by initializing x as the largest singular vector of the matrix S = 1m \u2211 i yiaiai\nT . Theorem 4.1 shows that when A is sampled from standard complex normal distribution, this initialization is accurate. In particular, if m \u2265 C1n log3 n for large enough C1 > 0, then whp we have \u2016x0 \u2212 x\u2217\u20162 \u2264 1/100 (or any other constant).\nTheorem 4.2 addresses the second key challenge and shows that a variant of AltMinPhase (see Algorithm 2) actually converges to the global optimum x\u2217 at linear rate. See section 4 for a detailed analysis of our algorithm.\nWe would like to stress that not only does a natural variant of our proposed algorithm have rigorous theoretical guarantees, it also is effective practically as each of its iterations is fast, has a closed form solution and does not require SVD computation. AltMinPhase has similar statistical complexity to that of PhaseLift and PhaseCut while being much more efficient computationally. In particular, for accuracy , we only need to solve each least squares problem only up to accuracy O ( ). Now, since the measurement matrix A is sampled from Gaussian with m > Cn, it is well conditioned. Hence, using conjugate gradient method, each such step takes O ( mn log 1 ) time. When m = O (n) and we have geometric convergence, the total time taken by the algorithm is O ( n2 log2 1 ) . SDP based methods on the other hand require \u2126(n3/ \u221a ) time. Moreover, our initialization step increases the likelihood of successful recovery as opposed to a random initialization (which has been considered so far in prior work). Refer Figure 1 for an empirical validation of these claims."}, {"heading": "4 Main Results: Analysis", "text": "In this section we describe the main contribution of this paper: provable statistical guarantees for the success of alternating minimization in solving the phase recovery problem. To this end, we consider the setting where each measurement vector ai is iid and is sampled from the standard complex normal distribution. We would like to stress that all the existing guarantees for phase recovery also use exactly the same setting [7, 6, 43]. Table 1 presents a comparison of the theoretical guarantees of Algorithm 2 as compared to PhaseLift and PhaseCut.\nOur proof for convergence of alternating minimization can be broken into two key results. We first show that if m \u2265 Cn log3 n, then whp the initialization step used by AltMinPhase returns x0 which is at most a constant distance away from x\u2217. Furthermore, that constant can be controlled by using more samples (see Theorem 4.1).\nWe then show that if xt is a fixed vector such that dist ( xt,x\u2217 ) < c (small enough) and A is sampled independently of xt with m > Cn (C large enough) then whp xt+1 satisfies: dist ( xt+1,x\u2217 ) < 34dist ( xt,x\u2217 ) (see Theorem 4.2). Note that our analysis critically requires xt\nto be \u201cfixed\u201d and be independent of the sample matrix A. Hence, we cannot re-use the same A in each iteration; instead, we need to resample A in every iteration. Using these results, we prove the correctness of Algorithm 2, which is a natural resampled version of AltMinPhase.\nAlgorithm 2 AltMinPhase with Resampling\ninput A,y, 1: t0 \u2190 c log 1 2: Partition y and (the corresponding columns of) A into t0 + 1 equal disjoint sets:\n(y0,A0), (y1,A1), \u00b7 \u00b7 \u00b7 , (yt0 ,At0) 3: x0 \u2190 top singular vector of \u2211 l ( y0l )2 a0` ( a0` )T 4: for t = 0, \u00b7 \u00b7 \u00b7 , t0 \u2212 1 do 5: Ct+1 \u2190 Diag ( Ph (( At+1 )T xt ))\n6: xt+1 \u2190 argminx\u2208Rn \u2225\u2225\u2225(At+1)T x\u2212Ct+1yt+1\u2225\u2225\u2225 2 7: end for\noutput xt0\nWe now present the two results mentioned above. All the supporting lemmas and proofs can be found in Appendix A. In the following theorems, wlog, we assume that \u2016x\u2217\u20162 = 1. Our first result guarantees a good initial vector.\nTheorem 4.1. There exists a constant C1 such that if m > C1 c2 n log3 n, then in Algorithm 2, with probability greater than 1\u2212 4/n2 we have:\n\u2016x0 \u2212 x\u2217\u20162 < c.\nThe second result proves geometric decay of error assuming a good initialization. Theorem 4.2. There exist constants c, c\u0302 and c\u0303 such that in iteration t of Algorithm 2, if dist ( xt,x\u2217 ) <\nc and the number of columns of At is greater than c\u0302 (\nlog 1\u03b7\n) n then, with probability more than 1\u2212\u03b7,\nwe have:\ndist ( xt+1,x\u2217 ) < 3\n4 dist\n( xt,x\u2217 ) , and \u2016xt+1 \u2212 x\u2217\u20162 < c\u0303 dist ( xt,x\u2217 ) .\nProof. For simplicity of notation in the proof of the theorem, we will use A for At+1, C for Ct+1, x for xt, x+ for xt+1, and y for yt+1. Now consider the update in the (t+ 1)th iteration:\nx+ = argmin x\u0303\u2208Rn\n\u2225\u2225AT x\u0303\u2212Cy\u2225\u2225 2 = ( AAT )\u22121 ACy = ( AAT )\u22121 ADATx\u2217, (3)\nwhere D is a diagonal matrix with Dll def = Ph ( a` Tx \u00b7 a`Tx\u2217 ) . Now (3) can be rewritten as:\nx+ = ( AAT )\u22121 ADATx\u2217 = x\u2217 + ( AAT )\u22121 A (D\u2212 I)ATx\u2217, (4)\nthat is, x+ can be viewed as a perturbation of x\u2217 and the goal is to bound the error term (the second term above). We break the proof into two main steps:\n1. \u2203 a constant c1 such that |\u3008x\u2217,x+\u3009| \u2265 1\u2212 c1dist (x,x\u2217) (see Lemma A.2), and\n2. |\u3008z,x+\u3009| \u2264 59dist (x,x \u2217), for all z s.t. zTx\u2217 = 0. (see Lemma A.4)\nAssuming the above two bounds and choosing c < 1100c1 , we can prove the theorem:\ndist ( x+,x\u2217 )2 <\n(25/81) \u00b7 dist (x,x\u2217) (1\u2212 c1dist (x,x\u2217))2 \u2264 9 16 dist (x,x\u2217)2 ,\nproving the first part of the theorem. The second part follows easily from (3) and Lemma A.2.\nIntuition and key challenge: If we look at step 6 of Algorithm 2, we see that, for the measurements, we use magnitudes calculated from x\u2217 and phases calculated from x. Intuitively, this means that we are trying to push x+ towards x\u2217 (since we use its magnitudes) and x (since we use its phases) at the same time. The key intuition behind the success of this procedure is that the push towards x\u2217 is stronger than the push towards x, when x is close to x\u2217. The key lemma that captures this effect is stated below:\nLemma 4.3. Let w1 and w2 be two independent standard complex Gaussian random variables 2. Let U = |w1|w2 ( Ph ( 1 + \u221a 1\u2212\u03b12w2 \u03b1|w1| ) \u2212 1 ) . Fix \u03b4 > 0. Then, there exists a constant \u03b3 > 0 such that if \u221a 1\u2212 \u03b12 < \u03b3, then: E [U ] \u2264 (1 + \u03b4) \u221a 1\u2212 \u03b12.\nSee Appendix A for a proof of the above lemma and how we use it to prove Theorem 4.2. Combining Theorems 4.1 and 4.2, we have the following theorem establishing the correctness\nof Algorithm 2.\nTheorem 4.4. Suppose the measurement vectors in (1) are independent standard complex normal vectors. For every \u03b7 > 0, there exists a constant c such that if m > cn ( log3 n+ log 1 log log 1 ) then, with probability greater than 1\u2212 \u03b7, Algorithm 2 outputs xt0 such that \u2016xt0 \u2212 x\u2217\u20162 < ."}, {"heading": "5 Sparse Phase Retrieval", "text": "In this section, we consider the case where x\u2217 is known to be sparse, with sparsity k. A natural and practical question to ask here is: can the sample and computational complexity of the recovery algorithm be improved when k n.\nRecently, [28] studied this problem for Gaussian A and showed that for `1 regularized PhaseLift, m = O(k2 log n) samples suffice for exact recovery of x\u2217. However, the computational complexity of this algorithm is still O(n3/ 2).\nIn this section, we provide a simple extension of our AltMinPhase algorithm that we call SparseAltMinPhase, for the case of sparse x\u2217. The main idea behind our algorithm is to first recover the support of x\u2217. Then, the problem reduces to phase retrieval of a k-dimensional signal. We then solve the reduced problem using Algorithm 2. The pseudocode for SparseAltMinPhase is presented in Algorithm 3. Table 2 provides a comparison of Algorithm 3 with `1-regularized PhaseLift in terms of sample complexity as well as computational complexity.\nThe following lemma shows that if the number of measurements is large enough, step 1 of SparseAltMinPhase recovers the support of x\u2217 correctly.\n2z is standard complex Gaussian if z = z1+iz2 where z1 and z2 are independent standard normal random variables.\nAlgorithm 3 SparseAltMinPhase\ninput A,y, k 1: S \u2190 top-k argmaxj\u2208[n] \u2211m i=1 |aijyi| {Pick indices of k largest absolute value inner product}\n2: Apply Algorithm 2 on AS ,yS and output the resulting vector with elements in S c set to zero.\nSample complexity Comp. complexity Algorithm 3 O ( k ( k log n+ log3 k + log 1 log log 1 )) O ( k2 ( kn log n+ log2 1 log log 1 )) `1-PhaseLift [28] O ( k2 log n ) O ( n3/ 2\n)\nTable 2: Comparison of Algorithm 3 with `1-PhaseLift when x \u2217 min = \u2126\n( 1/ \u221a k ) . Note that the\ncomplexity of Algorithm 3 is dominated by the support finding step. If k = O (1), Algorithm 3 runs in quasi-linear time.\nLemma 5.1. Suppose x\u2217 is k-sparse with support S and \u2016x\u2217\u20162 = 1. If ai are standard complex Gaussian random vectors and m > c\n(x\u2217min) 4 log\nn \u03b4 , then Algorithm 3 recovers S with probability greater\nthan 1\u2212 \u03b4, where x\u2217min is the minimum non-zero entry of x\u2217.\nThe key step of our proof is to show that if j \u2208 supp(x\u2217), then random variable Zij = \u2211\ni |aijyi| has significantly higher mean than for the case when j /\u2208 supp(x\u2217). Now, by applying appropriate concentration bounds, we can ensure that minj\u2208supp(x\u2217) |Zij | > maxj /\u2208supp(x\u2217) |Zij | and hence our algorithm never picks up an element outside the true support set supp(x\u2217). See Appendix B for a detailed proof of the above lemma.\nThe correctness of Algorithm 3 now is a direct consequence of Lemma 5.1 and Theorem 4.4. For the special case where each non-zero value in x\u2217 is from {\u2212 1\u221a\nk , 1\u221a k }, we have the following\ncorollary:\nCorollary 5.2. Suppose x\u2217 is k-sparse with non-zero elements \u00b1 1\u221a k . If the number of measurements m > c ( k2 log n\u03b4 + k log 2 k + k log 1 ) , then Algorithm 3 will recover x\u2217 up to accuracy with probability greater than 1\u2212 \u03b4."}, {"heading": "6 Experiments", "text": "In this section, we present experimental evaluation of AltMinPhase (Algorithm 1) and compare its performance with the SDP based methods PhaseLift [7] and PhaseCut [43]. We also empirically demonstrate the advantage of our initialization procedure over random initialization (denoted by AltMin (random init)), which has thus far been considered in the literature [17, 15, 43, 5]. AltMin (random init) is the same as AltMinPhase except that step 1 of Algorithm 1 is replaced with:x0 \u2190 Uniformly random vector from the unit sphere.\nIn the noiseless setting, a trial is said to succeed if the output x satisfies \u2016x\u2212 x\u2217\u20162 < 10\u22122. For a given dimension, we do a linear search for smallest m (number of samples) such that empirical success ratio over 20 runs is at least 0.8. We implemented our methods in Matlab, while we obtained the code for PhaseLift and PhaseCut from the authors of [35] and [43] respectively.\nWe now present results from our experiments in three different settings. Independent Random Gaussian Measurements: Each measurement vector ai is generated from the standard complex Gaussian distribution. This measurement scheme was first suggested\nby [7] and till date, this is the only scheme with theoretical guarantees. Multiple Random Illumination Filters: We now present our results for the setting where the measurements are obtained using multiple illumination filters; this setting was suggested by [5]. In particular, choose J vectors z(1), \u00b7 \u00b7 \u00b7 , z(J) and compute the following discrete Fourier transforms:\nx\u0302(u) = DFT ( x\u2217 \u00b7 \u2217 z(u) ) ,\nwhere \u00b7\u2217 denotes component-wise multiplication. Our measurements will then be the magnitudes of components of the vectors x\u0302(1), \u00b7 \u00b7 \u00b7 , x\u0302(J). The above measurement scheme can be implemented by modulating the light beam or by the use of masks; see [5] for more details.\nFor this setting, we conduct a similar set of experiments as the previous setting. That is, we vary dimensionality of the true signal z(u) (generated from the Gaussian distribution)and then empirically determine measurement and computational cost of each algorithm. Figures 2 (a) and (b) present our experimental results for this measurement scheme. Here again, we make similar observations as the last setting. That is, the measurement complexity of AltMinPhase is similar to PhaseCut and PhaseLift, but AltMinPhase is orders of magnitude faster than PhaseLift and PhaseCut. Note that Figure 2 is on a log-scale.\nNoisy Phase Retrieval: Finally, we study our method in the following noisy measurement scheme:\nyi = |\u3008ai,x\u2217 + wi\u3009| for i = 1, . . . ,m, (5)\nwhere wi is the noise in the i-th measurement and is sampled from N (0, \u03c32). We fix n = 64 and m = 6n. We then vary the amount of noise added \u03c3 and measure the `2 error in recovery, i.e., \u2016x \u2212 x\u2217\u20162, where x is the recovered vector. Figure 2(c) compares the performance of various methods with varying amount of noise. We observe that our method outperforms PhaseLift and has similar recovery error as PhaseCut."}, {"heading": "A Proofs for Section 4", "text": "A.1 Proof of the Initialization Step Proof of Theorem 4.1. Recall that x0 is the top singular vector of S = 1n \u2211\n` |a`Tx\u2217|2a`a`T . As a` are rotationally invariant random variables, wlog, we can assume that x\u2217 = e1 where e1 is the first canonical basis vector. Also note that E [ |\u3008a, e1\u3009|2aaT ] = D, where D is a diagonal matrix with D11 = Ea\u223cNC(0,1)[|a| 4] = 8 and Dii = Ea\u223cNC(0,1),b\u223cNC(0,1)[|a|\n2|b|2] = 1,\u2200i > 1. We break our proof of the theorem into two steps:\n(1): Show that, with probability > 1\u2212 2 n2 : \u2016S\u2212D\u20162 < c/4. (2): Use (1) to prove the theorem.\nProof of Step (2): We have \u3008x0,Sx0\u3009 \u2264 c/4+3 (( x0 )T\ne1\n)2 + \u2211n\ni=2(x 0 i) 2 = c/4+2\n(( x0 )T\ne1\n)2 +\n1. On the other hand, since x0 is the top singular value of S, by using triangle inequality, we have \u3008x0,Sx0\u3009 > 3\u2212 c/4. Hence, \u3008x0, e1\u30092 > 1\u2212 c/2. This yields \u2016x0 \u2212 x\u2217\u201622 = 2\u2212 2\u3008x0, e1\u30092 < c.\nProof of Step (1): We now complete our proof by proving (1). To this end, we use the following matrix concentration result from [40]:\nTheorem A.1 (Theorem 1.5 of [40]). Consider a finite sequence Xi of self-adjoint independent random matrices with dimensions n\u00d7n. Assume that E[Xi] = 0 and \u2016Xi\u20162 \u2264 R,\u2200i, almost surely. Let \u03c32 := \u2016 \u2211 i E[Xi]\u20162. Then the following holds \u2200\u03bd \u2265 0:\nP ( \u2016 1 m m\u2211 i=1 Xi\u20162 \u2265 \u03bd ) \u2264 2n exp ( \u2212m2\u03bd2 \u03c32 +Rm\u03bd/3 ) .\nNote that Theorem A.1 assumes max` |a1`|2\u2016a`\u20162 to be bounded, where a1` is the first component of a`. However, a` is a normal random variable and hence can be unbounded. We address this issue by observing that probability that Pr(\u2016a`\u20162 \u2265 2n OR |a1`|2 \u2265 2 logm) \u2264 2 exp(\u2212n/2) + 1m2 . Hence, for large enough n, c\u0302 and m > c\u0302n, w.p. 1\u2212 3\nm2 ,\nmax ` |a1`|2\u2016a`\u20162 \u2264 4n log(m). (6)\nNow, consider truncated random variable a\u0303` s.t. a\u0303` = a` if |a1`|2 \u2264 2 log(m)&\u2016a`\u20162 \u2264 2n and a\u0303` = 0 otherwise. Now, note that a\u0303` is symmetric around origin and also E[a\u0303i`a\u0303j`] = 0,\u2200i 6= j. Also, E[|a\u0303i`|2] \u2264 1. Hence, \u2016E[|a\u03031`|2\u2016a\u0303`\u20162a\u0303`a\u0303\u2020`]\u20162 \u2264 4n log(m). Now, applying Theorem A.1 given above, we get (w.p. \u2265 1\u2212 1/n2)\n\u2016 1 m \u2211 ` |a\u03031`|2a\u0303`a\u0303\u2020` \u2212 E[|a\u03031`| 2a\u0303`a\u0303 \u2020 `]\u20162 \u2264 4n log(n) log(m)\u221a m .\nFurthermore, a` = a\u0303` with probability larger than 1\u2212 3m2 . Hence, w.p. \u2265 1\u2212 4 n2 :\n\u2016S \u2212 E[|a\u03031` |2a\u0303`a\u0303 \u2020 `]\u20162 \u2264 4n log(n) log(m)\u221a m .\nNow, the remaining task is to show that \u2016E[|a\u03031` |2a\u0303`a\u0303 \u2020 `] \u2212 E[|a 1 ` |2a`a \u2020 `]\u20162 \u2264 1 m . This follows easily by observing that E[a\u0303i`a\u0303 j ` ] = 0 and by bounding E[|a\u0303 1 ` |2|a\u0303i`|2 \u2212 |a1` |2|ai`|2 \u2264 1/m by using a simple second and fourth moment calculations for the normal distribution.\nA.2 Proof of per step reduction in error\nIn all the lemmas in this section, \u03b4 is a small numerical constant (can be taken to be 0.01).\nLemma A.2. Assume the hypothesis of Theorem 4.2 and let x+ be as defined in (3). Then, there exists an absolute numerical constant c such that the following holds (w.p. \u2265 1 \u2212 \u03b74 ):\u2225\u2225\u2225(AAT )\u22121A (D\u2212 I)ATx\u2217\u2225\u2225\u2225\n2 < cdist (x\u2217,x) .\nProof. Using (4) and the fact that \u2016x\u2217\u20162 = 1, x\u2217Tx+ = 1 + x\u2217T ( AAT )\u22121 A (D\u2212 I)ATx\u2217. That\nis, |x\u2217Tx+| \u2265 1\u2212\u2016 (\n1 2mAA T )\u22121 \u20162\u2016 1\u221a2mA\u20162\u2016 1\u221a2m (D\u2212 I)ATx\u2217\u20162. Now, using standard bounds on\nthe singular values of Gaussian matrices [42] and assuming m > c\u0302 log 1\u03b7n, we have (w.p. \u2265 1\u2212 \u03b7 4 ): \u2016 (\n1 2mAA T )\u22121 \u20162 \u2264 1/(1 \u2212 2/\u221ac\u0302)2 and \u2016A\u20162 \u2264 1 + 2/\u221ac\u0302. Note that both the quantities can be bounded by constants that are close to 1 by selecting a large enough c\u0302. Also note that 12mAA T converges to I (the identity matrix), or equivalently 1mAA T converges to 2I since the elements of A are standard normal complex random variables and not standard normal real random variables. The key challenge now is to bound \u2225\u2225(D\u2212 I)ATx\u2217\u2225\u2225 2 by c \u221a mdist ( x\u2217,xt ) for a global constant\nc > 0. Note that since (4) is invariant with respect to \u2225\u2225xt\u2225\u2225\n2 , we can assume that \u2225\u2225xt\u2225\u2225 2\n= 1. Note further that, since the distribution of A is rotationally invariant and is independent of x\u2217 and xt, wlog, we can assume that x\u2217 = e1 and x t = \u03b1e1 + \u221a\n1\u2212 \u03b12e2, where \u03b1 = \u3008xt,x\u2217\u3009. Hence, \u2225\u2225(D\u2212 I)ATe1\u2225\u222522 = \u2211ml=1 |a1l|2 \u2223\u2223\u2223Ph((\u03b1a1l +\u221a1\u2212 \u03b12a2l) a1l)\u2212 1\u2223\u2223\u22232 = \u2211ml=1 U`, where Ul is given by,\nUl def = |a1l|2 \u2223\u2223\u2223Ph((\u03b1a1l +\u221a1\u2212 \u03b12a2l) a1l)\u2212 1\u2223\u2223\u22232 . (7) Using Lemma A.3 finishes the proof.\nThe following lemma, Lemma A.3 shows that if U` are as defined in Lemma A.2 then, the sum of U`, 1 \u2264 ` \u2264 m concentrates well around E [U`] and also E [U`] \u2264 c \u221a mdist ( x\u2217,xt ) . The proof of Lemma A.3 requires careful analysis as it provides tail bound and expectation bound of a random variable that is a product of correlated sub-exponential complex random variables.\nLemma A.3. Assume the hypothesis of Lemma A.2. Let U` be as defined in (7) and let each a1l, a2l, \u22001 \u2264 l \u2264 m be sampled from standard normal distribution for complex numbers. Then, with probability greater than 1\u2212 \u03b74 , we have: \u2211m l=1 Ul \u2264 c2m(1\u2212 \u03b12), for a global constant c > 0.\nProof of Lemma A.3. We first estimate P [Ul > t] so as to:\n1. Calculate E [Ul] and,\n2. Show that Ul is a subexponential random variable and use that fact to derive concentration bounds.\nNow, P [Ul > t] = \u222b\u221e\u221a\nt 2\np|a1l|(s)P [ Wl > \u221a t s \u2223\u2223\u2223|a1l| = s] ds, where, Wl def = \u2223\u2223\u2223Ph((\u03b1a1l +\u221a1\u2212 \u03b12a2l) a1l)\u2212 1\u2223\u2223\u2223 .\nP [ Wl > \u221a t\ns \u2223\u2223\u2223\u2223|a1l| = s] = P [\u2223\u2223\u2223Ph((\u03b1a1l +\u221a1\u2212 \u03b12a2l) a1l)\u2212 1\u2223\u2223\u2223 > \u221ats \u2223\u2223\u2223\u2223|a1l| = s]\n= P [\u2223\u2223\u2223\u2223\u2223Ph ( 1 + \u221a 1\u2212 \u03b12a2l \u03b1a1l ) \u2212 1 \u2223\u2223\u2223\u2223\u2223 > \u221a t s \u2223\u2223\u2223\u2223\u2223|a1l| = s ]\n(\u03b61)\n\u2264 P [\u221a 1\u2212 \u03b12 |a2l| \u03b1 |a2l| > c \u221a t s \u2223\u2223\u2223\u2223\u2223|a1l| = s ]\n= P [ |a2l| > c\u03b1 \u221a t\u221a\n1\u2212 \u03b12 ] (\u03b62)\n\u2264 exp ( 1\u2212 c\u03b1 2t\n1\u2212 \u03b12\n) ,\nwhere (\u03b61) follows from Lemma A.7 and (\u03b62) follows from the fact that a2l is a sub-gaussian random variable. So we have:\nP [Ul > t] \u2264 \u222b \u221e \u221a t\n2\nexp ( 1\u2212 c\u03b1 2t\n1\u2212 \u03b12\n) ds = exp ( 1\u2212 c\u03b1 2t\n1\u2212 \u03b12 )\u222b \u221e \u221a t\n2\nse\u2212 s2 2 ds = exp ( 1\u2212 ct\n1\u2212 \u03b12\n) .\n(8)\nUsing this, we have the following bound on the expected value of Ul: E [Ul] = \u222b \u221e 0 P [Ul > t] dt \u2264 \u222b \u221e 0 exp ( 1\u2212 ct 1\u2212 \u03b12 ) dt \u2264 c ( 1\u2212 \u03b12 ) . (9)\nFrom (8), we see that Ul is a subexponential random variable with parameter c ( 1\u2212 \u03b12 ) . Using Proposition 5.16 from [42], we obtain:\nP [\u2223\u2223\u2223\u2223\u2223 m\u2211 l=1 Ul \u2212 E [Ul] \u2223\u2223\u2223\u2223\u2223 > \u03b4m (1\u2212 \u03b12) ] \u2264 2 exp ( \u2212min ( c\u03b42m2 ( 1\u2212 \u03b12 )2 (1\u2212 \u03b12)2m , c\u03b4m ( 1\u2212 \u03b12 ) 1\u2212 \u03b12 )) \u2264 2 exp ( \u2212c\u03b42m ) \u2264 \u03b7\n4 .\nSo, with probability greater than 1\u2212 \u03b74 , we have: m\u2211 l=1 Ul \u2264 c2m(1\u2212 \u03b12).\nThis proves the lemma.\nLemma A.4. Assume the hypothesis of Theorem 4.2 and let x+ be as defined in (3). Then, \u2200z s.t. \u3008z,x\u2217\u3009 = 0, the following holds (w.p. \u2265 1\u2212 \u03b74e \u2212n): |\u3008z,x+\u3009| \u2264 59dist (x \u2217,x). Proof. Fix z such that \u3008z,x\u2217\u3009 = 0. Since the distribution of A is rotationally invariant, wlog we can assume that: a) x\u2217 = e1, b) x = \u03b1e1 + \u221a 1\u2212 \u03b12e2 where \u03b1 \u2208 R and \u03b1 \u2265 0 and c)\nz = \u03b2e2 + \u221a\n1\u2212 |\u03b2|2e3 for some \u03b2 \u2208 C. Note that we first prove the lemma for a fixed z and then using union bound, we obtain the result \u2200z \u2208 Cn. We have:\u2223\u2223\u3008z,x+\u3009\u2223\u2223 \u2264 |\u03b2| |\u3008e2,x+\u3009|+\u221a1\u2212 |\u03b2|2|\u3008e3,x+\u3009|. (10)\nNow,\u2223\u2223e2Tx+\u2223\u2223 = \u2223\u2223\u2223e2T (AAT )\u22121A (D\u2212 I)ATe1\u2223\u2223\u2223 \u2264 1\n2m \u2223\u2223\u2223\u2223\u2223e2T (( 1 2m AAT )\u22121 \u2212 I ) A (D\u2212 I)ATe1 \u2223\u2223\u2223\u2223\u2223+ 12m \u2223\u2223e2TA (D\u2212 I)ATe1\u2223\u2223 \u2264 1\n2m \u2225\u2225\u2225\u2225\u2225 ( 1 2m AAT )\u22121 \u2212 I \u2225\u2225\u2225\u2225\u2225 2 \u2016A\u20162 \u2225\u2225(D\u2212 I)ATe1\u2225\u22252 + 12m \u2223\u2223e2TA (D\u2212 I)ATe1\u2223\u2223 ,\n\u2264 4c\u221a c\u0302 dist\n( xt,x\u2217 ) + 1\n2m \u2223\u2223e2TA (D\u2212 I)ATe1\u2223\u2223 , (11) where the last inequality follows from the proof of Lemma A.2.\nSimilarly,\u2223\u2223e3Tx+\u2223\u2223 = \u2223\u2223\u2223e3T (AAT )\u22121A (D\u2212 I)ATe1\u2223\u2223\u2223 \u2264 1\n2m \u2223\u2223\u2223\u2223\u2223e3T (( 1 2m AAT )\u22121 \u2212 I ) A (D\u2212 I)ATe1 \u2223\u2223\u2223\u2223\u2223+ 12m \u2223\u2223e3TA (D\u2212 I)ATe1\u2223\u2223 \u2264 1\n2m \u2225\u2225\u2225\u2225\u2225 ( 1 2m AAT )\u22121 \u2212 I \u2225\u2225\u2225\u2225\u2225 2 \u2016A\u20162 \u2225\u2225(D\u2212 I)ATe1\u2225\u22252 + 12m \u2223\u2223e3TA (D\u2212 I)ATe1\u2223\u2223\n\u2264 4c\u221a c\u0302 dist\n( xt,x\u2217 ) + 1\n2m \u2223\u2223e3TA (D\u2212 I)ATe1\u2223\u2223 , (12) Again, the last inequality follows from the proof of Lemma A.2. The lemma now follows by using (10), (11), (12) along with Lemmas A.5 and A.6.\nLemma A.5. Assume the hypothesis of Theorem 4.2 and the notation therein. Then,\u2223\u2223e2TA (D\u2212 I)ATe1\u2223\u2223 \u2264 100 99 m \u221a 1\u2212 \u03b12,\nwith probability greater than 1\u2212 \u03b710e \u2212n.\nProof. We have:\ne2 TA (D\u2212 I)ATe1 = m\u2211 l=1 a1la2l ( Ph (( \u03b1a1l + \u221a 1\u2212 \u03b12a2l ) a1l ) \u2212 1 )\n= m\u2211 l=1 |a1l| a\u20322l ( Ph ( \u03b1 |a1l|+ \u221a 1\u2212 \u03b12a\u20322l ) \u2212 1 ) ,\nwhere a\u20322l def = a2lPh (a1l) is identically distributed to a2l and is independent of |a1l|. Define the random variable Ul as:\nUl def = |a1l| a\u20322l\n( Ph ( 1 +\n\u221a 1\u2212 \u03b12a\u20322l \u03b1 |a1l|\n) \u2212 1 ) .\nSimilar to Lemma A.2, we will calculate P [Ul > t] to show that Ul is subexponential and use it to derive concentration bounds. However, using the above estimate to bound E [Ul] will result in a weak bound that we will not be able to use. Lemma 4.3 bounds E [Ul] using a different technique carefully.\nP [|Ul| > t] \u2264 P [ |a1l| \u2223\u2223a\u20322l\u2223\u2223 c\u221a1\u2212 \u03b12 |a\u20322l|\u03b1 |a1l| > t ]\n= P [\u2223\u2223a\u20322l\u2223\u22232 > c\u03b1t\u221a\n1\u2212 \u03b12\n] \u2264 exp ( 1\u2212 c\u03b1t\u221a\n1\u2212 \u03b12\n) ,\nwhere the last step follows from the fact that a\u20322l is a subgaussian random variable and hence |a\u20322l| 2 is a subexponential random variable. Using Proposition 5.16 from [42], we obtain:\nP [\u2223\u2223\u2223\u2223\u2223 m\u2211 l=1 Ul \u2212 E [Ul] \u2223\u2223\u2223\u2223\u2223 > \u03b4m\u221a1\u2212 \u03b12 ] \u2264 2 exp ( \u2212min ( c\u03b42m2 ( 1\u2212 \u03b12 ) (1\u2212 \u03b12)m , c\u03b4m \u221a 1\u2212 \u03b12\u221a 1\u2212 \u03b12 )) \u2264 2 exp ( \u2212c\u03b42m ) \u2264 \u03b7\n10 exp (\u2212n) .\nUsing Lemma 4.3, we obtain:\u2223\u2223e2TA (D\u2212 I)ATe1\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 m\u2211 l=1 Ul \u2223\u2223\u2223\u2223\u2223 \u2264 (1 + \u03b4)m\u221a1\u2212 \u03b12, with probability greater than 1\u2212 \u03b710 exp(\u2212n). This proves the lemma.\nProof of Lemma 4.3. Let w2 = |w2| ei\u03b8. Then |w1| , |w2| and \u03b8 are all independent random variables. \u03b8 is a uniform random variable over [\u2212\u03c0, \u03c0] and |w1| and |w2| are identically distributed with probability distribution function:\np(x) = x exp ( \u2212x 2\n2\n) 1{x\u22650}.\nWe have:\nE [U ] = E [ |w1| |w2| ei\u03b8 ( Ph ( 1 + \u221a 1\u2212 \u03b12 |w2| e\u2212i\u03b8\n\u03b1 |w1|\n) \u2212 1 )]\n= E [ |w1| |w2|E [ ei\u03b8 ( Ph ( 1 + \u221a 1\u2212 \u03b12 |w2| e\u2212i\u03b8\n\u03b1 |w1|\n) \u2212 1 )]\u2223\u2223\u2223\u2223\u2223|w1| , |w2| ]\nLet \u03b2 def = \u221a 1\u2212\u03b12|w2| \u03b1|w1| . We will first calculate E\n[ ei\u03b8Ph ( 1 + \u03b2e\u2212i\u03b8 )\u2223\u2223|w1| , |w2|]. Note that the above expectation is taken only over the randomness in \u03b8. For simplicity of notation, we will drop the conditioning variables, and calculate the above expectation in terms of \u03b2.\nei\u03b8Ph ( 1 + \u03b2e\u2212i\u03b8 ) = (cos \u03b8 + i sin \u03b8) 1 + \u03b2 cos \u03b8 \u2212 i\u03b2 sin \u03b8[\n(1 + \u03b2 cos \u03b8)2 + \u03b22 sin2 \u03b8 ] 1 2\n= cos \u03b8 + \u03b2 + i sin \u03b8\n(1 + \u03b22 + 2\u03b2 cos \u03b8) 1 2\n.\nWe will first calculate the imaginary part of the above expectation: Im ( E [ ei\u03b8Ph ( 1 + \u03b2e\u2212i\u03b8 )]) = E [ sin \u03b8\n(1 + \u03b22 + 2\u03b2 cos \u03b8) 1 2\n] = 0, (13)\nwhere the last step follows because we are taking the expectation of an odd function. Focusing on the real part, we let:\nF (\u03b2) def = E\n[ cos \u03b8 + \u03b2\n(1 + \u03b22 + 2\u03b2 cos \u03b8) 1 2\n]\n= 1\n2\u03c0 \u222b \u03c0 \u2212\u03c0\ncos \u03b8 + \u03b2\n(1 + \u03b22 + 2\u03b2 cos \u03b8) 1 2\nd\u03b8.\nNote that F (\u03b2) : R \u2192 R and F (0) = 0. We will show that there is a small absolute numerical constant \u03b3 (depending on \u03b4) such that:\n0 < \u03b2 < \u03b3 \u21d2 |F (\u03b2)| \u2264 (1 2 + \u03b4)\u03b2. (14)\nWe show this by calculating F \u2032(0) and using the continuity of F \u2032(\u03b2) at \u03b2 = 0. We first calculate F \u2032(\u03b2) as follows:\nF \u2032(\u03b2) = 1\n2\u03c0 \u222b \u03c0 \u2212\u03c0\n1\n(1 + \u03b22 + 2\u03b2 cos \u03b8) 1 2 \u2212 (cos \u03b8 + \u03b2) (\u03b2 + cos \u03b8) (1 + \u03b22 + 2\u03b2 cos \u03b8) 3 2 d\u03b8\n= 1\n2\u03c0 \u222b \u03c0 \u2212\u03c0\nsin2 \u03b8\n(1 + \u03b22 + 2\u03b2 cos \u03b8) 3 2\nd\u03b8\nFrom the above, we see that F \u2032(0) = 12 and (14) then follows from the continuity of F \u2032(\u03b2) at \u03b2 = 0. Getting back to the expected value of U , we have:\n|E [U ]| = \u2223\u2223\u2223\u2223\u2223E [ |w1| |w2|F (\u221a 1\u2212 \u03b12 |w2| \u03b1 |w1| ) 1{\u221a\n1\u2212\u03b12|w2| \u03b1|w1| <\u03b3\n} ]\n+E [ |w1| |w2|F (\u221a 1\u2212 \u03b12 |w2| \u03b1 |w1| ) 1{\u221a\n1\u2212\u03b12|w2| \u03b1|w1|\n\u2265\u03b3 } ]\u2223\u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223\u2223E [ |w1| |w2|F (\u221a 1\u2212 \u03b12 |w2| \u03b1 |w1| ) 1{\u221a\n1\u2212\u03b12|w2| \u03b1|w1| <\u03b3\n} ]\u2223\u2223\u2223\u2223\u2223\n+ \u2223\u2223\u2223\u2223\u2223E [ |w1| |w2|F (\u221a 1\u2212 \u03b12 |w2| \u03b1 |w1| ) 1{\u221a\n1\u2212\u03b12|w2| \u03b1|w1|\n\u2265\u03b3 } ]\u2223\u2223\u2223\u2223\u2223\n(\u03b61) \u2264 ( 1\n2 + \u03b4\n) E [ |w1| |w2|\n\u221a 1\u2212 \u03b12 |w2| \u03b1 |w1|\n] + E [ |w1| |w2|1{\u221a1\u2212\u03b12|w2| \u03b1|w1| \u2265\u03b3 } ] ,\n=\n( 1\n2 + \u03b4 )(\u221a 1\u2212 \u03b12 \u03b1 ) E [ |w2|2 ] + E [ |w1| |w2|1{\u221a1\u2212\u03b12|w2| \u03b1|w1| \u2265\u03b3 } ] ,\n(\u03b62) = (1 + 2\u03b4) (\u221a 1\u2212 \u03b12 \u03b1 ) + E [ |w1| |w2|1{\u221a1\u2212\u03b12|w2| \u03b1|w1| \u2265\u03b3 } ] , (15)\nwhere (\u03b61) follows from (14) and the fact that |F (\u03b2)| \u2264 1 for every \u03b2 and (\u03b62) follows from the fact that E [ |z2|2 ] = 2. We will now bound the second term in the above inequality. We start with the following integral: \u222b \u221e t s2e\u2212 s2 2 ds = \u2212 \u222b \u221e t sd ( e\u2212 s2 2\n) = te\u2212 t2 2 +\n\u222b \u221e t e\u2212 s2 2 ds \u2264 (t+ e)e\u2212 t2 c , (16)\nwhere c is some constant. The last step follows from standard bounds on the tail probabilities of gaussian random variables. We now bound the second term of (15) as follows:\nE [ |w1| |w2|1{\u221a1\u2212\u03b12|w2| \u03b1|w1| \u2265\u03b3 } ] = \u222b \u221e 0 t2e\u2212 t2 2 \u222b \u221e \u03b1t\u221a 1\u2212\u03b12 s2e\u2212 s2 2 dsdt\n(\u03b61) \u2264 \u222b \u221e 0 t2e\u2212 t2 2 ( \u03b1t\u221a 1\u2212 \u03b12 + e ) e \u2212 \u03b1 2t2 c(1\u2212\u03b12)dt\n\u2264 \u222b \u221e 0 ( \u03b1t3\u221a 1\u2212 \u03b12 + et2 ) e \u2212 t 2 c(1\u2212\u03b12)dt\n= \u03b1\u221a\n1\u2212 \u03b12 \u222b \u221e 0 t3e \u2212 t 2 c(1\u2212\u03b12)dt+ e \u222b \u221e 0 t2e \u2212 t 2 c(1\u2212\u03b12)dt\n(\u03b62) \u2264 c ( 1\u2212 \u03b12 ) 3 2 (\u03b63) \u2264 \u03b4 \u221a 1\u2212 \u03b12\nwhere (\u03b61) follows from (16), (\u03b62) follows from the formulae for second and third absolute moments of gaussian random variables and (\u03b63) follows from the fact that 1 \u2212 \u03b12 < \u03b4. Plugging the above inequality in (15), we obtain:\n|E [U ]| \u2264 (1 + 2\u03b4) (\u221a 1\u2212 \u03b12 \u03b1 ) + \u03b4 \u221a 1\u2212 \u03b12 \u2264 (1 + 4\u03b4) \u221a 1\u2212 \u03b12,\nwhere we used the fact that \u03b1 \u2265 1\u2212 \u03b42 . This proves the lemma.\nLemma A.6. Assume the hypothesis of Theorem 4.2 and the notation therein. Then,\u2223\u2223e3TA (D\u2212 I)ATe1\u2223\u2223 \u2264 \u03b4m\u221a1\u2212 \u03b12, with probability greater than 1\u2212 \u03b710e \u2212n.\nProof. The proof of this lemma is very similar to that of Lemma A.5. We have:\ne3 TA (D\u2212 I)ATe1 = m\u2211 l=1 a1la3l ( Ph (( \u03b1a1l + a2l \u221a 1\u2212 \u03b12a3l ) a1l ) \u2212 1 )\n= m\u2211 l=1 |a1l| a\u20323l ( Ph ( \u03b1 |a1l|+ a\u20322l \u221a 1\u2212 \u03b12 ) \u2212 1 ) ,\nwhere a\u20323l def = a3lPh (a1l) is identically distributed to a3l and is independent of |a1l| and a\u20322l. Define the random variable Ul as:\nUl def = |a1l| a\u20323l\n( Ph ( 1 + a\u20322l \u221a\n1\u2212 \u03b12 \u03b1 |a1l|\n) \u2212 1 ) .\nSince a\u20323l has mean zero and is independent of everything else, we have:\nE [Ul] = 0.\nSimilar to Lemma A.5, we will calculate P [Ul > t] to show that Ul is subexponential and use it to derive concentration bounds.\nP [|Ul| > t] \u2264 P [ |a1l| \u2223\u2223a\u20323l\u2223\u2223 c\u221a1\u2212 \u03b12 |a\u20322l|\u03b1 |a1l| > t ]\n= P [\u2223\u2223a\u20322la\u20323l\u2223\u2223 > c\u03b1t\u221a\n1\u2212 \u03b12\n] \u2264 exp ( 1\u2212 c\u03b1t\u221a\n1\u2212 \u03b12\n) ,\nwhere the last step follows from the fact that a\u20322l and a \u2032 3l are independent subgaussian random variables and hence |a\u20322la\u20323l| is a subexponential random variable. Using Proposition 5.16 from [42], we obtain:\nP [\u2223\u2223\u2223\u2223\u2223 m\u2211 l=1 Ul \u2212 E [Ul] \u2223\u2223\u2223\u2223\u2223 > \u03b4m\u221a1\u2212 \u03b12 ] \u2264 2 exp ( \u2212min ( c\u03b42m2 ( 1\u2212 \u03b12 ) (1\u2212 \u03b12)m , c\u03b4m \u221a 1\u2212 \u03b12\u221a 1\u2212 \u03b12 )) \u2264 2 exp ( \u2212c\u03b42m ) \u2264 \u03b7\n10 exp (\u2212n) .\nHence, we have:\n\u2223\u2223e3TA (D\u2212 I)ATe1\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 m\u2211 l=1 Ul \u2223\u2223\u2223\u2223\u2223 \u2264 \u03b4m\u221a1\u2212 \u03b12, with probability greater than 1\u2212 \u03b710 exp(\u2212n). This proves the lemma.\nLemma A.7. For every w \u2208 C, we have:\n|Ph (1 + w)\u2212 1| \u2264 2 |w| .\nProof. The proof is straight forward:\n|Ph (1 + w)\u2212 1| \u2264 |Ph (1 + w)\u2212 (1 + w)|+ |w| = |1\u2212 |1 + w||+ |w| \u2264 2 |w| ."}, {"heading": "B Proofs for Section 5", "text": "Proof of Lemma 5.1. For every j \u2208 [n] and i \u2208 [m], consider the random variable Zij def = |aijyi|. We have the following:\n\u2022 if j \u2208 S, then\nE [Zij ] = 2\n\u03c0\n(\u221a 1\u2212 ( x\u2217j )2 + x\u2217j arcsinx \u2217 j )\n\u2265 2 \u03c0\n( 1\u2212 5\n6\n( x\u2217j )2 \u2212 1\n6\n( x\u2217j )4 + x\u2217j ( x\u2217j + 1\n6\n( x\u2217j )3))\n\u2265 2 \u03c0 + 1 6 (x\u2217min) 2 ,\nwhere the first step follows from Corollary 3.1 in [27] and the second step follows from the Taylor series expansions of \u221a 1\u2212 x2 and arcsin(x),\n\u2022 if j /\u2208 S, then E [Zij ] = E [|aij |]E [|yi|] = 2\u03c0 and finally,\n\u2022 for every j \u2208 [n], Zij is a sub-exponential random variable with parameter c = O(1) (since it is a product of two standard normal random variables).\nUsing the hypothesis of the theorem about m, we have: \u2022 for any j \u2208 S, P [\n1 m \u2211m i=1 Zij \u2212 ( 2 \u03c0 + 1 12 (x \u2217 min) 2 ) < 0 ] \u2264 exp ( \u2212c (x\u2217min) 4m ) \u2264 \u03b4n\u2212c, and\n\u2022 for any j /\u2208 S, P [\n1 m \u2211m i=1 Zij \u2212 ( 2 \u03c0 + 1 12 (x \u2217 min) 2 ) > 0 ] \u2264 exp ( \u2212c (x\u2217min) 4m ) \u2264 \u03b4n\u2212c.\nApplying a union bound to the above, we see that with probability greater than 1 \u2212 \u03b4, there is a separation in the values of 1m \u2211m i=1 Zij for j \u2208 S and j /\u2208 S. This proves the theorem."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Phase retrieval problems involve solving linear equations, but with missing sign (or phase, for<lb>complex numbers) information. Over the last two decades, a popular generic empirical approach<lb>to the many variants of this problem has been one of alternating minimization; i.e. alternating<lb>between estimating the missing phase information, and the candidate solution. In this paper, we<lb>show that a simple alternating minimization algorithm geometrically converges to the solution<lb>of one such problem \u2013 finding a vector x from y,A, where y = |Ax| and |z| denotes a vector<lb>of element-wise magnitudes of z \u2013 under the assumption that A is Gaussian.<lb>Empirically, our algorithm performs similar to recently proposed convex techniques for this<lb>variant (which are based on \u201clifting\u201d to a convex matrix problem) in sample complexity and<lb>robustness to noise. However, our algorithm is much more efficient and can scale to large<lb>problems. Analytically, we show geometric convergence to the solution, and sample complexity<lb>that is off by log factors from obvious lower bounds. We also establish close to optimal scaling<lb>for the case when the unknown vector is sparse. Our work represents the only known theoretical<lb>guarantee for alternating minimization for any variant of phase retrieval problems in the non-<lb>convex setting.", "creator": "LaTeX with hyperref package"}}}