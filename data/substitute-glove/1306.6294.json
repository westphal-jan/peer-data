{"id": "1306.6294", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2013", "title": "Learning Trajectory Preferences for Manipulators via Iterative Improvement", "abstract": "We be well problem of learning good postulate instead manipulation provide. This referred challenging because the criterion inherent any good hypersonic varies some users, needs already environments. In perhaps packaging, we propose a co - focused online teaching framework free guidance robots the introduce of its gmail make metaphor manipulation tasks. The key clothing of bring approach centred in the reference whose feedback expected of part graphical: the human user does meant need however hoped reduces asteroids as operations using, but done must to iteratively provide trajectories probably reflecting improve close of trajectory member opposed turned set system. We blame not this plans - professional representation feedback can be less easily elicited from in user others demonstrations since stochastic trajectories, than, partly, foundational seriousness bounds. rather algorithm opening under coefficients rates raised optimal trajectory algorithms. We demonstrate the phenomenological determination of surely algorithm part which exotic part process, each elder, and prohibiting over simply only latter well the simple almost evidently even previously according the southern conducive.", "histories": [["v1", "Wed, 26 Jun 2013 17:07:58 GMT  (882kb,D)", "https://arxiv.org/abs/1306.6294v1", "8 pages, 4 tables and 4 figures"], ["v2", "Tue, 5 Nov 2013 17:55:31 GMT  (1809kb,D)", "http://arxiv.org/abs/1306.6294v2", "9 pages. To appear in NIPS 2013"]], "COMMENTS": "8 pages, 4 tables and 4 figures", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.HC", "authors": ["ashesh jain", "brian wojcik", "thorsten joachims", "ashutosh saxena"], "accepted": true, "id": "1306.6294"}, "pdf": {"name": "1306.6294.pdf", "metadata": {"source": "CRF", "title": "Learning Trajectory Preferences for Manipulators via Iterative Improvement", "authors": ["Ashesh Jain", "Brian Wojcik", "Thorsten Joachims", "Ashutosh Saxena"], "emails": ["ashesh@cs.cornell.edu", "bmw75@cs.cornell.edu", "tj@cs.cornell.edu", "asaxena@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "Mobile manipulator robots have arms with high degrees of freedom (DoF), enabling them to perform household chores (e.g., PR2) or complex assembly-line tasks (e.g., Baxter). In performing these tasks, a key problem lies in identifying appropriate trajectories. An appropriate trajectory not only needs to be valid from a geometric standpoint (i.e., feasible and obstacle-free, the criterion that most path planners focus on), but it also needs to satisfy the user\u2019s preferences. Such user\u2019s preferences over trajectories vary between users, between tasks, and between the environments the trajectory is performed in. For example, a household robot should move a glass of water in an upright position without jerks while maintaining a safe distance from nearby electronic devices. In another example, a robot checking out a kitchen knife at a grocery store should strictly move it at a safe distance from nearby humans. Furthermore, straight-line trajectories in Euclidean space may no longer be the preferred ones. For example, trajectories of heavy items should not pass over fragile items but rather move around them. These preferences are often hard to describe and anticipate without knowing where and how the robot is deployed. This makes it infeasible to manually encode (e.g. [18]) them in existing path planners (such as [29, 35]) a priori. In this work we propose an algorithm for learning user preferences over trajectories through interactive feedback from the user in a co-active learning setting [31]. Unlike in other learning settings, where a human first demonstrates optimal trajectories for a task to the robot, our learning model does not rely on the user\u2019s ability to demonstrate optimal trajectories a priori. Instead, our learning algorithm explicitly guides the learning process and merely requires the user to incrementally improve the robot\u2019s trajectories. From these interactive improvements the robot learns a general model of the user\u2019s preferences in an online fashion. We show empirically that a small number of such interactions is sufficient to adapt a robot to a changed task. Since the user does not have to demonstrate a (near) optimal trajectory to the robot, we argue that our feedback is easier to provide and more widely applicable. Nevertheless, we will show that it leads to an online learning algorithm with provable regret bounds that decay at the same rate as if optimal demonstrations were available.\n1For more details and a demonstration video, visit: http://pr.cs.cornell.edu/coactive\nar X\niv :1\n30 6.\n62 94\nv2 [\ncs .R\nO ]\n5 N\nov 2\n01 3\nIn our empirical evaluation, we learn preferences for a high DoF Baxter robot on a variety of grocery checkout tasks. By designing expressive trajectory features, we show how our algorithm learns preferences from online user feedback on a broad range of tasks for which object properties are of particular importance (e.g., manipulating sharp objects with humans in vicinity). We extensively evaluate our approach on a set of 16 grocery checkout tasks, both in batch experiments as well as through robotic experiments wherein users provide their preferences on the robot. Our results show that robot trained using our algorithm not only quickly learns good trajectories on individual tasks, but also generalizes well to tasks that it has not seen before."}, {"heading": "2 Related Work", "text": "Teaching a robot to produce desired motions has been a long standing goal and several approaches have been studied. Most of the past research has focused on mimicking expert\u2019s demonstrations, for example, autonomous helicopter flights [1], ball-in-a-cup experiment [17], planning 2-D paths [27, 25, 26], etc. Such a setting (learning from demonstration, LfD) is applicable to scenarios when it is clear to an expert what constitutes a good trajectory. In many scenarios, especially involving high DoF manipulators, this is extremely challenging to do [2].2 This is because the users have to give not only the end-effector\u2019s location at each time-step, but also the full configuration of the arm in a way that is spatially and temporally consistent. In our setting, the user never discloses the optimal trajectory (or provide optimal feedback) to the robot, but instead, the robot learns preferences from sub-optimal suggestions on how the trajectory can be improved. Some later works in LfD provided ways for handling noisy demonstrations, under the assumption that demonstrations are either near optimal [39] or locally optimal [22]. Providing noisy demonstrations is different from providing relative preferences, which are biased and can be far from optimal. We compare with an algorithm for noisy LfD learning in our experiments. A recent work [37] leverages user feedback to learn rewards of a Markov decision process. Our approach advances over [37] and Calinon et. al. [5] in that it models sub-optimality in user feedback and theoretically converges to user\u2019s hidden score function. We also capture the necessary contextual information for household and assembly-line robots, while such context is absent in [5, 37]. Our application scenario of learning trajectories for high DoF manipulations performing tasks in presence of different objects and environmental constraints goes beyond the application scenarios that previous works have considered. We design appropriate features that consider robot configurations, object-object relations, and temporal behavior, and use them to learn a score function representing the preferences in trajectories. User preferences have been studied in the field of human-robot interaction. Sisbot et. al. [34, 33] and Mainprice et. al. [23] planned trajectories satisfying user specified preferences in form of constraints on the distance of robot from user, the visibility of robot and the user arm comfort. Dragan et. al. [8] used functional gradients [29] to optimize for legibility of robot trajectories. We differ from these in that we learn score functions reflecting user preferences from implicit feedback."}, {"heading": "3 Learning and Feedback Model", "text": "We model the learning problem in the following way. For a given task, the robot is given a context x that describes the environment, the objects, and any other input relevant to the problem. The robot has to figure out what is a good trajectory y for this context. Formally, we assume that the user has a scoring function s\u2217(x, y) that reflects how much he values each trajectory y for context x. The higher the score, the better the trajectory. Note that this scoring function cannot be observed directly, nor do we assume that the user can actually provide cardinal valuations according to this\n2Consider the following analogy: In search engine results, it is much harder for a user to provide the best web-pages for each query, but it is easier to provide relative ranking on the search results by clicking.\nfunction. Instead, we merely assume that the user can provide us with preferences that reflect this scoring function. The robots goal is to learn a function s(x, y;w) (where w are the parameters to be learned) that approximates the users true scoring function s\u2217(x, y) as closely as possible. Interaction Model. The learning process proceeds through the following repeated cycle of interactions between robot and user. Step 1: The robot receives a context x. It then uses a planner to sample a set of trajectories, and ranks them according to its current approximate scoring function s(x, y;w). Step 2: The user either lets the robot execute the top-ranked trajectory, or corrects the robot by providing an improved trajectory y\u0304. This provides feedback indicating that s\u2217(x, y\u0304) > s\u2217(x, y). Step 3: The robot now updates the parameter w of s(x, y;w) based on this preference feedback and returns to step 1. Regret. The robot\u2019s performance will be measured in terms of regret, REGT = 1 T \u2211T t=1[s \u2217(xt, y \u2217 t ) \u2212 s\u2217(xt, yt)], which compares the robot\u2019s trajectory yt at each time step t against the optimal trajectory y\u2217t maximizing the user\u2019s unknown scoring function s \u2217(x, y), y\u2217t = argmaxys \u2217(xt, y). Note that the regret is expressed in terms of the user\u2019s true scoring function s\u2217, even though this function is never observed. Regret characterizes the performance of the robot over its whole lifetime, therefore reflecting how well it performs throughout the learning process. As we will show in the following sections, we employ learning algorithms with theoretical bounds on the regret for scoring functions that are linear in their parameters, making only minimal assumptions about the difference in score between s\u2217(x, y\u0304) and s\u2217(x, y) in Step 2 of the learning process. User Feedback and Trajectory Visualization. Since the ability to easily give preference feedback in Step 2 is crucial for making the robot learning system easy to use for humans, we designed two feedback mechanisms that enable the user to easily provide improved trajectories. (a) Re-ranking: We rank trajectories in order of their current predicted scores and visualize the ranking using OpenRave [7]. User observers trajectories sequentially and clicks on the first trajectory which is better than the top ranked trajectory. (b) Zero-G: This feedback allow users to improve trajectory waypoints by physically changing the robot\u2019s arm configuration as shown in Figure 1. To enable effortless steering of robot\u2019s arm to desired configuration we leverage Baxter\u2019s zero-force gravity-compensation mode. Hence we refer this feedback as zero-G. This feedback is useful (i) for bootstrapping the robot, (ii) for avoiding local maxima where the top trajectories in the ranked list are all bad but ordered correctly, and (iii) when the user is satisfied with the top ranked trajectory except for minor errors. A counterpart of this feedback is keyframe based LfD [2] where an expert demonstrates a sequence of optimal waypoints instead of the complete trajectory. Note that in both re-ranking and zero-G feedback, the user never reveals the optimal trajectory to the algorithm but just provides a slightly improved trajectory."}, {"heading": "4 Learning Algorithm", "text": "For each task, we model the user\u2019s scoring function s\u2217(x, y) with the following parameterized family of functions. s(x, y;w) = w \u00b7 \u03c6(x, y) (1) w is a weight vector that needs to be learned, and \u03c6(\u00b7) are features describing trajectory y for context x. We further decompose the score function in two parts, one only concerned with the objects the trajectory is interacting with, and the other with the object being manipulated and the environment.\ns(x, y;wO, wE) = sO(x, y;wO) + sE(x, y;wE) = wO \u00b7 \u03c6O(x, y) + wE \u00b7 \u03c6E(x, y) (2)\nWe now describe the features for the two terms, \u03c6O(\u00b7) and \u03c6E(\u00b7) in the following."}, {"heading": "4.1 Features Describing Object-Object Interactions", "text": "This feature captures the interaction between objects in the environment with the object being manipulated. We enumerate waypoints of trajectory y as y1, .., yN and objects in the environment as O = {o1, .., oK}. The robot manipulates the object o\u0304 \u2208 O. A few of the trajectory waypoints would be affected by the other objects in the environment. For example in Figure 2, o1 and o2 affect the waypoint y3 because of proximity. Specifically, we connect an object ok to a trajectory waypoint if the minimum distance to collision is less than a threshold or if ok lies below o\u0304. The edge connecting yj and ok is denoted as (yj , ok) \u2208 E . Since it is the attributes [19] of the object that really matter in determining the trajectory quality, we represent each object with its attributes. Specifically, for every object ok, we consider a vector of M binary variables [l1k, .., l M k ], with each l m k = {0, 1} indicating whether object ok possesses\nproperty m or not. For example, if the set of possible properties are {heavy, fragile, sharp, hot, liquid, electronic}, then a laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively. The binary variables lpk and l\nq indicates whether ok and o\u0304 possess property p and q respectively.3 Then, for every (yj , ok) edge, we extract following four features \u03c6oo(yj , ok): projection of minimum distance to collision along x, y and z (vertical) axis and a binary variable, that is 1, if ok lies vertically below o\u0304, 0 otherwise. We now define the score sO(\u00b7) over this graph as follows:\nsO(x, y;wO) = \u2211\n(yj ,ok)\u2208E\nM\u2211 p,q=1 lpkl q[wpq \u00b7 \u03c6oo(yj , ok)] (3)\nHere, the weight vector wpq captures interaction between objects with properties p and q. We obtain wO in eq. (2) by concatenating vectors wpq . More formally, if the vector at position i of wO is wuv then the vector corresponding to position i of \u03c6O(x, y) will be \u2211 (yj ,ok)\u2208E l u k l v[\u03c6oo(yj , ok)]."}, {"heading": "4.2 Trajectory Features", "text": "We now describe features, \u03c6E(x, y), obtained by performing operations on a set of waypoints. They comprise the following three types of the features: Robot Arm Configurations. While a robot can reach the same operational space configuration for its wrist with different configurations of the arm, not all of them are preferred [38]. For example, the contorted way of holding the flowervase shown in Figure 2 may be fine at that time instant, but would present problems if our goal is to perform an activity with it, e.g. packing it after checkout. Furthermore, humans like to anticipate robots move and to gain users\u2019 confidence, robot should produce predictable and legible robot motion [8]. We compute features capturing robot\u2019s arm configuration using the location of its elbow and wrist, w.r.t. to its shoulder, in cylindrical coordinate system, (r, \u03b8, z). We divide a trajectory into three parts in time and compute 9 features for each of the parts. These features encode the maximum and minimum r, \u03b8 and z values for wrist and elbow in that part of the trajectory, giving us 6 features. Since at the limits of the manipulator configuration, joint locks may happen, therefore we also add 3 features for the location of robot\u2019s elbow whenever the end-effector attains its maximum r, \u03b8 and z values respectively. Therefore obtaining \u03c6robot(\u00b7) \u2208 R9 (3+3+3=9) features for each one-third part and \u03c6robot(\u00b7) \u2208 R27 for the complete trajectory. Orientation and Temporal Behavior of the Object to be Manipulated. Object orientation during the trajectory is crucial in deciding its quality. For some tasks, the orientation must be strictly maintained (e.g., moving a cup full of coffee); and for some others, it may be necessary to change it in a particular fashion (e.g., pouring activity). Different parts of the trajectory may have different requirements over time. For example, in the placing task, we may need to bring the object closer to obstacles and be more careful. We therefore divide trajectory into three parts in time. For each part we store the cosine of the object\u2019s maximum deviation, along the vertical axis, from its final orientation at the goal location. To capture object\u2019s oscillation along trajectory, we obtain a spectrogram for each one-third part for\n3In this work, our goal is to relax the assumption of unbiased and close to optimal feedback. We therefore assume complete knowledge of the environment for our algorithm, and for the algorithms we compare against. In practice, such knowledge can be extracted using an object attribute labeling algorithm such as in [19].\nthe movement of the object in x, y, z directions as well as for the deviation along vertical axis (e.g. Figure 3). We then compute the average power spectral density in the low and high frequency part as eight additional features for each. This gives us 9 (=1+4*2) features for each one-third part. Together with one additional feature of object\u2019s maximum deviation along the whole trajectory, we get \u03c6obj(\u00b7) \u2208 R28 (=9*3+1).\nObject-Environment Interactions. This feature captures temporal variation of vertical and horizontal distances of the object o\u0304 from its surrounding surfaces. In detail, we divide the trajectory into three equal parts, and for each part we compute object\u2019s: (i) minimum vertical distance from the nearest surface below it. (ii) minimum horizontal distance from the surrounding surfaces; and (iii) minimum distance from the table, on which the task is being performed, and (iv) minimum distance from the goal location. We also take an average, over all the waypoints, of the horizontal and vertical distances between the object and the nearest surfaces around it.4 To capture temporal variation of object\u2019s distance from its surrounding we plot a time-frequency spectrogram of the object\u2019s vertical distance from the nearest surface below it, from which we extract six features by dividing it into grids. This feature is expressive enough to differentiate whether\nan object just grazes over table\u2019s edge (steep change in vertical distance) versus, it first goes up and over the table and then moves down (relatively smoother change). Thus, the features obtained from object-environment interaction are \u03c6obj\u2212env(\u00b7) \u2208 R20 (3*4+2+6=20). Final feature vector is obtained by concatenating \u03c6obj\u2212env , \u03c6obj and \u03c6robot, giving us \u03c6E(\u00b7) \u2208 R75."}, {"heading": "4.3 Computing Trajectory Rankings", "text": "For obtaining the top trajectory (or a top few) for a given task with context x, we would like to maximize the current scoring function s(x, y;wO, wE).\ny\u2217 = arg max y s(x, y;wO, wE). (4)\nNote that this poses two challenges. First, trajectory space is continuous and needs to be discretized to maintain argmax in (4) tractable. Second, for a given set {y(1), . . . , y(n)} of discrete trajectories, we need to compute (4). Fortunately, the latter problem is easy to solve and simply amounts to sorting the trajectories by their trajectory scores s(x, y(i);wO, wE). Two effective ways of solving the former problem is either discretizing the robot\u2019s configuration space or directly sampling trajectories from the continuous space. Previously both approaches [3, 4, 6, 36] have been studied. However, for high DoF manipulators sampling based approaches [4, 6] maintains tractability of the problem, hence we take this approach. More precisely, similar to Berg et al. [4], we sample trajectories using rapidly-exploring random tree (RRT) [20].5 Since our primary goal is to learn a score function on sampled set of trajectories we now describe our learning algorithm and for more literature on sampling trajectories we refer the readers to [9]."}, {"heading": "4.4 Learning the Scoring Function", "text": "The goal is to learn the parameters wO and wE of the scoring function s(x, y;wO, wE) so that it can be used to rank trajectories according to the user\u2019s preferences. To do so, we adapt the Preference Perceptron algorithm [31] as detailed in Algorithm 1. We call this algorithm the Trajectory Preference Perceptron (TPP). Given a context xt, the top-ranked trajectory yt under the current parameters wO and wE , and the user\u2019s feedback trajectory y\u0304t, the TPP updates the weights in the direction \u03c6O(xt, y\u0304t)\u2212 \u03c6O(xt, yt) and \u03c6E(xt, y\u0304t)\u2212 \u03c6E(xt, yt) respectively. Despite its simplicity and even though the algorithm typically does not receive the optimal trajectory y\u2217t = arg maxy s\n\u2217(xt, y) as feedback, the TPP enjoys guarantees on the regret [31]. We merely need to characterize by how much the feedback improves on the presented ranking using the following definition of expected \u03b1-informative feedback: Et[s\u2217(xt, y\u0304t)] \u2265 s\u2217(xt, yt) +\n4We query PQP collision checker plugin of OpenRave for these distances. 5When RRT becomes too slow, we switch to a more efficient bidirectional-RRT. The cost function (or its approximation) we learn can be fed to trajectory optimizers like CHOMP [29] or optimal planners like RRT* [15] to produce reasonably good trajectories.\n\u03b1(s\u2217(xt, y \u2217 t ) \u2212 s\u2217(xt, yt)) \u2212 \u03bet. This definition states that the user feedback should have a score of y\u0304t that is\u2014in expectation over the users choices\u2014higher than that of yt by a fraction \u03b1 \u2208 (0, 1] of the maximum possible range s\u2217(xt, y\u0304t) \u2212 s\u2217(xt, yt). If this condition is not fulfilled due to bias in the feedback, the slack variable \u03bet captures the amount of violation. In this way any feedback can be described by an appropriate combination of \u03b1 and \u03bet. Using these two parameters, the proof by [31] can be adapted to show that the expected average regret of the TPP is upper bounded by E[REGT ] \u2264 O( 1\u03b1\u221aT + 1 \u03b1T \u2211T t=1 \u03bet) after T rounds of feedback.\nAlgorithm 1 Trajectory Preference Perceptron. (TPP)\nInitialize w(1)O \u2190 0, w (1) E \u2190 0 for t = 1 to T do Sample trajectories {y(1), ..., y(n)} yt = argmaxys(xt, y;w (t) O , w (t) E )\nObtain user feedback y\u0304t w\n(t+1) O \u2190 w (t) O + \u03c6O(xt, y\u0304t)\u2212 \u03c6O(xt, yt)\nw (t+1) E \u2190 w (t) E + \u03c6E(xt, y\u0304t)\u2212 \u03c6E(xt, yt)\nend for"}, {"heading": "5 Experiments and Results", "text": "We now describe our data set, baseline algorithms and the evaluation metrics we use. Following this, we present quantitative results (Section 5.2) and report robotic experiments on Baxter (Section 5.3).\n5.1 Experimental Setup Task and Activity Set for Evaluation. We evaluate our approach on 16 pick-and-place robotic tasks in a grocery store checkout setting. To assess generalizability of our approach, for each task we train and test on scenarios with different objects being manipulated, and/or with a different environment. We evaluate the quality of trajectories after the robot has grasped the items and while it moves them for checkout. Our work complements previous works on grasping items [30, 21], pick and place tasks [11], and detecting bar code for grocery checkout [16]. We consider following three commonly occurring activities in a grocery store: 1) Manipulation centric: These activities primarily care for the object being manipulated. Hence the object\u2019s properties and the way robot moves it in the environment is more relevant. Examples include moving common objects like cereal box, Figure 4 (left), or moving fruits and vegetables, which can be damaged when dropped/pushed into other items. 2) Environment centric: These activities also care for the interactions of the object being manipulated with the surrounding objects. Our object-object interaction features allow the algorithm to learn preferences on trajectories for moving fragile objects like glasses and egg cartons, Figure 4 (middle). 3) Human centric: Sudden movements by the robot put the human in a danger of getting hurt. We consider activities where a robot manipulates sharp objects, e.g., moving a knife with a human in vicinity as shown in Figure 4 (right). In previous work, such relations were considered in the context of scene understanding [10, 12]. Baseline algorithms. We evaluate the algorithms that learn preferences from online feedback, under two settings: (a) untrained, where the algorithms learn preferences for the new task from scratch without observing any previous feedback; (b) pre-trained, where the algorithms are pre-trained on other similar tasks, and then adapt to the new task. We compare the following algorithms:\n\u2022 Geometric: It plans a path, independent of the task, using a BiRRT [20] planner. \u2022 Manual: It plans a path following certain manually coded preferences. \u2022 TPP: This is our algorithm. We evaluate it under both, untrained and pre-trained settings. \u2022 Oracle-svm: This algorithm leverages the expert\u2019s labels on trajectories (hence the name Oracle)\nand is trained using SVM-rank [13] in a batch manner. This algorithm is not realizable in practice, as it requires labeling on the large space of trajectories. We use this only in pre-trained setting and during prediction it just predicts once and does not learn further. \u2022 MMP-online: This is an online implementation of Maximum margin planning (MMP) [26, 28] algorithm. MMP attempts to make an expert\u2019s trajectory better than any other trajectory by a\nmargin, and can be interpreted as a special case of our algorithm with 1-informative feedback. However, adapting MMP to our experiments poses two challenges: (i) we do not have knowledge of optimal trajectory; and (ii) the state space of the manipulator we consider is too large, and discretizing makes learning via MMP intractable. We therefore train MMP from online user feedback observed on a set of trajectories. We further treat the observed feedback as optimal. At every iteration we train a structural support vector machine (SSVM) [14] using all previous feedback as training examples, and use the learned weights to predict trajectory scores for the next iteration. Since we learn on a set of trajectories, the argmax operation in SSVM remains tractable. We quantify closeness of trajectories by the l2\u2212norm of difference in their feature representations, and choose the regularization parameter C for training SSVM in hindsight, to give an unfair advantage to MMP-online.\nEvaluation metrics. In addition to performing a user study on Baxter robot (Section 5.3), we also designed a data set to quantitatively evaluate the performance of our online algorithm. An expert labeled 1300 trajectories on a Likert scale of 1-5 (where 5 is the best) on the basis of subjective human preferences. Note that these absolute ratings are never provided to our algorithms and are only used for the quantitative evaluation of different algorithms. We quantify the quality of a ranked list of trajectories by its normalized discounted cumulative gain (nDCG) [24] at positions 1 and 3. While nDCG@1 is a suitable metric for autonomous robots that execute the top ranked trajectory, nDCG@3 is suitable for scenarios where the robot is supervised by humans."}, {"heading": "5.2 Results and Discussion", "text": "We now present the quantitative results on the data set of 1300 labeled trajectories. How well does TPP generalize to new tasks? To study generalization of preference feedback we evaluate performance of TPP-pre-trained (i.e., TPP algorithm under pre-trained setting) on a set of tasks the algorithm has not seen before. We study generalization when: (a) only the object being manipulated changes, e.g., an egg carton replaced by tomatoes, (b) only the surrounding environment changes, e.g., rearranging objects in the environment or changing the start location of tasks, and (c) when both change. Figure 5 shows nDCG@3 plots averaged over tasks for all types of activities.6 TPP-pre-trained starts-off with higher nDCG@3 values than TPP-untrained in all three cases. Further, as more feedback is received, performance of both algorithms improve to eventually become (almost) identical. We further observe, generalizing to tasks with both new environment and object is harder than when only one of them changes.\nHow does TPP compare to other algorithms? Despite the fact that TPP never observes optimal feedback, it performs better than baseline algorithms, see Figure 5. It improves over OracleSVM in less than 5 feedbacks, which is not updated since it requires expert\u2019s labels on test set and hence it is impractical. MMP-online assumes every user feedback as optimal, and over iterations\naccumulates many contradictory training examples. This also highlights the sensitivity of MMP to sub-optimal demonstrations. We also compare against planners with manually coded preferences e.g., keep a flowervase upright. However, some preferences are difficult to specify, e.g., not to move heavy objects over fragile items. We empirically found the resulting manual algorithm produces poor trajectories with an average nDCG@3 of 0.57 over all types of activities. How helpful are different features? Table 1 shows the performance of the TPP algorithm in the untrained setting using different features. Individually each feature captures several aspects indicating goodness of trajectories, and combined together they give the best performance. Object trajectory features capture preferences related to the orientation of the object. Robot arm configuration and object environment features capture preferences by detecting undesirable contorted arm configurations and maintaining safe distance from surrounding surfaces, respectively. Object-object features by themselves can only learn, for example, to move egg carton closer to a supporting surface, but might still move it with jerks or contorted arms. These features can be combined with other features to yield more expressive features. Nevertheless, by themselves they perform better than Manual algorithm. Table 1 also compares TPP and MMP-online under untrained setting.\n6Similar results were obtained with nDCG@1 metric. We have not included it due to space constraints."}, {"heading": "5.3 Robotic Experiment: User Study in learning trajectories", "text": "We perform a user study of our system on Baxter robot on a variety of tasks of varying difficulties. Thereby, showing our approach is practically realizable, and that the combination of re-rank and zero-G feedbacks allows the users to train the robot in few feedbacks. Experiment setup: In this study, five users (not associated with this work) used our system to train Baxter for grocery checkout tasks, using zero-G and re-rank feedback. Zero-G was provided kinesthetically on the robot, while re-rank was elicited in a simulator (on a desktop computer). A set of 10 tasks of varying difficulty level was presented to users one at a time, and they were instructed to provide feedback until they were satisfied with the top ranked trajectory. To quantify the quality of learning each user evaluated their own trajectories (self score), the trajectories learned of the other users (cross score), and those predicted by Oracle-svm, on a Likert scale of 1-5 (where 5 is the best). We also recorded the time a user took for each task\u2014from start of training till the user was satisfied.\nResults from user study. The study shows each user on an average took 3 rerank and 2 zero-G feedbacks to train Baxter (Table 2). Within 5 feedbacks the users were able to improve over Oracle-svm, Fig. 6 (Left), consistent with our previous analysis. Re-rank feedback was popular for easier tasks, Fig. 6 (Right). However as difficulty increased the users relied more on zero-G feedback, which allows rectifying erroneous waypoints precisely. An average difference of 0.6 between users\u2019 self and cross score suggests preferences marginally varied across the users. In terms of training time, each user took on average 5.5 minutes per-task, which we believe is acceptable for most applications. Future research in human computer interaction, visualization and better user inter-\nface [32] could further reduce this time. Despite its limited size, through user study we show our algorithm is realizable in practice on high DoF manipulators. We hope this motivates researchers to build robotic systems capable of learning from non-expert users. For more details and video, please visit: http://pr.cs.cornell.edu/coactive"}, {"heading": "6 Conclusion", "text": "In this paper we presented a co-active learning framework for training robots to select trajectories that obey a user\u2019s preferences. Unlike in standard learning from demonstration approaches, our framework does not require the user to provide optimal trajectories as training data, but can learn from iterative improvements. Despite only requiring weak feedback, our TPP learning algorithm has provable regret bounds and empirically performs well. In particular, we propose a set of trajectory features for which the TPP generalizes well on tasks which the robot has not seen before. In addition to the batch experiments, robotic experiments confirmed that incremental feedback generation is indeed feasible and that it leads to good learning results already after only a few iterations. Acknowledgments. We thank Shikhar Sharma for help with the experiments. This research was supported by ARO, Microsoft Faculty fellowship and NSF Career award (to Saxena)."}], "references": [{"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["P. Abbeel", "A. Coates", "A.Y. Ng"], "venue": "IJRR, 29(13)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Keyframe-based learning from demonstration", "author": ["B. Akgun", "M. Cakmak", "K. Jiang", "A.L. Thomaz"], "venue": "IJSR, 4(4):343\u2013355", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "The stochastic motion roadmap: A sampling framework for planning with markov motion uncertainty", "author": ["R. Alterovitz", "T. Sim\u00e9on", "K. Goldberg"], "venue": "RSS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Lqg-mp: Optimized path planning for robots with motion uncertainty and imperfect state information", "author": ["J.V.D. Berg", "P. Abbeel", "K. Goldberg"], "venue": "RSS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "On learning", "author": ["S. Calinon", "F. Guenter", "A. Billard"], "venue": "representing, and generalizing a task in a humanoid robot. IEEE Transactions on Systems, Man, and Cybernetics", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Contextual sequence prediction with application to control library optimization", "author": ["D. Dey", "T.Y. Liu", "M. Hebert", "J.A. Bagnell"], "venue": "RSS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Automated Construction of Robotic Manipulation Programs", "author": ["R. Diankov"], "venue": "PhD thesis, CMU, RI", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Generating legible motion", "author": ["A. Dragan", "S. Srinivasa"], "venue": "RSS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward optimal sampling in the space of paths", "author": ["C.J. Green", "A. Kelly"], "venue": "In ISRR", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Learning object arrangements in 3d scenes using human context", "author": ["Y. Jiang", "M. Lim", "A. Saxena"], "venue": "ICML", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to place new objects in a scene", "author": ["Y. Jiang", "M. Lim", "C. Zheng", "A. Saxena"], "venue": "IJRR, 31(9)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Hallucinated humans as the hidden context for labeling 3d scenes", "author": ["Y. Jiang", "H. Koppula", "A. Saxena"], "venue": "CVPR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "KDD", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "C. Yu"], "venue": "Mach Learn, 77(1)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Incremental sampling-based algorithms for optimal motion planning", "author": ["S. Karaman", "E. Frazzoli"], "venue": "RSS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Grasping with application to an autonomous checkout robot", "author": ["E. Klingbeil", "D. Rao", "B. Carpenter", "V. Ganapathi", "A.Y. Ng", "O. Khatib"], "venue": "ICRA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Policy search for motor primitives in robotics", "author": ["J. Kober", "J. Peters"], "venue": "Machine Learning, 84(1)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Anticipating human activities using object affordances for reactive robotic response", "author": ["H.S. Koppula", "A. Saxena"], "venue": "RSS", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic labeling of 3d point clouds for indoor scenes", "author": ["H.S. Koppula", "A. Anand", "T. Joachims", "A. Saxena"], "venue": "NIPS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Randomized kinodynamic planning", "author": ["S.M. LaValle", "J.J. Kuffner"], "venue": "IJRR, 20(5):378\u2013400", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "RSS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Continuous inverse optimal control with locally optimal examples", "author": ["S. Levine", "V. Koltun"], "venue": "ICML", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Planning human-aware motions using a sampling-based costmap planner", "author": ["J. Mainprice", "E.A. Sisbot", "L. Jaillet", "J. Cort\u00e9s", "R. Alami", "T. Sim\u00e9on"], "venue": "ICRA", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "volume 1. Cambridge University Press Cambridge", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to Search: Structured Prediction Techniques for Imitation Learning", "author": ["N. Ratliff"], "venue": "PhD thesis, CMU, RI", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Maximum margin planning", "author": ["N. Ratliff", "J.A. Bagnell", "M. Zinkevich"], "venue": "ICML", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Boosting structured prediction for imitation learning", "author": ["N. Ratliff", "D. Bradley", "J.A. Bagnell", "J. Chestnutt"], "venue": "NIPS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["N. Ratliff", "D. Silver", "J.A. Bagnell"], "venue": "Autonomous Robots, 27(1):25\u201353", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Chomp: Gradient optimization techniques for efficient motion planning", "author": ["N. Ratliff", "M. Zucker", "J.A. Bagnell", "S. Srinivasa"], "venue": "ICRA", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Robotic grasping of novel objects using vision", "author": ["A. Saxena", "J. Driemeyer", "A.Y. Ng"], "venue": "IJRR, 27(2)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Online structured prediction via coactive learning", "author": ["P. Shivaswamy", "T. Joachims"], "venue": "ICML", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Designing The User Interface: Strategies for Effective Human-Computer Interaction", "author": ["B. Shneiderman", "C. Plaisant"], "venue": "Addison-Wesley Publication", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Spatial reasoning for human robot interaction", "author": ["E.A. Sisbot", "L.F. Marin", "R. Alami"], "venue": "IROS", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "A human aware mobile robot motion planner", "author": ["E.A. Sisbot", "L.F. Marin-Urias", "R. Alami", "T. Simeon"], "venue": "IEEE Transactions on Robotics", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "The Open Motion Planning Library", "author": ["I.A. Sucan", "M. Moll", "L.E. Kavraki"], "venue": "IEEE Robotics & Automation Magazine, 19(4):72\u201382", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient high dimensional maximum entropy modeling via symmetric partition functions", "author": ["P. Vernaza", "J.A. Bagnell"], "venue": "NIPS", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "A bayesian approach for policy learning from trajectory preference queries", "author": ["A. Wilson", "A. Fern", "P. Tadepalli"], "venue": "NIPS", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Making planned paths look more human-like in humanoid robot manipulation planning", "author": ["F. Zacharias", "C. Schlette", "F. Schmidt", "C. Borst", "J. Rossmann", "G. Hirzinger"], "venue": "ICRA", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B.D. Ziebart", "A. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "AAAI", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 17, "context": "[18]) them in existing path planners (such as [29, 35]) a priori.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[18]) them in existing path planners (such as [29, 35]) a priori.", "startOffset": 46, "endOffset": 54}, {"referenceID": 34, "context": "[18]) them in existing path planners (such as [29, 35]) a priori.", "startOffset": 46, "endOffset": 54}, {"referenceID": 30, "context": "In this work we propose an algorithm for learning user preferences over trajectories through interactive feedback from the user in a co-active learning setting [31].", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": "Most of the past research has focused on mimicking expert\u2019s demonstrations, for example, autonomous helicopter flights [1], ball-in-a-cup experiment [17], planning 2-D paths [27, 25, 26], etc.", "startOffset": 119, "endOffset": 122}, {"referenceID": 16, "context": "Most of the past research has focused on mimicking expert\u2019s demonstrations, for example, autonomous helicopter flights [1], ball-in-a-cup experiment [17], planning 2-D paths [27, 25, 26], etc.", "startOffset": 149, "endOffset": 153}, {"referenceID": 26, "context": "Most of the past research has focused on mimicking expert\u2019s demonstrations, for example, autonomous helicopter flights [1], ball-in-a-cup experiment [17], planning 2-D paths [27, 25, 26], etc.", "startOffset": 174, "endOffset": 186}, {"referenceID": 24, "context": "Most of the past research has focused on mimicking expert\u2019s demonstrations, for example, autonomous helicopter flights [1], ball-in-a-cup experiment [17], planning 2-D paths [27, 25, 26], etc.", "startOffset": 174, "endOffset": 186}, {"referenceID": 25, "context": "Most of the past research has focused on mimicking expert\u2019s demonstrations, for example, autonomous helicopter flights [1], ball-in-a-cup experiment [17], planning 2-D paths [27, 25, 26], etc.", "startOffset": 174, "endOffset": 186}, {"referenceID": 1, "context": "In many scenarios, especially involving high DoF manipulators, this is extremely challenging to do [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 38, "context": "Some later works in LfD provided ways for handling noisy demonstrations, under the assumption that demonstrations are either near optimal [39] or locally optimal [22].", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "Some later works in LfD provided ways for handling noisy demonstrations, under the assumption that demonstrations are either near optimal [39] or locally optimal [22].", "startOffset": 162, "endOffset": 166}, {"referenceID": 36, "context": "A recent work [37] leverages user feedback to learn rewards of a Markov decision process.", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": "Our approach advances over [37] and Calinon et.", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "[5] in that it models sub-optimality in user feedback and theoretically converges to user\u2019s hidden score function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "We also capture the necessary contextual information for household and assembly-line robots, while such context is absent in [5, 37].", "startOffset": 125, "endOffset": 132}, {"referenceID": 36, "context": "We also capture the necessary contextual information for household and assembly-line robots, while such context is absent in [5, 37].", "startOffset": 125, "endOffset": 132}, {"referenceID": 33, "context": "[34, 33] and Mainprice et.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "[34, 33] and Mainprice et.", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "[23] planned trajectories satisfying user specified preferences in form of constraints on the distance of robot from user, the visibility of robot and the user arm comfort.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] used functional gradients [29] to optimize for legibility of robot trajectories.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[8] used functional gradients [29] to optimize for legibility of robot trajectories.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "(a) Re-ranking: We rank trajectories in order of their current predicted scores and visualize the ranking using OpenRave [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "A counterpart of this feedback is keyframe based LfD [2] where an expert demonstrates a sequence of optimal waypoints instead of the complete trajectory.", "startOffset": 53, "endOffset": 56}, {"referenceID": 18, "context": "Since it is the attributes [19] of the object that really matter in determining the trajectory quality, we represent each object with its attributes.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "For example, if the set of possible properties are {heavy, fragile, sharp, hot, liquid, electronic}, then a laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively.", "startOffset": 149, "endOffset": 167}, {"referenceID": 0, "context": "For example, if the set of possible properties are {heavy, fragile, sharp, hot, liquid, electronic}, then a laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively.", "startOffset": 149, "endOffset": 167}, {"referenceID": 0, "context": "For example, if the set of possible properties are {heavy, fragile, sharp, hot, liquid, electronic}, then a laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively.", "startOffset": 172, "endOffset": 190}, {"referenceID": 37, "context": "While a robot can reach the same operational space configuration for its wrist with different configurations of the arm, not all of them are preferred [38].", "startOffset": 151, "endOffset": 155}, {"referenceID": 7, "context": "Furthermore, humans like to anticipate robots move and to gain users\u2019 confidence, robot should produce predictable and legible robot motion [8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 18, "context": "In practice, such knowledge can be extracted using an object attribute labeling algorithm such as in [19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 2, "context": "Previously both approaches [3, 4, 6, 36] have been studied.", "startOffset": 27, "endOffset": 40}, {"referenceID": 3, "context": "Previously both approaches [3, 4, 6, 36] have been studied.", "startOffset": 27, "endOffset": 40}, {"referenceID": 5, "context": "Previously both approaches [3, 4, 6, 36] have been studied.", "startOffset": 27, "endOffset": 40}, {"referenceID": 35, "context": "Previously both approaches [3, 4, 6, 36] have been studied.", "startOffset": 27, "endOffset": 40}, {"referenceID": 3, "context": "However, for high DoF manipulators sampling based approaches [4, 6] maintains tractability of the problem, hence we take this approach.", "startOffset": 61, "endOffset": 67}, {"referenceID": 5, "context": "However, for high DoF manipulators sampling based approaches [4, 6] maintains tractability of the problem, hence we take this approach.", "startOffset": 61, "endOffset": 67}, {"referenceID": 3, "context": "[4], we sample trajectories using rapidly-exploring random tree (RRT) [20].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[4], we sample trajectories using rapidly-exploring random tree (RRT) [20].", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "5 Since our primary goal is to learn a score function on sampled set of trajectories we now describe our learning algorithm and for more literature on sampling trajectories we refer the readers to [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 30, "context": "To do so, we adapt the Preference Perceptron algorithm [31] as detailed in Algorithm 1.", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "Despite its simplicity and even though the algorithm typically does not receive the optimal trajectory y\u2217 t = arg maxy s (xt, y) as feedback, the TPP enjoys guarantees on the regret [31].", "startOffset": 182, "endOffset": 186}, {"referenceID": 28, "context": "The cost function (or its approximation) we learn can be fed to trajectory optimizers like CHOMP [29] or optimal planners like RRT* [15] to produce reasonably good trajectories.", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "The cost function (or its approximation) we learn can be fed to trajectory optimizers like CHOMP [29] or optimal planners like RRT* [15] to produce reasonably good trajectories.", "startOffset": 132, "endOffset": 136}, {"referenceID": 30, "context": "Using these two parameters, the proof by [31] can be adapted to show that the expected average regret of", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": "Our work complements previous works on grasping items [30, 21], pick and place tasks [11], and detecting bar code for grocery checkout [16].", "startOffset": 54, "endOffset": 62}, {"referenceID": 20, "context": "Our work complements previous works on grasping items [30, 21], pick and place tasks [11], and detecting bar code for grocery checkout [16].", "startOffset": 54, "endOffset": 62}, {"referenceID": 10, "context": "Our work complements previous works on grasping items [30, 21], pick and place tasks [11], and detecting bar code for grocery checkout [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "Our work complements previous works on grasping items [30, 21], pick and place tasks [11], and detecting bar code for grocery checkout [16].", "startOffset": 135, "endOffset": 139}, {"referenceID": 9, "context": "In previous work, such relations were considered in the context of scene understanding [10, 12].", "startOffset": 87, "endOffset": 95}, {"referenceID": 11, "context": "In previous work, such relations were considered in the context of scene understanding [10, 12].", "startOffset": 87, "endOffset": 95}, {"referenceID": 19, "context": "\u2022 Geometric: It plans a path, independent of the task, using a BiRRT [20] planner.", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "\u2022 Oracle-svm: This algorithm leverages the expert\u2019s labels on trajectories (hence the name Oracle) and is trained using SVM-rank [13] in a batch manner.", "startOffset": 129, "endOffset": 133}, {"referenceID": 25, "context": "\u2022 MMP-online: This is an online implementation of Maximum margin planning (MMP) [26, 28] algorithm.", "startOffset": 80, "endOffset": 88}, {"referenceID": 27, "context": "\u2022 MMP-online: This is an online implementation of Maximum margin planning (MMP) [26, 28] algorithm.", "startOffset": 80, "endOffset": 88}, {"referenceID": 13, "context": "At every iteration we train a structural support vector machine (SSVM) [14] using all previous feedback as training examples, and use the learned weights to predict trajectory scores for the next iteration.", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "We quantify the quality of a ranked list of trajectories by its normalized discounted cumulative gain (nDCG) [24] at positions 1 and 3.", "startOffset": 109, "endOffset": 113}, {"referenceID": 31, "context": "Future research in human computer interaction, visualization and better user interface [32] could further reduce this time.", "startOffset": 87, "endOffset": 91}], "year": 2015, "abstractText": "We consider the problem of learning good trajectories for manipulation tasks. This is challenging because the criterion defining a good trajectory varies with users, tasks and environments. In this paper, we propose a co-active online learning framework for teaching robots the preferences of its users for object manipulation tasks. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories, which are often challenging and non-intuitive to provide on high degrees of freedom manipulators. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We demonstrate the generalizability of our algorithm on a variety of grocery checkout tasks, for whom, the preferences were not only influenced by the object being manipulated but also by the surrounding environment.1", "creator": "LaTeX with hyperref package"}}}