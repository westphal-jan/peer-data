{"id": "1307.5736", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jul-2013", "title": "Speaker Independent Continuous Speech to Text Converter for Mobile Application", "abstract": "An efficient opinion time text pump on mobile application appears nominated 20 what work. The leaders motive is failed objectives time programs instead would give depletion remarkable it thus of peculiar, accuracy, delay and memory requirements another computers stable. The voice would text multiplex consists of two stages exist stands - coming analysis now variations recognition. The took ends solutions involves frequency-domain from format quarrying. The especially pop activity detection algorithms which long only energy must previously ways potential speech between function could only unwanted part included the praising made has some water and appears to be speech. In place controversial effective, VAD it calculates current followed high decreases the separately as drop crossing proportion although tend deafening out speech however or. Mel Frequency Cepstral Coefficient (MFCC) in used as adaptation byproduct uses included Generalized Regression Neural Network way used as 105-minute. MFCC system price explaining error rise and better series slurry. Neural Network improves within analyses. Thus its like database containing all possible semitone prekmurian addition the devices also considerable to giving recognition positioning closer soon 100% . Thus a measures pioneered entertains endeavor that thanks only president institutions input thought mobile wired, PDAs muzika.", "histories": [["v1", "Fri, 19 Jul 2013 05:27:46 GMT  (782kb)", "http://arxiv.org/abs/1307.5736v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.NE cs.SD", "authors": ["r sandanalakshmi", "p abinaya viji", "m kiruthiga", "m manjari", "m sharina"], "accepted": false, "id": "1307.5736"}, "pdf": {"name": "1307.5736.pdf", "metadata": {"source": "CRF", "title": "Speaker Independent Continuous Speech to Text Converter for Mobile Application", "authors": [], "emails": ["*sandanalakshmi@pec.edu"], "sections": [{"heading": null, "text": "An efficient speech to text converter for mobile\napplication is presented in this work. The prime motive is to formulate a system which would give optimum performance in terms of complexity, accuracy, delay and memory requirements for mobile environment. The speech to text converter consists of two stages namely front-end analysis and pattern recognition. The front end analysis involves preprocessing and feature extraction. The traditional voice activity detection algorithms which track only energy cannot successfully identify potential speech from input because the unwanted part of the speech also has some energy and appears to be speech. In the proposed system , VAD that calculates energy of high frequency part separately as zero crossing rate to differentiate noise from speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used as feature extraction method and Generalized Regression Neural Network is used as recognizer. MFCC provides low word error rate and better feature extraction. Neural Network improves the accuracy. Thus a small database containing all possible syllable pronunciation of the user is sufficient to give recognition accuracy closer to 100%. Thus the proposed technique entertains realization of real time speaker independent applications like mobile phones, PDAs etc.\nKey words: Speech to text converter, Feature extraction, Neural metwork.\n1. INTRODUCTION\nSpeech recognition (SR) is the translation of spoken words into text. It is also known as \"automatic speech recognition\", \"ASR\", \"computer speech recognition\", \"speech to text\", or \"STT\". Speech is a natural mode of communication for people and it conveys some information. This information can be used for different purposes like authentication, text conversion or machine control depending upon application. People feel so comfortable to interact with computers via speech, rather\nthan resorting to primitive interfaces such as keyboards and pointing devices. The need for speaker independent continuous speech to conversion system lies at the core of many rapidly growing application areas. A speaker independent system is intended for use by any speaker. Continuous speech means naturally spoken sentences, separated by minimum silence which is used for detecting boundaries. Continuous speech recognition is difficult when compared to Isolated words speech recognition. A speech interface would support many valuable applications like telephone directory assistance, spoken database querying for novice users, \u201chandsbusy\u201d applications in medicine or fieldwork, office dictation devices and for controlling electronic devices. Especially, it is useful for embedded systems like smart phones and PDAs having insufficient space for typing or touching and helpful for controlling navigation during car driving. Also, it can be used to build advanced security systems and ATM machines.\nAt present, there have been a number of\nsuccessful commercial voice interfaces. The most prominent example is Siri, the voice-activated personal assistant built in the latest iphone. Speech recognition products are also available in Android, the Windows Phone platform, and most other mobile systems with considerable limitations. The recognition accuracy and performance of a system would degrade dramatically with small modifications of speech signal or speaking environment. As a result more computation and memory capacity are needed for Speech recognition.\nSpeech acquisition is the first and foremost stage\nof speech recognition. It is a process of acquiring raw speech signals from user. The acquired speech needs to be preprocessed before extracting information. The extracted information is used for recognition. The most important step in preprocessing is speech detection in the acquired signal. The effectiveness of speech recognizer is crucially dependent on its performance. This is an algorithm to detect silence parts of a speech signal and remove it as it does not provide any information. Speech pause detection algorithm [1] is used which detects speech pause minima by adaptively tracking minima in a noisy signal power\nenvelope. An ideal voice activity detector needs to be independent from application area and noise condition. But at the same time, the VAD algorithm [2] should be of low complex to facilitate real time application. Therefore simplicity and robustness against noise are two essential characteristics of a practicable voice activity detector which is incorporated in the proposed system. The traditional voice activity detection algorithms which track only energy cannot successfully identify potential speech from input. VAD should be most sensitive as the unwanted part of the speech also has some energy and appears to be speech. This demands to use a VAD that also calculates energy of high frequency part separately as ZCR to differentiate noise from speech [3]. The preprocessed signal needs to be represented in a compact manner for further process. Speech carries much linguistic information which is required for recognition task. This feature extraction technique is based on Mel scale. MFCC [4] overcomes this limitation. Vocal tract including tongue, teeth etc. determines the sound generated by the human. The shape of the vocal tract determines what sound comes out. If we are able to determine this shape, accurate representation of the speech sample can be produced. The shape of the vocal tract manifests itself in the envelope of the short time power spectrum, and the job of MFCCs is to accurately represent this envelope. MFCC is proved to be more efficient by comparing MFCC with other feature extraction techniques.[5]. In recent years Neural Network is beginning to gain importance. This paved a path for efficient deployment of recognizer. The usability of neural network for speech recognizer proved to be a suitable technology [6] and it outperforms traditional recognizers like Hidden Markov Model (HMM), Dynamic Time Warping (DTW), Vector Quantization (VQ) etc. The promising results were obtained using Radial Basis Neural Network as recognizer [7]. It also is found that RBF trains and tests faster than Multilayer Perceptron (MLP) Neural Networks. Moreover RBF gives more accurate result than MLP [8]. In the proposed system GRNN [9] is used as recognizer. A Generalized Regression Neural Network is often used for function approximation. It is similar to Radial Basis Neural Network but has a special linear layer in addition to RBF network. It is found that the performance of GRNN is superior to the other classifiers namely Linear and MLP Neural Networks [10]."}, {"heading": "2. PREPROCESSING", "text": ""}, {"heading": "2.1 SPEECH ACQUISITION", "text": "An acoustic speech signal exists as pressure\nvariations in the air. A microphone converts these pressure variations into an electric current. To process the speech signal digitally, it is necessary to make the analog\nwaveform discrete in both time (sample) and amplitude (quantize). This A-to-D conversion is generally accomplished by digital signal processing hardware on the computer\u201fs sound card. The spoken voice frequency lies between 300 to 3400 Hertz. So a sampling frequency of 8 KHz is chosen to satisfy Nyquist criterion. The speech signal is recorded with a sampling rate of 8 KHz.. And the data is saved with distinct name with \u201e.wav\u201f as extension."}, {"heading": "2.2 VOICE ACTIVITY DETECTION", "text": "Voice activity detection is a technique to detect\nthe un-silenced part of the incoming speech signal. In general, the acquired speech signal of 1 second duration (8000 samples) starts and ends with silence which accounts for nearly 6000 samples. VAD can be performed by two algorithms. The first algorithm uses signal features based on energy level and the second algorithm uses signal features based on the rate of zero crossings. The combination of both gives good result that is used in the proposed system. A basic VAD works on the principle of extracting measured features from the incoming audio signal, which is divided into frame size of 150 ms duration and frame increment size of 40 ms duration. The extracted signal features based on energy level and zero crossing rate from the audio signal are then compared to a threshold and then VAD decision is computed.\nSTEP 1: If the feature of the input frame exceed the estimated threshold value, a VAD decision (VAD = 1) is computed which declares that speech is present.\nSTEP 2: Otherwise, a VAD decision (VAD = 0) is computed which declares the absence of speech in the input frame."}, {"heading": "2.3 PRE-EMPHASIS", "text": "In order to flatten speech spectrum, a pre-emphasis\nfilter is used before spectral analysis. Its aim is to compensate the high-frequency part of the speech signal that was suppressed during the human sound production mechanism. Thus high frequency signals have less amplitude. This gives rise to a negative spectral slope and is compensated by appropriate pre-emphasis filter. The ztransform of the filter is given by [2]:\n- - (1)\nWhere \u201ea\u201f is the filter coefficient and is chosen as 0.9375\nThe output signal after pre-emphasis is given by [2]:\n- - (2)\n(2.2)\n(2.1)\nWhere y(n) - output signal s(n) - input speech signal"}, {"heading": "2.4 FRAMING", "text": "Speech is a highly non-stationary signal. Hence\nspeech analysis must be carried out on short segments across which the speech signal is assumed to be stationary. For this the speech signal is divided into frames of small duration typically 20 to 40ms with overlap of 10 to 15ms for short-time spectral analysis."}, {"heading": "2.5 WINDOWING", "text": "Windowing minimizes the discontinuities by\ntapering the signals to quite small values (nearly zeros) at the edges of a frame. Hamming window is also called the raised cosine window. The Hamming window defined as [3]\n- (3)\nThe output speech signal can be described as\n(4)\nEach frame s(n) is multiplied by hamming window w(n) of 25ms duration.\n3. FEATURE EXTRACTION\nFeature extraction is the process of distinguishing features from the input signal. It is about reducing the dimensionality of the input-vector while still maintaining the uniqueness of the signal. Good features possess the attributes of being simple to extract, invariant against time translations and overall amplitude variation, and capable of discriminating different phonetic categories. Features are extracted using MFCC technique [4]."}, {"heading": "3.1 MEL-FREQUENCY CEPSTRAL COEFFICIENT ALGORITHM", "text": "MFCCs are commonly derived as follows:\n On short time scales the audio signal doesn't\nchange much. The frame time chosen is 20-40ms frames.\n The power spectrum of each frame is passed\nthrough the FFT block. FFT is used for identifying the frequencies present in the frame.\n The logarithmic compression is performed to\nmatch features more closely to what humans actually hear\n The periodogram spectral estimate still contains\na lot of information not required for Automatic Speech Recognition (ASR). For this reason we take clumps of periodogram bins and sum them up to get an idea of how much energy exists in various frequency regions. This is performed by our Mel filter bank. The Mel scale tells us exactly how to space our filter banks and how wide to make them.\n The inverse fast Fourier transform of the Mel log\npower gives the MFCC coefficients."}, {"heading": "4. SPEECH RECOGNIZER USING NEURAL NETWORK", "text": "The role of a recognizer is to assign the feature\nvector provided by the feature extractor to a category. It aims at measuring the similarity between an input speech and a reference pattern or model which is obtained during training. A neural network is a computational program that is designed to model the way in which the brain performs a particular task or function of interest; the network is usually implemented using electronic components or simulated in software on a digital computer."}, {"heading": "4.1 PROPOSED NN RECOGNIZER", "text": "The goal of speech regression analysis is to\nmodel a recognizer by providing a finite set of observations of speech and its associated target values such that it gives most probable output value for an unknown speech input. The regression equation can be expressed as [9]\n(5)\n(6)\nwhere wij is the target output corresponding to input\ntraining vector xi and j th output.\nThe above equations (5) and (6) can be implemented as the\nfollowing neural network structure.\n The input layer has neurons which represent the\ninput patterns. It passes the input vector to each of the neuron in the pattern layer.\n The pattern layer has one neuron for each pattern.\nDuring training all variations of input vectors and its associated desired output or target (wij) will be provided to the network. The neuron in the pattern layer compute Ci for the input provided to it.\nDuring testing the neuron will compute hi using Ci and Xi as shown below[9]:\n(7)\n(8)\nwhere is the spread (width) of the kernel function.\nSpread denotes the distance an input vector must\nbe from a neuron's Ci in order to be correctly classified. The value of spread chosen for the implemented network is 0.5.The summation layer has two units N and D where N\ncomputes and D calculates\n.\nThe output unit divides N by D which gives the prediction result. The above implementation is illustrated in the figure shown below:\nThe above neural network is known as General\nRegression Neural Network (GRNN). It is one of the radial basis networks. The hidden layer neurons of this network are referred as radial basis neurons as they employ radial basis kernel function. The proposed network does superior function approximation and thus is well suited for speech recognition."}, {"heading": "5. DATABASE CREATION", "text": "In order to facilitate the training and testing of the\nrecognizer, speech database is required. A variety of speech samples were obtained from different speakers to form the speech database. Three kinds of databases were created for\n Digit Recognition\n Word recognition\n Transcription of title\n5.1 Digit Recognition:\nThe digit database consists of ten utterances \u201ezero\u201f\nto \u201enine\u201f collected from both male and female speakers. The database is divided into training and testing. Training set is used to train the neural network. Testing set is used to test the performance of neural network. For speaker dependent recognition, excellent recognition accuracy closer to 100% is achieved. This recognizer can be very well adapted for voice dialing."}, {"heading": "6. RESULTS AND DISCUSSION", "text": "6.1 Syllable Level Word Recognition:\nTo perform syllable based implementation of word\nrecognition, a separate database was created with speech samples from 2 female speakers. 36 syllables, namely, \u201ebe\u201f, \u201ecom\u201f, \u201ein\u201f, \u201eout\u201f, \u201eply\u201f, \u201epose\u201f, \u201epress\u201f, \u201eside\u201f, \u201esup\u201f , \u201ein\u201f, \u201evi\u201f, \u201eta\u201f, \u201etion\u201f, \u201eti\u201f, \u201ema\u201f, \u201eno\u201f, \u201ere\u201f, \u201epo\u201f, \u201esi\u201f, \u201elo\u201f, \u201ede\u201f, \u201edi\u201f, \u201eca, \u201ecar\u201f, \u201ena\u201f, \u201emo\u201f, \u201ego\u201f, \u201eto\u201f, \u201elo\u201f, \u201ela\u201f, \u201era\u201f, \u201eing\u201f, \u201evo\u201f, \u201ere\u201f, \u201eva\u201f, \u201edo\u201f were selected. 4 samples were taken for each word. The number of words in a language is numerous. Training a network for each word requires huge memory which is not realistic. To reduce memory requirement, language structure can be exploited. The number of syllables in a language is comparatively much lesser than that of words. A word is formed by the combination of one or more syllables and many words could be built up from a single syllable. This idea has lead to training of another GRNN recognizer for syllables instead of complete words. The recognizer was designed with 36 syllables from which 80 plus words could be formed. The testing of the recognizer gave heartening results.\n6.2 Transcription:\nTranscription is a process of converting\ncontinuous speech into text. For real-time scenario, speech signal for recognition would never be an isolated word but rather a continuous stream of words. Syllable level recognition with minimum pauses can also extended for transcription purposes. To perform transcription of the title \u201cSPEAKER INDEPENDENT CONTINUOUS SPEECH TO TEXT CONVERSION SYSTEM\u201d a separate database was created, consisting of each word. The samples were collected from both male and female speakers.\nTable.2 Recognition Accuracy\n* Accuracy = Number of words correctly recognized Total number of words\n Comparison chart for elapsed time\nThe table consisting the correlation values\nbetween two same words ( ba and ba ), between two similar sounding words ( ba and pa), between two distinct word(one and eight), elapsed time and correlation comparison with noise is shown. The correlation analysis has been performed on RASTAPLP, MFCC, BFCC, RPLP, MFPLP, PLP features. From the performance chart given above it is obvious that the MFCC technique gives well distinct features than other techniques."}, {"heading": "7. SUMMARY AND CONCLUSION", "text": "In this work, a continuous speech to text conversion system has been modeled using VAD, MFCC and GRNN network.. The proposed model has several advantageous characteristics such as fast learning, flexible network size and robustness to speaker variability (ability to recognize the same words pronounced in various manners). GRNN promises to be a successful and powerful alternative to the conventional speech recognizers.\nThe designed recognition system with all the above\nsalient features has been utilized for developing a digit recognition system, a syllable level word recognition system and a transcription system. Digit recognition can find applications in voice dialing systems. And it needs to be a speaker independent system. In syllable level word recognition system, the recognizer was trained for syllables. These syllables were separately recognized and concatenated to output the complete word as text. In Transcription system, the duration of an input speech signal is made longer and the corresponding text output is obtained. The recognizer was trained for individual words. This system acts as a basis for real time speech recognition products, where input will never be a single word. Good recognition accuracy has been achieved in both cases. These implementations illustrate the potential of optimal configurations of key ASR components.\nTheoretically, the accuracy increases with the\nincrease in training data. As a result, memory needed also increases. It is found that carefully forming the database helps a lot in reducing memory requirements and increases recognition accuracy. In training phase, the words are spoken clearly so that it avoids general variations and confusions. In testing phase, the speech signal with minimum pauses should be given as an input. This enables the recognizer to discriminate the words effectively. For future work ,the language modeling of HMM can also be utilized in neural network implementation to build an efficient hybrid HMM-NN recognizer including better acoustic modeling accuracy, better context sensitivity, more natural discrimination and a more economical use of parameters."}], "references": [{"title": "A simple but efficient real-time Voice Activity Detection algorithm", "author": ["M.H. Moattar", "M.M. Homayounpour"], "venue": "European Signal Processing Conference (EUSIPCO),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Enhanced Voice Activity Detection Using Acoustic Event Detection", "author": ["Namgook Cho", "Eun-Kyoung Kim"], "venue": "And Classification\u201d,IEEE Transactions On Consumer Electronics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Chiu-Sing CHOY and Kong-Pang PUN, \u201cAn Efficient MFCC Extraction Method in Speech Recognition", "author": ["Wei HAN", "Cheong-Fat CHAN"], "venue": "IEEE International Symposium on Circuits and Systems (ISCAS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Mahesh Chandra", "author": ["A.N. Mishra"], "venue": "Astik Biswas, S. N. Sharan , \u201cRobust Features for Connected Hindi Digits Recognition\u201d, International Journal of Signal Processing, Image Processing and Pattern Recognition ,Vol. 4, No. 2, June", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Digit Recognition Using Neura l Networks", "author": ["Chin Luh Tan", "Adznan Jantan"], "venue": "Malaysian Journal Of Computer Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Neural Networks Used For Speech Recognition", "author": ["Wouter Gevaert", "Georgi Tsenov", "Valeri Mladenov"], "venue": "Journal Of Automatic Control, University of Belgrade,Vol.20,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "But at the same time, the VAD algorithm [2] should be of low complex to facilitate real time application.", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "This demands to use a VAD that also calculates energy of high frequency part separately as ZCR to differentiate noise from speech [3].", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "MFCC [4] overcomes this limitation.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "The usability of neural network for speech recognizer proved to be a suitable technology [6] and it outperforms traditional recognizers like Hidden Markov Model (HMM), Dynamic Time Warping (DTW), Vector Quantization (VQ) etc.", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Moreover RBF gives more accurate result than MLP [8].", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "The ztransform of the filter is given by [2]:", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "9375 The output signal after pre-emphasis is given by [2]:", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "The Hamming window defined as [3]", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "Features are extracted using MFCC technique [4].", "startOffset": 44, "endOffset": 47}], "year": 2013, "abstractText": "An efficient speech to text converter for mobile application is presented in this work. The prime motive is to formulate a system which would give optimum performance in terms of complexity, accuracy, delay and memory requirements for mobile environment. The speech to text converter consists of two stages namely front-end analysis and pattern recognition. The front end analysis involves preprocessing and feature extraction. The traditional voice activity detection algorithms which track only energy cannot successfully identify potential speech from input because the unwanted part of the speech also has some energy and appears to be speech. In the proposed system , VAD that calculates energy of high frequency part separately as zero crossing rate to differentiate noise from speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used as feature extraction method and Generalized Regression Neural Network is used as recognizer. MFCC provides low word error rate and better feature extraction. Neural Network improves the accuracy. Thus a small database containing all possible syllable pronunciation of the user is sufficient to give recognition accuracy closer to 100%. Thus the proposed technique entertains realization of real time speaker independent applications like mobile phones, PDAs etc.", "creator": "Microsoft\u00ae Office Word 2007"}}}