{"id": "1308.2218", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Aug-2013", "title": "Coding for Random Projections", "abstract": "The alternative 's precisely substantially has become seemed such up which - production applications ago inference learning, information retrieval, pnoc - nanoscience once own applications. Using was well - and repetition funding addition the projected according, result he/she the three the scraping needed work each overall values way how to 2020 these sheets, often significantly further 's increasing the the algorithm, 1998 storage revenue as actually is computational dash. In for paper, we study on others own subtle non-standard structuring, focusing on on difficult is similarity estimation and that an supports to staff linear classifiers. We creativity make uniform quantization readjusts for higher provides influential calculations (Datar nos. -. 1968 ). Indeed, get favor that in sometimes investigating real-time giving because a the than fact mix suffices. Furthermore, we also develop way non - uniform 31 - so coding scheme way generally choral but having practice, as admitted by going experiments end rehabilitation linear hoped formula_19 machines (SVM ).", "histories": [["v1", "Fri, 9 Aug 2013 19:50:24 GMT  (192kb)", "http://arxiv.org/abs/1308.2218v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.IT math.IT stat.CO", "authors": ["ping li 0001", "michael mitzenmacher", "anshumali shrivastava"], "accepted": true, "id": "1308.2218"}, "pdf": {"name": "1308.2218.pdf", "metadata": {"source": "CRF", "title": "Coding for Random Projections", "authors": ["Ping Li"], "emails": ["pl314@rci.rutgers.edu", "michaelm@eecs.harvard.edu", "anshu@cs.cornell.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 8.\n22 18"}, {"heading": "1 Introduction", "text": "The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25]. In this paper, we study a number of simple and effective schemes for coding the projected data, with the focus on similarity estimation and training linear classifiers [15, 23, 9, 2]. We will closely compare our method with the influential prior coding scheme in [8].\nConsider two high-dimensional vectors, u, v \u2208 RD. The idea is to multiply them with a random normal projection matrix R \u2208 RD\u00d7k (where k \u226a D), to generate two (much) shorter vectors x, y:\nx = u\u00d7R \u2208 Rk, y = v \u00d7R \u2208 Rk, R = {rij}Di=1kj=1, rij \u223c N(0, 1) i.i.d. (1)\nIn real applications, the dataset will consist of a large number of vectors (not just two). Without loss of generality, we use one pair of data vectors (u, v) to demonstrate our results.\nIn this study, for convenience, we assume that the marginal Euclidian norms of the original data vectors, i.e., \u2016u\u2016, \u2016v\u2016, are known. This assumption is reasonable in practice [18]. For example, the input data for feeding to a support vector machine (SVM) are usually normalized, i.e., \u2016u\u2016 = \u2016v\u2016 = 1. Computing the marginal norms for the entire dataset only requires one linear scan of the data, which is anyway needed during data collection/processing. Without loss of generality, we assume \u2016u\u2016 = \u2016v\u2016 = 1 in this paper. The joint distribution of (xj , yj) is hence a bi-variant normal:\n[\nxj yj\n] \u223c N ([\n0 0\n]\n,\n[\n1 \u03c1 \u03c1 1\n])\n, i.i.d. j = 1, 2, ..., k. (2)\nwhere \u03c1 = \u2211D i=1 uivi (assuming \u2016u\u2016 = \u2016v\u2016 = 1). For convenience and brevity, we also restrict our attention to \u03c1 \u2265 0, which is a common scenario in practice. Throughout the paper, we adopt the conventional notation for the standard normal pdf \u03c6(x) and cdf \u03a6(x):\n\u03c6(x) = 1\u221a 2\u03c0 e\u2212 x2 2 , \u03a6(x) =\n\u222b x\n\u2212\u221e \u03c6(x)dx (3)"}, {"heading": "1.1 Uniform Quantization", "text": "Our first proposal is perhaps the most intuitive scheme, based on a simple uniform quantization:\nh(j)w (u) = \u230axj/w\u230b , h(j)w (v) = \u230ayj/w\u230b (4)\nwhere w > 0 is the bin width and \u230a.\u230b is the standard floor operation, i.e., \u230az\u230b is the largest integer which is smaller than or equal to z. For example, \u230a3.1\u230b = 3, \u230a4.99\u230b = 4, \u230a\u22123.1\u230b = \u22124. Later in the paper we will also use the standard ceiling operation \u2308.\u2309. We show that the collision probability Pw = Pr ( h (j) w (u) = h (j) w (v) ) is a monotonically increasing function of the similarity \u03c1, making (4) a suitable coding scheme for similarity estimation and near neighbor search. The potential benefits of coding with a small number of bits arise because the (uncoded) projected data, xj = \u2211D i=1 uirij and yj = \u2211D\ni=1 virij , being real-valued numbers, are neither convenient/economical for storage and transmission, nor well-suited for indexing.\nSince the original data are assumed to be normalized, i.e., \u2016u\u2016 = \u2016v\u2016 = 1, the marginal distribution of xj (and yj) is the standard normal, which decays rapidly at the tail, e.g., 1 \u2212 \u03a6(3) = 10\u22123,\n1 \u2212 \u03a6(6) = 9.9 \u00d7 10\u221210. If we use 6 as cutoff, i.e., values with absolute value greater than 6 are just treated as \u22126 and 6, then the number of bits needed to represent the bin the value lies in is 1 + log2 \u2308 6 w \u2309\n. In particular, if we choose the bin width w \u2265 6, we can just record the sign of the outcome (i.e., a one-bit scheme). In general, the optimum choice of w depends on the similarity \u03c1 and the task. In this paper we focus on the task of similarity estimation (of \u03c1) and we will provide the optimum w values for all similarity levels. Interestingly, using our uniform quantization scheme, we find in a certain range the optimum w values are quite large, and in particular are larger than 6.\nWe can build linear classifier (e.g., linear SVM) using coded random projections. For example, assume the projected values are within (\u22126, 6). If w = 2, then the code values output by hw will be within the set {\u22123,\u22122,\u22121, 0, 1, 2}. This means we can represent a projected value using a vector of length 6 (with exactly one 1) and the total length of the new feature vector (fed to a linear SVM) will be 6 \u00d7 k. See more details in Section 6. This trick was also recently used for linear learning with binary data based on b-bit minwise hashing [19, 20]. Of course, we can also use the method to speed up kernel evaluations for kernel SVM with high-dimensional data.\nNear neighbor search is a basic problem studied since the early days of modern computing [12] with applications throughout computer science. The use of coded projection data for near neighbor search is closely related to locality sensitive hashing (LSH) [14]. For example, using k projections and a bin width w, we can naturally build a hash table with (\n2\u2308 6w \u2309 )k\nbuckets. We map every data vector in the dataset to one of the buckets. For a query data vector, we search for similar data vectors in the same bucket. Because the concept of LSH is well-known, we do not elaborate on the details. Compared to [8], our proposed coding scheme has better performance for near neighbor search; the analysis will be reported in a separate technical report. This paper focuses on similarity estimation."}, {"heading": "1.2 Advantages over the Window-and-Offset Coding Scheme", "text": "[8] proposed the following well-known coding scheme, which uses windows and a random offset:\nh(j)w,q(u) =\n\u230a\nxj + qj w\n\u230b\n, h(j)w,q(v) =\n\u230a\nyj + qj w\n\u230b\n(5)\nwhere qj \u223c uniform(0, w). [8] showed that the collision probability can be written as\nPw,q =Pr ( h(j)w,q(u) = h (j) w,q(v) ) =\n\u222b w\n0\n1\u221a d 2\u03c6\n(\nt\u221a d\n)(\n1\u2212 t w\n)\ndt (6)\nwhere d = ||u \u2212 v||2 = 2(1 \u2212 \u03c1) is the Euclidean distance between u and v. The difference between (5) and our proposal (4) is that we do not use the additional randomization with q \u223c uniform(0, w) (i.e., the offset). By comparing them closely, we will demonstrate the following advantages of our scheme:\n1. Operationally, our scheme hw is simpler than hw,q.\n2. With a fixed w, our scheme hw is always more accurate than hw,q, often significantly so.\n3. For each coding scheme, we can separately find the optimum bin width w. We will show that the optimized hw is also more accurate than optimized hw,q, often significantly so.\n4. For a wide range of \u03c1 values (e.g., \u03c1 < 0.56), the optimum w values for our scheme hw are relatively large (e.g., > 6), while for the existing scheme hw,q, the optimum w values are small (e.g., about 1). This means hw requires a smaller number of bits than hw,q.\nIn summary, uniform quantization is simpler, more accurate, and uses fewer bits than the influential prior work [8] which uses the window with the random offset."}, {"heading": "1.3 Organization", "text": "In Section 2, we analyze the collision probability for the uniform quantization scheme and then compare it with the collision probability of the well-known prior work [8] which uses an additional random offset. Because the collision probabilities are monotone functions of the similarity \u03c1, we can always estimate \u03c1 from the observed (empirical) collision probabilities. In Section 3, we theoretically compare the estimation variances of these two schemes and conclude that the random offset step in [8] is not needed.\nIn Section 4, we develop a 2-bit non-unform coding scheme and demonstrate that its performance largely matches the performance of the uniform quantization scheme (which requires storing more bits). Interestingly, for certain range of the similarity \u03c1, we observe that only one bit is needed. Thus, Section 5 is devoted to comparing the 1-bit scheme with our proposed methods. The comparisons show that the 1-bit scheme does not perform as well when the similarity \u03c1 is high (which is often the case applications are interested in). In Section 6, we provide a set of experiments on training linear SVM using all the coding schemes we have studied. The experimental results basically confirm the variance analysis. Section 7 presents several directions for related future research. Finally, Section 8 concludes the paper."}, {"heading": "2 The Collision Probability of Uninform Quantization hw", "text": "To use our coding scheme hw (4), we need to evaluate Pw = Pr ( h (j) w (u) = h (j) w (v) ) , the collision probability. From practitioners\u2019 perspective, as long as Pw is a monotonically increasing function of the similarity \u03c1, it is a suitable coding scheme. In other words, it does not matter whether Pw has a closed-form expression, as long as we can demonstrate its advantage over the alternative [8], whose collision probability is denoted by Pw,q. Note that Pw,q can be expressed in a closed-form in terms of the standard \u03c6 and \u03a6 functions:\nPw,q = Pr ( h(j)w,q(u) = h (j) w,q(v) ) =2\u03a6\n(\nw\u221a d\n)\n\u2212 1\u2212 2\u221a 2\u03c0w/ \u221a d +\n2\nw/ \u221a d \u03c6\n(\nw\u221a d\n)\n(7)\nRecall d = 2(1\u2212 \u03c1) is the Euclidean distance \u2016u\u2212 v\u20162. It is clear that Pw,q \u2192 1 as w \u2192 \u221e.\nThe following Lemma 1 will help derive the collision probability Pw (in Theorem 1).\nLemma 1 Assume [\nx y\n] \u223c N ([\n0 0\n]\n,\n[\n1 \u03c1 \u03c1 1\n])\n, \u03c1 \u2265 0. Then\nQs,t (\u03c1) = Pr (x \u2208 [s, t], y \u2208 [s, t]) = \u222b t\ns \u03c6(z)\n[\n\u03a6\n(\nt\u2212 \u03c1z \u221a\n1\u2212 \u03c12\n) \u2212 \u03a6 ( s\u2212 \u03c1z \u221a\n1\u2212 \u03c12\n)]\ndz (8)\n\u2202Qs,t(\u03c1)\n\u2202\u03c1 =\n1\n2\u03c0\n1 (1\u2212 \u03c12)1/2 ( e \u2212 t 2 (1+\u03c1) + e \u2212 s 2 (1+\u03c1) \u2212 2e\u2212 t2+s2\u22122st\u03c1 2(1\u2212\u03c12) ) \u2265 0 (9)\nProof: See Appendix A.\nTheorem 1 The collision probability of the coding scheme hw defined in (4) is\nPw = 2\n\u221e \u2211\ni=0\n\u222b (i+1)w\niw \u03c6(z)\n[\n\u03a6\n(\n(i+ 1)w \u2212 \u03c1z \u221a\n1\u2212 \u03c12\n) \u2212 \u03a6 ( iw \u2212 \u03c1z \u221a\n1\u2212 \u03c12\n)]\ndz (10)\nwhich is a monotonically increasing function of \u03c1. In particular, when \u03c1 = 0, we have\nPw = 2 \u221e \u2211\ni=0\n[\u03a6 ((i+ 1)w) \u2212 \u03a6 (iw)]2 (11)\nProof: The proof follows from Lemma 1 by using s = iw and t = (i+ 1)w, i = 0, 1, ....\nFigure 1 plots both Pw and Pw,q for selected \u03c1 values. The difference between Pw and Pw,q becomes apparent after about w > 2. For example, when \u03c1 = 0, Pw quickly approaches the limit 0.5 while Pw,q keeps increasing (to 1) as w increases. Intuitively, the fact that Pw,q \u2192 1 when \u03c1 = 0, is undesirable because it means two orthogonal vectors will have the same coded value. Thus, it is not surprising that our proposed scheme hw will have better performance than hw,q. We will analyze their theoretical variances to provide precise comparisons.\n3 Analysis of Two Coding Schemes (hw and hw,q) for Similarity Estimation\nIn both schemes (corresponding to hw and hw,q), the collision probabilities Pw and Pw,q are monotonically increasing functions of the similarity \u03c1. Since there is a one-to-one mapping between \u03c1 and Pw, we can tabulate Pw for each \u03c1 (for example, at a precision of 10\u22123). From k independent projections, we can compute the empirical P\u0302w and P\u0302w,q and find the estimates, denoted by \u03c1\u0302w and \u03c1\u0302w,q, respectively, from the tables. In this section, we compare the estimation variances for these two estimators, to demonstrate advantage of the proposed coding scheme hw.\nTheorem 2 provides the variance of hw, for estimating \u03c1 from k random projections.\nTheorem 2\nV ar (\u03c1\u0302w,q) = Vw,q k +O\n(\n1\nk2\n)\n, where (12)\nVw,q = d 2/4\n\n\nw/ \u221a d\n\u03c6 ( w/ \u221a d ) \u2212 1/ \u221a 2\u03c0\n\n\n2\nPw,q(1\u2212 Pw,q), d = 2(1\u2212 \u03c1) (13)\nProof: See Appendix B.\nFigure 2 plots the variance factor Vw,q defined in (13) without the d 2 4 term. (Recall d = 2(1 \u2212 \u03c1).) The minimum is 7.6797 (keeping four digits), attained at w/ \u221a d = 1.6476. The plot also suggests that the performance of this popular scheme can be sensitive to the choice of the bin width w. This is a practical disadvantage. Since we do not know \u03c1 (or d) in advance and we must specify w in advance, the performance of this scheme might be unsatisfactory, as one can not really find one \u201coptimum\u201d w for all pairs in a dataset.\nIn comparison, our proposed scheme has smaller variance and is not as sensitive to the choice of w.\nTheorem 3\nV ar (\u03c1\u0302w) = Vw k +O\n(\n1\nk2\n)\n, where (14)\nVw = \u03c02(1\u2212 \u03c12)Pw(1\u2212 Pw) [\n\u2211\u221e i=0\n(\ne \u2212 (i+1)\n2w2\n(1+\u03c1) + e \u2212 i\n2w2 (1+\u03c1) \u2212 2e\u2212 w2 2(1\u2212\u03c12) e\u2212 i(i+1)w2 1+\u03c1\n)]2 (15)\nIn particular, when \u03c1 = 0, we have\nVw|\u03c1=0 = [ \u2211\u221e i=0 (\u03a6((i+ 1)w) \u2212 \u03a6(iw))2\n\u2211\u221e i=0 (\u03c6((i+ 1)w) \u2212 \u03c6(iw))2\n][\n1/2 \u2212\u2211\u221ei=0 (\u03a6((i+ 1)w) \u2212 \u03a6(iw))2 \u2211\u221e\ni=0 (\u03c6((i+ 1)w) \u2212 \u03c6(iw))2\n]\n(16)\nProof: See Appendix C.\nRemark: At \u03c1 = 0, the minimum is Vw = \u03c0 2 4 attained at w \u2192 \u221e, as shown in Figure 3. Note that when w \u2192 \u221e, we have \u2211\u221ei=0 (\u03a6((i+ 1)w) \u2212 \u03a6(iw)) 2 \u2192 1/4 and \u2211\u221ei=0 (\u03c6((i+ 1)w) \u2212 \u03c6(iw)) 2 \u2192 1/(2\u03c0), and hence Vw|\u03c1=0 \u2192 [ 1/4 1/(2\u03c0) ] [ 1/2\u22121/4 1/(2\u03c0) ] = \u03c0 2 4 . In comparison, Theorem 2 says that when \u03c1 = 0 (i.e., d = 2) we have Vw,q = 7.6797, which is significantly larger than \u03c02/4 = 2.4674.\nTo compare the variances of the two estimators, V ar (\u03c1\u0302w) and V ar (\u03c1\u0302w,q), we compare their leading constants, Vw and Vw,q. Figure 4 plots the Vw and Vw,q at selected \u03c1 values, verifying that (i) the variance of the proposed scheme (4) can be significantly lower than the existing scheme (5); and (ii) the performance of the proposed scheme is not as sensitive to the choice of w (e.g., when w > 2).\nIt is also informative to compare Vw and Vw,q at their \u201coptimum\u201d w values (for fixed \u03c1). Note that Vw is not sensitive to w once w > 1 \u223c 2. The left panel of Figure 5 plots the best values for Vw and Vw,q, confirming that Vw is significantly lower than Vw,q at smaller \u03c1 values (e.g., \u03c1 < 0.56).\nThe right panel of Figure 5 plots the optimum w values (for fixed \u03c1). Around \u03c1 = 0.56, the optimum w for Vw becomes significantly larger than 6 and may not be reliably evaluated. From the remark for Theorem 3, we know that at \u03c1 = 0 the optimum w grows to \u221e. Thus, we can conclude that if \u03c1 < 0.56, it suffices to implement our coding scheme using just 1 bit (i.e., signs of the projected data). In comparison, for the existing scheme hw,q, the optimum w varies much slower. Even at \u03c1 = 0, the optimum w is around 2. This means hw,q will always need to use more bits than hw, to code the projected data. This is another advantage of our proposed scheme.\nIn practice, we do not know \u03c1 in advance and we often care about data vector pairs of high similarities. When \u03c1 > 0.56, Figure 4 and Figure 5 illustrate that we might want to choose small w values (e.g., w < 1). However, using a small w value will hurt the performance in pairs of the low similarities. This dilemma motivates us to develop non-uniform coding schemes."}, {"heading": "4 A 2-Bit Non-Uniform Coding Scheme", "text": "If we quantize the projected data according to the four regions (\u2212\u221e,\u2212w), [\u2212w, 0), [0, w), [w,\u221e), we obtain a 2-bit non-uniform scheme. At the risk of abusing notation, we name this scheme \u201chw,2\u201d, not to be confused with the name of the existing scheme hw,q.\nAccording to Lemma 1, hw,2 is also a valid coding scheme. We can theoretically compute the collision probability, denoted by Pw,2, which is again a monotonically increasing function of the similarity \u03c1. With k projections, we can estimate \u03c1 from the empirical observation of Pw,2 and we denote this estimator by \u03c1\u0302w,2.\nTheorem 4 provides the expressions for Pw,2 and V ar (\u03c1\u0302w,2).\nTheorem 4\nPw,2 = Pr ( h (j) w,2(u) = h (j) w,2(v) ) =\n{\n1\u2212 1 \u03c0 cos\u22121 \u03c1\n}\n\u2212 4 \u222b w\n0 \u03c6(z)\u03a6\n(\n\u2212w + \u03c1z \u221a\n1\u2212 \u03c12\n)\ndz (17)\nV ar (\u03c1\u0302w,2) = Vw,2 k +O\n(\n1\nk2\n)\n, where Vw,2 = \u03c02(1\u2212 \u03c12)Pw,2(1\u2212 Pw,2) [\n1\u2212 2e\u2212 w2 2(1\u2212\u03c12) + 2e\u2212 w2 1+\u03c1\n]2 (18)\nProof: See Appendix D.\nFigure 6 plots Pw,2 (together with Pw) for selected \u03c1 values. When w > 1, Pw,2 and Pw largely overlap. For small w, the two probabilities behave very differently, as expected. Note Pw,2 has the same value at w = 0 and w = \u221e, and in fact, when w = 0 or w = \u221e, we just need one bit (i.e., the signs). Note that Pw,2 and Pw differ significantly at small w. Will this be beneficial? The answer again depends on \u03c1.\nFigure 7 plots both Vw,2 and Vw at selected \u03c1 values, to compare their variances. For \u03c1 \u2264 0.5, the variance of the estimator using the 2-bit scheme hw,2 is significantly lower than that of hw. However, when \u03c1 is high, Vw,2 might be somewhat higher than Vw. This means that, in general, we expect the performance of hw,2 will be similar to hw. When applications mainly care about highly similar data pairs, we expect hw will have (slightly) better performance (at the cost of more bits).\nFinally, Figure 8 presents the smallest Vw,2 values and the optimum w values at which the smallest Vw,2 are attained. This plot verifies that hw and hw,2 should perform very similarly, although hw will have better performance at high \u03c1. Also, for a wide range, e.g., \u03c1 \u2208 [0.2 0.62], it is preferable to implement hw,2 using just 1 bit because the optimum w values are large.\n5 The 1-Bit Scheme h1 and Comparisons with hw,2 and hw\nWhen w > 6, it is sufficient to implement hw or hw,2 using just one bit, because the normal probability density decays very rapidly: 1 \u2212 \u03a6(6) = 9.9 \u00d7 10\u221210. Note that we need to consider a very small tail probability because there are many data pairs in a large dataset, not just one pair. With the 1-bit scheme, we simply code the projected data by recording their signs. We denote this scheme by h1, and the corresponding collision probability by P1, and the corresponding estimator by \u03c1\u03021.\nFrom Theorem 4, by setting w = 0 (or equivalently w = \u221e), we can directly infer\nP1 = Pr ( h (j) 1 (u) = h (j) 1 (v) ) = 1\u2212 1 \u03c0 cos\u22121 \u03c1 (19)\nV ar (\u03c1\u03021) = V1 k +O\n(\n1\nk2\n)\n, where V1 = \u03c0 2(1\u2212 \u03c12)P1(1\u2212 P1) (20)\nThis collision probability is widely known [13]. The work of [4] also popularized the use 1-bit coding. The variance was analyzed and compared with a maximum likelihood estimator in [18].\nFigure 9 and Figure 10 plot the ratios of the variances: V ar(\u03c1\u03021)V ar(\u03c1\u0302w) and V ar(\u03c1\u03021) V ar(\u03c1\u0302w,2) , to illustrate how much we lose in accuracy by using only one bit. Note \u03c1\u03021 is not related to the bin width w while V ar (\u03c1\u0302w) and V ar (\u03c1\u0302w,2) are functions of w. In Figure 9, we plot the maximum values of the ratios, i.e., we use the smallest V ar (\u03c1\u0302w) and V ar (\u03c1\u0302w,2) at each \u03c1. The ratios demonstrate that potentially both hw and hw,2 could substantially outperform h1, the 1-bit scheme.\nNote that in Figure 9, we plot 1 \u2212 \u03c1 in the horizontal axis with log-scale, so that the high similarity region can be visualized better. In practice, many applications are often more interested in the high similarity region, for example, duplicate detections.\nIn practice, we must pre-specify the quantization bin width w in advance. Thus, the improvement of hw and hw,2 over the 1-bit scheme h1 will not be as drastic as shown in Figure 9. For more realistic comparisons, Figure 10 plots V ar(\u03c1\u03021)V ar(\u03c1\u0302w) and V ar(\u03c1\u03021) V ar(\u03c1\u0302w,2)\n, for fixed w values. This figure advocates the recommendation of the 2-bit coding scheme hw,2:\n1. In the high similarity region, hw,2 significantly outperforms h1. The improvement drops as w becomes larger (e.g., w > 1). hw also works well, in fact better than hw,2 when w is small.\n2. In the low similarity region, hw,2 still outperforms h1 unless \u03c1 is very low and w is not small. Note that the performance of hw is noticeably worse than hw,2 and h1 when \u03c1 is low.\nThus, we believe the 2-bit scheme hw,2 with w around 0.75 provides an overall good compromise. In fact, this is consistent with our observation in the SVM experiments in Section 6.\nCan we simply use the 1-bit scheme? When w = 0.75, in the high similarity region, the variance ratio V ar(\u03c1\u03021)V ar(\u03c1\u0302w,2) is between 2 and 3. Note that, per projected data value, the 1-bit scheme requires 1 bit but the 2-bit scheme needs 2 bits. In a sense, the performance of hw,2 and h1 is actually similar in terms of the total number bits to store the (coded) projected data, according the analysis in this paper.\nFor similarity estimation, we believe it is preferable to use the 2-bit scheme, for the following reasons:\n\u2022 The processing cost of the 2-bit scheme would be lower. If we use k projections for the 1-bit scheme and k/2 projections for the 2-bit scheme, although they have the same storage cost, the processing cost of hw,2 for generating the projections would be only 1/2 of h1. For very high-dimensional data, the processing cost can be substantial.\n\u2022 As we will show in Section 6, when we train a linear classifier (e.g., using LIBLINEAR), we need to expand the projected data into a binary vector with exact k 1\u2019s if we use k projections for both h1 and hw,2. For this application, we observe the training time is mainly determined by the number of nonzero entries and the quality of the input data. Even with the same k, we observe the training speed on the input data generated by hw,2 is often slightly faster than using the data generated by h1.\n\u2022 In this study, we restrict our attention to linear estimators (which can be written as inner products)\nby simply using the (overall) collision probability, e.g., Pw,2 = Pr ( h (j) w,2(u) = h (j) w,2(v) ) . There is significant room for improvement by using more refined estimators. For example, we can treat this problem as a contingency table whose cell probabilities are functions of the similarity \u03c1 and hence we can estimate \u03c1 by solving a maximum likelihood equation. Such an estimator is still useful for many applications (e.g., nonlinear kernel SVM). We will report that work separately, to maintain the simplicity of this paper.\nNote that quantization is a non-reversible process. Once we quantize the data by the 1-bit scheme, there is no hope of recovering any information other than the signs. Our work provides the necessary theoretical justifications for making practical choices of the coding schemes."}, {"heading": "6 An Experimental Study for Training Linear SVM", "text": "We conduct experiments with random projections for training (L2-regularized) linear SVM (e.g., LIBLINEAR [9]) on three high-dimensional datasets: ARCENE, FARM, URL, which are available from the UCI repository. The original URL dataset has about 2.4 million examples (collected in 120 days) in 3231961 dimensions. We only used the data from the first day, with 10000 examples for training and 10000 for testing. The FARM dataset has 2059 training and 2084 testing examples in 54877 dimensions. The ARCENE dataset contains 100 training and 100 testing examples in 10000 dimensions.\nWe implement the four coding schemes studied in this paper: hw,q, hw, hw,2, and h1. Recall hw,q [8] was based on uniform quantization plus a random offset, with bin width w. Here, we first illustrate exactly how we utilize the coded data for training linear SVM. Suppose we use hw,2 and w = 0.75. We can code an original projected value x into a vector of length 4 (i.e., 2-bit):\nx \u2208 (\u2212\u221e \u2212 0.75) \u21d2 [1 0 0 0], x \u2208 [\u22120.75 0) \u21d2 [0 1 0 0], x \u2208 [0 0.75) \u21d2 [0 0 1 0], x \u2208 [0.75 \u221e) \u21d2 [0 0 0 1]\nThis way, with k projections, for each feature vector, we obtain a new vector of length 4k with exactly k 1\u2019s. This new vector is then fed to a solver such as LIBLINEAR. Recently, this strategy was adopted for linear learning with binary data based on b-bit minwise hashing [19, 20].\nSimilarly, when using h1, the dimension of the new vector is 2k with exactly k 1\u2019s. For hw and hw,q, we must specify a cutoff value such as 6 otherwise they are \u201cinfinite precision\u201d schemes. Practically speaking, because the normal density decays very rapidly at the tail (e.g., 1\u2212\u03a6(6) = 9.9\u00d7 10\u221210), we essentially do not suffer from information loss if we choose a large enough cutoff such as 6.\nFigure 11 reports the test accuracies on the URL data, for comparing hw,q with hw. The results basically confirm our analysis of the estimation variances. For small bin width w, the two schemes perform very similarly. However, when using a relatively large w, the scheme hw,q suffers from noticeable reduction of classification accuracies. The experimental results on the other two datasets demonstrate the same phenomenon. This experiment confirms that the step of random offset in hw,q is not needed, at least for similarity estimation and training linear classifiers.\nThere is one tuning parameter C in linear SVM. Figure 11 reports the accuracies for a wide range of C values, from 10\u22123 to 103. Before we feed the data to LIBLINEAR, we always normalize them to have unit norm (which is a recommended practice). Our experience is that, with normalized input data, the best accuracies are often attained around C = 1, as verified in Figure 11. For other figures in this section, we will only report C from 10\u22123 to 10.\nFigure 12 reports the test classification accuracies (averaged over 20 repetitions) for the URL dataset. When w = 0.5 \u223c 1, both hw and hw,2 produce similar results as using the original projected data. The 1-bit scheme h1 is obviously less competitive. We provide similar plots (Figure 13) for the FARM dataset.\nWe summarize the experiments in Figure 14 for all three datasets. The upper panels report, for each k, the best (highest) test classification accuracies among all C values and w values (for hw,2 and hw). The results show a clear trend: (i) the 1-bit (h1) scheme produces noticeably lower accuracies compared to others; (ii) the performances of hw,2 and hw are quite similar. The bottom panels of Figure 14 report the w values at which the best accuracies were attained. For hw,2, the optimum w values are often close to 1. One interesting observation is that for the FARM dataset, using the coded data (by hw or hw,2) can actually produce better accuracy than using the original (uncoded) data, when k is not large. This phenomenon may not be too surprising because quantization may be also viewed as some form of regularization and in some cases may help boost the performance."}, {"heading": "7 Future Work", "text": "This paper only studies linear estimators, which can be written as inner products. Linear estimators are extremely useful because they allow highly efficient implementation of linear classifiers (e.g., linear SVM) and near neighbor search methods using hash tables. For applications that allow nonlinear estimators (e.g., nonlinear kernel SVM), we can substantially improve linear estimators by solving nonlinear MLE (maximum likelihood) equations. The analysis will be reported separately.\nOur work is, to an extent, inspired by the recent work on b-bit minwise hashing [19, 20], which also proposed a coding scheme for minwise hashing and applied it to learning applications where the data are binary and sparse. Our work is for general data types, as opposed to binary, sparse data. We expect coding methods will also prove valuable for other variations of random projections, including the count-min sketch [5] and related variants [26] and very sparse random projections [17]. Another potentially interesting future direction is to develop refined coding schemes for improving sign stable projections [21] (which are useful for \u03c72 similarity estimation, a popular similarity measure in computer vision and NLP)."}, {"heading": "8 Conclusion", "text": "The method of random projections has become a standard algorithmic approach for computing distances or correlations in massive, high-dimensional datasets. A compact representation (coding) of the projected data is crucial for efficient transmission, retrieval, and energy consumption. We have compared a simple scheme based on uniform quantization with the influential coding scheme using windows with a random offset [8]; our scheme appears operationally simpler, more accurate, not as sensitive to parameters (e.g., the widow/bin width w), and uses fewer bits. We furthermore develop a 2-bit non-uniform coding scheme which performs similarly to uniform quantization. Our experiments with linear SVM on several real-world highdimensional datasets confirm the efficacy of the two proposed coding schemes. Based on the theoretical analysis and empirical evidence, we recommend the use of the 2-bit non-uniform coding scheme with the first bin width w = 0.7 \u223c 1, especially when the target similarity level is high."}, {"heading": "A Proof of Lemma 1", "text": "The joint density function of (x, y) is f(x, y; \u03c1) = 1 2\u03c0 \u221a 1\u2212\u03c12 e \u2212x\n2 \u22122\u03c1xy+y2\n2(1\u2212\u03c12) , \u22121 \u2264 \u03c1 \u2264 1. In this paper\nwe focus on \u03c1 \u2265 0. We use the usual notation for standard normal pdf and cdf: \u03c6(x) = 1\u221a 2\u03c0 e\u2212x 2/2, \u03a6(x) = \u222b x \u2212\u221e \u03c6(x)dx. The probability Qs,t can be simplified to be\nQs,t =\n\u222b t\ns\n\u222b t\ns\n1\n2\u03c0 \u221a 1\u2212 \u03c12 e \u2212x\n2 \u22122\u03c1xy+y2\n2(1\u2212\u03c12) dxdy\n=\n\u222b t\ns\n\u222b t\ns\n1\n2\u03c0 \u221a 1\u2212 \u03c12 e \u2212x\n2 \u22122\u03c1xy+y2\n2(1\u2212\u03c12) dydx\n=\n\u222b t\ns\n1\n2\u03c0 \u221a 1\u2212 \u03c12 e\u2212\nx2\n2\n\u222b t\ns e \u2212 (y\u2212\u03c1x)\n2\n2(1\u2212\u03c12) dydx\n=\n\u222b t\ns\n1\n2\u03c0 \u221a 1\u2212 \u03c12 e\u2212\nx2\n2\n\u222b t\u2212\u03c1x\u221a 1\u2212\u03c12\ns\u2212\u03c1x\u221a 1\u2212\u03c12\ne\u2212 u2 2 \u221a 1\u2212 \u03c12dudx\n=\n\u222b t\ns\n1\u221a 2\u03c0 e\u2212 x2 2\n\u222b t\u2212\u03c1x\u221a 1\u2212\u03c12\ns\u2212\u03c1x\u221a 1\u2212\u03c12\n1\u221a 2\u03c0 e\u2212 u2 2 dudx\n=\n\u222b t\ns\n1\u221a 2\u03c0 e\u2212 x2 2\n[\n\u03a6\n(\nt\u2212 \u03c1x \u221a\n1\u2212 \u03c12\n) \u2212 \u03a6 ( s\u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)]\ndx\nNext we evaluate its derivative \u2202Qs,t(\u03c1,s)\u2202\u03c1 .\n\u2202Qs,t(\u03c1, s)\n\u2202\u03c1 =\n\u222b t\ns\n1\u221a 2\u03c0 e\u2212 x2 2\n(\n\u03c6\n(\nt\u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)\n\u2212x+ t\u03c1 (1\u2212 \u03c12)3/2 \u2212 \u03c6\n(\ns\u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)\n\u2212x+ s\u03c1 (1\u2212 \u03c12)3/2\n)\ndx\nNote that\n\u2202\n\u2202\u03c1\n(\n\u03a6\n(\nt\u2212 \u03c1x \u221a\n1\u2212 \u03c12\n))\n= \u03c6\n(\nt\u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)\n\u2212x+ t\u03c1 (1\u2212 \u03c12)3/2\nand\n\u222b t\ns\n1\u221a 2\u03c0 e\u2212 x2 2 \u03c6\n(\nt\u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)\n\u2212x+ t\u03c1 (1\u2212 \u03c12)3/2 dx\n=\n\u222b t\ns\n1 2\u03c0 e \u2212x\n2+t2\u22122t\u03c1x 2(1\u2212\u03c12) \u2212x+ t\u03c1\n(1\u2212 \u03c12)3/2 dx\n=\n\u222b t\ns\n1 2\u03c0 e \u2212 (x\u2212t\u03c1)\n2\n2(1\u2212\u03c12) e\u2212 t2 2 \u2212x+ t\u03c1\n(1\u2212 \u03c12)3/2 dx\n= 1\n2\u03c0\n1\n(1\u2212 \u03c12)1/2 e \u2212t2/2 e\n\u2212 (x\u2212t\u03c1) 2\n2(1\u2212\u03c12)\n\u2223 \u2223 \u2223 \u2223\nt\ns\n= 1\n2\u03c0\n1\n(1\u2212 \u03c12)1/2 e \u2212t2/2\n[\ne \u2212 t 2(1\u2212\u03c1) 2(1+\u03c1) \u2212 e\u2212 (s\u2212t\u03c1)2 2(1\u2212\u03c12) ]\nand\n\u222b t\ns\n1\u221a 2\u03c0 e\u2212 x2 2 \u03c6\n(\ns\u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)\n\u2212x+ s\u03c1 (1\u2212 \u03c12)3/2 dx\n= 1\n2\u03c0\n1\n(1\u2212 \u03c12)1/2 e \u2212s2/2 e\n\u2212 (x\u2212s\u03c1) 2\n2(1\u2212\u03c12)\n\u2223 \u2223 \u2223 \u2223\nt\ns\n= 1\n2\u03c0\n1\n(1\u2212 \u03c12)1/2 e \u2212s2/2\n[\n\u2212e\u2212 s2(1\u2212\u03c1) 2(1+\u03c1) + e \u2212 (t\u2212s\u03c1) 2 2(1\u2212\u03c12) ]\nCombining the results, we obtain\n\u2202Qs,t(\u03c1, s)\n\u2202\u03c1\n=\n\u222b t\ns\n1\u221a 2\u03c0 e\u2212 x2 2\n(\n\u03c6\n(\nt\u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)\n\u2212x+ t\u03c1 (1\u2212 \u03c12)3/2 \u2212 \u03c6\n(\ns\u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)\n\u2212x+ s\u03c1 (1\u2212 \u03c12)3/2\n)\ndx\n= 1\n2\u03c0\n1\n(1\u2212 \u03c12)1/2 e \u2212t2/2\n[\ne \u2212 t 2(1\u2212\u03c1) 2(1+\u03c1) \u2212 e\u2212 (s\u2212t\u03c1)2 2(1\u2212\u03c12) ]\n\u2212 1 2\u03c0\n1\n(1\u2212 \u03c12)1/2 e \u2212s2/2\n[\n\u2212e\u2212 s2(1\u2212\u03c1) 2(1+\u03c1) + e \u2212 (t\u2212s\u03c1) 2 2(1\u2212\u03c12) ]\n= 1\n2\u03c0\n1 (1\u2212 \u03c12)1/2 ( e \u2212 t 2 (1+\u03c1) \u2212 e\u2212 t2+s2\u22122st\u03c1 2(1\u2212\u03c12) + e \u2212 s 2 (1+\u03c1) \u2212 e\u2212 t2+s2\u22122st\u03c1 2(1\u2212\u03c12) )\n= 1\n2\u03c0\n1 (1\u2212 \u03c12)1/2 ( e \u2212 t 2 (1+\u03c1) + e \u2212 s 2 (1+\u03c1) \u2212 2e\u2212 t2+s2\u22122st\u03c1 2(1\u2212\u03c12) )\n= 1\n2\u03c0\n1\n(1\u2212 \u03c12)1/2\n[\n(\ne \u2212 t\n2 2(1+\u03c1) \u2212 e\u2212 s2 2(1+\u03c1)\n)2\n+ 2e \u2212 t\n2+s2 2(1+\u03c1) \u2212 2e\u2212 t2+s2\u22122st\u03c1 2(1\u2212\u03c12)\n]\n\u22650\nThe last inequality holds because [\n\u2212 t 2 + s2\n2(1 + \u03c1)\n] \u2212 [ \u2212 t 2 + s2 \u2212 2st\u03c1 2(1\u2212 \u03c12) ]\n= 1\n2(1\u2212 \u03c12) [ \u2212(t2 + s2)(1\u2212 \u03c1) + t2 + s2 \u2212 2st\u03c1 ]\n= \u03c1\n2(1\u2212 \u03c12) [s\u2212 t] 2\n\u22650\nThis completes the proof."}, {"heading": "B Proof of Theorem 2", "text": "From the collision probability, Pw,q = 2\u03a6 (\nw\u221a d\n)\n\u2212 1 \u2212 2\u221a 2\u03c0w/ \u221a d + 2 w/ \u221a d \u03c6 ( w\u221a d ) , we can estimate d (and\n\u03c1). Recall d = 2(1 \u2212 \u03c1). We denote the estimator by d\u0302w,q (and \u03c1\u0302w,q), from the empirical probability P\u0302w,q, which is estimated without bias from k projections.\nNote that d\u0302w,q = g(P\u0302w,q) for a nonlinear function g. As k \u2192 \u221e, the estimator d\u0302w,q is asymptotically unbiased. The variance can be determined by the \u201cdelta\u201d method:\nV ar (\nd\u0302w,q\n) = V ar (\nP\u0302w,q\n)\n[ g\u2032 (Pw,q) ]2 +O\n(\n1\nk2\n)\n= 1\nk Pw,q (1\u2212 Pw,q)\n[ g\u2032 (Pw,q) ]2 +O\n(\n1\nk2\n)\nSince\n\u2202Pw,q \u2202d =\u2212 \u03c6 ( w\u221a d ) wd\u22123/2 \u2212 1\u221a 2\u03c0w d\u22121/2 + 1 w d\u22121/2\u03c6 ( w\u221a d ) + 1 w/ \u221a d \u03c6 ( w\u221a d )( w\u221a d ) wd\u22123/2\n= 1\nw/ \u221a d d\u22121\u03c6\n(\nw\u221a d\n)\n\u2212 1\u221a 2\u03c0w/ \u221a d d\u22121\nwe have\ng\u2032 (Pw,q) = 1\n\u2202Pw,q \u2202d\n= w/\n\u221a d\n\u03c6 ( w/ \u221a d ) \u2212 1/ \u221a 2\u03c0 d\nand\nV ar (\nd\u0302w,q\n) = d2\nk\n\n\nw/ \u221a d\n\u03c6 ( w/ \u221a d ) \u2212 1/ \u221a 2\u03c0\n\n\n2\nPw,q(1\u2212 Pw,q) +O ( 1\nk2\n)\nBecause \u03c1 = 1\u2212 d/2, we know that\nV ar (\u03c1\u0302w,q) = 1\n4 V ar\n(\nd\u0302w,q\n) = Vw,q k +O\n(\n1\nk2\n)\nwhere\nVw,q = d 2/4\n\n\nw/ \u221a d\n\u03c6 ( w/ \u221a d ) \u2212 1/ \u221a 2\u03c0\n\n\n2\nPw,q(1\u2212 Pw,q)\nThis completes the proof."}, {"heading": "C Proof of Theorem 3", "text": "This proof is similar to the proof of Theorem 2. To evaluate the asymptotic variance, we need to compute \u2202Pw \u2202\u03c1 :\n\u2202Pw \u2202\u03c1 = 1 \u03c0\n1 (1\u2212 \u03c12)1/2 \u221e \u2211\ni=0\n(\ne \u2212 (i+1)\n2w2\n(1+\u03c1) + e \u2212 i\n2w2 (1+\u03c1) \u2212 2e\u2212 (i+1)2w2+i2w2\u22122i(i+1)w2\u03c1 2(1\u2212\u03c12) )\n= 1\n\u03c0\n1 (1\u2212 \u03c12)1/2 \u221e \u2211\ni=0\n(\ne \u2212 (i+1)\n2w2\n(1+\u03c1) + e \u2212 i\n2w2 (1+\u03c1) \u2212 2e\u2212 w2 2(1\u2212\u03c12) e\u2212 i(i+1)w2 1+\u03c1\n)\nThus,\nV ar (\u03c1\u0302w) = Vw k +O\n(\n1\nk2\n)\n, where\nVw = \u03c02(1\u2212 \u03c12)Pw(1\u2212 Pw) [\n\u2211\u221e i=0\n(\ne \u2212 (i+1)\n2w2\n(1+\u03c1) + e \u2212 i\n2w2 (1+\u03c1) \u2212 2e\u2212 w2 2(1\u2212\u03c12) e \u2212 i(i+1)w 2 1+\u03c1\n)]2\nNext, we consider the special case with \u03c1 \u2192 0.\nPw|\u03c1=0 =2 \u221e \u2211\ni=0\n(\u03a6((i+ 1)w) \u2212 \u03a6(iw))2 = 2 \u221e \u2211\ni=0\n(\n\u222b (i+1)w\niw \u03c6(x)dx\n)2\n= 2w2 \u221e \u2211\ni=0\n( \u222b i+1\ni \u03c6(wx)dx\n)2\n\u2202Pw \u2202\u03c1\n\u2223 \u2223 \u2223 \u2223\n\u03c1=0\n= 1\n\u03c0\n\u221e \u2211\ni=0\n(\ne\u2212(i+1) 2w2/2 \u2212 e\u2212i2w2/2 )2 = 2\n\u221e \u2211\ni=0\n(\u03c6((i+ 1)w) \u2212 \u03c6(iw))2\nCombining the results, we obtain\nVw|\u03c1=0 = 2w2 \u2211\u221e i=0\n(\n\u222b i+1 i \u03c6(wx)dx\n)2 (\n1\u2212 2w2 \u2211\u221ei=0 ( \u222b i+1 i \u03c6(wx)dx\n)2 )\n(\n1 \u03c0 \u2211\u221e i=0 ( e\u2212(i+1) 2w2/2 \u2212 e\u2212i2w2/2\n)2 )2\n=\n\u2211\u221e i=0 (\u03a6((i+ 1)w) \u2212 \u03a6(iw))2\n(\n1/2 \u2212 \u2211\u221e i=0 (\u03a6((i+ 1)w)\u2212 \u03a6(iw))2 )\n(\n\u2211\u221e i=0 (\u03c6((i+ 1)w) \u2212 \u03c6(iw))\n2 )2"}, {"heading": "D Proof of Theorem 4", "text": "Pw,2 = Pr ( h (j) w,2(u) = h (j) w,2(v) )\n=2\n\u222b w\n0 \u03c6(x)\n[\n\u03a6\n(\nw \u2212 \u03c1x \u221a\n1\u2212 \u03c12\n) \u2212 \u03a6 ( \u2212\u03c1x \u221a\n1\u2212 \u03c12\n)]\ndx+ 2\n\u222b \u221e\nw \u03c6(x)\n[ 1\u2212 \u03a6 ( w \u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)]\ndx\n=2\n\u222b w\n0 \u03c6(x)\n[\n\u03a6\n(\nw \u2212 \u03c1x \u221a\n1\u2212 \u03c12\n) \u2212 1 + \u03a6 ( \u03c1x \u221a\n1\u2212 \u03c12\n)]\ndx+ 2\n\u222b \u221e\nw \u03c6(x)\n[ 1\u2212 \u03a6 ( w \u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)]\ndx\n=4\n\u222b w\n0 \u03c6(x)\n[\n\u03a6\n(\nw \u2212 \u03c1x \u221a\n1\u2212 \u03c12\n) \u2212 1 ] dx+ 2 \u222b w\n0 \u03c6(x)\u03a6\n(\n\u03c1x \u221a\n1\u2212 \u03c12\n)\ndx+ 2\n\u222b \u221e\n0 \u03c6(x)\n[ 1\u2212 \u03a6 ( w \u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)]\ndx\n=\u2212 4 \u222b w\n0 \u03c6(x)\u03a6\n(\n\u2212w + \u03c1x \u221a\n1\u2212 \u03c12\n)\ndx+ 2\n\u222b w\n0 \u03c6(x)\u03a6\n(\n\u03c1x \u221a\n1\u2212 \u03c12\n)\ndx+ 2\n\u222b \u221e\n0 \u03c6(x)\n[ 1\u2212 \u03a6 ( w \u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)]\ndx\n=1\u2212 1 \u03c0 cos\u22121 \u03c1\u2212 4\n\u222b w\n0 \u03c6(x)\u03a6\n(\n\u2212w + \u03c1x \u221a\n1\u2212 \u03c12\n)\ndx\nWe need to show\ng(\u03c1) =\n\u222b w\n0 \u03c6(x)\u03a6\n(\n\u03c1x \u221a\n1\u2212 \u03c12\n)\ndx+\n\u222b \u221e\n0 \u03c6(x)\n[ 1\u2212 \u03a6 ( w \u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)]\ndx = 1 2 \u2212 1 2\u03c0 cos\u22121 \u03c1\nBecause\ng\u2032(\u03c1) =\n\u222b w\n0 \u03c6(x)\u03c6\n(\n\u03c1x \u221a\n1\u2212 \u03c12\n)\nx (1\u2212 \u03c12)3/2xdx+ \u222b \u221e 0 \u03c6(x)\u03c6\n(\nw \u2212 \u03c1x \u221a\n1\u2212 \u03c12\n)\nx\u2212 \u03c1w (1\u2212 \u03c1)3/2 dx\n=\n\u222b w\n0\n1 2\u03c0 e \u2212 x\n2\n2(1\u2212\u03c12) x (1\u2212 \u03c12)3/2 dx+ \u222b \u221e\n0\n1 2\u03c0 e \u2212 (x\u2212\u03c1w)\n2+w2\u2212w2\u03c12 2(1\u2212\u03c12) x\u2212 \u03c1w\n(1\u2212 \u03c12)3/2xdx\n= 1\n2\u03c0\n1 (1\u2212 \u03c12)1/2 ( 1\u2212 e\u2212 w2 2(1\u2212\u03c12) ) + 1 2\u03c0\n1\n(1\u2212 \u03c12)1/2 e \u2212w\n2 2 e \u2212 \u03c1\n2w2\n2(1\u2212\u03c12)\n= 1\n2\u03c0\n1\n(1\u2212 \u03c12)1/2\nwe know\ng(\u03c1) =\n\u222b \u03c1\n0 g\u2032(\u03c1)d\u03c1+ g(0)\n= 1\n2\u03c0 sin\u22121 \u03c1+ (\u03a6(w) \u2212 1/2)/2 + (1\u2212\u03a6(w))/2\n= 1\n2\u03c0 sin\u22121 \u03c1+\n1 4 = 1 2 \u2212 1 2\u03c0 cos\u22121 \u03c1\nAlso,\n\u2202Pw,2 \u2202\u03c1 = 1 \u03c0 1 \u221a 1\u2212 \u03c12 \u2212 4\n\u222b w\n0 \u03c6(x)\u03c6\n(\n\u2212w + \u03c1x \u221a\n1\u2212 \u03c12\n)\nx\u2212 \u03c1w (1\u2212 \u03c12)3/2 dx\n= 1\n\u03c0\n1 \u221a 1\u2212 \u03c12 \u2212 4 2\u03c0\n1\n(1\u2212 \u03c12)1/2 e \u2212w\n2\n2\n(\ne \u2212 \u03c1\n2w2 2(1\u2212\u03c12) \u2212 e\u2212 (w\u2212\u03c1w)2 2(1\u2212\u03c12) )\n= 1\n\u03c0\n1 \u221a 1\u2212 \u03c12 \u2212 2 \u03c0 1 \u221a 1\u2212 \u03c12\n(\ne \u2212 w\n2\n2(1\u2212\u03c12) \u2212 e\u2212 w2 1+\u03c1\n)\nThus, combining the results, we obtain\nV ar (\u03c1\u0302w,2) = Vw,2 k +O\n(\n1\nk2\n)\nwhere\nVw,2 = \u03c02(1\u2212 \u03c12)Pw,2(1\u2212 Pw,2) [\n1\u2212 2e\u2212 w2 2(1\u2212\u03c12) + 2e\u2212 w2 1+\u03c1\n]2\nThis completes the proof."}], "references": [{"title": "Random projection in dimensionality reduction: Applications to image and text data", "author": ["Ella Bingham", "Heikki Mannila"], "venue": "In KDD,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Finding motifs using random projections", "author": ["Jeremy Buhler", "Martin Tompa"], "venue": "Journal of Computational Biology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Moses S. Charikar"], "venue": "In STOC,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "An improved data stream summary: the count-min sketch and its applications", "author": ["Graham Cormode", "S. Muthukrishnan"], "venue": "Journal of Algorithm,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In FOCS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Experiments with random projection", "author": ["Sanjoy Dasgupta"], "venue": "In UAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["Mayur Datar", "Nicole Immorlica", "Piotr Indyk", "Vahab S. Mirrokn"], "venue": "In SCG,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Experiments with random projections for machine learning", "author": ["Dmitriy Fradkin", "David Madigan"], "venue": "In KDD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Learning the structure of manifolds using random projections", "author": ["Yoav Freund", "Sanjoy Dasgupta", "Mayank Kabra", "Nakul Verma"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "An algorithm for finding nearest neighbors", "author": ["Jerome H. Friedman", "F. Baskett", "L. Shustek"], "venue": "IEEE Transactions on Computers,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1975}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["Michel X. Goemans", "David P. Williamson"], "venue": "Journal of ACM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1995}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani"], "venue": "In STOC,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Training linear svms in linear time", "author": ["Thorsten Joachims"], "venue": "In KDD,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Extensions of Lipschitz mapping into Hilbert space", "author": ["William B. Johnson", "Joram Lindenstrauss"], "venue": "Contemporary Mathematics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1984}, {"title": "Very sparse stable random projections for dimension reduction in l\u03b1 (0 < \u03b1 \u2264 2) norm", "author": ["Ping Li"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Improving random projections using marginal information", "author": ["Ping Li", "Trevor J. Hastie", "Kenneth W. Church"], "venue": "In COLT,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "b-bit minwise hashing", "author": ["Ping Li", "Arnd Christian K\u00f6nig"], "venue": "In WWW,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "One permutation hashing", "author": ["Ping Li", "Art B Owen", "Cun-Hui Zhang"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Sign stable projections, sign cauchy projections and chi-square kernels", "author": ["Ping Li", "Gennady Samorodnitsky", "John Hopcroft"], "venue": "Technical report,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Latent semantic indexing: A probabilistic analysis", "author": ["Christos H. Papadimitriou", "Prabhakar Raghavan", "Hisao Tamaki", "Santosh Vempala"], "venue": "In PODS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "The Random Projection Method", "author": ["Santosh Vempala"], "venue": "American Mathematical Society, Providence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Efficient nonnegative matrix factorization with random projections", "author": ["Fei Wang", "Ping Li"], "venue": "In SDM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Feature hashing for large scale multitask learning", "author": ["Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "We demonstrate that uniform quantization outperforms the standard existing influential method [8].", "startOffset": 94, "endOffset": 97}, {"referenceID": 20, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 4, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 0, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 1, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 8, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 16, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 9, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 22, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 5, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 14, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 23, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 13, "context": "In this paper, we study a number of simple and effective schemes for coding the projected data, with the focus on similarity estimation and training linear classifiers [15, 23, 9, 2].", "startOffset": 168, "endOffset": 182}, {"referenceID": 21, "context": "In this paper, we study a number of simple and effective schemes for coding the projected data, with the focus on similarity estimation and training linear classifiers [15, 23, 9, 2].", "startOffset": 168, "endOffset": 182}, {"referenceID": 7, "context": "In this paper, we study a number of simple and effective schemes for coding the projected data, with the focus on similarity estimation and training linear classifiers [15, 23, 9, 2].", "startOffset": 168, "endOffset": 182}, {"referenceID": 6, "context": "We will closely compare our method with the influential prior coding scheme in [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 16, "context": "This assumption is reasonable in practice [18].", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "This trick was also recently used for linear learning with binary data based on b-bit minwise hashing [19, 20].", "startOffset": 102, "endOffset": 110}, {"referenceID": 18, "context": "This trick was also recently used for linear learning with binary data based on b-bit minwise hashing [19, 20].", "startOffset": 102, "endOffset": 110}, {"referenceID": 10, "context": "Near neighbor search is a basic problem studied since the early days of modern computing [12] with applications throughout computer science.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "The use of coded projection data for near neighbor search is closely related to locality sensitive hashing (LSH) [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 6, "context": "Compared to [8], our proposed coding scheme has better performance for near neighbor search; the analysis will be reported in a separate technical report.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "2 Advantages over the Window-and-Offset Coding Scheme [8] proposed the following well-known coding scheme, which uses windows and a random offset: h w,q(u) = \u230a xj + qj w \u230b", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "[8] showed that the collision probability can be written as Pw,q =Pr ( h w,q(u) = h (j) w,q(v) )", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "In summary, uniform quantization is simpler, more accurate, and uses fewer bits than the influential prior work [8] which uses the window with the random offset.", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": "3 Organization In Section 2, we analyze the collision probability for the uniform quantization scheme and then compare it with the collision probability of the well-known prior work [8] which uses an additional random offset.", "startOffset": 182, "endOffset": 185}, {"referenceID": 6, "context": "In Section 3, we theoretically compare the estimation variances of these two schemes and conclude that the random offset step in [8] is not needed.", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "In other words, it does not matter whether Pw has a closed-form expression, as long as we can demonstrate its advantage over the alternative [8], whose collision probability is denoted by Pw,q.", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "Our proposed scheme (hw) has smaller collision probabilities than the existing scheme [8] (hw,q), especially when w > 2.", "startOffset": 86, "endOffset": 89}, {"referenceID": 11, "context": ", where V1 = \u03c0 (1\u2212 \u03c1)P1(1\u2212 P1) (20) This collision probability is widely known [13].", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "The work of [4] also popularized the use 1-bit coding.", "startOffset": 12, "endOffset": 15}, {"referenceID": 16, "context": "The variance was analyzed and compared with a maximum likelihood estimator in [18].", "startOffset": 78, "endOffset": 82}, {"referenceID": 7, "context": ", LIBLINEAR [9]) on three high-dimensional datasets: ARCENE, FARM, URL, which are available from the UCI repository.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "Recall hw,q [8] was based on uniform quantization plus a random offset, with bin width w.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "75) \u21d2 [1 0 0 0], x \u2208 [\u22120.", "startOffset": 6, "endOffset": 15}, {"referenceID": 0, "context": "75 0) \u21d2 [0 1 0 0], x \u2208 [0 0.", "startOffset": 8, "endOffset": 17}, {"referenceID": 0, "context": "75) \u21d2 [0 0 1 0], x \u2208 [0.", "startOffset": 6, "endOffset": 15}, {"referenceID": 0, "context": "75 \u221e) \u21d2 [0 0 0 1] This way, with k projections, for each feature vector, we obtain a new vector of length 4k with exactly k 1\u2019s.", "startOffset": 8, "endOffset": 17}, {"referenceID": 17, "context": "Recently, this strategy was adopted for linear learning with binary data based on b-bit minwise hashing [19, 20].", "startOffset": 104, "endOffset": 112}, {"referenceID": 18, "context": "Recently, this strategy was adopted for linear learning with binary data based on b-bit minwise hashing [19, 20].", "startOffset": 104, "endOffset": 112}, {"referenceID": 6, "context": "Recall hw,q [8] was based on uniform quantization plus a random offset, with bin length w.", "startOffset": 12, "endOffset": 15}, {"referenceID": 17, "context": "Our work is, to an extent, inspired by the recent work on b-bit minwise hashing [19, 20], which also proposed a coding scheme for minwise hashing and applied it to learning applications where the data are binary and sparse.", "startOffset": 80, "endOffset": 88}, {"referenceID": 18, "context": "Our work is, to an extent, inspired by the recent work on b-bit minwise hashing [19, 20], which also proposed a coding scheme for minwise hashing and applied it to learning applications where the data are binary and sparse.", "startOffset": 80, "endOffset": 88}, {"referenceID": 3, "context": "We expect coding methods will also prove valuable for other variations of random projections, including the count-min sketch [5] and related variants [26] and very sparse random projections [17].", "startOffset": 125, "endOffset": 128}, {"referenceID": 24, "context": "We expect coding methods will also prove valuable for other variations of random projections, including the count-min sketch [5] and related variants [26] and very sparse random projections [17].", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "We expect coding methods will also prove valuable for other variations of random projections, including the count-min sketch [5] and related variants [26] and very sparse random projections [17].", "startOffset": 190, "endOffset": 194}, {"referenceID": 19, "context": "Another potentially interesting future direction is to develop refined coding schemes for improving sign stable projections [21] (which are useful for \u03c72 similarity estimation, a popular similarity measure in computer vision and NLP).", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "We have compared a simple scheme based on uniform quantization with the influential coding scheme using windows with a random offset [8]; our scheme appears operationally simpler, more accurate, not as sensitive to parameters (e.", "startOffset": 133, "endOffset": 136}], "year": 2013, "abstractText": "The method of random projections has become very popular for large-scale applications in statistical learning, information retrieval, bio-informatics and other applications. Using a well-designed coding scheme for the projected data, which determines the number of bits needed for each projected value and how to allocate these bits, can significantly improve the effectiveness of the algorithm, in storage cost as well as computational speed. In this paper, we study a number of simple coding schemes, focusing on the task of similarity estimation and on an application to training linear classifiers. We demonstrate that uniform quantization outperforms the standard existing influential method [8]. Indeed, we argue that in many cases coding with just a small number of bits suffices. Furthermore, we also develop a nonuniform 2-bit coding scheme that generally performs well in practice, as confirmed by our experiments on training linear support vector machines (SVM).", "creator": "LaTeX with hyperref package"}}}