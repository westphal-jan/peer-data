{"id": "1309.5843", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2013", "title": "Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet", "abstract": "Assigning a response such negative knock to a phrases run and interpretation (him. e. turn word ' b prior polarity) because a focused task for unexpectedly analysis. In the science, various focusing based next SentiWordNet have immediately controversial. In this paper, done choices the most less same uses hands left newly similar than and formats all of tried then a opportunities context to see n't fashioned none could further ways the inference known after modulating hundreds. Using separate furthermore versions of SentiWordNet had testing regression most updated models within solve bringing datasets, our learning approach none reengineered itself segment computations, expertise entire beginning commission - of - the - art approach was resource commentary ' prior modulators but sentiment analyze. We begin wish charges images imagine immaturity a calculated remainder polarity scores still translates Part of Speech and annotator female that considered.", "histories": [["v1", "Mon, 23 Sep 2013 15:26:09 GMT  (31kb)", "http://arxiv.org/abs/1309.5843v1", "To appear in Proceedings of EMNLP 2013"]], "COMMENTS": "To appear in Proceedings of EMNLP 2013", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marco guerini", "lorenzo gatti", "marco turchi"], "accepted": true, "id": "1309.5843"}, "pdf": {"name": "1309.5843.pdf", "metadata": {"source": "CRF", "title": "Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet", "authors": ["Marco Guerini", "Marco Turchi"], "emails": ["m.guerini@trentorise.eu", "l.gatti@trentorise.eu", "turchi@fbk.eu"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 9.\n58 43\nv1 [\ncs .C\nL ]\n2 3\nSe p\nAssigning a positive or negative score to a word out of context (i.e. a word\u2019s prior polarity) is a challenging task for sentiment analysis. In the literature, various approaches based on SentiWordNet have been proposed. In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores. Using two different versions of SentiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words\u2019 prior polarity for sentiment analysis. We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered."}, {"heading": "1 Introduction", "text": "Many approaches to sentiment analysis make use of lexical resources \u2013 i.e. lists of positive and negative words \u2013 often deployed as baselines or as features for other methods (usually machine learning based) for sentiment analysis research (Liu and Zhang, 2012). In these lexica, words are associated with their prior polarity, i.e. if that word out of context evokes something positive or something negative. For example, wonderful has a positive connotation \u2013 prior polarity \u2013 while horrible has\na negative one. These approaches have the advantage of not needing deep semantic analysis or word sense disambiguation to assign an affective score to a word and are domain independent (they are thus less precise but more portable).\nSentiWordNet (henceforth SWN) is one of these resources and has been widely adopted since it provides a broad-coverage lexicon \u2013 built in a semi-automatic manner \u2013 for English (Esuli and Sebastiani, 2006). Given that SWN provides polarities scores for each word sense (also called \u2018posterior polarities\u2019), it is necessary to derive prior polarities from the posteriors. For example, the word cold has a posterior polarity for the meaning \u201chaving a low temperature\u201d \u2013 like in \u201ccold beer\u201d \u2013 that is different from the one in \u201ccold person\u201d which refers to \u201cbeing emotionless\u201d. This information must be considered when reconstructing the prior polarity of cold.\nSeveral formulae to compute prior polarities starting from posterior polarities scores have been used in the literature. However, their performance varies significantly depending on the adopted variant. We show that researchers have not paid sufficient attention to this posterior-to-prior polarity issue. Indeed, we show that some variants outperform others on different datasets and can represent a fairer state-ofthe-art approach using SWN. On top of this, we attempt to outperform the state-of-the-art formula using a learning framework that combines the various formulae together.\nIn detail, we will address five main research questions: (i) is there any relevant difference in the posterior-to-prior polarity formulae performance\n(both in regression and classification tasks), (ii) is there any relevant variation in prior polarity values if we use different releases of SWN (i.e. SWN1 or SWN3), (iii) can a learning framework boost performance of such formulae, (iv) considering word Part of Speech (PoS), is there any relevant difference in formulae performance, (v) considering the gender dimension of the annotators (male/female) and the sentiment dimension (positive/negative), is there any relevant difference in SWN performance.\nIn Section 2 we briefly describe our approach and how it differentiates from similar sentiment analysis tasks. Then, in Sections 3 and 4, we present SentiWordNet and overview various posterior-to-prior polarity formulae based on this resource that appeared in the literature (included some new ones we identified as potentially relevant). In Section 5 we describe the learning approach adopted on priorpolarity formulae. In Section 6 we introduce the ANEW and General Inquirer resources that will be used as gold standards. Finally, in the two last sections, we present a series of experiments, both in regression and classification tasks, that give an answer to the aforementioned research questions. The results support the hypothesis that using a learning framework we can improve on state-of-the-art performance and that there are some interesting phenomena connected to PoS and annotator gender."}, {"heading": "2 Proposed Approach", "text": "In the broad field of Sentiment Analysis we will focus on the specific problem of posterior-to-prior polarity assessment, using both regression and classification experiments. A general overview on the field and possible approaches can be found in (Pang and Lee, 2008) or (Liu and Zhang, 2012).\nFor the regression task, we tackled the problem of assigning affective scores (along a continuum between -1 and 1) to words using the posterior-to-prior polarity formulae. For the classification task (assessing whether a word is either positive or negative) we used the same formulae, but considering just the sign of the result. In these experiments we will also use a learning framework which combines the various formulae together. The underlying hypothesis is that by blending these formulae, and looking at the same information from different perspectives (i.e. the pos-\nterior polarities provided by SWN combined in various ways), we can give a better prediction.\nThe regression task is harder than binary classification, since we want to assess not only that pretty, beautiful and gorgeous are positive words, but also to define a partial or total order so that gorgeous is more positive than beautiful which, in turn, is more positive than pretty. This is fundamental for tasks such as affective modification of existing texts, where words\u2019 polarity together with their score are necessary for creating multiple graded variations of the original text (Guerini et al., 2008). Some of the work that addresses the problem of sentiment strength are presented in (Wilson et al., 2004; Paltoglou et al., 2010), however, their approach is modeled as a multi-class classification problem (neutral, low, medium or high sentiment) at the sentence level, rather than a regression problem at the word level. Other works such as (Neviarouskaya et al., 2011) use a fine grained classification approach too, but they consider emotion categories (anger, joy, fear, etc.), rather than sentiment strength categories. On the other hand, even if approaches that go beyond pure prior polarities \u2013 e.g. using word bigram features (Wang and Manning, 2012) \u2013 are better for sentiment analysis tasks, there are tasks that are intrinsically based on the notion of words\u2019 prior polarity. Consider copywriting, where evocative names are a key element to a successful product (O\u0308zbal and Strapparava, 2012; O\u0308zbal et al., 2012). In such cases no context is given and the brand name alone, with its perceived prior polarity, is responsible for stating the area of competition and evoking semantic associations. For example Mitsubishi changed the name of one of its SUV for the Spanish market, since the original name Pajero had a very negative prior polarity, as it meant \u2018wanker\u2019 in Spanish (Piller, 2003).\nTo our knowledge, the only work trying to address the SWN posterior-to-prior polarity issue, comparing some of the approaches appeared in the literature is (Gatti and Guerini, 2012). However, in our previous study we only considered a regression framework, we did not use machine learning and we only tested SWN1. So, we took this work as a starting point for our analysis and expanded on it."}, {"heading": "3 SentiWordNet", "text": "SentiWordNet (Esuli and Sebastiani, 2006) is a lexical resource in which each entry is a set of lemma-PoS pairs sharing the same meaning, called \u201csynset\u201d. Each synset s is associated with the numerical scores Pos(s) and Neg(s), which range from 0 to 1. These scores \u2013 automatically assigned starting from a bunch of seed terms \u2013 represent the positive and negative valence (or posterior polarity) of the synset and are inherited by each lemma-PoS in the synset. According to the structure of SentiWordNet, each pair can have more than one sense and each of them takes the form of lemma#PoS#sense-number, where the smallest sense-number corresponds to the most frequent sense.\nObviously, different senses can have different polarities. In Table 1, the first 5 senses of cold#a present all possible combinations, included mixed scores (cold#a#4), where positive and negative valences are assigned to the same sense. Intuitively, mixed scores for the same sense are acceptable, as in \u201ccold beer\u201d (positive) vs. \u201ccold pizza\u201d (negative).\nIn our experiments we use two different versions of SWN: SentiWordNet 1.0 (SWN1), the first release of SWN, and its updated version SentiWordNet 3.0 (Baccianella et al., 2010) \u2013 SWN3. In SWN3 the annotation algorithm used in SWN1 was revised, leading to an increase in the accuracy of posterior polarities over the previous version."}, {"heading": "4 Prior Polarities Formulae", "text": "In this section we review the main strategies for computing prior polarities used in previous studies. All the proposed approaches try to estimate the prior polarity score from the posterior polarities of all the senses for a single lemma-PoS. Given a lemma-PoS with n senses (lemma#PoS#n), every formula f is independently applied to all the Pos(s) and Neg(s) . This produces two scores,\nf(posScore) and f(negScore), for each lemmaPoS. To obtain a unique prior polarity for each lemma-PoS, f(posScore) and f(negScore) can be mapped according to different strategies:\nfm =\n\n \n \nf(posScore) if f(posScore) \u2265\nf(negScore)\n\u2212f(negScore) otherwise\nfd = f(posScore)\u2212 f(negScore)\nwhere fm computes the absolute maximum of the two scores, while fd computes the difference between them. It is worth noting that f(negScore) is always positive by construction. To obtain a final prior polarity that ranges from -1 to 1, the negative sign is imposed. So, considering the first 5 senses of cold#a in Table 1, f(posScore) will be derived from the Pos(s) values <0.0, 0.0, 0.0, 0.125, 0.625>, while f(negScore) from <0.750, 0.750, 0.0, 0.375, 0.0>. Then, the final polarity strength returned will be either fm or fd.\nThe formulae (f ) we tested are the following: fs. In this formula only the first (and thus most frequent) sense is considered for the given lemma#PoS. This is equivalent to considering only the SWN score for lemma#PoS#1. Based on (Neviarouskaya et al., 2009; Agrawal and Siddiqui, 2009; Guerini et al., 2008; Chowdhury et al., 2013), this is the most basic form of prior polarities.\nmean. It calculates the mean of the positive and negative scores for all the senses of the given lemma#PoS. This formula has been used in (Thet et al., 2009; Denecke, 2009; Devitt and Ahmad, 2007; Sing et al., 2012).\nuni. Based on (Neviarouskaya et al., 2009), it considers only those senses that have a Pos(s) greater than or equal to the corresponding Neg(s), and greater than 0 (the stronglyPos set). In case posScore is equal to negScore, the one with the highest weight is returned, where weights are defined as the cardinality of stronglyPos divided by the total number of senses. The same applies for the negative senses. This is the only method, together with rnd, for which we cannot apply fd, as it returns a positive or negative score according to the weight.\nuniw. Like uni but without the weighting system. w1. This formula weighs each sense with a geometric series of ratio 1/2. The rationale behind this choice is based on the assumption that more frequent senses should bear more \u201caffective weight\u201d than rare senses when computing the prior polarity of a word. The system presented in (Chaumartin, 2007) uses a similar approach of weighted mean.\nw2. Similar to the previous one, this formula weighs each lemma with a harmonic series, see for example (Denecke, 2008).\nOn top of these formulae, we implemented some new formulae that were relevant to our task and have not been implemented before. These formulae mimic the ones discussed previously, but they are built under a different assumption: that the saliency (Giora, 1997) of a word\u2019s prior polarity might be more related to its posterior polarities score, rather than to sense frequencies. Thus we ordered posScore and negScore by strength, giving more relevance to \u2018valenced\u2019 senses. For instance, in Table 1, posScore and negScore for cold#a become <0.625, 0.125, 0.0, 0.0, 0.0>and <0.750, 0.750, 0.375, 0.0, 0.0> respectively.\nw1s and w1n. Like w1 and w2, but senses are ordered by strength (sorting Pos(s) and Neg(s) independently).\nw1n and w2n. Like w1 and w2 respectively, but without considering senses that have a 0 score for both Pos(s) and Neg(s). Our motivation is that \u201cempty\u201d senses are mostly noise.\nw1sn and w2sn. Like w1s and w2s, but without considering senses that have a 0 score for both Pos(s) and Neg(s).\nmedian: return the median of the senses ordered by polarity score.\nAll these prior polarities formulae are compared against two gold standards (one for regression, one for classification) both one by one, as in the works mentioned above, and combined together in a learning framework (to see whether combining these features \u2013 that capture different aspect of prior polarities \u2013 can further improve the results).\nFinally, we implemented two variants of a prior polarity random baseline to asses possible advantages of approaches using SWN:\nrnd. This formula represents the basic baseline random approach. It simply returns a random number between -1 and 1 for any given lemma#PoS.\nswnrnd. This formula represents an advanced random approach that incorporates some \u201cknowledge\u201d from SWN. It takes the scores of a random sense for the given lemma#PoS. We believe this is a fairer baseline than rnd since SWN information can possibly constrain the values. A similar approach has been used in (Qu et al., 2008)."}, {"heading": "5 Learning Algorithms", "text": "We used two non-parametric learning approaches, Support Vector Machines (SVMs) (Shawe-Taylor and Cristianini, 2004) and Gaussian Processes (GPs) (Rasmussen and Williams, 2006), to test the performance of all the metrics in conjunction. SVMs are non-parametric deterministic algorithms that have been widely used in several fields, in particular in NLP where they are the state-of-the-art for various tasks. GPs, on the other hand, are an extremely flexible non-parametric probabilistic framework able to explicitly model uncertainty, that, despite being considered state-ofthe-art in regression, have rarely been used in NLP. To our knowledge only two previous works did so (Polajnar et al., 2011; Cohn and Specia, 2013).\nBoth methods take advantage of the kernel trick, a technique used to embed the original feature space into an alternative space where data may be linearly separable. This is performed by the kernel function that transforms the input data in a new structure, called kernel. How it is used to produce the prediction is one of the main differences between SVMs and GPs. In classification SVMs use the geometric mean to discriminate between the positive and negative classes, while the GP model uses the posterior probability distribution over each class. Both frameworks support learning algorithms for regression and classification. An exhaustive explanation of the two methodologies can be found in (Shawe-Taylor and Cristianini, 2004) and (Rasmussen and Williams, 2006).\nIn the SVM experiments, we use C-SVM and \u01eb-SVM implemented in the LIBSVM toolbox (Chang and Lin, 2011). The selection of the kernel (linear, polynomial, radial basis function and sig-\nmoid) and the optimization of the parameters are carried out through grid search in 10-fold crossvalidation.\nGP regression models with Gaussian noise are a rare exception where the exact inference with likelihood functions is tractable, see \u00a72 in (Rasmussen and Williams, 2006). Unfortunately, this is not valid for the classification task \u2013 see \u00a73 in (Rasmussen and Williams, 2006) \u2013 where an approximation method is required. In this work, we use the Laplace approximation method proposed in (Williams and Barber, 1998). Different kernels are tested (covariance for constant functions, linear with and without automatic relevance determination (ARD)1, Matern, neural network, etc.2) and the linear logistic (lll) and probit regression (prl) likelihood functions are evaluated in classification. In our classification experiments we tried all possible combinations of kernels and likelihood functions, while in the regression tests we ranged only on different kernels. All the GP models were implemented using the GPML Matlab toolbox.Unlike SVMs, the optimization of the kernel parameters can be performed without using grid search, but the optimal parameters can be obtained iteratively, by maximizing the marginal likelihood (or in classification, the Laplace approximation of the marginal likelihood). We fix at 100 the maximum number of iterations.\nAn interesting property of the GPs is their capability of weighting the features differently according to their importance in the data. This is referred to as the automatic variance determination kernel. As demonstrated in (Weston et al., 2000), SVMs can benefit from the application of feature selection techniques especially when there are highly redundant features. Since the prior polarities formulae tend to cluster in groups that provide similar results (Gatti and Guerini, 2012) \u2013 creating noise for the learner \u2013 we want to understand whether feature selection approaches can boost the performance of SVMs. For this reason, we also test feature selection prior to the SVM training. For that we used Randomized Lasso, or stability selection (Meinshausen and Bu\u0308hlmann, 2010). Re-sampling of the training data is performed several times and\n1linone and linard in the result tables, respectively. 2More detailed information on the available kernels are in\n\u00a74 (Rasmussen and Williams, 2006)\na Lasso regression model is fit on each sample. Features that appear in a given number of samples are retained. Both the fraction of the data to be sampled and the threshold to select the features can be configured. In our experiments we set the sampling fraction to 75%, the selection threshold to 25% and the number of re-samples to 1,000. We refer to these as SVMfs."}, {"heading": "6 Gold Standards", "text": "To assess how well prior polarity formulae perform, a gold standard with word polarities provided by human annotators is needed. There are many such resources in the literature, each with different coverage and annotation characteristics. ANEW (Bradley and Lang, 1999) rates the valence score of 1,034 words, which were presented in isolation to annotators. The SO-CAL entries (Taboada et al., 2011) were collected from corpus data and then manually tagged by a small number of annotators with a multi-class label. These ratings were further validated through crowdsourcing. Other resources, such as the General Inquirer lexicon (Stone et al., 1966), provide a binomial classification (either positive or negative) of sentiment-bearing words. The resource presented in (Wilson et al., 2005) uses a similar binomial annotation for single words; another interesting resource is WordNetAffect (Strapparava and Valitutti, 2004) but it labels words senses and it cannot be used for the prior polarity validation task.\nIn the following we describe in detail the two resources we used for our experiments, namely ANEW for the regression experiments and the General Inquirer (GI) for the classification ones."}, {"heading": "6.1 ANEW", "text": "ANEW (Bradley and Lang, 1999) is a resource developed to provide a set of normative emotional ratings for a large number of words (roughly 1 thousand) in the English language. It contains a set of words that have been rated in terms of pleasure (affective valence), arousal, and dominance. In particular for our task we considered the valence dimension. Since words were presented to subjects in isolation (i.e. no context was provided) this resource represents a human validation of prior polar-\nities scores for the given words, and can be used as a gold standard. For each word ANEW provides two main metrics: anew\u00b5, which correspond to the average of annotators votes, and anew\u03c3, which gives the variance in annotators scores for the given word. In the same way these metrics are also provided for the male/female annotator groups."}, {"heading": "6.2 General Inquirer", "text": "The Harvard General Inquirer dictionary is a widely used resource, built for automatic text analysis (Stone et al., 1966). Its latest revision3 contains 11789 words, tagged with 182 semantic and pragmatic labels, as well as with their part of speech. Words and their categories were initially taken from the Harvard IV-4 Psychosociological Dictionary (Dunphy et al., 1974) and the Lasswell Value Dictionary (Lasswell and Namenwirth, 1969). For this paper we consider the Positiv and Negativ categories (1,915 words the former, 2,291 words the latter, for a total of 4,206 affective words)."}, {"heading": "7 Experiments", "text": "In order to use the ANEW dataset to measure prior polarities formulae performance, we had to assign a PoS to all the words to obtain the SWN lemma#PoS format. To do so, we proceeded as follows: for each word, check if it is present among both SWN1 and SWN3 lemmas; if not, lemmatize the word with the TextPro tool suite (Pianta et al., 2008) and check if the lemma is present instead4. If it is not found (i.e., the word cannot be aligned automatically), remove the word from the list (this was the case for 30 words of the 1,034 present in ANEW). The remaining 1,004 lemmas were then associated with all the PoS present in SWN to get the final lemma#PoS. Note that a lemma can have more than one PoS, for example, writer is present only as a noun (writer#n), while yellow is present as a verb, a noun and an adjective (yellow#v, yellow#n, yellow#a). This gave us a list of 1,484 words in the lemma#PoS format.\nIn a similar way we pre-processed the GI words that uses the generic modif label to indicate ei-\n3http://www.wjh.harvard.edu/\u02dcinquirer/ 4We did not lemmatize everything to avoid duplications (for example, if we lemmatize the ANEW entry addicted, we obtain addict, which is already present in ANEW).\nther adjective or adverb (noun and verb PoS were instead consistently used). Finally, all the sensedisambiguated words in the lemma#PoS#n format were discarded (1,114 words out of the 4,206 words with positive or negative valence).\nAfter the two datasets were built this way, we removed the words for which the posScore and negScore contained all 0 in both SWN1 and SWN3 (523 lemma#PoS for ANEW and 484 for the GI dataset), since these words are not informative for our experiments. The final dataset included 961 entries for ANEW and 2,557 for GI. For each lemma#PoS in GI and ANEW, we then applied the prior polarity formulae described in Section 4, using both SWN1 and SWN3 and annotated the results.\nAccording to the nature of the human labels (real numbers or -1/1), we ran several regression and classification experiments. In both cases, each dataset was randomly split into 70% for training and the remaining for test. This process was repeated 5 times to generate different splits. For each partition, optimization of the learning algorithm parameters was performed on the training data (in 10-fold crossvalidation for SVMs). Training and test sets were normalized using the z-score.\nTo evaluate the performance of our regression experiments on ANEW we used the Mean Absolute Error (MAE), that averages the error over a given test set. Accuracy was used for the classification experiments on GI instead. We opted for accuracy \u2013 rather than F1 \u2013 since for us True Negatives have same importance as True Positives. For each experiments we reported the average performance and the standard deviation over the 5 random splits. In the following sections, to check if there was a statistically significant difference in the results, we used Student\u2019s t-test for regression experiments, while an approximate randomization test (Yeh, 2000) was used for the classification experiments.\nIn Tables 2 and 3, the results of regression experiments over the ANEW dataset, using SWN1 and SWN3, are presented. The results of the classification experiments over the GI dataset, using SWN1 and SWN3 are shown in Tables 4 and 5. For the sake of interpretability, results are divided according to the main approaches: randoms, posterior-toprior formulae, learning algorithms. Note that for classification we report the generics f and not the\nfm and fd variants. In fact, both versions always return the same classification answer (we are classifying according to the sign of f result and not its strength). For the GPs, we report the two best configurations only."}, {"heading": "8 General Discussion", "text": "In this section we sum up the main results of our analysis, providing an answer to the various questions we introduced at the beginning of the paper:\nSentiWordNet improves over random. One of the first things worth noting \u2013 in Tables 2, 3, 4 and 5 \u2013 is that the random approach (rnd), as expected, is the worst performing metric, while all other approaches, based on SWN, have statistically signif-\nicant improvements both for MAE and for Accuracy (p < 0.001). So, using SWN for posteriorto-prior polarity computation brings benefits, since it increases the performance above the baseline in words\u2019 prior polarity assessment.\nSWN3 is better than SWN1. With respect to SWN1, using SWN3 enhances performance, both in regression (MAE \u00b5 0.398 vs. 0.366, p < 0.001) and classification (Accuracy \u00b5 0.710 vs. 0.771, p < 0.001) tasks. Since many of the approaches described in the literature use SWN1 their results should be revised and SWN3 should be used as standard. This difference in performance can be partially explained by the fact that, even after preprocessing, for the ANEW dataset 137 lemma#PoS\nhave all senses equal to 0 in SWN1, while in SWN3 they are just 48. In the GI lexicon the numbers are 233 for SWN1 and 69 for SWN3.\nNot all formulae are created equal. The formulae described in Section 4 have very different results, along a continuum. While inspecting every difference in performance is out of the scope of the present paper, we can see that there is a strong difference between best and worst performing formulae both in regression (in Table 2 w2nm is better than uniwm, in Table 3 w2nm is better than maxm) and classification (in Table 4 w1snm is better than fsm,in Table 5 w2m is better than fsm) and these differences are all statistically significant (p < 0.001). Again, these results indicate that the previous experiments in the literature that use SWN as a baseline should be revised to take these results into account. Furthermore, the new formulae we introduced, based on the \u201cposterior polarities saliency\u201d hypothesis, proved to be among the best performing in all experiments. This entails that there is room for inspecting new formulae variants other than those already proposed in the literature.\nSelecting just one sense is not a good choice. On a side note, the approaches that rely on only one sense polarity (namely fs, median and max) have\nsimilar results which do not differ significantly from swnrnd (for maxm, fsd and fsm in Table 2, and for maxm in Table 3). These same approaches are also far from the best performing formulae: in Table 3, mediand differs from w2nm (p < 0.05), as do maxm, maxd, fsm and fsd (p < 0.001); in Table 3, fs, max and median in both their fm and fd variants are significantly different from the best performing w2nm (p < 0.001). For classification, in Table 4 and 5 the difference between the corresponding best performing formula and the single senses formulae is always significant (at least p < 0.01). Among other things, this finding entails, surprisingly, that taking the first sense of a lemma#PoS in some cases has no improvement over taking a random sense, and that in all cases it is one of the worst approaches with SWN . This is surprising since in many NLP tasks, such as word sense disambiguation, algorithms based on most frequent sense represent a very strong baseline5.\nLearning improvements. Combining the formulae in a learning framework further improves the results over the best performing formulae, both in regression (MAE\u00b5 with SWN1 0.366 vs. 0.391,\n5In SemEval 2010, only 5 participants out of 29 performed better than the most frequent threshold (Agirre et al., 2010).\np < 0.001; MAE\u00b5 with SWN3 0.333 vs. 0.359, p < 0.001) and in classification (Accuracy\u00b5 for SWN1 is 0.743 vs. 0.719, p < 0.001; Accuracy\u00b5 for SWN3 is 0.792 vs. 0.781, not significant p = 0.07). Another thing worth noting is that, in regression, GPs are outperformed by both versions of SVM (p < 0.001), see Tables 2 and 3. This is in contrast with the results presented in (Cohn and Specia, 2013), where GPs on the single task are on average better than SVMs. In classification, GPs have similar performance to SVM without feature selection, and in some cases (see Table 5) even slightly better. Analyzing the selected kernels for GPs and SVMs, we notice that in most of the splits SVMs prefer the radial based function, while the best performance with the GPs are obtained with linear kernels with and without ARD. There is no significant difference in using linear logistic and probit regression likelihoods. In all our experiments, SVM with feature selection leads to the best performance. This is not surprising due the high level of redundancy in the formulae scores. Interestingly, inspecting the most frequent selected features by SVMfs, we see that features from different groups are selected, and even the worst performing formulae can add information, confirming the idea that viewing the same information from different perspectives (i.e. the posterior polarities provided by SWN combined in various ways) can give better predictions.\nTo sum up: the new state-of-the-art performance level in prior-polarity computation is represented by the SVMfs approach using SWN3, and this should be used as the reference from now on."}, {"heading": "9 PoS and Gender Experiments", "text": "Next, we wanted to understand if the performance of our approach, using SWN3, was consistent across word PoS. In Table 6 we report the results for the best performing formulae and learning algorithm on the GI PoS classes. In particular for ADJ there are 1,073 words, 922 for NOUN and 508 for VERB. We discarded adverbs since the class was too small to allow reliable evaluation and efficient learning (only 54 instances). The results show a greater accuracy for adjectives (p < 0.01), while performance for nouns and verbs are similar.\nFinally we test against the male and female ratings provided by ANEW. As can be seen from Table 7, SWN approaches are far more precise in predicting Male judgments rather than Female ones (MAE\u00b5 goes from 0.392 to 0.323 with the best formula and from 0.369 to 0.292 with SVMfs, both differences are significant p < 0.001). Instead, in Table 8 \u2013 which displays the results along gender and polarity dimensions \u2013 there is no statistically significant difference in MAE on positive words between male and female, while there is a strong statistical significance for negative words (p < 0.001).\nInterestingly, there is also a large difference between positive and negative affective words (both for male and female dimensions). This difference is maximum for male scores on positive words compared to female scores on negative words (0.283 vs. 0.399, p < 0.001). Recent work by Warriner et al. (2013) inspected the differences in prior polarity assessment due to gender.\nAt this stage we can only note that prior polarities calculated with SWN are closer to ANEW male annotations than female ones. Understanding why this happens would require an accurate examination of the methods used to create WordNet and SWN (which will be the focus of our future work)."}, {"heading": "10 Conclusions", "text": "We have presented a study on the posterior-to-prior polarity issue, i.e. the problem of computing words\u2019 prior polarity starting from their posterior polarities. Using two different versions of SentiWordNet and 30 different approaches that have been proposed in the literature, we have shown that researchers have not paid sufficient attention to this issue. Indeed, we showed that the better variants outperform the others on different datasets both in regression and classification tasks, and that they can represent a fairer state-of-art baseline approach using SentiWordNet. On top of this, we also showed that these state-ofthe-art formulae can be further outperformed using a learning framework that combines the various formulae together. We conclude our analysis with some experiments investigating the impact of word PoS and annotator gender in gold standards, showing interesting phenomena that requires further investigation."}, {"heading": "Acknowledgments", "text": "The authors thank Jose\u0301 Camargo De Souza for his help with feature selection. This work has been partially supported by the Trento RISE PerTe project."}], "references": [{"title": "Semeval-2010 task 17: All-words word sense disambiguation on a specific domain", "author": ["Agirre et al.2010] E. Agirre", "O.L. De Lacalle", "C. Fellbaum", "S.K. Hsieh", "M. Tesconi", "M. Monachini", "P. Vossen", "R. Segers"], "venue": "In Proceedings of the 5th International", "citeRegEx": "Agirre et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2010}, {"title": "Using syntactic and contextual information for sentiment polarity analysis", "author": ["Agrawal", "Siddiqui2009] S. Agrawal", "T.J. Siddiqui"], "venue": "In Proceedings of the 2nd International Conference on Interaction", "citeRegEx": "Agrawal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2009}, {"title": "SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining", "author": ["A. Esuli", "F. Sebastiani"], "venue": "In Proceedings of the 7th Conference on International Language Resources and Evaluation", "citeRegEx": "Baccianella et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baccianella et al\\.", "year": 2010}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Lin2011] C.C. Chang", "C.J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Fbk: Sentiment analysis in twitter with tweetsted", "author": ["M. Guerini", "S. Tonelli", "A. Lavelli"], "venue": "In Second Joint Conference on Lexical and Computational Semantics (*SEM): Proceedings of the Seventh International", "citeRegEx": "Chowdhury et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chowdhury et al\\.", "year": 2013}, {"title": "Modelling annotator bias with multi-task gaussian processes: An application to machine translation quality estimation", "author": ["Cohn", "Specia2013] T. Cohn", "L. Specia"], "venue": "In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Cohn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2013}, {"title": "Accessing medical experiences and information", "author": ["K. Denecke"], "venue": "In Proceedings of the 18th European Conference on Artificial Intelligence, Workshop on Mining Social Data (MSoDa \u201908),", "citeRegEx": "Denecke.,? \\Q2008\\E", "shortCiteRegEx": "Denecke.", "year": 2008}, {"title": "Are SentiWordNet scores suited for multi-domain sentiment classification", "author": ["K. Denecke"], "venue": "In Proceedings of the 4th International Conference on Digital Information Management (ICDIM", "citeRegEx": "Denecke.,? \\Q2009\\E", "shortCiteRegEx": "Denecke.", "year": 2009}, {"title": "Sentiment polarity identification in financial news: A cohesion-based approach", "author": ["Devitt", "Ahmad2007] A. Devitt", "K. Ahmad"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Devitt et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Devitt et al\\.", "year": 2007}, {"title": "SentiWordNet: A publicly available lexical resource for opinion mining", "author": ["Esuli", "Sebastiani2006] A. Esuli", "F. Sebastiani"], "venue": "In Proceedings of the 5th Conference on International Language Resources and Evaluation (LREC", "citeRegEx": "Esuli et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Esuli et al\\.", "year": 2006}, {"title": "Assessing sentiment strength in words prior polarities", "author": ["Gatti", "Guerini2012] L. Gatti", "M. Guerini"], "venue": "In Proceedings of the 24th International Conference on Computational Linguistics (COLING", "citeRegEx": "Gatti et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gatti et al\\.", "year": 2012}, {"title": "Understanding figurative and literal language: The graded salience hypothesis", "author": ["R. Giora"], "venue": "Cognitive Linguistics,", "citeRegEx": "Giora.,? \\Q1997\\E", "shortCiteRegEx": "Giora.", "year": 1997}, {"title": "Valentino: A tool for valence shifting of natural language texts", "author": ["Guerini et al.2008] M. Guerini", "O. Stock", "C. Strapparava"], "venue": "In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Guerini et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Guerini et al\\.", "year": 2008}, {"title": "A survey of opinion mining and sentiment analysis", "author": ["Liu", "Zhang2012] B. Liu", "L. Zhang"], "venue": "Mining Text Data,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Stability selection", "author": ["Meinshausen", "B\u00fchlmann2010] N. Meinshausen", "P. B\u00fchlmann"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Meinshausen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Meinshausen et al\\.", "year": 2010}, {"title": "Sentiful: Generating a reliable lexicon for sentiment analysis", "author": ["Neviarouskaya", "H. Prendinger", "M. Ishizuka."], "venue": "Proceedings of the 3rd Affective Computing and Intelligent Interaction (ACII \u201909), pages 363\u2014368,", "citeRegEx": "Neviarouskaya et al\\.,? 2009", "shortCiteRegEx": "Neviarouskaya et al\\.", "year": 2009}, {"title": "Affect analysis model: novel rule-based approach to affect sensing from text", "author": ["Neviarouskaya", "H. Prendinger", "M. Ishizuka."], "venue": "Natural Language Engineering, 17(1):95.", "citeRegEx": "Neviarouskaya et al\\.,? 2011", "shortCiteRegEx": "Neviarouskaya et al\\.", "year": 2011}, {"title": "A computational approach to the automation of creative naming", "author": ["\u00d6zbal", "Strapparava2012] G. \u00d6zbal", "C. Strapparava"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "\u00d6zbal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "\u00d6zbal et al\\.", "year": 2012}, {"title": "Brand Pitt: A corpus to explore the art of naming", "author": ["\u00d6zbal et al.2012] G. \u00d6zbal", "C. Strapparava", "M. Guerini"], "venue": "In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "\u00d6zbal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "\u00d6zbal et al\\.", "year": 2012}, {"title": "Online textual communications annotated with grades of emotion strength", "author": ["M. Thelwall", "K. Buckley"], "venue": "In Proceedings of the 3rd International Workshop of Emotion: Corpora for research on Emotion and Affect (satellite", "citeRegEx": "Paltoglou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Paltoglou et al\\.", "year": 2010}, {"title": "Opinion mining and sentiment analysis", "author": ["Pang", "Lee2008] B. Pang", "L. Lee"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "The TextPro tool suite", "author": ["Pianta et al.2008] E. Pianta", "C. Girardi", "R. Zanoli"], "venue": "In Proceedings of the 6th International Conference on Language Resources", "citeRegEx": "Pianta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pianta et al\\.", "year": 2008}, {"title": "Advertising as a site of language contact", "author": ["I. Piller"], "venue": "Annual Review of Applied Linguistics,", "citeRegEx": "Piller.,? \\Q2003\\E", "shortCiteRegEx": "Piller.", "year": 2003}, {"title": "Protein interaction detection in sentences via gaussian processes: a preliminary evaluation", "author": ["Polajnar et al.2011] T. Polajnar", "S. Rogers", "M. Girolami"], "venue": "International journal of data mining and bioinformatics,", "citeRegEx": "Polajnar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Polajnar et al\\.", "year": 2011}, {"title": "Sentence level subjectivity and sentiment analysis experiments in NTCIR-7 MOAT challenge", "author": ["Qu et al.2008] L. Qu", "C. Toprak", "N. Jakob", "I. Gurevych"], "venue": "In Proceedings of the 7th NTCIR Workshop Meeting (NTCIR", "citeRegEx": "Qu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2008}, {"title": "Gaussian processes for machine learning", "author": ["Rasmussen", "Williams2006] C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2006}, {"title": "Kernel methods for pattern analysis", "author": ["Shawe-Taylor", "Cristianini2004] J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor et al\\.", "year": 2004}, {"title": "Development of a novel algorithm for sentiment analysis based on adverb-adjective-noun combinations", "author": ["Sing et al.2012] J.K. Sing", "S. Sarkar", "T.K. Mitra"], "venue": "In Proceedings of the 3rd National Conference on Emerging Trends and Applications in Com-", "citeRegEx": "Sing et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sing et al\\.", "year": 2012}, {"title": "The General Inquirer: A Computer Approach to Content Analysis", "author": ["Stone et al.1966] P.J. Stone", "D.C. Dunphy", "M.S. Smith"], "venue": null, "citeRegEx": "Stone et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Stone et al\\.", "year": 1966}, {"title": "WordNet-Affect: an affective extension of WordNet", "author": ["Strapparava", "Valitutti2004] C. Strapparava", "A. Valitutti"], "venue": "In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Strapparava et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Strapparava et al\\.", "year": 2004}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["Taboada et al.2011] M. Taboada", "J. Brooke", "M. Tofiloski", "K. Voll", "M. Stede"], "venue": null, "citeRegEx": "Taboada et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taboada et al\\.", "year": 2011}, {"title": "Sentiment analysis of movie reviews on discussion boards using a linguistic approach", "author": ["Thet et al.2009] T.T. Thet", "J.C. Na", "C.S.G. Khoo", "S. Shakthikumar"], "venue": "In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opin-", "citeRegEx": "Thet et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Thet et al\\.", "year": 2009}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Wang", "Manning2012] S. Wang", "C.D. Manning"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Norms of valence, arousal, and dominance for 13,915 english lemmas", "author": ["V. Kuperman", "M. Brysbaert"], "venue": "Behavior research methods,", "citeRegEx": "Warriner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Warriner et al\\.", "year": 2013}, {"title": "Feature selection for SVMs", "author": ["Weston et al.2000] J. Weston", "S. Mukherjee", "O. Chapelle", "M. Pontil", "T. Poggio", "V. Vapnik"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2000}, {"title": "Bayesian classification with gaussian processes", "author": ["Williams", "Barber1998] C.K.I. Williams", "D. Barber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions", "citeRegEx": "Williams et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1998}, {"title": "Just how mad are you? Finding strong and weak opinion clauses", "author": ["Wilson et al.2004] T. Wilson", "J. Wiebe", "R. Hwa"], "venue": null, "citeRegEx": "Wilson et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2004}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["Wilson et al.2005] T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "In Proceedings of the Conference", "citeRegEx": "Wilson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "More accurate tests for the statistical significance of result differences", "author": ["A. Yeh"], "venue": "In Proceedings of the 18th International Conference on Compu-", "citeRegEx": "Yeh.,? \\Q2000\\E", "shortCiteRegEx": "Yeh.", "year": 2000}], "referenceMentions": [{"referenceID": 12, "context": "This is fundamental for tasks such as affective modification of existing texts, where words\u2019 polarity together with their score are necessary for creating multiple graded variations of the original text (Guerini et al., 2008).", "startOffset": 203, "endOffset": 225}, {"referenceID": 36, "context": "Some of the work that addresses the problem of sentiment strength are presented in (Wilson et al., 2004; Paltoglou et al., 2010), however, their approach is modeled as a multi-class classification problem (neutral, low, medium or high sentiment) at the sentence level, rather than a regression problem at the word level.", "startOffset": 83, "endOffset": 128}, {"referenceID": 19, "context": "Some of the work that addresses the problem of sentiment strength are presented in (Wilson et al., 2004; Paltoglou et al., 2010), however, their approach is modeled as a multi-class classification problem (neutral, low, medium or high sentiment) at the sentence level, rather than a regression problem at the word level.", "startOffset": 83, "endOffset": 128}, {"referenceID": 16, "context": "Other works such as (Neviarouskaya et al., 2011) use a fine grained classification approach too, but they consider emotion categories (anger, joy, fear, etc.", "startOffset": 20, "endOffset": 48}, {"referenceID": 17, "context": "Consider copywriting, where evocative names are a key element to a successful product (\u00d6zbal and Strapparava, 2012; \u00d6zbal et al., 2012).", "startOffset": 86, "endOffset": 135}, {"referenceID": 22, "context": "For example Mitsubishi changed the name of one of its SUV for the Spanish market, since the original name Pajero had a very negative prior polarity, as it meant \u2018wanker\u2019 in Spanish (Piller, 2003).", "startOffset": 181, "endOffset": 195}, {"referenceID": 2, "context": "0 (Baccianella et al., 2010) \u2013 SWN3.", "startOffset": 2, "endOffset": 28}, {"referenceID": 15, "context": "Based on (Neviarouskaya et al., 2009; Agrawal and Siddiqui, 2009; Guerini et al., 2008; Chowdhury et al., 2013), this is the most basic form of prior polarities.", "startOffset": 9, "endOffset": 111}, {"referenceID": 12, "context": "Based on (Neviarouskaya et al., 2009; Agrawal and Siddiqui, 2009; Guerini et al., 2008; Chowdhury et al., 2013), this is the most basic form of prior polarities.", "startOffset": 9, "endOffset": 111}, {"referenceID": 4, "context": "Based on (Neviarouskaya et al., 2009; Agrawal and Siddiqui, 2009; Guerini et al., 2008; Chowdhury et al., 2013), this is the most basic form of prior polarities.", "startOffset": 9, "endOffset": 111}, {"referenceID": 31, "context": "This formula has been used in (Thet et al., 2009; Denecke, 2009; Devitt and Ahmad, 2007; Sing et al., 2012).", "startOffset": 30, "endOffset": 107}, {"referenceID": 7, "context": "This formula has been used in (Thet et al., 2009; Denecke, 2009; Devitt and Ahmad, 2007; Sing et al., 2012).", "startOffset": 30, "endOffset": 107}, {"referenceID": 27, "context": "This formula has been used in (Thet et al., 2009; Denecke, 2009; Devitt and Ahmad, 2007; Sing et al., 2012).", "startOffset": 30, "endOffset": 107}, {"referenceID": 15, "context": "Based on (Neviarouskaya et al., 2009), it considers only those senses that have a Pos(s) greater than or equal to the corresponding Neg(s), and greater than 0 (the stronglyPos set).", "startOffset": 9, "endOffset": 37}, {"referenceID": 6, "context": "Similar to the previous one, this formula weighs each lemma with a harmonic series, see for example (Denecke, 2008).", "startOffset": 100, "endOffset": 115}, {"referenceID": 11, "context": "These formulae mimic the ones discussed previously, but they are built under a different assumption: that the saliency (Giora, 1997) of a word\u2019s prior polarity might be more related to its posterior polarities score, rather than to sense frequencies.", "startOffset": 119, "endOffset": 132}, {"referenceID": 24, "context": "A similar approach has been used in (Qu et al., 2008).", "startOffset": 36, "endOffset": 53}, {"referenceID": 23, "context": "To our knowledge only two previous works did so (Polajnar et al., 2011; Cohn and Specia, 2013).", "startOffset": 48, "endOffset": 94}, {"referenceID": 34, "context": "As demonstrated in (Weston et al., 2000), SVMs can benefit from the application of feature selection techniques especially when there are highly redundant features.", "startOffset": 19, "endOffset": 40}, {"referenceID": 30, "context": "The SO-CAL entries (Taboada et al., 2011) were collected from corpus data and then manually tagged by a small number of annotators with a multi-class label.", "startOffset": 19, "endOffset": 41}, {"referenceID": 28, "context": "Other resources, such as the General Inquirer lexicon (Stone et al., 1966), provide a binomial classification (either positive or negative) of sentiment-bearing words.", "startOffset": 54, "endOffset": 74}, {"referenceID": 37, "context": "The resource presented in (Wilson et al., 2005) uses a similar binomial annotation for single words; another interesting resource is WordNetAffect (Strapparava and Valitutti, 2004) but it labels words senses and it cannot be used for the prior polarity validation task.", "startOffset": 26, "endOffset": 47}, {"referenceID": 28, "context": "The Harvard General Inquirer dictionary is a widely used resource, built for automatic text analysis (Stone et al., 1966).", "startOffset": 101, "endOffset": 121}, {"referenceID": 21, "context": "To do so, we proceeded as follows: for each word, check if it is present among both SWN1 and SWN3 lemmas; if not, lemmatize the word with the TextPro tool suite (Pianta et al., 2008) and check if the lemma is present instead4.", "startOffset": 161, "endOffset": 182}, {"referenceID": 38, "context": "In the following sections, to check if there was a statistically significant difference in the results, we used Student\u2019s t-test for regression experiments, while an approximate randomization test (Yeh, 2000) was used for the classification experiments.", "startOffset": 197, "endOffset": 208}, {"referenceID": 0, "context": "In SemEval 2010, only 5 participants out of 29 performed better than the most frequent threshold (Agirre et al., 2010).", "startOffset": 97, "endOffset": 118}, {"referenceID": 33, "context": "Recent work by Warriner et al. (2013) inspected the differences in prior polarity assessment due to gender.", "startOffset": 15, "endOffset": 38}], "year": 2013, "abstractText": "Assigning a positive or negative score to a word out of context (i.e. a word\u2019s prior polarity) is a challenging task for sentiment analysis. In the literature, various approaches based on SentiWordNet have been proposed. In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores. Using two different versions of SentiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words\u2019 prior polarity for sentiment analysis. We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered.", "creator": "LaTeX with hyperref package"}}}