{"id": "1311.2271", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2013", "title": "More data speeds up training time in learning halfspaces over sparse vectors", "abstract": "The proportion availability of releases was particularly years recently over several authors any ca whether done problem possible to easier web when is {\\ hy computational} processing. That longer, if more available unlike such, beyond place containing qualities apply, is it possible though without the every examples now drive turning the computation time required their perform end learning preparing?", "histories": [["v1", "Sun, 10 Nov 2013 13:28:19 GMT  (19kb)", "http://arxiv.org/abs/1311.2271v1", "13 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amit daniely", "nati linial", "shai shalev-shwartz"], "accepted": true, "id": "1311.2271"}, "pdf": {"name": "1311.2271.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n31 1.\n22 71\nv1 [\ncs .L\nG ]\n1 0\nN ov\nThe increased availability of data in recent years has led several authors to ask whether it is possible to use data as a computational resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task?\nWe give the first positive answer to this question for a natural supervised learning problem \u2014 we consider agnostic PAC learning of halfspaces over 3-sparse vectors in {\u22121, 1, 0}n. This class is inefficiently learnable using O ( n/\u01eb2 )\nexamples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random 3CNF formulas is hard, it is impossible to efficiently learn this class using only O ( n/\u01eb2 )\nexamples. We further show that under stronger hardness assumptions, even O ( n1.499/\u01eb2 )\nexamples do not suffice. On the other hand, we show a new algorithm that learns this class efficiently using \u2126\u0303 ( n2/\u01eb2 )\nexamples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem."}, {"heading": "1 Introduction", "text": "In the modern digital period, we are facing a rapid growth of available datasets in science and technology. In most computing tasks (e.g. storing and searching in such datasets), large datasets are a burden and require more computation. However, for learning tasks the situation is radically different. A simple observation is that more data can never hinder you from performing a task. If you have more data than you need, just ignore it!\nA basic question is how to learn from \u201cbig data\u201d. The statistical learning literature classically studies questions like \u201chow much data is needed to perform a learning task?\u201d or \u201chow does accuracy improve as the amount of data grows?\u201d etc. In the modern, \u201cdata revolution era\u201d, it is often the case that the amount of data available far exceeds the information theoretic requirements. We can wonder whether this, seemingly redundant data, can be used for other purposes. An intriguing question in this vein, studied recently by several researchers ([Decatur et al., 1998, Servedio., 2000, Shalev-Shwartz et al., 2012, Berthet and Rigollet, 2013, Chandrasekaran and Jordan, 2013]), is the following\nQuestion 1: Are there any learning tasks in which more data, beyond the information theoretic barrier, can provably be leveraged to speed up computation time?\nThe main contributions of this work are:\n\u2022 Conditioning on the hardness of refuting random 3CNF formulas, we give the first example of a natural supervised learning problem for which the answer to Question 1 is positive.\n\u2022 To prove this, we present a novel technique to establish computational-statistical tradeoffs in supervised learning problems. To the best of our knowledge, this is the first such a result that is not based on cryptographic primitives.\nAdditional contributions are non trivial efficient algorithms for learning halfspaces over 2-sparse\nand 3-sparse vectors using O\u0303 ( n \u01eb2 )\nand O\u0303 ( n2\n\u01eb2\n)\nexamples respectively.\nThe natural learning problem we consider is the task of learning the class of halfspaces over k-sparse vectors. Here, the instance space is the space of k-sparse vectors,\nCn,k = {x \u2208 {\u22121, 1, 0}n | |{i | xi 6= 0}| \u2264 k} , and the hypothesis class is halfspaces over k-sparse vectors, namely\nHn,k = {hw,b : Cn,k \u2192 {\u00b11} | hw,b(x) = sign(\u3008w, x\u3009 + b), w \u2208 Rn, b \u2208 R} , where \u3008\u00b7, \u00b7\u3009 denotes the standard inner product in Rn. We consider the standard setting of agnostic PAC learning, which models the realistic scenario where the labels are not necessarily fully determined by some hypothesis from Hn,k. Note that in the realizable case, i.e. when some hypothesis from Hn,k has zero error, the problem of learning halfspaces is easy even over Rn.\nIn addition, we allow improper learning (a.k.a. representation independent learning), namely, the learning algorithm is not restricted to output a hypothesis from Hn,k, but only should output a hypothesis whose error is not much larger than the error of the best hypothesis in Hn,k. This gives the learner a lot of flexibility in choosing an appropriate representation of the problem. This additional freedom to the learner makes it much harder to prove lower bounds in this model. Concretely, it is not clear how to use standard reductions from NP hard problems in order to establish lower bounds for improper learning (moreover, Applebaum et al. [2008] give evidence that such simple reductions do not exist).\nThe classes Hn,k and similar classes have been studied by several authors (e.g. Long. and Servedio [2013]). They naturally arise in learning scenarios in which the set of all possible features is very large, but each example has only a small number of active features. For example:\n\u2022 Predicting an advertisement based on a search query: Here, the possible features of each instance are all English words, whereas the active features are only the set of words given in the query.\n\u2022 Learning Preferences [Hazan et al., 2012]: Here, we have n players. A ranking of the players is a permutation \u03c3 : [n] \u2192 [n] (think of \u03c3(i) as the rank of the i\u2019th player). Each ranking induces a preference h\u03c3 over the ordered pairs, such that h\u03c3(i, j) = 1 iff i is ranked higher that j. Namely,\nh\u03c3(i, j) =\n{\n1 \u03c3(i) > \u03c3(j)\n\u22121 \u03c3(i) < \u03c3(j) The objective here is to learn the class, Pn, of all possible preferences. The problem of learning preferences is related to the problem of learning Hn,2: if we associate each pair (i, j) with the vector in Cn,2 whose i\u2019th coordinate is 1 and whose j\u2019th coordinate is \u22121, it is not hard to see that Pn \u2282 Hn,2: for every \u03c3, h\u03c3 = hw,0 for the vector w \u2208 Rn, given by wi = \u03c3(i). Therefore, every upper bound for Hn,2 implies an upper bound for Pn, while every lower bound for Pn implies a lower bound for Hn,2. Since VC(Pn) = n and VC(Hn,2) = n + 1, the information theoretic barrier to learn these classes is \u0398 ( n \u01eb2 ) . In Hazan et al. [2012] it was shown that Pn can be efficiently learnt using O ( n log3(n) \u01eb2 ) examples. In section 4, we extend this result to Hn,2.\nWe will show a positive answer to Question 1 for the class Hn,3. To do so, we show1 the following: 1In fact, similar results hold for every constant k \u2265 3. Indeed, since Hn,3 \u2282 Hn,k for every k \u2265 3, it is trivial that item 3 below holds for every k \u2265 3. The upper bound given in item 1 holds for every k. For item 2,\n1. Ignoring computational issues, it is possible to learn the class Hn,3 using O ( n \u01eb2 ) examples. 2. It is also possible to efficiently learn Hn,3 if we are provided with a larger training set (of size \u2126\u0303 ( n2\n\u01eb2\n)\n). This is formalized in Theorem 3.1.\n3. It is impossible to efficiently learn Hn,3, if we are only provided with a training set of size O (\nn \u01eb2\n)\nunder Feige\u2019s assumption regarding the hardness of refuting random 3CNF formulas [Feige, 2002]. Furthermore, for every \u03b1 \u2208 [0, 0.5), it is impossible to learn efficiently with a training set of size O ( n1+\u03b1\n\u01eb2\n)\nunder a stronger hardness assumption. This\nis formalized in Theorem 4.1.\nA graphical illustration of our main results is given below:\nruntime\n2O(n)\n> poly(n)\nnO(1)\nexamples n2n1.5n\nThe proof of item 1 above is easy \u2013 simply note that Hn,3 has VC dimension n+ 1.\nItem 2 is proved in section 4, relying on the results of Hazan et al. [2012]. We note, however, that a weaker result, that still suffices for answering Question 1 in the affirmative, can be proven using a naive improper learning algorithm. In particular, we show below how to learn Hn,3 efficiently with a sample of \u2126 ( n3\n\u01eb2\n)\nexamples. The idea is to replace the class Hn,3 with the class {\u00b11}Cn,3 containing all functions from Cn,3 to {\u00b11}. Clearly, this class contains Hn,3. In addition, we can efficiently find a function f that minimizes the empirical training error over a training set S as follows: For every x \u2208 Cn,k, if x does not appear at all in the training set we will set f(x) arbitrarily to 1. Otherwise, we will set f(x) to be the majority of the labels in the training set that correspond to x. Finally, note that the VC dimension of {\u00b11}Cn,3 is smaller than n3 (since |Cn,3| < n3). Hence, standard generalization results (e.g. Vapnik [1995]) implies that a training set size of \u2126 ( n3\n\u01eb2\n)\nsuffices for learning this class.\nItem 3 is shown in section 3 by presenting a novel technique for establishing statisticalcomputational tradeoffs.\nThe class Hn,2. Our main result gives a positive answer to Question 1 for the task of improperly learning Hn,k for k \u2265 3. A natural question is what happens for k = 2 and k = 1. Since VC(Hn,1) = VC(Hn,2) = n + 1, the information theoretic barrier for learning these classes is \u0398 (\nn \u01eb2\n)\n. In section 4, we prove that Hn,2 (and, consequently, Hn,1 \u2282 Hn,2) can be learnt using O (\nn log3(n) \u01eb2\n)\nexamples, indicating that significant computational-statistical tradeoffs start to mani-\nfest themselves only for k \u2265 3."}, {"heading": "1.1 Previous approaches, difficulties, and our techniques", "text": "[Decatur et al., 1998] and [Servedio., 2000] gave positive answers to Question 1 in the realizable PAC learning model. Under cryptographic assumptions, they showed that there exist binary learning problems, in which more data can provably be used to speed up training time. [Shalev-Shwartz et al., 2012] showed a similar result for the agnostic PAC learning model. In all of these papers, the main idea is to construct a hypothesis class based on a one-way function. However, the constructed\nit is not hard to show that Hn,k can be learnt using a sample of \u2126 ( nk\n\u01eb2\n)\nexamples by a naive improper learning\nalgorithm, similar to the algorithm we describe in this section for k = 3.\nclasses are of a very synthetic nature, and are of almost no practical interest. This is mainly due to the construction technique which is based on one way functions. In this work, instead of using cryptographic assumptions, we rely on the hardness of refuting random 3CNF formulas. The simplicity and flexibility of 3CNF formulas enable us to derive lower bounds for natural classes such as halfspaces.\nRecently, [Berthet and Rigollet, 2013] gave a positive answer to Question 1 in the context of unsupervised learning. Concretely, they studied the problem of sparse PCA, namely, finding a sparse vector that maximizes the variance of an unsupervised data. Conditioning on the hardness of the planted clique problem, they gave a positive answer to Question 1 for sparse PCA. Our work, as well as the previous work of Decatur et al. [1998], Servedio. [2000], Shalev-Shwartz et al. [2012], studies Question 1 in the supervised learning setup. We emphasize that unsupervised learning problems are radically different than supervised learning problems in the context of deriving lower bounds. The main reason for the difference is that in supervised learning problems, the learner is allowed to employ improper learning, which gives it a lot of power in choosing an adequate representation of the data. For example, the upper bound we have derived for the class of sparse halfspaces switched from representing hypotheses as halfspaces to representation of hypotheses as tables over Cn,3, which made the learning problem easy from the computational perspective. The crux of the difficulty in constructing lower bounds is due to this freedom of the learner in choosing a convenient representation. This difficulty does not arise in the problem of sparse PCA detection, since there the learner must output a good sparse vector. Therefore, it is not clear whether the approach given in [Berthet and Rigollet, 2013] can be used to establish computational-statistical gaps in supervised learning problems."}, {"heading": "2 Background and notation", "text": "For hypothesis class H \u2282 {\u00b11}X and a set Y \u2282 X , we define the restriction of H to Y by H|Y = {h|Y | h \u2208 H}. We denote by J = Jn the all-ones n\u00d7 n matrix. We denote the j\u2019th vector in the standard basis of Rn by ej ."}, {"heading": "2.1 Learning Algorithms", "text": "For h : Cn,3 \u2192 {\u00b11} and a distribution D on Cn,3 \u00d7 {\u00b11} we denote the error of h w.r.t. D by ErrD(h) = Pr(x,y)\u223cD (h(x) 6= y). For H \u2282 {\u00b11}Cn,3 we denote the error of H w.r.t. D by ErrD(H) = minh\u2208H ErrD(h). For a sample S \u2208 (Cn,3 \u00d7 {\u00b11})m we denote by ErrS(h) (resp. ErrS(H)) the error of h (resp. H) w.r.t. the empirical distribution induces by the sample S. A learning algorithm, L, receives a sample S \u2208 (Cn,3 \u00d7 {\u00b11})m and return a hypothesis L(S) : Cn,3 \u2192 {\u00b11}. We say that L learns Hn,3 using m(n, \u01eb) examples if,2 for every distribution D on Cn,3 \u00d7 {\u00b11} and a sample S of more than m(n, \u01eb) i.i.d. examples drawn from D,\nPr S (ErrD(L(S)) > ErrD(H3,n) + \u01eb) <\n1\n10\nThe algorithm L is efficient if it runs in polynomial time in the sample size and returns a hypothesis that can be evaluated in polynomial time.\n2.2 Refuting random 3SAT formulas\nWe frequently view a boolean assignment to variables x1, . . . , xn as a vector in Rn. It is convenient, therefore, to assume that boolean variables take values in {\u00b11} and to denote negation by \u201c \u2212 \u201d (instead of the usual \u201c\u00ac\u201d). An n-variables 3CNF clause is a boolean formula of the form\nC(x) = (\u22121)j1xi1 \u2228 (\u22121)j2xi2 \u2228 (\u22121)j1xi3 , x \u2208 {\u00b11}n\nAn n-variables 3CNF formula is a boolean formula of the form\n\u03c6(x) = \u2227mi=1Ci(x) , 2For simplicity, we require the algorithm to succeed with probability of at least 9/10. This can be easily amplified to probability of at least 1 \u2212 \u03b4, as in the usual definition of agnostic PAC learning, while increasing the sample complexity by a factor of log(1/\u03b4).\nwhere every Ci is a 3CNF clause. Define the value, Val(\u03c6), of \u03c6 as the maximal fraction of clauses that can be simultaneously satisfied. If Val(\u03c6) = 1, we say the \u03c6 is satisfiable. By 3CNFn,m we denote the set of 3CNF formulas with n variables and m clauses.\nRefuting random 3CNF formulas has been studied extensively (see e.g. a special issue of TCS Dubios et al. [2001]). It is known that for large enough \u2206 (\u2206 = 6 will suffice) a random formula in 3CNFn,\u2206n is not satisfiable with probability 1 \u2212 o(1). Moreover, for every 0 \u2264 \u01eb < 14 , and a large enough \u2206 = \u2206(\u01eb), the value of a random formula 3CNFn,\u2206n is \u2264 1\u2212 \u01eb with probability 1\u2212 o(1). The problem of refuting random 3CNF concerns efficient algorithms that provide a proof that a random 3CNF is not satisfiable, or far from being satisfiable. This can be thought of as a game between an adversary and an algorithm. The adversary should produce a 3CNF-formula. It can either produce a satisfiable formula, or, produce a formula uniformly at random. The algorithm should identify whether the produced formula is random or satisfiable.\nFormally, let \u2206 : N \u2192 N and 0 \u2264 \u01eb < 14 . We say that an efficient algorithm, A, \u01eb-refutes random 3CNF with ratio \u2206 if its input is \u03c6 \u2208 3CNFn,n\u2206(n), its output is either \u201ctypical\u201d or \u201cexceptional\u201d and it satisfies:\n\u2022 Soundness: If Val(\u03c6) \u2265 1\u2212 \u01eb, then\nPr Rand. coins of A (A(\u03c6) = \u201cexceptional\u201d) \u2265 3 4\n\u2022 Completeness: For every n, Pr\nRand. coins of A, \u03c6\u223cUni(3CNFn,n\u2206(n)) (A(\u03c6) = \u201ctypical\u201d) \u2265 1\u2212 o(1)\nBy a standard repetition argument, the probability of 34 can be amplified to 1\u22122\u2212n, while efficiency is preserved. Thus, given such an (amplified) algorithm, if A(\u03c6) = \u201ctypical\u201d, then with confidence of 1\u2212 2\u2212n we know that Val(\u03c6) < 1\u2212 \u01eb. Since for random \u03c6 \u2208 3CNFn,n\u2206(n), A(\u03c6) = \u201ctypical\u201d with probability 1 \u2212 o(1), such an algorithm provides, for most 3CNF formulas a proof that their value is less that 1\u2212 \u01eb. Note that an algorithm that \u01eb-refutes random 3CNF with ratio \u2206 also \u01eb\u2032-refutes random 3CNF with ratio \u2206 for every 0 \u2264 \u01eb\u2032 \u2264 \u01eb. Thus, the task of refuting random 3CNF\u2019s gets easier as \u01eb gets smaller. Most of the research concerns the case \u01eb = 0. Here, it is not hard to see that the task is getting easier as \u2206 grows. The best known algorithm [Feige and Ofek, 2007] 0-refutes random 3CNF with ratio \u2206(n) = \u2126( \u221a n). In Feige [2002] it was conjectured that for constant \u2206 no efficient algorithm can provide a proof that a random 3CNF is not satisfiable:\nConjecture 2.1 (R3SAT hardness assumption \u2013 [Feige, 2002]). For every \u01eb > 0 and for every large enough integer \u2206 > \u22060(\u01eb) there exists no efficient algorithm that \u01eb-refutes random 3CNF formulas with ratio \u2206.\nIn fact, for all we know, the following conjecture may be true for every 0 \u2264 \u00b5 \u2264 0.5. Conjecture 2.2 (\u00b5-R3SAT hardness assumption). For every \u01eb > 0 and for every integer \u2206 > \u22060(\u01eb) there exists no efficient algorithm that \u01eb-refutes random 3CNF with ratio \u2206 \u00b7 n\u00b5.\nNote that Feige\u2019s conjecture is equivalent to the 0-R3SAT hardness assumption.\n3 Lower bounds for learning Hn,3 Theorem 3.1 (main). Let 0 \u2264 \u00b5 \u2264 0.5. If the \u00b5-R3SAT hardness assumption (conjecture 2.2) is true, then there exists no efficient learning algorithm that learns the class Hn,3 using O ( n1+\u00b5\n\u01eb2\n)\nexamples.\nIn the proof of Theorem 3.1 we rely on the validity of a conjecture, similar to conjecture 2.2 for 3- variables majority formulas. Following an argument from [Feige, 2002] (Theorem 3.2) the validity of the conjecture on which we rely for majority formulas follows the validity of conjecture 2.2.\nDefine \u2200(x1, x2, x3) \u2208 {\u00b11}3, MAJ(x1, x2, x3) := sign(x1 + x2 + x3)\nAn n-variables 3MAJ clause is a boolean formula of the form\nC(x) = MAJ((\u22121)j1xi1 , (\u22121)j2xi2 , (\u22121)j1xi3), x \u2208 {\u00b11}n\nAn n-variables 3MAJ formula is a boolean formula of the form\n\u03c6(x) = \u2227mi=1Ci(x) where theCi\u2019s are 3MAJ clauses. By 3MAJn,m we denote the set of 3MAJ formulas with n variables and m clauses.\nTheorem 3.2 ([Feige, 2002]). Let 0 \u2264 \u00b5 \u2264 0.5. If the \u00b5-R3SAT hardness assumption is true, then for every \u01eb > 0 and for every large enough integer \u2206 > \u22060(\u01eb) there exists no efficient algorithm with the following properties.\n\u2022 Its input is \u03c6 \u2208 3MAJn,\u2206n1+\u00b5 , and its output is either \u201ctypical\u201d or \u201cexceptional\u201d.\n\u2022 If Val(\u03c6) \u2265 34 \u2212 \u01eb, then\nPr Rand. coins of A (A(\u03c6) = \u201cexceptional\u201d) \u2265 3 4\n\u2022 For every n, Pr\nRand. coins of A, \u03c6\u223cUni(3MAJ n,\u2206n1+\u00b5)\n(A(\u03c6) = \u201ctypical\u201d) \u2265 1\u2212 o(1)\nNext, we prove Theorem 3.1. In fact, we will prove a slightly stronger result. Namely, define the subclass Hdn,3 \u2282 Hn,3, of homogenous halfspaces with binary weights, given by Hdn,3 = {hw,0 | w \u2208 {\u00b11}n}. As we show, under the \u00b5-R3SAT hardness assumption, it is impossible to efficiently learn this subclass using only O ( n1+\u00b5\n\u01eb2\n)\nexamples.\nProof idea: We will reduce the task of refuting random 3MAJ formulas with linear number of clauses to the task of (improperly) learningHdn,3 with linear number of samples. The first step will be to construct a transformation that associates every 3MAJ clause with two examples in Cn,3\u00d7{\u00b11}, and every assignment with a hypothesis in Hdn,3. As we will show, the hypothesis corresponding to an assignment \u03c8 is correct on the two examples corresponding to a clause C if and only if \u03c8 satisfies C. With that interpretation at hand, every 3MAJ formula \u03c6 can be thought of as a distribution D\u03c6 on Cn,3 \u00d7 {\u00b11}, which is the empirical distribution induced by \u03c8\u2019s clauses. It holds furthermore that ErrD\u03c6(Hdn,3) = 1\u2212Val(\u03c6).\nSuppose now that we are given an efficient learning algorithm for Hdn,3, that uses \u03ba n\u01eb2 examples, for some \u03ba > 0. To construct an efficient algorithm for refuting 3MAJ-formulas, we simply feed the learning algorithm with \u03ba n0.012 examples drawn from D\u03c6 and answer \u201cexceptional\u201d if the error of the hypothesis returned by the algorithm is small. If \u03c6 is (almost) satisfiable, the algorithm is guaranteed to return a hypothesis with a small error. On the other hand, if \u03c6 is far from being satisfiable, ErrD\u03c6(Hdn,3) is large. If the learning algorithm is proper, then it must return a hypothesis from Hdn,3 and therefore it would necessarily return a hypothesis with a large error. This argument can be used to show that, unless NP = RP , learning Hdn,3 with a proper efficient algorithm is impossible. However, here we want to rule out improper algorithms as well.\nThe crux of the construction is that if \u03c6 is random, no algorithm (even improper and even inefficient) can return a hypothesis with a small error. The reason for that is that since the sample provided to the algorithm consists of only \u03ba n0.012 samples, the algorithm won\u2019t see most of \u03c8\u2019s clauses, and, consequently, the produced hypothesis h will be independent of them. Since these clauses are random, h is likely to err on about half of them, so that ErrD\u03c6(h) will be close to half!\nTo summarize we constructed an efficient algorithm with the following properties: if \u03c6 is almost satisfiable, the algorithm will return a hypothesis with a small error, and then we will declare \u201cexceptional\u201d, while for random \u03c6, the algorithm will return a hypothesis with a large error, and we will declare \u201ctypical\u201d.\nOur construction crucially relies on the restriction to learning algorithm with a small sample complexity. Indeed, if the learning algorithm obtains more than n1+\u00b5 examples, then it will see most of \u03c8\u2019s clauses, and therefore it might succeed in \u201clearning\u201d even when the source of the formula is random. Therefore, we will declare \u201cexceptional\u201d even when the source is random.\nProof. (of theorem 3.1) Assume by way of contradiction that the \u00b5-R3SAT hardness assumption is true and yet there exists an efficient learning algorithm that learns the class Hn,3 using O ( n1+\u00b5\n\u01eb2\n)\nexamples. Setting \u01eb = 1100 , we conclude that there exists an efficient algorithm L and a constant \u03ba > 0 such that given a sample S of more than \u03ba \u00b7 n1+\u00b5 examples drawn from a distribution D on Cn,3 \u00d7 {\u00b11}, returns a classifier L(S) : Cn,3 \u2192 {\u00b11} such that\n\u2022 L(S) can be evaluated efficiently.\n\u2022 W.p. \u2265 34 over the choice of S, ErrD(L(S)) \u2264 ErrD(Hn,3) + 1100 .\nFix \u2206 large enough such that \u2206 > 100\u03ba and the conclusion of Theorem 3.2 holds with \u01eb = 1100 . We will construct an algorithm, A, contradicting Theorem 3.2. On input \u03c6 \u2208 3MAJn,\u2206n1+\u00b5 consisting of the 3MAJ clauses C1, . . . , C\u2206n1+\u00b5 , the algorithm A proceeds as follows\n1. Generate a sample S consisting of \u2206n1+\u00b5 examples as follows. For every clause, Ck = MAJ((\u22121)j1xi1 , (\u22121)j2xi2 , (\u22121)j3xi3 ), generate an example (xk, yk) \u2208 Cn,3 \u00d7 {\u00b11} by choosing b \u2208 {\u00b11} at random and letting\n(xk, yk) = b \u00b7 ( 3 \u2211\nl=1\n(\u22121)jleil , 1 ) \u2208 Cn,3 \u00d7 {\u00b11} .\nFor example, if n = 6, the clause is MAJ(\u2212x2, x3, x6) and b = \u22121, we generate the example\n((0, 1,\u22121, 0, 0,\u22121),\u22121)\n2. Choose a sample S1 consisting of \u2206n 1+\u00b5 100 \u2265 \u03ba \u00b7 n1+\u00b5 examples by choosing at random (with repetitions) examples from S.\n3. Let h = L(S1). If ErrS(h) \u2264 38 , return \u201cexceptional\u201d. Otherwise, return \u201ctypical\u201d.\nWe claim that A contradicts Theorem 3.2. Clearly, A runs in polynomial time. It remains to show that\n\u2022 If Val(\u03c6) \u2265 34 \u2212 1100 , then\nPr Rand. coins of A (A(\u03c6) = \u201cexceptional\u201d) \u2265 3 4\n\u2022 For every n,\nPr Rand. coins of A, \u03c6\u223cUni(3MAJ n,\u2206n1+\u00b5) (A(\u03c6) = \u201ctypical\u201d) \u2265 1\u2212 o(1)\nAssume first that \u03c6 \u2208 3MAJn,\u2206n1+\u00b5 is chosen at random. Given the sample S1, the sample S2 := S \\S1 is a sample of |S2| i.i.d. examples which are independent from the sample S1, and hence also from h = L(S1). Moreover, for every example (xk, yk) \u2208 S2, yk is a Bernoulli random variable with parameter 12 which is independent of xk. To see that, note that an example whose instance is xk can be generated by exactly two clauses \u2013 one corresponds to yk = 1, while the other corresponds to yk = \u22121 (e.g., the instance (1,\u22121, 0, 1) can be generated from the clause MAJ(x1,\u2212x2, x4) and b = 1 or the clause MAJ(\u2212x1, x2,\u2212x4) and b = \u22121). Thus, given the instance xk , the probability that yk = 1 is 12 , independent of xk.\nIt follows that ErrS2(h) is an average of at least ( 1\u2212 1100 ) \u2206n1+\u00b5 independent Bernoulli random variable. By Chernoff\u2019s bound, with probability \u2265 1\u2212 o(1), ErrS2(h) > 12 \u2212 1100 . Thus,\nErrS(h) \u2265 ( 1\u2212 1 100 ) ErrS2(h) \u2265 ( 1\u2212 1 100 ) \u00b7 ( 1 2 \u2212 1 100 ) > 3 8\nAnd the algorithm will output \u201ctypical\u201d.\nAssume now that Val(\u03c6) \u2265 34 \u2212 1100 and let \u03c8 \u2208 {\u00b11}n be an assignment that indicates that. Let \u03a8 \u2208 Hn,3 be the hypothesis \u03a8(x) = sign (\u3008\u03c8, x\u3009). It can be easily checked that \u03a8(xk) = yk if and only if \u03c8 satisfies Ck . Since Val(\u03c6) \u2265 34 \u2212 1100 , it follows that\nErrS(\u03a8) \u2264 1\n4 +\n1\n100 .\nThus,\nErrS(Hn,3) \u2264 1\n4 +\n1\n100 .\nBy the choice of L, with probability \u2265 1\u2212 14 = 34 ,\nErrS(h) \u2264 1\n4 +\n1\n100 +\n1\n100 <\n3\n8\nand the algorithm will return \u201cexceptional\u201d.\n4 Upper bounds for learning Hn,2 and Hn,3 The following theorem derives upper bounds for learning Hn,2 and Hn,3. Its proof relies on results from Hazan et al. [2012] about learning \u03b2-decomposable matrices, and due to the lack of space is given in the appendix.\nTheorem 4.1.\n\u2022 There exists an efficient algorithm that learns Hn,2 using O ( n log3(n) \u01eb2 ) examples\n\u2022 There exists an efficient algorithm that learns Hn,3 using O ( n2 log3(n) \u01eb2 ) examples"}, {"heading": "5 Discussion", "text": "We formally established a computational-sample complexity tradeoff for the task of (agnostically and improperly) PAC learning of halfspaces over 3-sparse vectors. Our proof of the lower bound relies on a novel, non cryptographic, technique for establishing such tradeoffs. We also derive a new non-trivial upper bound for this task.\nOpen questions. An obvious open question is to close the gap between the lower and upper bounds. We conjecture that Hn,3 can be learnt efficiently using a sample of O\u0303 ( n1.5\n\u01eb2\n)\nexamples. Also, we\nbelieve that our new proof technique can be used for establishing computational-sample complexity tradeoffs for other natural learning problems.\nAcknowledgements: Amit Daniely is a recipient of the Google Europe Fellowship in Learning Theory, and this research is supported in part by this Google Fellowship. Nati Linial is supported by grants from ISF, BSF and I-Core. Shai Shalev-Shwartz is supported by the Israeli Science Foundation grant number 590-10."}, {"heading": "A Proof of Theorem 4.1", "text": "The proof of the theorem relies on results from Hazan et al. [2012] about learning \u03b2-decomposable matrices. Let W be an n\u00d7mmatrix. We define the symmetrization of W to be the (n+m)\u00d7(n+m) matrix\nsym(W ) =\n[\n0 W WT 0\n]\nWe say that W is \u03b2-decomposable if there exist positive semi-definite matrices P,N for which\nsym(W ) = P \u2212N \u2200i, Pii, Nii \u2264 \u03b2\nEach matrix in {\u00b11}n\u00d7m can be naturally interpreted as a hypothesis on [n]\u00d7 [m]. We say that a learning algorithm L learns a class Hn \u2282 {\u00b11}Xn using m(n, \u01eb, \u03b4) examples if, for every distribution D on Xn \u00d7 {\u00b11} and a sample S of more than m(n, \u01eb, \u03b4) i.i.d. examples drawn from D,\nPr S (ErrD(L(S)) > ErrD(Hn) + \u01eb) < \u03b4\nHazan et al. [2012] have proved3 that\nTheorem A.1. Hazan et al. [2012] The hypothesis class of \u03b2-decomposable n \u00d7m matrices with \u00b11 entries ban be efficiently learnt using a sample of O (\n\u03b22(n+m) log(n+m)+log(1/\u03b4) \u01eb2\n)\nexamples.\nWe start with a generic reduction from a problem of learning a class Gn over an instance space Xn \u2282 {\u22121, 1, 0}n to the problem of learning \u03b2(n)-decomposable matrices. We say that Gn is realized by mn \u00d7mn matrices that are \u03b2(n)-decomposable if there exists a mapping \u03c8n : Xn \u2192 [mn]\u00d7 [mn] such that for every h \u2208 Gn there exists a \u03b2(n)-decomposable mn \u00d7mn matrix W for which \u2200x \u2208 Xn, h(x) = W\u03c8n(x). The mapping \u03c8n is called a realization of Gn. In the case that the mapping \u03c8n can be computed in time polynomial in n, we say that Gn is efficiently realized and \u03c8n is an efficient realization. It follows from Theorem A.1 that:\nCorollary A.2. If Gn is efficiently realized by mn \u00d7mn matrices that are \u03b2(n)-decomposable then Gn can be efficiently learnt using a sample of O ( \u03b2(n)2mn log(mn)+log(1/\u03b4) \u01eb2 ) examples.\nWe now turn to the proof of Theorem 4.1. We start with the first assertion, about learning Hn,2. The idea will be to partition the instance space into a disjoint union of subsets and show that the restriction of the hypothesis class to each subset can be efficiently realized by \u03b2(n)-decomposable. Concretely, we decompose Cn,2 into a disjoint union of five sets\nCn,2 = \u222a2r=\u22122Arn where\nArn =\n{\nx \u2208 Cn,2 | n \u2211\ni=1\nxi = r\n}\n.\nIn section A.1 we will prove that\nLemma A.3. For every \u22122 \u2264 r \u2264 2, Hn,2|Arn can be efficiently realized by n\u00d7n matrices that are O(log(n))-decomposable.\nTo glue together the five restrictions, we will rely on the following Lemma, whose proof is given in section A.1.\nLemma A.4. Let X1, ..., Xk be partition of a domain X and let H be a hypothesis class over X . Define Hi = H |Xi . Suppose the for every Hi there exist a learning algorithm that learns Hi using \u2264 C(d + log(1/\u03b4))/\u01eb2 examples, for some constant C \u2265 8. Consider the algorithm A which\n3The result of Hazan et al. [2012] is more general than what is stated here. Also, Hazan et al. [2012] considered the online scenario. The result for the statistical scenario, as stated here, can be derived by applying standard online-to-batch conversions (see for example Cesa-Bianchi et al. [2001]).\nreceives an i.i.d. training set S of m examples from X \u00d7 {0, 1} and applies the learning algorithm for each Hi on the examples in S that belongs to Xi. Then, A learns H using at most\n2Ck(d+ log(2k/\u03b4))\n\u01eb2\nexamples.\nThe first part of Theorem 4.1 is therefore follows from Lemma A.3, Lemma A.4 and Corollary A.2.\nHaving the first part of Theorem 4.1 and Lemma A.4 at hand, it is not hard to prove the second part of Theorem 4.1:\nFor 1 \u2264 i \u2264 n\u2212 2 and b \u2208 {\u00b11} define Dn,i,b = {x \u2208 Cn,3 | xi = b and \u2200j < i, xj = 0}\nLet \u03c8n : Cn,3 \u2192 Cn,2 be the mapping that zeros the first non zero coordinate. It is not hard to see that Hn,3|Dn,i,b = { h \u25e6 \u03c8n|Dn,i,b | h \u2208 Hn,2 }\n. Therefore Hn,3|Dn,i,b can be identified with Hn,2 using the mapping \u03c8n, and therefore can efficiently learnt using O ( n log3(n)+log(1/\u03b4) \u01eb2 ) examples (the dependency on \u03b4 does not appear in the statement, but can be easily inferred from the proof). The second part of Theorem 4.1 is therefore follows from the first part of the Theorem and Lemma A.4.\nA.1 Proofs of Lemma A.3 and Lemma A.4\nIn the proof, we will rely on the following facts. The tensor product of two matrices A \u2208 Mn\u00d7m and B \u2208 Mk\u00d7l is defined as the (n \u00b7 k)\u00d7 (m \u00b7 l) matrix\nA\u2297B =\n\n\n A1,1 \u00b7B \u00b7 \u00b7 \u00b7 A1,m \u00b7B ... . . . ...\nAn,1 \u00b7B \u00b7 \u00b7 \u00b7 Am,m \u00b7B\n\n\n\nProposition A.5. Let W be a \u03b2-decomposable matrix and let A be a PSD matrix whose diagonal entries are upper bounded by \u03b1. Then W \u2297A is (\u03b1 \u00b7 \u03b2)-decomposable.\nProof. It is not hard to see that for every matrix W and a symmetric matrix A,\nsym(W )\u2297A = sym(W \u2297A)\nMoreover, since the tensor product of two PSD matrices is PSD, if sym(W ) = P \u2212 N is a \u03b2decomposition of W , then sym(W \u2297A) = P \u2297A\u2212N \u2297A is a (\u03b1 \u00b7 \u03b2)-decomposition of W \u2297A.\nProposition A.6. If W is a \u03b2-decomposable matrix, then so is every matrix obtained from W by iteratively deleting rows and columns.\nProof. It is enough to show that deleting one row or column leaves W \u03b2-decomposable. Suppose that W \u2032 is obtained from W \u2208 Mn\u00d7m by deleting the i\u2019th row (the proof for deleting columns is similar). It is not hard to see that sym(W \u2032) is the i\u2019th principal minor of sym(W ). Therefore, since principal minors of PSD matrices are PSD matrices as well, if sym(W ) = P\u2212N is \u03b2-decomposition of W then sym(W \u2032) = [P ]i,i \u2212 [N ]i,i is a \u03b2-decomposition of W \u2032.\nProposition A.7. Hazan et al. [2012] Let Tn be the upper triangular matrix whose all entries in the diagonal and above are 1, and whose all entries beneath the diagonal are \u22121. Then Tn is O(log(n))-decomposable.\nLastly, we will also need the following generalization of proposition A.7\nProposition A.8. Let W be an n \u00d7 n \u00b11 matrix. Assume that there exists a sequence 0 \u2264 j(1), . . . , j(n) \u2264 n such that\nWij =\n{\n\u22121 j \u2264 j(i) 1 j > j(i)\nThen, W is O(log(n))-decomposable.\nProof. Since switching rows of a \u03b2-decomposable matrix leaves a \u03b2-decomposable matrix, we can assume without loss of generality that j(1) \u2264 j(2) \u2264 . . . \u2264 j(n). Let J be the n\u00d7n all ones matrix. It is not hard to see that W can be obtained from Tn \u2297 J by iteratively deleting rows and columns. Combining propositions A.5, A.6 and A.7, we conclude that W is O(log(n))-decomposable, as required.\nWe are now ready to prove Lemma A.3\nProof. (of Lemma A.3) Denote Arn = Hn,2|Arn . We split into cases. Case 1, r=0: Note that A0n = {ei \u2212 ej | i, j \u2208 [n]}. Define \u03c8n : A0n \u2192 [n]\u00d7 [n] by \u03c8n(ei \u2212 ej) = (i, j). We claim that \u03c8n is an efficient realization of A0n by n \u00d7 n matrices that are O(log(n)) decomposable. Indeed, let h = hw,b \u2208 A0n, and let W be the n \u00d7 n matrix Wij = W\u03c8n(ei\u2212ej) = h(ei \u2212 ej). It is enough to show that W is O(log(n))-decomposable. We can rename the coordinates so that\nw1 \u2265 w2 \u2265 . . . \u2265 wn (1) From equation (1), it is not hard to see that there exist numbers\n0 \u2264 j(1) \u2264 j(2) \u2264 . . . \u2264 j(n) \u2264 n for which\nWij =\n{\n\u22121 j \u2264 j(i) 1 j > j(i)\nThe conclusion follows from Proposition A.8\nCase 2, r=2 and r=-2: We confine ourselves to the case r = 2. The case r = \u22122 is similar. Note that A2n = {ei + ej | i 6= j \u2208 [n]}. Define \u03c8n : A2n \u2192 [n] \u00d7 [n] by \u03c8n(ei + ej) = (i, j). We claim that \u03c8n is an efficient realization of A2n by n\u00d7 n matrices that are O(log(n)) decomposable. Indeed, let h = hw,b \u2208 A2n, and let W be the n\u00d7 n matrix Wij = W\u03c8n(ei+ej) = h(ei + ej). It is enough to show that W is O(log(n))-decomposable.\nWe can rename the coordinates so that\nw1 \u2264 w2 \u2264 . . . \u2264 wn (2) From equation (2), it is not hard to see that there exist numbers\nn \u2265 j(1) \u2265 j(2) \u2265 . . . \u2265 j(n) \u2265 0 for which\nWij =\n{\n\u22121 j \u2264 j(i) 1 j > j(i)\nThe conclusion follows from Proposition A.8\nCase 3, r=1 and r=-1: We confine ourselves to the case r = 1. The case r = \u22121 is similar. Note that A1n = {ei | i \u2208 [n]}. Define \u03c8n : A0n \u2192 [n]\u00d7[n] by \u03c8n(ei) = (i, i). We claim that \u03c8n is an efficient realization of A1n by n \u00d7 n matrices that are 3-decomposable (let alone, log(n)-decomposable). Indeed, let h = hw,b \u2208 A1n, and let W be the n \u00d7 n matrix with Wii = W\u03c8n(ei) = h(ei) and \u22121 outside the diagonal. It is enough to show that W is 3-decomposable. Since J is 1-decomposable, it is enough to show that W + J is 2-decomposable. However, it is not hard to see that every diagonal matrix D is (maxi |Dii|)-decomposable.\nProof. (of Lemma A.4) Let S = (x1, y1), . . . , (xm, ym) be a training set and let m\u0302i be the number of examples in S that belong to Xi. Given that the values of the random variables m\u03021, . . . , m\u0302i is determined, we have that w.p. of at least 1\u2212 \u03b4,\n\u2200i, ErrDi(hi)\u2212 ErrDi(h\u2217) \u2264 \u221a C(d+ log(k/\u03b4))\nm\u0302i ,\nwhere Di is the induced distribution over Xi, hi is the output of the i\u2019th algorithm, and h\u2217 is the optimal hypothesis w.r.t. the original distribution D. Define,\nmi = max{C(d+ log(k/\u03b4)), m\u0302i} . It follows from the above that we also have, w.p. at least 1\u2212 \u03b4, for every i,\nErrDi(hi)\u2212 ErrDi(h\u2217) \u2264 \u221a C(d+ log(k/\u03b4))\nmi =: \u01ebi.\nLet \u03b1i = D{(x, y) : x \u2208 Xi}, and note that \u2211 i \u03b1i = 1. Therefore,\nErrD(hS)\u2212 ErrD(h\u2217) \u2264 \u2211\ni\n\u03b1i\u01ebi = \u2211\ni\n\u221a \u03b1i \u221a \u03b1i\u01eb2i\n\u2264 \u221a \u2211\ni\n\u03b1i\n\u221a\n\u2211\ni\n\u03b1i\u01eb2i =\n\u221a\n\u2211\ni\n\u03b1i\u01eb2i\n=\n\u221a\nC(d+ log(k/\u03b4))\nm\n\u221a\n\u2211\ni\n\u03b1im\nmi .\nNext note that if \u03b1im < C(d+ log(k/\u03b4)) then \u03b1im/mi \u2264 1. Otherwise, using Chernoff\u2019s inequality, for every i we have\nPr[mi < 0.5\u03b1im] \u2264 e\u2212\u03b1im/8 \u2264 e\u2212(d+log(k/\u03b4)) = e\u2212d \u03b4 k \u2264 \u03b4 k .\nTherefore, by the union bound,\nPr[\u2203i : mi < 0.5\u03b1im] \u2264 \u03b4. It follows that with probability of at least 1\u2212 \u03b4,\n\u221a\n\u2211\ni\n\u03b1im\nmi \u2264\n\u221a 2k .\nAll in all, we have shown that with probability of at least 1\u2212 2\u03b4 it holds that\nErrD(hS)\u2212 ErrD(h\u2217) \u2264 \u221a 2Ck(d+ log(k/\u03b4))\nm .\nTherefore, the the algorithm learns H using\n\u2264 2Ck(d+ log(2k/\u03b4)) \u01eb2\nexamples."}], "references": [{"title": "On basing lower-bounds for learning on worstcase assumptions", "author": ["Benny Applebaum", "Boaz Barak", "David Xiao"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Applebaum et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Applebaum et al\\.", "year": 2008}, {"title": "Complexity theoretic lower bounds for sparse principal component detection", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": "In COLT,", "citeRegEx": "Berthet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2013}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Nicolo Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2001}, {"title": "Computational and statistical tradeoffs via convex relaxation", "author": ["Venkat Chandrasekaran", "Michael I. Jordan"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Chandrasekaran and Jordan.,? \\Q2013\\E", "shortCiteRegEx": "Chandrasekaran and Jordan.", "year": 2013}, {"title": "Computational sample complexity", "author": ["S. Decatur", "O. Goldreich", "D. Ron"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Decatur et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Decatur et al\\.", "year": 1998}, {"title": "Zecchina (Guest Editors)", "author": ["O. Dubios", "R. Monasson", "B. Selma"], "venue": "Phase Transitions in Combinatorial Problems. Theoretical Computer Science,", "citeRegEx": "Dubios et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dubios et al\\.", "year": 2001}, {"title": "Relations between average case complexity and approximation complexity", "author": ["U. Feige"], "venue": "In STOC, pages 534\u2013543,", "citeRegEx": "Feige.,? \\Q2002\\E", "shortCiteRegEx": "Feige.", "year": 2002}, {"title": "Easily refutable subformulas of large random 3cnf formulas", "author": ["Uriel Feige", "Eran Ofek"], "venue": "Theory of Computing,", "citeRegEx": "Feige and Ofek.,? \\Q2007\\E", "shortCiteRegEx": "Feige and Ofek.", "year": 2007}, {"title": "Near-optimal algorithms for online matrix prediction", "author": ["E. Hazan", "S. Kale", "S. Shalev-Shwartz"], "venue": "In COLT,", "citeRegEx": "Hazan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2012}, {"title": "Low-weight halfspaces for sparse boolean vectors", "author": ["P. Long", "R. Servedio"], "venue": "ITCS,", "citeRegEx": "Long. and Servedio.,? \\Q2013\\E", "shortCiteRegEx": "Long. and Servedio.", "year": 2013}, {"title": "Computational sample complexity and attribute-efficient learning", "author": ["R. Servedio"], "venue": "J. of Comput. Syst. Sci.,", "citeRegEx": "Servedio.,? \\Q2000\\E", "shortCiteRegEx": "Servedio.", "year": 2000}, {"title": "Using more data to speed-up training time", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Eran Tromer"], "venue": "In AISTATS,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2012}, {"title": "The hypothesis class of \u03b2-decomposable n \u00d7m matrices with \u00b11 entries ban be efficiently learnt using a sample of O", "author": ["Hazan"], "venue": null, "citeRegEx": "Hazan,? \\Q2012\\E", "shortCiteRegEx": "Hazan", "year": 2012}, {"title": "Hi there exist a learning algorithm that learns Hi using \u2264 C(d + log(1/\u03b4))/\u01eb examples, for some constant C \u2265 8. Consider the algorithm A which The result of Hazan et al", "author": ["Hazan"], "venue": null, "citeRegEx": "Hazan,? \\Q2012\\E", "shortCiteRegEx": "Hazan", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "\u2022 Learning Preferences [Hazan et al., 2012]: Here, we have n players.", "startOffset": 23, "endOffset": 43}, {"referenceID": 0, "context": "Concretely, it is not clear how to use standard reductions from NP hard problems in order to establish lower bounds for improper learning (moreover, Applebaum et al. [2008] give evidence that such simple reductions do not exist).", "startOffset": 149, "endOffset": 173}, {"referenceID": 0, "context": "Concretely, it is not clear how to use standard reductions from NP hard problems in order to establish lower bounds for improper learning (moreover, Applebaum et al. [2008] give evidence that such simple reductions do not exist). The classes Hn,k and similar classes have been studied by several authors (e.g. Long. and Servedio [2013]).", "startOffset": 149, "endOffset": 336}, {"referenceID": 8, "context": "In Hazan et al. [2012] it was shown that Pn can be efficiently learnt using O ( n log(n) \u01eb2 )", "startOffset": 3, "endOffset": 23}, {"referenceID": 6, "context": "under Feige\u2019s assumption regarding the hardness of refuting random 3CNF formulas [Feige, 2002].", "startOffset": 81, "endOffset": 94}, {"referenceID": 8, "context": "Item 2 is proved in section 4, relying on the results of Hazan et al. [2012]. We note, however, that a weaker result, that still suffices for answering Question 1 in the affirmative, can be proven using a naive improper learning algorithm.", "startOffset": 57, "endOffset": 77}, {"referenceID": 4, "context": "1 Previous approaches, difficulties, and our techniques [Decatur et al., 1998] and [Servedio.", "startOffset": 56, "endOffset": 78}, {"referenceID": 10, "context": ", 1998] and [Servedio., 2000] gave positive answers to Question 1 in the realizable PAC learning model.", "startOffset": 12, "endOffset": 29}, {"referenceID": 11, "context": "[Shalev-Shwartz et al., 2012] showed a similar result for the agnostic PAC learning model.", "startOffset": 0, "endOffset": 29}, {"referenceID": 1, "context": "Recently, [Berthet and Rigollet, 2013] gave a positive answer to Question 1 in the context of unsupervised learning.", "startOffset": 10, "endOffset": 38}, {"referenceID": 1, "context": "Therefore, it is not clear whether the approach given in [Berthet and Rigollet, 2013] can be used to establish computational-statistical gaps in supervised learning problems.", "startOffset": 57, "endOffset": 85}, {"referenceID": 1, "context": "Recently, [Berthet and Rigollet, 2013] gave a positive answer to Question 1 in the context of unsupervised learning. Concretely, they studied the problem of sparse PCA, namely, finding a sparse vector that maximizes the variance of an unsupervised data. Conditioning on the hardness of the planted clique problem, they gave a positive answer to Question 1 for sparse PCA. Our work, as well as the previous work of Decatur et al. [1998], Servedio.", "startOffset": 11, "endOffset": 436}, {"referenceID": 1, "context": "Recently, [Berthet and Rigollet, 2013] gave a positive answer to Question 1 in the context of unsupervised learning. Concretely, they studied the problem of sparse PCA, namely, finding a sparse vector that maximizes the variance of an unsupervised data. Conditioning on the hardness of the planted clique problem, they gave a positive answer to Question 1 for sparse PCA. Our work, as well as the previous work of Decatur et al. [1998], Servedio. [2000], Shalev-Shwartz et al.", "startOffset": 11, "endOffset": 454}, {"referenceID": 1, "context": "Recently, [Berthet and Rigollet, 2013] gave a positive answer to Question 1 in the context of unsupervised learning. Concretely, they studied the problem of sparse PCA, namely, finding a sparse vector that maximizes the variance of an unsupervised data. Conditioning on the hardness of the planted clique problem, they gave a positive answer to Question 1 for sparse PCA. Our work, as well as the previous work of Decatur et al. [1998], Servedio. [2000], Shalev-Shwartz et al. [2012], studies Question 1 in the supervised learning setup.", "startOffset": 11, "endOffset": 484}, {"referenceID": 5, "context": "a special issue of TCS Dubios et al. [2001]).", "startOffset": 23, "endOffset": 44}, {"referenceID": 7, "context": "The best known algorithm [Feige and Ofek, 2007] 0-refutes random 3CNF with ratio \u2206(n) = \u03a9( \u221a n).", "startOffset": 25, "endOffset": 47}, {"referenceID": 6, "context": "1 (R3SAT hardness assumption \u2013 [Feige, 2002]).", "startOffset": 31, "endOffset": 44}, {"referenceID": 6, "context": "The best known algorithm [Feige and Ofek, 2007] 0-refutes random 3CNF with ratio \u2206(n) = \u03a9( \u221a n). In Feige [2002] it was conjectured that for constant \u2206 no efficient algorithm can provide a proof that a random 3CNF is not satisfiable: Conjecture 2.", "startOffset": 26, "endOffset": 113}, {"referenceID": 6, "context": "Following an argument from [Feige, 2002] (Theorem 3.", "startOffset": 27, "endOffset": 40}, {"referenceID": 6, "context": "2 ([Feige, 2002]).", "startOffset": 3, "endOffset": 16}, {"referenceID": 8, "context": "Its proof relies on results from Hazan et al. [2012] about learning \u03b2-decomposable matrices, and due to the lack of space is given in the appendix.", "startOffset": 33, "endOffset": 53}], "year": 2013, "abstractText": "The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a computational resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task? We give the first positive answer to this question for a natural supervised learning problem \u2014 we consider agnostic PAC learning of halfspaces over 3-sparse vectors in {\u22121, 1, 0}n. This class is inefficiently learnable using O ( n/\u01eb ) examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random 3CNF formulas is hard, it is impossible to efficiently learn this class using only O ( n/\u01eb ) examples. We further show that under stronger hardness assumptions, even O ( n/\u01eb ) examples do not suffice. On the other hand, we show a new algorithm that learns this class efficiently using \u03a9\u0303 ( n/\u01eb ) examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem.", "creator": "LaTeX with hyperref package"}}}