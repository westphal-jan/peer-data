{"id": "1311.6838", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2013", "title": "Learning Prices for Repeated Auctions with Strategic Buyers", "abstract": "Inspired subsequently lot - they publisher interests for forums lighting promotional, we consider was find of mixups any broker ' s value distribution same called think end the buyer both taken conversing turn his sells through a posted - steady mechanism. We model own buyer turned long diplomacy detective, same goal long to ability her long - term surplus, more mean are realized in mechanisms even maximize the best-selling ' s when - although savings. We define where natural notion raised aims conveyed - - - one fell revenue one measured threat a incontrovertible (non - strategic) retail. We present bookseller algorithms would rather take - (strategic) - contrary however the buyer lure victim priority 9.5 - - - definitely. mails. the issuer fit reflecting advertise not developers sooner done rarely later. We provided give a lower across saturday build disbelief come reduction brought the buyer ' updates discounting katabatic and videos, entire changes, though difficult seller formulation agree risking linear strategic embarrassment if this exception no bookings.", "histories": [["v1", "Tue, 26 Nov 2013 22:53:13 GMT  (31kb)", "http://arxiv.org/abs/1311.6838v1", "Neural Information Processing Systems (NIPS 2013)"]], "COMMENTS": "Neural Information Processing Systems (NIPS 2013)", "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["kareem amin", "afshin rostamizadeh", "umar syed"], "accepted": true, "id": "1311.6838"}, "pdf": {"name": "1311.6838.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["akareem@cis.upenn.edu", "rostami@google.com", "usyed@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 1.\n68 38\nv1 [\ncs .L\nG ]\n2 6\nN ov"}, {"heading": "1 Introduction", "text": "Online display advertising inventory \u2014 e.g., space for banner ads on web pages \u2014 is often sold via automated transactions on real-time ad exchanges. When a user visits a web page whose advertising inventory is managed by an ad exchange, a description of the web page, the user, and other relevant properties of the impression, along with a reserve price for the impression, is transmitted to bidding servers operating on behalf of advertisers. These servers process the data about the impression and respond to the exchange with a bid. The highest bidder wins the right to display an advertisement on the web page to the user, provided that the bid is above the reserve price. The amount charged the winner, if there is one, is settled according to a second-price auction. The winner is charged the maximum of the second-highest bid and the reserve price.\nAd exchanges have been a boon for advertisers, since rich and real-time data about impressions allow them to target their bids to only those impressions that they value. However, this precise targeting has an unfortunate side effect for web page publishers. A nontrivial fraction of ad exchange auctions involve only a single bidder. Without competitive pressure from other bidders, the task of maximizing the publisher\u2019s revenue falls entirely to the reserve price setting mechanism. Secondprice auctions with a single bidder are equivalent to posted-price auctions. The seller offers a price for a good, and a buyer decides whether to accept or reject the price (i.e., whether to bid above or below the reserve price).\nIn this paper, we consider online learning algorithms for setting prices in posted-price auctions where the seller repeatedly interacts with the same buyer over a number of rounds, a common occurrence in ad exchanges where the same buyer might be interested in buying thousands of user impressions daily. In each round t, the seller offers a good to a buyer for price pt. The buyer\u2019s value vt for the good is drawn independently from a fixed value distribution. Both vt and the value distribution are known to the buyer, but neither is observed by the seller. If the buyer accepts price pt, the seller receives revenue pt, and the buyer receives surplus vt \u2212 pt. Since the same buyer participates in\nthe auction in each round, the seller has the opportunity to learn about the buyer\u2019s value distribution and set prices accordingly. Notice that in worst-case repeated auctions there is no such opportunity to learn, while standard Bayesian auctions assume knowledge of a value distribution, but avoid addressing how or why the auctioneer was ever able to estimate this distribution.\nTaken as an online learning problem, we can view this as a \u2018bandit\u2019 problem [18, 16], since the revenue for any price not offered is not observed (e.g., even if a buyer rejects a price, she may well have accepted a lower price). The seller\u2019s goal is to maximize his expected revenue over all T rounds. One straightforward way for the seller to set prices would therefore be to use a noregret bandit algorithm, which minimizes the difference between seller\u2019s revenue and the revenue that would have been earned by offering the best fixed price p\u2217 in hindsight for all T rounds; for a no-regret algorithm (such as UCB [3] or EXP3 [4]), this difference is o(T ). However, we argue that traditional no-regret algorithms are inadequate for this problem. Consider the motivations of a buyer interacting with an ad exchange where the prices are set by a no-regret algorithm, and suppose for simplicity that the buyer has a fixed value vt = v for all t. The goal of the buyer is to acquire the most valuable advertising inventory for the least total cost, i.e., to maximize her total surplus \u2211\nt v \u2212 pt, where the sum is over rounds where the buyer accepts the seller\u2019s price. A naive buyer might simply accept the seller\u2019s price pt if and only if vt \u2265 pt; a buyer who behaves this way is called truthful. Against a truthful buyer any no-regret algorithm will eventually learn to offer prices pt \u2248 v on nearly all rounds. But a more savvy buyer will notice that if she rejects prices in earlier rounds, then she will tend to see lower prices in later rounds. Indeed, suppose the buyer only accepts prices below some small amount \u01eb. Then any no-regret algorithm will learn that offering prices above \u01eb results in zero revenue, and will eventually offer prices below that threshold on nearly all rounds. In fact, the smaller the learner\u2019s regret, the faster this convergence occurs. If v \u226b \u01eb then the deceptive buyer strategy results in a large gain in total surplus for the buyer, and a large loss in total revenue for the seller, relative to the truthful buyer. While the no-regret guarantee certainly holds \u2014 in hindsight, the best price is indeed \u01eb \u2014 it seems fairly useless.\nIn this paper, we propose a definition of strategic regret that accounts for the buyer\u2019s incentives, and give algorithms that are no-regret with respect to this definition. In our setting, the seller chooses a learning algorithm for selecting prices and announces this algorithm to the buyer. We assume that the buyer will examine this algorithm and adopt whatever strategy maximizes her expected surplus over all T rounds. We define the seller\u2019s strategic regret to be the difference between his expected revenue and the expected revenue he would have earned if, rather than using his chosen algorithm to set prices, he had instead offered the best fixed price p\u2217 on all rounds and the buyer had been truthful. As we have seen, this revenue can be much higher than the revenue of the best fixed price in hindsight (in the example above, p\u2217 = v). Unless noted otherwise, throughout the remainder of the paper the term \u201cregret\u201d will refer to strategic regret.\nWe make one further assumption about buyer behavior, which is based on the observation that in many important real-world markets \u2014 and particularly in online advertising \u2014 sellers are far more willing to wait for revenue than buyers are willing to wait for goods. For example, advertisers are often interested in showing ads to users who have recently viewed their products online (this practice is called \u2018retargeting\u2019), and the value of these user impressions decays rapidly over time. Or consider an advertising campaign that is tied to a product launch. A user impression that is purchased long after the launch (such as the release of a movie) is almost worthless. To model this phenomenon we multiply the buyer\u2019s surplus in each round by a discount factor: If the buyer accepts the seller\u2019s price pt in round t, she receives surplus \u03b3t(vt \u2212 pt), where {\u03b3t} is a nonincreasing sequence contained in the interval (0, 1]. We call T\u03b3 = \u2211T t=1 \u03b3t the buyer\u2019s \u2018horizon\u2019, since it is analogous to the seller\u2019s horizon T . The buyer\u2019s horizon plays a central role in our analysis.\nSummary of results: In Sections 4 and 5 we assume that discount rates decrease geometrically: \u03b3t = \u03b3\nt\u22121 for some \u03b3 \u2208 (0, 1]. In Section 4 we consider the special case that the buyer has a fixed value vt = v for all rounds t, and give an algorithm with regret at most O(T\u03b3 \u221a T ). In Section 5 we allow the vt to be drawn from any distribution that satisfies a certain smoothness assumption, and give an algorithm with regret at most O\u0303(T\u03b1 + T 1/\u03b1\u03b3 ) where \u03b1 \u2208 (0, 1) is a user-selected parameter. Note that for either algorithm to be no-regret (i.e., for regret to be o(T )), we need that T\u03b3 = o(T ). In Section 6 we prove that this requirement is necessary for no-regret: any seller algorithm has regret at least \u2126(T\u03b3). The lower bound is proved via a reduction to a non-repeated, or \u2018single-shot\u2019, auction. That our regret bounds should depend so crucially on T\u03b3 is foreshadowed by the example above, in\nwhich a deceptive buyer foregoes surplus in early rounds to obtain even more surplus is later rounds. A buyer with a short horizon T\u03b3 will be unable to execute this strategy, as she will not be capable of bearing the short-term costs required to manipulate the seller."}, {"heading": "2 Related work", "text": "Kleinberg and Leighton study a posted price repeated auction with goods sold sequentially to T bidders who either all have the same fixed private value, private values drawn from a fixed distribution, or private values that are chosen by an oblivious adversary (an adversary that acts independently of observed seller behavior) [15] (see also [7, 8, 14]). Cesa-Bianchi et al. study a related problem of setting the reserve price in a second price auction with multiple (but not repeated) bidders at each round [9]. Note that none of these previous works allow for the possibility of a strategic buyer, i.e. one that acts non-truthfully in order to maximize its surplus. This is because a new buyer is considered at each time step and if the seller behavior depends only on previous buyers, then the setting immediately becomes strategyproof.\nContrary to what is studied in these previous theoretical settings, electronic exchanges in practice see the same buyer appearing in multiple auctions and, thus, the buyer has incentive to act strategically. In fact, [12] finds empirical evidence of buyers\u2019 strategic behavior in sponsored search auctions, which in turn negatively affects the seller\u2019s revenue. In the economics literature, \u2018intertemporal price discrimination\u2019 refers to the practice of using a buyer\u2019s past purchasing behavior to set future prices. Previous work [1, 13] has shown, as we do in Section 6, that a seller cannot benefit from conditioning prices on past behavior if the buyer is not myopic and can respond strategically. However, in contrast to our work, these results assume that the seller knows the buyer\u2019s value distribution.\nOur setting can be modeled as a nonzero sum repeated game of incomplete information, and there is extensive literature on this topic. However, most previous work has focused only on characterizing the equilibria of these games. Further, our game has a particular structure that allows us to design seller algorithms that are much more efficient than generic algorithms for solving repeated games.\nTwo settings that are distinct from what we consider in this paper, but where mechanism design and learning are combined, are the multi-armed bandit mechanism design problem [6, 5, 11] and the incentive compatible regression/classification problem [10, 17]. The former problem is motivated by sponsored search auctions, where the challenge is to elicit truthful values from multiple bidding advertisers while also efficiently estimating the click-through rate of the set of ads that are to be allocated. The latter problem involves learning a discriminative classifier or regression function in the batch setting with training examples that are labeled by selfish agents. The goal is then to minimize error with respect to the truthful labels.\nFinally, Arora et al. proposed a notion of regret for online learning algorithms, called policy regret, that accounts for the possibility that the adversary may adapt to the learning algorithm\u2019s behavior [2]. This resembles the ability, in our setting, of a strategic buyer to adapt to the seller algorithm\u2019s behavior. However, even this stronger definition of regret is inadequate for our setting. This is because policy regret is equivalent to standard regret when the adversary is oblivious, and as we explained in the previous section, there is an oblivious buyer strategy such that the seller\u2019s standard regret is small, but his regret with respect to the best fixed price against a truthful buyer is large."}, {"heading": "3 Preliminaries and Model", "text": "We consider a posted-price model for a single buyer repeatedly purchasing items from a single seller. Associated with the buyer is a fixed distribution D over the interval [0, 1], which is known only to the buyer. On each round t, the buyer receives a value vt \u2208 V \u2286 [0, 1] from the distribution D. The seller, without observing this value, then posts a price pt \u2208 P \u2286 [0, 1]. Finally, the buyer selects an allocation decision at \u2208 {0, 1}. On each round t, the buyer receives an instantaneous surplus of at(vt \u2212 pt), and the seller receives an instantaneous revenue of atpt. We will be primarily interested in designing the seller\u2019s learning algorithm, which we will denote A. Let v1:t denote the sequence of values observed on the first t rounds, (v1, ..., vt), defining p1:t and a1:t analogously. A is an algorithm that selects each price pt as a (possibly randomized) function of (p1:t\u22121, a1:t\u22121). As is common in mechanism design, we assume that the seller announces his\nchoice of algorithm A in advance. The buyer then selects her allocation strategy in response. The buyer\u2019s allocation strategy B generates allocation decisions at as a (possibly randomized) function of (D, v1:t, p1:t, a1:t\u22121). Notice that a choice of A, B and D fixes a distribution over the sequences a1:T and p1:T . This in turn defines the seller\u2019s total expected revenue:\nSellerRevenue(A,B,D, T ) = E [\n\u2211T t=1 atpt\n\u2223 \u2223 A,B,D ] .\nIn the most general setting, we will consider a buyer whose surplus may be discounted through time. In fact, our lower bounds will demonstrate that a sufficiently decaying discount rate is necessary for a no-regret learning algorithm. We will imagine therefore that there exists a nonincreasing sequence {\u03b3t \u2208 (0, 1]} for the buyer. For a choice of T , we will define the effective \u201ctime-horizon\u201d for the buyer as T\u03b3 = \u2211T t=1 \u03b3t. The buyer\u2019s expected total discounted surplus is given by:\nBuyerSurplus(A,B,D, T ) = E [\n\u2211T t=1 \u03b3tat(vt \u2212 pt)\n\u2223 \u2223 A,B,D ] .\nWe assume that the seller is faced with a strategic buyer who adapts to the choice of A. Thus, let B\u2217(A,D) be a surplus-maximizing buyer for seller algorithm A and value distribution is D. In other words, for all strategies B we have\nBuyerSurplus(A,B\u2217(A,D),D, T ) \u2265 BuyerSurplus(A,B,D, T ).\nWe are now prepared to define the seller\u2019s regret. Let p\u2217 = argmaxp\u2208P pPrD[v \u2265 p], the revenuemaximizing choice of price for a seller that knows the distribution D, and simply posts a price of p\u2217 on every round. Against such a pricing strategy, it is in the buyer\u2019s best interest to be truthful, accepting if and only if vt \u2265 p\u2217, and the seller would receive a revenue of Tp\u2217 Prv\u223cD[v \u2265 p\u2217]. Informally, a no-regret algorithm is able to learn D from previous interactions with the buyer, and converge to selecting a price close to p\u2217. We therefore define regret as:\nRegret(A,D, T ) = Tp\u2217 Prv\u223cD[v \u2265 p\u2217]\u2212 SellerRevenue(A,B\u2217(A,D),D, T ). Finally, we will be interested in algorithms that attain o(T ) regret (meaning the averaged regret goes to zero as T \u2192 \u221e) for the worst-case D. In other words, we say A is no-regret if supD Regret(A,D, T ) = o(T ). Note that this definition of worst-case regret only assumes that Nature\u2019s behavior (i.e., the value distribution) is worst-case; the buyer\u2019s behavior is always presumed to be surplus maximizing."}, {"heading": "4 Fixed Value Setting", "text": "In this section we consider the case of a single unknown fixed buyer value, that is V = {v} for some v \u2208 (0, 1]. We show that in this setting a very simple pricing algorithm with monotonically decreasing price offerings is able to achieve O(T\u03b3 \u221a T ) when the buyer discount is \u03b3t = \u03b3t\u22121. Due to space constraints many of the proofs for this section appear in Appendix A.\nMonotone algorithm: Choose parameter \u03b2 \u2208 (0, 1), and initialize a0 = 1 and p0 = 1. In each round t \u2265 1 let pt = \u03b21\u2212at\u22121pt\u22121.\nIn the Monotone algorithm, the seller starts at the maximum price of 1, and decreases the price by a factor of \u03b2 whenever the buyer rejects the price, and otherwise leaves it unchanged. Since Monotone is deterministic and the buyer\u2019s value v is fixed, the surplus-maximizing buyer algorithm B\u2217(Monotone, v) is characterized by a deterministic allocation sequence a\u22171:T \u2208 {0, 1}T .1\nThe following lemma partially characterizes the optimal buyer allocation sequence.\nLemma 1. The sequence a\u22171, . . . , a \u2217 T is monotonically nondecreasing.\n1 If there are multiple optimal sequences, the buyer can then choose to randomize over the set of sequences. In such a case, the worst case distribution (for the seller) is the one that always selects the revenue minimizing optimal sequence. In that case, let a\u22171:T denote the revenue-minimizing buyer-optimal sequence.\nIn other words, once a buyer decides to start accepting the offered price at a certain time step, she will keep accepting from that point on. The main idea behind the proof is to show that if there does exist some time step t\u2032 where a\u2217t\u2032 = 1 and a \u2217 t\u2032+1 = 0, then swapping the values so that a \u2217 t\u2032 = 0 and a\u2217t\u2032+1 = 1 (as well potentially swapping another pair of values) will result in a sequence with strictly better surplus, thereby contradicting the optimality of a\u22171:T . The full proof is shown in Section A.1.\nNow, to finish characterizing the optimal allocation sequence, we provide the following lemma, which describes time steps where the buyer has with certainty begun to accept the offered price.\nLemma 2. Let c\u03b2,\u03b3 = 1 + (1 \u2212 \u03b2)T\u03b3 and d\u03b2,\u03b3 = log(\nc\u03b2,\u03b3 v )\nlog(1/\u03b2) , then for any t > d\u03b2,\u03b3 we have a\u2217t+1 = 1.\nA detailed proof is presented in Section A.2. These lemmas imply the following regret bound.\nTheorem 1. Regret(Monotone, v, T ) \u2264 vT ( 1\u2212 \u03b2c\u03b2,\u03b3 ) + v\u03b2 ( d\u03b2,\u03b3 c\u03b2,\u03b3 + 1c\u03b2,\u03b3 ) .\nProof. By Lemmas 1 and 2 we receive no revenue until at most round \u2308d\u03b2,\u03b3\u2309 + 1, and from that round onwards we receive at least revenue \u03b2\u2308d\u03b2,\u03b3\u2309 per round. Thus\nRegret(Monotone, v, T ) = vT \u2212 T \u2211\nt=\u2308d\u03b2,\u03b3\u2309+1 \u03b2\u2308d\u03b2,\u03b3\u2309 \u2264 vT \u2212 (T \u2212 d\u03b2,\u03b3 \u2212 1)\u03b2d\u03b2,\u03b3+1\nNoting that \u03b2d\u03b2,\u03b3 = vc\u03b2,\u03b3 and rearranging proves the theorem.\nTuning the learning parameter simplifies the bound further and provides a O(T\u03b3 \u221a T ) regret bound. Note that this tuning parameter does not assume knowledge of the buyer\u2019s discount parameter \u03b3. Corollary 1. If \u03b2 = \u221a T\n1+ \u221a T\nthen Regret(Monotone, v, T ) \u2264 \u221a T ( 4vT\u03b3 + 2v log ( 1 v )) + v .\nThe computation used to derive this corollary are found in Section A.3. This corollary shows that it is indeed possible to achieve no-regret against a strategic buyer with a unknown fixed value as long as T\u03b3 = o( \u221a T ). That is, the effective buyer horizon must be more than a constant factor smaller than the square-root of the game\u2019s finite horizon."}, {"heading": "5 Stochastic Value Setting", "text": "We next give a seller algorithm that attains no-regret when the set of prices P is finite, the buyer\u2019s discount is \u03b3t = \u03b3t\u22121, and the buyer\u2019s value vt for each round is drawn from a fixed distribution D that satistfies a certain continuity assumption, detailed below.\nPhased algorithm: Choose parameter \u03b1 \u2208 (0, 1). Define Ti \u2261 2i and Si \u2261 min (\nTi |P| , T \u03b1 i\n)\n. For each phase i = 1, 2, 3, . . . of length Ti rounds:\nOffer each price p \u2208 P for Si rounds, in some fixed order; these are the explore rounds. Let Ap,i = Number of explore rounds in phase i where price p was offered and the buyer accepted. For the remainingTi\u2212|P|Si rounds of phase i, offer price p\u0303i = argmaxp\u2208P p\nAp,i Si in each round; these are the exploit rounds.\nThe Phased algorithm proceeds across a number of phases. Each phase consists of explore rounds followed by exploit rounds. During explore rounds, the algorithm selects each price in some fixed order. During exploit rounds, the algorithm repeatedly selects the price that realized the greatest revenue during the immediately preceding explore rounds.\nFirst notice that a strategic buyer has no incentive to lie during exploit rounds (i.e. it will accept any price pt < vt and reject any price pt > vt), since its decisions there do not affect any of its future prices. Thus, the exploit rounds are the time at which the seller can exploit what it has learned from the buyer during exploration. Alternatively, if the buyer has successfully manipulated the seller into offering a low price, we can view the buyer as \u201cexploiting\u201d the seller.\nDuring explore rounds, on the other hand, the strategic buyer can benefit by telling lies which will cause it to witness better prices during the corresponding exploit rounds. However, the value of these lies to the buyer will depend on the fraction of the phase consisting of explore rounds. Taken to the extreme, if the entire phase consists of explore rounds, the buyer is not interested in lying. In general, the more explore rounds, the more revenue has to be sacrificed by a buyer that is lying during the explore rounds. For the myopic buyer, the loss of enough immediate revenue at some point ceases to justify her potential gains in the future exploit rounds.\nThus, while traditional algorithms like UCB balance exploration and exploitation to ensure confidence in the observed payoffs of sampled arms, our Phased algorithm explores for two purposes: to ensure accurate estimates, and to dampen the buyer\u2019s incentive to mislead the seller. The seller\u2019s balancing act is to explore for long enough to learn the buyer\u2019s value distribution, but leave enough exploit rounds to benefit from the knowledge.\nContinuity of the value distribution The preceding argument required that the distribution D does not exhibit a certain pathology. There cannot be two prices p, p\u2032 that are very close but pPrv\u223cD[v \u2265 p] and p\u2032 Prv\u223cD[v \u2265 p\u2032] are very different. Otherwise, the buyer is largely indifferent to being offered prices p or p\u2032, but distinguishing between the two prices is essential for the seller during exploit rounds. Thus, we assume that the value distribution D is K-Lipschitz, which eliminates this problem: Defining F (p) \u2261 Prv\u223cD[v \u2265 p], we assume there exists K > 0 such that |F (p) \u2212 F (p\u2032)| \u2264 K|p \u2212 p\u2032| for all p, p\u2032 \u2208 [0, 1]. This assumption is quite mild, as our Phased algorithm does not need to know K , and the dependence of the regret rate on K will be logarithmic. Theorem 2. Assume F (p) \u2261 Prv\u223cD[v \u2265 p] is K-Lipschitz. Let \u2206 = minp\u2208P\\{p\u2217} p\u2217F (p\u2217) \u2212 pF (p), where p\u2217 = argmaxp\u2208P pF (p). For any parameter \u03b1 \u2208 (0, 1) of the Phased algorithm there exist constants c1, c2, c3, c4 such that\nRegret(Phased,D, T ) \u2264 c1|P|T\u03b1 + c2 |P|2 \u22062/\u03b1 (logT )1/\u03b1\n+ c3 |P|2 \u22061/\u03b1 T 1/\u03b1\u03b3 (log T + log(K/\u2206)) 1/\u03b1 + c4|P|\n= O\u0303(T\u03b1 + T 1/\u03b1\u03b3 ).\nThe complete proof of Theorem 2 is rather technical, and is provided in Appendix B.\nTo gain further intuition about the upper bounds proved in this section and the previous section, it helps to parametrize the buyer\u2019s horizon T\u03b3 as a function of T , e.g. T\u03b3 = T c for 0 \u2264 c \u2264 1. Writing it in this fashion, we see that the Monotone algorithm has regret at most O(T c+ 1 2 ), and the Phased algorithm has regret at most O\u0303(T \u221a c) if we choose \u03b1 = \u221a c. The lower bound proved in the next section states that, in the worst case, any seller algorithm will incur a regret of at least \u2126(T c)."}, {"heading": "6 Lower Bound", "text": "In this section we state the main lower bound, which establishes a connection between the regret of any seller algorithm and the buyer\u2019s discounting. Specifically, we prove that the regret of any seller algorithm is \u2126(T\u03b3). Note that when T = T\u03b3 \u2014 i.e., the buyer does not discount her future surplus \u2014 our lower bound proves that no-regret seller algorithms do not exist, and thus it is impossible for the seller to take advantage of learned information. For example, consider the seller algorithm that uniformly selects prices pt from [0, 1]. The optimal buyer algorithm is truthful, accepting if pt < vt, as the seller algorithm is non-adaptive, and the buyer does not gain any advantage by being more strategic. In such a scenario the seller would quickly learn a good estimate of the value distribution D. What is surprising is that a seller cannot use this information if the buyer does not discount her future surplus. If the seller attempts to leverage information learned through interactions with the buyer, the buyer can react accordingly to negate this advantage.\nThe lower bound further relates regret in the repeated setting to regret in a particular single-shot game between the buyer and the seller. This demonstrates that, against a non-discounted buyer, the seller is no better off in the repeated setting than he would be by repeatedly implementing such a single-shot mechanism (ignoring previous interactions with the buyer). In the following section we describe the simple single-shot game."}, {"heading": "6.1 Single-Shot Auction", "text": "We call the following game the single-shot auction. A seller selects a family of distributions S indexed by b \u2208 [0, 1], where each Sb is a distribution on [0, 1]\u00d7 {0, 1}. The family S is revealed to a buyer with unknown value v \u2208 [0, 1], who then must select a bid b \u2208 [0, 1], and then (p, a) \u223c Sb is drawn from the corresponding distribution.\nAs usual, the buyer gets a surplus of a(v \u2212 p), while the seller enjoys a revenue of ap. We restrict the set of seller strategies to distributions that are incentive compatible and rational. S is incentive compatible if for all b, v \u2208 [0, 1], E(p,a)\u223cSb [a(v\u2212p)] \u2264 E(p,a)\u223cSv [a(v\u2212p)]. It is rational if for all v, E(p,a)\u223cSv [a(v\u2212p)] \u2265 0 (i.e. any buyer maximizing expected surplus is actually incentivised to play the game). Incentive compatible and rational strategies exist: drawing p from a fixed distribution (i.e. all Sb are the same), and letting a = 1{b \u2265 p} suffices.2\nWe define the regret in the single-shot setting of any incentive-compatible and rational strategy S with respect to value v as\nSSRegret(S, v) = v \u2212 E(p,a)\u223cSv [ap].\nThe following loose lower bound on SSRegret(S, v) is straightforward, and establishes that a seller\u2019s revenue cannot be a constant fraction of the buyer\u2019s value for all v. The full proof is provided in the appendix (Section C.1).\nLemma 3. For any incentive compatible and rational strategy S there exists v \u2208 [0, 1] such that SSRegret(S, v) \u2265 112 ."}, {"heading": "6.2 Repeated Auction", "text": "Returning to the repeated setting, our main lower bound will make use of the following technical lemma, the full proof of which is provided in the appendix (Section C.1). Informally, the Lemma states that the surplus enjoyed by an optimal buyer algorithm would only increase if this surplus were viewed without discounting.\nLemma 4. Let the buyer\u2019s discount sequence {\u03b3t} be positive and nonincreasing. For any seller algorithm A, value distribution D, and surplus-maximizing buyer algorithm B\u2217(A,D), E [\n\u2211T t=1 \u03b3tat(vt \u2212 pt)\n] \u2264 E [\n\u2211T t=1 at(vt \u2212 pt)\n]\nNotice if at(vt \u2212 pt) \u2265 0 for all t, then the Lemma 4 is trivial. This would occur if the buyer only ever accepts prices less than its value (at = 1 only if pt \u2264 vt). However, Lemma 4 is interesting in that it holds for any seller algorithm A. It\u2019s easy to imagine a seller algorithm that incentivizes the buyer to sometimes accept a price pt > vt with the promise that this will generate better prices in the future (e.g. setting pt\u2032 = 1 and offering pt = 0 for all t > t\u2032 only if at\u2032 = 1 and otherwise setting pt = 1 for all t > t\u2032).\nLemmas 3 and 4 let us prove our main lower bound.\nTheorem 3. Fix a positive, nonincreasing, discount sequence {\u03b3t}. Let A be any seller algorithm for the repeated setting. There exists a buyer value distribution D such that Regret(A,D, T ) \u2265 1 12T\u03b3 . In particular, if T\u03b3 = \u2126(T ), no-regret is impossible.\nProof. Let {ab,t, pb,t} be the sequence of prices and allocations generated by playing B\u2217(A, b) against A. For each b \u2208 [0, 1] and (p, a) \u2208 [0, 1) \u00d7 {0, 1}, let \u00b5b(p, a) = 1T\u03b3 \u2211T t=1 \u03b3t1{ab,t = a}1{pb,t = p}. Notice that \u00b5b(p, a) > 0 for countably many (p, a) and let \u2126b = {(p, a) \u2208 [0, 1]\u00d7 {0, 1} : \u00b5b(p, a) > 0}. We think of \u00b5b as being a distribution. It\u2019s in fact a random measure since the {ab,t, pb,t} are themselves random. One could imagine generating \u00b5b by playing B\u2217(A, b) against A and observing the sequence {ab,t, pb,t}. Every time we observe a price pb,t = p and allocation ab,t = a, we assign 1T\u03b3 \u03b3t additional mass to (p, a) in \u00b5b. This is impossible in practice, but the random measure \u00b5b has a well-defined distribution.\nNow consider the following strategy S for the single-shot setting. Sb is induced by drawing a \u00b5b, then drawing (p, a) \u223c \u00b5b. Note that for any b \u2208 [0, 1] and any measurable function f\n2This subclass of auctions is even ex post rational.\nE(p,a)\u223cSb [f(a, p)] = E\u00b5b\u223cSb [ E(p,a)\u223c\u00b5b [f(a, b) | \u00b5b] ] = 1T\u03b3 E [ \u2211T t=1 \u03b3tf(ab,t, pb,t) ] .\nThus the strategy S is incentive compatible, since for any b, v \u2208 [0, 1]\nE(p,a)\u223cSb [a(v \u2212 p)] = 1\nT\u03b3 E\n[\nT \u2211\nt=1\n\u03b3tab,t(v \u2212 pb,t) ] = 1\nT\u03b3 BuyerSurplus(A,B\u2217(A, b), v, T )\n\u2264 1 T\u03b3 BuyerSurplus(A,B\u2217(A, v), v, T ) = 1 T\u03b3 E\n[\nT \u2211\nt=1\n\u03b3tav,t(v \u2212 pv,t) ] = E(p,a)\u223cSv [a(v \u2212 p)]\nwhere the inequality follows from the fact that B\u2217(A, v) is a surplus-maximizing algorithm for a buyer whose value is v. The strategy S is also rational, since for any v \u2208 [0, 1]\nE(p,a)\u223cSv [a(v \u2212 p)] = 1\nT\u03b3 E\n[\nT \u2211\nt=1\n\u03b3tav,t(v \u2212 pv,t) ] = 1\nT\u03b3 BuyerSurplus(A,B\u2217(A, v), v, T ) \u2265 0\nwhere the inequality follows from the fact that a surplus-maximizing buyer algorithm cannot earn negative surplus, as a buyer can always reject every price and earn zero surplus.\nLet rt = 1\u2212 \u03b3t and Tr = \u2211T t=1 rt. Note that rt \u2265 0. We have the following for any v \u2208 [0, 1]:\nT\u03b3SSRegret(S, v) = T\u03b3 ( v \u2212 E(p,a)\u223cSv [ap] ) = T\u03b3\n(\nv \u2212 1 T\u03b3 E\n[\nT \u2211\nt=1\n\u03b3tav,tpv,t\n])\n= T\u03b3v \u2212 E [ T \u2211\nt=1\n\u03b3tav,tpv,t\n] = (T \u2212 Tr)v \u2212 E [ T \u2211\nt=1\n(1 \u2212 rt)av,tpv,t ]\n= Tv \u2212 E [ T \u2211\nt=1\nav,tpv,t\n]\n+ E\n[\nT \u2211\nt=1\nrtav,tpv,t\n]\n\u2212 Trv\n= Regret(A, v, T )+E [ T \u2211\nt=1\nrtav,tpv,t\n] \u2212Trv = Regret(A, v, T )+E [ T \u2211\nt=1\nrt(av,tpv,t \u2212 v) ]\nA closer look at the quantity E [\n\u2211T t=1 rt(av,tpv,t \u2212 v)\n] , tells us that: E [\n\u2211T t=1 rt(av,tpv,t \u2212 v)\n]\n\u2264\nE [\n\u2211T t=1 rtav,t(pv,t \u2212 v)\n] = \u2212E [\n\u2211T t=1(1\u2212 \u03b3t)av,t(v \u2212 pv,t)\n]\n\u2264 0, where the last inequality follows from Lemma 4. Therefore T\u03b3SSRegret(S, v) \u2264 Regret(A, v, T ) and taking D to be the point-mass on the value v \u2208 [0, 1] which realizes Lemma 3 proves the statement of the theorem."}, {"heading": "7 Conclusion", "text": "In this work, we have analyzed the performance of revenue maximizing algorithms in the setting of a repeated posted-price auction with a strategic buyer. We show that if the buyer values inventory in the present more than in the far future, no-regret (with respect to revenue gained against a truthful buyer) learning is possible. Furthermore, we provide lower bounds that show such an assumption is in fact necessary. These are the first bounds of this type for the presented setting. Future directions of study include studying buyer behavior under weaker polynomial discounting rates as well understanding when existing \u201coff-the-shelf\u201d bandit-algorithm (UCB, or EXP3), perhaps with slight modifications, are able to perform well against strategic buyers."}, {"heading": "Acknowledgements", "text": "We thank Corinna Cortes, Gagan Goel, Yishay Mansour, Hamid Nazerzadeh and Noam Nisan for early comments on this work and pointers to relevent literature."}, {"heading": "C Lower Bound Proofs", "text": "C.1 Proof of Lemma 3\nProof. Fix a incentive compatible and rational strategy S. Let SellerRevenue(b) = E(p,a)\u223cSb [ap] be the seller\u2019s expected revenue if the buyer bids b, and let BuyerSurplus(b, v) = E(p,a)\u223cSb [a(v \u2212 p)] be the buyer\u2019s expected surplus if she bids b and her value is v. It suffices to show that there exists v \u2208 [0, 1] such that v \u2212 SellerRevenue(v) \u2265 112 . Before proceeding, we establish some properties of S. Incentive compatibility of S ensures that\nBuyerSurplus(v, v) \u2265 BuyerSurplus(b, v) (26) for all b, v \u2208 [0, 1], and rationality of S ensures that\nBuyerSurplus(v, v) \u2265 0 (27) for all v \u2208 [0, 1]. Also\nSellerRevenue(b) + BuyerSurplus(b, v) = E(p,a)\u223cSb [a]v (28)\nfor all b, v \u2208 [0, 1], which follows directly from definitions, and SellerRevenue(v) \u2264 E(p,a)\u223cSv [a]v (29)\nfor all v \u2208 [0, 1], which follows from rationality: By (28) we have BuyerSurplus(v, v) = E(p,a)\u223cSv [a]v \u2212 SellerRevenue(v), and thus if (29) were false we would have BuyerSurplus(v, v) < 0, which contradicts (27).\nNow observe that for any b, v \u2208 [0, 1] v \u2212 SellerRevenue(v) \u2265 E(p,a)\u223cSv [a]v \u2212 SellerRevenue(v)\n= BuyerSurplus(v, v) (30)\n\u2265 BuyerSurplus(b, v) (31) = E(p,a)\u223cSb [a(v \u2212 p)] = E(p,a)\u223cSb [a]v \u2212 E(p,a)\u223cSb [ap] = E(p,a)\u223cSb [a]v \u2212 SellerRevenue(b) \u2265 ( SellerRevenue(b)\nb\n)\nv \u2212 SellerRevenue(b) (32)\n= (v \u2212 b) ( SellerRevenue(b)\nb\n)\nwhere (30) follows from (28), (31) follows from (26), and (32) follows from (29). Now let b = 14 and v = 12 . If v \u2212 SellerRevenue(v) \u2265 16 we are done. Otherwise the first and last lines from the above chain of inequalities and v \u2212 SellerRevenue(v) < 16 imply\nSellerRevenue(b) b \u2264 v \u2212 SellerRevenue(v) v \u2212 b < 1 6 1 v \u2212 b = 2 3\nwhich can be rearranged into b\u2212 SellerRevenue(b) \u2265 13b \u2265 112 .\nC.2 Proof of Lemma 4\nProof. It will be convenient to define the following (all expectations in these definitions are with respect to A,D and B\u2217(A,D)):\nrev(t1, t2) = E\n[\nt2 \u2211\nt=t1\natpt\n]\nsur(t1, t2) = E\n[\nt2 \u2211\nt=t1\n\u03b3tat(vt \u2212 pt) ]\nudsur(t1, t2) = E\n[\nt2 \u2211\nt=t1\nat(vt \u2212 pt) ]\ntotval(t1, t2) = E\n[\nt2 \u2211\nt=t1\natvt\n]\nwhere \u201cudsur\u201d stands for \u201cundiscounted surplus\u201d and \u201ctotval\u201d stands for \u201ctotal value\u201d. Note that by definition\nrev(t1, t2) + udsur(t1, t2) = totval(t1, t2). (33)\nAlso, since B\u2217(A,D) is a surplus-maximizing buyer strategy, sur(t, T ) \u2265 0 for all rounds t, because otherwise the buyer could increase her surplus by following B\u2217(A,D) until round t \u2212 1 and then selecting at\u2032 = 0 for all rounds t\u2032 \u2265 t. We will first prove that sur(t, T ) \u2264 \u03b3tudsur(t, T ) for all rounds t. The proof will proceed by induction. For the base case, we have sur(T, T ) = \u03b3Tudsur(T, T ) by definition. Now assume for the inductive hypothesis that sur(t + 1, T ) \u2264 \u03b3t+1udsur(t + 1, T ). Since sur(t + 1, T ) \u2265 0 and \u03b3t+1 > 0, by the inductive hypothesis we have udsur(t+ 1, T ) \u2265 0. Therefore\nsur(t, T ) = sur(t, t) + sur(t+ 1, T )\n= \u03b3tudsur(t, t) + sur(t+ 1, T ) \u2264 \u03b3tudsur(t, t) + \u03b3t+1udsur(t+ 1, T ) (34) \u2264 \u03b3tudsur(t, t) + \u03b3tudsur(t+ 1, T ) (35) = \u03b3tudsur(t, T )\nwhere Eq. (34) follows from the inductive hypothesis and Eq. (35) follows because udsur(t+1, T ) \u2265 0 and \u03b3t \u2265 \u03b3t+1. Thus sur(t, T ) \u2264 \u03b3tudsur(t, T ). Since sur(1, T ) \u2264 \u03b31udsur(1, T ) and \u03b31 \u2264 1, by Eq. (33) we have rev(1, T ) + sur(1, T ) \u2264 totval(1, T ), which proves the lemma."}], "references": [{"title": "Conditioning prices on purchase history", "author": ["Alessandro Acquisti", "Hal R. Varian"], "venue": "Marketing Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Online bandit learning against an adaptive adversary: from regret to policy regret", "author": ["Raman Arora", "Ofer Dekel", "Ambuj Tewari"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "Journal on Computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Truthful mechanisms with implicit payment computation", "author": ["Moshe Babaioff", "Robert D Kleinberg", "Aleksandrs Slivkins"], "venue": "In Proceedings of the Conference on Electronic Commerce,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Characterizing truthful multiarmed bandit mechanisms", "author": ["Moshe Babaioff", "Yogeshwer Sharma", "Aleksandrs Slivkins"], "venue": "In Proceedings of Conference on Electronic Commerce,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Incentive-compatible online auctions for digital goods", "author": ["Ziv Bar-Yossef", "Kirsten Hildrum", "Felix Wu"], "venue": "In Proceedings of Symposium on Discrete Algorithms,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Online learning in online auctions", "author": ["Avrim Blum", "Vijay Kumar", "Atri Rudra", "Felix Wu"], "venue": "In Proceedings Symposium on Discrete algorithms,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Regret minimization for reserve prices in second-price auctions", "author": ["Nicolo Cesa-Bianchi", "Claudio Gentile", "Yishay Mansour"], "venue": "In Proceedings of the Symposium on Discrete Algorithms. SIAM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Incentive compatible regression learning", "author": ["Ofer Dekel", "Felix Fischer", "Ariel D Procaccia"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "The price of truthfulness for pay-per-click auctions", "author": ["Nikhil R Devanur", "Sham M Kakade"], "venue": "In Proceedings of the Conference on Electronic commerce,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Strategic bidder behavior in sponsored search auctions", "author": ["Benjamin Edelman", "Michael Ostrovsky"], "venue": "Decision support systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Behavior-Based Price Discrimination and Customer Recognition", "author": ["Drew Fudenberg", "J. Miguel Villas-Boas"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "The value of knowing a demand curve: Bounds on regret for online posted-price auctions", "author": ["Robert Kleinberg", "Tom Leighton"], "venue": "In Symposium on Foundations of Computer Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Algorithms for the multi-armed bandit problem", "author": ["Volodymyr Kuleshov", "Doina Precup"], "venue": "Journal of Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Strategyproof classification with shared inputs", "author": ["Reshef Meir", "Ariel D Procaccia", "Jeffrey S Rosenschein"], "venue": "Proc. of 21st IJCAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}], "referenceMentions": [{"referenceID": 14, "context": "Taken as an online learning problem, we can view this as a \u2018bandit\u2019 problem [18, 16], since the revenue for any price not offered is not observed (e.", "startOffset": 76, "endOffset": 84}, {"referenceID": 2, "context": "One straightforward way for the seller to set prices would therefore be to use a noregret bandit algorithm, which minimizes the difference between seller\u2019s revenue and the revenue that would have been earned by offering the best fixed price p\u2217 in hindsight for all T rounds; for a no-regret algorithm (such as UCB [3] or EXP3 [4]), this difference is o(T ).", "startOffset": 314, "endOffset": 317}, {"referenceID": 3, "context": "One straightforward way for the seller to set prices would therefore be to use a noregret bandit algorithm, which minimizes the difference between seller\u2019s revenue and the revenue that would have been earned by offering the best fixed price p\u2217 in hindsight for all T rounds; for a no-regret algorithm (such as UCB [3] or EXP3 [4]), this difference is o(T ).", "startOffset": 326, "endOffset": 329}, {"referenceID": 13, "context": "2 Related work Kleinberg and Leighton study a posted price repeated auction with goods sold sequentially to T bidders who either all have the same fixed private value, private values drawn from a fixed distribution, or private values that are chosen by an oblivious adversary (an adversary that acts independently of observed seller behavior) [15] (see also [7, 8, 14]).", "startOffset": 343, "endOffset": 347}, {"referenceID": 6, "context": "2 Related work Kleinberg and Leighton study a posted price repeated auction with goods sold sequentially to T bidders who either all have the same fixed private value, private values drawn from a fixed distribution, or private values that are chosen by an oblivious adversary (an adversary that acts independently of observed seller behavior) [15] (see also [7, 8, 14]).", "startOffset": 358, "endOffset": 368}, {"referenceID": 7, "context": "2 Related work Kleinberg and Leighton study a posted price repeated auction with goods sold sequentially to T bidders who either all have the same fixed private value, private values drawn from a fixed distribution, or private values that are chosen by an oblivious adversary (an adversary that acts independently of observed seller behavior) [15] (see also [7, 8, 14]).", "startOffset": 358, "endOffset": 368}, {"referenceID": 8, "context": "study a related problem of setting the reserve price in a second price auction with multiple (but not repeated) bidders at each round [9].", "startOffset": 134, "endOffset": 137}, {"referenceID": 11, "context": "In fact, [12] finds empirical evidence of buyers\u2019 strategic behavior in sponsored search auctions, which in turn negatively affects the seller\u2019s revenue.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "Previous work [1, 13] has shown, as we do in Section 6, that a seller cannot benefit from conditioning prices on past behavior if the buyer is not myopic and can respond strategically.", "startOffset": 14, "endOffset": 21}, {"referenceID": 12, "context": "Previous work [1, 13] has shown, as we do in Section 6, that a seller cannot benefit from conditioning prices on past behavior if the buyer is not myopic and can respond strategically.", "startOffset": 14, "endOffset": 21}, {"referenceID": 5, "context": "Two settings that are distinct from what we consider in this paper, but where mechanism design and learning are combined, are the multi-armed bandit mechanism design problem [6, 5, 11] and the incentive compatible regression/classification problem [10, 17].", "startOffset": 174, "endOffset": 184}, {"referenceID": 4, "context": "Two settings that are distinct from what we consider in this paper, but where mechanism design and learning are combined, are the multi-armed bandit mechanism design problem [6, 5, 11] and the incentive compatible regression/classification problem [10, 17].", "startOffset": 174, "endOffset": 184}, {"referenceID": 10, "context": "Two settings that are distinct from what we consider in this paper, but where mechanism design and learning are combined, are the multi-armed bandit mechanism design problem [6, 5, 11] and the incentive compatible regression/classification problem [10, 17].", "startOffset": 174, "endOffset": 184}, {"referenceID": 9, "context": "Two settings that are distinct from what we consider in this paper, but where mechanism design and learning are combined, are the multi-armed bandit mechanism design problem [6, 5, 11] and the incentive compatible regression/classification problem [10, 17].", "startOffset": 248, "endOffset": 256}, {"referenceID": 15, "context": "Two settings that are distinct from what we consider in this paper, but where mechanism design and learning are combined, are the multi-armed bandit mechanism design problem [6, 5, 11] and the incentive compatible regression/classification problem [10, 17].", "startOffset": 248, "endOffset": 256}, {"referenceID": 1, "context": "proposed a notion of regret for online learning algorithms, called policy regret, that accounts for the possibility that the adversary may adapt to the learning algorithm\u2019s behavior [2].", "startOffset": 182, "endOffset": 185}, {"referenceID": 0, "context": "Associated with the buyer is a fixed distribution D over the interval [0, 1], which is known only to the buyer.", "startOffset": 70, "endOffset": 76}, {"referenceID": 0, "context": "On each round t, the buyer receives a value vt \u2208 V \u2286 [0, 1] from the distribution D.", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "The seller, without observing this value, then posts a price pt \u2208 P \u2286 [0, 1].", "startOffset": 70, "endOffset": 76}, {"referenceID": 0, "context": "Thus, we assume that the value distribution D is K-Lipschitz, which eliminates this problem: Defining F (p) \u2261 Prv\u223cD[v \u2265 p], we assume there exists K > 0 such that |F (p) \u2212 F (p\u2032)| \u2264 K|p \u2212 p\u2032| for all p, p\u2032 \u2208 [0, 1].", "startOffset": 208, "endOffset": 214}, {"referenceID": 0, "context": "For example, consider the seller algorithm that uniformly selects prices pt from [0, 1].", "startOffset": 81, "endOffset": 87}, {"referenceID": 0, "context": "A seller selects a family of distributions S indexed by b \u2208 [0, 1], where each Sb is a distribution on [0, 1]\u00d7 {0, 1}.", "startOffset": 60, "endOffset": 66}, {"referenceID": 0, "context": "A seller selects a family of distributions S indexed by b \u2208 [0, 1], where each Sb is a distribution on [0, 1]\u00d7 {0, 1}.", "startOffset": 103, "endOffset": 109}, {"referenceID": 0, "context": "The family S is revealed to a buyer with unknown value v \u2208 [0, 1], who then must select a bid b \u2208 [0, 1], and then (p, a) \u223c Sb is drawn from the corresponding distribution.", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "The family S is revealed to a buyer with unknown value v \u2208 [0, 1], who then must select a bid b \u2208 [0, 1], and then (p, a) \u223c Sb is drawn from the corresponding distribution.", "startOffset": 98, "endOffset": 104}, {"referenceID": 0, "context": "S is incentive compatible if for all b, v \u2208 [0, 1], E(p,a)\u223cSb [a(v\u2212p)] \u2264 E(p,a)\u223cSv [a(v\u2212p)].", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "For any incentive compatible and rational strategy S there exists v \u2208 [0, 1] such that SSRegret(S, v) \u2265 1 12 .", "startOffset": 70, "endOffset": 76}, {"referenceID": 0, "context": "For each b \u2208 [0, 1] and (p, a) \u2208 [0, 1) \u00d7 {0, 1}, let \u03bcb(p, a) = 1 T\u03b3 \u2211T t=1 \u03b3t1{ab,t = a}1{pb,t = p}.", "startOffset": 13, "endOffset": 19}, {"referenceID": 0, "context": "Notice that \u03bcb(p, a) > 0 for countably many (p, a) and let \u03a9b = {(p, a) \u2208 [0, 1]\u00d7 {0, 1} : \u03bcb(p, a) > 0}.", "startOffset": 74, "endOffset": 80}, {"referenceID": 0, "context": "Note that for any b \u2208 [0, 1] and any measurable function f This subclass of auctions is even ex post rational.", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": "Thus the strategy S is incentive compatible, since for any b, v \u2208 [0, 1] E(p,a)\u223cSb [a(v \u2212 p)] = 1 T\u03b3 E [ T", "startOffset": 66, "endOffset": 72}, {"referenceID": 0, "context": "The strategy S is also rational, since for any v \u2208 [0, 1] E(p,a)\u223cSv [a(v \u2212 p)] = 1 T\u03b3 E [ T", "startOffset": 51, "endOffset": 57}, {"referenceID": 0, "context": "We have the following for any v \u2208 [0, 1]: T\u03b3SSRegret(S, v) = T\u03b3 ( v \u2212 E(p,a)\u223cSv [ap] )", "startOffset": 34, "endOffset": 40}, {"referenceID": 0, "context": "Therefore T\u03b3SSRegret(S, v) \u2264 Regret(A, v, T ) and taking D to be the point-mass on the value v \u2208 [0, 1] which realizes Lemma 3 proves the statement of the theorem.", "startOffset": 97, "endOffset": 103}, {"referenceID": 0, "context": "References [1] Alessandro Acquisti and Hal R.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Raman Arora, Ofer Dekel, and Ambuj Tewari.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Peter Auer, Nicol\u00f2 Cesa-Bianchi, and Paul Fischer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Moshe Babaioff, Robert D Kleinberg, and Aleksandrs Slivkins.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Moshe Babaioff, Yogeshwer Sharma, and Aleksandrs Slivkins.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Ziv Bar-Yossef, Kirsten Hildrum, and Felix Wu.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Avrim Blum, Vijay Kumar, Atri Rudra, and Felix Wu.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Nicolo Cesa-Bianchi, Claudio Gentile, and Yishay Mansour.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Ofer Dekel, Felix Fischer, and Ariel D Procaccia.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Nikhil R Devanur and Sham M Kakade.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Benjamin Edelman and Michael Ostrovsky.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Drew Fudenberg and J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Robert Kleinberg and Tom Leighton.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Volodymyr Kuleshov and Doina Precup.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Reshef Meir, Ariel D Procaccia, and Jeffrey S Rosenschein.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "It suffices to show that there exists v \u2208 [0, 1] such that v \u2212 SellerRevenue(v) \u2265 1 12 .", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "Incentive compatibility of S ensures that BuyerSurplus(v, v) \u2265 BuyerSurplus(b, v) (26) for all b, v \u2208 [0, 1], and rationality of S ensures that BuyerSurplus(v, v) \u2265 0 (27) for all v \u2208 [0, 1].", "startOffset": 102, "endOffset": 108}, {"referenceID": 0, "context": "Incentive compatibility of S ensures that BuyerSurplus(v, v) \u2265 BuyerSurplus(b, v) (26) for all b, v \u2208 [0, 1], and rationality of S ensures that BuyerSurplus(v, v) \u2265 0 (27) for all v \u2208 [0, 1].", "startOffset": 184, "endOffset": 190}, {"referenceID": 0, "context": "Also SellerRevenue(b) + BuyerSurplus(b, v) = E(p,a)\u223cSb [a]v (28) for all b, v \u2208 [0, 1], which follows directly from definitions, and SellerRevenue(v) \u2264 E(p,a)\u223cSv [a]v (29) for all v \u2208 [0, 1], which follows from rationality: By (28) we have BuyerSurplus(v, v) = E(p,a)\u223cSv [a]v \u2212 SellerRevenue(v), and thus if (29) were false we would have BuyerSurplus(v, v) < 0, which contradicts (27).", "startOffset": 80, "endOffset": 86}, {"referenceID": 0, "context": "Also SellerRevenue(b) + BuyerSurplus(b, v) = E(p,a)\u223cSb [a]v (28) for all b, v \u2208 [0, 1], which follows directly from definitions, and SellerRevenue(v) \u2264 E(p,a)\u223cSv [a]v (29) for all v \u2208 [0, 1], which follows from rationality: By (28) we have BuyerSurplus(v, v) = E(p,a)\u223cSv [a]v \u2212 SellerRevenue(v), and thus if (29) were false we would have BuyerSurplus(v, v) < 0, which contradicts (27).", "startOffset": 184, "endOffset": 190}, {"referenceID": 0, "context": "Now observe that for any b, v \u2208 [0, 1] v \u2212 SellerRevenue(v) \u2265 E(p,a)\u223cSv [a]v \u2212 SellerRevenue(v) = BuyerSurplus(v, v) (30) \u2265 BuyerSurplus(b, v) (31) = E(p,a)\u223cSb [a(v \u2212 p)] = E(p,a)\u223cSb [a]v \u2212 E(p,a)\u223cSb [ap] = E(p,a)\u223cSb [a]v \u2212 SellerRevenue(b) \u2265 (", "startOffset": 32, "endOffset": 38}], "year": 2013, "abstractText": null, "creator": "LaTeX with hyperref package"}}}