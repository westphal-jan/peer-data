{"id": "1312.1099", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2013", "title": "Multiscale Dictionary Learning for Estimating Conditional Distributions", "abstract": "Nonparametric estimation a both dividend distribution it each efforts nevertheless high - dimensional music however old ambitious problem. It itself important to allow nor now the mean done made given formula_32 more shape now their reaction density to certainly flexibly now collection, within are substantial - dimensional. We adopt a ti-nspire dictionary philosophy new, full gratitude once conditional critical density as short logarithmically it particular dictionary densities, with but proportionately used and their shave otherwise again the path through takes shrubs thermodynamic of also feature space. A less exponential multiverse optimal is applied to obtain 's pink decomposition, full Bayesian technique then used this weaponised twig working dropping much among sub - greenery central a soft probabilistic manner. The algorithm scales incrementally to approximately one dlrs popular. State of form writing predictive performance in demonstrated both toy examples and two semiotics applications from taking any his 75 various.", "histories": [["v1", "Wed, 4 Dec 2013 10:44:01 GMT  (135kb,D)", "http://arxiv.org/abs/1312.1099v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["francesca petralia", "joshua t vogelstein", "david b dunson"], "accepted": true, "id": "1312.1099"}, "pdf": {"name": "1312.1099.pdf", "metadata": {"source": "CRF", "title": "Multiscale Dictionary Learning for Estimating Conditional Distributions", "authors": ["Francesca Petralia", "Joshua Vogelstein", "David B. Dunson"], "emails": ["francesca.petralia@mssm.edu", "jo.vo@duke.edu", "dunson@stat.duke.edu"], "sections": [{"heading": null, "text": "given high-dimensional features is a challenging problem. It is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional. We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space. A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different subtrees in a soft probabilistic manner. The algorithm scales efficiently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features."}, {"heading": "1 Introduction", "text": "Massive datasets are becoming an ubiquitous by-product of modern scientific and industrial applications. These data present statistical and computational challenges because many previously developed analysis approaches do not scaleup sufficiently. Challenges arise because of the ultra high-dimensionality and relatively low sample size. Parsimonious models for such big data assume that the density in the ambient space concentrates around a lower-dimensional (possibly nonlinear) subspace. A plethora of methods are emerging to estimate such lower-dimensional subspaces [25, 2].\nar X\niv :1\n31 2.\n10 99\nv1 [\nWe are interested in using such lower-dimensional embeddings to obtain estimates of the conditional distribution of some target variable(s). This conditional density estimation setting arises in a number of important application areas, including neuroscience, genetics, and video processing. For example, one might desire automated estimation of a predictive density for a neurologic phenotype of interest, such as intelligence, on the basis of available data for a patient including neuroimaging. The challenge is to estimate the probability density function of the phenotype nonparametrically based on a 106 dimensional image of the subject\u2019s brain. It is crucial to avoid parametric assumptions on the density, such as Gaussianity, while allowing the density to change flexibly with predictors. Otherwise, one can obtain misleading predictions and poorly characterize predictive uncertainty.\nThere is a rich machine learning and statistical literature on conditional density estimation of a response y \u2208 Y given a set of features (predictors) x = (x1, x2, . . . , xp)\nT \u2208 X\u2286 Rp. Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28]. However, there has been limited consideration of scaling to large p settings, with the variational Bayes approach of [29] being a notable exception. For dimensionality reduction, [29] follow a greedy variable selection algorithm. Their approach does not scale to the sized applications we are interested in. For example, in a problem with p = 1, 000 and n = 500, they reported a CPU time of 51.7 minutes for a single analysis. We are interested in problems with p having many more orders of magnitude, requiring a faster computing time while also accommodating flexible nonlinear dimensionality reduction (variable selection is a limited sort of dimension reduction). To our knowledge, there are no nonparametric density regression competitors to our approach, which maintain a characterization of uncertainty in estimating the conditional densities; rather, all sufficiently scalable algorithms provide point predictions and/or rely on restrictive assumptions such as linearity.\nIn big data problems, scaling is often accomplished using divide-and-conquer techniques. However, as the number of features increases, the problem of finding the best splitting attribute becomes intractable, so that CART, MARS and multiple tree models cannot be efficiently applied. Similarly, mixture of experts becomes computationally demanding, since both mixture weights and dictionary densities are predictor dependent. To improve efficiency, sparse extensions relying on different variable selection algorithms have been proposed [21]. However, performing variable selection in high dimensions is effectively intractable: algorithms need to efficiently search for the best subsets of predictors to include in weight and mean functions within a mixture model, an NP-hard problem [14].\nIn order to efficiently deal with massive datasets, we propose a novel multiscale approach which starts by learning a multiscale dictionary of densities. This tree is efficiently learned in a first stage using a fast and scalable graph partitioning algorithm applied to the high-dimensional observations [19]. Expressing the conditional densities f(y|x) for each x \u2208 X as a convex combi-\nnation of coarse-to-fine scale dictionary densities, the learning problem in the second stage estimates the corresponding multiscale probability tree. This is accomplished in a Bayesian manner using a novel multiscale stick-breaking process, which allows the data to inform about the optimal bias-variance tradeoff; weighting coarse scale dictionary densities more highly decreases variance while adding to bias. This results in a model that borrows information across different resolution levels and reaches a good compromise in terms of the bias-variance tradeoff. We show that the algorithm scales efficiently to millions of features."}, {"heading": "2 Setting", "text": "Let X : \u2126 \u2192 X \u2286 Rp be a p-dimensional Euclidean vector-valued predictor random variable, taking values x \u2208 X , with a marginal probability distribution fX . Similarly, let Y : \u2126\u2192 Y be a target-valued random variable (e.g., Y \u2286 R). For inferential expedience, we posit the existence of a latent variable \u03b7 : \u2126 \u2192 M \u2286 X , where M is only d \u201cdimensional\u201d and d p. Note that M need not be a linear subspace of X , rather, M could be, for example, a union or affine subspaces, or a smooth compact Riemannian manifold. Regardless of the nature of M, we assume that we can approximately decompose the joint distribution as follows, fX,Y,\u03b7 = fX,Y |\u03b7f\u03b7 = fY |X,\u03b7fX|\u03b7f\u03b7 \u2248 fY |\u03b7fX|\u03b7f\u03b7. Hence, we assume that the signal approximately concentrates around a low-dimensional latent space, fY |X,\u03b7 = fY |\u03b7. This is a much less restrictive assumption than the commonplace assumption in manifold learning that the marginal distribution fX concentrates around a low-dimensional latent space.\nTo provide some intuition for our model, we provide the following concrete example where the distribution of y \u2208 R is a Gaussian function of the coordinate \u03b7 \u2208 M along the swissroll, which is embedded in a high-dimensional ambient space. Specifically, we sample the manifold coordinate, \u03b7 \u223c U(0, 1). We sample x = (x1, . . . , xp) T as follows\nx1 = \u03b7 sin(\u03b7) ; x2 = \u03b7 cos(\u03b7) ; xr \u223c N (0, 1) r \u2208 {3, . . . , p}\nFinally, we sample y from N (\u00b5(\u03b7), \u03c3(\u03b7)). Clearly, x and y are conditionally independent given \u03b7, which is the low-dimensional signal manifold. In particular, x lives on a swissroll embedded in a p-dimensional ambient space, but y is only a function of the coordinate \u03b7 along the swissrollM. The left panels of Figure 1 depict this example when \u00b5(\u03b7) = \u03b7 and \u03c3(\u03b7) = \u03b7 + 1."}, {"heading": "3 Goal", "text": "Our goal is to develop an approach to learn about fY |X from n pairs of observations that we assume are exchangeable samples from the joint distribution, (xi, yi) \u223c fX,Y \u2208 F . Let Dn = {(xi, yi)}i\u2208[n], where [n] = {1, . . . , n}. More specifically, we seek to obtain a posterior over fY |X . We insist that our approach satisfies several desiderata, including most importantly: (i) scales up to p \u2248 106 in reasonable time, (ii) yields good empirical results, and (iii) automatically adapts to the complexity of the data corpus. To our knowledge, no extant\napproach for estimating conditional densities or posteriors thereof satisfies even our first criterion."}, {"heading": "4 Methodology", "text": ""}, {"heading": "4.1 Ms. Deeds Framework", "text": "We propose here a general modular approach which we refer to as multiscale dictionary learning for estimating conditional distributions (\u201cMs. Deeds\u201d). Ms. Deeds consists of two components: (i) a tree decomposition of the space, and (ii) an assumed form of the conditional probability model.\nTree Decomposition A tree decomposition \u03c4 yields a multiscale partition of the data or the ambient space in which the data live. Let (W, \u03c1W , FW ) be a measurable metric space, where FW is a Borel probability measure, W, and \u03c1W : W \u00d7W \u2192 R is a metric on W. Let BWr (w) be the \u03c1W -ball inside W of radius r > 0 centered at w \u2208 W. For example, W could be the data corpus Dn, or it could be X \u00d7 Y. We define a tree decomposition as in [2, 6]. A partition tree \u03c4 of W consists of a collection of cells, \u03c4 = {Cj,k}j\u2208Z,k\u2208Kj . At each scale j, the set of cells Cj = {Cj,k}k\u2208Kj provides a disjoint partition of W almost everywhere. We define j = 0 as the root node. For each j > 0, each set has a unique parent node. Denote\nAj,k = {(j\u2032, k\u2032) : Cj,k \u2286 Cj\u2032,k\u2032 , j\u2032 < j} , Dj,k = {(j\u2032, k\u2032) : Cj\u2032,k\u2032 \u2286 Cj,k, j\u2032 > j}\nrespectively the ancestors and the descendants of node (j, k). Unlike classical harmonic theory which presupposes \u03c4 (e.g., in wavelets [8]), we choose to learn \u03c4 from the data. Previously, Chen et al. [6] developed a multiscale measure estimation strategy, and proved that there exists a scale j such that the approximate measure is within some bound of the true measure, under certain relatively general assumptions. We decided to simply partition the x\u2019s, ignoring the y\u2019s in the partitioning strategy. Our justification for this choice is as follows. First, sometimes there are many different y\u2019s for many different applications. In such cases, we do not want to bias the partitioning to any specific y\u2019s, all the more so when new unknown y\u2019s may later emerge. Second, because the x\u2019s are so much higher dimensional than the y\u2019s in our applications of interest, the partitions would be dominated by the x\u2019s, unless we chose a partitioning strategy that emphasized the y\u2019s. Thus, our strategy mitigates this difficulty (while certainly introducing others).\nGiven that we are going to partition using only the x\u2019s, we still face the choice of precisely how to partition. A fully Bayesian approach would construct a large number of partitions, and integrate over them to obtain posteriors. However, such a fully Bayesian strategy remains computationally intractable at scale, so we adopt a hybrid strategy. Specifically, we employ METIS [19], a wellknown relatively efficient multiscale partitioning algorithm with demonstrably good empirical performance on a wide range of graphs. Given n observations, i.e. xi = (xi1, . . . , xip)T \u2208 X for i \u2208 [n], the graph construction follows via computing all pairwise distances using \u03c1(xu, xv) = \u2016x\u0303u \u2212 x\u0303v\u20162, where x\u0303 is the whitened x (i.e., mean subtracted and variance normalized). We let there be an edge between xu and xv whenever e\u2212\u03c1(xu,xv) 2\n> t, where t is some threshold chosen to elicit the desired sparsity level. Applying METIS recursively on the graph constructed in this way yields a single tree (see supplementary material for further details).\nConditional Probability Model Given the tree decomposition of the data, we place a non-parametric prior over the tree. Specifically, we define fY |X as\nfY |X = \u2211 j\u2208Z \u03c0j,kj(x)fj,kj(x)(y|x) (1)\nwhere kj(x) is the set at scale j where x has been allocated and \u03c0j,kj (x) are weights across scales such that \u2211 j\u2208Z \u03c0j,kj(x) = 1. We let weights in Eq. (1) be generated by a stick-breaking process [26]. For each node Cj,k in the partition tree, we define a stick length Vj,k \u223c Beta(1, \u03b1). The parameter \u03b1 encodes the complexity of the model, with \u03b1 = 0 corresponding to the case in which f(y|x) = f(y). The stick-breaking process is defined as\n\u03c0j,k = Vj,k \u220f\n(j\u2032,k\u2032)\u2208Aj,k\n[1\u2212 Vj\u2032,k\u2032 ] , (2)\nwhere \u2211\n(j\u2032,k\u2032)\u2208Aj,k \u03c0j\u2032,k\u2032 = 1. The implication of this is that each scale within a path is weighted to optimize the bias/variance trade-off across scales. We refer\nto this prior as a multiscale stick-breaking process. Note that this Bayesian nonparametric prior assigns a positive probability to all possible paths, including those not observed in the training data. Thus, by adopting this Bayesian formulation, we are able to obtain posterior estimates for any newly observed data, regardless of the amount and variability of training data. This is a pragmatically useful feature of the Bayesian formulation, in addition to the alleviation of the need to choose a scale [6].\nEach fj,k in Eq. (1) is an element of a family of distributions. This family might be quite general, e.g., all possible conditional densities, or quite simple, e.g., Gaussian distributions. Moreover, the family can adapt with j or k, being more complex at the coarser scales (for which nj,k\u2019s are larger), and simpler for the finer scales (or partitions with fewer samples). We let the family of conditional densities for y be Gaussian for simplicity, that is, we assume that fj,k = N (\u00b5j,k, \u03c3j,k) with \u00b5j,k \u2208 R and \u03c3j,k \u2208 R+. Because we are interested in posteriors over the conditional distribution fY |X , we place relatively uninformative but conjugate priors on \u00b5j,k and \u03c3j,k, specifically, assuming the y\u2019s have been whitened and are unidimensional, \u00b5j,k \u223c N (0, 1) and \u03c3j,k = IG(a, b). Obviously, other choices, such as finite or infinite mixtures of Gaussians are also possible for continuous valued data."}, {"heading": "4.2 Inference", "text": "We introduce the latent variable `i \u2208 Z, for i = [n], denoting the multiscale level used by the ith observation. Let nj,k be the number of observations in Cj,k. Let kh(xi) be a variable indicating the set at level h where xi has been allocated. Each Gibbs sampler iteration can be summarized in the following steps:\n(i) Update `i by sampling from the multinomial full conditional: Pr(`i = j | \u00b7) = \u03c0j,kj(xi)fj,kj(xi)(yi|xi)/ \u2211 s\u2208Z \u03c0s,ks(xi)fs,ks(xi)(yi|xi)\n(ii) Update stick-breaking random variable Vj,k, for any j \u2208 Z and k \u2208 Kj , from Beta(\u03b2\u2032, \u03b1\u2032) with \u03b2\u2032 = 1 + nj,k and \u03b1\u2032 = \u03b1+ \u2211 (r,s)\u2208Dj,k nr,s.\n(iii) Update \u00b5j,k and \u03c3j,k, for any j \u2208 Z and k \u2208 Kj , by sampling from \u00b5j,k \u223c N (\u03c5j,k\u03bdj,ky\u0304j,k, \u03c5j,k) , \u03c3j,k \u223c IG ( a\u03c3, b+ 0.5 \u2211 i\u2208Ij,k (yi \u2212 \u00b5j,k) 2 ) where \u03c5j,k = (1 + \u03bdj,k)\u22121, \u03bdj,k = nj,k/\u03c3j,k a\u03c3 = a+ nj,k/2, y\u0304j,k being the average of the observations {yi} allocated to cell Cj,k and Ij,k = {i : `i = j, xi \u2208 Cj,k}.\nTo make predictions, the Gibbs sampler was run with up to 20, 000 iterations, including a burn-in of 1, 000 (see Supplementary material for details). Gibbs sampler chains were stopped testing normality of normalized averages of functions of the Markov chain [5]. Parameters (a, b) and \u03b1 involved in the prior density of parameters \u03c3j,k\u2019s and Vj,k\u2019s were set to (3, 1) and 1, respectively. All predictions used a leave-one-out strategy."}, {"heading": "4.3 Simulation Studies", "text": "In order to assess the predictive performance of the proposed model, we considered the four different simulation scenarios described below.\n(1) Nonlinear Mixture: We first consider the following nonlinear joint model\ny|\u03b7 \u223c |\u03b7|N (\u00b51, \u03c31) + (1\u2212 |\u03b7|)N (\u00b52, \u03c32),\nxr|\u03b7 \u223c N (\u03b7, \u03c3x) r \u2208 {1, 2, . . . , p} , \u03b7 \u223c sin[U(0, c)]\nIn the simulations we let (\u00b51, \u03c31) = (\u22122, 1), (\u00b52, \u03c32) = (2, 1), \u03c3x = 0.1, and c = 20, and p = 1000. Thus, fY |X is a highly nonlinear function of x, and even \u03b7, and x is high-dimensional.\n(2) Swissroll : We then return to the swissroll example of Figure 1; in Figure 3 we show results for (\u00b5, \u03c3) = (\u03b7, 1).\n(3) Linear Subspace: Letting \u0393 \u2208 Rp+1\u00d7d be a matrix with orthonormal columns and \u0398 be a d\u00d7 d diagonal matrix, we assume the following model for z = (y, xT )T :\nz|\u03b7 \u223c Np+1 (\u2126\u03b7, I) ,\nwhere \u2126 = \u0393\u0398, \u0393 is uniformly sampled from the Stiefel manifold, \u03b8ii \u223c IG(a\u03b8, b\u03b8) for i \u2208 {1, . . . , d} and all other elements of \u0398 are zero, and \u03b7 \u223c Nd(0, I). In the simulation, we let q = d = 5, (\u03b1\u03b8, \u03b2\u03b8) = (1, 0.25).\n(4) Union of Linear Subspaces: This model is a direct extension of the linear subspace model described in (3). Specifically, we assume\nz|\u03b7 \u223c G\u2211 g=1 \u03c9gNp+1(\u2126g\u03b7, I),\n\u03c9 \u223c Dirichlet(\u03b1) , \u03b7 \u223c Nd(0, I),\nwhere \u2126g = \u0393g\u0398g, \u0393g is a matrix with orthonormal columns sampled uniformly from the Stiefel manifold and \u0398g is a (d\u00d7 d) diagonal matrix with \u03b8ii \u223c IG(ag, bg) for i \u2208 {1, . . . , g}. In the simulation, we let G = 5, \u03b1 = (1, . . . , 1)T, (\u03b1g, \u03b2g) = (\u03b1\u03b8, \u03b2\u03b8) as above."}, {"heading": "4.4 Neuroscience Applications", "text": "We assessed the predictive performance of the proposed method on two very different neuroimaging datasets. For all analyses, each variable was normalized by subtracting its mean and dividing by its standard deviation. The prior specification and Gibbs sampler described in \u00a74.1 and 4.2 were utilized.\nIn the first experiment we investigated the extent to which we could predict creativity (as measured via the Composite Creativity Index [3]) via a structural\nconnectome dataset collected at the Mind Research Network (data were collected as described in Jung et al. [18]). For each subject, we estimate a 70 vertex undirected weighted brain-graph using the Magnetic Resonance Connectome Automated Pipeline (MRCAP) [12] from diffusion tensor imaging data [20]. Because our graphs are undirected and lack self-loops, we have a total of p =( 70 2 ) = 2, 415 potential weighted edges. The p-dimensional feature vector is defined by the natural logarithm of the vectorized matrix described above. The second dataset comes from a resting-state functional magnetic resonance experiment as part of the Autism Brain Imaging Data Exchange [1]. We selected the Yale Child Study Center for analysis. Each brain-image was processed using the Configurable Pipeline for Analysis of Connectomes (CPAC) [27]. For each subject, we computed a measure of normalized power at each voxel called fALFF [30]. To ensure the existence of nonlinear signal relating these predictors, we let yi correspond to an estimate of overall head motion in the scanner, called mean framewise displacement (FD) computed as described in Power et al. [24]. In total, there were p = 902, 629 voxels."}, {"heading": "4.5 Evaluation Criteria", "text": "To compare algorithmic performance we considered rAm defined as\nrAm = \u03c6(MSB)/\u03c6(A),\nwhere \u03c6 is the quantity of interest (for example, CPU time in seconds or mean squared error), MSB is our approach and A is the competitor algorithm. To obtain mean-squared error estimates from MSB, we select our posterior mean as a point-estimate (the comparison algorithms do not generate posterior predictions, only point estimates). For each simulation scenario, we sampled multiple datasets and compute the matched distribution of rAm. In other words, rather than running simulations and reporting the distribution of performance for each algorithm, we compare the algorithms per simulation. This provides a much more informative indication of algorithmic performance, in that we indicate the fraction of simulations one algorithm outperforms another on some metric. For each example, we sampled 20 datasets to obtain estimates of the distribution over rAm. All experiments were performed on a typical workstation, Intel Core i7-2600K Quad-Core Processor with 8192 MB of RAM."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 Illustrative Example", "text": "The middle and right panels of Figure 1 depict the quality of partitioning and density estimation for the swissroll example described in \u00a72, with the ambient dimension p = 1000 and the predictive manifold dimension d = 1. We sampled n = 104 samples for this illustration. At scale 3 we have 4 partitions, and at scale 4 we have 8 (note that the partition tree, in general, need not be binary). The top panels are color coded to indicate which xi\u2019s fall into which partition. Although imperfect, it should be clear that the data are partitioned very well.\nThe bottom panels show the resulting estimate of the posteriors at the two scales. These posteriors are piecewise constant , as they are invariant to the manifold coordinate within a given partition.\nTo obviate the need to choose a scale to use to make a prediction, we choose to adopt a Bayesian approach and integrate across scales. Figure 2 shows the estimated density of two observations of model (1) with parameters (\u00b51, \u03c31) = (\u22122, 1), (\u00b52, \u03c32) = (2, 1), \u03c3x = 0.1, and c = 20 for different sample sizes. Posteriors of the conditional density fY |X were computed for various sample sizes. Figure 2 suggests that our estimate of fY |X approaches the true density as the number of observations in the training set increases. We are unable to compare our strategy for posterior estimation to previous literature because we are unaware of previous Bayesian approaches for this problem that scale up to problems of this size. Therefore, we numerically compare the performance of our point-estimates (which we define as the posterior mean of f\u0302Y |X) with the predictions of the competitor algorithms."}, {"heading": "5.2 Quantitative Comparisons for Simulated Data", "text": "Figure 3 compares the numerical performance of our algorithm (MSB) with Lasso (black), CART (red), and PC regression (green) in terms of both meansquared error (top) and CPU time (bottom) for models (2), (3), and (4) in the left, middle, and right panels respectively. These figures show relative performance on a per simulation basis, thus enabling a much more powerful comparison than averaging performance for each algorithm over a set of simulations. Note that these three simulations span a wide range of models, including nonlinear smooth manifolds such as the swissroll (model 2), relatively simple linear subspace manifolds (model 3), and a union of linear subspaces model (model 4 ; which is neither linear nor a manifold).\nIn terms of predictive accuracy, the top panels show that for all three simulations, in every dimensionality that we considered\u2014including p = 0.5 \u00d7 106\u2014 MSB is more accurate than either Lasso, CART, or PC regression. Note that this is the case even though MSB provides much more information about the posterior fY |X , yielding an entire posterior over fY |X , rather than merely a point estimate.\nIn terms of computational time, MSB is much faster than the competitors for large p and n, as shown in the bottom three panels. The supplementary materials show that computational time for MSB is relatively constant as a function of p, whereas Lasso\u2019s computational time grows considerably with p. Thus, for large enough p, MSB is significantly faster that Lasso. MSB is faster than CART and PC regression for all p and n under consideration. Thus, it is clear from these simulations that MSB has better scaling properties\u2014in terms of both predictive accuracy and computational time\u2014than the competitor methods."}, {"heading": "5.3 Quantitative Comparisons for Neuroscience Applications", "text": "Table 1 shows the mean and standard deviation of point-estimate predictions per subject (using leave-one-out) for the two neuroscience applications that we investigated: (i) predicting creativity from diffusion MRI (creativity) and, (ii) predicting head motion based on functional MRI (movement). For the creativity application, p was relatively small, \u201cmerely\u201d 2, 415, so we could run Lasso, CART, and random forests (RF) [4]. For the movement application, p was nearly one million.\nFor both applications, MSB yielded improved predictive accuracy over all competitors. Although CART and Lasso were faster than MSB on the relatively low-dimensional predictor example (creativity), their computational scaling was poor, such that CART yielded a memory fault on the higher-dimensional case, and Lasso required substantially more time than MSB."}, {"heading": "6 Discussion", "text": "In this work we have introduced a general formalism to estimate conditional distributions via multiscale dictionary learning. An important property of any such strategy is the ability to scale up to ultrahigh-dimensional predictors. We considered simulations and real-data examples where the dimensionality of the predictor space approached one million. To our knowledge, no other approach to learn conditional distributions can run at this scale. Our approach explicitly assumes that the posterior fY |X can be well approximated by projecting x onto a lower-dimensional space, fY |X \u2248 fY |\u03b7, where \u03b7 \u2208M \u2282 Rd, and x \u2208 Rd. Note that this assumption is much less restrictive than assuming that x is close to a low-dimensional space; rather, we only assume that the part of fX that \u201cmatters\u201d to predict y lives near a low-dimensional subspace. Because a fully Bayesian\nstrategy remains computationally intractable at this scale, we developed an empirical Bayes approach, estimating the partition tree based on the data, but integrating over scales and posteriors.\nWe demonstrate that even though we obtain posteriors over the conditional distribution fY |X , our approach, dubbed multiscale stick-breaking (MSB), outperforms several standard machine learning algorithms in terms of both predictive accuracy and computational time, as the sample size (n) and ambient dimension (p) increase. This improvement was demonstrated when theM was a swissroll, a latent subspace, a union of latent subspaces, and real data (for which the latent space may not even exist).\nIn future work, we will extend these numerical results to obtain theory on posterior convergence. Indeed, while multiscale methods benefit from a rich theoretical foundation [2], the relative advantages and disadvantages of a fully Bayesian approach, in which one can estimate posteriors over all functionals of fY |X at all scales, remains relatively unexplored.\nReferences [1] ABIDE. http://fcon_1000.projects.nitrc.org/indi/abide/.\n[2] W.K. Allard, G. Chen, and M. Maggioni. Multiscale geometric methods for data sets II: geometric wavelets. Applied and Computational Harmonic Analysis, 32:435\u2013462, 2012.\n[3] R. Arden, R. S. Chavez, R. Grazioplene, and R. E. Jung. Neuroimaging creativity: a psychometric view. Behavioural brain research, 214:143\u2013156, 2010.\n[4] Leo Breiman. Statistical Modeling : The Two Cultures. Statistical Science, 16(3):199\u2013231, 2001.\n[5] Didier Chauveau and Jean Diebolt. An automated stopping rule for mcmc convergence assessment. Computational Statistics, 14:419\u2013442, 1998.\n[6] G. Chen, M. Iwen, S. Chin, and M. Maggioni. A fast multiscale framework for data in high-dimensions: Measure estimation, anomaly detection, and compressive measurements. In VCIP, 2012 IEEE, 2012.\n[7] Y. Chung and D. B. Dunson. Nonparametric Bayes conditional distribution modeling with variable selection. Journal of the American Statistical Association, 104:1646\u20131660, 2009.\n[8] Ingrid Daubechies. Ten Lectures on Wavelets (CBMS-NSF Regional Conference Series in Applied Mathematics). SIAM: Society for Industrial and Applied Mathematics, 1992.\n[9] D. B. Dunson, N. Pillai, and J. H. Park. Bayesian density regression. Journal of the Royal Statistical Society Series B-Statistical Methodology, 69:163\u2013183, 2007.\n[10] J. Q. Fan, Q. W. Yao, and H. Tong. Estimation of conditional densities and sensitivity measures in nonlinear dynamical systems. Biometrika, 83:189\u2013206, 1996.\n[11] G. Fu, F. Y. Shih, and H. Wang. A kernel-based parametric method for conditional density estimation. Pattern recognition, 44:284\u2013294, 2011.\n[12] W.R\u0303. Gray, J.\u00c3. Bogovic, J.T\u0303. Vogelstein, B.\u00c3. Landman, J\u02d9 L. Prince, and R.J\u0303. Vogelstein. Magnetic resonance connectome automated pipeline: an overview. IEEE pulse, 3(2):42\u20138, March 2010.\n[13] J. E. Griffin and M. F. J. Steel. Order-based dependent Dirichlet processes. Journal of the American Statistical Association, 101:179\u2013194, 2006.\n[14] Isabelle Guyon and Andr\u00e9 Elisseeff. An introduction to variable and feature selection. The Journal of Machine Learning Research, 3:1157\u20131182, 2003.\n[15] M. P. Holmes, G. A. Gray, and C. L. Isbell. Fast kernel conditional density estimation: a dual-tree Monte Carlo approach. Computational statistics & data analysis, 54:1707\u20131718, 2010.\n[16] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixture of local experts. Neural Computation, 3:79\u201387, 1991.\n[17] W. X. Jiang and M. A. Tanner. Hierarchical mixtures-of-experts for exponential family regression models: approximation and maximum likelihood estimation. Annals of Statistics, 27:987\u20131011, 1999.\n[18] R.E\u0303. Jung, R. Grazioplene, A. Caprihan, R.S\u0303. Chavez, and R.J\u0303. Haier. White matter integrity, creativity, and psychopathology: Disentangling constructs with diffusion tensor imaging. PloS one, 5(3):e9818, 2010.\n[19] G. Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs. SIAM Journal on Scientific Computing 20, 1:359\u00d0392, 1999.\n[20] Susumu Mori and Jiangyang Zhang. Principles of diffusion tensor imaging and its applications to basic neuroscience research. Neuron, 51(5):527\u201339, September 2006.\n[21] I. Mossavat and O. Amft. Sparse bayesian hierarchical mixture of experts. IEEE Statistical Signal Processing Workshop (SSP), 2011.\n[22] A. Norets and J. Pelenis. Bayesian modeling of joint and conditional distributions. Journal of Econometrics, 168:332\u2013346, 2012.\n[23] D. J. Nott, S. L. Tan, M. Villani, and R. Kohn. Regression density estimation with variational methods and stochastic approximation. Journal of Computational and Graphical Statistics, 21:797\u2013820, 2012.\n[24] J. D. Power, K. A. Barnes, C. J. Stone, and R. A. Olshen. Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion. Neuroimage, 59:2142\u20132154, 2012.\n[25] I. U. Rahman, I. Drori, V. C. Stodden, and D. L. Donoho. Multiscale representations for manifold- valued data. SIAM J. Multiscale Model, 4:1201\u20131232, 2005.\n[26] J. Sethuraman. A constructive de\u00denition of Dirichlet priors. Statistica Sinica, 4:639\u2013650, 1994.\n[27] S. Sikka, J.T\u0303. Vogelstein, and M.P\u0303. Milham. Towards Automated Analysis of Connectomes: The Configurable Pipeline for the Analysis of Connectomes (CPAC). Neuroinformatics, 2012.\n[28] S. T. Tokdar, Y. M. Zhu, and J. K. Ghosh. Bayesian density regression with logistic Gaussian process and subspace projection. Bayesian Analysis, 5:319\u2013344, 2010.\n[29] M. N. Tran, D. J. Nott, and R. Kohn. Simultaneous variable selection and component selection for regression density estimation with mixtures of heteroscedastic experts. Electronic Journal of Statistics, 6:1170\u20131199, 2012.\n[30] Q-H. Zou, C-Z. Zhu, Y. Yang, X-N. Zuo, X-Y. Long, Q-J. Cao, Y-FW\u0307ang, and Y-F. Zang. An improved approach to detection of amplitude of low-frequency fluctuation (ALFF) for resting-state fMRI: fractional ALFF. Journal of neuroscience methods, 172(1):137\u2013141, July 2008."}], "references": [{"title": "Multiscale geometric methods for data sets II: geometric wavelets", "author": ["W.K. Allard", "G. Chen", "M. Maggioni"], "venue": "Applied and Computational Harmonic Analysis, 32:435\u2013462", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Neuroimaging creativity: a psychometric view", "author": ["R. Arden", "R.S. Chavez", "R. Grazioplene", "R.E. Jung"], "venue": "Behavioural brain research, 214:143\u2013156", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical Modeling : The Two Cultures", "author": ["Leo Breiman"], "venue": "Statistical Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "An automated stopping rule for mcmc convergence assessment", "author": ["Didier Chauveau", "Jean Diebolt"], "venue": "Computational Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "A fast multiscale framework for data in high-dimensions: Measure estimation", "author": ["G. Chen", "M. Iwen", "S. Chin", "M. Maggioni"], "venue": "anomaly detection, and compressive measurements. In VCIP, 2012 IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonparametric Bayes conditional distribution modeling with variable selection", "author": ["Y. Chung", "D.B. Dunson"], "venue": "Journal of the American Statistical Association, 104:1646\u20131660", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Ten Lectures on Wavelets (CBMS-NSF Regional Conference Series in Applied Mathematics)", "author": ["Ingrid Daubechies"], "venue": "SIAM: Society for Industrial and Applied Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "Bayesian density regression", "author": ["D.B. Dunson", "N. Pillai", "J.H. Park"], "venue": "Journal of the Royal Statistical Society Series B-Statistical Methodology, 69:163\u2013183", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Estimation of conditional densities and sensitivity measures in nonlinear dynamical systems", "author": ["J.Q. Fan", "Q.W. Yao", "H. Tong"], "venue": "Biometrika, 83:189\u2013206", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "A kernel-based parametric method for conditional density estimation", "author": ["G. Fu", "F.Y. Shih", "H. Wang"], "venue": "Pattern recognition, 44:284\u2013294", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Magnetic resonance connectome automated pipeline: an overview", "author": ["W.R\u0303. Gray", "J.\u00c3. Bogovic", "J.T\u0303. Vogelstein", "B.\u00c3. Landman", "J \u0307 L. Prince", "R.J\u0303. Vogelstein"], "venue": "IEEE pulse,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Order-based dependent Dirichlet processes", "author": ["J.E. Griffin", "M.F.J. Steel"], "venue": "Journal of the American Statistical Association, 101:179\u2013194", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Fast kernel conditional density estimation: a dual-tree Monte Carlo approach", "author": ["M.P. Holmes", "G.A. Gray", "C.L. Isbell"], "venue": "Computational statistics & data analysis, 54:1707\u20131718", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive mixture of local experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S.J. Nowlan", "G.E. Hinton"], "venue": "Neural Computation, 3:79\u201387", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1991}, {"title": "Hierarchical mixtures-of-experts for exponential family regression models: approximation and maximum likelihood estimation", "author": ["W.X. Jiang", "M.A. Tanner"], "venue": "Annals of Statistics, 27:987\u20131011", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "A", "author": ["R.\u1ebc. Jung", "R. Grazioplene"], "venue": "Caprihan, R.S\u0303. Chavez, and R.J\u0303. Haier. White matter integrity, creativity, and psychopathology: Disentangling constructs with diffusion tensor imaging. PloS one, 5(3):e9818", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast and high quality multilevel scheme for partitioning irregular graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "SIAM Journal on Scientific Computing 20, 1:359\u00d0392", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Principles of diffusion tensor imaging and its applications to basic neuroscience research", "author": ["Susumu Mori", "Jiangyang Zhang"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Sparse bayesian hierarchical mixture of experts", "author": ["I. Mossavat", "O. Amft"], "venue": "IEEE Statistical Signal Processing Workshop (SSP)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian modeling of joint and conditional distributions", "author": ["A. Norets", "J. Pelenis"], "venue": "Journal of Econometrics, 168:332\u2013346", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Regression density estimation with variational methods and stochastic approximation", "author": ["D.J. Nott", "S.L. Tan", "M. Villani", "R. Kohn"], "venue": "Journal of Computational and Graphical Statistics, 21:797\u2013820", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion", "author": ["J.D. Power", "K.A. Barnes", "C.J. Stone", "R.A. Olshen"], "venue": "Neuroimage, 59:2142\u20132154", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiscale representations for manifold- valued data", "author": ["I.U. Rahman", "I. Drori", "V.C. Stodden", "D.L. Donoho"], "venue": "SIAM J. Multiscale Model, 4:1201\u20131232", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "A constructive de\u00denition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica, 4:639\u2013650", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "Towards Automated Analysis of Connectomes: The Configurable Pipeline for the Analysis of Connectomes", "author": ["S. Sikka", "J.T\u0303. Vogelstein", "M.P\u0303. Milham"], "venue": "(C- PAC). Neuroinformatics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Bayesian density regression with logistic Gaussian process and subspace projection", "author": ["S.T. Tokdar", "Y.M. Zhu", "J.K. Ghosh"], "venue": "Bayesian Analysis, 5:319\u2013344", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Simultaneous variable selection and component selection for regression density estimation with mixtures of heteroscedastic experts", "author": ["M.N. Tran", "D.J. Nott", "R. Kohn"], "venue": "Electronic Journal of Statistics, 6:1170\u20131199", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Y-F\u1e86ang", "author": ["Q-H. Zou", "C-Z. Zhu", "Y. Yang", "X-N. Zuo", "X-Y. Long", "Q-J. Cao"], "venue": "and Y-F. Zang. An improved approach to detection of amplitude of low-frequency fluctuation (ALFF) for resting-state fMRI: fractional ALFF. Journal of neuroscience methods, 172", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 23, "context": "A plethora of methods are emerging to estimate such lower-dimensional subspaces [25, 2].", "startOffset": 80, "endOffset": 87}, {"referenceID": 0, "context": "A plethora of methods are emerging to estimate such lower-dimensional subspaces [25, 2].", "startOffset": 80, "endOffset": 87}, {"referenceID": 14, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 59, "endOffset": 67}, {"referenceID": 15, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 59, "endOffset": 67}, {"referenceID": 8, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 84, "endOffset": 96}, {"referenceID": 13, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 84, "endOffset": 96}, {"referenceID": 9, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 84, "endOffset": 96}, {"referenceID": 21, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 129, "endOffset": 141}, {"referenceID": 27, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 129, "endOffset": 141}, {"referenceID": 20, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 129, "endOffset": 141}, {"referenceID": 11, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 170, "endOffset": 184}, {"referenceID": 7, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 170, "endOffset": 184}, {"referenceID": 5, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 170, "endOffset": 184}, {"referenceID": 26, "context": "Common approaches include hierarchical mixtures of experts [16, 17], kernel methods [10, 15, 11], Bayesian finite mixture models [23, 29, 22] and Bayesian nonparametrics [13, 9, 7, 28].", "startOffset": 170, "endOffset": 184}, {"referenceID": 27, "context": "However, there has been limited consideration of scaling to large p settings, with the variational Bayes approach of [29] being a notable exception.", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "For dimensionality reduction, [29] follow a greedy variable selection algorithm.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "To improve efficiency, sparse extensions relying on different variable selection algorithms have been proposed [21].", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "However, performing variable selection in high dimensions is effectively intractable: algorithms need to efficiently search for the best subsets of predictors to include in weight and mean functions within a mixture model, an NP-hard problem [14].", "startOffset": 242, "endOffset": 246}, {"referenceID": 17, "context": "This tree is efficiently learned in a first stage using a fast and scalable graph partitioning algorithm applied to the high-dimensional observations [19].", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "We define a tree decomposition as in [2, 6].", "startOffset": 37, "endOffset": 43}, {"referenceID": 4, "context": "We define a tree decomposition as in [2, 6].", "startOffset": 37, "endOffset": 43}, {"referenceID": 6, "context": ", in wavelets [8]), we choose to learn \u03c4 from the data.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "[6] developed a multiscale measure estimation strategy, and proved that there exists a scale j such that the approximate measure is within some bound of the true measure, under certain relatively general assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Specifically, we employ METIS [19], a wellknown relatively efficient multiscale partitioning algorithm with demonstrably good empirical performance on a wide range of graphs.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "(1) be generated by a stick-breaking process [26].", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "This is a pragmatically useful feature of the Bayesian formulation, in addition to the alleviation of the need to choose a scale [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "Gibbs sampler chains were stopped testing normality of normalized averages of functions of the Markov chain [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "In the first experiment we investigated the extent to which we could predict creativity (as measured via the Composite Creativity Index [3]) via a structural", "startOffset": 136, "endOffset": 139}, {"referenceID": 16, "context": "[18]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "For each subject, we estimate a 70 vertex undirected weighted brain-graph using the Magnetic Resonance Connectome Automated Pipeline (MRCAP) [12] from diffusion tensor imaging data [20].", "startOffset": 141, "endOffset": 145}, {"referenceID": 18, "context": "For each subject, we estimate a 70 vertex undirected weighted brain-graph using the Magnetic Resonance Connectome Automated Pipeline (MRCAP) [12] from diffusion tensor imaging data [20].", "startOffset": 181, "endOffset": 185}, {"referenceID": 25, "context": "Each brain-image was processed using the Configurable Pipeline for Analysis of Connectomes (CPAC) [27].", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "For each subject, we computed a measure of normalized power at each voxel called fALFF [30].", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "For the creativity application, p was relatively small, \u201cmerely\u201d 2, 415, so we could run Lasso, CART, and random forests (RF) [4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "Indeed, while multiscale methods benefit from a rich theoretical foundation [2], the relative advantages and disadvantages of a fully Bayesian approach, in which one can estimate posteriors over all functionals of fY |X at all scales, remains relatively unexplored.", "startOffset": 76, "endOffset": 79}], "year": 2013, "abstractText": "Nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem. It is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional. We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space. A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different subtrees in a soft probabilistic manner. The algorithm scales efficiently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features.", "creator": "LaTeX with hyperref package"}}}