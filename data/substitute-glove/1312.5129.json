{"id": "1312.5129", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2013", "title": "Deep Learning Embeddings for Discontinuous Linguistic Units", "abstract": "Deep learners non-rational have he later one for frequently natural arabic processing (NLP) inability. Embeddings most mostly computed for same whereas although once number particular followed contents have current an alone use linguistic units like isomers especially phrases. In this hand, thought think that learning embeddings from discontinuous linguistic military consider its be existence. In an mechanical therapy taking coreference resolution, our featured no unlike embeddings classes making than something can embeddings.", "histories": [["v1", "Wed, 18 Dec 2013 13:34:16 GMT  (16kb)", "https://arxiv.org/abs/1312.5129v1", null], ["v2", "Thu, 19 Dec 2013 11:01:02 GMT  (15kb)", "http://arxiv.org/abs/1312.5129v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "hinrich sch\\\"utze"], "accepted": false, "id": "1312.5129"}, "pdf": {"name": "1312.5129.pdf", "metadata": {"source": "CRF", "title": "Deep Learning Embeddings for Discontinuous Linguistic Units", "authors": ["Wenpeng Yin"], "emails": ["wenpeng@cis.lmu.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n51 29\nv2 [\ncs .C\nL ]\n1 9\nDeep learning embeddings have been successfully used for many natural language processing problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like morphemes and phrases. In this paper, we argue that learning embeddings for discontinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings."}, {"heading": "1 Motivation", "text": "One advantage of recent work in deep learning on natural language processing (NLP) is that linguistic units are represented by rich and informative embeddings. These embeddings support better performance on a variety of NLP tasks (Collobert et al., 2011) than symbolic linguistic representations that do not directly represent information about similarity and other linguistic properties. Embeddings are mostly derived for word forms although a number of recent papers have extended this to other linguistic units like morphemes (Luong et al., 2013) and phrases (Mikolov et al., 2013). Thus, an important question is: what are the basic linguistic units that should be represented by embeddings in a deep learning NLP system? In this paper, we argue that certain discontinuous linguistic units should also have embeddings. We will restrict ourselves to the arguably simplest possible type of discontinuity: two noncontinous words. For example, in the sentence \u201cthis tea helped me to relax\u201d, \u201chelped*to\u201d is one of several such two-word discontinuities. We will refer to discontinuous linguistic units like \u201chelped*to\u201d as minimal contexts (MC) for reasons that will become clear presently.\nWe can approach the question of what basic linguistic units should have representations from a practical as well as from a cognitive point of view. In practical terms, we want representations to be optimized for good generalization. There are many situations where a particular task involving a phrase cannot be solved based on the phrase itself, but it can be solved by analyzing the context of the phrase. For example, if a coreference resolution system needs to determine whether the unknown word \u201cXiulan\u201d (a Chinese first name) in \u201che helped Xiulan to find a flat\u201d refers to an animate or an inanimate entity, then the minimal context \u201chelped*to\u201d is a good indicator for the animacy of the unknown word \u2013 whereas the unknown word itself provides no clue.\nFrom a cognitive point of view, it can be argued that many basic units that the human cognitive system uses are also discontinuous. Particularly convincing examples for such units are phrasal verbs in English, which frequently occur discontinuously. It is implausible to suppose that we retrieve atomic representations for, say, \u201ckeep\u201d, \u201cup\u201d, \u201cunder\u201d and \u201cin\u201d and then combine them to form the meanings of phrases like \u201ckeep him up\u201d, \u201ckeep them under\u201d, \u201ckeep it in\u201d. Rather, it is more plausible\nthat we recognize \u201ckeep up\u201d, \u201ckeep under\u201d and \u201ckeep in\u201d as relevant basic linguistic units in these contexts and that the human cognitive systems represents them as units.\nThis paper presents an initial study of minimal context embeddings and shows that they are better suited for a classification task needed for coreference resolution than word embeddings. Our conclusion is that minimal contexts (as well as inflected word forms, morphemes and phrases) should be considered as basic units that we need to learn embeddings for."}, {"heading": "2 Experimental setup", "text": ""}, {"heading": "2.1 Embedding learning", "text": "With English Gigaword Corpus, we use the skip-gram model as implemented in word2vec1 (Mikolov et al., 2013) to induce embeddings. To be able to use word2vec directly without code changes, we represent the corpus as a sequence of sentences, each consisting of two tokens: an MC (written as the two enclosing words separated by a star) and a word that occurs between the two enclosing words. The distance k between the two enclosing words can be varied. In our experiments, we use either distance k = 2 or distance 2 \u2264 k \u2264 3. For example, for k = 2, the trigram wi\u22121 wi wi+1 generates the single sentence \u201cwi\u22121*wi+1 wi\u201d; and for 2 \u2264 k \u2264 3, the fourgram wi\u22122 wi\u22121 wi wi+1 generates the four sentences \u201cwi\u22122*wi wi\u22121\u201d, \u201cwi\u22121*wi+1 wi\u201d, \u201cwi\u22122*wi+1 wi\u22121\u201d and \u201cwi\u22122*wi+1 wi\u201d.\nNote that the reformated corpus enables word2vec to learn embeddings for single words and MCs simultaneously, we discard the word embeddings, and yet compute standard word embeddings on the original corpus using word2vec skip-gram model. In experiments, embedding size is set to 200."}, {"heading": "2.2 Markable classification task", "text": "A markable is a linguistic expression that refers to an entity in the real world or another linguistic expression. Examples of markables include noun phrases (\u201cthe man\u201d), named entities (\u201cPeter\u201d) and nested noun phrases (\u201ctheir\u201d). We address the task of animacy classification of markables: classifying them as animate/inanimate. This feature is useful for coreference resolution systems because only animate markables can be referred to using masculine and feminine pronouns in English like \u201chim\u201d and \u201cshe\u201d. Thus, this is an important clue for automatically clustering the markables of a document into correct coreference chains.\nTo create training and test sets, we extract all 39,689 coreference chains from the CoNLL2012 OntoNotes corpus.2 We label chains that contain one of the markables \u201cshe\u201d, \u201cher\u201d, \u201che\u201d, \u201chim\u201d or \u201chis\u201d as animate and chains that contain one of \u201cit\u201d or \u201cits\u201d as inanimate.\nWe extract 39,942 markables and their corresponding MCs from the 10,361 animate and inanimate chains where an MC simply is the pair of the two words occurring to the left and right of the markable. The gold label of a markable and its MC is the animacy status of its chain: either animate or inanimate. We divide all MCs having received an embedding in the embedding learning phase into a training set of 11,301 (8097 animate, 3204 inanimate) and a balanced test set of 4036.\nWe use LIBLINEAR3 for classification, with penalty factors 3 and 1 for inanimate and animate classes, respectively, because the training data are unbalanced."}, {"heading": "3 Experimental results", "text": "We compare the following representations for animacy classification of markables. (i) MC: minimal context embeddings with k = 2 and 2 \u2264 k \u2264 3; (ii) concatenation: concatenation of the embeddings of the two enclosing words where the embeddings are either standard word2vec embeddings (see Section 2.1) or the embeddings published by Collobert et al. (2011);4 (iii) the bag-of-words (BOW)\n1https://code.google.com/p/word2vec/ 2http://conll.cemantix.org/2012/data.html 3https://github.com/bwaldvogel/liblinear-java 4http://metaoptimize.com/projects/wordreprs/\nrepresentation of a minimal context: the concatentation of two one-hot vectors of dimensionality V where V is the size of the vocabulary. The first (resp. second) vector is the one-hot vector for the left (resp. right) word of the MC. Experimental results are shown in Table 1.\nThe results show that MC embeddings have an obvious advantage in this classification task, both for k = 2 and 2 \u2264 k \u2264 3. This validates our hypothesis that learning embeddings for discontinuous linguistic units is promising.\nIn our error analysis, we found two types of frequent errors. (i) Unspecific MCs. Many MCs are equally appropriate for animate and inanimate markables. Examples of such MCs include \u201ctake*in\u201d, \u201ckeep*alive\u201d and \u201cthen*goes\u201d. (ii) Untypical use of specific MCs. Even MCs that are specific with respect to what type of markable they enclose sometimes occur with the \u201cwrong\u201d type of markable. For example, most markables occurring in the MC \u201cof*whose\u201d are animate because \u201cwhose\u201d usually refers to an animate markable. However, in the context \u201c. . . the southeastern area of Fujian whose economy is the most active\u201d the enclosed markable is Fujian, a province of China. This example shows that \u201cwhose\u201d occasionally refers to an inanimate entity even though these cases are infrequent."}, {"heading": "4 Related work", "text": "Most work on embeddings has focused on word forms with a few exceptions, notably embeddings for stems and morphemes (Luong et al., 2013) and for phrases (Mikolov et al., 2013). To the best of our knowledge, our work is the first to learn embeddings for discontinuous linguistic units.\nAn alternative to learning an embedding for a linguistic unit is to calculate its distributed representation from the distributed representations of its parts; the best known work along those lines is (Socher et al., 2012, 2010, 2011). This approach is superior for units that are compositional, i.e., whose properties are systematically predictable from their parts. Our approach (as well as similar work on continuous phrases) only makes sense for noncompositional units."}, {"heading": "5 Conclusion and Future Work", "text": "We have argued that discontinuous linguistic units are part of the inventory of linguistic units that we should compute embeddings for and we have shown that such embeddings are superior to word form embeddings in a coreference resolution task.\nIt is obvious that we cannot and do not want to compute embeddings for all possible discontinuous linguistic units. Similarly, the subset of phrases that embeddings are computed for should be carefully selected. In future work, we plan to address the question of how to select a subset of linguistic units \u2013 e.g., those that are least compositional \u2013 when inducing embeddings."}], "references": [{"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["M.T. Luong", "R. Socher", "C.D. Manning"], "venue": "in: Proceedings of the Conference on Computational Natural Language Learning,", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "in: Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "in: Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Socher et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "in: Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "These embeddings support better performance on a variety of NLP tasks (Collobert et al., 2011) than symbolic linguistic representations that do not directly represent information about similarity and other linguistic properties.", "startOffset": 70, "endOffset": 94}, {"referenceID": 1, "context": "Embeddings are mostly derived for word forms although a number of recent papers have extended this to other linguistic units like morphemes (Luong et al., 2013) and phrases (Mikolov et al.", "startOffset": 140, "endOffset": 160}, {"referenceID": 2, "context": ", 2013) and phrases (Mikolov et al., 2013).", "startOffset": 20, "endOffset": 42}, {"referenceID": 2, "context": "With English Gigaword Corpus, we use the skip-gram model as implemented in word2vec1 (Mikolov et al., 2013) to induce embeddings.", "startOffset": 85, "endOffset": 107}, {"referenceID": 0, "context": "1) or the embeddings published by Collobert et al. (2011);4 (iii) the bag-of-words (BOW) https://code.", "startOffset": 34, "endOffset": 58}, {"referenceID": 1, "context": "Most work on embeddings has focused on word forms with a few exceptions, notably embeddings for stems and morphemes (Luong et al., 2013) and for phrases (Mikolov et al.", "startOffset": 116, "endOffset": 136}, {"referenceID": 2, "context": ", 2013) and for phrases (Mikolov et al., 2013).", "startOffset": 24, "endOffset": 46}], "year": 2013, "abstractText": "Deep learning embeddings have been successfully used for many natural language processing problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like morphemes and phrases. In this paper, we argue that learning embeddings for discontinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings.", "creator": "LaTeX with hyperref package"}}}