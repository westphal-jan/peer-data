{"id": "1312.5457", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Codebook based Audio Feature Representation for Music Information Retrieval", "abstract": "Digital music has especially comic in brought web in further recent. Automated recommendation utilize tend essential work cellphones giving discover composer able me them another young to reach depends shows. When real-time annotations and user measure moreover itself quite (e. pts. special key talents) there computer should adequately on \\ emph {contain of} methods. Besides similar rifle context tools for classification they real-time, a key component own also approving is making \\ emph {using reference representation }.", "histories": [["v1", "Thu, 19 Dec 2013 09:40:03 GMT  (211kb,D)", "http://arxiv.org/abs/1312.5457v1", "Journal paper. Submitted to IEEE transactions on Audio, Speech and Language Processing. Submitted on Dec 18th, 2013"]], "COMMENTS": "Journal paper. Submitted to IEEE transactions on Audio, Speech and Language Processing. Submitted on Dec 18th, 2013", "reviews": [], "SUBJECTS": "cs.IR cs.LG cs.MM", "authors": ["yonatan vaizman", "brian mcfee", "gert lanckriet"], "accepted": false, "id": "1312.5457"}, "pdf": {"name": "1312.5457.pdf", "metadata": {"source": "CRF", "title": "Codebook based Audio Feature Representation for Music Information Retrieval", "authors": ["Yonatan Vaizman", "Brian McFee", "Gert Lanckriet"], "emails": [], "sections": [{"heading": null, "text": "Good representations should capture informative musical patterns in the audio signal of songs. These representations should be concise, to enable efficient (low storage, easy indexing, fast search) management of huge music repositories, and should also be easy and fast to compute, to enable real-time interaction with a user supplying new songs to the system.\nBefore designing new audio features, we explore the usage of traditional local features, while adding a stage of encoding with a pre-computed codebook and a stage of pooling to get compact vectorial representations. We experiment with different encoding methods, namely the LASSO, vector quantization (VQ) and cosine similarity (CS). We evaluate the representations\u2019 quality in two music information retrieval applications: queryby-tag and query-by-example. Our results show that concise representations can be used for successful performance in both applications. We recommend using top-\u03c4 VQ encoding, which consistently performs well in both applications, and requires much less computation time than the LASSO.\nIndex Terms\u2014Music recommendation, audio representations, vector quantization, sparse coding.\nI. INTRODUCTION\nIN the recent decades digital music has become more ac-cessible and music sources have become very prolific. Web servers for music exploration and recommendation contain huge repositories of music items. Hence, clever automation is required for generating good recommendation and enabling efficient search of music. Two of the most useful interfaces for a user to get music recommendations are query-by-tag (QbT) and query-by-example (QbE). In query-by-tag the system ranks music items according to relevance to a tag word (ultimately to a free-text search query), describing some semantic meaning of the desired music (emotional content, specific instruments, musical style, etc.). In query-by-example the system ranks music items according to relevance or similarity to a given music example (a song that the user already likes). This can be done in the form of an online radio, automatically creating a playlist for the user, and ultimately with an interface that enables the user to upload music clips unknown to the system and find similar music. For both these search interfaces\nY. Vaizman and G. Lanckriet are with the Department of Electrical and Computer Engineering, University of California, San Diego.\nB. McFee is with the Center for Jazz Studies and LabROSA, Columbia University, New York.\nsome annotation or indexing of the songs in the repository is required. Pre-existing meta-data of the music (e.g. title, artist, lyrics, genre, instruments) is one source of such annotations and it can assist in retrieving desired items. Such information can be given with the media files as they are added to the repository (title, track duration, artist etc.), collected by experts (as done by the Music Genome Project1, where music experts were hired to listen to the songs and manually annotate them with relevant tags) or gathered by users of the web-service (Last.FM2).\nWhereas the \u201cexpert\u201d method to gather meta-data is labor intensive and costly, the \u201cuser\u201d method is less reliable and prone to inconsistent descriptions. Another source of useful knowledge is past records of user preferences, either of specific users, for personalization purposes, or of crowds of users, for general recommendation. Such an approach is called collaborative filtering, and it leverages co-preference of many users. For instance if many users like both artist A and artist B, and a new user likes to listen to artist A, the system will recommend artist B to that user. The collaborative filtering approach is only applicable when there is a large history of usage (plays) by many users. A recommendation system that relies solely on this approach will never suggest songs by new, unfamiliar artists, even though they are potentially suitable for some users.\nSince the availability of useful meta-data and user preference data is limited, large scale music repositories must rely on content based systems to perform efficient automatic music recommendation. Such systems should be \u201cmusically intelligent\u201d, meaning they should analyze digital audio signals of music and extract meaningful information. In the past decade much research was dedicated to constructing content based systems for music information retrieval (MIR) tasks such as music classification (to artist, genre, etc. [1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]). The focus was mostly on machine learning algorithms that utilize basic audio features to perform the task.\nIn this work we use simple retrieval systems and focus on comparing different audio features and representation. Before we examine new low-level audio features, we try to make the most of traditional features, based on mel scaled spectra of short time frames. We add a stage of encoding these frame feature vectors with a pre-computed codebook, and a stage of pooling the coded frames (temporal integration) to get a summarized fixed-dimension representation of a whole song.\n1http://www.pandora.com/mgp.shtml 2http://www.last.fm/\nar X\niv :1\n31 2.\n54 57\nv1 [\ncs .I\nR ]\n1 9\nD ec\n2 01\n3\nThe encoding detects informative local patterns and represents the frames at a higher level. The pooling stage makes the representation of a whole song compact and easy to work with (low storage, fast computation and communication), and it creates a representation that has the same dimension for all songs, regardless of their durations. We show how the same concise representation can be useful for both query-by-tag and query-by-example retrieval."}, {"heading": "A. Related work", "text": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]). Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]). While MFCC is considered as capturing timbral qualities of the sound, the CQT and chroma features are designed for harmonic properties of the music (or melodic, if using patches of multiple frames). Hamel et al. suggested using principal component analysis (PCA) whitening of mel scaled spectral features as alternative to MFCC [32]. Some works combine heterogeneous acoustic analysis features, such as zero crossing rate, spectral flatness, estimated tempo, amplitude modulation features etc. ([1], [8], [26], [33], [34]).\nLow-level audio features are typically extracted from short time frames of the musical clip, then some temporal integration is done. Sometimes an early integration is performed, by taking statistics (mean, variance, covariance, etc.) of the feature vector over longer segments, or over the entire song (e.g. [7], [16]). Sometimes late integration is performed, for instance: each short segment is classified and for the entire musical clip a majority vote is taken over the multiple segments\u2019 declared labels (e.g. [9]). Such late integration systems require more computing time, since the classification operation should be done to every frame, instead of to a single vector representation per song.\nAnother approach for temporal integration is getting a compact representation of a song by generative modeling. In this approach the whole song is described using a parametric structure that models how the song\u2019s feature vector time series was generated. Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]). Although these models have been shown very useful and some of them are also time-efficient to work with, the representation of a song using a statistical model is less convenient than a vectorial representation. The former requires retrieval systems that fit specifically to the generative model while the later can be processed by many generic machine learning tools. Computing similarity between two songs is not straight forward using a generative model (although there are some ways to handle it, like the probability product kernel ([18], [36], [38])), whereas for vectorial representation there\nare many efficient generic ways to compute similarity between two vectors of the same dimension. In [36] the song level generative model (multivariate autoregressive mixture) was actually used to create a kind of vectorial representation for a song by describing the frequency response of the generative model\u2019s dynamic system, but still, being a mixture model, the resulted representation was a bag of four vectors, and not a single vectorial representation.\nEncoding of low-level features using a pre-calculated codebook was examined for audio and music. Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level. Sparse representations were also applied directly to time domain audio signals, with either predetermined kernel functions (Gammatone) or with a trained codebook ([6], [40]). As alternative to the heavy computational cost of solving optimization criteria (like the LASSO) greedy algorithms like matching pursuit have also been applied ([6], [39], [40]).\nHeterogeneous and multi-layer systems have been proposed. The bag of systems approach combined various generative models as codewords ([22]). Multi-modal signals (audio and image) were combined in a single framework ([41]). Even the codebook training scheme, which was usually unsupervised, was combined with supervision to get a boosted representation for a specific application ([12], [41]). Deep belief networks were used in [9], also combining unsupervised network weights training with supervised fine tuning. In [13] audio features were processed in two layers of encoding with codebooks.\nSeveral works invested in comparing different encoding schemes for audio, music and image. Nam et al. examined different variations of low-level audio processing, and compared different encoding methods (VQ, the LASSO and sparse restricted Boltzman machine) for music annotation and retrieval with the CAL500 dataset [21]. Yeh et al. reported to find superiority of sparsity-enforced dictionary learning and L1-regularized encoding over regular VQ for genre classification. In [42] Coates and Ng examined the usage of different combinations of dictionary training algorithms and encoding algorithms to better explain the successful performance of sparse coding in previous works. They concluded that the dictionary training stage has less of an impact on the final performance than the encoding stage and that the main merit of sparse coding may be due to its nonlinearity, which can be achieved also with simpler encoders that apply some nonlinear soft thresholding. In [43] Coates et al. examined various parameters of early feature extraction for images (such as the density of the extracted patches) and showed that when properly extracting features, one can use simple and efficient algorithms (k-means clustering and single layer neural network) and achieve image classification performance as high as other, more complex systems."}, {"heading": "B. Our contribution", "text": "In this work we look for compact audio content representations that will be powerful for two different MIR applications:\nquery-by-tag and query-by-example. We perform a large scale evaluation, using the CAL10k and Last.FM datasets. We assess the effect of various design choices in the \u201clow-level-feature, encoding, pooling\u201d scheme, and eventually recommend a representation \u201crecipe\u201d (based on vector quantization) that is efficient to compute, and has consistent high performance in both MIR applications.\nThe remainder of the paper is organized as follows: in ?? we describe the audio representations that we compare, including the low-level audio features, the encoding methods and pooling. In ?? we describe the MIR tasks that we evaluate, query-by-tag and query-by-example retrieval. In ?? we specify the dataset used, the data processing stages and the experiments performed. In ?? we describe our results, followed by conclusions in ??."}, {"heading": "II. SONG REPRESENTATION", "text": "We examine the encoding-pooling scheme to get a compact representation for each song (or musical piece). The scheme is comprised of three stages:\n1) Short time frame features: each song is processed to a time series of low-level feature vectors, X \u2208 Rd\u00d7T (T time frames, from each a d dimensional feature vector is extracted). 2) Encoding: each feature vector xt \u2208 Rd is then encoded to a code vector ct \u2208 Rk, using a pre-calculated dictionary D \u2208 Rd\u00d7k, a codebook of k \u201cbasis vectors\u201d of dimension d. We get the encoded song C \u2208 Rk\u00d7T . 3) Pooling: the coded frame vectors are pooled together to a single compact vector v \u2208 Rk.\nThis approach is also known as the bag of features (BoF) approach: where features are collected from different patches of an object (small two-dimensional patches of an image, short time frames of a song, etc.) to form a variable-size set of detected features. The pooling stage enables us to have a unified dimension to the representations of all songs, regardless of the songs\u2019 durations. A common way to pool the low-level frame vectors together is to take some statistic of them, typically their mean. For a monotonic, short song, such a statistic may be a good representative of the properties of the song.\nHowever, a typical song is prone to changes in the spectral content, and a simple statistic pooling function over the lowlevel feature frames may not represent it well. For that reason the second stage (encoding) was introduced. In a coded vector, each entry encodes the presence/absence/prominence of a specific pattern in that frame. The pre-trained codebook holds codewords (patterns) that are supposed to roughly represent the variety of prominent patterns in songs. The use of sparsity in the encoding (having only few basis vectors active in each frame), promotes selecting codewords that represent typical whole sound patterns (comprised of possibly many frequency bands). The pooling of these coded vectors is meaningful: using mean pooling gives a histogram representation, stating the frequency of occurrence of each sound pattern, while using max-abs (maximum absolute value) pooling gives more of an indication representation \u2014 for each sound pattern, did\nit appear anytime in the song, and in what strength. For some encoding methods it is appropriate to take absolute value and treat negative values far from zero as strong values. In our experiments we used three encoding systems, the LASSO ([44]), vector quantization (VQ), and cosine similarity (CS) (all explained later), and applied both mean and max-abs pooling functions to the coded vectors."}, {"heading": "A. Low-level audio features", "text": "In this work we use spectral features that are commonly assumed to capture timbral qualities. Since we are not interested in melodic or harmonic information, but rather general sound similarity, or semantic representation, we assume timbral features to be appropriate here (an assumption that is worth examination). Our low-level features are based on mel frequency spectra (MFS), which are calculated by computing the short time Fourier transform (STFT), summarizing the spread of energy along mel scaled frequency bins, and compressing the values with logarithm. Mel frequency cepstral coefficients (MFCCs [30]) are the result of further processing MFS, using discrete cosine transform (DCT), in order to both create uncorrelated features from the correlated frequency bins, and reduce the feature dimension. In addition to the traditional DCT we alternatively process the MFS with another method for decorrelating, based on principal component analysis (PCA). Processing details are specified in ??."}, {"heading": "B. Encoding with the LASSO", "text": "The least absolute shrinkage and selection operator (the LASSO) was suggested as an optimization criterion for linear regression that selects only few of the regression coefficients to have effective magnitude, while the rest of the coefficients are either shrunk or even nullified [44]. The LASSO does that by balancing between the regression error (squared error) and an L1 norm penalty over the regression coefficients, which typically generates sparse coefficients. Usage of the LASSO\u2019s regression coefficients as a representation of the input is often referred to as \u201csparse coding\u201d. In our formulation, the encoding of a feature vector xt using the LASSO criterion is:\nct = argmin c\u2208Rk\n1 2 \u2016 xt \u2212Dc \u201622 +\u03bb \u2016 c \u20161.\nIntuitively it seems that such a sparse linear combination might represent separation of the music signal to meaningful components (e.g. separate instruments). However, this is not necessarily the case since the LASSO allows coefficients to be negative and the subtraction of codewords from the linear combination has little physical interpretability when describing how musical sounds are generated. To solve the LASSO optimization problem we used the alternating direction method of multipliers (ADMM) algorithm. The general algorithm, and a specific version for the LASSO are detailed in [45]. The \u03bb parameter can be interpreted as a sparsity parameter: the larger it is, the more weight will be dedicated to the L1 penalty, and the resulted code will typically be more sparse."}, {"heading": "C. Encoding with vector quantization (VQ)", "text": "In vector quantization (VQ) a continuous multi-dimensional vector space is quantized to a discrete finite set of bins, each having its own representative vector. The training of a VQ codebook is essentially a clustering that describes the distribution of vectors in the space. During encoding, each frame\u2019s feature vector xt is quantized to the closest codeword in the codebook, meaning it is encoded as ct, a sparse binary vector with just a single \u201con\u201d value, in the index of the codeword that has smallest distance to it (we use Euclidean distance). It is also possible to use a softer version of VQ, selecting for each feature vector xt the \u03c4 nearest neighbors among the k codewords, creating a code vector ct with \u03c4 \u201con\u201d values and k \u2212 \u03c4 \u201coff\u201d values:\nct(j) = 1\n\u03c4 1 [Dj \u2208 \u03c4 -nearest neighbors of xt] ,\nj \u2208 {1, 2, . . . , k}.\nSuch a soft version can be more stable: whenever a feature vector has multiple codewords in similar vicinity (quantization ambiguity), the hard threshold of selecting just one codeword will result in distorted, noise-sensitive code, while using top\u03c4 quantization will be more robust. This version also adds flexibility and richness to the representation: instead of having k possible codes for every frame, we get ( k \u03c4 ) possible codes. Of course, if \u03c4 is too large, we may end up with codes that are trivial \u2014 all the songs will have similar representations and all the distinguishing information will be lost. The sparsity parameter \u03c4 here is actually a density parameter, with larger values causing denser codes. By adjusting \u03c4 we can directly control the level of sparsity of the code, unlike in the LASSO, where the effect of adjusting the parameter \u03bb is indirect, and depends on the data. The values in the coded vectors are binary (either 0 or 1\u03c4 ). Using max-abs pooling on these code vectors will result in binary final representations. Using mean pooling results in a codeword histogram representation with richer values. We only use mean pooling for VQ in our experiments.\nIn [29] it was shown that for codeword histogram representations (VQ encoding and mean pooling), it was beneficial to take the square root of every entry, consequently transforming the song representation vectors from points on a simplex\n( k\u2211 j=1 |vj | = 1) to points on the positive orthant of a sphere\n( k\u2211 j=1 |vj |2 = 1). The authors called it PPK transformation, since a dot product between two transformed vectors is equivalent to the probability product kernel (PPK) with power 0.5 on the original codeword histograms [38]. We also experiment with the PPK-transformed versions of the codeword histogram representations."}, {"heading": "D. Encoding with cosine similarity (CS)", "text": "VQ encoding is simple and fast to compute (unlike the LASSO, whose solving algorithms, like ADMM, are iterative and slow). However, it involves a hard threshold (even when \u03c4 > 1) that possibly distorts the data and misses important\ninformation. When VQ is used for communication and reconstruction of signal it is necessary to use this thresholding in order to have a low bit rate (transmitting just the index of the closest codeword).\nHowever, in our case of encoding songs for retrieval we have other requirements. As an alternative to VQ we experiment with another form of encoding, where each dictionary codeword is being used as a linear filter over the feature vectors: instead of calculating the distance between each feature vector and each codeword (as done in VQ), we calculate a similarity between them \u2014 the (normalized) dot product between the feature vector and the codeword: \u3008xt,Dj\u3009\u2016xt\u20162 . Since the codewords we trained are forced to have unit L2 norm, this is equivalent to the cosine similarity (CS). The codewords act as pattern matching filters, where frames with close patterns get higher response.\nFor the CS encoding we use the same codebooks that are used for VQ. For each frame, selecting the closest (by Euclidean distance) codeword is equivalent to selecting the codeword with largest CS with the frame. So CS can serve as a softer version of VQ. The L2 normalization of each frame (to get CS instead of unnormalized dot product) is important to avoid having a bias towards frames that have large magnitudes, and can dominate over all other frames in the pooling stage. In our preliminary experiments we verified that this normalization is indeed significantly beneficial to the performance. The CS regards only to the \u201cshape\u201d of the pattern but not to its magnitude and gives a fair \u201cvote\u201d also to frames with low power. Unlike the unnormalized dot product the response values of CS are limited to the range [\u22121, 1], and are easier to interpret and to further process.\nIn the last stage of the encoding we introduce non-linearity in the form of the shrinkage function y(x) = sign(x) \u2217 max(|x|\u2212\u03b8, 0) (values with magnitude less than \u03b8 are nullified and larger magnitude values remain with linear, but shrinked, response). Using \u03b8 = 0 maintains the linear responses of the filters, while \u03b8 > 0 introduces sparsity, leaving only the stronger responses. Such a nonlinear function is sometimes called \u201csoft thresholding\u201d and was used in various works before to compete with the successful \u201csparse coding\u201d (the LASSO) in a fast feed-forward way ([42])."}, {"heading": "E. Dictionary training", "text": "The training of the dictionaries (codebooks) is performed with the online learning algorithm for sparse coding presented by Mairal et al. ([46]). As an initialization stage we apply online k-means to a stream of training d-dimensional feature vectors, to cluster them to an initial codebook of k codewords. This initial dictionary is then given to the online algorithm, which alternates between encoding a small batch of new instances using the current dictionary, and updating the dictionary using the newly encoded instances. In each iteration the updated codewords are normalized to have unit L2 norm."}, {"heading": "III. MIR TASKS", "text": "We examine the usage of the various song representations for two basic MIR applications, with the hope to find stable\nrepresentations that are consistently successful in both tasks. We use simple, linear machine learning methods, seeing as our goal here is finding useful song representations, rather than finding sophisticated new learning algorithms."}, {"heading": "A. Query-by-tag (QbT)", "text": "We use L2-regularized logistic regression as a tag model. For each semantic tag we use the positively and negatively labeled training instances (k-dimensional song vectors) to train a tag model. Then for each song in the test set and for each tag we use the trained tag model to estimate the probability of the tag being relevant to the song (the likelihood of the songvector given the tag model). For each song, the vector of taglikelihoods is then normalized to be a categorical probability over the tags, also known as the semantic multinomial (SMN) representation of a song.\nRetrieval: For each tag the songs in the test set are ranked according to their SMN value relevant to the tag. Per-tag area under curve (AUC), precision at top-10 (P@10) and average precision (AP) are calculated as done in [15], [20]. These pertag scores are averages over the tags to get a general score (mean (over tags) AP is abbreviated MAP)."}, {"heading": "B. Query-by-example (QbE)", "text": "Given a query song, whose audio content is represented as vector q \u2208 Rk, our query-by-example system calculates its distance dist(q, r) from each repository song r \u2208 Rk and the recommendation retrieval result is the repository songs ranked in increasing order of distance from the query song. The Euclidean distance is a possible simple distance measure between songs\u2019 representations. However, it grants equal weight to each of the vectors\u2019 dimensions, and it is possible that there are dimensions that carry most of the relevant information, while other dimensions carry just noise. For that reason, we use a more general metric as distance measure, the Mahalanobis\ndistance: dist(q, r) = \u221a\n(q \u2212 r)TW (q \u2212 r), when W \u2208 Rk\u00d7k is the parameter matrix for the metric (W has to be a positive semidefinite matrix for a valid metric).\nIn [47] McFee et al. presented a framework for using a metric for query-by-example recommendation systems, and a learning algorithm \u2014 metric learning to rank (MLR) \u2014 for training the metric parameter matrix W to optimize various ranking performance measures. In [29] the authors further demonstrated the usage of MLR for music recommendation, and the usage of collaborative filtering data to train the metric, and to test the ranking quality. Here we followed the same scheme: collaborative filtering data are used to define artistartist similarity (or relevance), and song-song binary relevance labels. MLR is then applied to training data to learn a metric W . The learnt metric is tested on a test set. Further details are provided in ??. Same as for query-by-tag, we apply the same scheme to different audio content representations and compare the performance of query-by-example."}, {"heading": "IV. EXPERIMENTAL SETUP", "text": ""}, {"heading": "A. Data", "text": "In this work we use the CAL10k dataset [48]. This dataset contains 10, 865 full-length songs from over 4, 500 different artists, ranging over 18 musical genres. Throughout the paper we use the convenient term \u201csong\u201d to refer to a music item/piece (even though many of the items in CAL10k are pieces of classical music and would commonly not be called songs). It also contains semantic tags harvested from the Pandora website, including 475 acoustic tags and 153 genre (and sub-genre) tags. These tag annotations were done by humans, musical experts. The songs in CAL10k are weakly labeled in the sense that if a song doesn\u2019t have a certain tag, it doesn\u2019t necessarily mean that the tag is not relevant for the song, but for evaluation we assume missing song-tag associations to be negative labels. We filter the tags to include only the 581 tags that have at least 30 songs associated with them.\nFor the query-by-example task we work with the intersection of artists from CAL10k and the Last.FM collaborative filter data, collected by Celma ([49] chapter 3). As done in [29] we calculate the artist-artist similarity based on Jaccard index ([50]) and the binary song-song relevance metric, which is used as the target metric to be emulated by MLR.\nFor the dictionary training we use external data \u2014 \u223c 3500 audio files of songs/clips by \u223c 700 artists that do not appear in CAL10k. These clips were harvested from various interfaces on the web and include both popular and classical music. This is unlike the sampling from within the experimental set, as was done in [21], which might cause over-fitting."}, {"heading": "B. Processing", "text": "Audio files are averaged to single channel (in case they are given in stereo) and re-sampled at 22, 050Hz. Feature extraction is done over half-overlapping short frames of 2, 048 samples (a feature vector once every 1, 024 samples, which is once every \u223c 46msec). The magnitude spectrum (magnitude DFT) of each frame is summarized into 34 Mel-scaled frequency bins, and log value is saved to produce initial MFS features. To get the MFCC features a further step of discrete cosine transform (DCT) is done and 13 coefficients are saved. The 1st and 2nd instantaneous derivatives are augmented to produce MFCC\u2206 (d = 39) and MFS\u2206 (d = 102) feature vectors. The next step is to standardize the features so that each dimension would have zero mean and unit variance (according to estimated statistics). In order to have comparable audio features, we reduce the dimension of the MFS\u2206 to 39 dimensions using a PCA projection matrix (pre-estimated from the dictionary training data) to get MFS\u2206PC features.\nThe dictionary training set is used to both estimate statistics over the raw features, and to train the dictionary: first the mean vector and vector of standard deviation of each dimension are calculated over the pool of low-level feature vectors (either MFCC\u2206 or MFS\u2206). Then all the vectors are standardized (by subtracting the mean vector and dividing each dimension by the appropriate standard deviation) to get the pool of standardized feature vectors. For the MFS\u2206 another stage of\nPCA projection is done (using a projection matrix that was also estimated from the same training set). From each training audio file a segment of 20 sec is randomly selected, processed and its feature vectors are added to a pool of vectors (resulting in 1.5 million vectors), which are scrambled to a random order and fed to the online dictionary training algorithm.\nFor each codebook size k the LASSO codebook is trained with \u03bb = 1 (this codebook is later used for the LASSO encoding with various values of \u03bb) and the VQ codebook is trained with \u03c4 = 1 (this codebook is later used for VQ encoding with various valued of \u03c4 and for CS encoding).\nFor training the logistic regression model of a tag, an internal cross validation is done over different combinations of parameters (weight of regularization, weight of false negative error, weight of false positive error), each of which could take values of [0.1, 1, 10, 100]. This cross validation is done using only the training set, and the parameter set selected is the one that optimizes the AUC. After selecting the best parameter set for a tag, the entire training set is used to train the tag model with these parameters.\nThe query-by-tag evaluation is done with 5-fold cross validation. For each fold no artist appears in both the train set and the test set. The performance scores that were averaged over tags in each fold, are then averaged over the five folds. The query-by-example evaluation is done with 10 splits of the data in the same manner as done in [29]. We use the AUC rank measure to define the MLR loss between two rankings (marked as \u2206(y\u2217, y) in [47]). For each split we train W over the train set with multiple values of the slack trade off parameter C (10\u22122, 10\u22121, . . . , 108) and for each value test the trained metric on the validation set. The metric that results in highest AUC measure on the validation set is then chosen and tested on the test set. We report the AUC results on the test set, averaged over the 10 splits.\nFor QbE PCA decorrelation and dimensionality reduction is performed on the data: in each split the PCA matrix is estimated from the train set and the song representation vectors (of train, validation and test set) are projected to a predetermined lower dimension (so the trained matrices W are in fact not (k \u00d7 k) but smaller). In [29] the heuristic was to reduce to the estimated effective dimensionality \u2014 meaning to project to the first PCs covering 0.95 of the covariance (as estimated from the train set). However, in our experiments we noticed that reducing to the effective dimensionality caused deterioration of performance when the effective dimensionality decreased, while keeping a fixed reduction-dimension kept stable or improving performance. So keeping 0.95 of the covariance is not the best practice. Instead, for every k we fix the dimension to reduce to (across different encoders and encoding parameters).\nWhen testing each of the 10 splits, each song in the query set (either the validation set or the test set) is used as a query to retrieve relevant songs from the train set \u2014 the train songs are ranked according to the trained metric and the ranking for the query song is evaluated (AUC score). The average over query songs is then taken."}, {"heading": "C. Experiments", "text": "Each experiment regards to a different type of audio-content representation. We experiment with different combinations of the following parameters: \u2022 low-level features: MFCC\u2206 or MFS\u2206PC, \u2022 codebook size k \u2208 {128, 256, 512, 1024}, \u2022 encoding method: the LASSO, VQ or CS, \u2022 encoding parameters:\n\u2013 the LASSO: \u03bb \u2208 {0.01, 0.1, 0.5, 1, 2, 10, 100}, \u2013 VQ: \u03c4 \u2208 {1, 2, 4, 8, 16, 32}, \u2013 CS: \u03b8 \u2208 {0, 0.1, 0.2 . . . , 0.9},\n\u2022 pooling function: either mean or max-abs, \u2022 VQ: either using PPK-transformation or not."}, {"heading": "V. RESULTS", "text": ""}, {"heading": "A. query-by-tag results", "text": "First, for comparison, we present baseline results: chance level scores are the result of scrambling the order of song labels and performing the query-by-tag task, while using the representations with MFS\u2206PC, k = 1024 and VQ encoding with \u03c4 = 8. Then, to control for the encoding methods in our scheme, we perform the experiments without the encoding stage (instead of encoding the feature vectors with a codebook, leaving them as low-level features and pooling them) for both the MFCC\u2206 and MFS\u2206PC low-level features. Finally, as alternative to the codebook based systems, we evaluate the HEM-GMM system, which is the suitable candidate from the generative models framework, being computation efficient and assuming bag of features (like our current codebook systems). We process the data as was done in [20] for HEM-GMM, using our current 5-fold partition. ?? presents these baselines.\nIn ???? we show plots for the P@10 rank measure (this measure is the more practical objective, since in real recommendation systems, the user typically only looks at the top of the ranked results). Graphical results for the other performance measures are provided in the supplementary material. In some plots error bars are added: the error bars represent the standard deviation of the score (over the five folds for query-by-tag, and over the 10 splits for query-by-example).\nLow-level features: ?? shows the query-by-tag results for comparison between the two low-level features: MFCC\u2206 and MFS\u2206PC. Each point in the graphs compares the performance using MFCC\u2206 (x-axis) to the performance using MFS\u2206PC (y-axis), when all the other parameters (k, encoding method, encoding parameter, pooling method) are the same. Multiple points with the same shape represent experiments with the same encoder and pooling, but different encoding parameter.\nThe main diagonal line (y = x) is added to emphasize the fact that in the majority of the experiments performance with MFS\u2206PC was better than MFCC\u2206. Statistical tests (paired two-tailed t-test between two arrays of \u223c 2900 per-fold-pertag scores) support the advantage of MFS\u2206PC: most comparisons show statistically significant advantage to MFS\u2206PC (all except six points on the plots. P-value well below 0.05), and only one point (for k = 128 with VQ and \u03c4 = 32) has significant advantage to MFCC\u2206.\nWhile it is expected that the data-driven decorrelation (PCA) performs better than the predetermined projection (DCT), it is interesting to see that the difference is not so dramatic (points are close to the main diagonal) \u2014 MFCC managed to achieve performance close to the data-trained method. Other than the advantage of training on music data, another explanation to the higher performance of MFS\u2206PC can be the effect of first taking a local dynamic structure (concatenating the \u201cdeltas\u201d to the features) and only then decorrelating the features-\u2206 version (as we did here for MFS\u2206PC).\nThese results also demonstrate the advantage of using some encoding over low-level features before pooling them: all these performances (for both MFCC\u2206 and MFS\u2206PC) are better than the baseline results with no encoding (??. The highest of the \u201cno encoding\u201d baselines is also added as reference line in the plots). We can also notice the improvement with increasing codebook sizes (the different subplots). Similar results are seen for the other performance measures (AUC, MAP) \u2014 graphs shown in the supplementary material. The remainder of the results focus on the MFS\u2206PC low-level features.\nThe LASSO encoding: ?? shows the query-by-tag results (P@10) with MFS\u2206PC features for the LASSO encoding. The LASSO is sensitive to the value of its sparsity parameter \u03bb. When \u03bb is too high (in this case \u03bb = 10, 100), the resulted code is too sparse and loses important information, causing deteriorated performance. When \u03bb is too small (0.01) the code is too dense. This doesn\u2019t effect much when using mean pooling, but harms performance for the max-abs pooling representation. Similar results are seen for AUC and MAP measures\n(supplementary material). There seems to be an advantage to using max-abs pooling over mean pooling, however this advantage is not apparent for the smaller codebook size (128) and not in the AUC performance.\nVQ encoding: ?? shows the results (P@10) with MFS\u2206PC features and for VQ encoding. These results depict a clear effect of the VQ density parameter \u03c4 : \u201csoftening\u201d the VQ by quantizing each frame to more than one codeword significantly improves the performance. There is an optimal peak for \u03c4 , typically at 8 or 16 \u2014 increasing \u03c4 further causes performance to deteriorate, especially with a small codebook. The effect of the PPK-transformation is small and inconsistent. These\ntrends are consistent also for AUC and MAP (supplementary material).\nCosine similarity encoding: The query-by-tag results (P@10) for CS encoding (??) demonstrate the effect of adjusting the sparsity parameter \u03b8 (the \u201cknee\u201d of the shrinkage function): the optimal value is not too small and not too large. This is more dramatically seen for the mean pooling: there is a significant advantage in adding some non-linearity (having \u03b8 > 0), and at the other end having the code too sparse (\u03b8 too large) causes a drastic reduction in performance. For maxabs pooling, generally performance was not as good as mean pooling, having a sharp peak at \u03b8 = 0.8.\n?? presents the three QbT measures for selected representations, and the generative model alternative (HEM-GMM) as baseline. For each measure, the leading system is marked in bold, and the other systems are compared to it by 2- tailed paired t-test between the two arrays of per-fold-per-tag scores (N = 2905). The p-values of the t-tests are written in parenthesis."}, {"heading": "B. Query-by-example results", "text": "Next, we examine the performance of the query-by-example task (AUC) for the various song representations. ?????? show the query-by-example results for the three encoding methods. The PCA dimension chosen for each k is written in parenthesis in the title of each subplot. We also experimented with higher PCA dimensions and got similar results (the performance values were slightly higher, but the comparisons among encoders or encoding parameters was the same. See supplementary material).\nAs expected all encoding methods show improvement with increasing codebook size k. For the LASSO (??), again, we see the sensitivity to \u03bb (this time mean pooling is also harmed by too low \u03bb). Unlike for query-by-tag, here there is no strong advantage of max-abs pooling over mean pooling.\nFor VQ (??) we get partial reproduction of the trends found by McFee et al. in [29]: improved performance with increasing codebook size and significant improvement when adding the PPK-transformation. However, since in [29] the representations were reduced to the estimated effective dimensionality, which was a decreasing function of \u03c4 , there was a different effect of \u03c4 than what we find here (where we fix the reduced dimension for a given k): where in [29], for k = 512, 1024 with PPK increasing \u03c4 seemed to hurt the performance, we show that when PCA is done to a fixed dimension, increasing \u03c4 can maintain a stable performance, and even slightly improve the performance (for both with/without PPK), peaking at around \u03c4 = 8.\nFor CS (??), unlike in query-by-tag, there is a significant advantage to max-abs pooling over mean pooling. We again see the damage of over-sparsity: max-abs pooling performance stays stable but decreases after \u03b8 = 0.8 and mean pooling performance increases with \u03b8 but after peaking early it decreases and stays low.\nBoth CS and the LASSO are sensitive to the selection of their sparsity parameter: selecting an inappropriate value results in poor performance of the representation. In practical\nsystems such methods require cross validation to select the appropriate parameter value. VQ, on the other hand, is less sensitive to its density parameter \u03c4 . This is perhaps due to the fact that \u03c4 directly controls the level of sparsity in the VQ code, whereas for CS and the LASSO the level of sparsity is regularized indirectly. VQ is a stable representation method that can be easily controlled and adjusted with little risk of harming its informative power. VQ consistently achieves highest query-by-example performance (this is also consistent when reducing to a higher PCA dimension. Supplementary material).\nComparing both MIR tasks ?? shows the performance of the same representations in both query-by-tag and query-by-example. The best parameter values from each encoder are presented. The best QbT per-\nformance is registered for the LASSO (with max-abs pooling) for k = 1024, where VQ is slightly behind. However, for QbE VQ consistently leads over the other encoding methods, and the same for QbT with k < 1024. VQ is a stable and reliable method for both MIR tasks."}, {"heading": "C. Encoding runtime", "text": "As we are searching for practical features and representations for large scale systems, computation resources should also be of consideration when selecting a preferred representation method. We compare the runtime complexity of the three encoding methods, from feature vector xt \u2208 Rd to code vector ct \u2208 Rk: \u2022 CS involves multiplying xt by matrix D (O(dk)), com-\nputing \u2016xt\u20162 (O(d)) and applying shrinkage to the cosine similarities (O(k)), resulting in total complexity of TCS = O(dk).\n\u2022 VQ involves the same matrix-vector multiplication and norm calculation to compute the Euclidean distances. Then O(c\u03c4,kk) is required to find the \u03c4 closest codewords (c\u03c4,k is a small number that depends logarithmically on either \u03c4 or k, depending on the algorithm used), resulting in total of TVQ = O((d+ c\u03c4,k)k). \u2022 The ADMM solution for the LASSO is an iterative procedure. Each iterations includes a multiplication of a (k \u00d7 k) matrix by a k dimensional vector (O(k2)), a shrinkage function (O(k)) and vector additions (O(k)), resulting in complexity of O(k2) per iteration. On top of that, there is O(dk) for once multiplying the dictionary matrix by the feature vector, and there are M iterations, until the procedure converges to -tolerance, so the complexity for the LASSO encoding becomes TLASSO = O(M k 2 + dk).\nCS is the lightest encoding method and VQ adds a bit more computation. Recently linear convergence rate was shown for solving the LASSO with ADMM [51], implying that M = O(log 1 ), but even with fast convergence ADMM is still heavier than VQ. This theoretical analysis is verified in empirical runtime measurements, presented in ??. We average over the same 50 songs, and use the same computer (PC\nlaptop) with single CPU core. The runtime tests fit a linear dependency on k for CS and for VQ (with slope depending on \u03c4 ) and a super-linear dependency on k for the LASSO.\nUsing the LASSO (with max-abs pooling) achieves highest performance scores in the query-by-tag task, but the price of runtime requirements is high, and it can be much reduced, by using VQ, while giving up only slightly on query-by-tag performance, and gaining better performance for query-byexample."}, {"heading": "VI. CONCLUSION", "text": "We show an advantage to using PCA decorrelation of MFS\u2206 features over MFCC. The difference is statistically significant, but small, showing that also the data-agnostic DCT manages to compress music data well. Increasing the codebook size (up to 1024) results in improved performance for all the encoding methods. The level of sparsity of the code has an effect (possibly indirect) on performance for all encoding methods, where optimality is achieved with codes that are not too sparse and not too dense. While the LASSO and CS can suffer sharp decrease of performance when adjusting their sparsity parameters, VQ is more robust, having smooth and controlled change in performance when adjusting its density parameter \u03c4 .\nWe find that a simple, efficient encoding method (VQ) can successfully compete with the more sophisticated method (the LASSO), achieving comparable, and even better performance, with much less computing resources. Using top-\u03c4 VQ with PPK transformation consistently achieves high performance (almost always beating other methods) in both query-by-tag and query-by-example. It is fast and easy to compute, and it is easily adjustable with its parameter \u03c4 . We recommend this representation method as a recipe to be applied to other lowlevel features, to represent various aspects of musical audio. The resulting representations are concise, easy to work with and powerful for music recommendation in large repositories."}], "references": [{"title": "Musical genre classification of audio signals", "author": ["G. Tzanetakis", "P. Cook"], "venue": "IEEE Transactions on speech and audio processing, vol. 10, no. 5, pp. 293\u2013302, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "An investigation of feature models for music genre classification using the support vector classifier", "author": ["A. Meng", "J. Shawe-Taylor"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2005, pp. 604\u2013609.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "A study on music genre classification based on universal acoustic models", "author": ["J. Reed", "C. Lee"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2006, pp. 89\u201394.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Classifying music audio with timbral and chroma features", "author": ["D.P. Ellis"], "venue": "ISMIR 2007: Proceedings of the 8th International Conference on Music Information Retrieval: September 23-27, 2007, Vienna, Austria. Austrian Computer Society, 2007, pp. 339\u2013340.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Shift-invariant sparse coding for audio classification.", "author": ["R. Grosse", "R. Raina", "H. Kwong", "A.Y. Ng"], "venue": "Conference on Uncertainty in AI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "on the use of sparse time-relative auditory codes for music.", "author": ["P.A. Manzagol", "T. Bertin-Mahieux", "D. Eck"], "venue": "International Society for Music Information Retrieval conference (ISMIR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Multiple-instance learning for music information retrieval", "author": ["M. Mandel", "D. Ellis"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2008, pp. 577\u2013582.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Temporal integration for audio classification with application to musical instrument classification", "author": ["C.J.S. Essid", "G. Richard"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, no. 1, pp. 174\u2013186, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning features from music audio with deep belief networks.", "author": ["P. Hamel", "D. Eck"], "venue": "International Society for Music Information Retrieval conference (ISMIR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Unsupervised learning of sparse features for scalable audio classification", "author": ["M. Henaff", "K. Jarrett", "K. Kavukcuoglu", "Y. LeCun"], "venue": "International Society for Music Information Retrieval conference (ISMIR), 2011, pp. 681\u2013686.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised learning of local features for music classification", "author": ["J. Wulfing", "M. Riedmiller"], "venue": "International Society for Music Information Retrieval conference (ISMIR), 2012, pp. 139\u2013144.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Supervised dictionary learning for music genre classification", "author": ["C.C.M. Yeh", "Y.H. Yang"], "venue": "ICMR, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Dual-layer bag-of-frames model for music genre classification", "author": ["C.-C.M. Yeh", "L. Su", "Y.-H. Yang"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Support vector machine active learning for music retrieval", "author": ["M. Mandel", "G. Poliner", "D. Ellis"], "venue": "Multimedia systems, vol. 12, no. 1, pp. 3\u201313, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Semantic annotation and retrieval of music and sound effects", "author": ["D. Turnbull", "L. Barrington", "D. Torres", "Lanckriet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatic generation of social tags for music recommendation", "author": ["D. Eck", "P. Lamere", "T. Bertin-Mahieux", "S. Green"], "venue": "Advances in Neural Information Processing Systems, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Autotagger: a model for predicting social tags from acoustic features on large music databases", "author": ["T. Bertin-Mahieux", "D. Eck", "F. Maillet", "P. Lamere"], "venue": "Journal of New Music Research, vol. 37, no. 2, pp. 115\u2013135, June 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Combining feature kernels for semantic music retrieval", "author": ["L. Barrington", "M. Yazdani", "D. Turnbull", "G. Lanckriet"], "venue": "2008, pp. 723\u2013728.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Using regression to combine data sources for semantic music discovery", "author": ["B. Tomasik", "J. Kim", "M. Ladlow", "M. Augat", "D. Tingle", "R. Wicentowski", "D. Turnbull"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2009, pp. 405\u2013410.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Time Series Models for Semantic Music Annotation", "author": ["E. Coviello", "A. Chan", "G. Lanckriet"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 19, no. 5, pp. 1343\u20131359, July 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning sparse feature representations for music annotation and retrieval", "author": ["J. Nam", "J. Herrera", "M. Slaney", "J. Smith"], "venue": "International Society for Music Information Retrieval conference (ISMIR), 2012, pp. 565\u2013570.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "A bag of systems representation for music auto-tagging", "author": ["K. Ellis", "E. Coviello", "A. Chan", "G. Lanckriet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Content-based retrieval of music and audio", "author": ["J.T. Foote"], "venue": "Voice, Video, and Data Communications. International Society for Optics and Photonics, 1997, pp. 138\u2013147.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "A music similarity function based on signal analysis", "author": ["B. Logan", "A. Salomon"], "venue": "IEEE International Conference on Multimedia and Expo, 2001, pp. 745\u2013748.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Music similarity measures: What\u2019s the use?", "author": ["J. Aucouturier", "F. Pachet"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Learning a metric for music similarity", "author": ["M. Slaney", "K. Weinberger", "W. White"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2008, pp. 313\u2013318.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Content-based musical similarity computation using the hierarchical Dirichlet process", "author": ["M. Hoffman", "D. Blei", "P. Cook"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2008, pp. 349\u2013354.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "An efficient hybrid music recommender system using an incrementally trainable probabilistic generative model", "author": ["K. Yoshii", "M. Goto", "K. Komatani", "T. Ogata", "H.G. Okuno"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 16, no. 2, pp. 435\u2013447, 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning content similarity for music recommendation", "author": ["B. McFee", "L. Barrington", "Lanckriet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 8, pp. 2207\u20132218, October 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Mel frequency cepstral coefficients for music modeling", "author": ["B. Logan"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), vol. 28, 2000.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2000}, {"title": "Large-scale cover song recognition using the 2d fourier transform magnitude", "author": ["T. Bertin-Mahieux", "D.P. Ellis"], "venue": "Proceedings of the 13th International Conference on Music Information Retrieval (ISMIR 2012), 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Temporal pooling and multiscale learning for automatic annotation and ranking of music audio.", "author": ["P. Hamel", "S. Lemieux", "Y. Bengio", "D. Eck"], "venue": "International Society for Music Information Retrieval conference (ISMIR),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Features for audio and music classification", "author": ["M. McKinney", "J. Breebaart"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2003, pp. 151 \u2013158.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Probabilistic combination of features for music classification", "author": ["A. Flexer", "F. Gouyon", "S. Dixon", "G. Widmer"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2006, pp. 111\u2013114.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "A largescale evaluation of acoustic and subjective music-similarity measures", "author": ["A. Berenzweig", "B. Logan", "D.P.W. Ellis", "B. Whitman"], "venue": "Computer Music Journal, vol. 28, no. 2, pp. 63\u201376, 2004.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "Multivariate Autoregressive Mixture Models for Music Autotagging", "author": ["E. Coviello", "Y. Vaizman", "A.B. Chan", "G. Lanckriet"], "venue": "13th International Society for Music Information Retrieval Conference (ISMIR 2012), 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "The variational hierarchical EM algorithm for clustering hidden Markov models", "author": ["E. Coviello", "A.B. Chan", "G. Lanckriet"], "venue": "Neural Information Processing Systems (NIPS 2012), 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Probability product kernels", "author": ["T. Jebara", "R. Kondor", "A. Howard"], "venue": "The Journal of Machine Learning Research, vol. 5, pp. 819\u2013844, 2004.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2004}, {"title": "Sound retrieval and ranking using sparse auditory representations", "author": ["R. Lyon", "M. Rehn", "S. Bengio", "T.C. Walters", "G. Chechik"], "venue": "Neural Computation, vol. 22, no. 9.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 0}, {"title": "Efficient auditory coding", "author": ["E.C. Smith", "M.S. Lewicki"], "venue": "Nature, vol. 439, pp. 978\u2013982, 2006.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Complex events detection using data-driven concepts", "author": ["Y. Yang", "M. Shah"], "venue": "ECCV, 2012, pp. 722\u2013735.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "International Conference on Machine Learning (ICML), 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "Journal of Machine Learning (JMLR), vol. 15, p. 48109, 2010.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), vol. 58, no. 1, pp. 267\u2013288, 1996.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1996}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2010.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 19\u201360, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Metric learning to rank", "author": ["B. McFee", "G. Lanckriet"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML\u201910), June 2010.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploring automatic music annotation with \u201cacoustically-objectiv\u201d tags", "author": ["D. Tingle", "Y.E. Kim", "D. Turnbull"], "venue": "Proc. MIR, New York, NY, USA, 2010.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Etude comparative de la distribution florale dans une portion des alpes et des jura", "author": ["P. Jaccard"], "venue": "Bulletin del la Societe Vaudoise des Sciences Naturelles, vol. 37, pp. 547\u2013579, 1901.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1901}, {"title": "On the linear convergence of the alternating direction method of multipliers", "author": ["M. Hong", "Z.-Q. Luo"], "venue": "arXiv preprint arXiv:1208.3922, 2012. PLACE PHOTO HERE  Yonatan Vaizman Biography text here.  PLACE PHOTO HERE Brian McFee Biography text here. PLACE PHOTO HERE  Gert Lanckriet Biography text here.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "[1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "[1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]).", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "[1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]).", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "[1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 28, "context": "[1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]).", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 100, "endOffset": 103}, {"referenceID": 11, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 123, "endOffset": 127}, {"referenceID": 18, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 129, "endOffset": 133}, {"referenceID": 22, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 24, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 140, "endOffset": 144}, {"referenceID": 26, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 146, "endOffset": 150}, {"referenceID": 29, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 200, "endOffset": 204}, {"referenceID": 11, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 205, "endOffset": 209}, {"referenceID": 15, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 211, "endOffset": 215}, {"referenceID": 16, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 217, "endOffset": 221}, {"referenceID": 3, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 363, "endOffset": 366}, {"referenceID": 17, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 368, "endOffset": 372}, {"referenceID": 30, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 374, "endOffset": 378}, {"referenceID": 31, "context": "suggested using principal component analysis (PCA) whitening of mel scaled spectral features as alternative to MFCC [32].", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "([1], [8], [26], [33], [34]).", "startOffset": 1, "endOffset": 4}, {"referenceID": 7, "context": "([1], [8], [26], [33], [34]).", "startOffset": 6, "endOffset": 9}, {"referenceID": 25, "context": "([1], [8], [26], [33], [34]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 32, "context": "([1], [8], [26], [33], [34]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "([1], [8], [26], [33], [34]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "[7], [16]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[7], [16]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "[9]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 47, "endOffset": 50}, {"referenceID": 13, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 26, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 88, "endOffset": 92}, {"referenceID": 33, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 94, "endOffset": 98}, {"referenceID": 34, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 113, "endOffset": 117}, {"referenceID": 1, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 125, "endOffset": 128}, {"referenceID": 7, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 130, "endOffset": 133}, {"referenceID": 35, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 141, "endOffset": 145}, {"referenceID": 2, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 153, "endOffset": 156}, {"referenceID": 7, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 158, "endOffset": 161}, {"referenceID": 36, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 163, "endOffset": 167}, {"referenceID": 26, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 175, "endOffset": 179}, {"referenceID": 17, "context": "Computing similarity between two songs is not straight forward using a generative model (although there are some ways to handle it, like the probability product kernel ([18], [36], [38])), whereas for vectorial representation there are many efficient generic ways to compute similarity between two vectors of the same dimension.", "startOffset": 169, "endOffset": 173}, {"referenceID": 35, "context": "Computing similarity between two songs is not straight forward using a generative model (although there are some ways to handle it, like the probability product kernel ([18], [36], [38])), whereas for vectorial representation there are many efficient generic ways to compute similarity between two vectors of the same dimension.", "startOffset": 175, "endOffset": 179}, {"referenceID": 37, "context": "Computing similarity between two songs is not straight forward using a generative model (although there are some ways to handle it, like the probability product kernel ([18], [36], [38])), whereas for vectorial representation there are many efficient generic ways to compute similarity between two vectors of the same dimension.", "startOffset": 181, "endOffset": 185}, {"referenceID": 35, "context": "In [36] the song level generative model (multivariate autoregressive mixture) was actually used to create a kind of vectorial representation for a song by describing the frequency response of the generative model\u2019s dynamic system, but still, being a mixture model, the resulted representation was a bag of four vectors, and not a single vectorial representation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 19, "endOffset": 23}, {"referenceID": 2, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 52, "endOffset": 55}, {"referenceID": 28, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 57, "endOffset": 61}, {"referenceID": 38, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 63, "endOffset": 67}, {"referenceID": 4, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 133, "endOffset": 137}, {"referenceID": 5, "context": "Sparse representations were also applied directly to time domain audio signals, with either predetermined kernel functions (Gammatone) or with a trained codebook ([6], [40]).", "startOffset": 163, "endOffset": 166}, {"referenceID": 39, "context": "Sparse representations were also applied directly to time domain audio signals, with either predetermined kernel functions (Gammatone) or with a trained codebook ([6], [40]).", "startOffset": 168, "endOffset": 172}, {"referenceID": 5, "context": "As alternative to the heavy computational cost of solving optimization criteria (like the LASSO) greedy algorithms like matching pursuit have also been applied ([6], [39], [40]).", "startOffset": 161, "endOffset": 164}, {"referenceID": 38, "context": "As alternative to the heavy computational cost of solving optimization criteria (like the LASSO) greedy algorithms like matching pursuit have also been applied ([6], [39], [40]).", "startOffset": 166, "endOffset": 170}, {"referenceID": 39, "context": "As alternative to the heavy computational cost of solving optimization criteria (like the LASSO) greedy algorithms like matching pursuit have also been applied ([6], [39], [40]).", "startOffset": 172, "endOffset": 176}, {"referenceID": 21, "context": "The bag of systems approach combined various generative models as codewords ([22]).", "startOffset": 77, "endOffset": 81}, {"referenceID": 40, "context": "Multi-modal signals (audio and image) were combined in a single framework ([41]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "Even the codebook training scheme, which was usually unsupervised, was combined with supervision to get a boosted representation for a specific application ([12], [41]).", "startOffset": 157, "endOffset": 161}, {"referenceID": 40, "context": "Even the codebook training scheme, which was usually unsupervised, was combined with supervision to get a boosted representation for a specific application ([12], [41]).", "startOffset": 163, "endOffset": 167}, {"referenceID": 8, "context": "Deep belief networks were used in [9], also combining unsupervised network weights training with supervised fine tuning.", "startOffset": 34, "endOffset": 37}, {"referenceID": 12, "context": "In [13] audio features were processed in two layers of encoding with codebooks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "examined different variations of low-level audio processing, and compared different encoding methods (VQ, the LASSO and sparse restricted Boltzman machine) for music annotation and retrieval with the CAL500 dataset [21].", "startOffset": 215, "endOffset": 219}, {"referenceID": 41, "context": "In [42] Coates and Ng examined the usage of different combinations of dictionary training algorithms and encoding algorithms to better explain the successful performance of sparse coding in previous works.", "startOffset": 3, "endOffset": 7}, {"referenceID": 42, "context": "In [43] Coates et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 43, "context": "In our experiments we used three encoding systems, the LASSO ([44]), vector quantization (VQ), and cosine similarity (CS) (all explained later), and applied both mean and max-abs pooling functions to the coded vectors.", "startOffset": 62, "endOffset": 66}, {"referenceID": 29, "context": "Mel frequency cepstral coefficients (MFCCs [30]) are the result of further processing MFS, using discrete cosine transform (DCT), in order to both create uncorrelated features from the correlated frequency bins, and reduce the feature dimension.", "startOffset": 43, "endOffset": 47}, {"referenceID": 43, "context": "The least absolute shrinkage and selection operator (the LASSO) was suggested as an optimization criterion for linear regression that selects only few of the regression coefficients to have effective magnitude, while the rest of the coefficients are either shrunk or even nullified [44].", "startOffset": 282, "endOffset": 286}, {"referenceID": 44, "context": "The general algorithm, and a specific version for the LASSO are detailed in [45].", "startOffset": 76, "endOffset": 80}, {"referenceID": 28, "context": "In [29] it was shown that for codeword histogram representations (VQ encoding and mean pooling), it was beneficial to take the square root of every entry, consequently transforming the song representation vectors from points on a simplex", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "5 on the original codeword histograms [38].", "startOffset": 38, "endOffset": 42}, {"referenceID": 41, "context": "LASSO) in a fast feed-forward way ([42]).", "startOffset": 35, "endOffset": 39}, {"referenceID": 45, "context": "([46]).", "startOffset": 1, "endOffset": 5}, {"referenceID": 14, "context": "Per-tag area under curve (AUC), precision at top-10 (P@10) and average precision (AP) are calculated as done in [15], [20].", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "Per-tag area under curve (AUC), precision at top-10 (P@10) and average precision (AP) are calculated as done in [15], [20].", "startOffset": 118, "endOffset": 122}, {"referenceID": 46, "context": "In [47] McFee et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "In [29] the authors further demonstrated the usage of MLR for music recommendation, and the usage of collaborative filtering data to train the metric, and to test the ranking quality.", "startOffset": 3, "endOffset": 7}, {"referenceID": 47, "context": "In this work we use the CAL10k dataset [48].", "startOffset": 39, "endOffset": 43}, {"referenceID": 28, "context": "As done in [29] we calculate the artist-artist similarity based on Jaccard index ([50]) and the binary song-song relevance metric, which is used as the target metric to be emulated by MLR.", "startOffset": 11, "endOffset": 15}, {"referenceID": 48, "context": "As done in [29] we calculate the artist-artist similarity based on Jaccard index ([50]) and the binary song-song relevance metric, which is used as the target metric to be emulated by MLR.", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "This is unlike the sampling from within the experimental set, as was done in [21], which might cause over-fitting.", "startOffset": 77, "endOffset": 81}, {"referenceID": 28, "context": "The query-by-example evaluation is done with 10 splits of the data in the same manner as done in [29].", "startOffset": 97, "endOffset": 101}, {"referenceID": 46, "context": "We use the AUC rank measure to define the MLR loss between two rankings (marked as \u2206(y\u2217, y) in [47]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "In [29] the heuristic was to reduce to the estimated effective dimensionality \u2014 meaning to project to the first PCs covering 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "We process the data as was done in [20] for HEM-GMM, using our current 5-fold partition.", "startOffset": 35, "endOffset": 39}, {"referenceID": 28, "context": "in [29]: improved performance with increasing codebook size and significant improvement when adding the", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "However, since in [29] the representations were reduced to the estimated effective dimensionality, which was a decreasing function of \u03c4 , there was a different effect of \u03c4 than what we find here (where we fix the reduced dimension for a given k): where in [29], for k = 512, 1024 with PPK increasing \u03c4 seemed to hurt the performance, we show that when PCA is done to a fixed dimension, increasing \u03c4 can maintain a stable performance, and even slightly improve the performance (for both with/without PPK), peaking at around \u03c4 = 8.", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "However, since in [29] the representations were reduced to the estimated effective dimensionality, which was a decreasing function of \u03c4 , there was a different effect of \u03c4 than what we find here (where we fix the reduced dimension for a given k): where in [29], for k = 512, 1024 with PPK increasing \u03c4 seemed to hurt the performance, we show that when PCA is done to a fixed dimension, increasing \u03c4 can maintain a stable performance, and even slightly improve the performance (for both with/without PPK), peaking at around \u03c4 = 8.", "startOffset": 256, "endOffset": 260}, {"referenceID": 49, "context": "Recently linear convergence rate was shown for solving the LASSO with ADMM [51], implying that M = O(log 1 ), but even with fast convergence ADMM is still heavier than VQ.", "startOffset": 75, "endOffset": 79}], "year": 2013, "abstractText": "Digital music has become prolific in the web in recent decades. Automated recommendation systems are essential for users to discover music they love and for artists to reach appropriate audience. When manual annotations and user preference data is lacking (e.g. for new artists) these systems must rely on content based methods. Besides powerful machine learning tools for classification and retrieval, a key component for successful recommendation is the audio content representation. Good representations should capture informative musical patterns in the audio signal of songs. These representations should be concise, to enable efficient (low storage, easy indexing, fast search) management of huge music repositories, and should also be easy and fast to compute, to enable real-time interaction with a user supplying new songs to the system. Before designing new audio features, we explore the usage of traditional local features, while adding a stage of encoding with a pre-computed codebook and a stage of pooling to get compact vectorial representations. We experiment with different encoding methods, namely the LASSO, vector quantization (VQ) and cosine similarity (CS). We evaluate the representations\u2019 quality in two music information retrieval applications: queryby-tag and query-by-example. Our results show that concise representations can be used for successful performance in both applications. We recommend using top-\u03c4 VQ encoding, which consistently performs well in both applications, and requires much less computation time than the LASSO.", "creator": "LaTeX with hyperref package"}}}