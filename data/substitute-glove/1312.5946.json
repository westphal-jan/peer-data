{"id": "1312.5946", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Adaptive Seeding for Gaussian Mixture Models", "abstract": "In means picture, better apply perfect and bowlers focuses to supersoldiers since Expectation - Maximization algorithm (EM) another sequential Gaussian mixture wheel. We taken new intussusception materials related on in there - refer $ K $ - means + + replication into over Gonzalez minimax. These alternatives where made wide other similar full second-order extensive up space tools, no have this present replacement took Gaussian mixture smaller them sustain instead the right choice large hyperparameters. In our timely we know our specific with a commonly variety formula_12 convolution formulas, an approach operates thursday agglomerative dichotomous clustering, along, developed, plain adaption included the Gonzalez diagram. Our nevertheless measured that algorithms pioneer from $ K $ - means + + traders though although methods.", "histories": [["v1", "Fri, 20 Dec 2013 14:08:48 GMT  (202kb,D)", "http://arxiv.org/abs/1312.5946v1", null], ["v2", "Mon, 1 Aug 2016 08:33:13 GMT  (203kb,D)", "http://arxiv.org/abs/1312.5946v2", "An improved version of this paper (Adaptive Seeding for Gaussian Mixture Models) has been accepted for publication in the Proceedings of the 20th Pacific Asia Conference on Knowledge Discovery and Data Mining (PAKDD) 2016 and is available via the DOI 10.1007/978-3-319-31750-2_24"], ["v3", "Tue, 30 May 2017 07:44:37 GMT  (652kb,D)", "http://arxiv.org/abs/1312.5946v3", "This is a preprint of a paper that has been accepted for publication in the Proceedings of the 20th Pacific Asia Conference on Knowledge Discovery and Data Mining (PAKDD) 2016. The final publication is available at link.springer.com (this http URL24)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["johannes bl\\\"omer", "kathrin bujna"], "accepted": false, "id": "1312.5946"}, "pdf": {"name": "1312.5946.pdf", "metadata": {"source": "CRF", "title": "Simple Methods for Initializing the EM Algorithm for Gaussian Mixture Models", "authors": ["Johannes Bl\u00f6mer", "Kathrin Bujna"], "emails": ["bloemer@uni-paderborn.de", "kathrin.bujna@uni-paderborn.de"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nGaussian mixture modelling is an important task in the field of clustering and pattern recognition. There are various approaches to estimate the parameters of a mixture such as graphical methods, moment matching, Bayesian approaches, and the method of maximum likelihood estimation (MLE). A widely used approach for the latter problem is the ExpectationMaximization (EM) algorithm [1]. Starting with some initial mixture model, the EM algorithm alternates between computing a lower bound of the log-likelihood and improving the current model with respect to this lower bound. The algorithm converges to a certain stationary point of the likelihood function. Unfortunately, the likelihood function is generally nonconvex, possessing many stationary points, including small local maxima, and even worse, local minima and saddle points (cf. [2], [3]). Moreover, the convergence of the EM algorithm to either type of point highly depends on the chosen initial model.\nA. K-MLE for Gaussian Mixture Models\nA Gaussian mixture model (GMM) is a parametric probability density function represented as a weighted sum of Gaussian densities. We denote the parameter vector defining a mixture of K Gaussians over RD by \u03b8 = {(wk, \u00b5k,\u03a3k)}k=1,...,K , where wk \u2208 R is the mixing weight ( \u2211K k=1 wk = 1), \u00b5k \u2208 RD is the mean and \u03a3k \u2208 RD\u00d7D is the covariance of the k-th mixture component. The probability density function (pdf) of a mixture given by \u03b8 is defined by N (x|\u03b8) = \u2211K k=1 wkN (x|\u00b5k,\u03a3k) , where N (\u00b7|\u00b5k,\u03a3k) represents the D-variate Gaussian pdf\nwith mean \u00b5k and covariance \u03a3k. Given a data set X = {x1, . . . , xN} \u2282 RD, the K-Maximum-Likelihood-Estimation (K-MLE) problem is to find a mixture of K Gaussians \u03b8 such that the log-likelihood L(X, \u03b8) = \u2211N n=1 log(N (xn|\u03b8)) is maximized. For K = 1 this problem has a closed-form solution (cf. [4]). For K > 1 there exist iterative algorithms to approximate the K-MLE. These algorithms cannot create a solution, but rather maintain a solution and improve it in each round. Thus, they need to be fed with some initial model. The most well-known of these algorithms is the EM algorithm, on which we will focus on this paper. We expect that the results that we present in this paper also apply to adaptions like the SEM algorithm."}, {"heading": "B. Related Work", "text": "The simplest and most widely used random initialization schemes are the so-called rndEM and emEM method (cf. [5]\u2013 [8]), which we will also refer to as \u201crandom initializations\u201d. These methods generate several candidates by drawing means uniformly at random from the input set and then using some (data dependent) approximation of the covariances and weights. In case of emEM, the candidates are improved by few iterations of the EM algorithm. Finally, these methods choose the candidate with the largest likelihood. Other popular approaches are based on hierarchical agglomerative clustering (HAC). For instance, in [6]\u2013[8] HAC (with Ward\u2019s criterion, average linkage, and a model-based distance measure, respectively) is used to obtain the means of the initial model. Since HAC is generally very slow, it is usually executed on a random sample of the input. The size of this sample has to be chosen appropriately and obviously depends on the size of the smallest cluster. Except for [7], [6] and [8] report that this method is generally outperformed by random initializations. Another approach that makes use of HAC is the multistage approach proposed in [7]. It aims at finding the best local modes of the data set in a reduced m\u2217 dimensional space and apply HAC only on these modes. However, the algorithm is rather time-consuming and the choice of m\u2217 is crucial. In [6] a density based approach is proposed. It initializes the means by choosing points which have a higher concentration of neighbors and then estimates the covariances by making use of a truncated normal distribution. In [9] a greedy algorithm ar X iv :1 31 2. 59 46 v1 [\ncs .L\nG ]\n2 0\nD ec\n2 01\n3\nis presented which does not need an initialization but instead constructs a whole sequence of mixture models with k = 1, . . . ,K components. Given a model \u03b8k with k components, the algorithm constructs various new candidates with k + 1 components and chooses the one with the highest likelihood. Each candidate is constructed by adding a new component to \u03b8k and improving it by executing the EM algorithm. Due to its run time, this algorithm is only feasible when the k-MLE problem has to be approximated for several values of k. There are further initialization methods which can be found, e.g., in [7], [10]\u2013[12]."}, {"heading": "C. Our Contribution", "text": "Clearly, there is no way to determine \u201cthe\u201d best initialization algorithm that outperforms all algorithms on all instances. The performance of an initialization will depend on the data and the allowed computational cost. Nonetheless, the initializations presented so far (except the simple random initializations) face mainly two problems: First, they are rather complex and time consuming. Second, the choice of hyperparameters is crucial for the outcome of the respective algorithm.\nIn this paper, we present simple and fast initialization methods inspired by methods for hard clustering problems. This closes the gap between the simple uniform methods and those methods that have been specifically designed for Gaussian mixture models. The initialization methods presented in this paper can be divided into two groups. First, we consider methods that directly apply known initialization methods for different K-clustering problems. Namely, we use the K-means++ algorithm [13], the Gonzalez algorithm, and a simple uniform initialization. There are already some practical applications of the K-means++ algorithm for the initialization of GMMs (e.g., in [14] these GMMs are used for speech recognition). Second, we discuss and define generalizations of these algorithms for Gaussian mixture models. Some initial research in this area has been done in [15], where a modification of the Gonzalez algorithm is considered. We extend this work by considering a further modification of the Gonzalez algorithm. Furthermore, we present a new algorithm based on the K-means++ algorithm. In our evaluation, we focus on the EM algorithm as the main learning algorithm and execute the initializations on a large number of classes of generated instances that share certain characteristics as well as several real world data sets. We compare the aforementioned algorithms with the algorithm presented in [15] and an algorithm based on HAC. Our results indicate that our adaption of the K-means++ algorithm and a method that makes use of the plain K-means++ algorithm perform best.\nII. INITIALIZATION ALGORITHMS"}, {"heading": "A. Finding Initial Means", "text": "We consider the following three known initialization methods for K-clustering algorithms. The first method is the socalled K-means++ initialization [13], which is intended to be used for the K-means clustering algorithm. In each round, K-means++ samples a new mean from the given data set\nMeans2GMM(X , {\u00b51, . . . , \u00b5k}) Algorithm 1 1: Derive partition {C1, . . . , CK} of X by assigning each\npoint xi \u2208 X to its closest mean. 2: Set \u00b5k = 1/|Ck| \u2211 x\u2208Ck x.\n3: Set \u03a3k = 1/|Ck| \u2211 x\u2208Ck(x\u2212 \u00b5k)(x\u2212 \u00b5k)\nT . 4: Set the weights to the fraction of points assigned to the\nrespective cluster, i.e. wk = |Ck|/|X|. 5: If \u03a3k is not positive definite, use a spherical covariance\nmatrix, i.e., let \u03a3k = 1/D\u00b7|Ck| \u2211N n=1 \u2016xn \u2212 \u00b5k\u20162 \u00b7 ID ,\nwhere ID denotes the D-dimensional identity matrix. 6: If \u03a3k is still not positive definite, let \u03a3k = ID.\nFig. 1. Construction of a Gaussian mixture model from K means.\nwith probability proportional to its K-means cost (with respect to the means chosen so far). In expectation, the costs are in O(log(K)\u00b7opt), where opt denotes the optimal K-means costs. This initialization is particularly interesting since the K-means algorithm can be seen as a special case of the EM (cf. [4, p.443]). The second method is the Gonzalez algorithm [16], which is a 2-approximation algorithm for the discrete radius K-clustering problem. Similar to K-means++, it determines the means iteratively. As the next mean it always chooses the data point with the largest euclidean distance from the already chosen means, i.e., with the largest K-means cost. Hence, K-means++ can be seen as a probabilistic variant of the Gonzalez algorithm. The third initialization method is to sample K means uniformly at random from the given data set (cf. rndEM in Sec. I-B)."}, {"heading": "B. Directly Using Initial Means", "text": "The following three methods directly use the aforementionied initializations. Gonzalez executes the Gonzales algorithm, Kmeans++ executes the K-means++ algorithm [13], and Uniform draws K means uniformly at random from the given data set. Then, they all modify and extend the resulting set of K means to a complete Gaussian mixture model by a method that we refer to as Means2GMM (cf. Fig. 1) . Given K means, Means2GMM reduces the MLE problem to K independent 1-MLE problems. That is, it partitions the data according to the K-means clustering induced by the means and then solves the 1-MLE problems with respect to each of the clusters (cf. [4]). However, it might happen that a cluster is too small. In such cases, Means2GMM determines an appropriate approximation (i.e., a spherical covariance matrix)."}, {"heading": "C. Transferring the Ideas to Gaussian Mixture Models", "text": "Implementing the ideas behind the Gonzalez and Kmeans++ algorithm for Gaussian mixture models raises two problems. First, we have to choose an appropriate density function according to which a point from the data set will be chosen (either by drawing according to the density or by determining the point with highest density). Second, we also have to determine how the current Gaussian mixture model is updated.\nTwo natural candidates for the density function are the inverse pdf of the Gaussian mixture and the negative loglikelihood. Unfortunately, the former is unsuited due to the exponential behavior of Gaussians, while the latter may take negative values. Thus, we use an approximation of the negative log-likelihood. To define this, consider the minimum negative log-likelihood of a point x taken over the individual components\nmink { \u2212 log ( (2\u03c0)D/2|\u03a3k|1/2 ) + 12 (x\u2212 \u00b5k)\u03a3 \u22121 k (x\u2212 \u00b5k) } . (1) Note that the logarithm of the normalization term of the Gaussian density may take negative values, while the Mahalanobis density (x\u2212\u00b5k)\u03a3\u22121k (x\u2212\u00b5k) is always non-negative. Assuming that the logarithm of the normalization term is positive, it would practically function as a shift towards a uniform distribution. The effect of this shift is that the probability for choosing outliers is reduced. Given the Gaussian mixture model \u03b8 = {(wk, \u00b5k,\u03a3k)}k=1,...,K , we denote the minimum Mahalanobis density of point x \u2208 Rd by\nm1(x|\u03b8) = min k=1,...,K (x\u2212 \u00b5k)T\u03a3\u22121k (x\u2212 \u00b5k) .\nTo keep the effect incurred by the (unfeasible) normalization term, we replace it by a constant. That is, for some fixed S \u2286 X , we define for all x \u2208 S and \u03b1 \u2208 [0, 1]\nm\u03b1,S(x|\u03b8) = \u03b1 m1(x|\u03b8)\u2211 y\u2208Sm1(y|\u03b8) + (1\u2212 \u03b1) 1 |S| .\nThis density corresponds to an experiment where x is drawn uniformly at random from S \u2286 X with probability 1 \u2212 \u03b1. Our adaptions draw a points according to density or choose the point with maximum density, respectively. Note that arg maxx\u2208Sm1(x|\u03b8) = arg maxx\u2208Sm\u03b1,S(x|\u03b8). Furthermore, points with maximum density are more likely to be outliers. To avoid the problem of only choosing such outliers, our adaption of Gonzalez first chooses a uniform subsample S of the input data set X and then only samples points from S. Given a sample point x \u2208 X , we have to determine how the complete Gaussian mixture model is updated1. To this end, our adaptions of the K-means++ and the Gonzalez algorithm simply apply Means2GMM to the set of points including the means of the current model \u03b8 and the newly drawn point x \u2208 X .\nOur adaptions, Adaptive and GonzalezForGMM, are shown in Fig. 2 and Fig. 3, respectively. In our experiments, we compare them with another adaption of the Gonzalez algorithm presented in [15], which we refer to as KwedlosGonzalez (cf. Fig. 4 and Fig. 5). In contrast to our adaption, it chooses the covariances randomly (in relation to the covariance of the overall data set X) and draws the weights uniformly at random.\n1 Our first attempts consisted in just adding a new component to the old mixture model, for instance, by choosing the covariances as in [17] (i.e., a spherical covariance depending on the minimum distances between means). However, this approach did not work out at all.\nAdaptive ( X \u2282 RD,K \u2208 N, \u03b1 \u2208 [0, 1] ) 1: compute the optimal 1-solution \u03b81 2: for For i = 2, . . . ,K do 3: Draw p from X with probability m\u03b1,S(x|\u03b8k\u22121) 4: Obtain \u03b8k by calling Means2GMM with all means from \u03b8k\u22121 and the point p as input.\n5: end for\n( )\n(\n)\n( )\nFig. 5. Generation of a random covariance matrix according to [15]."}, {"heading": "III. EXPERIMENTS", "text": "Besides the presented algorithms, we evaluated a method using HAC with average linkage cost. That is, a method that executes HAC on a uniform sample of size s \u00b7 |X| of the input set X and then used Means2GMM. For this approach and for the algorithms presented in Sec. II-C, we only tested a few, reasonable values for hyperparameters, i.e., s = 0.1 and \u03b1 \u2208 {1, 0.5}.\nTo get comparable results, all methods were implemented in C++ (available at [18]). Since all presented methods are randomized, we executed each initialization with 30 different\nseeds2 to obtain reasonable results. As a stopping criterion for the EM algorithm we simply used a fixed number of rounds. Usually, a stopping criterion based on the relative change of the likelihood is used. However, this requires choosing a reasonable threshold, which is crucial not only for the resulting likelihood but also for the run time. After some initial experimental evaluations, we chose a run time of 50 rounds."}, {"heading": "A. Data", "text": "1) Real World Data Sets: We use four publicly available data sets. The first data set is provided by the ELKI project [19] and based on the Amsterdam Library of Object Images (ALOI) [20], which consists of 110 250 pictures of 1 000 small objects (taken under various conditions). We use a 27-dimensional feature vector set that is based on color histograms in HSV color space (cf. [21]). We created the second data set, denoted as Cities data set, from the data provided by the GeoNames geographical database (http://www.geonames.org/), which contains several features of 135 082 cities around the world with a population of at least 1000. Our data set is a projection of the geographic coordinates of the cities onto a plane. The third one is the Forest Covertype data set which contains 581 012 data points and is available from the UCI Machine Learning Library [22]. To get reasonable data, we ignored the class labels and 44 binary attributes and only used the remaining 10 coordinates. The fourth data set is the Spambase data set which is also provided by the UCI Machine Learning Library [22]. It contains 57 features of 4601 e-mails and a binary classification whether the e-mail was considered spam or not. Again, we ignored the latter.\n2) Generated Data Sets: We created different test sets which contain data sets with certain characteristics. That is, for each test set we created Gaussian mixture models that share certain predefined properties. Then, to obtain a data set, we drew data points according to a mixture and added a certain amount of noise points.\nOur generation of Gaussian mixture models focused on the following four properties. First, the components of a Gaussian mixture model can either be spherical or elliptical. We describe the eccentricity of covariance matrix \u03a3k by ek =\nmaxd \u03bbkd mind \u03bbkd , where \u03bb2kd denotes the d-th eigenvalue of \u03a3k. Second, components can have different sizes, in terms of the smallest eigenvalue of the corresponding covariance matrices. Third, the Gaussian components can overlap more or less. Following [17], we define the separation c\u03b8 of a Gaussian mixture model \u03b8 by\nc\u03b8 = min c\u2208R; l,k=1,...,K \u2016\u00b5l \u2212 \u00b5k\u2016\u221a max {trace(\u03a3l), trace(\u03a3k)} .\n2 Furthermore, since our EM implementation includes an randomized error handling, we executed the EM with 3 different seeds. The EM encounters a problem when the expected number of points generated by a component is too small or the computed covariances are not positive semi-definite. In the first case, we sample a new mean and covariance. In the latter case, we first try to mix the underdetermined covariance with the previous covariance, then simply keep the old covariance if necessary.\nFinally, when drawing points according to a mixture, its weights determine the expected number of points are drawn according to a component. To control these properties, we generated random Gaussian mixture models as follows. Initially, the means are drawn uniformly at random from a cube with a fixed side length. The values of the weights are determined deterministically by normalizing the K values in {2cw\u00b7i}i=1,...,K by their sum. By choosing cw appropriately, we obtain a certain sequence of weights. To obtain a random covariance matrix whose eigenvalues satisfy predetermined properties, we first determine eigenvalues \u03bb2k1, . . . , \u03bb 2 kD that match our restrictions. Then, we determine a random orthonormal matrix Q \u2208 RD\u00d7D (by QR-decomposition of a matrix with independently and uniformly drawn values) and set QT diag(\u03bb2k1, . . . , \u03bb 2 kD)Q to be the k-th covariance matrix. To ensure different sized components, we initialize the minimum eigenvalues of each component appropriately. To control the eccentricity, we set the maximum eigenvalues according to the minimum eigenvalues and the chosen eccentricity and fill the remaining eigenvalues uniformly at random. Finally, the means are scaled as to fit the predefined separation3.\nTable I gives an overview of the test sets that we considered. Each combination of values in a row describes a single test set, i.e., there are 12 Spherical, 9 Elliptical, and 9 EllipticalDifficult test sets. For each test set, we generated 30 data sets by choosing 30 Gaussian mixture models with K = 10 components randomly (as described before) and drawing 10 000 points according to each model. To obtain noisy data sets, we only drew 9 000 points according to the mixture model and added 1 000 uniform noise points by constructing a bounding box of all drawn points, elongating its side lengths by a factor 1.2 and then drawing points uniformly at random from the resized box. Our implementation available at [18] offers the opportunity to generate the data."}, {"heading": "B. Results", "text": "In our evaluation we focus on the quality of the initial and the final solution. The complete results can also be found at [18].\n3Thus, when keeping the remaining parameters fixed, we obtain basically (except the scaled means) the same mixture models and can evaluate the impact of different degrees of separation.\n1) Generated Data Sets: Remember, we have generated a huge amount of data. For every data set, we calculate the average value and the variance of the negative log-likelihood of the solutions obtained by an initialization method. In order to summarize this information with respect to a fixed test set, we proceed in the following two steps4: First, we create rankings of the initialization methods. That is, for each data set we obtain four different rankings based on initial and final solution and measured by average and variance of the negative log-likelihood. Second, we count the respective times a certain rank has been achieved. Since we can not depicit all the (12 + 9 + 9) \u00b7 2 \u00b7 8 = 480 summarized rankings in this paper, we made them available at [18] along with some further summaries. To give an overall impression of the results, we summed up the rankings obtained for data sets contained in the Spherical, Elliptical, and EllipticalDifficult test sets, respectively. Furthermore, we summed up the rankings obtained for data sets with separation 0.5, 1, and 2, respectively. The results are depicted in Tab. II, III, IV, and V.\nFrom our experiments, the following comments can be made: We constructed spherical as well as (difficult) elliptical data sets. Some authors pointed out that methods based on K-means meay encounter problems if the given clusters are skewed (e.g. [6]). However, this is not confirmed by our experiments since the overall impression is the same: The methods Kmeans++ and Adaptive perform best. Note that we also experimented with different choices of the parameter \u03b1 for Adaptive and found that it is rather insensitive, given \u03b1 \u2208 [0.5, 0.9]. Moreover, we investigated the impact of noise. As to be expected, given noisy data, the different versions of the Gonzalez algorithm lag far behind the other methods. This is most likely due to the fact that outliers are chosen as means. Furthermore, we tested different degrees of separation. For separation c\u03b8 \u2265 1 the methods Kmeans++ and Adaptive clearly outperform the other methods. Even for c\u03b8 = 0.5, where the situation is not quite as clear, these methods perform well. Generally, the method that uses HAC and Kwedlo\u2019s Gonzalez version, which uses basically random covariances, are outperformed by the other methods. In addition, from the rankings according to the variances we see that their results also vary the most. Last but not least, notice that an initial solution with high likelihood does not necessarily result in a final solution with high likelihood (compared to other initialization techniques).\n2) Real World Data Sets: The results for the real world data sets are depicted in Fig. 6. They confirm our impression regarding the adaptive initialization methods (i.e., Kmeans++ and Adaptive). Regarding the Aloi, Cities, and Spambase, these methods are among the top performers. An slight exception is their performance with respect to the Covertype data set, where Gonzalez performs surprisingly well.\n4 Note that simply averaging over the evaluations of different data sets is not meaningful since their (optimal) likelihoods may deviate significantly."}, {"heading": "C. Conclusion", "text": "If you need a fast and simple initialization method that does not need any training of hyperparameters then we suggest to use one of the following. Given a data set that presumably contains noise, the Adaptive initialization method with \u03b1 = 0.5 is a good choice. If there is no noise, then simply use the plain K-means++ initialization followed by Means2GMM."}], "references": [{"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B: Statistical Methodology, vol. 39, no. 1, pp. 1\u201338, 1977.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1977}, {"title": "On the convergence properties of the EM algorithm", "author": ["C. Wu"], "venue": "The Annals of Statistics, vol. 11, no. 1, pp. 95\u2013103, 1983.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1983}, {"title": "The EM Algorithm and Extensions (Wiley Series in Probability and Statistics), 2nd ed", "author": ["G.J. McLachlan", "T. Krishnan"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Choosing starting values for the EM algorithm for getting the highest likelihood in multivariate Gaussian mixture models", "author": ["C. Biernacki", "G. Celeux", "G. Govaert"], "venue": "Comput. Stat. Data Anal., vol. 41, no. 3-4, pp. 561\u2013575, Jan. 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Initializing the EM algorithm in Gaussian mixture models with an unknown number of components", "author": ["V. Melnykov", "I. Melnykov"], "venue": "Computational Statistics & Data Analysis, Nov. 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Initializing Partition-Optimization Algorithms", "author": ["R. Maitra"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics, vol. 6, no. 1, pp. 144\u2013157, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "An Experimental Comparison of Several Clustering and Initialization Methods", "author": ["M. Meil\u0103", "D. Heckerman"], "venue": "Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann, Inc., San Francisco, CA, 1998, pp. 386\u2013395.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient greedy learning of Gaussian mixture models", "author": ["J.J. Verbeek", "N. Vlassis", "B. Kr\u00f6se"], "venue": "Neural computation, vol. 15, no. 2, pp. 469\u2013 485, 2003.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Accelerated quantification of Bayesian networks with incomplete data", "author": ["B. Thiesson"], "venue": "University of Aalborg, Institute for Electronic Systems, Department of Mathematics and Computer Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "Initialization of Iterative Refinement Clustering Algorithms.", "author": ["U.M. Fayyad", "C. Reina", "P.S. Bradley"], "venue": "in KDD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Initializing EM using the properties of its trajectories in Gaussian mixtures.", "author": ["C. Biernacki"], "venue": "Statistics and Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SODA, N. Bansal, K. Pruhs, and C. Stein, Eds. SIAM, 2007, pp. 1027\u20131035.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "On the initialization of dynamic models for speech features", "author": ["A. Krueger", "V. Leutnant", "R. Haeb-Umbach", "M. Ackermann", "J. Bloemer"], "venue": "ITG- Fachbericht-Sprachkommunikation 2010, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "A New Method for Random Initialization of the EM Algorithm for Multivariate Gaussian Mixture Learning", "author": ["W. Kwedlo"], "venue": "Proceedings of the 8th International Conference on Computer Recognition Systems CORES 2013. Springer International Publishing, 2013, pp. 81\u201390.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["T.F. Gonzalez"], "venue": "Theoretical Computer Science, vol. 38, pp. 293\u2013306, 1985.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1985}, {"title": "Learning Mixtures of Gaussians", "author": ["S. Dasgupta"], "venue": "FOCS. IEEE Computer Society, 1999, pp. 634\u2013644.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Evaluation of Clusterings \u2013 Metrics and Visual Support", "author": ["E. Achtert", "S. Goldhofer", "H.-P. Kriegel", "E. Schubert", "A. Zimek"], "venue": "Data Engineering, International Conference on, vol. 0, pp. 1285\u20131288, 2012. [Online]. Available: http://elki.dbs.ifi.lmu.de/wiki/DataSets/MultiView", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "The Amsterdam Library of Object Images", "author": ["J.M. Geusebroek", "G.J. Burghouts", "A.W.M. Smeulders"], "venue": "International Journal of Computer Vision, vol. 6, no. 1, pp. 103\u2013112, 2005.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Evaluation of Multiple Clustering Solutions", "author": ["H.-P. Kriegel", "E. Schubert", "A. Zimek"], "venue": "Proc. ECML PKDD Workshop MultiClust, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "UCI machine learning repository", "author": ["D.N.A. Asuncion"], "venue": "2007. [Online]. Available: http://www.ics.uci.edu/\u223cmlearn/MLRepository.html", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "A widely used approach for the latter problem is the ExpectationMaximization (EM) algorithm [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "[2], [3]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[2], [3]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "[4]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5]\u2013 [8]), which we will also refer to as \u201crandom initializations\u201d.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[5]\u2013 [8]), which we will also refer to as \u201crandom initializations\u201d.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "For instance, in [6]\u2013[8] HAC (with Ward\u2019s criterion, average linkage, and a model-based distance measure, re-", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "For instance, in [6]\u2013[8] HAC (with Ward\u2019s criterion, average linkage, and a model-based distance measure, re-", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "Except for [7], [6] and [8] report that this", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "Except for [7], [6] and [8] report that this", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "Except for [7], [6] and [8] report that this", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "Another approach that makes use of HAC is the multistage approach proposed in [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "[6] a density based approach is proposed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "In [9] a greedy algorithm ar X iv :1 31 2.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": ", in [7], [10]\u2013[12].", "startOffset": 5, "endOffset": 8}, {"referenceID": 9, "context": ", in [7], [10]\u2013[12].", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": ", in [7], [10]\u2013[12].", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "Namely, we use the K-means++ algorithm [13], the Gonzalez algorithm, and a simple uniform initialization.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": ", in [14] these GMMs are used for speech recognition).", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "Some initial research in this area has been done in [15], where a modification of the Gonzalez algorithm is considered.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "presented in [15] and an algorithm based on HAC.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "called K-means++ initialization [13], which is intended to be used for the K-means clustering algorithm.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "The second method is the Gonzalez algorithm [16], which is a 2-approximation algorithm for the discrete radius K-clustering problem.", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "Gonzalez executes the Gonzales algorithm, Kmeans++ executes the K-means++ algorithm [13], and Uniform draws K means uniformly at random from the given data set.", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "[4]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "That is, for some fixed S \u2286 X , we define for all x \u2208 S and \u03b1 \u2208 [0, 1]", "startOffset": 64, "endOffset": 70}, {"referenceID": 14, "context": "In our experiments, we compare them with another adaption of the Gonzalez algorithm presented in [15], which we refer to as", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "1 Our first attempts consisted in just adding a new component to the old mixture model, for instance, by choosing the covariances as in [17] (i.", "startOffset": 136, "endOffset": 140}, {"referenceID": 0, "context": "Adaptive ( X \u2282 R,K \u2208 N, \u03b1 \u2208 [0, 1] )", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": ",K uniformly at random from [0, 1] and normalize them by \u2211K k=1 wk.", "startOffset": 28, "endOffset": 34}, {"referenceID": 14, "context": "Adaption of the Gonzalez algorithm according to [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "Generation of a random covariance matrix according to [15].", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "The first data set is provided by the ELKI project [19] and based on the Amsterdam Library of Object Images (ALOI) [20], which consists of 110 250 pictures of 1 000 small objects (taken under various conditions).", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "The first data set is provided by the ELKI project [19] and based on the Amsterdam Library of Object Images (ALOI) [20], which consists of 110 250 pictures of 1 000 small objects (taken under various conditions).", "startOffset": 115, "endOffset": 119}, {"referenceID": 19, "context": "[21]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "The third one is the Forest Covertype data set which contains 581 012 data points and is available from the UCI Machine Learning Library [22].", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "The fourth data set is the Spambase data set which is also provided by the UCI Machine Learning Library [22].", "startOffset": 104, "endOffset": 108}, {"referenceID": 16, "context": "Following [17], we define the separation c\u03b8 of a Gaussian mixture model \u03b8 by", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "5, 1, 2 uniform different [1, 10]", "startOffset": 26, "endOffset": 33}, {"referenceID": 9, "context": "5, 1, 2 uniform different [1, 10]", "startOffset": 26, "endOffset": 33}, {"referenceID": 5, "context": "[6]).", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "In this paper, we consider simple and fast approaches to initialize the Expectation-Maximization algorithm (EM) for multivariate Gaussian mixture models. We present new initialization methods based on the well-known K-means++ algorithm and the Gonzalez algorithm. These methods close the gap between simple uniform initialization techniques and complex methods, that have been specifically designed for Gaussian mixture models and depend on the right choice of hyperparameters. In our evaluation we compare our methods with a commonly used random initialization method, an approach based on agglomerative hierarchical clustering, and a known, plain adaption of the Gonzalez algorithm. Our results indicate that algorithms based on K-means++ outperform the other methods.", "creator": "LaTeX with hyperref package"}}}