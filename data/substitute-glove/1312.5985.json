{"id": "1312.5985", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Learning Type-Driven Tensor-Based Meaning Representations", "abstract": "This paper probes now well of 3rd - order tensors representing of tonal of reciprocal inflected. The reason representations are similar taken a modified - weak matrix - based adaptive implementation, between came newly emergence an from popularizing distributional parsing. Standard analytical months the synaptic system testament are be, learn similar vector, into which diagnosis on its selectional recognition - little task each set simple 2 - dimensional sentence hubble. Promising results are initial however has qualified paroles - based baseline. We notion example agreements part work whole transitive semantically, each to fell - simulations prisoner spaces, itself an interesting out extremely problem for as machine learning new to consider.", "histories": [["v1", "Fri, 20 Dec 2013 15:21:15 GMT  (57kb,D)", "https://arxiv.org/abs/1312.5985v1", "Submitted as part of the open review process for ICLR'14. The paper contains 9 pages, 3 figures, 4 tables"], ["v2", "Tue, 18 Feb 2014 15:27:24 GMT  (55kb,D)", "http://arxiv.org/abs/1312.5985v2", "Submitted as part of the open review process for ICLR'14. The paper contains 10 pages, 3 figures, 4 tables"]], "COMMENTS": "Submitted as part of the open review process for ICLR'14. The paper contains 9 pages, 3 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["tamara polajnar", "luana fagarasan", "stephen clark"], "accepted": false, "id": "1312.5985"}, "pdf": {"name": "1312.5985.pdf", "metadata": {"source": "CRF", "title": "Learning Type-Driven Tensor-Based Meaning Representations", "authors": ["Tamara Polajnar"], "emails": ["first.last@cl.cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7]. The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning [26, 31] with the more traditional compositional methods from formal semantics [13]. Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models handle the unbounded nature of natural language, as well as providing established accounts of logical words, quantification, and inference.\nOne promising approach which attempts to combine elements of compositional and distributional semantics is by Coecke et al. [10]. The underlying idea is to take the type-driven approach from formal semantics \u2014 in particular the idea that the meanings of complex grammatical types should be represented as functions \u2014 and apply it to distributional representations. Since the mathematics of distributional semantics is provided by linear algebra, a natural set of functions to consider is the set of linear maps. Coecke et al. recognize that there is a natural correspondence from complex grammatical types to tensors (multi-linear maps), so that the meaning of an adjective, for example, is represented by a matrix (a 2nd-order tensor)1 and the meaning of a transitive verb is represented by a 3rd-order tensor. Coecke et al. use the grammar of pregroups as the syntactic machinery to construct distributional meaning representations, since both pregroups and vector spaces can be seen as examples of the same abstract structure, which leads to a particularly clean mathematical description of the compositional process. However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Categorial Grammar [29], and also to phrase-structure grammars in a way that a formal linguist would recognize [2]. Clark [7] provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in [10]. Section 2 repeats some of this description.\n1This same insight lies behind the work of Baroni and Zamparelli [3].\nar X\niv :1\n31 2.\n59 85\nv2 [\ncs .C\nL ]\n1 8\nFe b\nA major open question associated with the tensor-based semantic framework is how to learn the tensors representing the meanings of words with complex types, such as verbs and adjectives. The framework is essentially a compositional framework, providing a recipe for how to combine distributional representations, but leaving open what the underlying vector spaces are and how they can be acquired. One significant challenge is an engineering one: in a wide-coverage grammar able to handle naturally occurring text, there will be a) a large lexicon with many word-category pairs requiring tensor representations; and b) many higher-order tensors with large numbers of parameters which need to be learned. In this paper we take a first step towards learning such representations, by learning tensors for transitive verbs.\nOne feature of the tensor-based framework is that it allows the meanings of words and phrases with different basic types, for example nouns and sentences, to live in different vector spaces; but this means that the sentence space must be specified in advance. In this paper we consider a simple sentence space: the \u201cplausibility space\u201d described by Clark [7], represented here as a probability distribution (and hence having only 2 dimensions). Logistic regression is used to learn a plausibility classifier. We begin with this simple space since we want to see the extent to which the tensor-based representations can be learned at all.\nOne goal of this paper is to introduce the problem of learning tensor-based semantic representations to the ML community. Current methods, for example the work of Socher [28], typically use only matrix representations, and also assume that words, phrases and sentences all live in the same vector space. The tensor-based semantic framework is more flexible, in that it allows different spaces for different grammatical types, which results from it being tied more closely to a type-driven syntactic description; however, this flexibility comes at a price, since there are many more paramaters to learn. Various communities are beginning to recognize the additional power that tensor representations can provide, through the capturing of interactions that are difficult to represent with vectors and matrices (see e.g. [25, 30, 11]). Hierarchical recursive structures in language potentially represent a large number of such interactions (the obvious example for this paper being the interaction between a transitive verb\u2019s subject and object), and present a significant challenge for machine learning."}, {"heading": "2 Syntactic Types to Tensors", "text": "The syntactic type of a transitive verb in English is (S\\NP)/NP (using Steedman [29] notation), meaning that a transitive verb is a function which takes an NP argument to the right, an NP argument to the left, and results in a sentence S . Such categories with slashes are complex categories; S and NP are basic or atomic categories. Interpreting such categories under the Coecke et al. framework is straightforward. First, for each atomic category there is a corresponding vector space; in this case the sentence space S and the noun space N.2 Hence the meaning of a noun or noun phrase, for example people, will be a vector in the noun space: \u2212\u2212\u2212\u2192people \u2208 N. In order to obtain the meaning of a transitive verb, each slash is replaced with a tensor product, so that the meaning of eat, for example, is a 3rd-order tensor: eat \u2208 S\u2297N\u2297N. Just as in the syntactic case, the meaning of a transitive verb is a function (a multi-linear map) which takes two noun vectors as arguments and returns a sentence vector.\nMeanings combine using tensor contraction, which can be thought of as a multi-linear generalisation of matrix multiplication [17]. Consider first the adjective-noun case, for example black cat. The syntactic type of black is N /N ; hence its meaning is a 2nd-order tensor (matrix): black \u2208 N\u2297N. In the syntax, N /N combines with N using the rule of forward application (N /N N \u21d2N ), which is an instance of function application. Function application is also used in the tensor-based semantics, which, for a matrix and vector argument, corresponds to matrix multiplication.\nFigure 1 shows how the syntactic types combine with a transitive verb, and the corresponding tensorbased semantic types. Note that, after the verb has combined with its object NP , the type of the verb phrase is S\\NP , with a corresponding meaning tensor (matrix) in S\u2297N . This matrix then combines with the subject vector, through matrix multiplication, to give a sentence vector.\nIn practice, using for example the wide-coverage grammar from CCGbank [19], there will be many types with more than 3 slashes, with corresponding higher-order tensors. For example, a common\n2In practice, for example using the CCG parser of Clark and Curran [8], there will be additional atomic categories, such as PP , but not many more.\ncategory for a preposition is the following: ((S\\NP)\\(S\\NP))/NP , which would be assigned to with in eat with a fork. (The way to read the syntactic type is as follows: with requires an NP argument to the right \u2013 a fork in this example \u2013 and then a verb phrase to the left \u2013 eat with type S\\NP \u2013 resulting in a verb phrase.) The corresponding meaning tensor lives in the space S\u2297N \u2297S\u2297N \u2297N , i.e. a 5th-order tensor. Categories with even more slashes are not uncommon, for example ((N /N )/(N /N ))/((N /N )/(N /N )). Clearly learning parameters for such tensors is highly challenging, and it is likely that lower dimensional approximations will be required. We leave investigation of such approximations to future work."}, {"heading": "3 Verb and Sentence Representation", "text": "As described above, in this paper we have chosen to focus on a two-dimensional \u201cplausibility space\u201d for the meanings of sentences. One way to think of this space is the simplest extension of truth values from the traditional truth-theoretic account to a real-valued setting.3 We also focus on the plausibility of transitive verb sentences with the simple subject verb object (SVO) grammatical structure, for example people eat fish (as in Figure 1). These sentences were generated automatically by finding specific transitive verbs in a dependency-parsed corpus and extracting the head nouns from the subject and the object (see Section 4). The nouns have atomic syntactic types and are represented by distributional semantic vectors, built using standard techniques [31], while the verb is a multi-linear map that takes in two nouns and outputs values in the plausibility space.\nWe define the plausibility space to have two dimensions, one corresponding to plausible and one corresponding to implausible. Hence the verb tensor outputs two real values for each subject-verbobject triple. If the vectors in noun space have dimensionality K and the sentence space has dimensionality S (two in this case), then the verb is a K \u00d7 K \u00d7 S tensor. We add some additional processing to the tensor network, following standard practice in neural networks and following [20], by passing the output values though a non-linear sigmoid function, and then creating a probability distribution over over two classes, plausible (>) and implausible (\u22a5), using a softmax function. In Section 3.1, we propose a two-class logistic regression classifier for simultaneous learning of the verb function and the plausibility space. This method was introduced in [20], but only implemented as a proof-of-concept with vectors of length 2 and small, manually created datasets based on propositional logic examples. In order to make the learning practical, given the large numbers of contextual features in the noun vectors, we employ a technique (described in Section 4.2) that improves low-dimensional singular value decomposition (SVD) [12], and thus enables us to effectively limit the number of parameters while learning from corpus-sized data. As a baseline we adapted a method from [18], where the verb is represented as the average of the Kronecker products of the subject and object vectors from the positive training data. This method does not produce a plausibility space, but plausibility of the subject-verb-object triple can be calculated using cosine similarity (see Section 3.2)."}, {"heading": "3.1 Tensor learning", "text": "Following [20], we learn the tensor values as parameters (V) of a regression algorithm. To represent this space as a distribution over two classes (>,\u22a5) we apply a sigmoid (\u03c3) to restrict the output to the [0,1] range and the softmax activation function (g) to balance the class probabilities. The full\n3We are not too concerned with the philosophical interpretation of these plausibility values; rather we see the plausibility sentence space as a useful inital testbed for the tensor-based semantic framework.\nparameter set which we need to optimise for is B = {V,\u0398}, where \u0398 = {\u03b8>, \u03b8\u22a5} are the softmax parameters for the two classes. For each verb we optimise the KL-divergence L between the training labels ti and classifier predictions using:\nO(B) = N\u2211 i=1 L ( ti, g ( \u03c3 ( (nis)V(n i o) T ) ,\u0398 )) + \u03bb 2 ||B||2 (1)\nwhere nis and n i o are the subject and object of the training instance i \u2208 N . The gold-standard distribution over training labels is defined as (1, 0) or (0, 1), depending on whether the training instance is a positive (plausible) or negative (implausible) example. The derivatives are obtained via the chain rule with respect to each set of parameters and gradient descent is performed using the Adagrad algorithm [14]. Tensor contraction is implemented using the Matlab Tensor Toolbox [1]."}, {"heading": "3.2 Baseline", "text": "The baseline is a simple corpus-driven approach of generating a matrix from an average of Kronecker products of the subject and object vectors from the positively labelled subset of the training data [18], for each verb. The intuition is that the matrix for each verb represents an average of the pairwise contextual features of a typical subject and object (as extracted from instances of the verb). For example, the matrix for eat may have a high value for the contextual feature pair (is human, is food) (assuming that the features are interpretable in this way). To determine the plausibility of a new subject-object pair for a particular verb, we calculate the Kronecker product of the subject and object noun vectors for this pair, and compare the resulting matrix with the average verb matrix using cosine similarity. Intuitively, the average verb matrix can be thought of as what the verb expects to see in terms of the contextual features of its subject and objects, and the cosine is determining the extent to which the particular argument pair satisfies those expectations. As well as being an intuitive corpus-based method for representing the meaning of a transitive verb, this method has also performed well experimentally [18], and hence we consider it to be a competitive baseline.\nFor label prediction, the cutoff is estimated at the break-even point of the receiver operator characteristic (ROC) generated by testing the positive and negative examples of the training data against the learned average matrix.4 In practice it would be more accurate to estimate the cutoff on a validation dataset, but some of the verbs have so few training instances that this was not possible."}, {"heading": "4 Data", "text": "To train a classifier for each verb, a dataset of positive and negative examples is required. While we can consider subject-verb-object triples that naturally occur in corpus data as positive, a technique for generating pseudo-negative examples is needed, which is described below."}, {"heading": "4.1 Training examples", "text": "In order to generate training data we made use of two large corpora: the Google Syntactic N-grams (GSN) [16] and the Wikipedia October 2013 dump. The Wikipedia corpus consists of the textual content tokenised using the Stanford NLP tools5 and parsed and lemmatised using the C&C parser and the Morpha lemmatiser [8, 22].\nWe first chose transitive verbs with different concreteness scores [5] and frequencies, in order to obtain a variety of verb types. Then the positive SVO examples were extracted from the GSN corpus. More precisely, we extracted all distinct syntactic trigrams of the form nsubj ROOT dobj, where the root of the phrase was one of our target verbs. We lemmatised the words using the NLTK6 lemmatiser and filtered these examples to retain only the ones that contain nouns that also occur in Wikipedia, obtaining the counts reported in Table 1.\nFor every positive (plausible) training example, we constructed a negative (implausible) one by replacing both the subject and the object with a confounder, using a standard technique from the\n4The break-even point is when the true positive rate is equal to the false positive rate. 5http://nlp.stanford.edu/software/index.shtml 6http://nltk.org/\nselection preferences literature [6]. A confounder was generated by choosing a random noun from the same frequency bucket as the original noun. Frequency buckets of size 10 were constructed by collecting noun frequency counts from the Wikipedia corpus. Table 2 presents a few pairs of positive and negative training examples."}, {"heading": "4.2 Noun representation", "text": "Distributional semantic models [31] encode word meaning in a vector format by counting cooccurrences with other words within a specified context, which can be defined in many ways, for example as a whole document, an N-word window, or a grammatical relation. In this paper we use sentence boundaries to define context windows. To generate noun context vectors, the Wikipedia corpus described above is scanned for the nouns that appear in the training data and the number of times a context word (cj) occurs within the same sentence as the target noun (wi) is recorded in the vector representing that noun. The context words are the top 10,000 most frequent lemmatised words in the whole corpus excluding stopwords. The raw co-occurrence counts are re-weighted using the standard tTest weighting scheme, where fwicj is the number of times target noun wi occurs with context word cj :\ntTest( ~wi, cj) = p(wi, cj)\u2212 p(wi)p(cj)\u221a\np(wi)p(cj) (2)\nwhere p(wi) = \u2211 j fwicj\u2211\nk \u2211 l fwkcl\n, p(cj) = \u2211 i fwicj\u2211\nk \u2211 l fwkcl , and p(wi, cj) = fwicj\u2211 k \u2211 l fwkcl .\nUsing all 10,000 context words would result in a large number of parameters for each verb tensor, and so we apply the following dimensionality reduction technique which makes training tractable. Considering tTest values as a ranking function, we choose the top N highest ranked context words for each noun. The value N is chosen by testing on the development subset of the MEN dataset (MENdev), a standard dataset for evaluating the quality of semantic vectors [4].7 The tTest weights span the range [\u22121, 1], but are generally tightly concentrated around zero. Hence an additional technique we use is to spread the range using row normalisation: ~w := ~w||~w||2 . Hence each noun vector now contains only N non-zero values, where each value is a (weighted, normalised) cooccurrence frequency.\n7The MEN dataset contains 3000 word pairs that were judged for similarity by human annotators. Of that 2000 are in the development subset and the remaining 1000 are used as test data.\nFinally, placing each noun vector as a row in a matrix results in a noun-context co-occurrence matrix. Singular value decomposition (SVD) is applied to this matrix, with 20 latent dimensions. Applying SVD to such a matrix is a standard technique for removing noise and uncovering the latent semantic dimensions in the data, and has been reported to improve performance on a number of semantic similarity tasks [31]. Together these two simple techniques [24] markedly improve the performance of SVD on smaller dimensions (K) on the MENdev set (see Figure 2), and enable us to train the verb tensors using 20-dimensional noun vectors. On an older, highly reported dataset of 353 word pairs [15] our vectors achieve the Spearman correlation of 0.63 without context selection and normalisation, and 0.60 with only 20 dimensions after these techniques have been applied.On MENtest we get 0.73 and 0.71, respectively."}, {"heading": "5 Experiments", "text": "We conducted three experiments. The first used all of the available training examples in 5 repetitions of a 2-fold cross-validation (5x2cv) experiment to evaluate the peak performance for each of the verbs (Table 3). The verbs with many subject-object pairs were capped at 4,000 instances (the 2,000 most frequent positive pairs and 2,000 pseudo-negative). We compared the performance of the baseline and tensor learning methods on 20 and 40 dimensional vectors.8\nThe performance was evaluated using the area under the ROC (AUC) and the F1 measure (based on precision and recall over the plausible class). The AUC evaluates whether a method is ranking positive examples above negative ones, regardless of the class cutoff value. F1 shows how accurately a method assigns the correct class label. Since the baseline uses ad hoc class assignment, AUC is the more fair measure.\nIn the second experiment, we repeated the 5x2cv with datasets of 52 training points for each verb, as this is the size of the smallest dataset of the verb CENSOR (Table 4). The points were randomly sampled from the datasets used in the first experiment. Finally, the four verbs with the largest datasets were used to examine how the performance of the methods change as the amount of training data increases. The 4,000 training samples were randomised and half was used for testing. We sampled between 10 and 1000 training triples from the other half (Figure 3)."}, {"heading": "5.1 Analysis", "text": "In general the tensor learning algorithm learns more effectively and with smaller variance than the baseline, particularly from the smaller dimensional noun vectors. The F1 scores indicate that learning is necessary for accurate classification while the baseline AUC results show that in principle only positive examples are necessary (since the baseline only sees positive examples). Analysis of errors shows that the baseline method mostly generates false negative errors (i.e. predicting implausible\n8We also implemented and experimented with a matrix method, which outputs a sigmoid transformed single plausibility value instead of the overparameterised 2-value softmax vector. This method performed worse than baseline and was thus left out of this paper.\nwhen the gold standard label is plausible), particularly on triples that contain nouns that have not been seen with the verb in the training data, which indicates that the baseline may not adequately generalise over the latent dimensions from the SVD. In contrast, tensor learning (TL) produces almost equal numbers of false positives and false negatives, but sometimes produces false negatives with some low frequency nouns (e.g. bourgeoisie idealize work), presumably because there is not enough information in the noun vector to decide on the correct class. TL also produces some false positive errors when either of the nouns is plausible (but the triple is implausible), which would suggest results may be improved by training with data where only one noun is confounded or treating negative data as possibly positive [21].\nBoth the full data and small data experiments indicate that IDEALIZE is the most difficult verb to learn. It has the twin properties of low frequency and low concreteness. In addition, it is likely to have low selectional preference strength, not selecting strongly for the semantic types of its arguments. Both verb frequency and concreteness have positive Spearman correlation with the TL AUC values from Tables 3 and 4. Frequency has much stronger correlation (Table 3:0.53, Table 4:0.31) than concreteness (Table 3:0.14, Table 4:0.08), even when all datasets are reduced to the same number of examples. This is probably due to the fact that the more frequent verbs occur in more frequent\ntriples, which are likely to contain highly frequent nouns, and hence have higher quality noun vectors. However, if we just consider the most frequent verbs (Figure 3) we can see that EAT, which has the highest concreteness (4.44), provides a much smoother learning curve and asymptotes quicker than the less concrete verbs APPLY (2.4), REDUCE (2), and JUSTIFY (1.45). From this brief analysis, we hypothesise that noun frequency, verb concreteness, and selectional preference strength of the verb for its arguments all influence the quality of the learned representation."}, {"heading": "6 Conclusion", "text": "In this paper we have investigated learning 3rd-order tensors to represent the semantics of transitive verbs, with a 2-dimensional \u201cplausibility\u201d sentence space. There are obvious connections with the large literature on selectional preference learning (see e.g. [27] for a recent paper); however, our goal is not to contribute to that literature, but rather to use a selectional preference task as a first corpus-driven test of the type-driven tensor-based semantic framework of Coecke et al., as well as introduce this framework to the machine learning community.\nWe have shown that standard techniques from the neural networks literature can be effectively applied to learning 3rd-order tensors from corpus data, with our results showing positive trends compared to a competitive corpus-based baseline. There is much work to be done in extending the techniques in this paper, both in terms of a higher-dimensional, potentially more structured, sentence space, and in terms of incorporating the many syntactic types making up a wide-coverage grammar. Since many of these types require higher order tensors than 3rd-order, we suggest that tensor decomposition techniques are likely to be necessary for practical performance."}, {"heading": "Acknowledgments", "text": "Tamara Polajnar is supported by ERC Starting Grant DisCoTex (306920). Stephen Clark is supported by ERC Starting Grant DisCoTex and EPSRC grant EP/I037512/1. Luana Fa\u030cga\u030cras\u0327an is supported by an EPSRC Doctoral Training Partnership award. Thanks to Laura Rimell and Jean Maillard for helpful discussion."}], "references": [{"title": "Matlab tensor toolbox version 2.5", "author": ["Brett W. Bader", "Tamara G. Kolda"], "venue": "Available online,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Frege in space: A program for compositional distributional semantics (to appear)", "author": ["M. Baroni", "R. Bernardi", "R. Zamparelli"], "venue": "Linguistic Issues in Language Technologies,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Distributional semantics in technicolor", "author": ["E. Bruni", "G. Boleda", "M. Baroni", "N. Tran"], "venue": "In Proceedings of the 50th Annual Meeting of the ACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Concreteness ratings for 40 thousand generally known English word lemmas", "author": ["Marc Brysbaert", "Amy Beth Warriner", "Victor Kuperman"], "venue": "Behavior research methods,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Improving the use of pseudo-words for evaluating selectional preferences", "author": ["Nathanael Chambers", "Dan Jurafsky"], "venue": "In Proceedings of the 48th Meeting of the ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Type-driven syntax and semantics for composing meaning vectors", "author": ["Stephen Clark"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Wide-coverage efficient statistical parsing with CCG and log-linear models", "author": ["Stephen Clark", "James R. Curran"], "venue": "Computational Linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "A context-theoretic framework for compositionality in distributional semantics", "author": ["Daoud Clarke"], "venue": "Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["B. Coecke", "M. Sadrzadeh", "S. Clark"], "venue": "Linguistic Analysis (Lambek Festschrift),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Multi-way tensor factorization for unsupervised lexical acquisition", "author": ["Tim Van de Cruys", "Laura Rimell", "Thierry Poibeau", "Anna Korhonen"], "venue": "In Proceedings of COLING", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1990}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "A dataset of syntactic-ngrams over time from a very large corpus of English books", "author": ["Yoav Goldberg", "Jon Orwant"], "venue": "In Second Joint Conference on Lexical and Computational Semantics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics", "author": ["Edward Grefenstette"], "venue": "PhD thesis, University of Oxford,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank", "author": ["Julia Hockenmaier", "Mark Steedman"], "venue": "Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Vector space semantic parsing: A framework for compositional vector space models", "author": ["Jayant Krishnamurthy", "Tom M Mitchell"], "venue": "In Proceedings of the 2013 ACL Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Learning with positive and unlabeled examples using weighted logistic regression", "author": ["Wee Sun Lee", "Bing Liu"], "venue": "In Proceedings of the Twentieth International Conference on Machine Learning (ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Applied morphological processing of English", "author": ["Guido Minnen", "John Carroll", "Darren Pearce"], "venue": "Natural Language Engineering,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "In Proceedings of ACL-", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Improving distributional semantic vectors through context selection and normalisation", "author": ["Tamara Polajnar", "Stephen Clark"], "venue": "In 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Factored 3-way restricted boltzmann machines for modeling natural images", "author": ["M. Ranzato", "A. Krizhevsky", "G.E. Hinton"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Automatic word sense discrimination", "author": ["Hinrich Sch\u00fctze"], "venue": "Computational Linguistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Latent variable models of selectional preference", "author": ["Diarmuid O Seaghdha"], "venue": "In Proceedings of ACL 2010, Uppsala,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "The Syntactic Process", "author": ["Mark Steedman"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "Modelling relational data using bayesian clustered tensor factorization", "author": ["I. Sutskever", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Cost-conscious comparison of supervised learning algorithms over multiple data sets", "author": ["Ayd\u0131n Ula\u015f", "Olcay Taner Y\u0131ld\u0131z", "Ethem Alpayd\u0131n"], "venue": "Pattern Recognition,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 2, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 9, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 16, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 8, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 26, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 6, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 24, "context": "The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning [26, 31] with the more traditional compositional methods from formal semantics [13].", "startOffset": 130, "endOffset": 138}, {"referenceID": 29, "context": "The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning [26, 31] with the more traditional compositional methods from formal semantics [13].", "startOffset": 130, "endOffset": 138}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Categorial Grammar [29], and also to phrase-structure grammars in a way that a formal linguist would recognize [2].", "startOffset": 135, "endOffset": 139}, {"referenceID": 1, "context": "However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Categorial Grammar [29], and also to phrase-structure grammars in a way that a formal linguist would recognize [2].", "startOffset": 227, "endOffset": 230}, {"referenceID": 6, "context": "Clark [7] provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in [10].", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "Clark [7] provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in [10].", "startOffset": 198, "endOffset": 202}, {"referenceID": 2, "context": "This same insight lies behind the work of Baroni and Zamparelli [3].", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "In this paper we consider a simple sentence space: the \u201cplausibility space\u201d described by Clark [7], represented here as a probability distribution (and hence having only 2 dimensions).", "startOffset": 95, "endOffset": 98}, {"referenceID": 26, "context": "Current methods, for example the work of Socher [28], typically use only matrix representations, and also assume that words, phrases and sentences all live in the same vector space.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "[25, 30, 11]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 28, "context": "[25, 30, 11]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 10, "context": "[25, 30, 11]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 27, "context": "The syntactic type of a transitive verb in English is (S\\NP)/NP (using Steedman [29] notation), meaning that a transitive verb is a function which takes an NP argument to the right, an NP argument to the left, and results in a sentence S .", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "Meanings combine using tensor contraction, which can be thought of as a multi-linear generalisation of matrix multiplication [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "In practice, using for example the wide-coverage grammar from CCGbank [19], there will be many types with more than 3 slashes, with corresponding higher-order tensors.", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "For example, a common In practice, for example using the CCG parser of Clark and Curran [8], there will be additional atomic categories, such as PP , but not many more.", "startOffset": 88, "endOffset": 91}, {"referenceID": 29, "context": "The nouns have atomic syntactic types and are represented by distributional semantic vectors, built using standard techniques [31], while the verb is a multi-linear map that takes in two nouns and outputs values in the plausibility space.", "startOffset": 126, "endOffset": 130}, {"referenceID": 18, "context": "We add some additional processing to the tensor network, following standard practice in neural networks and following [20], by passing the output values though a non-linear sigmoid function, and then creating a probability distribution over over two classes, plausible (>) and implausible (\u22a5), using a softmax function.", "startOffset": 118, "endOffset": 122}, {"referenceID": 18, "context": "This method was introduced in [20], but only implemented as a proof-of-concept with vectors of length 2 and small, manually created datasets based on propositional logic examples.", "startOffset": 30, "endOffset": 34}, {"referenceID": 11, "context": "2) that improves low-dimensional singular value decomposition (SVD) [12], and thus enables us to effectively limit the number of parameters while learning from corpus-sized data.", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "As a baseline we adapted a method from [18], where the verb is represented as the average of the Kronecker products of the subject and object vectors from the positive training data.", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "Following [20], we learn the tensor values as parameters (V) of a regression algorithm.", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "To represent this space as a distribution over two classes (>,\u22a5) we apply a sigmoid (\u03c3) to restrict the output to the [0,1] range and the softmax activation function (g) to balance the class probabilities.", "startOffset": 118, "endOffset": 123}, {"referenceID": 12, "context": "The derivatives are obtained via the chain rule with respect to each set of parameters and gradient descent is performed using the Adagrad algorithm [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "Tensor contraction is implemented using the Matlab Tensor Toolbox [1].", "startOffset": 66, "endOffset": 69}, {"referenceID": 16, "context": "The baseline is a simple corpus-driven approach of generating a matrix from an average of Kronecker products of the subject and object vectors from the positively labelled subset of the training data [18], for each verb.", "startOffset": 200, "endOffset": 204}, {"referenceID": 16, "context": "As well as being an intuitive corpus-based method for representing the meaning of a transitive verb, this method has also performed well experimentally [18], and hence we consider it to be a competitive baseline.", "startOffset": 152, "endOffset": 156}, {"referenceID": 14, "context": "In order to generate training data we made use of two large corpora: the Google Syntactic N-grams (GSN) [16] and the Wikipedia October 2013 dump.", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "The Wikipedia corpus consists of the textual content tokenised using the Stanford NLP tools5 and parsed and lemmatised using the C&C parser and the Morpha lemmatiser [8, 22].", "startOffset": 166, "endOffset": 173}, {"referenceID": 20, "context": "The Wikipedia corpus consists of the textual content tokenised using the Stanford NLP tools5 and parsed and lemmatised using the C&C parser and the Morpha lemmatiser [8, 22].", "startOffset": 166, "endOffset": 173}, {"referenceID": 4, "context": "We first chose transitive verbs with different concreteness scores [5] and frequencies, in order to obtain a variety of verb types.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "selection preferences literature [6].", "startOffset": 33, "endOffset": 36}, {"referenceID": 29, "context": "Distributional semantic models [31] encode word meaning in a vector format by counting cooccurrences with other words within a specified context, which can be defined in many ways, for example as a whole document, an N-word window, or a grammatical relation.", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "The value N is chosen by testing on the development subset of the MEN dataset (MENdev), a standard dataset for evaluating the quality of semantic vectors [4].", "startOffset": 154, "endOffset": 157}, {"referenceID": 29, "context": "Applying SVD to such a matrix is a standard technique for removing noise and uncovering the latent semantic dimensions in the data, and has been reported to improve performance on a number of semantic similarity tasks [31].", "startOffset": 218, "endOffset": 222}, {"referenceID": 22, "context": "Together these two simple techniques [24] markedly improve the performance of SVD on smaller dimensions (K) on the MENdev set (see Figure 2), and enable us to train the verb tensors using 20-dimensional noun vectors.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "On an older, highly reported dataset of 353 word pairs [15] our vectors achieve the Spearman correlation of 0.", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "Bold indicates that the method performs better, and \u2020 that the result is significant according to the 5x2cv F-test [32].", "startOffset": 115, "endOffset": 119}, {"referenceID": 19, "context": "TL also produces some false positive errors when either of the nouns is plausible (but the triple is implausible), which would suggest results may be improved by training with data where only one noun is confounded or treating negative data as possibly positive [21].", "startOffset": 262, "endOffset": 266}, {"referenceID": 25, "context": "[27] for a recent paper); however, our goal is not to contribute to that literature, but rather to use a selectional preference task as a first corpus-driven test of the type-driven tensor-based semantic framework of Coecke et al.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.", "creator": "LaTeX with hyperref package"}}}