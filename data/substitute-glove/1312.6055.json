{"id": "1312.6055", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Unit Tests for Stochastic Optimization", "abstract": "Optimization over stochastic solubility descent goes also important requires brought rather large - scale machine learning arithmetic. A long distinct which all workaround parameters same been approach; however, it yet possibility whether rather algorithms are robust and widely accordance leaving those similarly modeling landscapes. In given information what strategies a catalog of unit concluded the generalized method. Each operates matches significantly evaluates an approximation nonlinear on a typical - improvements, concentrated, and instance - quite difficulty, instance so in real - event phases where most their topics are entangled. Passing these officer toxicology name not assure, but absolutely changes put any nonlinear had lawsuit to decidability or responsiveness. We give addition quantitative and qualitative comparison on a dozen served variables. The concluded outlines same back - related, extensible, with maybe to apply time washington real-world.", "histories": [["v1", "Fri, 20 Dec 2013 17:44:06 GMT  (2443kb)", "http://arxiv.org/abs/1312.6055v1", "Initial submission to ICLR 2014"], ["v2", "Tue, 7 Jan 2014 20:43:40 GMT  (2923kb)", "http://arxiv.org/abs/1312.6055v2", "Submission to ICLR 2014 (revised for minor improvements, pre-reviews)"], ["v3", "Tue, 25 Feb 2014 18:16:54 GMT  (7536kb,D)", "http://arxiv.org/abs/1312.6055v3", "Final submission to ICLR 2014 (revised according to reviews, additional results added)"]], "COMMENTS": "Initial submission to ICLR 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tom schaul", "ioannis antonoglou", "david silver"], "accepted": true, "id": "1312.6055"}, "pdf": {"name": "1312.6055.pdf", "metadata": {"source": "CRF", "title": "Unit Tests for Stochastic Optimization", "authors": ["Tom Schaul"], "emails": ["tom.schaul@deepmind.com", "ioannis@deepmind.com", "david@deepmind.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n60 55\nv1 [\ncs .L\nG ]\n2 0"}, {"heading": "1 Introduction", "text": "Stochastic optimization [1] is among the most widely used components in large-scale machine learning, thanks to its linear complexity, efficient data usage, and often superior generalization [2, 3, 4]. In this context, numerous variants of stochastic gradient descent have been proposed, in order to improve performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9]. These algorithms may derive from simplifying assumptions on the optimization landscape [10], but in practice, they tend to be used as general-purpose tools, often outside of the space of assumptions their designers intended. The troublesome conclusion is that practitioners find it difficult to discern where potential weaknesses of new (or old) algorithms may lie [11], and when they are applicable \u2013 an issue that is separate from raw performance. This results in essentially a trial-and-error procedure for finding the appropriate algorithm variant and hyper-parameter settings, every time that the dataset, loss function, regularization parameters, or model architecture change [12].\nThe objective of this paper is to establish a collection of benchmarks to evaluate stochastic optimization algorithms and guide algorithm design toward robust variants. Our approach is akin to unit testing, in that it evaluates algorithms on a very broad range of small-scale, isolated, and wellunderstood difficulties, rather than in real-world scenarios where many such issues are entangled. Passing these unit tests is not sufficient, but absolutely necessary for any algorithms with claims to generality or robustness. This is a similar approach to the very fruitful one taken by the black-box optimization community [13, 14].\nThe core assumption we make is that stochastic optimization algorithms are acting locally, that is, they aim for a short-term reduction in loss given the current noisy gradient information, and possibly some internal variables that capture local properties of the optimization landscape. This property stems from computational efficiency concerns, but it has the additional benefits of minimizing initialization bias and allowing for non-stationary optimization. We therefore concentrate on building\nlocal unit tests, that investigate algorithm dynamics on a broad range of local scenarios, because we expect that detecting local failure modes will flag an algorithm as unlikely to be robust on more complex tasks.\nOur divide-and-conquer approach consists of disentangling potential difficulties and testing them in isolation or in simple couplings. Given that our unit tests are small and quick to evaluate, we can have a much larger collection of them, testing hundreds of qualitatively different aspects in less time than it would take to optimize a single traditional benchmark to convergence, thus allowing us to spot and address potential weaknesses early.\nOur main contribution is a testing framework, with unit tests designed to test aspects such as: discontinuous or non-differentiable surfaces, curvature scales, various noise conditions and outliers, saddle-points and plateaus, cliffs and asymmetry, and curl and bootstrapping. It also allows test cases to be concatenated by chaining them in a temporal series, or by combining them into multidimensional unit tests (with or without variable coupling). We give initial quantitative and qualitative results on a dozen established algorithms.\nWe do not expect this to replace traditional benchmark domains that are closer to the real-world, but to complement it in terms of breadth and robustness. We have tried to keep the framework general and extendable, in the hope it will further grow in diversity, and help others in doing robust algorithm design."}, {"heading": "2 Unit test Construction", "text": "Our testing framework is an open-source library containing a collection of unit tests and visualization tools. Each unit test is defined by a prototype function to be optimized, a prototypical scale, a noise prototype, and optionally a non-stationarity prototype. A prototype function is the concatenation of one or more local shape prototypes. A multi-dimensional unit test is a composition of onedimensional unit tests, optionally with a rotation prototype or gradient curl prototype."}, {"heading": "2.1 Shape Prototypes", "text": "Local shape prototypes are functions defined on an interval, and our collection includes linear slopes (zero curvature), quadratic curves (fixed curvature), convex or concave curves (varying curvature), and curves with exponentially increasing or decreasing slope. Further, there are a number of nondifferentiable local shape prototypes (absolute value, rectified-linear, or cliffs). All of these occur in realistic learning scenarios, for example in logistic regression the loss surface is part concave and part convex, an MSE loss is the prototypical quadratic bowl, but then regularization such as L1 introduces non-differentiable bends (as do rectified-linear or maxout units in deep learning [15, 16]). Steep cliffs in the loss surface are a common occurrence when training recurrent neural networks, as discussed in [11]. See the top rows of Figure 1 for some examples."}, {"heading": "2.2 One-dimensional Concatenation", "text": "In our framework, we can chain together a number of shape prototypes, in such a way that the resulting function is continuous and differentiable at all junction points. We can thus produce many prototype functions that closely mimic existing functions, e.g., the Laplace function, sinusoids, saddlepoints, step-functions, etc. See the bottom rows of Figure 1 for some examples.\nA single scale parameter determines the scaling of a concatenated function across all its shapes using the junction constraints. Varying the scales is an important aspect of testing robustness because it is not possible to guarantee well-scaled gradients without substantial overhead. In many learning problems, effort is put into proper normalization [17], but that is insufficient to guarantee homogeneous scaling all throughout the layers of a deep neural network."}, {"heading": "2.3 Noise Prototypes", "text": "The distinguishing feature of stochastic gradient optimization (compared to batch methods) is that it relies on sample gradients (coming from a subset of even a single element of the dataset) which are inherently noisy. In out unit tests, we model this by four types of stochasticity:\n\u2022 Scale-independent additive Gaussian noise on the gradients, which is equivalent to random translations of inputs in a linear model with MSE loss. Note that this type of noise flips the sign of the gradient near the optimum and makes it difficult to approach precisely.\n\u2022 Multiplicative (scale-dependent) Gaussian noise on the gradients, which multiplies the gradients by a positive random number (signs are preserved). This corresponds to a learning scenario where the loss curvature is different for different samples near the current point.\n\u2022 Additive Cauchy noise mimicking the presence of outliers in the dataset.\n\u2022 Mask-out noise, which zeros the gradient (independently for each dimension) with a certain probability. This mimics both training with drop-out [18], and scenarios with rectified linear units where a unit will be inactive for some input samples, but not for others.\nFor the first three, we can vary the noise scale, while for mask-out we pick a drop-out frequency. See Figure 2 for an illustration of the first two noise prototypes. Noise prototypes and prototype functions can be combined independently into one-dimensional unit tests."}, {"heading": "2.4 Multi-dimensional Composition", "text": "High-dimensional unit tests are constructed by composing together one-dimensional unit tests. For example for two one-dimensional prototype shapes La and Lb combined with a p-norm, the composition is L(a,b)(\u03b8) = (La(\u03b81)p + Lb(\u03b82)p) 1 p . Noise prototypes are composed independently of\nshape prototypes. While the 1D prototypes they are composed from may be concatenations, higherdimensional prototypes are not concatenated themselves. Various levels of conditioning can be achieved by having dramatically different scales in different component dimensions.\nIn addition to which prototypes are combined and how they are scaled, we permit a rotation in input space, which couples the dimensions together and avoids axis-alignment. These rotations are particularly important for testing diagonal/element-wise optimization algorithms."}, {"heading": "2.5 Gradient Curl", "text": "In reinforcement learning a value function (the expected discounted reward for each state) can be learned using temporal-difference learning (TD), an update procedure that uses bootstrapping: i.e. it pulls the value of the current state towards the value of its successor state [19]. These stochastic update directions are not proper gradients of any scalar energy field [20], but they still form a (more general) vector field with non-zero curl, where the objective for the optimization algorithm is to converge to its fixed-point(s). See Figure 4 for a detailed example. We implemented this aspect by allowing different amounts of curl to be added on top of a multi-dimensional vector field in our unit tests, which is done by rotating the produced gradient vectors, using a fixed rotation matrix."}, {"heading": "2.6 Non-stationarity", "text": "In many settings it is necessary to optimize a non-stationary objective function. This may typically occur in a non-stationary task where the problem to be solved changes over time. However, nonstationary optimization can even be important in large stationary tasks, when the algorithm itself chooses to track a particular aspect of the problem, rather than solve the problem globally [21]. In addition, reinforcement learning (RL) tasks often involve non-stationary optimization. For example, many RL algorithms proceed by evaluating the value function using the TD algorithm described in the previous section. This results in two sources of non-stationarity: the target value changes at every step (resulting in the previously described curl); and also the state distribution changes as the value function improves and better actions are selected. These algorithms can be therefore be viewed as a non-stationary loss function, but whose optimum moves as a function of the current parameter values.\nTo test non-stationarity, we allow the location of the optimum to move smoothly via a translation of the parameter space, for various movement speeds, and for various levels of stochasticity in its movement. In order to pass these unit tests, an optimization algorithm must be able to successfully track the optimum. A type of non-stationarity that involves more abrupt switching is discussed in section 4.1."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Setup and Algorithms", "text": "For our experiments, we test the candidate algorithms on 1003 unit tests. Each algorithm-unit test pairing is repeated 10 times, but with reusing the same 10 random seeds across all algorithms and setups. For each run k, we compute the true expected loss at the parameter value reached after 100 update steps L(k) = E [ L (\n\u03b8 (k) 100\n)]\n.\nThe algorithms evaluated are SGD with fixed learning rate \u03b70 \u2208 [10\u22126, 10], SGD with annealing with decay factor in [10\u22122, 1] and initial rates \u03b70, SGD with momentum (regular or Nesterov\u2019s variant [22]) [0.1, 0.999] and initial rates \u03b70, SGD with parameter averaging [] with decay term in [10\u22124, 0.5] and exponent in { 12 , 3 4 , 1}, ADAGRAD [10] with initial rates \u03b70, ADADELTA [23] with decay parameter (1 \u2212 \u03b3) \u2208 [10\u22124, 0.5] and regularizer in [10\u22126, 10\u22122, the incremental delta-bardelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u03b70, RMSprop [26] with minimal learning rates \u03b70, maximal learning rates in [10, 103] and decay parameter \u03b3, as well as conjugate gradients. For the hyper-parameters ranges, we always consider one value per order of magnitude, and exhaustively sweep all combinations."}, {"heading": "3.2 Reference performance", "text": "Each unit test is associated with a reference performanceLsgd, and a corresponding reference learning rate \u03b7best, which are determined by doing a parameter sweep over all fixed learning rates for SGD (34 values spread uniformly on log-scale between 10\u221210 and 10), and retaining the best-performing one.\nIn our aggregate plots, unit tests are sorted (per group) by their reference learning rate, i.e., those that require small steps on the left, and those where large steps are best on the right. Algorithm setups are sorted as well, on the vertical axis, by their median performance on a reference unit test (quadratic, additive noise)."}, {"heading": "3.3 Qualitative Evaluation", "text": "The algorithm performance L(k) is converted to a normalized value\nL(k)norm = L(k) \u2212 Linit Lsgd \u2212 Linit\nwhere Linit = E[L(\u03b80)] is the expected loss value at the initial point. In other words, a normalized value near zero corresponds to no progress, negative denotes divergence, and a value near one is equivalent to the best SGD. Based on these results, we assign a qualitative color value to the performance of each algorithm setup on each unit test, to able to represent it in a single pixel in the resulting figures:\n\u2022 Red: Divergence or numerical instability in all run.\n\u2022 Violet: Divergence or numerical instability in at least one run.\n\u2022 Orange: Insufficient progress: median(Lnorm) < 0.1\n\u2022 Yellow: Good progress: median(Lnorm) > 0.1 and high variability: Lnorm < 0.1 for at least 14 of the runs. \u2022 Green: Good progress: median(Lnorm) > 0.1 and low variability: Lnorm < 0.1 for at most 14 of the runs. \u2022 Blue: Excellent progress: median(Lnorm) > 2."}, {"heading": "3.4 Results", "text": "Figure 5 shows the qualitative results of all algorithm variants on all 1003 unit tests. There is a wealth of information in these visualizations. For example the relatively scarce amount of blue\nFigure 5: Qualitative results for all algorithm variants (350) on all unit tests (800 one-dimensional ones and 200 multi-dimensional ones). Each column is one unit test, organized by type, and each group of rows is one algorithm, with one set of hyper-parameters per row. The color code is: red/violet=divergence, orange=slow, yellow=variability, green=acceptable, blue=excellent (see main text for details)\nindicate that it is difficult to substantially beat well-tuned SGD in performance on most unit tests. Another unsurprising conclusion is that hyper-parameter tuning matters much less for the adaptive algorithms (ADAGRAD, ADADELTA, RPROP, RMSprop) than for the non-adaptive SGD variants. Also, while some unit tests are more tricky than others on average, there is quite some diversity in the sense that some algorithms may outdo SGD on a unit test where other algorithms fail (especially on the non-differentiable functions)."}, {"heading": "4 Realism and Future Work", "text": "We do not expect this to replace real-world benchmark domains, but to complement them with our suite of unit tests. Still, it is important to have sufficient coverage of the types of potential difficulties encountered in realistic settings. To a much lesser degree, we may not want to clutter the test suite with unit tests that measure issues which never occur in realistic problems.\nIt is not straightforward to map very high-dimensional real-world loss functions down to lowdimensional prototype shapes, but it is not impossible. For example, in Figure 7 we show a few random projections in parameter space of the loss function in an MNIST classification task with an MLP [27]. We defer a fuller investigation of this type, namely obtaining statistics on how commonly different prototypes are occurring, to future work.\nHowever, the unit tests capture the properties of some examples that can be analyzed. One of them was discussed in section 2.5, another one is the simple loss function of a one-dimensional autoencoder:\nL\u03b8(x) = (x+ \u03b82 \u00b7 \u03c3(x \u00b7 \u03b81)) 2\nwhere \u03c3 is the sigmoid function. Even in the absence of noise, this minimal scenario has a saddlepoint near \u03b8 = (0, 0), a plateau shape away from the axes, a cliff shape near the vertical axis, and a correlated valley near \u03b8 = (1, 1) for example (see Figure 6). All of these prototypical shapes are included in our set of unit tests.\nAn alternative approach is predictive: if the performance on the unit tests is highly predictive of an algorithm\u2019s performance on a some real-world task, then those unit tests must be capturing the essential aspects of the task. Again, building such a predictor is an objective for future work."}, {"heading": "4.1 Algorithm Dynamics", "text": "Our long-term objective is to be able to do systematic testing and a full investigation of the optimization dynamics for a given algorithm. Of course, it is not possible to test it exhaustively on all\npossible loss functions (because there are infinitely many), but a divide-and-conquer approach may be the next best thing. For this, we introduce the notion of algorithm state st, which is changing during optimization (e.g., the current stepsize or momentum). Now, a long optimization process can be seen as the chaining of a number of unit tests, while preserving the algorithm state in-between them. Our hypothesis is that the set of all possible chains of unit tests in our collection covers most of the qualitatively different (stationary or non-stationary) loss functions an optimization algorithm may encounter.\nTo evaluate an algorithm\u2019s robustness (rather than its expected performance), we can assume that an adversary picks the worst-case unit tests at each step in the sequence. An algorithm is only truly robust if for any sequence of unit tests, when starting in s0, it almost surely does not diverge. Besides the worst-case, we may also want to study typical expected behavior, namely whether the dynamics have an attractor s\u2217 in the algorithm\u2019s state space. If an attractor exists where the algorithm is stable, then it becomes useful to look at the secondary criterion for the algorithm, namely its expected (normalized) performance. We conjecture that this analysis may lead to novel insights into how to design robust and adaptive optimization algorithms."}, {"heading": "5 Conclusion", "text": "This paper established a large collection of simple comparative benchmarks to evaluate stochastic optimization algorithms, on a broad range of small-scale, isolated, and well-understood difficulties. This approach helps disentangle issues that tend to be confounded in real-world scenarios, while\nretaining realistic properties. Our initial results on a dozen established algorithms (under a variety of different hyperparameter settings) show that robustness is non-trivial, and that different algorithms struggle on different unit tests. The testing framework is open-source, extensible to new function classes, and easy to use for evaluating the robustness of new algorithms."}, {"heading": "A Appendix: Framework Software", "text": "As part of this work a software framework was developed for the computing and managing all the results obtained for all the different configurations of function prototypes and algorithms. The main component of the system is a database where all the results are stored and can be easily retrieved by querying the database accordingly. The building blocks of this database are the individual experiments, where each experiment is associated to a unit test and an algorithm with fixed parameters. An instance of an experiment database can either be loaded from the disk, or it can be created on the fly by running the associated experiments as needed. The code below creates a database and runs all the experiments for all the readily available algorithms and default unit tests, and then saves them to disk:\nrequire \u2019experiment\u2019 local db = experimentsDB() db:runExperiments() db:save(\u2019experimentsDB\u2019)\nThis database now can be loaded from the disk, and the user can query it in order to retrieve specific experiments, using filters. An example is shown below:\nlocal db = experimentsDB() db:load(\u2019experimentsDB\u2019) local experiments = db:filter({fun={\u2019quad\u2019, \u2019line\u2019},\nalgo={\u2019sgd\u2019}, learningRate=1e-4})\nThe code above loads an experiment database from the disk and it retrieves all the experiments for all the quadratic and line prototype shapes, for all different types of noise and all scales, further selecting the subset of experiments to those optimized using SGD with learningRate equal to 1e4. The user can rerun the extracted experiments or have access to the associated results, i.e., the expected value of the function in different optimization steps, along with the associated parameters values. In order to qualitatively assess the results the following code can be used:\ndb:ComputeReferenceValues() db:cleanDB() db:assessPerformance() db:plotExperiments()\nThe code above computes the reference expected values for each prototype function, it removes the experiments for which no reference value is available, then it qualitatively assesses the performance of all the available experiments and finally it plots the results given the color configuration described in section 3.3. It is really easy to add a new algorithm in the database in order to evaluate its robustness. The code below illustrates a simple example:\ndb:addAlgorithm(algoname, algofun, opt) db:testAlgorithm(algoname) db:plotExperiments({}, {algoname})\nHere a new algorithm with name algoname, function instance algo (which should satisfy the optim interface), and a table of different parameter configurations opt is added to the database and it is tested under all available functions prototypes. Finally, the last line plots a graph with all the results for this algorithm.\nIt is also possible to add a set of new unit tests to the database, and subsequently run a set of experiments associated with them. There are different parameters to be defined for the creation of a set of unit tests (that allow wildcard specification too):\n1. the concatenated shape prototypes for each dimension,\n2. the noise prototype to be applied to each dimension,\n3. the scale of each dimension of the function,\n4. in case of multivariate unit tests, a parameter specifies which p-norm is used for the combination,\n5. a rotation parameter that induces correlation of the different parameter dimensions, and\n6. a curl parameter that changes the vector field of a multivariate function."}], "references": [{"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1951}, {"title": "Online Algorithms and Stochastic Approximations", "author": ["L\u00e9on Bottou"], "venue": "Online Learning and Neural Networks", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Large Scale Online Learning", "author": ["L\u00e9on Bottou", "Yann LeCun"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "The Tradeoffs of Large Scale Learning", "author": ["L\u00e9on Bottou", "Olivier Bousquet"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Adaptive Algorithms and Stochastic Approximations", "author": ["A. Benveniste", "M. Metivier", "P. Priouret"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Topmoumoute online natural gradient", "author": ["N. Le Roux", "P.A. Manzagol", "Y. Bengio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent", "author": ["Antoine Bordes", "L\u00e9on Bottou", "Patrick Gallinari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient Descent", "author": ["Wei Xu"], "venue": "ArXiv-CoRR, abs/1107.2490,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "No More Pesky Learning Rates", "author": ["Tom Schaul", "Sixin Zhang", "Yann LeCun"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John C. Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Understanding the exploding gradient problem", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Real-parameter black-box optimization benchmarking 2010", "author": ["Nikolaus Hansen", "Anne Auger", "Steffen Finck", "Raymond Ros"], "venue": "Experimental setup", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Comparing results of 31 algorithms from the black-box optimization benchmarking BBOB-2009", "author": ["Nikolaus Hansen", "Anne Auger", "Raymond Ros", "Steffen Finck", "Petr Po\u0161\u0131\u0301k"], "venue": "In Proceedings of the 12th annual conference companion on Genetic and evolutionary computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Efficient BackProp", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Temporal-difference methods and Markov models", "author": ["Etienne Barnard"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "On the role of tracking in stationary environments", "author": ["Richard S. Sutton", "Anna Koop", "David Silver"], "venue": "In Proceedings of the Twenty-Fourth International Conference on Machine Learning (ICML", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Interior-point polynomial algorithms in convex programming, volume 13", "author": ["Yurii Nesterov", "Arkadii Semenovich Nemirovskii"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Adapting bias by gradient descent: An incremental version of delta-bardelta", "author": ["Richard S Sutton"], "venue": "In AAAI,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1992}, {"title": "A direct adaptive method for faster backpropagation learning: The RPROP algorithm", "author": ["Martin Riedmiller", "Heinrich Braun"], "venue": "In Neural Networks,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1993}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T Tieleman", "G Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Stochastic optimization [1] is among the most widely used components in large-scale machine learning, thanks to its linear complexity, efficient data usage, and often superior generalization [2, 3, 4].", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "Stochastic optimization [1] is among the most widely used components in large-scale machine learning, thanks to its linear complexity, efficient data usage, and often superior generalization [2, 3, 4].", "startOffset": 191, "endOffset": 200}, {"referenceID": 2, "context": "Stochastic optimization [1] is among the most widely used components in large-scale machine learning, thanks to its linear complexity, efficient data usage, and often superior generalization [2, 3, 4].", "startOffset": 191, "endOffset": 200}, {"referenceID": 3, "context": "Stochastic optimization [1] is among the most widely used components in large-scale machine learning, thanks to its linear complexity, efficient data usage, and often superior generalization [2, 3, 4].", "startOffset": 191, "endOffset": 200}, {"referenceID": 4, "context": "In this context, numerous variants of stochastic gradient descent have been proposed, in order to improve performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9].", "startOffset": 155, "endOffset": 170}, {"referenceID": 5, "context": "In this context, numerous variants of stochastic gradient descent have been proposed, in order to improve performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9].", "startOffset": 155, "endOffset": 170}, {"referenceID": 6, "context": "In this context, numerous variants of stochastic gradient descent have been proposed, in order to improve performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9].", "startOffset": 155, "endOffset": 170}, {"referenceID": 7, "context": "In this context, numerous variants of stochastic gradient descent have been proposed, in order to improve performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9].", "startOffset": 155, "endOffset": 170}, {"referenceID": 8, "context": "In this context, numerous variants of stochastic gradient descent have been proposed, in order to improve performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9].", "startOffset": 155, "endOffset": 170}, {"referenceID": 9, "context": "These algorithms may derive from simplifying assumptions on the optimization landscape [10], but in practice, they tend to be used as general-purpose tools, often outside of the space of assumptions their designers intended.", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "The troublesome conclusion is that practitioners find it difficult to discern where potential weaknesses of new (or old) algorithms may lie [11], and when they are applicable \u2013 an issue that is separate from raw performance.", "startOffset": 140, "endOffset": 144}, {"referenceID": 11, "context": "This results in essentially a trial-and-error procedure for finding the appropriate algorithm variant and hyper-parameter settings, every time that the dataset, loss function, regularization parameters, or model architecture change [12].", "startOffset": 232, "endOffset": 236}, {"referenceID": 12, "context": "This is a similar approach to the very fruitful one taken by the black-box optimization community [13, 14].", "startOffset": 98, "endOffset": 106}, {"referenceID": 13, "context": "This is a similar approach to the very fruitful one taken by the black-box optimization community [13, 14].", "startOffset": 98, "endOffset": 106}, {"referenceID": 14, "context": "All of these occur in realistic learning scenarios, for example in logistic regression the loss surface is part concave and part convex, an MSE loss is the prototypical quadratic bowl, but then regularization such as L1 introduces non-differentiable bends (as do rectified-linear or maxout units in deep learning [15, 16]).", "startOffset": 313, "endOffset": 321}, {"referenceID": 10, "context": "Steep cliffs in the loss surface are a common occurrence when training recurrent neural networks, as discussed in [11].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "In many learning problems, effort is put into proper normalization [17], but that is insufficient to guarantee homogeneous scaling all throughout the layers of a deep neural network.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "This mimics both training with drop-out [18], and scenarios with rectified linear units where a unit will be inactive for some input samples, but not for others.", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "it pulls the value of the current state towards the value of its successor state [19].", "startOffset": 81, "endOffset": 85}, {"referenceID": 18, "context": "These stochastic update directions are not proper gradients of any scalar energy field [20], but they still form a (more general) vector field with non-zero curl, where the objective for the optimization algorithm is to converge to its fixed-point(s).", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "However, nonstationary optimization can even be important in large stationary tasks, when the algorithm itself chooses to track a particular aspect of the problem, rather than solve the problem globally [21].", "startOffset": 203, "endOffset": 207}, {"referenceID": 9, "context": "The algorithms evaluated are SGD with fixed learning rate \u03b70 \u2208 [10, 10], SGD with annealing with decay factor in [10, 1] and initial rates \u03b70, SGD with momentum (regular or Nesterov\u2019s variant [22]) [0.", "startOffset": 63, "endOffset": 71}, {"referenceID": 9, "context": "The algorithms evaluated are SGD with fixed learning rate \u03b70 \u2208 [10, 10], SGD with annealing with decay factor in [10, 1] and initial rates \u03b70, SGD with momentum (regular or Nesterov\u2019s variant [22]) [0.", "startOffset": 63, "endOffset": 71}, {"referenceID": 9, "context": "The algorithms evaluated are SGD with fixed learning rate \u03b70 \u2208 [10, 10], SGD with annealing with decay factor in [10, 1] and initial rates \u03b70, SGD with momentum (regular or Nesterov\u2019s variant [22]) [0.", "startOffset": 113, "endOffset": 120}, {"referenceID": 0, "context": "The algorithms evaluated are SGD with fixed learning rate \u03b70 \u2208 [10, 10], SGD with annealing with decay factor in [10, 1] and initial rates \u03b70, SGD with momentum (regular or Nesterov\u2019s variant [22]) [0.", "startOffset": 113, "endOffset": 120}, {"referenceID": 20, "context": "The algorithms evaluated are SGD with fixed learning rate \u03b70 \u2208 [10, 10], SGD with annealing with decay factor in [10, 1] and initial rates \u03b70, SGD with momentum (regular or Nesterov\u2019s variant [22]) [0.", "startOffset": 192, "endOffset": 196}, {"referenceID": 9, "context": "5] and exponent in { 1 2 , 3 4 , 1}, ADAGRAD [10] with initial rates \u03b70, ADADELTA [23] with decay parameter (1 \u2212 \u03b3) \u2208 [10, 0.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "5] and exponent in { 1 2 , 3 4 , 1}, ADAGRAD [10] with initial rates \u03b70, ADADELTA [23] with decay parameter (1 \u2212 \u03b3) \u2208 [10, 0.", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "5] and regularizer in [10, 10, the incremental delta-bardelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u03b70, RMSprop [26] with minimal learning rates \u03b70, maximal learning rates in [10, 10] and decay parameter \u03b3, as well as conjugate gradients.", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "5] and regularizer in [10, 10, the incremental delta-bardelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u03b70, RMSprop [26] with minimal learning rates \u03b70, maximal learning rates in [10, 10] and decay parameter \u03b3, as well as conjugate gradients.", "startOffset": 91, "endOffset": 95}, {"referenceID": 24, "context": "5] and regularizer in [10, 10, the incremental delta-bardelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u03b70, RMSprop [26] with minimal learning rates \u03b70, maximal learning rates in [10, 10] and decay parameter \u03b3, as well as conjugate gradients.", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "5] and regularizer in [10, 10, the incremental delta-bardelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u03b70, RMSprop [26] with minimal learning rates \u03b70, maximal learning rates in [10, 10] and decay parameter \u03b3, as well as conjugate gradients.", "startOffset": 194, "endOffset": 202}, {"referenceID": 9, "context": "5] and regularizer in [10, 10, the incremental delta-bardelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u03b70, RMSprop [26] with minimal learning rates \u03b70, maximal learning rates in [10, 10] and decay parameter \u03b3, as well as conjugate gradients.", "startOffset": 194, "endOffset": 202}], "year": 2013, "abstractText": "Optimization by stochastic gradient descent is an important component of many large-scale machine learning algorithms. A wide variety of such optimization algorithms have been devised; however, it is unclear whether these algorithms are robust and widely applicable across many different optimization landscapes. In this paper we develop a collection of unit tests for stochastic optimization. Each unit test rapidly evaluates an optimization algorithm on a small-scale, isolated, and well-understood difficulty, rather than in real-world scenarios where many such issues are entangled. Passing these unit tests is not sufficient, but absolutely necessary for any algorithms with claims to generality or robustness. We give initial quantitative and qualitative results on a dozen established algorithms. The testing framework is open-source, extensible, and easy to apply to new algorithms.", "creator": "LaTeX with hyperref package"}}}