{"id": "1312.6594", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Sequentially Generated Instance-Dependent Image Representations for Classification", "abstract": "In this letter, we alleged, it accord for image classification should prototyped generates inverse singular. Our way way based took same gearbox process that learns alone perspectives way separate other of any touch year order to infer its maximum. In particular, the choice of are a specific take their image, j. it followed actual content some listed selected regions. The capacity of itself system even either compile image information often many as main non-invasive autonomous best either the problem to learn well in revenues measurement possibilities which exploiting a dynamicly generated terms over each pictures. We utmost held create ' rose abilities day also starting of presents - management consortium made classification tasks that reflects its learned exploration over frequentist abilities.", "histories": [["v1", "Fri, 20 Dec 2013 16:36:40 GMT  (7524kb,D)", "https://arxiv.org/abs/1312.6594v1", null], ["v2", "Tue, 7 Jan 2014 14:44:42 GMT  (3587kb,D)", "http://arxiv.org/abs/1312.6594v2", null], ["v3", "Tue, 11 Feb 2014 17:07:21 GMT  (6918kb,D)", "http://arxiv.org/abs/1312.6594v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["gabriel dulac-arnold", "ludovic denoyer", "nicolas thome", "matthieu cord", "patrick gallinari"], "accepted": true, "id": "1312.6594"}, "pdf": {"name": "1312.6594.pdf", "metadata": {"source": "CRF", "title": "Sequentially Generated Instance-Dependent Image Representations for Classification", "authors": ["Gabriel Dulac-Arnold", "Ludovic Denoyer", "Nicolas Thome", "Matthieu Cord", "Patrick Gallinari"], "emails": ["firstname.lastname@lip6.fr"], "sections": [{"heading": null, "text": "000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054\n055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109"}, {"heading": "1. Introduction", "text": "Many computer vision models are developped with a specific image classification task in mind, adapted to a particular representation such as the bag-of-words (BoW) model or low-level local features (Sivic & Zisserman, 2003; van Gemert et al., 2010). In these representations, all the image information is used to take the decision, even in the spatial BoW extension of Lazebnik et al. (Lazebnik et al., 2006).\nHowever, as pointed out in recent work, humans do not need to consider the entire image to be able to interpret it (Sharma et al., 2012). On the contrary, humans are able to rapidly pick out the important regions of an image necessary to interpret it. This fact suggests that concentrating on specific subset of image regions in an intelligent manner should be sufficient to properly classify an image. In addition to simply selecting regions of an image, our system\nFigure 1. Illustration of our classification framework for two test images (first and second line): According to the content of the center region, the next region to visit is selected (red arrow). Again, depending on the two first regions\u2019 contents, a third one is selected, and so on. After B iterations, the final classification is achieved. As the first region is the same on both images, the second region explored is the same, but as these new regions\u2019 contents differs, the next regions considered by the algorithm are different.\ncan actively decide to consider certain regions of an image in more detail by increasing the BoW resolution for specific sub-regions of an image. This allows the system to adaptively use more or less resources when classifying images of varying complexity. Similar performance-oriented goals have been recently put forward by Karayev et al. (Karayev et al., 2012).\nImportantly, this process is instance-specific, allowing the algorithm to adapt the choice of regions for each processed image. We are able to learn such a model by leveraging the datum-Wise classification framework (Dulac-Arnold et al., 2012a), which is able to learn adaptive classification policies using reinforcement learning (RL). We show that during inference, a significant speed-up is obtained by only computing the local features on the selected regions, while preserving acceptable inference accuracy w.r.t. fullinformation models. The rest of the paper is organized as follows. Section 2 presents related works and highlights\nar X\niv :1\n31 2.\n65 94\nv3 [\ncs .C\nV ]\n1 1\nFe b\n110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 our contributions. Section 3 gives the theoretical background of our sequential model, and Section 4 details the training algorithm. Finally, Section 5 reports classification scores on two challenging image datasets."}, {"heading": "2. Background", "text": "The standard image classification pipeline follows three steps (Boureau et al., 2010) \u2014 (i) low-level local descriptor extraction, (ii) coding, and (iii) pooling \u2014 to get feature vectors that are then used for classification. This strategy was definitively popularized in computer vision with the bag-of-words formalism using SIFT local features (Sivic & Zisserman, 2003). Alternatives to the standard coding scheme have been proposed, such as local soft coding (Liu et al., 2011) or sparse coding (Boureau et al., 2010). After the coding phase, most traditional BoW approaches use sum pooling or max pooling. The spatial pyramid matching (SPM) strategy (Lazebnik et al., 2006) extends the pooling by considering a fixed predetermined spatial image pyramid. Many feature detectors have been proposed to get salient areas, affine regions, and points of interest (Mikolajczyk & Schmid, 2005) on images. However, in contrast to the task of matching a specific target image or object, methods for category classification show better performance when using a uniform feature sampling over a dense grid on the image (Chatfield et al., 2011). Other approaches studying are motivated by human eye fixation or salient object detection (Borji et al., 2012; Chang et al., 2011). Recently, several approaches combine dense sampling and saliency map or spatial weighting to obtain powerful image representations (Su & Jurie, 2012; Feng et al., 2011). Sharma et al. (Sharma et al., 2012) proposes a scheme to learn discriminative saliency maps at an image region level. They use the SPM scheme and apply weights to each block of the pyramid to get a global saliency map. In the case of multiclass classification, the weights are learned for each class in a discriminative way using one-against-all binary classification, and the final decision depends on the image content using a latent SVM representation. Other methods based on latent SVM formulation also attempt to jointly encode spatial and content information in image classification. Parizi et al. (Parizi et al., 2012) introduce a reconfigurable model where each region is equiped with a latent variable representing a topic, such that only regions with similar topics are matched together in the final representation. This model provides a flexible framework, overcoming the shortcoming of the fixed spatial grid used in SPM. In all these approaches, the whole image has to be processed and all the information is used to classify, even if some regions of the image contain some misleading or irrelevant contents. We propose a strategy to overcome these limitations: we avoid processing the whole image by focusing only on the most pertinent regions relative to the image classification task. In a way more drastic than Sharma\u2019s approach (Sharma et al., 2012), we constrain our system to take a decision by considering only a fixed number of regions of the test image, thus allowing the computation of the local features to be significantly reduced. The most important aspect of our method is the region selection model. In short, our model is effectively a learned sequential decision policy that sequentially chooses the best region to visit given a set of previously visited regions. Both the locality and actual contents of a region are used in a joint manner to represent the set of visited regions. Inspired by reinforcement learning algorithms, we propose a dedicated algorithm to learn the region selection policy used in our image classification task. More recent work uses similar techniques to find optimal orders for anytime object detection tasks (Karayev et al., 2012). Similar work has been presented that uses a foveal glimpse simulation, but the learning approach is quite different (Larochelle & Hinton, 2010). Sequential learning techniques have been recently applied to different standard classification tasks. In (Dulac-Arnold et al., 2012b), the authors propose to use leinforcement learning models for learning sparse classifiers on vectors, (Busa-Fekete et al., 2012) use sequential techniques for learning a cascade of classifiers depending on the content of the inputs, while (Dulac-Arnold et al., 2011) is an application of sequential learning models to text classification. Finally, (Ru\u0308ckstie\u00df et al.) propose a generic model able to minimize the data consumption with sequential online feature selection. If our approach shares some common ideas with these recent works, we propose an original method that has been developed to handle the specific problem of classifying images using a small set of regions and a new learning algorithm which is efficient both in term of speed and performance. To summarize, the contributions presented in this paper are as follows: \u2022 We propose a sequential model that, given an image, first selects a subset of relevant regions in this image, and then classifies it. The advantages of such a method are: (i) The classification decision is based only on the features of the acquired regions, resulting in a speedup of the classification algorithm during inference. (ii) The algorithm is able to ignore misleading or irrelevant regions. (iii) The way regions are selected de-\n220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273\n275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329\npends both on the position but also on the content of the regions, resulting in a model able to adapt its behavior to the content of each image being classified. (iv) At last, the model is a multiclass model and the regions selection policy is learned globally for all the classes while other existing methods usually apply a category-specific region selection scheme. \u2022 We propose a new learning algorithm inspired from reinforcement learning techniques adapted to the particular problem faced here. \u2022 We present an experimental evaluation of this method on three different classical datasets and propose a qualitative study explaining the behaviour of this model."}, {"heading": "3. Classification model", "text": ""}, {"heading": "3.1. Notations", "text": "Let us denote X the set of possible images and Y the discrete set of C categories. A classifier is a parametrized function f\u03b8 such that f\u03b8 : X \u2192 Y where f\u03b8(x) = y means that category1 y has been predicted for image x. To learn f\u03b8, a set of ` labeled training images Strain = {(x1, y1), ..., (x`, y`)} is provided to the system. We also consider for x a fixed grid N \u00d7 M of regions {rxi }i \u2264N\u00d7M where rxi is the i-th region as illustrated in Fig. 2 (left). The set of all possible regions is denoted R, and R(x) corresponds to the set of regions over image x.2 rxi is represented by a feature vector \u03c6(r x i ) of size K. We use a SIFT bag-of-words representation in our experiments."}, {"heading": "3.2. Model formalization", "text": "The classifier is modeled as a sequential decision process that, given an image, first sequentially selects regions, and then classifies the image using the information available in the visited regions. At each step, the classifier has already selected a sequence of regions denoted (sx1 , .., s x t ) where sxt is the index of the region of x selected at step t. The sequence (sx1 , .., s x t ) is thus a representation tailored to the specific image and the current classification task. S(x) denotes the set of all possible trajectories over image x and St(x) the trajectories composed of t selected regions. Given a fixed budget B, new regions are acquired resulting in a trajectory of size B. Given this trajectory, the classifier then decides which category to assign to the image. There are two important aspects of our approach: First, the way 1We consider in this paper the case of monolabel classification where one input is associated to exactly one possible category. 2Note that all the images have the same N \u00d7M number of regions.\nFigure 2. (left) Index of the regions of an image decomposed of 4 \u00d7 4 regions. (right) Example of possible trajectory (6, 10, 11, 2, 7).\nthese regions are acquired depends on the content of the previously acquired regions \u2014 c.f. Section 3.2.3 \u2014 resulting in a classifier that is able to adapt its representation to each image being classified, thus selecting the best regions for each image. Second, the final decision is made given the features of the acquired regions only, without needing the computation of the features for the other regions, thus resulting in both a speed-up of the classification process \u2014 not all features have to be computed \u2014 but also, for some cases as described in Section 5, in an improvement of the classification rate due to the exclusion of noisy regions. We now give details concerning the features, the classification phase \u2014 which classifies the image given the B previously selected regions \u2014 and the exploration phase \u2014 which selects B \u2212 1 additional regions3 over an image to classify."}, {"heading": "3.2.1. FEATURE FUNCTION", "text": "As previously explained, the \u03a6 function aims at aggregating the content of already visited regions. Based on the K length vector \u03c6(rxi ), we consider the following \u0393 mapping into a larger space of size K\u00d7 (N \u00d7M) as in (Parizi et al., 2012): \u0393(\u03c6(rxi ),K) = (0 . . . 0 \u03c6(r x i ) 0 . . . 0) T where \u03c6(rxi ) is positioned at index i\u00d7K. The global feature function \u03a6 is therefore defined as: \u03a6(sx1 , ...s x t ) = t\u2211 i=1 \u0393(\u03c6(rxsxi ),K). (1) The goal of such a transformation is to conserve the information concerning the index of the region, which corresponds to the actual position of the region within the source image."}, {"heading": "3.2.2. CLASSIFICATION PHASE", "text": "The classification phase consists in classifying an image given B acquired regions denoted (sx1 , ...s x B). First, this 3Note that we consider that the first region acquired by the classifier is a predefined central region of the image.\n330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383\n385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 set is transformed to a global feature vector that aggregates the individual features of each of its regions using \u03a6. The classification is performed by using a classification function denoted f\u03b8 defined as: f\u03b8 : { RK\u00d7N\u00d7M \u2192 Y f\u03b8(\u03a6(s x 1 , ...s x B)) = y , (2) where y is the predicted category. \u03b8 is the set of parameters that is learned using the training set as described in Section 4. Note that this function is computed by using as an input the vectorial representation of the sequence of regions \u03a6(sx1 , ...s x B)."}, {"heading": "3.2.3. EXPLORATION PHASE", "text": "In order to sequentially acquire the different regions of an image, the classification process follows an exploration policy denoted \u03c0\u03b3 where \u03b3 is the set of parameters of this policy. A specific exploration policy is used at each timestep, and as such, \u03c0 can be decomposed into a sequence of sub-policies \u03c0 = (\u03c01, \u03c02, ..., \u03c0B\u22121) such that \u03c0t computes the region to acquire, at time t, given a sequence of t previously acquired regions: \u2200t, \u03c0t : { RK\u00d7N\u00d7M \u2192 N \u00d7M \u03c0t(\u03a6(sx1 , ..., s x t )) = s x t+1 , (3) where sxt+1 is the index of the next region r x sxt+1 to acquire. \u03c0t can be viewed as a multiclass classifier which predicts the index of the next region to acquire, given the features of the previously acquired regions, and \u03c0t is restricted to predicting the index of a region that has not been previously acquired. In this paper, we consider t policies \u03c0\u03b3t parametrized by \u03b3t. Similarly to the classification function f\u03b8, \u03c0\u03b3t takes as an input the vectorial representation of the current sequence of acquired regions \u03a6(sx1 , .., s x t ) and outputs a region index to be considered. We define this policy as a multiclass one-against-all hinge loss perceptron in this paper, but any multiclass classifier such as an SVM or a neural networks could be used as well."}, {"heading": "3.2.4. FINAL INFERENCE POLICY", "text": "A complete classifier policy is defined by both an exploration policy (\u03c01, ..., \u03c0B\u22121) plus a classification policy f\u03b8. The final inference process is described in Algorithm 1 and consists in sequentially acquiring new regions (lines 1\u20134) and then computing the predicted category using the previously acquired regions (line 5)."}, {"heading": "4. Learning Algorithm", "text": "The idea of the learning algorithm is the following: the classification policy is learned starting from the end. Algorithm 1 Inference Algorithm: only B regions are acquired for classification Require: B: budget Require: (\u03c01, ..., \u03c0B\u22121): exploration policy Require: f\u03b8: classification policy Require: x: input image 1: Acquire region sx1 i.e. the central region of the image 2: for i = 1 to B \u2212 1 do 3: Acquire region sxi+1 using \u03c0\u03b3i(\u03a6(s x 1 , ..., s x i )) 4: end for 5: Compute category y = f\u03b8(\u03a6(sx1 , ..., s x B)) 6: return y Algorithm 2 Complete Learning algorithm Require: (x1, ..., x`): Training set of images Require: (y1, ..., y`): Training labels 1: Learn f\u03b8 using algorithm 3 2: for k = B \u2212 1 to 1 do 3: Use previoulsy learned sub-policies \u03c0k+1\u03b3 , .., \u03c0 B\u22121 \u03b3 , f\u03b8 to learn \u03c0\u03b3k using algorithm 4 4: end for 5: return Final policy: (\u03c0\u03b31 , ...\u03c0\u03b3B\u22121 , f\u03b8) We begin by first learning f\u03b8 and then we sequentially learn \u03c0\u03b3B\u22121 , \u03c0\u03b3B\u22122 , up to \u03c0\u03b31 . The underlying idea is to begin by learning a good f\u03b8 classification policy able to obtain good performance given any subset of B regions. The learning of \u03c0\u03b3B\u22121 , \u03c0\u03b3B\u22122 , ... to \u03c0\u03b31 aims at acquiring relevant regions i.e. regions that will help f\u03b8 to take the right decision. The complete learning algorithm is given in Algorithm 2 and provides the general idea behind our method. At each iteration of the algorithm, a set of learning states is sampled from the training images using a uniform random distribution. For each sampled state, the previously learned sub-policies are then used to simulate \u2013 using Monte Carlo techniques \u2013 the behavior of the algorithm and thus to provide supervision to the sub-policy we are learning. The detailed process is given in Sections 4.1 and 4.2. Note that this learning algorithm is original and derived from both the rollout classification policy iteration (RCPI) method and the Fitted-Q Learning model that are generic Reinforcement Learning models proposed in (Dimitrakakis & Lagoudakis, 2008) and (Ernst et al., 2005). Our method is an adaptation of these two classical algorithms in the particular case described here.\n440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493\n495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549\nAlgorithm 3 Classification Policy Learning Algorithm Require: (x1, ..., x`): Training set of images Require: (y1, ..., y`): Training labels Require: B: Budget Require: n: 1: T = {} {Training set} 2: {For each training image} 3: for xi do 4: for k = 1 to n do 5: Sample B regions (sxi1 , ..., s xi B ) using random exploration policy (\u03c0random, ..., \u03c0random) 6: T \u2190 T \u22c3 (\u03a6(sxi1 , ..., s xi B ), yi) 7: end for 8: end for 9: Learn f\u03b8 on T using a classical learning algorithm 10: return f\u03b8"}, {"heading": "4.1. Learning the Classification Policy", "text": "Our approach for learning the classification policy is described in Algorithm 3. The underlying idea is to automatically learn from the training set a classifier which is optimal for classifying any subset of B regions for any input image. The classification policy f\u03b8 is learned on a large training set of images which have had B regions uniformly sampled \u2014 lines 5 and 6 of Alg. 3 \u2014 over the training images. Each set of regions is transformed to a feature vector using the \u03a6 function and provided to the learning algorithm using the label of the image as the supervision. This set of regions and labels is used to train f\u03b8 \u2013 line 9. At the end of the process, f\u03b8 is a classifier able to properly predict the category of any image given any randomly sampled set of B regions. As explained in the next section, the goal of learning the exploration policy is to improve the quality of f\u03b8 by finding a good representation instead of using a uniform sampling approach. This is done by finding a region selection policy that provides image-specific subsets of B image regions that are most likely to increase the classifier\u2019s classification accuracy."}, {"heading": "4.2. Learning the Optimal Exploration Policy", "text": "Consider now that f\u03b8 has been properly learned. Given a new image x, using only f\u03b8 applied to a uniformly sampled set of regions has one main drawback: for some sampled sets, f\u03b8 will certainly predict the right classification label, but for other samples, it will make a classification error, particularly for samples that contain irrelevant or misleading regions. The goal of the exploration policy \u03c0 is thus to provide f\u03b8 with a set of good regions i.e. a set of regions on which the classification function will predict the correct category. In other words, \u03c0 aims at reducing the error rate\nFigure 3. Illustration of the learning algorithm for \u03c0\u03b3B\u22121 . On a sample (sx1 , ...sxB\u22121) of B \u2212 1 regions, all remaining regions sxB are considered and classified by f\u03b8 (left) (simulation step, lines 6-8 of Algorithm 4). For some regions \u2013 the first one here \u2013 f\u03b8 computes the right label, for other regions f\u03b8 makes a prediction error (line 9 of Alg. 4). The regions on which f\u03b8 provides the good label are considered as training examples for learning \u03c0\u03b3B\u22121 (line 10).\nof f\u03b8 by changing the way regions are sampled. The complete learning method is given in Algorithm 4. Given this principle, the idea of how to learn \u03c0 is as follows: consider the case where f\u03b8 has been learned and we are currently learning \u03c0\u03b3B\u22121 i.e. the sub-policy that aims at acquiring the B-th and final region. Given any sample of B \u2212 1 regions (sx1 , ...sxB\u22121), \u03c0\u03b3B\u22121 can decide to acquire any of the remaining regions. If it acquires some of these regions, the classification policy will predict the right category while, for some other regions, f\u03b8 will predict the incorrect category as illustrated in Fig. 3. The method we propose consists thus in simulating the decision of f\u03b8 over all the possible B-th regions that can be acquired by \u03c0\u03b3B\u22121 , and to use the regions that correspond to a good final classification decision as supervised examples for learning \u03c0\u03b3B\u22121 . The result of this learning is to obtain a sub-policy that tends to select regions for which f\u03b8 will be able to properly predict. The same type of reasoning can be also used for learning the other policies i.e. \u03c0\u03b3B\u22122 will be learned in order to improve the quality of the sub-policy (\u03c0\u03b3B\u22121 , f\u03b8), \u03c0\u03b3B\u22123 will be learned in order to improve the quality of the subpolicy (\u03c0\u03b3B\u22122 , \u03c0\u03b3B\u22121 , f\u03b8) and so on. When learning subpolicy \u03c0\u03b3t , we first start by building a simulation set where each element is an image represented by t \u2212 1 randomly sampled regions (line 5). Then, for each element, we test each possible remaining regions (lines 6-7) by simulating the previously learnt sub-policies (\u03c0\u03b3t+1 , ..., f\u03b8) (line 8). We thus build a training set (lines 9-10) that is used to learn \u03c0\u03b3t (line 15).\n550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603\n605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 Algorithm 4 Exploration Sub-policy \u03c0\u03b3k Learn. Algorithm Require: (\u03c0k+1\u03b3 , ..., \u03c0B\u22121\u03b3 , f\u03b8): previously learned subpolicies Require: (x1, ..., x`): Training set of images Require: (y1, ..., y`): Training labels 1: T = {} {Training set} 2: {For each training image} 3: for xi do 4: for k = 1 to n do 5: Sample k \u2212 1 regions (sxi1 , ..., s xi k ) using random exploration policy 6: {For each region that has not been acquired} 7: for sxik+1 /\u2208 s xi 1 , ..., s xi k do 8: Use sub-policies (\u03c0k+1\u03b3 , ..., \u03c0 B\u22121 \u03b3 , f\u03b8) from (sxi1 , ..., s xi k , s xi k+1) to compute y\u0302 9: if y\u0302 == yi then 10: T \u2190 T \u22c3 (\u03a6(sxi1 , ..., s xi k ), s xi k+1) 11: end if 12: end for 13: end for 14: end for 15: Learn \u03c0k\u03b3 on T using a classical learning algorithm 16: return \u03c0k\u03b3"}, {"heading": "4.3. Complexity", "text": "The learning complexity of our method is the following: in order to simulate the behavior of the different sub-policies, we have to compute the features over all the regions of the training images. Moreover, for each training image, many samples of regions will be built (line 5 of Algorithms 3 and 4). Let us denote ` the number of training images and k the number of sample built for each image, at each step of the learning. The final complexity4 is O(N \u00d7M \u00d7 C + B \u00d7R(`\u00d7 k)) where R(n) corresponds to the complexity of learning over n examples. R depends on the machine learning algorithm used for representing f\u03b8 and \u03c0. If we consider a classical BoW model, this complexity becomes O(N\u00d7M\u00d7C+R(`)). Our method is thus slower to learn than standard models but still reasonable enough to allow the model to be learnt on large datasets."}, {"heading": "5. Experiments", "text": "We evaluate the proposed method on two challenging image databases corresponding to different tasks: finegrained image classification (People Playing Musical Instruments dataset5) (Yao & Fei-Fei, 2010) and scene 4We do not consider the complexity of the simulation phase of the algorithm i.e. line 8 of Algorithm 4 which is usually negligible w.r.t the other factors of the learning method. 5http://ai.stanford.edu/ bangpeng/ppmi.html recognition (15-scenes dataset) (Lazebnik et al., 2006). Let us detail the experimental setup by presenting the low-level image features and the chosen vectorial representation for each region. We densely extract gray SIFT descriptors using the VLFEAT library (Vedaldi & Fulkerson, 2010). These local features are computed at a single scale (s = 16 pixels) and with a constant step size d. For 15-scenes we use d = 8 pixels while d = 4 pixels for PPMI. Each region is represented by a BoW vector generated from SIFT descriptors (Sivic & Zisserman, 2003). We run a K-Means algorithm by randomly sampling about 1 million descriptors in each database to produce a dictionnary of M = 200 codeword elements. Each SIFT is then projected on the dictionary using hard assignement, and the codes are aggregated with sum pooling. The histogram is further `2 normalized. Finally, we take the square root of each element thus generating a Bahttacharyya kernel feature map. The standard learning algorithm for f\u03b8 and \u03c0\u03b3 is a oneagainst-all hinge-loss perceptron learned with a gradient descent algorithm. The gradient descent step and number of iterations have been tuned over the training set in order to maximize the accuracy of the classifier. We have chosen to create 10 sequences of regions for each training images, resulting in a training set of size 10 \u00d7 ` for each classifier \u2013 f\u03b8 and \u03c0\u03b3t \u2013 learned by our method. Performance with more samples has been computed but is not reported here since it is equivalent to the one obtained with 10 samples per image."}, {"heading": "5.1. Experimental Results", "text": "In order to evaluate the performance of our method, we use the standard metrics for the two considered databases: multi-class accuracy for 15-scenes, and average accuracy over the 12 independently learned binary tasks for PPMI6. We randomly sample training/testing images on 5 splits of the data, and the final performance corresponds to the average performance obtained on the 5 runs. We run experiments with different values of B. The baseline model performance is the one obtained where B = 16 i.e. all the regions are acquired by the model and the classification decision is based on the whole image. The results obtained with such a baseline approach are on par with previously published state-of-the art performances with a similar setup. For example, we reach 77.7% accuracy in the 15-Scene database, exactly matching the performances reported in (Parizi et al., 2012) 7. Although absolute per6Note that we do not report MAP metrics for PPMI, since our model produces a category label but not a score for each image. 7Their SBoW method matches our pipeline: mono-scale SIFT extracted with the same step size, same dictionary size, same coding/pooling schemes on a 4\u00d7 4 grid, and normalization policy.\n660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713\n715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769\n0.0 0.2 0.4 0.6 0.8 1.0 % of regions 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 a cc u ra cy Learned Random Guitar\nFigure 4. Accuracy on the Guitar dataset from PPMI with varying values of B.\nformance can still be improved using more advanced lowlevel feature extraction or mid-level feature embedding, our main purpose here is to validate the relative performance advantage of the proposed method. Figures 5 and 6 show the accuracy obtained for the the PPMI dataset and the 15 Scenes dataset. These figures present two measures: the performance obtained using a uniformly sampled subset of B regions using f\u03b8 \u2014 in red \u2014, and the performance when the regions are sampled following the learned exploration policy \u03c0\u03b3 \u2014 in blue. Performance on a small set of regions vs performance of the baseline model: When comparing the accuracy of our method with B < 16 to the performance of the baseline method (B = 16), one can see that given a reasonable value of B, our model is competitive with the classical approach. For example, acquiring B = 10 for the 15 Scenes dataset and for B = 8 for the PPMI dataset allows one to obtain accuracy which is similar to the model withB = 16. This means that our method is able to classify as well as standard approach using only half of the regions. Moreover, on PPMI, for B = 8, 10 and 12, our model clearly outperforms the baseline. This illustrates the ability of the learning algorithm to focus on relevant areas of the image, ignoring noisy or misleading regions. The generalization capacity of the prediction is thus improved. Learned exploration policy vs random exploration policy: Now, when comparing the performance of random exploration policy w.r.t. learned exploration policy, one can see that in almost all cases, the learned version is equivalent or better than the random one. For example, on the PPMI corpus with B = 8, learning how to acquire the regions allow one to obtain an improvement of about 4% in term of accuracy. This shows that our model is able, particularly on the PPMI dataset, to discover relevant regions depending on their content. This improvement is less important\n60 62 64 66 68 70 72 74 76 78 80 2 4 6 8 10 12 14 16 A cc u ra cy B Random Exploration Policy Learned Exploration Policy\nFigure 5. Mean accuracy on the PPMI dataset depending on the budget, B. We can see that the learned exploration policy (in red) has a better average accuracy for almost all budgets, especially when considering 8-12 regions.\nwhen the number of acquired regions is large. This is due to the fact that, when acquiring for example B = 12 regions, even at random, the relevant information has a high chance of being acquired. The same effect also happens when B is low e.g. when B = 2, the information acquired is not sufficient to allow for good classification. This shows that our method is particularly interesting when the number of acquired regions is between about 30% and 60% of the overall image. On certain specific datasets performance gains with our method can be quit important. Performance for varying values of B for the Guitar dataset of PPMI is illustrated in Figure 4, where we can see that as the percentage of regions acquired decreases (decreasing B), the random exploration policy\u2019s performance degrades quickly whereas our method is able to maintain adequate performance, until B becomes too small. The difference in performance between PPMI and 15 Scenes is likely linked to the fact that detecting instrument play in an image requires specific regions to be available (the face and the flute for example), whereas the scene can be inferred from a wider array of regions. Complexity Analysis: Let us denote C the cost of computing the features over a region of the image, and F the cost of computing f\u03b8 or \u03c0\u03b3i . The final inference complexity is O(B(C + F )). As we use linear classifiers in all our experiments, classification time is insignificant in comparison to SIFT computation (F << C), and complexity is therefore reduced to O(BC). In comparison to the cost of a classical BoW model O(NMB), the proposed model thus results in a speed-up of N\u00d7MB . Qualitative Results & Analysis: Figures 7 and 8 illustrate the learned exploration policy for the class flute of the\n770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823\n825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879\n30 35 40 45 50 55 60 65 70 75 80 2 4 6 8 10 12 14 16 A cc u ra cy B Random Exploration Policy Learned Exploration Policy\nFigure 6. Mean Accuracy on 15 Scenes dataset depending on the budget, B. On this dataset, the learned exploration policy (red) is only slightly better than\nFigure 7. Trajectories computed on PPMI-Flute (test). Each node corresponds to a region \u2014 label is (x, y)-coordinates. Each edge i \u2192 j means that j has been acquired just after having acquired i. The color of the edge represents the proportion of infered trajectories that contain the i\u2192 j transition.\nPPMI dataset. Figure 7 summarizes the regions visited over the testing images. Each region corresponds to a node of the graph, and an edge from region i to region j means that in at least one testing image, regions j has been acquired immediately after region i. We can see that the algorithm is focusing its attention around 5 regions that are certainly relevant for many pictures, i.e. (1; 3), (4; 2), (4; 3), (3; 3) and (3; 1). On the other hand, Figure 7 shows that at the beginning the algorithm tends to explore many different regions after the initial region. This shows that the the model starts by first exploring the image \u2014 until having acquired 2 regions \u2014 before focusing its attention on the regions that are the most relevant for predicting the category. Figure 8 shows the average behavior of our algorithm for B = 4 and B = 8.We notice that about half of the acquired regions \u2014 2 for B = 4 and 4 for B = 8 \u2014 are much more frequently explored than the other regions. These regions certainly correspond to regions that generally carry relevant information on all the images.\nFigure 8. Most frequent acquired regions for B = 4 (left) and B = 8 (right) on the PPMI Flute Dataset. The darker, the more frequent the region has been acquired for classifying.\nFigure 9. Two examples of regions acquired with B = 4 on the PPMI-Flute dataset. This example shows the ability of the model to adapt to different images where it is able to discover a flute.\nThe images in Figure 8 can be interpreted as a spatial \u201cprior\u201d for a specific classification task. For example, for discriminating playing v.s. holding flute with B = 2, regions (3; 2) and (3; 1) are the more informative on average. However, since the decision is instance-based in our method, this spatial prior is balanced with the specific visual content of each test image. Our approach therefore shares some similarities with the reconfigurable model of (Parizi et al., 2012) using latent SVM. One example of instance-based classification is illustrated in figure 9, where the regions visited with B = 4 are shown. We can see that the set of regions visited changes between the left and right image. This illustrates the ability of our method to automatically adapt its choice of representation to the content of the image it is classifying."}, {"heading": "6. Conclusion", "text": "In this paper, we introduced an adaptive representation process for image classification. The presented strategy combines both an exploration strategy used to find the best subset of regions for each image, and the final classification algorithm. New regions are iteratively selected based on the location and content of the previous ones. The resulting scheme produces an effective instance-based classification algorithm. We demonstrated the strategy\u2019s pertinence on two different image classification datasets. When using our exploration strategy limited to half of the regions of the\n880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933\n935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 images, we obtained a significant gain relative to baseline methods."}], "references": [{"title": "Salient object detection: A benchmark", "author": ["A. Borji", "D.N. Sihite", "L. Itti"], "venue": "In ECCV,", "citeRegEx": "Borji et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Borji et al\\.", "year": 2012}, {"title": "Learning mid-level features for recognition", "author": ["Boureau", "Y-Lan", "Bach", "Francis", "LeCun", "Yann", "Ponce", "Jean"], "venue": "In CVPR, pp", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "Fast classification using sparse decision DAGs", "author": ["R. Busa-Fekete", "D. Benbouzid", "B. K\u00e9gl"], "venue": "ICML", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2012}, {"title": "Fusing generic objectness and visual saliency for salient object detection", "author": ["Chang", "KY", "T.L. Liu", "H.T. Chen", "Lai", "SH"], "venue": "In ICCV,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "The devil is in the details: an evaluation of recent feature encoding methods", "author": ["K. Chatfield", "V. Lempitsky", "A. Vedaldi", "A. Zisserman"], "venue": "In Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "Chatfield et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2011}, {"title": "Rollout sampling approximate policy iteration", "author": ["Dimitrakakis", "Christos", "Lagoudakis", "Michail G"], "venue": "Machine Learning,", "citeRegEx": "Dimitrakakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dimitrakakis et al\\.", "year": 2008}, {"title": "Text classification: A sequential reading approach", "author": ["Dulac-Arnold", "Gabriel", "Denoyer", "Ludovic", "Gallinari", "Patrick"], "venue": "In ECIR, pp", "citeRegEx": "Dulac.Arnold et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dulac.Arnold et al\\.", "year": 2011}, {"title": "Sequential approaches for learning datum-wise sparse representations", "author": ["Dulac-Arnold", "Gabriel", "Denoyer", "Ludovic", "Preux", "Philippe", "Gallinari", "Patrick"], "venue": "Machine Learning,", "citeRegEx": "Dulac.Arnold et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dulac.Arnold et al\\.", "year": 2012}, {"title": "Sequential approaches for learning datum-wise sparse representations", "author": ["Dulac-Arnold", "Gabriel", "Denoyer", "Ludovic", "Preux", "Philippe", "Gallinari", "Patrick"], "venue": "Machine Learning,", "citeRegEx": "Dulac.Arnold et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dulac.Arnold et al\\.", "year": 2012}, {"title": "Treebased batch mode reinforcement learning", "author": ["Ernst", "Damien", "Geurts", "Pierre", "Wehenkel", "Louis"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Ernst et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ernst et al\\.", "year": 2005}, {"title": "Geometric lp-norm feature pooling for image classification", "author": ["Feng", "Jiashi", "Ni", "Bingbing", "Tian", "Qi", "Yan", "Shuicheng"], "venue": "In CVPR, pp", "citeRegEx": "Feng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2011}, {"title": "Timely object recognition", "author": ["Karayev", "Sergey", "Baumgartner", "Tobias", "Fritz", "Mario", "Darrell", "Trevor"], "venue": "In NIPS, pp", "citeRegEx": "Karayev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Karayev et al\\.", "year": 2012}, {"title": "Learning to combine foveal glimpses with a third-order Boltzmann machine", "author": ["H Larochelle", "Hinton", "GE"], "venue": "NIPS, pp", "citeRegEx": "Larochelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2010}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In CVPR,", "citeRegEx": "Lazebnik et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lazebnik et al\\.", "year": 2006}, {"title": "A performance evaluation of local descriptors", "author": ["Mikolajczyk", "Krystian", "Schmid", "Cordelia"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Mikolajczyk et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Mikolajczyk et al\\.", "year": 2005}, {"title": "Reconfigurable models for scene recognition", "author": ["Parizi", "Sobhan Naderi", "Oberlin", "John G", "Felzenszwalb", "Pedro F"], "venue": "In CVPR, pp", "citeRegEx": "Parizi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Parizi et al\\.", "year": 2012}, {"title": "Discriminative spatial saliency for image classification", "author": ["Sharma", "Gaurav", "Jurie", "Fr\u00e9d\u00e9ric", "Schmid", "Cordelia"], "venue": "In CVPR, pp", "citeRegEx": "Sharma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2012}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "In ICCV,", "citeRegEx": "Sivic and Zisserman,? \\Q2003\\E", "shortCiteRegEx": "Sivic and Zisserman", "year": 2003}, {"title": "Improving image classification using semantic attributes", "author": ["Su", "Yu", "Jurie", "Fr\u00e9d\u00e9ric"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Su et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Su et al\\.", "year": 2012}, {"title": "Visual word ambiguity", "author": ["van Gemert", "Jan", "Veenman", "Cor J", "Smeulders", "Arnold W. M", "Geusebroek", "Jan-Mark"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Gemert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gemert et al\\.", "year": 2010}, {"title": "Vlfeat \u2013 an open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "In Proc. of the 18th annual ACM Intl. Conf. on Multimedia,", "citeRegEx": "Vedaldi and Fulkerson,? \\Q2010\\E", "shortCiteRegEx": "Vedaldi and Fulkerson", "year": 2010}, {"title": "Grouplet: A structured image representation for recognizing human and object interactions", "author": ["Yao", "Bangpeng", "Fei-Fei", "Li"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 13, "context": "(Lazebnik et al., 2006).", "startOffset": 0, "endOffset": 23}, {"referenceID": 16, "context": "need to consider the entire image to be able to interpret it (Sharma et al., 2012).", "startOffset": 61, "endOffset": 82}, {"referenceID": 11, "context": "(Karayev et al., 2012).", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "The standard image classification pipeline follows three steps (Boureau et al., 2010) \u2014 (i) low-level local descriptor extraction, (ii) coding, and (iii) pooling \u2014 to get feature vectors that are then used for classification.", "startOffset": 63, "endOffset": 85}, {"referenceID": 1, "context": ", 2011) or sparse coding (Boureau et al., 2010).", "startOffset": 25, "endOffset": 47}, {"referenceID": 13, "context": "The spatial pyramid matching (SPM) strategy (Lazebnik et al., 2006) extends the pooling by considering a fixed predetermined spatial image pyramid.", "startOffset": 44, "endOffset": 67}, {"referenceID": 4, "context": "However, in contrast to the task of matching a specific target image or object, methods for category classification show better performance when using a uniform feature sampling over a dense grid on the image (Chatfield et al., 2011).", "startOffset": 209, "endOffset": 233}, {"referenceID": 0, "context": "Other approaches studying are motivated by human eye fixation or salient object detection (Borji et al., 2012; Chang et al., 2011).", "startOffset": 90, "endOffset": 130}, {"referenceID": 3, "context": "Other approaches studying are motivated by human eye fixation or salient object detection (Borji et al., 2012; Chang et al., 2011).", "startOffset": 90, "endOffset": 130}, {"referenceID": 10, "context": "Recently, several approaches combine dense sampling and saliency map or spatial weighting to obtain powerful image representations (Su & Jurie, 2012; Feng et al., 2011).", "startOffset": 131, "endOffset": 168}, {"referenceID": 16, "context": "(Sharma et al., 2012) proposes a scheme to learn discriminative saliency maps at an image region level.", "startOffset": 0, "endOffset": 21}, {"referenceID": 15, "context": "(Parizi et al., 2012) introduce a reconfigurable model where each region is equiped with a latent variable representing a topic, such that only regions with similar topics are matched together in the final representation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "In a way more drastic than Sharma\u2019s approach (Sharma et al., 2012), we constrain our system to take a decision by considering only a fixed number of regions of the test image, thus allowing the computation of the local features to be significantly reduced.", "startOffset": 45, "endOffset": 66}, {"referenceID": 11, "context": "More recent work uses similar techniques to find optimal orders for anytime object detection tasks (Karayev et al., 2012).", "startOffset": 99, "endOffset": 121}, {"referenceID": 2, "context": ", 2012b), the authors propose to use leinforcement learning models for learning sparse classifiers on vectors, (Busa-Fekete et al., 2012) use sequential techniques for learning a cascade of classifiers depending on the content of the inputs, while (Dulac-Arnold et al.", "startOffset": 111, "endOffset": 137}, {"referenceID": 6, "context": ", 2012) use sequential techniques for learning a cascade of classifiers depending on the content of the inputs, while (Dulac-Arnold et al., 2011) is an application of sequential learning models to text classification.", "startOffset": 118, "endOffset": 145}, {"referenceID": 15, "context": "Based on the K length vector \u03c6(r i ), we consider the following \u0393 mapping into a larger space of size K\u00d7 (N \u00d7M) as in (Parizi et al., 2012): \u0393(\u03c6(r i ),K) = (0 .", "startOffset": 118, "endOffset": 139}, {"referenceID": 9, "context": "Note that this learning algorithm is original and derived from both the rollout classification policy iteration (RCPI) method and the Fitted-Q Learning model that are generic Reinforcement Learning models proposed in (Dimitrakakis & Lagoudakis, 2008) and (Ernst et al., 2005).", "startOffset": 255, "endOffset": 275}, {"referenceID": 13, "context": "html recognition (15-scenes dataset) (Lazebnik et al., 2006).", "startOffset": 37, "endOffset": 60}, {"referenceID": 15, "context": "7% accuracy in the 15-Scene database, exactly matching the performances reported in (Parizi et al., 2012) 7.", "startOffset": 84, "endOffset": 105}, {"referenceID": 15, "context": "Our approach therefore shares some similarities with the reconfigurable model of (Parizi et al., 2012) using latent SVM.", "startOffset": 81, "endOffset": 102}], "year": 2014, "abstractText": "In this paper, we investigate a new framework for image classification that adaptively generates spatial representations. Our strategy is based on a sequential process that learns to explore the different regions of any image in order to infer its category. In particular, the choice of regions is specific to each image, directed by the actual content of previously selected regions.The capacity of the system to handle incomplete image information as well as its adaptive region selection allow the system to perform well in budgeted classification tasks by exploiting a dynamicly generated representation of each image. We demonstrate the system\u2019s abilities in a series of image-based exploration and classification tasks that highlight its learned exploration and inference abilities.", "creator": "LaTeX with hyperref package"}}}