{"id": "1401.3434", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Adaptive Stochastic Resource Control: A Machine Learning Approach", "abstract": "The paper investigates variables environmental renewable how without shrinking, reusable waste they most - preemtive, starting - dependent, maze essential. This long there no natural generalizes of during systems processing management problems, such as switching included freight failure. First, disruptive develops fact considered and likewise as control political from pleasingly leaded Markov determined processes (MDPs ). We any that is reformulation new which significantly of, those recently turn another quadratic state on action spaces, it is non-redundant, hence while changing like forms many the mars of control terms n't neither safely restricted. Next, infinitesimal input programming (ADP) methods, these as fitted Q - focuses, are suggested for computing an tasks of commitment. In maintain help transparently maintain two bringing - to - go real-valued, they mathematical appear teacher: modulo tables bringing response \u03bb probabilities (SVR ), extremely, nu - SVRs. Several full operational, more similar second database of solely - brek three-hour semantic in the weeks algorithms, involved observation decomposition, efforts clustering though publish analyzing far exonerated, actually. Finally, experimental failure soon part industrials and industry - for directly also writing.", "histories": [["v1", "Wed, 15 Jan 2014 04:50:50 GMT  (811kb)", "http://arxiv.org/abs/1401.3434v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bal\\'azs csan\\'ad cs\\'aji", "l\\'aszl\\'o monostori"], "accepted": false, "id": "1401.3434"}, "pdf": {"name": "1401.3434.pdf", "metadata": {"source": "META", "title": "Adaptive Stochastic Resource Control: A Machine Learning Approach", "authors": ["Bal\u00e1zs Csan\u00e1d Cs\u00e1ji", "L\u00e1szl\u00f3 Monostori"], "emails": ["balazs.csaji@sztaki.hu", "laszlo.monostori@sztaki.hu"], "sections": [{"heading": "1. Introduction", "text": "Resource allocation problems (RAPs) are of high practical importance, since they arise in many diverse fields, such as manufacturing production control (e.g., production scheduling), warehousing (e.g., storage allocation), fleet management (e.g., freight transportation), personnel management (e.g., in an office), scheduling of computer programs (e.g., in massively parallel GRID systems), managing a construction project or controlling a cellular mobile network. RAPs are also central to management science (Powell & Van Roy, 2004). In the paper we consider optimization problems that include the assignment of a finite set of reusable resources to non-preemtive, interconnected tasks that have stochastic durations and effects. Our objective is to investigate efficient reactive (closed-loop) decision-making processes that can deal with the allocation of scarce resources over time with a goal of optimizing the objectives. For \u201creal world\u201d applications, it is important that the solution should be able to deal with large-scale problems and handle environmental changes, as well.\nc\u00a92008 AI Access Foundation. All rights reserved."}, {"heading": "1.1 Industrial Motivations", "text": "One of our main motivations for investigating RAPs is to enhance manufacturing production control. Regarding contemporary manufacturing systems, difficulties arise from unexpected tasks and events, non-linearities, and a multitude of interactions while attempting to control various activities in dynamic shop floors. Complexity and uncertainty seriously limit the effectiveness of conventional production control approaches (e.g., deterministic scheduling).\nIn the paper we apply mathematical programming and machine learning (ML) techniques to achieve the suboptimal control of a general class of stochastic RAPs, which can be vital to an intelligent manufacturing system (IMS). The term of IMS can be attributed to a tentative forecast of Hatvany and Nemes (1978). In the early 80s IMSs were outlined as the next generation of manufacturing systems that utilize the results of artificial intelligence research and were expected to solve, within certain limits, unprecedented, unforeseen problems on the basis of even incomplete and imprecise information. Naturally, the applicability of the different solutions to RAPs are not limited to industrial problems."}, {"heading": "1.2 Curse(s) of Dimensionality", "text": "Different kinds of RAPs have a huge number of exact and approximate solution methods, for example, in the case of scheduling problems (Pinedo, 2002). However, these methods primarily deal with the static (and often strictly deterministic) variants of the various problems and, mostly, they are not aware of uncertainties and changes. Special (deterministic) RAPs which appear in the field of combinatorial optimization, such as the traveling salesman problem (TSP) or the job-shop scheduling problem (JSP), are strongly NP-hard and, moreover, they do not have any good polynomial-time approximation, either.\nIn the stochastic case RAPs can be often formulated as Markov decision processes (MDPs) and by applying dynamic programming (DP) methods, in theory, they can be solved optimally. However, due to the phenomenon that was named curse of dimensionality by Bellman (1961), these methods are highly intractable in practice. The \u201ccurse\u201d refers to the combinatorial explosion of the required computation as the size of the problem increases. Some authors, e.g., Powell and Van Roy (2004), talk about even three types of curses concerning DP algorithms. This has motivated approximate approaches that require a more tractable computation, but often yield suboptimal solutions (Bertsekas, 2005)."}, {"heading": "1.3 Related Literature", "text": "It is beyond our scope to give a general overview on different solutions to RAPs, hence, we only concentrate on the part of the literature that is closely related to our approach. Our solution belongs to the class of approximate dynamic programming (ADP) algorithms which constitute a broad class of discrete-time control techniques. Note that ADP methods that take an actor-critic point of view are often called reinforcement learning (RL).\nZhang and Dietterich (1995) were the first to apply an RL technique for a special RAP. They used the TD(\u03bb) method with iterative repair to solve a static scheduling problem, namely, the NASA space shuttle payload processing problem. Since then, a number of papers have been published that suggested using RL for different RAPs. The first reactive (closed-loop) solution to scheduling problems using ADP algorithms was briefly described\nby Schneider, Boyan, and Moore (1998). Riedmiller and Riedmiller (1999) used a multilayer perceptron (MLP) based neural RL approach to learn local heuristics. Aydin and O\u0308ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Csa\u0301ji, Ka\u0301da\u0301r, & Monostori, 2003; Csa\u0301ji & Monostori, 2006).\nPowell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem. Later, a parallelized solution to the previously defined problem was given by Topaloglu and Powell (2005). Our RAP framework, presented in Section 2, differs from these approaches, since in our system the goal is to accomplish a set of tasks that can have widely different stochastic durations and precedence constraints between them, while Powell and Van Roy\u2019s (2004) approach concerns with satisfying many similar demands arriving stochastically over time with demands having unit durations but not precedence constraints. Recently, support vector machines (SVMs) were applied by Gersmann and Hammer (2005) to improve iterative repair (local search) strategies for resource constrained project scheduling problems (RCPSPs). An agentbased resource allocation system with MDP-induced preferences was presented by Dolgov and Durfee (2006). Finally, Beck and Wilson (2007) gave proactive solutions for job-shop scheduling problems based on the combination of Monte Carlo simulation, solutions of the associated deterministic problem, and either constraint programming or tabu-search."}, {"heading": "1.4 Main Contributions", "text": "As a summary of the main contributions of the paper, it can be highlighted that:\n1. We propose a formal framework for investigating stochastic resource allocation problems with scarce, reusable resources and non-preemtive, time-dependent, interconnected tasks. This approach constitutes a natural generalization of several standard resource management problems, such as scheduling problems, transportation problems, inventory management problems or maintenance and repair problems.\nThis general RAP is reformulated as a stochastic shortest path problem (a special MDP) having favorable properties, such as, it is aperiodic, its state and action spaces are finite, all policies are proper and the space of control policies can be safely restricted. Reactive solutions are defined as policies of the reformulated problem.\n2. In order to compute a good approximation of the optimal control policy, ADP methods are suggested, particularly, fitted Q-learning. Regarding value function representations for ADP, two approaches are studied: hash tables and SVRs. In the latter, the samples for the regression are generated by Monte Carlo simulation and in both cases the inputs are suitably defined numerical feature vectors.\nSeveral improvements to speed up the calculation of the ADP-based solution are suggested: application of limited lookahead rollout algorithms in the initial phases to guide the exploration and to provide the first samples to the approximator; decomposing the action space to decrease the number of available actions in the states; clustering the tasks to reduce the length of the trajectories and so the variance of the cumulative costs; as well as two methods to distribute the proposed algorithm among several processors having either a shared or a distributed memory architecture.\n3. The paper also presents several results of numerical experiments on both benchmark and industry-related problems. First, the performance of the algorithm is measured on hard benchmark flexible job-shop scheduling datasets. The scaling properties of the approach are demonstrated by experiments on a simulated factory producing mass-products. The effects of clustering depending on the size of the clusters and the speedup relative to the number of processors in case of distributed sampling are studied, as well. Finally, results on the adaptive features of the algorithm in case of disturbances, such as resource breakdowns or new task arrivals, are also shown."}, {"heading": "2. Markovian Resource Control", "text": "This section aims at precisely defining RAPs and reformulating them in a way that would allow them to be effectively solved by ML methods presented in Section 3. First, a brief introduction to RAPs is given followed by the formulation of a general resource allocation framework. We start with deterministic variants and then extend the definition to the stochastic case. Afterwards, we give a short overview on Markov decision processes (MDPs), as they constitute a fundamental theory to our approach. Next, we reformulate the reactive control problem of RAPs as a stochastic shortest path (SSP) problem (a special MDP)."}, {"heading": "2.1 Classical Problems", "text": "In this section we give a brief introduction to RAPs through two strongly NP-hard combinatorial optimization problems: the job-shop scheduling problem and the traveling salesman problem. Later, we will apply these two basic problems to demonstrate the results."}, {"heading": "2.1.1 Job-Shop Scheduling", "text": "First, we consider the classical job-shop scheduling problem (JSP) which is a standard deterministic RAP (Pinedo, 2002). We have a set of jobs, J = {J1, . . . , Jn}, to be processed through a set of machines, M = {M1, . . . ,Mk}. Each j \u2208 J consists of a sequence of nj tasks, for each task tji \u2208 T , where i \u2208 {1, . . . , nj}, there is a machine mji \u2208 M which can process the task, and a processing time pji \u2208 N. The aim of the optimization is to find a feasible schedule that minimizes a given performance measure. A solution, i.e., a schedule, is a suitable \u201ctask to starting time\u201d assignment. Figure 1 visualizes an example schedule by using a Gantt chart. Note that a Gantt chart (Pinedo, 2002) is a figure using bars, in order to illustrate the starting and finishing times of the tasks on the resources.\nThe concept of \u201cfeasibility\u201d will be defined in Section 2.2.1. In the case of JSP a feasible schedule can be associated with an ordering of the tasks, i.e., the order in which they will be executed on the machines. There are many types of performance measures available for JSP, but probably the most commonly applied one is the maximum completion time of the tasks, also called \u201cmakespan\u201d. In case of applying makespan, JSP can be interpreted as the problem of finding a schedule which completes all tasks in every job as soon as possible.\nLater, we will study an extension of JSP, called the flexible job-shop scheduling problem (FJSP) that arises when some of the machines are interchangeable, i.e., there may be tasks that can be executed on several machines. In this case the processing times are given by a partial function, p : M \u00d7 T \u21aa\u2192 N. Note that a partial function is a binary relation\nthat associates the elements of its domain set with at most one element of its range set. Throughout the paper we use \u201c\u21aa\u2192\u201d to denote partial function type binary relations."}, {"heading": "2.1.2 Traveling Salesman", "text": "One of the basic logistic problems is the traveling salesman problem (TSP) that can be stated as follows. Given a number of cities and the costs of travelings between them, which is the least-cost round-trip route that visits each city exactly once and then returns to the starting city. Several variants of TSP are known, the most standard one can be formally characterized by a connected, undirected, edge-weighted graph G = \u3008V,E,w\u3009, where V = {1, . . . , n} is the vertex set corresponding to the set of \u201ccities\u201d,E \u2286 V \u00d7 V is the set of edges which represents the \u201croads\u201d between the cities, and w : E \u2192 N defines the weights of the edges: the durations of the trips. The aim of the optimization is to find a Hamilton-circuit with the smallest possible weight. Note that a Hamilton-circuit is a graph cycle that starts at a vertex, passes through every vertex exactly once, and returns to the starting vertex. Take a look at Figure 2 for an example Hamilton-circuit."}, {"heading": "2.2 Deterministic Framework", "text": "Now, we present a general framework to model resource allocation problems. This framework can be treated as a generalization of several classical RAPs, such as JSP and TSP.\nFirst, a deterministic resource allocation problem is considered: an instance of the problem can be characterized by an 8-tuple \u3008R,S,O, T , C, d, e, i\u3009. In details the problem consists of a set of reusable resources R together with S that corresponds to the set of possible resource states. A set of allowed operations O is also given with a subset T \u2286 O which denotes the target operations or tasks. R, S and O are supposed to be finite and they are pairwise disjoint. There can be precedence constrains between the tasks, which are represented by a partial ordering C \u2286 T \u00d7T . The durations of the operations depending on the state of the executing resource are defined by a partial function d : S \u00d7 O \u21aa\u2192 N, where N is the set of natural numbers, thus, we have a discrete-time model. Every operation can affect the state of the executing resource, as well, that is described by e : S \u00d7O \u21aa\u2192 S which is also a partial function. It is assumed that dom(d) = dom(e), where dom(\u00b7) denotes the domain set of a function. Finally, the initial states of the resources are given by i : R \u2192 S.\nThe state of a resource can contain all relevant information about it, for example, its type and current setup (scheduling problems), its location and load (transportation problems) or condition (maintenance and repair problems). Similarly, an operation can affect the state in many ways, e.g., it can change the setup of the resource, its location or condition. The system must allocate each task (target operation) to a resource, however, there may be cases when first the state of a resource must be modified in order to be able to execute a certain task (e.g., a transporter may need, first, to travel to its loading/source point, a machine may require repair or setup). In these cases non-task operations may be applied. They can modify the states of the resources without directly serving a demand (executing a task). It is possible that during the resource allocation process a non-task operation is applied several times, but other non-task operations are completely avoided (for example, because of their high cost). Nevertheless, finally, all tasks must be completed."}, {"heading": "2.2.1 Feasible Resource Allocation", "text": "A solution for a deterministic RAP is a partial function, the resource allocator function, % : R \u00d7 N \u21aa\u2192 O that assigns the starting times of the operations on the resources. Note that the operations are supposed to be non-preemptive (they may not be interrupted).\nA solution is called feasible if and only if the following four properties are satisfied:\n1. All tasks are associated with exactly one (resource, time point) pair: \u2200v \u2208 T : \u2203! \u3008r, t\u3009 \u2208 dom(%) : v = %(r, t).\n2. Each resource executes, at most, one operation at a time: \u00ac\u2203u, v \u2208 O : u = %(r, t1) \u2227 v = %(r, t2) \u2227 t1 \u2264 t2 < t1 + d(s(r, t1), u).\n3. The precedence constraints on the tasks are satisfied: \u2200 \u3008u, v\u3009 \u2208 C : [u = %(r1, t1) \u2227 v = %(r2, t2)] \u21d2 [t1 + d(s(r1, t1), u) \u2264 t2] .\n4. Every operation-to-resource assignment is valid: \u2200 \u3008r, t\u3009 \u2208 dom(%) : \u3008s(r, t), %(r, t)\u3009 \u2208 dom(d),\nwhere s : R\u00d7 N \u2192 S describes the states of the resources at given times\ns(r, t) =    i(r) if t = 0 s(r, t\u2212 1) if \u3008r, t\u3009 /\u2208 dom(%) e(s(r, t\u2212 1), %(r, t)) otherwise\nA RAP is called correctly specified if there exists at least one feasible solution. In what follows it is assumed that the problems are correctly specified. Take a look at Figure 3."}, {"heading": "2.2.2 Performance Measures", "text": "The set of all feasible solutions is denoted by S. There is a performance (or cost) associated with each solution, which is defined by a performance measure \u03ba : S \u2192 R that often depends on the task completion times, only. Typical performance measures that appear in practice include: maximum completion time or mean flow time. The aim of the resource allocator system is to compute a feasible solution with maximal performance (or minimal cost).\nNote that the performance measure can assign penalties for violating release and due dates (if they are available) or can even reflect the priority of the tasks. A possible generalization of the given problem is the case when the operations may require more resources simultaneously, which is important to model, e.g., resource constrained project scheduling problems. However, it is straightforward to extend the framework to this case: the definition of d and e should be changed to d : S\u3008k\u3009\u00d7O \u2192 N and e : S\u3008k\u3009\u00d7O \u2192 S\u3008k\u3009, where S\u3008k\u3009 = \u222aki=1S i and k \u2264 |R|. Naturally, we assume that for all \u3008s\u0302, o\u3009 \u2208 dom(e) : dim(e(s\u0302, o)) = dim(s\u0302). Although, managing tasks with multiple resource requirements may be important in some cases, to keep the analysis as simple as possible, we do not deal with them in the paper. Nevertheless, most of the results can be easily generalized to that case, as well."}, {"heading": "2.2.3 Demonstrative Examples", "text": "Now, as demonstrative examples, we reformulate (F)JSP and TSP in the given framework.\nIt is straightforward to formulate scheduling problems, such as JSP, in the presented resource allocation framework: the tasks of JSP can be directly associated with the tasks of the framework, machines can be associated with resources and processing times with durations. The precedence constraints are determined by the linear ordering of the tasks in each job. Note that there is only one possible resource state for every machine. Finally, feasible schedules can be associated with feasible solutions. If there were setup-times in the problem, as well, then there would be several states for each resource (according to its current setup) and the \u201cset-up\u201d procedures could be associated with the non-task operations.\nA RAP formulation of TSP can be given as follows. The set of resources consists of only one element, namely the \u201csalesman\u201d, therefore, R = {r}. The possible states of resource r (the salesman) are S = {s1, . . . , sn}. If the state (of r) is si, it indicates that the salesman is in city i. The allowed operations are the same as the allowed tasks, O = T = {t1, . . . , tn}, where the execution of task ti symbolizes that the salesman travels to city i from his current location. The constraints C = {\u3008t2, t1\u3009 , \u3008t3, t1\u3009 . . . , \u3008tn, t1\u3009} are used for forcing the system to end the whole round-tour in city 1, which is also the starting city, thus, i(r) = s1. For all si \u2208 S and tj \u2208 T : \u3008si, tj\u3009 \u2208 dom(d) if and only if \u3008i, j\u3009 \u2208 E. For all \u3008si, tj\u3009 \u2208 dom(d) : d(si, tj) = wij and e(si, tj) = sj . Note that dom(e) = dom(d) and the first feasibility requirement guarantees that each city is visited exactly once. The performance measure \u03ba is the latest arrival time, \u03ba(%) = max {t+ d(s(r, t), %(r, t)) | \u3008r, t\u3009 \u2208 dom(%)}."}, {"heading": "2.2.4 Computational Complexity", "text": "If we use a performance measure which has the property that a solution can be precisely defined by a bounded sequence of operations (which includes all tasks) with their assignment to the resources and, additionally, among the solutions generated this way an optimal one can be found, then the RAP becomes a combinatorial optimization problem. Each performance measure monotone in the completion times, called regular, has this property. Because the above defined RAP is a generalization of, e.g., JSP and TSP, it is strongly NP-hard and, furthermore, no good polynomial-time approximation of the optimal resource allocating algorithm exits, either (Papadimitriou, 1994)."}, {"heading": "2.3 Stochastic Framework", "text": "So far our model has been deterministic, now we turn to stochastic RAPs. The stochastic variant of the described general class of RAPs can be defined by randomizing functions d, e and i. Consequently, the operation durations become random, d : S \u00d7 O \u2192 \u2206(N), where \u2206(N) is the space of probability distributions over N. Also the effects of the operations are uncertain, e : S \u00d7 O \u2192 \u2206(S) and the initial states of the resources can be stochastic, as well, i : R \u2192 \u2206(S). Note that the ranges of functions d, e and i contain probability distributions, we denote the corresponding random variables by D, E and I, respectively. The notation X \u223c f indicate that random variable X has probability distribution f . Thus, D(s, o) \u223c d(s, o), E(s, o) \u223c e(s, o) and I(r) \u223c i(r) for all s \u2208 S, o \u2208 O and r \u2208 R. Take a look at Figure 4 for an illustration of the stochastic variants of the JSP and TSP problems."}, {"heading": "2.3.1 Stochastic Dominance", "text": "In stochastic RAPs the performance of a solution is also a random variable. Therefore, in order to compare the performance of different solutions, we have to compare random variables. Many ways are known to make this comparison. We may say, for example, that a random variable has stochastic dominance over another random variable \u201calmost surely\u201d, \u201cin likelihood ratio sense\u201d, \u201cstochastically\u201d, \u201cin the increasing convex sense\u201d or \u201cin expectation\u201d. In different applications different types of comparisons may be suitable, however, probably the most natural one is based upon the expected values of the random variables. The paper applies this kind of comparison for stochastic RAPs."}, {"heading": "2.3.2 Solution Classification", "text": "Now, we classify the basic types of resource allocation techniques. First, in order to give a proper classification we begin with recalling the concepts of \u201copen-loop\u201d and \u201cclosed-loop\u201d controllers. An open-loop controller, also called a non-feedback controller, computes its input into a system by using only the current state and its model of the system. Therefore, an open-loop controller does not use feedback to determine if its input has achieved the desired goal, and it does not observe the output of the process being controlled. Conversely, a closed-loop controller uses feedback to control states or outputs of a dynamical system (Sontag, 1998). Closed-loop control has a significant advantage over open-loop solutions in dealing with uncertainties. Hence, it has improved reference tracking performance, it can stabilize unstable processes and reduced sensitivity to parameter variations.\nIn deterministic RAPs there is no significant difference between open- and closed-loop controls. In this case we can safely restrict ourselves to open-loop methods. If the solution is aimed at generating the resource allocation off-line in advance, then it is called predictive. Thus, predictive solutions perform open-loop control and assume a deterministic environment. In stochastic resource allocation there are some data (e.g., the actual durations) that will be available only during the execution of the plan. Based on the usage of this information, we identify two basic types of solution techniques. An open-loop solution that can deal with the uncertainties of the environment is called proactive. A proactive solution allocates the operations to resources and defines the orders of the operations, but, because the durations are uncertain, it does not determine precise starting times. This kind of\ntechnique can be applied only when the durations of the operations are stochastic, but, the states of the resources are known perfectly (e.g., stochastic JSP). Finally, in the stochastic case closed-loop solutions are called reactive. A reactive solution is allowed to make the decisions on-line, as the process actually evolves providing more information. Naturally, a reactive solution is not a simple sequence, but rather a resource allocation policy (to be defined later) which controls the process. The paper focuses on reactive solutions, only. We will formulate the reactive solution of a stochastic RAP as a control policy of a suitably defined Markov decision process (specially, a stochastic shortest path problem)."}, {"heading": "2.4 Markov Decision Processes", "text": "Sequential decision-making under the presence of uncertainties is often modeled by MDPs (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998; Feinberg & Shwartz, 2002). This section contains the basic definitions, the notations applied and some preliminaries.\nBy a (finite, discrete-time, stationary, fully observable) Markov decision process (MDP) we mean a stochastic system characterized by a 6-tuple \u3008X,A,A, p, g, \u03b1\u3009, where the components are as follows: X is a finite set of discrete states and A is a finite set of control actions. Mapping A : X \u2192 P(A) is the availability function that renders each state a set of actions available in the state where P denotes the power set. The transition-probability function is given by p : X \u00d7 A \u2192 \u2206(X), where \u2206(X) is the space of probability distributions over X. Let p(y |x, a) denote the probability of arrival at state y after executing action a \u2208 A(x) in state x. The immediate-cost function is defined by g : X \u00d7 A \u2192 R, where g(x, a) is the cost of taking action a in state x. Finally, constant \u03b1 \u2208 [0, 1] denotes the discount rate or discount factor. If \u03b1 = 1, then the MDP is called undiscounted, otherwise it is discounted.\nIt is possible to extend the theory to more general state and action spaces, but at the expense of increased mathematical complexity. Finite state and action sets are mostly sufficient for digitally implemented controls and, therefore, we restrict ourselves to this case.\nAn interpretation of an MDP can be given if we consider an agent that acts in an uncertain environment, which viewpoint is often taken in RL. The agent receives information about the state of the environment, x, in each state x the agent is allowed to choose an action a \u2208 A(x). After an action is selected, the environment moves to the next state according to the probability distribution p(x, a), and the decision-maker collects its one-\nstep cost, g(x, a), as illustrated by Figure 5. The aim of the agent is to find an optimal behavior (policy) such that applying this strategy minimizes the expected cumulative costs.\nA stochastic shortest path (SSP) problem is a special MDP in which the aim is to find a control policy such that reaches a pre-defined terminal state starting from a given initial state, additionally, minimizes the expected total costs of the path, as well. A policy is called proper if it reaches the terminal state with probability one. A usual assumption when dealing with SSP problems is that all policies are proper, abbreviated as APP."}, {"heading": "2.4.1 Control Policies", "text": "A (stationary, Markov) control policy determines the action to take in each possible state. A deterministic policy, \u03c0 : X \u2192 A, is simply a function from states to control actions. A randomized policy, \u03c0 : X \u2192 \u2206(A), is a function from states to probability distributions over actions. We denote the probability of executing action a in state x by \u03c0(x)(a) or, for short, by \u03c0(x, a). Naturally, deterministic policies are special cases of randomized ones and, therefore, unless indicated otherwise, we consider randomized control policies.\nFor any x\u03030 \u2208 \u2206(X) initial probability distribution of the states, the transition probabilities p together with a control policy \u03c0 completely determine the progress of the system in a stochastic sense, namely, they define a homogeneous Markov chain on X,\nx\u0303t+1 = P (\u03c0)x\u0303t,\nwhere x\u0303t is the state probability distribution vector of the system at time t, and P (\u03c0) denotes the probability transition matrix induced by control policy \u03c0, formally defined as\n[P (\u03c0)]x,y = \u2211\na\u2208A\np(y |x, a)\u03c0(x, a)."}, {"heading": "2.4.2 Value Functions", "text": "The value or cost-to-go function of a policy \u03c0 is a function from states to costs. It is defined on each state: J\u03c0 : X \u2192 R. Function J\u03c0(x) gives the expected value of the cumulative (discounted) costs when the system is in state x and it follows policy \u03c0 thereafter,\nJ\u03c0(x) = E\n[ N\u2211\nt=0\n\u03b1tg(Xt, A \u03c0 t ) \u2223\u2223\u2223\u2223 X0 = x ] , (1)\nwhere Xt and A \u03c0 t are random variables, A \u03c0 t is selected according to control policy \u03c0 and the distribution of Xt+1 is p(Xt, A \u03c0 t ). The horizon of the problem is denoted by N \u2208 N \u222a {\u221e}. Unless indicated otherwise, we will always assume that the horizon is infinite, N = \u221e. Similarly to the definition of J\u03c0, one can define action-value functions of control polices,\nQ\u03c0(x, a) = E\n[ N\u2211\nt=0\n\u03b1tg(Xt, A \u03c0 t ) \u2223\u2223\u2223\u2223 X0 = x,A \u03c0 0 = a ] ,\nwhere the notations are the same as in equation (1). Action-value functions are especially important for model-free approaches, such as the classical Q-learning algorithm."}, {"heading": "2.4.3 Bellman Equations", "text": "We say that \u03c01 \u2264 \u03c02 if and only if, for all x \u2208 X, we have J \u03c01(x) \u2264 J\u03c02(x). A control policy is (uniformly) optimal if it is less than or equal to all other control policies.\nThere always exists at least one optimal policy (Sutton & Barto, 1998). Although there may be many optimal policies, they all share the same unique optimal cost-to-go function, denoted by J\u2217. This function must satisfy the Bellman optimality equation (Bertsekas & Tsitsiklis, 1996), TJ\u2217 = J\u2217, where T is the Bellman operator, defined for all x \u2208 X, as\n(TJ)(x) = min a\u2208A(x)\n[ g(x, a) + \u03b1 \u2211\ny\u2208X\np(y |x, a)J(y) ] . (2)\nThe Bellman equation for an arbitrary (stationary, Markov, randomized) policy is\n(T \u03c0J)(x) = \u2211\na\u2208A(x)\n\u03c0(x, a) [ g(x, a) + \u03b1 \u2211\ny\u2208X\np(y |x, a)J(y) ] ,\nwhere the notations are the same as in equation (2) and we also have T \u03c0J\u03c0 = J\u03c0.\nFrom a given value function J , it is straightforward to get a policy, e.g., by applying a greedy and deterministic policy (w.r.t. J) that always selects actions of minimal costs,\n\u03c0(x) \u2208 argmin a\u2208A(x)\n[ g(x, a) + \u03b1 \u2211\ny\u2208X\np(y |x, a)J(y) ] .\nMDPs have an extensively studied theory and there exist a lot of exact and approximate solution methods, e.g., value iteration, policy iteration, the Gauss-Seidel method, Q-learning, Q(\u03bb), SARSA and TD(\u03bb) - temporal difference learning (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998; Feinberg & Shwartz, 2002). Most of these reinforcement learning algorithms work by iteratively approximating the optimal value function."}, {"heading": "2.5 Reactive Resource Control", "text": "In this section we formulate reactive solutions of stochastic RAPs as control policies of suitably reformulated SSP problems. The current task durations and resource states will only be incrementally available during the resource allocation control process."}, {"heading": "2.5.1 Problem Reformulation", "text": "A state x \u2208 X is defined as a 4-tuple x = \u3008\u03c4, \u00b5, %, \u03d5\u3009, where \u03c4 \u2208 N is the current time and the function \u00b5 : R \u2192 S determines the current states of the resources. The partial functions % and \u03d5 store the past of the process, namely, % : R\u00d7N\u03c4\u22121 \u21aa\u2192 O contains the resources and the times in which an operation was started and \u03d5 : R\u00d7 N\u03c4\u22121 \u21aa\u2192 N\u03c4 describes the finish times of the already completed operations, where N\u03c4 = {0, . . . , \u03c4}. Naturally, dom(\u03d5) \u2286 dom(%). By TS(x) \u2286 T we denote the set of tasks which have been started in state x (before the current time \u03c4) and by TF (x) \u2286 TS(x) the set of tasks that have been finished already in state x. It is easy to see that TS(x) = rng(%) \u2229 T and TF (x) = rng(%|dom(\u03d5)) \u2229 T , where rng(\u00b7) denotes the range or image set of a function. The process starts from an initial state\nxs = \u30080, \u00b5, \u2205, \u2205\u3009, which corresponds to the situation at time zero when none of the operations have been started. The initial probability distribution, \u03b2, can be calculated as follows\n\u03b2(xs) = P (\u00b5(r1) = I(r1), . . . , \u00b5(rn) = I(rn)) ,\nwhere I(r) \u223c i(r) denotes the random variable that determines the initial state of resource r \u2208 R and n = |R|. Thus, \u03b2 renders initial states to resources according to the (multivariate) probability distribution I that is a component of the RAP. We introduce a set of terminal states, as well. A state x is considered as a terminal state (x \u2208 T) in two cases. First, if all the tasks are finished in the state, formally, if TF (x) = T and it can be reached from a state x\u0302, where TF (x\u0302) 6= T . Second, if the system reached a state where no tasks or operations can be executed, in other words, if the allowed set of actions is empty, A(x) = \u2205.\nIt is easy to see that, in theory, we can aggregate all terminal states to a global unique terminal state and introduce a new unique initial state, x0, that has only one available action which takes us randomly (with \u03b2 distribution) to the real initial states. Then, the problem becomes a stochastic shortest path problem and the aim can be described as finding a routing having minimal expected cost from the new initial state to the goal state.\nAt every time \u03c4 the system is informed on the finished operations, and it can decide on the operations to apply (and by which resources). The control action space contains operation-resource assignments avr \u2208 A, where v \u2208 O and r \u2208 R, and a special await control that corresponds to the action when the system does not start a new operation at the current time. In a non-terminal state x = \u3008\u03c4, \u00b5, %, \u03d5\u3009 the available actions are\nawait \u2208 A(x) \u21d4 TS(x) \\ TF (x) 6= \u2205\n\u2200v \u2208 O : \u2200r \u2208 R : avr \u2208 A(x) \u21d4 (v \u2208 O \\ TS(x) \u2227 \u2200 \u3008r\u0302, t\u3009 \u2208 dom(%) \\ dom(\u03d5) : r\u0302 6= r \u2227\n\u2227 \u3008\u00b5(r), v\u3009 \u2208 dom(d) \u2227 v \u2208 T \u21d2 (\u2200u \u2208 T : \u3008u, v\u3009 \u2208 C \u21d2 u \u2208 TF (x)))\nThus, action await is available in every state with an unfinished operation; action avr is available in states in which resource r is idle, it can process operation v, additionally, if v is a task, then it was not executed earlier and its precedence constraints are satisfied.\nIf an action avr \u2208 A(x) is executed in a state x = \u3008\u03c4, \u00b5, %, \u03d5\u3009, then the system moves with probability one to a new state x\u0302 = \u3008\u03c4, \u00b5, %\u0302, \u03d5\u3009, where %\u0302 = % \u222a {\u3008\u3008r, t\u3009 , v\u3009}. Note that we treat functions as sets of ordered pairs. The resulting x\u0302 corresponds to the state where operation v has started on resource r if the previous state of the environment was x.\nThe effect of the await action is that from x = \u3008\u03c4, \u00b5, %, \u03d5\u3009 it takes to an x\u0302 = \u3008\u03c4 + 1, \u00b5\u0302, %, \u03d5\u0302\u3009, where an unfinished operation %(r, t) that was started at t on r finishes with probability\nP(\u3008r, t\u3009 \u2208 dom(\u03d5\u0302) | x, \u3008r, t\u3009 \u2208 dom(%) \\ dom(\u03d5)) = P(D(\u00b5(r), %(r, t)) + t = \u03c4)\nP(D(\u00b5(r), %(r, t)) + t \u2265 \u03c4) ,\nwhere D(s, v) \u223c d(s, v) is a random variable that determines the duration of operation v when it is executed by a resource which has state s. This quantity is called completion rate in stochastic scheduling theory and hazard rate in reliability theory. We remark that for operations with continuous durations, this quantity is defined by f(t)/(1\u2212 F (t)), where f denotes the density function and F the distribution of the random variable that determines the duration of the operation. If operation v = %(r, t) has finished (\u3008r, t\u3009 \u2208 dom(\u03d5\u0302)), then\n\u03d5\u0302(r, t) = \u03c4 and \u00b5\u0302(r) = E(r, v), where E(r, v) \u223c e(r, v) is a random variable that determines the new state of resource r after it has executed operation v. Except the extension of its domain set, the other values of function \u03d5 do not change, consequently, \u2200 \u3008r, t\u3009 \u2208 dom(\u03d5) : \u03d5\u0302(r, t) = \u03d5(r, t). In other words, \u03d5\u0302 is a conservative extension of \u03d5, formally, \u03d5 \u2286 \u03d5\u0302.\nThe immediate-cost function g for a given \u03ba performance measure is defined as follows. Assume that \u03ba depends only on the operation-resource assignments and the completion times. Let x = \u3008\u03c4, \u00b5, %, \u03d5\u3009 and x\u0302 = \u3008\u03c4\u0302 , \u00b5\u0302, %\u0302, \u03d5\u0302\u3009. Then, in general, if the system arrives at state x\u0302 after executing control action a in state x, it incurs cost \u03ba(%, \u03d5)\u2212 \u03ba(%\u0302, \u03d5\u0302).\nNote that, though, in Section 2.2.2 performance measures were defined on complete solutions, for most measures applied in practice, such as total completion time or weighted total lateness, it is straightforward to generalize the performance measure to partial solutions, as well. One may, for example, treat the partial solution of a problem as a complete solution of a smaller (sub)problem, namely, a problem with fewer tasks to be completed.\nIf the control process has failed, more precisely, if it was not possible to finish all tasks, then the immediate-cost function should render penalties (depending on the specific problem) regarding the non-completed tasks proportional to the number of these failed tasks."}, {"heading": "2.5.2 Favorable Features", "text": "Let us call the introduced SSPs, which describe stochastic RAPs, RAP-MDPs. In this section we overview some basic properties of RAP-MDPs. First, it is straightforward to see that these MDPs have finite action spaces, since |A| \u2264 |R| |O|+ 1 always holds.\nThough, the state space of a RAP-MDP is denumerable in general, if the allowed number of non-task operations is bounded and the random variables describing the operation durations are finite, the state space of the reformulated MDP becomes finite, as well.\nWe may also observe that RAP-MDPs are acyclic (or aperiodic), viz., none of the states can appear multiple times, because during the resource allocation process \u03c4 and dom(%) are non-decreasing and, additionally, each time the state changes, the quantity \u03c4 + |dom(%)| strictly increases. Therefore, the system cannot reach the same state twice. As an immediate consequence, all policies eventually terminate (if the MDP was finite) and, thus, are proper.\nFor the effective computation of a good control policy, it is important to try to reduce the number of states. We can do so by recognizing that if the performance measure \u03ba is non-decreasing in the completion times, then an optimal control policy of the reformulated RAP-MDP can be found among the policies which start new operations only at times when another operation has been finished or in an initial state. This statement can be supported by the fact that without increasing the cost (\u03ba is non-decreasing) every operation can be shifted earlier on the resource which was assigned to it until it reaches another operation, or until it reaches a time when one of its preceding tasks is finished (if the operation was a task with precedence constrains), or, ultimately, until time zero. Note that most of the performance measures used in practice (e.g., makespan, weighted completion time, average tardiness) are non-decreasing. As a consequence, the states in which no operation has been finished can be omitted, except the initial states. Therefore, each await action may lead to a state where an operation has been finished. We may consider it, as the system executes automatically an await action in the omitted states. By this way, the state space can be decreased and, therefore, a good control policy can be calculated more effectively."}, {"heading": "2.5.3 Composable Measures", "text": "For a large class of performance measures, the state representation can be simplified by leaving out the past of the process. In order to do so, we must require that the performance measure be composable with a suitable function. In general, we call a function f : P(X) \u2192 R \u03b3-composable if for any A,B \u2286 X, A\u2229B = \u2205 it holds that \u03b3(f(A), f(B)) = f(A\u222aB), where \u03b3 : R\u00d7R \u2192 R is called the composition function, and X is an arbitrary set. This definition can be directly applied to performance measures. If a performance measure, for example, is \u03b3-composable, it indicates that the value of any complete solution can be computed from the values of its disjoint subsolutions (solutions to subproblems) with function \u03b3. In practical situations the composition function is often the max, the min or the \u201c+\u201d function.\nIf the performance measure \u03ba is \u03b3-composable, then the past can be omitted from the state representation, because the performance can be calculated incrementally. Thus, a state can be described as x = \u3008\u03c4\u0304 , \u03ba\u0304, \u00b5\u0304, TU \u3009, where \u03c4\u0304 \u2208 N, as previously, is the current time, \u03ba\u0304 \u2208 R contains the performance of the current (partial) solution and TU is the set of unfinished tasks. The function \u00b5\u0304 : R \u2192 S \u00d7 (O \u222a {\u03b9})\u00d7 N determines the current states of the resources together with the operations currently executed by them (or \u03b9 if a resource is idle) and the starting times of the operations (needed to compute their completion rates).\nIn order to keep the analysis as simple as possible, we restrict ourselves to composable functions, since almost all performance measures that appear in practice are \u03b3-composable for a suitable \u03b3 (e.g., makespan or total production time is max-composable)."}, {"heading": "2.5.4 Reactive Solutions", "text": "Now, we are in a position to define the concept of reactive solutions for stochastic RAPs. A reactive solution is a (stationary, Markov) control policy of the reformulated SSP problem. A reactive solution performs a closed-loop control, since at each time step the controller is informed about the current state of system and it can choose a control action based upon this information. Section 3 deals with the computation of effective control policies."}, {"heading": "3. Solution Methods", "text": "In this section we aim at giving an effective solution to large-scale RAPs in uncertain and changing environments with the help of different machine learning approaches. First, we overview some approximate dynamic programming methods to compute a \u201cgood\u201d policy. Afterwards, we investigate two function approximation techniques to enhance the solution. Clustering, rollout algorithm and action space decomposition as well as distributed sampling are also considered, as they can speedup the computation of a good control policy considerably and, therefore, are important additions if we face large-scale problems."}, {"heading": "3.1 Approximate Dynamic Programming", "text": "In the previous sections we have formulated RAPs as acyclic (aperiodic) SSP problems. Now, we face the challenge of finding a good policy. In theory, the optimal value function of a finite MDP can be exactly computed by dynamic programming (DP) methods, such as value iteration or the Gauss-Seidel method. Alternatively, an exact optimal policy can be directly calculated by policy iteration. However, due to the \u201ccurse of dimensionality\u201d, computing\nan exact optimal solution by these methods is practically infeasible, e.g., typically both the required amount of computation and the needed storage space, viz., memory, grows quickly with the size of the problem. In order to handle the \u201ccurse\u201d, we should apply approximate dynamic programming (ADP) techniques to achieve a good approximation of an optimal policy. Here, we suggest using sampling-based fitted Q-learning (FQL). In each trial a Monte-Carlo estimate of the value function is computed and projected onto a suitable function space. The methods described in this section (FQL, MCMC and the Boltzmann formula) should be applied simultaneously, in order to achieve an efficient solution."}, {"heading": "3.1.1 Fitted Q-learning", "text": "Watkins\u2019 Q-learning is a very popular off-policy model-free reinforcement learning algorithm (Even-Dar & Mansour, 2003). It works with action-value functions and iteratively approximates the optimal value function. The one-step Q-learning rule is defined as follows\nQi+1(x, a) = (1\u2212 \u03b3i(x, a))Qi(x, a) + \u03b3i(x, a)(T\u0303iQi)(x, a),\n(T\u0303iQi)(x, a) = g(x, a) + \u03b1 min B\u2208A(Y ) Qi(Y,B),\nwhere \u03b3i(x, a) are the learning rates and Y is a random variable representing a state generated from the pair (x, a) by simulation, that is, according to the probability distribution p(x, a). It is known (Bertsekas & Tsitsiklis, 1996) that if \u03b3i(x) \u2208 [0, 1] and they satisfy\n\u221e\u2211\ni=0\n\u03b3i(x, a) = \u221e and \u221e\u2211\ni=0\n\u03b32i (x, a) <\u221e,\nthen the Q-learning algorithm converges with probability one to the optimal action-value function, Q\u2217, in the case of lookup table representation when each state-action value is stored independently. We speak about the method of fitted Q-learning (FQL) when the value function is represented by a (typically parametric) function from a suitable function space, F , and after each iteration, the updated value function is projected back onto F .\nA useful observation is that we need the \u201clearning rate\u201d parameters only to overcome the effects of random disturbances. However, if we deal with deterministic problems, this part of the method can be simplified. The resulting algorithm simply updates Q(x, a) with the minimum of the previously stored estimation and the current outcome of the simulation, which is also the core idea of the LRTA* algorithm (Bulitko & Lee, 2006). When we dealt with deterministic resource allocation problems, we applied this simplification, as well."}, {"heading": "3.1.2 Evaluation by Simulation", "text": "Naturally, in large-scale problems we cannot update all states at once. Therefore, we perform Markov chain Monte Carlo (MCMC) simulations (Hastings, 1970; Andrieu, Freitas, Doucet, & Jordan, 2003) to generate samples with the model, which are used for computing the new approximation of the estimated cost-to-go function. Thus, the set of states to be updated in episode i, namely Xi, is generated by simulation. Because RAP-MDPs are acyclic, we apply prioritized sweeping, which means that after each iteration, the cost-to-go estimations are updated in the reverse order in which they appeared during the simulation.\nAssume, for example, that Xi = { xi1, x i 2, . . . , x i ti } is the set of states for the update of the value function after iteration i, where j < k implies that xij appeared earlier during the simulation than xik. In this case the order in which the updates are performed, is x i ti , . . . , xi1. Moreover, we do not need a uniformly optimal value function, it is enough to have a good approximation of the optimal cost-to-go function for the relevant states. A state is called relevant if it can appear with positive probability during the application of an optimal policy. Therefore, it is sufficient to consider the case when xi1 = x0, where x i 1 is the first state in episode i and x0 is the (aggregated) initial state of the SSP problem."}, {"heading": "3.1.3 The Boltzmann Formula", "text": "In order to ensure the convergence of the FQL algorithm, one must guarantee that each cost-to-go estimation be continuously updated. A technique used often to balance between exploration and exploitation is the Boltzmann formula (also called softmin action selection):\n\u03c0i(x, a) = exp(\u2212Qi(x, a)/\u03c4)\u2211\nb\u2208A(x)\nexp(\u2212Qi(x, b)/\u03c4) ,\nwhere \u03c4 \u2265 0 is the Boltzmann (or Gibbs) temperature, i is the episode number. It is easy to see that high temperatures cause the actions to be (nearly) equiprobable, low ones cause a greater difference in selection probability for actions that differ in their value estimations. Note that here we applied the Boltzmann formula for minimization, viz., small values result in high probability. It is advised to extend this approach by a variant of simulated annealing (Kirkpatrick, Gelatt, & Vecchi, 1983) or Metropolis algorithm (Metropolis, Rosenbluth, Rosenbluth, Teller, & Teller, 1953), which means that \u03c4 should be decreased over time, at a suitable, e.g., logarithmic, rate (Singh, Jaakkola, Littman, & Szepesva\u0301ri, 2000)."}, {"heading": "3.2 Cost-to-Go Representations", "text": "In Section 3.1 we suggested FQL for iteratively approximating the optimal value function. However, the question of a suitable function space, onto which the resulted value functions can be effectively projected, remained open. In order to deal with large-scale problems (or problems with continuous state spaces) this question is crucial. In this section, first, we suggest features for stochastic RAPs, then describe two methods that can be applied to compactly represent value functions. The first and simpler one applies hash tables while the second, more sophisticated one, builds upon the theory of support vector machines."}, {"heading": "3.2.1 Feature Vectors", "text": "In order to efficiently apply a function approximator, first, the states and the actions of the reformulated MDP should be associated with numerical vectors representing, e.g., typical features of the system. In the case of stochastic RAPs, we suggest using features as follows:\n\u2022 For each resource in R, the resource state id, the operation id of the operation being currently processed by the resource (could be idle), as well as the starting time of the last (and currently unfinished) operation can be a feature. If the model is available to the system, the expected ready time of the resource should be stored instead.\n\u2022 For each task in T , the task state id could be treated as a feature that can assume one of the following values: \u201cnot available\u201d (e.g., some precedence constraints are not satisfied), \u201cready for execution\u201d, \u201cbeing processed\u201d or \u201cfinished\u201d. It is also advised to apply \u201c1-out-of-n\u201d coding, viz., each value should be associated with a separate bit.\n\u2022 In case we use action-value functions, for each action (resource-operation assignment) the resource id and the operation id could be stored. If the model is available, then the expected finish time of the operation should also be taken into account.\nIn the case of a model-free approach which applies action-value functions, for example, the feature vector would have 3\u00b7|R|+|T |+2 components. Note that for features representing temporal values, it is advised to use relative time values instead of absolute ones."}, {"heading": "3.2.2 Hash Tables", "text": "Suppose that we have a vector w = \u3008w1, w2, . . . , wk\u3009, where each component wi corresponds to a feature of a state or an action. Usually, the value estimations for all of these vectors cannot be stored in the memory. In this case one of the simplest methods to be applied is to represent the estimations in a hash table. A hash table is, basically, a dictionary in which keys are mapped to array positions by hash functions. If all components can assume finite values, e.g., in our finite-state, discrete-time case, then a key could be generated as follows. Let us suppose that for all wi we have 0 \u2264 wi < mi, then w can be seen as a number in a mixed radix numeral system and, therefore, a unique key can be calculated as\n\u03d5(w) = k\u2211\ni=1\nwi\ni\u22121\u220f\nj=1\nmj ,\nwhere \u03d5(w) denotes the key of w, and the value of an empty product is treated as one.\nThe hash function, \u03c8, maps feature vector keys to memory positions. More precisely, if we have memory for storing only d value estimations, then the hash function takes the form \u03c8 : rng(\u03d5) \u2192 {0, . . . , d\u2212 1}, where rng(\u00b7) denotes the range set of a function.\nIt is advised to apply a d that is prime. In this case a usual hashing function choice is \u03c8(x) = y if and only if y \u2261 x (mod d), namely, if y is congruent to x modulo d.\nHaving the keys of more than one item map to the same position is called a collision. In the case of RAP-MDPs we suggest a collision resolution method as follows. Suppose that during a value update the feature vector of a state (or a state-action pair) maps to a position that is already occupied by another estimation corresponding to another item (which can be detected, e.g., by storing the keys). Then we have a collision and the estimation of the new item should overwrite the old estimation if and only if the MDP state corresponding to the new item appears with higher probability during execution starting from the (aggregated) initial state than the one corresponding to the old item. In case of a model-free approach, the item having a state with smaller current time component can be kept.\nDespite its simplicity, the hash table representation has several disadvantages, e.g., it still needs a lot of memory to work efficiently, it cannot easily handle continuous values and, it only stores individual data, moreover, it does not generalize to \u201csimilar\u201d items. In the next section we present a statistical approach that can deal with these issues, as well."}, {"heading": "3.2.3 Support Vector Regression", "text": "A promising choice for compactly representing the cost-to-go function is to use support vector regression (SVR) from statistical learning theory. For maintaining the value function estimations, we suggest using \u03bd-SVRs which were proposed by Scho\u0308lkopf, Smola, Williamson, and Bartlett (2000). They have an advantage over classical \u03b5-SVRs according to which, through the new parameter \u03bd, the number of support vectors can be controlled. Additionally, parameter \u03b5 can be eliminated. First, the core ideas of \u03bd-SVRs are presented.\nIn general, SVR addresses the problem as follows. We are given a sample, a set of data points {\u3008x1, y1\u3009 , . . . , \u3008xl, yl\u3009}, such that xi \u2208 X is an input, where X is a measurable space, and yi \u2208 R is the target output. For simplicity, we shall assume that X \u2286 R\nk, where k \u2208 N. The aim of the learning process is to find a function f : X \u2192 R with a small risk\nR[f ] =\n\u222b\nX l(f, x, y)dP (x, y), (3)\nwhere P is a probability measure, which is responsible for the generation of the observations and l is a loss function, such as l(f, x, y) = (f(x) \u2212 y)2. A common error function used in SVRs is the so-called \u03b5-insensitive loss function, |f(x)\u2212 y|\u03b5 = max {0, |f(x)\u2212 y| \u2212 \u03b5}. Unfortunately, we cannot minimize (3) directly, since we do not know P , we are given the sample, only (generated, e.g., by simulation). We try to obtain a small risk by minimizing the regularized risk functional in which we average over the training sample\n1 2 \u2016w\u20162 + C \u00b7R\u03b5emp[f ], (4)\nwhere, \u2016w\u20162 is a term that characterizes the model complexity and C > 0 a constant that determines the trade-off between the flatness of the regression and the amount up to which deviations larger than \u03b5 are tolerated. The function R\u03b5emp[f ] is defined as follows\nR\u03b5emp[f ] = 1\nl\nl\u2211\ni=1\n|f(xi)\u2212 yi|\u03b5.\nIt measures the \u03b5-insensitive average training error. The problem which arises when we try to minimize (4) is called empirical risk minimization (ERM). In regression problems we usually have a Hilbert space F , containing X \u2192 R type (typically non-linear) functions, and our aim is to find a function f that is \u201cclose\u201d to yi in each xi and takes the form\nf(x) = \u2211\nj wj\u03c6j(x) + b = w\nT\u03c6(x) + b,\nwhere \u03c6j \u2208 F , wj \u2208 R and b \u2208 R. Using Lagrange multiplier techniques, we can rewrite the regression problem in its dual form (Scho\u0308lkopf et al., 2000) and arrive at the final \u03bd-SVR optimization problem. The resulting regression estimate then takes the form as follows\nf(x) = l\u2211\ni=1\n(\u03b1\u2217i \u2212 \u03b1i)K(xi, x) + b,\nwhere \u03b1i and \u03b1 \u2217 i are the Lagrange multipliers, and K denotes an inner product kernel defined by K(x, y) = \u3008\u03c6(x), \u03c6(y)\u3009, where \u3008\u00b7, \u00b7\u3009 denotes inner product. Note that \u03b1i, \u03b1 \u2217 i 6= 0\nholds usually only for a small subset of training samples, furthermore, parameter b (and \u03b5) can be determined as well, by applying the Karush-Kuhn-Tucker (KKT) conditions.\nMercer\u2019s theorem in functional analysis characterizes which functions correspond to an inner product in some space F . Basic kernel types include linear, polynomial, Gaussian and sigmoid functions. In our experiments with RAP-MDPs we have used Gaussian kernels which are also called radial basis function (RBF) kernels. RBF kernels are defined by K(x, y) = exp(\u2212\u2016x\u2212 y\u20162 /(2\u03c32)), where \u03c3 > 0 is an adjustable kernel parameter.\nA variant of the fitted Q-learning algorithm combined with regression and softmin action selection is described in Table 1. It simulates a state-action trajectory with the model and updates the estimated values of only the state-action pairs which appeared during the simulation. Most of our RAP solutions described in the paper are based on this algorithm.\nThe notations of the pseudocode shown in Table 1 are as follows. Variable i contains the episode number, ti is the length of episode i and j is a parameter for time-steps inside an episode. The Boltzmann temperature is denoted by \u03c4 , \u03c0i is the control policy applied in episode i and x0 is the (aggregated) initial state. State x i j and action a i j correspond to step j in episode i. Function h computes features for state-action pairs while \u03b3i denotes learning rates. Finally, Li denotes the regression sample and Qi is the fitted value function.\nAlthough, support vector regression offers an elegant and efficient solution to the value function representation problem, we presented the hash table representation possibility not only because it is much easier to implement, but also because it requires less computation, thus, provides faster solutions. Moreover, the values of the hash table could be accessed independently; this was one of the reasons why we applied hash tables when we dealt with distributed solutions, e.g., on architectures with uniform memory access. Nevertheless, SVRs have other advantages, most importantly, they can \u201cgeneralize\u201d to \u201csimilar\u201d data."}, {"heading": "3.3 Additional Improvements", "text": "Computing a (close-to) optimal solution with RL methods, such as (fitted) Q-learning, could be very inefficient in large-scale systems, even if we apply prioritized sweeping and a capable representation. In this section we present some additional improvements in order to speed up to optimization process, even at the expense of achieving only suboptimal solutions."}, {"heading": "3.3.1 Rollout Algorithms", "text": "During our experiments, which are presented in Section 4, it turned out that using a suboptimal base policy, such as a greedy policy with respect to the immediate costs, to guide the exploration, speeds up the optimization considerably. Therefore, at the initial stage we suggest applying a rollout policy, which is a limited lookahead policy, with the optimal cost-to-go approximated by the cost-to-go of the base policy (Bertsekas, 2001). In order to introduce the concept more precisely, let \u03c0\u0302 be the greedy policy w.r.t. immediate-costs,\n\u03c0\u0302(x) \u2208 argmin a\u2208A(x) g(x, a).\nThe value function of \u03c0\u0302 is denoted by J \u03c0\u0302. The one-step lookahead rollout policy \u03c0 based on policy \u03c0\u0302, which is an improvement of \u03c0\u0302 (cf. policy iteration), can be calculated by\n\u03c0(x) \u2208 argmin a\u2208A(x) E\n[ G(x, a) + J \u03c0\u0302(Y ) ] ,\nwhere Y is a random variable representing a state generated from the pair (x, a) by simulation, that is, according to probability distribution p(x, a). The expected value (viz., the expected costs and the cost-to-go of the base policy) is approximated by Monte Carlo simulation of several trajectories that start at the current state. If the problem is deterministic, then a single simulation trajectory suffices, and the calculations are greatly simplified.\nTake a look at Figure 6 for an illustration. In scheduling theory, a similar (but simplified) concept can be found and a rollout policy would be called a dispatching rule.\nThe two main issues why we suggest the application of rollout algorithms in the initial stages of value function approximation-based reinforcement learning are as follows:\n1. We need several initial samples before the first application of approximation techniques and these first samples can be generated by simulations guided by a rollout policy.\n2. General reinforcement learning methods perform quite poorly in practice without any initial guidance. However, the learning algorithm can start improving the rollout policy \u03c0, especially, in case we apply (fitted) Q-learning, it can learn directly from the trajectories generated by a rollout policy, since it is an off-policy learning method.\nOur numerical experiments showed that rollout algorithms provide significant speedup."}, {"heading": "3.3.2 Action Space Decomposition", "text": "In large-scale problems the set of available actions in a state may be very large, which can slow down the system significantly. In the current formulation of the RAP the number of available actions in a state is O(|T | |R|). Though, even in real world situations |R| is, usually, not very large, but T could contain thousands of tasks. Here, we suggest decomposing the action space as shown in Figure 7. First, the system selects a task, only, and it moves to a new state where this task is fixed and an executing resource should be selected. In this case the state description can be extended by a new variable \u03c4 \u2208 T \u222a {\u2205}, where \u2205 denotes the case when no task has been selected yet. In every other case the system should select an executing resource for the selected task. Consequently, the new action space is A = A1 \u222a A2, where A1 = { av | v \u2208 T } \u222a {a\u03c9} and A2 = { ar | r \u2208 R}. As a result, we radically decreased the number of available actions, however, the number of possible states was increased. Our experiments showed that it was a reasonable trade-off."}, {"heading": "3.3.3 Clustering the Tasks", "text": "The idea of divide-and-conquer is widely used in artificial intelligence and recently it has appeared in the theory of dealing with large-scale MDPs. Partitioning a problem into several smaller subproblems is also often applied to decrease computational complexity in combinatorial optimization problems, for example, in scheduling theory.\nWe propose a simple and still efficient partitioning method for a practically very important class of performance measures. In real world situations the tasks very often have release dates and due dates, and the performance measure, e.g., total lateness and number of tardy tasks, depends on meeting the deadlines. Note that these measures are regular. We denote the (possibly randomized) functions defining the release and due dates of the tasks by A : T \u2192 N and B : T \u2192 N, respectively. In this section we restrict ourselves to performance measures that are regular and depend on due dates. In order to cluster the tasks, we need the definition of weighted expected slack time which is given as follows\nSw(v) = \u2211\ns\u2208\u0393(v)\nw(s)E [ B(v)\u2212A(v)\u2212D(s, v) ] ,\nwhere \u0393(v) = { s \u2208 S | \u3008s, v\u3009 \u2208 dom(D) } denotes the set of resource states in which task v can be processed, and w(s) are weights corresponding, for example, to the likelihood that resource state s appears during execution, or they can be simply w(s) = 1/ |\u0393(v)|.\nIn order to increase computational speed, we suggest clustering the tasks in T into successive disjoint subsets T1, . . . , Tk according to the precedence constraints and the expected slack times; take a look at Figure 8 for an illustration. The basic idea behind our approach is that we should handle the most constrained tasks first. Therefore, ideally, if Ti and Tj are two clusters and i < j, then tasks in Ti had expected slack times smaller than tasks in Tj . However, in most of the cases clustering is not so simple, since the precedence constraints must also be taken into account and this clustering criterion has the priority. Thus, if \u3008u, v\u3009 \u2208 C, u \u2208 Ti and v \u2208 Tj , then i \u2264 j must hold. During learning, first, tasks in T1 are allocated to resources, only. After some episodes, we fix the allocation policy concerning tasks in T1 and we start sampling to achieve a good policy for tasks in T2, and so on.\nNaturally, clustering the tasks is a two-edged weapon, making too small clusters may seriously decrease the performance of the best achievable policy, making too large clusters\nmay considerably slow down the system. This technique, however, has several advantages, e.g., (1) it effectively decreases the search space; (2) further reduces the number of available actions in the states; and, additionally (3) speeds up the learning, since the sample trajectories become smaller (only a small part of the tasks is allocated in a trial and, consequently, the variance of the total costs is decreased). The effects of clustering relative to the size of the clusters were analyzed experimentally and are presented in Section 4.5."}, {"heading": "3.3.4 Distributed Sampling", "text": "Finally, we argue that the presented approach can be easily modified in order to allow computing a policy on several processors in a distributed way. Parallel computing can further speed up the calculation of the solution. We will consider extensions of the algorithm using both shared memory and distributed memory architectures. Let us suppose we have k processors, and denote the set of all processors by P = {p1, p2, . . . , pk}.\nIn case we have a parallel system with a shared memory architecture, e.g., UMA (uniform memory access), then it is straightforward to parallelize the computation of a control policy. Namely, each processor p \u2208 P can sample the search space independently, while by using the same, shared value function. The (joint) policy can be calculated using this common, global value function, e.g., the greedy policy w.r.t. this function can be applied.\nParallelizing the solution by using an architecture with distributed memory is more challenging. Probably the simplest way to parallelize our approach to several processors with distributed memory is to let the processors search independently by letting them working with their own, local value functions. After a given time or number of iterations, we may treat the best achieved solution as the joint policy. More precisely, if we denote the aggregated initial state by x0, then the joint control policy \u03c0 can be defined as follows\n\u03c0 \u2208 argmin \u03c0p (p\u2208P) J\u03c0p(x0) or \u03c0 \u2208 argmin \u03c0p (p\u2208P) min a\u2208A(x0) Q\u03c0p(x0, a),\nwhere J\u03c0p and Q\u03c0p are (approximate) state- and action-value functions calculated by processor p \u2208 P. Control policy \u03c0p is the solution of processor p after a given number of iterations. During our numerical experiments we usually applied 104 iterations.\nNaturally, there could be many (more sophisticated) ways to parallelize the computation using several processors with distributed memory. For example, from time to time the processors could exchange some of their best episodes (trajectories with the lowest costs) and learn from the experiments of the others. In this way, they could help improve the value functions of each other. Our numerical experiments, presented in Section 4.3, showed that even in the simplest case, distributing the calculation speeds up the optimization considerably. Moreover, in the case of shared memory the speedup was almost linear.\nAs parallel computing represents a very promising way do deal with large-scale systems, their further theoretical and experimental investigation would be very important. For example, by harmonizing the exploration of the processors, the speedup could be improved."}, {"heading": "4. Experimental Results", "text": "In this section some experimental results on both benchmark and industry-related problems are presented. These experiments highlight some characteristics of the solution."}, {"heading": "4.1 Testing Methodology", "text": "In order to experimentally study our resource control approach, a simulation environment was developed in C++. We applied FQL and, in most of the cases, SVRs which were realized by the LIBSVM free library for support vector machines (Chang & Lin, 2001). After centering and scaling the data into interval [0, 1], we used Gaussian kernels and shrinking techniques. We always applied rollout algorithms and action decomposition, but clustering was only used in tests presented in Section 4.5, furthermore, distributed sampling was only applied in test shown in Section 4.3. In both of the latter cases (tests for clustering and distributed sampling) we used hash tables with approximately 256Mb hash memory.\nThe performance of the algorithm was measured as follows. Testing took place in two main fields: the first one was a benchmark scheduling dataset of hard problems, the other one was a simulation of a \u201creal world\u201d production control problem. In the first case the best solution, viz., the optimal value of the (aggregated) initial state, J\u2217(x0) = minaQ\n\u2217(x0, a), was known for most of the test instances. Some \u201cvery hard\u201d instances occurred for which only lower and upper bounds were known, e.g., J\u22171 (x0) \u2264 J \u2217(x0) \u2264 J \u2217 2 (x0). In these cases we assumed that J\u2217(x0) \u2248 (J \u2217 1 (x0) + J \u2217 2 (x0))/2. Since these estimations were \u201cgood\u201d (viz., the length of the intervals were short), this simplification might not introduce considerable error to our performance estimations. In the latter test case we have generated the problems with a generator in a way that J\u2217(x0) was known concerning the constructed problems.\nThe performance presented in the tables of the section, more precisely the average, Ei, and the standard deviation, \u03c3(Ei), of the error in iteration i were computed as follows\nEi = 1\nN\nN\u2211\nj=1\n[ G ij \u2212 J \u2217(x0) ] , and \u03c3(Ei) = \u221a\u221a\u221a\u221a 1 N N\u2211\nj=1\n[ G ij \u2212 J \u2217(x0)\u2212 Ei ]2 ,\nwhere G ij denotes the cumulative incurred costs in iteration i of sample j and N is the sample size. Unless indicated otherwise, the sample contained the results of 100 simulation trials for each parameter configuration (which is associated with the rows of the tables).\nAs it was shown in Section 2.5.2, RAP-MDPs are aperiodic, moreover, they have the APP property, therefore, discounting is not necessary to achieve a well-defined problem. However, in order to enhance learning, it is still advised to apply discounting and, therefore, to give less credit to events which are farther from the current decision point. Heuristically, we suggest applying \u03b1 = 0.95 for middle-sized RAPs (e.g., with few hundreds of tasks), such as the problems of the benchmark dataset, and \u03b1 = 0.99 for large-scale RAPs (e.g., with few thousands of tasks), such as the problems of the industry-related experiments."}, {"heading": "4.2 Benchmark Datasets", "text": "The ADP based resource control approach was tested on Hurink\u2019s benchmark dataset (Hurink, Jurisch, & Thole, 1994). It contains flexible job-shop scheduling problems (FJSPs) with 6\u201330 jobs (30\u2013225 tasks) and 5\u201315 machines. The applied performance measure is the maximum completion time of the tasks (makespan). These problems are \u201chard\u201d, which means, e.g., that standard dispatching rules or heuristics perform poorly on them. This dataset consists of four subsets, each subset contains about 60 problems. The subsets (sdata, edata, rdata, vdata) differ in the ratio of machine interchangeability (flexibility), which is\nshown in the \u201cflex(ib)\u201d columns in Tables 3 and 2. The columns with label \u201cn iters\u201d (and \u201cavg err\u201d) show the average error after carrying out altogether \u201cn\u201d iterations. The \u201cstd dev\u201d columns in the tables of the section contain the standard deviation of the sample.\nTable 2 illustrates the performance on some typical dataset instances and also gives some details on them, e.g., the number of machines and jobs (columns with labels \u201cmcs\u201d and \u201cjbs\u201d). In Table 3 the summarized performance on the benchmark datasets is shown.\nSimple dispatching rules (which are often applied in practice), such as greedy ones, perform poorly on these benchmark datasets. Their average error is around 25\u201330 %. In contrast, Table 3 demonstrates that using our method, the average error is less than 5 % after 10 000 iterations. It shows that learning is beneficial for this type of problems.\nThe best performance on these benchmark datasets was achieved by Mastrolilli and Gambardella (2000). Though, their algorithm performs slightly better than ours, their solution exploits the (unrealistic) specialties of the dataset, e.g., the durations do not depend on the resources; the tasks are linearly ordered in the jobs; each job consists of the same\nnumber of tasks. Moreover, it cannot be easily generalized to stochastic resource control problem our algorithm faces. Therefore, the comparison of the solutions is hard."}, {"heading": "4.3 Distributed Sampling", "text": "The possible parallelizations of the presented method was also investigated, i.e., the speedup of the system relative to the number of processors (in practise, the multiprocessor environment was emulated on a single processor, only). The average number of iterations was studied, until the system could reach a solution with less than 5% error on Hurink\u2019s dataset. The average speed of a single processor was treated as a unit, for comparison.\nIn Figure 9 two cases are shown: in the first case (rear dark bars) each processor could access a common global value function. It means that each processor could read and write the same global value function, but otherwise, they searched (sampled the search space) independently. Figure 9 demonstrates that in this case the speedup was almost linear.\nIn the second case (front light bars) each processor had its own (local) value function (which is more realistic in a strongly distributed system, such as a GRID) and, after the search had been finished, these individual value functions were compared. Therefore, all of the processors had estimations of their own, and after the search, the local solution of the best performing processor was selected. Figure 9 shows the achieved speedup in case we stopped the simulation if any of the processors achieved a solution with less than 5% error.\nThe experiments show that the computation of the resource allocator function can be effectively distributed, even if there is not a commonly accessible value function available."}, {"heading": "4.4 Industry Related Tests", "text": "We also initiated experiments on a simulated factory by modeling the structure of a real plant producing customized mass-products, especially, light bulbs. These industrial data came from a huge national industry-academia project, for research and development of solutions which support manufacturing enterprises in coping with the requirements of adaptiveness, realtimeness and cooperativeness (Monostori, Kis, Ka\u0301da\u0301r, Va\u0301ncza, & Erdo\u030bs, 2008).\nSince, we did not have access to historical data concerning past orders, we used randomly generated orders (jobs) with random due dates. The tasks and the process-plans of the jobs, however, covered real products; as well as, the resources covered real machine types. In this plant the machines require product-type dependent setup times, and there are some special tasks that have durations but that do not require any resources to be processed, for example, cooling down. Another feature of the plant is that at some previously given time points preemptions are allowed, e.g., at the end of a work shift. The applied performance measure was to minimize the number of late jobs, viz., jobs that are finished after their due dates, and an additional secondary measure was to minimize the total cumulative lateness, which can be applied to compare two schedules having the same number of late jobs.\nDuring these experiments the jobs and their due dates were generated by a special parameterizable generator in a way that optimally none of the jobs were late. Therefore, it was known that J\u2217(x0) = 0 and the error of the algorithm was computed accordingly.\nIn the first case, shown in Table 4, we applied 16 machines and 100 random jobs, which altogether contained more than 200 tasks. The convergence properties were studied relative to the optimal slack ratio. In the deterministic case, e.g., the slack ratio of a solution is\n\u03a6(%) = 1\nn\nn\u2211\ni=1\nB(Ji)\u2212 F (Ji) B(Ji)\u2212A(Ji) ,\nwhere n is the number of jobs; A(J) and B(J) denote the release and due date of job J , respectively; F (J) is the finish time of job J relative to solution %, namely, the latest finish time of the tasks in the job. Roughly, the slack ratio measures the tightness of the solution, for example, if \u03a6(%) > 0, then it shows that the jobs were, on the average, finished before their due dates and if \u03a6(%) < 0, then it indicates that, approximately, many jobs were late. If \u03a6(%) = 0, then it shows that if all the jobs meet their due dates, each job was finished\njust in time, there were no spare (residual) times. Under the optimal slack ratio we mean the maximal achievable slack ratio (by an optimal solution). During the experiments these values were known because of the special construction of the test problem instances. We applied the optimal slack ratio to measure how \u201chard\u201d a problem is. The first column of Table 4 shows the optimal slack ratio in percentage, e.g., 30% means a 0.3 slack ratio.\nIn the second case, shown in Table 5, we have fixed the optimal slack ratio of the system to 10 % and investigated the convergence speed relative to the plant size (number of machines) and the number of tasks. In the last two experiments (configuration having 2000 and 10 000 tasks) only 10 samples were generated, because of the long runtime. The computation of 10 000 iterations took approximately 30 minutes for the 50 machines & 2000 tasks configuration and 3 hours for the 100 machines & 10000 tasks configuration1.\nThe results demonstrate that the ADP and adaptive sampling based solution scales well with both the slack ratio and the size (the number of machines and task) of the problem."}, {"heading": "4.5 Clustering Experiments", "text": "The effectiveness of clustering on industry-related data was also studied. We considered a system with 60 resources and 1000 random tasks distributed among 400\u2013500 jobs (there were approximately 1000\u20132000 precedence constraints). The tasks were generated in a way that, in the optimal case, none of them are late and the slack ratio is about 20%.\nFirst, the tasks were ordered according to their slack times and then they were clustered. We applied 104 iterations on each cluster. The computational time in case of using only one cluster was treated as a unit. In Table 6 the average and the standard deviation of the error and the computational speedup are shown relative to the number tasks in a cluster.\nThe results demonstrate that partitioning the search space not only results in a greater speed, but it is often accompanied by better solutions. The latter phenomenon can be explained by the fact that using smaller sample trajectories generates smaller variance that is preferable for learning. On the other hand, making too small clusters may decrease the performance (e.g., making 50 clusters with 20 tasks in the current case). In our particular case applying 20 clusters with approximately 50 tasks in each cluster balances good performance (3.02% error on the average) with remarkable speedup (approximately 3.28\u00d7).\n1. The tests were performed on a Centrino (Core-Duo) 1660Mhz CPU (\u2248 P4 3GHz) with 1Gb RAM.\nAs clustering the tasks represents a considerable help in dealing with large-scale RAPs, their further theoretical and experimental investigations are very promising."}, {"heading": "4.6 Adaptation to Disturbances", "text": "In order to verify the proposed algorithm in changing environments, experiments were initiated and carried out on random JSPs with the aim of minimizing the makespan. The adaptive features of the system were tested by confronting it with unexpected events, such as: resource breakdowns, new resource availability (Figure 10), new job arrivals or job cancellations (Figure 11). In Figures 10 and 11 the horizontal axis represents time, while the vertical one, the achieved performance measure. The figures were made by averaging hundred random samples. In these tests 20 machines were used with few dozens of jobs.\nDuring each test episode there was an unexpected event (disturbance) at time t = 100. After the change took place, we considered two possibilities: we either restarted the iterative scheduling process from scratch or continued the learning, using the current (obsolete) value function. We experienced that the latter approach is much more efficient.\nAn explanation of this phenomenon can be that the value functions of control policies and the optimal value function Lipschitz continuously depend on the transition-probability and the immediate-cost functions of the MDP (Csa\u0301ji, 2008). Therefore, small changes in the environmental dynamics cannot cause arbitrary large changes in the value function. The results of our numerical experiments, shown in Figures 10 and 11, are indicative of the phenomenon that the average change of the value function is not very large. Consequently, applying the obsolete value function after a change took place in the MDP is preferable over restarting the whole optimization process from scratch. This adaptive feature makes ADP/RL based approaches even more attractive for practical applications.\nThe results, black curves, show the case when the obsolete value function approximation was applied after the change took place. The performance which would arise if the system recomputed the whole schedule from scratch is drawn in gray in part (a) of Figure 10.\nOne can notice that even if the problem became \u201ceasier\u201d after the change in the environment (at time t = 100), for example, a new resource was available (part (b) of Figure 10) or a job was cancelled (part (b) of Figure 11), the performance started to slightly decrease (\u03ba started to slightly increase) after the event. This phenomenon can be explained by the fact that even in these special cases the system had to \u201cexplore\u201d the new configuration."}, {"heading": "5. Concluding Remarks", "text": "Efficient allocation of scarce, reusable resources over time in uncertain and dynamic environments is an important problem that arises in many real world domains, such as production control. The paper took a machine learning (ML) approach to this problem. First, a general resource allocation framework was presented and, in order to define reactive solutions, it was reformulated as a stochastic shortest path problem, a special Markov decision process (MDP). The core idea of the solution was the application of approximate dynamic programming (ADP) and reinforcement learning (RL) techniques with Monte Carlo simulation to stochastic resource allocation problems (RAPs). Regarding compact value function representations, two approaches were studied: hash table and support vector regression (SVR), specially \u03bd-SVRs. Afterwards, several additional improvements, such as the application of limited-lookahead rollout algorithms in the initial phases, action space decomposition, task clustering and distributed sampling, were suggested for speeding up the computation of a good control policy. Finally, the effectiveness of the approach was demonstrated by results of numerical simulation experiments on both benchmark and industry-related data. These experiments also supported the adaptive capabilities of the proposed method.\nThere are several advantages why ML based resource allocation is preferable to other kinds of RAP solutions, e.g., classical approaches. These favorable features are as follows:\n1. The presented RAP framework is very general, it can model several resource management problems that appear in practice, such as scheduling problems, transportation problems, inventory management problems or maintenance and repair problems.\n2. ADP/RL based methods essentially face the problem under the presence of uncertainties, since their theoretical foundation is provided by MDPs. Moreover, they can adapt to unexpected changes in the environmental dynamics, such as breakdowns.\n3. Additionally, for most algorithms theoretical guarantees of finding (approximately) optimal solutions, at least in the limit, are known. As demonstrated by our experiments, the actual convergence speed for RAPs is usually high, especially in the case of applying the described improvements, such as clustering or distributed sampling.\n4. The simulation experiments on industrial data also demonstrate that ADP/RL based solutions scale well with the workload and the size of the problem and, therefore, they can be effectively applied to handle real world RAPs, such as production scheduling.\n5. Domain specific knowledge can also be incorporated into the solution. The base policy of the rollout algorithm, for example, can reflect a priori knowledge about the structure of the problem; later this knowledge may appear in the exploration strategy.\n6. Finally, the proposed method constitutes an any-time solution, since the sampling can be stopped after any number of iterations. By this way, the amount of computational time can be controlled, which is also an important practical advantage.\nConsequently, ML approaches have great potentials in dealing with real world RAPs, since they can handle large-scale problems even in dynamic and uncertain environments.\nSeveral further research directions are possible. Now, as a conclusion to the paper, we highlight some of them. The suggested improvements, such as clustering and distributed sampling, should be further investigated since they resulted in considerable speedup. The guidance of reinforcement learning with rollout algorithms might be effectively applied in other applications, as well. The theoretical analysis of the average effects of environmental changes on the value functions could result in new approaches to handle disturbances. Another promising direction would be to extend the solution in a way which also takes risk into account and, e.g., minimizes not only the expected value of the total costs but also the deviation, as a secondary criterion. Finally, trying to apply the solution in a pilot project to control a real plant would be interesting and could motivate further research directions."}, {"heading": "6. Acknowledgments", "text": "The work was supported by the Hungarian Scientific Research Fund (OTKA), Grant No. T73376, and by the EU-funded project \u201cColl-Plexity\u201d, Grant No. 12781 (NEST). Bala\u0301zs Csana\u0301d Csa\u0301ji greatly acknowledges the scholarship of the Hungarian Academy of Sciences. The authors express their thanks to Tama\u0301s Kis for his contribution related to the tests on industrial data and to Csaba Szepesva\u0301ri for the helpful discussions on machine learning."}], "references": [{"title": "An introduction to MCMC (Markov Chain Monte Carlo) for machine learning", "author": ["C. Andrieu", "N.D. Freitas", "A. Doucet", "M.I. Jordan"], "venue": "Machine Learning,", "citeRegEx": "Andrieu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Andrieu et al\\.", "year": 2003}, {"title": "Dynamic job-shop scheduling using reinforcement learning agents", "author": ["M.E. Aydin", "E. \u00d6ztemel"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Aydin and \u00d6ztemel,? \\Q2000\\E", "shortCiteRegEx": "Aydin and \u00d6ztemel", "year": 2000}, {"title": "Proactive algorithms for job shop scheduling with probabilistic durations", "author": ["J.C. Beck", "N. Wilson"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Beck and Wilson,? \\Q2007\\E", "shortCiteRegEx": "Beck and Wilson", "year": 2007}, {"title": "Adaptive Control Processes", "author": ["R.E. Bellman"], "venue": "Princeton University Press.", "citeRegEx": "Bellman,? 1961", "shortCiteRegEx": "Bellman", "year": 1961}, {"title": "Dynamic programming and suboptimal control: A survey from ADP to MPC", "author": ["D.P. Bertsekas"], "venue": "European Journal of Control, 11 (4\u20135), 310\u2013334.", "citeRegEx": "Bertsekas,? 2005", "shortCiteRegEx": "Bertsekas", "year": 2005}, {"title": "Dynamic Programming and Optimal Control (2nd edition)", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific, Belmont, Massachusetts.", "citeRegEx": "Bertsekas,? 2001", "shortCiteRegEx": "Bertsekas", "year": 2001}, {"title": "Learning in real-time search: A unifying framework", "author": ["V. Bulitko", "G. Lee"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bulitko and Lee,? \\Q2006\\E", "shortCiteRegEx": "Bulitko and Lee", "year": 2006}, {"title": "LIBSVM: A library for support vector machines. Software available on-line at http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm", "author": ["C.C. Chang", "C.J. Lin"], "venue": null, "citeRegEx": "Chang and Lin,? \\Q2001\\E", "shortCiteRegEx": "Chang and Lin", "year": 2001}, {"title": "Adaptive Resource Control: Machine Learning Approaches to Resource Allocation in Uncertain and Changing Environments", "author": ["Cs\u00e1ji", "B. Cs."], "venue": "Ph.D. thesis, Faculty of Informatics, E\u00f6tv\u00f6s Lor\u00e1nd University, Budapest.", "citeRegEx": "Cs\u00e1ji and Cs.,? 2008", "shortCiteRegEx": "Cs\u00e1ji and Cs.", "year": 2008}, {"title": "Improving multi-agent based scheduling by neurodynamic programming", "author": ["Cs\u00e1ji", "B. Cs", "B. K\u00e1d\u00e1r", "L. Monostori"], "venue": "In Proceedings of the 1st International Conference on Holonic and Mult-Agent Systems for Manufacturing,", "citeRegEx": "Cs\u00e1ji et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Cs\u00e1ji et al\\.", "year": 2003}, {"title": "Adaptive sampling based large-scale stochastic resource control", "author": ["Cs\u00e1ji", "B. Cs", "L. Monostori"], "venue": "In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI", "citeRegEx": "Cs\u00e1ji et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cs\u00e1ji et al\\.", "year": 2006}, {"title": "Resource allocation among agents with MDP-induced preferences", "author": ["D.A. Dolgov", "E.H. Durfee"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dolgov and Durfee,? \\Q2006\\E", "shortCiteRegEx": "Dolgov and Durfee", "year": 2006}, {"title": "Learning rates for Q-learning", "author": ["E. Even-Dar", "Y. Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Even.Dar and Mansour,? \\Q2003\\E", "shortCiteRegEx": "Even.Dar and Mansour", "year": 2003}, {"title": "Handbook of Markov Decision Processes: Methods and Applications", "author": ["E.A. Feinberg", "A. Shwartz"], "venue": null, "citeRegEx": "Feinberg and Shwartz,? \\Q2002\\E", "shortCiteRegEx": "Feinberg and Shwartz", "year": 2002}, {"title": "Improving iterative repair strategies for scheduling with the SVM", "author": ["K. Gersmann", "B. Hammer"], "venue": null, "citeRegEx": "Gersmann and Hammer,? \\Q2005\\E", "shortCiteRegEx": "Gersmann and Hammer", "year": 2005}, {"title": "Monte Carlo sampling methods using Markov chains and their application", "author": ["W.K. Hastings"], "venue": "Biometrika, 57, 97\u2013109.", "citeRegEx": "Hastings,? 1970", "shortCiteRegEx": "Hastings", "year": 1970}, {"title": "Intelligent manufacturing systems - a tentative forecast", "author": ["J. Hatvany", "L. Nemes"], "venue": "Proceedings of the 7th IFAC World Congress,", "citeRegEx": "Hatvany and Nemes,? \\Q1978\\E", "shortCiteRegEx": "Hatvany and Nemes", "year": 1978}, {"title": "Tabu search for the job shop scheduling problem with multi-purpose machines", "author": ["E. Hurink", "B. Jurisch", "M. Thole"], "venue": "Operations Research Spektrum,", "citeRegEx": "Hurink et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Hurink et al\\.", "year": 1994}, {"title": "Effective neighborhood functions for the flexible job shop problem", "author": ["M. Mastrolilli", "L.M. Gambardella"], "venue": "Journal of Scheduling,", "citeRegEx": "Mastrolilli and Gambardella,? \\Q2000\\E", "shortCiteRegEx": "Mastrolilli and Gambardella", "year": 2000}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A. Rosenbluth", "M. Rosenbluth", "A. Teller", "E. Teller"], "venue": "Journal of Chemical Physics,", "citeRegEx": "Metropolis et al\\.,? \\Q1953\\E", "shortCiteRegEx": "Metropolis et al\\.", "year": 1953}, {"title": "Real-time cooperative enterprises for mass-customized production", "author": ["L. Monostori", "T. Kis", "B. K\u00e1d\u00e1r", "J. V\u00e1ncza", "G. Erd\u0151s"], "venue": "International Journal of Computer Integrated Manufacturing,", "citeRegEx": "Monostori et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Monostori et al\\.", "year": 2008}, {"title": "Computational Complexity", "author": ["C.H. Papadimitriou"], "venue": "Addison-Wesley.", "citeRegEx": "Papadimitriou,? 1994", "shortCiteRegEx": "Papadimitriou", "year": 1994}, {"title": "Scheduling: Theory, Algorithms, and Systems", "author": ["M. Pinedo"], "venue": "Prentice-Hall.", "citeRegEx": "Pinedo,? 2002", "shortCiteRegEx": "Pinedo", "year": 2002}, {"title": "Handbook of Learning and Approximate Dynamic Programming, chap. Approximate Dynamic Programming for High-Dimensional Resource Allocation Problems, pp. 261\u2013283", "author": ["W.B. Powell", "B. Van Roy"], "venue": null, "citeRegEx": "Powell and Roy,? \\Q2004\\E", "shortCiteRegEx": "Powell and Roy", "year": 2004}, {"title": "A neural reinforcement learning approach to learn local dispatching policies in production scheduling", "author": ["S. Riedmiller", "M. Riedmiller"], "venue": "In Proceedings of the 16th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Riedmiller and Riedmiller,? \\Q1999\\E", "shortCiteRegEx": "Riedmiller and Riedmiller", "year": 1999}, {"title": "Value function based production scheduling", "author": ["J.G. Schneider", "J.A. Boyan", "A.W. Moore"], "venue": "In Proceedings of the 15th International Conference on Machine Learning,", "citeRegEx": "Schneider et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Schneider et al\\.", "year": 1998}, {"title": "New support vector algorithms", "author": ["B. Sch\u00f6lkopf", "A. Smola", "R.C. Williamson", "P.L. Bartlett"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2000}, {"title": "Convergence results for single-step on-policy reinforcement-learning algorithms", "author": ["S. Singh", "T. Jaakkola", "M. Littman", "Szepesv\u00e1ri", "Cs"], "venue": "Machine Learning,", "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Mathematical Control Theory: Deterministic Finite Dimensional Systems", "author": ["E.D. Sontag"], "venue": "Springer, New York.", "citeRegEx": "Sontag,? 1998", "shortCiteRegEx": "Sontag", "year": 1998}, {"title": "Reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "A distributed decision-making structure for dynamic resource allocation using nonlinear function approximators", "author": ["H. Topaloglu", "W.B. Powell"], "venue": "Operations Research,", "citeRegEx": "Topaloglu and Powell,? \\Q2005\\E", "shortCiteRegEx": "Topaloglu and Powell", "year": 2005}, {"title": "A reinforcement learning approach to job-shop scheduling", "author": ["W. Zhang", "T. Dietterich"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Zhang and Dietterich,? \\Q1995\\E", "shortCiteRegEx": "Zhang and Dietterich", "year": 1995}], "referenceMentions": [{"referenceID": 16, "context": "The term of IMS can be attributed to a tentative forecast of Hatvany and Nemes (1978). In the early 80s IMSs were outlined as the next generation of manufacturing systems that utilize the results of artificial intelligence research and were expected to solve, within certain limits, unprecedented, unforeseen problems on the basis of even incomplete and imprecise information.", "startOffset": 61, "endOffset": 86}, {"referenceID": 22, "context": "Different kinds of RAPs have a huge number of exact and approximate solution methods, for example, in the case of scheduling problems (Pinedo, 2002).", "startOffset": 134, "endOffset": 148}, {"referenceID": 4, "context": "This has motivated approximate approaches that require a more tractable computation, but often yield suboptimal solutions (Bertsekas, 2005).", "startOffset": 122, "endOffset": 139}, {"referenceID": 3, "context": "However, due to the phenomenon that was named curse of dimensionality by Bellman (1961), these methods are highly intractable in practice.", "startOffset": 73, "endOffset": 88}, {"referenceID": 3, "context": "However, due to the phenomenon that was named curse of dimensionality by Bellman (1961), these methods are highly intractable in practice. The \u201ccurse\u201d refers to the combinatorial explosion of the required computation as the size of the problem increases. Some authors, e.g., Powell and Van Roy (2004), talk about even three types of curses concerning DP algorithms.", "startOffset": 73, "endOffset": 301}, {"referenceID": 31, "context": "Zhang and Dietterich (1995) were the first to apply an RL technique for a special RAP.", "startOffset": 0, "endOffset": 28}, {"referenceID": 20, "context": "Riedmiller and Riedmiller (1999) used a multilayer perceptron (MLP) based neural RL approach to learn local heuristics.", "startOffset": 0, "endOffset": 33}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Cs\u00e1ji, K\u00e1d\u00e1r, & Monostori, 2003; Cs\u00e1ji & Monostori, 2006). Powell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem.", "startOffset": 0, "endOffset": 305}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Cs\u00e1ji, K\u00e1d\u00e1r, & Monostori, 2003; Cs\u00e1ji & Monostori, 2006). Powell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem. Later, a parallelized solution to the previously defined problem was given by Topaloglu and Powell (2005). Our RAP framework, presented in Section 2, differs from these approaches, since in our system the goal is to accomplish a set of tasks that can have widely different stochastic durations and precedence constraints between them, while Powell and Van Roy\u2019s (2004) approach concerns with satisfying many similar demands arriving stochastically over time with demands having unit durations but not precedence constraints.", "startOffset": 0, "endOffset": 515}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Cs\u00e1ji, K\u00e1d\u00e1r, & Monostori, 2003; Cs\u00e1ji & Monostori, 2006). Powell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem. Later, a parallelized solution to the previously defined problem was given by Topaloglu and Powell (2005). Our RAP framework, presented in Section 2, differs from these approaches, since in our system the goal is to accomplish a set of tasks that can have widely different stochastic durations and precedence constraints between them, while Powell and Van Roy\u2019s (2004) approach concerns with satisfying many similar demands arriving stochastically over time with demands having unit durations but not precedence constraints.", "startOffset": 0, "endOffset": 778}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Cs\u00e1ji, K\u00e1d\u00e1r, & Monostori, 2003; Cs\u00e1ji & Monostori, 2006). Powell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem. Later, a parallelized solution to the previously defined problem was given by Topaloglu and Powell (2005). Our RAP framework, presented in Section 2, differs from these approaches, since in our system the goal is to accomplish a set of tasks that can have widely different stochastic durations and precedence constraints between them, while Powell and Van Roy\u2019s (2004) approach concerns with satisfying many similar demands arriving stochastically over time with demands having unit durations but not precedence constraints. Recently, support vector machines (SVMs) were applied by Gersmann and Hammer (2005) to improve iterative repair (local search) strategies for resource constrained project scheduling problems (RCPSPs).", "startOffset": 0, "endOffset": 1018}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Cs\u00e1ji, K\u00e1d\u00e1r, & Monostori, 2003; Cs\u00e1ji & Monostori, 2006). Powell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem. Later, a parallelized solution to the previously defined problem was given by Topaloglu and Powell (2005). Our RAP framework, presented in Section 2, differs from these approaches, since in our system the goal is to accomplish a set of tasks that can have widely different stochastic durations and precedence constraints between them, while Powell and Van Roy\u2019s (2004) approach concerns with satisfying many similar demands arriving stochastically over time with demands having unit durations but not precedence constraints. Recently, support vector machines (SVMs) were applied by Gersmann and Hammer (2005) to improve iterative repair (local search) strategies for resource constrained project scheduling problems (RCPSPs). An agentbased resource allocation system with MDP-induced preferences was presented by Dolgov and Durfee (2006). Finally, Beck and Wilson (2007) gave proactive solutions for job-shop scheduling problems based on the combination of Monte Carlo simulation, solutions of the associated deterministic problem, and either constraint programming or tabu-search.", "startOffset": 0, "endOffset": 1247}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Cs\u00e1ji, K\u00e1d\u00e1r, & Monostori, 2003; Cs\u00e1ji & Monostori, 2006). Powell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem. Later, a parallelized solution to the previously defined problem was given by Topaloglu and Powell (2005). Our RAP framework, presented in Section 2, differs from these approaches, since in our system the goal is to accomplish a set of tasks that can have widely different stochastic durations and precedence constraints between them, while Powell and Van Roy\u2019s (2004) approach concerns with satisfying many similar demands arriving stochastically over time with demands having unit durations but not precedence constraints. Recently, support vector machines (SVMs) were applied by Gersmann and Hammer (2005) to improve iterative repair (local search) strategies for resource constrained project scheduling problems (RCPSPs). An agentbased resource allocation system with MDP-induced preferences was presented by Dolgov and Durfee (2006). Finally, Beck and Wilson (2007) gave proactive solutions for job-shop scheduling problems based on the combination of Monte Carlo simulation, solutions of the associated deterministic problem, and either constraint programming or tabu-search.", "startOffset": 0, "endOffset": 1280}, {"referenceID": 22, "context": "First, we consider the classical job-shop scheduling problem (JSP) which is a standard deterministic RAP (Pinedo, 2002).", "startOffset": 105, "endOffset": 119}, {"referenceID": 22, "context": "Note that a Gantt chart (Pinedo, 2002) is a figure using bars, in order to illustrate the starting and finishing times of the tasks on the resources.", "startOffset": 24, "endOffset": 38}, {"referenceID": 21, "context": ", JSP and TSP, it is strongly NP-hard and, furthermore, no good polynomial-time approximation of the optimal resource allocating algorithm exits, either (Papadimitriou, 1994).", "startOffset": 153, "endOffset": 174}, {"referenceID": 28, "context": "Conversely, a closed-loop controller uses feedback to control states or outputs of a dynamical system (Sontag, 1998).", "startOffset": 102, "endOffset": 116}, {"referenceID": 15, "context": "Therefore, we perform Markov chain Monte Carlo (MCMC) simulations (Hastings, 1970; Andrieu, Freitas, Doucet, & Jordan, 2003) to generate samples with the model, which are used for computing the new approximation of the estimated cost-to-go function.", "startOffset": 66, "endOffset": 124}, {"referenceID": 26, "context": "Using Lagrange multiplier techniques, we can rewrite the regression problem in its dual form (Sch\u00f6lkopf et al., 2000) and arrive at the final \u03bd-SVR optimization problem.", "startOffset": 93, "endOffset": 117}, {"referenceID": 5, "context": "Therefore, at the initial stage we suggest applying a rollout policy, which is a limited lookahead policy, with the optimal cost-to-go approximated by the cost-to-go of the base policy (Bertsekas, 2001).", "startOffset": 185, "endOffset": 202}, {"referenceID": 18, "context": "The best performance on these benchmark datasets was achieved by Mastrolilli and Gambardella (2000). Though, their algorithm performs slightly better than ours, their solution exploits the (unrealistic) specialties of the dataset, e.", "startOffset": 65, "endOffset": 100}], "year": 2008, "abstractText": "The paper investigates stochastic resource allocation problems with scarce, reusable resources and non-preemtive, time-dependent, interconnected tasks. This approach is a natural generalization of several standard resource management problems, such as scheduling and transportation problems. First, reactive solutions are considered and defined as control policies of suitably reformulated Markov decision processes (MDPs). We argue that this reformulation has several favorable properties, such as it has finite state and action spaces, it is aperiodic, hence all policies are proper and the space of control policies can be safely restricted. Next, approximate dynamic programming (ADP) methods, such as fitted Q-learning, are suggested for computing an efficient control policy. In order to compactly maintain the cost-to-go function, two representations are studied: hash tables and support vector regression (SVR), particularly, \u03bd-SVRs. Several additional improvements, such as the application of limited-lookahead rollout algorithms in the initial phases, action space decomposition, task clustering and distributed sampling are investigated, too. Finally, experimental results on both benchmark and industry-related data are presented.", "creator": "dvips(k) 5.96dev Copyright 2007 Radical Eye Software"}}}