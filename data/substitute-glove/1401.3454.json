{"id": "1401.3454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics", "abstract": "Several multiagent reinforcement lesson (MARL) algorithms should all proposed to upgrade cia decisions. Due to early empirical of the unfortunately, the overwhelmingly example both according developed MARL stochastic stated smugglers neither earlier perhaps knowledge of made underlying game (examples several Nash algebraic) having / ordinary frequently other other any and took rewards kept to.", "histories": [["v1", "Wed, 15 Jan 2014 05:13:47 GMT  (515kb)", "http://arxiv.org/abs/1401.3454v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.MA", "authors": ["sherief abdallah", "victor lesser"], "accepted": false, "id": "1401.3454"}, "pdf": {"name": "1401.3454.pdf", "metadata": {"source": "CRF", "title": "A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics", "authors": ["Sherief Abdallah", "Victor Lesser"], "emails": ["SHERIEF.ABDALLAH@BUID.AC.AE", "LESSER@CS.UMASS.EDU"], "sections": [{"heading": null, "text": "agents\u2019 decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received.\nWe introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapley\u2019s game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently.\nAn important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPL\u2019s convergence is difficult, because of the non-linear nature of WPL\u2019s dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPL\u2019s dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms."}, {"heading": "1. Introduction", "text": "The decision problem of an agent can be viewed as selecting a particular action at a given state. A well-known simple example of single-agent decision problem is the multi-armed bandit (MAB) problem: an agent needs to choose a lever among a set of available levers. The reward of executing each action is drawn randomly according to a fixed distribution. The agent\u2019s goal is to choose\nc\u00a92008 AI Access Foundation. All rights reserved.\nthe lever (the action) with the highest expected reward. In order to do so, the agent samples the underlying reward distribution of each action (by trying different actions and observing the resulting rewards). The goal of reinforcement learning algorithms in general is to eventually stabilize (converge) to a strategy that maximizes the agent\u2019s payoff. Traditional reinforcement learning algorithms (such as Q-learning) guarantee convergence to the optimal policy in a stationary environment (Sutton & Barto, 1999), which simply means that the reward distribution associated with each action is fixed and does not change over time.\nIn a multiagent system, the reward each agent receives for executing a particular action depends on other agents\u2019 actions as well. For example, consider extending the MAB problem to the multiagent case. The reward agent A gets for choosing lever 1 depends on which lever agent B has chosen. If both agents A and B are learning and adapting their strategies, the stationary assumption of the single-agent case is violated (the reward distribution is changing) and therefore single-agent reinforcement learning techniques are not guaranteed to converge. Furthermore, in a multi-agent context the optimality criterion is not as clear as in the single agent case. Ideally, we want all agents to reach the equilibrium that maximizes their individual payoffs. However, when agents do not communicate and/or agents are not cooperative, reaching a globally optimal equilibrium is not always attainable (Claus & Boutilier, 1998). An alternative goal that we pursue here is converging to the Nash Equilibrium (NE) (Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007), which is by definition a local maximum across agents (no agent can do better by deviating unilaterally from the NE).\nAn important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how policies of multiple learning agents evolve over time while interacting with one another. Such an analysis not only reveals whether agents using a particular MARL algorithm will eventually converge, but also points out features of the MARL algorithm that are exhibited during the convergence period. Analyzing the dynamics of a MARL algorithm in even the simplest domains (particularly, two-player-two-action games) is a challenging task and therefore was performed on only few, relatively simple, MARL algorithms (Singh, Kearns, & Mansour, 2000; Bowling & Veloso, 2002) and for a restrictive subset of games. These analyzed dynamics were either linear or piece-wise linear.\nRecently, several multi-agent reinforcement learning (MARL) algorithms have been proposed and studied (Claus & Boutilier, 1998; Singh et al., 2000; Peshkin, Kim, Meuleau, & Kaelbling, 2000; Littman, 2001; Bowling & Veloso, 2002; Hu & Wellman, 2003; Bowling, 2005; Abdallah & Lesser, 2006; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007). Most of the MARL algorithms assumed an agent knew the underlying game structure, or the Nash Equilibrium (NE) (Bowling & Veloso, 2002; Banerjee & Peng, 2007). Some even required knowing what actions other agents executed and what rewards they received (Hu & Wellman, 2003; Conitzer & Sandholm, 2007). These assumptions are restrictive in open domains with limited communication (such as ebay or Peer-to-Peer file-sharing) where an agent rarely knows of other agents\u2019 existence let alone observing their actions and knowing how their actions affect the agent\u2019s local reward.\nOn the other hand, if agents are unaware of the underlying game (particularly the NE) and they are not observing each other, then even a simple game with two players and two actions can be challenging. For example, suppose Player 1 observes that its getting reward 10 from executing its first action and reward 8 for executing action 2. As time passes, and Player 2 changes its policy, Player 1 observes a switch in the reward associated with each action: action 1 now has a reward of 7 and action 2 has a reward of 9. Note that in both cases Player 1 is unaware of its own NE strategy\nand is oblivious to the current policy of Player 2. The only feedback Player 1 is getting is the change in its reward function, which in turn depends on Player 2\u2019s policy. The same situation applies in reverse to Player 2.\nIn this paper we propose a new MARL algorithm that enables agents to converge to a Nash Equilibrium, in benchmark games, assuming each agent is oblivious to other agents and receives only one type of feedback: the reward associated with choosing a given action. The new algorithm is called the Weighted Policy Learner or WPL for reasons that will become clear shortly. We experimentally show that WPL converges in well-known benchmark two-player-two-action games. Furthermore, we show that WPL converges in the challenging Shapley\u2019s game, where state-of-theart MARL algorithms failed to converge (Bowling & Veloso, 2002; Bowling, 2005),1 unlike WPL. We also show that WPL outperforms state-of-the-art MARL algorithms (shorter time to converge, better performance during convergence, and better average reward) in a more realistic domain of 100 agents interacting and learning with one another. We also analyze WPL\u2019s dynamics and show that they are non-linear. This non-linearity made our attempt to solve, symbolically, the differential equations representing WPL\u2019s dynamics unsuccessful. Instead, we solve the equations numerically and verify that our theoretical analysis and our experimental results are consistent. We compare the dynamics of WPL with earlier MARL algorithms and discuss interesting differences and similarities.\nThe paper is organized as follows. Section 2 lays out the necessary background, including game theory and closely related MARL algorithms. Section 3 describes our proposed algorithm. Section 4 analyzes WPL\u2019s dynamics for this restricted class of games and compares it to previous algorithms\u2019 dynamics. Section 5 discusses our experimental results. We conclude in Section 6."}, {"heading": "2. Background", "text": "In this section we introduce the necessary background for our contribution. First, a brief review of relevant game theory definitions is given. Then a review of relevant MARL algorithms is provided, with particular focus on gradient-ascent MARL (GA-MARL) algorithms that are closely related to our algorithm."}, {"heading": "2.1 Game Theory", "text": "Game theory provides a framework for modeling agents\u2019 interaction and was used by previous researchers in order to analyze the convergence properties of MARL algorithms (Claus & Boutilier, 1998; Singh et al., 2000; Bowling & Veloso, 2002; Wang & Sandholm, 2003; Bowling, 2005; Conitzer & Sandholm, 2007; Abdallah & Lesser, 2006). A game specifies, in a compact and simple manner, how the payoff of an agent depends on other agents\u2019 actions. A (normal form) game is defined by the tuple \u3008n,A1, ..., An, R1, ..., Rn\u3009, where n is the number of players2 in the game, Ai is the set of actions available to agent i, and Ri : A1 \u00d7 ... \u00d7 An \u2192 < is the reward (payoff) agent i will receive as a function of the joint action executed by all agents. If the game has only two players, then it is convenient to define their reward functions as a payoff matrix as shown in Table 1. Each cell (i, j) in the matrix represents the payoff received by the row player (Player 1) and the column player (Player 2), respectively, if the row player plays action i and the column player\n1. Except for MARL algorithms that assumed agents know information about the underlying game (Hu & Wellman, 2003; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007). 2. We use the terms agent and player interchangeably.\nplays action j. Table 1 and Table 2 provide example benchmark games that were used in evaluating previous MARL algorithms (Bowling & Veloso, 2002; Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007).\nA policy (or a strategy) of an agent i is denoted by \u03c0i \u2208 PD(Ai), where PD(Ai) is the set of probability distributions over actions Ai. The probability of choosing an action ak according to policy \u03c0i is \u03c0i(ak). A policy is deterministic or pure if the probability of playing one action is 1 while the probability of playing other actions is 0, (i.e. \u2203k : \u03c0i(ak) = 1 AND \u2200l 6= k : \u03c0i(al) = 0), otherwise the policy is stochastic or mixed.\nA joint policy \u03c0 is the collection of individual agents\u2019 policies, i.e. \u03c0 = \u3008\u03c01, \u03c02, ..., \u03c0n\u3009, where n is the number of agents. For convenience, the joint policy is usually expressed as \u03c0 = \u3008\u03c0i, \u03c0\u2212i\u3009, where \u03c0\u2212i is the collection of all policies of agents other than agent i.\nLet variable A\u2212i = {\u3008a1, ..., an\u3009 : aj \u2208 Aj \u2227 i 6= j}. The expected reward agent i would get,\nif agents follow a joint policy \u03c0, is Vi(\u3008\u03c0i, \u03c0\u2212i\u3009) = \u2211 ai\u2208Ai \u2211\na\u2212i\u2208A\u2212i \u03c0i(ai)\u03c0\u2212i(a\u2212i).Ri(ai, a\u2212i), i.e. the reward averaged over the joint policy. A joint policy is a Nash Equilibrium, or NE, iff no agent can get higher expected reward by changing its policy unilaterally. More formally, \u3008\u03c0\u2217i , \u03c0\u2217\u2212i\u3009 is an NE iff \u2200i : Vi(\u3008\u03c0\u2217i , \u03c0\u2217\u2212i\u3009) \u2265 Vi(\u3008\u03c0i, \u03c0\u2217\u2212i\u3009). An NE is pure if all its constituting policies are pure. Otherwise the NE is called mixed or stochastic. Any game has at least one Nash equilibrium, but may not have any pure (deterministic) equilibrium.\nConsider again the benchmark games in Table 1. The coordination game (Table 1(a)) is an example of games that have at least one pure NE. The matching pennies game (Table 1(b)) is an example of games that do not have any pure NE and only have a mixed NE (where each player plays a1 and a2 with equal probability). Convergence of GA-MARL algorithms in games with pure\nNE is easier than games where the only NE is mixed (Singh et al., 2000; Bowling & Veloso, 2002; Zinkevich, 2003). The tricky game is similar to matching pennies game in that it only has one mixed NE (no pure NE), yet some GA-MARL algorithms succeeded in converging in the matching pennies games, while failing in the tricky game (Bowling & Veloso, 2002).\nTable 2 shows well-known 2-player-3-action benchmark games. Shapley\u2019s game, in particular, has received considerable attention from MARL community (Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007) as it remains challenging for state-of-the-art MARL algorithms despite its apparent simplicity (and similarity to the rock-paper-scissors game which is not as challenging). Our proposed algorithm is the first MARL algorithm to converge in Shapley\u2019s game without observing other agents or knowing the underlying NE strategy.\nBefore introducing MARL algorithms in the following section, two issues are worth noting. The first issue is the general assumption in the MARL context: agents play the same game repeatedly for a large number of times. This is a necessary assumption in order for agents to gain experience and learn. The second issue is that the games\u2019 rewards, described in this section, are deterministic given the joint action. However, from each agent\u2019s perspective, the rewards are stochastic because of the randomness caused by the other agents\u2019 actions in the system."}, {"heading": "2.2 Multiagent Reinforcement Learning, MARL", "text": "Early MARL algorithms were based on the Q-learning algorithm (Sutton & Barto, 1999), and therefore could only learn a deterministic policy (Claus & Boutilier, 1998), significantly limiting their applicability in competitive and partially observable domains. The Joint Action Learner (Claus & Boutilier, 1998) is an example of this family of algorithms that also required the knowledge of other agents\u2019 chosen actions.\nAnother class of MARL algorithms is the class of Equilibrium learners such as Nash-Q (Hu & Wellman, 2003), OAL (Wang & Sandholm, 2003), and AWESOME (Conitzer & Sandholm, 2007). Most of these algorithms assumed that each agent observed other agents\u2019 actions in addition to knowing the underlying game.3 Each agent then computed the Nash Equilibria. The purpose of learning is to allow agents to converge to a particular Nash Equilibrium (in case all agents execute the same MARL algorithm).\nObserving other agents or knowing the underlying game structure are not applicable in open and large domains, which motivated the development of gradient ascent learners. Gradient ascent MARL algorithms (GA-MARL) learn a stochastic policy by directly following the expected reward gradient. The ability to learn a stochastic policy is particularly important when the world is not fully observable or has a competitive nature. Consider for example a blind robot in a maze (i.e. the robot cannot distinguish between maze locations). Any deterministic policy (i.e. one that always chooses a particular action everywhere) may never escape the maze, while a stochastic policy that chooses each action with non-zero probability will eventually escape the maze.4 Similarly, in a competitive domain a stochastic policy may be the only stable policy (e.g. the Nash Equilibrium in a competitive game). The remainder of this section focuses on this family of algorithms as it is closely related to our proposed algorithm.\n3. Nash-Q did not require knowing the underlying game but required observing other agents rewards in addition to their chosen actions. 4. Note that a uniformly random policy may not be optimal if the maze is biased in a specific direction.\nThe Infinitesimal Gradient Ascent algorithm (IGA) (Singh et al., 2000) and its generalization (Generalized IGA or GIGA) (Zinkevich, 2003) were proved to converge in games with pure NE. However, both algorithms failed to converge in games with mixed NE, and therefore may not be suitable for applications that require a mixed policy.\nSeveral modifications to IGA and GIGA were proposed to avoid divergence in games with mixed NE, including: IGA/PHC-WoLF (Bowling & Veloso, 2002), PHC-PDWoLF (Banerjee & Peng, 2003), and GIGA-WoLF (Bowling, 2005). They all used some form of the Win or Learn Fast heuristics (Bowling & Veloso, 2002), whose purpose is to speedup learning if the agent is doing worse than its NE policy (losing) and to slow down learning if the agent is doing better than the NE policy. The main problem with this heuristic is that an agent cannot know whether it is doing better or worse than its NE policy without knowing the underlying game a prior. Therefore, a practical implementation of the WoLF heuristic needed to use approximation methods for predicting the performance of the agent\u2019s NE policy. Our algorithm, called WPL, uses a different heuristic that does not require knowing the NE policy and consequently, as we show, converges in both the tricky game and Shapley\u2019s game, where algorithms relying on the WoLF heuristic failed. Furthermore, we show that in a large-scale partially-observable domain (the distributed task allocation problem, DTAP) WPL outperforms state-of-the-art GA-MARL algorithm GIGA-WoLF.\nThe following section reviews in further detail the well known GA-MARL algorithms IGA, IGA-WoLF, and GIGA-WoLF which we will use to compare our algorithm against."}, {"heading": "2.3 Gradient-Ascent MARL Algorithms", "text": "The first GA-MARL algorithm whose dynamics were analyzed is the Infinitesimal Gradient Ascent (IGA) (Singh et al., 2000). IGA is a simple gradient ascent algorithm where each agent i updates its policy \u03c0i to follow the gradient of expected payoffs (or the value function) Vi, as illustrated by the following equations.\n\u2206\u03c0t+1i \u2190 \u03b7 \u2202Vi(\u03c0t) \u2202\u03c0i\n\u03c0t+1i \u2190 projection(\u03c0 t i + \u2206\u03c0 t+1 i )\nParameter \u03b7 is called the policy-learning-rate and approaches zero in the limit (\u03b7 \u2192 0), hence the word Infinitesimal in IGA. Function projection projects the updated policy to the space of valid policies. The original IGA paper (Singh et al., 2000) defined the projection function (and therefore IGA) in case each agent has only two actions to choose from. A general definition of the projection function was later developed and the resulting algorithm was called Generalized IGA or GIGA (Zinkevich, 2003). The generalized function is projection(x) = argminx\u2032:valid(x\u2032)|x\u2212 x\u2032|, where |x \u2212 x\u2032| is the Euclidean distance between x and x\u2032. A valid policy \u03c0 (over a set of actions A) must satisfy two constraints: \u2200a \u2208 A : 1 \u2265 \u03c0(a) \u2265 0 and \u2211 \u03c0 = \u2211 a\u2208A \u03c0(a) = 1.\nIn other words, the space of valid policies is a simplex, which is a line segment (0,1),(1,0) in case of two actions, a triangular surface (1,0,0), (0,1,0), (0,0,1) in case of three actions, and so on. A joint policy, \u03c0, is a point in this simplex. It is quite possible (especially when trying to follow an approximate policy gradient) that \u2211 \u03c0 deviates from 1 and goes beyond the simplex. The generalized projection function projects an invalid policy to the closest valid policy within the simplex (Zinkevich, 2003).\nThe IGA algorithm did not converge in all two-player-two-action games (Singh et al., 2000). Algorithm IGA-WoLF (WoLF stands for Win or Learn Fast) was proposed (Bowling & Veloso, 2002) in order to improve convergence properties of IGA by using two different learning rates as follows. If a player is getting an average reward lower than the reward it would get for executing its NE strategy, then the learning rate should be large. Otherwise (the player current policy is better than its NE policy), the learning rate should be small. More formally,\n\u2206\u03c0i(a)\u2190 \u2202Vi(\u03c0) \u2202\u03c0i (a) \u00b7 { \u03b7lose if Vi(\u03c0i, \u03c0\u2212i) < Vi(\u03c0\u2217i , \u03c0\u2212i) \u03b7win otherwise\n\u03c0i \u2190 projection(\u03c0i + \u2206\u03c0i)\nWhere \u03c0\u2217i is the NE policy for agent i and \u03b7lose > \u03b7win are the learning rates. The dynamics of IGA-WoLF have been analyzed and proved to converge in all 2-player-2-action games (Bowling & Veloso, 2002), as we briefly review Section 2.4. IGA-WoLF has limited practical use, because it required each agent to know its equilibrium policy (which means knowing the underlying game). An approximation to IGA-WoLF was proposed, PHC-WoLF, where an agent\u2019s NE strategy is approximated by averaging the agent\u2019s own policy over time (Bowling & Veloso, 2002). The approximate algorithm, however, failed to converge in the tricky game shown in Table 1(c).\nThe GIGA-WoLF algorithm extended the GIGA algorithm with the WoLF heuristic. GIGAWoLF kept track of two policies \u03c0 and z. Policy \u03c0 was used to select actions for execution. Policy z was used to approximate the NE policy. The update equations of GIGA-WoLF are (Bowling, 2005, 2004):\n\u03c0\u0302t+1 = projection(\u03c0t + \u03b4rt) (1) zt+1 = projection(\u03c0t + \u03b4rt/3) (2)\n\u03b7t+1 = min (\n1, ||zt+1 \u2212 zt|| zt+1 \u2212 \u03c0\u0302t\n) (3)\n\u03c0t+1 = \u03c0\u0302t+1 + \u03b7t+1(zt+1 \u2212 \u03c0\u0302t+1) (4)\nThe main idea is a variant of the WoLF heuristics (Bowling & Veloso, 2002). An agent i changes its policy \u03c0i faster if the agent is performing worse than policy z, i.e. V \u03c0i < V z . Because z moves slower than \u03c0, GIGA-WoLF uses z to realize that it needs to change the current gradient direction. This approximation allows GIGA-WoLF to converge in the tricky game, but not in Shapley\u2019s game (Bowling, 2005).\nThe following section briefly reviews the analysis of IGA\u2019s and IGA-WoLF\u2019s dynamics, showing how a joint policy of two agents evolves over time. We will then build on this analysis in Section 4 when we analyze WPL\u2019s dynamics."}, {"heading": "2.4 Dynamics of GA-MARL Algorithms", "text": "Differential equations were used to model the dynamics of IGA (Singh et al., 2000). To simplify analysis, the authors only considered two-player-two-action games, as we also do here. We will refer to the joint policy of the two players at time t by the probabilities of choosing the first action (pt, qt), where \u03c01 = (pt, 1 \u2212 pt) is the policy of player 1 and \u03c02 = (qt, 1 \u2212 qt) is the policy of\nplayer 2. The t notation will be omitted when it does not affect clarity (for example, when we are considering only one point in time).\nIGA\u2019s update equations can be simplified to be (note that rij and cij are the payoffs for row and column players respectively):\npt+1 = pt + \u03b7 \u2202Vr(pt, qt)\n\u2202p = pt + \u03b7(Vr(1, qt)\u2212 Vr(0, qt))\nwhere\nVr(1, qt)\u2212 Vr(0, qt) = ( r11q t + r12(1\u2212 qt) ) \u2212 ( r21q t + r22(1\u2212 qt) )\n= qt(r11 \u2212 r12 \u2212 r21 + r22) + (r12 \u2212 r22) = u1qt + u2 (5)\nand similarly,\nqt+1 = qt + \u03b7 \u00b7 (u3pt + u4)\nwhere u1, u2, u3, and u4 are game-dependent constants having the following values.\nu1 = r11 \u2212 r12 \u2212 r21 + r22 u2 = r12 \u2212 r22 u3 = c11 \u2212 c12 \u2212 c21 + c22 u4 = c21 \u2212 c22\nIn analyzing IGA, the original paper distinguished between three types of games depending on the u1 and u3 parameters: u1u3 > 0, u1u3 = 0, and u1u3 < 0. Figure 1(a), Figure 1(b), and Figure 1(c) are taken from (Bowling & Veloso, 2002) and illustrate the dynamics of IGA for each of the three cases. Each figure displays the space of joint policies, where the horizontal axis represent the policy of the row player, p, and the vertical axis represents the policy of the column player q.5\nThe joint policy of the two agents evolves over time by following the directed lines in the figures. For example, Figure 1(b) illustrates that starting from any joint policy, the two players will eventually evolve to one of two equilibriums, either the bottom-left corner, or the upper-right corner. It should be noted that, for simplicity, the dynamics shown in Figure 1 are unconstrained: the effect of the projection function (the simplex described in Section 2.3) is not shown. In IGA\u2019s analysis (Singh et al., 2000), the effect of the projection function was taken into account by considering all possible locations of the simplex.\nIn the first and the second cases, IGA converged. In the third case IGA oscillated around the equilibrium strategy, without ever converging to it. This happens because at any point of time, unless both agents are at the NE, one agent is better off changing its policy to be closer to its NE strategy, while the other agent is better off changing its policy to be further away from its NE strategy. These roles switch as one of the agents crosses its NE strategy as shown in Figure 1(c).\nAs described in Section 2.3, IGA-WoLF was proposed (Bowling & Veloso, 2002) as an extension to IGA that ensures convergence in the third case. The idea of IGA-WoLF, as described earlier,\n5. Note that Figure 1(b) and Figure 1(c) are divided into four quadrants A, B, C, and D for clarification. The gradient direction in each quadrant is illustrated by arrows in Figure 1(c).\nis to have two learning rates by which an agent is moving toward or away from the NE. The following equations capture the dynamics of IGA-WoLF, where \u3008p\u2217, q\u2217\u3009 is the NE. The equations use a factored form of \u03b7lose and \u03b7win.\npt = pt\u22121 + \u03b7(u1qt\u22121 + u2) \u00b7 { llose if Vr(pt\u22121, qt\u22121) < Vr(p\u2217, qt\u22121) lwin otherwise\nqt = qt\u22121 + \u03b7(u3pt\u22121 + u4) \u00b7 { llose if Vc(qt\u22121, pt\u22121) < Vc(q\u2217, pt\u22121) lwin otherwise\nThe dynamics of IGA-WoLF is best illustrated visually by Figure 2, which is taken from (Bowling & Veloso, 2002). The switching between learning rates causes switching between ellipses of smaller diameters, eventually leading to convergence.\nThe main limitation of IGA-WoLF is that it assumed each agent knows the NE policy (needed to switch between the two modes of the learning rate). The following section presents WPL, which does not make this assumption, at the expense of having more complex dynamics as we show in Section 4."}, {"heading": "3. Weighted Policy Learner (WPL)", "text": "We propose in this paper the Weighted Policy Learner (WPL) algorithm, which has the following update equations:\n\u2206\u03c0i(a)\u2190 \u2202Vi(\u03c0) \u2202\u03c0i(a) \u00b7 \u03b7 \u00b7\n{ \u03c0i(a) if\n\u2202Vi(\u03c0) \u2202\u03c0i(a) < 0 1\u2212 \u03c0i(a) otherwise\n\u03c0i \u2190 projection(\u03c0i + \u2206\u03c0i)\nThe projection function is adopted from GIGA (Zinkevich, 2003) with minor modification:\u2200a : 1 \u2265 \u03c0(a) \u2265 (the modification ensures a minimum amount of exploration ). The algorithm works as follows. If the gradient for a particular action is negative then the gradient is weighted by \u03c0i(a), otherwise (gradient is positive) the gradient is weighted by (1\u2212\u03c0i(a)). So effectively, the probability of choosing a good action increases by a rate that decreases as the probability approaches 1 (or the boundary of the simplex). Similarly, the probability of choosing a bad action decreases at a rate that also decreases as the probability approaches zero.\nSo unless the gradient direction changes, a WPL agent decreases its learning rate as the agent gets closer to simplex boundary. This means, for a 2-player-2-action game, a WPL agent will move towards its NE strategy (away from the simplex boundary) faster than moving away from its NE strategy (towards the simplex boundary), because the NE strategy is always inside the simplex.\nWPL is biased against pure (deterministic) policies and will reach a pure policy only in the limit (because the rate of updating the policy approaches zero). This theoretical limitation is of little practical concern for two reasons. The first reason is exploration: 100% pure strategies are bad because they prevent agents from exploring other actions (in an open dynamic environment the reward of an action may change over time).\nThe second reason is that if an action dominates another action (the case when a game has a pure NE), then \u2202Vi(\u03c0)\u2202\u03c0i(a) can be large enough that \u2202Vi(\u03c0) \u2202\u03c0i(a)\n\u00b7 \u03b7 > 1. In this case WPL will jump to a pure policy in one step.6 Note that in Section 4, in order to simplify theoretical analysis, we assume \u03b7 \u2192 0. From a practical perspective, however, \u03b7 is set to value close to 0, but never 0. This holds\n6. In fact, WPL can even go beyond the simplex of valid policies, that is when the projection function comes into play\nfor all gradient-ascent MARL algorithms (Bowling & Veloso, 2002; Bowling, 2005; Banerjee & Peng, 2007).\nThere are key differences between IGA-WoLF and WPL\u2019s update rules despite the apparent similarity. Both algorithms have two modes of learning rates (corresponding to the two conditions in the policy update rule). However, IGA-WoLF needs to know the equilibrium strategy in order to distinguish between the two modes, unlike WPL that needs to know only the value of each action. Another difference is that IGA-WoLF used two fixed learning rates (\u03b7l > \u03b7w) for the two modes, while WPL uses a continuous spectrum of learning rates, depending on the current policy. This can be understood from the definition of \u2206\u03c0i in WPL\u2019s update equations, which includes an additional (continuous) scaling factor: \u03c0ti . This particular feature causes WPL\u2019s dynamics to be non-linear, as we discuss in the following section."}, {"heading": "4. Analyzing WPL\u2019s Dynamics", "text": "Claim 1 WPL has non-linear dynamics.\nProof. The policies of two agents following WPL can be expressed as follows\nqt \u2190 qt\u22121 + \u03b7(u3pt\u22121 + u4) {\n1\u2212 qt\u22121 if u3pt\u22121 + u4 > 0 qt\u22121 otherwise\nand\npt \u2190 pt\u22121 + \u03b7(u1qt\u22121 + u2) {\n1\u2212 pt\u22121 if u1qt\u22121 + u2 > 0 pt\u22121 otherwise\nNote that u1qt\u22121 + u2 = Vr(1, qt\u22121)\u2212 Vr(0, qt\u22121) from Equation 5. It then follows that\npt \u2212 pt\u22121\n\u03b7 (t\u2212 (t\u2212 1)) = (u1qt\u22121 + u2) {\n1\u2212 pt\u22121 if Vr(1, qt\u22121) > Vr(0, qt\u22121) pt\u22121 otherwise\nand analogously for the column player\nqt \u2212 qt\u22121\n\u03b7 (t\u2212 (t\u2212 1)) = (u3pt\u22121 + u4) {\n1\u2212 qt\u22121 if Vc(1, pt\u22121) > Vc(0, pt\u22121) qt\u22121 otherwise\nAs \u03b7 \u2192 0, the equations above become differential:\nq\u2032(t) = (u3pt\u22121 + u4) {\n1\u2212 qt\u22121 if Vc(1, pt\u22121) > Vc(0, pt\u22121) qt\u22121 otherwise\n(6)\np\u2032(t) = (u1qt\u22121 + u2) {\n1\u2212 pt\u22121 if Vr(1, qt\u22121) > Vr(0, qt\u22121) pt\u22121 otherwise\n(7)\nNotice here that the NE strategy does not appear in WPL\u2019s equations, unlike IGA-WoLF. Furthermore, while IGA and IGA-WoLF needed to take the projection function into account, we can safely ignore the projection function while analyzing the dynamics of WPL for two-player-twoaction games. This is due to the way WPL scales the learning rate using the current policy. By the definition of p\u2032(t), a positive p\u2032(t) approaches zero as p approaches one and a negative p\u2032(t) approaches zero as p approaches zero. In other words, as p (or q) approaches 0 or 1, the learning rate approaches zero, and therefore p (or q) will never go beyond the simplex of valid policies.7 This observation will become apparent in Section 4.2 when we compare the dynamics of WPL to the dynamics of both IGA and IGA-WoLF.\nFollowing IGA-WoLF\u2019s analysis (Bowling & Veloso, 2002), as illustrated in Figure 3, we will focus on the challenging case when there is no deterministic NE (the NE is inside the joint policy space) and analyze how p and q evolve over time. This is the case where the original IGA oscillated as shown in Figure 1. It is important to note, however, that all gradient ascent MARL algorithms (including WPL) converge in 2x2 games cases where there is (at least) one pure NE, because the dynamics do not have any loops and eventually lead to one of the pure equilibriums (Singh et al., 2000; Bowling & Veloso, 2002).\nWe will solve WPL\u2019s differential equations for the period 0 \u2192 T4 in Figure 3, assuming that Player 2 starts at its NE policy q\u2217 at time 0 and returns to q\u2217 at time T4. If we can prove that, over the period 0 \u2192 T4, Player 2\u2019s policy p(t) gets closer to the NE policy p\u2217, i.e. pmin2 \u2212 pmin1 > 0 in Figure 3, then by induction the next time period p will get closer to the equilibrium and so on.\nFor readability, p and q will be used instead of p(t) and q(t) for the remainder of this section. Without loss of generality, we can assume that u1q+u2 > 0 iff q > q\u2217 and u3p+u4 > 0 iff p < p\u2217. The overall period 0 \u2192 T4 is divided into four intervals defined by times 0, T1, T2, T3, and T4. Each period corresponds to one combination of p and q as follows. For the first period 0 \u2192 T1, p < p\u2217 and q > q\u2217, therefore agent p is better off moving toward the NE, while agent q is better off moving away from the NE. The differential equations can be solved by dividing p\u2032 and q\u2032\ndp dq = (1\u2212 p)(u1q + u2) (1\u2212 q)(u3p+ u4)\nThen by separation we have\u222b p\u2217 pmin1 u3p+ u4 1\u2212 p dp = \u222b qmax q\u2217 u1q + u2 1\u2212 q dq\n\u2212u3(p\u2217 \u2212 pmin1) + (u3 + u4)ln 1\u2212 pmin1\n1\u2212 p\u2217 =\n\u2212u1(qmax \u2212 q\u2217) + (u1 + u2)ln 1\u2212 q\u2217\n1\u2212 qmax 7. In practice WPL still needs the projection function because \u03b7 must be set to a small value strictly larger than 0.\nSimilarly, for period T1\u2192 T2, where p > p\u2217 and q > q\u2217\n\u2212u3(pmax \u2212 p\u2217) + (u3 + u4)ln 1\u2212 p\u2217\n1\u2212 pmax =\nu1(q\u2217 \u2212 qmax) + u2ln q\u2217\nqmax\nand for period T2\u2192 T3, where p > p\u2217 and q < q\u2217\nu3(p\u2217 \u2212 pmax) + u4ln p\u2217\npmax = u1(qmin \u2212 q\u2217) + u2ln qmin q\u2217\nand finally for period T3\u2192 T4, where p < p\u2217 and q < q\u2217\nu3(pmin2 \u2212 p\u2217) + u4ln pmin2 p\u2217 =\n\u2212u1(q\u2217 \u2212 qmin) + (u1 + u2)ln 1\u2212 qmin 1\u2212 q\u2217\nThese are 4 non-linear equations (note the existence of both x and ln(x) in all equations) in 5 unknowns (pmin, pmin2, pmax, qmin1, qmax), along with the inequalities governing the constants u1, u2, u3, and u4.\nBecause WPL\u2019s dynamics are non-linear, we could not obtain a closed-form solution and therefore we could not formally prove WPL\u2019s convergence.8 Instead, we solve the equations numerically in the following section. Although a numerical solution is still not a formal proof, it provides useful insights into understanding WPL\u2019s dynamics.\n8. If the equations were linear, we could have substituted for all unknowns in terms of pmin1 and p2min and easily determined whether pmin2 \u2212 pmin1 > 0, similar to IGA-WoLF."}, {"heading": "4.1 Numerical Solution", "text": "We have used Matlab to solve the equations numerically. Figure 4 shows the theoretical behavior predicted by our model for the matching-pennies game. There is a clear resemblance to the actual (experimental) behavior for the same game (Figure 5). Note that the time-scale on the horizontal axes of both figures are effectively the same, because what is displayed on the horizontal axis in Figure 5 is decision steps in the simulation. When multiplied by the actual policy-learning-rate \u03b7 (the time step) used in the experiments, 0.001, both axes become identical.\nFigure 6 plots p(t) versus q(t), for a game with NE= (0.9, 0.9) (u1 = 0.5, u2 = \u22120.45, u3 = \u22120.5, , u4 = 0.45) and starting from 157 different initial joint policies (40 initial policies over each side of the joint policy space). Figure 7 plots p(t) and q(t) against time, showing convergence from each of the 157 initial joint policies.\nWe repeated the above numerical solution for 100 different NE(s) that make a 10x10 grid in the p-q space (and starting from the 157 boundary initial joint policies for each individual NE). The WPL algorithm converges to the NE in a spiral fashion similar to the specific case in Figure 6 in all the 100 NE(s). Instead of drawing 100 figures (one for each NE), Figure 8 plots the merge of the 100 figures in a compact way: plotting agents\u2019 joint policy from time 700 to time 800 (which is enough for convergence as Figure 7 shows). The two agents converge in all the 100 NE cases, as indicated by the centric points. Note that if the algorithm was not converging, then the joint policies trajectories after time 700 should occupy more space, because the 157 initial joint policies are on the policy space boundary."}, {"heading": "4.2 Comparing Dynamics of IGA, IGA-WoLF, and WPL", "text": "With differential equations modeling each of the three algorithms, we now compare their dynamics and point out the main distinguishing characteristics of WPL. Matlab was again used to solve the differential equations (of the three algorithms) numerically. Figure 9 shows the dynamics for a game with u1u3 < 0 and the NE=(0.5,0.5). The joint strategy moves in clockwise direction. The dynamics of WPL are very close to IGA-WoLF, with IGA-WoLF converging a bit faster (after one complete round around the NE, IGA-WoLF is closer to the NE than WPL). It is still impressive that WPL has comparable performance to IGA-WoLF, since WPL does not require agents to know their NE strategy or the underlying game a priori, unlike IGA-WoLF.\nFigure 10 shows the dynamics in a game with u1u3 < 0 but the NE=(0.5,0.1). Three interesting regions in the figure are designated with A,B, and C. Region A shows that both IGA and IGAWoLF dynamics are discontinuous due to the effect of the projection function. Because WPL uses a smooth policy weighting scheme, the dynamics remain continuous. This is also true in region B. In region C, WPL initially deviates from the NE more than IGA, but eventually converges as well. The reason is that because the NE, in this case, is closer to the boundary, policy weighting makes the vertical player move at a much slower pace when moving downward (the right half) than the horizontal player.\nFigure 11 shows the dynamics for the coordination game (Table 1(a)), starting from initial joint policy (0.1,0.6). The coordination game has two NEs: (0,0) and (1,1). All algorithms converge to the closer NE, (0,0), but again we see that both IGA and IGA-WoLF have discontinuity in their dynamics, unlike WPL which smoothly converge to the NE. Notice that WPL converges to a pure NE in the limit, but the graph shows the joint policy space, so there is no depiction of time.\nThe continuity of WPL\u2019s dynamics comes into play when the target policy is mixed. While IGA, GIGA, and GIGA-WoLF algorithms go through extreme deterministic policies during the transient period prior to convergence, which can cause ripple effect in realistic settings where large number of agents are interacting asynchronously (e.g. through a network). WPL never reaches such extremes and the experimental results in Section 5.4 show that GIGA-WoLF takes significantly more time to converge compared to WPL in a domain of 100 agents.\nThe following section presents our experimental results for both benchmark 2-player-2-action games and larger games involving more actions and more agents."}, {"heading": "5. Experimental Results", "text": "This section is divided into three parts. Section 5.1 discusses main learning parameters that need to be set in practice. Section 5.2 presents the experimental results for the 2x2 benchmark games. Section 5.3 presents the experimental results for domains with larger number of actions and/or agents."}, {"heading": "5.1 Learning Parameters", "text": "Conducting experiments for WPL involves setting two main parameters: the policy-learning-rate, \u03b7, and the value-learning-rate, \u03b1. In theory (as discussed in Section 4) the policy-learning-rate, \u03b7, should approach zero. In practice, however, this is not possible and we have tried setting \u03b7 to different small values between 0.01 and 0.00001. The smaller \u03b7 is, the longer it will take WPL to converge, and the smaller the oscillations around the NE will become (and vice versa), similar to previous GA-MARL algorithms. A reasonable value that we have used in most of our experiments is \u03b7 = 0.002.\nThe value-learning-rate \u03b1 is used to compute the expected reward of an action a at time t, or rt(a), which is not known a priori. The common approach, which was used in previous GA-MARL algorithms and we also use here, is using the equation rt+1(a) \u2190 \u03b1Rt + (1 \u2212 \u03b1)rt(a), where Rt is the sample reward received at time t and 0 \u2264 \u03b1 \u2264 1 (Sutton & Barto, 1999). We have tried three values for \u03b1: 0.01,0.1, and 1."}, {"heading": "5.2 Benchmark 2x2 Games", "text": "We have implemented a set of previous MARL algorithms for comparison: PHC-WoLF (the realistic implementation of IGA-WoLF), GIGA, and GIGA-WoLF. In our experiments we have not used any decaying rates. The reason is that in an open system where dynamics can continuously change, one would want learning to be continuous as well. We have fixed the exploration rate to 0.1 (which comes into play in the modified projection function in Section 3), the policy-learning-rate \u03b7 to 0.002, and the value-learning-rate \u03b1 to 0.1 (unless otherwise specified).\nThe first set of experiments show the results of applying our algorithm on the three benchmark games described in Table 1 over 10 simulation runs. Figure 12 plots \u03c0(r1) and \u03c0(c1) (i.e. the probability of choosing the first action for the row player and the column player respectively) against time. For the matching pennies and the tricky games, the initial joint policy is ([0.1, 0.9]r, [0.9, 0.1]c) (we also tested 0.5,0.5 as initial joint policy with similar results) and the plot is the average across the 10 runs, with standard deviation shown as vertical bar. In the coordination game we plotted a single run because there are two probable pure NEs and averaging over runs will not capture that. WPL converges to one of the NE strategies in all the runs.\nFigure 13 shows the convergence of GIGA, PHC-WoLF, and GIGA-WoLF algorithms for the coordination game. As expected, GIGA, PHC-WoLF, and GIGA-WoLF converges to one of the NE strategies faster, because WPL is slightly biased against pure strategies.\nFigure 14 shows the convergence of previous GA-MARL algorithms for the matching pennies game. GIGA hopelessly oscillates around the NE, as expected. PHC-WoLF is better, but with relatively high standard deviation across runs (when compared to WPL). GIGA-WoLF has comparable performance to WPL, but GIGA-WoLF takes longer to converge (the width of oscillations around the NE is higher than WPL).\nThe tricky game deserves more attention as it is one of the challenging two-player-two-action games (Bowling & Veloso, 2002). Figure 15 shows the convergence of PHC-WoLF, GIGA, and GIGA-WoLF. Only GIGA-WoLF converges but with slower rate than our approach (Figure 12(c)). The performance of GIGA, PHC-WoLF, and GIGA-WoLF conforms to the results reported previously by their authors (Bowling & Veloso, 2002; Bowling, 2005). The remainder of our experiments (Section 5.3) focuses on GIGA-WoLF because it has performed the best among previous GA-MARL algorithms."}, {"heading": "5.3 Games Larger than 2x2", "text": "Figures 16 and 17 plot the policy of the row player over time for the games in Table 2 for both WPL and GIGA-WoLF when the initial joint strategy is ([0.1, 0.8, 0.1]r, [0.8, 0.1, 0.1]c) (we have also tried ([13 , 1 3 , 1 3 ]r, [ 1 3 , 1 3 , 1 3 ]c), which produced similar results), \u03b7 = 0.001, and for two values of \u03b1: 0.1 and 1. For the rock-paper-scissors game (Figure 16) both GIGA-WoLF and WPL converge when \u03b1 = 0.1, while only WPL converges when \u03b1 = 1. In Shapley\u2019s game (Figure 17) GIGAWoLF keeps oscillating for both \u03b1 = 1 and \u03b1 = 0.1 (GIGA-WoLF\u2019s performance gets worse as \u03b1\nincreases). WPL on the other hand performs better as \u03b1 increases and converges in Shapley\u2019s game when \u03b1 = 1.\nWe believe the reason is that small \u03b1 leads to an out-of-date reward estimate which in turn leads agents to continuously chase the NE without successfully converging. Smaller \u03b1 means more samples contribute to the computed expected reward. Using more samples to estimate the reward makes the estimate more accurate. However, the time required to get more samples may in fact degrade the accuracy of reward estimate, because the expected reward changes over time.\nSetting the value of \u03b1 always to 1 can result in sub-optimal performance, as illustrated by the following experiment. Consider the biased game shown in Table 3. The NE policy of the biased game is mixed with probabilities not uniform across actions, unlike previous benchmark games that had a mixed NE with uniform probability across actions. When \u03b1 was set to 0.01, WPL converges to a policy close to the NE policy as shown in Figure 18. When \u03b1 = 1, WPL converged to a policy far from the NE policy as shown in Figure 19.\nTo understand the effect of having \u03b1 = 1 in case of WPL, consider the following scenario in the biased game. Suppose the policy of the column player is fixed at 0.7,0.3. If the value of \u03b1 is small, then the action value function approximates well the expected value of each action. The value of the first row action (from the row player perspective) = 0.7\u00d71 + 0.3\u00d71.85= 1.255. Similarly, the value of the second row action = 0.7 \u00d71.15 + 0.3\u00d71 = 1.105. Unless the column player changes its policy, the row player gradient clearly points toward increasing the probability of choosing the first action (up to 1). Now consider the case when \u03b1 = 1. In that case the action value reflects the latest (sample) reward, not the average. In this case, the probability of choosing the first action, p, increases on average by\n\u2206 = 0.7\u00d7 [0.7\u00d7 (1\u2212 1.15)\u00d7 p+ 0.35\u00d7 (1\u2212 1)] + 0.3\u00d7 [0.7\u00d7 (1.85\u2212 1.15)\u00d7 (1\u2212 p) + 0.3\u00d7 (1.85\u2212 1)\u00d7 (1\u2212 p)]\n= \u22120.297p+ 0.2235\nwhich means that the row player\u2019s policy will effectively stabilize when \u2206 = 0 or p = 0.75. In other words, it is possible for players to stabilize at an equilibrium that is not an NE. Note that this problem occurs mainly in biased games. In common benchmark games where the NE is uniform across actions or pure, WPL will still converge to an NE even if \u03b1 = 1, as shown in Figure 16 and Figure 17.\nTuning \u03b1 is not an easy task when the expected reward itself is dynamically changing (because other agents are changing their policies concurrently). We are currently investigating an extension to\nWPL that automatically recognizes oscillations and adjusts the value of \u03b1 accordingly. Preliminary results are promising.\nThe following section illustrates the applicability of our algorithm by applying WPL in a more realistic setting involving 100 agents."}, {"heading": "5.4 Distributed Task Allocation Problem (DTAP)", "text": "We use a simplified version of the distributed task allocation domain (DTAP) (Abdallah & Lesser, 2007), where the goal of the multiagent system is to assign tasks to agents such that the service time of each task is minimized. For illustration, consider the example scenario depicted in Figure 20. Agent A0 receives task T1, which can be executed by any of the agents A0, A1, A2, A3, and A4. All agents other than agent A4 are overloaded, and therefore the best option for agent A0 is to forward task T1 to agent A2 which in turn forwards the task to its left neighbor (A5) until task T1 reaches agent A4. Although agent A0 does not know that A4 is under-loaded (because agent A0 interacts only with its immediate neighbors), agent A0 will eventually learn (through experience and interaction with its neighbors) that sending task T1 to agent A2 is the best action without even knowing that agent A4 exists.\nQ-learning is not appropriate in such setting due to communication delay (which results in partial observability). For example, even if two neighbors have practically the same load, Q-learning will assign all incoming requests to one of the neighbors until a feedback is received later indicating change in the load. It should be noted that Q-learning was successfully used in the packet routing domain (Boyan & Littman, 1994; Dutta, Jennings, & Moreau, 2005), where load balancing is not\nthe main concern (the main objective is routing a packet from a particular source to a particular destination).\nNow let us define the different aspects of the DTAP domain more formally. Each time unit, agents make decisions regarding all task requests received during this time unit. For each task, the agent can either execute the task locally or send the task to a neighboring agent. If an agent decides to execute the task locally, the agent adds the task to its local queue, where tasks are executed on a first come first serve basis, with unlimited queue length.\nEach agent has a physical location. Communication delay between two agents is proportional to the Euclidean distance between them, one time unit per distance unit. Agents interact via two types of messages. A REQUEST message \u3008i, j, T \u3009 indicates a request sent from agent i to agent j requesting task T . An UPDATE message \u3008i, j, T,R\u3009 indicates a feedback (reward signal) from agent i to agent j that task T tookR time steps to complete (the time steps are computed since agent i received T \u2019s request). The main goal of DTAP is to reduce the total service time, averaged over tasks, ATST =\u2211 T\u2208T\u03c4 TST (T )\n|T \u03c4 | , where T \u03c4 is the set of task requests received during a time period \u03c4 and TST is the\ntotal time a task spends in the system. TST consists of the time for routing a task request through the network, the time a task request spends in the local queue, and the time of actually executing the task.\nIt should be noted that although the underlying simulator have different underlying states, we deliberately made agent oblivious to these states. The only feedback an agent gets (consistent with our initial claim) is its own reward. The agents learn a joint policy that makes a good compromise over the different unobserved states (because the agents can not distinguish between these states).\nWe have evaluated WPL\u2019s performance using the following setting.9 100 agents are organized in a 10x10 grid. Communication delay between two adjacent agents is two time units. Tasks arrive at the 4x4 sub-grid at the center at rate 0.5 tasks/time unit. All agents can execute a task with a rate of 0.1 task/time unit (both task arrival and service durations follow an exponential distribution).\nFigure 21 shows the results of applying GIGA, GIGA-WoLF and WPL using value-learning-rate of 1 and policy-learning-rate of 0.0001. GIGA fail to converge, while WPL and GIGA-WoLF do converge. WPL converges faster and to a better ATST: GIGA-WoLF\u2019s ATST converges to around 100 time units, while WPL\u2019s ATST converges to around 70 time units.\nWe believe GIGA-WoLF\u2019s slow convergence is due to the way GIGA-WoLF works. GIGAWoLF relies on learning a slowly moving policy in addition to the actual policy \u03c0 in order to approximate the NE policy. This requires more time than the WPL algorithm. Furthermore, GIGA-WoLF\u2019s dynamics can be discontinuous prior to convergence and reach extreme deterministic policies even if the NE policy is mixed. In a large system, this can have ripple effect and slow system-wide convergence. WPL, on the other hand, has continuous dynamics, allowing faster collective convergence."}, {"heading": "6. Conclusion and Future Work", "text": "This work presents WPL, a gradient ascent multiagent reinforcement learning algorithm (GAMARL) that assumes an agent neither knows the underlying game nor observes other agents. We experimentally show that WPL converges in benchmark 2-player-2-action games. We also show\n9. The simulator is available online at http://www.cs.umass.edu/\u02dcshario/dtap.html.\nthat WPL converges in Shapley\u2019s game where none of the previous GA-MARL algorithms successfully converged. We verify the practicality of our algorithm in the distributed task allocation domain with a network of 100 agents. WPL outperforms the state-of-the-art GA-MARL algorithms in both the speed of convergence and the expected reward. We analyze the dynamics of WPL and show that it has continuous non-linear dynamics, while previous GA-MARL algorithms had discontinuous dynamics. We show that our predicted theoretical behavior is consistent with our experimental results.\nIn this work we briefly illustrated the importance of value-learning-rate and how it affects convergence, particularly for our proposed algorithm WPL. Finding the right balance and theoretically analyzing how it affects convergence are interesting research questions. We are currently investigating an extension to WPL that automatically finds a good value-learning-rate. Preliminary experimental results are promising.\nAnother future direction we are considering is extending our theoretical analysis to games with more actions and more players in order to verify our experimental findings in Shapley\u2019s game and the distributed task allocation domain. We are currently investigating alternative methodologies for analyzing dynamics, including the evolutionary dynamics (Tuyls, \u2019t Hoen, & Vanschoenwinkel, 2006) and Lyapunov stability theory (Khalil, 2002)."}, {"heading": "7. Acknowledgments", "text": "This work is based on our previous conference publications (Abdallah & Lesser, 2006, 2008)."}], "references": [{"title": "Learning the task allocation game", "author": ["S. Abdallah", "V. Lesser"], "venue": "In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Abdallah and Lesser,? \\Q2006\\E", "shortCiteRegEx": "Abdallah and Lesser", "year": 2006}, {"title": "Multiagent reinforcement learning and self-organization in a network of agents", "author": ["S. Abdallah", "V. Lesser"], "venue": "In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems", "citeRegEx": "Abdallah and Lesser,? \\Q2007\\E", "shortCiteRegEx": "Abdallah and Lesser", "year": 2007}, {"title": "Non-linear dynamics in multiagent reinforcement learning algorithms", "author": ["S. Abdallah", "V. Lesser"], "venue": "In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Abdallah and Lesser,? \\Q2008\\E", "shortCiteRegEx": "Abdallah and Lesser", "year": 2008}, {"title": "Adaptive policy gradient in multiagent learning", "author": ["B. Banerjee", "J. Peng"], "venue": "In Proceedings of the International Joint Conference on Autonomous Agents and Multi Agent Systems,", "citeRegEx": "Banerjee and Peng,? \\Q2003\\E", "shortCiteRegEx": "Banerjee and Peng", "year": 2003}, {"title": "Generalized multiagent learning with performance bound", "author": ["B. Banerjee", "J. Peng"], "venue": "Autonomous Agents and Multiagent Systems,", "citeRegEx": "Banerjee and Peng,? \\Q2007\\E", "shortCiteRegEx": "Banerjee and Peng", "year": 2007}, {"title": "Convergence and no-regret in multiagent learning", "author": ["M. Bowling"], "venue": "Tech. rep., University of Alberta", "citeRegEx": "Bowling,? \\Q2004\\E", "shortCiteRegEx": "Bowling", "year": 2004}, {"title": "Convergence and no-regret in multiagent learning", "author": ["M. Bowling"], "venue": "In Proceedings of the Annual Conference on Advances in Neural Information Processing Systems,", "citeRegEx": "Bowling,? \\Q2005\\E", "shortCiteRegEx": "Bowling", "year": 2005}, {"title": "Multiagent learning using a variable learning rate", "author": ["M. Bowling", "M. Veloso"], "venue": "Artificial Intelligence,", "citeRegEx": "Bowling and Veloso,? \\Q2002\\E", "shortCiteRegEx": "Bowling and Veloso", "year": 2002}, {"title": "Packet routing in dynamically changing networks: A reinforcement learning approach", "author": ["J.A. Boyan", "M.L. Littman"], "venue": "In Proceedings of the Annual Conference on Advances in Neural Information Processing Systems,", "citeRegEx": "Boyan and Littman,? \\Q1994\\E", "shortCiteRegEx": "Boyan and Littman", "year": 1994}, {"title": "The dynamics of reinforcement learning in cooperative multiagent systems", "author": ["C. Claus", "C. Boutilier"], "venue": "In Proceedings of the National Conference on Artificial intelligence/Innovative Applications of Artificial Intelligence,", "citeRegEx": "Claus and Boutilier,? \\Q1998\\E", "shortCiteRegEx": "Claus and Boutilier", "year": 1998}, {"title": "AWESOME: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents", "author": ["V. Conitzer", "T. Sandholm"], "venue": "Machine Learning,", "citeRegEx": "Conitzer and Sandholm,? \\Q2007\\E", "shortCiteRegEx": "Conitzer and Sandholm", "year": 2007}, {"title": "Cooperative information sharing to improve distributed learning in multi-agent systems", "author": ["P.S. Dutta", "N.R. Jennings", "L. Moreau"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dutta et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dutta et al\\.", "year": 2005}, {"title": "Nash Q-learning for general-sum stochastic games", "author": ["J. Hu", "M.P. Wellman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hu and Wellman,? \\Q2003\\E", "shortCiteRegEx": "Hu and Wellman", "year": 2003}, {"title": "Nonlinear Systems. Prentice-Hall, Upper Saddle River, NJ, USA", "author": ["H.K. Khalil"], "venue": null, "citeRegEx": "Khalil,? \\Q2002\\E", "shortCiteRegEx": "Khalil", "year": 2002}, {"title": "Value-function reinforcement learning in Markov games", "author": ["M. Littman"], "venue": "Cognitive Systems Research,", "citeRegEx": "Littman,? \\Q2001\\E", "shortCiteRegEx": "Littman", "year": 2001}, {"title": "Learning to cooperate via policy search", "author": ["L. Peshkin", "Kim", "K.-E", "N. Meuleau", "L.P. Kaelbling"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Peshkin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Peshkin et al\\.", "year": 2000}, {"title": "Nash convergence of gradient dynamics in generalsum games", "author": ["S. Singh", "M. Kearns", "Y. Mansour"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1999\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1999}, {"title": "An evolutionary dynamical analysis of multi-agent learning in iterated games", "author": ["K. Tuyls", "P.J. \u2019t Hoen", "B. Vanschoenwinkel"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Tuyls et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tuyls et al\\.", "year": 2006}, {"title": "Reinforcement learning to play an optimal Nash equilibrium in team Markov games", "author": ["X. Wang", "T. Sandholm"], "venue": "In Proceedings of the Annual Conference on Advances in Neural Information Processing Systems,", "citeRegEx": "Wang and Sandholm,? \\Q2003\\E", "shortCiteRegEx": "Wang and Sandholm", "year": 2003}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": "An alternative goal that we pursue here is converging to the Nash Equilibrium (NE) (Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007), which is by definition a local maximum across agents (no agent can do better by deviating unilaterally from the NE).", "startOffset": 83, "endOffset": 148}, {"referenceID": 16, "context": "Recently, several multi-agent reinforcement learning (MARL) algorithms have been proposed and studied (Claus & Boutilier, 1998; Singh et al., 2000; Peshkin, Kim, Meuleau, & Kaelbling, 2000; Littman, 2001; Bowling & Veloso, 2002; Hu & Wellman, 2003; Bowling, 2005; Abdallah & Lesser, 2006; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007).", "startOffset": 102, "endOffset": 338}, {"referenceID": 14, "context": "Recently, several multi-agent reinforcement learning (MARL) algorithms have been proposed and studied (Claus & Boutilier, 1998; Singh et al., 2000; Peshkin, Kim, Meuleau, & Kaelbling, 2000; Littman, 2001; Bowling & Veloso, 2002; Hu & Wellman, 2003; Bowling, 2005; Abdallah & Lesser, 2006; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007).", "startOffset": 102, "endOffset": 338}, {"referenceID": 6, "context": "Recently, several multi-agent reinforcement learning (MARL) algorithms have been proposed and studied (Claus & Boutilier, 1998; Singh et al., 2000; Peshkin, Kim, Meuleau, & Kaelbling, 2000; Littman, 2001; Bowling & Veloso, 2002; Hu & Wellman, 2003; Bowling, 2005; Abdallah & Lesser, 2006; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007).", "startOffset": 102, "endOffset": 338}, {"referenceID": 6, "context": "Furthermore, we show that WPL converges in the challenging Shapley\u2019s game, where state-of-theart MARL algorithms failed to converge (Bowling & Veloso, 2002; Bowling, 2005),1 unlike WPL.", "startOffset": 132, "endOffset": 171}, {"referenceID": 16, "context": "1 Game Theory Game theory provides a framework for modeling agents\u2019 interaction and was used by previous researchers in order to analyze the convergence properties of MARL algorithms (Claus & Boutilier, 1998; Singh et al., 2000; Bowling & Veloso, 2002; Wang & Sandholm, 2003; Bowling, 2005; Conitzer & Sandholm, 2007; Abdallah & Lesser, 2006).", "startOffset": 183, "endOffset": 342}, {"referenceID": 6, "context": "1 Game Theory Game theory provides a framework for modeling agents\u2019 interaction and was used by previous researchers in order to analyze the convergence properties of MARL algorithms (Claus & Boutilier, 1998; Singh et al., 2000; Bowling & Veloso, 2002; Wang & Sandholm, 2003; Bowling, 2005; Conitzer & Sandholm, 2007; Abdallah & Lesser, 2006).", "startOffset": 183, "endOffset": 342}, {"referenceID": 6, "context": "Table 1 and Table 2 provide example benchmark games that were used in evaluating previous MARL algorithms (Bowling & Veloso, 2002; Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007).", "startOffset": 106, "endOffset": 195}, {"referenceID": 16, "context": "NE is easier than games where the only NE is mixed (Singh et al., 2000; Bowling & Veloso, 2002; Zinkevich, 2003).", "startOffset": 51, "endOffset": 112}, {"referenceID": 20, "context": "NE is easier than games where the only NE is mixed (Singh et al., 2000; Bowling & Veloso, 2002; Zinkevich, 2003).", "startOffset": 51, "endOffset": 112}, {"referenceID": 6, "context": "Shapley\u2019s game, in particular, has received considerable attention from MARL community (Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007) as it remains challenging for state-of-the-art MARL algorithms despite its apparent simplicity (and similarity to the rock-paper-scissors game which is not as challenging).", "startOffset": 87, "endOffset": 152}, {"referenceID": 16, "context": "The Infinitesimal Gradient Ascent algorithm (IGA) (Singh et al., 2000) and its generalization (Generalized IGA or GIGA) (Zinkevich, 2003) were proved to converge in games with pure NE.", "startOffset": 50, "endOffset": 70}, {"referenceID": 20, "context": ", 2000) and its generalization (Generalized IGA or GIGA) (Zinkevich, 2003) were proved to converge in games with pure NE.", "startOffset": 57, "endOffset": 74}, {"referenceID": 6, "context": "Several modifications to IGA and GIGA were proposed to avoid divergence in games with mixed NE, including: IGA/PHC-WoLF (Bowling & Veloso, 2002), PHC-PDWoLF (Banerjee & Peng, 2003), and GIGA-WoLF (Bowling, 2005).", "startOffset": 196, "endOffset": 211}, {"referenceID": 16, "context": "3 Gradient-Ascent MARL Algorithms The first GA-MARL algorithm whose dynamics were analyzed is the Infinitesimal Gradient Ascent (IGA) (Singh et al., 2000).", "startOffset": 134, "endOffset": 154}, {"referenceID": 16, "context": "The original IGA paper (Singh et al., 2000) defined the projection function (and therefore IGA) in case each agent has only two actions to choose from.", "startOffset": 23, "endOffset": 43}, {"referenceID": 20, "context": "A general definition of the projection function was later developed and the resulting algorithm was called Generalized IGA or GIGA (Zinkevich, 2003).", "startOffset": 131, "endOffset": 148}, {"referenceID": 20, "context": "The generalized projection function projects an invalid policy to the closest valid policy within the simplex (Zinkevich, 2003).", "startOffset": 110, "endOffset": 127}, {"referenceID": 16, "context": "The IGA algorithm did not converge in all two-player-two-action games (Singh et al., 2000).", "startOffset": 70, "endOffset": 90}, {"referenceID": 6, "context": "This approximation allows GIGA-WoLF to converge in the tricky game, but not in Shapley\u2019s game (Bowling, 2005).", "startOffset": 94, "endOffset": 109}, {"referenceID": 16, "context": "4 Dynamics of GA-MARL Algorithms Differential equations were used to model the dynamics of IGA (Singh et al., 2000).", "startOffset": 95, "endOffset": 115}, {"referenceID": 16, "context": "In IGA\u2019s analysis (Singh et al., 2000), the effect of the projection function was taken into account by considering all possible locations of the simplex.", "startOffset": 18, "endOffset": 38}, {"referenceID": 20, "context": "\u2206\u03c0i(a)\u2190 \u2202Vi(\u03c0) \u2202\u03c0i(a) \u00b7 \u03b7 \u00b7 { \u03c0i(a) if \u2202Vi(\u03c0) \u2202\u03c0i(a) < 0 1\u2212 \u03c0i(a) otherwise \u03c0i \u2190 projection(\u03c0i + \u2206\u03c0i) The projection function is adopted from GIGA (Zinkevich, 2003) with minor modification:\u2200a : 1 \u2265 \u03c0(a) \u2265 (the modification ensures a minimum amount of exploration ).", "startOffset": 147, "endOffset": 164}, {"referenceID": 6, "context": "for all gradient-ascent MARL algorithms (Bowling & Veloso, 2002; Bowling, 2005; Banerjee & Peng, 2007).", "startOffset": 40, "endOffset": 102}, {"referenceID": 16, "context": "It is important to note, however, that all gradient ascent MARL algorithms (including WPL) converge in 2x2 games cases where there is (at least) one pure NE, because the dynamics do not have any loops and eventually lead to one of the pure equilibriums (Singh et al., 2000; Bowling & Veloso, 2002).", "startOffset": 253, "endOffset": 297}, {"referenceID": 6, "context": "The performance of GIGA, PHC-WoLF, and GIGA-WoLF conforms to the results reported previously by their authors (Bowling & Veloso, 2002; Bowling, 2005).", "startOffset": 110, "endOffset": 149}, {"referenceID": 13, "context": "We are currently investigating alternative methodologies for analyzing dynamics, including the evolutionary dynamics (Tuyls, \u2019t Hoen, & Vanschoenwinkel, 2006) and Lyapunov stability theory (Khalil, 2002).", "startOffset": 189, "endOffset": 203}], "year": 2008, "abstractText": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents\u2019 decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapley\u2019s game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently. An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPL\u2019s convergence is difficult, because of the non-linear nature of WPL\u2019s dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPL\u2019s dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms.", "creator": "TeX"}}}