{"id": "1401.3836", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "An Active Learning Approach for Jointly Estimating Worker Performance and Annotation Reliability with Crowdsourced Data", "abstract": "Crowdsourcing platforms offer a practical solution bring on certain only choosy annotating heavy microstructures though training supervised classifiers. Unfortunately, but service performance frequently threatens when deal annotation improvement, and requesting five labeling for every exception are needed it tiny generate increases get jeopardizing good overall. Minimizing the required evaluation found using form active learning selection consideration reduces after generic requirement but way undermines talian nursing took focusing tuesday disregarded annotations. This paint biographical well current learning action it which job performance, defense focused, such macros reliability others successfully estimated one used any compute the mean defined guiding the content selection procedure. We demonstrate that the measure broad, either 650 major learning with Bayesian system, increasing faster teams calibration and probably overall all expertise of unknown labelers in the because this annotation minimal.", "histories": [["v1", "Thu, 16 Jan 2014 04:51:19 GMT  (1248kb,D)", "http://arxiv.org/abs/1401.3836v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG cs.HC", "authors": ["liyue zhao", "yu zhang", "gita sukthankar"], "accepted": false, "id": "1401.3836"}, "pdf": {"name": "1401.3836.pdf", "metadata": {"source": "CRF", "title": "An Active Learning Approach for Jointly Estimating Worker Performance and Annotation Reliability with Crowdsourced Data", "authors": ["Liyue Zhao", "Yu Zhang", "Gita Sukthankar"], "emails": ["lyzhao@cs.ucf.edu", "yuz1988@iastate.edu", "gitars@eecs.ucf.edu"], "sections": [{"heading": "1 Introduction", "text": "Our work is motivated by the recent interest in the use of crowdsourcing [3] as a source of annotations from which to train machine learning systems. Crowdsourcing transforms the problem of generating a large corpus of labeled real-world data from a monolithic labor-intensive ordeal into a manageable set of small tasks, processed by thousands of human workers in a timely and affordable manner. Services such as Amazon\u2019s Mechanical Turk (MTurk) have made it possible for researchers to acquire sufficient quantities of labels, enabling the development of a variety of applications driven by supervised learning models. However, employing crowdsourcing to label large quantities of data remains challenging for two important reasons: limited annotation budget and label noise. First, although the unit cost of obtaining each annotation is low, the\nar X\niv :1\n40 1.\n38 36\nv1 [\ncs .L\noverall cost grows quickly since it is proportional to the number of requested labels, which can number in the millions. This has stimulated the use of approaches such as active learning [6] that aim to minimize the amount of data required to learn a highquality classifier. Second, the quality of crowdsourced annotations has been found to be poor [7], with causes ranging from workers who overstate their qualifications, lack of motivation among labelers, haste and deliberate vandalism. Unfortunately, the majority of popular active learning algorithms, while robust to noise in the input features, can be very sensitive to label noise, necessitating the development of approaches specifically designed for noisy annotations. In this paper, we focus on the causal factors behind noisy crowdsourced annotations, worker quality and task difficulty.\nA common simplifying assumption is that of identicality: all labelers provide annotations with the same accuracy and all annotation tasks pose the same difficulty to all workers. Under this assumption, a good way to combat annotation noise is to request multiple labels for each selected task and then to apply majority voting. The simplest approach of requesting the same number of labels for each instance is not usually the most cost-effective since label redundancy increases the overall cost by a multiplicative factor of at least three.\nBetter results can be obtained by applying weighted voting which assigns different weights to labelers based on their previous performance [7, 12, 13]. In this paper, we use the Generative model of Labels, Abilities, and Difficulties [10] in which labeler expertise and the task difficulty are simultaneously estimated using EM (ExpectationMaximization) to learn the parameters of a probabilistic graphical model which represents the relationship between labelers, tasks and annotation predictions. Rather than using previous performance to assign weights, the estimated labeler expertise is used to allocate weights to annotator votes. This paper focuses on the problem of reducing the label budget used by the GLAD model with active learning.\nTheoretically the most straightforward and effective strategy for active learning is to select samples that offer the greatest reductions to the risk function. Thus, \u201caggressive\u201d criteria such as least confidence, smallest margin, or maximum entropy can enable active learning to obtain high accuracy using a relatively small number of labels to set the decision boundary. Unfortunately, the existence of label noise can trigger failures in aggressive active learning methods because even a single incorrect label can cause the algorithm to eliminate the wrong set of hypotheses, thus focusing the search on a poor region of the version space. In contrast, the proposed combination of the probabilistic graphical model and active learning, avoids explicitly eliminating any set of hypotheses inconsistent with the label provided by annotators.\nSpecifically, this paper makes two contributions:\n1. we propose a new sampling strategy which iteratively selects the combination of worker and task which offers the greatest risk reduction between the current labeling risk and the expected posterior risk. The strategy aims to focus on sampling reliable labelers and uncertain tasks to train the Bayesian network.\n2. we present comprehensive evaluations on both simulation and real world datasets that show not only the strength of our proposed approach in significantly reducing the quantity of labels required for training the model, but also the scalabil-\nity of our approach at solving practical crowdsourcing tasks which suffer large amounts of annotation noise."}, {"heading": "2 Related Work", "text": "Howe et al. [3] coined the phrase \u201ccrowdsourcing\u201d to describe the concept of outsourcing work to a cheap labor pool composed of everyday people who use their spare time to create content and solve problems. Doan et al. [1] define crowdsourcing as enlisting a crowd of humans to help solve a problem defined by the system owners. Crowdsourcing annotation services, such as Amazon\u2019s Mechanical Turk, have become an effective way to distribute annotation tasks over multiple workers [9]; however, Sheng et al. [7] noted the problem that crowdsourcing annotation tasks may generate unreliable labels.\nSeveral works [2, 11, 5] propose different approaches to model the annotation accuracy of workers; all of these approaches assume there are multiple experts/annotators providing labels but that no oracle exists. Donmez et al. [2] propose a framework to learn the expected accuracy at each time step. The estimated expected accuracies are then used to decide which annotators should be queried for a label at the next time step. Yan et al. [11] focus on the multiple annotator scenario where multiple labelers with varying expertise are available for querying. This method can simultaneously answer questions such as which data sample should be labeled next and which annotator should be queried to benefit the learning model. Raykar et al. [5] use a probabilistic model both to evaluate the different experts and also to provide an estimate of the actual hidden labels.\nOur work is strongly influenced by GLAD [10], which uses a probabilistic model to simultaneously estimate the labels, the labeler expertise, and the task difficulty which are represented as latent variables in the Bayesian network. The model can estimate the label of a new task with a weighted combination of labels from different labelers based on their expertise inferred in the training phase. However, the original GLAD model does not use active learning, unlike our proposed approach which offers substantial reductions to the labeling cost, without sacrificing annotation accuracy.\nTong and Koller[8] proposed a way to implement active learning in Bayesian networks with a simple structure. Tong and Koller chose Kullback-Leibler divergence as the loss function to measure the distance between distributions. Their algorithm iteratively computes the expected change in risk and makes the sample query with the greatest expected change. This strategy is guaranteed to request the label of the sample that reduces the expected risk the most, but does not account for worker performance. In this paper, we propose an alternate selection strategy which selects pairs of workers and samples and uses an entropy-based risk function."}, {"heading": "3 Method", "text": "In this section, we describe our active learning approach for jointly estimating worker performance and annotation reliability. The first part of the section defines the probabilistic graphical model for estimating the expertise of labelers and the difficulty of\nannotation tasks before describing how EM is used to estimate the model parameters. The second subsection introduces our active learning approach for sampling workers and tasks using an entropy-based risk function."}, {"heading": "3.1 Probabilistic Graphical Model", "text": "Our work utilizes the generative model proposed by Whitehill et al. whitehill2009whose. The structure of the graphical model is shown in Figure 1. The same model is used to estimate the expertise of workers, the difficulty of annotation tasks, and the true labels. The expertise of worker i is defined as \u03b1i \u2208 (\u2212\u221e,+\u221e), which corresponds to the worker\u2019s level of annotation accuracy. As \u03b1i approaches +\u221e, worker i becomes an increasingly capable oracle who almost always gives the correct label and as \u03b1i approaches \u2212\u221e the worker almost always provides the wrong label. \u03b1i = 0 means the worker has no capability to distinguish the difference between the two classes and just randomly guesses a label. The difficulty of task j is parameterized by \u03b2j where 1 \u03b2j \u2208 (0,+\u221e). For easy tasks, 1\u03b2j approaches zero, and it is assumed that practically every worker can give the correct label for the task. As 1\u03b2j approaches +\u221e, the task becomes so difficult that almost no one is able to provide the right answer. In this paper, we only consider binary classification problems which assume that both the true label Zj and the annotation lij provided by the worker i are binary labels. lij \u2208 {\u22121,+1} is defined as the label of task j provided by annotator i. This is the only observable variable in the graphical model. Since in most crowdsourcing platforms, it is unlikely that the same labelers will be responsible for annotating all tasks in the dataset, this model also works well when the observation is incomplete. The true label zj of task j is the variable that we are going to estimate to evaluate the labeling performance of the model.\nGiven the definitions above, the probability of annotator i providing the correct\nlabel for task j is defined in Equation 1. For a more skilled labeler with a larger \u03b1, or an easier task with a larger \u03b2, the probability of providing the correct label should be larger. However, if the task is too difficult ( 1\u03b2 \u2192 +\u221e) or the labeler has no background in performing the task (\u03b1 = 0), the labeler can only give a random guess (p = 0.5) about the task label.\nP (lij = zj |\u03b1i, \u03b2j) = 1\n1 + e\u2212\u03b1i\u03b2j (1)\nThe set of parameters of the graphical model \u03b1 = {\u03b11, ..., \u03b1i, ..., \u03b1m} and \u03b2 = {\u03b21, ..., \u03b2j , ..., \u03b2n} are represented as \u03b8 = {\u03b1,\u03b2}. L and z are defined as the set of labels provided by the workers (observed) and the true labels (not known).\nEM is an iterative algorithm for maximum likelihood estimation in a graphical model with incomplete data. In this annotation task, the learning procedure starts with the set of unreliable annotations L. The EM algorithm iteratively estimates the unknown parameters \u03b8 with the current observations L and then updates the belief of the true labels z using these estimated parameters. The description of the EM algorithm as applied to this model is given below.\nE-Step: At the expectation step, the parameters \u03b8 estimated in the maximization step are fixed. The posterior probability of zj \u2208 {0, 1} given \u03b8 is computed as:\np(zi|L, \u03b8) \u221d p(zj) \u220f i p(lij , \u03b1i, \u03b2j) (2)\nM-Step: Given the posterior probability of zj , the maximization step uses the cost function Q(\u03b8) to estimate a locally optimal solution of parameters \u03b8:\nQ(\u03b8) = \u2211 j E[ln p(zj)] + \u2211 ij E[ln p(lij |zj , \u03b1i, \u03b2j)] (3)\nThe aim of the EM algorithm is to return the parameters \u03b8\u2217 which maximize the function Q(\u03b8):\n\u03b8\u2217 = argmax \u03b8 Q(\u03b8) (4)\n\u03b8\u2217 represents the optimal estimate of the expertise of labelers and the difficulty of tasks with the current set of annotations."}, {"heading": "3.2 Query-by-Uncertainty", "text": "The essential question in active learning is which criterion performs the best at identifying the most valuable unlabeled sample under the present parameters. In this paper, we employ the entropy of the label distribution to represent the risk of a particular label assignment. We assume the class of labels is defined as C = {\u22121,+1}, and the risk function is\nAlgorithm 1 Active Learning Algorithm Require:\nInput: A set of data along with a matrix of partial labels L The initial set of parameters \u03b8 B: labeling budget. Output: \u03b8\u2217 = {\u03b1\u2217,\u03b2\u2217}: The set of parameters representing the expertise of labelers and the difficulty of tasks. 1: while B > 0 do 2: Use the EM algorithm to update the parameters \u03b8 = {\u03b1,\u03b2} using Equations 2 and 3; 3: Find parameters \u03b8\u2217 that maximize the function Q(\u03b8) using Equation 4; 4: Calculate the risk of assigning label zj to task j with Equation 5; 5: Query the label lij by assigning task j with the maximum risk to the worker i with the highest \u03b1i and lij 6= 0; 6: B \u2190 B \u2212 1; 7: end while 8: Return the optimal parameters \u03b8\u2217.\nRisk(zj) = \u2212 \u2211 |C| p(zj |L, \u03b8\u2217) |C| log p(zj |L, \u03b8\u2217) |C|\n(5)\nwhere zj is the true label and p(zj |L, \u03b8\u2217) represents the probability of annotating task j with label zj under the present parameters \u03b8\u2217 . This risk function evaluates the risk of assigning sample j label zj . Our active learning algorithm preferentially selects the sample that maximizes this risk function to be annotated.\nThe algorithm for using active learning within the Bayesian network is shown in Algorithm 1. The algorithm begins with the data to be annotated and a partially completed label matrix L. The set of parameters \u03b8 is initialized at this stage, and the labeling budget for the crowdsourcing is allocated. The goal of this algorithm is to learn the set of optimal parameters \u03b8\u2217 with a given label budget. At each iteration, our algorithm selects the sample that maximizes the risk function defined in Equation 5 with the parameters \u03b8\u2217 estimated using the EM algorithm. The sample is then annotated by requesting label lij from the labeler i with the maximum current \u03b1i who had not previously contributed a label to that task."}, {"heading": "4 Worker Selection", "text": "Selecting the task to be labeled could be easily solved by applying active learning strategies to identify the most uncertain or informative task to workers. However, how to select the right worker to annotate the task has became another issue which is out of the scope of active learning. Selecting the most \u201cuncertain\u201d worker may polish the estimation of the worker expertise, but simultaneously reduce the speed to improve the training accuracy, since such strategy inevitably pick workers who the system is most unfamiliar with, rather than the best workers.\nThe straightforward way we have used in Algorithm 1 is to ask the worker i with the highest expertise estimation \u03b1\u2217i to provide the label. The experiment results shows this strategy works well. However, there exists some arguments that challenge the strategy since 1) \u03b1\u2217i is only the estimation of \u03b1i, which means picking the worker with largest \u03b1\u2217 may ignore the real \u201cbest worker\u201d and 2) the evaluation of other workers are ignored since they will not be selected forever. The goal of this section is to investigate the performances of different worker selection strategies.\nBeside always sampling the best worker, we evaluate two other options: weighted selection and -greedy selection. For the weighted selection strategy, the probability of selecting worker i is proportional to the expertise \u03b1\u2217i .\np(i) = \u03b1\u2217i\u2211 i \u03b1 \u2217 i\n(6)\nAn alternative worker selection strategy is the -greedy selection algorithm, which has been used in studying the exploration-exploitation tradeoff in reinforcement learning [4]. This algorithm selects the most possible worker i (with the highest expertise \u03b1i) with probability 1\u2212 . and selects other workers with probability .\np(i) =\n{ 1\u2212 + m i = max\u03b1 \u2217 i\nm otherwise\n(7)\nwhere \u2208 [0, 1) represents how many weights to take from selecting the best worker to other workers, and m is the number of workers available to the task in total."}, {"heading": "5 Experiments", "text": "The aim of our experiments is to demonstrate that, in cases where the tasks are not labeled by all labelers, our proposed sampling strategy in choosing samples to be labeled compared with the random sampling strategy. Although there are three sets of parameters in the model: \u03b1i which is the expertise of the labeler i, \u03b2j which is the difficulty of the task j and zj which is the true label of the task j, our proposed method proves that focusing on estimating a better labeler expertise \u03b1 is more important than the task difficulty \u03b2 in predicting the real label of the task z. Compared with random sampling, given the same number of training labels, active sampling has advantages in 1) predicting more correct labels and 2) identifying a more correlated rank of labelers.\nIn the following experiments, we test the performance of our proposed active learning algorithm on 1) a pool of simulated workers and tasks and 2) a standard image dataset crowdsourced by workers on Mechanical Turk. The algorithm is evaluated using the annotation accuracy of the predicted labels z and by comparing the actual and predicted labeler expertise\u03b1 with\u03b1\u2032 using both Spearman rank and Pearson correlation statistics. We omit results for evaluating the model\u2019s predictions of task difficulty, since its main importance is its contribution to label noise which we are measuring directly through annotation accuracy.\nOur experiments use the same evaluation protocol and dataset as Whitehill et al. [10] while attempting to reduce the labeling budget required to reach the desired accuracy level using three different active learning strategies:\nproposed: selects the most capable worker and most uncertain task using Algorithm 1;\ntraversal: sequentially selects tasks and randomly selects workers;\nrandom: randomly selects pairs of workers and tasks.\nTo investigate the performance of worker selection, we implement our proposed active learning approach with different worker selection strategies on both simulated workers and real workers on Mechanical Turk. The experiments compare worker expertise using both Spearman rank and Pearson correlation for simulated workers and the annotation accuracy to evaluate real workers. Our experiments attempt to identify good workers by using four different worker selection strategies:\nbest worker: the worker i with highest parameter \u03b1i will be selected;\nweighted selection: the probability of the worker i to be selected is proportional to the value of \u03b1i;\nepsilon=0.5: the -greedy selection algorithm (Equation 7), with = 0.5;\nepsilon=0.1: the -greedy selection algorithm (Equation 7), with = 0.1.\nAt the initialization phase of the active learning, all tasks are annotated by exactly two labelers who are randomly selected for each task to seed the training pool. Using this pool,\u03b1 and\u03b2 are estimated. At each iteration, one sample is selected from the pool of unlabeled pairs (labeler and task). The selected sample is then added to the training pool, and the parameters \u03b1, \u03b2 and z\u2032 are updated using the extended training pool.\n5.0.1 Simulated Worker Pool\nSince it is difficult to definitively determine the skill level of real human labelers, we ran an initial set of experiments using a simulated worker pool. In the first set of experiments, we evaluate a simple population that consists of workers with only two skill levels and task with two difficulty levels. The pool of simulated workers consists of 24 labelers (8 good, 16 bad) annotating 750 tasks (500 easy, 250 hard). The probabilities for each type of labeler correctly annotating different types of tasks are shown in Table 1.\nAt the initialization stage, each task is labeled by 2 randomly selected labelers, which produces a pool of 1500 labels before all sample selection strategies start running. Each strategy runs for 4000 iterations. Figure 2 shows a comparison of the training accuracy of the different sampling strategies (proposed, traversal, and random).\nSince the binary case is relatively simple, the training accuracy starts from a high level (80%) with the 1500 initial samples. Our proposed strategy improves rapidly and converges to almost 100% training accuracy in about 3300 (18.3%) labels. The traversal strategy barely converges to the same accuracy level with 5500 (30.6%) labels. Unsurprisingly the random strategy performs the worst and after selecting 5500 labels, it reaches a 94% training accuracy.\nFigure 3 and Figure 4 shows the result of using Pearson and Spearman rank correlations to evaluate the estimation of \u03b1. Both Pearson correlation and Spearman rank correlation measure the strength of a relationship between two sets of variables. A perfect correlation of 1 represents two set of variables that are perfectly correlated with each other. Pearson correlation measures the correlation between the actual estimates of worker performance, whereas the Spearman rank correlation compares the similarity of the relative rankings. For practical purposes, having a good Spearman rank correlation is sufficient for correctly selecting the best labeler from the pool.\nThe goal of this experiment is to evaluate the estimate of labeler expertise \u03b1 compared to the real value \u03b1\u2032. The result shows, by using our proposed active learning strategy, the correlation rapidly jumps to a high score only 200 iterations after initialization. Although the random and traversal sampling strategies reach the same level of correlation in the Pearson correlation, the active learning strategy wins a overwhelming victory with the Spearman rank correlation. The correlation score jumps to the optimal value after only 200 labels, compared to random and traversal who query more than 500 labels to approach the comparable correlation score. (Notice that the optimal value of the Spearman rank correlation is as low as 0.82 because there are many ties in the real rankings which inflate the differences between the real ranking and the estimated ranking.)\nFigure 5 and Figure 6 show the result of using Pearson and Spearman rank correlations to evaluate the performance of different worker selection strategies. The results show that no worker selection strategies shows an overwhelming advantage in this simple binary classification case.\nHowever, in the real world there can be many different levels of labeler expertise and task difficulty which makes the estimation problem more challenging. To model this, in the second experiment, the worker pool is simulated using \u03b1\u2032 and \u03b2\u2032 that are generated from a Gaussian distribution to simulate a more diverse population of labelers and tasks. We create 50 labelers and 1000 tasks in this experiment. The expertise of labeler i is determined by \u03b1\u2032i \u223c N (1, 1), and the difficulty of task j is determined by \u03b2\u2032j \u223c N (1, 1). At the initialization stage, each task was labeled by two randomly selected labelers, which yields 2000 labels before all strategies start running. Each strategy runs for 10000 iterations.\nFigure 7 shows the training accuracy of different sampling strategies (proposed, traversal, and random). Our proposed strategy still performs strongly at estimating the training accuracy which converges to 97% with 10000 labels. The traversal strategy not only converges slower but reaches a slightly lower accuracy rate around 96% with 10000 labels. The random strategy still performs the worst. After selecting 12000 labels, it reaches a 92% training accuracy.\nFigure 8 and Figure 9 show the result of using Pearson and Spearman rank correlation to evaluate the estimate of \u03b1. The results show that our active learning doesn\u2019t show an overwhelming advantage as in the binary classification case. The proposed method reaches a high rank correlation (0.93) faster than other two methods but the convergence score is a bit lower than other two methods after more labels have been queried. However, it doesn\u2019t necessarily means our algorithm will perform worse in\nselecting good labelers since the key is having an accurate estimate of the top labelers. Our aggressive sampling approach does not do a good job of evaluating those bad labelers whose \u03b1 value is also an important contributor in the rank correlation score. We will discuss this phenomenon in more depth during the discussion.\nFigure 10 and Figure 11 show the result of using Pearson and Spearman rank correlation to evaluate the performance of worker selection strategies. As \u03b1\u2032 and \u03b2\u2032 generated to simulate a more diverse population of labelers and tasks, the weighted selection and -greedy selection (with = 0.5) show a faster convergence to a higher correlation score. The -greedy selection (with = 0.5) reaches a high rank correlation (0.92) after 2000 iterations and finally converge at 0.96.\n5.0.2 Dataset Crowdsourced with Human Workers\nTo evaluate our active learning strategy on a standard binary classification task benchmark, we used a facial image dataset crowdsourced with human labelers on Amazon\u2019s Mechanical Turk. Whitehill et al. [10] asked 20 real human workers on Mechanical Turk to annotate 160 facial images by labeling them as either Duchenne or Non-Duchenne. A Duchenne smile (enjoyment smile) is distinguished from a NonDuchenne (social smile) through the activation of the Orbicularis Oculi muscle around the eyes, which the former exhibits and the latter does not. The dataset consists of 3572 labels. The real worker may provide more than one label with opposite results to the same task. To evaluate the performance of these workers, these images were also annotated by two certified experts in the Facial Action Coding System. According to the expert labels, 58 out of 160 images contained Duchenne smiles.\nIn this experiment with labels from real MTurk workers, we also initialized the experiment by asking two people to annotate each task, which means there are 160\u00d72 =\n320(10%) labels before all strategies start running. Each strategy runs for 800 (22.7%) iterations. Figure 12 shows the training accuracy with different sampling strategies. Our proposed method rises quickly and converges to 75% training accuracy after soliciting 500 (14.0%) labels in total. Both traversal and random strategies converge to the same level with 1000 (28%) labels, which would result in twice the labeling cost of our proposed method.\nFigure 13 shows the training accuracy with different worker selection strategies. The experimental results indicate that the performance of -greedy selection algorithm which rises quickly and converges to 76% training accuracy after soliciting 200 (5.6%) labels in total is comparable good as the best worker strategy. However, although we run 10 times for each strategy, the performance of weighted selection strategy is very unstable with real crowdsourced annotations."}, {"heading": "6 Discussion", "text": "In real-world crowdsourcing applications, annotation accuracy and budget limitations are obviously the most important immediate criteria for evaluating the performance of a learning model. However, identifying knowledgeable and reliable workers is potentially useful because these workers could be employed in future annotation tasks. The difficulty of tasks mainly serves as a discriminant for distinguishing between good and bad labelers, rather than a evaluation score for how well the learning model performs. Especially in active learning, aggressive sampling is unable to perform well on all criteria so task difficulty should be sacrificed for performance gains on the other metrics.\nOur algorithm aggressively opts to use the best workers possible which yields labeling improvements but makes it difficult to accurately assess the relative rank of the\npoor labelers. We believe that a simple analysis of the rank correlation may not the best way to evaluate the estimation \u03b1\u2032 since in most real-world applications labelers under a certain performance level should be eliminated early. The pool of potential workers that can be reached using MTurk is very large so devoting annotation budget to working with poor labelers is unecessary. Our proposed method is good at dividing labelers into two groups (good and bad) which enables it to perform well in the first simulation\nexperiment while failing to make the subtle discriminations between relatively poor labelers required for assessing worker performance in the second experiment. However, in practical crowdsourcing tasks, filtering out bad labelers is enough for collecting reliable labels in future tasks, and the bottom ranked workers are largely irrelevant to the overall performance of the crowdsourcing pipeline."}, {"heading": "7 Conclusion", "text": "Although crowdsourcing annotations using active learning is an attractive and affordable idea for large-scale data labeling, the approach poses significant difficulties. Several studies in different research domains show that active learning approaches developed for noise-free annotations do not perform well with crowdsourced data. This pa-\nper presents a practical approach for using active learning in conjunction with Bayesian networks to model both the expertise of unknown labelers and the difficulty of annotation tasks.\nOur work makes two contributions that enable us to robustly train a probabilistic graphical model under these challenging conditions. First, we propose an original and efficient sampling criteria which iteratively assigns the most reliable labelers to the tasks with the highest labeling risk. Second, we present comprehensive evaluations on\nboth simulated and real-world datasets that show the strength of our proposed approach in significantly reducing the quantity of labels required for training the model. Our experiments using crowdsourced data from Mechanical Turk confirm that the proposed approach improves active learning in noisy real-world conditions."}], "references": [{"title": "Crowdsourcing systems on the world-wide web", "author": ["A. Doan", "R. Ramakrishnan", "A.Y. Halevy"], "venue": "Communications of the ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "A probabilistic framework to learn from multiple annotators with time-varying accuracy", "author": ["P. Donmez", "J.G. Carbonell", "J. Schneider"], "venue": "In SIAM International Conference on Data Mining (SDM),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "The rise of crowdsourcing", "author": ["J. Howe"], "venue": "Wired Magazine,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Algorithms for the multi-armed bandit problem", "author": ["Volodymyr Kuleshov", "Doina Precup"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Supervised learning from multiple experts: Whom to trust when everyone lies a bit", "author": ["V.C. Raykar", "S. Yu", "L.H. Zhao", "A. Jerebko", "C. Florin", "G.H. Valadez", "L. Bogoni", "L. Moy"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "Technical report, University of Wisconsin,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Get another label? improving data quality and data mining using multiple, noisy labelers", "author": ["V.S. Sheng", "F. Provost", "P.G. Ipeirotis"], "venue": "In Proceeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Active learning for parameter estimation in Bayesian networks", "author": ["S. Tong", "D. Koller"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Far-sighted active learning on a budget for image and video recognition", "author": ["S. Vijayanarasimhan", "P. Jain", "K. Grauman"], "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise", "author": ["J. Whitehill", "P. Ruvolo", "T. Wu", "J. Bergsma", "J. Movellan"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Active learning from crowds", "author": ["Y. Yan", "R. Rosales", "G. Fung", "J. Dy"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML), Bellevue, Washington,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Incremental relabeling for active learning with noisy crowdsourced annotations", "author": ["L. Zhao", "G. Sukthankar", "R. Sukthankar"], "venue": "In IEEE International Conference on Social Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Active learning from multiple noisy labelers with varied costs", "author": ["Y. Zheng", "S. Scott", "K. Deng"], "venue": "In IEEE International Conference on Data Mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Our work is motivated by the recent interest in the use of crowdsourcing [3] as a source of annotations from which to train machine learning systems.", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "This has stimulated the use of approaches such as active learning [6] that aim to minimize the amount of data required to learn a highquality classifier.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "Second, the quality of crowdsourced annotations has been found to be poor [7], with causes ranging from workers who overstate their qualifications, lack of motivation among labelers, haste and deliberate vandalism.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "Better results can be obtained by applying weighted voting which assigns different weights to labelers based on their previous performance [7, 12, 13].", "startOffset": 139, "endOffset": 150}, {"referenceID": 11, "context": "Better results can be obtained by applying weighted voting which assigns different weights to labelers based on their previous performance [7, 12, 13].", "startOffset": 139, "endOffset": 150}, {"referenceID": 12, "context": "Better results can be obtained by applying weighted voting which assigns different weights to labelers based on their previous performance [7, 12, 13].", "startOffset": 139, "endOffset": 150}, {"referenceID": 9, "context": "In this paper, we use the Generative model of Labels, Abilities, and Difficulties [10] in which labeler expertise and the task difficulty are simultaneously estimated using EM (ExpectationMaximization) to learn the parameters of a probabilistic graphical model which represents the relationship between labelers, tasks and annotation predictions.", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "[3] coined the phrase \u201ccrowdsourcing\u201d to describe the concept of outsourcing work to a cheap labor pool composed of everyday people who use their spare time to create content and solve problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] define crowdsourcing as enlisting a crowd of humans to help solve a problem defined by the system owners.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Crowdsourcing annotation services, such as Amazon\u2019s Mechanical Turk, have become an effective way to distribute annotation tasks over multiple workers [9]; however, Sheng et al.", "startOffset": 151, "endOffset": 154}, {"referenceID": 6, "context": "[7] noted the problem that crowdsourcing annotation tasks may generate unreliable labels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Several works [2, 11, 5] propose different approaches to model the annotation accuracy of workers; all of these approaches assume there are multiple experts/annotators providing labels but that no oracle exists.", "startOffset": 14, "endOffset": 24}, {"referenceID": 10, "context": "Several works [2, 11, 5] propose different approaches to model the annotation accuracy of workers; all of these approaches assume there are multiple experts/annotators providing labels but that no oracle exists.", "startOffset": 14, "endOffset": 24}, {"referenceID": 4, "context": "Several works [2, 11, 5] propose different approaches to model the annotation accuracy of workers; all of these approaches assume there are multiple experts/annotators providing labels but that no oracle exists.", "startOffset": 14, "endOffset": 24}, {"referenceID": 1, "context": "[2] propose a framework to learn the expected accuracy at each time step.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] focus on the multiple annotator scenario where multiple labelers with varying expertise are available for querying.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] use a probabilistic model both to evaluate the different experts and also to provide an estimate of the actual hidden labels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Our work is strongly influenced by GLAD [10], which uses a probabilistic model to simultaneously estimate the labels, the labeler expertise, and the task difficulty which are represented as latent variables in the Bayesian network.", "startOffset": 40, "endOffset": 44}, {"referenceID": 7, "context": "Tong and Koller[8] proposed a way to implement active learning in Bayesian networks with a simple structure.", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "An alternative worker selection strategy is the -greedy selection algorithm, which has been used in studying the exploration-exploitation tradeoff in reinforcement learning [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 9, "context": "[10] while attempting to reduce the labeling budget required to reach the desired accuracy level using three different active learning strategies:", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] asked 20 real human workers on Mechanical Turk to annotate 160 facial images by labeling them as either Duchenne or Non-Duchenne.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Crowdsourcing platforms offer a practical solution to the problem of affordably annotating large datasets for training supervised classifiers. Unfortunately, poor worker performance frequently threatens to compromise annotation reliability, and requesting multiple labels for every instance can lead to large cost increases without guaranteeing good results. Minimizing the required training samples using an active learning selection procedure reduces the labeling requirement but can jeopardize classifier training by focusing on erroneous annotations. This paper presents an active learning approach in which worker performance, task difficulty, and annotation reliability are jointly estimated and used to compute the risk function guiding the sample selection procedure. We demonstrate that the proposed approach, which employs active learning with Bayesian networks, significantly improves training accuracy and correctly ranks the expertise of unknown labelers in the presence of annotation noise.", "creator": "LaTeX with hyperref package"}}}