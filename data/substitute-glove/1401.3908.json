{"id": "1401.3908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity", "abstract": "In automatic summarization, centrality - other - clarity instance that the less critical create an rather analysis source, own a artist according identification suggesting, geographical once part all northern verses, changes a representation now describe notion makes peculiar (graph, variation, incl. ). We assess put main paradigms, when requiring a move spacial - it aspects model addition assault fuzing that system leaving through makes of policy pieces one better estimated the purposes content. Geometric explore is addition well compute predictive relatedness. Centrality (usefulness) kind determined supported giving the turn static problem (many nor only several contents ), and by taking break costs the existence to minor topics or semicircular mathematics now called information sources to appear posited. The procedure consists place much, for or trip several the signals example, a establishment set and/or only of the seen semantically domestic poems. Then, the determination of the most fully content much accomplishment by accomplish the passages question occurrence during now regional few of backing match. This technology distribution veidekke all-pro that are menu, and language - and code - member. Thorough steering evaluation new that main method implies state - country - form - fine performance, both same released text, along requiring sampled letter summarization, several when compared they albeit more area methods.", "histories": [["v1", "Thu, 16 Jan 2014 05:23:22 GMT  (736kb)", "http://arxiv.org/abs/1401.3908v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["ricardo ribeiro", "david martins de matos"], "accepted": false, "id": "1401.3908"}, "pdf": {"name": "1401.3908.pdf", "metadata": {"source": "CRF", "title": "Revisiting Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity", "authors": ["Ricardo Ribeiro", "David Martins de Matos"], "emails": ["ricardo.ribeiro@inesc-id.pt", "david.matos@inesc-id.pt"], "sections": [{"heading": "1. Introduction", "text": "A summary conveys to the end user the most relevant content of one or more information sources, in a concise and comprehensible manner. Several difficulties arise when addressing this problem, but one of utmost importance is how to assess the significant content. Usually, approaches vary in complexity if processing text or speech. While in text summarization, up-to-date systems make use of complex information, such as syntactic (Vanderwende, Suzuki, Brockett, & Nenkova, 2007), semantic (Tucker & Spa\u0308rck Jones, 2005), and discourse information (Harabagiu & Lacatusu, 2005; Uze\u0302da, Pardo, & Nunes, 2010), either to assess relevance or reduce the length of the output, common approaches to speech summarization try to cope with speech-related issues by using speech-specific information (for example, prosodic features, Maskey & Hirschberg, 2005, or recognition confidence scores, Zechner & Waibel, 2000) or by improving the intelligibility of the output of an automatic speech\nc\u00a92011 AI Access Foundation. All rights reserved.\nrecognition system (by using related information, Ribeiro & de Matos, 2008a). In fact, spoken language summarization is often considered a much harder task than text summarization (McKeown, Hirschberg, Galley, & Maskey, 2005; Furui, 2007): problems like speech recognition errors, disfluencies, and the accurate identification of sentence boundaries not only increase the difficulty in determining the salient information, but also constrain the applicability of text summarization techniques to speech summarization (although in the presence of planned speech, as it partly happens in the broadcast news domain, that portability is more feasible, Christensen, Gotoh, Kolluru, & Renals, 2003). Nonetheless, shallow text summarization approaches such as Latent Semantic Analysis (LSA) (Landauer, Foltz, & Laham, 1998; Gong & Liu, 2001) and Maximal Marginal Relevance (MMR) (Carbonell & Goldstein, 1998) seem to achieve performances comparable to the ones using specific speech-related features (Penn & Zhu, 2008).\nFollowing the determination of the relevant content, the summary must be composed and presented to the user. If the identified content consists of passages found in the input source that are glued together to form the summary, that summary is usually designated as extract ; on the other hand, when the important content is devised as a series of concepts that are fused into a smaller set and then used to generate a new, concise, and informative text, we are in the presence of an abstract. In between extraction and concept-to-text generation, especially in text summarization, text-to-text generation methods, which rely on text rewriting\u2014paraphrasing\u2014, of which sentence compression is a major representative, are becoming an up-to-date subject (Cohn & Lapata, 2009). Given the hardness of abstraction, the bulk of the work in the area consists of extractive summarization.\nA common family of approaches to the identification of the relevant content is the centrality family. These methods base the detection of the most salient passages on the identification of the central passages of the input source(s). One of the main representatives of this family is centroid-based summarization. Centroid-based methods build on the idea of a pseudo-passage that represents the central topic of the input source (the centroid) selecting as passages (x) to be included in the summary the ones that are close to the centroid. Pioneer work (on multi-document summarization) by Radev, Hatzivassiloglou, and McKeown (1999) and Radev, Jing, and Budzikowska (2000) creates clusters of documents by representing each document as a tf-idf vector; the centroid of each cluster is also defined as a tf-idf vector, with the coordinates corresponding to the weighted average of the tf-idf values of the documents of the cluster; finally, sentences that contain the words of the centroids are presumably the best representatives of the topic of the cluster, thus being the best candidates to belonging to the summary.\ncentrality(x) = similarity(x, centroid) (1)\nAnother approach to centrality estimation is to compare each candidate passage to every other passage (y) and select the ones with higher scores (the ones that are closer to every other passage). One simple way to do this is to represent passages as vectors using a weighting scheme like the aforementioned tf-idf ; then, passage similarity can be assessed using, for instance, the cosine, assigning to each passage a centrality score as defined in Eq. 2.\ncentrality(x) = 1\nN \u2211 y similarity(x, y) (2)\nThese scores are then used to create a sentence ranking: sentences with highest scores are selected to create the summary.\nA major problem of this relevance paradigm is that by taking into account the entire input source in this manner, either to estimate centroids or average distances of input source passages, we may be selecting extracts that being central to the input source are, however, not the most relevant ones. In cognitive terms, the information reduction techniques in the summarization process are quite close to the discourse understanding process (EndresNiggemeyer, 1998), which, at a certain level, works by applying rules that help uncovering the macrostructure of the discourse. One of these rules, deletion, is used to eliminate from the understanding process propositions that are not relevant to the interpretation of the subsequent ones. This means that it is common to find, in the input sources to be summarized, lateral issues or considerations that are not relevant to devise the salient information (discourse structure-based summarization is based on the relevance of nuclear text segments, Marcu, 2000; Uze\u0302da et al., 2010), and that may affect centrality-based summarization methods by inducing inadequate centroids or decreasing the scores of more suitable sentences.\nAs argued by previous work (Gong & Liu, 2001; Steyvers & Griffiths, 2007), we also assume that input sources are mixtures of topics, and propose to address that aspect using the input source itself as guidance. By associating to each passage of the input source a support set consisting only of the most semantically related passages in the same input source, groups of related passages are uncovered, each one constituting a latent topic (the union of the supports sets whose intersection is not empty). In the creation of these support sets, semantic relatedness is assessed by geometric proximity. Moreover, while similar work usually explores different weighting schemes to address specific issues of the task under research (Ora\u0306san, Pekar, & Hasler, 2004; Murray & Renals, 2007; Ribeiro & de Matos, 2008b), we explore different geometric distances as similarity measures, analyzing their performance in context (the impact of different metrics from both theoretical and empirical perspectives in a clustering setting was shown in Aggarwal, Hinneburg, & Keim, 2001). To build the summary, we select the sentences that occur in the largest number of support sets\u2014 hence, the most central sentences, without the problem that affects previous centrality-based summarization.\nOur method produces generic, language- and domain-independent summaries, with low computational requirements. We test our approach both in speech and text data. In the empirical evaluation of the model over text data, we used an experimental setup previously used in published work (Mihalcea & Tarau, 2005; Antiqueira, Oliveira Jr., da Fontoura Costa, & Nunes, 2009), which enabled an informative comparison to the existing approaches. In what concerns the speech experiments, we also used a corpus collected in previous work (Ribeiro & de Matos, 2008a), as well as the published results. This allowed us to compare our model to state-of-the-art work.\nThe rest of this document is structured as follows: in Section 2, we analyze representative models of both centrality-as-relevance approaches\u2014passage-to-centroid similarity-based centrality and pair-wise passage similarity-based centrality; Section 3 describes the support sets-based relevance model; the evaluation of the model is presented in Section 4, where we compare its performance against other centrality-as-relevance models and discuss the achieved results; final remarks conclude the document."}, {"heading": "2. Centrality-as-Relevance", "text": "There are two main approaches to centrality-based summarization: passage-to-centroid similarity and pair-wise passage similarity."}, {"heading": "2.1 Passage-to-Centroid Similarity-based Centrality", "text": "In centroid-based summarization, passage centrality is defined by the similarity between the passage and a pseudo-passage that, considering a geometrical representation of the input source, is the center of the space defined by the passages of the input source, the centroid. The work in multi-document summarization by Radev et al. (1999, 2000) and Radev, Jing, Stys\u0301, and Tam (2004) and the work developed by Lin and Hovy (2000) are examples of this approach.\nRadev et al. present a centroid-based multi-document summarizer (MEAD) that has as input a cluster of documents. Associated to each cluster of documents is a centroid. Documents are represented by vectors of tf-idf weights and the centroid of each cluster consists of a vector which coordinates are the weighted averages of the tf-idf values of the documents of the cluster, above a pre-defined threshold. Thus, the centroid of a cluster of documents is, in this case, a pseudo-document composed by the terms that are statistically relevant. Given a cluster of documents segmented into sentences IS , {s1, s2, . . . , sN}, a centroid C, and a compression rate, summarization is done by selecting the appropriate number (according to the compression rate) of the sentences with the highest scores assigned by a linear function (Eq. 3) of the following features: centroid value (Ci), position value (Pi), and first-sentence overlap value (Fi).\nscore(si) = wcCi + wpPi + wfFi, 1 \u2264 i \u2264 N (3)\nThe centroid value, defined as Ci = \u2211\nt\u2208si Ct,i, establishes that the sentences closer to the centroid (the ones that contain more terms t from the centroid) have higher scores. Position value (Pi) scores sentences according to their position in the encompassing document. Finally, first-sentence overlap value (Fi) scores sentences according to their similarity to the first sentence of the document.\nLin and Hovy (2000) designate the centroid as topic signature and define it as a set of related terms: TS , {topic,< (t1, w1), . . . , (tT , wT ) >}, where ti represents a term related to the topic topic and wi is an associated weight that represents the degree of correlation of ti to the topic. Topic signatures are computed from a corpus of documents, previously classified as relevant or non-relevant for a given topic, using the log-likelihood-ratio-based quantity \u22122log(\u03bb). This quantity, due to its asymptotic relation to the \u03c72 distribution as well as the adequacy of the log-likelihood-ratio to sparse data, is used to rank the terms that will define the signature, and to select a cut-off value that will establish the number of terms in the signature. Summarization is carried out by ranking the sentences according to the topic signature score and selecting the top ranked ones. The topic signature score (tss) is computed in a similar manner to MEAD\u2019s centroid value: given an input source IS , {p1, p2, . . . , pN}, where pi , \u3008t1, . . . , tM \u3009, the most relevant passages are the ones with more words from the topic (Eq. 4).\ntss(pi) = M\u2211 j=1 wj ,with wj weight of tj as defined in a topic signature (4)"}, {"heading": "2.2 Pair-wise Passage Similarity-based Centrality", "text": "In pair-wise passage similarity-based summarization, passage centrality is defined by the similarity between each passage and every other passage. The work presented by Erkan and Radev (2004), as well as the work developed by Mihalcea and Tarau (2005), are examples of this approach.\nErkan and Radev (2004) propose three graph-based approaches to pair-wise passage similarity-based summarization with similar performance: degree centrality, LexRank, and continuous LexRank. Degree centrality is based on the degree of a vertex. Pair-wise sentence similarity is used to build a graph representation of the input source: vertices are sentences and edges connect vertices which corresponding sentences are similar above a given threshold. Sentences similar to a large number of other sentences are considered the most central (relevant) ones. Degree centrality is similar to the model we propose. However, in the model we propose, we introduce the concept of support set to allow the use of a different threshold for each sentence. This improves the representation of each sentence, leading to the creation of better summaries.\nLexRank, based on Google\u2019s PageRank (Brin & Page, 1998), builds on degree centrality (degree) by making the centrality of a sentence s be influenced by similar sentences, the adjacent ones in the graph representation (Eq. 5).\ncentralityScore(s) = \u2211\nt\u2208adj[s]\ncentralityScore(t)\ndegree(t) (5)\nThe ranking model is similar to PageRank except in what concerns the similarity (adjacency) graph, that, in this case, is undirected (Eq. 6, d is a damping factor and N the number of sentences).\ncentrality(s) = d\nN + (1\u2212 d) \u2211 t\u2208adj[s] centrality(t) degree(t) (6)\nContinuous LexRank is a weighted version of LexRank (it uses Eq. 7 instead of Eq. 5).\ncentralityScore(s) = \u2211\nt\u2208adj[s]\nsim(s, t)\u2211 u\u2208adj[t] sim(u, t) centralityScore(t) (7)\nMihalcea and Tarau (2005), in addition to Google\u2019s PageRank, also explore the HITS algorithm (Kleinberg, 1999) to perform graph-based extractive text summarization: again, documents are represented as networks of sentences and these networks are used to globally determine the importance of each sentence. As it happens in the models proposed by Erkan and Radev, sentences are vertices (V ) and edges (w) between vertices are established by passage similarity. The TextRank (Mihalcea & Tarau, 2004)\u2014how the model based on PageRank was designated and the main contribution\u2014formalization is similar to Continuous LexRank (see Eq. 8), although Mihalcea and Tarau also explore directed graphs in the\nrepresentation of the text12. For summarization, the best results were obtained using a backward directed graph: the orientation of the edges from a vertex representing a sentence is to vertices representing previous sentences in the input source.\nTextRank(Vs) = (1\u2212 d) + d \u2217 \u2211\nVt\u2208In[Vs]\nw(Vt, Vs)\u2211 Vu\u2208Out[Vt]w(Vt, Vu) TextRank(Vt) (8)\nPassage similarity is based on content overlap3 and is defined in Eq. 9. Given two sets P , p1, p2, ..., pn and Q , q1, q2, ..., qn, each corresponding to a passage, similarity consists in the cardinality of the intersection over the sum of the logarithms of the cardinality of each set.\nw(VP , VQ) = sim(P,Q) = |{t : t \u2208 P \u2227 t \u2208 Q}| log(|P |) + log(|Q|)\n(9)\nA similar graph-based approach is described by Antiqueira et al. (2009). This work uses complex networks to perform extractive text summarization. Documents are also represented as networks, where the sentences are the nodes and the connections between nodes are established between sentences sharing common meaningful nouns."}, {"heading": "2.3 Beyond Automatic Summarization", "text": "Apart from summarization, and considering that PageRank and HITS stem from the area of Information Retrieval, centrality-based methods similar to the ones previously described have been successfully applied to re-rank sets of documents returned by retrieval methods.\nKurland and Lee (2005, 2010) present a set of graph-based algorithms, named influx, that are similar to our model, to reorder a previously retrieved collection of documents (C). The method starts by defining a k -nearest-neighbor (kNN) graph over the initial collection based on generation links defined as in Eq. 10 (KL, Kullback-Leibler divergence; MLE, maximum-likelihood estimate; \u00b5, smoothing-parameter of a Dirichlet-smoothed version of p\u0303(\u00b7); d and s, documents).\npKL,\u00b5d (s) , exp ( \u2212KL ( p\u0303MLEs (\u00b7) \u2223\u2223\u2223\u2223\u2223\u2223 p\u0303[\u00b5]d (\u00b7))) (10) Centrality is determined as defined in Eq. 11. Edges can be weighted (weight given by pKL,\u00b5d (s)) or not (weight is 1). Edges corresponding to generation probabilities below the k highest ones are not considered.\ncentralityScore(d) , \u2211 o\u2208C wt(o\u2192 d) (11)\n1. In \u201cA Language Independent Algorithm for Single and Multiple Document Summarization\u201d (Mihalcea & Tarau, 2005), the weighted PageRank equation has a minor difference from the one in \u201cTextRank: Bringing Order into Texts\u201d (Mihalcea & Tarau, 2004). The latter presents the correct equation. 2. Although both LexRank and TextRank are based on PageRank, different equations are used in their formalization. The equation used in TextRank formalization is the same of PageRank original publication, however PageRank authors observe that the PageRanks form a probability distribution over Web pages, so the sum of all Web pages\u2019 PageRanks will be one. This indicates the need of the normalization factor that is observed in LexRank formalization and currently assumed to be the correct PageRank formalization. 3. The metric proposed by Mihalcea and Tarau (2004) has an unresolved issue: the denominator is 0 when comparing two equal sentences with length one (something that can happen when processing speech transcriptions). Instead, the Jaccard similarity coefficient (1901) could be used.\nThere are also recursive versions of this centrality model, which are similar to PageRank/LexRank and Continuous LexRank."}, {"heading": "3. Support Sets and Geometric Proximity", "text": "In this work, we hypothesize that input sources to be summarized comprehend different topics (lateral issues beyond the main topic), and model this idea by defining a support set, based on semantic relatedness, for every passage in the input source. Semantic relatedness is estimated within the geometric framework, where we explore several distance metrics to compute proximity. The most relevant content is determined by computing the most central passages given the collection of support sets. The proposed model estimates the most salient passages of an input source, based exclusively on information drawn from the used input source."}, {"heading": "3.1 Model", "text": "The leading concept in our model is the concept of support set: the first step of our method to assess the relevant content is to create a support set for each passage of the input source by computing the similarity between each passage and the remaining ones, selecting the closest passages to belong to the support set. The most relevant passages are the ones that occur in the largest number of support sets.\nGiven a segmented information source I , p1, p2, ..., pN , support sets Si associated with each passage pi are defined as indicated in Eq. 12 (sim() is a similarity function, and \u03b5i is a threshold).\nSi , {s \u2208 I : sim(s, pi) > \u03b5i \u2227 s 6= pi} (12)\nThe most relevant segments are given by selecting the passages that satisfy Eq. 13.\narg max s\u2208\u222ani=1Si \u2223\u2223{Si : s \u2208 Si}\u2223\u2223 (13) A major difference from previous centrality models and the main reason to introduce the support sets is that by allowing different thresholds to each set (\u03b5i), we let centrality be influenced by the latent topics that emerge from the groups of related passages. In the degenerate case where all \u03b5i are equal, we fall into the degree centrality model proposed by Erkan and Radev (2004). But using, for instance, a na\u0308\u0131ve approach of having dynamic thresholds (\u03b5i) set by limiting the cardinality of the support sets (a kNN approach), centrality is changed because each support set has only the most semantically related passages of each passage. From a graph theory perspective, this means that the underlying representation is not undirected, and the support set can be interpreted as the passages recommended by the passage associated to the support set. This contrasts with both LexRank models, which are based on undirected graphs. On the other hand, the models proposed by Mihalcea and Tarau (2005) are closer to our work in the sense that they explore directed graphs, although only in a simple way (graphs can only be directed forward or backward). Nonetheless, semantic relatedness (content overlap) and centrality assessment (performed by the graph ranking algorithms HITS and PageRank) is quite different from our proposal. In what concerns the work of Kurland and Lee (2005, 2010), which considering this kNN\napproach to the definition of the support set size, is the most similar to our ideias, although not addressing automatic summarization, the neighborhood definition strategy is different than ours: Kurland and Lee base neighborhood definition on generation probabilities (Eq. 10), while we explore geometric proximity. Nevertheless, from the perspective of our model, the kNN approach to support set definition is only a possible strategy (others can be used): our model can be seen as a generalization of both kNN and \u03b5NN approaches, since what we propose is the use of differentiated thresholds (\u03b5i) for each support set (Eq. 12)."}, {"heading": "3.2 Semantic Space", "text": "We represent the input source I in a term by passages matrix A, where each matrix element aij = f(ti, pj) is a function that relates the occurrences of each term ti within each passage pj (T is the number of different terms; N is the number of passages).\nA = a1,1 . . . a1,N. . . aT,1 . . . aT,N  (14) In what concerns the definition of the weighting function f(ti, pj), several term weighting schemes have been explored in the literature\u2014for the analysis of the impact of different weighting schemes on either text or speech summarization see the work of Ora\u0306san et al. (2004), and Murray and Renals (2007) or Ribeiro and de Matos (2008b), respectively. Since the exact nature of the weighting function, although relevant, is not central to our work, we opted for normalized frequency for simplicity, as defined in Eq. 15, where ni,j is the number of occurrences of term ti in passage pj .\nf(ti, pj) = tfi = ni,j\u2211 k nk,j\n(15)\nNevertheless, this is in line with the work of Sahlgren (2006) that shows that in several tasks concerning term semantic relatedness, one of the most effective weighting schemes for small contexts is the binary term weighting scheme (Eq. 16), alongside raw or dampened counts, that is, weighting schemes, based on the frequency, that do not use global weights (note also that in such small contexts, most of the words have frequency 1, which normalized or not is similar to the binary weighting scheme).\nf(ti, pj) = { 1 if ti \u2208 pj 0 if ti /\u2208 pj\n(16)"}, {"heading": "3.3 Semantic Relatedness", "text": "As indicated by Sahlgren (2006), the meanings-are-locations metaphor is completely vacuous without the similarity-is-proximity metaphor. In that sense, we explore the prevalent distance measures found in the literature, based on the general Minkowski distance (Eq. 17).\ndistminkowski(x,y) = ( n\u2211 i=1 |xi \u2212 yi|N ) 1 N (17)\nSemantic relatedness is computed using the Manhattan distance (N = 1, Eq. 19), the Euclidean distance (N = 2, Eq. 20), the Chebyshev distance (N \u2192 \u221e, Eq. 21), and fractional distance metrics (we experimented with N = 0.1, N = 0.5, N = 0.75, and N = 1.(3). Note that, when 0 < N < 1, Eq. 17 does not represent a metric, since the triangle inequality does not hold (Koosis, 1998, page 70). In this case, it is common to use the variation defined in Eq. 18.\ndistN (x,y) = n\u2211 i=1 |xi \u2212 yi|N , 0 < N < 1 (18)\nMoreover, we also experiment with the general Minkowski equation, using the tuple dimension as N .\ndistmanhattan(x,y) = n\u2211 i=1 |xi \u2212 yi| (19)\ndisteuclidean(x,y) = \u221a\u221a\u221a\u221a n\u2211 i=1 (xi \u2212 yi)2 (20)\ndistchebyshev(x,y) = lim N\u2192\u221e ( n\u2211 i=1 |xi \u2212 yi|N ) 1 N = max i (|xi \u2212 yi|) (21)\nThe cosine similarity (Eq. 22), since it is one of the most used similarity metrics, especially when using spatial metaphors for computing semantic relatedness, was also part of our experiments.\nsimcos(x,y) = x \u00b7 y \u2016x\u2016\u2016y\u2016 = \u2211n i=1 xiyi\u221a\u2211n\ni=1 x 2 i \u221a\u2211n i=1 y 2 i\n(22)\nGrounding semantic relatedness on geometric proximity enables a solid analysis of the various similarity metrics. For instance, when using the Euclidean distance (Eq. 20), differences between tuple coordinate values less than 1 make passages closer, while values greater than 1 make passages more distant; Chebyshev\u2019s distance (Eq. 21) only takes into account one coordinate: the one with the greatest difference between the two passages; and, the Manhattan distance (Eq. 19) considers all coordinates evenly. In the cosine similarity (Eq. 22), tuples representing passages are vectors and the angle they form establishes their relatedness. In contrast, Mihalcea and Tarau (2005) and Antiqueira et al. (2009) define passage similarity as content overlap. Figure 1 (N ranges from 0.1, with an almost imperceptible graphical representation, to N \u2192 \u221e, a square) shows how the unit circle is affected by the several geometric distances (Manhattan, N = 1, and Euclidean, N = 2, are highlighted).\nAlthough geometric proximity enables a solid analysis of the effects of using a specific metric, it mainly relies on lexical overlap. Other metrics could be used, although the costs in terms of the required resources would increase. Examples are corpus-based vector space models of semantics (Turney & Pantel, 2010), like LSA (Landauer et al., 1998), Hyperspace Analogue to Language (Lund, Burgess, & Atchley, 1995), or Random Indexing (Kanerva, Kristoferson, & Holst, 2000; Kanerva & Sahlgren, 2001), or similarity metrics based on knowledge-rich semantic resources, such as WordNet (Fellbaum, 1998)."}, {"heading": "3.4 Threshold Estimation", "text": "As previously mentioned, a simple approach to threshold estimation is to define a fixed cardinality for all support sets, a kNN approach. This means that thresholds, although unknown, are different for each support set.\nA simple heuristic that allows to automatically set per passage thresholds is to select as members of the support set the passages which distance to the passage associated to the support set under construction is smaller than the average distance. In the next sections, we explore several heuristics inspired by the nature of the problem that can be used as possibly better approaches to threshold estimation. However, this subject merits further study."}, {"heading": "3.4.1 Heuristics Based on Distance Progression Analysis", "text": "One possible approach is to analyze the progression of the distance values between each passage and the remaining ones in the creation of the respective support set. This type of heuristics uses a sorted permutation, di1 \u2264 di2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 diN\u22121, of the distances of the passages, sk, to the passage pi (corresponding to the support set under construction), with dik = dist(sk, pi), 1 \u2264 k \u2264 N \u2212 1, and N the number of passages.\nWe explore three approaches: a standard deviation-based approach, where \u03b5i is given by Eq. 23, with \u03b1 a parameter that controls the width of interval around the average distance in relation to the standard deviation; an approach based on the diminishing differences between consecutive distances, dik+2 \u2212 dik+1 < dik+1 \u2212 dik, 1 \u2264 k \u2264 N \u2212 3, where \u03b5i = dik+2, such that k is the largest one that \u22001\u2264j\u2264k+1j : dij+2\u2212dij+1 < dij+1\u2212dij ; and, an approach based on the\naverage difference between consecutive distances, dik+1\u2212dik < \u2211N\u22122 l=1 (d i l+1\u2212d i l) N\u22122 , 1 \u2264 k \u2264 N\u22122,\nwhere \u03b5i = dik+1 , such that k is the largest one that \u22001\u2264j\u2264kj : dij+1 \u2212 dij < \u2211N\u22122 l=1 (d i l+1\u2212d i l) N\u22122 .\n\u03b5i = \u00b5i \u2212 \u03b1\u03c3i, with (23)\n\u00b5i = 1\nN \u2212 1 N\u22121\u2211 k=1 dik, and \u03c3i = \u221a\u221a\u221a\u221a 1 N \u2212 1 N\u22121\u2211 k=1 (dik \u2212 \u00b5i)2"}, {"heading": "3.4.2 Heuristics Based on Passage Order", "text": "The estimation of specific thresholds aims at defining support sets containing the most important passages to the passage under analysis. In that sense, in this set of heuristics we explore the structure of the input source to partition the candidate passages to be in the support set in two subsets: the ones closer to the passage associated with the support set under construction, and the ones further appart.\nThese heuristics use a permutation, di1, d i 2, \u00b7 \u00b7 \u00b7 , diN\u22121, of the distances of the passages, sk, to the passage, pi, related to the support set under construction, with d i k = dist(sk, pi), 1 \u2264 k \u2264 N \u2212 1, corresponding to the order of occurrence of passages sk in the input source. Algorithm 1 describes the generic procedure."}, {"heading": "3.4.3 Heuristics Based on Weighted Graph Creation Techniques", "text": "There are several ways to define a weighted graph, given a dataset. The main ideia is that similar nodes must be connected by an edge with a large weight. In this set of heuristics, we explore two weight functions (Zhu, 2005) (Eqs. 24 and 25) considering that if the returned value is above a given threshold, \u03b4, the passage sk belongs to the support set of passage pi, with dik = dist(sk, pi).\nexp(\u2212(dik \u2212 min 1\u2264j\u2264N\u22121 (dij)) 2/\u03b12) > \u03b4 (24)\n( tanh(\u2212\u03b1(dik \u2212\n1\nN \u2212 1 N\u22121\u2211 j=1 dij)) + 1 ) /2 > \u03b4 (25)"}, {"heading": "3.5 Integrating Additional Information", "text": "As argued by Wan, Yang, and Xiao (2007) and Ribeiro and de Matos (2008a), the use of additional related information helps to build a better understanding of a given subject, thus improving summarization performance. Wan et al. propose a graph-based ranking model that uses several documents about a given topic to summarize a single one of them. Ribeiro and de Matos, using the LSA framework, present a method that combines the input source\nInput: Two values r1 and r2, each a representative of a subset, and the set of the passages sk and corresponding distances d i k to the passage associated with the support set under construction Output: The support set of the passage under analysis\nR1 \u2190 \u2205, R2 \u2190 \u2205; for k \u2190 1 to N \u2212 1 do\nif |r1 \u2212 dik | < |r2 \u2212 dik | then r1 \u2190 (r1 + dik)/2; R1 \u2190 R1 \u222a {sik}; else r2 \u2190 (r2 + dik)/2; R2 \u2190 R2 \u222a {sik}; end\nend l\u2190 arg min1\u2264k\u2264N\u22121(dik); if sl \u2208 R1 then\nreturn R1; else\nreturn R2; end\nAlgorithm 1: Generic passage order-based heuristic.\nconsisting of a spoken document, with related textual background information, to cope with the difficulties of speech-to-text summarization.\nThe model we propose may be easily expanded to integrate additional information. By using both an information source I , p1, p2, ..., pN and a source for additional relevant information B, we may redefine Eq. 12 as shown in Eq. 26 to integrate the additional information.\nSi , {s \u2208 I \u222aB : sim(s, pi) > \u03b5i \u2227 s 6= pi} (26)\nMatrix A (from Eq. 14) should be redefined as indicated in Eq. 27, where aidkj represents the weight of term ti, 1 \u2264 i \u2264 T (T is the number terms), in passage pdkj , 1 \u2264 k \u2264 D (D is the number of documents used as additional information) with dk1 \u2264 dkj \u2264 dks , of document dk; and ainl , 1 \u2264 l \u2264 s, are the elements associated with the input source to be summarized.\nA =  a1d11 ... a1d1s ... a1dD1 ... a1dDs a1n1 ... a1ns... aTd11 ... aTd1s ... aTdD1 ... aTdDs aTn1 ... aTns  (27) Given the new definition of support set and a common representation for the additional information, the most relevant content is still assessed using Eq. 13.\nThe same line of thought can be applied to extend the model to multi-document summarization."}, {"heading": "4. Evaluation", "text": "Summary evaluation is a research subject by itself. Several evaluation models have been put forward in the last decade: beyond the long-established precision and recall (mostly useful when evaluating extractive summarization using also extractive summaries as models), literature is filled with metrics (some are automatic, others manual) like Relative utility (Radev et al., 2000; Radev & Tam, 2003), SummACCY (Hori, Hori, & Furui, 2003), ROUGE (Lin, 2004), VERT (de Oliveira, Torrens, Cidral, Schossland, & Bittencourt, 2008), or the Pyramid method (Nenkova, Passonneau, & McKeown, 2007). For a more comprehensive analysis of the evaluation field see the work by Nenkova (2006) and Nenkova et al. (2007).\nDespite the number of approaches to summary evaluation, the most widely used metric is still ROUGE and is the one we use in our study. We chose ROUGE not only owing to its wide adoption, but also because one of the data sets used in our evaluation has been used in published studies, allowing us to easily compare the performance of our model with other known systems.\nROUGE-N =\n\u2211 S\u2208{Reference Summaries} \u2211 gramN\u2208S countmatch(gramN )\u2211\nS\u2208{Reference Summaries} \u2211 gramN\u2208S count(gramN ) (28)\nNamely, we use the ROUGE-1 score, known to correlate well with human judgment (Lin, 2004). ROUGE-N is defined in Eq. 28. Moreover, we estimate confidence intervals using non-parametric bootstrap with 1000 resamplings (Mooney & Duval, 1993).\nSince we are proposing a generic summarization model, we conducted experiments both in text and speech data."}, {"heading": "4.1 Experiment 1: Text", "text": "In this section, we describe the experiments performed and analyze the corresponding results when using as input source written text."}, {"heading": "4.1.1 Data", "text": "The used corpus, known as TeMa\u0301rio, consists of 100 newspaper articles in Brazilian Portuguese (Pardo & Rino, 2003). Although our model is general and language-independent, this corpus was used in several published studies, allowing us to perform an informed comparison of our results. The articles in the corpus cover several domains, such as \u201cworld\u201d, \u201cpolitics\u201d, and \u201cforeign affairs\u201d. For each of the 100 newspaper articles, there is a reference human-produced summary. The text was tokenized and punctuation removed, maintaining sentence boundary information. Table 1 sumarizes the properties of this data set."}, {"heading": "4.1.2 Evaluation Setup", "text": "To compare the performance of our model when the input is not affected by speech-related phenomena, we use previously published state-of-the-art results for text summarization. However, since there was no information available about any kind of preprocessing for the previous studies, we could not guarantee a fair comparison of our results with the previous ones, without the definition of an adequate methodology for the comparisons.\nThe following systems were evaluated using the TeMa\u0301rio dataset:\n\u2022 a set of graph-based summarizers presented by Mihalcea and Tarau (2005), namely PageRank Backward, HITSA Backward and HITSH Forward;\n\u2022 SuPor-v2 (Leite, Rino, Pardo, & Nunes, 2007), a classifier-based system that uses features like the occurrence of proper nouns, lexical chaining, and an ontology;\n\u2022 two modified versions of Mihalcea\u2019s PageRank Undirected, called TextRank + Thesaurus and TextRank + Stem + StopwordsRem(oval) presented by Leite et al. (2007); and,\n\u2022 several complex networks summarizers proposed by Antiqueira et al. (2009).\nConsidering the preprocessing step we applied to the corpus and the observed differences in the published results, we found it important to evaluate the systems under the same conditions. Thus, we implemented the following centrality models:\n\u2022 Uniform Influx (corresponds to the non-recursive, unweighted version of the model), proposed by Kurland and Lee (2005, 2010) for re-ranking in document retrieval (we experimented with several k in graph definition, the sames used for support set cardinality in the kNN strategy, and \u00b5\u201410, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000\u2014, and present only the best results);\n\u2022 PageRank, proposed by both Mihalcea and Tarau (2004, 2005) and Erkan and Radev (2004) (passage similarity metrics differ and Mihalcea and Tarau also explore directed graphs);\n\u2022 Degree centrality as proposed by Erkan and Radev (2004) (we experimented with several thresholds \u03b4, ranging from 0.01 to 0.09, and show only the best results); and,\n\u2022 Baseline, in which the ranking is defined by the order of the sentences in the news article, with relevance decreasing from the begining to the end.\nTable 2 further discriminates PageRank-based models. PageRank over a directed forward graph performs consistently worse (Mihalcea & Tarau, 2005) than over undirected and directed backward graphs, and it was not included in our trials. Degree and Continuous LexRank bound the performance of the LexRank model, and are the ones we use in this\nevaluation. Moreover, to assess the influence of the similarity metrics in these graphbased centrality models, we tested the best-performing metric of our model, the Manhattan distance, with the PageRank model. Additionally, given that the models proposed by Erkan and Radev (2004) use idf, we present some results (clearly identified) using both weighting schemes: using and not using idf.\nConcerning summary size, the number of words in the generated summaries directly depends on the number of words of the reference abstracts, which consisted in compressing the input sources to 25-30% of the original size."}, {"heading": "4.1.3 Results", "text": "Table 3 illustrates the comparison between the previously proposed models and our model. In this table, our model is identified in boldface by the distance name, and the conditions used by that particular instance. Every time the best performance is achieved by an instance using supports sets whose cardinality is specified in absolute terms (1\u20135), we also present the best performance using support sets whose cardinality is specified in relative terms (10%\u201390% of the input source). For the fractional metrics, we also present the value of N in Eq. 17, if N \u2265 1, or Eq. 18, if 0 < N < 1. For the automatically set thresholds, we identify which heuristic produced the best results using the following notation: H0 means the heuristic based on the average distance; H1 means heuristics based on the analysis of the distances progression, with H1.1 corresponding to the one based on the standard deviation, H1.2 corresponding to the one based on the diminishing differences between consecutive distances, and H1.3 corresponding to the one based on the average difference between consecutive distances; H2 means heuristics based on passage order, with H2.1 using as r1 the minimum distance, and as r2 the average of the distances, H2.2 using as r1 the minimum distance, and as r2 the maximum distance, and H2.3, using as r1 the distance of the first passage and r2 the distance of the second passage, according to the required permutation defined in Section 3.4.2; H3 means heuristics based on weighted graph creation techniques, with H3.1 based on Eq. 24, and H3.2 based on Eq. 25.\nThe best overall results were obtained by the support sets-based centrality model using both the Fractional, with N = 1.(3) and using idf, and the Manhattan distance. The next best-performing variants of our model were Cosine, Minkowski (N defined by the dimension of the semantic space), and Euclidean, all over-performing both TextRank Undirected and the Uniform Influx model. The best PageRank variant, using a backward directed graph and the cosine similarity with idf, achieved a performance similar to the Cosine (SSC = 4, idf ) and the Minkowski (SSC = 2) variants of our model. TextRank Undirected, Uniform Influx, and Continuous LexRank (idf ) obtained performances similar to the Euclidean (SSC\n= 5, idf ) and the Cosine (90%) variants. Notice that although not exhaustively analyzing the effects of term weighting, the use of idf clearly benefits some metrics: see, for instance, the Cosine and Fractional N = 1.(3) variants of our model, the PageRank variants based on the cosine similarity, and Degree. It is relevant to note that our model, which has low computational requirements, achieves results comparable to graph-based state-of-the-art systems (Ceylan, Mihalcea, O\u0308zertem, Lloret, & Palomar, 2010; Antiqueira et al., 2009). Notice that although the estimated confidence intervals overlap, the performance of the Manhattan SCC=2 variant is significantly better, using the directional Wilcoxon signed rank test with continuity correction, than the ones of TextRank Undirected, (W = 2584, p < 0.05), Uniform Influx (W = 2740, p < 0.05), and also Continuous LexRank (W = 2381.5, p < 0.1).4 The only variants of our model that perform below the baseline are the Fractional variants with N < 1. Fractional distances with N < 1, as can be seen by the effect of the metric on the unit circle (Figure 1), increase the distance between all passages, negatively influencing the construction of the support sets and, consequently the estimation of relevant content.\nConcerning the automatically set per passage thresholds, it is possible to observe that the best overall performance was achieved by a metric, Fractional N = 1.(3), with idf, using the heuristic based on the average difference between consecutive distances. For Cosine, Manhattan, Euclidean, and Minkowski variants, the heuristic based on the average distance (Cosine) and the heuristics based on passage order achieved results comparable to the best performing kNN approaches. For Chebyshev and Fractional (with N < 1) variants the best results were obtained using the heuristics based on the analysis of the progression of the distances.\nFigure 2 shows the improvements over the baseline and over the previous best-performing system. It is possible to perceive that the greatest performance jumps are introduced by Euclidean (10%) and Euclidean (H2.3), Minkowski (SSC=2), and the best-performing Manhattan, all instances of the support sets-based relevance model. Additionally, it is important to notice that the improvement of CN-Voting over the baseline (computed in the same conditions of CN-Voting) is of only 1%, having a performance worse than the poorest TextRank version which had an improvement over the baseline of 1.6%. In what concerns the linguistic knowledge-based systems (SuPor-2 and the enriched versions of TextRank Undirected), we cannot make an informed assessment of their performance since we cannot substantiate the used baseline, taken from the work of Mihalcea and Tarau (2005). Nonetheless, using that baseline, it is clear that linguistic information improves the performance of extractive summarizers beyond what we achieved with our model: improvements over the baseline range from 9% to 17.5%. Notice however, that it would be possible to enrich our model with linguistic information, in the same manner of TextRank.\nRegarding the effect of the similarity metric on the PageRank-based systems, it is possible to observe that PageRank Undirected based on Content Overlap (TextRank Undirected) has a better performance than when similarity is based on a geometric metric\u2014either Manhattan or Cosine (Continuous LexRank). However, the same does not happen when considering the results obtained by the several variants of PageRank Backward. Although the use of Content Overlap, in fact, leads to a better performance than using a Manhattan-based\n4. Statistical tests were computed using R (R Development Core Team, 2009).\nsimilarity metric, the use of the cosine similarity results in a performance comparable to the one of using the Content Overlap metric. The Manhattan-based similarity metric is defined in Eq. 29.\nsimmanhattan(x,y) = 1\n1 + distmanhattan(x,y) (29)"}, {"heading": "4.2 Experiment 2: Speech", "text": "In this section, we describe the experiments performed and analyze the corresponding results when using as input source automatically transcribed speech."}, {"heading": "4.2.1 Data", "text": "To evaluate our ideas in the speech processing setting, we used the same data of Ribeiro and de Matos (2008a): the automatic transcriptions of 15 broadcast news stories in European Portuguese, part of a news program. Subject areas include \u201csociety\u201d, \u201cpolitics\u201d, \u201csports\u201d, among others. Table 4 details the corpus composition. For each news story, there is a human-produced reference summary, which is an abstract. The average word recognition error rate is 19.5% and automatic sentence segmentation attained a slot error rate (SER, commonly used to evaluate this kind of task) of 90.2%. As it is possible to observe in Table 4, it is important to distinguish between the notion of sentence in written text and that of sentence-like unit (SU) in speech data. Note, in particular, the difference in the average number of words per sentence in the summary versus the average number of words per SU in the news story. According to Liu, Shriberg, Stolcke, Hillard, Ostendorf, and Harper (2006), the concept of SU is different from the concept of sentence in written text, since, although semantically complete, SUs can be smaller than a sentence. This is corroborated by the fact that it is possible to find news stories with SUs of length 1 (this corpus has 8 SUs of length 1). Beyond the definition of SU, note that an SER of 90.2% is a high value: currently, the automatic punctuation module responsible for delimiting SUs achieves an SER of 62.2%, using prosodic information (Batista, Moniz, Trancoso, Meinedo, Mata, & Mamede, 2010)."}, {"heading": "4.2.2 Evaluation Setup", "text": "Regarding speech summarization, even considering the difficulties concerning the applicability of text summarization methods to spoken documents, shallow approaches like LSA or MMR seem to achieve performances comparable to the ones using specific speech-related features (Penn & Zhu, 2008), especially in unsupervised approaches. Given the implemented models, in this experiment we compare the support sets relevance model to the following systems:\n\u2022 An LSA baseline.\n\u2022 The following graph-based methods: Uniform Influx (Kurland & Lee, 2005, 2010), Continuous LexRank and Degree centrality (Erkan & Radev, 2004), and TextRank (Mihalcea & Tarau, 2004, 2005).\n\u2022 The method proposed by Ribeiro and de Matos (2008a), which explores the use of additional related information, less prone to speech-related errors (e.g. from online newspapers), to improve speech summarization (Mixed-Source).\n\u2022 Two human summarizers (extractive) using as source the automatic speech transcriptions of the news stories (Human Extractive).\nBefore analyzing the results, it is important to examine human performance. One of the relevant issues that should be assessed is the level of agreement between the two human summarizers: this was accomplished using the kappa coefficient (Carletta, 1996), for which we obtained a value of 0.425, what is considered a fair to moderate/good agreement (Landis & Kosh, 1977; Fleiss, 1981). Concerning the selected sentences, Figure 3 shows that human summarizer H2 consistently selected the first n sentences, and that in H1 choices there is also a noticeable preference for the first sentences of each news story.\nTo be able to perform a good assessment of the automatic models, we conducted two experiments: in the first one, the number of SUs extracted to compose the automatic summaries was defined in accordance to the number of sentences of the reference human abstracts (which consisted in compressing the input source to about 10% of the original size); in the second experiment, the number of extracted SUs of the automatic summaries was determined by the size of the shortest corresponding human extractive summary. Notice that Mixed-Source and human summaries are the same in both experiments."}, {"heading": "4.2.3 Results", "text": "Table 5 shows the ROUGE-1 scores obtained, for both speech experiments. In this table, it is possible to find more than one instance of some models, since sometimes the bestperforming variant when using as summary size the size of the abstracts was different from the one using as summary size the size of the human extracts.\nA first observation concerns a particular aspect of corpus: as it can be seen, especially in the experiment using as reference size the size of the shortest human extracts, both Human 2 and First Sentences summarizers attained the same ROUGE-1 scores (this does not happen in the experiment using the abstracts size only, due to the fact that First Sentences summaries are shorter, adapted to the experiment required size, than the ones of Human 2, which were not changed). In fact, the summaries are equal, which shows a consistent bias indicating that the most relevant sentences tend to occur in the beginning of the news stories. This bias, although not surprising, since the corpus is composed of broadcast news stories, is also not that common as can be seen in previous work (Ribeiro & de Matos, 2007; Lin, Yeh, & Chen, 2010). Second, it is interesting to notice the performance of the PageRank-based models: while in text there is no observable trend concerning the directionality of the graph, and both LexRank versions performed above the baseline, in speech only the backward versions achieved a good performance (the four undirected versions performed around the baseline, with LexRank obtaining results below the LSA baseline, with exception for the experiment using the extracts size and idf ). From a models perspective, and considering the performance of backward versions in both text and speech, the use of backward directionality seems the main reason for the good performance in speech, where input sources consist of transcriptions of broadcast news stories from a news program. In fact, as mentioned before, this kind of input source is usually short (cf. Table 4) and the main information is given in the opening of the news story. This suggests that directionality introduces position information in the model, which is only relevant for specific types of input source (this is also discussed in Mihalcea & Tarau, 2005). Moreover, note that Continuous LexRank performance was close to the LSA Baseline, which implies that the model is quite susceptible to the referred bias, to the noisy input, or to both. Taking into consideration that the model is based on pair-wise passage similarity and that one of the best-performing support sets-based instance was Cosine, the same similarity metric used by LexRank, it seems that the model was not able to account for the structure of the input sources of this data set. In fact, Degree centrality, also based on the cosine similarity performed better than all PageRank Undirected models. The Influx model performed close to Degree centrality, far from the best performing approaches, which, in this case, suggests that the method for generating the graph, the generation probabilities, is affected by the noisy input, especially when considering small contexts like passages. Approaches based on generation probabilities seem more adequate to larger contexts, such as documents (Kurland & Lee, 2005, 2010; Erkan, 2006a). Erkan (2006b) mentions that results in query-based summarization using generation probabilities were worse than the ones obtained by LexRank in generic summarization.\nConcerning the overall results, performance varies according to the size of the summaries. When using the abstracts size, the best-performing instance is Cosine with idf using an heuristic based on the passage order; when using the reference extracts size, the best performance was achieved by the backward PageRank model, followed by the Chebyshev variant also using an heuristic based on passage order and the same Cosine variant. Both variants achieved better results than TextRank Backward. Given the success of the heuristic H2.3 in these experiments, it seems that this heuristic may also be introducing position information in the model. Although not achieving the best performance in the experiment using the extracts size, there is no significant difference between the best sup-\nport sets-based relevance model instance, the Chebyshev variant using an heuristic based on passage order, and the ones achieved by human summarizers: applying the directional Wilcoxon signed rank test with continuity correction, the test values when using the shortest human extracts size are W = 53, p = 0.5. This means a state-of-the-art performance in the experiment using the abstracts size, and comparable to a human (results similar to First Sentence, which is similar to Human Extractive 2) when using the shortest human extracts size. In fact, Chebyshev (10%), to avoid the influence of possible position information, is also not significantly different than Human Extractive 2 (W = 11, p = 0.2092). Cosine with idf and using H2.3 has a better performance with statistical significance than Degree with \u03b4 = 0.06 (W = 53.5, p < 0.005 when using the abstracts size; W = 54, p < 0.005 when using the shortest human extracts size), TextRank Undirected (W = 92.5, p < 0.05 when using the abstracts size; W = 96, p < 0.05 when using the shortest human extracts size), and Uniform Influx (W = 60, p < 0.01 when using the abstracts size; W = 51, p < 0.06 when using the shortest human extracts size), using the same statistical test. The obtained results, in both speech transcriptions and written text, suggest that the model is robust, being able to detect the most relevant content without specific information of where it should be found and performing well in the presence of noisy input. Moreover, cosine similarity seems to be a good metric to use in the proposed model, performing among the top ranking variants, in both written and spoken language.\nFractional variants with N < 1 were, again, the worst performing approaches (we did not include values for the automatically set per passage thresholds in Table 5, since they were worse than the simple kNN approach) because their effect on the similarity assessment boosts the influence of the recognition errors. On the other hand, Chebyshev seems more imune to that influence: the single use of the maximal difference through all the dimensions makes it less prone to noise (recognition errors). The same happens with the variant using the generic Minkowski distance with N equal to the number of dimensions of the semantic space.\nFigures 4 and 5 shows the performance variation introduced by the different approaches. Notice that, in the speech experiments, performance increments are a magnitude higher when compared to the ones in written text. Overall, the Chebyshev variant of the support sets-based relevance model introduces the highest relative gains, close to 10% in the experiment using the abstracts size, close to 5% in the experiment using the extracts size. In the experiment using the extracts size, TextRank Undirected also achieves relative gains of near 10% over the previous best-performing system, the LSA baseline. Similar relative improvements are introduced by the human summarizers in the experiment using the abstracts size. As expected, increasing the size of the summaries increases the coverage of the human abstracts (bottom of Figure 5).\nFurther, comparing our model to more complex (not centrality-based), state-of-the-art models like the one presented by Lin et al. (2010) suggests that at least similar performance is attained: the relative performance increment of our model over LexRank is of 57.4% and 39.8% (both speech experiments), whereas the relative gain of the best variant of the model proposed by Lin et al. over LexRank is of 39.6%. Note that this can only be taken as indicative, since an accurate comparison is not possible because data sets differ, Lin et al. do not explicit which variant of LexRank is used, and do not address statistical significance.\nSummary Size Determined by Human Abstracts"}, {"heading": "4.3 Influence of the Size of the Support Sets on the Assessment of Relevance", "text": "We do not propose a method for determining an optimum size for the support sets. Nonetheless, we analyze the influence of the support set size on the assessment of the relevant content, both in text and speech.\nFigure 6 depicts the behavior of the model variants with a performance above the baseline over written text, while Figure 7 illustrates the variants under the same conditions over\nSummary Size Determined by Shortest Human Extracts\nautomatic speech transcriptions (in this case, error bars were omitted for clarity). We analyze the general performance of those variants, considering as support set size the number of passages of the input source in 10% increments. Given the average size of an input source, both in written text (Table 1), and speech transcriptions (Table 4), absolute cardinalities (SSC) ranging from 1 to 5 passages broadly cover possible sizes in the interval 0-10%.\nA first observation concerns the fact that varying the cardinality of the support sets when the input sources consist of written text has a smooth effect over the performance. This allows the analysis of generic tendencies. In contrast, when processing automatic\nCosine\nManhattan (N=1)\nspeech transcriptions, it is possible to perceive several irregularities. These irregularities can have two different causes: the intrinsic characteristics of speech transcriptions such as recognition errors, sentence boundary detection errors, and the type of discourse; or, the specificities of the data set\u2014in particular, the global size of the corpus and the specific news story structure. However, considering the performance of the metrics over both text and speech, the irregularities seem to be mainly caused by the intrinsic properties of speech transcriptions and the specific structure of the news story.\nCosine\nManhattan (N=1)\nConcerning performance itself, in written text, the best performances are achieved using low cardinalities (absolute cardinalities of 2 or 3 passages, or of about 10% of the passages of the input source). Moreover, an increase in the size of the support sets leads to a decay of the results (except when using the cosine similarity). When processing automatic speech transcriptions, it is difficult to find a clear definition of when the best results are achieved. Considering absolute cardinalities, with the exception of the Manhattan distance, every variant has a peak when using support sets with 4 passages. However, it is not possible to extend such line of thought to relative sizes due to the previously referred irregularities. Nonetheless, higher cardinalities (70%\u201390%) lead to worse results, what is expected given the nature of the model (again with exception of when using the cosine similarity). In addition, note that increasing the size of the summaries improves the distinction from the baseline (summaries based on the size of the shortest human extracts are longer than the ones based on the size of the human abstracts). This means that the model is robust to needs regarding summary size, continuing to select good content even for larger summaries."}, {"heading": "5. Conclusions", "text": "The number of up-to-date examples of work on automatic summarization using centralitybased relevance models is significant (Garg, Favre, Reidhammer, & Hakkani-Tu\u0308r, 2009; Antiqueira et al., 2009; Ceylan et al., 2010; Wan, Li, & Xiao, 2010). In our work, we assessed the main approaches of the centrality-as-relevance paradigm, and introduced a new centrality-based relevance model for automatic summarization. Our model uses support sets to better characterize the information sources to be summarized, leading to a better estimation of the relevant content. In fact, we assume that input sources comprehend several topics that are uncovered by associating to each passage a support set composed by the most semantically related passages. Building on the ideas of Ruge (1992), [...] the model of semantic space in which the relative position of two terms determines the semantic similarity better fits the imagination of human intuition [about] semantic similarity [...], semantic relatedness was computed by geometric proximity. We explore several metrics and analyze their impact on the proposed model as well as (to a certain extent) on the related work. Centrality (relevance) is determined by taking into account the whole input source, and not only local information, using the support sets-based representation. Moreover, although not formally analyzed, notice that the proposed model has low computational requirements.\nWe conducted a thorough automatic evaluation, experimenting our model both on written text and transcribed speech summarization. The obtained results suggest that the model is robust, being able to detect the most relevant content without specific information of where it should be found and performing well in the presence of noisy input, such as automatic speech transcriptions. However, it must be taken into consideration that the use of ROUGE in summary evaluation, although generalized, allowing to easily compare results and replicate experiments, is not an ideal scenario, and consequently, results should be corroborated by a perceptual evaluation. The outcome of the performed trials show that the proposed model achieves state-of-the-art performance in both text and speech summarization, including when compared to considerably more complex approaches. Nonetheless, we identified some limitations. First, although grounding semantic similarity on geometric\nproximity, in the current experiments we rely mainly on lexical overlap. While maintaining the semantic approach, the use of more complex methods (Turney & Pantel, 2010) may improve the assessment of semantic similarity. Second, we did not address a specific procedure for estimating optimum thresholds, leaving it for future research. Nonetheless, we explored several heuristics that achieved top ranking performance. Moreover, we carried out in this document an analysis that provides some clues for the adequate dimension of the support sets, but a more analytical analysis should be performed."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers for their insightful comments. This work was supported by FCT (INESC-ID multiannual funding) through the PIDDAC Program funds."}], "references": [{"title": "On the Surprising Behavior of Distance Metrics in High Dimensional Space", "author": ["C.C. Aggarwal", "A. Hinneburg", "D.A. Keim"], "venue": "Database Theory \u2014 ICDT", "citeRegEx": "Aggarwal et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Aggarwal et al\\.", "year": 2001}, {"title": "A complex network approach to text summarization", "author": ["L. Antiqueira", "O.N. Oliveira Jr.", "L. da Fontoura Costa", "M.G.V. Nunes"], "venue": "Information Sciences,", "citeRegEx": "Antiqueira et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Antiqueira et al\\.", "year": 2009}, {"title": "Extending the punctuation module for European Portuguese", "author": ["F. Batista", "H. Moniz", "I. Trancoso", "H. Meinedo", "A.I. Mata", "N.J. Mamede"], "venue": "In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH", "citeRegEx": "Batista et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Batista et al\\.", "year": 2010}, {"title": "The anatomy of a large-scale hypertextual Web search engine", "author": ["S. Brin", "L. Page"], "venue": "Computer Networks and ISDN Systems,", "citeRegEx": "Brin and Page,? \\Q1998\\E", "shortCiteRegEx": "Brin and Page", "year": 1998}, {"title": "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "SIGIR", "citeRegEx": "Carbonell and Goldstein,? \\Q1998\\E", "shortCiteRegEx": "Carbonell and Goldstein", "year": 1998}, {"title": "Assessing agreement on classification tasks: The kappa statistic", "author": ["J. Carletta"], "venue": "Computational Linguistics, 22 (2), 249\u2013254.", "citeRegEx": "Carletta,? 1996", "shortCiteRegEx": "Carletta", "year": 1996}, {"title": "Quantifying the Limits and Success of Extractive Summarization Systems Across Domains", "author": ["H. Ceylan", "R. Mihalcea", "U. \u00d6zertem", "E. Lloret", "M. Palomar"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL,", "citeRegEx": "Ceylan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ceylan et al\\.", "year": 2010}, {"title": "Are Extractive Text Summarisation Techniques Portable To Broadcast News", "author": ["H. Christensen", "Y. Gotoh", "B. Kolluru", "S. Renals"], "venue": "In Proceedings of the IEEE Work-", "citeRegEx": "Christensen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Christensen et al\\.", "year": 2003}, {"title": "Sentence Compression as Tree Transduction", "author": ["T. Cohn", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cohn and Lapata,? \\Q2009\\E", "shortCiteRegEx": "Cohn and Lapata", "year": 2009}, {"title": "Evaluating Summaries Automatically \u2013 a system proposal", "author": ["P.C.F. de Oliveira", "E.W. Torrens", "A. Cidral", "S. Schossland", "E. Bittencourt"], "venue": "In Proceedings of the Sixth International Language Resources and Evaluation", "citeRegEx": "Oliveira et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oliveira et al\\.", "year": 2008}, {"title": "Summarizing Information", "author": ["B. Endres-Niggemeyer"], "venue": "Springer.", "citeRegEx": "Endres.Niggemeyer,? 1998", "shortCiteRegEx": "Endres.Niggemeyer", "year": 1998}, {"title": "Language Model-Based Document Clustering Using Random Walks", "author": ["G. Erkan"], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pp. 479\u2013486. Association for Computational Linguistics.", "citeRegEx": "Erkan,? 2006a", "shortCiteRegEx": "Erkan", "year": 2006}, {"title": "Using Biased Random Walks for Focused Summarization", "author": ["G. Erkan"], "venue": "Proceedings of the Document Understanding Conference.", "citeRegEx": "Erkan,? 2006b", "shortCiteRegEx": "Erkan", "year": 2006}, {"title": "LexRank: Graph-based Centrality as Salience in Text Summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Erkan and Radev,? \\Q2004\\E", "shortCiteRegEx": "Erkan and Radev", "year": 2004}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": null, "citeRegEx": "Fellbaum,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum", "year": 1998}, {"title": "Recent Advances in Automatic Speech Summarization", "author": ["S. Furui"], "venue": "Proceedings of the 8th Conference on Recherche d\u2019Information Assist\u00e9e par Ordinateur (RIAO). Centre des Hautes \u00c9tudes Internationales d\u2019Informatique Documentaire.", "citeRegEx": "Furui,? 2007", "shortCiteRegEx": "Furui", "year": 2007}, {"title": "ClusterRank: A Graph Based Method for Meeting Summarization", "author": ["N. Garg", "B. Favre", "K. Reidhammer", "D. Hakkani-T\u00fcr"], "venue": "In Proceedings of the 10th Annual Conference of the International Speech Communication Association (INTERSPEECH", "citeRegEx": "Garg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Garg et al\\.", "year": 2009}, {"title": "Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis", "author": ["Y. Gong", "X. Liu"], "venue": "In SIGIR 2001: Proceedings of the 24st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Gong and Liu,? \\Q2001\\E", "shortCiteRegEx": "Gong and Liu", "year": 2001}, {"title": "Topic Themes for Multi-Document Summarization", "author": ["S. Harabagiu", "F. Lacatusu"], "venue": "In SIGIR 2005: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Harabagiu and Lacatusu,? \\Q2005\\E", "shortCiteRegEx": "Harabagiu and Lacatusu", "year": 2005}, {"title": "Evaluation Method for Automatic Speech Summarization", "author": ["C. Hori", "T. Hori", "S. Furui"], "venue": "In Proceedings of the 8th EUROSPEECH - INTERSPEECH", "citeRegEx": "Hori et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hori et al\\.", "year": 2003}, {"title": "\u00c9tude comparative de la distribution florale dans une portion des Alpes et des Jura", "author": ["P. Jaccard"], "venue": "Bulletin del la Soci\u00e9t\u00e9 Vaudoise des Sciences Naturelles, 37, 547\u2013579.", "citeRegEx": "Jaccard,? 1901", "shortCiteRegEx": "Jaccard", "year": 1901}, {"title": "Random Indexing of text samples for Latent Semantic Analysis", "author": ["P. Kanerva", "J. Kristoferson", "A. Holst"], "venue": "Proceedings of the 22nd annual conference of the Cognitive Science Society,", "citeRegEx": "Kanerva et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kanerva et al\\.", "year": 2000}, {"title": "Foundations of real-world intelligence, chap. From words to understanding, pp. 294\u2013311", "author": ["P. Kanerva", "M. Sahlgren"], "venue": "No. 26. Center for the Study of Language and Information", "citeRegEx": "Kanerva and Sahlgren,? \\Q2001\\E", "shortCiteRegEx": "Kanerva and Sahlgren", "year": 2001}, {"title": "Authoritative Sources in a Hyperlinked Environment", "author": ["J.M. Kleinberg"], "venue": "Journal of the ACM, 46 (5), 604\u2013632.", "citeRegEx": "Kleinberg,? 1999", "shortCiteRegEx": "Kleinberg", "year": 1999}, {"title": "Introduction to Hp Spaces", "author": ["P. Koosis"], "venue": "Cambridge Universisty Press. Kurland, O., & Lee, L. (2005). PageRank without Hyperlinks: Structural Re-Ranking using Links Induced by Language Models. In SIGIR 2005: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 306\u2013313. ACM.", "citeRegEx": "Koosis,? 1998", "shortCiteRegEx": "Koosis", "year": 1998}, {"title": "PageRank without Hyperlinks: Structural Reranking using Links Induced by Language Models", "author": ["O. Kurland", "L. Lee"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "Kurland and Lee,? \\Q2010\\E", "shortCiteRegEx": "Kurland and Lee", "year": 2010}, {"title": "An Introduction to Latent Semantic Analysis", "author": ["T.K. Landauer", "P.W. Foltz", "D. Laham"], "venue": "Discourse Processes,", "citeRegEx": "Landauer et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "The Measurement of Observer Agreement for Categorical", "author": ["J.R. Landis", "G.G. Kosh"], "venue": "Data. Biometrics,", "citeRegEx": "Landis and Kosh,? \\Q1977\\E", "shortCiteRegEx": "Landis and Kosh", "year": 1977}, {"title": "Extractive Automatic Summarization: Does more linguitic knowledge make a difference", "author": ["D.S. Leite", "L.H.M. Rino", "T.A.S. Pardo", "M.G.V. Nunes"], "venue": "In Proceedings of the Second Workshop on TextGraphs: Graph-based Algorithms for Natural Language Processing,", "citeRegEx": "Leite et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Leite et al\\.", "year": 2007}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author": ["Lin", "C.-Y."], "venue": "Moens, M.-F., & Szpakowicz, S. (Eds.), Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pp. 74\u201381. Association for Computational Linguistics.", "citeRegEx": "Lin and C..Y.,? 2004", "shortCiteRegEx": "Lin and C..Y.", "year": 2004}, {"title": "The Automated Acquisition of Topic Signatures for Text Summarization", "author": ["Lin", "C.-Y", "E. Hovy"], "venue": "Coling", "citeRegEx": "Lin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2000}, {"title": "Extractive Speech Summarization \u2013 From the View of Decision Theory", "author": ["Lin", "S.-H", "Yeh", "Y.-M", "B. Chen"], "venue": "In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH", "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Enriching Speech Recognition with Automatic Detection of Sentence Boundaries and Disfluencies", "author": ["Y. Liu", "E. Shriberg", "A. Stolcke", "D. Hillard", "M. Ostendorf", "M. Harper"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "Liu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Semantic and associative priming in highdimensional semantic space", "author": ["K. Lund", "C. Burgess", "R.A. Atchley"], "venue": "Proceedings of the 17th annual conference of the Cognitive Science Society,", "citeRegEx": "Lund et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Lund et al\\.", "year": 1995}, {"title": "The Theory and Practice of Discourse Parsing and Summarization", "author": ["D. Marcu"], "venue": "The MIT Press.", "citeRegEx": "Marcu,? 2000", "shortCiteRegEx": "Marcu", "year": 2000}, {"title": "Comparing Lexical, Acoustic/Prosodic, Strucural and Discourse Features for Speech Summarization", "author": ["S.R. Maskey", "J. Hirschberg"], "venue": "In Proceedings of the 9th EUROSPEECH - INTERSPEECH", "citeRegEx": "Maskey and Hirschberg,? \\Q2005\\E", "shortCiteRegEx": "Maskey and Hirschberg", "year": 2005}, {"title": "From Text to Speech Summarization", "author": ["K.R. McKeown", "J. Hirschberg", "M. Galley", "S.R. Maskey"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings,", "citeRegEx": "McKeown et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McKeown et al\\.", "year": 2005}, {"title": "TextRank: Bringing Order into Texts", "author": ["R. Mihalcea", "P. Tarau"], "venue": "In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mihalcea and Tarau,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea and Tarau", "year": 2004}, {"title": "A Language Independent Algorithm for Single and Multiple Document Summarization", "author": ["R. Mihalcea", "P. Tarau"], "venue": "In Proceedings of the Second International Joint Conference on Natural Language Processing: Companion Volume to the Proceedings of Conference including Posters/Demos and Tutorial Abstracts,", "citeRegEx": "Mihalcea and Tarau,? \\Q2005\\E", "shortCiteRegEx": "Mihalcea and Tarau", "year": 2005}, {"title": "Bootstrapping : a nonparametric approach to statistical inference", "author": ["C.Z. Mooney", "R.D. Duval"], "venue": null, "citeRegEx": "Mooney and Duval,? \\Q1993\\E", "shortCiteRegEx": "Mooney and Duval", "year": 1993}, {"title": "Term-Weighting for Summarization of Multi-Party Spoken Dialogues", "author": ["G. Murray", "S. Renals"], "venue": "Machine Learning for Multimodal Interaction IV,", "citeRegEx": "Murray and Renals,? \\Q2007\\E", "shortCiteRegEx": "Murray and Renals", "year": 2007}, {"title": "Summarization Evaluation for Text and Speech: Issues and Approaches", "author": ["A. Nenkova"], "venue": "Proceedings of INTERSPEECH 2006 - ICSLP, pp. 1527\u20131530. ISCA.", "citeRegEx": "Nenkova,? 2006", "shortCiteRegEx": "Nenkova", "year": 2006}, {"title": "The pyramid method: incorporating human content selection variation in summarization evaluation", "author": ["A. Nenkova", "R. Passonneau", "K. McKeown"], "venue": "ACM Transactions on Speech and Language Processing,", "citeRegEx": "Nenkova et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nenkova et al\\.", "year": 2007}, {"title": "A comparison of summarisation methods based on term specificity estimation", "author": ["C. Or\u0103san", "V. Pekar", "L. Hasler"], "venue": "In Proceedings of the Fourth International Language Resources and Evaluation", "citeRegEx": "Or\u0103san et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Or\u0103san et al\\.", "year": 2004}, {"title": "TeMario: a corpus for automatic text summarization", "author": ["T.A.S. Pardo", "L.H.M. Rino"], "venue": "Tech. rep. NILC-TR-03-09, Nu\u0301cleo Interinstitucional de Lingu\u0308\u0301\u0131stica Computacional (NILC),", "citeRegEx": "Pardo and Rino,? \\Q2003\\E", "shortCiteRegEx": "Pardo and Rino", "year": 2003}, {"title": "A Critical Reassessment of Evaluation Baselines for Speech Summarization", "author": ["G. Penn", "X. Zhu"], "venue": "In Proceeding of ACL-08: HLT,", "citeRegEx": "Penn and Zhu,? \\Q2008\\E", "shortCiteRegEx": "Penn and Zhu", "year": 2008}, {"title": "R: A Language and Environment for Statistical Computing", "author": ["R Development Core Team"], "venue": "R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0.", "citeRegEx": "Team,? 2009", "shortCiteRegEx": "Team", "year": 2009}, {"title": "A Description of the CIDR System as Used for TDT-2", "author": ["D.R. Radev", "V. Hatzivassiloglou", "K.R. McKeown"], "venue": "In Proceedings of the DARPA Broadcast News Workshop", "citeRegEx": "Radev et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Radev et al\\.", "year": 1999}, {"title": "Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies", "author": ["D.R. Radev", "H. Jing", "M. Budzikowska"], "venue": null, "citeRegEx": "Radev et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2000}, {"title": "Centroid-based summarization of multiple documents", "author": ["D.R. Radev", "H. Jing", "M. Sty\u015b", "D. Tam"], "venue": "Information Processing and Management,", "citeRegEx": "Radev et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2004}, {"title": "Summarization Evaluation using Relative Utility", "author": ["D.R. Radev", "D. Tam"], "venue": "In Proceedings of the 12th international conference on Information and Knowledge Management,", "citeRegEx": "Radev and Tam,? \\Q2003\\E", "shortCiteRegEx": "Radev and Tam", "year": 2003}, {"title": "Extractive Summarization of Broadcast News: Comparing Strategies for European Portuguese", "author": ["R. Ribeiro", "D.M. de Matos"], "venue": "Text, Speech and Dialogue \u2013 10th International Conference,", "citeRegEx": "Ribeiro and Matos,? \\Q2007\\E", "shortCiteRegEx": "Ribeiro and Matos", "year": 2007}, {"title": "Mixed-Source Multi-Document Speech-to-Text Summarization", "author": ["R. Ribeiro", "D.M. de Matos"], "venue": "In Coling 2008: Proceedings of the 2nd workshop on Multi-source Multilingual Information Extraction and Summarization,", "citeRegEx": "Ribeiro and Matos,? \\Q2008\\E", "shortCiteRegEx": "Ribeiro and Matos", "year": 2008}, {"title": "Using Prior Knowledge to Assess Relevance in Speech Summarization", "author": ["R. Ribeiro", "D.M. de Matos"], "venue": "IEEE Workshop on Spoken Language Technology,", "citeRegEx": "Ribeiro and Matos,? \\Q2008\\E", "shortCiteRegEx": "Ribeiro and Matos", "year": 2008}, {"title": "Experiments on linguistically-based term associations", "author": ["G. Ruge"], "venue": "Information Processing and Management, 28 (3), 317\u2013332.", "citeRegEx": "Ruge,? 1992", "shortCiteRegEx": "Ruge", "year": 1992}, {"title": "The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces", "author": ["M. Sahlgren"], "venue": "Ph.D. thesis, Stockholm University.", "citeRegEx": "Sahlgren,? 2006", "shortCiteRegEx": "Sahlgren", "year": 2006}, {"title": "Handbook of Latent Semantic Analysis, chap. Probabilistic Topic Models, pp. 427\u2013448", "author": ["M. Steyvers", "T. Griffiths"], "venue": null, "citeRegEx": "Steyvers and Griffiths,? \\Q2007\\E", "shortCiteRegEx": "Steyvers and Griffiths", "year": 2007}, {"title": "Between shallow and deep: an experiment in automatic summarising", "author": ["R.I. Tucker", "K. Sp\u00e4rck Jones"], "venue": "Tech. rep. 632,", "citeRegEx": "Tucker and Jones,? \\Q2005\\E", "shortCiteRegEx": "Tucker and Jones", "year": 2005}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "A comprehensive comparative evaluation of RST-based summarization methods", "author": ["V.R. Uz\u00eada", "T.A.S. Pardo", "M.G.V. Nunes"], "venue": "ACM Transactions on Speech and Language Processing,", "citeRegEx": "Uz\u00eada et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Uz\u00eada et al\\.", "year": 2010}, {"title": "Beyond SumBasic: Task-focused summarization and lexical expansion", "author": ["L. Vanderwende", "H. Suzuki", "C. Brockett", "A. Nenkova"], "venue": "Information Processing and Management,", "citeRegEx": "Vanderwende et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vanderwende et al\\.", "year": 2007}, {"title": "EUSUM: Extracting Easy-to-Understand English Summaries for Non-Native Readers", "author": ["X. Wan", "H. Li", "J. Xiao"], "venue": "In SIGIR 2010: Proceedings of the 33th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Wan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2010}, {"title": "CollabSum: Exploiting Multiple Document Clustering for Collaborative Single Document Summarizations", "author": ["X. Wan", "J. Yang", "J. Xiao"], "venue": "In SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Wan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2007}, {"title": "Minimizing Word Error Rate in Textual Summaries of Spoken Language", "author": ["K. Zechner", "A. Waibel"], "venue": "In Proceedings of the 1st conference of the North American chapter of the ACL,", "citeRegEx": "Zechner and Waibel,? \\Q2000\\E", "shortCiteRegEx": "Zechner and Waibel", "year": 2000}, {"title": "Semi-Supervised Learning with Graphs", "author": ["X. Zhu"], "venue": "Ph.D. thesis, Language Technologies Institute, School of Computer Science, Carnegie Mellon University.", "citeRegEx": "Zhu,? 2005", "shortCiteRegEx": "Zhu", "year": 2005}], "referenceMentions": [{"referenceID": 15, "context": "In fact, spoken language summarization is often considered a much harder task than text summarization (McKeown, Hirschberg, Galley, & Maskey, 2005; Furui, 2007): problems like speech recognition errors, disfluencies, and the accurate identification of sentence boundaries not only increase the difficulty in determining the salient information, but also constrain the applicability of text summarization techniques to speech summarization (although in the presence of planned speech, as it partly happens in the broadcast news domain, that portability is more feasible, Christensen, Gotoh, Kolluru, & Renals, 2003).", "startOffset": 102, "endOffset": 160}, {"referenceID": 15, "context": "In fact, spoken language summarization is often considered a much harder task than text summarization (McKeown, Hirschberg, Galley, & Maskey, 2005; Furui, 2007): problems like speech recognition errors, disfluencies, and the accurate identification of sentence boundaries not only increase the difficulty in determining the salient information, but also constrain the applicability of text summarization techniques to speech summarization (although in the presence of planned speech, as it partly happens in the broadcast news domain, that portability is more feasible, Christensen, Gotoh, Kolluru, & Renals, 2003). Nonetheless, shallow text summarization approaches such as Latent Semantic Analysis (LSA) (Landauer, Foltz, & Laham, 1998; Gong & Liu, 2001) and Maximal Marginal Relevance (MMR) (Carbonell & Goldstein, 1998) seem to achieve performances comparable to the ones using specific speech-related features (Penn & Zhu, 2008). Following the determination of the relevant content, the summary must be composed and presented to the user. If the identified content consists of passages found in the input source that are glued together to form the summary, that summary is usually designated as extract ; on the other hand, when the important content is devised as a series of concepts that are fused into a smaller set and then used to generate a new, concise, and informative text, we are in the presence of an abstract. In between extraction and concept-to-text generation, especially in text summarization, text-to-text generation methods, which rely on text rewriting\u2014paraphrasing\u2014, of which sentence compression is a major representative, are becoming an up-to-date subject (Cohn & Lapata, 2009). Given the hardness of abstraction, the bulk of the work in the area consists of extractive summarization. A common family of approaches to the identification of the relevant content is the centrality family. These methods base the detection of the most salient passages on the identification of the central passages of the input source(s). One of the main representatives of this family is centroid-based summarization. Centroid-based methods build on the idea of a pseudo-passage that represents the central topic of the input source (the centroid) selecting as passages (x) to be included in the summary the ones that are close to the centroid. Pioneer work (on multi-document summarization) by Radev, Hatzivassiloglou, and McKeown (1999) and Radev, Jing, and Budzikowska (2000) creates clusters of documents by representing each document as a tf-idf vector; the centroid of each cluster is also defined as a tf-idf vector, with the coordinates corresponding to the weighted average of the tf-idf values of the documents of the cluster; finally, sentences that contain the words of the centroids are presumably the best representatives of the topic of the cluster, thus being the best candidates to belonging to the summary.", "startOffset": 148, "endOffset": 2449}, {"referenceID": 15, "context": "In fact, spoken language summarization is often considered a much harder task than text summarization (McKeown, Hirschberg, Galley, & Maskey, 2005; Furui, 2007): problems like speech recognition errors, disfluencies, and the accurate identification of sentence boundaries not only increase the difficulty in determining the salient information, but also constrain the applicability of text summarization techniques to speech summarization (although in the presence of planned speech, as it partly happens in the broadcast news domain, that portability is more feasible, Christensen, Gotoh, Kolluru, & Renals, 2003). Nonetheless, shallow text summarization approaches such as Latent Semantic Analysis (LSA) (Landauer, Foltz, & Laham, 1998; Gong & Liu, 2001) and Maximal Marginal Relevance (MMR) (Carbonell & Goldstein, 1998) seem to achieve performances comparable to the ones using specific speech-related features (Penn & Zhu, 2008). Following the determination of the relevant content, the summary must be composed and presented to the user. If the identified content consists of passages found in the input source that are glued together to form the summary, that summary is usually designated as extract ; on the other hand, when the important content is devised as a series of concepts that are fused into a smaller set and then used to generate a new, concise, and informative text, we are in the presence of an abstract. In between extraction and concept-to-text generation, especially in text summarization, text-to-text generation methods, which rely on text rewriting\u2014paraphrasing\u2014, of which sentence compression is a major representative, are becoming an up-to-date subject (Cohn & Lapata, 2009). Given the hardness of abstraction, the bulk of the work in the area consists of extractive summarization. A common family of approaches to the identification of the relevant content is the centrality family. These methods base the detection of the most salient passages on the identification of the central passages of the input source(s). One of the main representatives of this family is centroid-based summarization. Centroid-based methods build on the idea of a pseudo-passage that represents the central topic of the input source (the centroid) selecting as passages (x) to be included in the summary the ones that are close to the centroid. Pioneer work (on multi-document summarization) by Radev, Hatzivassiloglou, and McKeown (1999) and Radev, Jing, and Budzikowska (2000) creates clusters of documents by representing each document as a tf-idf vector; the centroid of each cluster is also defined as a tf-idf vector, with the coordinates corresponding to the weighted average of the tf-idf values of the documents of the cluster; finally, sentences that contain the words of the centroids are presumably the best representatives of the topic of the cluster, thus being the best candidates to belonging to the summary.", "startOffset": 148, "endOffset": 2489}, {"referenceID": 59, "context": "This means that it is common to find, in the input sources to be summarized, lateral issues or considerations that are not relevant to devise the salient information (discourse structure-based summarization is based on the relevance of nuclear text segments, Marcu, 2000; Uz\u00eada et al., 2010), and that may affect centrality-based summarization methods by inducing inadequate centroids or decreasing the scores of more suitable sentences.", "startOffset": 166, "endOffset": 291}, {"referenceID": 47, "context": "The work in multi-document summarization by Radev et al. (1999, 2000) and Radev, Jing, Sty\u015b, and Tam (2004) and the work developed by Lin and Hovy (2000) are examples of this approach.", "startOffset": 44, "endOffset": 108}, {"referenceID": 47, "context": "The work in multi-document summarization by Radev et al. (1999, 2000) and Radev, Jing, Sty\u015b, and Tam (2004) and the work developed by Lin and Hovy (2000) are examples of this approach.", "startOffset": 44, "endOffset": 154}, {"referenceID": 11, "context": "The work presented by Erkan and Radev (2004), as well as the work developed by Mihalcea and Tarau (2005), are examples of this approach.", "startOffset": 22, "endOffset": 45}, {"referenceID": 11, "context": "The work presented by Erkan and Radev (2004), as well as the work developed by Mihalcea and Tarau (2005), are examples of this approach.", "startOffset": 22, "endOffset": 105}, {"referenceID": 11, "context": "The work presented by Erkan and Radev (2004), as well as the work developed by Mihalcea and Tarau (2005), are examples of this approach. Erkan and Radev (2004) propose three graph-based approaches to pair-wise passage similarity-based summarization with similar performance: degree centrality, LexRank, and continuous LexRank.", "startOffset": 22, "endOffset": 160}, {"referenceID": 23, "context": "Mihalcea and Tarau (2005), in addition to Google\u2019s PageRank, also explore the HITS algorithm (Kleinberg, 1999) to perform graph-based extractive text summarization: again, documents are represented as networks of sentences and these networks are used to globally determine the importance of each sentence.", "startOffset": 93, "endOffset": 110}, {"referenceID": 1, "context": "A similar graph-based approach is described by Antiqueira et al. (2009). This work uses complex networks to perform extractive text summarization.", "startOffset": 47, "endOffset": 72}, {"referenceID": 36, "context": "The metric proposed by Mihalcea and Tarau (2004) has an unresolved issue: the denominator is 0 when comparing two equal sentences with length one (something that can happen when processing speech transcriptions).", "startOffset": 23, "endOffset": 49}, {"referenceID": 20, "context": "Instead, the Jaccard similarity coefficient (1901) could be used.", "startOffset": 13, "endOffset": 51}, {"referenceID": 11, "context": "In the degenerate case where all \u03b5i are equal, we fall into the degree centrality model proposed by Erkan and Radev (2004). But using, for instance, a n\u00e4\u0131ve approach of having dynamic thresholds (\u03b5i) set by limiting the cardinality of the support sets (a kNN approach), centrality is changed because each support set has only the most semantically related passages of each passage.", "startOffset": 100, "endOffset": 123}, {"referenceID": 11, "context": "In the degenerate case where all \u03b5i are equal, we fall into the degree centrality model proposed by Erkan and Radev (2004). But using, for instance, a n\u00e4\u0131ve approach of having dynamic thresholds (\u03b5i) set by limiting the cardinality of the support sets (a kNN approach), centrality is changed because each support set has only the most semantically related passages of each passage. From a graph theory perspective, this means that the underlying representation is not undirected, and the support set can be interpreted as the passages recommended by the passage associated to the support set. This contrasts with both LexRank models, which are based on undirected graphs. On the other hand, the models proposed by Mihalcea and Tarau (2005) are closer to our work in the sense that they explore directed graphs, although only in a simple way (graphs can only be directed forward or backward).", "startOffset": 100, "endOffset": 740}, {"referenceID": 42, "context": "In what concerns the definition of the weighting function f(ti, pj), several term weighting schemes have been explored in the literature\u2014for the analysis of the impact of different weighting schemes on either text or speech summarization see the work of Or\u0103san et al. (2004), and Murray and Renals (2007) or Ribeiro and de Matos (2008b), respectively.", "startOffset": 254, "endOffset": 275}, {"referenceID": 40, "context": "(2004), and Murray and Renals (2007) or Ribeiro and de Matos (2008b), respectively.", "startOffset": 12, "endOffset": 37}, {"referenceID": 40, "context": "(2004), and Murray and Renals (2007) or Ribeiro and de Matos (2008b), respectively.", "startOffset": 12, "endOffset": 69}, {"referenceID": 55, "context": "Nevertheless, this is in line with the work of Sahlgren (2006) that shows that in several tasks concerning term semantic relatedness, one of the most effective weighting schemes for small contexts is the binary term weighting scheme (Eq.", "startOffset": 47, "endOffset": 63}, {"referenceID": 55, "context": "As indicated by Sahlgren (2006), the meanings-are-locations metaphor is completely vacuous without the similarity-is-proximity metaphor.", "startOffset": 16, "endOffset": 32}, {"referenceID": 26, "context": "Examples are corpus-based vector space models of semantics (Turney & Pantel, 2010), like LSA (Landauer et al., 1998), Hyperspace Analogue to Language (Lund, Burgess, & Atchley, 1995), or Random Indexing (Kanerva, Kristoferson, & Holst, 2000; Kanerva & Sahlgren, 2001), or similarity metrics based on knowledge-rich semantic resources, such as WordNet (Fellbaum, 1998).", "startOffset": 93, "endOffset": 116}, {"referenceID": 14, "context": ", 1998), Hyperspace Analogue to Language (Lund, Burgess, & Atchley, 1995), or Random Indexing (Kanerva, Kristoferson, & Holst, 2000; Kanerva & Sahlgren, 2001), or similarity metrics based on knowledge-rich semantic resources, such as WordNet (Fellbaum, 1998).", "startOffset": 242, "endOffset": 258}, {"referenceID": 34, "context": "In contrast, Mihalcea and Tarau (2005) and Antiqueira et al.", "startOffset": 13, "endOffset": 39}, {"referenceID": 1, "context": "In contrast, Mihalcea and Tarau (2005) and Antiqueira et al. (2009) define passage similarity as content overlap.", "startOffset": 43, "endOffset": 68}, {"referenceID": 64, "context": "In this set of heuristics, we explore two weight functions (Zhu, 2005) (Eqs.", "startOffset": 59, "endOffset": 70}, {"referenceID": 48, "context": "Several evaluation models have been put forward in the last decade: beyond the long-established precision and recall (mostly useful when evaluating extractive summarization using also extractive summaries as models), literature is filled with metrics (some are automatic, others manual) like Relative utility (Radev et al., 2000; Radev & Tam, 2003), SummACCY (Hori, Hori, & Furui, 2003), ROUGE (Lin, 2004), VERT (de Oliveira, Torrens, Cidral, Schossland, & Bittencourt, 2008), or the Pyramid method (Nenkova, Passonneau, & McKeown, 2007).", "startOffset": 309, "endOffset": 348}, {"referenceID": 15, "context": ", 2000; Radev & Tam, 2003), SummACCY (Hori, Hori, & Furui, 2003), ROUGE (Lin, 2004), VERT (de Oliveira, Torrens, Cidral, Schossland, & Bittencourt, 2008), or the Pyramid method (Nenkova, Passonneau, & McKeown, 2007). For a more comprehensive analysis of the evaluation field see the work by Nenkova (2006) and Nenkova et al.", "startOffset": 52, "endOffset": 306}, {"referenceID": 15, "context": ", 2000; Radev & Tam, 2003), SummACCY (Hori, Hori, & Furui, 2003), ROUGE (Lin, 2004), VERT (de Oliveira, Torrens, Cidral, Schossland, & Bittencourt, 2008), or the Pyramid method (Nenkova, Passonneau, & McKeown, 2007). For a more comprehensive analysis of the evaluation field see the work by Nenkova (2006) and Nenkova et al. (2007). Despite the number of approaches to summary evaluation, the most widely used metric is still ROUGE and is the one we use in our study.", "startOffset": 52, "endOffset": 332}, {"referenceID": 37, "context": "\u2022 a set of graph-based summarizers presented by Mihalcea and Tarau (2005), namely PageRank Backward, HITSA Backward and HITSH Forward; \u2022 SuPor-v2 (Leite, Rino, Pardo, & Nunes, 2007), a classifier-based system that uses features like the occurrence of proper nouns, lexical chaining, and an ontology;", "startOffset": 48, "endOffset": 74}, {"referenceID": 28, "context": "\u2022 two modified versions of Mihalcea\u2019s PageRank Undirected, called TextRank + Thesaurus and TextRank + Stem + StopwordsRem(oval) presented by Leite et al. (2007); and,", "startOffset": 141, "endOffset": 161}, {"referenceID": 1, "context": "\u2022 several complex networks summarizers proposed by Antiqueira et al. (2009).", "startOffset": 51, "endOffset": 76}, {"referenceID": 11, "context": "\u2022 PageRank, proposed by both Mihalcea and Tarau (2004, 2005) and Erkan and Radev (2004) (passage similarity metrics differ and Mihalcea and Tarau also explore directed graphs);", "startOffset": 65, "endOffset": 88}, {"referenceID": 11, "context": "\u2022 Degree centrality as proposed by Erkan and Radev (2004) (we experimented with several thresholds \u03b4, ranging from 0.", "startOffset": 35, "endOffset": 58}, {"referenceID": 11, "context": "Additionally, given that the models proposed by Erkan and Radev (2004) use idf, we present some results (clearly identified) using both weighting schemes: using and not using idf.", "startOffset": 48, "endOffset": 71}, {"referenceID": 1, "context": "It is relevant to note that our model, which has low computational requirements, achieves results comparable to graph-based state-of-the-art systems (Ceylan, Mihalcea, \u00d6zertem, Lloret, & Palomar, 2010; Antiqueira et al., 2009).", "startOffset": 149, "endOffset": 226}, {"referenceID": 1, "context": "It is relevant to note that our model, which has low computational requirements, achieves results comparable to graph-based state-of-the-art systems (Ceylan, Mihalcea, \u00d6zertem, Lloret, & Palomar, 2010; Antiqueira et al., 2009). Notice that although the estimated confidence intervals overlap, the performance of the Manhattan SCC=2 variant is significantly better, using the directional Wilcoxon signed rank test with continuity correction, than the ones of TextRank Undirected, (W = 2584, p < 0.05), Uniform Influx (W = 2740, p < 0.05), and also Continuous LexRank (W = 2381.5, p < 0.1).4 The only variants of our model that perform below the baseline are the Fractional variants with N < 1. Fractional distances with N < 1, as can be seen by the effect of the metric on the unit circle (Figure 1), increase the distance between all passages, negatively influencing the construction of the support sets and, consequently the estimation of relevant content. Concerning the automatically set per passage thresholds, it is possible to observe that the best overall performance was achieved by a metric, Fractional N = 1.(3), with idf, using the heuristic based on the average difference between consecutive distances. For Cosine, Manhattan, Euclidean, and Minkowski variants, the heuristic based on the average distance (Cosine) and the heuristics based on passage order achieved results comparable to the best performing kNN approaches. For Chebyshev and Fractional (with N < 1) variants the best results were obtained using the heuristics based on the analysis of the progression of the distances. Figure 2 shows the improvements over the baseline and over the previous best-performing system. It is possible to perceive that the greatest performance jumps are introduced by Euclidean (10%) and Euclidean (H2.3), Minkowski (SSC=2), and the best-performing Manhattan, all instances of the support sets-based relevance model. Additionally, it is important to notice that the improvement of CN-Voting over the baseline (computed in the same conditions of CN-Voting) is of only 1%, having a performance worse than the poorest TextRank version which had an improvement over the baseline of 1.6%. In what concerns the linguistic knowledge-based systems (SuPor-2 and the enriched versions of TextRank Undirected), we cannot make an informed assessment of their performance since we cannot substantiate the used baseline, taken from the work of Mihalcea and Tarau (2005). Nonetheless, using that baseline, it is clear that linguistic information improves the performance of extractive summarizers beyond what we achieved with our model: improvements over the baseline range from 9% to 17.", "startOffset": 202, "endOffset": 2463}, {"referenceID": 5, "context": "One of the relevant issues that should be assessed is the level of agreement between the two human summarizers: this was accomplished using the kappa coefficient (Carletta, 1996), for which we obtained a value of 0.", "startOffset": 162, "endOffset": 178}, {"referenceID": 11, "context": "Approaches based on generation probabilities seem more adequate to larger contexts, such as documents (Kurland & Lee, 2005, 2010; Erkan, 2006a).", "startOffset": 102, "endOffset": 143}, {"referenceID": 11, "context": "Approaches based on generation probabilities seem more adequate to larger contexts, such as documents (Kurland & Lee, 2005, 2010; Erkan, 2006a). Erkan (2006b) mentions that results in query-based summarization using generation probabilities were worse than the ones obtained by LexRank in generic summarization.", "startOffset": 130, "endOffset": 159}, {"referenceID": 30, "context": "Further, comparing our model to more complex (not centrality-based), state-of-the-art models like the one presented by Lin et al. (2010) suggests that at least similar performance is attained: the relative performance increment of our model over LexRank is of 57.", "startOffset": 119, "endOffset": 137}, {"referenceID": 1, "context": "The number of up-to-date examples of work on automatic summarization using centralitybased relevance models is significant (Garg, Favre, Reidhammer, & Hakkani-T\u00fcr, 2009; Antiqueira et al., 2009; Ceylan et al., 2010; Wan, Li, & Xiao, 2010).", "startOffset": 123, "endOffset": 238}, {"referenceID": 6, "context": "The number of up-to-date examples of work on automatic summarization using centralitybased relevance models is significant (Garg, Favre, Reidhammer, & Hakkani-T\u00fcr, 2009; Antiqueira et al., 2009; Ceylan et al., 2010; Wan, Li, & Xiao, 2010).", "startOffset": 123, "endOffset": 238}, {"referenceID": 1, "context": "The number of up-to-date examples of work on automatic summarization using centralitybased relevance models is significant (Garg, Favre, Reidhammer, & Hakkani-T\u00fcr, 2009; Antiqueira et al., 2009; Ceylan et al., 2010; Wan, Li, & Xiao, 2010). In our work, we assessed the main approaches of the centrality-as-relevance paradigm, and introduced a new centrality-based relevance model for automatic summarization. Our model uses support sets to better characterize the information sources to be summarized, leading to a better estimation of the relevant content. In fact, we assume that input sources comprehend several topics that are uncovered by associating to each passage a support set composed by the most semantically related passages. Building on the ideas of Ruge (1992), [.", "startOffset": 170, "endOffset": 775}], "year": 2011, "abstractText": "In automatic summarization, centrality-as-relevance means that the most important content of an information source, or a collection of information sources, corresponds to the most central passages, considering a representation where such notion makes sense (graph, spatial, etc.). We assess the main paradigms, and introduce a new centrality-based relevance model for automatic summarization that relies on the use of support sets to better estimate the relevant content. Geometric proximity is used to compute semantic relatedness. Centrality (relevance) is determined by considering the whole input source (and not only local information), and by taking into account the existence of minor topics or lateral subjects in the information sources to be summarized. The method consists in creating, for each passage of the input source, a support set consisting only of the most semantically related passages. Then, the determination of the most relevant content is achieved by selecting the passages that occur in the largest number of support sets. This model produces extractive summaries that are generic, and languageand domainindependent. Thorough automatic evaluation shows that the method achieves state-of-theart performance, both in written text, and automatically transcribed speech summarization, including when compared to considerably more complex approaches.", "creator": "TeX"}}}